{"id": "2508.13681", "pdf": "https://arxiv.org/pdf/2508.13681", "abs": "https://arxiv.org/abs/2508.13681", "authors": ["Ladan Khaloopour", "Matthias Hollick", "Vahid Jamali"], "title": "CKM-Assisted Physical-Layer Security for Resilience Against Unknown Eavesdropping Location", "categories": ["eess.SP"], "comment": null, "summary": "Channel Knowledge Map (CKM) is an emerging data-driven toolbox that captures\nour awareness of the wireless channel and enables efficient communication and\nresource allocation beyond the state of the art. In this work, we consider CKM\nfor improving physical-layer security (PLS) in the presence of a passive\neavesdropper (Eve), without making any assumptions about Eve's location or\nchannel state information (CSI). We employ highly directional mmWave\ntransmissions, with the confidential message jointly encoded across multiple\nbeams. By exploiting CKM, we derive an algorithm for time and power allocation\namong the beams that maximizes the absolute secrecy rate under the worst-case\nscenario for Eve's location.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u4fe1\u9053\u77e5\u8bc6\u5730\u56fe\uff08CKM\uff09\u4f18\u5316\u6beb\u7c73\u6ce2\u591a\u6ce2\u675f\u4f20\u8f93\uff0c\u63d0\u5347\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u65e0\u9700\u7a83\u542c\u8005\u4f4d\u7f6e\u6216\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7CKM\u63d0\u5347\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u7a83\u542c\u8005\u4f4d\u7f6e\u548c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u672a\u77e5\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9ad8\u5b9a\u5411\u6beb\u7c73\u6ce2\u4f20\u8f93\uff0c\u5728\u591a\u4e2a\u6ce2\u675f\u4e0a\u8054\u5408\u7f16\u7801\u673a\u5bc6\u4fe1\u606f\uff0c\u5229\u7528CKM\u8bbe\u8ba1\u65f6\u9699\u548c\u529f\u7387\u5206\u914d\u7b97\u6cd5\u6700\u5927\u5316\u7edd\u5bf9\u4fdd\u5bc6\u7387\u3002", "result": "\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\uff08\u7a83\u542c\u8005\u4f4d\u7f6e\u672a\u77e5\uff09\u6210\u529f\u4f18\u5316\u4e86\u4fdd\u5bc6\u7387\u3002", "conclusion": "CKM\u53ef\u4f5c\u4e3a\u9ad8\u6548\u5de5\u5177\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u5b89\u5168\u6027\uff0c\u5c24\u5176\u5728\u6beb\u7c73\u6ce2\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2508.13714", "pdf": "https://arxiv.org/pdf/2508.13714", "abs": "https://arxiv.org/abs/2508.13714", "authors": ["Donatella Darsena", "Francesco Verde", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Airy beams for near-field communications: Fundamentals, potentials, and limitations", "categories": ["eess.SP"], "comment": "28 pages, 29 figures", "summary": "In next-generation wireless networks, the combination of electrically large\nradiating apertures and high-frequency transmission extends the radiating\nnear-field region around the transmitter. In this region, unlike in the far\nfield, the wavefront is nonplanar, which provides additional degrees of freedom\nto shape and steer the transmitted beam in a desired manner. In this paper, we\nfocus on Airy beams, which may exhibit several highly desirable properties in\nthe near-field region. Ideally, these beams follow self-accelerating (curved)\ntrajectories, demonstrate resilience to perturbations through self-healing, and\nmaintain a consistent intensity profile across all planes perpendicular to the\npropagation direction, making them effectively diffraction-free. Specifically,\nwe first present the underlying principles of self-accelerating beams radiated\nby continuous aperture field distributions. We then address several challenges\nregarding the generation of Airy beams, including their exponential decay due\nto finite energy constraints and spatial truncation of the aperture. Moreover,\nwe examine their free-space propagation characteristics. The second part of the\npaper focuses on the propagation behavior of Airy beams in non-line-of-sight\n(NLoS) scenarios. A comparison is also presented between Airy beams and\nGaussian beams. Our theoretical and numerical results show that Airy beams may\noffer a performance advantage over Gaussian beams in certain NLoS channels,\nprovided that their key properties are largely preserved, specifically,\nself-acceleration along a parabolic trajectory and diffraction-free\npropagation. In the presence of an obstacle, this requires that the portion of\nthe transmit aperture with a clear line-of-sight to the receiver is\nsufficiently large.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8fd1\u573a\u533a\u57dfAiry\u5149\u675f\u7684\u751f\u6210\u3001\u4f20\u64ad\u7279\u6027\u53ca\u5176\u5728\u975e\u89c6\u8ddd\uff08NLoS\uff09\u573a\u666f\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u8fd1\u573a\u533a\u57df\u7684\u975e\u5e73\u9762\u6ce2\u524d\u4e3a\u5149\u675f\u5f62\u72b6\u548c\u65b9\u5411\u7684\u8c03\u63a7\u63d0\u4f9b\u4e86\u989d\u5916\u81ea\u7531\u5ea6\uff0cAiry\u5149\u675f\u56e0\u5176\u81ea\u52a0\u901f\u3001\u81ea\u6108\u548c\u884d\u5c04\u81ea\u7531\u7279\u6027\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8fde\u7eed\u5b54\u5f84\u573a\u5206\u5e03\u751f\u6210Airy\u5149\u675f\u7684\u539f\u7406\uff0c\u63a2\u8ba8\u5176\u751f\u6210\u6311\u6218\uff08\u5982\u6709\u9650\u80fd\u91cf\u7ea6\u675f\u548c\u5b54\u5f84\u622a\u65ad\uff09\u3002\u6bd4\u8f83\u5206\u6790\u4e86Airy\u5149\u675f\u4e0e\u9ad8\u65af\u5149\u675f\u5728\u975e\u89c6\u8ddd\u573a\u666f\u4e2d\u7684\u4f20\u64ad\u884c\u4e3a\u3002", "result": "\u7406\u8bba\u53ca\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cAiry\u5149\u675f\u5728\u4fdd\u6301\u5173\u952e\u7279\u6027\uff08\u5982\u81ea\u52a0\u901f\u548c\u884d\u5c04\u81ea\u7531\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u7279\u5b9aNLoS\u4fe1\u9053\u4e2d\u6027\u80fd\u4f18\u4e8e\u9ad8\u65af\u5149\u675f\u3002", "conclusion": "Airy\u5149\u675f\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u5907\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u975e\u89c6\u8ddd\u573a\u666f\u4e2d\uff0c\u4f46\u9700\u4fdd\u8bc1\u53d1\u5c04\u5b54\u5f84\u8db3\u591f\u5927\u4ee5\u7ef4\u6301\u5176\u7279\u6027\u3002"}}
{"id": "2508.13771", "pdf": "https://arxiv.org/pdf/2508.13771", "abs": "https://arxiv.org/abs/2508.13771", "authors": ["Mustafa S. Abbas", "Zahra Mobini", "Hien Quoc Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free Massive MIMO", "categories": ["eess.SP"], "comment": null, "summary": "Joint unicast and multicast transmissions are becoming increasingly important\nin practical wireless systems, such as Internet of Things networks. This paper\ninvestigates a cell-free massive multiple-input multiple-output system that\nsimultaneously supports both transmission types, with multicast serving\nmultiple groups. Exact closed-form expressions for the achievable downlink\nspectral efficiency (SE) of both unicast and multicast users are derived for\nzero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum\nSE (SSE) maximization problem is formulated to jointly optimize the access\npoint (AP) selection and power allocation. The optimization framework accounts\nfor practical constraints, including the maximum transmit power per AP,\nfronthaul capacity limitations between APs and the central processing unit, and\nquality-of-service requirements for all users. The resulting non-convex\noptimization problem is reformulated into a tractable structure, and an\naccelerated projected gradient (APG)-based algorithm is developed to\nefficiently obtain near-optimal solutions. As a performance benchmark, a\nsuccessive convex approximation (SCA)-based algorithm is also implemented.\nSimulation results demonstrate that the proposed joint optimization approach\nsignificantly enhances the SSE across various system setups and precoding\nstrategies. In particular, the APG-based algorithm achieves substantial\ncomplexity reduction while maintaining competitive performance, making it\nwell-suited for large-scale practical deployments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u8054\u5408\u5355\u64ad\u548c\u591a\u64ad\u4f20\u8f93\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u901f\u6295\u5f71\u68af\u5ea6\u7684\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u7f51\u7edc\u7b49\u65e0\u7ebf\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u8054\u5408\u5355\u64ad\u548c\u591a\u64ad\u4f20\u8f93\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f18\u5316\u6b64\u7c7b\u7cfb\u7edf\u7684\u9891\u8c31\u6548\u7387\u548c\u8d44\u6e90\u5206\u914d\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u5355\u64ad\u548c\u591a\u64ad\u7528\u6237\u7684\u9891\u8c31\u6548\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a0\u6743\u548c\u9891\u8c31\u6548\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u901f\u6295\u5f71\u68af\u5ea6\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u7cfb\u7edf\u8bbe\u7f6e\u548c\u9884\u7f16\u7801\u7b56\u7565\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9891\u8c31\u6548\u7387\uff0cAPG\u7b97\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "conclusion": "\u8bba\u6587\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\uff0c\u5c24\u5176\u662fAPG\u7b97\u6cd5\u5728\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2508.13818", "pdf": "https://arxiv.org/pdf/2508.13818", "abs": "https://arxiv.org/abs/2508.13818", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Wanting Lyu", "Dusit Niyato", "Dong In Kim", "Guangyi Liu", "Ning Wei"], "title": "Robust Optimization for Movable Antenna-aided Cell-Free ISAC with Time Synchronization Errors", "categories": ["eess.SP"], "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) system, which\neffectively mitigates intra-cell interference and provides precise sensing\naccuracy, is a promising technology for future 6G networks. However, to fully\ncapitalize on the potential of CF-ISAC, accurate time synchronization (TS)\nbetween access points (APs) is critical. Due to the limitations of current\nsynchronization technologies, TS errors have become a significant challenge in\nthe development of the CF-ISAC system. In this paper, we propose a novel\nCF-ISAC architecture based on movable antennas (MAs), which exploits spatial\ndiversity to enhance communication rates, maintain sensing accuracy, and reduce\nthe impact of TS errors. We formulate a worst-case sensing accuracy\noptimization problem for TS errors to address this challenge, deriving the\nworst-case Cram\\'er-Rao lower bound (CRLB). Subsequently, we develop a joint\noptimization framework for AP beamforming and MA positions to satisfy\ncommunication rate constraints while improving sensing accuracy. A robust\noptimization framework is designed for the highly complex and non-convex\nproblem. Specifically, we employ manifold optimization (MO) to solve the\nworst-case sensing accuracy optimization problem. Then, we propose an\nMA-enabled meta-reinforcement learning (MA-MetaRL) to design optimization\nvariables while satisfying constraints on MA positions, communication rate, and\ntransmit power, thereby improving sensing accuracy. The simulation results\ndemonstrate that the proposed robust optimization algorithm significantly\nimproves the accuracy of the detection and is strong against TS errors.\nMoreover, compared to conventional fixed position antenna (FPA) technologies,\nthe proposed MA-aided CF-ISAC architecture achieves higher system capacity,\nthus validating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684CF-ISAC\u67b6\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4\u591a\u6837\u6027\u589e\u5f3a\u901a\u4fe1\u901f\u7387\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u964d\u4f4e\u65f6\u95f4\u540c\u6b65\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u540c\u6b65\u8bef\u5dee\u5df2\u6210\u4e3aCF-ISAC\u7cfb\u7edf\u53d1\u5c55\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u67b6\u6784\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6d41\u5f62\u4f18\u5316\uff08MO\uff09\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u611f\u77e5\u7cbe\u5ea6\u4f18\u5316\u95ee\u9898\u5efa\u6a21\uff0c\u7ed3\u5408MA-MetaRL\u7b97\u6cd5\u8bbe\u8ba1\u4f18\u5316\u53d8\u91cf\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u5bf9\u65f6\u95f4\u540c\u6b65\u8bef\u5dee\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u7cfb\u7edf\u5bb9\u91cf\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u6280\u672f\u3002", "conclusion": "\u57fa\u4e8eMA\u7684CF-ISAC\u67b6\u6784\u5728\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u540c\u6b65\u8bef\u5dee\u95ee\u9898\u3002"}}
{"id": "2508.13303", "pdf": "https://arxiv.org/pdf/2508.13303", "abs": "https://arxiv.org/abs/2508.13303", "authors": ["Yingfan Zhou", "Philip Sanderink", "Sigurd Jager Lemming", "Cheng Fang"], "title": "Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "High-fidelity personalized human musculoskeletal models are crucial for\nsimulating realistic behavior of physically coupled human-robot interactive\nsystems and verifying their safety-critical applications in simulations before\nactual deployment, such as human-robot co-transportation and rehabilitation\nthrough robotic exoskeletons. Identifying subject-specific Hill-type muscle\nmodel parameters and bone dynamic parameters is essential for a personalized\nmusculoskeletal model, but very challenging due to the difficulty of measuring\nthe internal biomechanical variables in vivo directly, especially the joint\ntorques. In this paper, we propose using Differentiable MusculoSkeletal Model\n(Diff-MSM) to simultaneously identify its muscle and bone parameters with an\nend-to-end automatic differentiation technique differentiating from the\nmeasurable muscle activation, through the joint torque, to the resulting\nobservable motion without the need to measure the internal joint torques.\nThrough extensive comparative simulations, the results manifested that our\nproposed method significantly outperformed the state-of-the-art baseline\nmethods, especially in terms of accurate estimation of the muscle parameters\n(i.e., initial guess sampled from a normal distribution with the mean being the\nground truth and the standard deviation being 10% of the ground truth could end\nup with an average of the percentage errors of the estimated values as low as\n0.05%). In addition to human musculoskeletal modeling and simulation, the new\nparameter identification technique with the Diff-MSM has great potential to\nenable new applications in muscle health monitoring, rehabilitation, and sports\nscience.", "AI": {"tldr": "\u63d0\u51fa\u4e86Diff-MSM\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u6280\u672f\u540c\u65f6\u8bc6\u522b\u808c\u8089\u548c\u9aa8\u9abc\u53c2\u6570\uff0c\u65e0\u9700\u76f4\u63a5\u6d4b\u91cf\u5173\u8282\u626d\u77e9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e2a\u6027\u5316\u4eba\u4f53\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u5bf9\u6a21\u62df\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u6d4b\u91cf\u5185\u90e8\u751f\u7269\u529b\u5b66\u53c2\u6570\uff08\u5982\u5173\u8282\u626d\u77e9\uff09\u56f0\u96be\u3002", "method": "\u4f7f\u7528Diff-MSM\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u6280\u672f\u4ece\u53ef\u6d4b\u91cf\u7684\u808c\u8089\u6fc0\u6d3b\u5230\u89c2\u6d4b\u8fd0\u52a8\u7684\u7aef\u5230\u7aef\u8bc6\u522b\u808c\u8089\u548c\u9aa8\u9abc\u53c2\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cDiff-MSM\u5728\u808c\u8089\u53c2\u6570\u4f30\u8ba1\u4e0a\u8bef\u5dee\u4f4e\u81f30.05%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Diff-MSM\u4e0d\u4ec5\u63d0\u5347\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u7cbe\u5ea6\uff0c\u8fd8\u5728\u808c\u8089\u5065\u5eb7\u76d1\u6d4b\u3001\u5eb7\u590d\u548c\u8fd0\u52a8\u79d1\u5b66\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.13839", "pdf": "https://arxiv.org/pdf/2508.13839", "abs": "https://arxiv.org/abs/2508.13839", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Zheng Dong", "Wanting Lyu", "Zeyuan Zhang", "Dusit Niyato", "Guangyi Liu", "Ning Wei"], "title": "Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems", "categories": ["eess.SP"], "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) architecture is\na promising enabler for 6G, offering spectrum efficiency and ubiquitous\ncoverage. However, real deployments suffer from hardware impairments,\nespecially nonlinear distortion from power amplifiers (PAs), which degrades\nboth communication and sensing. To address this, we propose a movable antenna\n(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.\nThe PAs nonlinearities are modeled by a third-order memoryless polynomial,\nwhere the third-order distortion coefficients (3RDCs) vary across access points\n(APs) due to hardware differences, aging, and environmental conditions. We\ndesign a distributed distortion-aware worst-case robust optimization framework\nthat explicitly incorporates uncertainty in 3RDCs. First, we analyze the\nworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)\nand communication rate. Then, to address the resulting non-convexity, we apply\nsuccessive convex approximation (SCA) for estimating the 3RDCs. With these, we\njointly optimize beamforming and MA positions under transmit power and sensing\nconstraints. To efficiently solve this highly non-convex problem, we develop an\nMA-enabled self-attention convolutional graph neural network (SACGNN)\nalgorithm. Simulations demonstrate that our method substantially enhances the\ncommunication-sensing trade-off under distortion and outperforms fixed-position\nantenna baselines in terms of robustness and capacity, thereby highlighting the\nadvantages of MA-aided CF-ISAC systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684CF-ISAC\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u529f\u7387\u653e\u5927\u5668\u5931\u771f\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u9c81\u68d2\u4f18\u5316\u548cSACGNN\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u7684CF-ISAC\u67b6\u6784\u9762\u4e34\u786c\u4ef6\u975e\u7ebf\u6027\u5931\u771f\u95ee\u9898\uff0c\u5f71\u54cd\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7MA\u548c\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7b2c\u4e09\u9636\u65e0\u8bb0\u5fc6\u591a\u9879\u5f0f\u5efa\u6a21PA\u975e\u7ebf\u6027\uff0c\u8bbe\u8ba1\u5206\u5e03\u5f0f\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u5f00\u53d1SACGNN\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548cMA\u4f4d\u7f6e\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5931\u771f\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1-\u611f\u77e5\u6743\u8861\uff0c\u5e76\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u57fa\u51c6\u3002", "conclusion": "MA\u8f85\u52a9\u7684CF-ISAC\u7cfb\u7edf\u5728\u786c\u4ef6\u5931\u771f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a6G\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13319", "pdf": "https://arxiv.org/pdf/2508.13319", "abs": "https://arxiv.org/abs/2508.13319", "authors": ["Kshitij Kavimandan", "Pooja Mangal", "Devanshi Mehta"], "title": "A Surveillance Based Interactive Robot", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.7"], "comment": "4 pages, 5 figures", "summary": "We build a mobile surveillance robot that streams video in real time and\nresponds to speech so a user can monitor and steer it from a phone or browser.\nThe system uses two Raspberry Pi 4 units: a front unit on a differential drive\nbase with camera, mic, and speaker, and a central unit that serves the live\nfeed and runs perception. Video is sent with FFmpeg. Objects in the scene are\ndetected using YOLOv3 to support navigation and event awareness. For voice\ninteraction, we use Python libraries for speech recognition, multilingual\ntranslation, and text-to-speech, so the robot can take spoken commands and read\nback responses in the requested language. A Kinect RGB-D sensor provides visual\ninput and obstacle cues. In indoor tests the robot detects common objects at\ninteractive frame rates on CPU, recognises commands reliably, and translates\nthem to actions without manual control. The design relies on off-the-shelf\nhardware and open software, making it easy to reproduce. We discuss limits and\npractical extensions, including sensor fusion with ultrasonic range data, GPU\nacceleration, and adding face and text recognition.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\uff0c\u5b9e\u65f6\u89c6\u9891\u6d41\u4e0e\u8bed\u97f3\u4ea4\u4e92\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u624b\u673a\u6216\u6d4f\u89c8\u5668\u76d1\u63a7\u548c\u64cd\u63a7\u3002", "motivation": "\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u590d\u5236\u7684\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\uff0c\u5177\u5907\u5b9e\u65f6\u89c6\u9891\u4f20\u8f93\u548c\u8bed\u97f3\u4ea4\u4e92\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u73af\u5883\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6811\u8393\u6d3e4\u5355\u5143\uff0c\u524d\u7aef\u5355\u5143\u914d\u5907\u6444\u50cf\u5934\u3001\u9ea6\u514b\u98ce\u548c\u626c\u58f0\u5668\uff0c\u4e2d\u592e\u5355\u5143\u8d1f\u8d23\u89c6\u9891\u6d41\u548c\u611f\u77e5\u5904\u7406\u3002\u91c7\u7528FFmpeg\u4f20\u8f93\u89c6\u9891\uff0cYOLOv3\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0cPython\u5e93\u5b9e\u73b0\u8bed\u97f3\u8bc6\u522b\u4e0e\u591a\u8bed\u8a00\u4ea4\u4e92\uff0cKinect\u4f20\u611f\u5668\u63d0\u4f9b\u89c6\u89c9\u548c\u969c\u788d\u7269\u4fe1\u606f\u3002", "result": "\u5728\u5ba4\u5185\u6d4b\u8bd5\u4e2d\uff0c\u673a\u5668\u4eba\u80fd\u4ee5\u4ea4\u4e92\u5e27\u7387\u68c0\u6d4b\u5e38\u89c1\u7269\u4f53\uff0c\u53ef\u9760\u8bc6\u522b\u8bed\u97f3\u6307\u4ee4\u5e76\u7ffb\u8bd1\u4e3a\u52a8\u4f5c\uff0c\u65e0\u9700\u624b\u52a8\u63a7\u5236\u3002", "conclusion": "\u7cfb\u7edf\u57fa\u4e8e\u73b0\u6210\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\uff0c\u6613\u4e8e\u590d\u5236\u3002\u672a\u6765\u53ef\u6269\u5c55\u5305\u62ec\u4f20\u611f\u5668\u878d\u5408\u3001GPU\u52a0\u901f\u53ca\u4eba\u8138\u548c\u6587\u672c\u8bc6\u522b\u3002"}}
{"id": "2508.13937", "pdf": "https://arxiv.org/pdf/2508.13937", "abs": "https://arxiv.org/abs/2508.13937", "authors": ["Halim Lee", "Jongmin Park", "Kwansik Park"], "title": "Evaluating Particle Filtering for RSS-Based Target Localization under Varying Noise Levels and Sensor Geometries", "categories": ["eess.SP"], "comment": null, "summary": "Target localization is a critical task in various applications, such as\nsearch and rescue, surveillance, and wireless sensor networks. When a target\nemits a radio frequency (RF) signal, spatially distributed sensors can collect\nsignal measurements to estimate the target's location. Among various\nmeasurement modalities, received signal strength (RSS) is particularly\nattractive due to its low cost, low power consumption, and ease of deployment.\nWhile particle filtering has previously been applied to RSS-based target\nlocalization, few studies have systematically analyzed its performance under\nvarying sensor geometries and RSS noise levels. This paper addresses this gap\nby designing and evaluating a particle filtering algorithm for localizing a\nstationary target. The proposed method is compared with a conventional\nRSS-based trilateration approach across different sensor configurations and\nnoise conditions. Simulation results indicate that particle filtering provides\nmore accurate target localization than trilateration, particularly in scenarios\nwith unfavorable sensor geometries and high RSS noise.", "AI": {"tldr": "\u7c92\u5b50\u6ee4\u6ce2\u5728RSS\u76ee\u6807\u5b9a\u4f4d\u4e2d\u7684\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4e09\u89d2\u6d4b\u91cf\uff0c\u5c24\u5176\u5728\u4f20\u611f\u5668\u51e0\u4f55\u5206\u5e03\u4e0d\u7406\u60f3\u548c\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u76ee\u6807\u5b9a\u4f4d\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u7c92\u5b50\u6ee4\u6ce2\u5728RSS\u76ee\u6807\u5b9a\u4f4d\u4e2d\u6027\u80fd\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d\u9759\u6b62\u76ee\u6807\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684RSS\u4e09\u89d2\u6d4b\u91cf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7c92\u5b50\u6ee4\u6ce2\u6bd4\u4e09\u89d2\u6d4b\u91cf\u66f4\u51c6\u786e\uff0c\u5c24\u5176\u662f\u5728\u4f20\u611f\u5668\u51e0\u4f55\u5206\u5e03\u4e0d\u7406\u60f3\u548c\u9ad8\u566a\u58f0\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u7c92\u5b50\u6ee4\u6ce2\u662fRSS\u76ee\u6807\u5b9a\u4f4d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2508.13392", "pdf": "https://arxiv.org/pdf/2508.13392", "abs": "https://arxiv.org/abs/2508.13392", "authors": ["Sidharth Talia", "Oren Salzman", "Siddhartha Srinivasa"], "title": "Incremental Generalized Hybrid A*", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "We address the problem of efficiently organizing search over very large\ntrees, which arises in many applications ranging from autonomous driving to\naerial vehicles. Here, we are motivated by off-road autonomy, where real-time\nplanning is essential. Classical approaches use graphs of motion primitives and\nexploit dominance to mitigate the curse of dimensionality and prune expansions\nefficiently. However, for complex dynamics, repeatedly solving two-point\nboundary-value problems makes graph construction too slow for fast kinodynamic\nplanning. Hybrid A* (HA*) addressed this challenge by searching over a tree of\nmotion primitives and introducing approximate pruning using a grid-based\ndominance check. However, choosing the grid resolution is difficult: too coarse\nrisks failure, while too fine leads to excessive expansions and slow planning.\nWe propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search\nframework that dynamically organizes vertex expansions without rigid pruning.\nIGHA* provably matches or outperforms HA*. For both on-road kinematic and\noff-road kinodynamic planning queries for a car-like robot, variants of IGHA*\nuse 6x fewer expansions to the best solution compared to an optimized version\nof HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*\noutperforms HA*M when both are used in the loop with a model predictive\ncontroller. We demonstrate real-time performance both in simulation and on a\nsmall-scale off-road vehicle, enabling fast, robust planning under complex\ndynamics. Code: https://github.com/personalrobotics/IGHAStar", "AI": {"tldr": "IGHA*\u662f\u4e00\u79cd\u52a8\u6001\u7ec4\u7ec7\u9876\u70b9\u6269\u5c55\u7684\u6811\u641c\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8def\u5f84\u89c4\u5212\uff0c\u6bd4HA*\u51cf\u5c116\u500d\u7684\u6269\u5c55\u6b21\u6570\uff0c\u5728\u590d\u6742\u52a8\u529b\u5b66\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u52a8\u529b\u5b66\u4e0b\uff0c\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff08\u5982HA*\uff09\u56e0\u7f51\u683c\u5206\u8fa8\u7387\u9009\u62e9\u56f0\u96be\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u5f15\u5165\u589e\u91cf\u5e7f\u4e49\u6df7\u5408A*\uff08IGHA*\uff09\uff0c\u52a8\u6001\u7ec4\u7ec7\u9876\u70b9\u6269\u5c55\u800c\u4e0d\u4f9d\u8d56\u521a\u6027\u4fee\u526a\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u529b\u5b66\u8def\u5f84\u89c4\u5212\u3002", "result": "IGHA*\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u4f18\u5316\u7248HA*\uff0c\u6269\u5c55\u6b21\u6570\u51cf\u5c116\u500d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "IGHA*\u663e\u8457\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u529b\u5b66\u573a\u666f\uff0c\u53ef\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.13181", "pdf": "https://arxiv.org/pdf/2508.13181", "abs": "https://arxiv.org/abs/2508.13181", "authors": ["Dominik Loroch", "Johannes Feldmann", "Vladimir Rybalkin", "Norbert Wehn"], "title": "Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices", "categories": ["cs.AR", "eess.SP"], "comment": "2025 IEEE 38th International System-on-Chip Conference (SOCC)", "summary": "Atrial fibrillation (AF) is a common arrhythmia and major risk factor for\ncardiovascular complications. While commercially available devices and\nsupporting Artificial Intelligence (AI) algorithms exist for reliable detection\nof AF, the scaling of this technology to the amount of people who need this\ndiagnosis is still a major challenge. This paper presents a novel wearable\ndevice, designed specifically for the early and reliable detection of AF. We\npresent an FPGA-based patch-style wearable monitor with embedded deep\nlearning-based AF detection. Operating with 3.8mW system power, which is 1-3\norders of magnitude lower than the state-of-the-art, the device enables\ncontinuous AF detection for over three weeks while achieving 95% accuracy,\nsurpassing cardiologist-level performance. A key innovation is the combination\nof energy-efficient hardware-software co-design and optimized power management\nthrough the application of hardware-aware neural architecture search. This\nadvancement represents a significant step toward scalable, reliable, and\nsustainable AF monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u901a\u8fc7FPGA\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u9ad8\u7cbe\u5ea6\u7684\u623f\u98a4\u68c0\u6d4b\uff0c\u6027\u80fd\u8d85\u8d8a\u4e13\u5bb6\u6c34\u5e73\u3002", "motivation": "\u623f\u98a4\uff08AF\uff09\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u4f46\u5176\u68c0\u6d4b\u6280\u672f\u7684\u666e\u53ca\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528FPGA\u786c\u4ef6\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u786c\u4ef6\u611f\u77e5\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4f18\u5316\u529f\u8017\u7ba1\u7406\u3002", "result": "\u8bbe\u5907\u529f\u8017\u4f4e\u81f33.8mW\uff0c\u53ef\u8fde\u7eed\u8fd0\u884c\u4e09\u5468\u4ee5\u4e0a\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe95%\uff0c\u4f18\u4e8e\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "\u8be5\u8bbe\u5907\u4e3a\u623f\u98a4\u76d1\u6d4b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2508.13407", "pdf": "https://arxiv.org/pdf/2508.13407", "abs": "https://arxiv.org/abs/2508.13407", "authors": ["Jiming Ren", "Xuan Lin", "Roman Mineyev", "Karen M. Feigh", "Samuel Coogan", "Ye Zhao"], "title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition", "categories": ["cs.RO"], "comment": "16 pages, 7 figures, 6 tables", "summary": "Task and motion planning under Signal Temporal Logic constraints is known to\nbe NP-hard. A common class of approaches formulates these hybrid problems,\nwhich involve discrete task scheduling and continuous motion planning, as\nmixed-integer programs (MIP). However, in applications for bipedal locomotion,\nintroduction of non-convex constraints such as kinematic reachability and\nfootstep rotation exacerbates the computational complexity of MIPs. In this\nwork, we present a method based on Benders Decomposition to address scenarios\nwhere solving the entire monolithic optimization problem is prohibitively\nintractable. Benders Decomposition proposes an iterative cutting-plane\ntechnique that partitions the problem into a master problem to prototype a plan\nthat meets the task specification, and a series of subproblems for kinematics\nand dynamics feasibility checks. Our experiments demonstrate that this method\nachieves faster planning compared to alternative algorithms for solving the\nresulting optimization program with nonlinear constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBenders\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u5207\u5272\u5e73\u9762\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u901f\u5ea6\u3002", "motivation": "\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\u56e0\u975e\u51f8\u7ea6\u675f\uff08\u5982\u8fd0\u52a8\u53ef\u8fbe\u6027\u548c\u6b65\u6001\u65cb\u8f6c\uff09\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u6c42\u89e3\u3002", "method": "\u91c7\u7528Benders\u5206\u89e3\u6280\u672f\uff0c\u5c06\u95ee\u9898\u5206\u4e3a\u4e3b\u95ee\u9898\uff08\u4efb\u52a1\u89c4\u5212\uff09\u548c\u5b50\u95ee\u9898\uff08\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u53ef\u884c\u6027\u68c0\u67e5\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u5207\u5272\u5e73\u9762\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u7ea6\u675f\u65f6\u6bd4\u4f20\u7edf\u7b97\u6cd5\u89c4\u5212\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "Benders\u5206\u89e3\u4e3a\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13521", "pdf": "https://arxiv.org/pdf/2508.13521", "abs": "https://arxiv.org/abs/2508.13521", "authors": ["Ahasan Ahamed", "Htet Myat", "Amita Rawat", "Lisa N McPhillips", "M Saif Islam"], "title": "AI-Augmented Photon-Trapping Spectrometer-on-a-Chip on Silicon Platform with Extended Near-Infrared Sensitivity", "categories": ["physics.optics", "eess.SP", "physics.comp-ph"], "comment": "Article: 24 pages, 8 figures, 1 table Supplementary: 8 pages, 6\n  figures, 1 table Submitted to Advanced Photonics (SPIE), currently under\n  review", "summary": "We present a compact, noise-resilient reconstructive spectrometer-on-a-chip\nthat achieves high-resolution hyperspectral imaging across an extended\nnear-infrared (NIR) range up to 1100nm. The device integrates monolithically\nfabricated silicon photodiodes enhanced with photon-trapping surface textures\n(PTST), enabling improved responsivity in the low-absorption NIR regime.\nLeveraging a fully connected neural network, we demonstrate accurate spectral\nreconstruction from only 16 uniquely engineered detectors, achieving <0.05 RMSE\nand 8nm resolution over a wide spectral range of 640nm to 1100nm. Our system\noutperforms conventional spectrometers, maintaining signal-to-noise ratio above\n30dB even with 40dB of added detector noise; extending functionality to longer\nwavelengths up to 1100nm, while the traditional spectrometers fail to perform\nbeyond 950nm due to poor detector efficiency and noise performance. With a\nfootprint of 0.4mm2, dynamic range of 50dB, ultrafast time response (57ps), and\nhigh photodiode gain (>7000), this AI-augmented silicon spectrometer is\nwell-suited for portable, real-time, and low-light applications in biomedical\nimaging, environmental monitoring, and remote sensing. The results establish a\npathway toward fully integrated, high-performance hyperspectral sensing in a\nCMOS-compatible platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u6297\u566a\u58f0\u7684\u7247\u4e0a\u91cd\u6784\u5149\u8c31\u4eea\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe1100nm\u7684\u8fd1\u7ea2\u5916\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u901a\u8fc7AI\u589e\u5f3a\u548c\u5149\u5b50\u9631\u7eb9\u7406\u6280\u672f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u5149\u8c31\u4eea\u3002", "motivation": "\u4f20\u7edf\u5149\u8c31\u4eea\u5728\u8d85\u8fc7950nm\u7684\u6ce2\u6bb5\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u4f53\u79ef\u5927\u3001\u566a\u58f0\u654f\u611f\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u7d27\u51d1\u3001\u6297\u566a\u58f0\u4e14\u8986\u76d6\u66f4\u957f\u6ce2\u6bb5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5355\u7247\u96c6\u6210\u7684\u7845\u5149\u7535\u4e8c\u6781\u7ba1\uff0c\u7ed3\u5408\u5149\u5b50\u9631\u8868\u9762\u7eb9\u7406\uff08PTST\uff09\u63d0\u9ad8\u8fd1\u7ea2\u5916\u54cd\u5e94\u7387\uff0c\u5e76\u4f7f\u7528\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u4ece16\u4e2a\u72ec\u7279\u8bbe\u8ba1\u7684\u63a2\u6d4b\u5668\u7cbe\u786e\u91cd\u5efa\u5149\u8c31\u3002", "result": "\u5149\u8c31\u91cd\u5efa\u8bef\u5dee<0.05 RMSE\uff0c\u5206\u8fa8\u7387\u4e3a8nm\uff0c\u4fe1\u566a\u6bd4>30dB\uff0c\u52a8\u6001\u8303\u56f450dB\uff0c\u54cd\u5e94\u65f6\u95f457ps\uff0c\u9002\u7528\u4e8e\u4fbf\u643a\u5f0f\u548c\u4f4e\u5149\u5e94\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8eCMOS\u7684\u9ad8\u6027\u80fd\u8d85\u5149\u8c31\u4f20\u611f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u751f\u7269\u533b\u5b66\u3001\u73af\u5883\u76d1\u6d4b\u548c\u9065\u611f\u7b49\u9886\u57df\u3002"}}
{"id": "2508.13444", "pdf": "https://arxiv.org/pdf/2508.13444", "abs": "https://arxiv.org/abs/2508.13444", "authors": ["Tianyu Li", "Jeonghwan Kim", "Wontaek Kim", "Donghoon Baek", "Seungeun Rho", "Sehoon Ha"], "title": "Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics", "categories": ["cs.RO"], "comment": "Workshop Submission", "summary": "Recent advances in whole-body robot control have enabled humanoid and legged\nrobots to execute increasingly agile and coordinated movements. However,\nstandardized benchmarks for evaluating robotic athletic performance in\nreal-world settings and in direct comparison to humans remain scarce. We\npresent Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable\npipeline that leverages motion-sensing console games to evaluate whole-body\nrobot control policies. Using Just Dance on the Nintendo Switch as a\nrepresentative example, our system captures, reconstructs, and retargets\nin-game choreography for robotic execution. We validate the system on a Unitree\nG1 humanoid with an open-source whole-body controller, establishing a\nquantitative baseline for the robot's performance against a human player. In\nthe paper, we discuss these results, which demonstrate the feasibility of using\ncommercial games platform as physically grounded benchmarks and motivate future\nwork to for benchmarking embodied AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u6613\u90e8\u7f72\u7684\u7ba1\u9053Switch4EAI\uff0c\u5229\u7528\u4f53\u611f\u6e38\u620f\u8bc4\u4f30\u5168\u8eab\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u4eba\u5f62\u673a\u5668\u4ebaUnitree G1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30\u673a\u5668\u4eba\u8fd0\u52a8\u6027\u80fd\uff0cSwitch4EAI\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u76f4\u63a5\u6bd4\u8f83\uff0c\u4e3a\u673a\u5668\u4eba\u6027\u80fd\u63d0\u4f9b\u5b9a\u91cf\u57fa\u51c6\u3002", "method": "\u901a\u8fc7Nintendo Switch\u6e38\u620f\uff08\u5982Just Dance\uff09\u6355\u6349\u3001\u91cd\u5efa\u5e76\u91cd\u65b0\u5b9a\u5411\u821e\u8e48\u52a8\u4f5c\uff0c\u4f9b\u673a\u5668\u4eba\u6267\u884c\uff0c\u5229\u7528\u5f00\u6e90\u5168\u8eab\u63a7\u5236\u5668\u9a8c\u8bc1\u7cfb\u7edf\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u6027\u80fd\u63d0\u4f9b\u4e86\u91cf\u5316\u57fa\u51c6\u3002", "conclusion": "Switch4EAI\u5c55\u793a\u4e86\u5546\u7528\u6e38\u620f\u5e73\u53f0\u4f5c\u4e3a\u7269\u7406\u57fa\u51c6\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u8bc4\u4f30\u5177\u8eabAI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.13555", "pdf": "https://arxiv.org/pdf/2508.13555", "abs": "https://arxiv.org/abs/2508.13555", "authors": ["Yubo Zhang", "Hassan ZivariFard", "Xiaodong Wang"], "title": "Power and Rate Allocations for Positive-rate Covert Communications in Block-Fading Channels", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "13 pages, 10 figures", "summary": "We aim to achieve keyless covert communication with a positive-rate in\nRayleigh block-fading channels. Specifically, the transmitter and the\nlegitimate receiver are assumed to have either causal or non-causal knowledge\nof the \\ac{CSI} for both the legitimate and the warden channels, while the\nwarden only knows the statistical distribution of the \\ac{CSI}. Two problem\nformulations are considered in this work: (a) Power allocation: maximizing the\nsum covert rate subject to a maximum power constraint, and (b) Rate allocation:\nminimizing the power consumption subject to a minimum covert rate constraint.\nBoth problems are formulated based on recent information theoretical results on\ncovert communication over state-dependent channels. When the \\ac{CSI} of each\nfading block is known non-causally, we propose a novel three-step method to\nsolve both the power and rate allocation problems. In the case where the\n\\ac{CSI} is known causally, the power allocation problem can be formulated as\n\\ac{MDP} and be solved using a \\ac{DDQN} approach. Although the rate allocation\nproblem under causal \\ac{CSI} does not directly conform to an \\ac{MDP}\nstructure, it can be approximately solved using the \\ac{DDQN} trained for power\nallocation. Simulation results demonstrate the effectiveness of the proposed\npower and rate allocation methods and provide comprehensive performance\ncomparisons across different allocation schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u745e\u5229\u5757\u8870\u843d\u4fe1\u9053\u4e2d\u5b9e\u73b0\u65e0\u5bc6\u94a5\u9690\u853d\u901a\u4fe1\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u529f\u7387\u5206\u914d\u548c\u901f\u7387\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5728\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u9650\u5236\u4e0b\u5b9e\u73b0\u9690\u853d\u901a\u4fe1\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u975e\u56e0\u679c\u548c\u56e0\u679cCSI\u6761\u4ef6\u4e0b\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6cd5\u89e3\u51b3\u975e\u56e0\u679cCSI\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u53ccQ\u7f51\u7edc\uff08DDQN\uff09\u65b9\u6cd5\u89e3\u51b3\u56e0\u679cCSI\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u529f\u7387\u548c\u901f\u7387\u5206\u914d\u65b9\u6cd5\u6709\u6548\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0d\u540c\u65b9\u6848\u4e4b\u95f4\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9690\u853d\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u975e\u56e0\u679cCSI\u6761\u4ef6\u4e0b\u7684\u4e09\u6b65\u6cd5\u4ee5\u53ca\u56e0\u679cCSI\u6761\u4ef6\u4e0b\u7684DDQN\u65b9\u6cd5\u3002"}}
{"id": "2508.13446", "pdf": "https://arxiv.org/pdf/2508.13446", "abs": "https://arxiv.org/abs/2508.13446", "authors": ["Catherine Glossop", "William Chen", "Arjun Bhorkar", "Dhruv Shah", "Sergey Levine"], "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Generalist robots should be able to understand and follow user instructions,\nbut current vision-language-action (VLA) models struggle with following\nfine-grained commands despite providing a powerful architecture for mapping\nopen-vocabulary natural language instructions to robot actions. One cause for\nthis is a lack of semantic diversity and language grounding in existing robot\ndatasets and, specifically, a lack of fine-grained task diversity for similar\nobservations. To address this, we present a novel method to augment existing\nrobot datasets by leveraging vision language models to create counterfactual\nlabels. Our method improves the language-following capabilities of VLAs by\nincreasing the diversity and granularity of language grounding for robot\ndatasets by generating counterfactual language and actions. We evaluate the\nresulting model's ability to follow language instructions, ranging from simple\nobject-centric commands to complex referential tasks, by conducting visual\nlanguage navigation experiments in 3 different indoor and outdoor environments.\nOur experiments demonstrate that counterfactual relabeling, without any\nadditional data collection, significantly improves instruction-following in VLA\npolicies, making them competitive with state-of-the-art methods and increasing\nsuccess rate by 27% on navigation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u6807\u7b7e\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6267\u884c\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u6267\u884c\u7528\u6237\u7ec6\u7c92\u5ea6\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u591a\u6837\u6027\u548c\u8bed\u8a00\u57fa\u7840\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u6807\u7b7e\uff0c\u589e\u5f3a\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u4efb\u52a1\u7684\u8bed\u8a00\u57fa\u7840\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u53cd\u4e8b\u5b9e\u91cd\u6807\u8bb0\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u5bfc\u822a\u4efb\u52a1\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8627%\u3002", "conclusion": "\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.13728", "pdf": "https://arxiv.org/pdf/2508.13728", "abs": "https://arxiv.org/abs/2508.13728", "authors": ["Sebastian Frey", "Giusy Spacone", "Andrea Cossettini", "Marco Guermandi", "Philipp Schilk", "Luca Benini", "Victor Kartsch"], "title": "BioGAP-Ultra: A Modular Edge-AI Platform for Wearable Multimodal Biosignal Acquisition and Processing", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "13 pages, 12 figures", "summary": "The growing demand for continuous physiological monitoring and human-machine\ninteraction in real-world settings calls for wearable platforms that are\nflexible, low-power, and capable of on-device intelligence. This work presents\nBioGAP-Ultra, an advanced multimodal biosensing platform that supports\nsynchronized acquisition of diverse electrophysiological and hemodynamic\nsignals such as EEG, EMG, ECG, and PPG while enabling embedded AI processing at\nstate-of-the-art energy efficiency. BioGAP-Ultra is a major extension of our\nprevious design, BioGAP [1], aimed at meeting the rapidly growing requirements\nof wearable biosensing applications. It features (i) increased on-device\nstorage (x2 SRAM, x4 FLASH), (ii) improved wireless connectivity (1.4 Mbit/s\nbandwidth, x4 higher than BioGAP), (iii) enhanced number of signal modalities\n(from 3 to 5) and analog input channels (x2). Further, it is complemented by a\ncomplete real-time visualization and analysis software suite, providing access\nto raw data and real-time configurability on a mobile phone. Electrical\ncharacterization and multiple case studies confirm the platform's robustness,\nconfigurability, and suitability for real-world multimodal biosignal\nacquisition and edge intelligence. Finally, we demonstrate the system's\nversatility through integration into various wearable form factors: an EEG-PPG\nheadband consuming 32.8 mW, an EMG sleeve at 26.7 mW, and an ECG-PPG chest band\nrequiring only 9.3 mW, tailored for diverse biosignal applications. All\nhardware and software design files are also released open-source with a\npermissive license.", "AI": {"tldr": "BioGAP-Ultra\u662f\u4e00\u6b3e\u5148\u8fdb\u7684\u751f\u7269\u4f20\u611f\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6a21\u6001\u4fe1\u53f7\u540c\u6b65\u91c7\u96c6\u4e0e\u5d4c\u5165\u5f0fAI\u5904\u7406\uff0c\u5177\u6709\u9ad8\u80fd\u6548\u3001\u4f4e\u529f\u8017\u548c\u5f00\u6e90\u7279\u6027\u3002", "motivation": "\u6ee1\u8db3\u5b9e\u65f6\u751f\u7406\u76d1\u6d4b\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u9700\u6c42\uff0c\u63d0\u5347\u7a7f\u6234\u5f0f\u8bbe\u5907\u7684\u7075\u6d3b\u6027\u548c\u667a\u80fd\u5316\u3002", "method": "\u6269\u5c55\u4e86BioGAP\u8bbe\u8ba1\uff0c\u589e\u52a0\u4e86\u5b58\u50a8\u3001\u65e0\u7ebf\u5e26\u5bbd\u548c\u4fe1\u53f7\u901a\u9053\uff0c\u5e76\u5f00\u53d1\u4e86\u914d\u5957\u7684\u5b9e\u65f6\u53ef\u89c6\u5316\u5206\u6790\u8f6f\u4ef6\u3002", "result": "\u5e73\u53f0\u5728\u591a\u79cd\u7a7f\u6234\u8bbe\u5907\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u80fd\uff08\u5982\u5934\u5e26\u3001\u8896\u5957\u548c\u80f8\u5e26\uff09\uff0c\u529f\u8017\u5206\u522b\u4f4e\u81f332.8 mW\u300126.7 mW\u548c9.3 mW\u3002", "conclusion": "BioGAP-Ultra\u4e3a\u5b9e\u65f6\u751f\u7269\u4fe1\u53f7\u91c7\u96c6\u548c\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u914d\u7f6e\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13457", "pdf": "https://arxiv.org/pdf/2508.13457", "abs": "https://arxiv.org/abs/2508.13457", "authors": ["Xu Yang", "Jun Ni", "Hengyang Feng", "Feiyu Wang", "Tiezhen Wang"], "title": "Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "An all-wheel omni-directional independent steering vehicle (AWOISV) is a\nspecialized all-wheel independent steering vehicle with each wheel capable of\nsteering up to 90{\\deg}, enabling unique maneuvers like yaw and diagonal\nmovement. This paper introduces a theoretical steering radius angle and\nsideslip angle (\\( \\theta_R \\)-\\(\\beta_R \\)) representation, based on the\nposition of the instantaneous center of rotation relative to the wheel rotation\ncenter, defining the motion modes and switching criteria for AWOISVs. A\ngeneralized \\( v\\)-\\(\\beta\\)-\\(r \\) dynamic model is developed with forward\nvelocity \\(v\\), sideslip angle \\(\\beta\\), and yaw rate \\(r\\) as states, and\n\\(\\theta_R\\) and \\(\\beta_R\\) as control inputs. This model decouples\nlongitudinal and lateral motions into forward and rotational motions, allowing\nseamless transitions across all motion modes under specific conditions. A\nfiltered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,\nachieving simultaneous tracking of lateral position and arbitrary heading\nangles, with robustness to model inaccuracies and parameter uncertainties.\nCo-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC\nenables high-precision control of both position and heading while ensuring\nexcellent real-time performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86\uff08AWOISV\uff09\u7684\u5e7f\u4e49\u52a8\u6001\u6a21\u578b\u548c\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f4d\u7f6e\u548c\u822a\u5411\u89d2\u8ddf\u8e2a\u3002", "motivation": "\u7814\u7a76\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86\uff08AWOISV\uff09\u7684\u72ec\u7279\u673a\u52a8\u6027\u9700\u6c42\uff0c\u63d0\u51fa\u901a\u7528\u52a8\u6001\u6a21\u578b\u548c\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u77ac\u65f6\u65cb\u8f6c\u4e2d\u5fc3\u5b9a\u4e49\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5f00\u53d1\u5e7f\u4e49\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u6ee4\u6ce2\u7ba1\u57fa\u7ebf\u6027\u65f6\u53d8MPC\uff08FT-LTVMPC\uff09\u63a7\u5236\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\uff0cFT-LTVMPC\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u63a7\u5236\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684FT-LTVMPC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86AWOISV\u7684\u591a\u6a21\u5f0f\u8fd0\u52a8\u548c\u9c81\u68d2\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2508.13783", "pdf": "https://arxiv.org/pdf/2508.13783", "abs": "https://arxiv.org/abs/2508.13783", "authors": ["Eike-Manuel Edelmann", "Alexander von Bank", "Laurent Schmalen"], "title": "Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems", "categories": ["cs.NE", "eess.SP"], "comment": "accepted for publication at ECOC 2025", "summary": "Neural encoding parameters for spiking neural networks (SNNs) are typically\nset heuristically. We propose a reinforcement learning-based algorithm to\noptimize them. Applied to an SNN-based equalizer and demapper in an IM/DD\nsystem, the method improves performance while reducing computational load and\nnetwork size.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\u4f18\u5316SNN\u4e2d\u7684\u795e\u7ecf\u7f16\u7801\u53c2\u6570\uff0c\u5e94\u7528\u4e8eIM/DD\u7cfb\u7edf\u540e\uff0c\u6027\u80fd\u63d0\u5347\u4e14\u8ba1\u7b97\u8d1f\u8f7d\u548c\u7f51\u7edc\u89c4\u6a21\u51cf\u5c0f\u3002", "motivation": "\u76ee\u524dSNN\u7684\u795e\u7ecf\u7f16\u7801\u53c2\u6570\u901a\u5e38\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u4f18\u5316\u673a\u5236\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316SNN\u7684\u795e\u7ecf\u7f16\u7801\u53c2\u6570\u3002", "result": "\u5728IM/DD\u7cfb\u7edf\u7684SNN\u5747\u8861\u5668\u548c\u89e3\u6620\u5c04\u5668\u4e2d\u5e94\u7528\uff0c\u6027\u80fd\u63d0\u5347\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u548c\u7f51\u7edc\u89c4\u6a21\u964d\u4f4e\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4f18\u5316SNN\u7f16\u7801\u53c2\u6570\u662f\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2508.13459", "pdf": "https://arxiv.org/pdf/2508.13459", "abs": "https://arxiv.org/abs/2508.13459", "authors": ["Rohan Chandra", "Shubham Singh", "Abhishek Jha", "Dannon Andrade", "Hriday Sainathuni", "Katia Sycara"], "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "The ``Last Mile Challenge'' has long been considered an important, yet\nunsolved, challenge for autonomous vehicles, public service robots, and\ndelivery robots. A central issue in this challenge is the ability of robots to\nnavigate constrained and cluttered environments (e.g., doorways, hallways,\ncorridor intersections), often while competing for space with other robots and\nhumans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs\nare tightly coupled, high-agency interactions that arise within general\nmulti-robot navigation (MRN) scenarios. They are identified through certain\ndistinct characteristics and require specialized metrics to evaluate them.\nTraditional navigation approaches designed for MRN do not perform well in SMGs,\nwhich has led to focused research on dedicated SMG solvers (navigation methods\nspecialized to navigate in SMGs), which has flourished in recent years.\nHowever, publications on SMG navigation research make different assumptions (on\ncentralized versus decentralized, observability, communication, cooperation,\netc.), and have different objective functions (safety versus liveness). These\nassumptions and objectives are sometimes implicitly assumed or described\ninformally. This makes it difficult to establish appropriate baselines for\ncomparison in research papers, as well as making it difficult for practitioners\nto find the papers relevant to their concrete application. Such ad-hoc\nrepresentation of the field also presents a barrier to new researchers wanting\nto start research in this area. SMG navigation research requires its own\ntaxonomy, definitions, and evaluation protocols to guide effective research\nmoving forward. This survey is the first to catalog SMG solvers using a\nwell-defined and unified taxonomy and to classify existing methods accordingly.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u201c\u6700\u540e\u4e00\u82f1\u91cc\u6311\u6218\u201d\u4e2d\u201c\u793e\u4ea4\u8ff7\u4f60\u6e38\u620f\uff08SMGs\uff09\u201d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u51fa\u9700\u8981\u4e13\u95e8\u7684\u5206\u7c7b\u548c\u8bc4\u4ef7\u6807\u51c6\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5728SMGs\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u9996\u6b21\u63d0\u51fa\u4f7f\u7528\u660e\u786e\u5b9a\u4e49\u7684\u5206\u7c7b\u6cd5\u5bf9SMG\u5bfc\u822a\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u548c\u8bc4\u4ef7\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684SMG\u5bfc\u822a\u7814\u7a76\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u7edf\u4e00\u7684\u5206\u7c7b\u548c\u8bc4\u4ef7\u6807\u51c6\u6709\u52a9\u4e8e\u63a8\u52a8SMG\u5bfc\u822a\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.13842", "pdf": "https://arxiv.org/pdf/2508.13842", "abs": "https://arxiv.org/abs/2508.13842", "authors": ["Chunjie Wang", "Xuhui Zhang", "Jinke Ren", "Wenchao Liu", "Shuqiang Wang", "Yanyan Shen", "Kejiang Ye", "Chengzhong Xu", "Dusit Niyato"], "title": "Joint Beamforming Design for RIS-Empowered NOMA-ISAC Systems", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "13 pages, 9 figures, this manuscript has been submitted to IEEE", "summary": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted\nintegrated sensing and communication (ISAC) system and proposes a joint\ncommunication and sensing beamforming design based on non-orthogonal multiple\naccess (NOMA) technology. The system employs a dual-functional base station\n(DFBS) to simultaneously serve multiple users and sense multiple targets with\nthe aid of RIS. To maximize the sum-rate of users, we jointly optimize the\nDFBS's active beamforming, the RIS's reflection coefficients, and the radar\nreceive filters. The optimization is performed under constraints including the\nradar signal-to-noise ratio thresholds, the user\nsignal-to-interference-plus-noise ratio requirements, the phase shifts of the\nRIS, the total transmit power, the receive filters, and the successive\ninterference cancellation decoding order. To tackle the complex\ninterdependencies and non-convex nature of the optimization problem, we\nintroduce an effective iterative algorithm based on the alternating\noptimization framework. Simulation results demonstrate that the proposed\nalgorithm outperforms baseline algorithms, highlighting its distinct advantages\nin the considered RIS-empowered NOMA-ISAC systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86RIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\uff0c\u63d0\u51fa\u57fa\u4e8eNOMA\u7684\u8054\u5408\u901a\u4fe1\u4e0e\u611f\u77e5\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316DFBS\u7684\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u3001RIS\u7684\u53cd\u5c04\u7cfb\u6570\u548c\u96f7\u8fbe\u63a5\u6536\u6ee4\u6ce2\u5668\uff0c\u6700\u5927\u5316\u7528\u6237\u7684\u603b\u901f\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728RIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\u4e2d\u540c\u65f6\u670d\u52a1\u591a\u7528\u6237\u548c\u591a\u76ee\u6807\u611f\u77e5\uff0c\u5e76\u6700\u5927\u5316\u7528\u6237\u603b\u901f\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\u7684\u6709\u6548\u8fed\u4ee3\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316DFBS\u7684\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u3001RIS\u7684\u53cd\u5c04\u7cfb\u6570\u548c\u96f7\u8fbe\u63a5\u6536\u6ee4\u6ce2\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5c55\u73b0\u4e86\u5728RIS\u8d4b\u80fd\u7684NOMA-ISAC\u7cfb\u7edf\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aRIS\u8d4b\u80fd\u7684ISAC\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u6027\u80fd\u663e\u8457\u3002"}}
{"id": "2508.13488", "pdf": "https://arxiv.org/pdf/2508.13488", "abs": "https://arxiv.org/abs/2508.13488", "authors": ["Jingwen Yu", "Jiayi Yang", "Anjun Hu", "Jiankun Wang", "Ping Tan", "Hong Zhang"], "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 figures", "summary": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROVER\u7684\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\uff0c\u5728\u91cd\u590d\u73af\u5883\u4e2d\u51cf\u5c11\u8bef\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5916\u89c2\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u7684\u65f6\u7a7a\u8fd0\u52a8\u7ebf\u7d22\uff08\u8f68\u8ff9\uff09\uff0c\u5bfc\u81f4\u5728\u91cd\u590d\u73af\u5883\u4e2d\u8bef\u68c0\u7387\u9ad8\u3002", "method": "ROVER\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u4f30\u8ba1\u8f68\u8ff9\uff0c\u5e76\u5229\u7528\u8bc4\u5206\u673a\u5236\u8bc4\u4f30\u95ed\u73af\u5019\u9009\u662f\u5426\u4e0e\u65e0\u95ed\u73af\u7684\u8f68\u8ff9\u5148\u9a8c\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cROVER\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u6709\u6548\uff0c\u4e14\u6210\u529f\u96c6\u6210\u5230\u5148\u8fdbSLAM\u7cfb\u7edf\u4e2d\u3002", "conclusion": "ROVER\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u5148\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u95ed\u73af\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.13513", "pdf": "https://arxiv.org/pdf/2508.13513", "abs": "https://arxiv.org/abs/2508.13513", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Cheng Zhou", "Wanli Xing", "Liang Lu", "Nikos G. Tsagarakis"], "title": "Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies", "categories": ["cs.RO"], "comment": null, "summary": "This work proposes a unified Hierarchical Model Predictive Control (H-MPC)\nfor modular manipulators across various morphologies, as the controller can\nadapt to different configurations to execute the given task without extensive\nparameter tuning in the controller. The H-MPC divides the control process into\ntwo levels: a high-level MPC and a low-level MPC. The high-level MPC predicts\nfuture states and provides trajectory information, while the low-level MPC\nrefines control actions by updating the predictive model based on this\nhigh-level information. This hierarchical structure allows for the integration\nof kinematic constraints and ensures smooth joint-space trajectories, even near\nsingular configurations. Moreover, the low-level MPC incorporates secondary\nlinearization by leveraging predictive information from the high-level MPC,\neffectively capturing the second-order Taylor expansion information of the\nkinematic model while still maintaining a linearized model formulation. This\napproach not only preserves the simplicity of a linear control model but also\nenhances the accuracy of the kinematic representation, thereby improving\noverall control precision and reliability. To validate the effectiveness of the\ncontrol policy, we conduct extensive evaluations across different manipulator\nmorphologies and demonstrate the execution of pick-and-place tasks in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5206\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08H-MPC\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5f62\u6001\u7684\u6a21\u5757\u5316\u673a\u68b0\u81c2\uff0c\u65e0\u9700\u5927\u91cf\u53c2\u6570\u8c03\u6574\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u914d\u7f6e\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u6a21\u5757\u5316\u673a\u68b0\u81c2\u5728\u4e0d\u540c\u5f62\u6001\u4e0b\u7684\u63a7\u5236\u95ee\u9898\uff0c\u51cf\u5c11\u63a7\u5236\u5668\u53c2\u6570\u8c03\u6574\u7684\u590d\u6742\u6027\u3002", "method": "\u5c06\u63a7\u5236\u8fc7\u7a0b\u5206\u4e3a\u9ad8\u5c42\u548c\u4f4e\u5c42MPC\uff1a\u9ad8\u5c42\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u63d0\u4f9b\u8f68\u8ff9\u4fe1\u606f\uff0c\u4f4e\u5c42\u57fa\u4e8e\u9ad8\u5c42\u4fe1\u606f\u4f18\u5316\u63a7\u5236\u52a8\u4f5c\uff0c\u540c\u65f6\u5f15\u5165\u4e8c\u6b21\u7ebf\u6027\u5316\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u81c2\u4e0a\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u6267\u884c\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53d6\u653e\u4efb\u52a1\u3002", "conclusion": "H-MPC\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u4e8c\u6b21\u7ebf\u6027\u5316\uff0c\u63d0\u9ad8\u4e86\u63a7\u5236\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u673a\u68b0\u81c2\u914d\u7f6e\u3002"}}
{"id": "2508.13531", "pdf": "https://arxiv.org/pdf/2508.13531", "abs": "https://arxiv.org/abs/2508.13531", "authors": ["Bolin Li", "Gewei Zuo", "Zhixiang Wang", "Xiaotian Ke", "Lijun Zhu", "Han Ding"], "title": "A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a control framework designed to enhance the stability and\nrobustness of legged robots in the presence of uncertainties, including model\nuncertainties, external disturbances, and faults. The framework enables the\nfull-state feedback estimator to estimate and compensate for uncertainties in\nwhole-body dynamics of the legged robots. First, we propose a novel moving\nhorizon extended state observer (MH-ESO) to estimate uncertainties and mitigate\nnoise in legged systems, which can be integrated into the framework for\ndisturbance compensation. Second, we introduce a three-level whole-body\ndisturbance rejection control framework (T-WB-DRC). Unlike the previous\ntwo-level approach, this three-level framework considers both the plan based on\nwhole-body dynamics without uncertainties and the plan based on dynamics with\nuncertainties, significantly improving payload transportation, external\ndisturbance rejection, and fault tolerance. Third, simulations of both humanoid\nand quadruped robots in the Gazebo simulator demonstrate the effectiveness and\nversatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped\nrobot validate the robustness and stability of the system when using T-WB-DRC\nunder various disturbance conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u7ea7\u5168\u8eab\u6270\u52a8\u6291\u5236\u63a7\u5236\uff08T-WB-DRC\uff09\u548c\u65b0\u578b\u79fb\u52a8\u6c34\u5e73\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\uff08MH-ESO\uff09\uff0c\u63d0\u9ad8\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u5728\u9762\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u5916\u90e8\u6270\u52a8\u548c\u6545\u969c\u65f6\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMH-ESO\u7528\u4e8e\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u5e76\u6291\u5236\u566a\u58f0\uff0c\u8bbe\u8ba1T-WB-DRC\u6846\u67b6\u4ee5\u540c\u65f6\u8003\u8651\u65e0\u6270\u52a8\u548c\u6709\u6270\u52a8\u7684\u52a8\u6001\u89c4\u5212\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86T-WB-DRC\u5728\u63d0\u9ad8\u7a33\u5b9a\u6027\u3001\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "T-WB-DRC\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6270\u52a8\u6761\u4ef6\u3002"}}
{"id": "2508.13534", "pdf": "https://arxiv.org/pdf/2508.13534", "abs": "https://arxiv.org/abs/2508.13534", "authors": ["Chao Tang", "Anxing Xiao", "Yuhong Deng", "Tianrun Hu", "Wenlong Dong", "Hanbo Zhang", "David Hsu", "Hong Zhang"], "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted to CoRL 2025", "summary": "Imitating tool manipulation from human videos offers an intuitive approach to\nteaching robots, while also providing a promising and scalable alternative to\nlabor-intensive teleoperation data collection for visuomotor policy learning.\nWhile humans can mimic tool manipulation behavior by observing others perform a\ntask just once and effortlessly transfer the skill to diverse tools for\nfunctionally equivalent tasks, current robots struggle to achieve this level of\ngeneralization. A key challenge lies in establishing function-level\ncorrespondences, considering the significant geometric variations among\nfunctionally similar tools, referred to as intra-function variations. To\naddress this challenge, we propose MimicFunc, a framework that establishes\nfunctional correspondences with function frame, a function-centric local\ncoordinate frame constructed with keypoint-based abstraction, for imitating\ntool manipulation skills. Experiments demonstrate that MimicFunc effectively\nenables the robot to generalize the skill from a single RGB-D human video to\nmanipulating novel tools for functionally equivalent tasks. Furthermore,\nleveraging MimicFunc's one-shot generalization capability, the generated\nrollouts can be used to train visuomotor policies without requiring\nlabor-intensive teleoperation data collection for novel objects. Our code and\nvideo are available at https://sites.google.com/view/mimicfunc.", "AI": {"tldr": "MimicFunc\u6846\u67b6\u901a\u8fc7\u529f\u80fd\u5e27\u5b9e\u73b0\u673a\u5668\u4eba\u4ece\u5355\u4e00\u4eba\u673a\u89c6\u9891\u4e2d\u6a21\u4eff\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\uff0c\u5e76\u63a8\u5e7f\u5230\u65b0\u5de5\u5177\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u89c2\u5bdf\u5feb\u901f\u6a21\u4eff\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u4e0d\u540c\u5de5\u5177\uff0c\u800c\u673a\u5668\u4eba\u76ee\u524d\u96be\u4ee5\u8fbe\u5230\u8fd9\u79cd\u6cdb\u5316\u6c34\u5e73\u3002", "method": "\u63d0\u51faMimicFunc\u6846\u67b6\uff0c\u5229\u7528\u529f\u80fd\u5e27\uff08\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u62bd\u8c61\uff09\u5efa\u7acb\u529f\u80fd\u7ea7\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMimicFunc\u80fd\u6709\u6548\u5b9e\u73b0\u5355\u6b21RGB-D\u89c6\u9891\u7684\u6280\u80fd\u6cdb\u5316\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "conclusion": "MimicFunc\u4e3a\u89e3\u51b3\u5de5\u5177\u51e0\u4f55\u5dee\u5f02\u5e26\u6765\u7684\u529f\u80fd\u5bf9\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.13699", "pdf": "https://arxiv.org/pdf/2508.13699", "abs": "https://arxiv.org/abs/2508.13699", "authors": ["Maren Raab", "Linda Miller", "Zhe Zeng", "Pascal Jansen", "Martin Baumann", "Johannes Kraus"], "title": "Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation", "categories": ["cs.RO"], "comment": null, "summary": "As autonomous robots become more common in public spaces, spontaneous\nencounters with laypersons are more frequent. For this, robots need to be\nequipped with communication strategies that enhance momentary transparency and\nreduce the probability of critical situations. Adapting these robotic\nstrategies requires consideration of robot movements, environmental conditions,\nand user characteristics and states. While numerous studies have investigated\nthe impact of distraction on pedestrians' movement behavior, limited research\nhas examined this behavior in the presence of autonomous robots. This research\naddresses the impact of robot type and robot movement pattern on distracted and\nundistracted pedestrians' movement behavior. In a field setting, unaware\npedestrians were videotaped while moving past two working, autonomous cleaning\nrobots. Out of N=498 observed pedestrians, approximately 8% were distracted by\nsmartphones. Distracted and undistracted pedestrians did not exhibit\nsignificant differences in their movement behaviors around the robots. Instead,\nboth the larger sweeping robot and the offset rectangular movement pattern\nsignificantly increased the number of lateral adaptations compared to the\nsmaller cleaning robot and the circular movement pattern. The offset\nrectangular movement pattern also led to significantly more close lateral\nadaptations. Depending on the robot type, the movement patterns led to\ndifferences in the distances of lateral adaptations. The study provides initial\ninsights into pedestrian movement behavior around an autonomous cleaning robot\nin public spaces, contributing to the growing field of HRI research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7c7b\u578b\u548c\u79fb\u52a8\u65b9\u5f0f\u7684\u81ea\u4e3b\u6e05\u6d01\u673a\u5668\u4eba\u5bf9\u5206\u6563\u548c\u672a\u5206\u6563\u884c\u4eba\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u884c\u4eba\u5bf9\u673a\u5668\u4eba\u7684\u884c\u4e3a\u53cd\u5e94\u4e3b\u8981\u53d7\u673a\u5668\u4eba\u5927\u5c0f\u548c\u79fb\u52a8\u6a21\u5f0f\u5f71\u54cd\uff0c\u800c\u975e\u6ce8\u610f\u529b\u72b6\u6001\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u7684\u666e\u53ca\uff0c\u5982\u4f55\u8bbe\u8ba1\u673a\u5668\u4eba\u7684\u901a\u4fe1\u7b56\u7565\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u51cf\u5c11\u4e34\u754c\u60c5\u51b5\u53d8\u5f97\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u5173\u4e8e\u673a\u5668\u4eba\u4e0e\u884c\u4eba\u884c\u4e3a\u4ea4\u4e92\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9e\u5730\u89c2\u5bdf\uff0c\u8bb0\u5f55\u884c\u4eba\u4e0e\u4e24\u79cd\u81ea\u4e3b\u6e05\u6d01\u673a\u5668\u4eba\uff08\u5927\u5c0f\u4e0d\u540c\u3001\u79fb\u52a8\u6a21\u5f0f\u4e0d\u540c\uff09\u4e92\u52a8\u65f6\u7684\u884c\u4e3a\uff0c\u5206\u6790\u5206\u6563\u548c\u672a\u5206\u6563\u884c\u4eba\u7684\u53cd\u5e94\u3002", "result": "\u884c\u4eba\u7684\u6ce8\u610f\u529b\u72b6\u6001\uff08\u662f\u5426\u5206\u5fc3\uff09\u5bf9\u884c\u4e3a\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u673a\u5668\u4eba\u5927\u5c0f\u548c\u79fb\u52a8\u6a21\u5f0f\uff08\u5982\u77e9\u5f62\u504f\u79fb\u8def\u5f84\uff09\u663e\u8457\u5f71\u54cd\u884c\u4eba\u7684\u6a2a\u5411\u8c03\u6574\u3002", "conclusion": "\u7814\u7a76\u4e3a\u516c\u5171\u573a\u6240\u673a\u5668\u4eba\u4e0e\u884c\u4eba\u4e92\u52a8\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u673a\u5668\u4eba\u7269\u7406\u7279\u6027\u548c\u79fb\u52a8\u6a21\u5f0f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.13785", "pdf": "https://arxiv.org/pdf/2508.13785", "abs": "https://arxiv.org/abs/2508.13785", "authors": ["Liyang Liu", "Ehsan Mihankhah", "Nathan Wallace", "Javier Martinez", "Andrew J. Hill"], "title": "Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot", "categories": ["cs.RO"], "comment": null, "summary": "In open-pit mining, holes are drilled into the surface of the excavation site\nand detonated with explosives to facilitate digging. These blast holes need to\nbe inspected internally for investigation of downhole material types and\nproperties. Knowing these properties can lead to significant savings in\nmaterial handling costs in downstream processes. Manual hole inspection is slow\nand expensive, with major limitations in revealing the geometric and geological\nproperties of the holes and their contents. This has been the motivation for\nthe development of our autonomous mine-site inspection robot - \"DIPPeR\". In\nthis paper, the automation aspect of the project is explained. We present a\nrobust blast hole seeking and detection framework that enables target-based\nnavigation and accurate down-hole sensor positioning. The pipeline first\nprocesses point-cloud data collected by the on-board LiDAR sensors, extracting\nthe cone-shaped volume of drill-waste above the ground. By projecting the 3D\ncone points into a virtual depth image, segmentation is achieved in the 2D\ndomain, yielding a circular hole at the image centre and a collared cone face.\nWe then identify the hole centre using a robust detection module while\nsuppressing non-maximum candidates, ensuring precise sensor placement for\ndown-hole inspection and avoiding collisions with the cavity wall. To enable\nautonomous hole-seeking, the pipeline automatically adjusts its projection\nparameters during robot navigation to account for variations in point sparsity\nand hole opening size, ensuring a consistent hole appearance in 2D images. This\nallows continuous tracking of the target hole as the robot approaches the goal\npoint. We demonstrate the effectiveness of our navigation and perception system\nin both high-fidelity simulation environments and on-site field tests. A\ndemonstration video is available at\n\"https://www.youtube.com/watch?v=fRNbcBcaSqE\".", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u7206\u7834\u5b54\u68c0\u6d4b\u673a\u5668\u4eba\u7cfb\u7edf\u201cDIPPeR\u201d\uff0c\u901a\u8fc7LiDAR\u548c\u56fe\u50cf\u5904\u7406\u6280\u672f\u5b9e\u73b0\u7cbe\u51c6\u5b9a\u4f4d\u548c\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9732\u5929\u77ff\u5751\u7206\u7834\u5b54\u68c0\u6d4b\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u7206\u7834\u5b54\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u83b7\u53d6\u5b54\u6d1e\u7684\u51e0\u4f55\u548c\u5730\u8d28\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u4f18\u5316\u7269\u6599\u5904\u7406\u6210\u672c\u3002", "method": "\u7ed3\u5408LiDAR\u70b9\u4e91\u6570\u636e\u5904\u7406\uff0c\u63d0\u53d6\u7206\u7834\u5b54\u7684\u4e09\u7ef4\u9525\u5f62\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u6df1\u5ea6\u56fe\u50cf\u5b9e\u73b02D\u5206\u5272\uff0c\u5229\u7528\u68c0\u6d4b\u6a21\u5757\u7cbe\u51c6\u5b9a\u4f4d\u5b54\u4e2d\u5fc3\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u4e0e\u4f20\u611f\u5668\u5b9a\u4f4d\u3002", "result": "\u7cfb\u7edf\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6301\u7eed\u8ddf\u8e2a\u76ee\u6807\u5b54\u6d1e\u5e76\u5b8c\u6210\u7cbe\u51c6\u68c0\u6d4b\u3002", "conclusion": "DIPPeR\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u7206\u7834\u5b54\u68c0\u6d4b\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4e3a\u9732\u5929\u77ff\u5751\u7684\u7269\u6599\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13795", "pdf": "https://arxiv.org/pdf/2508.13795", "abs": "https://arxiv.org/abs/2508.13795", "authors": ["Haitham El-Hussieny"], "title": "Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a data-driven control framework for quadrotor systems\nthat integrates a deep Koopman operator with model predictive control (DK-MPC).\nThe deep Koopman operator is trained on sampled flight data to construct a\nhigh-dimensional latent representation in which the nonlinear quadrotor\ndynamics are approximated by linear models. This linearization enables the\napplication of MPC to efficiently optimize control actions over a finite\nprediction horizon, ensuring accurate trajectory tracking and stabilization.\nThe proposed DK-MPC approach is validated through a series of\ntrajectory-following and point-stabilization numerical experiments, where it\ndemonstrates superior tracking accuracy and significantly lower computation\ntime compared to conventional nonlinear MPC. These results highlight the\npotential of Koopman-based learning methods to handle complex quadrotor\ndynamics while meeting the real-time requirements of embedded flight control.\nFuture work will focus on extending the framework to more agile flight\nscenarios and improving robustness against external disturbances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6Koopman\u7b97\u5b50\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08DK-MPC\uff09\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u7cfb\u7edf\uff0c\u5c55\u73b0\u4e86\u66f4\u9ad8\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u66f4\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56db\u65cb\u7ffc\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u63a7\u5236\u4e2d\u7684\u590d\u6742\u6027\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u63a7\u5236\u3002", "method": "\u8bad\u7ec3\u6df1\u5ea6Koopman\u7b97\u5b50\u4ece\u98de\u884c\u6570\u636e\u4e2d\u5b66\u4e60\u9ad8\u7ef4\u7ebf\u6027\u8868\u793a\uff0c\u5e76\u5c06\u5176\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7ed3\u5408\uff0c\u5b9e\u73b0\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u7ebf\u6027\u5316\u63a7\u5236\u3002", "result": "\u5728\u8f68\u8ff9\u8ddf\u8e2a\u548c\u70b9\u7a33\u5b9a\u5b9e\u9a8c\u4e2d\uff0cDK-MPC\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u975e\u7ebf\u6027MPC\u66f4\u9ad8\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "Koopman\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u56db\u65cb\u7ffc\u52a8\u529b\u5b66\u65f6\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u4f18\u5316\u6846\u67b6\u4ee5\u5e94\u5bf9\u66f4\u654f\u6377\u7684\u98de\u884c\u573a\u666f\u548c\u5916\u90e8\u5e72\u6270\u3002"}}
{"id": "2508.13877", "pdf": "https://arxiv.org/pdf/2508.13877", "abs": "https://arxiv.org/abs/2508.13877", "authors": ["Rathnam Vidushika Rasanji", "Jin Wei-Kocsis", "Jiansong Zhang", "Dongming Gan", "Ragu Athinarayanan", "Paul Asunda"], "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGDT\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u7b26\u53f7\u673a\u5236\u548c\u56e0\u679c\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\u3002", "motivation": "\u589e\u5f3a\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u6570\u636e\u5bc6\u96c6\u6027\u548c\u4f9d\u8d56\u4e8eMDP\u5047\u8bbe\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u52a8\u6001\u548c\u957f\u671f\u4f9d\u8d56\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "SGDT\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u751f\u6210\u9ad8\u7ea7\u4efb\u52a1\u5bfc\u5411\u8ba1\u5212\uff0c\u6307\u5bfc\u76ee\u6807\u6761\u4ef6\u51b3\u7b56\u53d8\u6362\u5668\u8fdb\u884c\u4f4e\u7ea7\u5e8f\u5217\u51b3\u7b56\u3002", "result": "SGDT\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u51b3\u7b56\u3002", "conclusion": "SGDT\u662f\u9996\u4e2a\u63a2\u7d22\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u7684\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u672f\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.13881", "pdf": "https://arxiv.org/pdf/2508.13881", "abs": "https://arxiv.org/abs/2508.13881", "authors": ["Zhaokun Chen", "Chaopeng Zhang", "Xiaohan Li", "Wenshuo Wang", "Gentiane Venture", "Junqiang Xi"], "title": "Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models", "categories": ["cs.RO"], "comment": null, "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u8bed\u4e49\u7279\u6743\u4fe1\u606f\uff08SPI\uff09\uff0c\u5c06\u9a7e\u9a76\u884c\u4e3a\u8bc6\u522b\u4e0e\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u63a8\u7406\u5bf9\u9f50\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u4f9d\u8d56\u4f4e\u5c42\u4f20\u611f\u5668\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u7b97\u6cd5\u5206\u7c7b\u4e0e\u4e13\u5bb6\u5224\u65ad\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faDriBehavGPT\u6a21\u5757\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u901a\u8fc7\u5d4c\u5165\u548c\u964d\u7ef4\u5c06\u5176\u8f6c\u6362\u4e3a\u673a\u5668\u5b66\u4e60\u53ef\u5904\u7406\u7684\u8868\u793a\uff0c\u5e76\u7ed3\u5408SVM+\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u63d0\u53477.6%\uff08\u8ddf\u8f66\uff09\u548c7.9%\uff08\u53d8\u9053\uff09\u3002", "conclusion": "\u8bed\u4e49\u884c\u4e3a\u8868\u5f81\u5bf9\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u548c\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u8bad\u7ec3\u540e\u4ec5\u9700\u4f20\u611f\u5668\u6570\u636e\u63a8\u7406\u3002"}}
{"id": "2508.13901", "pdf": "https://arxiv.org/pdf/2508.13901", "abs": "https://arxiv.org/abs/2508.13901", "authors": ["Yihao Lu", "Hao Tang"], "title": "Multimodal Data Storage and Retrieval for Embodied AI: A Survey", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied AI (EAI) agents continuously interact with the physical world,\ngenerating vast, heterogeneous multimodal data streams that traditional\nmanagement systems are ill-equipped to handle. In this survey, we first\nsystematically evaluate five storage architectures (Graph Databases,\nMulti-Model Databases, Data Lakes, Vector Databases, and Time-Series\nDatabases), focusing on their suitability for addressing EAI's core\nrequirements, including physical grounding, low-latency access, and dynamic\nscalability. We then analyze five retrieval paradigms (Fusion Strategy-Based\nRetrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based\nRetrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based\nOptimization), revealing a fundamental tension between achieving long-term\nsemantic coherence and maintaining real-time responsiveness. Based on this\ncomprehensive analysis, we identify key bottlenecks, spanning from the\nfoundational Physical Grounding Gap to systemic challenges in cross-modal\nintegration, dynamic adaptation, and open-world generalization. Finally, we\noutline a forward-looking research agenda encompassing physics-aware data\nmodels, adaptive storage-retrieval co-optimization, and standardized\nbenchmarking, to guide future research toward principled data management\nsolutions for EAI. Our survey is based on a comprehensive review of more than\n180 related studies, providing a rigorous roadmap for designing the robust,\nhigh-performance data management frameworks essential for the next generation\nof autonomous embodied systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e94\u79cd\u5b58\u50a8\u67b6\u6784\u548c\u4e94\u79cd\u68c0\u7d22\u8303\u5f0f\uff0c\u4ee5\u89e3\u51b3\u5177\u8eabAI\uff08EAI\uff09\u7684\u6570\u636e\u7ba1\u7406\u9700\u6c42\uff0c\u63ed\u793a\u4e86\u957f\u671f\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u5b9e\u65f6\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u5177\u8eabAI\u751f\u6210\u7684\u591a\u6a21\u6001\u3001\u52a8\u6001\u6269\u5c55\u7684\u6570\u636e\u6d41\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9002\u5408\u7684\u5b58\u50a8\u548c\u68c0\u7d22\u65b9\u6848\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9180\u591a\u9879\u76f8\u5173\u7814\u7a76\u7684\u7efc\u8ff0\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u5b58\u50a8\u67b6\u6784\u548c\u4e94\u79cd\u68c0\u7d22\u8303\u5f0f\u7684\u9002\u7528\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9EAI\u6838\u5fc3\u9700\u6c42\u7684\u6ee1\u8db3\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b58\u50a8\u548c\u68c0\u7d22\u5728\u6ee1\u8db3EAI\u9700\u6c42\u65f6\u5b58\u5728\u5173\u952e\u74f6\u9888\uff0c\u5982\u7269\u7406\u57fa\u7840\u5dee\u8ddd\u3001\u8de8\u6a21\u6001\u96c6\u6210\u6311\u6218\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3a\u672a\u6765\u5177\u8eabAI\u7684\u9ad8\u6027\u80fd\u6570\u636e\u7ba1\u7406\u6846\u67b6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u548c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u5efa\u8bae\u3002"}}
{"id": "2508.13964", "pdf": "https://arxiv.org/pdf/2508.13964", "abs": "https://arxiv.org/abs/2508.13964", "authors": ["Martijn Cramer", "Yanming Wu", "David De Schepper", "Eric Demeester"], "title": "Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation", "categories": ["cs.RO", "cs.CV"], "comment": "13 pages, 25 figures", "summary": "Due to high-mix-low-volume production, sheet-metal workshops today are\nchallenged by small series and varying orders. As standard automation solutions\ntend to fall short, SMEs resort to repetitive manual labour impacting\nproduction costs and leading to tech-skilled workforces not being used to their\nfull potential. The COOCK+ ROBUST project aims to transform cobots into mobile\nand reconfigurable production assistants by integrating existing technologies,\nincluding 3D object recognition and localisation. This article explores both\nthe opportunities and challenges of enhancing cobotic systems with these\ntechnologies in an industrial setting, outlining the key steps involved in the\nprocess. Additionally, insights from a past project, carried out by the ACRO\nresearch unit in collaboration with an industrial partner, serves as a concrete\nimplementation example throughout.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u75283D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u6280\u672f\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u8f6c\u5316\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u751f\u4ea7\u52a9\u624b\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u5e76\u4ee5\u5b9e\u9645\u6848\u4f8b\u4e3a\u53c2\u8003\u3002", "motivation": "\u9ad8\u6df7\u5408\u4f4e\u6279\u91cf\u751f\u4ea7\u6a21\u5f0f\u4e0b\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u9762\u4e34\u5c0f\u6279\u91cf\u591a\u53d8\u8ba2\u5355\u7684\u6311\u6218\uff0c\u6807\u51c6\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u5bfc\u81f4\u751f\u4ea7\u6548\u7387\u4f4e\u4e0b\u548c\u6280\u672f\u4eba\u624d\u6f5c\u80fd\u672a\u88ab\u5145\u5206\u91ca\u653e\u3002", "method": "\u901a\u8fc7\u6574\u54083D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u7b49\u73b0\u6709\u6280\u672f\uff0c\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u8f6c\u5316\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u7684\u751f\u4ea7\u52a9\u624b\uff0c\u5e76\u7ed3\u5408\u5b9e\u9645\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u589e\u5f3a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u5173\u952e\u6b65\u9aa4\u3002", "conclusion": "\u8be5\u9879\u76ee\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u751f\u4ea7\u6d41\u7a0b\u5e76\u63d0\u5347\u6280\u672f\u4eba\u624d\u5229\u7528\u7387\u3002"}}
{"id": "2508.13976", "pdf": "https://arxiv.org/pdf/2508.13976", "abs": "https://arxiv.org/abs/2508.13976", "authors": ["Carlo Mazzola", "Hassan Ali", "Krist\u00edna Malinovsk\u00e1", "Igor Farka\u0161"], "title": "Toward an Interaction-Centered Approach to Robot Trustworthiness", "categories": ["cs.RO"], "comment": "4 pages, presented at TRUST workshop, organised in conjunction with\n  the IEEE RO-MAN 2025 conference, held in Eindhoven, Netherlands", "summary": "As robots get more integrated into human environments, fostering\ntrustworthiness in embodied robotic agents becomes paramount for an effective\nand safe human-robot interaction (HRI). To achieve that, HRI applications must\npromote human trust that aligns with robot skills and avoid misplaced trust or\novertrust, which can pose safety risks and ethical concerns. To achieve that,\nHRI applications must promote human trust that aligns with robot skills and\navoid misplaced trust or overtrust, which can pose safety risks and ethical\nconcerns. In this position paper, we outline an interaction-based framework for\nbuilding trust through mutual understanding between humans and robots. We\nemphasize two main pillars: human awareness and transparency, referring to the\nrobot ability to interpret human actions accurately and to clearly communicate\nits intentions and goals, respectively. By integrating these two pillars,\nrobots can behave in a manner that aligns with human expectations and needs\nwhile providing their human partners with both comprehension and control over\ntheir actions. We also introduce four components that we think are important\nfor bridging the gap between a human-perceived sense of trust and a robot true\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u76f8\u4e92\u7406\u89e3\u6765\u5efa\u7acb\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\uff0c\u5e76\u5f15\u5165\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u4ee5\u51cf\u5c11\u4fe1\u4efb\u4e0e\u5b9e\u9645\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u66f4\u591a\u5730\u878d\u5165\u4eba\u7c7b\u73af\u5883\uff0c\u5efa\u7acb\u53ef\u4fe1\u8d56\u7684\u673a\u5668\u4eba\u4ee3\u7406\u5bf9\u4e8e\u5b89\u5168\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u6846\u67b6\uff0c\u5305\u62ec\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\u4e24\u5927\u652f\u67f1\uff0c\u4ee5\u53ca\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u4eba\u7c7b\u671f\u671b\uff0c\u540c\u65f6\u8ba9\u4eba\u7c7b\u7406\u89e3\u548c\u63a7\u5236\u673a\u5668\u4eba\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u51cf\u5c11\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4fe1\u4efb\u504f\u5dee\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u4e0e\u6548\u679c\u3002"}}
{"id": "2508.13982", "pdf": "https://arxiv.org/pdf/2508.13982", "abs": "https://arxiv.org/abs/2508.13982", "authors": ["Sydney Thompson", "Kate Candon", "Marynel V\u00e1zquez"], "title": "The Social Context of Human-Robot Interactions", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "I.2.9; I.2"], "comment": "To be published in Annual Review of Control, Robotics, and Autonomous\n  Systems", "summary": "The Human-Robot Interaction (HRI) community often highlights the social\ncontext of an interaction as a key consideration when designing, implementing,\nand evaluating robot behavior. Unfortunately, researchers use the term \"social\ncontext\" in varied ways. This can lead to miscommunication, making it\nchallenging to draw connections between related work on understanding and\nmodeling the social contexts of human-robot interactions. To address this gap,\nwe survey the HRI literature for existing definitions and uses of the term\n\"social context\". Then, we propose a conceptual model for describing the social\ncontext of a human-robot interaction. We apply this model to existing work, and\nwe discuss a range of attributes of social contexts that can help researchers\nplan for interactions, develop behavior models for robots, and gain insights\nafter interactions have taken place. We conclude with a discussion of open\nresearch questions in relation to understanding and modeling the social\ncontexts of human-robot interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8c03\u67e5HRI\u6587\u732e\u4e2d\u201c\u793e\u4ea4\u60c5\u5883\u201d\u7684\u5b9a\u4e49\u548c\u4f7f\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u793e\u4ea4\u60c5\u5883\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "HRI\u9886\u57df\u4e2d\u201c\u793e\u4ea4\u60c5\u5883\u201d\u4e00\u8bcd\u4f7f\u7528\u4e0d\u7edf\u4e00\uff0c\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u548c\u7814\u7a76\u96be\u4ee5\u8fde\u63a5\uff0c\u56e0\u6b64\u9700\u8981\u660e\u786e\u5176\u5b9a\u4e49\u548c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u67e5\u68b3\u7406\u201c\u793e\u4ea4\u60c5\u5883\u201d\u7684\u73b0\u6709\u5b9a\u4e49\u548c\u4f7f\u7528\uff0c\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u73b0\u6709\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u793e\u4ea4\u60c5\u5883\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u89c4\u5212\u4ea4\u4e92\u3001\u5f00\u53d1\u673a\u5668\u4eba\u884c\u4e3a\u6a21\u578b\u53ca\u4e8b\u540e\u5206\u6790\u3002", "conclusion": "\u603b\u7ed3\u4e86\u793e\u4ea4\u60c5\u5883\u5efa\u6a21\u7684\u5f00\u653e\u7814\u7a76\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.13998", "pdf": "https://arxiv.org/pdf/2508.13998", "abs": "https://arxiv.org/abs/2508.13998", "authors": ["Yifu Yuan", "Haiqin Cui", "Yaoting Huang", "Yibin Chen", "Fei Ni", "Zibin Dong", "Pengyi Li", "Yan Zheng", "Jianye Hao"], "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Embodied-R1 technical report", "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u6307\u5411\u201d\u4f5c\u4e3a\u7edf\u4e00\u4e2d\u95f4\u8868\u793a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u5177\u8eabAI\u4e2d\u7684\u201c\u770b\u5230\u5230\u505a\u5230\u201d\u5dee\u8ddd\u95ee\u9898\u3002\u901a\u8fc7\u8bbe\u8ba1Embodied-R1\u6a21\u578b\u548c\u5927\u578b\u6570\u636e\u96c6Embodied-Points-200K\uff0c\u7ed3\u5408\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5177\u8eabAI\u4e2d\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u5177\u8eab\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4e2d\u95f4\u8868\u793a\u201c\u6307\u5411\u201d\u6765\u5f25\u5408\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4e0e\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faEmbodied-R1\u6a21\u578b\uff0c\u6784\u5efa\u5927\u578b\u6570\u636e\u96c6Embodied-Points-200K\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u572811\u4e2a\u5177\u8eab\u7a7a\u95f4\u548c\u6307\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u663e\u8457\uff08SIMPLEREnv\u4e2d56.2%\u6210\u529f\u7387\uff0c8\u4e2aXArm\u4efb\u52a1\u4e2d87.5%\u6210\u529f\u7387\uff09\uff0c62%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6307\u5411\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\u4e0eRFT\u8bad\u7ec3\u8303\u5f0f\u4e3a\u673a\u5668\u4eba\u4e2d\u611f\u77e5-\u52a8\u4f5c\u5dee\u8ddd\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14042", "pdf": "https://arxiv.org/pdf/2508.14042", "abs": "https://arxiv.org/abs/2508.14042", "authors": ["Zhuoling Li", "Xiaoyang Wu", "Zhenhua Xu", "Hengshuang Zhao"], "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGEM\u7684\u71b5\u57fa\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5c11\u91cf\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u52a8\u6001\u7269\u4f53\u64cd\u7eb5\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7531\u4e8e\u52a8\u6001\u7269\u4f53\u64cd\u7eb5\u5728\u63d0\u5347\u5236\u9020\u6548\u7387\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6f14\u793a\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u6f14\u793a\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u71b5\u57fa\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u8be5\u6846\u67b6\u63d0\u51fa\u4e86GEM\u7cfb\u7edf\u3002", "result": "GEM\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u771f\u5b9e\u9910\u5385\u7684\u9910\u5177\u6536\u96c6\u4efb\u52a1\uff0c\u6210\u529f\u7387\u8d85\u8fc797%\u3002", "conclusion": "GEM\u8bc1\u660e\u4e86\u5728\u5c11\u91cf\u6f14\u793a\u4e0b\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u52a8\u6001\u7269\u4f53\u64cd\u7eb5\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2409.11041", "pdf": "https://arxiv.org/pdf/2409.11041", "abs": "https://arxiv.org/abs/2409.11041", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming", "categories": ["cs.CL", "cs.RO"], "comment": "Accepted to ITL4HRI workshop at RO-MAN 2025 conference", "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u751f\u6210\u4ee3\u7801\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u534f\u4f5c\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u5c40\u9650\u6027\u3002\u5b9a\u4e49\u4e86RATS\u4efb\u52a1\uff0c\u521b\u5efa\u6570\u636e\u96c6\u8bc4\u4f30LLMs\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u751f\u6210\u57fa\u672c\u6307\u4ee4\u5e8f\u5217\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u62bd\u8c61\u4ee3\u7801\u751f\u6210\u4e0a\u4ecd\u6709\u56f0\u96be\u3002", "motivation": "\u4f20\u7edf\u534f\u4f5c\u673a\u5668\u4eba\u7f16\u7a0b\u9700\u8981\u4e13\u5bb6\u6216\u624b\u52a8\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u8868\u8fbe\u6027\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5b9a\u4e49RATS\u4efb\u52a1\uff0c\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5229\u7528LLMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u751f\u6210\u4ee3\u7801\u5e76\u8bc4\u4f30\u5176\u80fd\u529b\u3002", "result": "LLMs\u80fd\u751f\u6210\u51c6\u786e\u7684\u6307\u4ee4\u5e8f\u5217\uff08\u4e00\u7ea7\u4ee3\u7801\uff09\uff0c\u4f46\u5728\u751f\u6210\u62bd\u8c61\u4ee3\u7801\uff08\u5982\u51fd\u6570\u3001\u5faa\u73af\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "LLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u652f\u6301\u66f4\u590d\u6742\u7684\u7f16\u7a0b\u9700\u6c42\u3002"}}
{"id": "2508.13339", "pdf": "https://arxiv.org/pdf/2508.13339", "abs": "https://arxiv.org/abs/2508.13339", "authors": ["Eugene T. Hamzezadeh", "Andrew J. Petruska"], "title": "Observed Control -- Linearly Scalable Nonlinear Model Predictive Control with Adaptive Horizons", "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY", "49M29 (Primary) 93B45, 93B52, 93B53 (Secondary)"], "comment": "16 pages, 8 figures. Submitted to IEEE Transactions on Automatic\n  Control 8/17/2025", "summary": "This work highlights the duality between state estimation methods and model\npredictive control. A predictive controller, observed control, is presented\nthat uses this duality to efficiently compute control actions with linear\ntime-horizon length scalability. The proposed algorithms provide exceptional\ncomputational efficiency, adaptive time horizon lengths, and early optimization\ntermination criteria. The use of Kalman smoothers as the backend optimization\nframework provides for a straightforward implementation supported by strong\ntheoretical guarantees. Additionally, a formulation is presented that separates\nlinear model predictive control into purely reactive and anticipatory\ncomponents, enabling any-time any-horizon observed control while ensuring\ncontroller stability for short time horizons. Finally, numerical case studies\nconfirm that nonlinear filter extensions, i.e., the extended Kalman filter and\nunscented Kalman filter, effectively extend observed control to nonlinear\nsystems and objectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e4b\u95f4\u7684\u5bf9\u5076\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u5177\u5907\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u81ea\u9002\u5e94\u65f6\u95f4\u8303\u56f4\u548c\u65e9\u671f\u7ec8\u6b62\u4f18\u5316\u7b49\u7279\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u72b6\u6001\u4f30\u8ba1\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u5bf9\u5076\u6027\uff0c\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u5b9e\u73b0\u7684\u63a7\u5236\u5668\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u5361\u5c14\u66fc\u5e73\u6ed1\u5668\u4f5c\u4e3a\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5206\u89e3\u4e3a\u53cd\u5e94\u6027\u548c\u524d\u77bb\u6027\u4e24\u90e8\u5206\uff0c\u5e76\u6269\u5c55\u5230\u975e\u7ebf\u6027\u7cfb\u7edf\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u975e\u7ebf\u6027\u6ee4\u6ce2\u5668\uff08\u5982\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u6210\u529f\u5c06\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u6269\u5c55\u5230\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u5668\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u7cfb\u7edf\u548c\u65f6\u95f4\u8303\u56f4\u3002"}}
{"id": "2508.13564", "pdf": "https://arxiv.org/pdf/2508.13564", "abs": "https://arxiv.org/abs/2508.13564", "authors": ["Zheng Tang", "Shuo Wang", "David C. Anastasiu", "Ming-Ching Chang", "Anuj Sharma", "Quan Kong", "Norimasa Kobori", "Munkhjargal Gochoo", "Ganzorig Batnasan", "Munkh-Erdene Otgonbold", "Fady Alnajjar", "Jun-Wei Hsieh", "Tomasz Kornuta", "Xiaolong Li", "Yilin Zhao", "Han Zhang", "Subhashree Radhakrishnan", "Arihant Jain", "Ratnesh Kumar", "Vidya N. Murali", "Yuxing Wang", "Sameer Satish Pusegaonkar", "Yizhou Wang", "Sujit Biswas", "Xunlei Wu", "Zhedong Zheng", "Pranamesh Chakraborty", "Rama Chellappa"], "title": "The 9th AI City Challenge", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Summary of the 9th AI City Challenge Workshop in conjunction with\n  ICCV 2025", "summary": "The ninth AI City Challenge continues to advance real-world applications of\ncomputer vision and AI in transportation, industrial automation, and public\nsafety. The 2025 edition featured four tracks and saw a 17% increase in\nparticipation, with 245 teams from 15 countries registered on the evaluation\nserver. Public release of challenge datasets led to over 30,000 downloads to\ndate. Track 1 focused on multi-class 3D multi-camera tracking, involving\npeople, humanoids, autonomous mobile robots, and forklifts, using detailed\ncalibration and 3D bounding box annotations. Track 2 tackled video question\nanswering in traffic safety, with multi-camera incident understanding enriched\nby 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic\nwarehouse environments, requiring AI systems to interpret RGB-D inputs and\nanswer spatial questions that combine perception, geometry, and language. Both\nTrack 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4\nemphasized efficient road object detection from fisheye cameras, supporting\nlightweight, real-time deployment on edge devices. The evaluation framework\nenforced submission limits and used a partially held-out test set to ensure\nfair benchmarking. Final rankings were revealed after the competition\nconcluded, fostering reproducibility and mitigating overfitting. Several teams\nachieved top-tier results, setting new benchmarks in multiple tasks.", "AI": {"tldr": "2025\u5e74AI City\u6311\u6218\u8d5b\u805a\u7126\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u4ea4\u901a\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u516c\u5171\u5b89\u5168\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5305\u542b\u56db\u4e2a\u8d5b\u9053\uff0c\u53c2\u8d5b\u56e2\u961f\u663e\u8457\u589e\u52a0\uff0c\u6570\u636e\u96c6\u4e0b\u8f7d\u91cf\u8d85\u8fc73\u4e07\u6b21\u3002", "motivation": "\u63a8\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAI\u5728\u4ea4\u901a\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u53ca\u516c\u5171\u5b89\u5168\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4fc3\u8fdb\u6280\u672f\u53d1\u5c55\u4e0e\u521b\u65b0\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u8d5b\u9053\uff083D\u591a\u76ee\u6807\u8ddf\u8e2a\u3001\u4ea4\u901a\u89c6\u9891\u95ee\u7b54\u3001\u4ed3\u50a8\u7a7a\u95f4\u63a8\u7406\u3001\u9c7c\u773c\u6444\u50cf\u5934\u76ee\u6807\u68c0\u6d4b\uff09\uff0c\u5229\u7528\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u7ade\u8d5b\u3002", "result": "\u53c2\u8d5b\u56e2\u961f\u6570\u91cf\u589e\u52a017%\uff0c\u591a\u4e2a\u56e2\u961f\u53d6\u5f97\u9876\u5c16\u6210\u7ee9\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8bbe\u7acb\u65b0\u6807\u6746\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u6280\u672f\u8fdb\u6b65\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.13656", "pdf": "https://arxiv.org/pdf/2508.13656", "abs": "https://arxiv.org/abs/2508.13656", "authors": ["Georg Schildbach", "Jasper Pflughaupt"], "title": "AutoMPC: A Code Generator for MPC-based Automated Driving", "categories": ["eess.SY", "cs.MS", "cs.RO", "cs.SY", "93-04"], "comment": "Technical Documentation", "summary": "Model Predictive Control (MPC) is a powerful technique to control nonlinear,\nmulti-input multi-output systems subject to input and state constraints. It is\nnow a standard tool for trajectory tracking control of automated vehicles. As\nsuch it has been used in many research and development projects. However, MPC\nfaces several challenges to be integrated into industrial production vehicles.\nThe most important ones are its high computational demands and the complexity\nof implementation. The software packages AutoMPC aims to address both of these\nchallenges. It builds on a robustified version of an active set algorithm for\nNonlinear MPC. The algorithm is embedded into a framework for vehicle\ntrajectory tracking, which makes it easy to used, yet highly customizable.\nAutomatic code generation transforms the selections into a standalone,\ncomputationally efficient C-code file with static memory allocation. As such it\ncan be readily deployed on a wide range of embedded platforms, e.g., based on\nMatlab/Simulink or Robot Operating System (ROS). Compared to a previous version\nof the code, the vehicle model and the numerical integration method can be\nmanually specified, besides basic algorithm parameters. All of this information\nand all specifications are directly baked into the generated C-code. The\nalgorithm is suitable driving scenarios at low or high speeds, even drifting,\nand supports direction changes. Multiple simulation scenarios show the\nversatility and effectiveness of the AutoMPC code, with the guarantee of a\nfeasible solution, a high degree of robustness, and computational efficiency.", "AI": {"tldr": "AutoMPC\u8f6f\u4ef6\u5305\u901a\u8fc7\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u548c\u9ad8\u6548\u7684C\u4ee3\u7801\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027MPC\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u5b9e\u73b0\u590d\u6742\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "MPC\u5728\u5de5\u4e1a\u8f66\u8f86\u96c6\u6210\u4e2d\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u548c\u5b9e\u73b0\u590d\u6742\u6027\u7684\u6311\u6218\uff0cAutoMPC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9c81\u68d2\u5316\u7684\u4e3b\u52a8\u96c6\u7b97\u6cd5\uff0c\u5d4c\u5165\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u6846\u67b6\uff0c\u652f\u6301\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u548c\u81ea\u5b9a\u4e49\u914d\u7f6e\u3002", "result": "AutoMPC\u5728\u591a\u79cd\u4eff\u771f\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u548c\u4f4e\u901f\u9a7e\u9a76\u3002", "conclusion": "AutoMPC\u4e3a\u975e\u7ebf\u6027MPC\u7684\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13775", "pdf": "https://arxiv.org/pdf/2508.13775", "abs": "https://arxiv.org/abs/2508.13775", "authors": ["Anas Gouda", "Shrutarv Awasthi", "Christian Blesing", "Lokeshwaran Manohar", "Frank Hoffmann", "Alice Kirchheim"], "title": "MR6D: Benchmarking 6D Pose Estimation for Mobile Robots", "categories": ["cs.CV", "cs.RO"], "comment": "accepted CVPR 2025 Workshop on Recovering 6D Object Pose (R6D)", "summary": "Existing 6D pose estimation datasets primarily focus on small household\nobjects typically handled by robot arm manipulators, limiting their relevance\nto mobile robotics. Mobile platforms often operate without manipulators,\ninteract with larger objects, and face challenges such as long-range\nperception, heavy self-occlusion, and diverse camera perspectives. While recent\nmodels generalize well to unseen objects, evaluations remain confined to\nhousehold-like settings that overlook these factors. We introduce MR6D, a\ndataset designed for 6D pose estimation for mobile robots in industrial\nenvironments. It includes 92 real-world scenes featuring 16 unique objects\nacross static and dynamic interactions. MR6D captures the challenges specific\nto mobile platforms, including distant viewpoints, varied object\nconfigurations, larger object sizes, and complex occlusion/self-occlusion\npatterns. Initial experiments reveal that current 6D pipelines underperform in\nthese settings, with 2D segmentation being another hurdle. MR6D establishes a\nfoundation for developing and evaluating pose estimation methods tailored to\nthe demands of mobile robotics. The dataset is available at\nhttps://huggingface.co/datasets/anas-gouda/mr6d.", "AI": {"tldr": "MR6D\u6570\u636e\u96c6\u9488\u5bf9\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba6D\u59ff\u6001\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u5c0f\u578b\u5bb6\u7528\u7269\u4f53\u7684\u5c40\u9650\u6027\uff0c\u6db5\u76d6\u4e86\u8fdc\u8ddd\u79bb\u89c6\u89d2\u3001\u5927\u7269\u4f53\u5c3a\u5bf8\u548c\u590d\u6742\u906e\u6321\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u67096D\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\u4e3b\u8981\u9488\u5bf9\u5c0f\u578b\u5bb6\u7528\u7269\u4f53\uff0c\u65e0\u6cd5\u6ee1\u8db3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u9700\u6c42\u3002", "method": "\u4ecb\u7ecd\u4e86MR6D\u6570\u636e\u96c6\uff0c\u5305\u542b92\u4e2a\u771f\u5b9e\u573a\u666f\u548c16\u4e2a\u72ec\u7279\u7269\u4f53\uff0c\u6db5\u76d6\u9759\u6001\u548c\u52a8\u6001\u4ea4\u4e92\uff0c\u7279\u522b\u5173\u6ce8\u8fdc\u8ddd\u79bb\u89c6\u89d2\u3001\u5927\u7269\u4f53\u5c3a\u5bf8\u548c\u590d\u6742\u906e\u6321\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u67096D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u8fd9\u4e9b\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f2D\u5206\u5272\u6210\u4e3a\u74f6\u9888\u3002", "conclusion": "MR6D\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2508.13802", "pdf": "https://arxiv.org/pdf/2508.13802", "abs": "https://arxiv.org/abs/2508.13802", "authors": ["Andreas Mueller"], "title": "A Screw Approach to the Approximation of the Local Geometry of the Configuration Space and of the set of Configurations of Certain Rank of Lower Pair Linkages", "categories": ["math.DG", "cs.NA", "cs.RO", "math.NA"], "comment": null, "summary": "A motion of a mechanism is a curve in its configuration space (c-space).\nSingularities of the c-space are kinematic singularities of the mechanism. Any\nmobility analysis of a particular mechanism amounts to investigating the\nc-space geometry at a given configuration. A higher-order analysis is necessary\nto determine the finite mobility. To this end, past research lead to approaches\nusing higher-order time derivatives of loop closure constraints assuming\n(implicitly) that all possible motions are smooth. This continuity assumption\nlimits the generality of these methods. In this paper an approach to the\nhigher-order local mobility analysis of lower pair multi-loop linkages is\npresented. This is based on a higher-order Taylor series expansion of the\ngeometric constraint mapping, for which a recursive algebraic expression in\nterms of joint screws is presented. An exhaustive local analysis includes\nanalysis of the set of constraint singularities (configurations where the\nconstraint Jacobian has certain corank). A local approximation of the set of\nconfigurations with certain rank is presented, along with an explicit\nexpression for the differentials of Jacobian minors in terms of instantaneous\njoint screws. The c-space and the set of points of certain corank are therewith\nlocally approximated by an algebraic variety determined algebraically from the\nmechanism's screw system. Results are shown for a simple planar 4-bar linkage,\nwhich exhibits a bifurcation singularity, and for a planar three-loop linkage\nexhibiting a cusp in c-space. The latter cannot be treated by the higher-order\nlocal analysis methods proposed in the literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9636\u5c40\u90e8\u79fb\u52a8\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u57fa\u4e8e\u51e0\u4f55\u7ea6\u675f\u6620\u5c04\u7684\u9ad8\u9636\u6cf0\u52d2\u5c55\u5f00\uff0c\u5e76\u7ed9\u51fa\u4e86\u4ee5\u5173\u8282\u87ba\u65cb\u8868\u793a\u7684\u9012\u5f52\u4ee3\u6570\u8868\u8fbe\u5f0f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u8fd0\u52a8\u90fd\u662f\u5149\u6ed1\u7684\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u9ad8\u9636\u5c40\u90e8\u79fb\u52a8\u6027\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9ad8\u9636\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u7684\u51e0\u4f55\u7ea6\u675f\u6620\u5c04\uff0c\u5e76\u5229\u7528\u5173\u8282\u87ba\u65cb\u7684\u9012\u5f52\u4ee3\u6570\u8868\u8fbe\u5f0f\u8fdb\u884c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5c40\u90e8\u8fd1\u4f3c\u5206\u6790\u4e86\u7ea6\u675f\u5947\u5f02\u70b9\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5e73\u97624\u6746\u673a\u6784\u548c\u4e09\u73af\u673a\u6784\u7684\u793a\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742\u5947\u5f02\u70b9\uff0c\u5982c-space\u4e2d\u7684\u5c16\u70b9\u3002"}}
{"id": "2508.14006", "pdf": "https://arxiv.org/pdf/2508.14006", "abs": "https://arxiv.org/abs/2508.14006", "authors": ["Mohamed Abouagour", "Eleftherios Garyfallidis"], "title": "ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans", "categories": ["cs.CV", "cs.RO", "68T45"], "comment": "18 pages, 3 figures, 4 tables", "summary": "We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally\nrich, and realistic residential floor plans, created to advance spatial AI\nresearch. Each plan includes precise annotations of architectural elements\n(walls, doors, windows, balconies) and functional spaces (such as kitchens,\nbedrooms, and bathrooms). ResPlan addresses key limitations of existing\ndatasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024)\nby offering enhanced visual fidelity and greater structural diversity,\nreflecting realistic and non-idealized residential layouts. Designed as a\nversatile, general-purpose resource, ResPlan supports a wide range of\napplications including robotics, reinforcement learning, generative AI, virtual\nand augmented reality, simulations, and game development. Plans are provided in\nboth geometric and graph-based formats, enabling direct integration into\nsimulation engines and fast 3D conversion. A key contribution is an open-source\npipeline for geometry cleaning, alignment, and annotation refinement.\nAdditionally, ResPlan includes structured representations of room connectivity,\nsupporting graph-based spatial reasoning tasks. Finally, we present comparative\nanalyses with existing benchmarks and outline several open benchmark tasks\nenabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale,\nrealism, and usability, providing a robust foundation for developing and\nbenchmarking next-generation spatial intelligence systems.", "AI": {"tldr": "ResPlan \u662f\u4e00\u4e2a\u5305\u542b 17,000 \u4efd\u8be6\u7ec6\u4f4f\u5b85\u5e73\u9762\u56fe\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u7a7a\u95f4 AI \u7814\u7a76\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u7ed3\u6784\u548c\u591a\u6837\u6027\u7684\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5982 RPLAN \u548c MSD \u5b58\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cResPlan \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u51e0\u4f55\u548c\u56fe\u5f62\u4e24\u79cd\u683c\u5f0f\u7684\u5e73\u9762\u56fe\uff0c\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u94fe\u7528\u4e8e\u51e0\u4f55\u6e05\u7406\u548c\u5bf9\u9f50\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u56fe\u7684\u63a8\u7406\u4efb\u52a1\u3002", "result": "ResPlan \u5728\u89c4\u6a21\u3001\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u663e\u8457\u8fdb\u6b65\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7a7a\u95f4\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "ResPlan \u901a\u8fc7\u589e\u5f3a\u7684\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u4e3a\u591a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u901a\u7528\u8d44\u6e90\uff0c\u5e76\u652f\u6301\u5f00\u653e\u57fa\u51c6\u4efb\u52a1\u7684\u5f00\u53d1\u3002"}}
