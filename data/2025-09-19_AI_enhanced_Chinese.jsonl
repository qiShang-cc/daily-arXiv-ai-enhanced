{"id": "2509.14295", "pdf": "https://arxiv.org/pdf/2509.14295", "abs": "https://arxiv.org/abs/2509.14295", "authors": ["Fanqi Kong", "Ruijie Zhang", "Huaxiao Yin", "Guibin Zhang", "Xiaofei Zhang", "Ziang Chen", "Zhaowei Zhang", "Xiaoyuan Zhang", "Song-Chun Zhu", "Xue Feng"], "title": "AEGIS: Automated Error Generation and Identification for Multi-Agent Systems", "categories": ["cs.RO"], "comment": null, "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAEGIS\u7684\u81ea\u52a8\u5316\u9519\u8bef\u751f\u6210\u4e0e\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u9519\u8bef\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u5b66\u4e60\u8303\u5f0f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u4e3b\u6027\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u7406\u89e3\u5176\u9519\u8bef\u6a21\u5f0f\u5bf9\u786e\u4fdd\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u6807\u6ce8\u7cbe\u786e\u7684\u9519\u8bef\u6570\u636e\u96c6\u800c\u53d7\u9650\u3002", "method": "AEGIS\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5411\u6210\u529f\u8f68\u8ff9\u4e2d\u6ce8\u5165\u53ef\u63a7\u4e14\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\uff0c\u521b\u5efa\u4e30\u5bcc\u7684\u771f\u5b9e\u5931\u8d25\u6570\u636e\u96c6\u3002\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u64cd\u7eb5\u5668\u8fdb\u884c\u9ad8\u7ea7\u653b\u51fb\uff08\u5982\u63d0\u793a\u6ce8\u5165\u548c\u54cd\u5e94\u635f\u574f\uff09\u4ee5\u8bf1\u5bfc\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528AEGIS\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\u4e2d\u5747\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u90e8\u5206\u6a21\u578b\u6027\u80fd\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u7684\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "AEGIS\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\uff0c\u5176\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u6846\u67b6\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.14303", "pdf": "https://arxiv.org/pdf/2509.14303", "abs": "https://arxiv.org/abs/2509.14303", "authors": ["Hao Jiang", "Zhipeng Zhang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Yuwen Heng", "Shuo Wang", "Jinhao Chai", "Zhuo Chen", "Hao Zhao", "Hao Sun", "Xi Zhang", "Anqing Jiang", "Chuan Hu"], "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent advances in end-to-end autonomous driving leverage multi-view images\nto construct BEV representations for motion planning. In motion planning,\nautonomous vehicles need considering both hard constraints imposed by\ngeometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,\nrule-based semantics with no explicit geometry (e.g., lane boundaries, traffic\npriors). However, existing end-to-end frameworks typically rely on BEV features\nlearned in an implicit manner, lacking explicit modeling of risk and guidance\npriors for safe and interpretable planning. To address this, we propose\nFlowDrive, a novel framework that introduces physically interpretable\nenergy-based flow fields-including risk potential and lane attraction fields-to\nencode semantic priors and safety cues into the BEV space. These flow-aware\nfeatures enable adaptive refinement of anchor trajectories and serve as\ninterpretable guidance for trajectory generation. Moreover, FlowDrive decouples\nmotion intent prediction from trajectory denoising via a conditional diffusion\nplanner with feature-level gating, alleviating task interference and enhancing\nmultimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that\nFlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,\nsurpassing prior baselines in both safety and planning quality. The project is\navailable at https://astrixdrive.github.io/FlowDrive.github.io/.", "AI": {"tldr": "FlowDrive\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u6d41\u573a\uff08\u5982\u98ce\u9669\u52bf\u80fd\u548c\u8f66\u9053\u5438\u5f15\u529b\u573a\uff09\u6765\u589e\u5f3aBEV\u7a7a\u95f4\u7684\u8bed\u4e49\u5148\u9a8c\u548c\u5b89\u5168\u63d0\u793a\uff0c\u63d0\u5347\u8f68\u8ff9\u89c4\u5212\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7f3a\u4e4f\u5bf9\u98ce\u9669\u548c\u8bed\u4e49\u5148\u9a8c\u7684\u663e\u5f0f\u5efa\u6a21\uff0cFlowDrive\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u89c4\u5212\u3002", "method": "FlowDrive\u901a\u8fc7\u80fd\u91cf\u6d41\u573a\u7f16\u7801\u8bed\u4e49\u548c\u5b89\u5168\u4fe1\u606f\uff0c\u5229\u7528\u6761\u4ef6\u6269\u6563\u89c4\u5212\u5668\u5206\u79bb\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u548c\u8f68\u8ff9\u53bb\u566a\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u591a\u6837\u6027\u3002", "result": "\u5728NAVSIM v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowDrive\u4ee586.3\u7684EPDMS\u5f97\u5206\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u89c4\u5212\u8d28\u91cf\u3002", "conclusion": "FlowDrive\u901a\u8fc7\u663e\u5f0f\u6d41\u573a\u548c\u6761\u4ef6\u6269\u6563\u89c4\u5212\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14342", "pdf": "https://arxiv.org/pdf/2509.14342", "abs": "https://arxiv.org/abs/2509.14342", "authors": ["Bikram Pandit", "Aayam Kumar Shrestha", "Alan Fern"], "title": "Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move", "categories": ["cs.RO"], "comment": null, "summary": "We study decentralized cooperative transport using teams of N-quadruped\nrobots with arm that must pinch, lift, and move ungraspable objects through\nphysical contact alone. Unlike prior work that relies on rigid mechanical\ncoupling between robots and objects, we address the more challenging setting\nwhere mechanically independent robots must coordinate through contact forces\nalone without any communication or centralized control. To this end, we employ\na hierarchical policy architecture that separates base locomotion from arm\ncontrol, and propose a constellation reward formulation that unifies position\nand orientation tracking to enforce rigid contact behavior. The key insight is\nencouraging robots to behave as if rigidly connected to the object through\ncareful reward design and training curriculum rather than explicit mechanical\nconstraints. Our approach enables coordination through shared policy parameters\nand implicit synchronization cues - scaling to arbitrary team sizes without\nretraining. We show extensive simulation experiments to demonstrate robust\ntransport across 2-10 robots on diverse object geometries and masses, along\nwith sim2real transfer results on lightweight objects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7N\u4e2a\u56db\u8db3\u673a\u5668\u4eba\u56e2\u961f\u5728\u65e0\u901a\u4fe1\u6216\u96c6\u4e2d\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u901a\u8fc7\u63a5\u89e6\u529b\u534f\u8c03\u8fd0\u8f93\u65e0\u6cd5\u6293\u53d6\u7684\u7269\u4f53\u3002\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u67b6\u6784\u548c\u661f\u5ea7\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u9690\u5f0f\u540c\u6b65\u3002", "motivation": "\u7814\u7a76\u591a\u673a\u5668\u4eba\u5728\u65e0\u673a\u68b0\u8026\u5408\u6216\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u901a\u8fc7\u63a5\u89e6\u529b\u534f\u540c\u8fd0\u8f93\u7269\u4f53\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u8fde\u63a5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\u67b6\u6784\uff08\u5206\u79bb\u57fa\u7840\u79fb\u52a8\u548c\u624b\u81c2\u63a7\u5236\uff09\u548c\u661f\u5ea7\u5956\u52b1\u516c\u5f0f\uff0c\u901a\u8fc7\u5956\u52b1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u8bfe\u7a0b\u9f13\u52b1\u673a\u5668\u4eba\u8868\u73b0\u5982\u521a\u6027\u8fde\u63a5\u3002", "result": "\u57282-10\u4e2a\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u51e0\u4f55\u548c\u8d28\u91cf\u7269\u4f53\u7684\u7a33\u5065\u8fd0\u8f93\uff0c\u5e76\u5c55\u793a\u4e86\u8f7b\u91cf\u7269\u4f53\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u9690\u5f0f\u540c\u6b65\u548c\u5171\u4eab\u7b56\u7565\u53c2\u6570\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u901a\u4fe1\u7684\u591a\u673a\u5668\u4eba\u534f\u540c\u8fd0\u8f93\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u56e2\u961f\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2509.14349", "pdf": "https://arxiv.org/pdf/2509.14349", "abs": "https://arxiv.org/abs/2509.14349", "authors": ["Zhengyang Kris Weng", "Matthew L. Elwin", "Han Liu"], "title": "LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "We introduce LeVR, a modular software framework designed to bridge two\ncritical gaps in robotic imitation learning. First, it provides robust and\nintuitive virtual reality (VR) teleoperation for data collection using robot\narms paired with dexterous hands, addressing a common limitation in existing\nsystems. Second, it natively integrates with the powerful LeRobot imitation\nlearning (IL) framework, enabling the use of VR-based teleoperation data and\nstreamlining the demonstration collection process. To demonstrate LeVR, we\nrelease LeFranX, an open-source implementation for the Franka FER arm and\nRobotEra XHand, two widely used research platforms. LeFranX delivers a\nseamless, end-to-end workflow from data collection to real-world policy\ndeployment. We validate our system by collecting a public dataset of 100 expert\ndemonstrations and use it to successfully fine-tune state-of-the-art visuomotor\npolicies. We provide our open-source framework, implementation, and dataset to\naccelerate IL research for the robotics community.", "AI": {"tldr": "LeVR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u8f6f\u4ef6\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u63d0\u4f9b\u57fa\u4e8eVR\u7684\u8fdc\u7a0b\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\uff0c\u5e76\u4e0eLeRobot\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u4e2d\u673a\u5668\u4eba\u624b\u81c2\u4e0e\u7075\u5de7\u624b\u7ed3\u5408\u7684\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u6f14\u793a\u6570\u636e\u7684\u6536\u96c6\u6d41\u7a0b\u3002", "method": "LeVR\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u8fdc\u7a0b\u64cd\u4f5c\u6536\u96c6\u6570\u636e\uff0c\u5e76\u4e0eLeRobot\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u96c6\u6210\uff0c\u53d1\u5e03\u5f00\u6e90\u7684LeFranX\u5b9e\u73b0\u7528\u4e8eFranka FER\u624b\u81c2\u548cRobotEra XHand\u3002", "result": "\u6210\u529f\u6536\u96c6\u4e86100\u4e2a\u4e13\u5bb6\u6f14\u793a\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u5148\u8fdb\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "conclusion": "LeVR\u6846\u67b6\u53ca\u5176\u5f00\u6e90\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u793e\u533a\u7684\u6a21\u4eff\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.14240", "pdf": "https://arxiv.org/pdf/2509.14240", "abs": "https://arxiv.org/abs/2509.14240", "authors": ["Nafize Ishtiaque Hossain", "Kundan Saha", "Atul Sharma", "Sameer Sonkusale"], "title": "In Planta Tattoo and Kirigami Sensors for Self-Powered Monitoring of Vapor Pressure Deficit and Growth Dynamics", "categories": ["eess.SP"], "comment": null, "summary": "We report a scalable, self-powered in planta sensor platform for continuous\nmonitoring of plant hydration and growth. The system integrates two components\na leaf mounted tattoo sensor for estimating vapor pressure deficit and a\nkirigami inspired strain sensor for tracking radial stem growth. Uniquely, the\ntattoo sensor serves a dual function measuring temperature and humidity beneath\nthe leaf surface while simultaneously harvesting power from ambient moisture\nvia a vanadium pentoxide nanosheet membrane. This moist-electric generator\nconfiguration enables energy-autonomous operation, delivering a power density\nof 0.1114 miroW per square cm. The V2O5 based sensor exhibits high sensitivity\nto humidity and temperature, enabling accurate VPD estimation for over 10 days\nuntil leaf senescence. The eutectogel based kirigami strain sensor, wrapped\naround the stem, offers a gauge factor of 1.5 and immunity to unrelated\nmechanical disturbances, allowing continuous growth tracking for more than 20\ndays. Both sensors are fabricated via cleanroom-free, roll to roll compatible\nmethods, underscoring their potential for large-scale agricultural deployment\nto monitor abiotic stress and improve crop management.", "AI": {"tldr": "\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u4f9b\u7535\u690d\u7269\u4f20\u611f\u5668\u5e73\u53f0\uff0c\u7528\u4e8e\u6301\u7eed\u76d1\u6d4b\u690d\u7269\u6c34\u5206\u548c\u751f\u957f\uff0c\u96c6\u6210\u53f6\u9762\u7eb9\u8eab\u4f20\u611f\u5668\u548c\u830e\u5e72\u5e94\u53d8\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u81ea\u4e3b\u4f9b\u7535\u548c\u957f\u671f\u76d1\u6d4b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u7535\u6e90\u3001\u53ef\u6301\u7eed\u76d1\u6d4b\u690d\u7269\u6c34\u5206\u548c\u751f\u957f\u7684\u4f20\u611f\u5668\uff0c\u4ee5\u6539\u5584\u519c\u4e1a\u7ba1\u7406\u5e76\u76d1\u6d4b\u975e\u751f\u7269\u80c1\u8feb\u3002", "method": "\u7ed3\u5408\u53f6\u9762\u7eb9\u8eab\u4f20\u611f\u5668\uff08\u6d4b\u91cf\u6e29\u6e7f\u5ea6\u5e76\u5229\u7528\u6c34\u5206\u53d1\u7535\uff09\u548c\u830e\u5e72\u5e94\u53d8\u4f20\u611f\u5668\uff08\u8ddf\u8e2a\u830e\u5e72\u751f\u957f\uff09\uff0c\u91c7\u7528\u65e0\u6d01\u51c0\u5ba4\u5236\u9020\u65b9\u6cd5\u3002", "result": "\u7eb9\u8eab\u4f20\u611f\u5668\u63d0\u4f9b0.1114\u5fae\u74e6/\u5e73\u65b9\u5398\u7c73\u7684\u529f\u7387\u5bc6\u5ea6\uff0c\u53ef\u6301\u7eed\u5de5\u4f5c10\u5929\uff1b\u5e94\u53d8\u4f20\u611f\u5668\u53ef\u6301\u7eed\u76d1\u6d4b20\u5929\u3002", "conclusion": "\u5e73\u53f0\u5177\u6709\u5927\u89c4\u6a21\u519c\u4e1a\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u4f5c\u7269\u7ba1\u7406\u548c\u975e\u751f\u7269\u80c1\u8feb\u76d1\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.14353", "pdf": "https://arxiv.org/pdf/2509.14353", "abs": "https://arxiv.org/abs/2509.14353", "authors": ["Dvij Kalaria", "Sudarshan S Harithas", "Pushkal Katara", "Sangkyung Kwak", "Sarthak Bhagat", "Shankar Sastry", "Srinath Sridhar", "Sai Vemprala", "Ashish Kapoor", "Jonathan Chung-Kuan Huang"], "title": "DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "(under submission)", "summary": "We introduce DreamControl, a novel methodology for learning autonomous\nwhole-body humanoid skills. DreamControl leverages the strengths of diffusion\nmodels and Reinforcement Learning (RL): our core innovation is the use of a\ndiffusion prior trained on human motion data, which subsequently guides an RL\npolicy in simulation to complete specific tasks of interest (e.g., opening a\ndrawer or picking up an object). We demonstrate that this human motion-informed\nprior allows RL to discover solutions unattainable by direct RL, and that\ndiffusion models inherently promote natural looking motions, aiding in\nsim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1\nrobot across a diverse set of challenging tasks involving simultaneous lower\nand upper body control and object interaction.", "AI": {"tldr": "DreamControl\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u8bad\u7ec3\u7684\u6269\u6563\u5148\u9a8c\u6307\u5bfcRL\u7b56\u7565\uff0c\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\u7684\u81ea\u7136\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u5b9e\u73b0\u7684\u590d\u6742\u4eba\u5f62\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u548c\u81ea\u7136\u52a8\u4f5c\u751f\u6210\u95ee\u9898\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u8bad\u7ec3\u7684\u6269\u6563\u5148\u9a8c\uff0c\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6a21\u62df\u4e2d\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u52a8\u4f5c\u66f4\u81ea\u7136\uff0c\u4fbf\u4e8e\u4eff\u771f\u5230\u5b9e\u9645\u7684\u8fc1\u79fb\u3002", "conclusion": "DreamControl\u901a\u8fc7\u6269\u6563\u5148\u9a8c\u4e0eRL\u7ed3\u5408\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u81ea\u7136\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14242", "pdf": "https://arxiv.org/pdf/2509.14242", "abs": "https://arxiv.org/abs/2509.14242", "authors": ["Jinshuai Gu", "Zenghui Lin", "Jingying Ma", "Jingyu Wang", "Linyan Zhang", "Rui Bai", "Zelin Tu", "Youyou Jiang", "Donglin Xie", "Yuxi Zhou", "Guoli Liu", "Shenda Hong"], "title": "Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment\ntechnique used globally, especially in underdeveloped countries. However, it is\ncurrently mainly used to identify the fetus's current status (e.g., fetal\nacidosis or hypoxia), and the potential of CTG in predicting future adverse\npregnancy outcomes has not been fully explored. We aim to develop an AI-based\nmodel that predicts biological age from CTG time series (named CTGage), then\ncalculate the age gap between CTGage and actual age (named CTGage-gap), and use\nthis gap as a new digital biomarker for future adverse pregnancy outcomes. The\nCTGage model is developed using 61,140 records from 11,385 pregnant women,\ncollected at Peking University People's Hospital between 2018 and 2022. For\nmodel training, a structurally designed 1D convolutional neural network is\nused, incorporating distribution-aligned augmented regression technology. The\nCTGage-gap is categorized into five groups: < -21 days (underestimation group),\n-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days\n(overestimation group). We further defined the underestimation group and\noverestimation group together as the high-risk group. We then compare the\nincidence of adverse outcomes and maternal diseases across these groups. The\naverage absolute error of the CTGage model is 10.91 days. When comparing the\noverestimation group with the normal group, premature infants incidence is\n5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is\n31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the\nnormal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and\nanaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial\nintelligence-derived CTGage can predict the future risk of adverse pregnancy\noutcomes and hold potential as a novel, non-invasive, and easily accessible\ndigital biomarker.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCTG\u6570\u636e\u7684AI\u6a21\u578b\uff08CTGage\uff09\uff0c\u901a\u8fc7\u8ba1\u7b97\u9884\u6d4b\u4e0e\u5b9e\u9645\u5e74\u9f84\u7684\u5dee\u8ddd\uff08CTGage-gap\uff09\uff0c\u4f5c\u4e3a\u9884\u6d4b\u4e0d\u826f\u598a\u5a20\u7ed3\u5c40\u7684\u65b0\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u63a2\u7d22CTG\u9884\u6d4b\u672a\u6765\u4e0d\u826f\u598a\u5a20\u7ed3\u5c40\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5f53\u524d\u4ec5\u7528\u4e8e\u8bc4\u4f30\u80ce\u513f\u5373\u65f6\u72b6\u6001\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u752861,140\u6761CTG\u8bb0\u5f55\uff0c\u8bbe\u8ba11D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u5206\u5e03\u5bf9\u9f50\u589e\u5f3a\u56de\u5f52\u6280\u672f\uff0c\u5c06CTGage-gap\u5206\u4e3a\u4e94\u7ec4\u5206\u6790\u98ce\u9669\u3002", "result": "CTGage\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee10.91\u5929\uff0c\u9ad8\u98ce\u9669\u7ec4\uff08\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u4f30\u8ba1\uff09\u7684\u4e0d\u826f\u598a\u5a20\u7ed3\u5c40\u548c\u6bcd\u4f53\u75be\u75c5\u53d1\u751f\u7387\u663e\u8457\u9ad8\u4e8e\u6b63\u5e38\u7ec4\u3002", "conclusion": "CTGage\u662f\u4e00\u79cd\u65b0\u578b\u3001\u975e\u4fb5\u5165\u6027\u4e14\u6613\u83b7\u53d6\u7684\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\uff0c\u80fd\u591f\u9884\u6d4b\u4e0d\u826f\u598a\u5a20\u98ce\u9669\u3002"}}
{"id": "2509.14380", "pdf": "https://arxiv.org/pdf/2509.14380", "abs": "https://arxiv.org/abs/2509.14380", "authors": ["Seoyeon Choi", "Kanghyun Ryu", "Jonghoon Ock", "Negar Mehr"], "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for\nlearning coordination in multi-agent systems. However, applying MARL to\nrobotics still remains challenging due to high-dimensional continuous joint\naction spaces, complex reward design, and non-stationary transitions inherent\nto decentralized settings. On the other hand, humans learn complex coordination\nthrough staged curricula, where long-horizon behaviors are progressively built\nupon simpler skills. Motivated by this, we propose CRAFT: Coaching\nReinforcement learning Autonomously using Foundation models for multi-robot\ncoordination Tasks, a framework that leverages the reasoning capabilities of\nfoundation models to act as a \"coach\" for multi-robot coordination. CRAFT\nautomatically decomposes long-horizon coordination tasks into sequences of\nsubtasks using the planning capability of Large Language Models (LLMs). In what\nfollows, CRAFT trains each subtask using reward functions generated by LLM, and\nrefines them through a Vision Language Model (VLM)-guided reward-refinement\nloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation\ntasks, demonstrating its capability to learn complex coordination behaviors. In\naddition, we validate the multi-quadruped navigation policy in real hardware\nexperiments.", "AI": {"tldr": "CRAFT\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f5c\u4e3a\u591a\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u7684\u201c\u6559\u7ec3\u201d\uff0c\u901a\u8fc7LLMs\u81ea\u52a8\u5206\u89e3\u4efb\u52a1\uff0c\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7VLM\u4f18\u5316\u5956\u52b1\uff0c\u6210\u529f\u5b66\u4e60\u4e86\u590d\u6742\u7684\u534f\u8c03\u884c\u4e3a\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5206\u9636\u6bb5\u5b66\u4e60\u590d\u6742\u534f\u8c03\u884c\u4e3a\uff0c\u53d7\u6b64\u542f\u53d1\uff0c\u63d0\u51faCRAFT\u6846\u67b6\u4ee5\u89e3\u51b3MARL\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u548c\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u7b49\u6311\u6218\u3002", "method": "CRAFT\u5229\u7528LLMs\u5206\u89e3\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\uff0c\u751f\u6210\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7VLM\u4f18\u5316\u5956\u52b1\uff0c\u8bad\u7ec3\u591a\u673a\u5668\u4eba\u534f\u8c03\u884c\u4e3a\u3002", "result": "CRAFT\u5728\u591a\u56db\u8db3\u5bfc\u822a\u548c\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u5b66\u4e60\u4e86\u590d\u6742\u534f\u8c03\u884c\u4e3a\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5bfc\u822a\u7b56\u7565\u3002", "conclusion": "CRAFT\u5c55\u793a\u4e86\u5229\u7528\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4e3aMARL\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14243", "pdf": "https://arxiv.org/pdf/2509.14243", "abs": "https://arxiv.org/abs/2509.14243", "authors": ["Xinjie Wang", "Zhongrui Li", "Peng Han", "Chunxin Yuan", "Jiexin Xu", "Zhiqiang Wei", "Jie Nie"], "title": "InWaveSR: Topography-Aware Super-Resolution Network for Internal Solitary Waves", "categories": ["eess.SP"], "comment": null, "summary": "The effective utilization of observational data is frequently hindered by\ninsufficient resolution. To address this problem, we present a new\nspatio-temporal super-resolution (STSR) model, called InWaveSR. It is built on\na deep learning framework with physical restrictions and can efficiently\ngenerate high-resolution data from low-resolution input, especially for data\nfeaturing internal solitary waves (ISWs). To increase generality and\ninterpretation, the model InWaveSR uses the primitive Navier-Stokes equations\nas the constraint, ensuring that the output results are physically consistent.\nIn addition, the proposed model incorporates an HF-ResBlock component that\ncombines the attention mechanism and the Fast Fourier Transform (FFT) method to\nimprove the performance of the model in capturing high-frequency\ncharacteristics. Simultaneously, in order to enhance the adaptability of the\nmodel to complicated bottom topography, an edge sampling and numerical\npre-processing method are carried out to optimize the training process. On\nevaluations using the in-situ observational ISW data, the proposed InWaveSR\nachieved a peak signal-to-noise ratio (PSNR) score of 36.2, higher than those\nof the traditional interpolation method and the previous neural network. This\nhighlights its significant superiority over traditional methods, demonstrating\nits excellent performance and reliability in high-resolution ISW\nreconstruction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u6a21\u578bInWaveSR\uff0c\u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\u89c2\u6d4b\u6570\u636e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5185\u5b64\u7acb\u6ce2\uff08ISWs\uff09\u65f6\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89c2\u6d4b\u6570\u636e\u7684\u5206\u8fa8\u7387\u4e0d\u8db3\u9650\u5236\u4e86\u5176\u6709\u6548\u5229\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "InWaveSR\u7ed3\u5408\u4e86\u7269\u7406\u7ea6\u675f\uff08Navier-Stokes\u65b9\u7a0b\uff09\u3001HF-ResBlock\u7ec4\u4ef6\uff08\u6ce8\u610f\u529b\u673a\u5236\u548cFFT\uff09\u4ee5\u53ca\u8fb9\u7f18\u91c7\u6837\u548c\u6570\u503c\u9884\u5904\u7406\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u5b9e\u6d4bISW\u6570\u636e\u4e0a\uff0cInWaveSR\u7684PSNR\u5f97\u5206\u4e3a36.2\uff0c\u4f18\u4e8e\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u548c\u4e4b\u524d\u795e\u7ecf\u7f51\u7edc\u3002", "conclusion": "InWaveSR\u5728\u9ad8\u5206\u8fa8\u7387ISW\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2509.14383", "pdf": "https://arxiv.org/pdf/2509.14383", "abs": "https://arxiv.org/abs/2509.14383", "authors": ["Yuhong Lu"], "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings", "categories": ["cs.RO", "cs.CV"], "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings.", "AI": {"tldr": "RBind\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5bf9\u6297\u4e0d\u53d8\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u5bf9\u6297\u6027\u548c\u81ea\u7136\u5e72\u6270\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7edf\u4e00\u7684\u6a21\u6001\u7f16\u7801\u5668\u5728\u673a\u5668\u4eba\u611f\u77e5\u548c\u51b3\u7b56\u4e2d\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u5bf9\u6297\u6027\u548c\u81ea\u7136\u5e72\u6270\uff0c\u9700\u8981\u63d0\u9ad8\u9c81\u68d2\u6027\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "RBind\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u65e0\u76d1\u7763\u5fae\u8c03\u5f3a\u5316\u89c6\u89c9\u7f16\u7801\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u8de8\u6a21\u6001\u5bf9\u5e94\u5173\u7cfb\uff0c\u6700\u5c0f\u5316\u5e72\u51c0/\u5bf9\u6297\u7279\u5f81\u4e0e\u6587\u672c\u951a\u70b9\u7684\u5dee\u5f02\uff0c\u5e76\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7c7b\u522b\u5206\u5e03\u5bf9\u9f50\u3002", "result": "RBind\u5728\u56fe\u50cf\u3001\u97f3\u9891\u3001\u70ed\u611f\u548c\u89c6\u9891\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5e72\u51c0\u51c6\u786e\u6027\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8eLanguageBind\u57fa\u51c6\u548c\u6807\u51c6\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "RBind\u5728\u4e0d\u727a\u7272\u6cdb\u5316\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u591a\u4f20\u611f\u5668\u611f\u77e5\u63d0\u4f9b\u4e86\u5b89\u5168\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14402", "pdf": "https://arxiv.org/pdf/2509.14402", "abs": "https://arxiv.org/abs/2509.14402", "authors": ["Jinshui Zhang", "Angel V Peterchev", "Stefan M Goetz"], "title": "Conditional Nearest Level Modulation for Improved Switching Dynamics in Asymmetric Multilevel Converters", "categories": ["eess.SP"], "comment": null, "summary": "Modular multilevel converters have promising applications in clean energy,\nelectric vehicles, and biomedical instrumentation, but need many modules to\nachieve fine output granularity, particularly of the voltage. Asymmetric\nmultilevel circuits introduce differences in module voltages so that the\nquantity of output levels grows exponentially with the number of modules.\nNearest-level modulation (NLM) is preferred over carrier-based methods in\nasymmetric circuits for its simplicity. However, the large number of output\nlevels can overwhelm NLM and cause excessive transistor switching on some\nmodules and output voltage spikes. We propose a conditional nearest-level\nmodulation (cNLM) by incorporating mathematical penalty models to regulate\nswitching dynamics. This approach improves output quality and reduces switching\nrates. Additionally, we present cNLM variations tailored for specific\nfunctions, such as enforcing a minimum switching interval. Experimental\nvalidation on an asymmetric multilevel prototype demonstrates that cNLM reduces\nthe total output distortion from 66.3% to 15.1% while cutting the switching\nrate to just 8% of the original NLM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u6700\u8fd1\u7535\u5e73\u8c03\u5236\uff08cNLM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u60e9\u7f5a\u6a21\u578b\u8c03\u8282\u5f00\u5173\u52a8\u6001\uff0c\u63d0\u9ad8\u4e86\u8f93\u51fa\u8d28\u91cf\u5e76\u964d\u4f4e\u4e86\u5f00\u5173\u7387\u3002", "motivation": "\u6a21\u5757\u5316\u591a\u7535\u5e73\u8f6c\u6362\u5668\u5728\u6e05\u6d01\u80fd\u6e90\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6a21\u5757\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u8f93\u51fa\u3002\u975e\u5bf9\u79f0\u591a\u7535\u5e73\u7535\u8def\u4e2d\uff0c\u8f93\u51fa\u7535\u5e73\u6570\u91cf\u968f\u6a21\u5757\u6570\u6307\u6570\u589e\u957f\uff0c\u4f20\u7edfNLM\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u5f00\u5173\u8fc7\u5ea6\u548c\u7535\u538b\u5c16\u5cf0\u3002", "method": "\u5f15\u5165\u6761\u4ef6\u6700\u8fd1\u7535\u5e73\u8c03\u5236\uff08cNLM\uff09\uff0c\u7ed3\u5408\u6570\u5b66\u60e9\u7f5a\u6a21\u578b\u8c03\u8282\u5f00\u5173\u52a8\u6001\uff0c\u5e76\u63d0\u4f9b\u9488\u5bf9\u7279\u5b9a\u529f\u80fd\u7684cNLM\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0ccNLM\u5c06\u603b\u8f93\u51fa\u5931\u771f\u4ece66.3%\u964d\u81f315.1%\uff0c\u5f00\u5173\u7387\u964d\u81f3\u539f\u59cbNLM\u76848%\u3002", "conclusion": "cNLM\u6709\u6548\u6539\u5584\u4e86\u8f93\u51fa\u7535\u538b\u8d28\u91cf\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u5173\u7387\uff0c\u9002\u7528\u4e8e\u975e\u5bf9\u79f0\u591a\u7535\u5e73\u7535\u8def\u3002"}}
{"id": "2509.14412", "pdf": "https://arxiv.org/pdf/2509.14412", "abs": "https://arxiv.org/abs/2509.14412", "authors": ["Artem Lykov", "Oleg Kobzarev", "Dzmitry Tsetserukou"], "title": "GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot", "categories": ["cs.RO"], "comment": null, "summary": "We present GestOS, a gesture-based operating system for high-level control of\nheterogeneous robot teams. Unlike prior systems that map gestures to fixed\ncommands or single-agent actions, GestOS interprets hand gestures semantically\nand dynamically distributes tasks across multiple robots based on their\ncapabilities, current state, and supported instruction sets. The system\ncombines lightweight visual perception with large language model (LLM)\nreasoning: hand poses are converted into structured textual descriptions, which\nthe LLM uses to infer intent and generate robot-specific commands. A robot\nselection module ensures that each gesture-triggered task is matched to the\nmost suitable agent in real time. This architecture enables context-aware,\nadaptive control without requiring explicit user specification of targets or\ncommands. By advancing gesture interaction from recognition to intelligent\norchestration, GestOS supports scalable, flexible, and user-friendly\ncollaboration with robotic systems in dynamic environments.", "AI": {"tldr": "GestOS\u662f\u4e00\u4e2a\u57fa\u4e8e\u624b\u52bf\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u52a8\u6001\u5206\u914d\u4efb\u52a1\u7ed9\u591a\u4e2a\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u89c6\u89c9\u611f\u77e5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u667a\u80fd\u63a7\u5236\u3002", "motivation": "\u63d0\u5347\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u9ad8\u6548\u534f\u4f5c\u80fd\u529b\uff0c\u6446\u8131\u4f20\u7edf\u56fa\u5b9a\u624b\u52bf\u547d\u4ee4\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5206\u914d\u3002", "method": "\u5229\u7528\u89c6\u89c9\u611f\u77e5\u5c06\u624b\u52bf\u8f6c\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u610f\u56fe\u5e76\u751f\u6210\u673a\u5668\u4eba\u7279\u5b9a\u6307\u4ee4\uff0c\u5b9e\u65f6\u5339\u914d\u6700\u9002\u5408\u7684\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u65e0\u9700\u7528\u6237\u660e\u786e\u6307\u5b9a\u76ee\u6807\u6216\u547d\u4ee4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7075\u6d3b\u7684\u673a\u5668\u4eba\u534f\u4f5c\u3002", "conclusion": "GestOS\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u7f16\u6392\uff0c\u63a8\u52a8\u4e86\u624b\u52bf\u4ea4\u4e92\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u534f\u4f5c\u3002"}}
{"id": "2509.14442", "pdf": "https://arxiv.org/pdf/2509.14442", "abs": "https://arxiv.org/abs/2509.14442", "authors": ["Arjun Teh", "Wael H. Ali", "Joshua Rapp", "Hassan Mansour"], "title": "Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography", "categories": ["eess.SP", "cs.LG"], "comment": "Presented in ISCS25", "summary": "We develop a framework for non-invasive volumetric indoor airflow estimation\nfrom a single viewpoint using background-oriented schlieren (BOS) measurements\nand physics-informed reconstruction. Our framework utilizes a light projector\nthat projects a pattern onto a target back-wall and a camera that observes\nsmall distortions in the light pattern. While the single-view BOS tomography\nproblem is severely ill-posed, our proposed framework addresses this using: (1)\nimproved ray tracing, (2) a physics-based light rendering approach and loss\nformulation, and (3) a physics-based regularization using a physics-informed\nneural network (PINN) to ensure that the reconstructed airflow is consistent\nwith the governing equations for buoyancy-driven flows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u89c6\u89d2BOS\u6d4b\u91cf\u548c\u7269\u7406\u4fe1\u606f\u91cd\u5efa\u7684\u975e\u4fb5\u5165\u5f0f\u5ba4\u5185\u6c14\u6d41\u4f53\u79ef\u4f30\u8ba1\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u5355\u89c6\u89d2BOS\u5c42\u6790\u6210\u50cf\u4e25\u91cd\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u6539\u8fdb\u5149\u7ebf\u8ffd\u8e2a\u3001\u7269\u7406\u6e32\u67d3\u548c\u635f\u5931\u516c\u5f0f\uff0c\u7ed3\u5408PINN\u8fdb\u884c\u7269\u7406\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u4e0e\u6d6e\u529b\u9a71\u52a8\u6d41\u63a7\u5236\u65b9\u7a0b\u4e00\u81f4\u7684\u6c14\u6d41\u91cd\u5efa\u3002", "conclusion": "\u6846\u67b6\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u63d0\u9ad8\u4e86\u5355\u89c6\u89d2BOS\u5728\u6c14\u6d41\u91cd\u5efa\u4e2d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.14421", "pdf": "https://arxiv.org/pdf/2509.14421", "abs": "https://arxiv.org/abs/2509.14421", "authors": ["Dario Tscholl", "Yashwanth Nakka", "Brian Gunter"], "title": "Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting", "categories": ["cs.RO"], "comment": "Preprint for IEEE L-CSS/ACC", "summary": "We present a perception-driven safety filter that converts each 3D Gaussian\nSplat (3DGS) into a closed-form forward collision cone, which in turn yields a\nfirst-order control barrier function (CBF) embedded within a quadratic program\n(QP). By exploiting the analytic geometry of splats, our formulation provides a\ncontinuous, closed-form representation of collision constraints that is both\nsimple and computationally efficient. Unlike distance-based CBFs, which tend to\nactivate reactively only when an obstacle is already close, our collision-cone\nCBF activates proactively, allowing the robot to adjust earlier and thereby\nproduce smoother and safer avoidance maneuvers at lower computational cost. We\nvalidate the method on a large synthetic scene with approximately 170k splats,\nwhere our filter reduces planning time by a factor of 3 and significantly\ndecreased trajectory jerk compared to a state-of-the-art 3DGS planner, while\nmaintaining the same level of safety. The approach is entirely analytic,\nrequires no high-order CBF extensions (HOCBFs), and generalizes naturally to\nrobots with physical extent through a principled Minkowski-sum inflation of the\nsplats. These properties make the method broadly applicable to real-time\nnavigation in cluttered, perception-derived extreme environments, including\nspace robotics and satellite systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5c063D\u9ad8\u65af\u6cfc\u6e85\u4f53\u8f6c\u5316\u4e3a\u5c01\u95ed\u5f62\u5f0f\u7684\u78b0\u649e\u9525\uff0c\u751f\u6210\u63a7\u5236\u969c\u788d\u51fd\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5e73\u6ed1\u7684\u907f\u969c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u8ddd\u79bbCBF\u56e0\u53cd\u5e94\u8fdf\u6ede\u5bfc\u81f4\u907f\u969c\u52a8\u4f5c\u4e0d\u591f\u5e73\u6ed1\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6cfc\u6e85\u4f53\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u6784\u5efa\u5c01\u95ed\u5f0f\u78b0\u649e\u9525\u548cCBF\uff0c\u5d4c\u5165\u4e8c\u6b21\u89c4\u5212\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u573a\u666f\u4e2d\u51cf\u5c11\u89c4\u5212\u65f6\u95f43\u500d\uff0c\u663e\u8457\u964d\u4f4e\u8f68\u8ff9\u6296\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9ad8\u9636CBF\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5bfc\u822a\u3002"}}
{"id": "2509.14447", "pdf": "https://arxiv.org/pdf/2509.14447", "abs": "https://arxiv.org/abs/2509.14447", "authors": ["Sriram V. C. Nallani", "Gautham Ramachandran", "Sahil S. Shah"], "title": "Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces", "categories": ["eess.SP"], "comment": "9 pages, 5 figures, submitted to ICLR 2026", "summary": "Brain-Computer Interfaces face challenges from neural signal instability and\nmemory constraints for real-time implantable applications. We introduce an\nonline SNN decoder using local three-factor learning rules with dual-timescale\neligibility traces that avoid backpropagation through time while maintaining\ncompetitive performance. Our approach combines error-modulated Hebbian updates,\nfast/slow trace consolidation, and adaptive learning rate control, requiring\nonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primate\ndatasets achieve comparable decoding accuracy (Pearson $R \\geq 0.63$ Zenodo, $R\n\\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence than\nBPTT-trained SNNs. Closed-loop simulations with synthetic neural populations\ndemonstrate adaptation to neural disruptions and learning from scratch without\noffline calibration. This work enables memory-efficient, continuously adaptive\nneural decoding suitable for resource-constrained implantable BCI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebfSNN\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5c40\u90e8\u4e09\u56e0\u5b50\u5b66\u4e60\u89c4\u5219\u548c\u53cc\u65f6\u95f4\u5c3a\u5ea6\u8d44\u683c\u8ff9\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u795e\u7ecf\u89e3\u7801\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8111\u673a\u63a5\u53e3\u4e2d\u795e\u7ecf\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u548c\u5b9e\u65f6\u690d\u5165\u5e94\u7528\u5185\u5b58\u53d7\u9650\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5728\u7ebfSNN\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u9519\u8bef\u8c03\u5236\u7684Hebbian\u66f4\u65b0\u3001\u5feb/\u6162\u8ff9\u6574\u5408\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u63a7\u5236\uff0c\u4ec5\u9700O(1)\u5185\u5b58\u3002", "result": "\u5728\u7075\u957f\u7c7b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u89e3\u7801\u7cbe\u5ea6\uff08Pearson R\u22650.63\u548cR\u22650.81\uff09\uff0c\u5185\u5b58\u51cf\u5c1128-35%\uff0c\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u690d\u5165\u5f0fBCI\u7cfb\u7edf\uff0c\u652f\u6301\u5185\u5b58\u9ad8\u6548\u4e14\u6301\u7eed\u81ea\u9002\u5e94\u7684\u795e\u7ecf\u89e3\u7801\u3002"}}
{"id": "2509.14431", "pdf": "https://arxiv.org/pdf/2509.14431", "abs": "https://arxiv.org/abs/2509.14431", "authors": ["Keqin Wang", "Tao Zhong", "David Chang", "Christine Allen-Blanchette"], "title": "Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm\nfor coordinating swarms of agents in complex decision-making, yet major\nchallenges remain. In competitive settings such as pursuer-evader tasks,\nsimultaneous adaptation can destabilize training; non-kinetic countermeasures\noften fail under adverse conditions; and policies trained in one configuration\nrarely generalize to environments with a different number of agents. To address\nthese issues, we propose the Local-Canonicalization Equivariant Graph Neural\nNetworks (LEGO) framework, which integrates seamlessly with popular MARL\nalgorithms such as MAPPO. LEGO employs graph neural networks to capture\npermutation equivariance and generalization to different agent numbers,\ncanonicalization to enforce E(n)-equivariance, and heterogeneous\nrepresentations to encode role-specific inductive biases. Experiments on\ncooperative and competitive swarm benchmarks show that LEGO outperforms strong\nbaselines and improves generalization. In real-world experiments, LEGO\ndemonstrates robustness to varying team sizes and agent failure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLEGO\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u89c4\u8303\u5316\u6280\u672f\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u7ade\u4e89\u6027\u4efb\u52a1\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u6311\u6218\u3002", "method": "LEGO\u6846\u67b6\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u89c4\u8303\u5316\u548c\u5f02\u6784\u8868\u793a\uff0c\u63d0\u5347MARL\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLEGO\u5728\u534f\u4f5c\u548c\u7ade\u4e89\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u667a\u80fd\u4f53\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "LEGO\u901a\u8fc7\u7ed3\u5408\u65b0\u9896\u7684\u6280\u672f\uff0c\u4e3aMARL\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14449", "pdf": "https://arxiv.org/pdf/2509.14449", "abs": "https://arxiv.org/abs/2509.14449", "authors": ["Mahdi Shamsi", "Hadi Zayyani", "Hasan Abu Hilal", "Mohammad Salman"], "title": "Secure Blind Graph Signal Recovery and Adversary Detection Using Smoothness Maximization", "categories": ["eess.SP"], "comment": null, "summary": "In this letter, we propose a secure blind Graph Signal Recovery (GSR)\nalgorithm that can detect adversary nodes. Some unknown adversaries are assumed\nto be injecting false data at their respective nodes in the graph. The number\nand location of adversaries are not known in advance and the goal is to recover\nthe graph signal in the presence of measurement noise and False Data Injection\n(FDI) caused by the adversaries. Consequently, the proposed algorithm would be\na perfect candidate to solve this challenging problem. Moreover, due to the\npresence of malicious nodes, the proposed method serves as a secure GSR\nalgorithm. For adversary detection, a statistical measure based on differential\nsmoothness is used. Specifically, the difference between the current observed\nsmoothness and the average smoothness excluding the corresponding node. This\ngenuine statistical approach leads to an effective and low-complexity adversary\ndetector. In addition, following malicious node detection, the GSR is performed\nusing a variant of smoothness maximization, which is solved efficiently as a\nfractional optimization problem using a Dinkelbach's algorithm. Analysis of the\ndetector, which determines the optimum threshold of the detector is also\npresented. Simulation results show a significant improvement of the proposed\nmethod in signal recovery compared to the median GSR algorithm and other\ncompeting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u7684\u76f2\u56fe\u4fe1\u53f7\u6062\u590d\uff08GSR\uff09\u7b97\u6cd5\uff0c\u80fd\u68c0\u6d4b\u654c\u5bf9\u8282\u70b9\uff0c\u6062\u590d\u4fe1\u53f7\u5e76\u5e94\u5bf9\u865a\u5047\u6570\u636e\u6ce8\u5165\uff08FDI\uff09\u548c\u6d4b\u91cf\u566a\u58f0\u3002", "motivation": "\u5728\u56fe\u4e2d\u654c\u5bf9\u8282\u70b9\u4f4d\u7f6e\u548c\u6570\u91cf\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u4fe1\u53f7\u6062\u590d\u548c\u5b89\u5168\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5dee\u5206\u5e73\u6ed1\u5ea6\u7684\u7edf\u8ba1\u68c0\u6d4b\u5668\u8bc6\u522b\u654c\u5bf9\u8282\u70b9\uff0c\u91c7\u7528\u5e73\u6ed1\u5ea6\u6700\u5927\u5316\u53d8\u4f53\u8fdb\u884cGSR\uff0c\u901a\u8fc7Dinkelbach\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\u5206\u6570\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fe1\u53f7\u6062\u590d\u4e0a\u4f18\u4e8e\u4e2d\u503cGSR\u7b97\u6cd5\u548c\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u662f\u5b89\u5168GSR\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u4f4e\u590d\u6742\u5ea6\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.14453", "pdf": "https://arxiv.org/pdf/2509.14453", "abs": "https://arxiv.org/abs/2509.14453", "authors": ["Gokul Puthumanaillam", "Ram Padmanabhan", "Jose Fuentes", "Nicole Cruz", "Paulo Padrao", "Ruben Hernandez", "Hao Jiang", "William Schafer", "Leonardo Bobadilla", "Melkior Ornik"], "title": "Online Learning of Deceptive Policies under Intermittent Observation", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "In supervisory control settings, autonomous systems are not monitored\ncontinuously. Instead, monitoring often occurs at sporadic intervals within\nknown bounds. We study the problem of deception, where an agent pursues a\nprivate objective while remaining plausibly compliant with a supervisor's\nreference policy when observations occur. Motivated by the behavior of real,\nhuman supervisors, we situate the problem within Theory of Mind: the\nrepresentation of what an observer believes and expects to see. We show that\nTheory of Mind can be repurposed to steer online reinforcement learning (RL)\ntoward such deceptive behavior. We model the supervisor's expectations and\ndistill from them a single, calibrated scalar -- the expected evidence of\ndeviation if an observation were to happen now. This scalar combines how unlike\nthe reference and current action distributions appear, with the agent's belief\nthat an observation is imminent. Injected as a state-dependent weight into a\nKL-regularized policy improvement step within an online RL loop, this scalar\ninforms a closed-form update that smoothly trades off self-interest and\ncompliance, thus sidestepping hand-crafted or heuristic policies. In\nreal-world, real-time hardware experiments on marine (ASV) and aerial (UAV)\nnavigation, our ToM-guided RL runs online, achieves high return and success\nwith observed-trace evidence calibrated to the supervisor's expectations.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u95f4\u6b47\u6027\u76d1\u63a7\u4e0b\uff0c\u667a\u80fd\u4f53\u5982\u4f55\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\uff08Theory of Mind\uff09\u5b9e\u73b0\u6b3a\u9a97\u6027\u884c\u4e3a\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u7ebf\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u5728\u76d1\u63a7\u95f4\u6b47\u6027\u65f6\u7684\u6b3a\u9a97\u6027\u95ee\u9898\uff0c\u6a21\u62df\u4eba\u7c7b\u76d1\u7763\u8005\u7684\u5fc3\u7406\u9884\u671f\uff0c\u4ee5\u5b9e\u73b0\u65e2\u6ee1\u8db3\u79c1\u5229\u53c8\u8868\u9762\u5408\u89c4\u7684\u884c\u4e3a\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5fc3\u7406\u7406\u8bba\u5efa\u6a21\u76d1\u7763\u8005\u7684\u9884\u671f\uff0c\u63d0\u53d6\u6807\u91cf\u5316\u7684\u9884\u671f\u504f\u5dee\u8bc1\u636e\uff0c\u5e76\u5c06\u5176\u878d\u5165KL\u6b63\u5219\u5316\u7684\u7b56\u7565\u6539\u8fdb\u6b65\u9aa4\uff0c\u4f18\u5316\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff08ASV\u548cUAV\u5bfc\u822a\uff09\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u56de\u62a5\u4e0e\u6210\u529f\uff0c\u5e76\u4e14\u89c2\u6d4b\u8ff9\u8bc1\u636e\u7b26\u5408\u76d1\u7763\u8005\u9884\u671f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5fc3\u7406\u7406\u8bba\u53ef\u4ee5\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6b3a\u9a97\u6027\u884c\u4e3a\uff0c\u65e0\u9700\u624b\u5de5\u7b56\u7565\uff0c\u4e14\u5728\u5b9e\u65f6\u786c\u4ef6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.14503", "pdf": "https://arxiv.org/pdf/2509.14503", "abs": "https://arxiv.org/abs/2509.14503", "authors": ["Zhongwen Sun", "Wei Chen", "Yuxuan Sun", "Bo Ai"], "title": "Age of Information Aided Intelligent Grant-Free Massive Access for Heterogeneous mMTC Traffic", "categories": ["eess.SP"], "comment": null, "summary": "With the arrival of 6G, the Internet of Things (IoT) traffic is becoming more\nand more complex and diverse. To meet the diverse service requirements of IoT\ndevices, massive machine-type communications (mMTC) becomes a typical scenario,\nand more recently, grant-free random access (GF-RA) presents a promising\ndirection due to its low signaling overhead. However, existing GF-RA research\nprimarily focuses on improving the accuracy of user detection and data\nrecovery, without considering the heterogeneity of traffic. In this paper, we\ninvestigate a non-orthogonal GF-RA scenario where two distinct types of traffic\ncoexist: event-triggered traffic with alarm devices (ADs), and status update\ntraffic with monitor devices (MDs). The goal is to simultaneously achieve high\ndetection success rates for ADs and high information timeliness for MDs. First,\nwe analyze the age-based random access scheme and optimize the access\nparameters to minimize the average age of information (AoI) of MDs. Then, we\ndesign an age-based prior information aided autoencoder (A-PIAAE) to jointly\ndetect active devices, together with learned pilots used in GF-RA to reduce\ninterference between non-orthogonal pilots. In the decoder, an Age-based\nLearned Iterative Shrinkage Thresholding Algorithm (LISTA-AGE) utilizing the\nAoI of MDs as the prior information is proposed to enhance active user\ndetection. Theoretical analysis is provided to demonstrate the proposed A-PIAAE\nhas better convergence performance. Experiments demonstrate the advantage of\nthe proposed method in reducing the average AoI of MDs and improving the\nsuccessful detection rate of ADs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e866G\u73af\u5883\u4e0b\u7269\u8054\u7f51\uff08IoT\uff09\u4e2d\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u6d41\u91cf\u7684\u975e\u6b63\u4ea4\u65e0\u6388\u6743\u968f\u673a\u63a5\u5165\uff08GF-RA\uff09\u573a\u666f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e74\u9f84\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u8b66\u62a5\u8bbe\u5907\uff08ADs\uff09\u7684\u68c0\u6d4b\u6210\u529f\u7387\u548c\u76d1\u6d4b\u8bbe\u5907\uff08MDs\uff09\u7684\u4fe1\u606f\u65f6\u6548\u6027\u3002", "motivation": "\u73b0\u6709GF-RA\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u68c0\u6d4b\u548c\u6570\u636e\u6062\u590d\u7684\u51c6\u786e\u6027\uff0c\u672a\u8003\u8651\u6d41\u91cf\u7684\u5f02\u6784\u6027\u3002\u672c\u6587\u65e8\u5728\u540c\u65f6\u6ee1\u8db3ADs\u7684\u9ad8\u68c0\u6d4b\u6210\u529f\u7387\u548cMDs\u7684\u9ad8\u4fe1\u606f\u65f6\u6548\u6027\u3002", "method": "1. \u5206\u6790\u5e76\u4f18\u5316\u57fa\u4e8e\u5e74\u9f84\u7684\u968f\u673a\u63a5\u5165\u65b9\u6848\u4ee5\u51cf\u5c11MDs\u7684\u5e73\u5747\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u30022. \u8bbe\u8ba1\u57fa\u4e8e\u5e74\u9f84\u7684\u5148\u9a8c\u4fe1\u606f\u8f85\u52a9\u81ea\u52a8\u7f16\u7801\u5668\uff08A-PIAAE\uff09\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u5bfc\u9891\u51cf\u5c11\u975e\u6b63\u4ea4\u5bfc\u9891\u95f4\u7684\u5e72\u6270\u30023. \u5728\u89e3\u7801\u5668\u4e2d\u63d0\u51faAge-based LISTA\u7b97\u6cd5\uff08LISTA-AGE\uff09\uff0c\u5229\u7528MDs\u7684AoI\u4f5c\u4e3a\u5148\u9a8c\u4fe1\u606f\u589e\u5f3a\u7528\u6237\u68c0\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eA-PIAAE\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u964d\u4f4eMDs\u7684\u5e73\u5747AoI\u548c\u63d0\u9ad8ADs\u7684\u68c0\u6d4b\u6210\u529f\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GF-RA\u4e2d\u6d41\u91cf\u5f02\u6784\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u4e86ADs\u7684\u68c0\u6d4b\u6027\u80fd\u548cMDs\u7684\u4fe1\u606f\u65f6\u6548\u6027\u3002"}}
{"id": "2509.14460", "pdf": "https://arxiv.org/pdf/2509.14460", "abs": "https://arxiv.org/abs/2509.14460", "authors": ["Abhiroop Ajith", "Constantinos Chamzas"], "title": "Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring", "categories": ["cs.RO"], "comment": null, "summary": "Learning abstractions directly from data is a core challenge in robotics.\nHumans naturally operate at an abstract level, reasoning over high-level\nsubgoals while delegating execution to low-level motor skills -- an ability\nthat enables efficient problem solving in complex environments. In robotics,\nabstractions and hierarchical reasoning have long been central to planning, yet\nthey are typically hand-engineered, demanding significant human effort and\nlimiting scalability. Automating the discovery of useful abstractions directly\nfrom visual data would make planning frameworks more scalable and more\napplicable to real-world robotic domains. In this work, we focus on\nrearrangement tasks where the state is represented with raw images, and propose\na method to induce discrete, graph-structured abstractions by combining\nstructural constraints with an attention-guided visual distance. Our approach\nleverages the inherent bipartite structure of rearrangement problems,\nintegrating structural constraints and visual embeddings into a unified\nframework. This enables the autonomous discovery of abstractions from vision\nalone, which can subsequently support high-level planning. We evaluate our\nmethod on two rearrangement tasks in simulation and show that it consistently\nidentifies meaningful abstractions that facilitate effective planning and\noutperform existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u5b66\u4e60\u62bd\u8c61\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u91cd\u6392\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u7ea6\u675f\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u89c6\u89c9\u8ddd\u79bb\uff0c\u63d0\u5347\u4e86\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u9ad8\u6548\u5730\u5728\u590d\u6742\u73af\u5883\u4e2d\u89e3\u51b3\u95ee\u9898\uff0c\u5f97\u76ca\u4e8e\u9ad8\u5c42\u6b21\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\u673a\u5668\u4eba\u9886\u57df\u7684\u62bd\u8c61\u5c42\u6b21\u901a\u5e38\u9700\u8981\u4eba\u5de5\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9002\u7528\u6027\u3002\u76f4\u63a5\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u81ea\u52a8\u5316\u53d1\u73b0\u6709\u7528\u7684\u62bd\u8c61\u662f\u63d0\u5347\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u7ea6\u675f\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u89c6\u89c9\u8ddd\u79bb\uff0c\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u8bf1\u5bfc\u79bb\u6563\u7684\u56fe\u7ed3\u6784\u62bd\u8c61\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u91cd\u6392\u4efb\u52a1\u56fa\u6709\u7684\u4e8c\u5206\u7ed3\u6784\uff0c\u5c06\u7ed3\u6784\u7ea6\u675f\u548c\u89c6\u89c9\u5d4c\u5165\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u4e24\u79cd\u91cd\u6392\u4efb\u52a1\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4e00\u81f4\u5730\u8bc6\u522b\u51fa\u6709\u610f\u4e49\u7684\u62bd\u8c61\u7ed3\u6784\uff0c\u6709\u6548\u652f\u6301\u9ad8\u5c42\u89c4\u5212\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u81ea\u52a8\u53d1\u73b0\u4e86\u6709\u7528\u7684\u62bd\u8c61\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u91cd\u6392\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u5347\u89c4\u5212\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14559", "pdf": "https://arxiv.org/pdf/2509.14559", "abs": "https://arxiv.org/abs/2509.14559", "authors": ["Paolo Torrado", "Anders Pearson", "Jason Klein", "Alexander Moscibroda", "Joshua Smith"], "title": "Radiolunadiff: Estimation of wireless network signal strength in lunar terrain", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel physics-informed deep learning architecture\nfor predicting radio maps over lunar terrain. Our approach integrates a\nphysics-based lunar terrain generator, which produces realistic topography\ninformed by publicly available NASA data, with a ray-tracing engine to create a\nhigh-fidelity dataset of radio propagation scenarios. Building on this dataset,\nwe introduce a triplet-UNet architecture, consisting of two standard UNets and\na diffusion network, to model complex propagation effects. Experimental results\ndemonstrate that our method outperforms existing deep learning approaches on\nour terrain dataset across various metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u6708\u7403\u5730\u5f62\u4e0a\u7684\u65e0\u7ebf\u7535\u5730\u56fe\u3002", "motivation": "\u7ed3\u5408\u7269\u7406\u57fa\u7840\u7684\u5730\u5f62\u751f\u6210\u5668\u548c\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u65e0\u7ebf\u7535\u4f20\u64ad\u6548\u5e94\u7684\u5efa\u6a21\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e09\u91cdUNet\u67b6\u6784\uff08\u4e24\u4e2a\u6807\u51c6UNet\u548c\u4e00\u4e2a\u6269\u6563\u7f51\u7edc\uff09\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u5730\u5f62\u751f\u6210\u5668\u548c\u5c04\u7ebf\u8ffd\u8e2a\u5f15\u64ce\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u590d\u6742\u5730\u5f62\u4e0b\u7684\u65e0\u7ebf\u7535\u4f20\u64ad\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14510", "pdf": "https://arxiv.org/pdf/2509.14510", "abs": "https://arxiv.org/abs/2509.14510", "authors": ["Sandra Q. Liu", "Yuxiang Ma", "Edward H. Adelson"], "title": "Object Recognition and Force Estimation with the GelSight Baby Fin Ray", "categories": ["cs.RO"], "comment": "Presented at CoRL 2023 as part of the workshop, \"Learning for Soft\n  Robots: Hard Challenges for Soft Systems\" (website:\n  https://sites.google.com/view/corl-2023-soft-robots-ws)", "summary": "Recent advances in soft robotic hands and tactile sensing have enabled both\nto perform an increasing number of complex tasks with the aid of machine\nlearning. In particular, we presented the GelSight Baby Fin Ray in our previous\nwork, which integrates a camera with a soft, compliant Fin Ray structure.\nCamera-based tactile sensing gives the GelSight Baby Fin Ray the ability to\ncapture rich contact information like forces, object geometries, and textures.\nMoreover, our previous work showed that the GelSight Baby Fin Ray can dig\nthrough clutter, and classify in-shell nuts. To further examine the potential\nof the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell\ntextures and to perform force and position estimation. We implement ablation\nstudies with popular neural network structures, including ResNet50, GoogLeNet,\nand 3- and 5-layer convolutional neural network (CNN) structures. We conclude\nthat machine learning is a promising technique to extract useful information\nfrom high-resolution tactile images and empower soft robotics to better\nunderstand and interact with the environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u548c\u89e6\u89c9\u4f20\u611f\u6280\u672f\u7684\u7ed3\u5408\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u63d0\u5347GelSight Baby Fin Ray\u5728\u7eb9\u7406\u5206\u7c7b\u3001\u529b\u548c\u4f4d\u7f6e\u4f30\u8ba1\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8fdb\u4e00\u6b65\u6316\u6398GelSight Baby Fin Ray\u7684\u6f5c\u529b\uff0c\u63a2\u7d22\u5176\u5728\u89e6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6444\u50cf\u673a\u89e6\u89c9\u4f20\u611f\u6280\u672f\uff0c\u7ed3\u5408\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff08\u5982ResNet50\u3001GoogLeNet\u548cCNN\uff09\u8fdb\u884c\u7eb9\u7406\u548c\u529b/\u4f4d\u7f6e\u4f30\u8ba1\u7684\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u673a\u5668\u5b66\u4e60\u80fd\u6709\u6548\u4ece\u89e6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u6709\u7528\u4fe1\u606f\uff0c\u63d0\u5347\u8f6f\u4f53\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u5176\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u73af\u5883\u3002"}}
{"id": "2509.14665", "pdf": "https://arxiv.org/pdf/2509.14665", "abs": "https://arxiv.org/abs/2509.14665", "authors": ["Tian-Yu Xiang", "Zheng Lei", "Xiao-Hu Zhou", "Xiao-Liang Xie", "Shi-Qi Liu", "Mei-Jiang Gui", "Hong-Yun Ou", "Xin-Zheng Huang", "Xin-Yi Fu", "Zeng-Guang Hou"], "title": "Task-Oriented Learning for Automatic EEG Denoising", "categories": ["eess.SP"], "comment": null, "summary": "Electroencephalography (EEG) denoising methods typically depend on manual\nintervention or clean reference signals. This work introduces a task-oriented\nlearning framework for automatic EEG denoising that uses only task labels\nwithout clean reference signals. EEG recordings are first decomposed into\ncomponents based on blind source separation (BSS) techniques. Then, a\nlearning-based selector assigns a retention probability to each component, and\nthe denoised signal is reconstructed as a probability-weighted combination. A\ndownstream proxy-task model evaluates the reconstructed signal, with its task\nloss supervising the selector in a collaborative optimization scheme that\nrelies solely on task labels, eliminating the need for clean EEG references.\nExperiments on three datasets spanning two paradigms and multiple noise\nconditions show consistent gains in both task performance (accuracy:\n$2.56\\%\\uparrow$) and standard signal-quality metrics (signal-to-noise-ratio:\n$0.82$\\,dB\\,$\\uparrow$). Further analyses demonstrate that the task-oriented\nlearning framework is algorithm-agnostic, as it accommodates diverse\ndecomposition techniques and network backbones for both the selector and the\nproxy model. These promising results indicate that the proposed task-oriented\nlearning framework is a practical EEG denoising solution with potential\nimplications for neuroscience research and EEG-based interaction systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u5bfc\u5411\u5b66\u4e60\u7684\u81ea\u52a8EEG\u53bb\u566a\u6846\u67b6\uff0c\u65e0\u9700\u5e72\u51c0\u53c2\u8003\u4fe1\u53f7\uff0c\u4ec5\u9700\u4efb\u52a1\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u53bb\u566a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684EEG\u53bb\u566a\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u6216\u5e72\u51c0\u53c2\u8003\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u5176\u81ea\u52a8\u5316\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4ec5\u9700\u4efb\u52a1\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u81ea\u52a8\u53bb\u566a\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u76f2\u6e90\u5206\u79bb\uff08BSS\uff09\u6280\u672f\u5206\u89e3EEG\u4fe1\u53f7\uff0c\u5b66\u4e60\u57fa\u9009\u62e9\u5668\u4e3a\u6bcf\u4e2a\u6210\u5206\u5206\u914d\u4fdd\u7559\u6982\u7387\uff0c\u57fa\u4e8e\u6982\u7387\u52a0\u6743\u91cd\u6784\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\u6a21\u578b\u4f18\u5316\u9009\u62e9\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6027\u80fd\uff08\u51c6\u786e\u7387\u63d0\u53472.56%\uff09\u548c\u4fe1\u53f7\u8d28\u91cf\uff08\u4fe1\u566a\u6bd4\u63d0\u53470.82dB\uff09\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5177\u6709\u7b97\u6cd5\u65e0\u5173\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u5bfc\u5411\u5b66\u4e60\u6846\u67b6\u662f\u4e00\u79cd\u5b9e\u7528\u7684EEG\u53bb\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u548cEEG\u4ea4\u4e92\u7cfb\u7edf\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2509.14516", "pdf": "https://arxiv.org/pdf/2509.14516", "abs": "https://arxiv.org/abs/2509.14516", "authors": ["Adam D. Hines", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods", "categories": ["cs.RO"], "comment": "8 pages, 6 figures, under review", "summary": "Event-based localization research and datasets are a rapidly growing area of\ninterest, with a tenfold increase in the cumulative total number of published\npapers on this topic over the past 10 years. Whilst the rapid expansion in the\nfield is exciting, it brings with it an associated challenge: a growth in the\nvariety of required code and package dependencies as well as data formats,\nmaking comparisons difficult and cumbersome for researchers to implement\nreliably. To address this challenge, we present Event-LAB: a new and unified\nframework for running several event-based localization methodologies across\nmultiple datasets. Event-LAB is implemented using the Pixi package and\ndependency manager, that enables a single command-line installation and\ninvocation for combinations of localization methods and datasets. To\ndemonstrate the capabilities of the framework, we implement two common\nevent-based localization pipelines: Visual Place Recognition (VPR) and\nSimultaneous Localization and Mapping (SLAM). We demonstrate the ability of the\nframework to systematically visualize and analyze the results of multiple\nmethods and datasets, revealing key insights such as the association of\nparameters that control event collection counts and window sizes for frame\ngeneration to large variations in performance. The results and analysis\ndemonstrate the importance of fairly comparing methodologies with consistent\nevent image generation parameters. Our Event-LAB framework provides this\nability for the research community, by contributing a streamlined workflow for\neasily setting up multiple conditions.", "AI": {"tldr": "Event-LAB\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4e8b\u4ef6\u5b9a\u4f4d\u7814\u7a76\u4e2d\u4ee3\u7801\u548c\u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7Pixi\u5305\u7ba1\u7406\u5b9e\u73b0\u4fbf\u6377\u5b89\u88c5\uff0c\u5e76\u652f\u6301\u591a\u79cd\u5b9a\u4f4d\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u5b9a\u4f4d\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4ee3\u7801\u548c\u6570\u636e\u96c6\u683c\u5f0f\u591a\u6837\u5316\u5bfc\u81f4\u6bd4\u8f83\u548c\u5b9e\u73b0\u56f0\u96be\u3002", "method": "\u63d0\u51faEvent-LAB\u6846\u67b6\uff0c\u4f7f\u7528Pixi\u5305\u7ba1\u7406\u5668\u5b9e\u73b0\u5355\u547d\u4ee4\u5b89\u88c5\u548c\u8c03\u7528\uff0c\u652f\u6301\u591a\u79cd\u5b9a\u4f4d\u65b9\u6cd5\uff08\u5982VPR\u548cSLAM\uff09\u3002", "result": "\u6846\u67b6\u80fd\u7cfb\u7edf\u5206\u6790\u7ed3\u679c\uff0c\u63ed\u793a\u4e8b\u4ef6\u6536\u96c6\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "Event-LAB\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6bd4\u8f83\u5e73\u53f0\uff0c\u5f3a\u8c03\u5728\u4e00\u81f4\u7684\u53c2\u6570\u4e0b\u516c\u5e73\u6bd4\u8f83\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.14710", "pdf": "https://arxiv.org/pdf/2509.14710", "abs": "https://arxiv.org/abs/2509.14710", "authors": ["Koya Sato"], "title": "Mitigating the Impact of Location Uncertainty on Radio Map-Based Predictive Rate Selection via Noisy-Input Gaussian Process", "categories": ["eess.SP", "cs.NI"], "comment": "6 pages, 8 figures. Accepted for presentation at 2025 IEEE GLOBECOM\n  Workshops: Workshop on Radio Maps for Communications and Sensing", "summary": "This paper proposes a predictive rate-selection framework based on Gaussian\nprocess (GP)-based radio map construction that is robust to location\nuncertainty. Radio maps are a promising tool for improving communication\nefficiency in 6G networks. Although they enable the design of location-based\nmaximum transmission rates by exploiting statistical channel information,\nexisting discussions often assume perfect (i.e., noiseless) location\ninformation during channel sensing. Since such information must be obtained\nfrom positioning systems such as global navigation satellite systems, it\ninevitably involves positioning errors; this location uncertainty can degrade\nthe reliability of radio map-based wireless systems. To mitigate this issue, we\nintroduce the noisy-input GP (NIGP), which treats location noise as additional\noutput noise by applying a Taylor approximation of the function of interest.\nNumerical results demonstrate that the proposed NIGP-based design achieves more\nreliable transmission-rate selection than pure GP and yields higher throughput\nthan path loss-based rate selection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u9884\u6d4b\u901f\u7387\u9009\u62e9\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad86G\u7f51\u7edc\u7684\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u65e0\u7ebf\u4fe1\u53f7\u5730\u56fe\u7684\u901f\u7387\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5b9a\u4f4d\u4fe1\u606f\u5b8c\u7f8e\uff0c\u4f46\u5b9e\u9645\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u4f1a\u964d\u4f4e\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u566a\u58f0\u8f93\u5165\u9ad8\u65af\u8fc7\u7a0b\uff08NIGP\uff09\uff0c\u901a\u8fc7\u6cf0\u52d2\u8fd1\u4f3c\u5c06\u5b9a\u4f4d\u566a\u58f0\u89c6\u4e3a\u989d\u5916\u7684\u8f93\u51fa\u566a\u58f0\uff0c\u4ece\u800c\u6784\u5efa\u66f4\u7a33\u5065\u7684\u65e0\u7ebf\u4fe1\u53f7\u5730\u56fe\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cNIGP\u8bbe\u8ba1\u6bd4\u7eaf\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u66f4\u53ef\u9760\uff0c\u4e14\u6bd4\u57fa\u4e8e\u8def\u5f84\u635f\u8017\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "conclusion": "NIGP\u6846\u67b6\u80fd\u6709\u6548\u63d0\u53476G\u7f51\u7edc\u4e2d\u901f\u7387\u9009\u62e9\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14530", "pdf": "https://arxiv.org/pdf/2509.14530", "abs": "https://arxiv.org/abs/2509.14530", "authors": ["Zhenghao Fei", "Wenwu Lu", "Linsheng Hou", "Chen Peng"], "title": "Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking", "categories": ["cs.RO"], "comment": null, "summary": "Strawberries naturally grow in clusters, interwoven with leaves, stems, and\nother fruits, which frequently leads to occlusion. This inherent growth habit\npresents a significant challenge for robotic picking, as traditional\npercept-plan-control systems struggle to reach fruits amid the clutter.\nEffectively picking an occluded strawberry demands dexterous manipulation to\ncarefully bypass or gently move the surrounding soft objects and precisely\naccess the ideal picking point located at the stem just above the calyx. To\naddress this challenge, we introduce a strawberry-picking robotic system that\nlearns from human demonstrations. Our system features a 4-DoF SCARA arm paired\nwith a human teleoperation interface for efficient data collection and\nleverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a\nfine-grained visuomotor picking policy. Experiments under various occlusion\nscenarios demonstrate that our modified approach significantly outperforms the\ndirect implementation of ACT, underscoring its potential for practical\napplication in occluded strawberry picking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8349\u8393\u91c7\u6458\u4e2d\u7531\u4e8e\u6c34\u679c\u88ab\u906e\u6321\u5bfc\u81f4\u673a\u5668\u4eba\u96be\u4ee5\u7cbe\u51c6\u91c7\u6458\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u793a\u8303\u5b66\u4e60\u7684\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u8349\u8393\u901a\u5e38\u751f\u957f\u5728\u5bc6\u96c6\u7684\u7c07\u4e2d\uff0c\u6613\u88ab\u53f6\u7247\u548c\u830e\u79c6\u906e\u6321\uff0c\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u96be\u4ee5\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7cbe\u51c6\u91c7\u6458\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75284\u81ea\u7531\u5ea6SCARA\u673a\u68b0\u81c2\u548c\u4eba\u7c7b\u9065\u63a7\u63a5\u53e3\u6536\u96c6\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6539\u8fdb\u7684End Pose Assisted Action Chunking Transformer\uff08ACT\uff09\u6a21\u578b\uff0c\u5f00\u53d1\u7cbe\u7ec6\u7684\u89c6\u89c9\u8fd0\u52a8\u91c7\u6458\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u906e\u6321\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u5e94\u7528ACT\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u91c7\u6458\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7c7b\u793a\u8303\u5b66\u4e60\u7684\u91c7\u6458\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u906e\u6321\u73af\u5883\u4e0b\u7684\u8349\u8393\u91c7\u6458\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14711", "pdf": "https://arxiv.org/pdf/2509.14711", "abs": "https://arxiv.org/abs/2509.14711", "authors": ["Ziwei Huang", "Shiliang Lu", "Lu Bai", "Xuesong Cai", "Xiang Cheng"], "title": "LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines", "categories": ["eess.SP"], "comment": null, "summary": "Based on Synesthesia of Machines (SoM), a large language model (LLM) is\nadapted for multipath generation (LLM4MG) for the first time. Considering a\ntypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new\nmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,\nincluding channel multipath information, millimeter wave (mmWave) radar sensory\ndata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based\non the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The\nproposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic\nspace through feature extraction and fusion networks. To further achieve\ngeneral knowledge transfer from the pre-trained LLaMA for multipath generation\nvia multi-modal sensory data, the low-rank adaptation (LoRA)\nparameter-efficient fine-tuning and propagation-aware prompt engineering are\nexploited. Simulation results demonstrate that the proposed LLM4MG outperforms\nconventional deep learning-based methods in terms of line-of-sight\n(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath\npower/delay generation precision with normalized mean square error (NMSE) of\n0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and\ncross-scenario generalization. The utility of the proposed LLM4MG is validated\nby real-world generalization. The necessity of high-precision multipath\ngeneration for system design is also demonstrated by channel capacity\ncomparison.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u8054\u89c9\uff08SoM\uff09\u7684\u591a\u8def\u5f84\u751f\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM4MG\uff09\uff0c\u9996\u6b21\u5c06LLM\u5e94\u7528\u4e8e\u591a\u8def\u5f84\u751f\u6210\u573a\u666f\uff0c\u5e76\u57286G\u8f66\u8054\u7f51\uff08V2I\uff09\u4e2d\u5c55\u793a\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u8def\u5f84\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u57286G\u8f66\u8054\u7f51\u573a\u666f\u4e0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u548c\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u5229\u7528LLaMA 3.2\u6a21\u578b\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\u7f51\u7edc\u5bf9\u9f50\u591a\u6a21\u6001\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408LoRA\u5fae\u8c03\u548c\u4f20\u64ad\u611f\u77e5\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM4MG\u5728LoS/NLoS\u5206\u7c7b\u3001\u591a\u8def\u5f84\u529f\u7387/\u5ef6\u8fdf\u751f\u6210\u7cbe\u5ea6\u53ca\u8de8\u573a\u666f\u6cdb\u5316\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "LLM4MG\u5728\u591a\u8def\u5f84\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8f66\u8054\u7f51\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.14531", "pdf": "https://arxiv.org/pdf/2509.14531", "abs": "https://arxiv.org/abs/2509.14531", "authors": ["Haoran Xiao", "Xue Wang", "Huimin Lu", "Zhiwen Zeng", "Zirui Guo", "Ziqi Ni", "Yicong Ye", "Wei Dai"], "title": "Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations", "categories": ["cs.RO"], "comment": null, "summary": "This paper addresses the challenges of automating vibratory sieve shaker\noperations in a materials laboratory, focusing on three critical tasks: 1)\ndual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in\noverlapping workspaces, and 3) obstructed powder sample container delivery with\norientation constraints. These tasks present significant challenges, including\ninefficient sampling in narrow passages, the need for smooth trajectories to\nprevent spillage, and suboptimal paths generated by conventional methods. To\novercome these challenges, we propose a hierarchical planning framework\ncombining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.\nThe former uses a finite Gaussian mixture model to improve sampling efficiency\nin narrow passages, while the latter refines paths by shortening, simplifying,\nimposing joint constraints, and B-spline smoothing. Experimental results\ndemonstrate the framework's effectiveness: planning time is reduced by up to\n80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes\nthe full vibratory sieve shaker operation workflow in a physical experiment,\nvalidating its practical applicability for complex laboratory automation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u632f\u52a8\u7b5b\u5206\u673a\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u91c7\u6837\u6548\u7387\u548c\u5e73\u6ed1\u8f68\u8ff9\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5212\u65f6\u95f4\u548c\u8def\u5f84\u70b9\u3002", "motivation": "\u81ea\u52a8\u5316\u632f\u52a8\u7b5b\u5206\u673a\u64cd\u4f5c\u9762\u4e34\u72ed\u7a84\u7a7a\u95f4\u3001\u5e73\u6ed1\u8f68\u8ff9\u548c\u8def\u5f84\u4f18\u5316\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u5148\u9a8c\u5f15\u5bfc\u8def\u5f84\u89c4\u5212\u548c\u591a\u6b65\u8f68\u8ff9\u4f18\u5316\u7684\u5206\u5c42\u6846\u67b6\uff0c\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6539\u8fdb\u91c7\u6837\u6548\u7387\uff0c\u901a\u8fc7\u8def\u5f84\u7f29\u77ed\u3001\u7b80\u5316\u548cB\u6837\u6761\u5e73\u6ed1\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u89c4\u5212\u65f6\u95f4\u51cf\u5c1180.4%\uff0c\u8def\u5f84\u70b9\u51cf\u5c1189.4%\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u632f\u52a8\u7b5b\u5206\u673a\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14764", "pdf": "https://arxiv.org/pdf/2509.14764", "abs": "https://arxiv.org/abs/2509.14764", "authors": ["Yuanyuan Yao", "Simon Geirnaert", "Tinne Tuytelaars", "Alexander Bertrand"], "title": "Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding", "categories": ["eess.SP", "cs.SD"], "comment": null, "summary": "Decoding the attended speaker in a multi-speaker environment from\nelectroencephalography (EEG) has attracted growing interest in recent years,\nwith neuro-steered hearing devices as a driver application. Current approaches\ntypically rely on ground-truth labels of the attended speaker during training,\nnecessitating calibration sessions for each user and each EEG set-up to achieve\noptimal performance. While unsupervised self-adaptive auditory attention\ndecoding (AAD) for stimulus reconstruction has been developed to eliminate the\nneed for labeled data, it suffers from an initialization bias that can\ncompromise performance. Although an unbiased variant has been proposed to\naddress this limitation, it introduces substantial computational complexity\nthat scales with data size. This paper presents three computationally efficient\nalternatives that achieve comparable performance, but with a significantly\nlower and constant computational cost. The code for the proposed algorithms is\navailable at https://github.com/YYao-42/Unsupervised_AAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e09\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u542c\u89c9\u6ce8\u610f\u529b\u89e3\u7801\uff08AAD\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7814\u7a76\u65e0\u76d1\u7763AAD\u4e2d\u7684\u521d\u59cb\u5316\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5bfb\u6c42\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u5bf9\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u4e3a\u65e0\u76d1\u7763AAD\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14548", "pdf": "https://arxiv.org/pdf/2509.14548", "abs": "https://arxiv.org/abs/2509.14548", "authors": ["Emily Sumner", "Deepak E. Gopinath", "Laporsha Dees", "Patricio Reyes Gomez", "Xiongyi Cui", "Andrew Silva", "Jean Costa", "Allison Morgan", "Mariah Schrum", "Tiffany L. Chen", "Avinash Balachandran", "Guy Rosman"], "title": "SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Curated datasets are essential for training and evaluating AI approaches, but\nare often lacking in domains where language and physical action are deeply\nintertwined. In particular, few datasets capture how people acquire embodied\nskills through verbal instruction over time. To address this gap, we introduce\nSimCoachCorpus: a unique dataset of race car simulator driving that allows for\nthe investigation of rich interactive phenomena during guided and unguided\nmotor skill acquisition. In this dataset, 29 humans were asked to drive in a\nsimulator around a race track for approximately ninety minutes. Fifteen\nparticipants were given personalized one-on-one instruction from a professional\nperformance driving coach, and 14 participants drove without coaching. \\name\\\nincludes embodied features such as vehicle state and inputs, map (track\nboundaries and raceline), and cone landmarks. These are synchronized with\nconcurrent verbal coaching from a professional coach and additional feedback at\nthe end of each lap. We further provide annotations of coaching categories for\neach concurrent feedback utterance, ratings on students' compliance with\ncoaching advice, and self-reported cognitive load and emotional state of\nparticipants (gathered from surveys during the study). The dataset includes\nover 20,000 concurrent feedback utterances, over 400 terminal feedback\nutterances, and over 40 hours of vehicle driving data. Our naturalistic dataset\ncan be used for investigating motor learning dynamics, exploring linguistic\nphenomena, and training computational models of teaching. We demonstrate\napplications of this dataset for in-context learning, imitation learning, and\ntopic modeling. The dataset introduced in this work will be released publicly\nupon publication of the peer-reviewed version of this paper. Researchers\ninterested in early access may register at\nhttps://tinyurl.com/SimCoachCorpusForm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SimCoachCorpus\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u8a00\u4e0e\u52a8\u4f5c\u7ed3\u5408\u7684\u6280\u80fd\u5b66\u4e60\uff0c\u5305\u542b\u9a7e\u9a76\u6a21\u62df\u6570\u636e\u4e0e\u6559\u7ec3\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u8bed\u8a00\u4e0e\u52a8\u4f5c\u7ed3\u5408\u7684\u6280\u80fd\u5b66\u4e60\u9886\u57df\u4e0d\u8db3\uff0cSimCoachCorpus\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u6559\u5b66\u4e0e\u5b66\u4e60\u52a8\u6001\u7814\u7a76\u3002", "method": "\u6536\u96c629\u540d\u53c2\u4e0e\u8005\u5728\u8d5b\u8f66\u6a21\u62df\u5668\u4e2d\u7684\u9a7e\u9a76\u6570\u636e\uff0c\u5206\u4e3a\u6709\u6559\u7ec3\u6307\u5bfc\u548c\u65e0\u6307\u5bfc\u4e24\u7ec4\uff0c\u8bb0\u5f55\u8f66\u8f86\u72b6\u6001\u3001\u8bed\u97f3\u53cd\u9988\u53ca\u8ba4\u77e5\u60c5\u7eea\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b2\u4e07\u6761\u5b9e\u65f6\u53cd\u9988\u3001400\u6761\u603b\u7ed3\u53cd\u9988\u53ca40\u5c0f\u65f6\u9a7e\u9a76\u6570\u636e\uff0c\u53ef\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3001\u8bed\u8a00\u73b0\u8c61\u5206\u6790\u53ca\u6559\u5b66\u8ba1\u7b97\u6a21\u578b\u3002", "conclusion": "SimCoachCorpus\u4e3a\u7814\u7a76\u8fd0\u52a8\u5b66\u4e60\u3001\u8bed\u8a00\u73b0\u8c61\u53ca\u6559\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\uff0c\u5e76\u5c06\u516c\u5f00\u5171\u4eab\u3002"}}
{"id": "2509.14809", "pdf": "https://arxiv.org/pdf/2509.14809", "abs": "https://arxiv.org/abs/2509.14809", "authors": ["Ning Wang", "Chenyu Zhang", "Yanshi Sun", "Minghui Min", "Shiyin Li"], "title": "Comparative Performance Analysis of Different Hybrid NOMA Schemes", "categories": ["eess.SP"], "comment": "9 pages, 6 figures. Paper submitted to IEEE Internet of Things\n  Journal, paper ID IoT-55019-2025", "summary": "Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages\nof pure NOMA and conventional OMA organically, has emerged as a highly\npromising multiple access technology for future wireless networks. Recent\nstudies have proposed various H-NOMA systems by employing different successive\ninterference cancellation (SIC) methods for the NOMA transmission phase.\nHowever, existing analyses typically assume a fixed channel gain order between\npaired users, despite the fact that channel coefficients follow random\ndistribution, leading to their magnitude relationships inherently stochastic\nand time varying. This paper analyzes the performance of three H-NOMA schemes\nunder stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA\nscheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;\nc) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical\nanalysis derives closed-form expressions for the probability that H-NOMA\nschemes underperform conventional OMA. Asymptotic results in the high\nsignal-to-noise ratio (SNR) regime are also developed. Simulation results\nvalidate our analysis and demonstrate the performance of H-NOMA schemes across\ndifferent SNR scenarios, providing a theoretical foundation for the deployment\nof H-NOMA in next-generation wireless systems.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e09\u79cdH-NOMA\uff08\u6df7\u5408\u975e\u6b63\u4ea4\u591a\u5740\uff09\u65b9\u6848\u5728\u968f\u673a\u4fe1\u9053\u589e\u76ca\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u4f4e\u4e8e\u4f20\u7edfOMA\u7684\u95ed\u5f0f\u6982\u7387\u8868\u8fbe\u5f0f\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4e0d\u540cSNR\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709H-NOMA\u5206\u6790\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u4fe1\u9053\u589e\u76ca\u987a\u5e8f\uff0c\u800c\u5ffd\u7565\u4e86\u4fe1\u9053\u7cfb\u6570\u7684\u968f\u673a\u6027\u548c\u65f6\u53d8\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cdH-NOMA\u65b9\u6848\uff08FSIC\u3001HSIC-NPA\u3001HSIC-PA\uff09\u5728\u968f\u673a\u4fe1\u9053\u589e\u76ca\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63a8\u5bfc\u4e86\u95ed\u5f0f\u8868\u8fbe\u5f0f\u548c\u9ad8SNR\u6e10\u8fd1\u7ed3\u679c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540cSNR\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3aH-NOMA\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "conclusion": "H-NOMA\u5728\u968f\u673a\u4fe1\u9053\u589e\u76ca\u4e0b\u7684\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfOMA\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.14564", "pdf": "https://arxiv.org/pdf/2509.14564", "abs": "https://arxiv.org/abs/2509.14564", "authors": ["Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints", "categories": ["cs.RO"], "comment": "6 pages, 7 figures", "summary": "This study presents a system integration approach for planning schedules,\nsequences, tasks, and motions for reconfigurable robots to automatically\ndisassemble constrained structures in a non-destructive manner. Such systems\nmust adapt their configuration and coordination to the target structure, but\nthe large and complex search space makes them prone to local optima. To address\nthis, we integrate multiple robot arms equipped with different types of tools,\ntogether with a rotary stage, into a reconfigurable setup. This flexible system\nis based on a hierarchical optimization method that generates plans meeting\nmultiple preferred conditions under mandatory requirements within a realistic\ntimeframe. The approach employs two many-objective genetic algorithms for\nsequence and task planning with motion evaluations, followed by constraint\nprogramming for scheduling. Because sequence planning has a much larger search\nspace, we introduce a chromosome initialization method tailored to constrained\nstructures to mitigate the risk of local optima. Simulation results demonstrate\nthat the proposed method effectively solves complex problems in reconfigurable\nrobotic disassembly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53ef\u91cd\u6784\u673a\u5668\u4eba\u81ea\u52a8\u975e\u7834\u574f\u6027\u62c6\u5378\u53d7\u9650\u7ed3\u6784\u7684\u7cfb\u7edf\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53ef\u91cd\u6784\u673a\u5668\u4eba\u5728\u590d\u6742\u641c\u7d22\u7a7a\u95f4\u4e2d\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u4e86\u591a\u79cd\u5de5\u5177\u7684\u591a\u673a\u68b0\u81c2\u4e0e\u65cb\u8f6c\u5e73\u53f0\uff0c\u91c7\u7528\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u76ee\u6807\u9057\u4f20\u7b97\u6cd5\u548c\u7ea6\u675f\u89c4\u5212\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u62c6\u5378\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u91cd\u6784\u673a\u5668\u4eba\u62c6\u5378\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.14836", "pdf": "https://arxiv.org/pdf/2509.14836", "abs": "https://arxiv.org/abs/2509.14836", "authors": ["Keitaro Yamashita", "Kazuki Naganuma", "Shunsuke Ono"], "title": "Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization", "categories": ["eess.SP", "cs.LG"], "comment": "Submitted to the IEEE Open Journal of Signal Processing", "summary": "This paper proposes a method for vertex-wise flexible sampling of a broad\nclass of graph signals, designed to attain the best possible recovery based on\nthe generalized sampling theory. This is achieved by designing a sampling\noperator by an optimization problem, which is inherently non-convex, as the\nbest possible recovery imposes a rank constraint. An existing method for\nvertex-wise flexible sampling is able to control the number of active vertices\nbut cannot incorporate prior knowledge of mandatory or forbidden vertices. To\naddress these challenges, we formulate the operator design as a problem that\nhandles a constraint of the number of active vertices and prior knowledge on\nspecific vertices for sampling, mandatory inclusion or exclusion. We\ntransformed this constrained problem into a difference-of-convex (DC)\noptimization problem by using the nuclear norm and a DC penalty for vertex\nselection. To solve this, we develop a convergent solver based on the general\ndouble-proximal gradient DC algorithm. The effectiveness of our method is\ndemonstrated through experiments on various graph signal models, including\nreal-world data, showing superior performance in the recovery accuracy by\ncomparing to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u91c7\u6837\u7406\u8bba\u7684\u56fe\u4fe1\u53f7\u9876\u70b9\u7075\u6d3b\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u8bbe\u8ba1\u91c7\u6837\u7b97\u5b50\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5fc5\u987b\u6216\u7981\u6b62\u91c7\u6837\u9876\u70b9\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7DC\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7ed3\u5408\u9876\u70b9\u5fc5\u987b\u6216\u7981\u6b62\u91c7\u6837\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u91c7\u6837\u7075\u6d3b\u6027\u548c\u6062\u590d\u7cbe\u5ea6\u3002", "method": "\u5c06\u91c7\u6837\u7b97\u5b50\u8bbe\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3aDC\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u6838\u8303\u6570\u548cDC\u60e9\u7f5a\u5904\u7406\u9876\u70b9\u9009\u62e9\u7ea6\u675f\uff0c\u5e76\u5f00\u53d1\u53cc\u8fd1\u7aef\u68af\u5ea6DC\u6c42\u89e3\u5668\u3002", "result": "\u5728\u591a\u79cd\u56fe\u4fe1\u53f7\u6a21\u578b\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6062\u590d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9876\u70b9\u7075\u6d3b\u91c7\u6837\u548c\u4fe1\u53f7\u6062\u590d\u7684\u7cbe\u5ea6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.14630", "pdf": "https://arxiv.org/pdf/2509.14630", "abs": "https://arxiv.org/abs/2509.14630", "authors": ["Anzhe Chen", "Yifei Yang", "Zhenjie Zhu", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong", "Yue Wang"], "title": "Toward Embodiment Equivariant Vision-Language-Action Policy", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action policies learn manipulation skills across tasks,\nenvironments and embodiments through large-scale pre-training. However, their\nability to generalize to novel robot configurations remains limited. Most\napproaches emphasize model size, dataset scale and diversity while paying less\nattention to the design of action spaces. This leads to the configuration\ngeneralization problem, which requires costly adaptation. We address this\nchallenge by formulating cross-embodiment pre-training as designing policies\nequivariant to embodiment configuration transformations. Building on this\nprinciple, we propose a framework that (i) establishes a embodiment\nequivariance theory for action space and policy design, (ii) introduces an\naction decoder that enforces configuration equivariance, and (iii) incorporates\na geometry-aware network architecture to enhance embodiment-agnostic spatial\nreasoning. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach improves pre-training effectiveness and enables\nefficient fine-tuning on novel robot embodiments. Our code is available at\nhttps://github.com/hhcaz/e2vla", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5bf9\u5177\u8eab\u914d\u7f6e\u53d8\u6362\u7b49\u53d8\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u5728\u65b0\u9896\u673a\u5668\u4eba\u914d\u7f6e\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u65b0\u7684\u673a\u5668\u4eba\u914d\u7f6e\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5ffd\u89c6\u4e86\u52a8\u4f5c\u7a7a\u95f4\u7684\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5305\u62ec\uff1a(i) \u5177\u8eab\u7b49\u53d8\u6027\u7406\u8bba\uff0c(ii) \u5f3a\u5236\u914d\u7f6e\u7b49\u53d8\u7684\u52a8\u4f5c\u89e3\u7801\u5668\uff0c(iii) \u51e0\u4f55\u611f\u77e5\u7684\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u80fd\u9ad8\u6548\u5fae\u8c03\u5230\u65b0\u7684\u673a\u5668\u4eba\u914d\u7f6e\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u8c03\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u5177\u8eab\u9884\u8bad\u7ec3\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.14909", "pdf": "https://arxiv.org/pdf/2509.14909", "abs": "https://arxiv.org/abs/2509.14909", "authors": ["Flor Ortiz", "Eva Lagunas"], "title": "Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite Networks", "categories": ["eess.SP"], "comment": null, "summary": "This letter investigates dynamic routing in Next-Generation Satellite Orbit\n(NGSO) constellations and proposes a hybrid strategy that combines precomputed\nrouting tables with a Deep Q-Learning (DQL) fallback mechanism. While fully\nRL-based schemes offer adaptability to topology dynamics, they often suffer\nfrom high complexity, long convergence times, and unstable performance under\nheavy traffic. In contrast, the proposed framework exploits deterministic table\nlookups under nominal conditions and selectively activates the DQL agent only\nwhen links become unavailable or congested. Simulation results in large-scale\nNGSO networks show that the hybrid approach consistently achieves higher packet\ndelivery ratio, lower end-to-end delay, shorter average hop count, and improved\nthroughput compared to a pure RL baseline. These findings highlight the\neffectiveness of hybrid routing as a scalable and resilient solution for\ndelay-sensitive satellite broadband services", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8ba1\u7b97\u8def\u7531\u8868\u548c\u6df1\u5ea6Q\u5b66\u4e60\uff08DQL\uff09\u5907\u7528\u673a\u5236\u7684\u6df7\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u4e0b\u4e00\u4ee3\u536b\u661f\u8f68\u9053\uff08NGSO\uff09\u661f\u5ea7\u4e2d\u7684\u52a8\u6001\u8def\u7531\u3002", "motivation": "\u7531\u4e8e\u5b8c\u5168\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u65b9\u6848\u5728\u62d3\u6251\u52a8\u6001\u53d8\u5316\u65f6\u9002\u5e94\u6027\u597d\u4f46\u590d\u6742\u5ea6\u9ad8\u3001\u6536\u655b\u65f6\u95f4\u957f\u4e14\u5728\u91cd\u8d1f\u8f7d\u4e0b\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u8def\u7531\u65b9\u6848\u3002", "method": "\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u8def\u7531\u8868\uff0c\u4ec5\u5728\u94fe\u8def\u4e0d\u53ef\u7528\u6216\u62e5\u585e\u65f6\u9009\u62e9\u6027\u6fc0\u6d3bDQL\u4ee3\u7406\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408\u7b56\u7565\u5728\u5927\u89c4\u6a21NGSO\u7f51\u7edc\u4e2d\u76f8\u6bd4\u7eafRL\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u7684\u5305\u4ea4\u4ed8\u7387\u3001\u66f4\u4f4e\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u66f4\u77ed\u7684\u5e73\u5747\u8df3\u6570\u548c\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u6df7\u5408\u8def\u7531\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u5907\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u536b\u661f\u5bbd\u5e26\u670d\u52a1\u3002"}}
{"id": "2509.14636", "pdf": "https://arxiv.org/pdf/2509.14636", "abs": "https://arxiv.org/abs/2509.14636", "authors": ["Yufei Wei", "Wangtao Lu", "Sha Lu", "Chenxiao Hu", "Fuzhang Han", "Rong Xiong", "Yue Wang"], "title": "BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots", "categories": ["cs.RO"], "comment": null, "summary": "Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace,\nfacilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF\nmodel for monocular visual odometry (MVO) in intelligent transportation\nsystems. However, existing BEV methods suffer from sparse supervision signals\nand information loss during perspective-to-BEV projection. We present\nBEV-ODOM2, an enhanced framework addressing both limitations without additional\nannotations. Our approach introduces: (1) dense BEV optical flow supervision\nconstructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV\nfusion that computes correlation volumes before projection to preserve 6-DoF\nmotion cues while maintaining scale consistency. The framework employs three\nsupervision levels derived solely from pose data: dense BEV flow, 5-DoF for the\nPV branch, and final 3-DoF output. Enhanced rotation sampling further balances\ndiverse motion patterns in training. Extensive evaluation on KITTI, NCLT,\nOxford, and our newly collected ZJH-VO multi-scale dataset demonstrates\nstate-of-the-art performance, achieving 40 improvement in RTE compared to\nprevious BEV methods. The ZJH-VO dataset, covering diverse ground vehicle\nscenarios from underground parking to outdoor plazas, is publicly available to\nfacilitate future research.", "AI": {"tldr": "BEV-ODOM2\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684BEV\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u96c6\u5149\u6d41\u76d1\u7763\u548cPV-BEV\u878d\u5408\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a00\u758f\u76d1\u7763\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684BEV\u8868\u793a\u65b9\u6cd5\u5728\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4e2d\u5b58\u5728\u7a00\u758f\u76d1\u7763\u4fe1\u53f7\u548c\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "BEV-ODOM2\u5f15\u5165\u4e86\u5bc6\u96c6BEV\u5149\u6d41\u76d1\u7763\u548cPV-BEV\u878d\u5408\u6280\u672f\uff0c\u5728\u4e0d\u589e\u52a0\u989d\u5916\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff08\u5305\u62ec\u65b0\u6536\u96c6\u7684ZJH-VO\u6570\u636e\u96c6\uff09\u5b9e\u73b0\u4e86state-of-the-art\u6027\u80fd\uff0cRTE\u63d0\u5347\u4e8640%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86BEV\u8868\u793a\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684ZJH-VO\u6570\u636e\u96c6\u3002"}}
{"id": "2509.15069", "pdf": "https://arxiv.org/pdf/2509.15069", "abs": "https://arxiv.org/abs/2509.15069", "authors": ["Deijany Rodriguez Linares", "Oksana Moryakova", "H\u00e5kan Johansson"], "title": "Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators", "categories": ["eess.SP", "cs.DS", "cs.NA", "math.NA"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This letter presents a novel approach for \\mbox{efficiently} computing\ntime-index powered weighted sums of the form $\\sum_{n=0}^{N-1} n^{K} v[n]$\nusing cascaded accumulators. Traditional direct computation requires\n$K{\\times}N$ general multiplications, which become prohibitive for large $N$,\nwhile alternative strategies based on lookup tables or signal reversal require\nstoring entire data blocks. By exploiting accumulator properties, the proposed\nmethod eliminates the need for such storage and reduces the multiplicative cost\nto only $K{+}1$ constant multiplications, enabling efficient real-time\nimplementation. The approach is particularly useful when such sums need to be\nefficiently computed in sample-by-sample processing systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea7\u8054\u7d2f\u52a0\u5668\u7684\u9ad8\u6548\u8ba1\u7b97\u65f6\u95f4\u7d22\u5f15\u52a0\u6743\u548c\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u76f4\u63a5\u8ba1\u7b97\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u73b0\u6709\u66ff\u4ee3\u65b9\u6848\u9700\u8981\u5b58\u50a8\u6574\u4e2a\u6570\u636e\u5757\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "method": "\u5229\u7528\u7d2f\u52a0\u5668\u7684\u7279\u6027\uff0c\u6d88\u9664\u4e86\u5b58\u50a8\u9700\u6c42\uff0c\u5e76\u5c06\u4e58\u6cd5\u6b21\u6570\u51cf\u5c11\u5230\u4ec5\u9700K+1\u6b21\u5e38\u6570\u4e58\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u9010\u5904\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5b9e\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ea7\u8054\u7d2f\u52a0\u5668\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u7d22\u5f15\u52a0\u6743\u548c\u8ba1\u7b97\u3002"}}
{"id": "2509.14641", "pdf": "https://arxiv.org/pdf/2509.14641", "abs": "https://arxiv.org/abs/2509.14641", "authors": ["Sibaek Lee", "Jiung Yeon", "Hyeonwoo Yu"], "title": "Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion", "categories": ["cs.RO"], "comment": null, "summary": "Dense 3D convolutions provide high accuracy for perception but are too\ncomputationally expensive for real-time robotic systems. Existing tri-plane\nmethods rely on 2D image features with interpolation, point-wise queries, and\nimplicit MLPs, which makes them computationally heavy and unsuitable for\nembedded 3D inference. As an alternative, we propose a novel interpolation-free\ntri-plane lifting and volumetric fusion framework, that directly projects 3D\nvoxels into plane features and reconstructs a feature volume through broadcast\nand summation. This shifts nonlinearity to 2D convolutions, reducing complexity\nwhile remaining fully parallelizable. To capture global context, we add a\nlow-resolution volumetric branch fused with the lifted features through a\nlightweight integration layer, yielding a design that is both efficient and\nend-to-end GPU-accelerated. To validate the effectiveness of the proposed\nmethod, we conduct experiments on classification, completion, segmentation, and\ndetection, and we map the trade-off between efficiency and accuracy across\ntasks. Results show that classification and completion retain or improve\naccuracy, while segmentation and detection trade modest drops in accuracy for\nsignificant computational savings. On-device benchmarks on an NVIDIA Jetson\nOrin nano confirm robust real-time throughput, demonstrating the suitability of\nthe approach for embedded robotic perception.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u63d2\u503c\u4e09\u5e73\u9762\u63d0\u5347\u4e0e\u4f53\u7d20\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f3D\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u4e09\u5e73\u9762\u65b9\u6cd5\u4f9d\u8d562D\u56fe\u50cf\u7279\u5f81\u548c\u63d2\u503c\uff0c\u8ba1\u7b97\u91cf\u5927\uff0c\u4e0d\u9002\u5408\u5d4c\u5165\u5f0f3D\u63a8\u7406\u3002", "method": "\u91c7\u7528\u65e0\u63d2\u503c\u7684\u4e09\u5e73\u9762\u63d0\u5347\u548c\u4f53\u7d20\u878d\u5408\uff0c\u901a\u8fc7\u5e7f\u64ad\u548c\u6c42\u548c\u91cd\u5efa\u7279\u5f81\u4f53\u79ef\uff0c\u5e76\u5f15\u5165\u4f4e\u5206\u8fa8\u7387\u4f53\u7d20\u5206\u652f\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5206\u7c7b\u548c\u5b8c\u6210\u4efb\u52a1\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u5206\u5272\u548c\u68c0\u6d4b\u4efb\u52a1\u5728\u7565\u5fae\u727a\u7272\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u9002\u5408\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u611f\u77e5\uff0c\u80fd\u5728NVIDIA Jetson Orin nano\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002"}}
{"id": "2509.15129", "pdf": "https://arxiv.org/pdf/2509.15129", "abs": "https://arxiv.org/abs/2509.15129", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard\nfor advanced sensing, interest in using Wi-Fi Channel State Information (CSI)\nfor remote sensing has surged. Recent findings indicate that learning a unified\nthree-dimensional motion representation through Doppler Radiance Fields (DoRFs)\nderived from CSI significantly improves the generalization capabilities of\nWi-Fi-based human activity recognition (HAR). Despite this progress, CSI\nsignals remain affected by asynchronous access point (AP) clocks and additive\nnoise from environmental and hardware sources. Consequently, even with existing\npreprocessing techniques, both the CSI data and Doppler velocity projections\nused in DoRFs are still susceptible to noise and outliers, limiting HAR\nperformance. To address this challenge, we propose a novel framework for\nmulti-antenna APs to suppress noise and identify the most informative antennas\nbased on DoRF fitting errors, which capture inconsistencies among Doppler\nvelocity projections. Experimental results on a challenging small-scale hand\ngesture recognition dataset demonstrate that the proposed DoRF-guided\nWi-Fi-based HAR approach significantly improves generalization capability,\npaving the way for robust real-world sensing deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5929\u7ebfAP\u7684\u6846\u67b6\uff0c\u5229\u7528DoRF\u62df\u5408\u8bef\u5dee\u6765\u6291\u5236\u566a\u58f0\u548c\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5929\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86Wi-Fi\u57fa\u4e8e\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Wi-Fi CSI\u4fe1\u53f7\u53d7\u5230\u5f02\u6b65AP\u65f6\u949f\u548c\u73af\u5883\u53ca\u786c\u4ef6\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86HAR\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6291\u5236\u566a\u58f0\u5e76\u63d0\u5347\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u901a\u8fc7DoRF\u62df\u5408\u8bef\u5dee\u6765\u6291\u5236\u566a\u58f0\u5e76\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5929\u7ebf\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u5929\u7ebfAP\u7684\u65b0\u6846\u67b6\u3002", "result": "\u5728\u5c0f\u89c4\u6a21\u624b\u52bf\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684DoRF\u5f15\u5bfc\u7684Wi-Fi HAR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.14687", "pdf": "https://arxiv.org/pdf/2509.14687", "abs": "https://arxiv.org/abs/2509.14687", "authors": ["Cong Tai", "Zhaoyu Zheng", "Haixu Long", "Hansheng Wu", "Haodong Xiang", "Zhengbin Long", "Jun Xiong", "Rong Shi", "Shizhuang Zhang", "Gang Qiu", "He Wang", "Ruifeng Li", "Jun Huang", "Bin Chang", "Shuai Feng", "Tao Shen"], "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI", "categories": ["cs.RO"], "comment": null, "summary": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces\nseveral fundamental challenges, including the high cost of data acquisition,\nthe lack of a standardized benchmark, and the significant gap between\nsimulation and the real world. To overcome these obstacles, we propose\nRealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror\nbuilds an efficient, low-cost data collection, model training, and inference\nsystem that enables end-to-end VLA research without requiring a real robot. To\nfacilitate model evolution and fair comparison, we also introduce a dedicated\nVLA benchmark for humanoid robots, featuring multiple scenarios, extensive\ntrajectories, and various VLA models. Furthermore, by integrating generative\nmodels and 3D Gaussian Splatting to reconstruct realistic environments and\nrobot models, we successfully demonstrate zero-shot Sim2Real transfer, where\nmodels trained exclusively on simulation data can perform tasks on a real robot\nseamlessly, without any fine-tuning. In conclusion, with the unification of\nthese critical components, RealMirror provides a robust framework that\nsignificantly accelerates the development of VLA models for humanoid robots.\nProject page: https://terminators2025.github.io/RealMirror.github.io", "AI": {"tldr": "RealMirror\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4eba\u5f62\u673a\u5668\u4eba\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u5e73\u53f0\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u6570\u636e\u6536\u96c6\u3001\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u96f6\u6b21\u8fc1\u79fb\u548c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86VLA\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4ebaVLA\u9886\u57df\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u4ee5\u53ca\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRealMirror\u5e73\u53f0\uff0c\u6574\u5408\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u6570\u636e\u6536\u96c6\u4e0e\u8bad\u7ec3\u7cfb\u7edf\u3001\u751f\u6210\u6a21\u578b\u548c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u652f\u6301\u7aef\u5230\u7aefVLA\u7814\u7a76\u4e0e\u96f6\u6b21Sim2Real\u8fc1\u79fb\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u672a\u7ecf\u5fae\u8c03\u7684\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u65e0\u7f1d\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u573a\u666f\u3001\u591a\u6a21\u578b\u7684VLA\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "RealMirror\u901a\u8fc7\u5173\u952e\u7ec4\u4ef6\u7684\u96c6\u6210\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4ebaVLA\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u663e\u8457\u52a0\u901f\u4e86\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.15162", "pdf": "https://arxiv.org/pdf/2509.15162", "abs": "https://arxiv.org/abs/2509.15162", "authors": ["Jingreng Lei", "Yang Li", "Ziyue Wang", "Qingfeng Lin", "Ya-Feng Liu", "Yik-Chung Wu"], "title": "A Unified Distributed Algorithm for Hybrid Near-Far Field Activity Detection in Cell-Free Massive MIMO", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A great amount of endeavor has recently been devoted to activity detection\nfor massive machine-type communications in cell-free multiple-input\nmultiple-output (MIMO) systems. However, as the number of antennas at the\naccess points (APs) increases, the Rayleigh distance that separates the\nnear-field and far-field regions also expands, rendering the conventional\nassumption of far-field propagation alone impractical. To address this\nchallenge, this paper establishes a covariance-based formulation that can\neffectively capture the statistical property of hybrid near-far field channels.\nBased on this formulation, we theoretically reveal that increasing the\nproportion of near-field channels enhances the detection performance.\nFurthermore, we propose a distributed algorithm, where each AP performs local\nactivity detection and only exchanges the detection results to the central\nprocessing unit, thus significantly reducing the computational complexity and\nthe communication overhead. Not only with convergence guarantee, the proposed\nalgorithm is unified in the sense that it can handle single-cell or cell-free\nsystems with either near-field or far-field devices as special cases.\nSimulation results validate the theoretical analyses and demonstrate the\nsuperior performance of the proposed approach compared with existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u6df7\u5408\u8fd1\u8fdc\u573a\u4fe1\u9053\u5efa\u6a21\uff0c\u63ed\u793a\u4e86\u8fd1\u573a\u4fe1\u9053\u6bd4\u4f8b\u589e\u52a0\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u589e\u5f3a\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u63a5\u5165\u70b9\u5929\u7ebf\u6570\u91cf\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u8fdc\u573a\u4f20\u64ad\u5047\u8bbe\u4e0d\u518d\u9002\u7528\uff0c\u9700\u89e3\u51b3\u6df7\u5408\u8fd1\u8fdc\u573a\u4fe1\u9053\u7684\u6d3b\u52a8\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u4fe1\u9053\u6a21\u578b\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5404\u63a5\u5165\u70b9\u8fdb\u884c\u672c\u5730\u68c0\u6d4b\u5e76\u4ea4\u6362\u7ed3\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd1\u573a\u4fe1\u9053\u6bd4\u4f8b\u589e\u52a0\u53ef\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0d\u4ec5\u6536\u655b\u6709\u4fdd\u969c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u5355\u5c0f\u533a\u6216\u65e0\u5c0f\u533a\u7cfb\u7edf\u7684\u8fd1\u8fdc\u573a\u8bbe\u5907\u3002"}}
{"id": "2509.14688", "pdf": "https://arxiv.org/pdf/2509.14688", "abs": "https://arxiv.org/abs/2509.14688", "authors": ["Yue Xu", "Litao Wei", "Pengyu An", "Qingyu Zhang", "Yong-Lu Li"], "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation", "categories": ["cs.RO"], "comment": "Accepted at CoRL 2025", "summary": "Tactile-aware robot learning faces critical challenges in data collection and\nrepresentation due to data scarcity and sparsity, and the absence of force\nfeedback in existing systems. To address these limitations, we introduce a\ntactile robot learning system with both hardware and algorithm innovations. We\npresent exUMI, an extensible data collection device that enhances the vanilla\nUMI with robust proprioception (via AR MoCap and rotary encoder), modular\nvisuo-tactile sensing, and automated calibration, achieving 100% data\nusability. Building on an efficient collection of over 1 M tactile frames, we\npropose Tactile Prediction Pretraining (TPP), a representation learning\nframework through action-aware temporal tactile prediction, capturing contact\ndynamics and mitigating tactile sparsity. Real-world experiments show that TPP\noutperforms traditional tactile imitation learning. Our work bridges the gap\nbetween human tactile intuition and robot learning through co-designed hardware\nand algorithms, offering open-source resources to advance contact-rich\nmanipulation research. Project page: https://silicx.github.io/exUMI.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u786c\u4ef6\u548c\u7b97\u6cd5\u521b\u65b0\u7684\u89e6\u89c9\u611f\u77e5\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edfexUMI\uff0c\u63d0\u51fa\u4e86Tactile Prediction Pretraining (TPP)\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u6536\u96c6\u548c\u8868\u5f81\u5b66\u4e60\u63d0\u5347\u4e86\u89e6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89e6\u89c9\u611f\u77e5\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u7a00\u758f\u4ee5\u53ca\u7f3a\u4e4f\u529b\u53cd\u9988\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86exUMI\u6570\u636e\u6536\u96c6\u8bbe\u5907\uff0c\u7ed3\u5408\u4e86\u5f3a\u5927\u7684\u672c\u4f53\u611f\u77e5\u548c\u6a21\u5757\u5316\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff1b\u63d0\u51faTPP\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u611f\u77e5\u7684\u89e6\u89c9\u9884\u6d4b\u8fdb\u884c\u8868\u5f81\u5b66\u4e60\u3002", "result": "\u5b9e\u73b0\u4e86100%\u6570\u636e\u53ef\u7528\u6027\uff0c\u6536\u96c6\u4e86\u8d85\u8fc7100\u4e07\u89e6\u89c9\u5e27\uff1bTPP\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u4f20\u7edf\u89e6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u786c\u4ef6\u548c\u7b97\u6cd5\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u586b\u8865\u4e86\u4eba\u7c7b\u89e6\u89c9\u76f4\u89c9\u4e0e\u673a\u5668\u4eba\u5b66\u4e60\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\u3002"}}
{"id": "2509.14628", "pdf": "https://arxiv.org/pdf/2509.14628", "abs": "https://arxiv.org/abs/2509.14628", "authors": ["Zhihui Gao", "Zhecun Liu", "Tingjun Chen"], "title": "Chameleon: Integrated Sensing and Communication with Sub-Symbol Beam Switching in mmWave Networks", "categories": ["cs.NI", "eess.SP"], "comment": "14 pages, 17 figures", "summary": "Next-generation cellular networks are envisioned to integrate sensing\ncapabilities with communication, particularly in the millimeter-wave (mmWave)\nspectrum, where beamforming using large-scale antenna arrays enables\ndirectional signal transmissions for improved spatial multiplexing. In current\n5G networks, however, beamforming is typically designed either for\ncommunication or sensing (e.g., beam training during link establishment). In\nthis paper, we present Chameleon, a novel framework that augments and rapidly\nswitches beamformers during each demodulation reference signal (DMRS) symbol to\nachieve integrated sensing and communication (ISAC) in 5G mmWave networks. Each\nbeamformer introduces an additional sensing beam toward target angles while\nmaintaining the communication beams toward multiple users. We implement\nChameleon on a 28 GHz software-defined radio testbed supporting over-the-air 5G\nphysical downlink shared channel (PDSCH) transmissions. Extensive experiments\nin open environments show that Chameleon achieves multi-user communication with\na sum data rate of up to 0.80 Gbps across two users. Simultaneously, Chameleon\nemploys a beamformer switching interval of only 0.24 {\\mu}s, therefore\nproducing a 31x31-point 2D imaging within just 0.875 ms. Leveraging machine\nlearning, Chameleon further enables object localization with median errors of\n0.14 m (distance) and 0.24{\\deg} (angle), and material classification with\n99.0% accuracy.", "AI": {"tldr": "Chameleon\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u57285G\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u5b9e\u73b0\u901a\u4fe1\u4e0e\u611f\u77e5\u878d\u5408\uff08ISAC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u5207\u6362\u6ce2\u675f\u6210\u5f62\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u901a\u4fe1\u548c\u76ee\u6807\u611f\u77e5\u3002", "motivation": "\u5f53\u524d5G\u7f51\u7edc\u7684\u6ce2\u675f\u6210\u5f62\u901a\u5e38\u4ec5\u9488\u5bf9\u901a\u4fe1\u6216\u611f\u77e5\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6beb\u7c73\u6ce2\u9891\u8c31\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u4e24\u8005\u529f\u80fd\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "Chameleon\u5728\u6bcf\u4e2a\u89e3\u8c03\u53c2\u8003\u4fe1\u53f7\u7b26\u53f7\u671f\u95f4\u5feb\u901f\u5207\u6362\u6ce2\u675f\u6210\u5f62\u5668\uff0c\u5f15\u5165\u989d\u5916\u7684\u611f\u77e5\u6ce2\u675f\uff0c\u540c\u65f6\u7ef4\u6301\u7528\u6237\u901a\u4fe1\u6ce2\u675f\u3002\u8be5\u65b9\u6cd5\u572828 GHz\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChameleon\u5b9e\u73b0\u4e860.80 Gbps\u7684\u603b\u6570\u636e\u901f\u7387\uff0c\u5e76\u80fd\u57280.875\u6beb\u79d2\u5185\u5b8c\u621031x31\u70b9\u76842D\u6210\u50cf\uff0c\u540c\u65f6\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u76ee\u6807\u5b9a\u4f4d\u548c\u6750\u6599\u5206\u7c7b\u3002", "conclusion": "Chameleon\u5c55\u793a\u4e86\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u901a\u4fe1\u4e0e\u611f\u77e5\u9ad8\u6548\u96c6\u6210\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.14698", "pdf": "https://arxiv.org/pdf/2509.14698", "abs": "https://arxiv.org/abs/2509.14698", "authors": ["Andreas Mueller"], "title": "Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA"], "comment": null, "summary": "This paper revisits a three-loop spatial linkage that was proposed in an ARK\n2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by\nEddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez\net. al. A local analysis shows that this linkage has a finite degree of freedom\n(DOF) 3 (and is thus overconstrained) while in its reference configuration the\ndifferential DOF is 5. It is shown that its configuration space is locally a\nsmooth manifold so that the reference configuration is not a c-space\nsingularity. It is shown that the differential DOF is locally constant, which\nmakes this linkage shaky (so that the reference configuration is not a\nsingularity). The higher-order local analysis is facilitated by the computation\nof the kinematic tangent cone as well as a local approximation of the c-space.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5206\u6790\u4e86Karl Wohlhart\u5728ARK 2004\u4e2d\u63d0\u51fa\u7684\u4e09\u73af\u7a7a\u95f4\u8fde\u6746\uff08\u57fa\u4e8eEddie Baker\u57281980\u5e74\u63d0\u51fa\u7684\u53cc\u73af\u8fde\u6746\uff09\uff0c\u53d1\u73b0\u5176\u5728\u53c2\u8003\u914d\u7f6e\u4e0b\u5fae\u5206\u81ea\u7531\u5ea6\u4e3a5\uff0c\u4f46\u5b9e\u9645\u81ea\u7531\u5ea6\u4e3a3\u3002\u8be5\u8fde\u6746\u7684\u6784\u578b\u7a7a\u95f4\u662f\u5c40\u90e8\u5149\u6ed1\u6d41\u5f62\uff0c\u4e14\u5176\u5fae\u5206\u81ea\u7531\u5ea6\u5c40\u90e8\u6052\u5b9a\uff0c\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u8fd9\u4e00\u4e09\u73af\u7a7a\u95f4\u8fde\u6746\u7684\u81ea\u7531\u5ea6\u7279\u6027\u53ca\u5176\u6784\u578b\u7a7a\u95f4\u7684\u5c40\u90e8\u6027\u8d28\uff0c\u5c24\u5176\u662f\u5176\u8fc7\u7ea6\u675f\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u5206\u6790\uff0c\u8ba1\u7b97\u4e86\u8fd0\u52a8\u5207\u9525\u548c\u6784\u578b\u7a7a\u95f4\u7684\u5c40\u90e8\u8fd1\u4f3c\uff0c\u9a8c\u8bc1\u4e86\u8fde\u6746\u7684\u5fae\u5206\u81ea\u7531\u5ea6\u5c40\u90e8\u6052\u5b9a\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8be5\u8fde\u6746\u5728\u53c2\u8003\u914d\u7f6e\u4e0b\u5fae\u5206\u81ea\u7531\u5ea6\u4e3a5\uff0c\u5b9e\u9645\u81ea\u7531\u5ea6\u4e3a3\uff0c\u8868\u73b0\u51fa\u8fc7\u7ea6\u675f\u548c\u4e0d\u7a33\u5b9a\u6027\uff1b\u6784\u578b\u7a7a\u95f4\u662f\u5c40\u90e8\u5149\u6ed1\u6d41\u5f62\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8be5\u8fde\u6746\u5728\u53c2\u8003\u914d\u7f6e\u4e0b\u867d\u5fae\u5206\u81ea\u7531\u5ea6\u8f83\u9ad8\uff0c\u4f46\u56e0\u6784\u578b\u7a7a\u95f4\u7684\u5c40\u90e8\u5149\u6ed1\u6027\uff0c\u5176\u8fd0\u52a8\u7279\u6027\u8868\u73b0\u4e3a\u4e0d\u7a33\u5b9a\u800c\u975e\u5947\u5f02\u3002"}}
{"id": "2509.14632", "pdf": "https://arxiv.org/pdf/2509.14632", "abs": "https://arxiv.org/abs/2509.14632", "authors": ["Miseul Kim", "Soo Jin Park", "Kyungguen Byun", "Hyeon-Kyeong Shin", "Sunkuk Moon", "Shuhua Zhang", "Erik Visser"], "title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation", "categories": ["eess.AS", "cs.AI", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Speaker diarization systems often struggle with high intrinsic intra-speaker\nvariability, such as shifts in emotion, health, or content. This can cause\nsegments from the same speaker to be misclassified as different individuals,\nfor example, when one raises their voice or speaks faster during conversation.\nTo address this, we propose a style-controllable speech generation model that\naugments speech across diverse styles while preserving the target speaker's\nidentity. The proposed system starts with diarized segments from a conventional\ndiarizer. For each diarized segment, it generates augmented speech samples\nenriched with phonetic and stylistic diversity. And then, speaker embeddings\nfrom both the original and generated audio are blended to enhance the system's\nrobustness in grouping segments with high intrinsic intra-speaker variability.\nWe validate our approach on a simulated emotional speech dataset and the\ntruncated AMI dataset, demonstrating significant improvements, with error rate\nreductions of 49% and 35% on each dataset, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u683c\u53ef\u63a7\u7684\u8bed\u97f3\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u589e\u5f3a\u8bed\u97f3\u7684\u591a\u6837\u6027\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7684\u4fdd\u7559\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bf4\u8bdd\u4eba\u5206\u533a\u7cfb\u7edf\u7684\u9519\u8bef\u7387\u3002", "motivation": "\u89e3\u51b3\u8bf4\u8bdd\u4eba\u5206\u533a\u7cfb\u7edf\u56e0\u8bf4\u8bdd\u4eba\u5185\u90e8\u7684\u9ad8\u53d8\u5f02\u6027\uff08\u5982\u60c5\u7eea\u3001\u5065\u5eb7\u6216\u5185\u5bb9\u53d8\u5316\uff09\u5bfc\u81f4\u7684\u8bef\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u98ce\u683c\u53ef\u63a7\u7684\u8bed\u97f3\u751f\u6210\u6a21\u578b\u4e3a\u6bcf\u4e2a\u5206\u533a\u7247\u6bb5\u751f\u6210\u591a\u6837\u5316\u7684\u8bed\u97f3\u6837\u672c\uff0c\u5e76\u5c06\u539f\u59cb\u548c\u751f\u6210\u97f3\u9891\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6df7\u5408\u4ee5\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6a21\u62df\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\u548c\u622a\u65ad\u7684AMI\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8649%\u548c35%\u7684\u9519\u8bef\u7387\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u98ce\u683c\u53ef\u63a7\u7684\u8bed\u97f3\u751f\u6210\u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6df7\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u5206\u533a\u7cfb\u7edf\u5728\u9ad8\u53d8\u5f02\u6027\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.14726", "pdf": "https://arxiv.org/pdf/2509.14726", "abs": "https://arxiv.org/abs/2509.14726", "authors": ["Fangguo Zhao", "Xin Guan", "Shuo Li"], "title": "Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI", "categories": ["cs.RO"], "comment": null, "summary": "While model-based controllers have demonstrated remarkable performance in\nautonomous drone racing, their performance is often constrained by the reliance\non pre-computed reference trajectories. Conventional approaches, such as\ntrajectory tracking, demand a dynamically feasible, full-state reference,\nwhereas contouring control relaxes this requirement to a geometric path but\nstill necessitates a reference. Recent advancements in reinforcement learning\n(RL) have revealed that many model-based controllers optimize surrogate\nobjectives, such as trajectory tracking, rather than the primary racing goal of\ndirectly maximizing progress through gates. Inspired by these findings, this\nwork introduces a reference-free method for time-optimal racing by\nincorporating this gate progress objective, derived from RL reward shaping,\ndirectly into the Model Predictive Path Integral (MPPI) formulation. The\nsampling-based nature of MPPI makes it uniquely capable of optimizing the\ndiscontinuous and non-differentiable objective in real-time. We also establish\na unified framework that leverages MPPI to systematically and fairly compare\nthree distinct objective functions with a consistent dynamics model and\nparameter set: classical trajectory tracking, contouring control, and the\nproposed gate progress objective. We compare the performance of these three\nobjectives when solved via both MPPI and a traditional gradient-based solver.\nOur results demonstrate that the proposed reference-free approach achieves\ncompetitive racing performance, rivaling or exceeding reference-based methods.\nVideos are available at https://zhaofangguo.github.io/racing_mppi/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06RL\u5956\u52b1\u76ee\u6807\u76f4\u63a5\u878d\u5165MPPI\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\u7684\u65e0\u4eba\u673a\u7ade\u901f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u53c2\u8003\u8f68\u8ff9\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u63a7\u5236\u5668\u4f9d\u8d56\u9884\u8ba1\u7b97\u8f68\u8ff9\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\u8868\u660e\u8fd9\u4e9b\u63a7\u5236\u5668\u4f18\u5316\u7684\u662f\u4ee3\u7406\u76ee\u6807\u800c\u975e\u5b9e\u9645\u7ade\u901f\u76ee\u6807\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u76f4\u63a5\u4f18\u5316\u7ade\u901f\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528MPPI\u6846\u67b6\uff0c\u5c06\u7ade\u901f\u76ee\u6807\u76f4\u63a5\u878d\u5165\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u4e0e\u4f20\u7edf\u68af\u5ea6\u6c42\u89e3\u5668\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u63d0\u51fa\u7684\u65e0\u53c2\u8003\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4f20\u7edf\u53c2\u8003\u65b9\u6cd5\u3002", "conclusion": "\u76f4\u63a5\u4f18\u5316\u7ade\u901f\u76ee\u6807\u7684\u65e0\u53c2\u8003\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14675", "pdf": "https://arxiv.org/pdf/2509.14675", "abs": "https://arxiv.org/abs/2509.14675", "authors": ["Xuanjun Chen", "Chia-Yu Hu", "I-Ming Lin", "Yi-Cheng Lin", "I-Hsiang Chiu", "You Zhang", "Sung-Feng Huang", "Yi-Hsuan Yang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "How Does Instrumental Music Help SingFake Detection?", "categories": ["cs.SD", "eess.AS", "eess.SP"], "comment": "Work in progress", "summary": "Although many models exist to detect singing voice deepfakes (SingFake), how\nthese models operate, particularly with instrumental accompaniment, is unclear.\nWe investigate how instrumental music affects SingFake detection from two\nperspectives. To investigate the behavioral effect, we test different\nbackbones, unpaired instrumental tracks, and frequency subbands. To analyze the\nrepresentational effect, we probe how fine-tuning alters encoders' speech and\nmusic capabilities. Our results show that instrumental accompaniment acts\nmainly as data augmentation rather than providing intrinsic cues (e.g., rhythm\nor harmony). Furthermore, fine-tuning increases reliance on shallow speaker\nfeatures while reducing sensitivity to content, paralinguistic, and semantic\ninformation. These insights clarify how models exploit vocal versus\ninstrumental cues and can inform the design of more interpretable and robust\nSingFake detection systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e50\u5668\u4f34\u594f\u5728\u6b4c\u5531\u58f0\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4e3b\u8981\u8d77\u6570\u636e\u589e\u5f3a\u4f5c\u7528\uff0c\u800c\u975e\u5185\u5728\u7ebf\u7d22\uff1b\u5fae\u8c03\u4f1a\u589e\u52a0\u5bf9\u6d45\u5c42\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u4f9d\u8d56\u3002", "motivation": "\u63a2\u8ba8\u4e50\u5668\u97f3\u4e50\u5bf9\u6b4c\u5531\u58f0\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u3001\u975e\u914d\u5bf9\u4e50\u5668\u8f68\u9053\u548c\u9891\u5e26\uff0c\u4ee5\u53ca\u5206\u6790\u5fae\u8c03\u5bf9\u7f16\u7801\u5668\u8bed\u97f3\u548c\u97f3\u4e50\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u4e50\u5668\u4f34\u594f\u66f4\u591a\u662f\u6570\u636e\u589e\u5f3a\uff0c\u5fae\u8c03\u4f1a\u964d\u4f4e\u5bf9\u5185\u5bb9\u3001\u526f\u8bed\u8a00\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u6b4c\u5531\u58f0\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2509.14748", "pdf": "https://arxiv.org/pdf/2509.14748", "abs": "https://arxiv.org/abs/2509.14748", "authors": ["Maria Ibrahim", "Alap Kshirsagar", "Dorothea Koert", "Jan Peters"], "title": "Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces", "categories": ["cs.RO", "cs.HC"], "comment": "8 pages", "summary": "Effective communication is essential for safety and efficiency in human-robot\ncollaboration, particularly in shared workspaces. This paper investigates the\nimpact of nonverbal communication on human-robot interaction (HRI) by\nintegrating reactive light signals and emotional displays into a robotic\nsystem. We equipped a Franka Emika Panda robot with an LED strip on its end\neffector and an animated facial display on a tablet to convey movement intent\nthrough colour-coded signals and facial expressions. We conducted a human-robot\ncollaboration experiment with 18 participants, evaluating three conditions: LED\nsignals alone, LED signals with reactive emotional displays, and LED signals\nwith pre-emptive emotional displays. We collected data through questionnaires\nand position tracking to assess anticipation of potential collisions, perceived\nclarity of communication, and task performance. The results indicate that while\nemotional displays increased the perceived interactivity of the robot, they did\nnot significantly improve collision anticipation, communication clarity, or\ntask efficiency compared to LED signals alone. These findings suggest that\nwhile emotional cues can enhance user engagement, their impact on task\nperformance in shared workspaces is limited.", "AI": {"tldr": "\u7814\u7a76\u4e86\u975e\u8bed\u8a00\u4ea4\u6d41\u5bf9\u4eba\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u60c5\u611f\u5c55\u793a\u867d\u589e\u5f3a\u4e92\u52a8\u6027\uff0c\u4f46\u5bf9\u4efb\u52a1\u8868\u73b0\u6539\u5584\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u975e\u8bed\u8a00\u4ea4\u6d41\uff08\u5982\u706f\u5149\u4fe1\u53f7\u548c\u60c5\u611f\u5c55\u793a\uff09\u5982\u4f55\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u5728\u673a\u5668\u4eba\u4e0a\u96c6\u6210LED\u706f\u5e26\u548c\u8868\u60c5\u663e\u793a\u88c5\u7f6e\uff0c\u901a\u8fc718\u540d\u53c2\u4e0e\u8005\u7684\u534f\u4f5c\u5b9e\u9a8c\u8bc4\u4f30\u706f\u5149\u4fe1\u53f7\u548c\u60c5\u611f\u5c55\u793a\u7684\u6548\u679c\u3002", "result": "\u60c5\u611f\u5c55\u793a\u867d\u589e\u52a0\u4e86\u4e92\u52a8\u611f\uff0c\u4f46\u5e76\u672a\u663e\u8457\u63d0\u5347\u78b0\u649e\u9884\u671f\u3001\u4ea4\u6d41\u6e05\u6670\u5ea6\u6216\u4efb\u52a1\u6548\u7387\u3002", "conclusion": "\u60c5\u611f\u63d0\u793a\u53ef\u589e\u5f3a\u7528\u6237\u53c2\u4e0e\u611f\uff0c\u4f46\u5bf9\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u4efb\u52a1\u8868\u73b0\u5f71\u54cd\u6709\u9650\u3002"}}
{"id": "2509.14677", "pdf": "https://arxiv.org/pdf/2509.14677", "abs": "https://arxiv.org/abs/2509.14677", "authors": ["Miseul Kim", "Seyun Um", "Hyeonjin Cha", "Hong-goo Kang"], "title": "SpeechMLC: Speech Multi-label Classification", "categories": ["eess.AS", "eess.SP"], "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a multi-label classification framework to detect\nmultiple speaking styles in a speech sample. Unlike previous studies that have\nprimarily focused on identifying a single target style, our framework\neffectively captures various speaker characteristics within a unified\nstructure, making it suitable for generalized human-computer interaction\napplications. The proposed framework integrates cross-attention mechanisms\nwithin a transformer decoder to extract salient features associated with each\ntarget label from the input speech. To mitigate the data imbalance inherent in\nmulti-label speech datasets, we employ a data augmentation technique based on a\nspeech generation model. We validate our model's effectiveness through multiple\nobjective evaluations on seen and unseen corpora. In addition, we provide an\nanalysis of the influence of human perception on classification accuracy by\nconsidering the impact of human labeling agreement on model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bed\u97f3\u6837\u672c\u4e2d\u7684\u591a\u79cd\u8bf4\u8bdd\u98ce\u683c\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e00\u8bf4\u8bdd\u98ce\u683c\u8bc6\u522b\uff0c\u800c\u591a\u6807\u7b7e\u5206\u7c7b\u80fd\u66f4\u5168\u9762\u5730\u6355\u6349\u8bf4\u8bdd\u8005\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u6cdb\u5316\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u3002", "method": "\u96c6\u6210\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\u89e3\u7801\u5668\u62bd\u53d6\u8bed\u97f3\u7279\u5f81\uff1b\u91c7\u7528\u57fa\u4e8e\u8bed\u97f3\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u53ef\u89c1\u548c\u672a\u89c1\u8bed\u6599\u4e0a\u901a\u8fc7\u5ba2\u89c2\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u7c7b\u6807\u8bb0\u4e00\u81f4\u6027\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc6\u522b\u591a\u79cd\u8bf4\u8bdd\u98ce\u683c\uff0c\u4e3a\u591a\u6807\u7b7e\u8bed\u97f3\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14758", "pdf": "https://arxiv.org/pdf/2509.14758", "abs": "https://arxiv.org/abs/2509.14758", "authors": ["Ihab Tabbara", "Yuxuan Yang", "Ahmad Hamzeh", "Maxwell Astafyev", "Hussein Sibai"], "title": "Designing Latent Safety Filters using Pre-Trained Vision Models", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff08PVRs\uff09\u5728\u89c6\u89c9\u5b89\u5168\u8fc7\u6ee4\u5668\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u4e86\u5176\u4f5c\u4e3a\u5206\u7c7b\u5668\u9aa8\u67b6\u3001\u57fa\u4e8eHJ\u53ef\u8fbe\u6027\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u9aa8\u67b6\u548c\u6f5c\u4e16\u754c\u6a21\u578b\u9aa8\u67b6\u7684\u4f18\u52a3\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u90e8\u7f72\u95ee\u9898\u3002", "motivation": "\u786e\u4fdd\u89c6\u89c9\u63a7\u5236\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u5728\u5173\u952e\u573a\u666f\u4e2d\u3002\u867d\u7136\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u4f20\u7edf\u63a7\u5236\u7cfb\u7edf\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4f46\u5b83\u4eec\u5728\u89c6\u89c9\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76PVRs\u5728\u89c6\u89c9\u5b89\u5168\u8fc7\u6ee4\u5668\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528PVRs\u4f5c\u4e3a\u5206\u7c7b\u5668\u9aa8\u67b6\u3001\u57fa\u4e8eHJ\u53ef\u8fbe\u6027\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u9aa8\u67b6\u548c\u6f5c\u4e16\u754c\u6a21\u578b\u9aa8\u67b6\u3002\u63a2\u8ba8\u4e86\u4ece\u5934\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u51bb\u7ed3PVRs\u7684\u6743\u8861\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u4e86PVRs\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u548cQ\u51fd\u6570\u5728\u5b89\u5168\u7b56\u7565\u5207\u6362\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u5efa\u8bae\u3002", "conclusion": "PVRs\u5728\u89c6\u89c9\u5b89\u5168\u8fc7\u6ee4\u5668\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\u3002\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u9700\u6743\u8861\u8bad\u7ec3\u7b56\u7565\u548c\u8d44\u6e90\u9650\u5236\u3002"}}
{"id": "2509.14789", "pdf": "https://arxiv.org/pdf/2509.14789", "abs": "https://arxiv.org/abs/2509.14789", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Replay speech attacks pose a significant threat to voice-controlled systems,\nespecially in smart environments where voice assistants are widely deployed.\nWhile multi-channel audio offers spatial cues that can enhance replay detection\nrobustness, existing datasets and methods predominantly rely on single-channel\nrecordings. In this work, we introduce an acoustic simulation framework\ndesigned to simulate multi-channel replay speech configurations using publicly\navailable resources. Our setup models both genuine and spoofed speech across\nvaried environments, including realistic microphone and loudspeaker impulse\nresponses, room acoustics, and noise conditions. The framework employs measured\nloudspeaker directionalities during the replay attack to improve the realism of\nthe simulation. We define two spoofing settings, which simulate whether a\nreverberant or an anechoic speech is used in the replay scenario, and evaluate\nthe impact of omnidirectional and diffuse noise on detection performance. Using\nthe state-of-the-art M-ALRAD model for replay speech detection, we demonstrate\nthat synthetic data can support the generalization capabilities of the detector\nacross unseen enclosures.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u62df\u591a\u901a\u9053\u91cd\u653e\u8bed\u97f3\u653b\u51fb\u7684\u58f0\u5b66\u6846\u67b6\uff0c\u5229\u7528\u516c\u5f00\u8d44\u6e90\u6a21\u62df\u771f\u5b9e\u548c\u6b3a\u9a97\u8bed\u97f3\uff0c\u4ee5\u63d0\u5347\u91cd\u653e\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u91cd\u653e\u8bed\u97f3\u653b\u51fb\u5bf9\u667a\u80fd\u73af\u5883\u4e2d\u7684\u8bed\u97f3\u63a7\u5236\u7cfb\u7edf\u6784\u6210\u5a01\u80c1\uff0c\u800c\u73b0\u6709\u6570\u636e\u548c\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u901a\u9053\u5f55\u97f3\uff0c\u9650\u5236\u4e86\u591a\u901a\u9053\u68c0\u6d4b\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u58f0\u5b66\u6a21\u62df\u6846\u67b6\uff0c\u6a21\u62df\u591a\u901a\u9053\u91cd\u653e\u8bed\u97f3\u914d\u7f6e\uff0c\u5305\u62ec\u771f\u5b9e\u548c\u6b3a\u9a97\u8bed\u97f3\u7684\u73af\u5883\u3001\u9ea6\u514b\u98ce\u548c\u626c\u58f0\u5668\u8109\u51b2\u54cd\u5e94\u3001\u623f\u95f4\u58f0\u5b66\u53ca\u566a\u58f0\u6761\u4ef6\u3002", "result": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684M-ALRAD\u6a21\u578b\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u80fd\u591f\u63d0\u5347\u68c0\u6d4b\u5668\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6a21\u62df\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u91cd\u653e\u8bed\u97f3\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u901a\u9053\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.14787", "pdf": "https://arxiv.org/pdf/2509.14787", "abs": "https://arxiv.org/abs/2509.14787", "authors": ["Qixuan Li", "Chen Le", "Dongyue Huang", "Jincheng Yu", "Xinlei Chen"], "title": "COMPASS: Confined-space Manipulation Planning with Active Sensing Strategy", "categories": ["cs.RO"], "comment": null, "summary": "Manipulation in confined and cluttered environments remains a significant\nchallenge due to partial observability and complex configuration spaces.\nEffective manipulation in such environments requires an intelligent exploration\nstrategy to safely understand the scene and search the target. In this paper,\nwe propose COMPASS, a multi-stage exploration and manipulation framework\nfeaturing a manipulation-aware sampling-based planner. First, we reduce\ncollision risks with a near-field awareness scan to build a local collision\nmap. Additionally, we employ a multi-objective utility function to find\nviewpoints that are both informative and conducive to subsequent manipulation.\nMoreover, we perform a constrained manipulation optimization strategy to\ngenerate manipulation poses that respect obstacle constraints. To\nsystematically evaluate method's performance under these difficulties, we\npropose a benchmark of confined-space exploration and manipulation containing\nfour level challenging scenarios. Compared to exploration methods designed for\nother robots and only considering information gain, our framework increases\nmanipulation success rate by 24.25% in simulations. Real-world experiments\ndemonstrate our method's capability for active sensing and manipulation in\nconfined environments.", "AI": {"tldr": "COMPASS\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u63a2\u7d22\u548c\u64cd\u4f5c\u7b56\u7565\uff0c\u7ed3\u5408\u78b0\u649e\u611f\u77e5\u89c4\u5212\u5668\u548c\u591a\u76ee\u6807\u6548\u7528\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53d7\u9650\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u6210\u529f\u7387\u3002", "motivation": "\u53d7\u9650\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u56e0\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u590d\u6742\u7684\u914d\u7f6e\u7a7a\u95f4\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u667a\u80fd\u63a2\u7d22\u7b56\u7565\u6765\u5b89\u5168\u7406\u89e3\u573a\u666f\u5e76\u641c\u7d22\u76ee\u6807\u3002", "method": "\u63d0\u51faCOMPASS\u6846\u67b6\uff0c\u5305\u62ec\u78b0\u649e\u611f\u77e5\u7684\u91c7\u6837\u89c4\u5212\u5668\u3001\u5c40\u90e8\u78b0\u649e\u5730\u56fe\u6784\u5efa\u3001\u591a\u76ee\u6807\u6548\u7528\u51fd\u6570\u9009\u62e9\u89c6\u70b9\uff0c\u4ee5\u53ca\u53d7\u9650\u64cd\u4f5c\u4f18\u5316\u7b56\u7565\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u4e2d\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u9ad824.25%\uff0c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u53d7\u9650\u73af\u5883\u4e2d\u4e3b\u52a8\u611f\u77e5\u548c\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "COMPASS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u9650\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u95ee\u9898\uff0c\u4e3a\u540c\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14848", "pdf": "https://arxiv.org/pdf/2509.14848", "abs": "https://arxiv.org/abs/2509.14848", "authors": ["Houssem Sifaou", "Osvaldo Simeone"], "title": "Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Optimizing a reinforcement learning (RL) policy typically requires extensive\ninteractions with a high-fidelity simulator of the environment, which are often\ncostly or impractical. Offline RL addresses this problem by allowing training\nfrom pre-collected data, but its effectiveness is strongly constrained by the\nsize and quality of the dataset. Hybrid offline-online RL leverages both\noffline data and interactions with a single simulator of the environment. In\nmany real-world scenarios, however, multiple simulators with varying levels of\nfidelity and computational cost are available. In this work, we study\nmulti-fidelity hybrid RL for policy optimization under a fixed cost budget. We\nintroduce multi-fidelity hybrid RL via information gain maximization\n(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity\nselection based on information gain maximization through a bootstrapping\napproach. Theoretical analysis establishes the no-regret property of\nMF-HRL-IGM, while empirical evaluations demonstrate its superior performance\ncompared to existing benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u6700\u5927\u5316\u7684\u591a\u4fdd\u771f\u5ea6\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08MF-HRL-IGM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u6570\u636e\u548c\u5728\u7ebf\u4ea4\u4e92\uff0c\u5728\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e0a\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8d28\u91cf\u548c\u89c4\u6a21\u3002\u591a\u4fdd\u771f\u5ea6\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u53ef\u5229\u7528\u4e0d\u540c\u4fdd\u771f\u5ea6\u7684\u6a21\u62df\u5668\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMF-HRL-IGM\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u6700\u5927\u5316\u9009\u62e9\u6a21\u62df\u5668\u4fdd\u771f\u5ea6\uff0c\u5e76\u7ed3\u5408\u5f15\u5bfc\u65b9\u6cd5\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eMF-HRL-IGM\u5177\u6709\u65e0\u540e\u6094\u6027\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "MF-HRL-IGM\u5728\u56fa\u5b9a\u6210\u672c\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u4f18\u5316\uff0c\u4e3a\u591a\u4fdd\u771f\u5ea6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14816", "pdf": "https://arxiv.org/pdf/2509.14816", "abs": "https://arxiv.org/abs/2509.14816", "authors": ["Humphrey Munn", "Brendan Tidd", "Peter B\u00f6hm", "Marcus Gallagher", "David Howard"], "title": "Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) robot controllers usually aggregate many task\nobjectives into one scalar reward. While large-scale proximal policy\noptimisation (PPO) has enabled impressive results such as robust robot\nlocomotion in the real world, many tasks still require careful reward tuning\nand are brittle to local optima. Tuning cost and sub-optimality grow with the\nnumber of objectives, limiting scalability. Modelling reward vectors and their\ntrade-offs can address these issues; however, multi-objective methods remain\nunderused in RL for robotics because of computational cost and optimisation\ndifficulty. In this work, we investigate the conflict between gradient\ncontributions for each objective that emerge from scalarising the task\nobjectives. In particular, we explicitly address the conflict between\ntask-based rewards and terms that regularise the policy towards realistic\nbehaviour. We propose GCR-PPO, a modification to actor-critic optimisation that\ndecomposes the actor update into objective-wise gradients using a multi-headed\ncritic and resolves conflicts based on the objective priority. Our methodology,\nGCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion\nbenchmarks and additional multi-objective modifications on two related tasks.\nWe show superior scalability compared to parallel PPO (p = 0.04), without\nsignificant computational overhead. We also show higher performance with more\nconflicting tasks. GCR-PPO improves on large-scale PPO with an average\nimprovement of 9.5%, with high-conflict tasks observing a greater improvement.\nThe code is available at https://github.com/humphreymunn/GCR-PPO.", "AI": {"tldr": "GCR-PPO\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u68af\u5ea6\u5206\u89e3\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u591a\u76ee\u6807\u4efb\u52a1\u7684\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u7531\u4e8e\u5956\u52b1\u6807\u91cf\u5316\u5bfc\u81f4\u68af\u5ea6\u51b2\u7a81\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u591a\u5934\u8bc4\u8bba\u5bb6\u548c\u4f18\u5148\u7ea7\u5206\u89e3\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u63d0\u51faGCR-PPO\u65b9\u6cd5\u3002", "result": "\u5728IsaacLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGCR-PPO\u6bd4\u5e76\u884cPPO\u5e73\u5747\u63d0\u53479.5%\uff0c\u9ad8\u51b2\u7a81\u4efb\u52a1\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "GCR-PPO\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2509.14854", "pdf": "https://arxiv.org/pdf/2509.14854", "abs": "https://arxiv.org/abs/2509.14854", "authors": ["Aaron Fainman", "Stefan Vlaski"], "title": "Graph-Aware Learning Rates for Decentralized Optimization", "categories": ["math.OC", "eess.SP"], "comment": null, "summary": "We propose an adaptive step-size rule for decentralized optimization.\nChoosing a step-size that balances convergence and stability is challenging.\nThis is amplified in the decentralized setting as agents observe only local\n(possibly stochastic) gradients and global information (like smoothness) is\nunavailable. We derive a step-size rule from first principles. The resulting\nformulation reduces to the well-known Polyak's rule in the single-agent\nsetting, and is suitable for use with stochastic gradients. The method is\nparameter free, apart from requiring the optimal objective value, which is\nreadily available in many applications. Numerical simulations demonstrate that\nthe performance is comparable to the optimally fine-tuned step-size.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u4e2d\u7684\u81ea\u9002\u5e94\u6b65\u957f\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u5e73\u8861\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u7684\u6311\u6218\uff0c\u9002\u7528\u4e8e\u968f\u673a\u68af\u5ea6\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u5728\u9009\u62e9\u6b65\u957f\u65f6\uff0c\u5982\u4f55\u5728\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u53ea\u80fd\u89c2\u5bdf\u5230\u5c40\u90e8\uff08\u53ef\u80fd\u662f\u968f\u673a\u7684\uff09\u68af\u5ea6\uff0c\u4e14\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\uff08\u5982\u5e73\u6ed1\u6027\uff09\u3002", "method": "\u4ece\u57fa\u672c\u539f\u7406\u63a8\u5bfc\u51fa\u4e00\u79cd\u6b65\u957f\u89c4\u5219\uff0c\u8be5\u89c4\u5219\u5728\u5355\u4ee3\u7406\u60c5\u51b5\u4e0b\u7b80\u5316\u4e3a\u4f17\u6240\u5468\u77e5\u7684Polyak\u89c4\u5219\uff0c\u5e76\u9002\u7528\u4e8e\u968f\u673a\u68af\u5ea6\uff0c\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\uff08\u4ec5\u9700\u6700\u4f18\u76ee\u6807\u503c\uff09\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u6700\u4f18\u8c03\u8c10\u6b65\u957f\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6b65\u957f\u89c4\u5219\u4e3a\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u968f\u673a\u68af\u5ea6\u573a\u666f\u3002"}}
{"id": "2509.14889", "pdf": "https://arxiv.org/pdf/2509.14889", "abs": "https://arxiv.org/abs/2509.14889", "authors": ["Nan Sun", "Yongchang Li", "Chenxu Wang", "Huiying Li", "Huaping Liu"], "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human", "categories": ["cs.RO"], "comment": "8 pages, 5 figures, 3 tables", "summary": "In this work, we present CollabVLA, a self-reflective vision-language-action\nframework that transforms a standard visuomotor policy into a collaborative\nassistant. CollabVLA tackles key limitations of prior VLAs, including domain\noverfitting, non-interpretable reasoning, and the high latency of auxiliary\ngenerative models, by integrating VLM-based reflective reasoning with\ndiffusion-based action generation under a mixture-of-experts design. Through a\ntwo-stage training recipe of action grounding and reflection tuning, it\nsupports explicit self-reflection and proactively solicits human guidance when\nconfronted with uncertainty or repeated failure. It cuts normalized Time by ~2x\nand Dream counts by ~4x vs. generative agents, achieving higher success rates,\nimproved interpretability, and balanced low latency compared with existing\nmethods. This work takes a pioneering step toward shifting VLAs from opaque\ncontrollers to genuinely assistive agents capable of reasoning, acting, and\ncollaborating with humans.", "AI": {"tldr": "CollabVLA\u662f\u4e00\u79cd\u81ea\u6211\u53cd\u601d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u65b9\u6cd5\u5b58\u5728\u9886\u57df\u8fc7\u62df\u5408\u3001\u4e0d\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u548c\u751f\u6210\u6a21\u578b\u9ad8\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0cCollabVLA\u65e8\u5728\u5c06\u5176\u8f6c\u53d8\u4e3a\u53ef\u534f\u4f5c\u7684\u52a9\u624b\u3002", "method": "\u7ed3\u5408VLM\u7684\u53cd\u601d\u63a8\u7406\u548c\u6269\u6563\u52a8\u4f5c\u751f\u6210\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u52a8\u4f5c\u63a5\u5730\u548c\u53cd\u601d\u8c03\u4f18\uff09\uff0c\u652f\u6301\u663e\u5f0f\u81ea\u6211\u53cd\u601d\u5e76\u4e3b\u52a8\u5bfb\u6c42\u4eba\u7c7b\u6307\u5bfc\u3002", "result": "\u76f8\u6bd4\u751f\u6210\u5f0f\u4ee3\u7406\uff0c\u5ef6\u8fdf\u964d\u4f4e\u7ea62\u500d\uff0cDream\u8ba1\u6570\u51cf\u5c11\u7ea64\u500d\uff0c\u6210\u529f\u7387\u548c\u53ef\u89e3\u91ca\u6027\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "CollabVLA\u662f\u5c06VLA\u4ece\u9ed1\u76d2\u63a7\u5236\u5668\u8f6c\u53d8\u4e3a\u53ef\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u667a\u80fd\u52a9\u624b\u7684\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2509.14887", "pdf": "https://arxiv.org/pdf/2509.14887", "abs": "https://arxiv.org/abs/2509.14887", "authors": ["Hoang-Son Nguyen", "Hoi-To Wai"], "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis", "categories": ["cs.LG", "eess.SP"], "comment": "7 pages, 3 figures", "summary": "Learning the graph underlying a networked system from nodal signals is\ncrucial to downstream tasks in graph signal processing and machine learning.\nThe presence of hidden nodes whose signals are not observable might corrupt the\nestimated graph. While existing works proposed various robustifications of\nvanilla graph learning objectives by explicitly accounting for the presence of\nthese hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic\napproaches is still underexplored. This work demonstrates that vanilla graph\ntopology learning methods are implicitly robust to partial observations of\nlow-pass filtered graph signals. We achieve this theoretical result through\nextending the restricted isometry property (RIP) to the Dirichlet energy\nfunction used in graph learning objectives. We show that smoothness-based graph\nlearning formulation (e.g., the GL-SigRep method) on partial observations can\nrecover the ground truth graph topology corresponding to the observed nodes.\nSynthetic and real data experiments corroborate our findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7f51\u7edc\u7cfb\u7edf\u4e2d\u5b58\u5728\u9690\u85cf\u8282\u70b9\u65f6\uff0c\u4f20\u7edf\u7684\u56fe\u62d3\u6251\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4f4e\u901a\u6ee4\u6ce2\u56fe\u4fe1\u53f7\u7684\u90e8\u5206\u89c2\u6d4b\u5177\u6709\u9690\u5f0f\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u9690\u85cf\u8282\u70b9\u5bf9\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u56fe\u62d3\u6251\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u4f20\u7edf\u65b9\u6cd5\u5728\u90e8\u5206\u89c2\u6d4b\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55Dirichlet\u80fd\u91cf\u51fd\u6570\u7684\u9650\u5236\u7b49\u8ddd\u6027\uff08RIP\uff09\uff0c\u5206\u6790\u5e73\u6ed1\u5ea6\u57fa\u4e8e\u7684\u56fe\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GL-SigRep\uff09\u5728\u90e8\u5206\u89c2\u6d4b\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u90e8\u5206\u89c2\u6d4b\u4e0b\u4ecd\u80fd\u6062\u590d\u89c2\u6d4b\u8282\u70b9\u7684\u771f\u5b9e\u56fe\u62d3\u6251\u3002", "conclusion": "\u4f20\u7edf\u7684\u56fe\u62d3\u6251\u5b66\u4e60\u65b9\u6cd5\u5bf9\u9690\u85cf\u8282\u70b9\u548c\u90e8\u5206\u89c2\u6d4b\u5177\u6709\u9690\u5f0f\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u4f4e\u901a\u6ee4\u6ce2\u56fe\u4fe1\u53f7\u3002"}}
{"id": "2509.14915", "pdf": "https://arxiv.org/pdf/2509.14915", "abs": "https://arxiv.org/abs/2509.14915", "authors": ["Shenghai Yuan", "Jason Wai Hao Yee", "Weixiang Guo", "Zhongyuan Liu", "Thien-Minh Nguyen", "Lihua Xie"], "title": "PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for\nnavigation and mapping, yet horizontally mounted LiDARs such as the MID360\ncapture few near-ground returns, limiting terrain awareness and degrading\nperformance in feature-scarce environments. Prior solutions - static tilt,\nactive rotation, or high-density sensors - either sacrifice horizontal\nperception or incur added actuators, cost, and power. We introduce PERAL, a\nperception-aware motion control framework for spherical robots that achieves\npassive LiDAR excitation without dedicated hardware. By modeling the coupling\nbetween internal differential-drive actuation and sensor attitude, PERAL\nsuperimposes bounded, non-periodic oscillations onto nominal goal- or\ntrajectory-tracking commands, enriching vertical scan diversity while\npreserving navigation accuracy. Implemented on a compact spherical robot, PERAL\nis validated across laboratory, corridor, and tactical environments.\nExperiments demonstrate up to 96 percent map completeness, a 27 percent\nreduction in trajectory tracking error, and robust near-ground human detection,\nall at lower weight, power, and cost compared with static tilt, active\nrotation, and fixed horizontal baselines. The design and code will be\nopen-sourced upon acceptance.", "AI": {"tldr": "PERAL\u662f\u4e00\u79cd\u611f\u77e5\u611f\u77e5\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u88ab\u52a8\u6fc0\u53d1LiDAR\u63d0\u5347\u5782\u76f4\u626b\u63cf\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u5730\u56fe\u5b8c\u6574\u6027\u548c\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u6210\u672c\u4e0e\u529f\u8017\u3002", "motivation": "\u4f20\u7edfLiDAR-IMU\u91cc\u7a0b\u8ba1\u5728\u6c34\u5e73\u5b89\u88c5LiDAR\u65f6\u6355\u83b7\u8fd1\u5730\u9762\u6570\u636e\u5c11\uff0c\u5f71\u54cd\u5730\u5f62\u611f\u77e5\u4e0e\u6027\u80fd\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u727a\u7272\u6c34\u5e73\u611f\u77e5\u6216\u589e\u52a0\u786c\u4ef6\u6210\u672c\u3002", "method": "PERAL\u901a\u8fc7\u5efa\u6a21\u5185\u90e8\u5dee\u901f\u9a71\u52a8\u4e0e\u4f20\u611f\u5668\u59ff\u6001\u7684\u8026\u5408\uff0c\u5728\u76ee\u6807\u8ddf\u8e2a\u547d\u4ee4\u4e2d\u53e0\u52a0\u6709\u754c\u975e\u5468\u671f\u6027\u632f\u8361\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5373\u53ef\u4e30\u5bcc\u5782\u76f4\u626b\u63cf\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPERAL\u5728\u591a\u79cd\u73af\u5883\u4e2d\u5b9e\u73b096%\u5730\u56fe\u5b8c\u6574\u6027\u3001\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e27%\uff0c\u5e76\u6709\u6548\u68c0\u6d4b\u8fd1\u5730\u9762\u7269\u4f53\uff0c\u4e14\u6210\u672c\u4e0e\u529f\u8017\u66f4\u4f4e\u3002", "conclusion": "PERAL\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86LiDAR-IMU\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7d27\u51d1\u578b\u7403\u5f62\u673a\u5668\u4eba\u3002"}}
{"id": "2509.14905", "pdf": "https://arxiv.org/pdf/2509.14905", "abs": "https://arxiv.org/abs/2509.14905", "authors": ["Wenyan Ma", "Lipeng Zhu", "Rui Zhang"], "title": "Movable-Antenna Trajectory Optimization for Wireless Sensing: CRB Scaling Laws over Time and Space", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "In this paper, we present a new wireless sensing system utilizing a movable\nantenna (MA) that continuously moves and receives sensing signals to enhance\nsensing performance over the conventional fixed-position antenna (FPA) sensing.\nWe show that the angle estimation performance is fundamentally determined by\nthe MA trajectory, and derive the Cramer-Rao bound (CRB) of the mean square\nerror (MSE) for angle-of-arrival (AoA) estimation as a function of the\ntrajectory for both one-dimensional (1D) and two-dimensional (2D) antenna\nmovement. For the 1D case, a globally optimal trajectory that minimizes the CRB\nis derived in closed form. Notably, the resulting CRB decreases cubically with\nsensing time in the time-constrained regime, whereas it decreases linearly with\nsensing time and quadratically with the movement line segment's length in the\nspace-constrained regime. For the 2D case, we aim to achieve the minimum of\nmaximum (min-max) CRBs of estimation MSE for the two AoAs with respect to the\nhorizontal and vertical axes. To this end, we design an efficient alternating\noptimization algorithm that iteratively updates the MA's horizontal or vertical\ncoordinates with the other being fixed, yielding a locally optimal trajectory.\nNumerical results show that the proposed 1D/2D MA-based sensing schemes\nsignificantly reduce both the CRB and actual AoA estimation MSE compared to\nconventional FPA-based sensing with uniform linear/planar arrays (ULAs/UPAs) as\nwell as various benchmark MA trajectories. Moreover, it is revealed that the\nsteering vectors of our designed 1D/2D MA trajectories have low correlation in\nthe angular domain, thereby effectively increasing the angular resolution for\nachieving higher AoA estimation accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65e0\u7ebf\u4f20\u611f\u7cfb\u7edf\uff0c\u5229\u7528\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u901a\u8fc7\u8fde\u7eed\u8fd0\u52a8\u63a5\u6536\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u5347\u4f20\u611f\u6027\u80fd\u3002\u7814\u7a76\u8868\u660e\uff0c\u89d2\u5ea6\u4f30\u8ba1\u6027\u80fd\u4e3b\u8981\u7531MA\u8f68\u8ff9\u51b3\u5b9a\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e00\u7ef4\uff081D\uff09\u548c\u4e8c\u7ef4\uff082D\uff09\u5929\u7ebf\u8fd0\u52a8\u4e0b\u7684Cramer-Rao\u8fb9\u754c\uff08CRB\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0cMA\u65b9\u6848\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\uff08FPA\uff09\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\uff08FPA\uff09\u4f20\u611f\u6027\u80fd\u6709\u9650\uff0c\u5e0c\u671b\u901a\u8fc7\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684\u52a8\u6001\u7279\u6027\u63d0\u5347\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u5206\u6790MA\u8f68\u8ff9\u5bf9CRB\u7684\u5f71\u54cd\uff0c\u63a8\u5bfc\u4e861D\u548c2D\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u8f68\u8ff9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u5c40\u90e8\u6700\u4f18\u89e3\u3002", "result": "MA\u65b9\u6848\u57281D/2D\u60c5\u51b5\u4e0b\u5747\u663e\u8457\u964d\u4f4eCRB\u548c\u5b9e\u9645\u89d2\u5ea6\u4f30\u8ba1\u8bef\u5dee\uff0c\u4e14\u5177\u6709\u9ad8\u89d2\u5ea6\u5206\u8fa8\u7387\u3002", "conclusion": "MA\u4f20\u611f\u7cfb\u7edf\u5728\u89d2\u5ea6\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u4f20\u611f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14932", "pdf": "https://arxiv.org/pdf/2509.14932", "abs": "https://arxiv.org/abs/2509.14932", "authors": ["Tobias J\u00fclg", "Pierre Krack", "Seongjin Bien", "Yannik Blei", "Khaled Gamal", "Ken Nakahara", "Johannes Hechtl", "Roberto Calandra", "Wolfram Burgard", "Florian Walter"], "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action models (VLAs) mark a major shift in robot learning.\nThey replace specialized architectures and task-tailored components of expert\npolicies with large-scale data collection and setup-specific fine-tuning. In\nthis machine learning-focused workflow that is centered around models and\nscalable training, traditional robotics software frameworks become a\nbottleneck, while robot simulations offer only limited support for\ntransitioning from and to real-world experiments. In this work, we close this\ngap by introducing Robot Control Stack (RCS), a lean ecosystem designed from\nthe ground up to support research in robot learning with large-scale generalist\npolicies. At its core, RCS features a modular and easily extensible layered\narchitecture with a unified interface for simulated and physical robots,\nfacilitating sim-to-real transfer. Despite its minimal footprint and\ndependencies, it offers a complete feature set, enabling both real-world\nexperiments and large-scale training in simulation. Our contribution is\ntwofold: First, we introduce the architecture of RCS and explain its design\nprinciples. Second, we evaluate its usability and performance along the\ndevelopment cycle of VLA and RL policies. Our experiments also provide an\nextensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed\nlight on how simulation data can improve real-world policy performance. Our\ncode, datasets, weights, and videos are available at:\nhttps://robotcontrolstack.github.io/", "AI": {"tldr": "Robot Control Stack (RCS) \u662f\u4e00\u4e2a\u4e13\u4e3a\u652f\u6301\u5927\u89c4\u6a21\u901a\u7528\u7b56\u7565\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u67b6\u6784\u548c\u7edf\u4e00\u63a5\u53e3\uff0c\u4fbf\u4e8e\u4eff\u771f\u4e0e\u5b9e\u7269\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u8f6c\u6362\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8f6f\u4ef6\u6846\u67b6\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u548c\u6a21\u578b\u4e2d\u5fc3\u5316\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u65f6\u6210\u4e3a\u74f6\u9888\uff0c\u4eff\u771f\u7cfb\u7edf\u5bf9\u771f\u5b9e\u5b9e\u9a8c\u7684\u8fc7\u6e21\u652f\u6301\u6709\u9650\u3002", "method": "RCS \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5206\u5c42\u67b6\u6784\uff0c\u652f\u6301\u4eff\u771f\u4e0e\u5b9e\u7269\u673a\u5668\u4eba\u7684\u7edf\u4e00\u63a5\u53e3\uff0c\u4fc3\u8fdb\u4eff\u771f\u5230\u771f\u5b9e\u7684\u8f6c\u6362\u3002", "result": "RCS \u5c55\u793a\u4e86\u5bf9 VLA \u548c RL \u7b56\u7565\u5f00\u53d1\u5468\u671f\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4eff\u771f\u6570\u636e\u5bf9\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u7b56\u7565\u6027\u80fd\u7684\u4f5c\u7528\u3002", "conclusion": "RCS \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u529f\u80fd\u5b8c\u5907\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u901a\u7528\u7b56\u7565\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.14934", "pdf": "https://arxiv.org/pdf/2509.14934", "abs": "https://arxiv.org/abs/2509.14934", "authors": ["Francisco Messina", "Francesca Ronchini", "Luca Comanducci", "Paolo Bestagini", "Fabio Antonacci"], "title": "Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": null, "summary": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6587\u672c\u5230\u97f3\u9891\u6269\u6563\u6a21\u578b\u4e2d\u901a\u8fc7\u53cd\u8bb0\u5fc6\u7b56\u7565\u51cf\u5c11\u6570\u636e\u590d\u5236\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAMG\u7684\u6280\u672f\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u97f3\u9891\u6a21\u578b\u4e2d\u5b58\u5728\u6570\u636e\u590d\u5236\u7684\u6311\u6218\uff0c\u5373\u6a21\u578b\u5728\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u65e0\u610f\u4e2d\u751f\u6210\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u4e00\u90e8\u5206\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528Anti-Memorization Guidance (AMG)\u6280\u672f\uff0c\u4fee\u6539\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u8bb0\u5fc6\u5316\u3002\u5b9e\u9a8c\u57fa\u4e8eStable Audio Open\u67b6\u6784\u53ca\u5176\u5f00\u6e90\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAMG\u663e\u8457\u51cf\u5c11\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u4e14\u672a\u5f71\u54cd\u97f3\u9891\u4fdd\u771f\u5ea6\u6216\u8bed\u4e49\u5bf9\u9f50\u3002", "conclusion": "AMG\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u6587\u672c\u5230\u97f3\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\u3002"}}
{"id": "2509.14935", "pdf": "https://arxiv.org/pdf/2509.14935", "abs": "https://arxiv.org/abs/2509.14935", "authors": ["Punith Reddy Vanteddu", "Davide Gorbani", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Fabio Bergonti", "Daniele Pucci"], "title": "CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a CAD-driven co-design framework for optimizing\njet-powered aerial humanoid robots to execute dynamically constrained\ntrajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments\n(DoE) approach is used to generate 5,000 geometrically varied and mechanically\nfeasible designs by modifying limb dimensions, jet interface geometry (e.g.,\nangle and offset), and overall mass distribution. Each model is constructed\nthrough CAD assemblies to ensure structural validity and compatibility with\nsimulation tools. To reduce computational cost and enable parameter sensitivity\nanalysis, the models are clustered using K-means, with representative centroids\nselected for evaluation. A minimum-jerk trajectory is used to assess flight\nperformance, providing position and velocity references for a momentum-based\nlinearized Model Predictive Control (MPC) strategy. A multi-objective\noptimization is then conducted using the NSGA-II algorithm, jointly exploring\nthe space of design centroids and MPC gain parameters. The objectives are to\nminimize trajectory tracking error and mechanical energy expenditure. The\nframework outputs a set of flight-ready humanoid configurations with validated\ncontrol parameters, offering a structured method for selecting and implementing\nfeasible aerial humanoid designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdCAD\u9a71\u52a8\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u55b7\u6c14\u52a8\u529b\u98de\u884c\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u52a8\u6001\u7ea6\u675f\u8f68\u8ff9\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u9a8c\u751f\u62105000\u4e2a\u51e0\u4f55\u548c\u673a\u68b0\u53ef\u884c\u7684\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u548c\u4f18\u5316\u9009\u51fa\u6700\u4f73\u914d\u7f6e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u55b7\u6c14\u52a8\u529b\u98de\u884c\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6267\u884c\u52a8\u6001\u7ea6\u675f\u8f68\u8ff9\u65f6\u7684\u8bbe\u8ba1\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u7ed3\u6784\u6709\u6548\u6027\u548c\u4eff\u771f\u517c\u5bb9\u6027\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u5b9e\u9a8c\uff08DoE\uff09\u751f\u6210\u8bbe\u8ba1\u53d8\u4f53\uff0c\u805a\u7c7b\u540e\u9009\u62e9\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u7ed3\u5408\u6700\u5c0f\u52a0\u52a0\u901f\u5ea6\u8f68\u8ff9\u548c\u57fa\u4e8e\u52a8\u91cf\u7684MPC\u7b56\u7565\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\uff08NSGA-II\uff09\u3002", "result": "\u8f93\u51fa\u4e86\u4e00\u7ec4\u98de\u884c\u5c31\u7eea\u7684\u4eba\u5f62\u673a\u5668\u4eba\u914d\u7f6e\uff0c\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u53c2\u6570\u548c\u63a7\u5236\u589e\u76ca\uff0c\u51cf\u5c11\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u548c\u80fd\u91cf\u6d88\u8017\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u55b7\u6c14\u52a8\u529b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.15085", "pdf": "https://arxiv.org/pdf/2509.15085", "abs": "https://arxiv.org/abs/2509.15085", "authors": ["Simon Welker", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram\nto an audio waveform, is still a key component in many text-to-speech (TTS)\nsystems today. Based on generative flow matching, our prior work on generative\nSTFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel\nfilterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for\nspeech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total\nlatency of 48 ms. We show real-time streaming capability at this latency not\nonly in theory, but in practice on a consumer laptop GPU. Furthermore, we show\nthat our model achieves substantially better PESQ and SI-SDR values compared to\nwell-established not streaming-capable baselines for Mel vocoding including\nHiFi-GAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMelFlow\u7684\u6d41\u5f0f\u751f\u6210\u6885\u5c14\u9891\u8c31\u58f0\u7801\u5668\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6027\u80fd\u3002", "motivation": "\u6885\u5c14\u9891\u8c31\u58f0\u7801\u5668\uff08Mel vocoding\uff09\u662f\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5f53\u524d\u65b9\u6848\u5728\u5b9e\u65f6\u6027\u548c\u6027\u80fd\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u6d41\u5339\u914d\u3001STFT\u76f8\u4f4d\u68c0\u7d22\uff08DiffPhase\uff09\u548c\u6885\u5c14\u6ee4\u6ce2\u5668\u7ec4\u7684\u4f2a\u9006\u64cd\u4f5c\uff0c\u63d0\u51fa\u6d41\u5f0f\u751f\u6210\u6a21\u578bMelFlow\u3002", "result": "MelFlow\u572816kHz\u91c7\u6837\u7387\u4e0b\u4ec548ms\u7684\u5ef6\u8fdf\uff0c\u4e14\u5728\u6d88\u8d39\u8005\u7b14\u8bb0\u672c\u7535\u8111GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u975e\u6d41\u5f0f\u57fa\u7ebf\uff08\u5982HiFi-GAN\uff09\u3002", "conclusion": "MelFlow\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u6d41\u5f0f\u6885\u5c14\u9891\u8c31\u58f0\u7801\u5668\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6TTS\u7cfb\u7edf\u3002"}}
{"id": "2509.14939", "pdf": "https://arxiv.org/pdf/2509.14939", "abs": "https://arxiv.org/abs/2509.14939", "authors": ["Hao Zhang", "Zhen Kan", "Weiwei Shang", "Yongduan Song"], "title": "A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects", "categories": ["cs.RO"], "comment": "Accepted by IEEE/ASME Transactions on Mechatronics", "summary": "Despite recent advances in dexterous manipulations, the manipulation of\narticulated objects and generalization across different categories remain\nsignificant challenges. To address these issues, we introduce DART, a novel\nframework that enhances a diffusion-based policy with affordance learning and\nlinear temporal logic (LTL) representations to improve the learning efficiency\nand generalizability of articulated dexterous manipulation. Specifically, DART\nleverages LTL to understand task semantics and affordance learning to identify\noptimal interaction points. The {diffusion-based policy} then generalizes these\ninteractions across various categories. Additionally, we exploit an\noptimization method based on interaction data to refine actions, overcoming the\nlimitations of traditional diffusion policies that typically rely on offline\nreinforcement learning or learning from demonstrations. Experimental results\ndemonstrate that DART outperforms most existing methods in manipulation\nability, generalization performance, transfer reasoning, and robustness. For\nmore information, visit our project website at:\nhttps://sites.google.com/view/dart0257/.", "AI": {"tldr": "DART\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u7b56\u7565\u3001\u529f\u80fd\u5b66\u4e60\u548c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\uff0c\u63d0\u5347\u4e86\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u94f0\u63a5\u7269\u4f53\u64cd\u4f5c\u548c\u8de8\u7c7b\u522b\u6cdb\u5316\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528LTL\u7406\u89e3\u4efb\u52a1\u8bed\u4e49\uff0c\u529f\u80fd\u5b66\u4e60\u8bc6\u522b\u6700\u4f73\u4ea4\u4e92\u70b9\uff0c\u6269\u6563\u7b56\u7565\u6cdb\u5316\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDART\u5728\u64cd\u4f5c\u80fd\u529b\u3001\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DART\u901a\u8fc7\u65b0\u9896\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u94f0\u63a5\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2509.15170", "pdf": "https://arxiv.org/pdf/2509.15170", "abs": "https://arxiv.org/abs/2509.15170", "authors": ["Aarushi Mahajan", "Wayne Burleson"], "title": "Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting", "categories": ["cs.CR", "cs.AI", "eess.SP"], "comment": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP)", "summary": "Radio frequency fingerprint identification (RFFI) distinguishes wireless\ndevices by the small variations in their analog circuits, avoiding heavy\ncryptographic authentication. While deep learning on spectrograms improves\naccuracy, models remain vulnerable to copying, tampering, and evasion. We\npresent a stronger RFFI system combining watermarking for ownership proof and\nanomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel\nspectrograms, we embed three watermarks: a simple trigger, an adversarially\ntrained trigger robust to noise and filtering, and a hidden gradient/weight\nsignature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler\n(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,\nour system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,\noffering verifiable, tamper-resistant authentication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6c34\u5370\u6280\u672f\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u7cfb\u7edf\u6613\u53d7\u590d\u5236\u3001\u7be1\u6539\u548c\u89c4\u907f\u653b\u51fb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u8ba4\u8bc1\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528ResNet-34\u5bf9log-Mel\u9891\u8c31\u56fe\u8fdb\u884c\u5206\u6790\uff0c\u5d4c\u5165\u4e09\u79cd\u6c34\u5370\uff0c\u5e76\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728LoRa\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u8fbe\u523094.6%\u7684\u51c6\u786e\u7387\u300198%\u7684\u6c34\u5370\u6210\u529f\u7387\u548c0.94\u7684AUROC\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u4e14\u9632\u7be1\u6539\u7684\u8ba4\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.14941", "pdf": "https://arxiv.org/pdf/2509.14941", "abs": "https://arxiv.org/abs/2509.14941", "authors": ["Zongyuan Shen", "Burhanuddin Shirose", "Prasanna Sriganesh", "Bhaskar Vundurthy", "Howie Choset", "Matthew Travers"], "title": "Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "Efficient coordination of multiple robots for coverage of large, unknown\nenvironments is a significant challenge that involves minimizing the total\ncoverage path length while reducing inter-robot conflicts. In this paper, we\nintroduce a Multi-robot Connectivity-Aware Planner (Multi-CAP), a hierarchical\ncoverage path planning algorithm that facilitates multi-robot coordination\nthrough a novel connectivity-aware approach. The algorithm constructs and\ndynamically maintains an adjacency graph that represents the environment as a\nset of connected subareas. Critically, we make the assumption that the\nenvironment, while unknown, is bounded. This allows for incremental refinement\nof the adjacency graph online to ensure its structure represents the physical\nlayout of the space, both in observed and unobserved areas of the map as robots\nexplore the environment. We frame the task of assigning subareas to robots as a\nVehicle Routing Problem (VRP), a well-studied problem for finding optimal\nroutes for a fleet of vehicles. This is used to compute disjoint tours that\nminimize redundant travel, assigning each robot a unique, non-conflicting set\nof subareas. Each robot then executes its assigned tour, independently adapting\nits coverage strategy within each subarea to minimize path length based on\nreal-time sensor observations of the subarea. We demonstrate through\nsimulations and multi-robot hardware experiments that Multi-CAP significantly\noutperforms state-of-the-art methods in key metrics, including coverage time,\ntotal path length, and path overlap ratio. Ablation studies further validate\nthe critical role of our connectivity-aware graph and the global tour planner\nin achieving these performance gains.", "AI": {"tldr": "\u591a\u673a\u5668\u4eba\u534f\u540c\u8986\u76d6\u672a\u77e5\u5927\u73af\u5883\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5Multi-CAP\uff0c\u901a\u8fc7\u52a8\u6001\u7ef4\u62a4\u90bb\u63a5\u56fe\u548cVRP\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8986\u76d6\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u534f\u540c\u8986\u76d6\u7684\u6311\u6218\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8def\u5f84\u957f\u5ea6\u548c\u51cf\u5c11\u51b2\u7a81\u3002", "method": "\u63d0\u51faMulti-CAP\u7b97\u6cd5\uff0c\u52a8\u6001\u7ef4\u62a4\u90bb\u63a5\u56fe\uff0c\u5c06\u5b50\u533a\u57df\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3aVRP\uff0c\u751f\u6210\u72ec\u7acb\u8986\u76d6\u8def\u5f84\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0cMulti-CAP\u5728\u8986\u76d6\u65f6\u95f4\u3001\u8def\u5f84\u957f\u5ea6\u548c\u91cd\u53e0\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Multi-CAP\u901a\u8fc7\u8fde\u63a5\u611f\u77e5\u56fe\u548c\u5168\u5c40\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u8986\u76d6\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.15184", "pdf": "https://arxiv.org/pdf/2509.15184", "abs": "https://arxiv.org/abs/2509.15184", "authors": ["Irtiza Hasan", "Ahmed Arafa"], "title": "Version Age of Information with Contact Mobility in Gossip Networks", "categories": ["cs.IT", "cs.NI", "eess.SP", "math.IT"], "comment": "To appear in the 2025 Allerton Conference on Communication, Control,\n  and Computing", "summary": "A gossip network is considered in which a source node updates its status\nwhile other nodes in the network aim at keeping track of it as it varies over\ntime. Information gets disseminated by the source sending status updates to the\nnodes, and the nodes gossiping with each other. In addition, the nodes in the\nnetwork are mobile, and can move to other nodes to get information, which we\nterm contact mobility. The goal for the nodes is to remain as fresh as\npossible, i.e., to have the same information as the source's. To evaluate the\nfreshness of information, we use the Version Age-of-Information (VAoI) metric,\ndefined as the difference between the version of information available at a\ngiven node and that at the source. We analyze the effect of contact mobility on\ninformation dissemination in the gossip network using a Stochastic Hybrid\nSystem (SHS) framework for different topologies and mobility scalings with\nincreasing number of nodes. It is shown that with the presence of contact\nmobility the freshness of the network improves in both ends of the network\nconnectivity spectrum: disconnected and fully connected gossip networks. We\nmathematically analyze the average version age scalings and validate our\ntheoretical results via simulations. Finally, we incorporate the cost of\nmobility for the network by formulating and solving an optimization problem\nthat minimizes a weighted sum of version age and mobility cost. Our results\nshow that contact mobility, with optimized mobility cost, improves the average\nversion age in the network.", "AI": {"tldr": "\u7814\u7a76\u4e86\u79fb\u52a8\u6027\u5bf9\u4fe1\u606f\u65b0\u9c9c\u5ea6\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u4f18\u5316\u79fb\u52a8\u6210\u672c\u63d0\u5347\u4e86\u4fe1\u606f\u4f20\u64ad\u6548\u7387\u3002", "motivation": "\u63a2\u7a76\u79fb\u52a8\u8282\u70b9\u5728\u7f51\u7edc\u4e2d\u5982\u4f55\u901a\u8fc7\u63a5\u89e6\u79fb\u52a8\u6027\uff08contact mobility\uff09\u63d0\u5347\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u7f51\u7edc\u62d3\u6251\u4e0b\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08SHS\uff09\u6846\u67b6\u5206\u6790\u4e0d\u540c\u62d3\u6251\u548c\u79fb\u52a8\u6027\u5bf9\u7248\u672c\u5e74\u9f84\uff08VAoI\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u79fb\u52a8\u6027\u663e\u8457\u6539\u5584\u4e86\u7f51\u7edc\u7684\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5b8c\u5168\u65ad\u5f00\u6216\u5b8c\u5168\u8fde\u63a5\u7684\u6781\u7aef\u7f51\u7edc\u4e2d\u3002\u4f18\u5316\u540e\u7684\u79fb\u52a8\u6210\u672c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5e73\u5747\u7248\u672c\u5e74\u9f84\u3002", "conclusion": "\u63a5\u89e6\u79fb\u52a8\u6027\u7ed3\u5408\u4f18\u5316\u7684\u79fb\u52a8\u6210\u672c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7f51\u7edc\u4fe1\u606f\u4f20\u64ad\u7684\u65b0\u9c9c\u5ea6\u3002"}}
{"id": "2509.14949", "pdf": "https://arxiv.org/pdf/2509.14949", "abs": "https://arxiv.org/abs/2509.14949", "authors": ["Laura Ribeiro", "Muhammad Shaheer", "Miguel Fernandez-Cortizas", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Human Interaction for Collaborative Semantic SLAM using Extended Reality", "categories": ["cs.RO", "cs.HC"], "comment": "7 pages, 5 figures, 3 tables", "summary": "Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot\nmaps with structural and semantic information, enabling robots to operate more\neffectively in complex environments. However, these systems struggle in\nreal-world scenarios with occlusions, incomplete data, or ambiguous geometries,\nas they cannot fully leverage the higher-level spatial and semantic knowledge\nhumans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semantic\nSLAM framework that uses a shared extended reality environment for real-time\ncollaboration. The system allows human operators to directly interact with and\nvisualize the robot's 3D scene graph, and add high-level semantic concepts\n(e.g., rooms or structural entities) into the mapping process. We propose a\ngraph-based semantic fusion methodology that integrates these human\ninterventions with robot perception, enabling scalable collaboration for\nenhanced situational awareness. Experimental evaluations on real-world\nconstruction site datasets demonstrate improvements in room detection accuracy,\nmap precision, and semantic completeness compared to automated baselines,\ndemonstrating both the effectiveness of the approach and its potential for\nfuture extensions.", "AI": {"tldr": "HICS-SLAM\u662f\u4e00\u79cd\u4eba\u5728\u73af\u8bed\u4e49SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u5730\u56fe\u6784\u5efa\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49SLAM\u5728\u906e\u6321\u3001\u6570\u636e\u4e0d\u5b8c\u6574\u6216\u51e0\u4f55\u6a21\u7cca\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u9ad8\u5c42\u7a7a\u95f4\u4e0e\u8bed\u4e49\u77e5\u8bc6\u7684\u5229\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u8bed\u4e49\u878d\u5408\u65b9\u6cd5\uff0c\u5141\u8bb8\u4eba\u7c7b\u76f4\u63a5\u5e72\u9884\u5e76\u6dfb\u52a0\u9ad8\u5c42\u8bed\u4e49\u6982\u5ff5\uff08\u5982\u623f\u95f4\u6216\u7ed3\u6784\u5b9e\u4f53\uff09\u5230\u673a\u5668\u4eba\u611f\u77e5\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u771f\u5b9e\u5efa\u7b51\u5de5\u5730\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u81ea\u52a8\u57fa\u7ebf\u65b9\u6cd5\uff0cHICS-SLAM\u5728\u623f\u95f4\u68c0\u6d4b\u7cbe\u5ea6\u3001\u5730\u56fe\u51c6\u786e\u6027\u548c\u8bed\u4e49\u5b8c\u6574\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HICS-SLAM\u6846\u67b6\u8bc1\u660e\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u8bed\u4e49SLAM\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u8fdb\u4e00\u6b65\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.14954", "pdf": "https://arxiv.org/pdf/2509.14954", "abs": "https://arxiv.org/abs/2509.14954", "authors": ["Xingchen Xu", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor", "categories": ["cs.RO"], "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2025. Please cite the proceedings version", "summary": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7eb9\u7406\u5206\u7c7b\uff0c\u901a\u8fc7NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u53d1\u73b0\u6ed1\u52a8+\u65cb\u8f6c\u52a8\u4f5c\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe87.33%\uff0c\u529f\u8017\u4ec58.04mW\u3002", "motivation": "\u53d7\u4eba\u7c7b\u63a2\u7d22\u7b56\u7565\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u63d0\u5347\u673a\u5668\u4eba\u7eb9\u7406\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528NeuroTac\u4f20\u611f\u5668\u91c7\u96c6\u4e0d\u540c\u63a2\u7d22\u52a8\u4f5c\uff08\u5982\u6ed1\u52a8\u3001\u65cb\u8f6c\u3001\u8f7b\u6572\u53ca\u5176\u7ec4\u5408\uff09\u7684\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u6570\u636e\uff0c\u8bc4\u4f30\u5176\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u6ed1\u52a8+\u65cb\u8f6c\u52a8\u4f5c\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe87.33%\uff0c\u529f\u8017\u4ec58.04mW\u3002", "conclusion": "\u6ed1\u52a8+\u65cb\u8f6c\u52a8\u4f5c\u4e3a\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u5728\u7eb9\u7406\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u7b56\u7565\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2509.14967", "pdf": "https://arxiv.org/pdf/2509.14967", "abs": "https://arxiv.org/abs/2509.14967", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery", "categories": ["cs.RO", "cs.HC"], "comment": "To be presented at the 1st Workshop on Intelligent Cobodied\n  Assistance and Robotic Empowerment (iCARE). 2025 Conference on Robot Learning\n  (CoRL)", "summary": "Effective human-robot collaboration in surgery is affected by the inherent\nambiguity of verbal communication. This paper presents a framework for a\nrobotic surgical assistant that interprets and disambiguates verbal\ninstructions from a surgeon by grounding them in the visual context of the\noperating field. The system employs a two-level affordance-based reasoning\nprocess that first analyzes the surgical scene using a multimodal\nvision-language model and then reasons about the instruction using a knowledge\nbase of tool capabilities. To ensure patient safety, a dual-set conformal\nprediction method is used to provide a statistically rigorous confidence\nmeasure for robot decisions, allowing it to identify and flag ambiguous\ncommands. We evaluated our framework on a curated dataset of ambiguous surgical\nrequests from cholecystectomy videos, demonstrating a general disambiguation\nrate of 60% and presenting a method for safer human-robot interaction in the\noperating room.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u673a\u5668\u4eba\u52a9\u624b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u6d88\u89e3\u5916\u79d1\u533b\u751f\u53e3\u5934\u6307\u4ee4\u7684\u6b67\u4e49\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u4e2d\u53e3\u5934\u6307\u4ee4\u7684\u6a21\u7cca\u6027\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5206\u6790\u624b\u672f\u573a\u666f\uff0c\u7ed3\u5408\u5de5\u5177\u80fd\u529b\u77e5\u8bc6\u5e93\u8fdb\u884c\u4e24\u7ea7\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u96c6\u5f62\u9884\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728\u80c6\u56ca\u5207\u9664\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u6d88\u6b67\u6210\u529f\u7387\u8fbe60%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u624b\u672f\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.14978", "pdf": "https://arxiv.org/pdf/2509.14978", "abs": "https://arxiv.org/abs/2509.14978", "authors": ["Yifan Zhai", "Rudolf Reiter", "Davide Scaramuzza"], "title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "Quadrotor navigation in unknown environments is critical for practical\nmissions such as search-and-rescue. Solving it requires addressing three key\nchallenges: the non-convexity of free space due to obstacles,\nquadrotor-specific dynamics and objectives, and the need for exploration of\nunknown regions to find a path to the goal. Recently, the Model Predictive Path\nIntegral (MPPI) method has emerged as a promising solution that solves the\nfirst two challenges. By leveraging sampling-based optimization, it can\neffectively handle non-convex free space while directly optimizing over the\nfull quadrotor dynamics, enabling the inclusion of quadrotor-specific costs\nsuch as energy consumption. However, its performance in unknown environments is\nlimited, as it lacks the ability to explore unknown regions when blocked by\nlarge obstacles. To solve this issue, we introduce Perception-Aware MPPI\n(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory\nonline based on perception objectives. Specifically, when the goal is occluded,\nPA-MPPI's perception cost biases trajectories that can perceive unknown\nregions. This expands the mapped traversable space and increases the likelihood\nof finding alternative paths to the goal. Through hardware experiments, we\ndemonstrate that PA-MPPI, running at 50 Hz with our efficient perception and\nmapping module, performs up to 100% better than the baseline in our challenging\nsettings where the state-of-the-art MPPI fails. In addition, we demonstrate\nthat PA-MPPI can be used as a safe and robust action policy for navigation\nfoundation models, which often provide goal poses that are not directly\nreachable.", "AI": {"tldr": "PA-MPPI\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u611f\u77e5\u6210\u672c\uff0c\u89e3\u51b3\u4e86MPPI\u5728\u672a\u77e5\u73af\u5883\u4e2d\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5728\u969c\u788d\u7269\u906e\u6321\u65f6\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u89e3\u51b3\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u3001\u98de\u884c\u5668\u52a8\u529b\u5b66\u548c\u63a2\u7d22\u9700\u6c42\uff0c\u5c24\u5176\u662fMPPI\u5728\u5927\u578b\u969c\u788d\u7269\u906e\u6321\u65f6\u7684\u6027\u80fd\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u611f\u77e5\u7684MPPI\uff08PA-MPPI\uff09\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u6574\u8f68\u8ff9\u4ee5\u9002\u5e94\u611f\u77e5\u76ee\u6807\uff0c\u5f53\u76ee\u6807\u88ab\u906e\u6321\u65f6\uff0c\u611f\u77e5\u6210\u672c\u4f1a\u504f\u5411\u4e8e\u80fd\u63a2\u7d22\u672a\u77e5\u533a\u57df\u7684\u8f68\u8ff9\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0cPA-MPPI\u572850Hz\u4e0b\u8fd0\u884c\uff0c\u6027\u80fd\u6bd4\u57fa\u7ebf\u63d0\u5347100%\uff0c\u4e14\u5728\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u4e2d\u80fd\u5b89\u5168\u53ef\u9760\u5730\u6267\u884c\u4e0d\u53ef\u8fbe\u76ee\u6807\u7684\u4efb\u52a1\u3002", "conclusion": "PA-MPPI\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u5bfc\u822a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b89\u5168\u53ef\u9760\u7684\u52a8\u4f5c\u7b56\u7565\u3002"}}
{"id": "2509.14980", "pdf": "https://arxiv.org/pdf/2509.14980", "abs": "https://arxiv.org/abs/2509.14980", "authors": ["Ju Dong", "Lei Zhang", "Liding Zhang", "Yao Ling", "Yu Fu", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://sites.google.com/view/m4diffuser, 10 pages, 9\n  figures", "summary": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser.", "AI": {"tldr": "M4Diffuser\u6846\u67b6\u7ed3\u5408\u591a\u89c6\u56fe\u6269\u6563\u7b56\u7565\u4e0e\u65b0\u578bReM-QP\u63a7\u5236\u5668\uff0c\u663e\u8457\u63d0\u5347\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5355\u89c6\u56fe\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u89c6\u91ce\u6709\u9650\u3001\u63a2\u7d22\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7ecf\u5178\u63a7\u5236\u5668\u5728\u5947\u5f02\u70b9\u9644\u8fd1\u6548\u7387\u548c\u53ef\u64cd\u4f5c\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6M4Diffuser\uff0c\u6574\u5408\u591a\u89c6\u56fe\u6269\u6563\u7b56\u7565\uff08\u5229\u7528\u591a\u6444\u50cf\u5934\u89c6\u89d2\u751f\u6210\u4efb\u52a1\u76f8\u5173\u76ee\u6807\uff09\u4e0eReM-QP\u63a7\u5236\u5668\uff08\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u5947\u5f02\u70b9\u9c81\u68d2\u6027\uff09\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad87-56%\uff0c\u78b0\u649e\u51cf\u5c113-31%\uff0c\u5e76\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "M4Diffuser\u4e3a\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6574\u4f53\u534f\u8c03\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.14984", "pdf": "https://arxiv.org/pdf/2509.14984", "abs": "https://arxiv.org/abs/2509.14984", "authors": ["Jo\u00e3o Dami\u00e3o Almeida", "Egidio Falotico", "Cecilia Laschi", "Jos\u00e9 Santos-Victor"], "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u624b\u5185\u7269\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\u4e2d\uff0c\u624b\u6307\u548c\u624b\u638c\u4e0d\u540c\u533a\u57df\u7684\u89e6\u89c9\u53cd\u9988\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u4e86\u4f18\u5316\u4f20\u611f\u5668\u5e03\u7f6e\u7684\u914d\u7f6e\u3002", "motivation": "\u5728\u4eba\u7c7b\u542f\u53d1\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u5206\u5e03\u5f0f\u89e6\u89c9\u4f20\u611f\u5bf9\u4e8e\u7cbe\u786e\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u611f\u5668\u7684\u4f18\u5316\u914d\u7f6e\u95ee\u9898\u590d\u6742\uff0c\u5c24\u5176\u662f\u975e\u6307\u5c16\u533a\u57df\u7684\u89e6\u89c9\u4fe1\u606f\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u89e6\u89c9\u53cd\u9988\u6765\u81ea\u624b\u6307\u548c\u624b\u638c\u4e0d\u540c\u533a\u57df\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u5176\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u9c81\u68d2\u6027\u7684\u4f5c\u7528\uff0c\u5e76\u63a2\u8ba8\u7269\u4f53\u7279\u6027\u4e0e\u6700\u4f18\u4f20\u611f\u5668\u5e03\u7f6e\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u80fd\u591f\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u548c\u7cbe\u5ea6\u7684\u89e6\u89c9\u4f20\u611f\u914d\u7f6e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u548c\u4f7f\u7528\u5177\u6709\u589e\u5f3a\u64cd\u4f5c\u80fd\u529b\u7684\u4eba\u5f62\u7ec8\u7aef\u6267\u884c\u5668\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.14992", "pdf": "https://arxiv.org/pdf/2509.14992", "abs": "https://arxiv.org/abs/2509.14992", "authors": ["Yifan Zhai", "Lorenzo Terenzi", "Patrick Frey", "Diego Garcia Soto", "Pascal Egli", "Marco Hutter"], "title": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning", "categories": ["cs.RO"], "comment": null, "summary": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faExT\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6f14\u793a\u6536\u96c6\u3001\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u6316\u6398\u7b56\u7565\u7684\u5fae\u8c03\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u7684\u9ad8\u6548\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u6316\u6398\u673a\u90e8\u7f72\u4e2d\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u5982\u672a\u89c1\u8fc7\u7684\u5de5\u5730\u6761\u4ef6\u548c\u786c\u4ef6\u914d\u7f6e\u7684\u9c81\u68d2\u6027\u5904\u7406\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7136\u540e\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff08RLFT\uff09\u8fdb\u884c\u4efb\u52a1\u6216\u6761\u4ef6\u9002\u914d\u3002", "result": "ExT\u7b56\u7565\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u8fbe\u5230\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e13\u95e8\u5355\u4efb\u52a1\u63a7\u5236\u5668\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u6761\u4ef6\u3002", "conclusion": "ExT\u4e3a\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u81ea\u4e3b\u6316\u6398\u63d0\u4f9b\u4e86\u6f5c\u5728\u57fa\u7840\u3002"}}
{"id": "2509.14999", "pdf": "https://arxiv.org/pdf/2509.14999", "abs": "https://arxiv.org/abs/2509.14999", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Linwei Zheng", "Xiaocong Li", "Ming Liu", "Jun Ma"], "title": "Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments", "categories": ["cs.RO"], "comment": null, "summary": "Reliable, drift-free global localization presents significant challenges yet\nremains crucial for autonomous navigation in large-scale dynamic environments.\nIn this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel\nOdometry fusion framework, which is specifically designed to provide\nhigh-precision state estimation and robust localization in large-scale dynamic\nenvironments. Our framework leverages an efficient semantic-voxel map\nrepresentation and employs an improved scan matching algorithm, which utilizes\nglobal semantic information to significantly reduce long-term trajectory drift.\nFurthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using\na tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter\n(iESKF). This ensures reliable localization without experiencing abnormal\ndrift. Moreover, to tackle the challenges posed by terrain variations and\ndynamic movements, we introduce a 3D adaptive scaling strategy that allows for\nflexible adjustments to wheel odometry measurement weights, thereby enhancing\nlocalization precision. This study presents extensive real-world experiments\nconducted in a one-million-square-meter automated port, encompassing 3,575\nhours of operational data from 35 Intelligent Guided Vehicles (IGVs). The\nresults consistently demonstrate that our system outperforms state-of-the-art\nLiDAR-based localization methods in large-scale dynamic environments,\nhighlighting the framework's reliability and practical value.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u7ed3\u5408\u8bed\u4e49-LiDAR-IMU-\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7684\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u548c\u9c81\u68d2\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u9760\u3001\u65e0\u6f02\u79fb\u7684\u5168\u5c40\u5b9a\u4f4d\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u6548\u8bed\u4e49\u4f53\u7d20\u5730\u56fe\u548c\u6539\u8fdb\u7684\u626b\u63cf\u5339\u914d\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u4f20\u611f\u5668\u878d\u5408\u7684iESKF\u6ee4\u6ce2\u5668\uff0c\u5e76\u5f15\u51653D\u81ea\u9002\u5e94\u7f29\u653e\u7b56\u7565\u3002", "result": "\u5728\u767e\u4e07\u5e73\u65b9\u7c73\u81ea\u52a8\u5316\u6e2f\u53e3\u7684\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9ad8\u53ef\u9760\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.15052", "pdf": "https://arxiv.org/pdf/2509.15052", "abs": "https://arxiv.org/abs/2509.15052", "authors": ["Walker Gosrich", "Saurav Agarwal", "Kashish Garg", "Siddharth Mayya", "Matthew Malencia", "Mark Yim", "Vijay Kumar"], "title": "Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships", "categories": ["cs.RO"], "comment": "20 pages, 19 figures, Accepted to IEEE Transactions on Robotics\n  (TR-O) August 2025", "summary": "We propose a new formulation for the multi-robot task allocation problem that\nincorporates (a) complex precedence relationships between tasks, (b) efficient\nintra-task coordination, and (c) cooperation through the formation of robot\ncoalitions. A task graph specifies the tasks and their relationships, and a set\nof reward functions models the effects of coalition size and preceding task\nperformance. Maximizing task rewards is NP-hard; hence, we propose network\nflow-based algorithms to approximate solutions efficiently. A novel online\nalgorithm performs iterative re-allocation, providing robustness to task\nfailures and model inaccuracies to achieve higher performance than offline\napproaches. We comprehensively evaluate the algorithms in a testbed with random\nmissions and reward functions and compare them to a mixed-integer solver and a\ngreedy heuristic. Additionally, we validate the overall approach in an advanced\nsimulator, modeling reward functions based on realistic physical phenomena and\nexecuting the tasks with realistic robot dynamics. Results establish efficacy\nin modeling complex missions and efficiency in generating high-fidelity task\nplans while leveraging task relationships.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u95f4\u590d\u6742\u5173\u7cfb\u3001\u534f\u8c03\u4e0e\u673a\u5668\u4eba\u8054\u76df\uff0c\u7f51\u7edc\u6d41\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3\uff0c\u5728\u7ebf\u7b97\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\u590d\u6742\u4efb\u52a1\u5173\u7cfb\u3001\u534f\u8c03\u4e0e\u5408\u4f5c\u95ee\u9898\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u56fe\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\uff0c\u5f15\u5165\u5956\u52b1\u51fd\u6570\u8861\u91cf\u8054\u76df\u89c4\u6a21\u548c\u524d\u7f6e\u4efb\u52a1\u8868\u73b0\uff0c\u91c7\u7528\u7f51\u7edc\u6d41\u7b97\u6cd5\u8fd1\u4f3c\u6c42\u89e3\uff0c\u5e76\u63d0\u51fa\u5728\u7ebf\u91cd\u5206\u914d\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u4efb\u52a1\u8ba1\u5212\uff0c\u4f18\u4e8e\u6df7\u5408\u6574\u6570\u6c42\u89e3\u5668\u548c\u8d2a\u5fc3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u590d\u6742\u4efb\u52a1\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u7b97\u6cd5\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.15061", "pdf": "https://arxiv.org/pdf/2509.15061", "abs": "https://arxiv.org/abs/2509.15061", "authors": ["Xingyao Lin", "Xinghao Zhu", "Tianyi Lu", "Sicheng Xie", "Hui Zhang", "Xipeng Qiu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue", "categories": ["cs.RO"], "comment": "9 pages, 4 figures", "summary": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Ask-to-Clarify\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u89e3\u51b3\u6a21\u7cca\u6307\u4ee4\u95ee\u9898\uff0c\u5e76\u751f\u6210\u7aef\u5230\u7aef\u7684\u4f4e\u7ea7\u522b\u64cd\u4f5c\uff0c\u6700\u7ec8\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709VLA\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eVLA\u7684\u5177\u8eab\u4ee3\u7406\u591a\u4e3a\u5355\u5411\u6a21\u5f0f\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e2d\u6a21\u7cca\u7684\u6307\u4ee4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u534f\u4f5c\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Ask-to-Clarify\u6846\u67b6\u7531\u534f\u4f5cVLM\u548c\u6269\u6563\u52a8\u4f5c\u6a21\u5757\u7ec4\u6210\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u77e5\u8bc6\u9694\u79bb\u7b56\u7565\u8bad\u7ec3\uff0c\u5305\u62ec\u5904\u7406\u6a21\u7cca\u5bf9\u8bdd\u548c\u751f\u6210\u52a8\u4f5c\u3002", "result": "\u8be5\u6846\u67b6\u57288\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709VLA\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Ask-to-Clarify\u6846\u67b6\u53ca\u8bad\u7ec3\u7b56\u7565\u4e3a\u5b9e\u73b0\u534f\u4f5c\u6027\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.15062", "pdf": "https://arxiv.org/pdf/2509.15062", "abs": "https://arxiv.org/abs/2509.15062", "authors": ["Tianxin Hu", "Weixiang Guo", "Ruimeng Liu", "Xinhang Xu", "Rui Qian", "Jinyu Chen", "Shenghai Yuan", "Lihua Xie"], "title": "Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power", "categories": ["cs.RO"], "comment": null, "summary": "Future planetary exploration rovers must operate for extended durations on\nhybrid power inputs that combine steady radioisotope thermoelectric generator\n(RTG) output with variable solar photovoltaic (PV) availability. While\nenergy-aware planning has been studied for aerial and underwater robots under\nbattery limits, few works for ground rovers explicitly model power flow or\nenforce instantaneous power constraints. Classical terrain-aware planners\nemphasize slope or traversability, and trajectory optimization methods\ntypically focus on geometric smoothness and dynamic feasibility, neglecting\nenergy feasibility. We present an energy-constrained trajectory planning\nframework that explicitly integrates physics-based models of translational,\nrotational, and resistive power with baseline subsystem loads, under hybrid\nRTG-solar input. By incorporating both cumulative energy budgets and\ninstantaneous power constraints into SE(2)-based polynomial trajectory\noptimization, the method ensures trajectories that are simultaneously smooth,\ndynamically feasible, and power-compliant. Simulation results on lunar-like\nterrain show that our planner generates trajectories with peak power within\n0.55 percent of the prescribed limit, while existing methods exceed limits by\nover 17 percent. This demonstrates a principled and practical approach to\nenergy-aware autonomy for long-duration planetary missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u91cf\u7ea6\u675f\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u884c\u661f\u63a2\u6d4b\u8f66\u5728\u6df7\u5408\u80fd\u6e90\u8f93\u5165\u4e0b\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u3002", "motivation": "\u672a\u6765\u884c\u661f\u63a2\u6d4b\u8f66\u9700\u8981\u5728\u6df7\u5408\u80fd\u6e90\uff08RTG\u548c\u592a\u9633\u80fd\uff09\u4e0b\u957f\u65f6\u95f4\u8fd0\u884c\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u672a\u5145\u5206\u7ed3\u5408\u529f\u7387\u6d41\u6216\u77ac\u65f6\u529f\u7387\u7ea6\u675f\u3002", "method": "\u901a\u8fc7\u5c06\u5e73\u79fb\u3001\u65cb\u8f6c\u548c\u963b\u529b\u529f\u7387\u7684\u7269\u7406\u6a21\u578b\u4e0e\u57fa\u7ebf\u5b50\u7cfb\u7edf\u8d1f\u8f7d\u7ed3\u5408\uff0c\u6846\u67b6\u5728SE(2)\u591a\u9879\u5f0f\u8f68\u8ff9\u4f18\u5316\u4e2d\u96c6\u6210\u4e86\u7d2f\u79ef\u80fd\u91cf\u9884\u7b97\u548c\u77ac\u65f6\u529f\u7387\u7ea6\u675f\u3002", "result": "\u5728\u6708\u8868\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u8f68\u8ff9\u5cf0\u503c\u529f\u7387\u4ec5\u8d85\u51fa\u9650\u52360.55%\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u8d85\u51fa17%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u884c\u661f\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u7b26\u5408\u80fd\u91cf\u7ea6\u675f\u53c8\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6848\u3002"}}
{"id": "2509.15153", "pdf": "https://arxiv.org/pdf/2509.15153", "abs": "https://arxiv.org/abs/2509.15153", "authors": ["Yating Lin", "Zixuan Huang", "Fan Yang", "Dmitry Berenson"], "title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Multivariate time-series anomaly detection, which is critical for identifying\nunexpected events, has been explored in the field of machine learning for\nseveral decades. However, directly applying these methods to data from forceful\ntool use tasks is challenging because streaming sensor data in the real world\ntends to be inherently noisy, exhibits non-stationary behavior, and varies\nacross different tasks and tools. To address these challenges, we propose a\nmethod, AnoF-Diff, based on the diffusion model to extract force-torque\nfeatures from time-series data and use force-torque features to detect\nanomalies. We compare our method with other state-of-the-art methods in terms\nof F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)\non four forceful tool-use tasks, demonstrating that our method has better\nperformance and is more robust to a noisy dataset. We also propose the method\nof parallel anomaly score evaluation based on one-step diffusion and\ndemonstrate how our method can be used for online anomaly detection in several\nforceful tool use experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5AnoF-Diff\uff0c\u7528\u4e8e\u4ece\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\u5e76\u68c0\u6d4b\u5f02\u5e38\u3002\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u548c\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u63d0\u53d6\u529b-\u626d\u77e9\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u5e76\u884c\u5f02\u5e38\u8bc4\u5206\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u5f3a\u529b\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\uff0cAnoF-Diff\u5728F1-score\u548cAUROC\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "AnoF-Diff\u5728\u566a\u58f0\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u66f4\u9c81\u68d2\uff0c\u4e14\u9002\u7528\u4e8e\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2509.15180", "pdf": "https://arxiv.org/pdf/2509.15180", "abs": "https://arxiv.org/abs/2509.15180", "authors": ["Yitian Gao", "Lucas Chen", "Priyanka Bhovad", "Sicheng Wang", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Parallel Simulation of Contact and Actuation for Soft Growing Robots", "categories": ["cs.RO"], "comment": "26 pages, 9 figures, 1 table. Under review", "summary": "Soft growing robots, commonly referred to as vine robots, have demonstrated\nremarkable ability to interact safely and robustly with unstructured and\ndynamic environments. It is therefore natural to exploit contact with the\nenvironment for planning and design optimization tasks. Previous research has\nfocused on planning under contact for passively deforming robots with\npre-formed bends. However, adding active steering to these soft growing robots\nis necessary for successful navigation in more complex environments. To this\nend, we develop a unified modeling framework that integrates vine robot growth,\nbending, actuation, and obstacle contact. We extend the beam moment model to\ninclude the effects of actuation on kinematics under growth and then use these\nmodels to develop a fast parallel simulation framework. We validate our model\nand simulator with real robot experiments. To showcase the capabilities of our\nframework, we apply our model in a design optimization task to find designs for\nvine robots navigating through cluttered environments, identifying designs that\nminimize the number of required actuators by exploiting environmental contacts.\nWe show the robustness of the designs to environmental and manufacturing\nuncertainties. Finally, we fabricate an optimized design and successfully\ndeploy it in an obstacle-rich environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u8f6f\u751f\u957f\u673a\u5668\u4eba\u7684\u751f\u957f\u3001\u5f2f\u66f2\u3001\u9a71\u52a8\u548c\u73af\u5883\u63a5\u89e6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bbe\u8ba1\u4f18\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u5e94\u7528\u3002", "motivation": "\u8f6f\u751f\u957f\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u9700\u8981\u4e3b\u52a8\u8f6c\u5411\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u73af\u5883\u3002", "method": "\u6269\u5c55\u4e86\u6881\u5f2f\u77e9\u6a21\u578b\u4ee5\u5305\u62ec\u9a71\u52a8\u5bf9\u751f\u957f\u8fc7\u7a0b\u4e2d\u8fd0\u52a8\u5b66\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u5feb\u901f\u5e76\u884c\u6a21\u62df\u6846\u67b6\u3002", "result": "\u6a21\u578b\u548c\u6a21\u62df\u5668\u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f18\u5316\u8bbe\u8ba1\u901a\u8fc7\u5229\u7528\u73af\u5883\u63a5\u89e6\u51cf\u5c11\u4e86\u9a71\u52a8\u5668\u7684\u6570\u91cf\u3002", "conclusion": "\u4f18\u5316\u8bbe\u8ba1\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u5236\u9020\u8bef\u5dee\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.14773", "pdf": "https://arxiv.org/pdf/2509.14773", "abs": "https://arxiv.org/abs/2509.14773", "authors": ["Yuan Gao", "Wei Dong"], "title": "A Real-Time Multi-Model Parametric Representation of Point Clouds", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In recent years, parametric representations of point clouds have been widely\napplied in tasks such as memory-efficient mapping and multi-robot\ncollaboration. Highly adaptive models, like spline surfaces or quadrics, are\ncomputationally expensive in detection or fitting. In contrast, real-time\nmethods, such as Gaussian mixture models or planes, have low degrees of\nfreedom, making high accuracy with few primitives difficult. To tackle this\nproblem, a multi-model parametric representation with real-time surface\ndetection and fitting is proposed. Specifically, the Gaussian mixture model is\nfirst employed to segment the point cloud into multiple clusters. Then, flat\nclusters are selected and merged into planes or curved surfaces. Planes can be\neasily fitted and delimited by a 2D voxel-based boundary description method.\nSurfaces with curvature are fitted by B-spline surfaces and the same boundary\ndescription method is employed. Through evaluations on multiple public\ndatasets, the proposed surface detection exhibits greater robustness than the\nstate-of-the-art approach, with 3.78 times improvement in efficiency.\nMeanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian\nmixture models, operating at 36.4 fps on a low-power onboard computer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u578b\u53c2\u6570\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9e\u65f6\u8868\u9762\u68c0\u6d4b\u4e0e\u62df\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53c2\u6570\u5316\u8868\u793a\u65b9\u6cd5\u4e2d\u9ad8\u9002\u5e94\u6027\u6a21\u578b\u8ba1\u7b97\u4ee3\u4ef7\u5927\uff0c\u800c\u5b9e\u65f6\u65b9\u6cd5\u81ea\u7531\u5ea6\u4f4e\u96be\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u5148\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5206\u5272\u70b9\u4e91\u4e3a\u591a\u7c07\uff0c\u518d\u7b5b\u9009\u5408\u5e76\u5e73\u5766\u7c07\u4e3a\u5e73\u9762\u6216\u66f2\u9762\uff0c\u5e73\u9762\u75282D\u4f53\u7d20\u8fb9\u754c\u63cf\u8ff0\uff0c\u66f2\u9762\u7528B\u6837\u6761\u62df\u5408\u5e76\u5e94\u7528\u76f8\u540c\u8fb9\u754c\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u7a33\u5065\uff0c\u6548\u7387\u63d0\u53473.78\u500d\uff0c\u7cbe\u5ea6\u6bd4\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u63d0\u9ad82\u500d\uff0c\u5728\u4f4e\u529f\u8017\u8ba1\u7b97\u5e73\u53f0\u4e0a\u5b9e\u73b036.4 fps\u3002", "conclusion": "\u8be5\u591a\u6a21\u578b\u53c2\u6570\u5316\u8868\u793a\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.14966", "pdf": "https://arxiv.org/pdf/2509.14966", "abs": "https://arxiv.org/abs/2509.14966", "authors": ["Xingwu Zhang", "Guanxuan Li", "Zhuocheng Zhang", "Zijun Long"], "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.", "AI": {"tldr": "RoboEye\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u54082D\u5916\u89c2\u7279\u5f81\u548c3D\u63a8\u7406\u4ee5\u63d0\u5347\u7535\u5546\u4ed3\u5e93\u4e2d\u7269\u54c1\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u56e0\u7c7b\u522b\u590d\u6742\u3001\u906e\u6321\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u7535\u5546\u4ed3\u5e93\u4e2d\u7269\u54c1\u79cd\u7c7b\u5feb\u901f\u589e\u957f\uff0c\u5bfc\u81f4\u81ea\u52a8\u5316\u5305\u88c5\u4e2d\u7269\u54c1\u8bc6\u522b\u7684\u6311\u6218\u589e\u52a0\uff0c\u5305\u62ec\u7c7b\u5185\u53d8\u5f02\u6027\u5927\u3001\u906e\u6321\u548c\u89c6\u89d2\u53d8\u5316\u7b49\uff0c\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u3002", "method": "RoboEye\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u63d0\u53d62D\u7279\u5f81\u751f\u6210\u5019\u9009\u6392\u540d\uff1b2) \u52a8\u60013D\u7279\u5f81\u589e\u5f3a\uff0c\u901a\u8fc73D\u68c0\u7d22\u53d8\u6362\u5668\u5b9e\u73b0\u5173\u952e\u70b9\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRoboEye\u5c06Recall@1\u63d0\u5347\u4e867.1%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672fRoboLLM\uff0c\u4e14\u4ec5\u9700RGB\u56fe\u50cf\uff0c\u65e0\u9700\u663e\u5f0f3D\u8f93\u5165\u3002", "conclusion": "RoboEye\u901a\u8fc7\u7ed3\u54082D\u548c3D\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u7269\u54c1\u8bc6\u522b\u6027\u80fd\uff0c\u4e14\u90e8\u7f72\u6210\u672c\u4f4e\u3002"}}
{"id": "2509.15136", "pdf": "https://arxiv.org/pdf/2509.15136", "abs": "https://arxiv.org/abs/2509.15136", "authors": ["Lohitvel Gopikannan", "Shashi Ranjan Kumar", "Abhinav Sinha"], "title": "Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": null, "summary": "This paper presents a cooperative guidance strategy for the simultaneous\ninterception of a constant-velocity, non-maneuvering target, addressing the\nrealistic scenario where only a subset of interceptors are equipped with\nonboard seekers. To overcome the resulting heterogeneity in target\nobservability, a fixed-time distributed observer is employed, enabling\nseeker-less interceptors to estimate the target state using information from\nseeker-equipped agents and local neighbors over a directed communication\ntopology. Departing from conventional strategies that approximate time-to-go\nvia linearization or small-angle assumptions, the proposed approach leverages\ndeviated pursuit guidance where the time-to-go expression is exact for such a\ntarget. Moreover, a higher-order sliding mode consensus protocol is utilized to\nestablish time-to-go consensus within a finite time. The effectiveness of the\nproposed guidance and estimation architecture is demonstrated through\nsimulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u5236\u5bfc\u7b56\u7565\uff0c\u7528\u4e8e\u62e6\u622a\u6052\u5b9a\u901f\u5ea6\u3001\u975e\u673a\u52a8\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u62e6\u622a\u5668\u4e0d\u5177\u5907\u5bfc\u5f15\u5934\u7684\u5b9e\u9645\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u4ec5\u90e8\u5206\u62e6\u622a\u5668\u914d\u5907\u5bfc\u5f15\u5934\uff0c\u5bfc\u81f4\u76ee\u6807\u89c2\u6d4b\u80fd\u529b\u4e0d\u5747\uff0c\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u56fa\u5b9a\u65f6\u95f4\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\uff0c\u4f7f\u65e0\u5bfc\u5f15\u5934\u62e6\u622a\u5668\u901a\u8fc7\u6709\u5bfc\u5f15\u5934\u7684\u4ee3\u7406\u548c\u90bb\u57df\u4fe1\u606f\u4f30\u8ba1\u76ee\u6807\u72b6\u6001\uff1b\u4f7f\u7528\u504f\u79fb\u8ffd\u8e2a\u5236\u5bfc\u548c\u6709\u9650\u65f6\u95f4\u9ad8\u9636\u6ed1\u6a21\u4e00\u81f4\u6027\u534f\u8bae\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u8bae\u7684\u5236\u5bfc\u548c\u4f30\u8ba1\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b56\u7565\u6210\u529f\u514b\u670d\u4e86\u76ee\u6807\u89c2\u6d4b\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u7cbe\u786e\u62e6\u622a\u3002"}}
{"id": "2509.15155", "pdf": "https://arxiv.org/pdf/2509.15155", "abs": "https://arxiv.org/abs/2509.15155", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Ayzaan Wahid", "Jonathan Tompson", "Pannag Sanketi", "Igor Mordatch"], "title": "Self-Improving Embodied Foundation Models", "categories": ["cs.LG", "cs.RO"], "comment": "Appearing in the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Foundation models trained on web-scale data have revolutionized robotics, but\ntheir application to low-level control remains largely limited to behavioral\ncloning. Drawing inspiration from the success of the reinforcement learning\nstage in fine-tuning large language models, we propose a two-stage\npost-training approach for robotics. The first stage, Supervised Fine-Tuning\n(SFT), fine-tunes pretrained foundation models using both: a) behavioral\ncloning, and b) steps-to-go prediction objectives. In the second stage,\nSelf-Improvement, steps-to-go prediction enables the extraction of a\nwell-shaped reward function and a robust success detector, enabling a fleet of\nrobots to autonomously practice downstream tasks with minimal human\nsupervision. Through extensive experiments on real-world and simulated robot\nembodiments, our novel post-training recipe unveils significant results on\nEmbodied Foundation Models. First, we demonstrate that the combination of SFT\nand Self-Improvement is significantly more sample-efficient than scaling\nimitation data collection for supervised learning, and that it leads to\npolicies with significantly higher success rates. Further ablations highlight\nthat the combination of web-scale pretraining and Self-Improvement is the key\nto this sample-efficiency. Next, we demonstrate that our proposed combination\nuniquely unlocks a capability that current methods cannot achieve: autonomously\npracticing and acquiring novel skills that generalize far beyond the behaviors\nobserved in the imitation learning datasets used during training. These\nfindings highlight the transformative potential of combining pretrained\nfoundation models with online Self-Improvement to enable autonomous skill\nacquisition in robotics. Our project website can be found at\nhttps://self-improving-efms.github.io .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u6211\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u6709\u6240\u7a81\u7834\uff0c\u4f46\u5176\u5728\u4f4e\u7ea7\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u4ecd\u5c40\u9650\u4e8e\u884c\u4e3a\u514b\u9686\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u6210\u529f\u7ecf\u9a8c\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4e3a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548c\u6b65\u9aa4\u9884\u6d4b\u76ee\u6807\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u81ea\u6211\u6539\u8fdb\uff0c\u901a\u8fc7\u6b65\u9aa4\u9884\u6d4b\u63d0\u53d6\u5956\u52b1\u51fd\u6570\u548c\u6210\u529f\u68c0\u6d4b\u5668\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u81ea\u4e3b\u7ec3\u4e60\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u5355\u7eaf\u7684\u884c\u4e3a\u514b\u9686\uff0c\u5e76\u80fd\u5b9e\u73b0\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u96c6\u8303\u56f4\u7684\u6280\u80fd\u6cdb\u5316\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u548c\u5728\u7ebf\u81ea\u6211\u6539\u8fdb\uff0c\u6709\u671b\u5b9e\u73b0\u673a\u5668\u4eba\u6280\u80fd\u7684\u81ea\u4e3b\u83b7\u53d6\u3002"}}
{"id": "2509.15212", "pdf": "https://arxiv.org/pdf/2509.15212", "abs": "https://arxiv.org/abs/2509.15212", "authors": ["Yuming Jiang", "Siteng Huang", "Shengke Xue", "Yaxi Zhao", "Jun Cen", "Sicong Leng", "Kehan Li", "Jiayan Guo", "Kexiang Wang", "Mingxiu Chen", "Fan Wang", "Deli Zhao", "Xin Li"], "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "GitHub Project: https://github.com/alibaba-damo-academy/RynnVLA-001", "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.", "AI": {"tldr": "RynnVLA-001\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u548cActionVAE\u589e\u5f3a\u52a8\u4f5c\u8868\u793a\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u901a\u8fc7\u9884\u8bad\u7ec3\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u5e27\u9884\u6d4b\u7684\u6865\u63a5\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u9884\u8bad\u7ec3\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u8f68\u8ff9\u611f\u77e5\u5efa\u6a21\u3002\u5f15\u5165ActionVAE\u538b\u7f29\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u4e0b\u6e38\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u9884\u8bad\u7ec3\u7b56\u7565\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2509.15219", "pdf": "https://arxiv.org/pdf/2509.15219", "abs": "https://arxiv.org/abs/2509.15219", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "categories": ["cs.CV", "cs.LG", "cs.MA", "cs.MM", "cs.RO", "68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12", "F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7"], "comment": null, "summary": "Trajectory prediction is a critical task in computer vision and autonomous\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\nvirtual reality. Existing methods often rely on complete and noise-free\nobservational data, overlooking the challenges associated with out-of-sight\nobjects and the inherent noise in sensor data caused by limited camera\ncoverage, obstructions, and the absence of ground truth for denoised\ntrajectories. These limitations pose safety risks and hinder reliable\nprediction in real-world scenarios. In this extended work, we present\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\nBuilding on our previous research, we broaden the scope of Out-of-Sight\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\nits applicability to autonomous driving, robotics, surveillance, and virtual\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\ncalibration to establish a vision-positioning mapping, addressing the lack of\nvisual references, while effectively denoising noisy sensor data in an\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\ndatasets, our approach achieves state-of-the-art performance in both trajectory\ndenoising and prediction, significantly surpassing previous baselines.\nAdditionally, we introduce comparisons with traditional denoising methods, such\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\nproviding a comprehensive benchmark. This work represents the first initiative\nto integrate vision-positioning projection for denoising noisy sensor\ntrajectories of out-of-sight agents, paving the way for future advances. The\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u2014\u2014\u89c6\u7ebf\u5916\u8f68\u8ff9\uff08OST\uff09\u9884\u6d4b\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u53bb\u566a\u4f20\u611f\u5668\u6570\u636e\uff0c\u9884\u6d4b\u89c6\u7ebf\u5916\u7269\u4f53\u7684\u65e0\u566a\u58f0\u8f68\u8ff9\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u65e0\u566a\u58f0\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u89c6\u7ebf\u5916\u7269\u4f53\u548c\u4f20\u611f\u5668\u566a\u58f0\u7684\u6311\u6218\uff0c\u8fd9\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u589e\u5f3a\u7248\u7684\u89c6\u89c9\u5b9a\u4f4d\u53bb\u566a\u6a21\u5757\uff0c\u901a\u8fc7\u76f8\u673a\u6821\u51c6\u5efa\u7acb\u89c6\u89c9\u5b9a\u4f4d\u6620\u5c04\uff0c\u65e0\u76d1\u7763\u5730\u53bb\u9664\u566a\u58f0\u3002", "result": "\u5728Vi-Fi\u548cJRDB\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8f68\u8ff9\u53bb\u566a\u548c\u9884\u6d4b\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u89c6\u89c9\u5b9a\u4f4d\u6295\u5f71\u7528\u4e8e\u53bb\u566a\u89c6\u7ebf\u5916\u7269\u4f53\u7684\u4f20\u611f\u5668\u8f68\u8ff9\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
