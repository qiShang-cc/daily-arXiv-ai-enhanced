{"id": "2508.21079", "pdf": "https://arxiv.org/pdf/2508.21079", "abs": "https://arxiv.org/abs/2508.21079", "authors": ["Kaixuan Bao", "Wei Xu", "Xiaohu You", "Derrick Wing Kwan Ng"], "title": "A Framework of Arithmetic-Level Variable Precision Computing for In-Memory Architecture: Case Study in MIMO Signal Processing", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "to appear in TMC", "summary": "Computational complexity poses a significant challenge in wireless\ncommunication. Most existing attempts aim to reduce it through\nalgorithm-specific approaches. However, the precision of computing, which\ndirectly relates to both computing performance and computational complexity, is\na dimension that is fundamental but rarely explored in the literature. With the\nemerging architecture of in-memory computing, variable precision computing\n(VPC) is enabled, allowing each arithmetic operation to be processed with a\ndistinct and specifically optimized computing precision. In this paper, we\nestablish a unified framework of arithmetic-level variable precision computing\n(AL-VPC), which aims to determine the optimized computing precision for each\narithmetic operation. We first develop an arithmetic propagation error model\nexploiting stochastic analysis, and then formulate a mathematical optimization\nproblem to strike balance between computing performance and computational\ncomplexity. Two algorithms, namely, offline VPC and online VPC, are proposed to\nsolve the problem considering various practical concerns. Particularly, in a\ncase study on zero-forcing (ZF) precoding, we reveal the Pareto boundary\nbetween computing performance and complexity, which exhibits up to a 60%\nsum-rate enhancement or equivalently up to a 30% complexity reduction compared\nto the traditional fixed-length methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7b97\u672f\u7ea7\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97\uff08AL-VPC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u7cbe\u5ea6\u5e73\u8861\u6027\u80fd\u548c\u590d\u6742\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u4e8660%\u7684\u541e\u5410\u91cf\u6216\u964d\u4f4e\u4e8630%\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u8ba1\u7b97\u590d\u6742\u5ea6\u662f\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u7b97\u6cd5\u7279\u5b9a\u4f18\u5316\u3002\u8ba1\u7b97\u7cbe\u5ea6\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u4f46\u5bf9\u8ba1\u7b97\u6027\u80fd\u548c\u590d\u6742\u5ea6\u81f3\u5173\u91cd\u8981\u7684\u7ef4\u5ea6\u3002", "method": "\u5efa\u7acb\u4e86AL-VPC\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u968f\u673a\u5206\u6790\u7684\u8bef\u5dee\u4f20\u64ad\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u79bb\u7ebfVPC\u548c\u5728\u7ebfVPC\u7b97\u6cd5\u6765\u4f18\u5316\u8ba1\u7b97\u7cbe\u5ea6\u3002", "result": "\u5728\u96f6\u8feb\u9884\u7f16\u7801\u6848\u4f8b\u4e2d\uff0c\u4e0e\u4f20\u7edf\u56fa\u5b9a\u7cbe\u5ea6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u5347\u4e8660%\u7684\u541e\u5410\u91cf\u6216\u964d\u4f4e\u4e8630%\u7684\u590d\u6742\u5ea6\u3002", "conclusion": "AL-VPC\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u8ba1\u7b97\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.21351", "pdf": "https://arxiv.org/pdf/2508.21351", "abs": "https://arxiv.org/abs/2508.21351", "authors": ["Alireza Fadakar", "Yuchen Zhang", "Hui Chen", "Musa Furkan Keskin", "Henk Wymeersch", "Andreas F. Molisch"], "title": "Hybrid Codebook Design for Localization Using Electromagnetically Reconfigurable Fluid Antenna System", "categories": ["eess.SP"], "comment": null, "summary": "Electromagnetically reconfigurable fluid antenna systems (ER-FAS) introduce\nadditional degrees of freedom in the electromagnetic (EM) domain by dynamically\nsteering per-antenna radiation patterns, thereby enhancing power efficiency in\nwireless links. Unlike prior works on spatially reconfigurable FAS, which\nadjust element positions, ER-FAS provides direct control over each element's EM\ncharacteristics to realize on-demand beam-pattern shaping. While existing\nstudies have exploited ER-FAS to boost spectral efficiency, this paper explores\nits application for downlink localization. We consider a multiple-input\nsingle-output (MISO) system in which a multi-antenna ER-FAS at the base station\nserves a single-antenna user equipment (UE). We consider two reconfigurability\nparadigms: (i) a synthesis model where each antenna generates desired\nbeampatterns from a finite set of EM basis functions, and (ii) a finite-state\nselection model in which each antenna selects a pattern from a predefined set\nof patterns. For both paradigms, we formulate the joint baseband (BB) and EM\nprecoder design to minimize the UE position error bound. In the synthesis case\nwe derive low-dimensional closed-form expressions for both the BB and EM\nprecoders. For the finite-state model we obtain closed-form BB structures and\npropose a low-complexity block-coordinate-descent algorithm for EM pattern\nselection. Analytical bounds and extensive simulations show that the proposed\nhybrid designs for ER-FAS substantially improve UE positioning accuracy over\ntraditional non-reconfigurable arrays.", "AI": {"tldr": "ER-FAS\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u5929\u7ebf\u8f90\u5c04\u6a21\u5f0f\u63d0\u5347\u65e0\u7ebf\u94fe\u8def\u7684\u529f\u7387\u6548\u7387\uff0c\u672c\u6587\u7814\u7a76\u4e86\u5176\u5728\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e24\u79cd\u6a21\u5f0f\u5e76\u8bbe\u8ba1\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u9884\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5229\u7528ER-FAS\u63d0\u5347\u9891\u8c31\u6548\u7387\uff0c\u672c\u6587\u63a2\u7d22\u5176\u5728\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5929\u7ebf\u8f90\u5c04\u6a21\u5f0f\u6765\u4f18\u5316\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cdER-FAS\u6a21\u5f0f\uff1a(i) \u57fa\u4e8eEM\u57fa\u51fd\u6570\u7684\u5408\u6210\u6a21\u578b\uff0c(ii) \u6709\u9650\u72b6\u6001\u9009\u62e9\u6a21\u578b\u3002\u9488\u5bf9\u6bcf\u79cd\u6a21\u5f0f\u8bbe\u8ba1\u4e86\u8054\u5408\u57fa\u5e26\u548cEM\u9884\u7f16\u7801\u5668\uff0c\u5206\u522b\u91c7\u7528\u95ed\u5f0f\u8868\u8fbe\u5f0f\u548c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684ER-FAS\u6df7\u5408\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u8bbe\u5907\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u975e\u53ef\u91cd\u6784\u5929\u7ebf\u9635\u5217\u3002", "conclusion": "ER-FAS\u5728\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e24\u79cd\u6a21\u5f0f\u7684\u8bbe\u8ba1\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0\u5b9a\u4f4d\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2508.21373", "pdf": "https://arxiv.org/pdf/2508.21373", "abs": "https://arxiv.org/abs/2508.21373", "authors": ["Niladri Halder", "Chandra R. Murthy"], "title": "Channel Estimation and Data Detection in DS-Spread Channels: A Unified Framework, Novel Algorithms, and Waveform Comparison", "categories": ["eess.SP"], "comment": "The paper is submitted for publication in IEEE Transactions on Signal\n  Processing and is under review now. It has 15 pages with 9 figures. This work\n  was presented in parts at IEEE ICASSP 2023 [1] and IEEE SPAWC 2023 [2]", "summary": "We present a unified receiver processing framework for communication over\ndelay-scale (DS)-spread channels that arise in underwater acoustic (UWA)\ncommunications that addresses both channel estimation (CE) and data detection\nfor different modulation waveforms, namely OFDM, OTFS, OCDM, and ODSS, through\na common input--output relation. Using this framework, we conduct a fair and\ncomprehensive comparative study of these waveforms under DS-spread UWA channels\nand similar receiver complexities.\n  We also develop a novel iterative variational Bayesian (VB) off-grid CE\nalgorithm to estimate the delay and scale parameters of the channel paths, via\ntwo approaches: a first-order approximation scheme (FVB) and a second-order\napproximation scheme (SVB). We propose a low-complexity variational soft symbol\ndetection (VSSD) algorithm that outputs soft symbols and log-likelihood ratios\nfor the data bits, and a data-aided iterative CE and data detection (ICED)\nscheme that utilizes detected data symbols as \\emph{virtual} pilots to further\nimprove the CE and data detection accuracy.\n  Our numerical results reveal the efficacy of the proposed algorithms for CE\nand data detection. In terms of relative performance of different waveforms, in\nuncoded communications, (a) with a low-complexity subcarrier-by-subcarrier\nequalizer, ODSS offers the best performance, followed by OCDM and OTFS, while\nOFDM performs the worst, and (b) with the VSSD algorithm, OTFS, OCDM, and ODSS\nperform similarly, and they outperform OFDM. With coded communications,\ninterestingly, all waveforms offer nearly the same BER when the VSSD receiver\nis employed. Hence, we conclude that when the receiver complexity is\nconstrained, waveform choice matters, especially under harsh channel\nconditions, whereas with more sophisticated receiver algorithms, these\ndifferences disappear.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u63a5\u6536\u5904\u7406\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86OFDM\u3001OTFS\u3001OCDM\u548cODSS\u8c03\u5236\u6ce2\u5f62\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\u548c\u4f4e\u590d\u6742\u5ea6\u7b26\u53f7\u68c0\u6d4b\u7b97\u6cd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u63a5\u6536\u590d\u6742\u5ea6\u53d7\u9650\u65f6\uff0c\u6ce2\u5f62\u9009\u62e9\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u968f\u7740\u7b97\u6cd5\u4f18\u5316\uff0c\u5dee\u5f02\u6d88\u5931\u3002", "motivation": "\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u4e2d\u7684\u5ef6\u8fdf-\u6269\u5c55\uff08DS\uff09\u6269\u5c55\u4fe1\u9053\u9700\u8981\u7edf\u4e00\u7684\u5904\u7406\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u4e0d\u540c\u8c03\u5236\u6ce2\u5f62\u7684\u516c\u5e73\u6bd4\u8f83\uff0c\u5e76\u89e3\u51b3\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u53d8\u5206\u8d1d\u53f6\u65af\u7684\u79bb\u683c\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\uff08FVB\u548cSVB\uff09\uff0c\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u53d8\u5206\u8f6f\u7b26\u53f7\u68c0\u6d4b\uff08VSSD\uff09\u7b97\u6cd5\uff0c\u4ee5\u53ca\u6570\u636e\u8f85\u52a9\u7684\u8fed\u4ee3\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\uff08ICED\uff09\u65b9\u6848\u3002", "result": "\u5728\u672a\u7f16\u7801\u901a\u4fe1\u4e2d\uff0cODSS\u6027\u80fd\u6700\u4f73\uff0cOTFS\u548cOCDM\u6b21\u4e4b\uff0cOFDM\u6700\u5dee\uff1b\u800c\u5728\u4f7f\u7528VSSD\u7b97\u6cd5\u65f6\uff0cOTFS\u3001OCDM\u548cODSS\u8868\u73b0\u76f8\u4f3c\u4e14\u4f18\u4e8eOFDM\u3002\u7f16\u7801\u901a\u4fe1\u4e2d\uff0c\u6240\u6709\u6ce2\u5f62\u6027\u80fd\u63a5\u8fd1\u3002", "conclusion": "\u63a5\u6536\u590d\u6742\u5ea6\u53d7\u9650\u65f6\u6ce2\u5f62\u9009\u62e9\u91cd\u8981\uff0c\u4f46\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u53ef\u6d88\u9664\u5dee\u5f02\uff1b\u5728\u6076\u52a3\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u6ce2\u5f62\u9009\u62e9\u5c24\u4e3a\u5173\u952e\u3002"}}
{"id": "2508.21412", "pdf": "https://arxiv.org/pdf/2508.21412", "abs": "https://arxiv.org/abs/2508.21412", "authors": ["Hang Sheng", "Hui Feng", "Junhao Yu", "Feng Ji", "Bo Hu"], "title": "Sampling Theory of Jointly Bandlimited Time-vertex Graph Signals", "categories": ["eess.SP"], "comment": "This paper was published in Signal Processing, Elsevier", "summary": "Time-vertex graph signal (TVGS) models describe time-varying data with\nirregular structures. The bandlimitedness in the joint time-vertex Fourier\nspectral domain reflects smoothness in both temporal and graph topology. In\nthis paper, we study the critical sampling of three types of TVGS including\ncontinuous-time signals, infinite-length sequences, and finite-length sequences\nin the time domain for each vertex on the graph. For a jointly bandlimited\nTVGS, we prove a lower bound on sampling density or sampling ratio, which\ndepends on the measure of the spectral support in the joint time-vertex Fourier\nspectral domain. We also provide a lower bound on the sampling density or\nsampling ratio of each vertex on sampling sets for perfect recovery. To\ndemonstrate that critical sampling is achievable, we propose the sampling and\nreconstruction procedures for the different types of TVGS. Finally, we show how\nthe proposed sampling schemes can be applied to numerical as well as real\ndatasets.", "AI": {"tldr": "\u7814\u7a76\u65f6\u95f4-\u9876\u70b9\u56fe\u4fe1\u53f7\uff08TVGS\uff09\u7684\u4e34\u754c\u91c7\u6837\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u5728\u8054\u5408\u65f6\u95f4-\u9876\u70b9\u9891\u8c31\u57df\u4e2d\u7684\u91c7\u6837\u5bc6\u5ea6\u6216\u91c7\u6837\u6bd4\u4e0b\u9650\uff0c\u5e76\u63d0\u51fa\u91c7\u6837\u4e0e\u91cd\u5efa\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4-\u9876\u70b9\u56fe\u4fe1\u53f7\u6a21\u578b\u80fd\u591f\u63cf\u8ff0\u5177\u6709\u4e0d\u89c4\u5219\u7ed3\u6784\u7684\u65f6\u53d8\u6570\u636e\uff0c\u4f46\u5176\u4e34\u754c\u91c7\u6837\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7814\u7a76\u4e09\u7c7bTVGS\uff08\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\u3001\u65e0\u9650\u957f\u5e8f\u5217\u548c\u6709\u9650\u957f\u5e8f\u5217\uff09\u7684\u4e34\u754c\u91c7\u6837\uff0c\u63a8\u5bfc\u91c7\u6837\u5bc6\u5ea6\u4e0b\u9650\uff0c\u5e76\u63d0\u51fa\u91c7\u6837\u4e0e\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u8054\u5408\u5e26\u9650TVGS\u7684\u91c7\u6837\u5bc6\u5ea6\u4e0b\u9650\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e34\u754c\u91c7\u6837\u5728TVGS\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u4e14\u5176\u91c7\u6837\u5bc6\u5ea6\u4e0b\u9650\u4e0e\u9891\u8c31\u652f\u6301\u6d4b\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.21112", "pdf": "https://arxiv.org/pdf/2508.21112", "abs": "https://arxiv.org/abs/2508.21112", "authors": ["Delin Qu", "Haoming Song", "Qizhi Chen", "Zhaoqing Chen", "Xianqiang Gao", "Xinyi Ye", "Qi Lv", "Modi Shi", "Guanghui Ren", "Cheng Ruan", "Maoqing Yao", "Haoran Yang", "Jiacheng Bao", "Bin Zhao", "Dong Wang"], "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEO-Robotics\u7684\u6a21\u578b\uff0c\u5305\u62ecEO-1\u548cEO-Data1.5M\u6570\u636e\u96c6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u8bad\u7ec3\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u4e0e\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u4ea4\u4e92\u4e2d\u672a\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684EO-1\u6a21\u578b\uff0c\u7ed3\u5408EO-Data1.5M\u6570\u636e\u96c6\uff0c\u91c7\u7528\u81ea\u52a8\u56de\u5f52\u89e3\u7801\u548c\u6d41\u5339\u914d\u53bb\u566a\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "EO-1\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u957f\u65f6\u7a0b\u548c\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u9a8c\u8bc1\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5f00\u53d1\u5148\u8fdb\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u4ea4\u9519\u5b66\u4e60\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.21415", "pdf": "https://arxiv.org/pdf/2508.21415", "abs": "https://arxiv.org/abs/2508.21415", "authors": ["Hang Sheng", "Qinji Shu", "Hui Feng", "Bo Hu"], "title": "Subset Random Sampling and Reconstruction of Finite Time-Vertex Graph Signals", "categories": ["eess.SP"], "comment": "This paper was published in IEEE Transactions on Signal and\n  Information Processing over Networks (2025)", "summary": "Finite time-vertex graph signals (FTVGS) provide an efficient representation\nfor capturing spatio-temporal correlations across multiple data sources on\nirregular structures. Although sampling and reconstruction of FTVGS with known\nspectral support have been extensively studied, the case of unknown spectral\nsupport requires further investigation. Existing random sampling methods may\nextract samples from any vertex at any time, but such strategies are not\nfriendly in practice, where sampling is typically limited to a subset of\nvertices and moments. To address this requirement, we propose a subset random\nsampling scheme for FTVGS. Specifically, we first randomly select a subset of\nrows and columns to form a submatrix, followed by random sampling within that\nsubmatrix. In theory, we provide sufficient conditions for reconstructing the\noriginal FTVGS with high probability. Additionally, we introduce a\nreconstruction framework incorporating low-rank, sparsity, and smoothness\npriors (LSSP), and verify the feasibility of the reconstruction and the\neffectiveness of the framework through experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u672a\u77e5\u9891\u8c31\u652f\u6301\u7684\u6709\u9650\u65f6\u95f4-\u9876\u70b9\u56fe\u4fe1\u53f7\uff08FTVGS\uff09\u7684\u5b50\u96c6\u968f\u673a\u91c7\u6837\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u91cd\u5efa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u5728\u672a\u77e5\u9891\u8c31\u652f\u6301\u4e0b\u7684FTVGS\u91c7\u6837\u7814\u7a76\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5b9e\u9645\u91c7\u6837\u901a\u5e38\u53d7\u9650\u4e8e\u90e8\u5206\u9876\u70b9\u548c\u65f6\u95f4\u70b9\u3002", "method": "\u63d0\u51fa\u5b50\u96c6\u968f\u673a\u91c7\u6837\u65b9\u6848\uff1a\u5148\u968f\u673a\u9009\u62e9\u884c\u548c\u5217\u5f62\u6210\u5b50\u77e9\u9635\uff0c\u518d\u5728\u5b50\u77e9\u9635\u5185\u968f\u673a\u91c7\u6837\uff1b\u7ed3\u5408\u4f4e\u79e9\u3001\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u5148\u9a8c\uff08LSSP\uff09\u6784\u5efa\u91cd\u5efa\u6846\u67b6\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u9ad8\u6982\u7387\u91cd\u5efaFTVGS\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cd\u5efa\u7684\u53ef\u884c\u6027\u548c\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b50\u96c6\u968f\u673a\u91c7\u6837\u65b9\u6848\u548cLSSP\u6846\u67b6\u4e3a\u672a\u77e5\u9891\u8c31\u652f\u6301\u7684FTVGS\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u91c7\u6837\u548c\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21163", "pdf": "https://arxiv.org/pdf/2508.21163", "abs": "https://arxiv.org/abs/2508.21163", "authors": ["Tarek Bouazza", "Soulaimane Berkane", "Minh-Duc Hua", "Tarek Hamel"], "title": "Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 6 figures. To appear in IEEE CDC 2025", "summary": "This paper presents a novel cascaded observer architecture that combines\noptical flow and IMU measurements to perform continuous monocular\nvisual-inertial odometry (VIO). The proposed solution estimates body-frame\nvelocity and gravity direction simultaneously by fusing velocity direction\ninformation from optical flow measurements with gyro and accelerometer data.\nThis fusion is achieved using a globally exponentially stable Riccati observer,\nwhich operates under persistently exciting translational motion conditions. The\nestimated gravity direction in the body frame is then employed, along with an\noptional magnetometer measurement, to design a complementary observer on\n$\\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer\narchitecture is shown to be almost globally asymptotically stable. To extract\nthe velocity direction from sparse optical flow data, a gradient descent\nalgorithm is developed to solve a constrained minimization problem on the unit\nsphere. The effectiveness of the proposed algorithms is validated through\nsimulation results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6d41\u548cIMU\u6d4b\u91cf\u7684\u65b0\u578b\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u8fde\u7eed\u5355\u76ee\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u76eeVIO\u4e2d\u8eab\u4f53\u5750\u6807\u7cfb\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\u7684\u540c\u65f6\u4f30\u8ba1\u95ee\u9898\uff0c\u5229\u7528\u5149\u6d41\u6d4b\u91cf\u4e0eIMU\u6570\u636e\u7684\u878d\u5408\u63d0\u9ad8\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u7684Riccati\u89c2\u6d4b\u5668\u878d\u5408\u5149\u6d41\u901f\u5ea6\u65b9\u5411\u4e0eIMU\u6570\u636e\uff1b\u5728SO(3)\u4e0a\u8bbe\u8ba1\u4e92\u8865\u89c2\u6d4b\u5668\u4f30\u8ba1\u59ff\u6001\uff1b\u5f00\u53d1\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4ece\u7a00\u758f\u5149\u6d41\u6570\u636e\u4e2d\u63d0\u53d6\u901f\u5ea6\u65b9\u5411\u3002", "result": "\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u901f\u5ea6\u4e0e\u91cd\u529b\u65b9\u5411\uff0c\u4e3a\u5355\u76eeVIO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21563", "pdf": "https://arxiv.org/pdf/2508.21563", "abs": "https://arxiv.org/abs/2508.21563", "authors": ["Pierluigi Poggiolini", "Yanchao Jiang", "Yifeng Gao", "Fabrizio Forghieri"], "title": "Polynomial Closed Form Model for Ultra-Wideband Transmission Systems", "categories": ["eess.SP"], "comment": "The paper is identical to a manuscript submitted to JLT in August\n  2025", "summary": "Ultrafast and accurate physical layer models are essential for designing,\noptimizing and managing ultra-wideband optical transmission systems. We present\na closed-form GN/EGN model, named Polynomial Closed-Form Model (PCFM),\nimproving reliability, accuracy, and generality. The key to deriving PCFM is\nexpressing the spatial power profile of each channel along a span as a\npolynomial. Then, under reasonable approximations, the integral calculation can\nbe carried out analytically, for any chosen degree of the polynomial. We\npresent a full detailed derivation of the model. We then validate it vs. the\nnumerically integrated GN-model in a challenging multiband (C+L+S) scenario,\nincluding Raman amplification and inter-channel Raman scattering. We then show\nthat the approach works well also in the special case of the presence of\nmultiple lumped loss along the fiber. Overall, the approach shows very good\naccuracy and broad applicability. A software implementing the model, fully\nreconfigurable to any type of system layout, is available for download under\nthe Creative Commons 4.0 License.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCFM\u7684\u95ed\u5f0fGN/EGN\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u8868\u8fbe\u7a7a\u95f4\u529f\u7387\u5206\u5e03\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u8bbe\u8ba1\u548c\u4f18\u5316\u8d85\u5bbd\u5e26\u5149\u4f20\u8f93\u7cfb\u7edf\u9700\u8981\u5feb\u901f\u4e14\u51c6\u786e\u7684\u7269\u7406\u5c42\u6a21\u578b\u3002", "method": "\u91c7\u7528\u591a\u9879\u5f0f\u8868\u793a\u901a\u9053\u7684\u7a7a\u95f4\u529f\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u89e3\u6790\u79ef\u5206\u8ba1\u7b97\u63a8\u5bfc\u95ed\u5f0f\u6a21\u578b\u3002", "result": "\u5728\u5305\u542b\u62c9\u66fc\u653e\u5927\u548c\u901a\u9053\u95f4\u62c9\u66fc\u6563\u5c04\u7684\u591a\u9891\u6bb5\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9ad8\u51c6\u786e\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "PCFM\u6a21\u578b\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u5173\u8f6f\u4ef6\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.21205", "pdf": "https://arxiv.org/pdf/2508.21205", "abs": "https://arxiv.org/abs/2508.21205", "authors": ["Usman A. Khan", "Mouhacine Benosman", "Wenliang Liu", "Federico Pecora", "Joseph W. Durham"], "title": "Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)", "categories": ["cs.RO", "cs.LG"], "comment": "2025 IEEE Conference on Decision and Control", "summary": "In this paper, we propose a novel methodology for path planning and\nscheduling for multi-robot navigation that is based on optimal transport theory\nand model predictive control. We consider a setup where $N$ robots are tasked\nto navigate to $M$ targets in a common space with obstacles. Mapping robots to\ntargets first and then planning paths can result in overlapping paths that lead\nto deadlocks. We derive a strategy based on optimal transport that not only\nprovides minimum cost paths from robots to targets but also guarantees\nnon-overlapping trajectories. We achieve this by discretizing the space of\ninterest into $K$ cells and by imposing a ${K\\times K}$ cost structure that\ndescribes the cost of transitioning from one cell to another. Optimal transport\nthen provides \\textit{optimal and non-overlapping} cell transitions for the\nrobots to reach the targets that can be readily deployed without any scheduling\nconsiderations. The proposed solution requires $\\unicode{x1D4AA}(K^3\\log K)$\ncomputations in the worst-case and $\\unicode{x1D4AA}(K^2\\log K)$ for\nwell-behaved problems. To further accommodate potentially overlapping\ntrajectories (unavoidable in certain situations) as well as robot dynamics, we\nshow that a temporal structure can be integrated into optimal transport with\nthe help of \\textit{replans} and \\textit{model predictive control}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e0e\u8c03\u5ea6\u65b0\u65b9\u6cd5\uff0c\u786e\u4fdd\u65e0\u91cd\u53e0\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u5171\u540c\u7a7a\u95f4\u5bfc\u822a\u65f6\u8def\u5f84\u91cd\u53e0\u5bfc\u81f4\u7684\u6b7b\u9501\u95ee\u9898\u3002", "method": "\u5c06\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u5355\u5143\u683c\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u89c4\u5212\u6700\u5c0f\u6210\u672c\u4e14\u65e0\u91cd\u53e0\u7684\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5904\u7406\u52a8\u6001\u8f68\u8ff9\u3002", "result": "\u5728\u6700\u4f18\u60c5\u51b5\u4e0b\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(K\u00b2log K)\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u4e3aO(K\u00b3log K)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u907f\u514d\u4e86\u8def\u5f84\u91cd\u53e0\uff0c\u4e14\u80fd\u7075\u6d3b\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u548c\u673a\u5668\u4eba\u52a8\u6001\u3002"}}
{"id": "2508.21614", "pdf": "https://arxiv.org/pdf/2508.21614", "abs": "https://arxiv.org/abs/2508.21614", "authors": ["He Huang", "Zeping Sui", "Zilong Liu", "Wei Huang", "Md. Noor-A-Rahim", "Haishi Wang", "Zhiheng Hu"], "title": "Energy Detection over Composite $\u03ba-\u03bc$ Shadowed Fading Channels with Inverse Gaussian Distribution in Ultra mMTC Networks", "categories": ["eess.SP"], "comment": "5 pages, 5 figures, submitted to IEEE TVT", "summary": "This paper investigates the characteristics of energy detection (ED) over\ncomposite $\\kappa$-$\\mu$ shadowed fading channels in ultra machine-type\ncommunication (mMTC) networks. We have derived the closed-form expressions of\nthe probability density function (PDF) of signal-to-noise ratio (SNR) based on\nthe Inverse Gaussian (\\emph{IG}) distribution. By adopting novel integration\nand mathematical transformation techniques, we derive a truncation-based\nclosed-form expression for the average detection probability for the first\ntime. It can be observed from our simulations that the number of propagation\npaths has a more pronounced effect on average detection probability compared to\naverage SNR, which is in contrast to earlier studies that focus on\ndevice-to-device networks. It suggests that for 6G mMTC network design, we\nshould consider enhancing transmitter-receiver placement and antenna alignment\nstrategies, rather than relying solely on increasing the device-to-device\naverage SNR.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8d85\u673a\u5668\u7c7b\u578b\u901a\u4fe1(mMTC)\u7f51\u7edc\u4e2d\u590d\u5408\u03ba-\u03bc\u9634\u5f71\u8870\u843d\u4fe1\u9053\u4e0a\u80fd\u91cf\u68c0\u6d4b(ED)\u7684\u7279\u6027\uff0c\u5e76\u57fa\u4e8e\u9006\u9ad8\u65af\u5206\u5e03\u63a8\u5bfc\u4e86\u4fe1\u53f7\u566a\u58f0\u6bd4(SNR)\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570(PDF)\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u57286G mMTC\u7f51\u7edc\u4e2d\uff0c\u4f20\u64ad\u8def\u5f84\u6570\u91cf\u5bf9\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u7684\u5f71\u54cd\uff0c\u4e0e\u4f20\u7edf\u7814\u7a76\u4e2d\u5173\u6ce8\u7684\u8bbe\u5907\u95f4\u5e73\u5747SNR\u4e0d\u540c\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u79ef\u5206\u548c\u6570\u5b66\u53d8\u6362\u6280\u672f\uff0c\u9996\u6b21\u63a8\u5bfc\u4e86\u57fa\u4e8e\u622a\u65ad\u7684\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u64ad\u8def\u5f84\u6570\u91cf\u5bf9\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u7684\u5f71\u54cd\u6bd4\u5e73\u5747SNR\u66f4\u663e\u8457\uff0c\u8fd9\u4e0e\u4ee5\u5f80\u4e13\u6ce8\u4e8e\u8bbe\u5907\u95f4\u7f51\u7edc\u7684\u7814\u7a76\u4e0d\u540c\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u57286G mMTC\u7f51\u7edc\u8bbe\u8ba1\u4e2d\uff0c\u5e94\u4f18\u5148\u8003\u8651\u589e\u5f3a\u53d1\u5c04\u673a-\u63a5\u6536\u673a\u653e\u7f6e\u548c\u5929\u7ebf\u5bf9\u51c6\u7b56\u7565\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u63d0\u9ad8\u8bbe\u5907\u95f4\u7684\u5e73\u5747SNR\u3002"}}
{"id": "2508.21221", "pdf": "https://arxiv.org/pdf/2508.21221", "abs": "https://arxiv.org/abs/2508.21221", "authors": ["Fatima Mumtaza Tourk", "Bishoy Galoaa", "Sanat Shajan", "Aaron J. Young", "Michael Everett", "Max K. Shepherd"], "title": "Uncertainty-Aware Ankle Exoskeleton Control", "categories": ["cs.RO"], "comment": null, "summary": "Lower limb exoskeletons show promise to assist human movement, but their\nutility is limited by controllers designed for discrete, predefined actions in\ncontrolled environments, restricting their real-world applicability. We present\nan uncertainty-aware control framework that enables ankle exoskeletons to\noperate safely across diverse scenarios by automatically disengaging when\nencountering unfamiliar movements. Our approach uses an uncertainty estimator\nto classify movements as similar (in-distribution) or different\n(out-of-distribution) relative to actions in the training set. We evaluated\nthree architectures (model ensembles, autoencoders, and generative adversarial\nnetworks) on an offline dataset and tested the strongest performing\narchitecture (ensemble of gait phase estimators) online. The online test\ndemonstrated the ability of our uncertainty estimator to turn assistance on and\noff as the user transitioned between in-distribution and out-of-distribution\ntasks (F1: 89.2). This new framework provides a path for exoskeletons to safely\nand autonomously support human movement in unstructured, everyday environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u8e1d\u5173\u8282\u5916\u9aa8\u9abc\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5b89\u5168\u8fd0\u884c\uff0c\u901a\u8fc7\u81ea\u52a8\u8131\u79bb\u5904\u7406\u964c\u751f\u52a8\u4f5c\u3002", "motivation": "\u73b0\u6709\u5916\u9aa8\u9abc\u63a7\u5236\u5668\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u52a8\u4f5c\u548c\u53d7\u63a7\u73af\u5883\uff0c\u9650\u5236\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u5206\u7c7b\u52a8\u4f5c\u662f\u5426\u5c5e\u4e8e\u8bad\u7ec3\u96c6\u8303\u56f4\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u67b6\u6784\uff0c\u5e76\u5728\u7ebf\u4e0a\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u8005\u3002", "result": "\u5728\u7ebf\u6d4b\u8bd5\u663e\u793a\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u80fd\u591f\u5728\u7528\u6237\u52a8\u4f5c\u5207\u6362\u65f6\u51c6\u786e\u5f00\u542f\u6216\u5173\u95ed\u8f85\u52a9\u529f\u80fd\uff08F1: 89.2\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5916\u9aa8\u9abc\u5728\u975e\u7ed3\u6784\u5316\u65e5\u5e38\u73af\u5883\u4e2d\u5b89\u5168\u81ea\u4e3b\u652f\u6301\u4eba\u7c7b\u8fd0\u52a8\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.21640", "pdf": "https://arxiv.org/pdf/2508.21640", "abs": "https://arxiv.org/abs/2508.21640", "authors": ["Amirhossein Azarbahram", "Onel L. A. L\u00f3pez", "Petar Popovski", "Matti Latva-aho"], "title": "On the Deployment of Multiple Radio Stripes for Large-Scale Near-Field RF Wireless Power Transfer", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates the deployment of radio stripe systems for indoor\nradio-frequency (RF) wireless power transfer (WPT) in line-of-sight near-field\nscenarios. The focus is on environments where energy demand is concentrated in\nspecific areas, referred to as 'hotspots', spatial zones with higher user\ndensity or consistent energy requirements. We formulate a joint clustering and\nradio stripe deployment problem that aims to maximize the minimum received\npower across all hotspots. To address the complexity, we decouple the problem\ninto two stages: i) clustering for assigning radio stripes to hotspots based on\ntheir spatial positions and near-field propagation characteristics, and ii)\nantenna element placement optimization. In particular, we propose four radio\nstripe deployment algorithms. Two are based on general successive convex\napproximation (SCA) and signomial programming (SGP) methods. The other two are\nshape-constrained solutions where antenna elements are arranged along either\nstraight lines or regular polygons, enabling simpler deployment. Numerical\nresults show that the proposed clustering method converges effectively, with\nChebyshev initialization significantly outperforming random initialization. The\noptimized deployments consistently outperform baseline benchmarks across a wide\nrange of frequencies and radio stripe lengths, while the polygon-shaped\ndeployment achieves better performance compared to other approaches. Meanwhile,\nthe line-shaped deployment demonstrates an advantage under high boresight gain\nsettings, benefiting from increased spatial diversity and broader angular\ncoverage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u7ebf\u7535\u6761\u7eb9\u7cfb\u7edf\u5728\u5ba4\u5185\u8fd1\u573a\u65e0\u7ebf\u5145\u7535\u573a\u666f\u4e2d\u7684\u90e8\u7f72\uff0c\u9488\u5bf9\u9ad8\u80fd\u91cf\u9700\u6c42\u533a\u57df\uff08\u70ed\u70b9\uff09\u63d0\u51fa\u4e86\u8054\u5408\u805a\u7c7b\u548c\u90e8\u7f72\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6bd4\u4e86\u56db\u79cd\u90e8\u7f72\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5ba4\u5185\u65e0\u7ebf\u5145\u7535\u7cfb\u7edf\u4e2d\u9ad8\u6548\u80fd\u91cf\u4f20\u8f93\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u5bc6\u96c6\u6216\u80fd\u91cf\u9700\u6c42\u96c6\u4e2d\u7684\u70ed\u70b9\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u7a7a\u95f4\u4f4d\u7f6e\u548c\u8fd1\u573a\u4f20\u64ad\u7279\u6027\u7684\u70ed\u70b9\u805a\u7c7b\uff1b2) \u5929\u7ebf\u5143\u4ef6\u653e\u7f6e\u4f18\u5316\uff0c\u5305\u62ec\u56db\u79cd\u90e8\u7f72\u7b97\u6cd5\uff08SCA\u3001SGP\u3001\u76f4\u7ebf\u5f62\u548c\u6b63\u591a\u8fb9\u5f62\uff09\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u805a\u7c7b\u65b9\u6cd5\u6709\u6548\u6536\u655b\uff0c\u4f18\u5316\u90e8\u7f72\u5728\u591a\u79cd\u9891\u7387\u548c\u5929\u7ebf\u957f\u5ea6\u4e0b\u4f18\u4e8e\u57fa\u51c6\uff0c\u591a\u8fb9\u5f62\u90e8\u7f72\u6027\u80fd\u6700\u4f73\uff0c\u76f4\u7ebf\u5f62\u90e8\u7f72\u5728\u9ad8\u589e\u76ca\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f18\u5316\u90e8\u7f72\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u65e0\u7ebf\u5145\u7535\u6548\u7387\uff0c\u591a\u8fb9\u5f62\u548c\u76f4\u7ebf\u5f62\u90e8\u7f72\u5404\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2508.21260", "pdf": "https://arxiv.org/pdf/2508.21260", "abs": "https://arxiv.org/abs/2508.21260", "authors": ["Tara Mina", "Lindsey Marinello", "John Christian"], "title": "Remarks on stochastic cloning and delayed-state filtering", "categories": ["cs.RO", "eess.SP", "math.ST", "stat.TH"], "comment": null, "summary": "Many estimation problems in robotics and navigation involve measurements that\ndepend on prior states. A prominent example is odometry, which measures the\nrelative change between states over time. Accurately handling these\ndelayed-state measurements requires capturing their correlations with prior\nstate estimates, and a widely used approach is stochastic cloning (SC), which\naugments the state vector to account for these correlations.\n  This work revisits a long-established but often overlooked alternative--the\ndelayed-state Kalman filter--and demonstrates that a properly derived filter\nyields exactly the same state and covariance update as SC, without requiring\nstate augmentation. Moreover, the generalized Kalman filter formulation\nprovides computational advantages, while also reducing memory requirements for\nhigher-dimensional states.\n  Our findings clarify a common misconception that Kalman filter variants are\ninherently unable to handle correlated delayed-state measurements,\ndemonstrating that an alternative formulation achieves the same results more\nefficiently.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4e0e\u968f\u673a\u514b\u9686\u65b9\u6cd5\u5728\u72b6\u6001\u548c\u534f\u65b9\u5dee\u66f4\u65b0\u4e0a\u7684\u7b49\u6548\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u548c\u5bfc\u822a\u4e2d\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u76f8\u5173\u95ee\u9898\uff0c\u6bd4\u8f83\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e0e\u968f\u673a\u514b\u9686\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u516c\u5f0f\uff0c\u907f\u514d\u72b6\u6001\u5411\u91cf\u589e\u5e7f\uff0c\u76f4\u63a5\u5904\u7406\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u3002", "result": "\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u72b6\u6001\u548c\u534f\u65b9\u5dee\u66f4\u65b0\u4e0a\u4e0e\u968f\u673a\u514b\u9686\u65b9\u6cd5\u5b8c\u5168\u4e00\u81f4\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "\u5ef6\u8fdf\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u662f\u5904\u7406\u76f8\u5173\u5ef6\u8fdf\u72b6\u6001\u6d4b\u91cf\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u7ea0\u6b63\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53\u65e0\u6cd5\u5904\u7406\u6b64\u7c7b\u6d4b\u91cf\u7684\u8bef\u89e3\u3002"}}
{"id": "2508.21652", "pdf": "https://arxiv.org/pdf/2508.21652", "abs": "https://arxiv.org/abs/2508.21652", "authors": ["Haozhe Tian", "Qiyu Rao", "Nina Moutonnet", "Pietro Ferraro", "Danilo Mandic"], "title": "Machine Intelligence on the Edge: Interpretable Cardiac Pattern Localisation Using Reinforcement Learning", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Matched filters are widely used to localise signal patterns due to their high\nefficiency and interpretability. However, their effectiveness deteriorates for\nlow signal-to-noise ratio (SNR) signals, such as those recorded on edge\ndevices, where prominent noise patterns can closely resemble the target within\nthe limited length of the filter. One example is the ear-electrocardiogram\n(ear-ECG), where the cardiac signal is attenuated and heavily corrupted by\nartefacts. To address this, we propose the Sequential Matched Filter (SMF), a\nparadigm that replaces the conventional single matched filter with a sequence\nof filters designed by a Reinforcement Learning agent. By formulating filter\ndesign as a sequential decision-making process, SMF adaptively design\nsignal-specific filter sequences that remain fully interpretable by revealing\nkey patterns driving the decision-making. The proposed SMF framework has strong\npotential for reliable and interpretable clinical decision support, as\ndemonstrated by its state-of-the-art R-peak detection and physiological state\nclassification performance on two challenging real-world ECG datasets. The\nproposed formulation can also be extended to a broad range of applications that\nrequire accurate pattern localisation from noise-corrupted signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u987a\u5e8f\u5339\u914d\u6ee4\u6ce2\u5668\uff08SMF\uff09\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5339\u914d\u6ee4\u6ce2\u5668\u5728\u4f4e\u4fe1\u566a\u6bd4\u4fe1\u53f7\u4e2d\u6548\u679c\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5fc3\u7535\u56feR\u5cf0\u68c0\u6d4b\u548c\u751f\u7406\u72b6\u6001\u5206\u7c7b\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u5339\u914d\u6ee4\u6ce2\u5668\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08\u5982\u8033\u5fc3\u7535\u56fe\uff09\u4fe1\u53f7\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u566a\u58f0\u4e0e\u76ee\u6807\u4fe1\u53f7\u76f8\u4f3c\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u987a\u5e8f\u6ee4\u6ce2\u5668\uff08SMF\uff09\uff0c\u5c06\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u751f\u6210\u4fe1\u53f7\u7279\u5b9a\u7684\u53ef\u89e3\u91ca\u6ee4\u6ce2\u5668\u5e8f\u5217\u3002", "result": "SMF\u5728\u4e24\u4e2a\u771f\u5b9e\u5fc3\u7535\u56fe\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86R\u5cf0\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SMF\u4e0d\u4ec5\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u566a\u58f0\u4fe1\u53f7\u6a21\u5f0f\u5b9a\u4f4d\u5e94\u7528\u3002"}}
{"id": "2508.21271", "pdf": "https://arxiv.org/pdf/2508.21271", "abs": "https://arxiv.org/abs/2508.21271", "authors": ["Pablo Moraes", "Monica Rodriguez", "Kristofer S. Kappel", "Hiago Sodre", "Santiago Fernandez", "Igor Nunes", "Bruna Guterres", "Ricardo Grando"], "title": "Mini Autonomous Car Driving based on 3D Convolutional Neural Networks", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous driving applications have become increasingly relevant in the\nautomotive industry due to their potential to enhance vehicle safety,\nefficiency, and user experience, thereby meeting the growing demand for\nsophisticated driving assistance features. However, the development of reliable\nand trustworthy autonomous systems poses challenges such as high complexity,\nprolonged training periods, and intrinsic levels of uncertainty. Mini\nAutonomous Cars (MACs) are used as a practical testbed, enabling validation of\nautonomous control methodologies on small-scale setups. This simplified and\ncost-effective environment facilitates rapid evaluation and comparison of\nmachine learning models, which is particularly useful for algorithms requiring\nonline training. To address these challenges, this work presents a methodology\nbased on RGB-D information and three-dimensional convolutional neural networks\n(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the\nproposed approach against recurrent neural networks (RNNs), with architectures\ntrained and tested on two simulated tracks with distinct environmental\nfeatures. Performance was assessed using task completion success, lap-time\nmetrics, and driving consistency. Results highlight how architectural\nmodifications and track complexity influence the models' generalization\ncapability and vehicle control performance. The proposed 3D CNN demonstrated\npromising results when compared with RNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-D\u4fe1\u606f\u548c3D CNN\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5fae\u578b\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08MACs\uff09\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u9a7e\u9a76\uff0c\u5e76\u901a\u8fc7\u4e0eRNN\u7684\u6bd4\u8f83\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u63d0\u9ad8\u8f66\u8f86\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5f00\u53d1\u9762\u4e34\u9ad8\u590d\u6742\u6027\u3001\u957f\u8bad\u7ec3\u5468\u671f\u548c\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\u3002MACs\u4f5c\u4e3a\u7b80\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u53ef\u5feb\u901f\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u4f7f\u7528RGB-D\u4fe1\u606f\u548c3D CNN\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5MACs\u7684\u81ea\u4e3b\u9a7e\u9a76\u80fd\u529b\uff0c\u5e76\u4e0eRNN\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "3D CNN\u5728\u4efb\u52a1\u5b8c\u6210\u6210\u529f\u7387\u3001\u5708\u65f6\u548c\u9a7e\u9a76\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8eRNN\uff0c\u4e14\u6a21\u578b\u67b6\u6784\u548c\u8d5b\u9053\u590d\u6742\u6027\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "3D CNN\u5728MACs\u7684\u81ea\u4e3b\u9a7e\u9a76\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u73af\u5883\u65f6\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2508.21724", "pdf": "https://arxiv.org/pdf/2508.21724", "abs": "https://arxiv.org/abs/2508.21724", "authors": ["Dario Sanalitro", "Marco Finocchiaro", "Pasquale Memmolo", "Emanuela Cutuli", "Maide Bucolo"], "title": "A Single Subject Machine Learning Based Classification of Motor Imagery EEGs", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "Conference Paper", "summary": "Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that\ndetect and interpret brain activity patterns linked to the mental visualization\nof movement, and then translate these into instructions for controlling\nexternal robotic or domotic devices. Such devices have the potential to be\nuseful in a broad variety of applications. While implementing a system that\nwould help individuals restore some freedom levels, the interpretation of\n(Electroencephalography) EEG data remains a complex and unsolved problem. In\nthe literature, the classification of left and right imagined movements has\nbeen extensively studied. This study introduces a novel pipeline that makes use\nof machine learning techniques for classifying MI EEG data. The entire\nframework is capable of accurately categorizing left and imagined motions, as\nwell as rest phases, for a set of 52 subjects who performed a MI task. We\ntrained a within subject model on each individual subject. The methodology has\nbeen offline evaluated and compared to four studies that are currently the\nstate-of-the-art regarding the specified dataset. The results show that our\nproposed framework could be used with MI-BCI systems in light of its failsafe\nclassification performances, i.e. 99.5% in accuracy", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u7528\u4e8e\u51c6\u786e\u5206\u7c7b\u8fd0\u52a8\u60f3\u8c61\u8111\u7535\u56fe\u6570\u636e\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u8fbe99.5%\u3002", "motivation": "\u8fd0\u52a8\u60f3\u8c61\u8111\u673a\u63a5\u53e3\uff08MI-BCI\uff09\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u8111\u7535\u56fe\u6570\u636e\u7684\u5206\u7c7b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u6784\u5efa\u5206\u7c7b\u6846\u67b6\uff0c\u8bad\u7ec3\u4e2a\u4f53\u5316\u6a21\u578b\uff0c\u5e76\u4e0e\u73b0\u6709\u56db\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5206\u7c7b\u8fd0\u52a8\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe\u523099.5%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728MI-BCI\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.21272", "pdf": "https://arxiv.org/pdf/2508.21272", "abs": "https://arxiv.org/abs/2508.21272", "authors": ["Jaehong Oh", "Seungjun Jung", "Sawoong Kim"], "title": "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609", "categories": ["cs.RO", "stat.CO"], "comment": "13 figures, 17 pages", "summary": "This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5c06\u6cd5\u5f8b\u884c\u52a8\u63a9\u7801DQN\u4e0e\u5b89\u5168ZYZ\u91cd\u65b0\u6293\u53d6\u7b56\u7565\u5e94\u7528\u4e8e6\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3bSoma\u7acb\u65b9\u4f53\u7ec4\u88c5\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u7206\u70b8\u3001\u4e0d\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u53ca\u7cfb\u7edf\u7ec4\u88c5\u7b56\u7565\u5b66\u4e60\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6cd5\u5f8b\u884c\u52a8\u63a9\u7801DQN\u4e0e\u5206\u5c42\u67b6\u6784\uff0c\u5c06Q\u51fd\u6570\u4f30\u8ba1\u5206\u89e3\u4e3a\u65b9\u5411\u548c\u4f4d\u7f6e\u7ec4\u4ef6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86Level 1 100%\u7684\u6210\u529f\u7387\uff0cLevel 2 92.9%\uff0cLevel 3 39.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u534f\u4f5c\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u81ea\u4e3b\u7ec4\u88c5\u5b66\u4e60\u3002"}}
{"id": "2508.21225", "pdf": "https://arxiv.org/pdf/2508.21225", "abs": "https://arxiv.org/abs/2508.21225", "authors": ["Abhijit Sinha", "Hemant Kumar Kathania", "Sudarsana Reddy Kadiri", "Shrikanth Narayanan"], "title": "Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for Children's Speech?", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "comment": "Accepted", "summary": "Automatic Speech Recognition (ASR) systems often struggle to accurately\nprocess children's speech due to its distinct and highly variable acoustic and\nlinguistic characteristics. While recent advancements in self-supervised\nlearning (SSL) models have greatly enhanced the transcription of adult speech,\naccurately transcribing children's speech remains a significant challenge. This\nstudy investigates the effectiveness of layer-wise features extracted from\nstate-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT,\nData2Vec, and WavLM in improving the performance of ASR for children's speech\nin zero-shot scenarios. A detailed analysis of features extracted from these\nmodels was conducted, integrating them into a simplified DNN-based ASR system\nusing the Kaldi toolkit. The analysis identified the most effective layers for\nenhancing ASR performance on children's speech in a zero-shot scenario, where\nWSJCAM0 adult speech was used for training and PFSTAR children speech for\ntesting. Experimental results indicated that Layer 22 of the Wav2Vec2 model\nachieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64%\nrelative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of\n10.65%). Additionally, age group-wise analysis demonstrated consistent\nperformance improvements with increasing age, along with significant gains\nobserved even in younger age groups using the SSL features. Further experiments\non the CMU Kids dataset confirmed similar trends, highlighting the\ngeneralizability of the proposed approach.", "AI": {"tldr": "\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5c42\u7ea7\u7279\u5f81\u5bf9\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u96f6\u6837\u672c\u6027\u80fd\u63d0\u5347\u4f5c\u7528\uff0c\u53d1\u73b0Wav2Vec2\u6a21\u578b\u7684\u7b2c22\u5c42\u5728WSJCAM0\u6570\u636e\u96c6\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\u6700\u4f4e\uff085.15%\uff09\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u56e0\u5176\u72ec\u7279\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u6027\uff0c\u73b0\u6709ASR\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\uff0cSSL\u6a21\u578b\u7684\u6539\u8fdb\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u53d6\u4e86Wav2Vec2\u3001HuBERT\u7b49SSL\u6a21\u578b\u7684\u5c42\u7ea7\u7279\u5f81\uff0c\u901a\u8fc7\u7b80\u5316DNN-Kaldi\u7cfb\u7edf\u6d4b\u8bd5\u5176\u5728\u513f\u7ae5\u8bed\u97f3\u96f6\u6837\u672c\u573a\u666f\u7684\u6027\u80fd\u3002", "result": "Wav2Vec2\u7684\u7b2c22\u5c42\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u9519\u8bef\u7387\uff08\u76f8\u5bf9\u63d0\u534751.64%\uff09\uff0c\u4e14\u5728\u591a\u4e2a\u5e74\u9f84\u7ec4\u548c\u6570\u636e\u96c6\uff08CMU Kids\uff09\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "SSL\u6a21\u578b\u7684\u7279\u5b9a\u5c42\u7ea7\u7279\u5f81\u80fd\u6709\u6548\u63d0\u5347\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5e74\u9f84\u8f83\u5927\u7684\u7ec4\u522b\u4e2d\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2508.21309", "pdf": "https://arxiv.org/pdf/2508.21309", "abs": "https://arxiv.org/abs/2508.21309", "authors": ["Seyed Ali Rakhshan", "Mehdi Golestani", "He Kong"], "title": "Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking", "categories": ["cs.RO"], "comment": "This paper has been accepted to the 2025 IEEE/RSJ IROS", "summary": "This paper addresses the challenge of assigning heterogeneous sensors (i.e.,\nrobots with varying sensing capabilities) for multi-target tracking. We\nclassify robots into two categories: (1) sufficient sensing robots, equipped\nwith range and bearing sensors, capable of independently tracking targets, and\n(2) limited sensing robots, which are equipped with only range or bearing\nsensors and need to at least form a pair to collaboratively track a target. Our\nobjective is to optimize tracking quality by minimizing uncertainty in target\nstate estimation through efficient robot-to-target assignment. By leveraging\nmatroid theory, we propose a greedy assignment algorithm that dynamically\nallocates robots to targets to maximize tracking quality. The algorithm\nguarantees constant-factor approximation bounds of 1/3 for arbitrary tracking\nquality functions and 1/2 for submodular functions, while maintaining\npolynomial-time complexity. Extensive simulations demonstrate the algorithm's\neffectiveness in accurately estimating and tracking targets over extended\nperiods. Furthermore, numerical results confirm that the algorithm's\nperformance is close to that of the optimal assignment, highlighting its\nrobustness and practical applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d2a\u5a6a\u7b97\u6cd5\u7684\u52a8\u6001\u5206\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u5f02\u6784\u4f20\u611f\u5668\uff08\u5982\u4e0d\u540c\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\uff09\u7684\u5206\u914d\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u8ddf\u8e2a\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u5f02\u6784\u4f20\u611f\u5668\u7684\u5206\u914d\u95ee\u9898\uff0c\u4f18\u5316\u76ee\u6807\u72b6\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5229\u7528\u62df\u9635\u7406\u8bba\uff0c\u63d0\u51fa\u8d2a\u5a6a\u5206\u914d\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u673a\u5668\u4eba\u5230\u76ee\u6807\uff0c\u6700\u5927\u5316\u8ddf\u8e2a\u8d28\u91cf\u3002", "result": "\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b01/3\u52301/2\u7684\u8fd1\u4f3c\u6027\u80fd\u4fdd\u8bc1\uff0c\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u5206\u914d\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.21248", "pdf": "https://arxiv.org/pdf/2508.21248", "abs": "https://arxiv.org/abs/2508.21248", "authors": ["Subham Kutum", "Abhijit Sinha", "Hemant Kumar Kathania", "Sudarsana Reddy Kadiri", "Mahesh Chandra Govil"], "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.SD", "eess.SP"], "comment": "Accepted", "summary": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in\nadult speech, but children's speech presents unique challenges for KWS systems\ndue to its distinct acoustic and linguistic characteristics. This paper\nintroduces a zero-shot KWS approach that leverages state-of-the-art\nself-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.\nFeatures are extracted layer-wise from these SSL models and used to train a\nKaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for\ntraining, while the PFSTAR children's speech dataset was used for testing,\ndemonstrating the zero-shot capability of our method. Our approach achieved\nstate-of-the-art results across all keyword sets for children's speech.\nNotably, the Wav2Vec2 model, particularly layer 22, performed the best,\ndelivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of\nfalse alarm and probability of miss of 0.0164 and 0.0547 respectively, for a\nset of 30 keywords. Furthermore, age-specific performance evaluation confirmed\nthe system's effectiveness across different age groups of children. To assess\nthe system's robustness against noise, additional experiments were conducted\nusing the best-performing layer of the best-performing Wav2Vec2 model. The\nresults demonstrated a significant improvement over traditional MFCC-based\nbaseline, emphasizing the potential of SSL embeddings even in noisy conditions.\nTo further generalize the KWS framework, the experiments were repeated for an\nadditional CMU dataset. Overall the results highlight the significant\ncontribution of SSL features in enhancing Zero-Shot KWS performance for\nchildren's speech, effectively addressing the challenges associated with the\ndistinct characteristics of child speakers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u96f6\u6837\u672c\u5173\u952e\u8bcd\u8bc6\u522b\u65b9\u6cd5\uff0c\u9488\u5bf9\u513f\u7ae5\u8bed\u97f3\u7684\u72ec\u7279\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u7684\u5173\u952e\u8bcd\u8bc6\u522b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u6210\u4eba\u8bed\u97f3\u8bbe\u8ba1\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u513f\u7ae5\u8bed\u97f3\u7684\u96f6\u6837\u672c\u5173\u952e\u8bcd\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u5229\u7528Wav2Vec2\u3001HuBERT\u548cData2Vec\u7b49\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u9010\u5c42\u63d0\u53d6\u7279\u5f81\uff0c\u7ed3\u5408Kaldi\u6846\u67b6\u7684DNN\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u6210\u4eba\u6570\u636e\u96c6\uff08WSJCAM0\uff09\u4e0a\u8bad\u7ec3\u540e\u76f4\u63a5\u5728\u513f\u7ae5\u6570\u636e\u96c6\uff08PFSTAR\uff09\u4e0a\u6d4b\u8bd5\u3002", "result": "Wav2Vec2\u6a21\u578b\uff08\u5c24\u5176\u662f\u7b2c22\u5c42\uff09\u8868\u73b0\u6700\u4f73\uff0cATWV\u548cMTWV\u5206\u6570\u5206\u522b\u4e3a0.691\u548c0.7003\uff0c\u8bef\u62a5\u7387\u548c\u6f0f\u68c0\u7387\u8f83\u4f4e\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e5f\u4f18\u4e8e\u4f20\u7edfMFCC\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u513f\u7ae5\u8bed\u97f3\u7684\u96f6\u6837\u672c\u5173\u952e\u8bcd\u8bc6\u522b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u513f\u7ae5\u8bed\u97f3\u72ec\u7279\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.21322", "pdf": "https://arxiv.org/pdf/2508.21322", "abs": "https://arxiv.org/abs/2508.21322", "authors": ["Haojie Bai", "Yang Wang", "Cong Guo", "Xiongwei Zhao", "Hai Zhu"], "title": "Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty", "categories": ["cs.RO"], "comment": null, "summary": "Achieving both safety guarantees and real-time performance in cooperative\nvehicle coordination remains a fundamental challenge, particularly in dynamic\nand uncertain environments. This paper presents a novel coordination framework\nthat resolves this challenge through three key innovations: 1) direct control\nof vehicles' trajectory distributions during coordination, formulated as a\nrobust cooperative planning problem with adaptive enhanced safety constraints,\nensuring a specified level of safety regarding the uncertainty of the\ninteractive trajectory, 2) a fully parallel ADMM-based distributed trajectory\nnegotiation (ADMM-DTN) algorithm that efficiently solves the optimization\nproblem while allowing configurable negotiation rounds to balance solution\nquality and computational resources, and 3) an interactive attention mechanism\nthat selectively focuses on critical interactive participants to further\nenhance computational efficiency. Both simulation results and practical\nexperiments demonstrate that our framework achieves significant advantages in\nsafety (reducing collision rates by up to 40.79\\% in various scenarios) and\nreal-time performance compared to state-of-the-art methods, while maintaining\nstrong scalability with increasing vehicle numbers. The proposed interactive\nattention mechanism further reduces the computational demand by 14.1\\%. The\nframework's effectiveness is further validated through real-world experiments\nwith unexpected dynamic obstacles, demonstrating robust coordination in complex\nenvironments. The experiment demo could be found at\nhttps://youtu.be/4PZwBnCsb6Q.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8f66\u8f86\u534f\u540c\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u8f68\u8ff9\u5206\u5e03\u3001\u5e76\u884c\u4f18\u5316\u7b97\u6cd5\u548c\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\u662f\u8f66\u8f86\u534f\u540c\u534f\u8c03\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "1) \u57fa\u4e8e\u9c81\u68d2\u534f\u540c\u89c4\u5212\u7684\u8f68\u8ff9\u5206\u5e03\u63a7\u5236\uff1b2) \u5e76\u884cADMM\u5206\u5e03\u5f0f\u8f68\u8ff9\u534f\u5546\u7b97\u6cd5\uff1b3) \u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u663e\u793a\uff0c\u5b89\u5168\u6027\u63d0\u534740.79%\uff0c\u8ba1\u7b97\u9700\u6c42\u51cf\u5c1114.1%\uff0c\u4e14\u5177\u5907\u5f3a\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u534f\u540c\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.21364", "pdf": "https://arxiv.org/pdf/2508.21364", "abs": "https://arxiv.org/abs/2508.21364", "authors": ["Alberto Bertipaglia", "Dariu M. Gavrila", "Barys Shyrokau"], "title": "Multi-Modal Model Predictive Path Integral Control for Collision Avoidance", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted as an oral presentation at the 29th IAVSD. August 18-22,\n  2025. Shanghai, China", "summary": "This paper proposes a novel approach to motion planning and decision-making\nfor automated vehicles, using a multi-modal Model Predictive Path Integral\ncontrol algorithm. The method samples with Sobol sequences around the prior\ninput and incorporates analytical solutions for collision avoidance. By\nleveraging multiple modes, the multi-modal control algorithm explores diverse\ntrajectories, such as manoeuvring around obstacles or stopping safely before\nthem, mitigating the risk of sub-optimal solutions. A non-linear single-track\nvehicle model with a Fiala tyre serves as the prediction model, and tyre force\nconstraints within the friction circle are enforced to ensure vehicle stability\nduring evasive manoeuvres. The optimised steering angle and longitudinal\nacceleration are computed to generate a collision-free trajectory and to\ncontrol the vehicle. In a high-fidelity simulation environment, we demonstrate\nthat the proposed algorithm can successfully avoid obstacles, keeping the\nvehicle stable while driving a double lane change manoeuvre on high and\nlow-friction road surfaces and occlusion scenarios with moving obstacles,\noutperforming a standard Model Predictive Path Integral approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u548c\u51b3\u7b56\u7684\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u548c\u78b0\u649e\u907f\u514d\u5206\u6790\u6765\u4f18\u5316\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u8f66\u8f86\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u907f\u969c\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u9700\u8981\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u51b3\u7b56\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u969c\u788d\u7269\u5e76\u4fdd\u6301\u8f66\u8f86\u7a33\u5b9a\u6027\u3002\u4f20\u7edf\u7684\u63a7\u5236\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u63a2\u7d22\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u5bfc\u81f4\u6b21\u4f18\u89e3\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc7Sobol\u5e8f\u5217\u91c7\u6837\u548c\u78b0\u649e\u907f\u514d\u5206\u6790\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u5355\u8f68\u8f66\u8f86\u6a21\u578b\u548c\u8f6e\u80ce\u529b\u7ea6\u675f\uff0c\u4f18\u5316\u8f6c\u5411\u89d2\u548c\u7eb5\u5411\u52a0\u901f\u5ea6\u4ee5\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u53cc\u8f66\u9053\u53d8\u6362\u7b49\u9ad8\u96be\u5ea6\u907f\u969c\u4efb\u52a1\uff0c\u5728\u4e0d\u540c\u6469\u64e6\u8def\u9762\u548c\u79fb\u52a8\u969c\u788d\u7269\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u63a7\u5236\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.21372", "pdf": "https://arxiv.org/pdf/2508.21372", "abs": "https://arxiv.org/abs/2508.21372", "authors": ["Til Spreuer", "Josef Hoppe", "Michael T. Schaub"], "title": "Faster Inference of Cell Complexes from Flows via Matrix Factorization", "categories": ["cs.SI", "cs.LG", "eess.SP"], "comment": "5 pages, 5 figures, accepted at EUSIPCO 2025 in Palermo, evaluation\n  code available at https://github.com/til-spreuer/mfci-workflow", "summary": "We consider the following inference problem: Given a set of edge-flow signals\nobserved on a graph, lift the graph to a cell complex, such that the observed\nedge-flow signals can be represented as a sparse combination of gradient and\ncurl flows on the cell complex. Specifically, we aim to augment the observed\ngraph by a set of 2-cells (polygons encircled by closed, non-intersecting\npaths), such that the eigenvectors of the Hodge Laplacian of the associated\ncell complex provide a sparse, interpretable representation of the observed\nedge flows on the graph. As it has been shown that the general problem is\nNP-hard in prior work, we here develop a novel matrix-factorization-based\nheuristic to solve the problem. Using computational experiments, we demonstrate\nthat our new approach is significantly less computationally expensive than\nprior heuristics, while achieving only marginally worse performance in most\nsettings. In fact, we find that for specifically noisy settings, our new\napproach outperforms the previous state of the art in both solution quality and\ncomputational speed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u5206\u89e3\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u56fe\u4e2d\u63d0\u5347\u7a00\u758f\u68af\u5ea6\u548c\u65cb\u5ea6\u6d41\u7684\u8fb9\u6d41\u4fe1\u53f7\u3002", "motivation": "\u89e3\u51b3\u56fe\u4e2d\u8fb9\u6d41\u4fe1\u53f7\u7684\u7a00\u758f\u8868\u793a\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u5347\u56fe\u4e3a\u7ec6\u80de\u590d\u5f62\uff0c\u63d0\u4f9b\u66f4\u7a00\u758f\u548c\u53ef\u89e3\u91ca\u7684\u8fb9\u6d41\u8868\u793a\u3002\u7531\u4e8e\u95ee\u9898\u5df2\u88ab\u8bc1\u660e\u662fNP\u96be\uff0c\u9700\u8981\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u5206\u89e3\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a02-\u7ec6\u80de\uff08\u7531\u975e\u76f8\u4ea4\u95ed\u5408\u8def\u5f84\u5305\u56f4\u7684\u591a\u8fb9\u5f62\uff09\u6765\u6784\u5efa\u7ec6\u80de\u590d\u5f62\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u7279\u522b\u5728\u566a\u58f0\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fb9\u6d41\u4fe1\u53f7\u7684\u7a00\u758f\u8868\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7f51\u7edc\u5206\u6790\u3002"}}
{"id": "2508.21375", "pdf": "https://arxiv.org/pdf/2508.21375", "abs": "https://arxiv.org/abs/2508.21375", "authors": ["Anuj Pasricha", "Joewie Koh", "Jay Vakil", "Alessandro Roncone"], "title": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation", "categories": ["cs.RO"], "comment": "Accepted to 2025 Conference on Robot Learning [CoRL]", "summary": "Nominal payload ratings for articulated robots are typically derived from\nworst-case configurations, resulting in uniform payload constraints across the\nentire workspace. This conservative approach severely underutilizes the robot's\ninherent capabilities -- our analysis demonstrates that manipulators can safely\nhandle payloads well above nominal capacity across broad regions of their\nworkspace while staying within joint angle, velocity, acceleration, and torque\nlimits. To address this gap between assumed and actual capability, we propose a\nnovel trajectory generation approach using denoising diffusion models that\nexplicitly incorporates payload constraints into the planning process. Unlike\ntraditional sampling-based methods that rely on inefficient trial-and-error,\noptimization-based methods that are prohibitively slow, or kinodynamic planners\nthat struggle with problem dimensionality, our approach generates dynamically\nfeasible joint-space trajectories in constant time that can be directly\nexecuted on physical hardware without post-processing. Experimental validation\non a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the\nworkspace remains accessible even with payloads exceeding 3 times the nominal\ncapacity. This expanded operational envelope highlights the importance of a\nmore nuanced consideration of payload dynamics in motion planning algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u673a\u68b0\u81c2\u8d1f\u8f7d\u9650\u5236\u7684\u4fdd\u5b88\u4f30\u8ba1\uff0c\u663e\u8457\u6269\u5927\u4e86\u5de5\u4f5c\u7a7a\u95f4\u7684\u53ef\u8fbe\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u673a\u68b0\u81c2\u7684\u8d1f\u8f7d\u8bc4\u7ea7\u57fa\u4e8e\u6700\u574f\u60c5\u51b5\u914d\u7f6e\uff0c\u5bfc\u81f4\u5de5\u4f5c\u7a7a\u95f4\u5229\u7528\u7387\u4f4e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u8d1f\u8f7d\u52a8\u6001\u8003\u8651\uff0c\u63d0\u5347\u673a\u68b0\u81c2\u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u751f\u6210\u8f68\u8ff9\uff0c\u5c06\u8d1f\u8f7d\u7ea6\u675f\u76f4\u63a5\u7eb3\u5165\u89c4\u5212\u8fc7\u7a0b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4f4e\u6548\u6216\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u57283\u500d\u6807\u79f0\u8d1f\u8f7d\u4e0b\uff0c\u4ecd\u53ef\u8bbf\u95ee67.6%\u7684\u5de5\u4f5c\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u66f4\u7ec6\u81f4\u5730\u8003\u8651\u8d1f\u8f7d\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21378", "pdf": "https://arxiv.org/pdf/2508.21378", "abs": "https://arxiv.org/abs/2508.21378", "authors": ["Chenduo Ying", "Linkang Du", "Peng Cheng", "Yuanchao Shu"], "title": "RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RoboInspector\uff0c\u7528\u4e8e\u5206\u6790LLM\u751f\u6210\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\u4ee3\u7801\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u751f\u6210\u53ef\u9760\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\u4ee3\u7801\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u4efb\u52a1\u7684\u591a\u6837\u6027\u548c\u7528\u6237\u6307\u4ee4\u7684\u590d\u6742\u6027\u3002", "method": "\u8bbe\u8ba1RoboInspector\u7ba1\u9053\uff0c\u4ece\u4efb\u52a1\u590d\u6742\u6027\u548c\u6307\u4ee4\u7c92\u5ea6\u4e24\u65b9\u9762\u5206\u6790\u7b56\u7565\u4ee3\u7801\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u5931\u8d25\u53cd\u9988\u6307\u5bfc\u7684\u6539\u8fdb\u65b9\u6cd5\u6765\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u56db\u79cd\u5bfc\u81f4\u64cd\u7eb5\u5931\u8d25\u7684\u4e0d\u53ef\u9760\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u5c06\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u63d0\u9ad8\u4e8635%\u3002", "conclusion": "RoboInspector\u4e3a\u5b9e\u8df5\u5f00\u53d1\u63d0\u4f9b\u4e86\u51cf\u5c11\u4e0d\u53ef\u9760\u6027\u7684\u89c1\u89e3\uff0c\u6539\u8fdb\u65b9\u6cd5\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2508.21455", "pdf": "https://arxiv.org/pdf/2508.21455", "abs": "https://arxiv.org/abs/2508.21455", "authors": ["Hariharan Arunachalam", "Phani Teja Singamaneni", "Rachid Alami"], "title": "Assessing Human Cooperation for Enhancing Social Robot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Socially aware robot navigation is a planning paradigm where the robot\nnavigates in human environments and tries to adhere to social constraints while\ninteracting with the humans in the scene. These navigation strategies were\nfurther improved using human prediction models, where the robot takes the\npotential future trajectory of humans while computing its own. Though these\nstrategies significantly improve the robot's behavior, it faces difficulties\nfrom time to time when the human behaves in an unexpected manner. This happens\nas the robot fails to understand human intentions and cooperativeness, and the\nhuman does not have a clear idea of what the robot is planning to do. In this\npaper, we aim to address this gap through effective communication at an\nappropriate time based on a geometric analysis of the context and human\ncooperativeness in head-on crossing scenarios. We provide an assessment\nmethodology and propose some evaluation metrics that could distinguish a\ncooperative human from a non-cooperative one. Further, we also show how\ngeometric reasoning can be used to generate appropriate verbal responses or\nrobot actions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5bfc\u822a\u4e2d\u5982\u4f55\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u548c\u4eba\u7c7b\u5408\u4f5c\u6027\u8bc4\u4f30\u6765\u4f18\u5316\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u7a81\u53d1\u884c\u4e3a\u65f6\u3002", "motivation": "\u73b0\u6709\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565\u867d\u80fd\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\uff0c\u4f46\u5728\u4eba\u7c7b\u884c\u4e3a\u610f\u5916\u65f6\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u610f\u56fe\u548c\u5408\u4f5c\u6027\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u51e0\u4f55\u5206\u6790\u548c\u4eba\u7c7b\u5408\u4f5c\u6027\u8bc4\u4f30\u7684\u6c9f\u901a\u7b56\u7565\uff0c\u751f\u6210\u9002\u5f53\u7684\u8bed\u8a00\u6216\u884c\u4e3a\u54cd\u5e94\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u53ca\u6307\u6807\uff0c\u53ef\u533a\u5206\u5408\u4f5c\u4e0e\u975e\u5408\u4f5c\u4eba\u7c7b\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u51e0\u4f55\u63a8\u7406\u751f\u6210\u54cd\u5e94\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u6c9f\u901a\u548c\u51e0\u4f55\u5206\u6790\uff0c\u80fd\u663e\u8457\u6539\u5584\u673a\u5668\u4eba\u5728\u7a81\u53d1\u60c5\u51b5\u4e0b\u7684\u5bfc\u822a\u8868\u73b0\u3002"}}
{"id": "2508.21501", "pdf": "https://arxiv.org/pdf/2508.21501", "abs": "https://arxiv.org/abs/2508.21501", "authors": ["Pierrick Lorang", "Hong Lu", "Johannes Huemer", "Patrik Zips", "Matthias Scheutz"], "title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting", "categories": ["cs.RO"], "comment": "Accepted at CoRL 2025; to appear in PMLR", "summary": "Imitation learning enables intelligent systems to acquire complex behaviors\nwith minimal supervision. However, existing methods often focus on\nshort-horizon skills, require large datasets, and struggle to solve\nlong-horizon tasks or generalize across task variations and distribution\nshifts. We propose a novel neuro-symbolic framework that jointly learns\ncontinuous control policies and symbolic domain abstractions from a few skill\ndemonstrations. Our method abstracts high-level task structures into a graph,\ndiscovers symbolic rules via an Answer Set Programming solver, and trains\nlow-level controllers using diffusion policy imitation learning. A high-level\noracle filters task-relevant information to focus each controller on a minimal\nobservation and action space. Our graph-based neuro-symbolic framework enables\ncapturing complex state transitions, including non-spatial and temporal\nrelations, that data-driven learning or clustering techniques often fail to\ndiscover in limited demonstration datasets. We validate our approach in six\ndomains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers\nof Hanoi environments, and a distinct Automated Forklift domain with two\nenvironments. The results demonstrate high data efficiency with as few as five\nskill demonstrations, strong zero- and few-shot generalizations, and\ninterpretable decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8fde\u7eed\u63a7\u5236\u7b56\u7565\u548c\u7b26\u53f7\u57df\u62bd\u8c61\uff0c\u4ece\u5c11\u91cf\u6280\u80fd\u6f14\u793a\u4e2d\u9ad8\u6548\u5b66\u4e60\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u6570\u636e\u6548\u7387\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u9700\u6c42\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u6269\u6563\u7b56\u7565\u6a21\u4eff\u5b66\u4e60\u548cASP\u6c42\u89e3\u5668\uff0c\u62bd\u8c61\u4efb\u52a1\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u9ad8\u7ea7Oracle\u805a\u7126\u63a7\u5236\u5668\u7684\u6700\u5c0f\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u5728\u516d\u4e2a\u9886\u57df\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9ad8\u6570\u636e\u6548\u7387\uff08\u4ec5\u97005\u6b21\u6f14\u793a\uff09\u3001\u5f3a\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u53ca\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.21549", "pdf": "https://arxiv.org/pdf/2508.21549", "abs": "https://arxiv.org/abs/2508.21549", "authors": ["Liding Zhang", "Kuanqi Cai", "Yu Zhang", "Zhenshan Bing", "Chaoqun Wang", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler", "categories": ["cs.RO"], "comment": null, "summary": "Path planning in robotics often involves solving continuously valued,\nhigh-dimensional problems. Popular informed approaches include graph-based\nsearches, such as A*, and sampling-based methods, such as Informed RRT*, which\nutilize informed set and anytime strategies to expedite path optimization\nincrementally. Informed sampling-based planners define informed sets as subsets\nof the problem domain based on the current best solution cost. However, when no\nsolution is found, these planners re-sample and explore the entire\nconfiguration space, which is time-consuming and computationally expensive.\nThis article introduces Multi-Informed Trees (MIT*), a novel planner that\nconstructs estimated informed sets based on prior admissible solution costs\nbefore finding the initial solution, thereby accelerating the initial\nconvergence rate. Moreover, MIT* employs an adaptive sampler that dynamically\nadjusts the sampling strategy based on the exploration process. Furthermore,\nMIT* utilizes length-related adaptive sparse collision checks to guide lazy\nreverse search. These features enhance path cost efficiency and computation\ntimes while ensuring high success rates in confined scenarios. Through a series\nof simulations and real-world experiments, it is confirmed that MIT*\noutperforms existing single-query, sampling-based planners for problems in R^4\nto R^16 and has been successfully applied to real-world robot manipulation\ntasks. A video showcasing our experimental results is available at:\nhttps://youtu.be/30RsBIdexTU", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMulti-Informed Trees (MIT*)\u7684\u65b0\u8def\u5f84\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u4fe1\u606f\u96c6\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u521d\u59cb\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u5668\u5728\u672a\u627e\u5230\u89e3\u65f6\u9700\u91cd\u65b0\u63a2\u7d22\u6574\u4e2a\u914d\u7f6e\u7a7a\u95f4\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MIT*\u901a\u8fc7\u9884\u5b9a\u4e49\u4fe1\u606f\u96c6\u548c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u7b56\u7565\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u540c\u65f6\u91c7\u7528\u7a00\u758f\u78b0\u649e\u68c0\u67e5\u5f15\u5bfc\u53cd\u5411\u641c\u7d22\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0cMIT*\u5728R^4\u5230R^16\u95ee\u9898\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "MIT*\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u548c\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2508.21592", "pdf": "https://arxiv.org/pdf/2508.21592", "abs": "https://arxiv.org/abs/2508.21592", "authors": ["Tianchen Sun", "Bingheng Wang", "Longbin Tang", "Yichao Gao", "Lin Zhao"], "title": "Learning Agile Gate Traversal via Analytical Optimal Policy Gradient", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "Traversing narrow gates presents a significant challenge and has become a\nstandard benchmark for evaluating agile and precise quadrotor flight.\nTraditional modularized autonomous flight stacks require extensive design and\nparameter tuning, while end-to-end reinforcement learning (RL) methods often\nsuffer from low sample efficiency and limited interpretability. In this work,\nwe present a novel hybrid framework that adaptively fine-tunes model predictive\ncontrol (MPC) parameters online using outputs from a neural network (NN)\ntrained offline. The NN jointly predicts a reference pose and cost-function\nweights, conditioned on the coordinates of the gate corners and the current\ndrone state. To achieve efficient training, we derive analytical policy\ngradients not only for the MPC module but also for an optimization-based gate\ntraversal detection module. Furthermore, we introduce a new formulation of the\nattitude tracking error that admits a simplified representation, facilitating\neffective learning with bounded gradients. Hardware experiments demonstrate\nthat our method enables fast and accurate quadrotor traversal through narrow\ngates in confined environments. It achieves several orders of magnitude\nimprovement in sample efficiency compared to naive end-to-end RL approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u6574MPC\u53c2\u6570\u63d0\u5347\u65e0\u4eba\u673a\u901a\u8fc7\u72ed\u7a84\u95e8\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8c03\u53c2\u56f0\u96be\u548c\u7aef\u5230\u7aefRL\u65b9\u6cd5\u7684\u6548\u7387\u4f4e\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u5757\u5316\u98de\u884c\u5806\u6808\u8bbe\u8ba1\u590d\u6742\u4e14\u9700\u8981\u5927\u91cf\u8c03\u53c2\uff0c\u800c\u7aef\u5230\u7aefRL\u65b9\u6cd5\u6837\u672c\u6548\u7387\u4f4e\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6846\u67b6\uff0c\u79bb\u7ebf\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u53c2\u8003\u59ff\u6001\u548c\u6210\u672c\u51fd\u6570\u6743\u91cd\uff0c\u7ed3\u5408\u5728\u7ebf\u8c03\u6574MPC\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u95e8\u901a\u8fc7\u68c0\u6d4b\u6a21\u5757\u548c\u5206\u6790\u7b56\u7565\u68af\u5ea6\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u5feb\u901f\u7cbe\u51c6\u901a\u8fc7\u80fd\u529b\uff0c\u6837\u672c\u6548\u7387\u6bd4\u7aef\u5230\u7aefRL\u65b9\u6cd5\u9ad8\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u517c\u5177\u9ad8\u6548\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u654f\u6377\u98de\u884c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21635", "pdf": "https://arxiv.org/pdf/2508.21635", "abs": "https://arxiv.org/abs/2508.21635", "authors": ["Nicolas Soncini", "Javier Cremona", "Erica Vidal", "Maximiliano Garc\u00eda", "Gast\u00f3n Castro", "Taih\u00fa Pire"], "title": "The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "I.2.9"], "comment": "First published on The International Journal of Robotics Research:\n  https://journals.sagepub.com/doi/10.1177/02783649251368909", "summary": "We present a multi-modal dataset collected in a soybean crop field,\ncomprising over two hours of recorded data from sensors such as stereo infrared\ncamera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single\nPoint Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel\nodometry. This dataset captures key challenges inherent to robotics in\nagricultural environments, including variations in natural lighting, motion\nblur, rough terrain, and long, perceptually aliased sequences. By addressing\nthese complexities, the dataset aims to support the development and\nbenchmarking of advanced algorithms for localization, mapping, perception, and\nnavigation in agricultural robotics. The platform and data collection system is\ndesigned to meet the key requirements for evaluating multi-modal SLAM systems,\nincluding hardware synchronization of sensors, 6-DOF ground truth and loops on\nlong trajectories.\n  We run multimodal state-of-the art SLAM methods on the dataset, showcasing\nthe existing limitations in their application on agricultural settings. The\ndataset and utilities to work with it are released on\nhttps://cifasis.github.io/rosariov2/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6536\u96c6\u4e8e\u5927\u8c46\u7530\uff0c\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e8\u5728\u652f\u6301\u519c\u4e1a\u673a\u5668\u4eba\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u590d\u6742\u5730\u5f62\uff0c\u4ee5\u4fc3\u8fdb\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u540c\u6b65\u591a\u4f20\u611f\u5668\uff08\u5982\u7ea2\u5916\u76f8\u673a\u3001GNSS\u7b49\uff09\u91c7\u96c6\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u5e73\u53f0\u6ee1\u8db3\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u8bc4\u4f30\u9700\u6c42\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6570\u636e\u5df2\u516c\u5f00\u4f9b\u7814\u7a76\u4f7f\u7528\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u6280\u672f\u7684\u7b97\u6cd5\u5f00\u53d1\u548c\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2508.21677", "pdf": "https://arxiv.org/pdf/2508.21677", "abs": "https://arxiv.org/abs/2508.21677", "authors": ["Bernhard Wullt", "Johannes K\u00f6hler", "Per Mattsson", "Mikeal Norrl\u00f6f", "Thomas B. Sch\u00f6n"], "title": "Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators", "categories": ["cs.RO"], "comment": null, "summary": "Industrial manipulators are normally operated in cluttered environments,\nmaking safe motion planning important. Furthermore, the presence of\nmodel-uncertainties make safe motion planning more difficult. Therefore, in\npractice the speed is limited in order to reduce the effect of disturbances.\nThere is a need for control methods that can guarantee safe motions that can be\nexecuted fast. We address this need by suggesting a novel model predictive\ncontrol (MPC) solution for manipulators, where our two main components are a\nrobust tube MPC and a corridor planning algorithm to obtain collision-free\nmotion. Our solution results in a convex MPC, which we can solve fast, making\nour method practically useful. We demonstrate the efficacy of our method in a\nsimulated environment with a 6 DOF industrial robot operating in cluttered\nenvironments with uncertainties in model parameters. We outperform benchmark\nmethods, both in terms of being able to work under higher levels of model\nuncertainties, while also yielding faster motion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u68b0\u81c2\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5feb\u901f\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u5de5\u4e1a\u673a\u68b0\u81c2\u5e38\u5728\u590d\u6742\u73af\u5883\u4e2d\u5de5\u4f5c\uff0c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\u4e86\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u7684\u96be\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u9650\u5236\u901f\u5ea6\u6765\u51cf\u5c11\u5e72\u6270\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7a33\u5065\u7ba1MPC\u548c\u8d70\u5eca\u89c4\u5212\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u53ef\u5feb\u901f\u6c42\u89e3\u7684\u51f8\u4f18\u5316MPC\u65b9\u6848\u3002", "result": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u6a21\u62df6\u81ea\u7531\u5ea6\u5de5\u4e1a\u673a\u5668\u4eba\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u8fd0\u52a8\u901f\u5ea6\u4e0a\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u8fd0\u52a8\u89c4\u5212\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.21690", "pdf": "https://arxiv.org/pdf/2508.21690", "abs": "https://arxiv.org/abs/2508.21690", "authors": ["Olger Siebinga", "David Abbink"], "title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?", "categories": ["cs.RO"], "comment": null, "summary": "Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u884c\u4eba\u95f4\u7684\u9690\u6027\u6c9f\u901a\u5931\u8d25\u73b0\u8c61\uff08\"\u4eba\u884c\u9053\u821e\u8e48\"\uff09\uff0c\u5e76\u63d0\u51fa\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u5b66\u4e60\u5982\u4f55\u4e0e\u884c\u4eba\u4e92\u52a8\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u5b89\u5168\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u4e0e\u884c\u4eba\u4e92\u52a8\u7684\u9690\u6027\u6c9f\u901a\u673a\u5236\uff0c\u4e3a\u8bbe\u8ba1\u5b89\u5168\u7684\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u7ed3\u5408Communication-Enabled Interaction\uff08CEI\uff09\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3RL\u4ee3\u7406\u4e0e\u884c\u4eba\u6a21\u578b\u4e92\u52a8\u3002", "result": "\u57fa\u7840RL\u4ee3\u7406\u6210\u529f\u5b66\u4e60\u4e86\u4e92\u52a8\u65b9\u5f0f\uff0c\u800c\u98ce\u9669\u89c4\u907f\u578bRL\u4ee3\u7406\u80fd\u901a\u8fc7\u52a8\u4f5c\u6709\u6548\u4f20\u8fbe\u610f\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u884c\u4eba\u611f\u77e5\u7684\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.21080", "pdf": "https://arxiv.org/pdf/2508.21080", "abs": "https://arxiv.org/abs/2508.21080", "authors": ["Ali K. AlShami", "Ryan Rabinowitz", "Maged Shoman", "Jianwu Fang", "Lukas Picek", "Shao-Yuan Lo", "Steve Cruz", "Khang Nhut Lam", "Nachiket Kamod", "Lei-Lei Li", "Jugal Kalita", "Terrance E. Boult"], "title": "2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving", "categories": ["cs.CV", "cs.RO", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8"], "comment": "11 pages, 2 figures, Accepted to ICCV 2025 Workshop on Out-of-Label\n  Hazards in Autonomous Driving (2COOOL)", "summary": "As the computer vision community advances autonomous driving algorithms,\nintegrating vision-based insights with sensor data remains essential for\nimproving perception, decision making, planning, prediction, simulation, and\ncontrol. Yet we must ask: Why don't we have entirely safe self-driving cars\nyet? A key part of the answer lies in addressing novel scenarios, one of the\nmost critical barriers to real-world deployment. Our 2COOOL workshop provides a\ndedicated forum for researchers and industry experts to push the state of the\nart in novelty handling, including out-of-distribution hazard detection,\nvision-language models for hazard understanding, new benchmarking and\nmethodologies, and safe autonomous driving practices. The 2nd Workshop on the\nChallenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held\nat the International Conference on Computer Vision (ICCV) 2025 in Honolulu,\nHawaii, on October 19, 2025. We aim to inspire the development of new\nalgorithms and systems for hazard avoidance, drawing on ideas from anomaly\ndetection, open-set recognition, open-vocabulary modeling, domain adaptation,\nand related fields. Building on the success of its inaugural edition at the\nWinter Conference on Applications of Computer Vision (WACV) 2025, the workshop\nwill feature a mix of academic and industry participation.", "AI": {"tldr": "2COOOL\u7814\u8ba8\u4f1a\u4e13\u6ce8\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u65b0\u578b\u573a\u666f\u7684\u5904\u7406\uff0c\u6d89\u53ca\u5f02\u5e38\u68c0\u6d4b\u3001\u5f00\u653e\u8bcd\u6c47\u5efa\u6a21\u7b49\u9886\u57df\uff0c\u65e8\u5728\u63a8\u52a8\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5c1a\u672a\u5b8c\u5168\u5b89\u5168\u7684\u5173\u952e\u539f\u56e0\u5728\u4e8e\u5904\u7406\u65b0\u578b\u573a\u666f\u7684\u80fd\u529b\u4e0d\u8db3\uff0c2COOOL\u7814\u8ba8\u4f1a\u81f4\u529b\u4e8e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7814\u8ba8\u4f1a\u5f62\u5f0f\uff0c\u6574\u5408\u5b66\u672f\u4e0e\u5de5\u4e1a\u754c\u8d44\u6e90\uff0c\u63a2\u7d22\u5305\u62ec\u5f02\u5e38\u68c0\u6d4b\u3001\u5f00\u653e\u8bcd\u6c47\u5efa\u6a21\u7b49\u65b9\u6cd5\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "result": "\u6210\u529f\u4e3e\u529e\u4e86\u9996\u5c4a\u7814\u8ba8\u4f1a\uff08WACV 2025\uff09\uff0c\u5e76\u8ba1\u5212\u5728ICCV 2025\u4e0a\u8fdb\u4e00\u6b65\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u4e0e\u6280\u672f\u53d1\u5c55\u3002", "conclusion": "2COOOL\u7814\u8ba8\u4f1a\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u65b0\u578b\u573a\u666f\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5e73\u53f0\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2508.21102", "pdf": "https://arxiv.org/pdf/2508.21102", "abs": "https://arxiv.org/abs/2508.21102", "authors": ["Kei Katsumata", "Yui Iioka", "Naoki Hosomi", "Teruhisa Misu", "Kentaro Yamada", "Komei Sugiura"], "title": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted for presentation at CoRL2025", "summary": "We focus on the task of identifying the location of target regions from a\nnatural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and\nsegmentation, particularly for stuff-type target regions with ambiguous\nboundaries. Existing methods often underperform in handling stuff-type target\nregions, in addition to absent or multiple targets. To overcome these\nlimitations, we propose GENNAV, which predicts target existence and generates\nsegmentation masks for multiple stuff-type target regions. To evaluate GENNAV,\nwe constructed a novel benchmark called GRiN-Drive, which includes three\ndistinct types of samples: no-target, single-target, and multi-target. GENNAV\nachieved superior performance over baseline methods on standard evaluation\nmetrics. Furthermore, we conducted real-world experiments with four automobiles\noperated in five geographically distinct urban areas to validate its zero-shot\ntransfer performance. In these experiments, GENNAV outperformed baseline\nmethods and demonstrated its robustness across diverse real-world environments.\nThe project page is available at https://gennav.vercel.app/.", "AI": {"tldr": "GENNAV\u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u79fb\u52a8\u8bbe\u5907\u62cd\u6444\u7684\u524d\u7f6e\u6444\u50cf\u5934\u56fe\u50cf\u4e2d\u8bc6\u522b\u76ee\u6807\u533a\u57df\u4f4d\u7f6e\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u6a21\u7cca\u8fb9\u754c\u7684stuff-type\u76ee\u6807\u533a\u57df\u63d0\u51fa\u6539\u8fdb\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406stuff-type\u76ee\u6807\u533a\u57df\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5f53\u76ee\u6807\u4e0d\u5b58\u5728\u6216\u591a\u4e2a\u76ee\u6807\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GENNAV\u7ed3\u5408\u76ee\u6807\u5b58\u5728\u6027\u9884\u6d4b\u548c\u5206\u5272\u63a9\u7801\u751f\u6210\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u76ee\u6807\u6a21\u7cca\u8fb9\u754c\u60c5\u51b5\uff0c\u5e76\u5728GRiN-Drive\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "GENNAV\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u9a7e\u9a76\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GENNAV\u5728\u5904\u7406\u6a21\u7cca\u8fb9\u754c\u548c\u591a\u76ee\u6807\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2508.21278", "pdf": "https://arxiv.org/pdf/2508.21278", "abs": "https://arxiv.org/abs/2508.21278", "authors": ["Yibin Sun", "Nick Lim", "Guilherme Weigert Cassales", "Heitor Murilo Gomes", "Bernhard Pfahringer", "Albert Bifet", "Anany Dwivedi"], "title": "Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning", "categories": ["cs.LG", "cs.RO"], "comment": "16 pages, 5 figures, 1 table, PRICAI25", "summary": "Detecting domain shifts in myoelectric activations poses a significant\nchallenge due to the inherent non-stationarity of electromyography (EMG)\nsignals. This paper explores the detection of domain shifts using data stream\n(DS) learning techniques, focusing on the DB6 dataset from the Ninapro\ndatabase. We define domains as distinct time-series segments based on different\nsubjects and recording sessions, applying Kernel Principal Component Analysis\n(KPCA) with a cosine kernel to pre-process and highlight these shifts. By\nevaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and\nADWIN, we reveal the limitations of current techniques in achieving high\nperformance for real-time domain shift detection in EMG signals. Our results\nunderscore the potential of streaming-based approaches for maintaining stable\nEMG decoding models, while highlighting areas for further research to enhance\nrobustness and accuracy in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u6570\u636e\u6d41\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u808c\u7535\u4fe1\u53f7\u4e2d\u7684\u9886\u57df\u504f\u79fb\uff0c\u4f7f\u7528KPCA\u9884\u5904\u7406\u4fe1\u53f7\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u5b9e\u65f6\u68c0\u6d4b\u7684\u6f5c\u5728\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u808c\u7535\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u5bfc\u81f4\u9886\u57df\u504f\u79fb\u68c0\u6d4b\u6210\u4e3a\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u6d41\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u65f6\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528KPCA\u548c\u4f59\u5f26\u6838\u9884\u5904\u7406\u4fe1\u53f7\uff0c\u5e76\u8bc4\u4f30CUSUM\u3001Page-Hinckley\u548cADWIN\u7b49\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u5728DB6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u808c\u7535\u4fe1\u53f7\u9886\u57df\u504f\u79fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4f46\u6d41\u5f0f\u5b66\u4e60\u65b9\u6cd5\u5c55\u793a\u4e86\u7a33\u5b9a\u89e3\u7801\u6a21\u578b\u7684\u6f5c\u529b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u63d0\u5347\u5b9e\u65f6\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.21316", "pdf": "https://arxiv.org/pdf/2508.21316", "abs": "https://arxiv.org/abs/2508.21316", "authors": ["Changheng Wang", "Zhiqing Wei", "Wangjun Jiang", "Haoyue Jiang", "Zhiyong Feng"], "title": "Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance with Variable Formation", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "The high mobility of unmanned aerial vehicles (UAVs) enables them to be used\nin various civilian fields, such as rescue and cargo transport. Path-following\nis a crucial way to perform these tasks while sensing and collision avoidance\nare essential for safe flight. In this paper, we investigate how to efficiently\nand accurately achieve path-following, obstacle sensing and avoidance subtasks,\nas well as their conflict-free fusion scheduling. Firstly, a high precision\ndeep reinforcement learning (DRL)-based UAV formation path-following model is\ndeveloped, and the reward function with adaptive weights is designed from the\nperspective of distance and velocity errors. Then, we use integrated sensing\nand communication (ISAC) signals to detect the obstacle and derive the\nCramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,\nbased on which we propose the variable formation enhanced obstacle position\nestimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme\nwithout pretraining is designed to solve the sparse reward. Finally, with the\naid of null space based (NSB) behavioral method, we present a hierarchical\nsubtasks fusion strategy. Simulation results demonstrate the effectiveness and\nsuperiority of the subtask algorithms and the hierarchical fusion strategy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u5728\u8def\u5f84\u8ddf\u8e2a\u3001\u969c\u788d\u7269\u611f\u77e5\u4e0e\u907f\u78b0\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u534f\u540c\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u5f84\u8ddf\u8e2a\u6a21\u578b\u3001\u969c\u788d\u7269\u611f\u77e5\u7b97\u6cd5\u548c\u5728\u7ebf\u907f\u78b0\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u5b9e\u73b0\u4efb\u52a1\u878d\u5408\u3002", "motivation": "\u65e0\u4eba\u673a\u7684\u9ad8\u673a\u52a8\u6027\u4f7f\u5176\u5728\u6551\u63f4\u3001\u8d27\u7269\u8fd0\u8f93\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u3001\u5b89\u5168\u5730\u5b8c\u6210\u8def\u5f84\u8ddf\u8e2a\u4e0e\u907f\u78b0\u4efb\u52a1\u4ecd\u9700\u89e3\u51b3\u3002", "method": "1. \u5f00\u53d1\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u7cbe\u5ea6\u8def\u5f84\u8ddf\u8e2a\u6a21\u578b\uff1b2. \u5229\u7528ISAC\u4fe1\u53f7\u68c0\u6d4b\u969c\u788d\u7269\u5e76\u63d0\u51faVFEO\u7b97\u6cd5\uff1b3. \u8bbe\u8ba1\u65e0\u9884\u8bad\u7ec3\u7684\u5728\u7ebf\u907f\u78b0\u65b9\u6848\uff1b4. \u63d0\u51fa\u57fa\u4e8eNSB\u7684\u5206\u5c42\u4efb\u52a1\u878d\u5408\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5b50\u4efb\u52a1\u7b97\u6cd5\u548c\u5206\u5c42\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\u4e0e\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u8def\u5f84\u8ddf\u8e2a\u4e0e\u907f\u78b0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7cbe\u51c6\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u534f\u540c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.21542", "pdf": "https://arxiv.org/pdf/2508.21542", "abs": "https://arxiv.org/abs/2508.21542", "authors": ["Ziwei Liao", "Mohamed Sayed", "Steven L. Waslander", "Sara Vicente", "Daniyar Turmukhambetov", "Michael Firman"], "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Main paper: 11 pages; Supplementary materials: 7 pages", "summary": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u5b8c\u6574\u76843D\u9ad8\u65af\u98de\u6e85\u573a\u666f\uff0c\u5305\u62ec\u906e\u6321\u90e8\u5206\u3002", "motivation": "\u9ad8\u65af\u98de\u6e85\u6280\u672f\u901a\u5e38\u9700\u8981\u5bc6\u96c6\u89c2\u6d4b\uff0c\u65e0\u6cd5\u91cd\u5efa\u906e\u6321\u548c\u672a\u89c2\u6d4b\u533a\u57df\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u6a21\u7cca\u3001\u4e0d\u771f\u5b9e\u4e14\u65e0\u6cd5\u6355\u6349\u591a\u79cd\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u91cd\u5efa\u5668\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5728\u5176\u4e0a\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u591a\u6837\u5316\u76843D\u91cd\u5efa\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf360\u5ea6\u6e32\u67d3\uff0c\u5b8c\u6210\u906e\u6321\u8868\u9762\uff0c\u5e76\u63d0\u4f9b\u591a\u6837\u5316\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5f0f\u5efa\u6a21\u89e3\u51b3\u4e86\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf3D\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2508.21637", "pdf": "https://arxiv.org/pdf/2508.21637", "abs": "https://arxiv.org/abs/2508.21637", "authors": ["Ramkumar Natarajan", "Muhammad Suhail Saleem", "William Xiao", "Sandip Aine", "Howie Choset", "Maxim Likhachev"], "title": "A-MHA*: Anytime Multi-Heuristic A*", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Multi-Heuristic A* (MHA*)\u7b97\u6cd5\uff0c\u79f0\u4e3aA-MHA*\uff0c\u4f7f\u5176\u5177\u5907\u968f\u65f6\uff08anytime\uff09\u6c42\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u5728\u6709\u9650\u65f6\u95f4\u5185\u5feb\u901f\u627e\u5230\u53ef\u884c\u89e3\u5e76\u6301\u7eed\u4f18\u5316\u3002", "motivation": "\u539f\u59cbMHA*\u7b97\u6cd5\u867d\u80fd\u5229\u7528\u591a\u4e2a\u542f\u53d1\u5f0f\u51fd\u6570\u52a0\u901f\u641c\u7d22\uff0c\u4f46\u5176\u4e00\u6b21\u6027\u6c42\u89e3\u7279\u6027\u65e0\u6cd5\u6301\u7eed\u4f18\u5316\u89e3\uff0c\u4e14\u9700\u8981\u624b\u52a8\u8c03\u6574\u81a8\u80c0\u56e0\u5b50\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u5e0c\u671b\u901a\u8fc7\u6269\u5c55MHA*\u5b9e\u73b0\u968f\u65f6\u6c42\u89e3\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5c06Anytime Repairing A* (ARA*)\u7684\u601d\u60f3\u5f15\u5165MHA*\u6846\u67b6\uff0c\u5f00\u53d1\u4e86A-MHA*\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86MHA*\u7684\u6b21\u4f18\u548c\u5b8c\u5907\u6027\u4fdd\u8bc1\uff0c\u5e76\u5b9e\u73b0\u4e86\u968f\u65f6\u6c42\u89e3\u7279\u6027\u3002", "result": "A-MHA*\u57283D\u8def\u5f84\u89c4\u5212\u548c\u6ed1\u5757\u62fc\u56fe\uff08sliding tiles puzzle\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u539fMHA*\u548c\u5176\u4ed6\u968f\u65f6\u7b97\u6cd5\u3002", "conclusion": "A-MHA*\u6210\u529f\u5730\u5c06ARA*\u7684\u968f\u65f6\u4f18\u5316\u80fd\u529b\u878d\u5165MHA*\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u89e3\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.21800", "pdf": "https://arxiv.org/pdf/2508.21800", "abs": "https://arxiv.org/abs/2508.21800", "authors": ["Hyeonseong Jeon", "Cheolhong Min", "Jaesik Park"], "title": "Tree-Guided Diffusion Planner", "categories": ["cs.AI", "cs.RO"], "comment": "20 pages, 11 figures, 14 tables (main paper + appendix) / under\n  review / project page will be available after the paper becomes public in\n  arxiv", "summary": "Planning with pretrained diffusion models has emerged as a promising approach\nfor solving test-time guided control problems. However, standard gradient\nguidance typically performs optimally under convex and differentiable reward\nlandscapes, showing substantially reduced effectiveness in real-world scenarios\ninvolving non-convex objectives, non-differentiable constraints, and\nmulti-reward structures. Furthermore, recent supervised planning approaches\nrequire task-specific training or value estimators, which limits test-time\nflexibility and zero-shot generalization. We propose a Tree-guided Diffusion\nPlanner (TDP), a zero-shot test-time planning framework that balances\nexploration and exploitation through structured trajectory generation. We frame\ntest-time planning as a tree search problem using a bi-level sampling process:\n(1) diverse parent trajectories are produced via training-free particle\nguidance to encourage broad exploration, and (2) sub-trajectories are refined\nthrough fast conditional denoising guided by task objectives. TDP addresses the\nlimitations of gradient guidance by exploring diverse trajectory regions and\nharnessing gradient information across this expanded solution space using only\npretrained models and test-time reward signals. We evaluate TDP on three\ndiverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze\nmulti-goal exploration. TDP consistently outperforms state-of-the-art\napproaches on all tasks. The project page can be found at:\ntree-diffusion-planner.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Tree-guided Diffusion Planner (TDP)\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8f68\u8ff9\u751f\u6210\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u5f15\u5bfc\u5728\u975e\u51f8\u3001\u4e0d\u53ef\u5fae\u5206\u548c\u591a\u5956\u52b1\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u5728\u975e\u51f8\u3001\u4e0d\u53ef\u5fae\u5206\u548c\u591a\u5956\u52b1\u7ed3\u6784\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684\u76d1\u7763\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u4ef7\u503c\u4f30\u8ba1\u5668\uff0c\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u7684\u7075\u6d3b\u6027\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "method": "TDP\u901a\u8fc7\u53cc\u5c42\u91c7\u6837\u8fc7\u7a0b\u5c06\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u89c6\u4e3a\u6811\u641c\u7d22\u95ee\u9898\uff1a\u9996\u5148\u751f\u6210\u591a\u6837\u5316\u7684\u7236\u8f68\u8ff9\u4ee5\u9f13\u52b1\u63a2\u7d22\uff0c\u7136\u540e\u901a\u8fc7\u5feb\u901f\u6761\u4ef6\u53bb\u566a\u7ec6\u5316\u5b50\u8f68\u8ff9\u3002", "result": "TDP\u5728\u8ff7\u5bab\u9ec4\u91d1\u62fe\u53d6\u3001\u673a\u5668\u4eba\u624b\u81c2\u5757\u64cd\u4f5c\u548cAntMaze\u591a\u76ee\u6807\u63a2\u7d22\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "TDP\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6d4b\u8bd5\u65f6\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u4e0d\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002"}}
