{"id": "2510.18978", "pdf": "https://arxiv.org/pdf/2510.18978", "abs": "https://arxiv.org/abs/2510.18978", "authors": ["Tomer Shaked", "Philipp del Hougne", "George C. Alexandropoulos", "Nir Shlezinger"], "title": "AI-Aided Annealed Langevin Dynamics for Rapid Optimization of Programmable Channels", "categories": ["eess.SP"], "comment": "5 pages, 3 figures. Accepted to IEEE Signal Processing and Wireless\n  Communications (SPAWC) 2025 conference", "summary": "Emerging technologies such as Reconfigurable Intelligent Surfaces (RIS) make\nit possible to optimize some parameters of wireless channels. Conventional\napproaches require relating the channel and its programmable parameters via a\nsimple model that supports rapid optimization, e.g., re-tuning the parameters\neach time the users move. However, in practice such models are often crude\napproximations of the channel, and a more faithful description can be obtained\nvia complex simulators, or only by measurements. In this work, we introduce a\nnovel approach for rapid optimization of programmable channels based on\nAI-aided Annealed Langevin Dynamics (ALD), which bypasses the need for explicit\nchannel modeling. By framing the ALD algorithm using the MAP estimate, we\ndesign a deep unfolded ALD algorithm that leverages a Deep Neural Network (DNN)\nto estimate score gradients for optimizing channel parameters. We introduce a\ntraining method that overcomes the need for channel modeling using zero-order\ngradients, combined with active learning to enhance generalization, enabling\noptimization in complex and dynamically changing environments. We evaluate the\nproposed method in RIS-aided scenarios subject to rich-scattering effects. Our\nresults demonstrate that our AI-aided ALD method enables rapid and reliable\nchannel parameter tuning with limited latency."}
{"id": "2510.19007", "pdf": "https://arxiv.org/pdf/2510.19007", "abs": "https://arxiv.org/abs/2510.19007", "authors": ["Haofan Dong", "Houtianfu Wang", "Hanlin Cai", "Ozgur B. Akan"], "title": "Fundamental Limits of Cooperative Integrated Sensing and Communications over Low-Earth Orbit THz Satellite Channels", "categories": ["eess.SP"], "comment": null, "summary": "Terahertz inter-satellite links enable unprecedented sensing precision for\nLow Earth Orbit (LEO) constellations, yet face fundamental bounds from hardware\nimpairments, pointing errors, and network interference. We develop a Network\nCram\\'er-Rao Lower Bound (N-CRLB) framework incorporating dynamic topology,\nhardware quality factor $\\Gamma_{\\text{eff}}$, phase noise $\\sigma^2_\\phi$, and\ncooperative effects through recursive Fisher Information analysis. Our analysis\nreveals three key insights: (i) hardware and phase noise create\npower-independent performance ceilings ($\\sigma_{\\text{ceiling}} \\propto\n\\sqrt{\\Gamma_{\\text{eff}}}$) and floors ($\\sigma_{\\text{floor}} \\propto\n\\sqrt{\\sigma^2_\\phi}/f_c$), with power-only scaling saturating above\n$\\text{SNR}_{\\text{crit}}=1/\\Gamma_{\\text{eff}}$; (ii) interference\ncoefficients $\\alpha_{\\ell m}$ enable opportunistic sensing with demonstrated\ngains of 5.5~dB under specific conditions (65~dB processing gain, 50~dBi\nantennas); (iii) measurement correlations from shared timing references, when\nproperly modeled, do not degrade performance and can provide common-mode\nrejection benefits compared to mismodeled independent-noise baselines.\nSub-millimeter ranging requires co-optimized hardware\n($\\Gamma_{\\text{eff}}<0.01$), oscillators ($\\sigma^2_\\phi<10^{-2}$), and\nappropriate 3D geometry configurations."}
{"id": "2510.19057", "pdf": "https://arxiv.org/pdf/2510.19057", "abs": "https://arxiv.org/abs/2510.19057", "authors": ["Anna Cetera", "Sima Ghafoori", "Ali Rabiee", "Mohammad Hassan Farhadi", "Yalda Shahriari", "Reza Abiri"], "title": "Macroscopic EEG Reveals Discriminative Low-Frequency Oscillations in Plan-to-Grasp Visuomotor Tasks", "categories": ["eess.SP", "cs.RO"], "comment": "12 pages, 8 figures, 1 table", "summary": "The vision-based grasping brain network integrates visual perception with\ncognitive and motor processes for visuomotor tasks. While invasive recordings\nhave successfully decoded localized neural activity related to grasp type\nplanning and execution, macroscopic neural activation patterns captured by\nnoninvasive electroencephalography (EEG) remain far less understood. We\nintroduce a novel vision-based grasping platform to investigate\ngrasp-type-specific (precision, power, no-grasp) neural activity across\nlarge-scale brain networks using EEG neuroimaging. The platform isolates\ngrasp-specific planning from its associated execution phases in naturalistic\nvisuomotor tasks, where the Filter-Bank Common Spatial Pattern (FBCSP)\ntechnique was designed to extract discriminative frequency-specific features\nwithin each phase. Support vector machine (SVM) classification discriminated\nbinary (precision vs. power, grasp vs. no-grasp) and multiclass (precision vs.\npower vs. no-grasp) scenarios for each phase, and were compared against\ntraditional Movement-Related Cortical Potential (MRCP) methods. Low-frequency\noscillations (0.5-8 Hz) carry grasp-related information established during\nplanning and maintained throughout execution, with consistent classification\nperformance across both phases (75.3-77.8\\%) for precision vs. power\ndiscrimination, compared to 61.1\\% using MRCP. Higher-frequency activity (12-40\nHz) showed phase-dependent results with 93.3\\% accuracy for grasp vs. no-grasp\nclassification but 61.2\\% for precision vs. power discrimination. Feature\nimportance using SVM coefficients identified discriminative features within\nfrontoparietal networks during planning and motor networks during execution.\nThis work demonstrated the role of low-frequency oscillations in decoding grasp\ntype during planning using noninvasive EEG."}
{"id": "2510.19209", "pdf": "https://arxiv.org/pdf/2510.19209", "abs": "https://arxiv.org/abs/2510.19209", "authors": ["Yining Li", "Ziwei Wan", "Chongjia Sun", "Kaijun Feng", "Keke Ying", "Wenyan Ma", "Lipeng Zhu", "Xiaodan Shao", "Zhenyu Xiao", "Zhen Gao"], "title": "AI Signal Processing Paradigm for Movable Antenna: From Geometric Optimization to Electromagnetic Reconfigurability", "categories": ["eess.SP"], "comment": null, "summary": "As 6G wireless communication systems evolve toward intelligence and high\nreconfigurability, the limitations of traditional fixed antenna (TFA) has\nbecome increasingly prominent, with geometrically movable antenna (GMA) and\nelectromagnetically reconfigurable antenna (ERA) emerging as key technologies\nto break through this bottleneck. GMA activates spatial degrees of freedom\n(DoF) by dynamically adjusting antenna positions, ERA regulates radiation\ncharacteristics using tunable metamaterials, thereby introducing DoF in the\nelectromagnetic domain. However, the ``geometric-electromagnetic dual\nreconfiguration\" paradigm formed by their integration poses severe challenges\nof high-dimensional hybrid optimization to signal processing. To address this\nissue, we integrate the geometric optimization of GMA and the electromagnetic\nreconfiguration of ERA for the first time, propose a unified modeling framework\nfor movable and reconfigurable antenna (MARA), investigate the channel modeling\nand spectral efficiency (SE) optimization for GMA, ERA, and MARA. Besides, we\nsystematically review artificial intelligence (AI)-based solutions, focusing on\nanalyzing the advantages of AI over traditional algorithms in high-dimensional\nnon-convex optimization computations. This paper fills the gap in existing\nliterature regarding the lack of a comprehensive review on the AI-driven signal\nprocessing paradigm under geometric-electromagnetic dual reconfiguration and\nprovides theoretical support for the design and optimization of 6G wireless\nsystems with high SE and flexibility."}
{"id": "2510.18986", "pdf": "https://arxiv.org/pdf/2510.18986", "abs": "https://arxiv.org/abs/2510.18986", "authors": ["Alberto Sanchez-Delgado", "JoÃ£o Carlos Virgolino Soares", "Victor Barasuol", "Claudio Semini"], "title": "Towards Proprioceptive Terrain Mapping with Quadruped Robots for Exploration in Planetary Permanently Shadowed Regions", "categories": ["cs.RO"], "comment": "Published in the Proceedings of the International Conference on Space\n  Robotics (iSpaRo 2025)", "summary": "Permanently Shadowed Regions (PSRs) near the lunar poles are of interest for\nfuture exploration due to their potential to contain water ice and preserve\ngeological records. Their complex, uneven terrain favors the use of legged\nrobots, which can traverse challenging surfaces while collecting in-situ data,\nand have proven effective in Earth analogs, including dark caves, when equipped\nwith onboard lighting. While exteroceptive sensors like cameras and lidars can\ncapture terrain geometry and even semantic information, they cannot quantify\nits physical interaction with the robot, a capability provided by\nproprioceptive sensing. We propose a terrain mapping framework for quadruped\nrobots, which estimates elevation, foot slippage, energy cost, and stability\nmargins from internal sensing during locomotion. These metrics are\nincrementally integrated into a multi-layer 2.5D gridmap that reflects terrain\ninteraction from the robot's perspective. The system is evaluated in a\nsimulator that mimics a lunar environment, using the 21 kg quadruped robot\nAliengo, showing consistent mapping performance under lunar gravity and terrain\nconditions."}
{"id": "2510.19256", "pdf": "https://arxiv.org/pdf/2510.19256", "abs": "https://arxiv.org/abs/2510.19256", "authors": ["Haiquan Zhao", "Bei Xu"], "title": "Generalized Modified Blake-Zisserman Robust Spline Adaptive Filter for Generalized Gaussian Noise", "categories": ["eess.SP"], "comment": null, "summary": "The spline adaptive filtering (SAF) algorithm-based information-theoretic\nlearning has exhibited strong convergence performance in nonlinear system\nidentification (NSI), establishing SAF as a promising framework for adaptive\nfiltering. However, existing SAF-based methods suffer from performance\ndegradation under generalized Gaussian noise (GGN) environment and exhibit\nsignificant steady-state misalignment under impulse noise. Moreover, prior\nresearch on SAF algorithms has not effectively addressed the adverse effects\ncaused by outliers. To overcome these challenges, the generalized modified\nBlake-Zisserman robust spline adaptive filtering (SAF-GMBZ) algorithm is\nproposed. Compared to conventional SAF algorithms, SAF-GMBZ exhibits superior\nlearning performance in GGN. Furthermore, the mean convergence ranges of the\nstep-sizes and the steady-state mean-square error (MSE) are calculated by\nintroducing the commonly utilized assumptions. To arrive at good convergence\naccuracy and noise cancellation capability in active noise control (ANC)\napplication, the filter-c GMBZ (FcGMBZ) algorithm is further developed based on\nSAF-GMBZ. Simulation results confirm the accuracy of the theoretical\nsteady-state MSE, and the superiority of the SAF-GMBZ algorithm under GGN\nenvironment in NSI, along with the effectiveness of the FcGMBZ algorithm in ANC\napplication under impulsive noise environment."}
{"id": "2510.18991", "pdf": "https://arxiv.org/pdf/2510.18991", "abs": "https://arxiv.org/abs/2510.18991", "authors": ["Chinmay Burgul", "Yewei Huang", "Michalis Chatzispyrou", "Ioannis Rekleitis", "Alberto Quattrini Li", "Marios Xanthidis"], "title": "Underwater Dense Mapping with the First Compact 3D Sonar", "categories": ["cs.RO"], "comment": "8 pages, 12 figures", "summary": "In the past decade, the adoption of compact 3D range sensors, such as LiDARs,\nhas driven the developments of robust state-estimation pipelines, making them a\nstandard sensor for aerial, ground, and space autonomy. Unfortunately, poor\npropagation of electromagnetic waves underwater, has limited the\nvisibility-independent sensing options of underwater state-estimation to\nacoustic range sensors, which provide 2D information including, at-best,\nspatially ambiguous information. This paper, to the best of our knowledge, is\nthe first study examining the performance, capacity, and opportunities arising\nfrom the recent introduction of the first compact 3D sonar. Towards that\npurpose, we introduce calibration procedures for extracting the extrinsics\nbetween the 3D sonar and a camera and we provide a study on acoustic response\nin different surfaces and materials. Moreover, we provide novel mapping and\nSLAM pipelines tested in deployments in underwater cave systems and other\ngeometrically and acoustically challenging underwater environments. Our\nassessment showcases the unique capacity of 3D sonars to capture consistent\nspatial information allowing for detailed reconstructions and localization in\ndatasets expanding to hundreds of meters. At the same time it highlights\nremaining challenges related to acoustic propagation, as found also in other\nacoustic sensors. Datasets collected for our evaluations would be released and\nshared with the community to enable further research advancements."}
{"id": "2510.19267", "pdf": "https://arxiv.org/pdf/2510.19267", "abs": "https://arxiv.org/abs/2510.19267", "authors": ["Shama Siddiqu", "Indrakshi Dey"], "title": "A Study on Delay Assessment for Heterogenous Traffic in VANET", "categories": ["eess.SP"], "comment": "5th International Conference on Computing and Communication Networks\n  (ICCCNet-2025). Manchester, UK", "summary": "Vehicular Ad hoc Networks (VANETs) comprise of multi-priority hetero-genous\nnodes, both stationary and/or mobile. The data generated by these nodes may\ninclude messages relating to information, safety, entertainment, traffic\nmanagement and emergency alerts. The data in the network needs dif-ferentiated\nservice based on the priority/urgency. Media Access Control (MAC) protocols\nhold a significant value for managing the data priority. This paper studies a\ncomparison of 802.11p which is a standard PHY and MAC protocol for VANET with a\nfragmentation-based protocol, FROG-MAC. The major design principle of 802.11-p\nis to allow direct Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I)\ncommunication without associa-tion, using Enhanced Distributed Channel Access\n(EDCA) to prioritize safety-critical messages. However, if non-critical\nmessages already start to transmit, the nodes with critical data have to wait.\nFROG-MAC reduces this delay by transmitting normal packets in fragments with\nshort pauses between them, al-lowing urgent packets to access the channel\nduring these intervals. Simula-tions have been performed to assess the delay\nand throughput for high and low priority data. We report that FROG-MAC improves\nboth the performance parameters due to offering an early channel access to the\nemergency traffic."}
{"id": "2510.18996", "pdf": "https://arxiv.org/pdf/2510.18996", "abs": "https://arxiv.org/abs/2510.18996", "authors": ["Susheel Vadakkekuruppath", "Herman B. Amundsen", "Jason M. O'Kane", "Marios Xanthidis"], "title": "SHRUMS: Sensor Hallucination for Real-time Underwater Motion Planning with a Compact 3D Sonar", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Autonomous navigation in 3D is a fundamental problem for autonomy. Despite\nmajor advancements in terrestrial and aerial settings due to improved range\nsensors including LiDAR, compact sensors with similar capabilities for\nunderwater robots have only recently become available, in the form of 3D\nsonars. This paper introduces a novel underwater 3D navigation pipeline, called\nSHRUMS (Sensor Hallucination for Robust Underwater Motion planning with 3D\nSonar). To the best of the authors' knowledge, SHRUMS is the first underwater\nautonomous navigation stack to integrate a 3D sonar. The proposed pipeline\nexhibits strong robustness while operating in complex 3D environments in spite\nof extremely poor visibility conditions. To accommodate the intricacies of the\nnovel sensor data stream while achieving real-time locally optimal performance,\nSHRUMS introduces the concept of hallucinating sensor measurements from\nnon-existent sensors with convenient arbitrary parameters, tailored to\napplication specific requirements. The proposed concepts are validated with\nreal 3D sonar sensor data, utilizing real inputs in challenging settings and\nlocal maps constructed in real-time. Field deployments validating the proposed\napproach in full are planned in the very near future."}
{"id": "2510.19269", "pdf": "https://arxiv.org/pdf/2510.19269", "abs": "https://arxiv.org/abs/2510.19269", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Mehar Ullah", "Indrakshi Dey"], "title": "IoT-Enabled Sleep Monitoring and Cognitive Assessment for Evaluating Teacher Well-Being", "categories": ["eess.SP", "cs.CY"], "comment": "22nd Int. Conference on Networking, Sensing, and Control (ICNSC),\n  Oulu, Finland", "summary": "Sleep quality is an important indicator of the efficient cognitive function\nfor high school teachers. Due to the high work stress and multi-tasking\nexpectations, the teachers often face issues with their sleep quality and\ncognitive function, which has a clearly negative influence on their teaching\nabilities. In this work, we propose a unique but simple method of deploying\nInternet of Things (IoT) technology to monitor the sleep quality of high school\nteachers at Pakistan. Smart watches embedded with pulse rate and SpO2 sensors\nwere used to collect data and categorize the sleep quality as \"poor\", \"fair\" or\n\"good\". Moreover, we used a psychological tool, Cognitive Assessment\nQuestionnaire (CAQ) for the self-assessment of teachers' cognitive function.\nThe study was conducted over 208 high school teachers from across Pakistan. It\nhas been found that most of the teachers had a poor sleep quality and cognitive\nfunction; The link between these two variables indicate that the workload and\nother factors must be improved for the teachers to ensure their well-being,\nwhich will in turn have a positive impact on their teaching quality."}
{"id": "2510.18999", "pdf": "https://arxiv.org/pdf/2510.18999", "abs": "https://arxiv.org/abs/2510.18999", "authors": ["Zhirui Dai", "Qihao Qian", "Tianxing Fan", "Nikolay Atanasov"], "title": "$\\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Estimation of signed distance functions (SDFs) from point cloud data has been\nshown to benefit many robot autonomy capabilities, including localization,\nmapping, motion planning, and control. Methods that support online and\nlarge-scale SDF reconstruction tend to rely on discrete volumetric data\nstructures, which affect the continuity and differentiability of the SDF\nestimates. Recently, using implicit features, neural network methods have\ndemonstrated high-fidelity and differentiable SDF reconstruction but they tend\nto be less efficient, can experience catastrophic forgetting and memory\nlimitations in large environments, and are often restricted to truncated SDFs.\nThis work proposes $\\nabla$-SDF, a hybrid method that combines an explicit\nprior obtained from gradient-augmented octree interpolation with an implicit\nneural residual. Our method achieves non-truncated (Euclidean) SDF\nreconstruction with computational and memory efficiency comparable to\nvolumetric methods and differentiability and accuracy comparable to neural\nnetwork methods. Extensive experiments demonstrate that \\methodname{}\noutperforms the state of the art in terms of accuracy and efficiency, providing\na scalable solution for downstream tasks in robotics and computer vision."}
{"id": "2510.19309", "pdf": "https://arxiv.org/pdf/2510.19309", "abs": "https://arxiv.org/abs/2510.19309", "authors": ["Shreyan Banerjee", "Aasifa Rounak", "Cathal Hoare", "Denis Dowling", "Vikram Pakrashi"], "title": "Neuromorphic computing for anomaly detection in a laser powder bed fusion process", "categories": ["eess.SP"], "comment": "Journal Article", "summary": "This study is the first application of spiking neural networks (SNNs) for\nanomaly detection in the Laser Powder Bed Fusion (LPBF) additive manufacturing\nprocess. The neural networks were used to identify print processing anomalies\ngenerated by dropping of laser energy during the printing of individual layers\nin a Ti-6Al-4V alloy lattice structures. Associated changes in the laser\ngenerated melt pool were observed using an in-process photodiode monitoring\ntechnique. photodiode sensors capturing plasma and infrared radiations\nreflected from the print bed of the metal 3D printer were utilized to detect\nsudden changes caused by anomalies during the printing process. The algorithm\nis first implemented on non-neuromorphic hardware including a central\nprocessing unit (CPU), on Field Programmable Gate Arrays (FPGA) and then on\nneuromorphic Intel's Loihi chip. Improved detection of anomalies is achieved by\nadjusting the spike latency of the neural network, which reduces masking of\ninformation by noise within the monitored temporal signal. The work\ndemonstrates the possibility of using low-power neuromorphic chips within an\nedge framework for anomaly detection in additive manufacturing and creates a\nframework for the process."}
{"id": "2510.19054", "pdf": "https://arxiv.org/pdf/2510.19054", "abs": "https://arxiv.org/abs/2510.19054", "authors": ["Shiyu Liu", "Ilija Hadzic", "Akshay Gupta", "Aliasghar Arab"], "title": "Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 5 figures, 3 tables, video available at\n  https://youtu.be/8l9s2Wb_vec, To appear at IEEE 2025 International Conference\n  on Advanced Robotics", "summary": "This paper addresses motion planning and con- trol of an overactuated 4-wheel\ndrive train with independent steering (4WIS) where mechanical constraints\nprevent the wheels from executing full 360-degree rotations (swerve). The\nconfiguration space of such a robot is constrained and contains discontinuities\nthat affect the smoothness of the robot motion. We introduce a mathematical\nformulation of the steering constraints and derive discontinuity planes that\npartition the velocity space into regions of smooth and efficient motion. We\nfurther design the motion planner for path tracking and ob- stacle avoidance\nthat explicitly accounts for swerve constraints and the velocity transition\nsmoothness. The motion controller uses local feedback to generate actuation\nfrom the desired velocity, while properly handling the discontinuity crossing\nby temporarily stopping the motion and repositioning the wheels. We implement\nthe proposed motion planner as an extension to ROS Navigation package and\nevaluate the system in simulation and on a physical robot."}
{"id": "2510.19360", "pdf": "https://arxiv.org/pdf/2510.19360", "abs": "https://arxiv.org/abs/2510.19360", "authors": ["Dongwon Kim", "Jiwan Seo", "Joonhyuk Kang"], "title": "Multi-code rate Task-Oriented Communication for Multi-Edge Cooperative Inference", "categories": ["eess.SP"], "comment": null, "summary": "The integration of artificial intelligence (AI) with the internet of things\n(IoT) enables task-oriented communication for multi-edge cooperative inference\nsystem, where edge devices transmit extracted features of local sensory data to\nan edge server to perform AI-driven tasks. However, the privacy concerns and\nlimited communication bandwidth pose fundamental challenges, since simultaneous\ntransmission of extracted features with a single fixed compression ratio from\nall devices leads to severe inefficiency in communication resource utilization.\nTo address this challenge, we propose a framework that dynamically adjusts the\ncode rate in feature extraction based on its importance to the downstream\ninference task by adopting a rate-adaptive quantization (RAQ) scheme.\nFurthermore, to select the code rate for each edge device under limited\nbandwidth constraint, a dynamic programming (DP) approach is leveraged to\nallocate the code rate across discrete code rate options. Experiments on\nmulti-view datasets demonstrate that the proposed frameworks significantly\noutperform the frameworks using fixed-rate quantization, achieving a favorable\nbalance between communication efficiency and inference performance under\nlimited bandwidth conditions."}
{"id": "2510.19058", "pdf": "https://arxiv.org/pdf/2510.19058", "abs": "https://arxiv.org/abs/2510.19058", "authors": ["Fausto Vega", "Jon Arrizabalaga", "Ryan Watson", "Zachary Manchester"], "title": "Convex Maneuver Planning for Spacecraft Collision Avoidance", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 6 figures, Accepted to International Space Robotics\n  Conference", "summary": "Conjunction analysis and maneuver planning for spacecraft collision avoidance\nremains a manual and time-consuming process, typically involving repeated\nforward simulations of hand-designed maneuvers. With the growing density of\nsatellites in low-Earth orbit (LEO), autonomy is becoming essential for\nefficiently evaluating and mitigating collisions. In this work, we present an\nalgorithm to design low-thrust collision-avoidance maneuvers for short-term\nconjunction events. We first formulate the problem as a nonconvex\nquadratically-constrained quadratic program (QCQP), which we then relax into a\nconvex semidefinite program (SDP) using Shor's relaxation. We demonstrate\nempirically that the relaxation is tight, which enables the recovery of\nglobally optimal solutions to the original nonconvex problem. Our formulation\nproduces a minimum-energy solution while ensuring a desired probability of\ncollision at the time of closest approach. Finally, if the desired probability\nof collision cannot be satisfied, we relax this constraint into a penalty,\nyielding a minimum-risk solution. We validate our algorithm with a\nhigh-fidelity simulation of a satellite conjunction in low-Earth orbit with a\nsimulated conjunction data message (CDM), demonstrating its effectiveness in\nreducing collision risk."}
{"id": "2510.19401", "pdf": "https://arxiv.org/pdf/2510.19401", "abs": "https://arxiv.org/abs/2510.19401", "authors": ["Tao Zhou", "Liying Geng", "Yiqun Liang", "Kaifeng Bao", "Tianyun Feng", "Liu Liu", "Bo Ai"], "title": "Ray-Tracing Based Narrow-Beam Channel Simulation, Characterization and Performance Evaluation for 5G-R Systems", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates narrow-beam channel characterization and performance\nevaluation for 5G for railway (5G-R) systems based on ray-tracing (RT)\nsimulation. Three representative high-speed railway (HSR) scenarios including\nviaduct, cutting, and station are established, and RT-based dynamic narrow-beam\nchannel simulations are conducted using a designed beam tracking scheme that\nensures continuous alignment with the moving train. The channel characteristics\nare analyzed in terms of both large-scale and small-scale fading, as well as\nnon-stationarity, providing statistical insights into path loss, shadow fading,\nfading severity, time-frequency-space dispersion, and stationarity interval.\nThe influence of beamwidth on these channel properties is also examined.\nFurthermore, the performance of 5G-R systems operating in such narrow-beam\nchannels is evaluated using the Vienna 5G simulator, with a focus on block\nerror rate, throughput, and spectral efficiency. A hardware-in-the-loop\nsimulation platform is developed to further assess synchronization signal\nreference signal received power, signal-to-interference-plus-noise ratio, and\nreference signal received quality. The results provide valuable guidance for\nthe design and optimization of 5G-R systems in HSR environments."}
{"id": "2510.19068", "pdf": "https://arxiv.org/pdf/2510.19068", "abs": "https://arxiv.org/abs/2510.19068", "authors": ["Shifa Sulaiman", "Mohammad Gohari", "Francesco Schetter", "Fanny Ficuciello"], "title": "A Learning-based Model Reference Adaptive Controller Implemented on a Prosthetic Hand Wrist", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "International Conference on Social Robotics + AI", "summary": "The functionality and natural motion of prosthetic hands remain limited by\nthe challenges in controlling compliant wrist mechanisms. Current control\nstrategies often lack adaptability and incur high computational costs, which\nimpedes real-time deployment in assistive robotics. To address this gap, this\nstudy presents a computationally efficient Neural Network (NN)-based Model\nReference Adaptive Controller (MRAC) for a tendon-driven soft continuum wrist\nintegrated with a prosthetic hand. The dynamic modeling of the wrist is\nformulated using Timoshenko beam theory, capturing both shear and bending\ndeformations. The proposed NN-MRAC estimates the required tendon forces from\ndeflection errors and minimizes deviation from a reference model through online\nadaptation. Simulation results demonstrate improved precision with a root mean\nsquare error (RMSE) of $6.14 \\times 10^{-4}$ m and a settling time of $3.2$s.\nExperimental validations confirm real-time applicability, with an average RMSE\nof $5.66 \\times 10^{-3}$ m, steady-state error of $8.05 \\times 10^{-3}$ m, and\nsettling time of $1.58$ s. These results highlight the potential of the\ncontroller to enhance motion accuracy and responsiveness in soft prosthetic\nsystems, thereby advancing the integration of adaptive intelligent control in\nwearable assistive devices."}
{"id": "2510.19402", "pdf": "https://arxiv.org/pdf/2510.19402", "abs": "https://arxiv.org/abs/2510.19402", "authors": ["Kaifeng Bao", "Tao Zhou", "Chaoyi Li", "Liu Liu", "Bo Ai"], "title": "A Novel Delay-Doppler Domain Channel Sounding Method for 6G High-Mobility Scenarios", "categories": ["eess.SP"], "comment": "13 pages, 14 figures", "summary": "Channel measurements are the prerequisite for applying emerging transmission\ntechnologies and designing communication systems. In sixth-generation (6G)\nsystem, conventional time or frequency domain channel sounding methods cannot\ndirectly obtain Doppler information induced by high-mobility scenarios. The\nchannel spreading function (CSF) simultaneously captures delay and Doppler\ninformation, while naturally characterizing the propagation environment in the\ndelay-Doppler (DD) domain. However, DD domain channel sounding methods remain\nunderexplored. This paper presents a novel DD domain channel sounding method\nfor 6G high-mobility scenarios. First, we introduce the waveform design for the\nsounding signal and analyze its sounding capability. Next, the methodology of\nDD domain channel sounding, including synchronization and CSF estimation, is\nthoroughly detailed. Additionally, an algorithm for enhancing measurement\nprecision is proposed. The performance of the proposed method is rigorously\nevaluated. Subsequently, a DD domain channel sounding system competent for 6G\nhigh-mobility scenarios is established. Finally, DD domain channel measurements\nare conducted for a vehicle-to-infrastructure scenario in urban environments.\nMeasurement results, including CSF, power delay profile, Doppler power spectral\ndensity, number of multipath components, and other characteristics, are\nderived, which confirm the effectiveness of the proposed method and offer\nhelpful insights for advancing research on 6G high-mobility communications."}
{"id": "2510.19074", "pdf": "https://arxiv.org/pdf/2510.19074", "abs": "https://arxiv.org/abs/2510.19074", "authors": ["Yilang Liu", "Haoxiang You", "Ian Abraham"], "title": "Sample-Based Hybrid Mode Control: Asymptotically Optimal Switching of Algorithmic and Non-Differentiable Control Modes", "categories": ["cs.RO"], "comment": null, "summary": "This paper investigates a sample-based solution to the hybrid mode control\nproblem across non-differentiable and algorithmic hybrid modes. Our approach\nreasons about a set of hybrid control modes as an integer-based optimization\nproblem where we select what mode to apply, when to switch to another mode, and\nthe duration for which we are in a given control mode. A sample-based variation\nis derived to efficiently search the integer domain for optimal solutions. We\nfind our formulation yields strong performance guarantees that can be applied\nto a number of robotics-related tasks. In addition, our approach is able to\nsynthesize complex algorithms and policies to compound behaviors and achieve\nchallenging tasks. Last, we demonstrate the effectiveness of our approach in\nreal-world robotic examples that require reactive switching between long-term\nplanning and high-frequency control."}
{"id": "2510.19521", "pdf": "https://arxiv.org/pdf/2510.19521", "abs": "https://arxiv.org/abs/2510.19521", "authors": ["Zexin Fang", "Bin Han", "Zhu Han", "Hans D. Schotten"], "title": "Network-Centric Anomaly Filtering and Spoofer localization for 5G-NR Localization in LAWNs", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates security vulnerabilities and countermeasures for 3rd\nGeneration Partnership Project (3GPP) Fifth Generation New Radio (5G-NR) Time\nDifference of Arrival (TDoA)-based unmanned aerial vehicle (UAV) localization\nin low-altitude urban environments. We first optimize node selection strategies\nunder Air to Ground (A2G) channel conditions, proving that optimal selection\ndepends on UAV altitude and deployment density. We propose lightweight User\nEquipment (UE)-assisted that reduce overhead while enhancing accuracy. We then\nexpose critical security vulnerabilities by introducing merged-peak spoofing\nattacks where rogue UAVs transmit multiple lower-power pulses that merge with\nlegitimate signals, bypassing existing detection methods. Through theoretical\nmodeling and sensitivity analysis, we quantify how synchronization quality and\ngeometric factors determine spoofing success probability, revealing fundamental\nweaknesses in current 3GPP positioning frameworks. To address these\nvulnerabilities, we design a network-centric anomaly detection framework at the\nLocalization Management Function (LMF) using existing 3GPP-specified\nparameters, coupled with a recursive gradient descent-based robust localization\nalgorithm that filters anomaly data while estimating UAV position. Our unified\nframework simultaneously provides robust victim localization and spoofer\nlocalization-capabilities not integrated in existing literature. Extensive\nsimulations validate the effectiveness of both optimization and security\nmechanisms for 3GPP-compliant UAV positioning."}
{"id": "2510.19081", "pdf": "https://arxiv.org/pdf/2510.19081", "abs": "https://arxiv.org/abs/2510.19081", "authors": ["Shifa Sulaiman", "Tobias Busk Jensen", "Stefan Hein Bengtson", "Simon BÃ¸gh"], "title": "Kinematic Analysis and Integration of Vision Algorithms for a Mobile Manipulator Employed Inside a Self-Driving Laboratory", "categories": ["cs.RO"], "comment": "International Journal of Intelligent Robotics and Applications 2025", "summary": "Recent advances in robotics and autonomous systems have broadened the use of\nrobots in laboratory settings, including automated synthesis, scalable reaction\nworkflows, and collaborative tasks in self-driving laboratories (SDLs). This\npaper presents a comprehensive development of a mobile manipulator designed to\nassist human operators in such autonomous lab environments. Kinematic modeling\nof the manipulator is carried out based on the Denavit Hartenberg (DH)\nconvention and inverse kinematics solution is determined to enable precise and\nadaptive manipulation capabilities. A key focus of this research is enhancing\nthe manipulator ability to reliably grasp textured objects as a critical\ncomponent of autonomous handling tasks. Advanced vision-based algorithms are\nimplemented to perform real-time object detection and pose estimation, guiding\nthe manipulator in dynamic grasping and following tasks. In this work, we\nintegrate a vision method that combines feature-based detection with\nhomography-driven pose estimation, leveraging depth information to represent an\nobject pose as a $2$D planar projection within $3$D space. This adaptive\ncapability enables the system to accommodate variations in object orientation\nand supports robust autonomous manipulation across diverse environments. By\nenabling autonomous experimentation and human-robot collaboration, this work\ncontributes to the scalability and reproducibility of next-generation chemical\nlaboratories"}
{"id": "2510.19525", "pdf": "https://arxiv.org/pdf/2510.19525", "abs": "https://arxiv.org/abs/2510.19525", "authors": ["Vincent Savaux", "Hyeon Seok Rou", "Zeping Sui", "Giuseppe Thadeu Freitas de Abreu", "Zilong Liu"], "title": "On the Robustness of AFDM and OTFS Against Passive Eavesdroppers", "categories": ["eess.SP"], "comment": "5 pages, 3 figures", "summary": "We investigate the robustness of affine frequency division multiplexing\n(AFDM) and orthogonal time frequency space (OTFS) waveforms against passive\neavesdroppers performing brute-force demodulation to intercepted signals, under\nthe assumption that eavesdroppers have no knowledge of chirp parameters (in\nAFDM) or the delay-Doppler grid configuration (in OTFS), such that they must\nsearch exhaustively over possible demodulation matrices. Analytical results\nshow that the brute-force complexity scales as $\\mathcal{O}(\\sqrt{N})$ for OTFS\nand $\\mathcal{O}(N^2)$ for AFDM, where $N$ is the number of subcarriers,\nindicating that AFDM has superior resilience over OTFS. Bit error rate (BER)\nsimulations confirm the analysis by showing that, with AFDM, the signal remains\nnearly undecodable at the eavesdropper, while OTFS allows partial signal\nrecovery under equivalent conditions."}
{"id": "2510.19101", "pdf": "https://arxiv.org/pdf/2510.19101", "abs": "https://arxiv.org/abs/2510.19101", "authors": ["Matthew Jiang", "Shipeng Liu", "Feifei Qian"], "title": "Safe Active Navigation and Exploration for Planetary Environments Using Proprioceptive Measurements", "categories": ["cs.RO"], "comment": null, "summary": "Legged robots can sense terrain through force interactions during locomotion,\noffering more reliable traversability estimates than remote sensing and serving\nas scouts for guiding wheeled rovers in challenging environments. However, even\nlegged scouts face challenges when traversing highly deformable or unstable\nterrain. We present Safe Active Exploration for Granular Terrain (SAEGT), a\nnavigation framework that enables legged robots to safely explore unknown\ngranular environments using proprioceptive sensing, particularly where visual\ninput fails to capture terrain deformability. SAEGT estimates the safe region\nand frontier region online from leg-terrain interactions using Gaussian Process\nregression for traversability assessment, with a reactive controller for\nreal-time safe exploration and navigation. SAEGT demonstrated its ability to\nsafely explore and navigate toward a specified goal using only proprioceptively\nestimated traversability in simulation."}
{"id": "2510.19636", "pdf": "https://arxiv.org/pdf/2510.19636", "abs": "https://arxiv.org/abs/2510.19636", "authors": ["Sahar Maleki", "Reza Lashgari", "Mahdi Aliyari Shoorehdeli", "Mohammad Komareji"], "title": "Multilayer Perceptron Neural Network Model: A Novel Approach for LFP Contrast Sensitivity Tuning", "categories": ["eess.SP"], "comment": null, "summary": "Local field potentials (LFPs) have been demonstrated to be an important\nmeasurement to study the activity of a local population of neurons. The\nresponse tunings of LFPs have been mostly reported as weaker and broader than\nspike tunings. Therefore, selecting optimized tuning methods is essential for\nappropriately evaluating the LFP responses and comparing them with neighboring\nspiking activity. In this paper, new models for tuning of the contrast response\nfunctions (CRFs) are proposed. To this end, luminance contrast-evoked LFP\nresponses recorded in primate primary visual cortex (V1) are first analyzed.\nThen, supersaturating CRFs are distinguished from linear and saturating CRFs by\nusing monotonicity index (MI). The supersaturated recording data are then\nidentified through static identification methods including multilayer\nperceptron (MLP) neural network, radial basis function (RBF) neural network,\nfuzzy model, neuro-fuzzy model, and the local linear model tree (LOLIMOT)\nalgorithm. Our results demonstrate that the MLP neural network, compared to\ntraditional and modified hyperbolic Naka-Rushton functions, exhibits superior\nperformance in tuning the local field potential responses to luminance contrast\nstimuli, resulting in successful tuning of a significantly higher number of\nneural recordings of all three types. These results suggest that the MLP neural\nnetwork model can be used as a novel approach to measure a better fitted\ncontrast sensitivity tuning curve of a population of neurons than other\ncurrently used models."}
{"id": "2510.19128", "pdf": "https://arxiv.org/pdf/2510.19128", "abs": "https://arxiv.org/abs/2510.19128", "authors": ["Mehran Ghafarian Tamizi", "Homayoun Honari", "Amir Mehdi Soufi Enayati", "Aleksey Nozdryn-Plotnicki", "Homayoun Najjaran"], "title": "A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model", "categories": ["cs.RO", "cs.AI", "68T40 (Primary), 70Q05 (Secondary)"], "comment": "20 pages, 9 figures", "summary": "Path planning for a robotic system in high-dimensional cluttered environments\nneeds to be efficient, safe, and adaptable for different environments and\nhardware. Conventional methods face high computation time and require extensive\nparameter tuning, while prior learning-based methods still fail to generalize\neffectively. The primary goal of this research is to develop a path planning\nframework capable of generalizing to unseen environments and new robotic\nmanipulators without the need for retraining. We present GADGET (Generalizable\nand Adaptive Diffusion-Guided Environment-aware Trajectory generation), a\ndiffusion-based planning model that generates joint-space trajectories\nconditioned on voxelized scene representations as well as start and goal\nconfigurations. A key innovation is GADGET's hybrid dual-conditioning mechanism\nthat combines classifier-free guidance via learned scene encoding with\nclassifier-guided Control Barrier Function (CBF) safety shaping, integrating\nenvironment awareness with real-time collision avoidance directly in the\ndenoising process. This design supports zero-shot transfer to new environments\nand robotic embodiments without retraining. Experimental results show that\nGADGET achieves high success rates with low collision intensity in\nspherical-obstacle, bin-picking, and shelf environments, with CBF guidance\nfurther improving safety. Moreover, comparative evaluations indicate strong\nperformance relative to both sampling-based and learning-based baselines.\nFurthermore, GADGET provides transferability across Franka Panda, Kinova Gen3\n(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates\nits ability to generate safe, collision-free trajectories in real-world\nsettings."}
{"id": "2510.19639", "pdf": "https://arxiv.org/pdf/2510.19639", "abs": "https://arxiv.org/abs/2510.19639", "authors": ["Chenxing Tan", "Yuguan Hou", "Hao Wang", "Zhonghao Yuan"], "title": "Micro-Doppler Energy-Based Robust Multi-Target Vital Signs Monitoring Using 77-GHz FMCW Radar with Spatiotemporal Adaptive Processing", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents a novel micro-Doppler energy-based framework for robust\nmulti-target vital signs monitoring using 77-GHz Frequency-Modulated\nContinuous-Wave (FMCW) radar. Unlike conventional phase-based methods that are\nsusceptible to environmental noise, random body movements, and stringent\ncalibration requirements, our approach exploits the energy variations in radar\nreturns induced by cardiopulmonary activities. The proposed system integrates a\ncomprehensive processing pipeline including space-time adaptive processing\n(STAP) for target detection and tracking, MUSIC algorithm for high-resolution\nangle estimation, and an innovative adaptive spectral filtering technique for\nvital signs extraction. We establish a rigorous mathematical framework that\nformalizes the relationship between micro-Doppler energy variations and\nphysiological activities, enabling robust separation of closely spaced targets.\nThe key innovation lies in the micro-Doppler energy extraction methodology that\nprovides inherent robustness to phase noise and motion artifacts. Experimental\nresults using millimeter-wave radar datasets demonstrate that the system can\naccurately detect and separate vital signs of up to four targets within\n\\SI{5}{\\meter} range, achieving mean absolute errors of \\SI{1.2}beats per\nminute and \\SI{2.3} beats per minute for respiration and heart rates,\nrespectively. The proposed approach demonstrates superior performance compared\nto traditional phase-based methods, particularly in challenging multi-target\nscenarios with environmental noise and subject movement."}
{"id": "2510.19200", "pdf": "https://arxiv.org/pdf/2510.19200", "abs": "https://arxiv.org/abs/2510.19200", "authors": ["Matteo Bortolon", "Nuno Ferreira Duarte", "Plinio Moreno", "Fabio Poiesi", "JosÃ© Santos-Victor", "Alessio Del Bue"], "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted IROS 2025", "summary": "Achieving dexterous robotic grasping with multi-fingered hands remains a\nsignificant challenge. While existing methods rely on complete 3D scans to\npredict grasp poses, these approaches face limitations due to the difficulty of\nacquiring high-quality 3D data in real-world scenarios. In this paper, we\nintroduce GRASPLAT, a novel grasping framework that leverages consistent 3D\ninformation while being trained solely on RGB images. Our key insight is that\nby synthesizing physically plausible images of a hand grasping an object, we\ncan regress the corresponding hand joints for a successful grasp. To achieve\nthis, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of\nreal hand-object interactions, enabling end-to-end training with RGB data.\nUnlike prior methods, our approach incorporates a photometric loss that refines\ngrasp predictions by minimizing discrepancies between rendered and real images.\nWe conduct extensive experiments on both synthetic and real-world grasping\ndatasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%\nover existing image-based methods. Project page:\nhttps://mbortolon97.github.io/grasplat/"}
{"id": "2510.19775", "pdf": "https://arxiv.org/pdf/2510.19775", "abs": "https://arxiv.org/abs/2510.19775", "authors": ["Ilija TanaskoviÄ", "Ljiljana B. LazareviÄ", "Goran KneÅ¾eviÄ", "Nikola MilosavljeviÄ", "Olga DubljeviÄ", "Bojana BjegojeviÄ", "Nadica MiljkoviÄ"], "title": "Interpretable machine learning for cardiogram-based biometrics", "categories": ["eess.SP"], "comment": null, "summary": "This study investigates the role of electrocardiogram (ECG) and impedance\ncardiogram (ICG) features in biometric identification, emphasizing their\ndiscriminative capacity and robustness to emotional variability. A total of 29\nfeatures spanning four domains (temporal, amplitude, slope, and morphological)\nare evaluated using random forest (RF) models combined with multiple\ninterpretability methods. Feature importance shows that both ECG- and\nICG-derived features are consistently ranked among the top 10 by Gini\nimportance, permutation importance, and SHAP values, with ECG features,\nparticularly QRS-centric descriptors, occupying the highest positions. In\nparallel, ICG BCX features contribute complementary, however, with lower\ncross-method stability. Correlation analysis reveals substantial\nmulticollinearity, where the RF distributes and diminishes importance across\nhighly correlated pairs, confirming reduced independent contributions.\nStatistical analysis identifies 14 features with significant differences\nbetween baseline and anger, without a clear pattern by domain. Feature\nselection with recursive feature elimination and genetic algorithms converges\non a subset (14 features) that attains accuracy within 1% of the full set\n(99%), improving efficiency in storage and computation. These complementary\nanalyses indicate that the individuality required for reliable identification\nis primarily encoded in QRS-related ECG features across all four domains.\nMeanwhile, BCX-derived ICG features contribute mainly through amplitude,\nproviding supportive but less stable discriminatory cues. The confirmed\nresilience of QRS-centric descriptors to emotional variation, where stable\ninter-individual differences in the QRS complex could be traced to variations\nin ventricular mass, conduction pathways, and thoracic geometry, may indicate\ntheir central role in future models of cardiogram-based identity."}
{"id": "2510.19268", "pdf": "https://arxiv.org/pdf/2510.19268", "abs": "https://arxiv.org/abs/2510.19268", "authors": ["Mingen Li", "Houjian Yu", "Yixuan Huang", "Youngjin Hong", "Changhyun Choi"], "title": "Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 6 figures, 3 tables", "summary": "Long-horizon routing tasks of deformable linear objects (DLOs), such as\ncables and ropes, are common in industrial assembly lines and everyday life.\nThese tasks are particularly challenging because they require robots to\nmanipulate DLO with long-horizon planning and reliable skill execution.\nSuccessfully completing such tasks demands adapting to their nonlinear\ndynamics, decomposing abstract routing goals, and generating multi-step plans\ncomposed of multiple skills, all of which require accurate high-level reasoning\nduring execution. In this paper, we propose a fully autonomous hierarchical\nframework for solving challenging DLO routing tasks. Given an implicit or\nexplicit routing goal expressed in language, our framework leverages\nvision-language models~(VLMs) for in-context high-level reasoning to synthesize\nfeasible plans, which are then executed by low-level skills trained via\nreinforcement learning. To improve robustness in long horizons, we further\nintroduce a failure recovery mechanism that reorients the DLO into\ninsertion-feasible states. Our approach generalizes to diverse scenes involving\nobject attributes, spatial descriptions, as well as implicit language commands.\nIt outperforms the next best baseline method by nearly 50% and achieves an\noverall success rate of 92.5% across long-horizon routing scenarios."}
{"id": "2510.18917", "pdf": "https://arxiv.org/pdf/2510.18917", "abs": "https://arxiv.org/abs/2510.18917", "authors": ["Mandip Goswami"], "title": "RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling", "categories": ["eess.AS", "eess.SP"], "comment": "8 pages, 3 figures", "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies."}
{"id": "2510.19289", "pdf": "https://arxiv.org/pdf/2510.19289", "abs": "https://arxiv.org/abs/2510.19289", "authors": ["Kefeng Huang", "Jonathon Pipe", "Alice E. Martin", "Tianyuan Wang", "Barnabas A. Franklin", "Andy M. Tyrrell", "Ian J. S. Fairlamb", "Jihong Zhu"], "title": "TARMAC: A Taxonomy for Robot Manipulation in Chemistry", "categories": ["cs.RO"], "comment": null, "summary": "Chemistry laboratory automation aims to increase throughput, reproducibility,\nand safety, yet many existing systems still depend on frequent human\nintervention. Advances in robotics have reduced this dependency, but without a\nstructured representation of the required skills, autonomy remains limited to\nbespoke, task-specific solutions with little capacity to transfer beyond their\ninitial design. Current experiment abstractions typically describe\nprotocol-level steps without specifying the robotic actions needed to execute\nthem. This highlights the lack of a systematic account of the manipulation\nskills required for robots in chemistry laboratories. To address this gap, we\nintroduce TARMAC - a Taxonomy for Robot Manipulation in Chemistry - a\ndomain-specific framework that defines and organizes the core manipulations\nneeded in laboratory practice. Based on annotated teaching-lab demonstrations\nand supported by experimental validation, TARMAC categorizes actions according\nto their functional role and physical execution requirements. Beyond serving as\na descriptive vocabulary, TARMAC can be instantiated as robot-executable\nprimitives and composed into higher-level macros, enabling skill reuse and\nsupporting scalable integration into long-horizon workflows. These\ncontributions provide a structured foundation for more flexible and autonomous\nlaboratory automation. More information is available at\nhttps://tarmac-paper.github.io/"}
{"id": "2510.19117", "pdf": "https://arxiv.org/pdf/2510.19117", "abs": "https://arxiv.org/abs/2510.19117", "authors": ["Valentin NoÃ«l"], "title": "A Graph Signal Processing Framework for Hallucination Detection in Large Language Models", "categories": ["cs.CL", "cs.LG", "eess.SP", "stat.ML"], "comment": "Preprint under review (2025). 11 pages, 7 figures. Code and scripts:\n  to be released", "summary": "Large language models achieve impressive results but distinguishing factual\nreasoning from hallucinations remains challenging. We propose a spectral\nanalysis framework that models transformer layers as dynamic graphs induced by\nattention, with token embeddings as signals on these graphs. Through graph\nsignal processing, we define diagnostics including Dirichlet energy, spectral\nentropy, and high-frequency energy ratios, with theoretical connections to\ncomputational stability. Experiments across GPT architectures suggest universal\nspectral patterns: factual statements exhibit consistent \"energy mountain\"\nbehavior with low-frequency convergence, while different hallucination types\nshow distinct signatures. Logical contradictions destabilize spectra with large\neffect sizes ($g>1.0$), semantic errors remain stable but show connectivity\ndrift, and substitution hallucinations display intermediate perturbations. A\nsimple detector using spectral signatures achieves 88.75% accuracy versus 75%\nfor perplexity-based baselines, demonstrating practical utility. These findings\nindicate that spectral geometry may capture reasoning patterns and error\nbehaviors, potentially offering a framework for hallucination detection in\nlarge language models."}
{"id": "2510.19356", "pdf": "https://arxiv.org/pdf/2510.19356", "abs": "https://arxiv.org/abs/2510.19356", "authors": ["Yu Fang", "Xinyu Wang", "Xuehe Zhang", "Wanli Xue", "Mingwei Zhang", "Shengyong Chen", "Jie Zhao"], "title": "Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model", "categories": ["cs.RO"], "comment": null, "summary": "The wide application of flow-matching methods has greatly promoted the\ndevelopment of robot imitation learning. However, these methods all face the\nproblem of high inference time. To address this issue, researchers have\nproposed distillation methods and consistency methods, but the performance of\nthese methods still struggles to compete with that of the original diffusion\nmodels and flow-matching models. In this article, we propose a one-step\nshortcut method with multi-step integration for robot imitation learning. To\nbalance the inference speed and performance, we extend the multi-step\nconsistency loss on the basis of the shortcut model, split the one-step loss\ninto multi-step losses, and improve the performance of one-step inference.\nSecondly, to solve the problem of unstable optimization of the multi-step loss\nand the original flow-matching loss, we propose an adaptive gradient allocation\nmethod to enhance the stability of the learning process. Finally, we evaluate\nthe proposed method in two simulation benchmarks and five real-world\nenvironment tasks. The experimental results verify the effectiveness of the\nproposed algorithm."}
{"id": "2510.19131", "pdf": "https://arxiv.org/pdf/2510.19131", "abs": "https://arxiv.org/abs/2510.19131", "authors": ["Valentin NoÃ«l"], "title": "Training-Free Spectral Fingerprints of Voice Processing in Transformers", "categories": ["cs.CL", "cs.LG", "eess.SP", "stat.ML"], "comment": "Preprint under review (2025). 12 pages, 8 figures", "summary": "Different transformer architectures implement identical linguistic\ncomputations via distinct connectivity patterns, yielding model imprinted\n``computational fingerprints'' detectable through spectral analysis. Using\ngraph signal processing on attention induced token graphs, we track changes in\nalgebraic connectivity (Fiedler value, $\\Delta\\lambda_2$) under voice\nalternation across 20 languages and three model families, with a prespecified\nearly window (layers 2--5). Our analysis uncovers clear architectural\nsignatures: Phi-3-Mini shows a dramatic English specific early layer disruption\n($\\overline{\\Delta\\lambda_2}_{[2,5]}\\!\\approx\\!-0.446$) while effects in 19\nother languages are minimal, consistent with public documentation that\npositions the model primarily for English use. Qwen2.5-7B displays small,\ndistributed shifts that are largest for morphologically rich languages, and\nLLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures\ncorrelate strongly with behavioral differences (Phi-3: $r=-0.976$) and are\nmodulated by targeted attention head ablations, linking the effect to early\nattention structure and confirming functional relevance. Taken together, the\nfindings are consistent with the view that training emphasis can leave\ndetectable computational imprints: specialized processing strategies that\nmanifest as measurable connectivity patterns during syntactic transformations.\nBeyond voice alternation, the framework differentiates reasoning modes,\nindicating utility as a simple, training free diagnostic for revealing\narchitectural biases and supporting model reliability analysis."}
{"id": "2510.19364", "pdf": "https://arxiv.org/pdf/2510.19364", "abs": "https://arxiv.org/abs/2510.19364", "authors": ["Golnaz Raja", "Ruslan Agishev", "MiloÅ¡ PrÃ¡gr", "Joni Pajarinen", "Karel Zimmermann", "Arun Kumar Singh", "Reza Ghabcheloo"], "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling", "categories": ["cs.RO"], "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Uncertainty-aware robot motion prediction is crucial for downstream\ntraversability estimation and safe autonomous navigation in unstructured,\noff-road environments, where terrain is heterogeneous and perceptual\nuncertainty is high. Most existing methods assume deterministic or spatially\nindependent terrain uncertainties, ignoring the inherent local correlations of\n3D spatial data and often producing unreliable predictions. In this work, we\nintroduce an efficient probabilistic framework that explicitly models spatially\ncorrelated aleatoric uncertainty over terrain parameters as a probabilistic\nworld model and propagates this uncertainty through a differentiable physics\nengine for probabilistic trajectory forecasting. By leveraging structured\nconvolutional operators, our approach provides high-resolution multivariate\npredictions at manageable computational cost. Experimental evaluation on a\npublicly available dataset shows significantly improved uncertainty estimation\nand trajectory prediction accuracy over aleatoric uncertainty estimation\nbaselines."}
{"id": "2510.19174", "pdf": "https://arxiv.org/pdf/2510.19174", "abs": "https://arxiv.org/abs/2510.19174", "authors": ["Yuanming Zhang", "Zeyan Song", "Jing Lu", "Fei Chen", "Zhibin Lin"], "title": "Auditory Attention Decoding from Ear-EEG Signals: A Dataset with Dynamic Attention Switching and Rigorous Cross-Validation", "categories": ["eess.AS", "eess.SP"], "comment": null, "summary": "Recent promising results in auditory attention decoding (AAD) using scalp\nelectroencephalography (EEG) have motivated the exploration of cEEGrid, a\nflexible and portable ear-EEG system. While prior cEEGrid-based studies have\nconfirmed the feasibility of AAD, they often neglect the dynamic nature of\nattentional states in real-world contexts. To address this gap, a novel cEEGrid\ndataset featuring three concurrent speakers distributed across three of five\ndistinct spatial locations is introduced. The novel dataset is designed to\nprobe attentional tracking and switching in realistic scenarios. Nested\nleave-one-out validation-an approach more rigorous than conventional\nsingle-loop leave-one-out validation-is employed to reduce biases stemming from\nEEG's intricate temporal dynamics. Four rule-based models are evaluated: Wiener\nfilter (WF), canonical component analysis (CCA), common spatial pattern (CSP)\nand Riemannian Geometry-based classifier (RGC). With a 30-second decision\nwindow, WF and CCA models achieve decoding accuracies of 41.5% and 41.4%,\nrespectively, while CSP and RGC models yield 37.8% and 37.6% accuracies using a\n10-second window. Notably, both WF and CCA successfully track attentional state\nswitches across all experimental tasks. Additionally, higher decoding\naccuracies are observed for electrodes positioned at the upper cEEGrid layout\nand near the listener's right ear. These findings underscore the utility of\ndynamic, ecologically valid paradigms and rigorous validation in advancing AAD\nresearch with cEEGrid."}
{"id": "2510.19373", "pdf": "https://arxiv.org/pdf/2510.19373", "abs": "https://arxiv.org/abs/2510.19373", "authors": ["Basavasagar Patil", "Sydney Belt", "Jayjun Lee", "Nima Fazeli", "Bernadette Bucher"], "title": "Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Increasingly large datasets of robot actions and sensory observations are\nbeing collected to train ever-larger neural networks. These datasets are\ncollected based on tasks and while these tasks may be distinct in their\ndescriptions, many involve very similar physical action sequences (e.g., 'pick\nup an apple' versus 'pick up an orange'). As a result, many datasets of robotic\ntasks are substantially imbalanced in terms of the physical robotic actions\nthey represent. In this work, we propose a simple sampling strategy for policy\ntraining that mitigates this imbalance. Our method requires only a few lines of\ncode to integrate into existing codebases and improves generalization. We\nevaluate our method in both pre-training small models and fine-tuning large\nfoundational models. Our results show substantial improvements on low-resource\ntasks compared to prior state-of-the-art methods, without degrading performance\non high-resource tasks. This enables more effective use of model capacity for\nmulti-task policies. We also further validate our approach in a real-world\nsetup on a Franka Panda robot arm across a diverse set of tasks."}
{"id": "2510.19415", "pdf": "https://arxiv.org/pdf/2510.19415", "abs": "https://arxiv.org/abs/2510.19415", "authors": ["Abdelrahman Sayed Sayed"], "title": "Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "9 pages, 6 figures, Accepted for publication in OCEANS 2023 -\n  Limerick", "summary": "The growing interest in ocean discovery imposes a need for inspection and\nintervention in confined and demanding environments. Eely's slender shape, in\naddition to its ability to change its body configurations, makes articulated\nunderwater robots an adequate option for such environments. However, operation\nof Eely in such environments imposes demanding requirements on the system, as\nit must deal with uncertain and unstructured environments, extreme\nenvironmental conditions, and reduced navigational capabilities. This paper\nproposes a Bayesian approach to assess the risks of losing Eely during two\nmission scenarios. The goal of this work is to improve Eely's performance and\nthe likelihood of mission success. Sensitivity analysis results are presented\nin order to demonstrate the causes having the highest impact on losing Eely."}
{"id": "2510.19430", "pdf": "https://arxiv.org/pdf/2510.19430", "abs": "https://arxiv.org/abs/2510.19430", "authors": ["GigaBrain Team", "Angen Ye", "Boyuan Wang", "Chaojun Ni", "Guan Huang", "Guosheng Zhao", "Haoyun Li", "Jie Li", "Jiagang Zhu", "Lv Feng", "Peng Li", "Qiuping Deng", "Runqi Ouyang", "Wenkang Qin", "Xinze Chen", "Xiaofeng Wang", "Yang Wang", "Yifan Li", "Yilong Li", "Yiran Ding", "Yuan Xu", "Yun Ye", "Yukun Zhou", "Zhehao Dong", "Zhenan Wang", "Zhichao Liu", "Zheng Zhu"], "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model", "categories": ["cs.RO", "cs.CV"], "comment": "https://gigabrain0.github.io/", "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin."}
{"id": "2510.19495", "pdf": "https://arxiv.org/pdf/2510.19495", "abs": "https://arxiv.org/abs/2510.19495", "authors": ["Kevin Huang", "Rosario Scalise", "Cleah Winston", "Ayush Agrawal", "Yunchu Zhang", "Rohan Baijal", "Markus Grotz", "Byron Boots", "Benjamin Burchfiel", "Hongkai Dai", "Masha Itkina", "Paarth Shah", "Abhishek Gupta"], "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics."}
{"id": "2510.19541", "pdf": "https://arxiv.org/pdf/2510.19541", "abs": "https://arxiv.org/abs/2510.19541", "authors": ["Francesco Schetter", "Shifa Sulaiman", "Shoby George", "Paolino De Risi", "Fanny Ficuciello"], "title": "Optimizing Prosthetic Wrist Movement: A Model Predictive Control Approach", "categories": ["cs.RO"], "comment": "International Conference on Social Robotics + AI 2025", "summary": "The integration of advanced control strategies into prosthetic hands is\nessential to improve their adaptability and performance. In this study, we\npresent an implementation of a Model Predictive Control (MPC) strategy to\nregulate the motions of a soft continuum wrist section attached to a\ntendon-driven prosthetic hand with less computational effort. MPC plays a\ncrucial role in enhancing the functionality and responsiveness of prosthetic\nhands. By leveraging predictive modeling, this approach enables precise\nmovement adjustments while accounting for dynamic user interactions. This\nadvanced control strategy allows for the anticipation of future movements and\nadjustments based on the current state of the prosthetic device and the\nintentions of the user. Kinematic and dynamic modelings are performed using\nEuler-Bernoulli beam and Lagrange methods respectively. Through simulation and\nexperimental validations, we demonstrate the effectiveness of MPC in optimizing\nwrist articulation and user control. Our findings suggest that this technique\nsignificantly improves the prosthetic hand dexterity, making movements more\nnatural and intuitive. This research contributes to the field of robotics and\nbiomedical engineering by offering a promising direction for intelligent\nprosthetic systems."}
{"id": "2510.19655", "pdf": "https://arxiv.org/pdf/2510.19655", "abs": "https://arxiv.org/abs/2510.19655", "authors": ["Hongyu Ding", "Ziming Xu", "Yudong Fang", "You Wu", "Zixuan Chen", "Jieqi Shi", "Jing Huo", "Yifan Zhang", "Yang Gao"], "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments", "categories": ["cs.RO"], "comment": null, "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment."}
{"id": "2510.19663", "pdf": "https://arxiv.org/pdf/2510.19663", "abs": "https://arxiv.org/abs/2510.19663", "authors": ["VojtÄch Vrba", "Viktor Walter", "Petr Å tÄpÃ¡n", "Martin Saska"], "title": "Fast Marker Detection for UV-Based Visual Relative Localisation in Agile UAV Swarms", "categories": ["cs.RO"], "comment": null, "summary": "A novel approach for the fast onboard detection of isolated markers for\nvisual relative localisation of multiple teammates in agile UAV swarms is\nintroduced in this paper. As the detection forms a key component of real-time\nlocalisation systems, a three-fold innovation is presented, consisting of an\noptimised procedure for CPUs, a GPU shader program, and a functionally\nequivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,\nthe mean processing time per pixel of input camera frames was accelerated by\ntwo to three orders of magnitude compared to the state of the art. For the\nlocalisation task, the proposed FPGA architecture offered the most significant\noverall acceleration by minimising the total delay from camera exposure to\ndetection results. Additionally, the proposed solutions were evaluated on\nvarious 32-bit and 64-bit embedded platforms to demonstrate their efficiency,\nas well as their feasibility for applications using low-end UAVs and MAVs.\nThus, it has become a crucial enabling technology for agile UAV swarming."}
{"id": "2510.19752", "pdf": "https://arxiv.org/pdf/2510.19752", "abs": "https://arxiv.org/abs/2510.19752", "authors": ["Ameesh Shah", "William Chen", "Adwait Godbole", "Federico Mora", "Sanjit A. Seshia", "Sergey Levine"], "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9; I.2.8"], "comment": "7 pages and appendix", "summary": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks."}
{"id": "2510.19766", "pdf": "https://arxiv.org/pdf/2510.19766", "abs": "https://arxiv.org/abs/2510.19766", "authors": ["Hongyu Ding", "Xinyue Liang", "Yudong Fang", "You Wu", "Jieqi Shi", "Jing Huo", "Wenbin Li", "Jing Wu", "Yu-Kun Lai", "Yang Gao"], "title": "SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we propose SEA, a novel approach for active robot exploration\nthrough semantic map prediction and a reinforcement learning-based hierarchical\nexploration policy. Unlike existing learning-based methods that rely on\none-step waypoint prediction, our approach enhances the agent's long-term\nenvironmental understanding to facilitate more efficient exploration. We\npropose an iterative prediction-exploration framework that explicitly predicts\nthe missing areas of the map based on current observations. The difference\nbetween the actual accumulated map and the predicted global map is then used to\nguide exploration. Additionally, we design a novel reward mechanism that\nleverages reinforcement learning to update the long-term exploration\nstrategies, enabling us to construct an accurate semantic map within limited\nsteps. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art exploration strategies, achieving superior\ncoverage ares of the global map within the same time constraints."}
{"id": "2510.18828", "pdf": "https://arxiv.org/pdf/2510.18828", "abs": "https://arxiv.org/abs/2510.18828", "authors": ["Yigit Korkmaz", "Urvi Bhuwania", "Ayush Jain", "Erdem BÄ±yÄ±k"], "title": "Actor-Free Continuous Control via Structurally Maximizable Q-Functions", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Value-based algorithms are a cornerstone of off-policy reinforcement learning\ndue to their simplicity and training stability. However, their use has\ntraditionally been restricted to discrete action spaces, as they rely on\nestimating Q-values for individual state-action pairs. In continuous action\nspaces, evaluating the Q-value over the entire action space becomes\ncomputationally infeasible. To address this, actor-critic methods are typically\nemployed, where a critic is trained on off-policy data to estimate Q-values,\nand an actor is trained to maximize the critic's output. Despite their\npopularity, these methods often suffer from instability during training. In\nthis work, we propose a purely value-based framework for continuous control\nthat revisits structural maximization of Q-functions, introducing a set of key\narchitectural and algorithmic choices to enable efficient and stable learning.\nWe evaluate the proposed actor-free Q-learning approach on a range of standard\nsimulation tasks, demonstrating performance and sample efficiency on par with\nstate-of-the-art baselines, without the cost of learning a separate actor.\nParticularly, in environments with constrained action spaces, where the value\nfunctions are typically non-smooth, our method with structural maximization\noutperforms traditional actor-critic methods with gradient-based maximization.\nWe have released our code at https://github.com/USC-Lira/Q3C."}
{"id": "2510.19001", "pdf": "https://arxiv.org/pdf/2510.19001", "abs": "https://arxiv.org/abs/2510.19001", "authors": ["Seungjun Yu", "Junsung Park", "Youngsun Lim", "Hyunjung Shim"], "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a two-phase vision-language QA system for autonomous driving that\nanswers high-level perception, prediction, and planning questions. In Phase-1,\na large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a\nshort temporal window of history, and a chain-of-thought prompt with few-shot\nexemplars. A self-consistency ensemble (multiple sampled reasoning chains)\nfurther improves answer reliability. In Phase-2, we augment the prompt with\nnuScenes scene metadata (object annotations, ego-vehicle state, etc.) and\ncategory-specific question instructions (separate prompts for perception,\nprediction, planning tasks). In experiments on a driving QA benchmark, our\napproach significantly outperforms the baseline Qwen2.5 models. For example,\nusing 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall\naccuracy (vs.62.61% with zero-shot); applying self-consistency raises this to\n66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%\naccuracy under severe visual corruption. These results demonstrate that\ncarefully engineered prompts and contextual grounding can greatly enhance\nhigh-level driving QA with pretrained vision-language models."}
{"id": "2510.19057", "pdf": "https://arxiv.org/pdf/2510.19057", "abs": "https://arxiv.org/abs/2510.19057", "authors": ["Anna Cetera", "Sima Ghafoori", "Ali Rabiee", "Mohammad Hassan Farhadi", "Yalda Shahriari", "Reza Abiri"], "title": "Macroscopic EEG Reveals Discriminative Low-Frequency Oscillations in Plan-to-Grasp Visuomotor Tasks", "categories": ["eess.SP", "cs.RO"], "comment": "12 pages, 8 figures, 1 table", "summary": "The vision-based grasping brain network integrates visual perception with\ncognitive and motor processes for visuomotor tasks. While invasive recordings\nhave successfully decoded localized neural activity related to grasp type\nplanning and execution, macroscopic neural activation patterns captured by\nnoninvasive electroencephalography (EEG) remain far less understood. We\nintroduce a novel vision-based grasping platform to investigate\ngrasp-type-specific (precision, power, no-grasp) neural activity across\nlarge-scale brain networks using EEG neuroimaging. The platform isolates\ngrasp-specific planning from its associated execution phases in naturalistic\nvisuomotor tasks, where the Filter-Bank Common Spatial Pattern (FBCSP)\ntechnique was designed to extract discriminative frequency-specific features\nwithin each phase. Support vector machine (SVM) classification discriminated\nbinary (precision vs. power, grasp vs. no-grasp) and multiclass (precision vs.\npower vs. no-grasp) scenarios for each phase, and were compared against\ntraditional Movement-Related Cortical Potential (MRCP) methods. Low-frequency\noscillations (0.5-8 Hz) carry grasp-related information established during\nplanning and maintained throughout execution, with consistent classification\nperformance across both phases (75.3-77.8\\%) for precision vs. power\ndiscrimination, compared to 61.1\\% using MRCP. Higher-frequency activity (12-40\nHz) showed phase-dependent results with 93.3\\% accuracy for grasp vs. no-grasp\nclassification but 61.2\\% for precision vs. power discrimination. Feature\nimportance using SVM coefficients identified discriminative features within\nfrontoparietal networks during planning and motor networks during execution.\nThis work demonstrated the role of low-frequency oscillations in decoding grasp\ntype during planning using noninvasive EEG."}
{"id": "2510.19072", "pdf": "https://arxiv.org/pdf/2510.19072", "abs": "https://arxiv.org/abs/2510.19072", "authors": ["Tomoki Arita", "Keisuke Okumura"], "title": "Local Guidance for Configuration-Based Multi-Agent Pathfinding", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": "10 pages", "summary": "Guidance is an emerging concept that improves the empirical performance of\nreal-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers\nadditional information to MAPF algorithms to mitigate congestion on a global\nscale by considering the collective behavior of all agents across the entire\nworkspace. This global perspective helps reduce agents' waiting times, thereby\nimproving overall coordination efficiency. In contrast, this study explores an\nalternative approach: providing local guidance in the vicinity of each agent.\nWhile such localized methods involve recomputation as agents move and may\nappear computationally demanding, we empirically demonstrate that supplying\ninformative spatiotemporal cues to the planner can significantly improve\nsolution quality without exceeding a moderate time budget. When applied to\nLaCAM, a leading configuration-based solver, this form of guidance establishes\na new performance frontier for MAPF."}
{"id": "2510.19250", "pdf": "https://arxiv.org/pdf/2510.19250", "abs": "https://arxiv.org/abs/2510.19250", "authors": ["Yuheng Wu", "Xiangbo Gao", "Quang Tau", "Zhengzhong Tu", "Dongman Lee"], "title": "Background Fades, Foreground Leads: Curriculum-Guided Background Pruning for Efficient Foreground-Centric Collaborative Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collaborative perception enhances the reliability and spatial coverage of\nautonomous vehicles by sharing complementary information across vehicles,\noffering a promising solution to long-tail scenarios that challenge\nsingle-vehicle perception. However, the bandwidth constraints of vehicular\nnetworks make transmitting the entire feature map impractical. Recent methods,\ntherefore, adopt a foreground-centric paradigm, transmitting only predicted\nforeground-region features while discarding the background, which encodes\nessential context. We propose FadeLead, a foreground-centric framework that\novercomes this limitation by learning to encapsulate background context into\ncompact foreground features during training. At the core of our design is a\ncurricular learning strategy that leverages background cues early on but\nprogressively prunes them away, forcing the model to internalize context into\nforeground representations without transmitting background itself. Extensive\nexperiments on both simulated and real-world benchmarks show that FadeLead\noutperforms prior methods under different bandwidth settings, underscoring the\neffectiveness of context-enriched foreground sharing."}
{"id": "2510.19352", "pdf": "https://arxiv.org/pdf/2510.19352", "abs": "https://arxiv.org/abs/2510.19352", "authors": ["Omer Tariq", "Muhammad Bilal", "Muneeb Ul Hassan", "Dongsoo Han", "Jon Crowcroft"], "title": "ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation", "categories": ["cs.LG", "cs.CR", "cs.RO", "68T07, 68T05, 68P27, 62M10", "I.2.6; I.5.1; I.2.9; K.4.1; K.6.5; C.3; G.3"], "comment": "14 pages, 8 figures, 3 tables", "summary": "Data-driven inertial sequence learning has revolutionized navigation in\nGPS-denied environments, offering superior odometric resolution compared to\ntraditional Bayesian methods. However, deep learning-based inertial tracking\nsystems remain vulnerable to privacy breaches that can expose sensitive\ntraining data. \\hl{Existing differential privacy solutions often compromise\nmodel performance by introducing excessive noise, particularly in\nhigh-frequency inertial measurements.} In this article, we propose ConvXformer,\na hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a\nhierarchical structure for robust inertial navigation. We propose an efficient\ndifferential privacy mechanism incorporating adaptive gradient clipping and\ngradient-aligned noise injection (GANI) to protect sensitive information while\nensuring model performance. Our framework leverages truncated singular value\ndecomposition for gradient processing, enabling precise control over the\nprivacy-utility trade-off. Comprehensive performance evaluations on benchmark\ndatasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses\nstate-of-the-art methods, achieving more than 40% improvement in positioning\naccuracy while ensuring $(\\epsilon,\\delta)$-differential privacy guarantees. To\nvalidate real-world performance, we introduce the Mech-IO dataset, collected\nfrom the mechanical engineering building at KAIST, where intense magnetic\nfields from industrial equipment induce significant sensor perturbations. This\ndemonstrated robustness under severe environmental distortions makes our\nframework well-suited for secure and intelligent navigation in cyber-physical\nsystems."}
{"id": "2510.19407", "pdf": "https://arxiv.org/pdf/2510.19407", "abs": "https://arxiv.org/abs/2510.19407", "authors": ["Vanshika Datta", "C. Nahak"], "title": "A Radius of Robust Feasibility Approach to Directional Sensors in Uncertain Terrain", "categories": ["math.OC", "cs.RO"], "comment": null, "summary": "A sensor has the ability to probe its surroundings. However, uncertainties in\nits exact location can significantly compromise its sensing performance. The\nradius of robust feasibility defines the maximum range within which robust\nfeasibility is ensured. This work introduces a novel approach integrating it\nwith the directional sensor networks to enhance coverage using a distributed\ngreedy algorithm. In particular, we provide an exact formula for the radius of\nrobust feasibility of sensors in a directional sensor network. The proposed\nmodel strategically orients the sensors in regions with high coverage\npotential, accounting for robustness in the face of uncertainty. We analyze the\nalgorithm's adaptability in dynamic environments, demonstrating its ability to\nenhance efficiency and robustness. Experimental results validate its efficacy\nin maximizing coverage and optimizing sensor orientations, highlighting its\npractical advantages for real-world scenarios."}
{"id": "2510.19654", "pdf": "https://arxiv.org/pdf/2510.19654", "abs": "https://arxiv.org/abs/2510.19654", "authors": ["Zhida Zhao", "Talas Fu", "Yifan Wang", "Lijun Wang", "Huchuan Lu"], "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": "Accepted by NuerIPS 2025 (Poster)", "summary": "Despite remarkable progress in driving world models, their potential for\nautonomous systems remains largely untapped: the world models are mostly\nlearned for world simulation and decoupled from trajectory planning. While\nrecent efforts aim to unify world modeling and planning in a single framework,\nthe synergistic facilitation mechanism of world modeling for planning still\nrequires further exploration. In this work, we introduce a new driving paradigm\nnamed Policy World Model (PWM), which not only integrates world modeling and\ntrajectory planning within a unified architecture, but is also able to benefit\nplanning using the learned world knowledge through the proposed action-free\nfuture state forecasting scheme. Through collaborative state-action prediction,\nPWM can mimic the human-like anticipatory perception, yielding more reliable\nplanning performance. To facilitate the efficiency of video forecasting, we\nfurther introduce a dynamically enhanced parallel token generation mechanism,\nequipped with a context-guided tokenizer and an adaptive dynamic focal loss.\nDespite utilizing only front camera input, our method matches or exceeds\nstate-of-the-art approaches that rely on multi-view and multi-modal inputs.\nCode and model weights will be released at\nhttps://github.com/6550Zhao/Policy-World-Model."}
{"id": "2510.19732", "pdf": "https://arxiv.org/pdf/2510.19732", "abs": "https://arxiv.org/abs/2510.19732", "authors": ["Gunshi Gupta", "Karmesh Yadav", "Zsolt Kira", "Yarin Gal", "Rahaf Aljundi"], "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Accepted for Spotlight Presentation at NeurIPS 2025", "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints."}
{"id": "2510.19818", "pdf": "https://arxiv.org/pdf/2510.19818", "abs": "https://arxiv.org/abs/2510.19818", "authors": ["Jacob Berg", "Chuning Zhu", "Yanda Bao", "Ishan Durugkar", "Abhishek Gupta"], "title": "Semantic World Models", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Planning with world models offers a powerful paradigm for robotic control.\nConventional approaches train a model to predict future frames conditioned on\ncurrent frames and actions, which can then be used for planning. However, the\nobjective of predicting future pixels is often at odds with the actual planning\nobjective; strong pixel reconstruction does not always correlate with good\nplanning decisions. This paper posits that instead of reconstructing future\nframes as pixels, world models only need to predict task-relevant semantic\ninformation about the future. For such prediction the paper poses world\nmodeling as a visual question answering problem about semantic information in\nfuture frames. This perspective allows world modeling to be approached with the\nsame tools underlying vision language models. Thus vision language models can\nbe trained as \"semantic\" world models through a supervised finetuning process\non image-action-text data, enabling planning for decision-making while\ninheriting many of the generalization and robustness properties from the\npretrained vision-language models. The paper demonstrates how such a semantic\nworld model can be used for policy improvement on open-ended robotics tasks,\nleading to significant generalization improvements over typical paradigms of\nreconstruction-based action-conditional world modeling. Website available at\nhttps://weirdlabuw.github.io/swm."}
