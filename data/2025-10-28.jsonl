{"id": "2510.21748", "pdf": "https://arxiv.org/pdf/2510.21748", "abs": "https://arxiv.org/abs/2510.21748", "authors": ["Kiana Kiashemshaki", "Sina Samieirad", "Sarvenaz Erfani", "Aryan Jalaeianbanayan", "Nasibeh Asadi Isakan", "Hossein Najafzadeh"], "title": "Automated Tinnitus Detection Through Dual-Modality Neuroimaging: EEG Microstate Analysis and Resting-State fMRI Classification Using Deep Learning", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-bio.NC"], "comment": null, "summary": "Objective: Tinnitus affects 10-15% of the population yet lacks objective\ndiagnostic biomarkers. This study applied machine learning to EEG and fMRI data\nto identify neural signatures distinguishing tinnitus patients from healthy\ncontrols. Methods: Two datasets were analyzed: 64-channel EEG recordings from\n80 participants (40 tinnitus, 40 controls) and resting-state fMRI data from 38\nparticipants (19 tinnitus, 19 controls). EEG analysis extracted microstate\nfeatures across four to seven clustering states and five frequency bands,\nproducing 440 features per subject. Global Field Power signals were also\ntransformed into wavelet images for deep learning. fMRI data were analyzed\nusing slice-wise convolutional neural networks and hybrid models combining\npre-trained architectures (VGG16, ResNet50) with Decision Tree, Random Forest,\nand SVM classifiers. Model performance was evaluated using 5-fold\ncross-validation based on accuracy, precision, recall, F1-score, and ROC-AUC.\nResults: EEG microstate analysis revealed altered network dynamics in tinnitus,\nparticularly reduced gamma-band microstate B occurrence (healthy: 56.56 vs\ntinnitus: 43.81, p < 0.001) and diminished alpha coverage. Tree-based\nclassifiers achieved up to 98.8% accuracy, while VGG16 on wavelet-transformed\nEEG yielded 95.4% and 94.1% accuracy for delta and alpha bands, respectively.\nfMRI analysis identified 12 high-performing axial slices (>=90% accuracy), with\nslice 17 reaching 99.0%. The hybrid VGG16-Decision Tree model achieved 98.95%\n+/- 2.94% accuracy. Conclusion: EEG and fMRI provided effective neural\nbiomarkers for tinnitus classification. Tree-based and hybrid models\ndemonstrated superior performance, highlighting tinnitus as a multi-network\ndisorder requiring multimodal analysis."}
{"id": "2510.21789", "pdf": "https://arxiv.org/pdf/2510.21789", "abs": "https://arxiv.org/abs/2510.21789", "authors": ["Beyazit Bestami Yuksel"], "title": "Monitoring Real-Time ECG Signals on Mobile Systems", "categories": ["eess.SP", "cs.HC"], "comment": "10 figure, 4 pages", "summary": "This study focuses on the connection of a development kit that enables\nreal-time monitoring of electrocardiogram (ECG) signals using a mobile system.\nA software developed on the Visual Studio .NET platform reads real-time ECG\nsignals from the human body through non invasive methods and displays them\ngraphically on the mobile system. ECG electrodes placed on specific areas of\nthe body using the method known as Einthoven's triangle. Subsequently, the\nsoftware initiates data flow through the serial port, and these data displayed\nas signal values on the mobile device's screen via a graphical interface. When\nthe monitored ECG signals fall below a certain threshold or reach a critical\nvalue, the system provides feedback with an alert based on medical data. The\ndeveloped system is fully portable. Additionally, the implemented system has\nthe potential to form the basis for a multi-purpose system in the future, such\nas online patient monitoring, patient location tracking, and even initial\nintervention using the defibrillation method."}
{"id": "2510.21969", "pdf": "https://arxiv.org/pdf/2510.21969", "abs": "https://arxiv.org/abs/2510.21969", "authors": ["Weiyu Chen", "Arnaud Delorme"], "title": "Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification", "categories": ["eess.SP", "cs.LG", "cs.NE"], "comment": "8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine\n  Learning for EEG Signal Processing (MLESP)", "summary": "Detecting single-trial P300 from EEG is difficult when only a few labeled\ntrials are available. When attempting to boost a small target set with a large\nsource dataset through transfer learning, cross-dataset shift arises. To\naddress this challenge, we study transfer between two public visual-oddball ERP\ndatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict\nsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). We\nintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which\ncombines (i) a target-weighted loss with warm-up tied to the square root of the\nsource/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared\naffine parameters and per-domain running statistics, and (iii) a parameter-free\nlogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)\nterm using the median-bandwidth heuristic. Implemented on an EEG Conformer,\nAS-MMD is backbone-agnostic and leaves the inference-time model unchanged.\nAcross both transfer directions, it outperforms target-only and pooled training\n(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with\ngains over pooling significant under corrected paired t-tests. Ablations\nattribute improvements to all three components."}
{"id": "2510.22180", "pdf": "https://arxiv.org/pdf/2510.22180", "abs": "https://arxiv.org/abs/2510.22180", "authors": ["Maximilian Bauhofer", "Marcus Henninger", "Meik Kottkamp", "Lucas Giroto", "Philip Grill", "Alexander Felix", "Thorsten Wild", "Stephan ten Brink", "Silvio Mandelli"], "title": "Experimental Demonstration of Multi-Object Tracking in Integrated Sensing and Communication", "categories": ["eess.SP"], "comment": null, "summary": "For a wide range of envisioned integrated sensing and communication (ISAC)\nuse cases, it is necessary to incorporate tracking techniques into cellular\ncommunication systems. While numerous multi-object tracking algorithms exist,\nthey have not yet been applied to real-world ISAC, with its challenges such as\nclutter and non-optimal hardware. In this work, we showcase multi-object\ntracking based on the probability hypothesis density (PHD) filter in the range\nand Doppler speed domain. The measurements are taken with a 5G compliant ISAC\nproof-of-concept in a real factory environment, where the pedestrian-like\nobjects are generated by a radar object emulator. We detail the complete\npipeline, from measurement acquisition to evaluation, with a focus on the\npost-processing of the raw captured data and the tracking itself. Our\nend-to-end evaluation and comparison to simulations show good multi-object\ntracking performance with mean absolute error <1.5m and detection rates >91%\nfor realistic but challenging scenarios."}
{"id": "2510.21732", "pdf": "https://arxiv.org/pdf/2510.21732", "abs": "https://arxiv.org/abs/2510.21732", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "A Robotic Stirring Method with Trajectory Optimization and Adaptive Speed Control for Accurate Pest Counting in Water Traps", "categories": ["cs.RO", "cs.CV"], "comment": "This paper has been submitted to ICRA 2026 and is currently under\n  review", "summary": "Accurate monitoring of pest population dynamics is crucial for informed\ndecision-making in precision agriculture. Currently, mainstream image-based\npest counting methods primarily rely on image processing combined with machine\nlearning or deep learning for pest counting. However, these methods have\nlimitations and struggle to handle situations involving pest occlusion. To\naddress this issue, this paper proposed a robotic stirring method with\ntrajectory optimization and adaptive speed control for accurate pest counting\nin water traps. First, we developed an automated stirring system for pest\ncounting in yellow water traps based on a robotic arm. Stirring alters the\ndistribution of pests in the yellow water trap, making some of the occluded\nindividuals visible for detection and counting. Then, we investigated the\nimpact of different stirring trajectories on pest counting performance and\nselected the optimal trajectory for pest counting. Specifically, we designed\nsix representative stirring trajectories, including circle, square, triangle,\nspiral, four small circles, and random lines, for the robotic arm to stir. And\nby comparing the overall average counting error and counting confidence of\ndifferent stirring trajectories across various pest density scenarios, we\ndetermined the optimal trajectory. Finally, we proposed a counting\nconfidence-driven closed-loop control system to achieve adaptive-speed\nstirring. It uses changes in pest counting confidence between consecutive\nframes as feedback to adjust the stirring speed. To the best of our knowledge,\nthis is the first study dedicated to investigating the effects of different\nstirring trajectories on object counting in the dynamic liquid environment and\nto implement adaptive-speed stirring for this type of task. Experimental\nresults show ..."}
{"id": "2510.22297", "pdf": "https://arxiv.org/pdf/2510.22297", "abs": "https://arxiv.org/abs/2510.22297", "authors": ["Alexander Felix", "Rudolf Hoffmann", "Marcus Henninger", "Stephan ten Brink", "Silvio Mandelli"], "title": "Angular Estimation Comparison with ISAC PoC", "categories": ["eess.SP"], "comment": null, "summary": "The introduction of Integrated Sensing and Communications (ISAC) in cellular\nsystems is not expected to result in a shift away from the popular choice of\ncost- and energy-efficient analog or hybrid beamforming structures. However,\nthis comes at the cost of limiting the angular capabilities to a confined space\nper acquisitions. Thus, as a prerequisite for the successful implementation of\nnumerous ISAC use cases, the need for an optimal angular estimation of targets\nand their separation based on the minimal number of angular samples arises.\n  In this work, different approaches for angular estimation based on a minimal,\nDFT-based set of angular samples are evaluated. The samples are acquired\nthrough sweeping multiple beams of an ISAC proof of concept (PoC) in the\nindustrial scenario of the ARENA2036. The study's findings indicate that\ninterpolation approaches are more effective for generalizing across different\ntypes of angular scenarios. While the orthogonal matching pursuit (OMP)\napproach exhibits the most accurate estimation for a single, strong and clearly\ndiscriminable target, the DFT-based interpolation approach demonstrates the\nbest overall estimation performance."}
{"id": "2510.21734", "pdf": "https://arxiv.org/pdf/2510.21734", "abs": "https://arxiv.org/abs/2510.21734", "authors": ["Giovanni Battista Regazzo", "Wim-Alexander Beckers", "Xuan Thao Ha", "Mouloud Ourak", "Johan Vlekken", "Emmanuel Vander Poorten"], "title": "Force-Displacement Profiling for Robot-Assisted Deployment of a Left Atrial Appendage Occluder Using FBG-EM Distal Sensing", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Presented at the Conference on New Technologies for Computer and\n  Robot Assisted Surgery (CRAS2025)", "summary": "Atrial fibrillation (AF) increases the risk of thromboembolic events due to\nimpaired function of the left atrial appendage (LAA). Left atrial appendage\nclosure (LAAC) is a minimally invasive intervention designed to reduce stroke\nrisk by sealing the LAA with an expandable occluder device. Current deployment\nrelies on manual catheter control and imaging modalities like fluoroscopy and\ntransesophageal echocardiography, which carry limitations including radiation\nexposure and limited positioning precision. In this study, we leverage a\npreviously developed force-sensing delivery sheath integrating fiber Bragg\ngratings (FBGs) at the interface between the catheter and the occluder.\nCombined with electromagnetic (EM) tracking, this setup enables real-time\nmeasurement of interaction forces and catheter tip position during\nrobot-assisted LAAC deployment in an anatomical phantom. We present a novel\nforce-displacement profiling method that characterizes occluder deployment\ndynamics and identifies key procedural steps without relying on ionizing\nradiation. The force profiles reveal low-magnitude interaction forces,\nsuggesting minimal mechanical stress on the surrounding anatomy. This approach\nshows promise in providing clinicians with enhanced intraoperative feedback,\nimproving deployment outcome. Future work will focus on automating deployment\nsteps classification and validating the sensing strategy in dynamic, realistic\nenvironments."}
{"id": "2510.22406", "pdf": "https://arxiv.org/pdf/2510.22406", "abs": "https://arxiv.org/abs/2510.22406", "authors": ["Anargyros Michaloliakos", "Benjamin J. Chang", "Lawrence A. Bergman", "Alexander F. Vakakis"], "title": "Data-driven, Wavelet-based Identification and Reduced-order Modeling of Linear Systems with Closely Spaced Modes", "categories": ["eess.SP"], "comment": null, "summary": "This work presents a purely data-driven, wavelet-based framework for modal\nidentification and reduced-order modeling of mechanical systems with assumed\nlinear dynamics characterized by closely spaced modes with classical or\nnon-classical damping distribution. Traditional Fourier-based methods often\nfail to reliably identify closely spaced modes or accurately capture modal\ninteractions and complexities. To address these limitations, we propose a\nmethodology leveraging the enhanced time -frequency resolution capabilities of\nthe continuous wavelet transform (CWT). By selecting appropriate harmonic\nregions within the wavelet spectra, we effectively isolate modes, and then\ninvert them back in the temporal domain by applying the inverse CWT (ICWT). In\nthis way we reconstruct the corresponding modal dynamics in the time domain.\nUsing the Hilbert transform, instantaneous phases are extracted for each\nidentified mode, enabling the introduction of a complexified modal matrix which\nrobustly characterizes the system's modal properties, even under challenging\nperturbations such as noise and uncertainties due to modal interference and\nunmodeled effects. The identified modal parameters are utilized to reconstruct\nthe frequency response functions (FRFs) of the system and to develop a\nreduced-order model (ROM) that captures accurately the system's dominant\ndynamical behavior valid in a specified frequency range.. Validation of the\nmethodology is conducted both with a numerical non-classical damping and an\nexperimental testbed representing a model of an airplane structure. Results\ndemonstrate the effectiveness of the proposed approach in resolving intricate\nmodal interactions and accurately reproducing the dynamic response of complex\nstructural systems."}
{"id": "2510.21735", "pdf": "https://arxiv.org/pdf/2510.21735", "abs": "https://arxiv.org/abs/2510.21735", "authors": ["Yuhui Liu", "Shian Wang", "Ansel Panicker", "Kate Embry", "Ayana Asanova", "Tianyi Li"], "title": "A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit\ndistinct vehicle dynamics. EVs provide rapid acceleration, with electric motors\nproducing peak power across a wider speed range, and achieve swift deceleration\nthrough regenerative braking. While existing microscopic models effectively\ncapture the driving behavior of ICE vehicles, a modeling framework that\naccurately describes the unique car-following dynamics of EVs is lacking.\nDeveloping such a model is essential given the increasing presence of EVs in\ntraffic, yet creating an easy-to-use and accurate analytical model remains\nchallenging.\n  To address these gaps, this study develops and validates a Phase-Aware AI\n(PAAI) car-following model specifically for EVs. The proposed model enhances\ntraditional physics-based frameworks with an AI component that recognizes and\nadapts to different driving phases, such as rapid acceleration and regenerative\nbraking. Using real-world trajectory data from vehicles equipped with adaptive\ncruise control (ACC), we conduct comprehensive simulations to validate the\nmodel's performance. The numerical results demonstrate that the PAAI model\nsignificantly improves prediction accuracy over traditional car-following\nmodels, providing an effective tool for accurately representing EV behavior in\ntraffic simulations."}
{"id": "2510.22417", "pdf": "https://arxiv.org/pdf/2510.22417", "abs": "https://arxiv.org/abs/2510.22417", "authors": ["Laura Train", "Rodrigo Castellanos", "Miguel Gómez-López"], "title": "Genetic Optimization of a Software-Defined GNSS Receiver", "categories": ["eess.SP", "cs.NE", "math.OC"], "comment": null, "summary": "Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)\nreceivers face significant limitations under high-dynamic conditions,\nparticularly in high-acceleration environments such as those experienced by\nlaunch vehicles. These performance degradations, often observed as\ndiscontinuities in the navigation solution, arise from the inability of\ntraditional tracking loop bandwidths to cope with rapid variations in\nsynchronization parameters. Software-Defined Radio (SDR) receivers overcome\nthese constraints by enabling flexible reconfiguration of tracking loops;\nhowever, manual tuning involves a complex, multidimensional search and seldom\nensures optimal performance. This work introduces a genetic algorithm-based\noptimization framework that autonomously explores the receiver configuration\nspace to determine optimal loop parameters for phase, frequency, and delay\ntracking. The approach is validated within an SDR environment using\nrealistically simulated GPS L1 signals for three representative dynamic regimes\n-guided rocket flight, Low Earth Orbit (LEO) satellite, and static\nreceiver-processed with the open-source GNSS-SDR architecture. Results\ndemonstrate that evolutionary optimization enables SDR receivers to maintain\nrobust and accurate Position, Velocity, and Time (PVT) solutions across diverse\ndynamic conditions. The optimized configurations yielded maximum position and\nvelocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and\n2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case."}
{"id": "2510.21736", "pdf": "https://arxiv.org/pdf/2510.21736", "abs": "https://arxiv.org/abs/2510.21736", "authors": ["Yuhui Liu", "Samannita Halder", "Shian Wang", "Tianyi Li"], "title": "Learn2Drive: A neural network-based framework for socially compliant automated vehicle control", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "This study introduces a novel control framework for adaptive cruise control\n(ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks\nand physics-informed constraints. As automated vehicles (AVs) adopt advanced\nfeatures like ACC, transportation systems are becoming increasingly intelligent\nand efficient. However, existing AV control strategies primarily focus on\noptimizing the performance of individual vehicles or platoons, often neglecting\ntheir interactions with human-driven vehicles (HVs) and the broader impact on\ntraffic flow. This oversight can exacerbate congestion and reduce overall\nsystem efficiency. To address this critical research gap, we propose a neural\nnetwork-based, socially compliant AV control framework that incorporates social\nvalue orientation (SVO). This framework enables AVs to account for their\ninfluence on HVs and traffic dynamics. By leveraging AVs as mobile traffic\nregulators, the proposed approach promotes adaptive driving behaviors that\nreduce congestion, improve traffic efficiency, and lower energy consumption.\nWithin this framework, we define utility functions for both AVs and HVs, which\nare optimized based on the SVO of each AV to balance its own control objectives\nwith broader traffic flow considerations. Numerical results demonstrate the\neffectiveness of the proposed method in adapting to varying traffic conditions,\nthereby enhancing system-wide efficiency. Specifically, when the AV's control\nmode shifts from prioritizing energy consumption to optimizing traffic flow\nefficiency, vehicles in the following platoon experience at least a 58.99%\nincrease in individual energy consumption alongside at least a 38.39%\nimprovement in individual average speed, indicating significant enhancements in\ntraffic dynamics."}
{"id": "2510.22472", "pdf": "https://arxiv.org/pdf/2510.22472", "abs": "https://arxiv.org/abs/2510.22472", "authors": ["Yohei Kono", "Yoshiyuki Tajima"], "title": "Data-driven Exponential Framing for Pulsive Temporal Patterns without Repetition or Singularity", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "16 pages", "summary": "Extracting pulsive temporal patterns from a small dataset without their\nrepetition or singularity shows significant importance in manufacturing\napplications but does not sufficiently attract scientific attention. We propose\nto quantify how long temporal patterns appear without relying on their\nrepetition or singularity, enabling to extract such temporal patterns from a\nsmall dataset. Inspired by the celebrated time delay embedding and data-driven\nHankel matrix analysis, we introduce a linear dynamical system model on the\ntime-delay coordinates behind the data to derive the discrete-time bases each\nof which has a distinct exponential decay constant. The derived bases are\nfitted onto subsequences that are extracted with a sliding window in order to\nquantify how long patterns are dominant in the set of subsequences. We call the\nquantification method Data-driven Exponential Framing (DEF). A toy model-based\nexperiment shows that DEF can identify multiple patterns with distinct lengths.\nDEF is also applied to electric current measurement on a punching machine,\nshowing its possibility to extract multiple patterns from real-world\noscillatory data."}
{"id": "2510.21739", "pdf": "https://arxiv.org/pdf/2510.21739", "abs": "https://arxiv.org/abs/2510.21739", "authors": ["Liangqi Yuan", "Chuhao Deng", "Dong-Jun Han", "Inseok Hwang", "Sabine Brunswicker", "Christopher G. Brinton"], "title": "Next-Generation LLM for UAV: From Natural Language to Autonomous Flight", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), their\ncapabilities in various automation domains, particularly Unmanned Aerial\nVehicle (UAV) operations, have garnered increasing attention. Current research\nremains predominantly constrained to small-scale UAV applications, with most\nstudies focusing on isolated components such as path planning for toy drones,\nwhile lacking comprehensive investigation of medium- and long-range UAV systems\nin real-world operational contexts. Larger UAV platforms introduce distinct\nchallenges, including stringent requirements for airport-based take-off and\nlanding procedures, adherence to complex regulatory frameworks, and specialized\noperational capabilities with elevated mission expectations. This position\npaper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive\ndemonstration and automation roadmap for integrating LLMs into multi-scale UAV\noperations. The NeLV system processes natural language instructions to\norchestrate short-, medium-, and long-range UAV missions through five key\ntechnical components: (i) LLM-as-Parser for instruction interpretation, (ii)\nRoute Planner for Points of Interest (POI) determination, (iii) Path Planner\nfor waypoint generation, (iv) Control Platform for executable trajectory\nimplementation, and (v) UAV monitoring. We demonstrate the system's feasibility\nthrough three representative use cases spanning different operational scales:\nmulti-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the\ncurrent implementation, we establish a five-level automation taxonomy that\ncharts the evolution from current LLM-as-Parser capabilities (Level 1) to fully\nautonomous LLM-as-Autopilot systems (Level 5), identifying technical\nprerequisites and research challenges at each stage."}
{"id": "2510.22557", "pdf": "https://arxiv.org/pdf/2510.22557", "abs": "https://arxiv.org/abs/2510.22557", "authors": ["Wang Liu", "Cunhua Pan", "Hong Ren", "Wei Zhang", "Cheng-Xiang Wang", "Jiangzhou Wang"], "title": "Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO", "categories": ["eess.SP"], "comment": null, "summary": "The emergence of extremely large-scale antenna arrays (ELAA) in\nmillimeter-wave (mmWave) communications, particularly in high-mobility\nscenarios, highlights the importance of near-field beam prediction. Unlike the\nconventional far-field assumption, near-field beam prediction requires\ncodebooks that jointly sample the angular and distance domains, which leads to\na dramatic increase in pilot overhead. Moreover, unlike the far-field case\nwhere the optimal beam evolution is temporally smooth, the optimal near-field\nbeam index exhibits abrupt and nonlinear dynamics due to its joint dependence\non user angle and distance, posing significant challenges for temporal\nmodeling. To address these challenges, we propose a novel Convolutional Neural\nNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beam\nprediction framework. Specifically, an uplink pilot transmission strategy is\ndesigned to enable efficient channel probing through widebeam analog precoding\nand frequency-varying digital precoding. The received pilot signals are\npreprocessed and passed through a CNN-based feature extractor, followed by a\nGPT-2 model that captures temporal dependencies across multiple frames and\ndirectly predicts the near-field beam index in an end-to-end manner."}
{"id": "2510.21744", "pdf": "https://arxiv.org/pdf/2510.21744", "abs": "https://arxiv.org/abs/2510.21744", "authors": ["Yanjia Huang", "Shuo Liu", "Sheng Liu", "Qingxiao Xu", "Mingyang Wu", "Xiangbo Gao", "Zhengzhong Tu"], "title": "FORGE-Tree: Diffusion-Forcing Tree Search for Long-Horizon Robot Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Long-horizon robot manipulation tasks remain challenging for\nVision-Language-Action (VLA) policies due to drift and exposure bias, often\ndenoise the entire trajectory with fixed hyperparameters, causing small\ngeometric errors to compound across stages and offering no mechanism to\nallocate extra test-time compute where clearances are tight. To address these\nchallenges, we introduce FORGE-Tree, a plug-in control layer that couples a\nstage-aligned Diffusion Forcing (DF) head with test-time Monte Carlo Tree\nDiffusion (MCTD). With a frozen VLA encoder, DF aligns timesteps to subtask\nstages; during inference we partially denoise only a target segment while\nkeeping other tokens frozen, turning trajectory refinement into a sequence of\nlocal edits. We then apply Monte Carlo Tree Diffusion to select the next\nsegment to refine. A scene graph supplies priors for expansion and geometry\nrelation-aware scoring for rollouts, yielding tree-structured denoising whose\nperformance scales with search budget while preserving the executed prefix.\nEvaluation on LIBERO, FORGE-Tree improves success rate by 13.4 to 17.2 pp over\nthe native VLA baselines with both OpenVLA and Octo-Base. Gains remain\nconsistent under comparable compute budgets, especially on long-horizon\nvariants. Videos available at: https://taco-group.github.io/FORGE-Tree/"}
{"id": "2510.22621", "pdf": "https://arxiv.org/pdf/2510.22621", "abs": "https://arxiv.org/abs/2510.22621", "authors": ["Md. Shahriar Sadid", "Ali A. Nasir", "Saad Al-Ahmadi", "Samir Al-Ghadhban"], "title": "Parametric Channel Estimation and Design for Active-RIS-Assisted Communications", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable Intelligent Surface (RIS) technology has emerged as a key\nenabler for future wireless communications. However, its potential is\nconstrained by the difficulty of acquiring accurate user-to-RIS channel state\ninformation (CSI), due to the cascaded channel structure and the high pilot\noverhead of non-parametric methods. Unlike a passive RIS, where the reflected\nsignal suffers from multiplicative path loss, an active RIS amplifies the\nsignal, improving its practicality in real deployments. In this letter, we\npropose a parametric channel estimation method tailored for active RISs. The\nproposed approach integrates an active RIS model with an adaptive Maximum\nLikelihood Estimator (MLE) to recover the main channel parameters using a\nminimal number of pilots. To further enhance performance, an adaptive active\nRIS configuration strategy is employed, which refines the beam direction based\non an initial user location estimate. Moreover, an orthogonal angle-pair\ncodebook is used instead of the conventional Discrete Fourier Transform (DFT)\ncodebook, significantly reducing the codebook size and ensuring reliable\noperation for both far-field and near-field users. Extensive simulations\ndemonstrate that the proposed method achieves near-optimal performance with\nvery few pilots compared to non-parametric approaches. Its performance is also\nbenchmarked against that of a traditional passive RIS under the same total\npower budget to ensure fairness. Results show that active RIS yields higher\nspectral efficiency (SE) by eliminating the multiplicative fading inherent in\npassive RISs and allocating more resources to data transmission"}
{"id": "2510.21746", "pdf": "https://arxiv.org/pdf/2510.21746", "abs": "https://arxiv.org/abs/2510.21746", "authors": ["Harris Song", "Long Le"], "title": "Avi: Action from Volumetric Inference", "categories": ["cs.RO"], "comment": "NeurIPS 2025 Workshop on Embodied World Models for Decision Making.\n  URL: https://avi-3drobot.github.io/", "summary": "We propose Avi, a novel 3D Vision-Language-Action (VLA) architecture that\nreframes robotic action generation as a problem of 3D perception and spatial\nreasoning, rather than low-level policy learning. While existing VLA models\nprimarily operate on 2D visual inputs and are trained end-to-end on\ntask-specific action policies, Avi leverages 3D point clouds and\nlanguage-grounded scene understanding to compute actions through classical\ngeometric transformations. Most notably, Avi does not train on previous action\ntokens, rather, we build upon a 3D Multi-modal Large Language Model (MLLM) to\ngenerate the next point cloud and explicitly calculate the actions through\nclassical transformations. This approach enables generalizable behaviors that\nare robust to occlusions, camera pose variations, and changes in viewpoint. By\ntreating the robotic decision-making process as a structured reasoning task\nover 3D representations, Avi bridges the gap between high-level language\ninstructions and low-level actuation without requiring opaque policy learning.\nOur preliminary results highlight the potential of 3D vision-language reasoning\nas a foundation for scalable, robust robotic systems. Check it out at\nhttps://avi-3drobot.github.io/."}
{"id": "2510.22731", "pdf": "https://arxiv.org/pdf/2510.22731", "abs": "https://arxiv.org/abs/2510.22731", "authors": ["Yong Huang", "Wenjing Wang", "Dalong Zhang", "Junjie Wang", "Chen Chen", "Yan Cao", "Wei Wang"], "title": "Enhancing WiFi CSI Fingerprinting: A Deep Auxiliary Learning Approach", "categories": ["eess.SP"], "comment": "To appear in the IEEE Internet of Things", "summary": "Radio frequency (RF) fingerprinting techniques provide a promising supplement\nto cryptography-based approaches but rely on dedicated equipment to capture\nin-phase and quadrature (IQ) samples, hindering their wide adoption. Recent\nadvances advocate easily obtainable channel state information (CSI) by\ncommercial WiFi devices for lightweight RF fingerprinting, while falling short\nin addressing the challenges of coarse granularity of CSI measurements in an\nopen-world setting. In this paper, we propose CSI2Q, a novel CSI fingerprinting\nsystem that achieves comparable performance to IQ-based approaches. Instead of\nextracting fingerprints directly from raw CSI measurements, CSI2Q first\ntransforms frequency-domain CSI measurements into time-domain signals that\nshare the same feature space with IQ samples. Then, we employ a deep auxiliary\nlearning strategy to transfer useful knowledge from an IQ fingerprinting model\nto the CSI counterpart. Finally, the trained CSI model is combined with an\nOpenMax function to estimate the likelihood of unknown ones. We evaluate CSI2Q\non one synthetic CSI dataset involving 85 devices and two real CSI datasets,\nincluding 10 and 25 WiFi routers, respectively. Our system achieves accuracy\nincreases of at least 16% on the synthetic CSI dataset, 20% on the in-lab CSI\ndataset, and 17% on the in-the-wild CSI dataset."}
{"id": "2510.21751", "pdf": "https://arxiv.org/pdf/2510.21751", "abs": "https://arxiv.org/abs/2510.21751", "authors": ["Van Nam Dinh", "Van Vy Phan", "Thai Son Dang", "Van Du Phan", "The Anh Mai", "Van Chuong Le", "Sy Phuong Ho", "Dinh Tu Duong", "Hung Cuong Ta"], "title": "Real-time Mixed-Integer Quadratic Programming for Driving Behavior-Inspired Speed Bump Optimal Trajectory Planning", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes a novel methodology for trajectory planning in autonomous\nvehicles (AVs), addressing the complex challenge of negotiating speed bumps\nwithin a unified Mixed-Integer Quadratic Programming (MIQP) framework. By\nleveraging Model Predictive Control (MPC), we develop trajectories that\noptimize both the traversal of speed bumps and overall passenger comfort. A key\ncontribution of this work is the formulation of speed bump handling constraints\nthat closely emulate human driving behavior, seamlessly integrating these with\nbroader road navigation requirements. Through extensive simulations in varied\nurban driving environments, we demonstrate the efficacy of our approach,\nhighlighting its ability to ensure smooth speed transitions over speed bumps\nwhile maintaining computational efficiency suitable for real-time deployment.\nThe method's capability to handle both static road features and dynamic\nconstraints, alongside expert human driving, represents a significant step\nforward in trajectory planning for urban"}
{"id": "2510.22772", "pdf": "https://arxiv.org/pdf/2510.22772", "abs": "https://arxiv.org/abs/2510.22772", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "Neural-HAR: A Dimension-Gated CNN Accelerator for Real-Time Radar Human Activity Recognition", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Radar-based human activity recognition (HAR) is attractive for unobtrusive\nand privacy-preserving monitoring, yet many CNN/RNN solutions remain too heavy\nfor edge deployment, and even lightweight ViT/SSM variants often exceed\npractical compute and memory budgets. We introduce Neural-HAR, a\ndimension-gated CNN accelerator tailored for real-time radar HAR on\nresource-constrained platforms. At its core is GateCNN, a parameter-efficient\nDoppler-temporal network that (i) embeds Doppler vectors to emphasize frequency\nevolution over time and (ii) applies dual-path gated convolutions that modulate\nDoppler-aware content features with temporal gates, complemented by a residual\npath for stable training. On the University of Glasgow UoG2020 continuous radar\ndataset, GateCNN attains 86.4% accuracy with only 2.7k parameters and 0.28M\nFLOPs per inference, comparable to CNN-BiGRU at a fraction of the complexity.\nOur FPGA prototype on Xilinx Zynq-7000 Z-7007S reaches 107.5 $\\mu$s latency and\n15 mW dynamic power using LUT-based ROM and distributed RAM only (zero\nDSP/BRAM), demonstrating real-time, energy-efficient edge inference. Code and\nHLS conversion scripts are available at https://github.com/lab-emi/AIRHAR."}
{"id": "2510.21758", "pdf": "https://arxiv.org/pdf/2510.21758", "abs": "https://arxiv.org/abs/2510.21758", "authors": ["Kumater Ter", "RexCharles Donatus", "Ore-Ofe Ajayi", "Daniel Udekwe"], "title": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems."}
{"id": "2510.22895", "pdf": "https://arxiv.org/pdf/2510.22895", "abs": "https://arxiv.org/abs/2510.22895", "authors": ["Wang Hao", "Kuang Zhang", "Hou Chengyu", "Yang Yifan", "Tan Chenxing", "Fu Weifeng"], "title": "Rmd: Robust Modal Decomposition with Constrained Bandwidth", "categories": ["eess.SP"], "comment": null, "summary": "Modal decomposition techniques, such as Empirical Mode Decomposition (EMD),\nVariational Mode Decomposition (VMD), and Singular Spectrum Analysis (SSA),\nhave advanced time-frequency signal analysis since the early 21st century.\nThese methods are generally classified into two categories: numerical\noptimization-based methods (EMD, VMD) and spectral decomposition methods (SSA)\nthat consider the physical meaning of signals. The former can produce spurious\nmodes due to the lack of physical constraints, while the latter is more\nsensitive to noise and struggles with nonlinear signals. Despite continuous\nimprovements in these methods, a modal decomposition approach that effectively\ncombines the strengths of both categories remains elusive. This paper thus\nproposes a Robust Modal Decomposition (RMD) method with constrained bandwidth,\nwhich preserves the intrinsic structure of the signal by mapping the time\nseries into its trajectory-GRAM matrix in phase space. Moreover, the method\nincorporates bandwidth constraints during the decomposition process, enhancing\nnoise resistance. Extensive experiments on synthetic and real-world datasets,\nincluding millimeter-wave radar echoes, electrocardiogram (ECG),\nphonocardiogram (PCG), and bearing fault detection data, demonstrate the\nmethod's effectiveness and versatility. All code and dataset samples are\navailable on GitHub: https://github.com/Einstein-sworder/RMD."}
{"id": "2510.21761", "pdf": "https://arxiv.org/pdf/2510.21761", "abs": "https://arxiv.org/abs/2510.21761", "authors": ["Jesse Atuhurra", "Hidetaka Kamigaito", "Taro Watanabe", "Koichiro Yoshino"], "title": "J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted to IROS2025", "summary": "We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot\nperception by providing detailed object attribute annotations within Japanese\nhuman-robot dialogue scenarios. J-ORA is designed to support three critical\nperception tasks, object identification, reference resolution, and next-action\nprediction, by leveraging a comprehensive template of attributes (e.g.,\ncategory, color, shape, size, material, and spatial relations). Extensive\nevaluations with both proprietary and open-source Vision Language Models (VLMs)\nreveal that incorporating detailed object attributes substantially improves\nmultimodal perception performance compared to without object attributes.\nDespite the improvement, we find that there still exists a gap between\nproprietary and open-source VLMs. In addition, our analysis of object\naffordances demonstrates varying abilities in understanding object\nfunctionality and contextual relationships across different VLMs. These\nfindings underscore the importance of rich, context-sensitive attribute\nannotations in advancing robot perception in dynamic environments. See project\npage at https://jatuhurrra.github.io/J-ORA/."}
{"id": "2510.22913", "pdf": "https://arxiv.org/pdf/2510.22913", "abs": "https://arxiv.org/abs/2510.22913", "authors": ["Thanyanee Srichaisak", "Arissa Ieochai", "Aueaphum Aueawattthanaphisut"], "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.RO", "q-bio.NC"], "comment": "19 pages, 7 figures, 5 Tables", "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical)."}
{"id": "2510.21771", "pdf": "https://arxiv.org/pdf/2510.21771", "abs": "https://arxiv.org/abs/2510.21771", "authors": ["Dharunish Yugeswardeenoo"], "title": "Improving the performance of AI-powered Affordable Robotics for Assistive Tasks", "categories": ["cs.RO"], "comment": "6 pages, 5 figures. Accepted to Conference on Robot Learning (CoRL\n  2025), Seoul, Korea", "summary": "By 2050, the global demand for assistive care is expected to reach 3.5\nbillion people, far outpacing the availability of human caregivers. Existing\nrobotic solutions remain expensive and require technical expertise, limiting\naccessibility. This work introduces a low-cost robotic arm for assistive tasks\nsuch as feeding, cleaning spills, and fetching medicine. The system uses\nimitation learning from demonstration videos, requiring no task-specific\nprogramming or manual labeling. The robot consists of six servo motors, dual\ncameras, and 3D-printed grippers. Data collection via teleoperation with a\nleader arm yielded 50,000 video frames across the three tasks. A novel Phased\nAction Chunking Transformer (PACT) captures temporal dependencies and segments\nmotion dynamics, while a Temporal Ensemble (TE) method refines trajectories to\nimprove accuracy and smoothness. Evaluated across five model sizes and four\narchitectures, with ten hours of real-world testing, the system achieved over\n90% task accuracy, up to 40% higher than baselines. PACT enabled a 5x model\nsize reduction while maintaining 75% accuracy. Saliency analysis showed\nreliance on key visual cues, and phase token gradients peaked at critical\ntrajectory moments, indicating effective temporal reasoning. Future work will\nexplore bimanual manipulation and mobility for expanded assistive capabilities."}
{"id": "2510.22947", "pdf": "https://arxiv.org/pdf/2510.22947", "abs": "https://arxiv.org/abs/2510.22947", "authors": ["Yi Tao", "Zhen Gao", "Fangquan Ye", "Jingbo Xu", "Tao Song", "Weidong Li", "Yu Su", "Lu Peng", "Xiaomei Wu", "Tong Qin", "Zhongxiang Li", "Dezhi Zheng"], "title": "Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy", "categories": ["eess.SP"], "comment": null, "summary": "The development of the low-altitude economy has led to a growing prominence\nof uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate\nidentification, real-time localization, and effective countermeasures have\nbecome core challenges in airspace security assurance. This paper introduces an\nintegrated UAV management and control system based on deep learning, which\nintegrates multimodal multi-sensor fusion perception, precise positioning, and\ncollaborative countermeasures. By incorporating deep learning methods, the\nsystem combines radio frequency (RF) spectral feature analysis, radar\ndetection, electro-optical identification, and other methods at the detection\nlevel to achieve the identification and classification of UAVs. At the\nlocalization level, the system relies on multi-sensor data fusion and the\nair-space-ground integrated communication network to conduct real-time tracking\nand prediction of UAV flight status, providing support for early warning and\ndecision-making. At the countermeasure level, it adopts comprehensive measures\nthat integrate ``soft kill'' and ``hard kill'', including technologies such as\nelectromagnetic signal jamming, navigation spoofing, and physical interception,\nto form a closed-loop management and control process from early warning to\nfinal disposal, which significantly enhances the response efficiency and\ndisposal accuracy of low-altitude UAV management."}
{"id": "2510.21773", "pdf": "https://arxiv.org/pdf/2510.21773", "abs": "https://arxiv.org/abs/2510.21773", "authors": ["Van Nam Dinh"], "title": "Real-Time QP Solvers: A Concise Review and Practical Guide Towards Legged Robots", "categories": ["cs.RO"], "comment": "6 pages, 1 figure, 2 tables", "summary": "Quadratic programming (QP) underpins real-time robotics by enabling\nefficient, constrained optimization in state estimation, motion planning, and\ncontrol. In legged locomotion and manipulation, essential modules like inverse\ndynamics, Model Predictive Control (MPC), and Whole-Body Control (WBC) are\ninherently QP-based, demanding reliable solutions amid tight timing, energy,\nand computational limits on embedded platforms. This paper presents a\ncomprehensive analysis and benchmarking study of cutting-edge QP solvers for\nlegged robotics. We begin by formulating the standard convex QP and classify\nsolvers into four principal algorithmic approaches, including interior-point\nmethods, active-set strategies, operator splitting schemes, and augmented\nLagrangian approaches. Each solver is examined in terms of algorithmic\nstructure, computational characteristics, and its ability to exploit problem\nstructure and warm-starting. Performance is evaluated using publicly available\nbenchmarks, focusing on metrics such as computation time, constraint\nsatisfaction, and robustness under perturbations. Feature tables and\ncomparisons yield practical guidance for solver selection, underscoring\ntrade-offs in speed, accuracy, and energy efficiency. Our findings emphasize\nthe synergy between solver, task, and hardware, sparse IPMs for long-horizon\nMPC, and dense active-set for high frequency WBC to advance agile, autonomous\nlegged systems, with emerging extensions to nonconvex and distributed QP."}
{"id": "2510.22948", "pdf": "https://arxiv.org/pdf/2510.22948", "abs": "https://arxiv.org/abs/2510.22948", "authors": ["Zhaoming Hu", "Ruikang Zhong", "Xidong Mu", "Dengao Li", "Yuanwei Liu"], "title": "PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming", "categories": ["eess.SP", "cs.AI", "cs.NI"], "comment": null, "summary": "A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)\narchitecture is investigated to improve the task offloading efficiency and\nlatency performance in dynamic wireless environments. By leveraging dielectric\nwaveguides and flexibly adjustable pinching antennas, PASS establishes\nshort-distance line-of-sight (LoS) links while effectively mitigating the\nsignificant path loss and potential signal blockage, making it a promising\nsolution for high-frequency MEC systems. We formulate a network latency\nminimization problem to joint optimize uplink PASS beamforming and task\noffloading. The resulting problem is modeled as a Markov decision process (MDP)\nand solved via the deep reinforcement learning (DRL) method. To address the\ninstability introduced by the $\\max$ operator in the objective function, we\npropose a load balancing-aware proximal policy optimization (LBPPO) algorithm.\nLBPPO incorporates both node-level and waveguide-level load balancing\ninformation into the policy design, maintaining computational and transmission\ndelay equilibrium, respectively. Simulation results demonstrate that the\nproposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit\nstronger convergence capability than fixed-PA baselines and conventional\nMIMO-assisted MEC, especially in scenarios with a large number of UEs or high\ntransmit power."}
{"id": "2510.21817", "pdf": "https://arxiv.org/pdf/2510.21817", "abs": "https://arxiv.org/abs/2510.21817", "authors": ["Xiaoyu Liu", "Chaoyou Fu", "Chi Yan", "Chu Wu", "Haihan Gao", "Yi-Fan Zhang", "Shaoqi Dong", "Cheng Qian", "Bin Luo", "Xiuyong Yang", "Guanwu Li", "Yusheng Cai", "Yunhang Shen", "Deqiang Jiang", "Haoyu Cao", "Xing Sun", "Caifeng Shan", "Ran He"], "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting", "categories": ["cs.RO", "cs.CL", "cs.LG"], "comment": "Homepage: https://lxysl.github.io/VITA-E/", "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants."}
{"id": "2510.23021", "pdf": "https://arxiv.org/pdf/2510.23021", "abs": "https://arxiv.org/abs/2510.23021", "authors": ["Xibin Jin", "Guoliang Li", "Shuai Wang", "Fan Liu", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Planning Oriented Integrated Sensing and Communication", "categories": ["eess.SP", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency."}
{"id": "2510.21854", "pdf": "https://arxiv.org/pdf/2510.21854", "abs": "https://arxiv.org/abs/2510.21854", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "A Literature Review On Stewart-Gough Platform Calibrations A Literature Review On Stewart-Gough Platform Calibrations", "categories": ["cs.RO"], "comment": null, "summary": "Researchers have studied Stewart-Gough platforms, also known as Gough-Stewart\nplatforms or hexapod platforms extensively for their inherent fine control\ncharacteristics. Their studies led to the potential deployment opportunities of\nStewart-Gough Platforms in many critical applications such as the medical\nfield, engineering machines, space research, electronic chip manufacturing,\nautomobile manufacturing, etc. Some of these applications need micro and\nnano-level movement control in 3D space for the motions to be precise,\ncomplicated, and repeatable; a Stewart-Gough platform fulfills these challenges\nsmartly. For this, the platform must be more accurate than the specified\napplication accuracy level and thus proper calibration for a parallel robot is\ncrucial. Forward kinematics-based calibration for these hexapod machines\nbecomes unnecessarily complex and inverse kinematics complete this task with\nmuch ease. To experiment with different calibration techniques, various\ncalibration approaches were implemented by using external instruments,\nconstraining one or more motions of the system, and using extra sensors for\nauto or self-calibration. This survey paid attention to those key\nmethodologies, their outcome, and important details related to inverse\nkinematic-based parallel robot calibrations. It was observed during this study\nthat the researchers focused on improving the accuracy of the platform position\nand orientation considering the errors contributed by one source or multiple\nsources. The error sources considered are mainly kinematic and structural, in\nsome cases, environmental factors also are reviewed, however, those\ncalibrations are done under no-load conditions. This study aims to review the\npresent state of the art in this field and highlight the processes and errors\nconsidered for the calibration of Stewart-Gough platforms."}
{"id": "2510.23147", "pdf": "https://arxiv.org/pdf/2510.23147", "abs": "https://arxiv.org/abs/2510.23147", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "HAPS-ISAC for 6G: Architecture, Design Trade-offs, and a Practical Roadmap", "categories": ["eess.SP"], "comment": null, "summary": "To meet the ambitious goals of next-generation 6G networks, including\nultra-high data rates and ubiquitous coverage, we propose a novel high-altitude\nplatform station (HAPS)-based integrated sensing and communication (ISAC)\narchitecture. Operating in the stratosphere, the HAPS functions as both a\npowerful communication hub and an advanced environmental sensor. Combined with\na fleet of cooperative uncrewed aerial vehicles (UAVs), this dual-purpose\nsystem forms a scalable and intelligent 3D network. Simulation results indicate\nthat this approach significantly boosts network performance, improves sensing\naccuracy, and ensures a fairer service distribution across users, outperforming\nconventional UAV-only baselines. We conclude by outlining the prospective\napplications and a deployment roadmap for this technology for smart cities and\nother large-scale environments."}
{"id": "2510.21860", "pdf": "https://arxiv.org/pdf/2510.21860", "abs": "https://arxiv.org/abs/2510.21860", "authors": ["Callum Sharrock", "Lukas Petersson", "Hanna Petersson", "Axel Backlund", "Axel Wennström", "Kristoffer Nordström", "Elias Aronsson"], "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present Butter-Bench, a benchmark evaluating large language model (LLM)\ncontrolled robots for practical intelligence, defined as the ability to\nnavigate the messiness of the physical world. Current state-of-the-art robotic\nsystems use a hierarchical architecture with LLMs in charge of high-level\nreasoning, and a Vision Language Action (VLA) model for low-level control.\nButter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs\nhave repeatedly surpassed humans in evaluations requiring analytical\nintelligence, we find humans still outperform LLMs on Butter-Bench. The best\nLLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs\nstruggled the most with multi-step spatial planning and social understanding.\nWe also evaluate LLMs that are fine-tuned for embodied reasoning and conclude\nthat this training does not improve their score on Butter-Bench."}
{"id": "2510.23186", "pdf": "https://arxiv.org/pdf/2510.23186", "abs": "https://arxiv.org/abs/2510.23186", "authors": ["Lukas Henneke", "Frank Kurth"], "title": "Approaching Domain Generalization with Embeddings for Robust Discrimination and Recognition of RF Communication Signals", "categories": ["eess.SP"], "comment": null, "summary": "Radio frequency (RF) signal recognition plays a critical role in modern\nwireless communication and security applications. Deep learning-based\napproaches have achieved strong performance but typically rely heavily on\nextensive training data and often fail to generalize to unseen signals. In this\npaper, we propose a method to learn discriminative embeddings without relying\non real-world RF signal recordings by training on signals of synthetic wireless\nprotocols. We validate the approach on a dataset of real RF signals and show\nthat the learned embeddings capture features enabling accurate discrimination\nof previously unseen real-world signals, highlighting its potential for robust\nRF signal classification and anomaly detection."}
{"id": "2510.21874", "pdf": "https://arxiv.org/pdf/2510.21874", "abs": "https://arxiv.org/abs/2510.21874", "authors": ["Shuning Zhang"], "title": "A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 8 figures", "summary": "Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must\ngenerate safe and energy-efficient trajectories under physical and\nenvironmental constraints. Traditional planners, such as A* and kinodynamic\nRRT*, often yield suboptimal or non-smooth paths due to discretization and\nsampling limitations. This paper presents a physics-informed neural network\n(PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle\navoidance directly into the learning process. Without requiring supervised\ndata, the PINN learns dynamically feasible and collision-free trajectories by\nminimizing physical residuals and risk-aware objectives. Comparative\nsimulations show that the proposed method outperforms A* and Kino-RRT* in\ncontrol energy, smoothness, and safety margin, while maintaining similar flight\nefficiency. The results highlight the potential of physics-informed learning to\nunify model-based and data-driven planning, providing a scalable and physically\nconsistent framework for UAV trajectory optimization."}
{"id": "2510.23355", "pdf": "https://arxiv.org/pdf/2510.23355", "abs": "https://arxiv.org/abs/2510.23355", "authors": ["Pengyu Gao", "Qu Luo", "Jing Zhu", "Gaojie Chen", "Pei Xiao", "Chuan Heng Foh"], "title": "Uplink SCMA-empowered Uncoordinated Random Access for Future mMTC", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, a novel uncoordinated random access (URA) protocol is\npresented to address the pressing demand for massive connectivity with low\naccess latency in future massive machine type communication (mMTC) scenarios.\nThe proposed URA scheme integrates the classical slotted ALOHA (S-ALOHA)\nprotocol with sparse code multiple access (SCMA) technique, referred to as\nSCMA-empowered URA. Specifically, active users randomly choose an SCMA codebook\nto access the communication network in an arbitrary time slot whenever they\nwant without scheduling. However, due to the lack of central coordination in\nthe proposed URA scheme, SCMA codebook collisions become inevitable, making\ndecoding challenging and leading to increased access failures. To cope with the\ndecoding issue, an interference-canceling (IC) first decoding strategy is\nproposed at the access point (AP), which can partially tackles collision\nproblems, contributing to a higher system throughput. Taking the proposed\nIC-first decoding strategy into account, a closed-form theoretical expression\nof the throughput is derived. Moreover, to alleviate the throughput degradation\nunder the congested user traffic, a user barring mechanism is introduced to\nmanage the traffic load. Firstly, a closed-form expression of idle codebook\nprobability is developed to help indicate the system state, i.e., congested or\nnot. Then, in addition to the estimated real-time load, the AP adaptively\nadjusts the access probability and redistributes the actual access load.\nFinally, simulation results demonstrate that the proposed SCMA-empowered URA\nscheme enjoys higher maximum throughput, compared to the conventional\northogonal multiple access (OMA) based URA scheme. Moreover, the accuracy of\nthe presented theoretical analysis and the effectiveness of the user barring\nmechanism are verified."}
{"id": "2510.21991", "pdf": "https://arxiv.org/pdf/2510.21991", "abs": "https://arxiv.org/abs/2510.21991", "authors": ["Mateo Clemente", "Leo Brunswic", "Rui Heng Yang", "Xuan Zhao", "Yasser Khalil", "Haoyu Lei", "Amir Rasouli", "Yinchuan Li"], "title": "Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 68T07, 68U35"], "comment": "16 pages, 11 figure, 2 tables, accepted at Neurips 2025", "summary": "Diffusion models, such as diffusion policy, have achieved state-of-the-art\nresults in robotic manipulation by imitating expert demonstrations. While\ndiffusion models were originally developed for vision tasks like image and\nvideo generation, many of their inference strategies have been directly\ntransferred to control domains without adaptation. In this work, we show that\nby tailoring the denoising process to the specific characteristics of embodied\nAI tasks -- particularly structured, low-dimensional nature of action\ndistributions -- diffusion policies can operate effectively with as few as 5\nneural function evaluations (NFE).\n  Building on this insight, we propose a population-based sampling strategy,\ngenetic denoising, which enhances both performance and stability by selecting\ndenoising trajectories with low out-of-distribution risk. Our method solves\nchallenging tasks with only 2 NFE while improving or matching performance. We\nevaluate our approach across 14 robotic manipulation tasks from D4RL and\nRobomimic, spanning multiple action horizons and inference budgets. In over 2\nmillion evaluations, our method consistently outperforms standard\ndiffusion-based policies, achieving up to 20\\% performance gains with\nsignificantly fewer inference steps."}
{"id": "2510.23440", "pdf": "https://arxiv.org/pdf/2510.23440", "abs": "https://arxiv.org/abs/2510.23440", "authors": ["Donatella Darsena", "Ivan Iudice", "Vincenzo Galdi", "Francesco Verde"], "title": "Randomized Space-Time Coded Stacked Intelligent Metasurfaces for Massive Multiuser Downlink Connectivity", "categories": ["eess.SP"], "comment": "12 pages, 6 figures, 2 tables", "summary": "Stacked intelligent metasurfaces (SIMs) represent a key enabler for\nnext-generation wireless networks, offering beamforming gains while\nsignificantly reducing radio-frequency chain requirements. In conventional\nspace-only SIM architectures, the rate of reconfigurability of the SIM is equal\nto the inverse of the channel coherence time. This paper investigates a novel\nbeamforming strategy for massive downlink connectivity using a randomized\nspace-time (ST) coded SIM. In addition to conventional space-only metasurface\nlayers, the proposed design integrates a ST metasurface layer at the input\nstage of the SIM that introduces random time variations over each channel\ncoherence time interval. These artificial time variations enable opportunistic\nuser scheduling and exploitation of multiuser diversity under slow channel\ndynamics. To mitigate the prohibitive overhead associated with full channel\nstate information at the transmitter (CSIT), we propose a partial-CSIT-based\nbeamforming scheme that leverages randomized steering vectors and limited\nuser-side feedback based on signal quality measurements. Numerical results\ndemonstrate that the proposed ST-SIM architecture achieves satisfactory\nsum-rate performance while significantly reducing CSIT acquisition and feedback\noverhead, thereby enabling scalable downlink connectivity in dense networks."}
{"id": "2510.22030", "pdf": "https://arxiv.org/pdf/2510.22030", "abs": "https://arxiv.org/abs/2510.22030", "authors": ["Harsha Karunanayaka", "Siavash Rezazadeh"], "title": "Estimation of Minimum Stride Frequency for the Frontal Plane Stability of Bipedal Systems", "categories": ["cs.RO"], "comment": null, "summary": "Stability of bipedal systems in frontal plane is affected by the hip offset,\nto the extent that adjusting stride time using feedforward retraction and\nextension of the legs can lead to stable oscillations without feedback control.\nThis feedforward stabilization can be leveraged to reduce the control effort\nand energy expenditure and increase the locomotion robustness. However, there\nis limited understanding of how key parameters, such as mass, stiffness, leg\nlength, and hip width, affect stability and the minimum stride frequency needed\nto maintain it. This study aims to address these gaps through analyzing how\nindividual model parameters and the system's natural frequency influence the\nminimum stride frequency required to maintain a stable cycle. We propose a\nmethod to predict the minimum stride frequency, and compare the predicted\nstride frequencies with actual values for randomly generated models. The\nfindings of this work provide a better understanding of the frontal plane\nstability mechanisms and how feedforward stabilization can be leveraged to\nreduce the control effort."}
{"id": "2510.23467", "pdf": "https://arxiv.org/pdf/2510.23467", "abs": "https://arxiv.org/abs/2510.23467", "authors": ["Shreya Khisa", "Ali Amhaz", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Joint Uplink and Downlink Resource Allocation and Antenna Activation for Pinching Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we explore a novel joint uplink and downlink framework\nutilizing a pinching antenna system (PASS). We consider two waveguides, one\ndedicated to transmission and one to reception, and both of them are connected\nto a base station (BS). Each type of waveguide consists of several pinching\nantennas (PAs) in some preconfigured positions. In this framework, we assume\nthe BS can serve downlink and uplink user equipments (UEs) at the same time\nusing the same spectrum resources through the presented PASS. In this aspect,\nwe formulate a sum rate optimization problem that jointly optimizes the antenna\nactivation factor, the BS transmit power, and the UE's transmit power, subject\nto power budget constraints for the BS and the UEs, as well as minimum rate\nrequirements for the UEs. The formulated problem is highly non-convex and\ndifficult to solve directly. Hence, we divide the main problem into two\nsub-problems: the antenna activation sub-problem and the power allocation\nsub-problem. Then, we solve the antenna activation problem utilizing a distance\nand spatial correlation-based algorithm. Meanwhile, the resource allocation\nproblem is solved using a successive convex approximation (SCA)-based\nalgorithm. Numerical results show that our proposed framework can achieve\naround 60-90\\% performance gains over its time division duplex (TDD) where the\nuplink and downlink transmissions are served in different orthogonal time\nslots."}
{"id": "2510.22113", "pdf": "https://arxiv.org/pdf/2510.22113", "abs": "https://arxiv.org/abs/2510.22113", "authors": ["Zitiantao Lin", "Yongpeng Sang", "Yang Ye"], "title": "RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation", "categories": ["cs.RO", "cs.HC"], "comment": "5 pages, 5 figures; Accepted to: 2025 IEEE 4th International\n  Conference on Intelligent Reality (ICIR 2025); Zitiantao Lin and Yongpeng\n  Sang contributed equally to this work (co-first authors). Corresponding\n  author: Yang Ye (y.ye@northeastern.edu)", "summary": "Robotic manipulators are increasingly used to assist individuals with\nmobility impairments in object retrieval. However, the predominant\njoystick-based control interfaces can be challenging due to high precision\nrequirements and unintuitive reference frames. Recent advances in human-robot\ninteraction have explored alternative modalities, yet many solutions still rely\non external screens or restrictive control schemes, limiting their\nintuitiveness and accessibility. To address these challenges, we present an\negocentric, gaze-guided robotic manipulation interface that leverages a\nwearable Mixed Reality (MR) headset. Our system enables users to interact\nseamlessly with real-world objects using natural gaze fixation from a\nfirst-person perspective, while providing augmented visual cues to confirm\nintent and leveraging a pretrained vision model and robotic arm for intent\nrecognition and object manipulation. Experimental results demonstrate that our\napproach significantly improves manipulation accuracy, reduces system latency,\nand achieves single-pass intention and object recognition accuracy greater than\n88% across multiple real-world scenarios. These results demonstrate the\nsystem's effectiveness in enhancing intuitiveness and accessibility,\nunderscoring its practical significance for assistive robotics applications."}
{"id": "2510.22021", "pdf": "https://arxiv.org/pdf/2510.22021", "abs": "https://arxiv.org/abs/2510.22021", "authors": ["Masoud Ataei", "Vikas Dhiman", "Mohammad Javad Khojasteh"], "title": "K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "Accepted at IEEE ACSSC, 9 pages and 3 figures", "summary": "Neural networks are parametric and powerful tools for function approximation,\nand the choice of architecture heavily influences their interpretability,\nefficiency, and generalization. In contrast, Gaussian processes (GPs) are\nnonparametric probabilistic models that define distributions over functions\nusing a kernel to capture correlations among data points. However, these models\nbecome computationally expensive for large-scale problems, as they require\ninverting a large covariance matrix. Kolmogorov- Arnold networks (KANs),\nsemi-parametric neural architectures, have emerged as a prominent approach for\nmodeling complex functions with structured and efficient representations\nthrough spline layers. Kurkova Kolmogorov-Arnold networks (KKANs) extend this\nidea by reducing the number of spline layers in KAN and replacing them with\nChebyshev layers and multi-layer perceptrons, thereby mapping inputs into\nhigher-dimensional spaces before applying spline-based transformations.\nCompared to KANs, KKANs perform more stable convergence during training, making\nthem a strong architecture for estimating operators and system modeling in\ndynamical systems. By enhancing the KKAN architecture, we develop a novel\nlearning algorithm, distance-aware error for Kurkova-Kolmogorov networks\n(K-DAREK), for efficient and interpretable function approximation with\nuncertainty quantification. Our approach establishes robust error bounds that\nare distance-aware; this means they reflect the proximity of a test point to\nits nearest training points. Through case studies on a safe control task, we\ndemonstrate that K-DAREK is about four times faster and ten times higher\ncomputationally efficiency than Ensemble of KANs, 8.6 times more scalable than\nGP by increasing the data size, and 50% safer than our previous work\ndistance-aware error for Kolmogorov networks (DAREK)."}
{"id": "2510.22126", "pdf": "https://arxiv.org/pdf/2510.22126", "abs": "https://arxiv.org/abs/2510.22126", "authors": ["Guanwen Xie", "Jingzehua Xu", "Jiwei Tang", "Yubo Huang", "Shuai Zhang", "Xiaofan Li"], "title": "EasyUUV: An LLM-Enhanced Universal and Lightweight Sim-to-Real Reinforcement Learning Framework for UUV Attitude Control", "categories": ["cs.RO"], "comment": "8 pages, 15 figures", "summary": "Despite recent advances in Unmanned Underwater Vehicle (UUV) attitude\ncontrol, existing methods still struggle with generalizability, robustness to\nreal-world disturbances, and efficient deployment. To address the above\nchallenges, this paper presents EasyUUV, a Large Language Model (LLM)-enhanced,\nuniversal, and lightweight simulation-to-reality reinforcement learning (RL)\nframework for robust attitude control of UUVs. EasyUUV combines parallelized RL\ntraining with a hybrid control architecture, where a learned policy outputs\nhigh-level attitude corrections executed by an adaptive S-Surface controller. A\nmultimodal LLM is further integrated to adaptively tune controller parameters\nat runtime using visual and textual feedback, enabling training-free adaptation\nto unmodeled dynamics. Also, we have developed a low-cost 6-DoF UUV platform\nand applied an RL policy trained through efficient parallelized simulation.\nExtensive simulation and real-world experiments validate the effectiveness and\noutstanding performance of EasyUUV in achieving robust and adaptive UUV\nattitude control across diverse underwater conditions. The source code is\navailable from the following website: https://360zmem.github.io/easyuuv/"}
{"id": "2510.22154", "pdf": "https://arxiv.org/pdf/2510.22154", "abs": "https://arxiv.org/abs/2510.22154", "authors": ["Yunhong Tao", "Wenbing Tao", "Xiang Xiang"], "title": "Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.MM", "eess.SP"], "comment": null, "summary": "Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nWith the advent of deep learning, the LLIE technique has achieved significant\nbreakthroughs. However, existing LLIE methods either ignore the important role\nof frequency domain information or fail to effectively promote the propagation\nand flow of information, limiting the LLIE performance. In this paper, we\ndevelop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE\nbased on two-stage architecture. To be specific, the first stage is designed to\nrestore the amplitude of low-light images to improve the lightness, and the\nsecond stage devotes to restore phase information to refine fine-grained\nstructures. Considering that Frequency domain and spatial domain information\nare complementary and both favorable for LLIE, we further develop two\nfrequency-spatial interaction blocks which mutually amalgamate the\ncomplementary spatial and frequency information to enhance the capability of\nthe model. In addition, we construct the Information Exchange Module (IEM) to\nassociate two stages by adequately incorporating cross-stage and cross-scale\nfeatures to effectively promote the propagation and flow of information in the\ntwo-stage network structure. Finally, we conduct experiments on several widely\nused benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate\nthat our method achieves the excellent performance in terms of visual results\nand quantitative metrics while preserving good model efficiency."}
{"id": "2510.22164", "pdf": "https://arxiv.org/pdf/2510.22164", "abs": "https://arxiv.org/abs/2510.22164", "authors": ["Jianeng Wang", "Matias Mattamala", "Christina Kassab", "Nived Chebrolu", "Guillaume Burger", "Fabio Elnecave", "Marine Petriaux", "Maurice Fallon"], "title": "LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong Safe Navigation of Exoskeletons", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Self-balancing exoskeletons offer a promising mobility solution for\nindividuals with lower-limb disabilities. For reliable long-term operation,\nthese exoskeletons require a perception system that is effective in changing\nenvironments. In this work, we introduce LT-Exosense, a vision-centric,\nmulti-session mapping system designed to support long-term (semi)-autonomous\nnavigation for exoskeleton users. LT-Exosense extends single-session mapping\ncapabilities by incrementally fusing spatial knowledge across multiple\nsessions, detecting environmental changes, and updating a persistent global\nmap. This representation enables intelligent path planning, which can adapt to\nnewly observed obstacles and can recover previous routes when obstructions are\nremoved. We validate LT-Exosense through several real-world experiments,\ndemonstrating a scalable multi-session map that achieves an average\npoint-to-point error below 5 cm when compared to ground-truth laser scans. We\nalso illustrate the potential application of adaptive path planning in\ndynamically changing indoor environments."}
{"id": "2510.22230", "pdf": "https://arxiv.org/pdf/2510.22230", "abs": "https://arxiv.org/abs/2510.22230", "authors": ["Ziqi Diao", "Xingyu Zhou", "Le Liang", "Shi Jin"], "title": "Robust MIMO Channel Estimation Using Energy-Based Generative Diffusion Models", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "5 pages, 4 figures, 1 table. This work has been submitted to the IEEE\n  for possible publication", "summary": "Channel estimation for massive multiple-input multiple-output (MIMO) systems\nis fundamentally constrained by excessive pilot overhead and high estimation\nlatency. To overcome these obstacles, recent studies have leveraged deep\ngenerative networks to capture the prior distribution of wireless channels. In\nthis paper, we propose a novel estimation framework that integrates an\nenergy-based generative diffusion model (DM) with the Metropolis-Hastings (MH)\nprinciple. By reparameterizing the diffusion process with an incorporated\nenergy function, the framework explicitly estimates the unnormalized log-prior,\nwhile MH corrections refine the sampling trajectory, mitigate deviations, and\nenhance robustness, ultimately enabling accurate posterior sampling for\nhigh-fidelity channel estimation. Numerical results reveal that the proposed\napproach significantly improves estimation accuracy compared with conventional\nparameterized DMs and other baseline methods, particularly in cases with\nlimited pilot overhead."}
{"id": "2510.22201", "pdf": "https://arxiv.org/pdf/2510.22201", "abs": "https://arxiv.org/abs/2510.22201", "authors": ["Minho Park", "Kinam Kim", "Junha Hyung", "Hyojin Jang", "Hoiyeong Jin", "Jooyeol Yun", "Hojoon Lee", "Jaegul Choo"], "title": "ACG: Action Coherence Guidance for Flow-based VLA models", "categories": ["cs.RO"], "comment": null, "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively."}
{"id": "2510.22364", "pdf": "https://arxiv.org/pdf/2510.22364", "abs": "https://arxiv.org/abs/2510.22364", "authors": ["Samir Damji", "Simrut Kurry", "Shazia'Ayn Babul", "Joydeep Bhattacharya", "Naznin Virji-Babul"], "title": "Tuned for Creativity? Graph-Theoretical Mapping of Resting-State EEG Reveals Neural Signatures of Creativity", "categories": ["q-bio.NC", "eess.SP", "q-bio.QM"], "comment": "27 pages, 6 figures, 2 tables", "summary": "Understanding how creativity is represented in the brain's intrinsic\nfunctional architecture remains a central challenge in cognitive neuroscience.\nWhile resting-state fMRI studies have revealed large-scale network correlates\nof creative potential, electroencephalography (EEG) offers a temporally precise\nand scalable approach to capture the fast oscillatory dynamics that underlie\nspontaneous neural organization. In this study, we used a data-driven network\napproach to examine whether resting-state EEG connectivity patterns\ndifferentiate individuals according to their creative abilities. Creativity was\nevaluated by: The Inventory of Creative Activities and Achievements (ICAA), The\nDivergent Association Task (DAT), The Matchstick Arithmetic Puzzles Task (MAPT)\nand Self-rating (SR) of creative ability in 30 healthy young adults.\nGraph-theoretical analyses were applied to functional connectivity matrices and\nclustered based on graph similarity. Two distinct participant clusters emerged,\ndiffering systematically across multiple dimensions of creativity. Cluster 1,\ncharacterized by consistently higher performance across multiple creativity\nvariables (ICAA, DAT, MAPT and SR), showed broad alpha-band hypoconnectivity,\nrelatively preserved left frontal connectivity and greater network modularity.\nCluster 0, associated with lower creativity scores, exhibited stronger overall\nconnectivity strength, reduced modularity and higher local clustering. These\nfindings suggest that resting-state EEG connectivity patterns can index stable\ncognitive traits such as creativity. More broadly, they point to an intrinsic\nneural signature of adaptive brain function marked by efficient yet flexible\nnetwork organization that may support creative and adaptive cognition."}
{"id": "2510.22204", "pdf": "https://arxiv.org/pdf/2510.22204", "abs": "https://arxiv.org/abs/2510.22204", "authors": ["Weixian Qian", "Sebastian Schroder", "Yao Deng", "Jiaohong Yao", "Linfeng Liang", "Xiao Cheng", "Richard Han", "Xi Zheng"], "title": "Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous landing in unstructured (cluttered, uneven, and map-poor)\nenvironments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet\npurely vision-based or deep learning models often falter under covariate shift\nand provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic\nframework that tightly couples two complementary pipelines: (i) an offline\npipeline, where Large Language Models (LLMs) and human-in-the-loop refinement\nsynthesize Scallop code from diverse landing scenarios, distilling\ngeneralizable and verifiable symbolic knowledge; and (ii) an online pipeline,\nwhere a compact foundation-based semantic segmentation model generates\nprobabilistic Scallop facts that are composed into semantic scene graphs for\nreal-time deductive reasoning. This design combines the perceptual strengths of\nlightweight foundation models with the interpretability and verifiability of\nsymbolic reasoning. Node attributes (e.g., flatness, area) and edge relations\n(adjacency, containment, proximity) are computed with geometric routines rather\nthan learned, avoiding the data dependence and latency of train-time graph\nbuilders. The resulting Scallop program encodes landing principles (avoid water\nand obstacles; prefer large, flat, accessible regions) and yields calibrated\nsafety scores with ranked Regions of Interest (ROIs) and human-readable\njustifications. Extensive evaluations across datasets, diverse simulation maps,\nand real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger\nrobustness to covariate shift, and superior efficiency compared with\nstate-of-the-art baselines, while advancing UAV safety and reliability in\nemergency response, surveillance, and delivery missions."}
{"id": "2510.22637", "pdf": "https://arxiv.org/pdf/2510.22637", "abs": "https://arxiv.org/abs/2510.22637", "authors": ["Yuval Bar Ilan", "Boaz Rafaely", "Vladimir Tourbabin"], "title": "HyBeam: Hybrid Microphone-Beamforming Array-Agnostic Speech Enhancement for Wearables", "categories": ["eess.AS", "eess.SP"], "comment": null, "summary": "Speech enhancement is a fundamental challenge in signal processing,\nparticularly when robustness is required across diverse acoustic conditions and\nmicrophone setups. Deep learning methods have been successful for speech\nenhancement, but often assume fixed array geometries, limiting their use in\nmobile, embedded, and wearable devices. Existing array-agnostic approaches\ntypically rely on either raw microphone signals or beamformer outputs, but both\nhave drawbacks under changing geometries. We introduce HyBeam, a hybrid\nframework that uses raw microphone signals at low frequencies and beamformer\nsignals at higher frequencies, exploiting their complementary strengths while\nremaining highly array-agnostic. Simulations across diverse rooms and wearable\narray configurations demonstrate that HyBeam consistently surpasses\nmicrophone-only and beamformer-only baselines in PESQ, STOI, and SI-SDR. A\nbandwise analysis shows that the hybrid approach leverages beamformer\ndirectivity at high frequencies and microphone cues at low frequencies,\noutperforming either method alone across all bands."}
{"id": "2510.22313", "pdf": "https://arxiv.org/pdf/2510.22313", "abs": "https://arxiv.org/abs/2510.22313", "authors": ["Chen Zhiqiang", "Le Gentil Cedric", "Lin Fuling", "Lu Minghao", "Qiao Qiyuan", "Xu Bowen", "Qi Yuhua", "Lu Peng"], "title": "Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in\ndynamic environments, where conventional methods often fail due to their\nstatic-world assumptions. Traditional LIO algorithms perform poorly when\ndynamic objects dominate the scenes, particularly in geometrically sparse\nenvironments. Current approaches to dynamic LIO face a fundamental challenge:\naccurate localization requires a reliable identification of static features,\nyet distinguishing dynamic objects necessitates precise pose estimation. Our\nsolution breaks this circular dependency by integrating dynamic awareness\ndirectly into the point cloud registration process. We introduce a novel\ndynamic-aware iterative closest point algorithm that leverages spatio-temporal\nnormal analysis, complemented by an efficient spatial consistency verification\nmethod to enhance static map construction. Experimental evaluations demonstrate\nsignificant performance improvements over state-of-the-art LIO systems in\nchallenging dynamic environments with limited geometric structure. The code and\ndataset are available at https://github.com/thisparticle/btsa."}
{"id": "2510.23078", "pdf": "https://arxiv.org/pdf/2510.23078", "abs": "https://arxiv.org/abs/2510.23078", "authors": ["Phonepaserth Sisaykeo", "Shogo Muramatsu"], "title": "Numerical Spectrum Linking: Identification of Governing PDE via Koopman-Chebyshev Approximation", "categories": ["math.NA", "cs.NA", "eess.SP"], "comment": "Submitted to IEEE ICASSP 2026", "summary": "A numerical framework is proposed for identifying partial differential\nequations (PDEs) governing dynamical systems directly from their observation\ndata using Chebyshev polynomial approximation. In contrast to data-driven\napproaches such as dynamic mode decomposition (DMD), which approximate the\nKoopman operator without a clear connection to differential operators, the\nproposed method constructs finite-dimensional Koopman matrices by projecting\nthe dynamics onto a Chebyshev basis, thereby capturing both differential and\nnonlinear terms. This establishes a numerical link between the Koopman and\ndifferential operators. Numerical experiments on benchmark dynamical systems\nconfirm the accuracy and efficiency of the approach, underscoring its potential\nfor interpretable operator learning. The framework also lays a foundation for\nfuture integration with symbolic regression, enabling the construction of\nexplicit mathematical models directly from data."}
{"id": "2510.22336", "pdf": "https://arxiv.org/pdf/2510.22336", "abs": "https://arxiv.org/abs/2510.22336", "authors": ["Bo Yue", "Sheng Xu", "Kui Jia", "Guiliang Liu"], "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that \\ourmethod{} achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design."}
{"id": "2510.23235", "pdf": "https://arxiv.org/pdf/2510.23235", "abs": "https://arxiv.org/abs/2510.23235", "authors": ["Anton Savostianov", "Michael T. Schaub", "Benjamin Stamm"], "title": "Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications", "categories": ["cs.LG", "cs.NA", "cs.SI", "eess.SP", "math.NA", "math.SP"], "comment": "13 pages", "summary": "Low-pass graph filters are fundamental for signal processing on graphs and\nother non-Euclidean domains. However, the computation of such filters for\nparametric graph families can be prohibitively expensive as computation of the\ncorresponding low-frequency subspaces, requires the repeated solution of an\neigenvalue problem. We suggest a novel algorithm of low-pass graph filter\ninterpolation based on Riemannian interpolation in normal coordinates on the\nGrassmann manifold. We derive an error bound estimate for the subspace\ninterpolation and suggest two possible applications for induced parametric\ngraph families. First, we argue that the temporal evolution of the node\nfeatures may be translated to the evolving graph topology via a similarity\ncorrection to adjust the homophily degree of the network. Second, we suggest a\ndot product graph family induced by a given static graph which allows to infer\nimproved message passing scheme for node classification facilitated by the\nfilter interpolation."}
{"id": "2510.22339", "pdf": "https://arxiv.org/pdf/2510.22339", "abs": "https://arxiv.org/abs/2510.22339", "authors": ["Enyi Wang", "Zhen Deng", "Chuanchuan Pan", "Bingwei He", "Jianwei Zhang"], "title": "Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks", "categories": ["cs.RO"], "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "This paper presents a learning-based approach for accurately estimating the\n3D shape of flexible continuum robots subjected to external loads. The proposed\nmethod introduces a spatiotemporal neural network architecture that fuses\nmulti-modal inputs, including current and historical tendon displacement data\nand RGB images, to generate point clouds representing the robot's deformed\nconfiguration. The network integrates a recurrent neural module for temporal\nfeature extraction, an encoding module for spatial feature extraction, and a\nmulti-modal fusion module to combine spatial features extracted from visual\ndata with temporal dependencies from historical actuator inputs. Continuous 3D\nshape reconstruction is achieved by fitting B\\'ezier curves to the predicted\npoint clouds. Experimental validation demonstrates that our approach achieves\nhigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and\n0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing for\nTDCRs. The results validate the efficacy of deep learning-based spatiotemporal\ndata fusion for precise shape estimation under loading conditions."}
{"id": "2510.23416", "pdf": "https://arxiv.org/pdf/2510.23416", "abs": "https://arxiv.org/abs/2510.23416", "authors": ["Marco Antonio Ortiz Rincon", "Yihui Yang", "Christoph Holst"], "title": "Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation", "categories": ["cs.CV", "eess.SP"], "comment": "10 pages, 7 figures. This manuscript is currently under review at the\n  International Journal of Applied Earth Observation and Geoinformation\n  (Elsevier). A preprint version will also be available on SSRN (Elsevier\n  Preprints) with a DOI once processed. This is the original preprint version\n  submitted for peer review", "summary": "This study presents a novel workflow designed to efficiently and accurately\nregister large-scale mobile laser scanning (MLS) point clouds to a target model\npoint cloud in urban street scenarios. This workflow specifically targets the\ncomplexities inherent in urban environments and adeptly addresses the\nchallenges of integrating point clouds that vary in density, noise\ncharacteristics, and occlusion scenarios, which are common in bustling city\ncenters. Two methodological advancements are introduced. First, the proposed\nSemi-sphere Check (SSC) preprocessing technique optimally fragments MLS\ntrajectory data by identifying mutually orthogonal planar surfaces. This step\nreduces the impact of MLS drift on the accuracy of the entire point cloud\nregistration, while ensuring sufficient geometric features within each fragment\nto avoid local minima. Second, we propose Planar Voxel-based Generalized\nIterative Closest Point (PV-GICP), a fine registration method that selectively\nutilizes planar surfaces within voxel partitions. This pre-process strategy not\nonly improves registration accuracy but also reduces computation time by more\nthan 50% compared to conventional point-to-plane ICP methods. Experiments on\nreal-world datasets from Munich's inner city demonstrate that our workflow\nachieves sub-0.01 m average registration accuracy while significantly\nshortening processing times. The results underscore the potential of the\nproposed methods to advance automated 3D urban modeling and updating, with\ndirect applications in urban planning, infrastructure management, and dynamic\ncity monitoring."}
{"id": "2510.22370", "pdf": "https://arxiv.org/pdf/2510.22370", "abs": "https://arxiv.org/abs/2510.22370", "authors": ["Seyed Ahmad Hosseini Miangoleh", "Amin Jalal Aghdasian", "Farzaneh Abdollahi"], "title": "BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SE"], "comment": "https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git", "summary": "In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available."}
{"id": "2510.23503", "pdf": "https://arxiv.org/pdf/2510.23503", "abs": "https://arxiv.org/abs/2510.23503", "authors": ["Fatemeh Zahra Safaeipour", "Jacob Chakareski", "Morteza Hashemi"], "title": "Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems", "categories": ["cs.DC", "cs.LG", "eess.SP"], "comment": null, "summary": "Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely\ninference tasks while operating with limited on-board computing and energy\nresources. In this paper, we investigate the problem of collaborative inference\nin wireless edge networks, where energy-constrained edge devices aim to\ncomplete inference tasks within given deadlines. These tasks are carried out\nusing neural networks, and the edge device seeks to optimize inference\nperformance under energy and delay constraints. The inference process can be\nsplit between the edge device and an edge server, thereby achieving\ncollaborative inference over wireless networks. We formulate an inference\nutility optimization problem subject to energy and delay constraints, and\npropose a novel solution called Bayes-Split-Edge, which leverages Bayesian\noptimization for collaborative split inference over wireless edge networks. Our\nsolution jointly optimizes the transmission power and the neural network split\npoint. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition\nfunction that balances inference task utility, sample efficiency, and\nconstraint violation penalties. We evaluate our approach using the VGG19 model\non the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world\nmMobile wireless channel datasets. Numerical results demonstrate that\nBayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to\nstandard Bayesian optimization and achieves near-linear convergence. It also\noutperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and\nProximal Policy Optimization (PPO), while matching exhaustive search\nperformance under tight constraints. These results confirm that the proposed\nframework provides a sample-efficient solution requiring maximum 20 function\nevaluations and constraint-aware optimization for wireless split inference in\nedge computing systems."}
{"id": "2510.22420", "pdf": "https://arxiv.org/pdf/2510.22420", "abs": "https://arxiv.org/abs/2510.22420", "authors": ["Mohammad Ali Labbaf Khaniki", "Fateme Taroodi", "Benyamin Safizadeh"], "title": "A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Controlling high-dimensional stochastic systems, critical in robotics,\nautonomous vehicles, and hyperchaotic systems, faces the curse of\ndimensionality, lacks temporal abstraction, and often fails to ensure\nstochastic stability. To overcome these limitations, this study introduces the\nMulti-Timescale Lyapunov-Constrained Hierarchical Reinforcement Learning\n(MTLHRL) framework. MTLHRL integrates a hierarchical policy within a\nsemi-Markov Decision Process (SMDP), featuring a high-level policy for\nstrategic planning and a low-level policy for reactive control, which\neffectively manages complex, multi-timescale decision-making and reduces\ndimensionality overhead. Stability is rigorously enforced using a neural\nLyapunov function optimized via Lagrangian relaxation and multi-timescale\nactor-critic updates, ensuring mean-square boundedness or asymptotic stability\nin the face of stochastic dynamics. The framework promotes efficient and\nreliable learning through trust-region constraints and decoupled optimization.\nExtensive simulations on an 8D hyperchaotic system and a 5-DOF robotic\nmanipulator demonstrate MTLHRL's empirical superiority. It significantly\noutperforms baseline methods in both stability and performance, recording the\nlowest error indices (e.g., Integral Absolute Error (IAE): 3.912 in\nhyperchaotic control and IAE: 1.623 in robotics), achieving faster convergence,\nand exhibiting superior disturbance rejection. MTLHRL offers a theoretically\ngrounded and practically viable solution for robust control of complex\nstochastic systems."}
{"id": "2510.22448", "pdf": "https://arxiv.org/pdf/2510.22448", "abs": "https://arxiv.org/abs/2510.22448", "authors": ["Pranup Chhetri", "Alejandro Torrejon", "Sergio Eslava", "Luis J. Manso"], "title": "A short methodological review on social robot navigation benchmarking", "categories": ["cs.RO", "I.2.9"], "comment": "18 pages, 14 of which references. 3 figures, 2 tables", "summary": "Social Robot Navigation is the skill that allows robots to move efficiently\nin human-populated environments while ensuring safety, comfort, and trust.\nUnlike other areas of research, the scientific community has not yet achieved\nan agreement on how Social Robot Navigation should be benchmarked. This is\nnotably important, as the lack of a de facto standard to benchmark Social Robot\nNavigation can hinder the progress of the field and may lead to contradicting\nconclusions. Motivated by this gap, we contribute with a short review focused\nexclusively on benchmarking trends in the period from January 2020 to July\n2025. Of the 130 papers identified by our search using IEEE Xplore, we analysed\nthe 85 papers that met the criteria of the review. This review addresses the\nmetrics used in the literature for benchmarking purposes, the algorithms\nemployed in such benchmarks, the use of human surveys for benchmarking, and how\nconclusions are drawn from the benchmarking results, when applicable."}
{"id": "2510.22465", "pdf": "https://arxiv.org/pdf/2510.22465", "abs": "https://arxiv.org/abs/2510.22465", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "Forward Kinematics Solution For A General Stewart Platform Through Iteration Based Simulation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a method to generate feasible, unique forward-kinematic\nsolutions for a general Stewart platform. This is done by using inverse\nkinematics to obtain valid workspace data and corresponding actuator lengths\nfor the moving platform. For parallel kinematic machines, such as the Stewart\nPlatform, inverse kinematics are straight forward, but the forward kinematics\nare complex and generates multiple solutions due to the closed loop structure\nof the kinematic links. In this research, a simple iterative algorithm has been\nused employing modified Denavit-Hartenberg convention. The outcome is\nencouraging as this method generates a single feasible forward kinematic\nsolution for each valid pose with the solved DH parameters and unlike earlier\nforward kinematics solutions, this unique solution does not need to be manually\nverified. Therefore, the forward kinematic solutions can be used directly for\nfurther calculations without the need for manual pose verification. This\ncapability is essential for the six degree of freedom materials testing system\ndeveloped by the authors in their laboratory. The developed system is aimed at\ncharacterizing additively manufactured materials under complex combined\nmultiple loading conditions. The material characterization is done by enabling\nhigh precision force control on the moving platform via in situ calibration of\nthe as-built kinematics of the Stewart Gough Platform."}
{"id": "2510.22504", "pdf": "https://arxiv.org/pdf/2510.22504", "abs": "https://arxiv.org/abs/2510.22504", "authors": ["Ciera McFarland", "Antonio Alvarez", "Sarah Taher", "Nathaniel Hanson", "Margaret McGuinness"], "title": "On Steerability Factors for Growing Vine Robots", "categories": ["cs.RO"], "comment": null, "summary": "Vine robots extend their tubular bodies by everting material from the tip,\nenabling navigation in complex environments with a minimalist soft body.\nDespite their promise for field applications, especially in the urban search\nand rescue domain, performance is constrained by the weight of attached sensors\nor tools, as well as other design and control choices. This work investigates\nhow tip load, pressure, length, diameter, and fabrication method shape vine\nrobot steerability--the ability to maneuver with controlled curvature--for\nrobots that steer with series pouch motor-style pneumatic actuators. We conduct\ntwo groups of experiments: (1) studying tip load, chamber pressure, length, and\ndiameter in a robot supporting itself against gravity, and (2) studying\nfabrication method and ratio of actuator to chamber pressure in a robot\nsupported on the ground. Results show that steerability decreases with\nincreasing tip load, is best at moderate chamber pressure, increases with\nlength, and is largely unaffected by diameter. Robots with actuators attached\non their exterior begin curving at low pressure ratios, but curvature saturates\nat high pressure ratios; those with actuators integrated into the robot body\nrequire higher pressure ratios to begin curving but achieve higher curvature\noverall. We demonstrate that robots optimized with these principles outperform\nthose with ad hoc parameters in a mobility task that involves maximizing upward\nand horizontal curvatures."}
{"id": "2510.22524", "pdf": "https://arxiv.org/pdf/2510.22524", "abs": "https://arxiv.org/abs/2510.22524", "authors": ["Shenbagaraj Kannapiran", "Elena Oikonomou", "Albert Chu", "Spring Berman", "Theodore P. Pavlic"], "title": "Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines", "categories": ["cs.RO"], "comment": null, "summary": "In natural systems, emergent structures often arise to balance competing\ndemands. Army ants, for example, form temporary \"walls\" that prevent\ninterference between foraging trails. Inspired by this behavior, we developed\ntwo decentralized controllers for heterogeneous robotic swarms to maintain\nspatial separation while executing concurrent tasks. The first is a\nfinite-state machine (FSM)-based controller that uses encounter-triggered\ntransitions to create rigid, stable walls. The second integrates FSM states\nwith a Deep Q-Network (DQN), dynamically optimizing separation through emergent\n\"demilitarized zones.\" In simulation, both controllers reduce mixing between\nsubgroups, with the DQN-enhanced controller improving adaptability and reducing\nmixing by 40-50% while achieving faster convergence."}
{"id": "2510.22568", "pdf": "https://arxiv.org/pdf/2510.22568", "abs": "https://arxiv.org/abs/2510.22568", "authors": ["Onur Akgün"], "title": "SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for\nLearning), a novel approach for training autonomous drones in multi-agent\nracing competitions. SPIRAL distinctively employs a self-play mechanism to\nincrementally cultivate complex racing behaviors within a challenging, dynamic\nenvironment. Through this self-play core, drones continuously compete against\nincreasingly proficient versions of themselves, naturally escalating the\ndifficulty of competitive interactions. This progressive learning journey\nguides agents from mastering fundamental flight control to executing\nsophisticated cooperative multi-drone racing strategies. Our method is designed\nfor versatility, allowing integration with any state-of-the-art Deep\nReinforcement Learning (DRL) algorithms within its self-play framework.\nSimulations demonstrate the significant advantages of SPIRAL and benchmark the\nperformance of various DRL algorithms operating within it. Consequently, we\ncontribute a versatile, scalable, and self-improving learning framework to the\nfield of autonomous drone racing. SPIRAL's capacity to autonomously generate\nappropriate and escalating challenges through its self-play dynamic offers a\npromising direction for developing robust and adaptive racing strategies in\nmulti-agent environments. This research opens new avenues for enhancing the\nperformance and reliability of autonomous racing drones in increasingly complex\nand competitive scenarios."}
{"id": "2510.22570", "pdf": "https://arxiv.org/pdf/2510.22570", "abs": "https://arxiv.org/abs/2510.22570", "authors": ["Onur Akgün"], "title": "Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "comment": "13 pages, 5 figures. This paper is currently under review at the\n  journal Engineering Applications of Artificial Intelligence. Supplementary\n  video: https://drive.google.com/file/d/1k7necen2DgIxaYT2alKK8-b20sE_AyDA/view\n  Source code and models: https://doi.org/10.5281/zenodo.17256943", "summary": "The coordination of multiple autonomous agents in high-speed, competitive\nenvironments represents a significant engineering challenge. This paper\npresents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone\nRacing), a reinforcement learning framework designed to solve this challenge in\nthe demanding domain of multi-drone racing. CRUISE overcomes key scalability\nlimitations by synergistically combining a progressive difficulty curriculum\nwith an efficient self-play mechanism to foster robust competitive behaviors.\nValidated in high-fidelity simulation with realistic quadrotor dynamics, the\nresulting policies significantly outperform both a standard reinforcement\nlearning baseline and a state-of-the-art game-theoretic planner. CRUISE\nachieves nearly double the planner's mean racing speed, maintains high success\nrates, and demonstrates robust scalability as agent density increases. Ablation\nstudies confirm that the curriculum structure is the critical component for\nthis performance leap. By providing a scalable and effective training\nmethodology, CRUISE advances the development of autonomous systems for dynamic,\ncompetitive tasks and serves as a blueprint for future real-world deployment."}
{"id": "2510.22600", "pdf": "https://arxiv.org/pdf/2510.22600", "abs": "https://arxiv.org/abs/2510.22600", "authors": ["Huilin Yin", "Zhaolin Yang", "Linchuan Zhang", "Gerhard Rigoll", "Johannes Betz"], "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience", "categories": ["cs.RO", "cs.AI"], "comment": "13 pages, 11 figures, under review", "summary": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely\nconstrained in environments where visual inputs suffer from noise and low\nillumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM\nframeworks achieve high-fidelity mapping under clean conditions, they remain\nvulnerable to compounded degradations that degrade mapping and tracking\nperformance. A key observation underlying our work is that the original 3DGS\nrendering pipeline inherently behaves as an implicit low-pass filter,\nattenuating high-frequency noise but also risking over-smoothing. Building on\nthis insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for\nnoise and low-light resilience. The framework integrates three innovations: a\nStructure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples\nrendered appearance, depth, and edge cues; an adaptive tracking objective with\nresidual balancing regularization; and a Contrastive Language-Image Pretraining\n(CLIP)-based enhancement module, selectively activated under compounded\ndegradations to restore semantic and structural fidelity. Comprehensive\nexperiments on Replica, TUM, and real-world sequences show that RoGER-SLAM\nconsistently improves trajectory accuracy and reconstruction quality compared\nwith other 3DGS-SLAM systems, especially under adverse imaging conditions."}
{"id": "2510.22680", "pdf": "https://arxiv.org/pdf/2510.22680", "abs": "https://arxiv.org/abs/2510.22680", "authors": ["Shireen Kudukkil Manchingal", "Armand Amaritei", "Mihir Gohad", "Maryam Sultana", "Julian F. P. Kooij", "Fabio Cuzzolin", "Andrew Bradley"], "title": "Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous Vehicle (AV) perception systems have advanced rapidly in recent\nyears, providing vehicles with the ability to accurately interpret their\nenvironment. Perception systems remain susceptible to errors caused by\noverly-confident predictions in the case of rare events or out-of-sample data.\nThis study equips an autonomous vehicle with the ability to 'know when it is\nuncertain', using an uncertainty-aware image classifier as part of the AV\nsoftware stack. Specifically, the study exploits the ability of Random-Set\nNeural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike\ntraditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets\nof classes, allowing the system to identify and signal uncertainty clearly in\nnovel or ambiguous scenarios. The system is tested in a real-world autonomous\nracing vehicle software stack, with the RS-NN classifying the layout of the\nroad ahead and providing the associated uncertainty of the prediction.\nPerformance of the RS-NN under a range of road conditions is compared against\ntraditional CNN and Bayesian neural networks, with the RS-NN achieving\nsignificantly higher accuracy and superior uncertainty calibration. This\nintegration of RS-NNs into Robot Operating System (ROS)-based vehicle control\npipeline demonstrates that predictive uncertainty can dynamically modulate\nvehicle speed, maintaining high-speed performance under confident predictions\nwhile proactively improving safety through speed reductions in uncertain\nscenarios. These results demonstrate the potential of uncertainty-aware neural\nnetworks - in particular RS-NNs - as a practical solution for safer and more\nrobust autonomous driving."}
{"id": "2510.22699", "pdf": "https://arxiv.org/pdf/2510.22699", "abs": "https://arxiv.org/abs/2510.22699", "authors": ["Matteo El-Hariry", "Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez"], "title": "RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets", "categories": ["cs.RO"], "comment": null, "summary": "The growing need for autonomous on-orbit services such as inspection,\nmaintenance, and situational awareness calls for intelligent spacecraft capable\nof complex maneuvers around large orbital targets. Traditional control systems\noften fall short in adaptability, especially under model uncertainties,\nmulti-spacecraft configurations, or dynamically evolving mission contexts. This\npaper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous\nVisual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB),\nwe simulate high-fidelity 6-DOF spacecraft dynamics and train agents using\nDreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as\nmodel-free baselines. Our investigation focuses on 3D proximity maneuvering\ntasks around targets such as the Lunar Gateway and other space assets. We\nevaluate task performance under two complementary regimes: generalized agents\ntrained on randomized velocity vectors, and specialized agents trained to\nfollow fixed trajectories emulating known inspection orbits. Furthermore, we\nassess the robustness and generalization of policies across multiple spacecraft\nmorphologies and mission domains. Results demonstrate that model-based RL\noffers promising capabilities in trajectory fidelity, and sample efficiency,\npaving the way for scalable, retrainable control solutions for future space\noperations"}
{"id": "2510.22738", "pdf": "https://arxiv.org/pdf/2510.22738", "abs": "https://arxiv.org/abs/2510.22738", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "SCAL for Pinch-Lifting: Complementary Rotational and Linear Prototypes for Environment-Adaptive Grasping", "categories": ["cs.RO"], "comment": "Preliminary version presented at the IROS 2025 CIM Workshop, where it\n  was selected as a Best Demo Award (Finalist) and subsequently received the\n  Best Demo Award after oral presentation", "summary": "This paper presents environment-adaptive pinch-lifting built on a\nslot-constrained adaptive linkage (SCAL) and instantiated in two complementary\nfingers: SCAL-R, a rotational-drive design with an active fingertip that folds\ninward after contact to form an envelope, and SCAL-L, a linear-drive design\nthat passively opens on contact to span wide or weak-feature objects. Both\nfingers convert surface following into an upward lifting branch while\nmaintaining fingertip orientation, enabling thin or low-profile targets to be\nraised from supports with minimal sensing and control. Two-finger grippers are\nfabricated via PLA-based 3D printing. Experiments evaluate (i)\ncontact-preserving sliding and pinch-lifting on tabletops, (ii) ramp\nnegotiation followed by lift, and (iii) handling of bulky objects via active\nenveloping (SCAL-R) or contact-triggered passive opening (SCAL-L). Across\ndozens of trials on small parts, boxes, jars, and tape rolls, both designs\nachieve consistent grasps with limited tuning. A quasi-static analysis provides\nclosed-form fingertip-force models for linear parallel pinching and two-point\nenveloping, offering geometry-aware guidance for design and operation. Overall,\nthe results indicate complementary operating regimes and a practical path to\nrobust, environment-adaptive grasping with simple actuation."}
{"id": "2510.22740", "pdf": "https://arxiv.org/pdf/2510.22740", "abs": "https://arxiv.org/abs/2510.22740", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "IEEE International Symposium on Multi-Robot & Multi-Agent Systems\n  (MRS) 2025", "summary": "We consider the distributed pose-graph optimization (PGO) problem, which is\nfundamental in accurate trajectory estimation in multi-robot simultaneous\nlocalization and mapping (SLAM). Conventional iterative approaches linearize a\nhighly non-convex optimization objective, requiring repeated solving of normal\nequations, which often converge to local minima and thus produce suboptimal\nestimates. We propose a scalable, outlier-robust distributed planar PGO\nframework using Multi-Agent Reinforcement Learning (MARL). We cast distributed\nPGO as a partially observable Markov game defined on local pose-graphs, where\neach action refines a single edge's pose estimate. A graph partitioner\ndecomposes the global pose graph, and each robot runs a recurrent\nedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating\nto denoise noisy edges. Robots sequentially refine poses through a hybrid\npolicy that utilizes prior action memory and graph embeddings. After local\ngraph correction, a consensus scheme reconciles inter-robot disagreements to\nproduce a globally consistent estimate. Our extensive evaluations on a\ncomprehensive suite of synthetic and real-world datasets demonstrate that our\nlearned MARL-based actors reduce the global objective by an average of 37.5%\nmore than the state-of-the-art distributed PGO framework, while enhancing\ninference efficiency by at least 6X. We also demonstrate that actor replication\nallows a single learned policy to scale effortlessly to substantially larger\nrobot teams without any retraining. Code is publicly available at\nhttps://github.com/herolab-uga/policies-over-poses."}
{"id": "2510.22754", "pdf": "https://arxiv.org/pdf/2510.22754", "abs": "https://arxiv.org/abs/2510.22754", "authors": ["Chunyu Li", "Shoubin Chen", "Dong Li", "Weixing Xue", "Qingquan Li"], "title": "TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments", "categories": ["cs.RO"], "comment": "Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "Multi-agent cooperative SLAM often encounters challenges in similar indoor\nenvironments characterized by repetitive structures, such as corridors and\nrooms. These challenges can lead to significant inaccuracies in shared location\nidentification when employing point cloud-based techniques. To mitigate these\nissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that\nintegrates text semantics and WiFi signal features to enhance location\nidentification and loop closure detection. TWC-SLAM comprises a single-agent\nfront-end odometry module based on FAST-LIO2, a location identification and\nloop closure detection module that leverages text semantics and WiFi features,\nand a global mapping module. The agents are equipped with sensors capable of\ncapturing textual information and detecting WiFi signals. By correlating these\ndata sources, TWC-SLAM establishes a common location, facilitating point cloud\nalignment across different agents' maps. Furthermore, the system employs loop\nclosure detection and optimization modules to achieve global optimization and\ncohesive mapping. We evaluated our approach using an indoor dataset featuring\nsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAM\nsignificantly improves the performance of cooperative SLAM systems in complex\nenvironments with repetitive architectural features."}
{"id": "2510.22784", "pdf": "https://arxiv.org/pdf/2510.22784", "abs": "https://arxiv.org/abs/2510.22784", "authors": ["Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "title": "PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Enabling robot teams to execute natural language commands requires\ntranslating high-level instructions into feasible, efficient multi-robot plans.\nWhile Large Language Models (LLMs) combined with Planning Domain Description\nLanguage (PDDL) offer promise for single-robot scenarios, existing approaches\nstruggle with multi-robot coordination due to brittle task decomposition, poor\nscalability, and low coordination efficiency.\n  We introduce PIP-LLM, a language-based coordination framework that consists\nof PDDL-based team-level planning and Integer Programming (IP) based\nrobot-level planning. PIP-LLMs first decomposes the command by translating the\ncommand into a team-level PDDL problem and solves it to obtain a team-level\nplan, abstracting away robot assignment. Each team-level action represents a\nsubtask to be finished by the team. Next, this plan is translated into a\ndependency graph representing the subtasks' dependency structure. Such a\ndependency graph is then used to guide the robot-level planning, in which each\nsubtask node will be formulated as an IP-based task allocation problem,\nexplicitly optimizing travel costs and workload while respecting robot\ncapabilities and user-defined constraints. This separation of planning from\nassignment allows PIP-LLM to avoid the pitfalls of syntax-based decomposition\nand scale to larger teams. Experiments across diverse tasks show that PIP-LLM\nimproves plan success rate, reduces maximum and average travel costs, and\nachieves better load balancing compared to state-of-the-art baselines."}
{"id": "2510.22789", "pdf": "https://arxiv.org/pdf/2510.22789", "abs": "https://arxiv.org/abs/2510.22789", "authors": ["Abhijeet M. Kulkarni", "Ioannis Poulakakis", "Guoquan Huang"], "title": "Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning", "categories": ["cs.RO"], "comment": null, "summary": "Accurate full-body motion prediction is essential for the safe, autonomous\nnavigation of legged robots, enabling critical capabilities like limb-level\ncollision checking in cluttered environments. Simplified kinematic models often\nfail to capture the complex, closed-loop dynamics of the robot and its\nlow-level controller, limiting their predictions to simple planar motion. To\naddress this, we present a learning-based observer-predictor framework that\naccurately predicts this motion. Our method features a neural observer with\nprovable UUB guarantees that provides a reliable latent state estimate from a\nhistory of proprioceptive measurements. This stable estimate initializes a\ncomputationally efficient predictor, designed for the rapid, parallel\nevaluation of thousands of potential trajectories required by modern\nsampling-based planners. We validated the system by integrating our neural\npredictor into an MPPI-based planner on a Vision 60 quadruped. Hardware\nexperiments successfully demonstrated effective, limb-aware motion planning in\na challenging, narrow passage and over small objects, highlighting our system's\nability to provide a robust foundation for high-performance, collision-aware\nplanning on dynamic robotic platforms."}
{"id": "2510.22821", "pdf": "https://arxiv.org/pdf/2510.22821", "abs": "https://arxiv.org/abs/2510.22821", "authors": ["Ricardo Vega", "Connor Mattson", "Kevin Zhu", "Daniel S. Brown", "Cameron Nowzari"], "title": "Analytical Swarm Chemistry: Characterization and Analysis of Emergent Swarm Behaviors", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "9 pages, 8 figures, 1 table", "summary": "Swarm robotics has potential for a wide variety of applications, but\nreal-world deployments remain rare due to the difficulty of predicting emergent\nbehaviors arising from simple local interactions. Traditional engineering\napproaches design controllers to achieve desired macroscopic outcomes under\nidealized conditions, while agent-based and artificial life studies explore\nemergent phenomena in a bottom-up, exploratory manner. In this work, we\nintroduce Analytical Swarm Chemistry, a framework that integrates concepts from\nengineering, agent-based and artificial life research, and chemistry. This\nframework combines macrostate definitions with phase diagram analysis to\nsystematically explore how swarm parameters influence emergent behavior.\nInspired by concepts from chemistry, the framework treats parameters like\nthermodynamic variables, enabling visualization of regions in parameter space\nthat give rise to specific behaviors. Applying this framework to agents with\nminimally viable capabilities, we identify sufficient conditions for behaviors\nsuch as milling and diffusion and uncover regions of the parameter space that\nreliably produce these behaviors. Preliminary validation on real robots\ndemonstrates that these regions correspond to observable behaviors in practice.\nBy providing a principled, interpretable approach, this framework lays the\ngroundwork for predictable and reliable emergent behavior in real-world swarm\nsystems."}
{"id": "2510.22825", "pdf": "https://arxiv.org/pdf/2510.22825", "abs": "https://arxiv.org/abs/2510.22825", "authors": ["Nan Zhang"], "title": "Kinematically Controllable Cable Robots with Reconfigurable End-effectors", "categories": ["cs.RO"], "comment": "7 pages, 12 figures, Technical Report", "summary": "To enlarge the translational workspace of cable-driven robots, one common\napproach is to increase the number of cables. However, this introduces two\nchallenges: (1) cable interference significantly reduces the rotational\nworkspace, and (2) the solution of tensions in cables becomes non-unique,\nresulting in difficulties for kinematic control of the robot. In this work, we\ndesign structurally simple reconfigurable end-effectors for cable robots. By\nincorporating a spring, a helical-grooved shaft, and a matching nut, relative\nlinear motions between end-effector components are converted into relative\nrotations, thereby expanding the rotational workspace of the mechanism.\nMeanwhile, a bearing is introduced to provide an additional rotational degree\nof freedom, making the mechanism non-redundant. As a result, the robot's motion\ncan be controlled purely through kinematics without additional tension sensing\nand control."}
{"id": "2510.22892", "pdf": "https://arxiv.org/pdf/2510.22892", "abs": "https://arxiv.org/abs/2510.22892", "authors": ["Jingzehua Xu", "Yangyang Li", "Yangfei Chen", "Guanwen Xie", "Shuai Zhang"], "title": "Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation."}
{"id": "2510.22917", "pdf": "https://arxiv.org/pdf/2510.22917", "abs": "https://arxiv.org/abs/2510.22917", "authors": ["Zecheng Yin", "Hao Zhao", "Zhen Li"], "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment", "categories": ["cs.RO", "cs.AI"], "comment": "under review", "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance."}
{"id": "2510.22949", "pdf": "https://arxiv.org/pdf/2510.22949", "abs": "https://arxiv.org/abs/2510.22949", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control", "categories": ["cs.RO", "cs.SY", "eess.SY", "93C10", "I.2.9; I.2.8; J.2"], "comment": "24 pages, journal", "summary": "This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications."}
{"id": "2510.23003", "pdf": "https://arxiv.org/pdf/2510.23003", "abs": "https://arxiv.org/abs/2510.23003", "authors": ["ZhengKai Huang", "YiKun Wang", "ChenYu Hui", "XiaoCheng"], "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases."}
{"id": "2510.23016", "pdf": "https://arxiv.org/pdf/2510.23016", "abs": "https://arxiv.org/abs/2510.23016", "authors": ["Zhuo Li", "Junjia Liu", "Dianxi Li", "Tao Teng", "Miao Li", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation", "categories": ["cs.RO"], "comment": "7 pages, 6 figures, Accepted and published in IROS 2025", "summary": "Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity."}
{"id": "2510.23057", "pdf": "https://arxiv.org/pdf/2510.23057", "abs": "https://arxiv.org/abs/2510.23057", "authors": ["Oskar Natan", "Jun Miura"], "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "comment": "Preprint notice, this manuscript has been submitted to IEEE sensors\n  journal for possible publication", "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC."}
{"id": "2510.23059", "pdf": "https://arxiv.org/pdf/2510.23059", "abs": "https://arxiv.org/abs/2510.23059", "authors": ["Yongtong Zhu", "Lei Li", "Iggy Qian", "WenBin Zhou", "Ye Yuan", "Qingdu Li", "Na Liu", "Jianwei Zhang"], "title": "Awakening Facial Emotional Expressions in Human-Robot", "categories": ["cs.RO"], "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025). 8 pages, 7 figures, IEEE two-column format", "summary": "The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects."}
{"id": "2510.23084", "pdf": "https://arxiv.org/pdf/2510.23084", "abs": "https://arxiv.org/abs/2510.23084", "authors": ["Sunyou Hwang", "Christophe De Wagter", "Bart Remes", "Guido de Croon"], "title": "Breaking the Circle: An Autonomous Control-Switching Strategy for Stable Orographic Soaring in MAVs", "categories": ["cs.RO"], "comment": "13 pages, 15 figures", "summary": "Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments."}
{"id": "2510.23109", "pdf": "https://arxiv.org/pdf/2510.23109", "abs": "https://arxiv.org/abs/2510.23109", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Ronald Naderer", "Andreas Mueller"], "title": "An Automated Tape Laying System Employing a Uniaxial Force Control Device", "categories": ["cs.RO"], "comment": "Proceedings ECCM21 - 21st European Conference on Composite Materials,\n  Nantes, France, 7-2024", "summary": "This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape."}
{"id": "2510.23119", "pdf": "https://arxiv.org/pdf/2510.23119", "abs": "https://arxiv.org/abs/2510.23119", "authors": ["Yi-Lin Wei", "Zhexi Luo", "Yuhao Lin", "Mu Lin", "Zhizhao Liang", "Shuoyu Chen", "Wei-Shi Zheng"], "title": "OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback", "categories": ["cs.RO"], "comment": "Project page: https://isee-laboratory.github.io/OmniDexGrasp/", "summary": "Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks."}
{"id": "2510.23121", "pdf": "https://arxiv.org/pdf/2510.23121", "abs": "https://arxiv.org/abs/2510.23121", "authors": ["Bharath Santhanam", "Alex Mitrevski", "Santosh Thoduka", "Sebastian Houben", "Teena Hassan"], "title": "Reliable Robotic Task Execution in the Face of Anomalies", "categories": ["cs.RO"], "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions."}
{"id": "2510.23129", "pdf": "https://arxiv.org/pdf/2510.23129", "abs": "https://arxiv.org/abs/2510.23129", "authors": ["Sabino Francesco Roselli", "Ze Zhang", "Knut Åkesson"], "title": "Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios."}
{"id": "2510.23176", "pdf": "https://arxiv.org/pdf/2510.23176", "abs": "https://arxiv.org/abs/2510.23176", "authors": ["Arnav Sukhija", "Lenart Treven", "Jin Cheng", "Florian Dörfler", "Stelian Coros", "Andreas Krause"], "title": "TARC: Time-Adaptive Robotic Control", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions."}
{"id": "2510.23204", "pdf": "https://arxiv.org/pdf/2510.23204", "abs": "https://arxiv.org/abs/2510.23204", "authors": ["Giulia Pusceddu", "Giulio Antonio Abbo", "Francesco Rea", "Tony Belpaeme", "Alessandra Sciutti"], "title": "If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams."}
{"id": "2510.23227", "pdf": "https://arxiv.org/pdf/2510.23227", "abs": "https://arxiv.org/abs/2510.23227", "authors": ["Klaus Zauner", "Josef El Dib", "Hubert Gattringer", "Andreas Mueller"], "title": "Workspace Registration and Collision Detection for Industrial Robotics Applications", "categories": ["cs.RO"], "comment": null, "summary": "Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment."}
{"id": "2510.23234", "pdf": "https://arxiv.org/pdf/2510.23234", "abs": "https://arxiv.org/abs/2510.23234", "authors": ["Klaus Zauner", "Hubert Gattringer", "Andreas Mueller"], "title": "Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation", "categories": ["cs.RO"], "comment": "Mechanics Based Design of Structures and Machines, December 2024", "summary": "Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator."}
{"id": "2510.23258", "pdf": "https://arxiv.org/pdf/2510.23258", "abs": "https://arxiv.org/abs/2510.23258", "authors": ["Riko Yokozawa", "Kentaro Fujii", "Yuta Nomura", "Shingo Murata"], "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Preprint version", "summary": "Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings."}
{"id": "2510.23286", "pdf": "https://arxiv.org/pdf/2510.23286", "abs": "https://arxiv.org/abs/2510.23286", "authors": ["Jin Huang", "Yingqiang Wang", "Haoda Li", "Zichen Liu", "Zhikun Wang", "Ying Chen"], "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation", "categories": ["cs.RO"], "comment": null, "summary": "In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems."}
{"id": "2510.23329", "pdf": "https://arxiv.org/pdf/2510.23329", "abs": "https://arxiv.org/abs/2510.23329", "authors": ["Shreya Santra", "Thomas Robbins", "Kazuya Yoshida"], "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon", "categories": ["cs.RO"], "comment": "6 pages, 7 figures. Accepted at IEEE iSpaRo 2025", "summary": "Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs."}
{"id": "2510.23357", "pdf": "https://arxiv.org/pdf/2510.23357", "abs": "https://arxiv.org/abs/2510.23357", "authors": ["Shaohan Bian", "Ying Zhang", "Guohui Tian", "Zhiqiang Miao", "Edmond Q. Wu", "Simon X. Yang", "Changchun Hua"], "title": "Large language model-based task planning for service robots: A review", "categories": ["cs.RO"], "comment": "Submitted to Biomimetic Intelligence and Robotics for possible\n  publication", "summary": "With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics."}
{"id": "2510.23359", "pdf": "https://arxiv.org/pdf/2510.23359", "abs": "https://arxiv.org/abs/2510.23359", "authors": ["Chungeng Tian", "Ning Hao", "Fenghua He"], "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation", "categories": ["cs.RO"], "comment": "This paper was submitted to IEEE RA-L on July 14, 2024, and accepted\n  on December 18, 2024. This version serves as the 'plus edition' of the\n  accepted paper, incorporating supplementary materials for completeness", "summary": "This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF."}
{"id": "2510.23386", "pdf": "https://arxiv.org/pdf/2510.23386", "abs": "https://arxiv.org/abs/2510.23386", "authors": ["Alvaro Paz", "Mahdi Hejrati", "Pauli Mustalahti", "Jouni Mattila"], "title": "Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks", "categories": ["cs.RO"], "comment": "This work has been submitted for possible publication in IEEE", "summary": "Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems."}
{"id": "2510.23495", "pdf": "https://arxiv.org/pdf/2510.23495", "abs": "https://arxiv.org/abs/2510.23495", "authors": ["Chenyang Ma", "Kai Lu", "Ruta Desai", "Xavier Puig", "Andrew Markham", "Niki Trigoni"], "title": "COOPERA: Continual Open-Ended Human-Robot Assistance", "categories": ["cs.RO"], "comment": "NeurIPS 2025 (Spotlight); Project Page:\n  https://dannymcy.github.io/coopera/", "summary": "To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/"}
{"id": "2510.23509", "pdf": "https://arxiv.org/pdf/2510.23509", "abs": "https://arxiv.org/abs/2510.23509", "authors": ["Weizheng Wang", "Obi Ike", "Soyun Choi", "Sungeun Hong", "Byung-Cheol Min"], "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model", "categories": ["cs.RO"], "comment": null, "summary": "Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM."}
{"id": "2510.23511", "pdf": "https://arxiv.org/pdf/2510.23511", "abs": "https://arxiv.org/abs/2510.23511", "authors": ["Bin Xie", "Erjin Zhou", "Fan Jia", "Hao Shi", "Haoqiang Fan", "Haowei Zhang", "Hebei Li", "Jianjian Sun", "Jie Bin", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Lin Sun", "Meng Zhang", "Peilong Han", "Ruitao Hao", "Ruitao Zhang", "Saike Huang", "Songhan Xie", "Tiancai Wang", "Tianle Liu", "Wenbin Tang", "Wenqi Zhu", "Yang Chen", "Yingfei Liu", "Yizhuang Zhou", "Yu Liu", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yuxiang Chen", "Ze Chen", "Zeming Li", "Zhao Wu", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan", "Ziyu Zhang"], "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox", "categories": ["cs.RO"], "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://dexbotic.com/. Code is available at\n  https://github.com/Dexmal/dexbotic", "summary": "In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry."}
{"id": "2510.23512", "pdf": "https://arxiv.org/pdf/2510.23512", "abs": "https://arxiv.org/abs/2510.23512", "authors": ["Martin Huber", "Nicola A. Cavalcanti", "Ayoob Davoodi", "Ruixuan Li", "Christopher E. Mower", "Fabio Carrillo", "Christoph J. Laux", "Francois Teyssere", "Thibault Chandanson", "Antoine Harlé", "Elie Saghbiny", "Mazda Farshad", "Guillaume Morel", "Emmanuel Vander Poorten", "Philipp Fürnstahl", "Sébastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery."}
{"id": "2510.23521", "pdf": "https://arxiv.org/pdf/2510.23521", "abs": "https://arxiv.org/abs/2510.23521", "authors": ["Anthony Opipari", "Aravindhan K Krishnan", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo", "Arnie Sen", "Odest Chadwicke Jenkins"], "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation", "categories": ["cs.RO"], "comment": "Accepted in IEEE Robotics and Automation Letters September 2025", "summary": "Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/"}
{"id": "2510.23571", "pdf": "https://arxiv.org/pdf/2510.23571", "abs": "https://arxiv.org/abs/2510.23571", "authors": ["Yash Jangir", "Yidi Zhang", "Kashu Yamazaki", "Chenyu Zhang", "Kuan-Hsun Tu", "Tsung-Wei Ke", "Lei Ke", "Yonatan Bisk", "Katerina Fragkiadaki"], "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Website: https://robotarenainf.github.io", "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape."}
{"id": "2510.23576", "pdf": "https://arxiv.org/pdf/2510.23576", "abs": "https://arxiv.org/abs/2510.23576", "authors": ["Anqi Li", "Zhiyong Wang", "Jiazhao Zhang", "Minghan Li", "Yunpeng Qi", "Zhibo Chen", "Zhizheng Zhang", "He Wang"], "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties."}
{"id": "2510.21716", "pdf": "https://arxiv.org/pdf/2510.21716", "abs": "https://arxiv.org/abs/2510.21716", "authors": ["Nicola Webb", "Zijun Huang", "Sanja Milivojevic", "Chris Baber", "Edmund R. Hunt"], "title": "When Robots Say No: Temporal Trust Recovery Through Explanation", "categories": ["cs.HC", "cs.CL", "cs.RO"], "comment": null, "summary": "Mobile robots with some degree of autonomy could deliver significant\nadvantages in high-risk missions such as search and rescue and firefighting.\nIntegrated into a human-robot team (HRT), robots could work effectively to help\nsearch hazardous buildings. User trust is a key enabler for HRT, but during a\nmission, trust can be damaged. With distributed situation awareness, such as\nwhen team members are working in different locations, users may be inclined to\ndoubt a robot's integrity if it declines to immediately change its priorities\non request. In this paper, we present the results of a computer-based study\ninvestigating on-mission trust dynamics in a high-stakes human-robot teaming\nscenario. Participants (n = 38) played an interactive firefighting game\nalongside a robot teammate, where a trust violation occurs owing to the robot\ndeclining to help the user immediately. We find that when the robot provides an\nexplanation for declining to help, trust better recovers over time, albeit\nfollowing an initial drop that is comparable to a baseline condition where an\nexplanation for refusal is not provided. Our findings indicate that trust can\nvary significantly during a mission, notably when robots do not immediately\nrespond to user requests, but that this trust violation can be largely\nameliorated over time if adequate explanation is provided."}
{"id": "2510.21738", "pdf": "https://arxiv.org/pdf/2510.21738", "abs": "https://arxiv.org/abs/2510.21738", "authors": ["Yifan Bai", "Shruti Kotpalliwar", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "Collaborative Task Assignment, Sequencing and Multi-agent Path-finding", "categories": ["cs.MA", "cs.RO"], "comment": null, "summary": "In this article, we address the problem of collaborative task assignment,\nsequencing, and multi-agent pathfinding (TSPF), where a team of agents must\nvisit a set of task locations without collisions while minimizing flowtime.\nTSPF incorporates agent-task compatibility constraints and ensures that all\ntasks are completed. We propose a Conflict-Based Search with Task Sequencing\n(CBS-TS), an optimal and complete algorithm that alternates between finding new\ntask sequences and resolving conflicts in the paths of current sequences.\nCBS-TS uses a mixed-integer linear program (MILP) to optimize task sequencing\nand employs Conflict-Based Search (CBS) with Multi-Label A* (MLA*) for\ncollision-free path planning within a search forest. By invoking MILP for the\nnext-best sequence only when needed, CBS-TS efficiently limits the search\nspace, enhancing computational efficiency while maintaining optimality. We\ncompare the performance of our CBS-TS against Conflict-based Steiner Search\n(CBSS), a baseline method that, with minor modifications, can address the TSPF\nproblem. Experimental results demonstrate that CBS-TS outperforms CBSS in most\ntesting scenarios, achieving higher success rates and consistently optimal\nsolutions, whereas CBSS achieves near-optimal solutions in some cases. The\nsupplementary video is available at https://youtu.be/QT8BYgvefmU."}
{"id": "2510.21785", "pdf": "https://arxiv.org/pdf/2510.21785", "abs": "https://arxiv.org/abs/2510.21785", "authors": ["Arun Muthukkumar"], "title": "Multi-Agent Pose Uncertainty: A Differentiable Rendering Cramér-Rao Bound", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "comment": "5 pages, 3 figures, 1 table. Presented at IEEE/CVF International\n  Conference on Computer Vision (ICCV 2025) and IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Pose estimation is essential for many applications within computer vision and\nrobotics. Despite its uses, few works provide rigorous uncertainty\nquantification for poses under dense or learned models. We derive a closed-form\nlower bound on the covariance of camera pose estimates by treating a\ndifferentiable renderer as a measurement function. Linearizing image formation\nwith respect to a small pose perturbation on the manifold yields a render-aware\nCram\\'er-Rao bound. Our approach reduces to classical bundle-adjustment\nuncertainty, ensuring continuity with vision theory. It also naturally extends\nto multi-agent settings by fusing Fisher information across cameras. Our\nstatistical formulation has downstream applications for tasks such as\ncooperative perception and novel view synthesis without requiring explicit\nkeypoint correspondences."}
{"id": "2510.21809", "pdf": "https://arxiv.org/pdf/2510.21809", "abs": "https://arxiv.org/abs/2510.21809", "authors": ["Haru Kondoh", "Asako Kanezaki"], "title": "Embodied Navigation with Auxiliary Task of Action Description Prediction", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025 Poster", "summary": "The field of multimodal robot navigation in indoor environments has garnered\nsignificant attention in recent years. However, as tasks and methods become\nmore advanced, the action decision systems tend to become more complex and\noperate as black-boxes. For a reliable system, the ability to explain or\ndescribe its decisions is crucial; however, there tends to be a trade-off in\nthat explainable systems can not outperform non-explainable systems in terms of\nperformance. In this paper, we propose incorporating the task of describing\nactions in language into the reinforcement learning of navigation as an\nauxiliary task. Existing studies have found it difficult to incorporate\ndescribing actions into reinforcement learning due to the absence of\nground-truth data. We address this issue by leveraging knowledge distillation\nfrom pre-trained description generation models, such as vision-language models.\nWe comprehensively evaluate our approach across various navigation tasks,\ndemonstrating that it can describe actions while attaining high navigation\nperformance. Furthermore, it achieves state-of-the-art performance in the\nparticularly challenging multimodal navigation task of semantic audio-visual\nnavigation."}
{"id": "2510.22141", "pdf": "https://arxiv.org/pdf/2510.22141", "abs": "https://arxiv.org/abs/2510.22141", "authors": ["Yuhang Gao", "Xiang Xiang", "Sheng Zhong", "Guoyou Wang"], "title": "LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO", "eess.IV"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown significant progress in open-set\nchallenges. However, the limited availability of 3D datasets hinders their\neffective application in 3D scene understanding. We propose LOC, a general\nlanguage-guided framework adaptable to various occupancy networks, supporting\nboth supervised and self-supervised learning paradigms. For self-supervised\ntasks, we employ a strategy that fuses multi-frame LiDAR points for\ndynamic/static scenes, using Poisson reconstruction to fill voids, and\nassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain\ncomprehensive voxel representations. To mitigate feature over-homogenization\ncaused by direct high-dimensional feature distillation, we introduce Densely\nContrastive Learning (DCL). DCL leverages dense voxel semantic information and\npredefined textual prompts. This efficiently enhances open-set recognition\nwithout dense pixel-level supervision, and our framework can also leverage\nexisting ground truth to further improve performance. Our model predicts dense\nvoxel features embedded in the CLIP feature space, integrating textual and\nimage pixel information, and classifies based on text and semantic similarity.\nExperiments on the nuScenes dataset demonstrate the method's superior\nperformance, achieving high-precision predictions for known classes and\ndistinguishing unknown classes without additional training data."}
{"id": "2510.22199", "pdf": "https://arxiv.org/pdf/2510.22199", "abs": "https://arxiv.org/abs/2510.22199", "authors": ["Kunal Bhosikar", "Siddharth Katageri", "Vivek Madhavaram", "Kai Han", "Charu Sharma"], "title": "MOGRAS: Human Motion with Grasping in 3D Scenes", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "British Machine Vision Conference Workshop - From Scene Understanding\n  to Human Modeling", "summary": "Generating realistic full-body motion interacting with objects is critical\nfor applications in robotics, virtual reality, and human-computer interaction.\nWhile existing methods can generate full-body motion within 3D scenes, they\noften lack the fidelity for fine-grained tasks like object grasping.\nConversely, methods that generate precise grasping motions typically ignore the\nsurrounding 3D scene. This gap, generating full-body grasping motions that are\nphysically plausible within a 3D scene, remains a significant challenge. To\naddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), a\nlarge-scale dataset that bridges this gap. MOGRAS provides pre-grasping\nfull-body walking motions and final grasping poses within richly annotated 3D\nindoor scenes. We leverage MOGRAS to benchmark existing full-body grasping\nmethods and demonstrate their limitations in scene-aware generation.\nFurthermore, we propose a simple yet effective method to adapt existing\napproaches to work seamlessly within 3D scenes. Through extensive quantitative\nand qualitative experiments, we validate the effectiveness of our dataset and\nhighlight the significant improvements our proposed method achieves, paving the\nway for more realistic human-scene interactions."}
{"id": "2510.22235", "pdf": "https://arxiv.org/pdf/2510.22235", "abs": "https://arxiv.org/abs/2510.22235", "authors": ["Yixiao Nie", "Yang Zhang", "Yingjie Jin", "Zhepeng Wang", "Xiu Li", "Xiang Li"], "title": "CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts", "categories": ["cs.MA", "cs.RO"], "comment": null, "summary": "The integration of self-driving cars and service robots is becoming\nincreasingly prevalent across a wide array of fields, playing a crucial and\nexpanding role in both industrial applications and everyday life. In parallel,\nthe rapid advancements in Large Language Models (LLMs) have garnered\nsubstantial attention and interest within the research community. This paper\nintroduces a novel vehicle-robot system that leverages the strengths of both\nautonomous vehicles and service robots. In our proposed system, two autonomous\nego-vehicles transports service robots to locations within an office park,\nwhere they perform a series of tasks. The study explores the feasibility and\npotential benefits of incorporating LLMs into this system, with the aim of\nenhancing operational efficiency and maximizing the potential of the\ncooperative mechanisms between the vehicles and the robots. This paper proposes\na novel inference mechanism which is called CGOT toward this type of system\nwhere an agent can carry another agent. Experimental results are presented to\nvalidate the performance of the proposed method."}
{"id": "2510.22434", "pdf": "https://arxiv.org/pdf/2510.22434", "abs": "https://arxiv.org/abs/2510.22434", "authors": ["Prajyot Pyati", "Navjot Kaur", "Saswata Jana", "Adri Bhattacharya", "Partha Sarathi Mandal"], "title": "Separation of Unconscious Robots with Obstructed Visibility", "categories": ["cs.DC", "cs.RO"], "comment": null, "summary": "We study a recently introduced \\textit{unconscious} mobile robot model, where\neach robot is associated with a \\textit{color}, which is visible to other\nrobots but not to itself. The robots are autonomous, anonymous, oblivious and\nsilent, operating in the Euclidean plane under the conventional\n\\textit{Look-Compute-Move} cycle. A primary task in this model is the\n\\textit{separation problem}, where unconscious robots sharing the same color\nmust separate from others, forming recognizable geometric shapes such as\ncircles, points, or lines. All prior works model the robots as\n\\textit{transparent}, enabling each to know the positions and colors of all\nother robots. In contrast, we model the robots as \\textit{opaque}, where a\nrobot can obstruct the visibility of two other robots, if it lies on the line\nsegment between them. Under this obstructed visibility, we consider a variant\nof the separation problem in which robots, starting from any arbitrary initial\nconfiguration, are required to separate into concentric semicircles. We present\na collision-free algorithm that solves the separation problem under a\nsemi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots.\nThe robots agree on one coordinate axis but have no knowledge of $n$."}
{"id": "2510.22529", "pdf": "https://arxiv.org/pdf/2510.22529", "abs": "https://arxiv.org/abs/2510.22529", "authors": ["Xiang Fei", "Tina Tian", "Howie Choset", "Lu Li"], "title": "Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing", "categories": ["cs.CV", "cs.RO"], "comment": "This paper has been accepted by IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025", "summary": "Loop closure is critical in Simultaneous Localization and Mapping (SLAM)\nsystems to reduce accumulative drift and ensure global mapping consistency.\nHowever, conventional methods struggle in perceptually aliased environments,\nsuch as narrow pipes, due to vector quantization, feature sparsity, and\nrepetitive textures, while existing solutions often incur high computational\ncosts. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure\ndetection method that achieves superior precision-recall, robustness, and\ncomputational efficiency. The core innovation lies in the introduction of word\ngroups, which captures the spatial co-occurrence and proximity of visual words\nto construct an online dictionary. Additionally, drawing inspiration from\nprobabilistic transition models, we incorporate temporal consistency directly\ninto similarity computation with an adaptive scheme, substantially improving\nprecision-recall performance. The method is further strengthened by a feature\ndistribution analysis module and dedicated post-verification mechanisms. To\nevaluate the effectiveness of our method, we conduct experiments on both public\ndatasets and a confined-pipe dataset we constructed. Results demonstrate that\nBoWG surpasses state-of-the-art methods, including both traditional and\nlearning-based approaches, in terms of precision-recall and computational\nefficiency. Our approach also exhibits excellent scalability, achieving an\naverage processing time of 16 ms per image across 17,565 images in the\nBicocca25b dataset."}
{"id": "2510.22672", "pdf": "https://arxiv.org/pdf/2510.22672", "abs": "https://arxiv.org/abs/2510.22672", "authors": ["Anna Deichler", "Jonas Beskow"], "title": "Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views", "categories": ["cs.CV", "cs.CL", "cs.RO", "I.2.10; I.2.9; I.2.7; H.5.2"], "comment": "10 pages, 6 figures, 2 tables. Accepted to the NeurIPS 2025 Workshop\n  on SPACE in Vision, Language, and Embodied AI (SpaVLE)", "summary": "We introduce Look and Tell, a multimodal dataset for studying referential\ncommunication across egocentric and exocentric perspectives. Using Meta Project\nAria smart glasses and stationary cameras, we recorded synchronized gaze,\nspeech, and video as 25 participants instructed a partner to identify\ningredients in a kitchen. Combined with 3D scene reconstructions, this setup\nprovides a benchmark for evaluating how different spatial representations (2D\nvs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67\nhours of recordings, including 2,707 richly annotated referential expressions,\nand is designed to advance the development of embodied agents that can\nunderstand and engage in situated dialogue."}
{"id": "2510.22732", "pdf": "https://arxiv.org/pdf/2510.22732", "abs": "https://arxiv.org/abs/2510.22732", "authors": ["Jiali Cheng", "Anjishnu Kumar", "Roshan Lal", "Rishi Rajasekaran", "Hani Ramezani", "Omar Zia Khan", "Oleg Rokhlenko", "Sunny Chiu-Webster", "Gang Hua", "Hadi Amiri"], "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "cs.MA", "cs.RO"], "comment": "9 pages, NeurIPS 2025 Workshop on Language Agents and World Models", "summary": "We observe that current state-of-the-art web-agents are unable to effectively\nadapt to new environments without neural network fine-tuning, without which\nthey produce inefficient execution plans due to a lack of awareness of the\nstructure and dynamics of the new environment. To address this limitation, we\nintroduce ATLAS (Actor-Critic Task-completion with Look-ahead Action\nSimulation), a memory-augmented agent that is able to make plans grounded in a\nmodel of the environment by simulating the consequences of those actions in\ncognitive space. Our agent starts by building a \"cognitive map\" by performing a\nlightweight curiosity driven exploration of the environment. The planner\nproposes candidate actions; the simulator predicts their consequences in\ncognitive space; a critic analyzes the options to select the best roll-out and\nupdate the original plan; and a browser executor performs the chosen action. On\nthe WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%\nsuccess rate for the previously published state-of-the-art. Unlike previous\nsystems, our modular architecture requires no website-specific LLM fine-tuning.\nAblations show sizable drops without the world-model, hierarchical planner, and\nlook-ahead-based replanner confirming their complementary roles within the\ndesign of our system"}
{"id": "2510.22846", "pdf": "https://arxiv.org/pdf/2510.22846", "abs": "https://arxiv.org/abs/2510.22846", "authors": ["Dmytro Vovchuk", "Oleg Torgovitsky", "Mykola Khobzei", "Vladyslav Tkach", "Sergey Geyman", "Anton Kharchevskii", "Andrey Sheleg", "Toms Salgals", "Vjaceslavs Bobrovs", "Shai Gizach", "Aviel Glam", "Niv Haim Mizrahi", "Alexander Liberzon", "Pavel Ginzburg"], "title": "Drone Carry-on Weight and Wind Flow Assessment via Micro-Doppler Analysis", "categories": ["physics.app-ph", "cs.RO"], "comment": null, "summary": "Remote monitoring of drones has become a global objective due to emerging\napplications in national security and managing aerial delivery traffic. Despite\ntheir relatively small size, drones can carry significant payloads, which\nrequire monitoring, especially in cases of unauthorized transportation of\ndangerous goods. A drone's flight dynamics heavily depend on outdoor wind\nconditions and the carry-on weight, which affect the tilt angle of a drone's\nbody and the rotation velocity of the blades. A surveillance radar can capture\nboth effects, provided a sufficient signal-to-noise ratio for the received\nechoes and an adjusted postprocessing detection algorithm. Here, we conduct a\nsystematic study to demonstrate that micro-Doppler analysis enables the\ndisentanglement of the impacts of wind and weight on a hovering drone. The\nphysics behind the effect is related to the flight controller, as the way the\ndrone counteracts weight and wind differs. When the payload is balanced, it\nimposes an additional load symmetrically on all four rotors, causing them to\nrotate faster, thereby generating a blade-related micro-Doppler shift at a\nhigher frequency. However, the impact of the wind is different. The wind\nattempts to displace the drone, and to counteract this, the drone tilts to the\nside. As a result, the forward and rear rotors rotate at different velocities\nto maintain the tilt angle of the drone body relative to the airflow direction.\nThis causes the splitting in the micro-Doppler spectra. By performing a set of\nexperiments in a controlled environment, specifically, an anechoic chamber for\nelectromagnetic isolation and a wind tunnel for imposing deterministic wind\nconditions, we demonstrate that both wind and payload details can be extracted\nusing a simple deterministic algorithm based on branching in the micro-Doppler\nspectra."}
{"id": "2510.22913", "pdf": "https://arxiv.org/pdf/2510.22913", "abs": "https://arxiv.org/abs/2510.22913", "authors": ["Thanyanee Srichaisak", "Arissa Ieochai", "Aueaphum Aueawattthanaphisut"], "title": "Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.RO", "q-bio.NC"], "comment": "19 pages, 7 figures, 5 Tables", "summary": "Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of\ndaily living (ADL) and reduce adherence to home rehabilitation. Objective: To\nassess technical feasibility and clinician-relevant signals of a sensor-fused\nwearable targeting the triceps brachii and extensor pollicis brevis. Methods: A\nlightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and\nflex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and\na safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).\nHealthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:\nTremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).\nSecondary: EMG median-frequency slope (fatigue trend), closed-loop latency,\nsession completion, and device-related adverse events. Analyses used\nsubject-level paired medians with BCa 95\\% CIs; exact Wilcoxon $p$-values are\nreported in the Results. Results: Assistance was associated with lower tremor\nprominence and improved task throughput: TI decreased by $-0.092$ (95\\% CI\n[$-0.102$, $-0.079$]), ROM increased by $+12.65\\%$ (95\\% CI [$+8.43$,\n$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\\% CI [$+2.61$, $+3.35$]).\nMedian on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were\ncompleted with no device-related adverse events. Conclusions: Multimodal\nsensing with low-latency, safety-bounded assistance produced improved movement\nquality (TI $\\downarrow$) and throughput (ROM, Reps $\\uparrow$) in a pilot\ntechnical-feasibility setting, supporting progression to IRB-approved patient\nstudies. Trial registration: Not applicable (pilot non-clinical)."}
{"id": "2510.23021", "pdf": "https://arxiv.org/pdf/2510.23021", "abs": "https://arxiv.org/abs/2510.23021", "authors": ["Xibin Jin", "Guoliang Li", "Shuai Wang", "Fan Liu", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Planning Oriented Integrated Sensing and Communication", "categories": ["eess.SP", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Integrated sensing and communication (ISAC) enables simultaneous\nlocalization, environment perception, and data exchange for connected\nautonomous vehicles. However, most existing ISAC designs prioritize sensing\naccuracy and communication throughput, treating all targets uniformly and\noverlooking the impact of critical obstacles on motion efficiency. To overcome\nthis limitation, we propose a planning-oriented ISAC (PISAC) framework that\nreduces the sensing uncertainty of planning-bottleneck obstacles and expands\nthe safe navigable path for the ego-vehicle, thereby bridging the gap between\nphysical-layer optimization and motion-level planning. The core of PISAC lies\nin deriving a closed-form safety bound that explicitly links ISAC transmit\npower to sensing uncertainty, based on the Cram\\'er-Rao Bound and occupancy\ninflation principles. Using this model, we formulate a bilevel power allocation\nand motion planning (PAMP) problem, where the inner layer optimizes the ISAC\nbeam power distribution and the outer layer computes a collision-free\ntrajectory under uncertainty-aware safety constraints. Comprehensive\nsimulations in high-fidelity urban driving environments demonstrate that PISAC\nachieves up to 40% higher success rates and over 5% shorter traversal times\nthan existing ISAC-based and communication-oriented benchmarks, validating its\neffectiveness in enhancing both safety and efficiency."}
{"id": "2510.23026", "pdf": "https://arxiv.org/pdf/2510.23026", "abs": "https://arxiv.org/abs/2510.23026", "authors": ["Crimson Stambaugh", "Rajesh P. N. Rao"], "title": "Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution", "categories": ["cs.AI", "cs.RO"], "comment": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESSAN) (under review)", "summary": "Recent studies demonstrate that diffusion planners benefit from sparse-step\nplanning over single-step planning. Training models to skip steps in their\ntrajectories helps capture long-term dependencies without additional or memory\ncomputational cost. However, predicting excessively sparse plans degrades\nperformance. We hypothesize this temporal density threshold is non-uniform\nacross a temporal horizon and that certain parts of a planned trajectory should\nbe more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion\nplanner where the densities throughout the horizon are tunable hyperparameters.\nMDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL\ntask domains."}
{"id": "2510.23087", "pdf": "https://arxiv.org/pdf/2510.23087", "abs": "https://arxiv.org/abs/2510.23087", "authors": ["Taoyu Wu", "Yiyi Miao", "Jiaxin Guo", "Ziyan Chen", "Sihang Zhao", "Zhuoxiao Li", "Zhe Tang", "Baoru Huang", "Limin Yu"], "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from\nendoscopic video is vital for downstream tasks and improved outcomes. However,\nendoscopic scenarios present unique challenges, including photometric\ninconsistencies, non-rigid tissue motion, and view-dependent highlights. Most\n3DGS-based methods that rely solely on appearance constraints for optimizing\n3DGS are often insufficient in this context, as these dynamic visual artifacts\ncan mislead the optimization process and lead to inaccurate reconstructions. To\naddress these limitations, we present EndoWave, a unified spatio-temporal\nGaussian Splatting framework by incorporating an optical flow-based geometric\nconstraint and a multi-resolution rational wavelet supervision. First, we adopt\na unified spatio-temporal Gaussian representation that directly optimizes\nprimitives in a 4D domain. Second, we propose a geometric constraint derived\nfrom optical flow to enhance temporal coherence and effectively constrain the\n3D structure of the scene. Third, we propose a multi-resolution rational\northogonal wavelet as a constraint, which can effectively separate the details\nof the endoscope and enhance the rendering performance. Extensive evaluations\non two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our\nmethod EndoWave achieves state-of-the-art reconstruction quality and visual\naccuracy compared to the baseline method."}
{"id": "2510.23296", "pdf": "https://arxiv.org/pdf/2510.23296", "abs": "https://arxiv.org/abs/2510.23296", "authors": ["Hai Yu", "Zhichao Yang", "Wei He", "Jianda Han", "Yongchun Fang", "Xiao Liang"], "title": "Payload trajectory tracking control for aerial transportation systems with cable length online optimization", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Cable-suspended aerial transportation systems are employed extensively across\nvarious industries. The capability to flexibly adjust the relative position\nbetween the multirotor and the payload has spurred growing interest in the\nsystem equipped with variable-length cable, promising broader application\npotential. Compared to systems with fixed-length cables, introducing the\nvariable-length cable adds a new degree of freedom. However, it also results in\nincreased nonlinearity and more complex dynamic coupling among the multirotor,\nthe cable and the payload, posing significant challenges in control design.\nThis paper introduces a backstepping control strategy tailored for aerial\ntransportation systems with variable-length cable, designed to precisely track\nthe payload trajectory while dynamically adjusting cable length. Then, a cable\nlength generator has been developed that achieves online optimization of the\ncable length while satisfying state constraints, thus balancing the\nmultirotor's motion and cable length changes without the need for manual\ntrajectory planning. The asymptotic stability of the closed-loop system is\nguaranteed through Lyapunov techniques and the growth restriction condition.\nFinally, simulation results confirm the efficacy of the proposed method in\nmanaging trajectory tracking and cable length adjustments effectively."}
{"id": "2510.23525", "pdf": "https://arxiv.org/pdf/2510.23525", "abs": "https://arxiv.org/abs/2510.23525", "authors": ["Wanmeng Li", "Simone Mosco", "Daniel Fusaro", "Alberto Pretto"], "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "This paper has been accepted for publication at the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous\nsystems is costly. To overcome this limitation, self-training-based\nUnsupervised Domain Adaptation (UDA) has been widely used to improve point\ncloud semantic segmentation by leveraging synthetic point cloud data. However,\nwe argue that existing methods do not effectively utilize unlabeled data, as\nthey either rely on predefined or fixed confidence thresholds, resulting in\nsuboptimal performance. In this paper, we propose a Dynamic Pseudo-Label\nFiltering (DPLF) scheme to enhance real data utilization in point cloud UDA\nsemantic segmentation. Additionally, we design a simple and efficient\nPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift\nbetween synthetic and real-world point clouds. Finally, we utilize data mixing\nconsistency loss to push the model to learn context-free representations. We\nimplement and thoroughly evaluate our approach through extensive comparisons\nwith state-of-the-art methods. Experiments on two challenging synthetic-to-real\npoint cloud semantic segmentation tasks demonstrate that our approach achieves\nsuperior performance. Ablation studies confirm the effectiveness of the DPLF\nand PG-DAP modules. We release the code of our method in this paper."}
{"id": "2510.23605", "pdf": "https://arxiv.org/pdf/2510.23605", "abs": "https://arxiv.org/abs/2510.23605", "authors": ["Shuhong Zheng", "Ashkan Mirzaei", "Igor Gilitschenski"], "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.RO"], "comment": "NeurIPS 2025, 38 pages, 22 figures", "summary": "Current 3D/4D generation methods are usually optimized for photorealism,\nefficiency, and aesthetics. However, they often fail to preserve the semantic\nidentity of the subject across different viewpoints. Adapting generation\nmethods with one or few images of a specific subject (also known as\nPersonalization or Subject-driven generation) allows generating visual content\nthat align with the identity of the subject. However, personalized 3D/4D\ngeneration is still largely underexplored. In this work, we introduce TIRE\n(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.\nIt takes an initial 3D asset produced by an existing 3D generative model as\ninput and uses video tracking to identify the regions that need to be modified.\nThen, we adopt a subject-driven 2D inpainting model for progressively infilling\nthe identified regions. Finally, we resplat the modified 2D multi-view\nobservations back to 3D while still maintaining consistency. Extensive\nexperiments demonstrate that our approach significantly improves identity\npreservation in 3D/4D generation compared to state-of-the-art methods. Our\nproject website is available at\nhttps://zsh2000.github.io/track-inpaint-resplat.github.io/."}
