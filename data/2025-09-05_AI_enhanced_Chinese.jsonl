{"id": "2509.03686", "pdf": "https://arxiv.org/pdf/2509.03686", "abs": "https://arxiv.org/abs/2509.03686", "authors": ["Hong Zhu", "Alexander Venus", "Erik Leitinger", "Klaus Witrisal"], "title": "Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals", "categories": ["eess.SP"], "comment": null, "summary": "Reliable and robust positioning of radio devices remains a challenging task\ndue to multipath propagation, hardware impairments, and interference from other\nradio transmitters. A frequently overlooked but critical factor is the agent\nitself, e.g., the user carrying the device, which potentially obstructs\nline-of-sight (LOS) links to the base stations (anchors). This paper addresses\nthe problem of accurate positioning in scenarios where LOS links are partially\nblocked by the agent. The agent is modeled as an extended object (EO) that\nscatters, attenuates, and blocks radio signals. We propose a Bayesian method\nthat fuses ``active'' measurements (between device and anchors) with\n``passive'' multistatic radar-type measurements (between anchors, reflected by\nthe EO). To handle measurement origin uncertainty, we introduce an multi-sensor\nand multiple-measurement probabilistic data association (PDA) algorithm that\njointly fuses all EO-related measurements. Furthermore, we develop an EO model\ntailored to agents such as human users, accounting for multiple reflections\nscattered off the body surface, and propose a simplified variant for\nlow-complexity implementation. Evaluation on both synthetic and real radio\nmeasurements demonstrates that the proposed algorithm outperforms conventional\nPDA methods based on point target assumptions, particularly during and after\nobstructed line-of-sight (OLOS) conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e3b\u52a8\u548c\u88ab\u52a8\u7684\u96f7\u8fbe\u6d4b\u91cf\uff0c\u4ee5\u89e3\u51b3\u7528\u6237\u906e\u6321\u5bfc\u81f4\u7684LOS\u963b\u585e\u95ee\u9898\uff0c\u5e76\u901a\u8fc7PDA\u7b97\u6cd5\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u65e0\u7ebf\u7535\u8bbe\u5907\u5b9a\u4f4d\u4e2d\uff0c\u7528\u6237\u81ea\u8eab\uff08\u5982\u643a\u5e26\u8bbe\u5907\u7684\u7528\u6237\uff09\u53ef\u80fd\u906e\u6321LOS\u94fe\u8def\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u4e0d\u51c6\u786e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u878d\u5408\u4e3b\u52a8\u6d4b\u91cf\u548c\u88ab\u52a8\u591a\u57fa\u5730\u96f7\u8fbe\u6d4b\u91cf\uff0c\u5f15\u5165\u591a\u4f20\u611f\u5668PDA\u7b97\u6cd5\u5904\u7406\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u7528\u6237\u7684\u6269\u5c55\u5bf9\u8c61\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u65e0\u7ebf\u7535\u6d4b\u91cf\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728OLOS\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u70b9\u76ee\u6807\u7684PDA\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bbe\u5907\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03825", "pdf": "https://arxiv.org/pdf/2509.03825", "abs": "https://arxiv.org/abs/2509.03825", "authors": ["Jeunghoon Lee"], "title": "Sensor placement for sparse force reconstruction", "categories": ["eess.SP"], "comment": null, "summary": "The present study proposes a Gram-matrix-based sensor placement strategy for\nsparse force reconstruction in the frequency domain. A modal decomposition of\nthe Gram matrix reveals that its structure is dominated by a few modes near the\ntarget frequency, and that each modal contribution reflects the spatial\ncorrelation of the corresponding mode shape. This suggests that placing sensors\nnear nodal regions where spatial correlation is low can reduce coherence in the\nfrequency response function (FRF) matrix and improve force reconstruction\naccuracy. To translate the physical insight into a practical design framework,\na greedy algorithm is proposed to select sensor locations that minimize the\noff-diagonal energy of the Gram matrix. Numerical simulations and experimental\nvalidations demonstrate that the proposed method yields robust and accurate\nforce estimation, outperforming heuristic sensor layouts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGram\u77e9\u9635\u7684\u4f20\u611f\u5668\u5e03\u7f6e\u7b56\u7565\uff0c\u7528\u4e8e\u9891\u57df\u4e2d\u7684\u7a00\u758f\u529b\u91cd\u5efa\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u611f\u5668\u4f4d\u7f6e\u4ee5\u63d0\u9ad8\u529b\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4f20\u611f\u5668\u5e03\u7f6e\u65b9\u6cd5\u5728\u529b\u91cd\u5efa\u4e2d\u5f80\u5f80\u4f9d\u8d56\u542f\u53d1\u5f0f\u5e03\u5c40\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9891\u54cd\u51fd\u6570\u77e9\u9635\u7684\u7ed3\u6784\u7279\u6027\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u5bf9Gram\u77e9\u9635\u8fdb\u884c\u6a21\u6001\u5206\u89e3\uff0c\u53d1\u73b0\u5176\u7ed3\u6784\u4e3b\u8981\u7531\u76ee\u6807\u9891\u7387\u9644\u8fd1\u7684\u5c11\u6570\u6a21\u6001\u4e3b\u5bfc\uff0c\u5e76\u63d0\u51fa\u4e86\u8d2a\u5a6a\u7b97\u6cd5\u4f18\u5316\u4f20\u611f\u5668\u4f4d\u7f6e\u4ee5\u51cf\u5c11\u77e9\u9635\u7684\u975e\u5bf9\u89d2\u80fd\u91cf\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u529b\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u5e03\u5c40\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f20\u611f\u5668\u5e03\u7f6e\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u6d1e\u5bdf\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u529b\u91cd\u5efa\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03979", "pdf": "https://arxiv.org/pdf/2509.03979", "abs": "https://arxiv.org/abs/2509.03979", "authors": ["Gilles Callebaut", "Jan Van Moer"], "title": "A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System", "categories": ["eess.SP"], "comment": null, "summary": "The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and\nbeekeeping. Locating nests is essential, but usually involves time-consuming\nmanual triangulation. We present a low-cost, open-source tracking system based\non Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and\na software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing\nthe BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY\nfor correlation-based detection. Using a Yagi antenna and PlutoSDR, the\nreceiver performs digital beam sweeping to determine the tag's direction. Field\ntests show reliable angular resolution at 50m and a communication range up to\n360m. While our modulation increases receiver complexity, it enables future\nimprovements such as multichannel spreading and tag identification. The design\nis fully open-source and provides a scalable framework for hornet tracking and\nrelated applications in environmental monitoring.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u84dd\u7259\u4f4e\u529f\u8017\uff08BLE\uff09\u7684\u4f4e\u6210\u672c\u5f00\u6e90\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9a\u4f4d\u4e9a\u6d32\u5927\u9ec4\u8702\u5de2\u7a74\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u4f2a\u566a\u58f0\u5e8f\u5217\u548c\u6570\u5b57\u6ce2\u675f\u626b\u63cf\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65b9\u5411\u68c0\u6d4b\u3002", "motivation": "\u4e9a\u6d32\u5927\u9ec4\u8702\u5bf9\u751f\u6001\u7cfb\u7edf\u548c\u517b\u8702\u4e1a\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f20\u7edf\u5b9a\u4f4d\u5de2\u7a74\u7684\u65b9\u6cd5\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u7cfb\u7edf\u7531\u8f7b\u91cf\u7ea7BLE\u6807\u7b7e\u548c\u57fa\u4e8eGNU Radio\u7684\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\u63a5\u6536\u5668\u7ec4\u6210\uff0c\u901a\u8fc7\u5d4c\u5165\u81ea\u5b9a\u4e49\u4f2a\u566a\u58f0\u5e8f\u5217\u5b9e\u73b0\u76f8\u5173\u68c0\u6d4b\uff0c\u5229\u7528Yagi\u5929\u7ebf\u548cPlutoSDR\u8fdb\u884c\u6570\u5b57\u6ce2\u675f\u626b\u63cf\u4ee5\u786e\u5b9a\u65b9\u5411\u3002", "result": "\u73b0\u573a\u6d4b\u8bd5\u663e\u793a\u7cfb\u7edf\u572850\u7c73\u8ddd\u79bb\u5185\u5177\u6709\u53ef\u9760\u7684\u89d2\u5206\u8fa8\u7387\uff0c\u901a\u4fe1\u8303\u56f4\u53ef\u8fbe360\u7c73\u3002", "conclusion": "\u7cfb\u7edf\u4e3a\u73af\u5883\u76d1\u6d4b\u4e2d\u7684\u5927\u9ec4\u8702\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u5c3d\u7ba1\u8c03\u5236\u65b9\u5f0f\u589e\u52a0\u4e86\u63a5\u6536\u5668\u590d\u6742\u5ea6\uff0c\u4f46\u652f\u6301\u672a\u6765\u7684\u591a\u901a\u9053\u6269\u5c55\u548c\u6807\u7b7e\u8bc6\u522b\u529f\u80fd\u3002"}}
{"id": "2509.03980", "pdf": "https://arxiv.org/pdf/2509.03980", "abs": "https://arxiv.org/abs/2509.03980", "authors": ["Alessandro Mirri", "Vishnu Teja Kunde", "Enrico Paolini", "Jean-Francois Chamberland"], "title": "Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access", "categories": ["eess.SP"], "comment": null, "summary": "This article addresses the problem of multiple preamble detection in random\naccess systems based on orthogonal time frequency space (OTFS) signaling. This\nchallenge is formulated as a structured sparse recovery problem in the complex\ndomain. To tackle it, the authors propose a new approximate message passing\n(AMP) algorithm that enforces double sparsity: the sparse selection of\npreambles and the inherent sparsity of OTFS signals in the delay-Doppler\ndomain. From an algorithmic standpoint, the non-separable complex sparsity\nconstraint necessitates a careful derivation and leads to the design of a novel\nAMP denoiser. Simulation results demonstrate that the proposed method achieves\nrobust detection performance and delivers significant gains over\nstate-of-the-art techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eOTFS\u4fe1\u53f7\u7cfb\u7edf\u4e2d\u524d\u5bfc\u68c0\u6d4b\u7684\u6539\u8fdbAMP\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u7a00\u758f\u6027\u89e3\u51b3\u4e86\u590d\u6742\u57df\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6062\u590d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8eOTFS\u4fe1\u53f7\u7684\u968f\u673a\u63a5\u5165\u7cfb\u7edf\u4e2d\u591a\u524d\u5bfc\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5173\u6ce8\u590d\u6742\u57df\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6062\u590d\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AMP\u7b97\u6cd5\uff0c\u5f3a\u5236\u53cc\u91cd\u7a00\u758f\u6027\uff1a\u524d\u5bfc\u7684\u7a00\u758f\u9009\u62e9\u548cOTFS\u4fe1\u53f7\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u4e2d\u7684\u56fa\u6709\u7a00\u758f\u6027\uff0c\u8bbe\u8ba1\u4e86\u65b0\u578bAMP\u53bb\u566a\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684AMP\u7b97\u6cd5\u5728OTFS\u4fe1\u53f7\u7684\u524d\u5bfc\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.03563", "pdf": "https://arxiv.org/pdf/2509.03563", "abs": "https://arxiv.org/abs/2509.03563", "authors": ["Quan Quan", "Jiwen Xu", "Runxiao Liu", "Yi Ding", "Jiaxing Che", "Kai-Yuan Cai"], "title": "Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach", "categories": ["cs.RO"], "comment": null, "summary": "In comparison with existing approaches, which struggle with scalability,\ncommunication dependency, and robustness against dynamic failures, cooperative\naerial transportation via robot swarms holds transformative potential for\nlogistics and disaster response. Here, we present a physics-inspired\ncooperative transportation approach for flying robot swarms that imitates the\ndissipative mechanics of table-leg load distribution. By developing a\ndecentralized dissipative force model, our approach enables autonomous\nformation stabilization and adaptive load allocation without the requirement of\nexplicit communication. Based on local neighbor robots and the suspended\npayload, each robot dynamically adjusts its position. This is similar to\nenergy-dissipating table leg reactions. The stability of the resultant control\nsystem is rigorously proved. Simulations demonstrate that the tracking errors\nof the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches\nunder the cases of capability variation, cable uncertainty, limited vision, and\npayload variation, respectively. In real-world experiments with six flying\nrobots, the cooperative aerial transportation system achieved a 94% success\nrate under single-robot failure, disconnection events, 25% payload variation,\nand 40% cable length uncertainty, demonstrating strong robustness under outdoor\nwinds up to Beaufort scale 4. Overall, this physics-inspired approach bridges\nswarm intelligence and mechanical stability principles, offering a scalable\nframework for heterogeneous aerial systems to collectively handle complex\ntransportation tasks in communication-constrained environments.", "AI": {"tldr": "\u98de\u673a\u5668\u4eba\u7fa4\u534f\u540c\u8fd0\u8f93\u7684\u65b0\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u684c\u817f\u529b\u5206\u6563\u673a\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u8d1f\u8f7d\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u901a\u4fe1\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u6545\u969c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u98de\u673a\u5668\u4eba\u7fa4\u534f\u540c\u8fd0\u8f93\u5728\u7269\u6d41\u548c\u707e\u5bb3\u54cd\u5e94\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u90bb\u5c45\u548c\u60ac\u6302\u8d1f\u8f7d\u7684\u53bb\u4e2d\u5fc3\u5316\u8017\u6563\u529b\u6a21\u578b\uff0c\u65e0\u9700\u663e\u5f0f\u901a\u4fe1\u5373\u53ef\u5b9e\u73b0\u81ea\u4e3b\u7f16\u961f\u7a33\u5b9a\u548c\u81ea\u9002\u5e94\u8d1f\u8f7d\u5206\u914d\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u5355\u673a\u6545\u969c\u3001\u8d1f\u8f7d\u53d8\u5316\u7b49\u60c5\u51b5\u4e0b\u6210\u529f\u7387\u8fbe94%\u3002", "conclusion": "\u8be5\u7269\u7406\u542f\u53d1\u65b9\u6cd5\u7ed3\u5408\u4e86\u7fa4\u4f53\u667a\u80fd\u548c\u673a\u68b0\u7a33\u5b9a\u6027\u539f\u5219\uff0c\u4e3a\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\u7684\u590d\u6742\u8fd0\u8f93\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2509.03983", "pdf": "https://arxiv.org/pdf/2509.03983", "abs": "https://arxiv.org/abs/2509.03983", "authors": ["Yutong Chen", "Cong Zhou", "Changsheng You", "Shuo Shi"], "title": "Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors", "categories": ["eess.SP"], "comment": null, "summary": "In this letter, we propose a joint frequency-space sparse reconstruction\nmethod for direction-of-arrival (DOA) estimation, which effectively addresses\nthe issues arising from the existence of coherent sources and array\namplitude-phase errors. Specifically, by using an auxiliary source with known\nangles, we first construct the real steering vectors (RSVs) based on the\nspectral peaks of received signals in the frequency domain, which serve as a\ncomplete basis matrix for compensation for amplitude-phase errors. Then, we\nleverage the spectral sparsity of snapshot data in the frequency domain and the\nspatial sparsity of incident directions to perform the DOA estimation according\nto the sparse reconstruction method. The proposed method does not require\niterative optimization, hence exhibiting low computational complexity.\nNumerical results demonstrate that the proposed DOA estimation method achieves\nhigher estimation accuracy for coherent sources as compared to various\nbenchmark schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u9891\u57df\u548c\u7a7a\u95f4\u7a00\u758f\u91cd\u5efa\u7684\u6ce2\u8fbe\u65b9\u5411\uff08DOA\uff09\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u76f8\u5e72\u6e90\u548c\u9635\u5217\u5e45\u76f8\u8bef\u5dee\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u76f8\u5e72\u6e90\u548c\u9635\u5217\u5e45\u76f8\u8bef\u5dee\u5bf9DOA\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u5229\u7528\u8f85\u52a9\u6e90\u6784\u5efa\u771f\u5b9e\u5bfc\u5411\u77e2\u91cf\uff08RSVs\uff09\uff0c\u7ed3\u5408\u9891\u57df\u7a00\u758f\u6027\u548c\u7a7a\u95f4\u7a00\u758f\u6027\u8fdb\u884c\u975e\u8fed\u4ee3\u7a00\u758f\u91cd\u5efa\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u76f8\u5e72\u6e90\u7684\u4f30\u8ba1\u7cbe\u5ea6\u9ad8\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4e14\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u76f8\u5e72\u6e90\u7684DOA\u4f30\u8ba1\u3002"}}
{"id": "2509.03638", "pdf": "https://arxiv.org/pdf/2509.03638", "abs": "https://arxiv.org/abs/2509.03638", "authors": ["David Alvear", "George Turkiyyah", "Shinkyu Park"], "title": "Cooperative Grasping for Collective Object Transport in Constrained Environments", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "We propose a novel framework for decision-making in cooperative grasping for\ntwo-robot object transport in constrained environments. The core of the\nframework is a Conditional Embedding (CE) model consisting of two neural\nnetworks that map grasp configuration information into an embedding space. The\nresulting embedding vectors are then used to identify feasible grasp\nconfigurations that allow two robots to collaboratively transport an object. To\nensure generalizability across diverse environments and object geometries, the\nneural networks are trained on a dataset comprising a range of environment maps\nand object shapes. We employ a supervised learning approach with negative\nsampling to ensure that the learned embeddings effectively distinguish between\nfeasible and infeasible grasp configurations. Evaluation results across a wide\nrange of environments and objects in simulations demonstrate the model's\nability to reliably identify feasible grasp configurations. We further validate\nthe framework through experiments on a physical robotic platform, confirming\nits practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u53cc\u673a\u5668\u4eba\u534f\u4f5c\u6293\u53d6\u548c\u7269\u4f53\u8fd0\u8f93\u7684\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u73af\u5883\u4e2d\u53cc\u673a\u5668\u4eba\u534f\u4f5c\u8fd0\u8f93\u7269\u4f53\u65f6\u6293\u53d6\u914d\u7f6e\u9009\u62e9\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5d4c\u5165\uff08CE\uff09\u6a21\u578b\uff0c\u5305\u542b\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u6293\u53d6\u914d\u7f6e\u4fe1\u606f\u6620\u5c04\u5230\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u8d1f\u91c7\u6837\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u53ef\u9760\u5730\u8bc6\u522b\u53ef\u884c\u6293\u53d6\u914d\u7f6e\uff0c\u5177\u6709\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cc\u673a\u5668\u4eba\u534f\u4f5c\u8fd0\u8f93\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04005", "pdf": "https://arxiv.org/pdf/2509.04005", "abs": "https://arxiv.org/abs/2509.04005", "authors": ["Mingze Gong", "Shuoyao Wang", "Shijian Gao", "Jia Yan", "Suzhi Bi"], "title": "Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation", "categories": ["eess.SP"], "comment": null, "summary": "Semantic communication (SemComm) has emerged as a new communication paradigm.\nTo enhance efficiency, multiple-input-multiple-output (MIMO) technology has\nbeen further integrated into SemComm systems. However, existing MIMO SemComm\nsystems assume perfect channel matrix estimation for channel-adaptive joint\nsource-channel coding, which is impractical due to hardware and pilot overhead\nconstraints. In this paper, we propose a semantic image transmission system\nwith channel matrix and channel noise adaptation, named HANA-JSCC, to cope with\nchannel estimation errors in MIMO systems. We propose a channel matrix adaptor\nthat collaborates with the channel codec to adapt to misaligned channel state\ninformation, thereby mitigating the impact of estimation errors. Since the\nrelationship between the estimated channel matrix and true channel matrix is\nill-posed (one-to-many), we further introduce a two-stage training strategy\nwith knowledge distillation to overcome the convergence difficulties caused by\nthe ill-posed problem. Comparing with the state-of-the-art benchmarks,\nHANA-JSCC achieves $0.40\\sim0.54$dB higher average performance across various\nnoise and estimation error levels in various datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHANA-JSCC\u7684\u8bed\u4e49\u56fe\u50cf\u4f20\u8f93\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3MIMO\u7cfb\u7edf\u4e2d\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u95ee\u9898\u3002\u901a\u8fc7\u4fe1\u9053\u77e9\u9635\u9002\u914d\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7cfb\u7edf\u5728\u566a\u58f0\u548c\u4f30\u8ba1\u8bef\u5dee\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709MIMO\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u5047\u8bbe\u4fe1\u9053\u77e9\u9635\u4f30\u8ba1\u5b8c\u7f8e\uff0c\u4f46\u5b9e\u9645\u4e2d\u7531\u4e8e\u786c\u4ef6\u548c\u5bfc\u9891\u5f00\u9500\u9650\u5236\uff0c\u8fd9\u4e00\u5047\u8bbe\u4e0d\u6210\u7acb\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u5e26\u6765\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86HANA-JSCC\u7cfb\u7edf\uff0c\u5305\u542b\u4fe1\u9053\u77e9\u9635\u9002\u914d\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u4ee5\u9002\u5e94\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u7684\u504f\u5dee\u5e76\u89e3\u51b3\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "result": "HANA-JSCC\u5728\u5404\u79cd\u566a\u58f0\u548c\u4f30\u8ba1\u8bef\u5dee\u6c34\u5e73\u4e0b\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u57fa\u51c6\u9ad8\u51fa0.40~0.54dB\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03658", "pdf": "https://arxiv.org/pdf/2509.03658", "abs": "https://arxiv.org/abs/2509.03658", "authors": ["Antonio Guillen-Perez"], "title": "Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "The ability to generate a diverse and plausible distribution of future\ntrajectories is a critical capability for autonomous vehicle planning systems.\nWhile recent generative models have shown promise, achieving high fidelity,\ncomputational efficiency, and precise control remains a significant challenge.\nIn this paper, we present the \\textbf{Efficient Virtuoso}, a conditional latent\ndiffusion model for goal-conditioned trajectory planning. Our approach\nintroduces a novel two-stage normalization pipeline that first scales\ntrajectories to preserve their geometric aspect ratio and then normalizes the\nresulting PCA latent space to ensure a stable training target. The denoising\nprocess is performed efficiently in this low-dimensional latent space by a\nsimple MLP denoiser, which is conditioned on a rich scene context fused by a\npowerful Transformer-based StateEncoder. We demonstrate that our method\nachieves state-of-the-art performance on the Waymo Open Motion Dataset,\nreaching a \\textbf{minADE of 0.25}. Furthermore, through a rigorous ablation\nstudy on goal representation, we provide a key insight: while a single endpoint\ngoal can resolve strategic ambiguity, a richer, multi-step sparse route is\nessential for enabling the precise, high-fidelity tactical execution that\nmirrors nuanced human driving behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f52\u4e00\u5316\u7ba1\u9053\u548c\u7b80\u5355MLP\u53bb\u566a\u5668\uff0c\u5b9e\u73b0\u4e86\u5728Waymo\u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u8f66\u8f86\u89c4\u5212\u7cfb\u7edf\u4e2d\u751f\u6210\u591a\u6837\u4e14\u5408\u7406\u672a\u6765\u8f68\u8ff9\u7684\u9ad8\u4fdd\u771f\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u786e\u63a7\u5236\u6311\u6218\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u4e24\u9636\u6bb5\u5f52\u4e00\u5316\u7ba1\u9053\uff08\u51e0\u4f55\u6bd4\u4f8b\u7f29\u653e\u548cPCA\u6f5c\u5728\u7a7a\u95f4\u5f52\u4e00\u5316\uff09\uff0c\u7ed3\u5408Transformer\u573a\u666f\u7f16\u7801\u5668\u548cMLP\u53bb\u566a\u5668\u3002", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u8fbe\u5230minADE 0.25\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u591a\u6b65\u9aa4\u7a00\u758f\u8def\u7ebf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5355\u7aef\u70b9\u76ee\u6807\u53ef\u89e3\u51b3\u6218\u7565\u6a21\u7cca\u6027\uff0c\u4f46\u591a\u6b65\u9aa4\u7a00\u758f\u8def\u7ebf\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6218\u672f\u6267\u884c\u7684\u5173\u952e\u3002"}}
{"id": "2509.04055", "pdf": "https://arxiv.org/pdf/2509.04055", "abs": "https://arxiv.org/abs/2509.04055", "authors": ["Benedikt Geiger", "Fan Liu", "Shihang Lu", "Andrej Rode", "Daniel Gil Gaviria", "Charlotte Muth", "Laurent Schmalen"], "title": "Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation", "categories": ["eess.SP"], "comment": "13 pages, 14 figures, Submitted to IEEE Transactions on\n  Communications (TCOM) for peer review", "summary": "Integrated sensing and communications (ISAC) promises new use cases for\nmobile communication systems by reusing the communication signal for radar-like\nsensing. However, sensing and communications (S&C) impose conflicting\nrequirements on the modulation format, resulting in a tradeoff between their\ncorresponding performance. This paper investigates constellation shaping as a\nmeans to simultaneously improve S&C performance in orthogonal frequency\ndivision multiplexing (OFDM)-based ISAC systems. We begin by deriving how the\ntransmit symbols affect detection performance and derive theoretical lower and\nupper bounds on the maximum achievable information rate under a given sensing\nconstraint. Using an autoencoder-based optimization, we investigate geometric,\nprobabilistic, and joint constellation shaping, where joint shaping combines\nboth approaches, employing both optimal maximum a-posteriori decoding and\npractical bit-metric decoding. Our results show that constellation shaping\nenables a flexible trade-off between S&C, can approach the derived upper bound,\nand significantly outperforms conventional modulation formats. Motivated by its\npractical implementation feasibility, we review probabilistic amplitude shaping\n(PAS) and propose a generalization tailored to ISAC. For this generalization,\nwe propose a low-complexity log-likelihood ratio computation with negligible\nrate loss. We demonstrate that combining conventional and generalized PAS\nenables a flexible and low-complexity tradeoff between S&C, closely approaching\nthe performance of joint constellation shaping.", "AI": {"tldr": "\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u901a\u8fc7\u91cd\u7528\u901a\u4fe1\u4fe1\u53f7\u8fdb\u884c\u96f7\u8fbe\u5f0f\u4f20\u611f\uff0c\u4f46\u8c03\u5236\u683c\u5f0f\u7684\u51b2\u7a81\u8981\u6c42\u5bfc\u81f4\u6027\u80fd\u6743\u8861\u3002\u672c\u6587\u901a\u8fc7\u661f\u5ea7\u6574\u5f62\uff08\u5305\u62ec\u51e0\u4f55\u3001\u6982\u7387\u548c\u8054\u5408\u6574\u5f62\uff09\u4f18\u5316\u6b63\u4ea4\u9891\u5206\u590d\u7528\uff08OFDM\uff09\u7cfb\u7edf\u7684S&C\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u5e7f\u4e49\u6982\u7387\u5e45\u5ea6\u6574\u5f62\uff08PAS\uff09\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u6b63\u4ea4\u9891\u5206\u590d\u7528\uff08OFDM\uff09\u7cfb\u7edf\u4e2d\u901a\u8fc7\u661f\u5ea7\u6574\u5f62\u540c\u65f6\u4f18\u5316\u4f20\u611f\u548c\u901a\u4fe1\u6027\u80fd\uff0c\u89e3\u51b3\u5176\u8c03\u5236\u683c\u5f0f\u7684\u51b2\u7a81\u8981\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u7f16\u7801\u5668\u4f18\u5316\u51e0\u4f55\u3001\u6982\u7387\u548c\u8054\u5408\u661f\u5ea7\u6574\u5f62\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u6982\u7387\u5e45\u5ea6\u6574\u5f62\uff08PAS\uff09\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u5bf9\u6570\u4f3c\u7136\u6bd4\u8ba1\u7b97\u3002", "result": "\u661f\u5ea7\u6574\u5f62\u5b9e\u73b0\u4e86S&C\u6027\u80fd\u7684\u7075\u6d3b\u6743\u8861\uff0c\u63a5\u8fd1\u7406\u8bba\u4e0a\u9650\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8c03\u5236\u683c\u5f0f\u3002\u5e7f\u4e49PAS\u65b9\u6cd5\u5177\u6709\u4f4e\u590d\u6742\u5ea6\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u661f\u5ea7\u6574\u5f62\u548c\u5e7f\u4e49PAS\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848\uff0c\u63a5\u8fd1\u8054\u5408\u6574\u5f62\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03690", "pdf": "https://arxiv.org/pdf/2509.03690", "abs": "https://arxiv.org/abs/2509.03690", "authors": ["Kelvin Daniel Gonzalez Amador"], "title": "Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet", "categories": ["cs.RO", "I.2.9"], "comment": "9 pages, 8 figures, 4 tables. Submitted as preprint", "summary": "Accessible communication through sign language is vital for deaf communities,\n1 yet robotic solutions are often costly and limited. This study presents\nVulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand\ncapable of reproducing the full 3 American Sign Language (ASL) alphabet (52\nsigns for right- and left-hand configurations). 4 The system employs 23\ndirect-drive servo actuators for precise finger and wrist movements, 5\ncontrolled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid\nupper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3\nachieves complete ASL 7 coverage with a reversible design. All CAD files and\ncode are released under permissive 8 open-source licenses to enable\nreplication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL\nhandshapes, while a participant study (n = 33) achieved 96.97% recognition 10\naccuracy, improving to 98.78% after video demonstration. VulcanV3 advances\nassistive 11 robotics by combining affordability, full ASL coverage, and\nambidexterity in an openly 12 shared platform, contributing to accessible\ncommunication technologies and inclusive 13 innovation.", "AI": {"tldr": "VulcanV3\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u5f00\u6e90\u673a\u5668\u4eba\u624b\uff0c\u80fd\u7cbe\u786e\u590d\u73b052\u79cd\u7f8e\u56fd\u624b\u8bed\u624b\u52bf\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\u8fbe98.78%\uff0c\u63a8\u52a8\u4e86\u65e0\u969c\u788d\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u804b\u4eba\u793e\u533a\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u624b\u8bed\u7684\u7cbe\u786e\u8868\u8fbe\uff0c\u5f25\u8865\u73b0\u6709\u9ad8\u6602\u4e14\u529f\u80fd\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u63d0\u51faVulcanV3\uff0c\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u30013D\u6253\u5370\u7684\u53cc\u624b\u901a\u7528\u673a\u5668\u4eba\u624b\uff0c\u91c7\u752823\u4e2a\u76f4\u63a5\u9a71\u52a8\u4f3a\u670d\u6267\u884c\u5668\u63a7\u5236\u624b\u6307\u548c\u624b\u8155\u52a8\u4f5c\uff0c\u901a\u8fc7Arduino Mega\u548c\u53ccPCA9685\u6a21\u5757\u5b9e\u73b0\u7cbe\u51c6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVulcanV3\u80fd\u591f\u51c6\u786e\u590d\u73b0\u5168\u90e852\u79cd\u7f8e\u56fd\u624b\u8bed\u5b57\u6bcd\u624b\u52bf\uff0c\u5e76\u5728\u53c2\u4e0e\u8005\u7814\u7a76\u4e2d\u8fbe\u523096.97%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff08\u89c6\u9891\u6f14\u793a\u540e\u63d0\u5347\u81f398.78%\uff09\u3002", "conclusion": "VulcanV3\u901a\u8fc7\u7ed3\u5408\u4f4e\u6210\u672c\u3001\u5b8c\u6574\u624b\u8bed\u8986\u76d6\u548c\u53cc\u624b\u901a\u7528\u8bbe\u8ba1\u4e8e\u4e00\u4e2a\u5f00\u653e\u5171\u4eab\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u548c\u65e0\u969c\u788d\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.04309", "pdf": "https://arxiv.org/pdf/2509.04309", "abs": "https://arxiv.org/abs/2509.04309", "authors": ["R. Zhang", "J. Xue", "T. Zhang"], "title": "Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection", "categories": ["eess.SP"], "comment": "25 pages, 20 figures, journal extended by an IEEE ICC conference\n  article", "summary": "Reliable slow-moving weak target detection in complicated environments is\nchallenging due to the masking effects from the surrounding strong reflectors.\nThe traditional Moving Target Indication (MTI) may suppress the echoes from not\nonly the static interference objects (IOs), but also the desired slow-moving\nweak target. According to the low-rank and sparse properties of the\nrange-velocity maps across different radar scans, a novel clutter suppression\nscheme based on the Go decomposition (Godec) framework is proposed in this\npaper. The simulation results show that with the existence of masking effects,\nthe target detection scheme based on Godec clutter suppression can reliably\ndetect the slow-moving weak target, compared to the traditional MTI-based\nscheme. Besides, the time consumption comparison is conducted, demonstrating\nthat the proposed solution is one that sacrifices time complexity in exchange\nfor enhanced reliability. Additionally, the tradeoffs among the number of false\nalarm cells, the detection probability and the iteration times for convergence\nhave been revealed, guiding parameter settings of the proposed solution in\npractical applications. Experiment validation is also conducted to verify the\nproposed solution, providing further insight into the scenarios where the\nsolution is most applicable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGo\u5206\u89e3\uff08Godec\uff09\u7684\u6742\u6ce2\u6291\u5236\u65b9\u6848\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u53ef\u9760\u68c0\u6d4b\u6162\u901f\u5f31\u76ee\u6807\u3002\u76f8\u6bd4\u4f20\u7edfMTI\u65b9\u6cd5\uff0cGodec\u65b9\u6848\u80fd\u66f4\u53ef\u9760\u5730\u68c0\u6d4b\u76ee\u6807\uff0c\u4f46\u727a\u7272\u4e86\u65f6\u95f4\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u73af\u5883\u4e2d\u6162\u901f\u5f31\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edfMTI\u65b9\u6cd5\u5bf9\u76ee\u6807\u4fe1\u53f7\u7684\u6291\u5236\u3002", "method": "\u5229\u7528\u8ddd\u79bb-\u901f\u5ea6\u6620\u5c04\u7684\u4f4e\u79e9\u548c\u7a00\u758f\u7279\u6027\uff0c\u57fa\u4e8eGodec\u6846\u67b6\u8bbe\u8ba1\u65b0\u578b\u6742\u6ce2\u6291\u5236\u65b9\u6848\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cGodec\u65b9\u6848\u5728\u5b58\u5728\u63a9\u853d\u6548\u5e94\u65f6\u80fd\u53ef\u9760\u68c0\u6d4b\u76ee\u6807\uff0c\u4f46\u65f6\u95f4\u6d88\u8017\u8f83\u9ad8\u3002", "conclusion": "Godec\u65b9\u6848\u5728\u53ef\u9760\u6027\u548c\u65f6\u95f4\u590d\u6742\u6027\u4e4b\u95f4\u505a\u51fa\u6743\u8861\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u4e86\u53c2\u6570\u8bbe\u7f6e\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.03804", "pdf": "https://arxiv.org/pdf/2509.03804", "abs": "https://arxiv.org/abs/2509.03804", "authors": ["Ad-Deen Mahbub", "Md Ragib Shaharear"], "title": "Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 10 figures", "summary": "Accurate real-time buoyancy modeling is essential for high-fidelity\nAutonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a\nnative buoyancy system, requiring external solutions for precise underwater\nphysics. This paper presents a novel convex hull-based approach to dynamically\ncompute the submerged volume of an AUV in real time. By extracting mesh\ngeometry from the simulation environment and calculating the hull portion\nintersecting the water level along the z-axis, our method enhances accuracy\nover traditional geometric approximations. A cross-sectional area extension\nreduces computational overhead, enabling efficient buoyant force updates that\nadapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested\non a custom AUV design for SAUVC 2025, this approach delivers real-time\nperformance and scalability, improving simulation fidelity for underwater\nrobotics research without precomputed hydrodynamic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u5305\u7684\u5b9e\u65f6\u6d6e\u529b\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347AUV\u6a21\u62df\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "NVIDIA Isaac Sim\u7f3a\u4e4f\u539f\u751f\u6d6e\u529b\u7cfb\u7edf\uff0c\u9700\u8981\u5916\u90e8\u89e3\u51b3\u65b9\u6848\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u6c34\u4e0b\u7269\u7406\u6a21\u62df\u3002", "method": "\u91c7\u7528\u51f8\u5305\u65b9\u6cd5\u52a8\u6001\u8ba1\u7b97AUV\u5728\u6c34\u4e2d\u7684\u4f53\u79ef\uff0c\u5e76\u7ed3\u5408\u6a2a\u622a\u9762\u79ef\u8ba1\u7b97\u4f18\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u65b9\u6cd5\u5728SAUVC 2025\u7684AUV\u8bbe\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u62df\u903c\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9884\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u6c34\u4e0b\u673a\u5668\u4eba\u7814\u7a76\u7684\u6a21\u62df\u7cbe\u5ea6\u3002"}}
{"id": "2509.04412", "pdf": "https://arxiv.org/pdf/2509.04412", "abs": "https://arxiv.org/abs/2509.04412", "authors": ["Guangyu Lei", "Yuqi Ping", "Tianhao Liang", "Huahao Ding", "Tingting Zhang"], "title": "Relative Localization of UAV Swarms in GNSS-Denied Conditions", "categories": ["eess.SP", "cs.SY", "eess.SY", "Primary 93C85, Secondary 68T42, 94A12, 90C90", "H.4.3"], "comment": "Manuscript submitted to IEEE Globecom 2025", "summary": "Relative localization of unmanned aerial vehicle (UAV) swarms in global\nnavigation satellite system (GNSS) denied environments is essential for\nemergency rescue and battlefield reconnaissance. Existing methods suffer from\nsignificant localization errors among UAVs due to packet loss and high\ncomputational complexity in large swarms. This paper proposes a\nclustering-based framework where the UAVs simultaneously use communication\nsignals for channel estimation and ranging. Firstly, the spectral clustering is\nutilized to divide the UAV swarm into different sub-clusters, where matrix\ncompletion and multidimensional scaling yield high-precision relative\ncoordinates. Subsequently, a global map is created by the inter-cluster anchor\nfusion. A case study of UAV integrated communication and sensing (ISAC) system\nis presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for\nranging and communication. Experimental results show that the proposed method\nreduces localization errors in large swarms and loss of range information. It\nalso explores the impact of signal parameters on communication and\nlocalization, highlighting the interplay between communication and localization\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u65e0\u4eba\u673a\u7fa4\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86GNSS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u8bef\u5dee\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002\u901a\u8fc7\u9891\u8c31\u805a\u7c7b\u548c\u4fe1\u53f7\u878d\u5408\uff0c\u63d0\u9ad8\u4e86\u5927\u96c6\u7fa4\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728GNSS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u7fa4\u7684\u76f8\u5bf9\u5b9a\u4f4d\u5bf9\u7d27\u6025\u6551\u63f4\u548c\u6218\u573a\u4fa6\u5bdf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u4e22\u5305\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u5bfc\u81f4\u5b9a\u4f4d\u8bef\u5dee\u8f83\u5927\u3002", "method": "\u5229\u7528\u9891\u8c31\u805a\u7c7b\u5c06\u65e0\u4eba\u673a\u7fa4\u5206\u6210\u5b50\u96c6\u7fa4\uff0c\u7ed3\u5408\u77e9\u9635\u8865\u5168\u548c\u591a\u7ef4\u7f29\u653e\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76f8\u5bf9\u5750\u6807\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u7c07\u95f4\u951a\u70b9\u878d\u5408\u751f\u6210\u5168\u5c40\u5730\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5927\u578b\u65e0\u4eba\u673a\u7fa4\u4e2d\u7684\u5b9a\u4f4d\u8bef\u5dee\u548c\u6d4b\u8ddd\u4fe1\u606f\u4e22\u5931\uff0c\u5e76\u63a2\u7d22\u4e86\u4fe1\u53f7\u53c2\u6570\u5bf9\u901a\u4fe1\u548c\u5b9a\u4f4d\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7fa4\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u5b9a\u4f4d\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2509.03842", "pdf": "https://arxiv.org/pdf/2509.03842", "abs": "https://arxiv.org/abs/2509.03842", "authors": ["Guanglu Jia", "Ceng Zhang", "Gregory S. Chirikjian"], "title": "INGRID: Intelligent Generative Robotic Design Using Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 6 figures", "summary": "The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.", "AI": {"tldr": "INGRID\u662f\u4e00\u79cd\u901a\u8fc7\u6df1\u5ea6\u96c6\u6210\u673a\u5668\u4eba\u7406\u8bba\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u8bbe\u8ba1\u5e76\u884c\u673a\u5668\u4eba\u673a\u5236\uff0c\u89e3\u653e\u673a\u5668\u4eba\u667a\u80fd\u7684\u786c\u4ef6\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u4e32\u884c\u673a\u5236\u786c\u4ef6\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u81ea\u52a8\u8bbe\u8ba1\u5e76\u884c\u673a\u5668\u4eba\u673a\u5236\u4ee5\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "INGRID\u7ed3\u5408\u4e86\u4e92\u53cd\u87ba\u65cb\u7406\u8bba\u548c\u8fd0\u52a8\u5b66\u7efc\u5408\u65b9\u6cd5\uff0c\u5c06\u8bbe\u8ba1\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7ea6\u675f\u5206\u6790\u3001\u8fd0\u52a8\u5173\u8282\u751f\u6210\u3001\u94fe\u6784\u5efa\u548c\u5b8c\u6574\u673a\u5236\u8bbe\u8ba1\u56db\u4e2a\u9010\u6b65\u4efb\u52a1\u3002", "result": "INGRID\u80fd\u591f\u751f\u6210\u5177\u6709\u56fa\u5b9a\u548c\u53ef\u53d8\u79fb\u52a8\u6027\u7684\u65b0\u578b\u5e76\u884c\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u4efb\u52a1\u5bfc\u5411\u8bbe\u8ba1\u80fd\u529b\u3002", "conclusion": "INGRID\u4e3a\u673a\u5236\u667a\u80fd\u5960\u5b9a\u57fa\u7840\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8bbe\u8ba1\u5b9a\u5236\u5316\u5e76\u884c\u673a\u5668\u4eba\uff0c\u63a8\u52a8\u5177\u8eabAI\u7cfb\u7edf\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2509.03521", "pdf": "https://arxiv.org/pdf/2509.03521", "abs": "https://arxiv.org/abs/2509.03521", "authors": ["Timothee Robert", "MohammadAli Shaeri", "Mahsa Shoaran"], "title": "BiND: A Neural Discriminator-Decoder for Accurate Bimanual Trajectory Prediction in Brain-Computer Interfaces", "categories": ["q-bio.NC", "cs.AI", "eess.SP"], "comment": "Accepted for publication in IEEE Neural Engineering (NER)\n  Conference'25", "summary": "Decoding bimanual hand movements from intracortical recordings remains a\ncritical challenge for brain-computer interfaces (BCIs), due to overlapping\nneural representations and nonlinear interlimb interactions. We introduce BiND\n(Bimanual Neural Discriminator-Decoder), a two-stage model that first\nclassifies motion type (unimanual left, unimanual right, or bimanual) and then\nuses specialized GRU-based decoders, augmented with a trial-relative time\nindex, to predict continuous 2D hand velocities. We benchmark BiND against six\nstate-of-the-art models (SVR, XGBoost, FNN, CNN, Transformer, GRU) on a\npublicly available 13-session intracortical dataset from a tetraplegic patient.\nBiND achieves a mean $R^2$ of 0.76 ($\\pm$0.01) for unimanual and 0.69\n($\\pm$0.03) for bimanual trajectory prediction, surpassing the next-best model\n(GRU) by 2% in both tasks. It also demonstrates greater robustness to session\nvariability than all other benchmarked models, with accuracy improvements of up\nto 4% compared to GRU in cross-session analyses. This highlights the\neffectiveness of task-aware discrimination and temporal modeling in enhancing\nbimanual decoding.", "AI": {"tldr": "BiND\u6a21\u578b\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u5206\u7c7b\u548cGRU\u89e3\u7801\uff09\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u53cc/\u5355\u624b\u8fd0\u52a8\u8f68\u8ff9\u7684\u89e3\u7801\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u56e0\u795e\u7ecf\u8868\u793a\u91cd\u53e0\u548c\u975e\u7ebf\u6027\u4ea4\u4e92\u5bfc\u81f4\u7684BCI\u89e3\u7801\u96be\u9898\u3002", "method": "BiND\u91c7\u7528\u5148\u5206\u7c7b\u540e\u89e3\u7801\u7684\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u7ed3\u5408GRU\u548c\u65f6\u95f4\u7d22\u5f15\u3002", "result": "\u5728\u5355/\u53cc\u624b\u4efb\u52a1\u4e2dBiND\u7684R\u00b2\u5206\u522b\u8fbe0.76\u548c0.69\uff0c\u4e14\u8de8\u4f1a\u8bdd\u7a33\u5b9a\u6027\u4f18\u4e8eGRU\u8fbe4%\u3002", "conclusion": "BiND\u9a8c\u8bc1\u4e86\u4efb\u52a1\u611f\u77e5\u548c\u65f6\u95f4\u5efa\u6a21\u5bf9\u63d0\u5347BCI\u89e3\u7801\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.03859", "pdf": "https://arxiv.org/pdf/2509.03859", "abs": "https://arxiv.org/abs/2509.03859", "authors": ["Haichao Zhang", "Haonan Yu", "Le Zhao", "Andrew Choi", "Qinxun Bai", "Yiqing Yang", "Wei Xu"], "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator", "categories": ["cs.RO"], "comment": "Project: https://horizonrobotics.github.io/gail/SLIM", "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u79fb\u52a8\u6293\u53d6\u4efb\u52a1\uff0c\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u8fd180%\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u79fb\u52a8\u6293\u53d6\u4efb\u52a1\u4e2d\u6280\u80fd\u591a\u6837\u6027\u3001\u957f\u4efb\u52a1\u5468\u671f\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u641c\u7d22\u3001\u63a5\u8fd1\u3001\u6293\u53d6\u3001\u8fd0\u8f93\u548c\u653e\u7f6e\u7b49\u884c\u4e3a\u7684\u65e0\u7f1d\u8854\u63a5\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\u6210\u529f\u7387\u8fbe\u5230\u8fd180%\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u591a\u79cd\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u79fb\u52a8\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03738", "pdf": "https://arxiv.org/pdf/2509.03738", "abs": "https://arxiv.org/abs/2509.03738", "authors": ["Bahareh Tolooshams", "Ailsa Shen", "Anima Anandkumar"], "title": "Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "comment": "Tolooshams and Shen has equal contribution. preprint", "summary": "We frame the problem of unifying representations in neural models as one of\nsparse model recovery and introduce a framework that extends sparse\nautoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,\nenabling mechanistic interpretability of large neural operators (NO). While the\nPlatonic Representation Hypothesis suggests that neural networks converge to\nsimilar representations across architectures, the representational properties\nof neural operators remain underexplored despite their growing importance in\nscientific computing. We compare the inference and training dynamics of SAEs,\nlifted-SAE, and SAE neural operators. We highlight how lifting and operator\nmodules introduce beneficial inductive biases, enabling faster recovery,\nimproved recovery of smooth concepts, and robust inference across varying\nresolutions, a property unique to neural operators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u795e\u7ecf\u6a21\u578b\u7684\u8868\u793a\uff0c\u5e76\u63a2\u8ba8\u4e86\u795e\u7ecf\u7b97\u5b50\u7684\u8868\u793a\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u7b97\u5b50\u7684\u8868\u793a\u7279\u6027\uff0c\u5c3d\u7ba1\u5176\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u8868\u793a\u7279\u6027\u4ecd\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5230\u63d0\u5347\u7a7a\u95f4\u548c\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\uff0c\u5f15\u5165\u4e86\u63d0\u5347SAE\u548cSAE\u795e\u7ecf\u7b97\u5b50\uff0c\u6bd4\u8f83\u4e86\u4e09\u8005\u7684\u63a8\u65ad\u548c\u8bad\u7ec3\u52a8\u6001\u3002", "result": "\u63d0\u5347\u548c\u7b97\u5b50\u6a21\u5757\u5f15\u5165\u4e86\u6709\u76ca\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6062\u590d\u3001\u6539\u8fdb\u7684\u5e73\u6ed1\u6982\u5ff5\u6062\u590d\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u9c81\u68d2\u63a8\u65ad\u3002", "conclusion": "\u6846\u67b6\u6210\u529f\u5c55\u793a\u4e86\u795e\u7ecf\u7b97\u5b50\u5728\u8868\u793a\u7edf\u4e00\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u7b97\u5b50\u7684\u673a\u7406\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.03889", "pdf": "https://arxiv.org/pdf/2509.03889", "abs": "https://arxiv.org/abs/2509.03889", "authors": ["Neha Sunil", "Megha Tippur", "Arnau Saumell", "Edward Adelson", "Alberto Rodriguez"], "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/", "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u81c2\u89c6\u89c9\u89e6\u89c9\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u590d\u6742\u914d\u7f6e\u7684\u8863\u7269\u64cd\u4f5c\u95ee\u9898\u3002", "motivation": "\u8863\u7269\u64cd\u4f5c\u56f0\u96be\u5728\u4e8e\u5176\u590d\u6742\u7684\u914d\u7f6e\u3001\u52a8\u6001\u7684\u6750\u6599\u7279\u6027\u53ca\u9891\u7e41\u7684\u81ea\u906e\u6321\u3002\u73b0\u6709\u7684\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5e73\u6574\u8863\u7269\u6216\u5047\u8bbe\u5173\u952e\u7279\u5f81\u53ef\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u5bc6\u96c6\u89c6\u89c9\u5bf9\u5e94\u548c\u89e6\u89c9\u76d1\u7763\u6293\u53d6\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u62df\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u53cd\u5e94\u72b6\u6001\u673a\u8c03\u6574\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5904\u7406\u9ad8\u5ea6\u906e\u6321\u7684\u684c\u9762\u548c\u7a7a\u4e2d\u914d\u7f6e\uff0c\u5e76\u5728\u6298\u53e0\u548c\u60ac\u6302\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6548\u679c\u3002", "conclusion": "\u5bc6\u96c6\u63cf\u8ff0\u7b26\u4e3a\u5176\u4ed6\u89c4\u5212\u6a21\u5f0f\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63a8\u52a8\u4e86\u8863\u7269\u64cd\u4f5c\u7684\u666e\u9002\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.04088", "pdf": "https://arxiv.org/pdf/2509.04088", "abs": "https://arxiv.org/abs/2509.04088", "authors": ["Farah Baracat", "Agnese Grison", "Dario Farina", "Giacomo Indiveri", "Elisa Donati"], "title": "Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays", "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "Restoring naturalistic finger control in assistive technologies requires the\ncontinuous decoding of motor intent with high accuracy, efficiency, and\nrobustness. Here, we present a spike-based decoding framework that integrates\nspiking neural networks (SNNs) with motor unit activity extracted from\nhigh-density intramuscular microelectrode arrays. We demonstrate simultaneous\nand proportional decoding of individual finger forces from motor unit spike\ntrains during isometric contractions at 15% of maximum voluntary contraction\nusing SNNs. We systematically evaluated alternative SNN decoder configurations\nand compared two possible input modalities: physiologically grounded motor unit\nspike trains and spike-encoded intramuscular EMG signals. Through this\ncomparison, we quantified trade-offs between decoding accuracy, memory\nfootprint, and robustness to input errors. The results showed that shallow SNNs\ncan reliably decode finger-level motor intent with competitive accuracy and\nminimal latency, while operating with reduced memory requirements and without\nthe need for external preprocessing buffers. This work provides a practical\nblueprint for integrating SNNs into finger-level force decoding systems,\ndemonstrating how the choice of input representation can be strategically\ntailored to meet application-specific requirements for accuracy, robustness,\nand memory efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9ad8\u5bc6\u5ea6\u808c\u5185\u5fae\u7535\u6781\u9635\u5217\u4e2d\u89e3\u7801\u624b\u6307\u529b\uff0c\u5bf9\u6bd4\u4e86\u4e24\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u9a8c\u8bc1\u4e86\u6d45\u5c42SNN\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u6062\u590d\u8f85\u52a9\u6280\u672f\u4e2d\u81ea\u7136\u7684\u624b\u6307\u63a7\u5236\u9700\u8981\u9ad8\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e2a\u4f53\u624b\u6307\u529b\u7684\u89e3\u7801\u3002", "method": "\u91c7\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7ed3\u5408\u4ece\u808c\u5185\u5fae\u7535\u6781\u9635\u5217\u63d0\u53d6\u7684\u8fd0\u52a8\u5355\u5143\u6d3b\u52a8\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540cSNN\u914d\u7f6e\u53ca\u8f93\u5165\u6a21\u6001\uff08\u8fd0\u52a8\u5355\u5143\u8109\u51b2\u5e8f\u5217\u4e0eEMG\u4fe1\u53f7\uff09\u7684\u89e3\u7801\u6548\u679c\u3002", "result": "\u6d45\u5c42SNN\u80fd\u4ee5\u4f4e\u5ef6\u8fdf\u3001\u5c0f\u5185\u5b58\u9700\u6c42\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u89e3\u7801\uff0c\u65e0\u9700\u5916\u90e8\u9884\u5904\u7406\u7f13\u51b2\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSNN\u5728\u624b\u6307\u529b\u89e3\u7801\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\uff0c\u8f93\u5165\u6a21\u6001\u7684\u9009\u62e9\u53ef\u6839\u636e\u5e94\u7528\u9700\u6c42\u4f18\u5316\u51c6\u786e\u6027\u3001\u7a33\u5065\u6027\u548c\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2509.04016", "pdf": "https://arxiv.org/pdf/2509.04016", "abs": "https://arxiv.org/abs/2509.04016", "authors": ["Branimir \u0106aran", "Vladimir Mili\u0107", "Marko \u0160vaco", "Bojan Jerbi\u0107"], "title": "Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot", "categories": ["cs.RO"], "comment": "ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT\n  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025", "summary": "This paper presents the design of a pose estimator for a four wheel\nindependent steer four wheel independent drive (4WIS4WID) wall climbing mobile\nrobot, based on the fusion of multimodal measurements, including wheel\nodometry, visual odometry, and an inertial measurement unit (IMU) data using\nExtended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose\nestimator is a critical component of wall climbing mobile robots, as their\noperational environment involves carrying precise measurement equipment and\nmaintenance tools in construction, requiring information about pose on the\nbuilding at the time of measurement. Due to the complex geometry and material\nproperties of building facades, the use of traditional localization sensors\nsuch as laser, ultrasonic, or radar is often infeasible for wall-climbing\nrobots. Moreover, GPS-based localization is generally unreliable in these\nenvironments because of signal degradation caused by reinforced concrete and\nelectromagnetic interference. Consequently, robot odometry remains the primary\nsource of velocity and position information, despite being susceptible to drift\ncaused by both systematic and non-systematic errors. The calibrations of the\nrobot's systematic parameters were conducted using nonlinear optimization and\nLevenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting\nmethods, while Genetic algorithm and Particle swarm were used as\nstochastic-based methods for kinematic parameter calibration. Performance and\nresults of the calibration methods and pose estimators were validated in detail\nwith experiments on the experimental mobile wall climbing robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6d4b\u91cf\u878d\u5408\uff08\u8f66\u8f6e\u91cc\u7a0b\u8ba1\u3001\u89c6\u89c9\u91cc\u7a0b\u8ba1\u548cIMU\u6570\u636e\uff09\u7684\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u548c\u9a71\u52a8\u7684\u722c\u5899\u673a\u5668\u4eba\u4f4d\u59ff\u4f30\u8ba1\u5668\u8bbe\u8ba1\uff0c\u91c7\u7528\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EKF\uff09\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u3002", "motivation": "\u722c\u5899\u673a\u5668\u4eba\u5728\u5efa\u7b51\u73af\u5883\u4e2d\u9700\u8981\u643a\u5e26\u7cbe\u786e\u6d4b\u91cf\u8bbe\u5907\u548c\u7ef4\u62a4\u5de5\u5177\uff0c\u56e0\u6b64\u5176\u4f4d\u59ff\u4fe1\u606f\u5bf9\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u5b9a\u4f4d\u4f20\u611f\u5668\uff08\u5982\u6fc0\u5149\u3001\u8d85\u58f0\u6ce2\u6216\u96f7\u8fbe\uff09\u56e0\u5efa\u7b51\u590d\u6742\u51e0\u4f55\u548c\u6750\u6599\u7279\u6027\u800c\u4e0d\u53ef\u884c\uff0cGPS\u4e5f\u56e0\u4fe1\u53f7\u8870\u51cf\u4e0d\u53ef\u9760\u3002", "method": "\u91c7\u7528EKF\u548cUKF\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u5206\u522b\u4f7f\u7528\u975e\u7ebf\u6027\u4f18\u5316\u3001Levenberg-Marquardt\u65b9\u6cd5\u53ca\u9057\u4f20\u7b97\u6cd5\u3001\u7c92\u5b50\u7fa4\u4f18\u5316\u8fdb\u884c\u7cfb\u7edf\u53c2\u6570\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u6821\u51c6\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6821\u51c6\u65b9\u6cd5\u548c\u4f4d\u59ff\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u4e0e\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u722c\u5899\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2509.04199", "pdf": "https://arxiv.org/pdf/2509.04199", "abs": "https://arxiv.org/abs/2509.04199", "authors": ["Dieter Schwarzmann", "Simon K\u00e4ser"], "title": "On the Effect of Sampling-Time Jitter", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "Submitted for review as letter in IEEE Journal for Transactions on\n  Control Systems Technology", "summary": "This brief, aimed at practitioners, offers an analysis of the effect of\nsampling-time jitter, i. e., the error produced by execution-time inaccuracies.\nWe propose reinterpreting jitter-afflicted linear time-invariant systems\nthrough equivalent jitter-free analogs. By constructing a perceived system that\nabsorbs the effects of timing perturbations into its dynamics, we find an\naffine scaling of jitter. We examine both measurement and implementation\nscenarios, demonstrating that the presence of jitter effectively scales the\nsystem matrices. Moreover, we observe that, in the Laplace domain, jitter can\nbe interpreted as a frequency scaling.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u91c7\u6837\u65f6\u95f4\u6296\u52a8\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5c06\u6296\u52a8\u5f71\u54cd\u7684\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u91cd\u65b0\u89e3\u91ca\u4e3a\u7b49\u6548\u7684\u65e0\u6296\u52a8\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u5206\u6790\u91c7\u6837\u65f6\u95f4\u6296\u52a8\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u611f\u77e5\u7cfb\u7edf\uff0c\u5c06\u65f6\u95f4\u6270\u52a8\u7684\u5f71\u54cd\u7eb3\u5165\u5176\u52a8\u6001\u4e2d\uff0c\u63d0\u51fa\u4e86\u7b49\u6548\u7684\u65e0\u6296\u52a8\u7cfb\u7edf\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6296\u52a8\u53ef\u4ee5\u89c6\u4e3a\u7cfb\u7edf\u77e9\u9635\u7684\u7f29\u653e\uff0c\u5e76\u5728\u62c9\u666e\u62c9\u65af\u57df\u4e2d\u8868\u73b0\u4e3a\u9891\u7387\u7f29\u653e\u3002", "conclusion": "\u6296\u52a8\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u53ef\u4ee5\u901a\u8fc7\u7b49\u6548\u6a21\u578b\u8fdb\u884c\u91cf\u5316\uff0c\u4e3a\u7cfb\u7edf\u5206\u6790\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.04018", "pdf": "https://arxiv.org/pdf/2509.04018", "abs": "https://arxiv.org/abs/2509.04018", "authors": ["Yifan Yang", "Zhixiang Duan", "Tianshi Xie", "Fuyu Cao", "Pinxi Shen", "Peili Song", "Piaopiao Jin", "Guokang Sun", "Shaoqing Xu", "Yangwei You", "Jingtai Liu"], "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.", "AI": {"tldr": "FPC-VLA\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u548c\u76d1\u7763\u5668\u7684\u53cc\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u7ea0\u6b63\u5931\u8d25\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u611f\u77e5\u89c4\u5212\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7075\u6d3b\u6027\u4e0d\u8db3\uff0cVLA\u7f3a\u4e4f\u5931\u8d25\u9884\u6d4b\u548c\u6062\u590d\u673a\u5236\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u67e5\u8be2\u8bc4\u4f30\u52a8\u4f5c\u53ef\u884c\u6027\uff0c\u751f\u6210\u7ea0\u6b63\u7b56\u7565\uff0c\u65e0\u4eba\u5de5\u6807\u6ce8\u9ad8\u6548\u8bad\u7ec3\uff0c\u76f8\u4f3c\u6027\u5f15\u5bfc\u878d\u5408\u6a21\u5757\u4f18\u5316\u52a8\u4f5c\u3002", "result": "\u5728SIMPLER\u3001LIBERO\u7b49\u6a21\u62df\u5e73\u53f0\u548c\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4efb\u52a1\u6210\u529f\u7387\u9ad8\u4e14\u6267\u884c\u65f6\u95f4\u5f71\u54cd\u5c0f\u3002", "conclusion": "FPC-VLA\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002"}}
{"id": "2509.04061", "pdf": "https://arxiv.org/pdf/2509.04061", "abs": "https://arxiv.org/abs/2509.04061", "authors": ["Ventseslav Yordanov", "Simon Sch\u00e4fer", "Alexander Mann", "Stefan Kowalewski", "Bassam Alrifaee", "Lutz Eckstein"], "title": "Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System", "categories": ["cs.RO"], "comment": "6 pages, 2 figures, this work was submitted to and accepted by IEEE\n  International Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "While current onboard state estimation methods are adequate for most driving\nand safety-related applications, they do not provide insights into the\ninteraction between tires and road surfaces. This paper explores a novel\ncommunication concept for efficiently transmitting integrated wheel sensor data\nfrom an ESP32 microcontroller. Our proposed approach utilizes a\npublish-subscribe system, surpassing comparable solutions in the literature\nregarding data transmission volume. We tested this approach on a drum tire test\nrig with our prototype sensors system utilizing a diverse selection of sample\nfrequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our\ncommunication concept. The implemented prototype sensor showcases minimal data\nloss, approximately 0.1 % of the sampled data, validating the reliability of\nour developed communication system. This work contributes to advancing\nreal-time data acquisition, providing insights into optimizing integrated wheel\nsensor communication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u901a\u4fe1\u6982\u5ff5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f20\u8f93\u6765\u81eaESP32\u5fae\u63a7\u5236\u5668\u7684\u96c6\u6210\u8f66\u8f6e\u4f20\u611f\u5668\u6570\u636e\uff0c\u6d4b\u8bd5\u663e\u793a\u6570\u636e\u4e22\u5931\u7387\u4ec5\u4e3a0.1%\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u8f66\u8f7d\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u8f6e\u80ce\u4e0e\u8def\u9762\u4ea4\u4e92\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u4fe1\u7cfb\u7edf\u6765\u4f20\u8f93\u96c6\u6210\u8f66\u8f6e\u4f20\u611f\u5668\u6570\u636e\u3002", "method": "\u91c7\u7528\u53d1\u5e03-\u8ba2\u9605\u7cfb\u7edf\uff0c\u5728\u8f6e\u80ce\u6d4b\u8bd5\u53f0\u4e0a\u4ee51 Hz\u81f332,000 Hz\u7684\u9891\u7387\u8303\u56f4\u6d4b\u8bd5\u539f\u578b\u4f20\u611f\u5668\u7cfb\u7edf\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u6570\u636e\u4e22\u5931\u7387\u7ea6\u4e3a0.1%\uff0c\u9a8c\u8bc1\u4e86\u901a\u4fe1\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63a8\u8fdb\u4e86\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u6280\u672f\uff0c\u4e3a\u4f18\u5316\u96c6\u6210\u8f66\u8f6e\u4f20\u611f\u5668\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.04063", "pdf": "https://arxiv.org/pdf/2509.04063", "abs": "https://arxiv.org/abs/2509.04063", "authors": ["Hongyin Zhang", "Shiyuan Zhang", "Junxi Jin", "Qixin Zeng", "Yifan Qiao", "Hongchao Lu", "Donglin Wang"], "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models based on flow matching have shown\nexcellent performance in general-purpose robotic manipulation tasks. However,\nthe action accuracy of these models on complex downstream tasks is\nunsatisfactory. One important reason is that these models rely solely on the\npost-training paradigm of imitation learning, which makes it difficult to have\na deeper understanding of the distribution properties of data quality, which is\nexactly what Reinforcement Learning (RL) excels at. In this paper, we\ntheoretically propose an offline RL post-training objective for VLA flow models\nand induce an efficient and feasible offline RL fine-tuning algorithm --\nAdaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted\nscaling factor in the VLA flow model loss, we construct a principled\nbias-variance trade-off objective function to optimally control the impact of\nRL signal on flow loss. ARFM adaptively balances RL advantage preservation and\nflow loss gradient variance control, resulting in a more stable and efficient\nfine-tuning process. Extensive simulation and real-world experimental results\nshow that ARFM exhibits excellent generalization, robustness, few-shot\nlearning, and continuous learning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9069\u61c9\u5f37\u5316\u6d41\u5339\u914d\uff08ARFM\uff09\u65b9\u6cd5\uff0c\u901a\u904e\u7d50\u5408\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u9032\u884c\u5fae\u8abf\uff0c\u63d0\u5347VLA\u6a21\u578b\u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u7684\u52d5\u4f5c\u6e96\u78ba\u6027\u3002", "motivation": "\u7576\u524d\u57fa\u65bc\u6d41\u5339\u914d\u7684VLA\u6a21\u578b\u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u52d5\u4f5c\u6e96\u78ba\u6027\u4e0d\u8db3\uff0c\u56e0\u5176\u50c5\u4f9d\u8cf4\u6a21\u4eff\u5b78\u7fd2\uff0c\u7f3a\u4e4f\u5c0d\u6578\u64da\u5206\u4f48\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u5fae\u8abf\u76ee\u6a19\uff0c\u5f15\u5165\u81ea\u9069\u61c9\u8abf\u6574\u7684\u7e2e\u653e\u56e0\u5b50\uff0c\u69cb\u5efa\u504f\u7f6e-\u65b9\u5dee\u6b0a\u8861\u7684\u76ee\u6a19\u51fd\u6578\uff0c\u5be6\u73fe\u7a69\u5b9a\u9ad8\u6548\u7684\u5fae\u8abf\u3002", "result": "\u5be6\u9a57\u986f\u793aARFM\u5728\u6cdb\u5316\u6027\u3001\u9b6f\u68d2\u6027\u3001\u5c11\u6a23\u672c\u5b78\u7fd2\u548c\u6301\u7e8c\u5b78\u7fd2\u65b9\u9762\u8868\u73fe\u512a\u7570\u3002", "conclusion": "ARFM\u7d50\u5408\u5f37\u5316\u5b78\u7fd2\u986f\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.04069", "pdf": "https://arxiv.org/pdf/2509.04069", "abs": "https://arxiv.org/abs/2509.04069", "authors": ["Chengyandan Shen", "Christoffer Sloth"], "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.", "AI": {"tldr": "DRLR\u6846\u67b6\u7ed3\u5408\u53c2\u8003\u7b56\u7565\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6548\u7387\uff0c\u6539\u8fdbIBRL\u7b97\u6cd5\u7684\u52a8\u4f5c\u9009\u62e9\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u901a\u8fc7\u7ed3\u5408\u793a\u8303\u6570\u636e\u6539\u8fdb\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u4ee5\u51cf\u5c11\u5f15\u5bfc\u8bef\u5dee\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u6539\u8fdbIBRL\u7b97\u6cd5\u7684\u52a8\u4f5c\u9009\u62e9\u6a21\u5757\uff0c\u63d0\u4f9b\u6821\u51c6\u7684Q\u503c\u4ee5\u51cf\u5c11\u5f15\u5bfc\u8bef\u5dee\uff0c\u5e76\u91c7\u7528SAC\u4f5c\u4e3a\u7b56\u7565\u4ee5\u907f\u514d\u6536\u655b\u81f3\u6b21\u4f18\u89e3\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\uff08\u88c5\u6876\u548c\u5f00\u62bd\u5c49\uff09\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u8f6e\u5f0f\u88c5\u8f7d\u673a\u4e0a\u6210\u529f\u90e8\u7f72\u88c5\u6876\u4efb\u52a1\u3002", "conclusion": "DRLR\u6846\u67b6\u5728\u5904\u7406\u4f4e\u7ef4\u548c\u9ad8\u7ef4\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u5f15\u5bfc\u8bef\u5dee\u5e76\u5b9e\u73b0sim2real\u8fc1\u79fb\u3002"}}
{"id": "2509.04076", "pdf": "https://arxiv.org/pdf/2509.04076", "abs": "https://arxiv.org/abs/2509.04076", "authors": ["Lennart Clasmeier", "Jan-Gerrit Habekost", "Connor G\u00e4de", "Philipp Allgeuer", "Stefan Wermter"], "title": "Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to ICANN 20255 Special Session on Neural Robotics", "summary": "We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u51cf\u5c11\u8fd0\u884c\u65f6\u9700\u6c42\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u89c4\u5212\u65b9\u6cd5\u8fd0\u884c\u65f6\u9700\u6c42\u9ad8\uff0c\u5e0c\u671b\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u70b9\u4e91\u5d4c\u5165\u548c\u5173\u952e\u70b9\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u5fae\u8c03\u6539\u8fdb\u6a21\u578b\u3002", "result": "\u6a21\u578b\u8fd0\u884c\u65f6\u6bd4\u6570\u503c\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u6d4b\u8bd5\u96c6\u78b0\u649e\u81ea\u7531\u89e3\u51b3\u65b9\u6848\u6210\u529f\u738790%\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2509.04094", "pdf": "https://arxiv.org/pdf/2509.04094", "abs": "https://arxiv.org/abs/2509.04094", "authors": ["Fatih Dursun", "Bruno Vilhena Adorno", "Simon Watson", "Wei Pan"], "title": "Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators", "categories": ["cs.RO"], "comment": "14 pages, 13 figures, 3 tables. Under Review for the IEEE\n  Transactions on Robotics (T-RO)", "summary": "Object reconstruction and inspection tasks play a crucial role in various\nrobotics applications. Identifying paths that reveal the most unknown areas of\nthe object becomes paramount in this context, as it directly affects\nefficiency, and this problem is known as the view path planning problem.\nCurrent methods often use sampling-based path planning techniques, evaluating\npotential views along the path to enhance reconstruction performance. However,\nthese methods are computationally expensive as they require evaluating several\ncandidate views on the path. To this end, we propose a computationally\nefficient solution that relies on calculating a focus point in the most\ninformative (unknown) region and having the robot maintain this point in the\ncamera field of view along the path. We incorporated this strategy into the\nwhole-body control of a mobile manipulator employing a visibility constraint\nwithout the need for an additional path planner. We conducted comprehensive and\nrealistic simulations using a large dataset of 114 diverse objects of varying\nsizes from 57 categories to compare our method with a sampling-based planning\nstrategy using Bayesian data analysis. Furthermore, we performed real-world\nexperiments with an 8-DoF mobile manipulator to demonstrate the proposed\nmethod's performance in practice. Our results suggest that there is no\nsignificant difference in object coverage and entropy. In contrast, our method\nis approximately nine times faster than the baseline sampling-based method in\nterms of the average time the robot spends between views.", "AI": {"tldr": "\u8be5\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u89c6\u56fe\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u8ba1\u7b97\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u533a\u57df\u4e2d\u7684\u7126\u70b9\u6765\u4f18\u5316\u673a\u5668\u4eba\u8def\u5f84\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u91c7\u6837\u89c4\u5212\u65b9\u6cd5\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5f53\u524d\u57fa\u4e8e\u91c7\u6837\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u7269\u4f53\u91cd\u5efa\u548c\u68c0\u67e5\u4efb\u52a1\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u8ba1\u7b97\u672a\u77e5\u533a\u57df\u4e2d\u7684\u7126\u70b9\u70b9\uff0c\u5e76\u8ba9\u673a\u5668\u4eba\u6cbf\u8def\u5f84\u4fdd\u6301\u8be5\u70b9\u5728\u76f8\u673a\u89c6\u573a\u4e2d\uff0c\u7ed3\u5408\u53ef\u89c1\u6027\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u8986\u76d6\u7387\u548c\u71b5\u65b9\u9762\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u901f\u5ea6\u63d0\u9ad8\u4e86\u7ea6\u4e5d\u500d\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u9ad8\u6548\u7684\u7269\u4f53\u91cd\u5efa\u548c\u68c0\u67e5\u4efb\u52a1\u3002"}}
{"id": "2509.04095", "pdf": "https://arxiv.org/pdf/2509.04095", "abs": "https://arxiv.org/abs/2509.04095", "authors": ["Achilleas Santi Seisa", "Viswa Narayanan Sankaranarayanan", "Gerasimos Damigos", "Sumeet Gajanan Satpute", "George Nikolakopoulos"], "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation", "categories": ["cs.RO", "cs.DC"], "comment": "6 pages, 7 figures, CCGridW 2025", "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bb9\u5668\u5316\u6280\u672f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u4e91\u548c\u8fb9\u7f18\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\u89e3\u51b3\u7f51\u7edc\u5ef6\u8fdf\u548c\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u4e91\u673a\u5668\u4eba\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u56e0\u7f51\u7edc\u5ef6\u8fdf\u3001\u5b89\u5168\u95ee\u9898\u548c\u8d44\u6e90\u7ba1\u7406\u9700\u6c42\uff0c\u5176\u4e0e\u4e91\u8ba1\u7b97\u6574\u5408\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u6846\u67b6\u7531\u5bb9\u5668\u5316\u4e91\u96c6\u7fa4\u548c\u673a\u5668\u4eba\u6a21\u62df\u73af\u5883\u7ec4\u6210\uff0c\u5229\u7528UDP\u96a7\u9053\u8fdb\u884c\u53cc\u5411\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u6d41\u91cf\u63a7\u5236\u6a21\u62df\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u4e91\u8f85\u52a9\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u8fdc\u7a0b\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u7f51\u7edc\u5ef6\u8fdf\u548c\u6296\u52a8\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u76f4\u89c2\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6d4b\u8bd5\u4e91\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u7f51\u7edc\u95ee\u9898\u3002"}}
{"id": "2509.04119", "pdf": "https://arxiv.org/pdf/2509.04119", "abs": "https://arxiv.org/abs/2509.04119", "authors": ["Ke Wu", "Yuhao Wang", "Kevin Henry", "Cesare Stefanini", "Gang Zheng"], "title": "Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation", "categories": ["cs.RO"], "comment": "Journal", "summary": "Continuum robots, inspired by octopus arms and elephant trunks, combine\ndexterity with intrinsic compliance, making them well suited for unstructured\nand confined environments. Yet their continuously deformable morphology poses\nchallenges for motion planning and control, calling for accurate but\nlightweight models. We propose the Lightweight Actuation Space Energy Modeling\n(LASEM) framework for cable driven continuum robots, which formulates actuation\npotential energy directly in actuation space. LASEM yields an analytical\nforward model derived from geometrically nonlinear beam and rod theories via\nHamilton's principle, while avoiding explicit modeling of cable backbone\ncontact. It accepts both force and displacement inputs, thereby unifying\nkinematic and static formulations. Assuming the friction is neglected, the\nframework generalizes to nonuniform geometries, arbitrary cable routings,\ndistributed loading and axial extensibility, while remaining computationally\nefficient for real-time use. Numerical simulations validate its accuracy, and a\nsemi-analytical iterative scheme is developed for inverse kinematics. To\naddress discretization in practical robots, LASEM further reformulates the\nfunctional minimization as a numerical optimization, which also naturally\nincorporates cable potential energy without explicit contact modeling.", "AI": {"tldr": "LASEM\u6846\u67b6\u4e3a\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u80fd\u91cf\u5efa\u6a21\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u9a71\u52a8\u7a7a\u95f4\u4e2d\u5efa\u6a21\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u7535\u7f06-\u9aa8\u67b6\u63a5\u89e6\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8ba1\u7b97\u3002", "motivation": "\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u56e0\u5176\u7075\u6d3b\u6027\u5728\u975e\u7ed3\u6784\u5316\u548c\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u8fde\u7eed\u53d8\u5f62\u7279\u6027\u7ed9\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u51c6\u786e\u4e14\u8f7b\u91cf\u5316\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faLASEM\u6846\u67b6\uff0c\u901a\u8fc7Hamilton\u539f\u7406\u4ece\u51e0\u4f55\u975e\u7ebf\u6027\u6881\u548c\u6746\u7406\u8bba\u63a8\u5bfc\u51fa\u89e3\u6790\u524d\u5411\u6a21\u578b\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u63a5\u89e6\u5efa\u6a21\uff0c\u652f\u6301\u529b\u548c\u4f4d\u79fb\u8f93\u5165\uff0c\u9002\u7528\u4e8e\u975e\u5747\u5300\u51e0\u4f55\u548c\u5206\u5e03\u5f0f\u52a0\u8f7d\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u534a\u89e3\u6790\u8fed\u4ee3\u65b9\u6848\u7528\u4e8e\u9006\u8fd0\u52a8\u5b66\uff1bLASEM\u5c06\u529f\u80fd\u6700\u5c0f\u5316\u8f6c\u5316\u4e3a\u6570\u503c\u4f18\u5316\uff0c\u81ea\u7136\u6574\u5408\u7535\u7f06\u52bf\u80fd\u800c\u65e0\u987b\u63a5\u89e6\u5efa\u6a21\u3002", "conclusion": "LASEM\u4e3a\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u5efa\u6a21\u548c\u63a7\u5236\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.04324", "pdf": "https://arxiv.org/pdf/2509.04324", "abs": "https://arxiv.org/abs/2509.04324", "authors": ["Chen Hu", "Shan Luo", "Letizia Gionfrida"], "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.", "AI": {"tldr": "\u57fa\u4e8eRGB-D\u89c6\u89c9\u3001\u5f00\u653e\u8bcd\u6c47\u63d0\u793a\u548c\u8bed\u97f3\u547d\u4ee4\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u6846\u67b6OVGrasp\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6293\u53d6\u8f85\u52a9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u52a8\u4f5c\u969c\u788d\u8005\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63d0\u4f9b\u6293\u53d6\u8f85\u52a9\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u8c61\u591a\u6837\u6027\u548c\u7528\u6237\u610f\u56fe\u7684\u591a\u53d8\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u51b3\u7b56\u673a\u5236\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u6d4b\u548c\u610f\u56fe\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u663e\u793aOVGrasp\u6293\u53d6\u80fd\u529b\u5f97\u5206\u4e3a87.00%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8fd0\u52a8\u5b66\u5bf9\u9f50\u66f4\u4f73\u3002", "conclusion": "OVGrasp\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u5f00\u653e\u8bcd\u6c47\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u8f85\u52a9\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.04441", "pdf": "https://arxiv.org/pdf/2509.04441", "abs": "https://arxiv.org/abs/2509.04441", "authors": ["Hao-Shu Fang", "Branden Romero", "Yichen Xie", "Arthur Hu", "Bo-Ruei Huang", "Juan Alvarez", "Matthew Kim", "Gabriel Margolis", "Kavya Anbarasu", "Masayoshi Tomizuka", "Edward Adelson", "Pulkit Agrawal"], "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "comment": "project page: https://dex-op.github.io", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86perioperation\u8303\u5f0f\uff0c\u901a\u8fc7DEXOP\u88ab\u52a8\u624b\u5916\u9aa8\u9abc\u6536\u96c6\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\uff0c\u4f18\u5316\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u4e2d\u7684\u6280\u80fd\u8f6c\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u548c\u5b66\u4e60\u6548\u7387\u3002", "method": "\u4f7f\u7528DEXOP\u624b\u5916\u9aa8\u9abc\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u548c\u59ff\u52bf\u955c\u50cf\u8bb0\u5f55\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u3002", "result": "DEXOP\u63d0\u9ad8\u4e86\u6570\u636e\u6536\u96c6\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "DEXOP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u80fd\u63a8\u52a8\u673a\u5668\u4eba\u7075\u5de7\u6027\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.04443", "pdf": "https://arxiv.org/pdf/2509.04443", "abs": "https://arxiv.org/abs/2509.04443", "authors": ["Lawrence Y. Zhu", "Pranav Kuppili", "Ryan Punamiya", "Patcharapong Aphiwetsa", "Dhruv Patel", "Simar Kareer", "Sehoon Ha", "Danfei Xu"], "title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data", "categories": ["cs.RO"], "comment": null, "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.", "AI": {"tldr": "EMMA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u5168\u8eab\u8fd0\u52a8\u6570\u636e\u548c\u9759\u6001\u673a\u5668\u4eba\u6570\u636e\uff0c\u65e0\u9700\u79fb\u52a8\u673a\u5668\u4eba\u9065\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u79fb\u52a8\u64cd\u4f5c\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u9065\u64cd\u4f5c\u7684\u57fa\u7ebf\u76f8\u5f53\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u64cd\u4f5c\u6a21\u4eff\u5b66\u4e60\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u6210\u672c\u9ad8\u95ee\u9898\u3002", "method": "\u5229\u7528\u4eba\u7c7b\u5168\u8eab\u8fd0\u52a8\u6570\u636e\u548c\u9759\u6001\u673a\u5668\u4eba\u6570\u636e\u8054\u5408\u8bad\u7ec3\u79fb\u52a8\u64cd\u4f5c\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cEMMA\u6027\u80fd\u4e0e\u9065\u64cd\u4f5c\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u65b0\u573a\u666f\u3002", "conclusion": "EMMA\u4e3a\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.03721", "pdf": "https://arxiv.org/pdf/2509.03721", "abs": "https://arxiv.org/abs/2509.03721", "authors": ["C\u00e9dric Join", "Michel Fliess"], "title": "Avoidance of an unexpected obstacle without reinforcement learning: Why not using advanced control-theoretic tools?", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": "IEEE 2025 - 13th International Conference on Systems and Control\n  (ICSC) - October 22-24, 2025 - Marrakesh, Morocco", "summary": "This communication on collision avoidance with unexpected obstacles is\nmotivated by some critical appraisals on reinforcement learning (RL) which\n\"requires ridiculously large numbers of trials to learn any new task\" (Yann\nLeCun). We use the classic Dubins' car in order to replace RL with\nflatness-based control, combined with the HEOL feedback setting, and the latest\nmodel-free predictive control approach. The two approaches lead to convincing\ncomputer experiments where the results with the model-based one are only\nslightly better. They exhibit a satisfactory robustness with respect to\nrandomly generated mismatches/disturbances, which become excellent in the\nmodel-free case. Those properties would have been perhaps difficult to obtain\nwith today's popular machine learning techniques in AI. Finally, we should\nemphasize that our two methods require a low computational burden.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u5766\u6027\u63a7\u5236\u548c\u6a21\u578b\u81ea\u7531\u9884\u6d4b\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0e\u610f\u5916\u969c\u788d\u7269\u7684\u78b0\u649e\u907f\u514d\u95ee\u9898\uff0c\u66ff\u4ee3\u4e86\u9700\u8981\u5927\u91cf\u8bd5\u9a8c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8ba1\u7b97\u8d1f\u62c5\u4f4e\u4e14\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u8bd5\u9a8c\u7684\u6279\u8bc4\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u78b0\u649e\u907f\u514d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5e73\u5766\u6027\u63a7\u5236\u548cHEOL\u53cd\u9988\u8bbe\u7f6e\uff0c\u4ee5\u53ca\u6700\u65b0\u7684\u6a21\u578b\u81ea\u7531\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528Dubins' car\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u81ea\u7531\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6a21\u578b\u65b9\u6cd5\uff0c\u4e14\u4e24\u79cd\u65b9\u6cd5\u5747\u8ba1\u7b97\u8d1f\u62c5\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u78b0\u649e\u907f\u514d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f18\u4e8e\u5f53\u524d\u6d41\u884c\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002"}}
{"id": "2509.03753", "pdf": "https://arxiv.org/pdf/2509.03753", "abs": "https://arxiv.org/abs/2509.03753", "authors": ["Michael Greer"], "title": "Memory Optimization for Convex Hull Support Point Queries", "categories": ["cs.GR", "cs.CG", "cs.RO", "68U05", "I.3.5"], "comment": "6 pages, 15 figures", "summary": "This paper evaluates several improvements to the memory layout of convex\nhulls to improve computation times for support point queries. The support point\nquery is a fundamental part of common collision algorithms, and the work\npresented achieves a significant speedup depending on the number of vertices of\nthe convex hull.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u51f8\u5305\u7684\u5185\u5b58\u5e03\u5c40\u6765\u63d0\u5347\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u5bf9\u5e38\u89c1\u78b0\u649e\u7b97\u6cd5\u6709\u663e\u8457\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u652f\u6301\u70b9\u67e5\u8be2\u662f\u78b0\u649e\u7b97\u6cd5\u7684\u5173\u952e\u90e8\u5206\uff0c\u63d0\u5347\u5176\u8ba1\u7b97\u6548\u7387\u5bf9\u6574\u4f53\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u4f18\u5316\u51f8\u5305\u7684\u5185\u5b58\u5e03\u5c40\u8bbe\u8ba1\u3002", "result": "\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u6548\u679c\u4e0e\u51f8\u5305\u7684\u9876\u70b9\u6570\u91cf\u76f8\u5173\u3002", "conclusion": "\u5185\u5b58\u5e03\u5c40\u4f18\u5316\u80fd\u6709\u6548\u52a0\u901f\u652f\u6301\u70b9\u67e5\u8be2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u78b0\u649e\u7b97\u6cd5\u573a\u666f\u3002"}}
{"id": "2509.04156", "pdf": "https://arxiv.org/pdf/2509.04156", "abs": "https://arxiv.org/abs/2509.04156", "authors": ["Serhii Svystun", "Pavlo Radiuk", "Oleksandr Melnychenko", "Oleg Savenko", "Anatoliy Sachenko"], "title": "YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T07, 68T45, 68U10, 68T40", "I.2.10; I.4.8; I.5.4; I.2.9"], "comment": "The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland", "summary": "Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u96c6\u6210YOLO\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u591a\u5149\u8c31\u6570\u636e\uff0c\u63d0\u5347\u4e86\u98ce\u7535\u573a\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u7f3a\u9677\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u914d\u5907\u5148\u8fdb\u4f20\u611f\u5668\u4e3a\u98ce\u7535\u573a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u53ef\u9760\u7f3a\u9677\u68c0\u6d4b\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u548c\u9ad8\u6548\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u901a\u7528YOLOv8\u6a21\u578b\u548c\u4e13\u7528\u70ed\u6a21\u578b\uff0c\u901a\u8fc7\u8fb9\u754c\u6846\u878d\u5408\u7b97\u6cd5\u6574\u5408\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684mAP@.5\u8fbe\u52300.93\uff0cF1\u5206\u6570\u4e3a0.90\uff0c\u4f18\u4e8e\u5355\u72ec\u7684YOLOv8\u6a21\u578b\u3002", "conclusion": "\u591aYOLO\u67b6\u6784\u4e0e\u591a\u5149\u8c31\u6570\u636e\u878d\u5408\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u548c\u70ed\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.04220", "pdf": "https://arxiv.org/pdf/2509.04220", "abs": "https://arxiv.org/abs/2509.04220", "authors": ["Max H. Cohen", "Eugene Lavretsky", "Aaron D. Ames"], "title": "Compatibility of Multiple Control Barrier Functions for Constrained Nonlinear Systems", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": "To appear at IEEE CDC 2025", "summary": "Control barrier functions (CBFs) are a powerful tool for the constrained\ncontrol of nonlinear systems; however, the majority of results in the\nliterature focus on systems subject to a single CBF constraint, making it\nchallenging to synthesize provably safe controllers that handle multiple state\nconstraints. This paper presents a framework for constrained control of\nnonlinear systems subject to box constraints on the systems' vector-valued\noutputs using multiple CBFs. Our results illustrate that when the output has a\nvector relative degree, the CBF constraints encoding these box constraints are\ncompatible, and the resulting optimization-based controller is locally\nLipschitz continuous and admits a closed-form expression. Additional results\nare presented to characterize the degradation of nominal tracking objectives in\nthe presence of safety constraints. Simulations of a planar quadrotor are\npresented to demonstrate the efficacy of the proposed framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u7ea6\u675f\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5411\u91cf\u503c\u8f93\u51fa\u7684\u7bb1\u578b\u7ea6\u675f\uff0c\u8bc1\u660e\u4e86\u5176\u5c40\u90e8Lipschitz\u8fde\u7eed\u6027\u548c\u95ed\u5f0f\u8868\u8fbe\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u5927\u591a\u805a\u7126\u4e8e\u5355\u4e00CBF\u7ea6\u675f\uff0c\u96be\u4ee5\u5408\u6210\u591a\u72b6\u6001\u7ea6\u675f\u7684\u53ef\u8bc1\u660e\u5b89\u5168\u63a7\u5236\u5668\u3002", "method": "\u91c7\u7528\u591aCBFs\u5904\u7406\u5411\u91cf\u503c\u8f93\u51fa\u7684\u7bb1\u578b\u7ea6\u675f\uff0c\u8981\u6c42\u8f93\u51fa\u5177\u6709\u5411\u91cf\u76f8\u5bf9\u5ea6\u3002", "result": "\u4f18\u5316\u63a7\u5236\u5668\u5177\u6709\u5c40\u90e8Lipschitz\u8fde\u7eed\u6027\u5e76\u652f\u6301\u95ed\u5f0f\u8868\u8fbe\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5b89\u5168\u7ea6\u675f\u5bf9\u540d\u4e49\u8ddf\u8e2a\u76ee\u6807\u7684\u6027\u80fd\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u5e73\u9762\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u6240\u63d0\u6846\u67b6\u5728\u591a\u7ea6\u675f\u6761\u4ef6\u4e0b\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2509.04356", "pdf": "https://arxiv.org/pdf/2509.04356", "abs": "https://arxiv.org/abs/2509.04356", "authors": ["Atikkhan Faridkhan Nilgar", "Kristof Van Laerhoven", "Ayub Kinoti"], "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.", "AI": {"tldr": "SRWToolkit\u662f\u4e00\u6b3e\u5f00\u6e90\u7684Wizard of Oz\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u57fa\u4e8e\u672c\u5730\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u793e\u4ea4\u673a\u5668\u4eba\u89d2\u8272\uff0c\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u5b9e\u65f6\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u591a\u4f9d\u8d56\u4e91\u7aefLLM\u670d\u52a1\uff0c\u7f3a\u4e4f\u672c\u5730\u5316\u548c\u6a21\u5757\u5316\u652f\u6301\uff0cSRWToolkit\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u4f9b\u57fa\u4e8e\u7f51\u9875\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u5de5\u5177\uff0c\u652f\u6301\u6587\u672c\u8f93\u5165\u3001\u8bed\u97f3\u5524\u9192\u548c\u5b9e\u65f6\u914d\u7f6e\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u672c\u5730LLM\u5b9e\u73b0\u8bbe\u5907\u7aef\u8fd0\u884c\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08n=11\uff09\u663e\u793a\u5de5\u5177\u5305\u5728\u53ef\u7528\u6027\u3001\u4fe1\u4efb\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "SRWToolkit\u652f\u6301\u9ad8\u6548\u5f00\u53d1\u5b9a\u5236\u5316\u673a\u5668\u4eba\u89d2\u8272\uff0c\u63a8\u52a8\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.04358", "pdf": "https://arxiv.org/pdf/2509.04358", "abs": "https://arxiv.org/abs/2509.04358", "authors": ["Atikkhan Faridkhan Nilgar", "Manuel Dietrich", "Kristof Van Laerhoven"], "title": "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.", "AI": {"tldr": "\u793e\u4f1a\u673a\u5668\u4eba\u5728\u5e78\u798f\u8f85\u5bfc\u4e2d\u88ab\u8ba4\u4e3a\u662f\u91cd\u8981\u652f\u6301\u8005\uff0c\u4f46\u7528\u6237\u9690\u79c1\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\u5305\u62ec\u4fe1\u606f\u4f7f\u7528\u7684\u900f\u660e\u5ea6\u3001\u7528\u6237\u63a7\u5236\u6c34\u5e73\u548c\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u5f0f\u3002\u7814\u7a76\u8868\u660e\u7528\u6237\u63a7\u5236\u5bf9\u9690\u79c1\u611f\u77e5\u5f71\u54cd\u6700\u5927\uff0c\u900f\u660e\u5ea6\u548c\u4e3b\u52a8\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5f71\u54cd\u7528\u6237\u5728\u4f7f\u7528\u793e\u4f1a\u673a\u5668\u4eba\u8fdb\u884c\u5e78\u798f\u8f85\u5bfc\u65f6\u9690\u79c1\u611f\u77e5\u7684\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u586b\u8865\u5f53\u524d\u5bf9\u9690\u79c1\u8ba4\u77e5\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff08N=200\uff09\u7cfb\u7edf\u5730\u8003\u5bdf\u900f\u660e\u5ea6\u3001\u7528\u6237\u63a7\u5236\u548c\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u5f0f\u5bf9\u9690\u79c1\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7528\u6237\u5373\u4f7f\u6388\u4e88\u673a\u5668\u4eba\u5bf9\u4e2a\u4eba\u6570\u636e\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u4ecd\u671f\u671b\u80fd\u591f\u660e\u786e\u63a7\u5236\u4fe1\u606f\u7684\u89e3\u91ca\u548c\u5171\u4eab\u65b9\u5f0f\u3002\u7528\u6237\u63a7\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u79c1\u9002\u5f53\u6027\u548c\u4fe1\u4efb\u8bc4\u5206\uff0c\u900f\u660e\u5ea6\u548c\u4e3b\u52a8\u6027\u7684\u5f71\u54cd\u8f83\u5c0f\u4e14\u4e0d\u663e\u8457\u3002", "conclusion": "\u4ec5\u63d0\u4f9b\u4fe1\u606f\u6216\u4e3b\u52a8\u5171\u4eab\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u7528\u6237\u9690\u79c1\u9700\u6c42\uff0c\u672a\u6765\u7814\u7a76\u9700\u5173\u6ce8\u5141\u8bb8\u7528\u6237\u7ba1\u7406\u673a\u5668\u4eba\u4fe1\u606f\u5904\u7406\u7684\u673a\u5236\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u627f\u62c5\u66f4\u4e3b\u52a8\u89d2\u8272\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.04399", "pdf": "https://arxiv.org/pdf/2509.04399", "abs": "https://arxiv.org/abs/2509.04399", "authors": ["Adrian Wiltz", "Dimos V. Dimarogonas"], "title": "Leveraging Equivariances and Symmetries in the Control Barrier Function Synthesis", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "15 pages", "summary": "The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u5bf9\u79f0\u6027\uff08\u6216\u79f0\u4e3a\u7b49\u53d8\u6027\uff09\u6765\u7b80\u5316\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u7684\u5408\u6210\u8fc7\u7a0b\u3002\u867d\u7136CBF\u672c\u8eab\u901a\u5e38\u4e0d\u5177\u5907\u5bf9\u79f0\u6027\uff0c\u4f46\u4f5c\u8005\u8bc1\u660e\u4e86\u52a8\u6001\u7cfb\u7edf\u7684\u7b49\u53d8\u6027\u548c\u7ea6\u675f\u7684\u5bf9\u79f0\u6027\u4f1a\u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u5728CBF\u4e2d\u8bf1\u5bfc\u51fa\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u90e8\u5206\u5df2\u77e5\u7684CBF\u548c\u975e\u5bf9\u79f0\u7ea6\u675f\u6765\u5408\u6210\u65b0\u7684CBF\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7814\u7a76\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u3002", "motivation": "CBF\u5408\u6210\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u6216\u590d\u6742\u6784\u9020\uff0c\u800c\u52a8\u6001\u7cfb\u7edf\u7684\u7ed3\u6784\u7279\u6027\u548c\u7ea6\u675f\u7684\u5bf9\u79f0\u6027\u53ef\u80fd\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u63a2\u7d22\u52a8\u6001\u7cfb\u7edf\u7684\u7b49\u53d8\u6027\u53ca\u5176\u4e0e\u7ea6\u675f\u5bf9\u79f0\u6027\u7684\u5173\u8054\uff0c\u4e3aCBF\u5408\u6210\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5229\u7528\u52a8\u6001\u7cfb\u7edf\u7684\u7b49\u53d8\u6027\u548c\u7ea6\u675f\u7684\u5bf9\u79f0\u6027\uff0c\u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u5728CBF\u4e2d\u8bf1\u5bfc\u51fa\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u5c06\u5176\u503c\u4ece\u5b50\u96c6\u63a8\u65ad\u5230\u6574\u4e2a\u57df\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u5982\u4f55\u7ed3\u5408\u90e8\u5206\u5df2\u77e5\u7684CBF\u548c\u975e\u5bf9\u79f0\u7ea6\u675f\u6765\u5408\u6210\u65b0\u7684CBF\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u52a8\u6001\u7cfb\u7edf\u7684\u7b49\u53d8\u6027\u53ef\u4ee5\u663e\u8457\u51cf\u5c11CBF\u5408\u6210\u7684\u8ba1\u7b97\u91cf\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u975e\u5bf9\u79f0\u7ea6\u675f\u7684CBF\u5408\u6210\u3002", "conclusion": "\u52a8\u6001\u7cfb\u7edf\u7684\u7b49\u53d8\u6027\u548c\u7ea6\u675f\u5bf9\u79f0\u6027\u53ef\u4ee5\u9ad8\u6548\u5730\u7528\u4e8eCBF\u5408\u6210\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u62d3\u5c55\u4e86\u5176\u5728\u975e\u5bf9\u79f0\u7ea6\u675f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.04413", "pdf": "https://arxiv.org/pdf/2509.04413", "abs": "https://arxiv.org/abs/2509.04413", "authors": ["Babak Esmaeili", "Hamidreza Modares"], "title": "SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "comment": "Submitted to IEEE Transactions on Automation Science and Engineering", "summary": "This paper proposes a fully data-driven motion-planning framework for\nhomogeneous linear multi-agent systems that operate in shared, obstacle-filled\nworkspaces without access to explicit system models. Each agent independently\nlearns its closed-loop behavior from experimental data by solving convex\nsemidefinite programs that generate locally invariant ellipsoids and\ncorresponding state-feedback gains. These ellipsoids, centered along grid-based\nwaypoints, certify the dynamic feasibility of short-range transitions and\ndefine safe regions of operation. A sampling-based planner constructs a tree of\nsuch waypoints, where transitions are allowed only when adjacent ellipsoids\noverlap, ensuring invariant-to-invariant transitions and continuous safety. All\nagents expand their trees simultaneously and are coordinated through a\nspace-time reservation table that guarantees inter-agent safety by preventing\nsimultaneous occupancy and head-on collisions. Each successful edge in the tree\nis equipped with its own local controller, enabling execution without\nre-solving optimization problems at runtime. The resulting trajectories are not\nonly dynamically feasible but also provably safe with respect to both\nenvironmental constraints and inter-agent collisions. Simulation results\ndemonstrate the effectiveness of the approach in synthesizing synchronized,\nsafe trajectories for multiple agents under shared dynamics and constraints,\nusing only data and convex optimization tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5c40\u90e8\u4e0d\u53d8\u91cf\u692d\u7403\u548c\u53cd\u9988\u589e\u76ca\u6784\u5efa\u5b89\u5168\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u65f6\u7a7a\u9884\u7559\u8868\u534f\u8c03\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u5171\u4eab\u4e14\u5145\u6ee1\u969c\u788d\u7684\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u8fd0\u52a8\u89c4\u5212\u7684\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u663e\u5f0f\u7cfb\u7edf\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u51f8\u534a\u5b9a\u89c4\u5212\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5c40\u90e8\u4e0d\u53d8\u91cf\u692d\u7403\u548c\u5bf9\u5e94\u53cd\u9988\u589e\u76ca\uff0c\u91c7\u6837\u89c4\u5212\u5668\u6784\u5efa\u8def\u5f84\u6811\uff0c\u5e76\u901a\u8fc7\u65f6\u7a7a\u9884\u7559\u8868\u534f\u8c03\u907f\u969c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6846\u67b6\u80fd\u751f\u6210\u52a8\u6001\u53ef\u884c\u4e14\u5b89\u5168\u7684\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ec5\u4f9d\u8d56\u6570\u636e\u548c\u51f8\u4f18\u5316\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u7684\u540c\u6b65\u4e0e\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u3002"}}
