{"id": "2508.20277", "pdf": "https://arxiv.org/pdf/2508.20277", "abs": "https://arxiv.org/abs/2508.20277", "authors": ["Xiaoyan Ma", "Shahryar Zehtabi", "Taejoon Kim", "Christopher G. Brinton"], "title": "Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates an OFDM-based over-the-air federated learning\n(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles\n(UAVs), transmit local machine learning (ML) models to a central parameter\nserver (PS) for global model aggregation. The high mobility of local devices\nresults in imperfect channel estimation, leading to a misalignment problem,\ni.e., the model parameters transmitted from different local devices do not\narrive at the central PS simultaneously. Moreover, the mobility introduces\ntime-varying uploading channels, which further complicates the aggregation\nprocess. All these factors collectively cause distortions in the OTA-FL\ntraining process which are underexplored. To quantify these effects, we first\nderive a closed-form expression for a single-round global model update in terms\nof these channel imperfections. We then extend our analysis to capture multiple\nrounds of global updates, yielding a bound on the accumulated error in OTA-FL.\nWe validate our theoretical results via extensive numerical simulations, which\ncorroborate our derived analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eOFDM\u7684\u7a7a\u4e2d\u8054\u90a6\u5b66\u4e60(OTA-FL)\u7cfb\u7edf\u4e2d\uff0c\u79fb\u52a8\u8bbe\u5907\u9ad8\u673a\u52a8\u6027\u5bfc\u81f4\u7684\u4fe1\u9053\u4f30\u8ba1\u4e0d\u5b8c\u7f8e\u53ca\u6a21\u578b\u53c2\u6570\u5bf9\u9f50\u95ee\u9898\uff0c\u9996\u6b21\u91cf\u5316\u4e86\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u591a\u79fb\u52a8\u8bbe\u5907\uff08\u5982\u65e0\u4eba\u673a\uff09\u5728\u9ad8\u673a\u52a8\u6027\u4e0b\u8fdb\u884cOTA-FL\u65f6\uff0c\u7531\u4e8e\u4fe1\u9053\u4f30\u8ba1\u4e0d\u5b8c\u7f8e\u548c\u65f6\u53d8\u4e0a\u4f20\u4fe1\u9053\u5bfc\u81f4\u7684\u6a21\u578b\u53c2\u6570\u5bf9\u9f50\u95ee\u9898\u53ca\u5176\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u5931\u771f\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63a8\u5bfc\u5355\u8f6e\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u91cf\u5316\u4fe1\u9053\u4e0d\u5b8c\u7f8e\u548c\u65f6\u53d8\u4fe1\u9053\u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u6269\u5c55\u5230\u591a\u8f6e\u5168\u5c40\u66f4\u65b0\u7684\u8bef\u5dee\u7d2f\u79ef\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6b63\u786e\u6027\uff0c\u8868\u660e\u9ad8\u673a\u52a8\u6027\u548c\u4fe1\u9053\u4e0d\u5b8c\u7f8e\u4f1a\u663e\u8457\u5f71\u54cdOTA-FL\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u9ad8\u673a\u52a8\u6027\u548c\u4fe1\u9053\u4e0d\u5b8c\u7f8e\u4f1a\u5bfc\u81f4OTA-FL\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5931\u771f\uff0c\u63d0\u51fa\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4fe1\u9053\u4f30\u8ba1\u548c\u6a21\u578b\u5bf9\u9f50\u673a\u5236\u3002"}}
{"id": "2508.20531", "pdf": "https://arxiv.org/pdf/2508.20531", "abs": "https://arxiv.org/abs/2508.20531", "authors": ["Chaoying Huang", "Wen Chen", "Qingqing Wu", "Xusheng Zhu", "Zhendong Li", "Ying Wang", "Jinhong Yuan"], "title": "Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a novel dual-intelligent reflecting surface (IRS) aided\ninterference-limited simultaneous wireless information and power transfer\n(SWIPT) system with independent power splitting (PS), where each receiving\nantenna applies different PS factors to offer an advantageous trade-off between\nthe useful information and harvested energy. We separately establish the near-\nand hybrid-field channel models for IRS-reflected links to evaluate the\nperformance gain more precisely and practically. Specifically, we formulate an\noptimization problem of maximizing the harvested power by jointly optimizing\ndual-IRS phase shifts, independent PS ratio, and receive beamforming vector in\nboth near- and hybrid-field cases. In the near-field case, the alternating\noptimization algorithm is proposed to solve the non-convex problem by applying\nthe Lagrange duality method and the difference-of-convex (DC) programming. In\nthe hybrid-field case, we first present an interesting result that the\nAP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which\nallows the optimization problem to be transformed into a convex one. Then, we\nderive the asymptotic performance of the combined channel gains in closed-form\nand analyze the characteristics of the dual-IRS. Numerical results validate our\nanalysis and indicate the performance gains of the proposed scheme that\ndual-IRS-aided SWIPT with independent PS over other benchmark schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u667a\u80fd\u53cd\u5c04\u9762\uff08IRS\uff09\u8f85\u52a9\u7684\u5e72\u6270\u9650\u5236\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\uff08SWIPT\uff09\u7cfb\u7edf\uff0c\u91c7\u7528\u72ec\u7acb\u529f\u7387\u5206\u5272\uff08PS\uff09\uff0c\u5e76\u901a\u8fc7\u8fd1\u573a\u548c\u6df7\u5408\u573a\u4fe1\u9053\u6a21\u578b\u8bc4\u4f30\u6027\u80fd\u589e\u76ca\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fe1\u606f\u4f20\u8f93\u548c\u80fd\u91cf\u6536\u96c6\u4e4b\u95f4\u5b9e\u73b0\u66f4\u4f18\u7684\u6743\u8861\uff0c\u540c\u65f6\u8003\u8651\u8fd1\u573a\u548c\u6df7\u5408\u573a\u901a\u4fe1\u573a\u666f\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5206\u522b\u5efa\u7acb\u8fd1\u573a\u548c\u6df7\u5408\u573a\u4fe1\u9053\u6a21\u578b\uff0c\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u548c\u51f8\u4f18\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u53ccIRS\u8f85\u52a9\u7684SWIPT\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u53ccIRS\u7ed3\u5408\u72ec\u7acbPS\u7684SWIPT\u7cfb\u7edf\u5728\u6027\u80fd\u548c\u7075\u6d3b\u6027\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8fd1\u573a\u548c\u6df7\u5408\u573a\u573a\u666f\u3002"}}
{"id": "2508.20535", "pdf": "https://arxiv.org/pdf/2508.20535", "abs": "https://arxiv.org/abs/2508.20535", "authors": ["Annika Stiehl", "Nicolas Weeger", "Christian Uhl", "Dominic Bechtold", "Nicole Ille", "Stefan Gei\u00dfels\u00f6der"], "title": "Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders", "categories": ["eess.SP"], "comment": "\\c{opyright} 2025 IEEE. Accepted in 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  2025", "summary": "Epilepsy is one of the most common neurological disorders. This disease\nrequires reliable and efficient seizure detection methods.\nElectroencephalography (EEG) is the gold standard for seizure monitoring, but\nits manual analysis is a time-consuming task that requires expert knowledge. In\naddition, there are no well-defined features that allow fully automated\nanalysis. Existing deep learning-based approaches struggle to achieve high\nsensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack\nconsistency in the optimal EEG input representation, whether in the time or\nfrequency domain. To address these issues, we propose a Deep Convolutional\nAutoencoder (DCAE) to extract low-dimensional latent representations that\npreserve essential EEG signal features. The ability of the model to preserve\nrelevant information was evaluated by comparing reconstruction errors based on\nboth time series and frequency-domain representations. Several autoencoders\nwith different loss functions based on time and frequency were trained and\nevaluated to determine their effectiveness in reconstructing EEG features. Our\nresults show that the DCAE model taking both time series and frequency losses\ninto account achieved the best reconstruction performance. This indicates that\nDeep Neural Networks with a single representation might not preserve the\nrelevant signal properties. This work provides insight into how deep learning\nmodels process EEG data and examines whether frequency information is captured\nwhen time series signals are used as input.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff08DCAE\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u635f\u5931\u51fd\u6570\u63d0\u53d6EEG\u4fe1\u53f7\u7684\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u766b\u75eb\u68c0\u6d4b\u7684\u654f\u611f\u6027\u548c\u964d\u4f4e\u8bef\u62a5\u7387\u3002", "motivation": "\u766b\u75eb\u662f\u5e38\u89c1\u7684\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u9700\u8981\u53ef\u9760\u7684\u53d1\u4f5c\u68c0\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u654f\u611f\u6027\u548c\u8bef\u62a5\u7387\u5b58\u5728\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u4e00\u81f4\u7684EEG\u8f93\u5165\u8868\u793a\u3002", "method": "\u4f7f\u7528DCAE\u63d0\u53d6EEG\u4fe1\u53f7\u7684\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u6bd4\u8f83\u65f6\u57df\u548c\u9891\u57df\u7684\u91cd\u5efa\u8bef\u5dee\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bad\u7ec3\u4e0d\u540c\u635f\u5931\u51fd\u6570\u7684\u81ea\u7f16\u7801\u5668\u3002", "result": "\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u635f\u5931\u7684DCAE\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8868\u660e\u5355\u4e00\u8868\u793a\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u4fdd\u7559\u4fe1\u53f7\u7684\u5b8c\u6574\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5904\u7406EEG\u6570\u636e\u7684\u673a\u5236\uff0c\u8868\u660e\u9891\u57df\u4fe1\u606f\u5728\u65f6\u57df\u4fe1\u53f7\u8f93\u5165\u4e2d\u672a\u88ab\u5b8c\u5168\u6355\u83b7\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.20602", "pdf": "https://arxiv.org/pdf/2508.20602", "abs": "https://arxiv.org/abs/2508.20602", "authors": ["Matthieu Correa", "Nicolas Vignais", "Isabelle A. Siegler", "Maxime Projetti"], "title": "Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis", "categories": ["eess.SP"], "comment": null, "summary": "Mechanomyography (MMG) is a promising tool for measuring muscle activity in\nthe field but its sensitivity to motion artifacts limits its application. In\nthis study, we proposed an adaptative filtering method for MMG accelerometers\nbased on the complete ensemble empirical mode decomposition, with adaptative\nnoise and spectral fuzzy entropy, to isolate motions artefacts from the MMG\nsignal in dynamic conditions. We compared our method with the traditional\nband-pass filtering technique, demonstrating better results concerning motion\nrecomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and\n0.842). Thus, this innovative method allows the filtering of motion artifacts\ndynamically in the 5-20 Hz bandwidth, which is not achievable with traditional\nmethod. However, the interpretation of accelerometric MMG signals from the\ntrunk and lower-limb muscles during walking or running should be approached\nwith great caution as impact-related accelerations are still present, though\ntheir exact quantity still needs to be quantified.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u6ee4\u6ce2\u7684MMG\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5206\u79bb\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u66f4\u4f18\uff0c\u4f46\u9700\u8c28\u614e\u5904\u7406\u8eaf\u5e72\u548c\u4e0b\u80a2\u808c\u8089\u4fe1\u53f7\u3002", "motivation": "MMG\u5728\u6d4b\u91cf\u808c\u8089\u6d3b\u52a8\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8fd0\u52a8\u4f2a\u5f71\u7684\u654f\u611f\u6027\u9650\u5236\u4e86\u5176\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u4fe1\u53f7\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u5b8c\u5168\u96c6\u6210\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u566a\u58f0\u548c\u8c31\u6a21\u7cca\u71b5\uff0c\u9694\u79bb\u52a8\u6001\u6761\u4ef6\u4e0b\u7684MMG\u4fe1\u53f7\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u5e26\u901a\u6ee4\u6ce2\u6280\u672f\uff0c\u65b0\u65b9\u6cd5\u5728\u4e09\u89d2\u808c\u548c\u7ad6\u810a\u808c\u7684\u4fe1\u53f7\u91cd\u6784\u4e0a\u8868\u73b0\u66f4\u4f18\uff08R\u00b2 = 0.907\u548c0.842\uff09\uff0c\u80fd\u52a8\u6001\u8fc7\u6ee45-20 Hz\u5e26\u5bbd\u7684\u4f2a\u5f71\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u52a8\u6001\u8fc7\u6ee4\u8fd0\u52a8\u4f2a\u5f71\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u9700\u6ce8\u610f\u8eaf\u5e72\u548c\u4e0b\u80a2\u808c\u8089\u4fe1\u53f7\u4e2d\u4ecd\u5b58\u5728\u4e0e\u51b2\u51fb\u76f8\u5173\u7684\u52a0\u901f\u5ea6\uff0c\u5176\u5177\u4f53\u5f71\u54cd\u5c1a\u9700\u8fdb\u4e00\u6b65\u91cf\u5316\u3002"}}
{"id": "2508.20457", "pdf": "https://arxiv.org/pdf/2508.20457", "abs": "https://arxiv.org/abs/2508.20457", "authors": ["Joonho Lee", "Yunho Kim", "Seokjoon Kim", "Quan Nguyen", "Youngjin Heo"], "title": "Learning Fast, Tool aware Collision Avoidance for Collaborative Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Ensuring safe and efficient operation of collaborative robots in human\nenvironments is challenging, especially in dynamic settings where both obstacle\nmotion and tasks change over time. Current robot controllers typically assume\nfull visibility and fixed tools, which can lead to collisions or overly\nconservative behavior. In our work, we introduce a tool-aware collision\navoidance system that adjusts in real time to different tool sizes and modes of\ntool-environment interaction. Using a learned perception model, our system\nfilters out robot and tool components from the point cloud, reasons about\noccluded area, and predicts collision under partial observability. We then use\na control policy trained via constrained reinforcement learning to produce\nsmooth avoidance maneuvers in under 10 milliseconds. In simulated and\nreal-world tests, our approach outperforms traditional approaches (APF, MPPI)\nin dynamic environments, while maintaining sub-millimeter accuracy. Moreover,\nour system operates with approximately 60% lower computational cost compared to\na state-of-the-art GPU-based planner. Our approach provides modular, efficient,\nand effective collision avoidance for robots operating in dynamic environments.\nWe integrate our method into a collaborative robot application and demonstrate\nits practical use for safe and responsive operation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u5177\u611f\u77e5\u7684\u5b9e\u65f6\u78b0\u649e\u907f\u514d\u7cfb\u7edf\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u534f\u4f5c\u673a\u5668\u4eba\u7684\u5b89\u5168\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u56e0\u5de5\u5177\u53d8\u5316\u548c\u90e8\u5206\u53ef\u89c1\u6027\u5bfc\u81f4\u7684\u78b0\u649e\u6216\u4fdd\u5b88\u884c\u4e3a\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5b66\u4e60\u611f\u77e5\u6a21\u578b\u548c\u53d7\u9650\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u65f6\u8c03\u6574\u5de5\u5177\u5927\u5c0f\u548c\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08APF\u3001MPPI\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e60%\uff0c\u4fdd\u6301\u4e9a\u6beb\u7c73\u7cbe\u5ea6\u3002", "conclusion": "\u7cfb\u7edf\u6a21\u5757\u5316\u3001\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2508.20761", "pdf": "https://arxiv.org/pdf/2508.20761", "abs": "https://arxiv.org/abs/2508.20761", "authors": ["Yaniv Mazor", "Tirza Routtenberg"], "title": "Weighted Bayesian Cram$\\acute{\\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation", "categories": ["eess.SP"], "comment": null, "summary": "Mixed-resolution architectures, combining high-resolution (analog) data with\ncoarsely quantized (e.g., 1-bit) data, are widely employed in emerging\ncommunication and radar systems to reduce hardware costs and power consumption.\nHowever, the use of coarsely quantized data introduces non-trivial tradeoffs in\nparameter estimation tasks. In this paper, we investigate the derivation of\nlower bounds for such systems. In particular, we develop the weighted Bayesian\nCramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight\nfunction. We demonstrate the special cases of: (i) the classical BCRB; (ii) the\nWBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse\nweighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight\nfunction. Based on the developed WBCRB, we propose a new method to approximate\nthe mean-squared-error (MSE) by partitioning the estimation problem into two\nregions: (a) where the 1-bit quantized data is informative; and (b) where it is\nsaturated. We apply region-specific WBCRB approximations in these regions to\nachieve an accurate composite MSE estimate. We derive the bounds and MSE\napproximation for the linear Gaussian orthonormal (LGO) model, which is\ncommonly used in practical signal processing applications. Our simulation\nresults demonstrate the use of the proposed bounds and approximation method in\nthe LGO model with a scalar unknown parameter. It is shown that the WBCRB\noutperforms the BCRB, where the BFIM-Inverse weighting version approaches the\noptimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is\ntighter and accurately predicts the non-monotonic behavior of the MSE in the\npresence of quantization errors.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df7\u5408\u5206\u8fa8\u7387\u7cfb\u7edf\u4e2d\u7684\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u52a0\u6743\u8d1d\u53f6\u65afCramer-Rao\u754c\uff08WBCRB\uff09\u6765\u91cf\u5316\u6027\u80fd\u6781\u9650\uff0c\u5e76\u901a\u8fc7\u5206\u533a\u65b9\u6cd5\u8fd1\u4f3c\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u3002", "motivation": "\u6df7\u5408\u5206\u8fa8\u7387\u7cfb\u7edf\u7ed3\u5408\u4e86\u9ad8\u5206\u8fa8\u7387\u548c\u7c97\u91cf\u5316\u6570\u636e\uff0c\u4ee5\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u548c\u529f\u8017\uff0c\u4f46\u7c97\u91cf\u5316\u6570\u636e\u5728\u53c2\u6570\u4f30\u8ba1\u4e2d\u5f15\u5165\u4e86\u6027\u80fd\u6298\u8877\u3002\u8bba\u6587\u65e8\u5728\u4e3a\u8fd9\u7c7b\u7cfb\u7edf\u63a8\u5bfc\u6027\u80fd\u4e0b\u9650\uff0c\u5e76\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684MSE\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u52a0\u6743\u8d1d\u53f6\u65afCramer-Rao\u754c\uff08WBCRB\uff09\uff0c\u6db5\u76d6\u7ecf\u5178BCRB\u548c\u5176\u4ed6\u4e24\u79cd\u7279\u6b8a\u52a0\u6743\u5f62\u5f0f\u3002\u901a\u8fc7\u5c06\u4f30\u8ba1\u95ee\u9898\u5206\u4e3a\u4fe1\u606f\u533a\u548c\u9971\u548c\u533a\uff0c\u91c7\u7528\u4e0d\u540c\u533a\u57df\u7684WBCRB\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7ec4\u5408\u51fa\u66f4\u51c6\u786e\u7684MSE\u4f30\u8ba1\u3002", "result": "\u5728LGO\u6a21\u578b\u4e2d\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cWBCRB\u4f18\u4e8eBCRB\uff0c\u4e14\u57fa\u4e8eBFIM\u9006\u52a0\u6743\u7684\u7248\u672c\u63a5\u8fd1\u6700\u4f18WBCRB\u3002WBCRB\u7684MSE\u8fd1\u4f3c\u66f4\u7d27\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u91cf\u5316\u8bef\u5dee\u4e0bMSE\u7684\u975e\u5355\u8c03\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684WBCRB\u548c\u5206\u533aMSE\u8fd1\u4f3c\u65b9\u6cd5\u4e3a\u6df7\u5408\u5206\u8fa8\u7387\u7cfb\u7edf\u7684\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u91cf\u5316\u8bef\u5dee\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.20547", "pdf": "https://arxiv.org/pdf/2508.20547", "abs": "https://arxiv.org/abs/2508.20547", "authors": ["Yunpeng Mei", "Hongjie Cao", "Yinqiu Xia", "Wei Xiao", "Zhaohan Feng", "Gang Wang", "Jie Chen"], "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time interactive grasp synthesis for dynamic objects remains challenging\nas existing methods fail to achieve low-latency inference while maintaining\npromptability. To bridge this gap, we propose SPGrasp (spatiotemporal\nprompt-driven dynamic grasp synthesis), a novel framework extending segment\nanything model v2 (SAMv2) for video stream grasp estimation. Our core\ninnovation integrates user prompts with spatiotemporal context, enabling\nreal-time interaction with end-to-end latency as low as 59 ms while ensuring\ntemporal consistency for dynamic objects. In benchmark evaluations, SPGrasp\nachieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on\nJacquard. On the challenging GraspNet-1Billion dataset under continuous\ntracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,\nrepresenting a 58.5% reduction compared to the prior state-of-the-art\npromptable method RoG-SAM while maintaining competitive accuracy. Real-world\nexperiments involving 13 moving objects demonstrate a 94.8% success rate in\ninteractive grasping scenarios. These results confirm SPGrasp effectively\nresolves the latency-interactivity trade-off in dynamic grasp synthesis. Code\nis available at https://github.com/sejmoonwei/SPGrasp.", "AI": {"tldr": "SPGrasp \u662f\u4e00\u79cd\u57fa\u4e8e SAMv2 \u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u52a8\u6001\u6293\u53d6\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u63d0\u793a\u548c\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\uff0859 ms\uff09\u548c\u9ad8\u51c6\u786e\u7387\uff0890.6%-93.8%\uff09\u7684\u52a8\u6001\u6293\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u6293\u53d6\u5408\u6210\u4e2d\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u4ea4\u4e92\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u517c\u987e\u8fd9\u4e24\u8005\u7684\u65b0\u6846\u67b6\u3002", "method": "SPGrasp \u6269\u5c55\u4e86 SAMv2 \u6a21\u578b\uff0c\u7ed3\u5408\u7528\u6237\u63d0\u793a\u548c\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u52a8\u6001\u6293\u53d6\u5408\u6210\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPGrasp \u7684\u6293\u53d6\u51c6\u786e\u7387\u8fbe\u5230 90.6% \u5230 93.8%\uff0c\u5e76\u4e14\u5728 GraspNet-1Billion \u4e0a\u5b9e\u73b0\u4e86 92.0% \u7684\u51c6\u786e\u7387\u548c 73.1 ms \u7684\u6bcf\u5e27\u5ef6\u8fdf\u3002", "conclusion": "SPGrasp \u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u6293\u53d6\u5408\u6210\u4e2d\u5ef6\u8fdf\u4e0e\u4ea4\u4e92\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.20864", "pdf": "https://arxiv.org/pdf/2508.20864", "abs": "https://arxiv.org/abs/2508.20864", "authors": ["Ehsan Sadeghi", "Paul Havinga"], "title": "Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar", "categories": ["eess.SP"], "comment": null, "summary": "This paper explores the deployment of mm-wave Frequency Modulated Continuous\nWave (FMCW) radar for vital sign detection across multiple scenarios. We focus\non overcoming the limitations of traditional sensing methods by enhancing\nsignal processing techniques to capture subtle physiological changes\neffectively. Our study introduces novel adaptations of the Prony and MUSIC\nalgorithms tailored for real-time heart and respiration rate monitoring,\nsignificantly advancing the accuracy and reliability of non-contact vital sign\nmonitoring using radar technologies. Notably, these algorithms demonstrate a\nrobust ability to suppress noise and harmonic interference. For instance, the\nmean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8\nand 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,\nrespectively. These results underscore the potential of FMCW radar as a\nreliable, non-invasive solution for continuous vital sign monitoring in\nhealthcare settings, particularly in clinical and emergency scenarios where\ntraditional contact-based monitoring is impractical.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u5728\u591a\u573a\u666f\u4e0b\u8fdb\u884c\u751f\u547d\u4f53\u5f81\u68c0\u6d4b\u7684\u90e8\u7f72\uff0c\u901a\u8fc7\u6539\u8fdb\u4fe1\u53f7\u5904\u7406\u6280\u672f\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u57fa\u4e8eProny\u548cMUSIC\u7b97\u6cd5\u7684\u5b9e\u65f6\u76d1\u6d4b\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u63a5\u89e6\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u514b\u670d\u4f20\u7edf\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u975e\u63a5\u89e6\u5f0f\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u4e34\u5e8a\u548c\u7d27\u6025\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684Prony\u548cMUSIC\u7b97\u6cd5\uff0c\u9488\u5bf9\u5b9e\u65f6\u5fc3\u7387\u548c\u547c\u5438\u7387\u76d1\u6d4b\u8fdb\u884c\u4f18\u5316\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u548c\u8c10\u6ce2\u5e72\u6270\u3002", "result": "MUSIC\u548cProny\u7b97\u6cd5\u5728\u5fc3\u7387\u68c0\u6d4b\u4e2d\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a1.8\u548c0.81\uff0c\u547c\u5438\u7387\u68c0\u6d4b\u4e2d\u5206\u522b\u4e3a1.01\u548c0.8\uff0c\u5c55\u793a\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FMCW\u96f7\u8fbe\u7ed3\u5408\u6539\u8fdb\u7b97\u6cd5\u4e3a\u975e\u63a5\u89e6\u5f0f\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u63a5\u89e6\u5f0f\u76d1\u6d4b\u4e0d\u9002\u7528\u7684\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.20561", "pdf": "https://arxiv.org/pdf/2508.20561", "abs": "https://arxiv.org/abs/2508.20561", "authors": ["Kipp McAdam Freud", "Yijiong Lin", "Nathan F. Lepora"], "title": "SimShear: Sim-to-Real Shear-based Tactile Servoing", "categories": ["cs.RO"], "comment": "2025 Conference on Robot Learning (CoRL)", "summary": "We present SimShear, a sim-to-real pipeline for tactile control that enables\nthe use of shear information without explicitly modeling shear dynamics in\nsimulation. Shear, arising from lateral movements across contact surfaces, is\ncritical for tasks involving dynamic object interactions but remains\nchallenging to simulate. To address this, we introduce shPix2pix, a\nshear-conditioned U-Net GAN that transforms simulated tactile images absent of\nshear, together with a vector encoding shear information, into realistic\nequivalents with shear deformations. This method outperforms baseline pix2pix\napproaches in simulating tactile images and in pose/shear prediction. We apply\nSimShear to two control tasks using a pair of low-cost desktop robotic arms\nequipped with a vision-based tactile sensor: (i) a tactile tracking task, where\na follower arm tracks a surface moved by a leader arm, and (ii) a collaborative\nco-lifting task, where both arms jointly hold an object while the leader\nfollows a prescribed trajectory. Our method maintains contact errors within 1\nto 2 mm across varied trajectories where shear sensing is essential, validating\nthe feasibility of sim-to-real shear modeling with rigid-body simulators and\nopening new directions for simulation in tactile robotics.", "AI": {"tldr": "SimShear \u662f\u4e00\u79cd\u5229\u7528\u526a\u5207\u4fe1\u606f\u7684\u89e6\u89c9\u63a7\u5236\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u6761\u4ef6 GAN \u751f\u6210\u5305\u542b\u526a\u5207\u53d8\u5f62\u7684\u89e6\u89c9\u56fe\u50cf\uff0c\u65e0\u9700\u663e\u5f0f\u6a21\u62df\u526a\u5207\u52a8\u6001\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u3002", "motivation": "\u526a\u5207\u529b\u5728\u52a8\u6001\u7269\u4f53\u4ea4\u4e92\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u62df\u526a\u5207\u52a8\u6001\u975e\u5e38\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u526a\u5207\u4fe1\u606f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u89e6\u89c9\u63a7\u5236\u4efb\u52a1\uff0c\u65e0\u9700\u590d\u6742\u6a21\u62df\u3002", "method": "\u63d0\u51fa shPix2pix\uff0c\u4e00\u79cd\u526a\u5207\u6761\u4ef6\u5316\u7684 U-Net GAN\uff0c\u5c06\u4e0d\u542b\u526a\u5207\u7684\u6a21\u62df\u89e6\u89c9\u56fe\u50cf\u4e0e\u526a\u5207\u5411\u91cf\u7ed3\u5408\uff0c\u751f\u6210\u5305\u542b\u526a\u5207\u53d8\u5f62\u7684\u771f\u5b9e\u89e6\u89c9\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u89e6\u89c9\u56fe\u50cf\u6a21\u62df\u548c\u59ff\u6001/\u526a\u5207\u9884\u6d4b\u4e0a\u4f18\u4e8e\u57fa\u7ebf pix2pix \u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4fdd\u6301\u63a5\u89e6\u8bef\u5dee\u5728 1-2 \u6beb\u7c73\u5185\u3002", "conclusion": "SimShear \u901a\u8fc7\u521a\u6027\u6a21\u62df\u5668\u5b9e\u73b0\u526a\u5207\u529b\u7684\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u5e94\u7528\uff0c\u4e3a\u89e6\u89c9\u673a\u5668\u4eba\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.20990", "pdf": "https://arxiv.org/pdf/2508.20990", "abs": "https://arxiv.org/abs/2508.20990", "authors": ["Hong-Yan Zhang", "Haoting Liu", "Rui-Jia Lin", "Yu Zhou"], "title": "A Correction for the Paper \"Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis\"", "categories": ["eess.SP"], "comment": "13 pages, 4 figures, 2 tables", "summary": "The symplectic geometry mode decomposition (SGMD) is a powerful method for\ndecomposing time series, which is based on the diagonal averaging principle\n(DAP) inherited from the singular spectrum analysis (SSA). Although the authors\nof SGMD method generalized the form of the trajectory matrix in SSA, the DAP is\nnot updated simultaneously. In this work, we pointed out the limitations of the\nSGMD method and fixed the bugs with the pulling back theorem for computing the\ngiven component of time series from the corresponding component of trajectory\nmatrix.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u4e86SGMD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u56de\u62c9\u5b9a\u7406\u4fee\u590d\u4e86\u5176\u7f3a\u9677\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u91cf\u3002", "motivation": "SGMD\u65b9\u6cd5\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5176\u7ee7\u627f\u81eaSSA\u7684\u5bf9\u89d2\u5e73\u5747\u539f\u7406\uff08DAP\uff09\u672a\u968f\u8f68\u8ff9\u77e9\u9635\u5f62\u5f0f\u7684\u6269\u5c55\u800c\u66f4\u65b0\uff0c\u5bfc\u81f4\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u5206\u91cf\u65f6\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u56de\u62c9\u5b9a\u7406\u4fee\u6b63SGMD\u65b9\u6cd5\u4e2d\u7684DAP\u95ee\u9898\uff0c\u4ee5\u51c6\u786e\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u5206\u91cf\u3002", "result": "\u4fee\u590d\u4e86SGMD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u3002", "conclusion": "\u56de\u62c9\u5b9a\u7406\u7684\u5e94\u7528\u6210\u529f\u4fee\u6b63\u4e86SGMD\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u63d0\u5347\u4e86\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e2d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.20661", "pdf": "https://arxiv.org/pdf/2508.20661", "abs": "https://arxiv.org/abs/2508.20661", "authors": ["TianChen Huang", "Wei Gao", "Runchen Xu", "Shiwu Zhang"], "title": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking", "categories": ["cs.RO"], "comment": null, "summary": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u57fa\u7840\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u72ed\u7a84\u6a2a\u6881\u4e0a\u884c\u8d70\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8026\u5408XCoM/LIPM\u6b65\u6001\u6a21\u677f\u4e0e\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u89c4\u5212\u5668\u53ca\u7b80\u5355\u4f4e\u5c42\u8ddf\u8e2a\u5668\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u6a2a\u6881\u7a7f\u8d8a\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u72ed\u7a84\u6a2a\u6881\u4e0a\u884c\u8d70\u65f6\u7a00\u758f\u3001\u5b89\u5168\u5173\u952e\u6027\u63a5\u89e6\u4ee5\u53ca\u7eaf\u5b66\u4e60\u7b56\u7565\u8106\u5f31\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u5e73\u5730\u4e0a\u8bad\u7ec3\u8ddf\u8e2a\u5668\u4ee5\u7a33\u5b9a\u8ddf\u8e2a\u6b65\u6001\u76ee\u6807\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u9ad8\u5c42\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u6b8b\u5dee\u7ec6\u5316\u6b65\u6001\u4ee5\u4f18\u5148\u5b89\u5168\u7cbe\u786e\u7684\u843d\u811a\u70b9\u3002", "result": "\u5728Unitree G1\u4e0a\u6210\u529f\u7a7f\u8d8a0.2\u7c73\u5bbd\u30013\u7c73\u957f\u7684\u6a2a\u6881\uff0c\u6b8b\u5dee\u7ec6\u5316\u5728\u6210\u529f\u7387\u3001\u4e2d\u7ebf\u8ddf\u8e2a\u548c\u5b89\u5168\u88d5\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u900f\u660e\u5206\u6790\u548c\u4f4e\u6469\u64e6\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u72ed\u7a84\u6a2a\u6881\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u7ed3\u5408\u4e86\u7269\u7406\u6a21\u578b\u4e0e\u5b66\u4e60\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5065\u8fd0\u52a8\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20193", "pdf": "https://arxiv.org/pdf/2508.20193", "abs": "https://arxiv.org/abs/2508.20193", "authors": ["Hossein Ahmadi", "Banafsheh Saffari"], "title": "Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Automatic modulation recognition (AMR) is critical for cognitive radio,\nspectrum monitoring, and secure wireless communication. However, existing\nsolutions often rely on large labeled datasets or multi-stage training\npipelines, which limit scalability and generalization in practice. We propose a\nunified Vision Transformer (ViT) framework that integrates supervised,\nself-supervised, and reconstruction objectives. The model combines a ViT\nencoder, a lightweight convolutional decoder, and a linear classifier; the\nreconstruction branch maps augmented signals back to their originals, anchoring\nthe encoder to fine-grained I/Q structure. This strategy promotes robust,\ndiscriminative feature learning during pretraining, while partial label\nsupervision in fine-tuning enables effective classification with limited\nlabels. On the RML2018.01A dataset, our approach outperforms supervised CNN and\nViT baselines in low-label regimes, approaches ResNet-level accuracy with only\n15-20% labeled data, and maintains strong performance across varying SNR\nlevels. Overall, the framework provides a simple, generalizable, and\nlabel-efficient solution for AMR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u76d1\u7763\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u76ee\u6807\uff0c\u89e3\u51b3\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728\u4f4e\u6807\u7b7e\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408ViT\u7f16\u7801\u5668\u3001\u8f7b\u91cf\u5377\u79ef\u89e3\u7801\u5668\u548c\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u91cd\u5efa\u76ee\u6807\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\uff0c\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u90e8\u5206\u6807\u7b7e\u76d1\u7763\u3002", "result": "\u5728RML2018.01A\u6570\u636e\u96c6\u4e0a\uff0c\u4f4e\u6807\u7b7e\u7387\u4e0b\u4f18\u4e8eCNN\u548cViT\u57fa\u7ebf\uff0c\u4ec5\u970015-20%\u6807\u6ce8\u6570\u636e\u5373\u53ef\u63a5\u8fd1ResNet\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAMR\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u6807\u7b7e\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.20664", "pdf": "https://arxiv.org/pdf/2508.20664", "abs": "https://arxiv.org/abs/2508.20664", "authors": ["Kan Chen", "Zhen Meng", "Xiangmin Xu", "Jiaming Yang", "Emma Li", "Philip G. Zhao"], "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": "This paper has submitted to IEEE Transactions on Mobile Computing", "summary": "Real-time human-device interaction in industrial Metaverse faces challenges\nsuch as high computational load, limited bandwidth, and strict latency. This\npaper proposes a task-oriented edge-assisted cross-system framework using\ndigital twins (DTs) to enable responsive interactions. By predicting operator\nmotions, the system supports: 1) proactive Metaverse rendering for visual\nfeedback, and 2) preemptive control of remote devices. The DTs are decoupled\ninto two virtual functions-visual display and robotic control-optimizing both\nperformance and adaptability. To enhance generalizability, we introduce the\nHuman-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which\ndynamically adjusts prediction horizons. Evaluation on two tasks demonstrates\nthe framework's effectiveness: in a Trajectory-Based Drawing Control task, it\nreduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene\nrepresentation task for nuclear decommissioning, it achieves a PSNR of 22.11,\nSSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's\ncapability to ensure spatial precision and visual fidelity in real-time,\nhigh-risk industrial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u8fb9\u7f18\u8f85\u52a9\u8de8\u7cfb\u7edf\u6846\u67b6\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5143\u5b87\u5b99\u4e2d\u7684\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\u3001\u6709\u9650\u5e26\u5bbd\u548c\u4e25\u683c\u5ef6\u8fdf\u95ee\u9898\u3002\u901a\u8fc7\u9884\u6d4b\u64cd\u4f5c\u5458\u52a8\u4f5c\u5e76\u91c7\u7528HITL-MAML\u7b97\u6cd5\uff0c\u7cfb\u7edf\u5728\u8f68\u8ff9\u7ed8\u5236\u548c3D\u573a\u666f\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5de5\u4e1a\u5143\u5b87\u5b99\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u9762\u4e34\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\u3001\u5e26\u5bbd\u9650\u5236\u548c\u4e25\u683c\u5ef6\u8fdf\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u65f6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u4efb\u52a1\u5bfc\u5411\u8fb9\u7f18\u8f85\u52a9\u6846\u67b6\uff0c\u5c06\u6570\u5b57\u5b6a\u751f\u5206\u89e3\u4e3a\u89c6\u89c9\u663e\u793a\u548c\u673a\u5668\u4eba\u63a7\u5236\u4e24\u4e2a\u865a\u62df\u529f\u80fd\uff0c\u5e76\u5f15\u5165HITL-MAML\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u8303\u56f4\u3002", "result": "\u5728\u8f68\u8ff9\u7ed8\u5236\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u52a0\u6743RMSE\u4ece0.0712\u7c73\u964d\u81f30.0101\u7c73\uff1b\u5728\u6838\u9000\u5f79\u5b9e\u65f63D\u573a\u666f\u8868\u793a\u4efb\u52a1\u4e2d\uff0cPSNR\u4e3a22.11\uff0cSSIM\u4e3a0.8729\uff0cLPIPS\u4e3a0.1298\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u786e\u4fdd\u9ad8\u98ce\u9669\u5de5\u4e1a\u73af\u5883\u4e2d\u5b9e\u65f6\u4ea4\u4e92\u7684\u7a7a\u95f4\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.20273", "pdf": "https://arxiv.org/pdf/2508.20273", "abs": "https://arxiv.org/abs/2508.20273", "authors": ["Yujin Kim", "Richa Namballa", "Magdalena Fuentes"], "title": "Live Vocal Extraction from K-pop Performances", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "2 pages + references, 1 figure, Extended Abstracts for the\n  Late-Breaking Demo Session of the 26th International Society for Music\n  Information Retrieval Conference", "summary": "K-pop's global success is fueled by its dynamic performances and vibrant fan\nengagement. Inspired by K-pop fan culture, we propose a methodology for\nautomatically extracting live vocals from performances. We use a combination of\nsource separation, cross-correlation, and amplitude scaling to automatically\nremove pre-recorded vocals and instrumentals from a live performance. Our\npreliminary work introduces the task of live vocal separation and provides a\nfoundation for future research in this topic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4eceK-pop\u73b0\u573a\u8868\u6f14\u4e2d\u81ea\u52a8\u63d0\u53d6\u73b0\u573a\u4eba\u58f0\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e90\u5206\u79bb\u3001\u4e92\u76f8\u5173\u548c\u632f\u5e45\u7f29\u653e\u6280\u672f\u3002", "motivation": "\u53d7K-pop\u7c89\u4e1d\u6587\u5316\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u573a\u8868\u6f14\u4e2d\u9884\u5f55\u4eba\u58f0\u4e0e\u97f3\u4e50\u7684\u5206\u79bb\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6e90\u5206\u79bb\u3001\u4e92\u76f8\u5173\u548c\u632f\u5e45\u7f29\u653e\u6280\u672f\uff0c\u81ea\u52a8\u53bb\u9664\u9884\u5f55\u4eba\u58f0\u548c\u4f34\u594f\u3002", "result": "\u521d\u6b65\u5de5\u4f5c\u4e3a\u73b0\u573a\u4eba\u58f0\u5206\u79bb\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.20688", "pdf": "https://arxiv.org/pdf/2508.20688", "abs": "https://arxiv.org/abs/2508.20688", "authors": ["Thanh Thi Nguyen", "Quoc Viet Hung Nguyen", "Jonathan Kua", "Imran Razzak", "Dung Nguyen", "Saeid Nahavandi"], "title": "Task Allocation for Autonomous Machines using Computational Intelligence and Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "Enabling multiple autonomous machines to perform reliably requires the\ndevelopment of efficient cooperative control algorithms. This paper presents a\nsurvey of algorithms that have been developed for controlling and coordinating\nautonomous machines in complex environments. We especially focus on task\nallocation methods using computational intelligence (CI) and deep reinforcement\nlearning (RL). The advantages and disadvantages of the surveyed methods are\nanalysed thoroughly. We also propose and discuss in detail various future\nresearch directions that shed light on how to improve existing algorithms or\ncreate new methods to enhance the employability and performance of autonomous\nmachines in real-world applications. The findings indicate that CI and deep RL\nmethods provide viable approaches to addressing complex task allocation\nproblems in dynamic and uncertain environments. The recent development of deep\nRL has greatly contributed to the literature on controlling and coordinating\nautonomous machines, and it has become a growing trend in this area. It is\nenvisaged that this paper will provide researchers and engineers with a\ncomprehensive overview of progress in machine learning research related to\nautonomous machines. It also highlights underexplored areas, identifies\nemerging methodologies, and suggests new avenues for exploration in future\nresearch within this domain.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7528\u4e8e\u63a7\u5236\u548c\u534f\u8c03\u81ea\u4e3b\u673a\u5668\u7684\u7b97\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u667a\u80fd\uff08CI\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63d0\u5347\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6548\u7684\u534f\u540c\u63a7\u5236\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u591a\u81ea\u4e3b\u673a\u5668\u7684\u53ef\u9760\u8fd0\u884c\u3002", "method": "\u7efc\u8ff0\u73b0\u6709\u7684\u63a7\u5236\u548c\u534f\u8c03\u81ea\u4e3b\u673a\u5668\u7684\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u57fa\u4e8eCI\u548c\u6df1\u5ea6RL\u7684\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u3002", "result": "CI\u548c\u6df1\u5ea6RL\u4e3a\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6df1\u5ea6RL\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u81ea\u4e3b\u673a\u5668\u63a7\u5236\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u7814\u7a76\u7684\u5168\u9762\u6982\u89c8\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.20336", "pdf": "https://arxiv.org/pdf/2508.20336", "abs": "https://arxiv.org/abs/2508.20336", "authors": ["Johnson Zhou", "Joseph West", "Krista A. Ehinger", "Zhenming Ren", "Sam E. John", "David B. Grayden"], "title": "Adaptive Segmentation of EEG for Machine Learning Applications", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "comment": null, "summary": "Objective. Electroencephalography (EEG) data is derived by sampling\ncontinuous neurological time series signals. In order to prepare EEG signals\nfor machine learning, the signal must be divided into manageable segments. The\ncurrent naive approach uses arbitrary fixed time slices, which may have limited\nbiological relevance because brain states are not confined to fixed intervals.\nWe investigate whether adaptive segmentation methods are beneficial for machine\nlearning EEG analysis.\n  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that\ncreates variable-length segments based on statistical differences in the EEG\ndata and propose ways to use them with modern machine learning approaches that\ntypically require fixed-length input. We assess CTXSEG using controllable\nsynthetic data generated by our novel signal generator CTXGEN. While our CTXSEG\nmethod has general utility, we validate it on a real-world use case by applying\nit to an EEG seizure detection problem. We compare the performance of CTXSEG\nwith fixed-length segmentation in the preprocessing step of a typical EEG\nmachine learning pipeline for seizure detection.\n  Main results. We found that using CTXSEG to prepare EEG data improves seizure\ndetection performance compared to fixed-length approaches when evaluated using\na standardized framework, without modifying the machine learning method, and\nrequires fewer segments.\n  Significance. This work demonstrates that adaptive segmentation with CTXSEG\ncan be readily applied to modern machine learning approaches, with potential to\nimprove performance. It is a promising alternative to fixed-length segmentation\nfor signal preprocessing and should be considered as part of the standard\npreprocessing repertoire in EEG machine learning applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5206\u5272\u65b9\u6cd5CTXSEG\uff0c\u7528\u4e8e\u6539\u8fdbEEG\u4fe1\u53f7\u7684\u5206\u5272\uff0c\u63d0\u5347\u4e86\u766b\u75eb\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u65f6\u95f4\u5206\u5272EEG\u4fe1\u53f7\u7684\u65b9\u6cd5\u7f3a\u4e4f\u751f\u7269\u5b66\u4f9d\u636e\uff0c\u4f5c\u8005\u7814\u7a76\u81ea\u9002\u5e94\u5206\u5272\u662f\u5426\u5bf9\u673a\u5668\u5b66\u4e60\u5206\u6790\u66f4\u6709\u5229\u3002", "method": "\u63d0\u51faCTXSEG\u65b9\u6cd5\uff0c\u57fa\u4e8eEEG\u6570\u636e\u7684\u7edf\u8ba1\u5dee\u5f02\u751f\u6210\u53ef\u53d8\u957f\u5ea6\u5206\u6bb5\uff0c\u5e76\u7ed3\u5408\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "CTXSEG\u5728\u766b\u75eb\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u56fa\u5b9a\u957f\u5ea6\u5206\u5272\uff0c\u4e14\u6240\u9700\u5206\u6bb5\u66f4\u5c11\u3002", "conclusion": "\u81ea\u9002\u5e94\u5206\u5272CTXSEG\u662fEEG\u673a\u5668\u5b66\u4e60\u9884\u5904\u7406\u7684\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2508.20740", "pdf": "https://arxiv.org/pdf/2508.20740", "abs": "https://arxiv.org/abs/2508.20740", "authors": ["Yuki Tanaka", "Seiichiro Katsura"], "title": "Non-expert to Expert Motion Translation Using Generative Adversarial Networks", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Decreasing skilled workers is a very serious problem in the world. To deal\nwith this problem, the skill transfer from experts to robots has been\nresearched. These methods which teach robots by human motion are called\nimitation learning. Experts' skills generally appear in not only position data,\nbut also force data. Thus, position and force data need to be saved and\nreproduced. To realize this, a lot of research has been conducted in the\nframework of a motion-copying system. Recent research uses machine learning\nmethods to generate motion commands. However, most of them could not change\ntasks by following human intention. Some of them can change tasks by\nconditional training, but the labels are limited. Thus, we propose the flexible\nmotion translation method by using Generative Adversarial Networks. The\nproposed method enables users to teach robots tasks by inputting data, and\nskills by a trained model. We evaluated the proposed system with a 3-DOF\ncalligraphy robot.", "AI": {"tldr": "\u6a21\u4eff\u5b66\u4e60\u7528\u4e8e\u673a\u5668\u4eba\u6280\u80fd\u8f6c\u79fb\uff0c\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u63d0\u51fa\u7075\u6d3b\u4efb\u52a1\u8c03\u6574\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u719f\u7ec3\u5de5\u4eba\u51cf\u5c11\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4ece\u4e13\u5bb6\u5411\u673a\u5668\u4eba\u8f6c\u79fb\u6280\u80fd\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u5f53\u524d\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6839\u636e\u4eba\u7c7b\u610f\u56fe\u7075\u6d3b\u8c03\u6574\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u6709\u6761\u4ef6\u8bad\u7ec3\u7684\u65b9\u6cd5\u6807\u7b7e\u6709\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u7075\u6d3b\u8fd0\u52a8\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u8f93\u5165\u6570\u636e\u548c\u8bad\u7ec3\u6a21\u578b\u5411\u673a\u5668\u4eba\u6559\u6388\u4efb\u52a1\u4e0e\u6280\u80fd\u3002", "result": "\u4f7f\u75283\u81ea\u7531\u5ea6\u4e66\u6cd5\u673a\u5668\u4eba\u5bf9\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u6280\u80fd\u7684\u7075\u6d3b\u8f6c\u79fb\u548c\u4efb\u52a1\u8c03\u6574\u3002"}}
{"id": "2508.20431", "pdf": "https://arxiv.org/pdf/2508.20431", "abs": "https://arxiv.org/abs/2508.20431", "authors": ["Antonio Victor Machado de Oliveira", "Debjit Sarkar", "Ali Hajimiri"], "title": "Electrical Impedance Tomography with an Integrated Picoliter-Volume Subtractive Microfluidic Chamber in 65 nm CMOS", "categories": ["physics.ins-det", "eess.SP"], "comment": "4 pages, 8 figures", "summary": "Electrical impedance tomography with fully integrated microfluidics and\nelectronics is presented for the first time in a CMOS chip. Chambers and\nelectrodes are fabricated in the interconnect layers of a 65 nm CMOS chip\nthrough post-processing, enabling picoliter-volumes to be processed and imaged.\nTomography maps are reconstructed by reading out voltages from a 16-element\nelectrode array and processing the data off-chip, and sources of variation in\nreconstruction are discussed. The EIT system presented in this work serves as a\nproof-of-concept towards using CMOS as a platform for co-integrated\nmicrofluidics and electronics.", "AI": {"tldr": "\u9996\u6b21\u5728CMOS\u82af\u7247\u4e2d\u5b9e\u73b0\u4e86\u5b8c\u5168\u96c6\u6210\u7684\u5fae\u6d41\u63a7\u4e0e\u7535\u5b50\u5b66\u7684\u7535\u963b\u6297\u65ad\u5c42\u6210\u50cf\u7cfb\u7edf\uff0c\u901a\u8fc7\u540e\u5904\u7406\u572865\u7eb3\u7c73CMOS\u82af\u7247\u7684\u4e92\u8fde\u5c42\u4e2d\u5236\u9020\u8154\u5ba4\u548c\u7535\u6781\uff0c\u5b9e\u73b0\u4e86\u76ae\u5347\u7ea7\u4f53\u79ef\u7684\u6837\u672c\u5904\u7406\u4e0e\u6210\u50cf\u3002", "motivation": "\u63a2\u7d22CMOS\u6280\u672f\u4f5c\u4e3a\u5fae\u6d41\u63a7\u548c\u7535\u5b50\u5b66\u5171\u96c6\u6210\u5e73\u53f0\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5fae\u6d41\u63a7\u6837\u672c\u6210\u50cf\u548c\u5206\u6790\u3002", "method": "\u572865\u7eb3\u7c73CMOS\u82af\u7247\u7684\u4e92\u8fde\u5c42\u4e2d\u5236\u9020\u5fae\u6d41\u63a7\u8154\u5ba4\u548c16\u7535\u6781\u9635\u5217\uff0c\u901a\u8fc7\u5916\u90e8\u6570\u636e\u5904\u7406\u91cd\u5efa\u65ad\u5c42\u56fe\u50cf\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u76ae\u5347\u7ea7\u6837\u672c\u7684\u7535\u963b\u6297\u65ad\u5c42\u6210\u50cf\uff0c\u5e76\u5206\u6790\u4e86\u7cfb\u7edf\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u6765\u6e90\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3aCMOS\u4f5c\u4e3a\u5fae\u6d41\u63a7\u4e0e\u7535\u5b50\u5b66\u5171\u96c6\u6210\u5e73\u53f0\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5fae\u6d41\u63a7\u6210\u50cf\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.20812", "pdf": "https://arxiv.org/pdf/2508.20812", "abs": "https://arxiv.org/abs/2508.20812", "authors": ["Lorenzo Busellato", "Federico Cunico", "Diego Dall'Alba", "Marco Emporio", "Andrea Giachetti", "Riccardo Muradore", "Marco Cristani"], "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "To enable flexible, high-throughput automation in settings where people and\nrobots share workspaces, collaborative robotic cells must reconcile stringent\nsafety guarantees with the need for responsive and effective behavior. A\ndynamic obstacle is the stochastic, task-dependent variability of human motion:\nwhen robots fall back on purely reactive or worst-case envelopes, they brake\nunnecessarily, stall task progress, and tamper with the fluidity that true\nHuman-Robot Interaction demands. In recent years, learning-based human-motion\nprediction has rapidly advanced, although most approaches produce worst-case\nscenario forecasts that often do not treat prediction uncertainty in a\nwell-structured way, resulting in over-conservative planning algorithms,\nlimiting their flexibility. We introduce Uncertainty-Aware Predictive Control\nBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistic\nhuman hand motion forecasting with the formal safety guarantees of Control\nBarrier Functions. In contrast to other variants, our framework allows for\ndynamic adjustment of the safety margin thanks to the human motion uncertainty\nestimation provided by a forecasting module. Thanks to uncertainty estimation,\nUA-PCBFs empower collaborative robots with a deeper understanding of future\nhuman states, facilitating more fluid and intelligent interactions through\ninformed motion planning. We validate UA-PCBFs through comprehensive real-world\nexperiments with an increasing level of realism, including automated setups (to\nperform exactly repeatable motions) with a robotic hand and direct human-robot\ninteractions (to validate promptness, usability, and human confidence).\nRelative to state-of-the-art HRI architectures, UA-PCBFs show better\nperformance in task-critical metrics, significantly reducing the number of\nviolations of the robot's safe space during interaction with respect to the\nstate-of-the-art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08UA-PCBFs\uff09\uff0c\u7ed3\u5408\u4e86\u6982\u7387\u6027\u4eba\u624b\u8fd0\u52a8\u9884\u6d4b\u4e0e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u7684\u6d41\u7545\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u4eba\u4e0e\u673a\u5668\u4eba\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u7684\u73af\u5883\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u8fc7\u5ea6\u4fdd\u5b88\u7684\u9884\u6d4b\u548c\u89c4\u5212\u9650\u5236\u4e86\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6548\u4e0e\u5b89\u5168\u7684\u9700\u6c42\u3002", "method": "UA-PCBFs\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b89\u5168\u8fb9\u754c\uff0c\u7ed3\u5408\u9884\u6d4b\u6a21\u5757\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUA-PCBFs\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u5b89\u5168\u7a7a\u95f4\u88ab\u4fb5\u72af\u7684\u6b21\u6570\uff0c\u63d0\u5347\u4e86\u4ea4\u4e92\u6d41\u7545\u6027\u3002", "conclusion": "UA-PCBFs\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.20704", "pdf": "https://arxiv.org/pdf/2508.20704", "abs": "https://arxiv.org/abs/2508.20704", "authors": ["Wei Jiang", "Hans D Schotten"], "title": "Achieving Optimal Performance-Cost Trade-Off in Hierarchical Cell-Free Massive MIMO", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "IEEE Globecom 2025", "summary": "Cell-free (CF) massive MIMO offers uniform service via distributed access\npoints (APs), which impose high deployment costs. A novel design called\nhierarchical cell-free (HCF) addresses this problem by replacing some APs with\na central base station, thereby lowering the costs of fronthaul network\n(wireless sites and fiber cables) while preserving performance. To identify the\noptimal uplink configuration in HCF massive MIMO, this paper provides the first\ncomprehensive analysis, benchmarking it against cellular and CF systems. We\ndevelop a unified analytical framework for spectral efficiency that supports\narbitrary combining schemes and introduce a novel hierarchical combining\napproach tailored to HCF two-tier architecture. Through analysis and evaluation\nof user fairness, system capacity, fronthaul requirements, and computational\ncomplexity, this paper identifies that HCF using centralized zero-forcing\ncombining achieves the optimal balance between performance and cost-efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u65e0\u8702\u7a9d\uff08HCF\uff09\u5927\u89c4\u6a21MIMO\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7528\u4e2d\u592e\u57fa\u7ad9\u66ff\u6362\u90e8\u5206\u63a5\u5165\u70b9\uff08AP\uff09\uff0c\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u901a\u8fc7\u5206\u6790\uff0c\u53d1\u73b0HCF\u91c7\u7528\u96c6\u4e2d\u5f0f\u96f6\u8feb\u7ec4\u5408\u5728\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u4e86\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u65e0\u8702\u7a9d\uff08CF\uff09\u5927\u89c4\u6a21MIMO\u56e0\u5206\u5e03\u5f0f\u63a5\u5165\u70b9\uff08AP\uff09\u90e8\u7f72\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5206\u5c42\u65e0\u8702\u7a9d\uff08HCF\uff09\u8bbe\u8ba1\uff0c\u4ee5\u964d\u4f4e\u524d\u4f20\u7f51\u7edc\u6210\u672c\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u652f\u6301\u4efb\u610f\u7ec4\u5408\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9HCF\u4e24\u5c42\u7ea7\u67b6\u6784\u7684\u5c42\u7ea7\u7ec4\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u7528\u6237\u516c\u5e73\u6027\u3001\u7cfb\u7edf\u5bb9\u91cf\u3001\u524d\u4f20\u9700\u6c42\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u8fdb\u884c\u4f18\u5316\u3002", "result": "HCF\u91c7\u7528\u96c6\u4e2d\u5f0f\u96f6\u8feb\u7ec4\u5408\u80fd\u591f\u5728\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "HCF\u8bbe\u8ba1\u5728\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u96c6\u4e2d\u5f0f\u96f6\u8feb\u7ec4\u5408\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u3002"}}
{"id": "2508.20831", "pdf": "https://arxiv.org/pdf/2508.20831", "abs": "https://arxiv.org/abs/2508.20831", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "A Soft Fabric-Based Thermal Haptic Device for VR and Teleoperation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel fabric-based thermal-haptic interface for virtual\nreality and teleoperation. It integrates pneumatic actuation and conductive\nfabric with an innovative ultra-lightweight design, achieving only 2~g for each\nfinger unit. By embedding heating elements within textile pneumatic chambers,\nthe system delivers modulated pressure and thermal stimuli to fingerpads\nthrough a fully soft, wearable interface.\n  Comprehensive characterization demonstrates rapid thermal modulation with\nheating rates up to 3$^{\\circ}$C/s, enabling dynamic thermal feedback for\nvirtual or teleoperation interactions. The pneumatic subsystem generates forces\nup to 8.93~N at 50~kPa, while optimization of fingerpad-actuator clearance\nenhances cooling efficiency with minimal force reduction. Experimental\nvalidation conducted with two different user studies shows high temperature\nidentification accuracy (0.98 overall) across three thermal levels, and\nsignificant manipulation improvements in a virtual pick-and-place tasks.\nResults show enhanced success rates (88.5\\% to 96.4\\%, p = 0.029) and improved\nforce control precision (p = 0.013) when haptic feedback is enabled, validating\nthe effectiveness of the integrated thermal-haptic approach for advanced\nhuman-machine interaction applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u7ec7\u7269\u7684\u70ed\u89e6\u89c9\u63a5\u53e3\uff0c\u7528\u4e8e\u865a\u62df\u73b0\u5b9e\u548c\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u5177\u6709\u8d85\u8f7b\u91cf\u5316\u8bbe\u8ba1\u548c\u5feb\u901f\u70ed\u8c03\u8282\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u548c\u8fdc\u7a0b\u64cd\u4f5c\u9700\u8981\u66f4\u81ea\u7136\u548c\u9ad8\u6548\u7684\u89e6\u89c9\u53cd\u9988\uff0c\u4f20\u7edf\u63a5\u53e3\u901a\u5e38\u7b28\u91cd\u4e14\u529f\u80fd\u5355\u4e00\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u5316\u3001\u591a\u529f\u80fd\u7684\u70ed\u89e6\u89c9\u63a5\u53e3\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u6c14\u52a8\u9a71\u52a8\u548c\u5bfc\u7535\u7ec7\u7269\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8d85\u8f7b\u91cf\u5316\u63a5\u53e3\uff0c\u6bcf\u6839\u624b\u6307\u5355\u5143\u4ec5\u91cd2\u514b\u3002\u63a5\u53e3\u5185\u7f6e\u52a0\u70ed\u5143\u4ef6\uff0c\u901a\u8fc7\u8f6f\u6027\u53ef\u7a7f\u6234\u8bbe\u8ba1\u5b9e\u73b0\u538b\u529b\u548c\u70ed\u523a\u6fc0\u7684\u8c03\u8282\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u63a5\u53e3\u80fd\u5feb\u901f\u8c03\u8282\u6e29\u5ea6\uff08\u6700\u9ad83\u00b0C/s\uff09\uff0c\u5e76\u63d0\u4f9b\u9ad8\u8fbe8.93N\u7684\u529b\u3002\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u9ad8\u6e29\u5ea6\u8bc6\u522b\u51c6\u786e\u7387\uff080.98\uff09\u548c\u663e\u8457\u63d0\u5347\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0888.5%\u523096.4%\uff09\u53ca\u529b\u63a7\u5236\u7cbe\u5ea6\u3002", "conclusion": "\u96c6\u6210\u7684\u70ed\u89e6\u89c9\u63a5\u53e3\u5728\u865a\u62df\u73b0\u5b9e\u548c\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u7ea7\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.20708", "pdf": "https://arxiv.org/pdf/2508.20708", "abs": "https://arxiv.org/abs/2508.20708", "authors": ["Wei Jiang", "Hans D. Schotten"], "title": "What is the Most Efficient Technique for Uplink Cell-Free Massive MIMO?", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "IEEE VTC 2025-Fall", "summary": "This paper seeks to determine the most efficient uplink technique for\ncell-free massive MIMO systems. Despite offering great advances, existing works\nsuffer from fragmented methodologies and inconsistent assumptions (e.g.,\nsingle- vs. multi-antenna access points, ideal vs. spatially correlated\nchannels). To address these limitations, we: (1) establish a unified analytical\nframework compatible with centralized/distributed processing and diverse\ncombining schemes; (2) develop a universal optimization strategy for max-min\npower control; and (3) conduct a holistic study among four critical metrics:\nworst-case user spectral efficiency (fairness), system capacity, fronthaul\nsignaling, and computational complexity. Through analyses and evaluation, this\nwork ultimately identifies the optimal uplink technique for practical cell-free\ndeployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u786e\u5b9a\u4e86\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u6700\u4f18\u5316\u7684\u4e0a\u884c\u94fe\u8def\u6280\u672f\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u65b9\u6cd5\u8bba\u5206\u6563\u4e14\u5047\u8bbe\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4f18\u5316\u3002", "method": "\u5efa\u7acb\u4e86\u517c\u5bb9\u96c6\u4e2d\u5f0f/\u5206\u5e03\u5f0f\u5904\u7406\u7684\u5206\u6790\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u901a\u7528\u7684\u6700\u5927\u6700\u5c0f\u529f\u7387\u63a7\u5236\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6027\u80fd\u6307\u6807\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5206\u6790\u548c\u8bc4\u4f30\uff0c\u786e\u5b9a\u4e86\u9002\u7528\u4e8e\u5b9e\u9645\u65e0\u8702\u7a9d\u90e8\u7f72\u7684\u6700\u4f18\u4e0a\u884c\u94fe\u8def\u6280\u672f\u3002", "conclusion": "\u672c\u8bba\u6587\u4e3a\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u4e0a\u884c\u94fe\u8def\u6280\u672f\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6027\u80fd\u8bc4\u4f30\u3002"}}
{"id": "2508.20836", "pdf": "https://arxiv.org/pdf/2508.20836", "abs": "https://arxiv.org/abs/2508.20836", "authors": ["Ahmed A. Elgohary", "Rohan Palanikumar", "Sameh A. Eisa"], "title": "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "In a recent effort, we successfully proposed a categorically novel approach\nto mimic the phenomenoa of hovering and source seeking by flapping insects and\nhummingbirds using a new extremum seeking control (ESC) approach. Said ESC\napproach was shown capable of characterizing the physics of hovering and source\nseeking by flapping systems, providing at the same time uniquely novel\nopportunity for a model-free, real-time biomimicry control design. In this\npaper, we experimentally test and verify, for the first time in the literature,\nthe potential of ESC in flapping robots to achieve model-free, real-time\ncontrolled hovering and source seeking. The results of this paper, while being\nrestricted to 1D, confirm the premise of introducing ESC as a natural control\nmethod and biomimicry mechanism to the field of flapping flight and robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6781\u503c\u641c\u7d22\u63a7\u5236\u65b9\u6cd5\uff0c\u6a21\u62df\u4e86\u6606\u866b\u548c\u8702\u9e1f\u7684\u60ac\u505c\u4e0e\u6e90\u641c\u5bfb\u884c\u4e3a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u4e0a\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u7684\u5b9e\u65f6\u63a7\u5236\u65b9\u6cd5\uff0c\u6a21\u4eff\u751f\u7269\u98de\u884c\u884c\u4e3a\uff0c\u63a8\u52a8\u6251\u7ffc\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6781\u503c\u641c\u7d22\u63a7\u5236(ESC)\u65b9\u6cd5\uff0c\u9996\u6b21\u5728\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5728\u4e00\u7ef4\u7a7a\u95f4\u4e2d\u8bc1\u5b9e\u4e86ESC\u4f5c\u4e3a\u6251\u7ffc\u98de\u884c\u81ea\u7136\u63a7\u5236\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "ESC\u4e3a\u6251\u7ffc\u98de\u884c\u548c\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4eff\u751f\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2508.20959", "pdf": "https://arxiv.org/pdf/2508.20959", "abs": "https://arxiv.org/abs/2508.20959", "authors": ["Curtis C. Johnson", "Daniel Webb", "David Hill", "Marc D. Killpack"], "title": "Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile Sensing", "categories": ["cs.RO", "eess.SP"], "comment": "In submission to IEEE Sensors", "summary": "Scaling tactile sensing for robust whole-body manipulation is a significant\nchallenge, often limited by wiring complexity, data throughput, and system\nreliability. This paper presents a complete architecture designed to overcome\nthese barriers. Our approach pairs open-source, fabric-based sensors with\ncustom readout electronics that reduce signal crosstalk to less than 3.3%\nthrough hardware-based mitigation. Critically, we introduce a novel,\ndaisy-chained SPI bus topology that avoids the practical limitations of common\nwireless protocols and the prohibitive wiring complexity of USB hub-based\nsystems. This architecture streams synchronized data from over 8,000 taxels\nacross 1 square meter of sensing area at update rates exceeding 50 FPS,\nconfirming its suitability for real-time control. We validate the system's\nefficacy in a whole-body grasping task where, without feedback, the robot's\nopen-loop trajectory results in an uncontrolled application of force that\nslowly crushes a deformable cardboard box. With real-time tactile feedback, the\nrobot transforms this motion into a gentle, stable grasp, successfully\nmanipulating the object without causing structural damage. This work provides a\nrobust and well-characterized platform to enable future research in advanced\nwhole-body control and physical human-robot interaction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u4f20\u611f\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6\u548c\u62d3\u6251\u4f18\u5316\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89e6\u89c9\u4f20\u611f\u5728\u5e03\u7ebf\u590d\u6742\u5ea6\u3001\u6570\u636e\u541e\u5410\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u65f6\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u89e6\u89c9\u4f20\u611f\u5728\u673a\u5668\u4eba\u5168\u8eab\u64cd\u4f5c\u4e2d\u7684\u5b9e\u73b0\u9762\u4e34\u5e03\u7ebf\u590d\u6742\u5ea6\u9ad8\u3001\u6570\u636e\u541e\u5410\u4e0d\u8db3\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u5dee\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ec7\u7269\u7684\u5f00\u6e90\u4f20\u611f\u5668\u548c\u5b9a\u5236\u8bfb\u53d6\u7535\u5b50\u8bbe\u5907\uff0c\u7ed3\u5408\u65b0\u578b\u83ca\u82b1\u94feSPI\u603b\u7ebf\u62d3\u6251\uff0c\u51cf\u5c11\u4fe1\u53f7\u4e32\u6270\u5e76\u4f18\u5316\u6570\u636e\u540c\u6b65\u4f20\u8f93\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u6bcf\u5e73\u65b9\u7c738,000\u4e2a\u89e6\u70b9\u7684\u6570\u636e\u540c\u6b65\u4f20\u8f93\uff0c\u66f4\u65b0\u901f\u7387\u8d85\u8fc750 FPS\uff0c\u5e76\u5728\u5b9e\u65f6\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u672a\u6765\u9ad8\u7ea7\u5168\u8eab\u63a7\u5236\u548c\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u5e73\u53f0\u3002"}}
{"id": "2508.20840", "pdf": "https://arxiv.org/pdf/2508.20840", "abs": "https://arxiv.org/abs/2508.20840", "authors": ["Qiao Sun", "Liujia Yang", "Wei Tang", "Wei Huang", "Kaixin Xu", "Yongchao Chen", "Mingyu Liu", "Jiange Yang", "Haoyi Zhu", "Yating Wang", "Tong He", "Yilun Chen", "Xili Dai", "Nanyang Ye", "Qinying Gu"], "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning", "categories": ["cs.RO", "cs.AI", "cs.MM"], "comment": null, "summary": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u59cb\u52a8\u4f5c\u7684\u5d4c\u5165\u4e16\u754c\u6a21\u578b\uff08PEWM\uff09\uff0c\u901a\u8fc7\u9650\u5236\u89c6\u9891\u751f\u6210\u7684\u65f6\u95f4\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u548c\u590d\u6742\u6027\u4e0a\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u7684\u5d4c\u5165\u4e16\u754c\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4ea4\u4e92\u6570\u636e\uff0c\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\u3001\u6536\u96c6\u56f0\u96be\u548c\u9ad8\u7ef4\u5ea6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u4e0e\u52a8\u4f5c\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u957f\u65f6\u89c6\u9891\u751f\u6210\u7684\u5b9e\u73b0\u3002", "method": "\u63d0\u51faPEWM\u65b9\u6cd5\uff0c\u9650\u5236\u89c6\u9891\u751f\u6210\u7684\u77ed\u65f6\u95f4\u8303\u56f4\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89c4\u5212\u548c\u8d77\u59cb-\u76ee\u6807\u70ed\u56fe\u5f15\u5bfc\uff08SGG\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u95ed\u73af\u63a7\u5236\u548c\u590d\u5408\u4efb\u52a1\u7684\u6cdb\u5316\u3002", "result": "PEWM\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3001\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u652f\u6301\u7ec6\u7c92\u5ea6\u8bed\u8a00-\u89c6\u89c9\u5bf9\u9f50\u548c\u590d\u5408\u4efb\u52a1\u7684\u7075\u6d3b\u63a7\u5236\u3002", "conclusion": "PEWM\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6a21\u578b\u7684\u65f6\u7a7a\u5148\u9a8c\u548cVLM\u7684\u8bed\u4e49\u611f\u77e5\uff0c\u5f25\u5408\u4e86\u7269\u7406\u4ea4\u4e92\u4e0e\u9ad8\u5c42\u63a8\u7406\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u901a\u7528\u5d4c\u5165\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.20871", "pdf": "https://arxiv.org/pdf/2508.20871", "abs": "https://arxiv.org/abs/2508.20871", "authors": ["Liding Zhang", "Kuanqi Cai", "Zhenshan Bing", "Chaoqun Wang", "Alois Knoll"], "title": "Genetic Informed Trees (GIT*): Path Planning via Reinforced Genetic Programming Heuristics", "categories": ["cs.RO"], "comment": null, "summary": "Optimal path planning involves finding a feasible state sequence between a\nstart and a goal that optimizes an objective. This process relies on heuristic\nfunctions to guide the search direction. While a robust function can improve\nsearch efficiency and solution quality, current methods often overlook\navailable environmental data and simplify the function structure due to the\ncomplexity of information relationships. This study introduces Genetic Informed\nTrees (GIT*), which improves upon Effort Informed Trees (EIT*) by integrating a\nwider array of environmental data, such as repulsive forces from obstacles and\nthe dynamic importance of vertices, to refine heuristic functions for better\nguidance. Furthermore, we integrated reinforced genetic programming (RGP),\nwhich combines genetic programming with reward system feedback to mutate\ngenotype-generative heuristic functions for GIT*. RGP leverages a multitude of\ndata types, thereby improving computational efficiency and solution quality\nwithin a set timeframe. Comparative analyses demonstrate that GIT* surpasses\nexisting single-query, sampling-based planners in problems ranging from R^4 to\nR^16 and was tested on a real-world mobile manipulation task. A video\nshowcasing our experimental results is available at\nhttps://youtu.be/URjXbc_BiYg", "AI": {"tldr": "GIT*\u901a\u8fc7\u6574\u5408\u66f4\u591a\u73af\u5883\u6570\u636e\u548c\u5f3a\u5316\u9057\u4f20\u7f16\u7a0b\u6765\u4f18\u5316\u542f\u53d1\u5f0f\u51fd\u6570\uff0c\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u4fe1\u606f\u5173\u7cfb\u590d\u6742\u6027\u800c\u7b80\u5316\u542f\u53d1\u5f0f\u51fd\u6570\uff0c\u5ffd\u7565\u4e86\u73af\u5883\u6570\u636e\u3002GIT*\u65e8\u5728\u901a\u8fc7\u66f4\u591a\u6570\u636e\u63d0\u5347\u89c4\u5212\u6548\u679c\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u9057\u4f20\u7f16\u7a0b\uff08RGP\uff09\uff0c\u5229\u7528\u969c\u788d\u6392\u65a5\u529b\u548c\u9876\u70b9\u52a8\u6001\u91cd\u8981\u6027\u7b49\u6570\u636e\u4f18\u5316\u542f\u53d1\u5f0f\u51fd\u6570\u3002", "result": "GIT*\u5728R^4\u5230R^16\u95ee\u9898\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "GIT*\u901a\u8fc7\u6570\u636e\u6574\u5408\u548cRGP\u663e\u8457\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2508.20884", "pdf": "https://arxiv.org/pdf/2508.20884", "abs": "https://arxiv.org/abs/2508.20884", "authors": ["Liding Zhang", "Qiyang Zong", "Yu Zhang", "Zhenshan Bing", "Alois Knoll"], "title": "Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning", "categories": ["cs.RO"], "comment": null, "summary": "Efficient motion planning algorithms are essential in robotics. Optimizing\nessential parameters, such as batch size and nearest neighbor selection in\nsampling-based methods, can enhance performance in the planning process.\nHowever, existing approaches often lack environmental adaptability. Inspired by\nthe method of the deep fuzzy neural networks, this work introduces\nLearning-based Informed Trees (LIT*), a sampling-based deep fuzzy\nlearning-based planner that dynamically adjusts batch size and nearest neighbor\nparameters to obstacle distributions in the configuration spaces. By encoding\nboth global and local ratios via valid and invalid states, LIT* differentiates\nbetween obstacle-sparse and obstacle-dense regions, leading to lower-cost paths\nand reduced computation time. Experimental results in high-dimensional spaces\ndemonstrate that LIT* achieves faster convergence and improved solution\nquality. It outperforms state-of-the-art single-query, sampling-based planners\nin environments ranging from R^8 to R^14 and is successfully validated on a\ndual-arm robot manipulation task. A video showcasing our experimental results\nis available at: https://youtu.be/NrNs9zebWWk", "AI": {"tldr": "LIT*\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u548c\u6df1\u5ea6\u6a21\u7cca\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6279\u6b21\u5927\u5c0f\u548c\u6700\u8fd1\u90bb\u53c2\u6570\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7f3a\u4e4f\u73af\u5883\u9002\u5e94\u6027\uff0c\u9700\u8981\u4f18\u5316\u6279\u6b21\u5927\u5c0f\u548c\u6700\u8fd1\u90bb\u9009\u62e9\u7b49\u53c2\u6570\u3002", "method": "\u63d0\u51faLIT*\uff0c\u5229\u7528\u6df1\u5ea6\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u53c2\u6570\uff0c\u533a\u5206\u969c\u788d\u7269\u7a00\u758f\u548c\u5bc6\u96c6\u533a\u57df\u3002", "result": "LIT*\u5728\u9ad8\u7ef4\u7a7a\u95f4\u5c55\u73b0\u66f4\u5feb\u6536\u655b\u548c\u66f4\u4f18\u8def\u5f84\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\u3002", "conclusion": "LIT*\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u8c03\u6574\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.20898", "pdf": "https://arxiv.org/pdf/2508.20898", "abs": "https://arxiv.org/abs/2508.20898", "authors": ["Jiaxi Huang", "Yan Huang", "Yixian Zhao", "Wenchao Meng", "Jinming Xu"], "title": "CoCoL: A Communication Efficient Decentralized Collaborative Method for Multi-Robot Systems", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": "Accepted by IROS2025", "summary": "Collaborative learning enhances the performance and adaptability of\nmulti-robot systems in complex tasks but faces significant challenges due to\nhigh communication overhead and data heterogeneity inherent in multi-robot\ntasks. To this end, we propose CoCoL, a Communication efficient decentralized\nCollaborative Learning method tailored for multi-robot systems with\nheterogeneous local datasets. Leveraging a mirror descent framework, CoCoL\nachieves remarkable communication efficiency with approximate Newton-type\nupdates by capturing the similarity between objective functions of robots, and\nreduces computational costs through inexact sub-problem solutions. Furthermore,\nthe integration of a gradient tracking scheme ensures its robustness against\ndata heterogeneity. Experimental results on three representative multi robot\ncollaborative learning tasks show the superiority of the proposed CoCoL in\nsignificantly reducing both the number of communication rounds and total\nbandwidth consumption while maintaining state-of-the-art accuracy. These\nbenefits are particularly evident in challenging scenarios involving non-IID\n(non-independent and identically distributed) data distribution, streaming\ndata, and time-varying network topologies.", "AI": {"tldr": "CoCoL\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u901a\u4fe1\u9ad8\u6548\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u534f\u4f5c\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9762\u4e34\u9ad8\u901a\u4fe1\u5f00\u9500\u548c\u6570\u636e\u5f02\u6784\u6027\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u955c\u50cf\u4e0b\u964d\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3c\u725b\u987f\u578b\u66f4\u65b0\u548c\u68af\u5ea6\u8ddf\u8e2a\u65b9\u6848\uff0c\u51cf\u5c11\u901a\u4fe1\u8f6e\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoCoL\u5728\u51cf\u5c11\u901a\u4fe1\u8f6e\u6570\u548c\u5e26\u5bbd\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u52a8\u6001\u7f51\u7edc\u62d3\u6251\u573a\u666f\u3002", "conclusion": "CoCoL\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u534f\u4f5c\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20899", "pdf": "https://arxiv.org/pdf/2508.20899", "abs": "https://arxiv.org/abs/2508.20899", "authors": ["Liding Zhang", "Zeqi Li", "Kuanqi Cai", "Qian Huang", "Zhenshan Bing", "Alois Knoll"], "title": "Language-Enhanced Mobile Manipulation for Efficient Object Search in Indoor Environments", "categories": ["cs.RO"], "comment": null, "summary": "Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u589e\u5f3a\u7684\u5c42\u6b21\u5bfc\u822a\u6846\u67b6GODHS\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u5c42\u6b21\u51b3\u7b56\u6765\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u641c\u7d22\u7269\u4f53\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u573a\u666f\u8868\u793a\u4ec5\u6355\u83b7\u9759\u6001\u8bed\u4e49\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u9650\u5236\u4e86\u5728\u964c\u751f\u73af\u5883\u4e2d\u6307\u5bfc\u7269\u4f53\u641c\u7d22\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faGODHS\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u573a\u666f\u8bed\u4e49\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u6b21\u51b3\u7b56\u548c\u542f\u53d1\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u4f18\u5316\u641c\u7d22\u8def\u5f84\u3002", "result": "\u5728Isaac Sim\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cGODHS\u6bd4\u4f20\u7edf\u975e\u8bed\u4e49\u641c\u7d22\u7b56\u7565\u66f4\u9ad8\u6548\u5730\u5b9a\u4f4d\u76ee\u6807\u7269\u4f53\u3002", "conclusion": "GODHS\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u548c\u5c42\u6b21\u5316\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u7269\u4f53\u641c\u7d22\u80fd\u529b\u3002"}}
{"id": "2508.20926", "pdf": "https://arxiv.org/pdf/2508.20926", "abs": "https://arxiv.org/abs/2508.20926", "authors": ["Gabriel Manuel Garcia", "Antoine Richard", "Miguel Olivares-Mendez"], "title": "PLUME: Procedural Layer Underground Modeling Engine", "categories": ["cs.RO"], "comment": null, "summary": "As space exploration advances, underground environments are becoming\nincreasingly attractive due to their potential to provide shelter, easier\naccess to resources, and enhanced scientific opportunities. Although such\nenvironments exist on Earth, they are often not easily accessible and do not\naccurately represent the diversity of underground environments found throughout\nthe solar system. This paper presents PLUME, a procedural generation framework\naimed at easily creating 3D underground environments. Its flexible structure\nallows for the continuous enhancement of various underground features, aligning\nwith our expanding understanding of the solar system. The environments\ngenerated using PLUME can be used for AI training, evaluating robotics\nalgorithms, 3D rendering, and facilitating rapid iteration on developed\nexploration algorithms. In this paper, it is demonstrated that PLUME has been\nused along with a robotic simulator. PLUME is open source and has been released\non Github. https://github.com/Gabryss/P.L.U.M.E", "AI": {"tldr": "PLUME\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u7a0b\u5e8f\u5316\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u521b\u5efa3D\u5730\u4e0b\u73af\u5883\uff0c\u652f\u6301AI\u8bad\u7ec3\u3001\u673a\u5668\u4eba\u7b97\u6cd5\u8bc4\u4f30\u7b49\u5e94\u7528\u3002", "motivation": "\u5730\u4e0b\u73af\u5883\u5728\u592a\u7a7a\u63a2\u7d22\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5730\u7403\u73af\u5883\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u592a\u9633\u7cfb\u591a\u6837\u6027\u3002", "method": "\u5f00\u53d1\u4e86PLUME\u6846\u67b6\uff0c\u91c7\u7528\u7075\u6d3b\u7ed3\u6784\u751f\u6210\u5730\u4e0b\u73af\u5883\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u4eba\u6a21\u62df\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "PLUME\u6210\u529f\u751f\u6210\u591a\u6837\u5730\u4e0b\u73af\u5883\uff0c\u53ef\u7528\u4e8e\u591a\u79cd\u6280\u672f\u5f00\u53d1\u548c\u6d4b\u8bd5\u3002", "conclusion": "PLUME\u4e3a\u592a\u7a7a\u63a2\u7d22\u6280\u672f\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.20981", "pdf": "https://arxiv.org/pdf/2508.20981", "abs": "https://arxiv.org/abs/2508.20981", "authors": ["Jiajie Li", "Boyang Sun", "Luca Di Giammarino", "Hermann Blum", "Marc Pollefeys"], "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Reliable localization is critical for robot navigation, yet most existing\nsystems implicitly assume that all viewing directions at a location are equally\ninformative. In practice, localization becomes unreliable when the robot\nobserves unmapped, ambiguous, or uninformative regions. To address this, we\npresent ActLoc, an active viewpoint-aware planning framework for enhancing\nlocalization accuracy for general robot navigation tasks. At its core, ActLoc\nemploys a largescale trained attention-based model for viewpoint selection. The\nmodel encodes a metric map and the camera poses used during map construction,\nand predicts localization accuracy across yaw and pitch directions at arbitrary\n3D locations. These per-point accuracy distributions are incorporated into a\npath planner, enabling the robot to actively select camera orientations that\nmaximize localization robustness while respecting task and motion constraints.\nActLoc achieves stateof-the-art results on single-viewpoint selection and\ngeneralizes effectively to fulltrajectory planning. Its modular design makes it\nreadily applicable to diverse robot navigation and inspection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aActLoc\u7684\u4e3b\u52a8\u89c6\u89d2\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u578b\u9009\u62e9\u89c6\u89d2\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u5b9a\u4f4d\u7cfb\u7edf\u5047\u8bbe\u6240\u6709\u89c6\u89d2\u540c\u7b49\u4fe1\u606f\u4e30\u5bcc\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\uff0c\u89c2\u5bdf\u672a\u6620\u5c04\u6216\u6a21\u7cca\u533a\u57df\u65f6\u5b9a\u4f4d\u4e0d\u53ef\u9760\u3002", "method": "ActLoc\u91c7\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u6a21\u578b\u9884\u6d4b3D\u4f4d\u7f6e\u4e0a\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8def\u5f84\u89c4\u5212\u5668\u4e2d\uff0c\u9009\u62e9\u6700\u4f18\u89c6\u89d2\u3002", "result": "ActLoc\u5728\u5355\u89c6\u89d2\u9009\u62e9\u548c\u5168\u8f68\u8ff9\u89c4\u5212\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "conclusion": "ActLoc\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5bfc\u822a\u548c\u68c0\u67e5\u4efb\u52a1\u3002"}}
{"id": "2508.20982", "pdf": "https://arxiv.org/pdf/2508.20982", "abs": "https://arxiv.org/abs/2508.20982", "authors": ["Junhao Gong", "Kit-Wa Sou", "Shoujie Li", "Changqing Guo", "Yan Huang", "Chuqiao Lyu", "Ziwu Song", "Wenbo Ding"], "title": "UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception", "categories": ["cs.RO"], "comment": "Accepted to IROS 2025", "summary": "Visuotactile sensors provide high-resolution tactile information but are\nincapable of perceiving the material features of objects. We present UltraTac,\nan integrated sensor that combines visuotactile imaging with ultrasound sensing\nthrough a coaxial optoacoustic architecture. The design shares structural\ncomponents and achieves consistent sensing regions for both modalities.\nAdditionally, we incorporate acoustic matching into the traditional\nvisuotactile sensor structure, enabling integration of the ultrasound sensing\nmodality without compromising visuotactile performance. Through tactile\nfeedback, we dynamically adjust the operating state of the ultrasound module to\nachieve flexible functional coordination. Systematic experiments demonstrate\nthree key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),\nmaterial classification (average accuracy: 99.20%), and texture-material\ndual-mode object recognition achieving 92.11% accuracy on a 15-class task.\nFinally, we integrate the sensor into a robotic manipulation system to\nconcurrently detect container surface patterns and internal content, which\nverifies its potential for advanced human-machine interaction and precise\nrobotic manipulation.", "AI": {"tldr": "UltraTac\u662f\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u89e6\u89c9\u6210\u50cf\u4e0e\u8d85\u58f0\u4f20\u611f\u7684\u96c6\u6210\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u540c\u8f74\u5149\u58f0\u67b6\u6784\u5b9e\u73b0\u9ad8\u5206\u8fa8\u89e6\u89c9\u4fe1\u606f\u4e0e\u6750\u6599\u7279\u5f81\u611f\u77e5\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u65e0\u6cd5\u611f\u77e5\u7269\u4f53\u7684\u6750\u6599\u7279\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u540c\u8f74\u5149\u58f0\u67b6\u6784\uff0c\u5171\u4eab\u7ed3\u6784\u7ec4\u4ef6\u548c\u4f20\u611f\u533a\u57df\uff0c\u5e76\u52a0\u5165\u58f0\u5b66\u5339\u914d\u6280\u672f\uff0c\u5b9e\u73b0\u89c6\u89c9\u89e6\u89c9\u4e0e\u8d85\u58f0\u4f20\u611f\u7684\u65e0\u635f\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4f20\u611f\u5668\u5177\u5907\u8fd1\u8ddd\u79bb\u4f20\u611f\uff083-8\u5398\u7c73\uff0cR\u00b2=0.90\uff09\u3001\u6750\u6599\u5206\u7c7b\uff08\u5e73\u5747\u51c6\u786e\u738799.20%\uff09\u548c\u7eb9\u7406-\u6750\u6599\u53cc\u6a21\u5f0f\u8bc6\u522b\uff0815\u7c7b\u4efb\u52a1\u51c6\u786e\u738792.11%\uff09\u3002", "conclusion": "UltraTac\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u7cbe\u786e\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.21007", "pdf": "https://arxiv.org/pdf/2508.21007", "abs": "https://arxiv.org/abs/2508.21007", "authors": ["Mateusz Jaszczuk", "Nadia Figueroa"], "title": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference", "categories": ["cs.RO"], "comment": "Accepted at 9th Annual Conference on Robot Learning. Project Website\n  - https://mateusz-jaszczuk.github.io/rme/", "summary": "With robots increasingly operating in human-centric environments, ensuring\nsoft and safe physical interactions, whether with humans, surroundings, or\nother machines, is essential. While compliant hardware can facilitate such\ninteractions, this work focuses on impedance controllers that allow\ntorque-controlled robots to safely and passively respond to contact while\naccurately executing tasks. From inverse dynamics to quadratic\nprogramming-based controllers, the effectiveness of these methods relies on\naccurate dynamics models of the robot and the object it manipulates. Any model\nmismatch results in task failures and unsafe behaviors. Thus, we introduce\nRapid Mismatch Estimation (RME), an adaptive, controller-agnostic,\nprobabilistic framework that estimates end-effector dynamics mismatches online,\nwithout relying on external force-torque sensors. From the robot's\nproprioceptive feedback, a Neural Network Model Mismatch Estimator generates a\nprior for a Variational Inference solver, which rapidly converges to the\nunknown parameters while quantifying uncertainty. With a real 7-DoF manipulator\ndriven by a state-of-the-art passive impedance controller, RME adapts to sudden\nchanges in mass and center of mass at the end-effector in $\\sim400$ ms, in\nstatic and dynamic settings. We demonstrate RME in a collaborative scenario\nwhere a human attaches an unknown basket to the robot's end-effector and\ndynamically adds/removes heavy items, showcasing fast and safe adaptation to\nchanging dynamics during physical interaction without any external sensory\nsystem.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Rapid Mismatch Estimation (RME)\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u4e0d\u4f9d\u8d56\u5916\u90e8\u529b\u4f20\u611f\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u4f30\u8ba1\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u52a8\u6001\u6a21\u578b\u4e0d\u5339\u914d\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u548c\u4ea4\u4e92\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u65e5\u76ca\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\uff0c\u786e\u4fdd\u8f6f\u6027\u548c\u5b89\u5168\u7684\u7269\u7406\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u6a21\u578b\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "method": "RME\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0d\u5339\u914d\u4f30\u8ba1\u5668\u548c\u53d8\u5206\u63a8\u65ad\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u672c\u4f53\u53cd\u9988\u5728\u7ebf\u4f30\u8ba1\u672a\u77e5\u53c2\u6570\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002", "result": "\u5728\u771f\u5b9e7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\uff0cRME\u5728\u7ea6400\u6beb\u79d2\u5185\u9002\u5e94\u672b\u7aef\u6267\u884c\u5668\u7684\u8d28\u91cf\u548c\u91cd\u5fc3\u53d8\u5316\uff0c\u5c55\u793a\u4e86\u5feb\u901f\u4e14\u5b89\u5168\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "RME\u5728\u534f\u4f5c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u52a8\u6001\u53d8\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u4f20\u611f\u7cfb\u7edf\u3002"}}
{"id": "2508.21043", "pdf": "https://arxiv.org/pdf/2508.21043", "abs": "https://arxiv.org/abs/2508.21043", "authors": ["Zhi Su", "Bike Zhang", "Nima Rahmanian", "Yuman Gao", "Qiayuan Liao", "Caitlin Regan", "Koushil Sreenath", "S. Shankar Sastry"], "title": "HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "Humanoid robots have recently achieved impressive progress in locomotion and\nwhole-body control, yet they remain constrained in tasks that demand rapid\ninteraction with dynamic environments through manipulation. Table tennis\nexemplifies such a challenge: with ball speeds exceeding 5 m/s, players must\nperceive, predict, and act within sub-second reaction times, requiring both\nagility and precision. To address this, we present a hierarchical framework for\nhumanoid table tennis that integrates a model-based planner for ball trajectory\nprediction and racket target planning with a reinforcement learning-based\nwhole-body controller. The planner determines striking position, velocity and\ntiming, while the controller generates coordinated arm and leg motions that\nmimic human strikes and maintain stability and agility across consecutive\nrallies. Moreover, to encourage natural movements, human motion references are\nincorporated during training. We validate our system on a general-purpose\nhumanoid robot, achieving up to 106 consecutive shots with a human opponent and\nsustained exchanges against another humanoid. These results demonstrate\nreal-world humanoid table tennis with sub-second reactive control, marking a\nstep toward agile and interactive humanoid behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5feb\u901f\u4ea4\u4e92\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u8eab\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u654f\u6377\u6027\u548c\u7cbe\u786e\u5ea6\u7684\u4e52\u4e53\u7403\u4efb\u52a1\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff08\u5982\u4e52\u4e53\u7403\uff09\u9700\u8981\u5feb\u901f\u611f\u77e5\u3001\u9884\u6d4b\u548c\u53cd\u5e94\uff0c\u4f46\u76ee\u524d\u6280\u672f\u4ecd\u53d7\u9650\u4e8e\u5b50\u79d2\u7ea7\u7684\u53cd\u5e94\u65f6\u95f4\u3002\u6587\u7ae0\u65e8\u5728\u901a\u8fc7\u7efc\u5408\u65b9\u6cd5\u63d0\u5347\u673a\u5668\u4eba\u7684\u654f\u6377\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u7403\u8f68\u8ff9\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u8eab\u63a7\u5236\u5668\uff0c\u5e76\u5f15\u5165\u4eba\u4f53\u8fd0\u52a8\u53c2\u8003\u4ee5\u751f\u6210\u81ea\u7136\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u6700\u591a106\u6b21\u8fde\u7eed\u51fb\u7403\uff0c\u5e76\u6210\u529f\u4e0e\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u5bf9\u624b\u8fdb\u884c\u6301\u7eed\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e52\u4e53\u7403\u4efb\u52a1\u4e2d\u7684\u5b50\u79d2\u7ea7\u53cd\u5e94\u63a7\u5236\uff0c\u4e3a\u654f\u6377\u548c\u4ea4\u4e92\u5f0f\u7684\u4eba\u5f62\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u6b65\u8fdb\u5c55\u3002"}}
{"id": "2508.21063", "pdf": "https://arxiv.org/pdf/2508.21063", "abs": "https://arxiv.org/abs/2508.21063", "authors": ["Ruixuan Liu", "Philip Huang", "Ava Pun", "Kangle Deng", "Shobhit Aggarwal", "Kevin Tang", "Michelle Liu", "Deva Ramanan", "Jun-Yan Zhu", "Jiaoyang Li", "Changliu Liu"], "title": "Prompt-to-Product: Generative Assembly via Bimanual Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 10 figures, 2 tables", "summary": "Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.", "AI": {"tldr": "\u901a\u8fc7Prompt-to-Product\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u53ef\u5b9e\u9645\u7ec4\u88c5\u7684\u4e50\u9ad8\u79ef\u6728\u4ea7\u54c1\u3002", "motivation": "\u964d\u4f4e\u8bbe\u8ba1\u7ec4\u88c5\u7684\u590d\u6742\u6027\u548c\u624b\u5de5\u5de5\u4f5c\u91cf\uff0c\u5c06\u7528\u6237\u521b\u610f\u5feb\u901f\u8f6c\u5316\u4e3a\u5b9e\u9645\u4ea7\u54c1\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u53ef\u7ec4\u88c5\u7684\u79ef\u6728\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u53cc\u624b\u673a\u5668\u4eba\u7cfb\u7edf\u5b8c\u6210\u6784\u9020\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u7ec4\u88c5\u4ea7\u54c1\u7684\u95e8\u69db\u548c\u5de5\u4f5c\u91cf\u3002", "conclusion": "Prompt-to-Product\u4e3a\u521b\u610f\u5230\u4ea7\u54c1\u7684\u8f6c\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21065", "pdf": "https://arxiv.org/pdf/2508.21065", "abs": "https://arxiv.org/abs/2508.21065", "authors": ["Jiahe Pan", "Jiaxu Xing", "Rudolf Reiter", "Yifan Zhai", "Elie Aljalbout", "Davide Scaramuzza"], "title": "Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation", "categories": ["cs.RO"], "comment": null, "summary": "Learning control policies in simulation enables rapid, safe, and\ncost-effective development of advanced robotic capabilities. However,\ntransferring these policies to the real world remains difficult due to the\nsim-to-real gap, where unmodeled dynamics and environmental disturbances can\ndegrade policy performance. Existing approaches, such as domain randomization\nand Real2Sim2Real pipelines, can improve policy robustness, but either struggle\nunder out-of-distribution conditions or require costly offline retraining. In\nthis work, we approach these problems from a different perspective. Instead of\nrelying on diverse training conditions before deployment, we focus on rapidly\nadapting the learned policy in the real world in an online fashion. To achieve\nthis, we propose a novel online adaptive learning framework that unifies\nresidual dynamics learning with real-time policy adaptation inside a\ndifferentiable simulation. Starting from a simple dynamics model, our framework\nrefines the model continuously with real-world data to capture unmodeled\neffects and disturbances such as payload changes and wind. The refined dynamics\nmodel is embedded in a differentiable simulation framework, enabling gradient\nbackpropagation through the dynamics and thus rapid, sample-efficient policy\nupdates beyond the reach of classical RL methods like PPO. All components of\nour system are designed for rapid adaptation, enabling the policy to adjust to\nunseen disturbances within 5 seconds of training. We validate the approach on\nagile quadrotor control under various disturbances in both simulation and the\nreal world. Our framework reduces hovering error by up to 81% compared to\nL1-MPC and 55% compared to DATT, while also demonstrating robustness in\nvision-based control without explicit state estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u672a\u5efa\u6a21\u52a8\u6001\u548c\u6270\u52a8\uff0c\u63d0\u5347\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8fc1\u79fb\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u5206\u5e03\u6761\u4ef6\u6216\u79bb\u7ebf\u91cd\u8bad\u7ec3\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u6b8b\u5dee\u52a8\u529b\u5b66\u5b66\u4e60\u548c\u5b9e\u65f6\u7b56\u7565\u8c03\u6574\uff0c\u5728\u53ef\u5fae\u5206\u4eff\u771f\u4e2d\u8fde\u7eed\u4f18\u5316\u6a21\u578b\u4ee5\u9002\u5e94\u672a\u5efa\u6a21\u6548\u5e94\u3002", "result": "\u57285\u79d2\u5185\u5b9e\u73b0\u7b56\u7565\u8c03\u6574\uff0c\u65e0\u4eba\u673a\u60ac\u505c\u8bef\u5dee\u964d\u4f4e81%\uff08\u76f8\u6bd4L1-MPC\uff09\u548c55%\uff08\u76f8\u6bd4DATT\uff09\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u7b56\u7565\u7684\u5feb\u901f\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2508.20203", "pdf": "https://arxiv.org/pdf/2508.20203", "abs": "https://arxiv.org/abs/2508.20203", "authors": ["Francesco Prignoli", "Francesco Borrelli", "Paolo Falcone", "Mark Pustilnik"], "title": "Regulation-Aware Game-Theoretic Motion Planning for Autonomous Racing", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "Accepted for presentation at the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025)", "summary": "This paper presents a regulation-aware motion planning framework for\nautonomous racing scenarios. Each agent solves a Regulation-Compliant Model\nPredictive Control problem, where racing rules - such as right-of-way and\ncollision avoidance responsibilities - are encoded using Mixed Logical\nDynamical constraints. We formalize the interaction between vehicles as a\nGeneralized Nash Equilibrium Problem (GNEP) and approximate its solution using\nan Iterative Best Response scheme. Building on this, we introduce the\nRegulation-Aware Game-Theoretic Planner (RA-GTP), in which the attacker reasons\nover the defender's regulation-constrained behavior. This game-theoretic layer\nenables the generation of overtaking strategies that are both safe and\nnon-conservative. Simulation results demonstrate that the RA-GTP outperforms\nbaseline methods that assume non-interacting or rule-agnostic opponent models,\nleading to more effective maneuvers while consistently maintaining compliance\nwith racing regulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u573a\u666f\u7684\u89c4\u5219\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u751f\u6210\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u8d85\u8f66\u7b56\u7565\u3002", "motivation": "\u8d5b\u8f66\u573a\u666f\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u9075\u5b88\u89c4\u5219\u5e76\u4e0e\u5176\u4ed6\u8f66\u8f86\u4e92\u52a8\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u89c4\u5219\u6216\u5bf9\u624b\u884c\u4e3a\uff0c\u5bfc\u81f4\u4fdd\u5b88\u6216\u4e0d\u5b89\u5168\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u903b\u8f91\u52a8\u6001\u7ea6\u675f\u7f16\u7801\u89c4\u5219\uff0c\u5c06\u8f66\u8f86\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94\u548c\u535a\u5f08\u8bba\u89c4\u5212\u5668RA-GTP\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cRA-GTP\u4f18\u4e8e\u5ffd\u7565\u5bf9\u624b\u6216\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u66f4\u6709\u6548\u4e14\u5408\u89c4\u7684\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u89c4\u5219\u7ea6\u675f\u4e0e\u535a\u5f08\u8bba\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u573a\u666f\u7684\u8fd0\u52a8\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2508.20762", "pdf": "https://arxiv.org/pdf/2508.20762", "abs": "https://arxiv.org/abs/2508.20762", "authors": ["Fachri Najm Noer Kartiman", "Rasim", "Yaya Wihardi", "Nurul Hasanah", "Oskar Natan", "Bambang Wahono", "Taufik Ibnu Salim"], "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "keywords-multitask learning, autonomous driving, end-to-end learning,\n  skip connections, swin transformer, self-attention mechanism. 12 pages", "summary": "Focusing on the development of an end-to-end autonomous vehicle model with\npixel-to-pixel context awareness, this research proposes the SKGE-Swin\narchitecture. This architecture utilizes the Swin Transformer with a skip-stage\nmechanism to broaden feature representation globally and at various network\nlevels. This approach enables the model to extract information from distant\npixels by leveraging the Swin Transformer's Shifted Window-based Multi-head\nSelf-Attention (SW-MSA) mechanism and to retain critical information from the\ninitial to the final stages of feature extraction, thereby enhancing its\ncapability to comprehend complex patterns in the vehicle's surroundings. The\nmodel is evaluated on the CARLA platform using adversarial scenarios to\nsimulate real-world conditions. Experimental results demonstrate that the\nSKGE-Swin architecture achieves a superior Driving Score compared to previous\nmethods. Furthermore, an ablation study will be conducted to evaluate the\ncontribution of each architectural component, including the influence of skip\nconnections and the use of the Swin Transformer, in improving model\nperformance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578bSKGE-Swin\uff0c\u5229\u7528Swin Transformer\u548c\u8df3\u8dc3\u7ea7\u673a\u5236\u589e\u5f3a\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728CARLA\u5e73\u53f0\u4e0a\u901a\u8fc7\u5bf9\u6297\u573a\u666f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u9a7e\u9a76\u8bc4\u5206\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4ece\u50cf\u7d20\u5c42\u9762\u7406\u89e3\u590d\u6742\u73af\u5883\u4fe1\u606f\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8f66\u8f86\u5bf9\u5468\u56f4\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528Swin Transformer\u7ed3\u5408\u8df3\u8dc3\u7ea7\u673a\u5236\uff0c\u5229\u7528SW-MSA\u673a\u5236\u63d0\u53d6\u8fdc\u8ddd\u79bb\u50cf\u7d20\u4fe1\u606f\uff0c\u5e76\u4fdd\u7559\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728CARLA\u5e73\u53f0\u4e0a\uff0cSKGE-Swin\u67b6\u6784\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a7e\u9a76\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "SKGE-Swin\u67b6\u6784\u901a\u8fc7\u7ed3\u5408Swin Transformer\u548c\u8df3\u8dc3\u7ea7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.20850", "pdf": "https://arxiv.org/pdf/2508.20850", "abs": "https://arxiv.org/abs/2508.20850", "authors": ["Tianyi Liu", "Hemma Philamore", "Benjamin Ward-Cherrier"], "title": "Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition", "categories": ["cs.NE", "cs.ET", "cs.RO"], "comment": null, "summary": "This study proposes a generalizable encoding strategy that maps tactile\nsensor data to electrical stimulation patterns, enabling neural organoids to\nperform an open-loop artificial tactile Braille classification task. Human\nforebrain organoids cultured on a low-density microelectrode array (MEA) are\nsystematically stimulated to characterize the relationship between electrical\nstimulation parameters (number of pulse, phase amplitude, phase duration, and\ntrigger delay) and organoid responses, measured as spike activity and spatial\ndisplacement of the center of activity. Implemented on event-based tactile\ninputs recorded from the Evetac sensor, our system achieved an average Braille\nletter classification accuracy of 61 percent with a single organoid, which\nincreased significantly to 83 percent when responses from a three-organoid\nensemble were combined. Additionally, the multi-organoid configuration\ndemonstrated enhanced robustness against various types of artificially\nintroduced noise. This research demonstrates the potential of organoids as\nlow-power, adaptive bio-hybrid computational elements and provides a\nfoundational encoding framework for future scalable bio-hybrid computing\narchitectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7f16\u7801\u7b56\u7565\uff0c\u5c06\u89e6\u89c9\u4f20\u611f\u5668\u6570\u636e\u6620\u5c04\u5230\u7535\u523a\u6fc0\u6a21\u5f0f\uff0c\u4f7f\u795e\u7ecf\u7c7b\u5668\u5b98\u80fd\u591f\u6267\u884c\u5f00\u73af\u4eba\u5de5\u89e6\u89c9\u76f2\u6587\u5206\u7c7b\u4efb\u52a1\u3002\u901a\u8fc7\u591a\u5668\u5b98\u914d\u7f6e\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6297\u566a\u6027\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7c7b\u5668\u5b98\u4f5c\u4e3a\u4f4e\u529f\u8017\u3001\u81ea\u9002\u5e94\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u5143\u4ef6\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u53ef\u6269\u5c55\u7684\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u57fa\u7840\u7f16\u7801\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u4f4e\u5bc6\u5ea6\u5fae\u7535\u6781\u9635\u5217\u57f9\u517b\u7684\u4eba\u524d\u8111\u7c7b\u5668\u5b98\uff0c\u901a\u8fc7\u7535\u523a\u6fc0\u53c2\u6570\uff08\u8109\u51b2\u6570\u3001\u76f8\u4f4d\u5e45\u5ea6\u3001\u76f8\u4f4d\u6301\u7eed\u65f6\u95f4\u548c\u89e6\u53d1\u5ef6\u8fdf\uff09\u548c\u7c7b\u5668\u5b98\u54cd\u5e94\uff08\u5cf0\u6d3b\u52a8\u4e0e\u6d3b\u52a8\u4e2d\u5fc3\u7684\u7a7a\u95f4\u4f4d\u79fb\uff09\u7684\u5173\u7cfb\u8fdb\u884c\u7cfb\u7edf\u8868\u5f81\u3002\u57fa\u4e8eEvetac\u4f20\u611f\u5668\u7684\u4e8b\u4ef6\u89e6\u89c9\u8f93\u5165\u5b9e\u73b0\u3002", "result": "\u5355\u4e2a\u7c7b\u5668\u5b98\u7684\u76f2\u6587\u5b57\u6bcd\u5206\u7c7b\u5e73\u5747\u51c6\u786e\u7387\u4e3a61\uff05\uff0c\u4e09\u5668\u5b98\u7ec4\u5408\u63d0\u5347\u81f383\uff05\u3002\u591a\u5668\u5b98\u914d\u7f6e\u589e\u5f3a\u4e86\u6297\u566a\u6027\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u7c7b\u5668\u5b98\u4f5c\u4e3a\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u5143\u4ef6\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u67b6\u6784\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2508.20892", "pdf": "https://arxiv.org/pdf/2508.20892", "abs": "https://arxiv.org/abs/2508.20892", "authors": ["Lo\u00efc Stratil", "Felix Fent", "Esteban Rivera", "Markus Lienkamp"], "title": "To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autonomous vehicle perception typically relies on modular pipelines that\ndecompose the task into detection, tracking, and prediction. While\ninterpretable, these pipelines suffer from error accumulation and limited\ninter-task synergy. Unified perception has emerged as a promising paradigm that\nintegrates these sub-tasks within a shared architecture, potentially improving\nrobustness, contextual reasoning, and efficiency while retaining interpretable\noutputs. In this survey, we provide a comprehensive overview of unified\nperception, introducing a holistic and systemic taxonomy that categorizes\nmethods along task integration, tracking formulation, and representation flow.\nWe define three paradigms -Early, Late, and Full Unified Perception- and\nsystematically review existing methods, their architectures, training\nstrategies, datasets used, and open-source availability, while highlighting\nfuture research directions. This work establishes the first comprehensive\nframework for understanding and advancing unified perception, consolidates\nfragmented efforts, and guides future research toward more robust,\ngeneralizable, and interpretable perception.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7684\u7edf\u4e00\u611f\u77e5\u8303\u5f0f\u7684\u7efc\u5408\u8c03\u67e5\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u65b9\u6cd5\u5e76\u5b9a\u4e49\u4e86\u4e09\u79cd\u8303\u793a\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u5757\u5316\u611f\u77e5\u7ba1\u9053\u7684\u9519\u8bef\u79ef\u7d2f\u548c\u5b50\u4efb\u52a1\u534f\u540c\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u6574\u5408\u3001\u8ddf\u8e2a\u516c\u5f0f\u5316\u548c\u8868\u793a\u6d41\u7684\u5206\u7c7b\u6cd5\uff0c\u5b9a\u4e49\u65e9\u671f\u3001\u665a\u671f\u548c\u5b8c\u5168\u7edf\u4e00\u611f\u77e5\u4e09\u79cd\u8303\u793a\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u6574\u7406\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u5f3a\u8c03\u4e86\u5f00\u6e90\u53ef\u7528\u6027\u3002", "conclusion": "\u7edf\u4e00\u611f\u77e5\u8303\u5f0f\u6709\u671b\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.20920", "pdf": "https://arxiv.org/pdf/2508.20920", "abs": "https://arxiv.org/abs/2508.20920", "authors": ["Enrico Martini", "Ho Jin Choi", "Nadia Figueroa", "Nicola Bombieri"], "title": "COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to Information Fusion", "summary": "In the era of Industry 5.0, monitoring human activity is essential for\nensuring both ergonomic safety and overall well-being. While multi-camera\ncentralized setups improve pose estimation accuracy, they often suffer from\nhigh computational costs and bandwidth requirements, limiting scalability and\nreal-time applicability. Distributing processing across edge devices can reduce\nnetwork bandwidth and computational load. On the other hand, the constrained\nresources of edge devices lead to accuracy degradation, and the distribution of\ncomputation leads to temporal and spatial inconsistencies. We address this\nchallenge by proposing COMETH (Convex Optimization for Multiview Estimation and\nTracking of Humans), a lightweight algorithm for real-time multi-view human\npose fusion that relies on three concepts: it integrates kinematic and\nbiomechanical constraints to increase the joint positioning accuracy; it\nemploys convex optimization-based inverse kinematics for spatial fusion; and it\nimplements a state observer to improve temporal consistency. We evaluate COMETH\non both public and industrial datasets, where it outperforms state-of-the-art\nmethods in localization, detection, and tracking accuracy. The proposed fusion\npipeline enables accurate and scalable human motion tracking, making it\nwell-suited for industrial and safety-critical applications. The code is\npublicly available at https://github.com/PARCO-LAB/COMETH.", "AI": {"tldr": "COMETH\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7b97\u6cd5\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u548c\u591a\u89c6\u89d2\u878d\u5408\u6280\u672f\uff0c\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\u548c\u8ba1\u7b97\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u5728\u5de5\u4e1a5.0\u65f6\u4ee3\uff0c\u591a\u6444\u50cf\u5934\u96c6\u4e2d\u5f0f\u8bbe\u7f6e\u867d\u7136\u63d0\u9ad8\u4e86\u59ff\u6001\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u4f46\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5e26\u5bbd\u9700\u6c42\u7684\u95ee\u9898\u3002\u5206\u5e03\u5f0f\u8fb9\u7f18\u8bbe\u5907\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u5374\u56e0\u8d44\u6e90\u53d7\u9650\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\u548c\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faCOMETH\u7b97\u6cd5\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u548c\u751f\u7269\u529b\u5b66\u7ea6\u675f\u3001\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u9006\u8fd0\u52a8\u5b66\u7a7a\u95f4\u878d\u5408\u6280\u672f\u4ee5\u53ca\u72b6\u6001\u89c2\u6d4b\u5668\uff0c\u4ee5\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728\u516c\u5f00\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cCOMETH\u5728\u5b9a\u4f4d\u3001\u68c0\u6d4b\u548c\u8ddf\u8e2a\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u7684\u4eba\u4f53\u8fd0\u52a8\u8ddf\u8e2a\u3002", "conclusion": "COMETH\u4e3a\u5de5\u4e1a\u548c\u5b89\u5168\u6027\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u4eba\u4f53\u59ff\u6001\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.21001", "pdf": "https://arxiv.org/pdf/2508.21001", "abs": "https://arxiv.org/abs/2508.21001", "authors": ["Yaniv Hassidof", "Tom Jurgenson", "Kiril Solovey"], "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Accepted to CoRL 2025. Project page:\n  https://sites.google.com/view/ditree", "summary": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a \\emph{provably-generalizable} framework leveraging diffusion\npolicies (DPs) as informed samplers to efficiently guide state-space search\nwithin SBPs. DiTree combines DP's ability to model complex distributions of\nexpert trajectories, conditioned on local observations, with the completeness\nof SBPs to yield \\emph{provably-safe} solutions within a few action propagation\niterations for complex dynamical systems. We demonstrate DiTree's power with an\nimplementation combining the popular RRT planner with a DP action sampler\ntrained on a \\emph{single environment}. In comprehensive evaluations on OOD\nscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than\nclassical SBPs), while improving the average success rate over DP and SBPs.\nDiTree is on average 3x faster than classical SBPs, and outperforms all other\napproaches by achieving roughly 30\\% higher success rate. Project webpage:\nhttps://sites.google.com/view/ditree.", "AI": {"tldr": "DiTree\u6846\u67b6\u7ed3\u5408\u6269\u6563\u7b56\u7565\uff08DP\uff09\u4e0e\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff08SBPs\uff09\uff0c\u663e\u8457\u63d0\u5347\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SBPs\uff08\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff09\u56e0\u65e0\u76ee\u6807\u52a8\u4f5c\u91c7\u6837\u5bfc\u81f4\u63a2\u7d22\u901f\u5ea6\u6162\uff0c\u4ee5\u53ca\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e2d\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDiTree\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u7b56\u7565\u4f5c\u4e3a\u667a\u80fd\u91c7\u6837\u5668\uff0c\u7ed3\u5408SBPs\u7684\u5b8c\u5907\u6027\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "DiTree\u5728OOD\u573a\u666f\u4e2d\u6bd4\u4f20\u7edfSBPs\u5feb3\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad830%\u3002", "conclusion": "DiTree\u901a\u8fc7\u7ed3\u5408DP\u548cSBPs\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21046", "pdf": "https://arxiv.org/pdf/2508.21046", "abs": "https://arxiv.org/abs/2508.21046", "authors": ["Wei Li", "Renshan Zhang", "Rui Shao", "Jie He", "Liqiang Nie"], "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification", "categories": ["cs.CV", "cs.RO"], "comment": "23 pages, 8 figures, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page", "summary": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.", "AI": {"tldr": "CogVLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u5bf9\u9f50\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u4ee4\u9a71\u52a8\u7684\u8def\u7531\u548c\u7a00\u758f\u5316\u65b9\u6cd5\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u56e0\u540e\u8bad\u7ec3\u9700\u6c42\u9ad8\u800c\u5bfc\u81f4\u7684\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u67b6\u6784\uff1aEFA-Routing\u9009\u62e9\u6027\u538b\u7f29\u89c6\u89c9\u6807\u8bb0\uff0cLFP-Routing\u526a\u679d\u65e0\u5173\u6807\u8bb0\uff0c\u4ee5\u53caCAtten\u7ed3\u5408\u6ce8\u610f\u673a\u5236\u652f\u6301\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5206\u522b\u8fbe\u523097.4%\u548c70.0%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u964d\u4f4e2.5\u500d\u8bad\u7ec3\u6210\u672c\u548c2.8\u500d\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "CogVLA\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
