{"id": "2508.19390", "pdf": "https://arxiv.org/pdf/2508.19390", "abs": "https://arxiv.org/abs/2508.19390", "authors": ["Jana Weber", "Marcel Weber", "Juan Miguel Lopez Alcaraz"], "title": "Depression diagnosis from patient interviews using multimodal machine learning", "categories": ["eess.SP"], "comment": "15 pages, 4 figures, source code under\n  https://github.com/UOLMDA2025/Depression", "summary": "Background: Depression is a major public health concern, affecting an\nestimated five percent of the global population. Early and accurate diagnosis\nis essential to initiate effective treatment, yet recognition remains\nchallenging in many clinical contexts. Speech, language, and behavioral cues\ncollected during patient interviews may provide objective markers that support\nclinical assessment.\n  Methods: We developed a diagnostic approach that integrates features derived\nfrom patient interviews, including speech patterns, linguistic characteristics,\nand structured clinical information. Separate models were trained for each\nmodality and subsequently combined through multimodal fusion to reflect the\ncomplexity of real-world psychiatric assessment. Model validity was assessed\nwith established performance metrics, and further evaluated using calibration\nand decision-analytic approaches to estimate potential clinical utility.\n  Results: The multimodal model achieved superior diagnostic accuracy compared\nto single-modality models, with an AUROC of 0.88 and an F1-score of 0.75.\nImportantly, the fused model demonstrated good calibration and offered higher\nnet clinical benefit compared to baseline strategies, highlighting its\npotential to assist clinicians in identifying patients with depression more\nreliably.\n  Conclusion: Multimodal analysis of patient interviews using machine learning\nmay serve as a valuable adjunct to psychiatric evaluation. By combining speech,\nlanguage, and clinical features, this approach provides a robust framework that\ncould enhance early detection of depressive disorders and support\nevidence-based decision-making in mental healthcare."}
{"id": "2508.19408", "pdf": "https://arxiv.org/pdf/2508.19408", "abs": "https://arxiv.org/abs/2508.19408", "authors": ["Vaclav Pavlicek", "Ayush Bhandari"], "title": "1-Bit Unlimited Sampling Beyond Fourier Domain: Low-Resolution Sampling of Quantization Noise", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "20 pages, accepted to IEEE Journal of Selected Topics in Signal\n  Processing", "summary": "Analog-to-digital converters (ADCs) play a critical role in digital signal\nacquisition across various applications, but their performance is inherently\nconstrained by sampling rates and bit budgets. This bit budget imposes a\ntrade-off between dynamic range (DR) and digital resolution, with ADC energy\nconsumption scaling linearly with sampling rate and exponentially with bit\ndepth. To bypass this, numerous approaches, including oversampling with\nlow-resolution ADCs, have been explored. A prominent example is 1-Bit ADCs with\nSigma-Delta Quantization (SDQ), a widely used consumer-grade solution. However,\nSDQs suffer from overloading or saturation issues, limiting their ability to\nhandle inputs with arbitrary DR. The Unlimited Sensing Framework (USF)\naddresses this challenge by injecting modulo non-linearity in hardware,\nresulting in a new digital sensing technology. In this paper, we introduce a\nnovel 1-Bit sampling architecture that extends both conventional 1-Bit SDQ and\nUSF. Our contributions are twofold: (1) We generalize the concept of noise\nshaping beyond the Fourier domain, allowing the inclusion of non-bandlimited\nsignals in the Fourier domain but bandlimited in alternative transform domains.\n(2) Building on this generalization, we develop a new transform-domain recovery\nmethod for 1-Bit USF. When applied to the Fourier domain, our method\ndemonstrates superior performance compared to existing time-domain techniques,\noffering reduced oversampling requirements and improved robustness. Extensive\nnumerical experiments validate our findings, laying the groundwork for a\nbroader generalization of 1-Bit sampling systems."}
{"id": "2508.19439", "pdf": "https://arxiv.org/pdf/2508.19439", "abs": "https://arxiv.org/abs/2508.19439", "authors": ["Jorge L. Gonzalez-Rios", "Eva Lagunas", "Hayder Al-Hraishawi", "Luis M. Garces-Socarras", "Symeon Chatzinotas"], "title": "In-Lab Carrier Aggregation Testbed for Satellite Communication Systems", "categories": ["eess.SP"], "comment": null, "summary": "Carrier Aggregation (CA) is a technique used in 5G and previous cellular\ngenerations to temporarily increase the data rate of a specific user during\npeak demand periods or to reduce carrier congestion. CA is achieved by\ncombining two or more carriers and providing a virtual, wider overall bandwidth\nto high-demand users of the system. CA was introduced in the 4G/LTE wireless\nera and has been proven effective in 5G as well, where it is said to play a\nsignificant role in efficient network capacity management. Given this success,\nthe satellite communication (SatCom) community has put its attention into CA\nand the potential benefits it can bring in terms of better spectrum utilization\nand better meeting the user traffic demand. While the theoretical evaluation of\nCA for SatCom has already been presented in several works, this article\npresents the design and results obtained with an experimentation testbed based\non Software Defined Radio (SDR) and a satellite channel emulator. We first\npresent the detailed implementation design, which includes a Gateway (GW)\nmodule responsible for PDU-scheduling across the aggregated carriers, and a\nUser Terminal (UT) module responsible for aggregating the multiple received\nstreams. The second part of the article presents the experimental evaluation,\nincluding CA over a single Geostationary (GEO) satellite, CA over a single\nMedium Earth Orbit (MEO) satellite, and CA combining carriers sent over GEO and\nMEO satellites. A key contribution of this work is the explicit consideration\nof multi-orbit scenarios in the testbed design and validation. The testing\nresults show promising benefits of CA over SatCom systems, motivating potential\nupcoming testing on over-the-air systems."}
{"id": "2508.19522", "pdf": "https://arxiv.org/pdf/2508.19522", "abs": "https://arxiv.org/abs/2508.19522", "authors": ["Si Wang", "Guoqiang Xiao"], "title": "Fourth-Order Hierarchical Array: A Novel Scheme for Sparse Array Design Based on Fourth-Order Difference Co-Array", "categories": ["eess.SP"], "comment": "Sparse linear array, fourth-order cumulant, mutual coupling,\n  redundancy, direction of arrival estimation", "summary": "Conventional array designs based on circular fourth-order cumulant typically\nadopt a single expression form of the fourth-order difference co-array (FODCA),\nwhich limits the achievable degrees of freedom (DOFs) and neglects the impact\nof mutual coupling among physical sensors. To address above issues, this paper\nproposes a novel scheme to design arrays with increased DOFs by combining\ndifferent forms of FODCA while accounting for mutual coupling. A novel\nfourth-order hierarchical array (FOHA) based on different forms of FODCA is\nconstructed using an arbitrary generator set. The analytical expression between\nthe coupling leakage of the generator and the resulting FOHA is derived. Two\nspecific FOHA configurations are presented with closed-form sensor placements.\nThe arrays not only offer increased DOFs for resolving more sources in\ndirection of-arrival (DOA) estimation but also effectively suppress mutual\ncoupling. Moreover, the redundancy of FODCA is examined, and it is shown that\narrays based on the proposed scheme achieve lower redundancy compared to\nexisting arrays based on FODCA. Meanwhile, the necessary and sufficient\nconditions for signal reconstruction by FOHA are derived. Compared with\nexisting arrays based on FODCA, the proposed arrays provide enhanced DOFs and\nimproved robustness against mutual coupling. Numerical simulations verify that\nFOHAs achieve superior DOA estimation performance compared with other sparse\nlinear arrays."}
{"id": "2508.19367", "pdf": "https://arxiv.org/pdf/2508.19367", "abs": "https://arxiv.org/abs/2508.19367", "authors": ["Alex Cuellar", "Ho Chit Siu", "Julie A Shah"], "title": "Inference of Human-derived Specifications of Object Placement via Demonstration", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,\nobject packing, sorting, and kitting), methods focused on understanding\nhuman-acceptable object configurations remain limited expressively with regard\nto capturing spatial relationships important to humans. To advance robotic\nunderstanding of human rules for object arrangement, we introduce\npositionally-augmented RCC (PARCC), a formal logic framework based on region\nconnection calculus (RCC) for describing the relative position of objects in\nspace. Additionally, we introduce an inference algorithm for learning PARCC\nspecifications via demonstrations. Finally, we present the results from a human\nstudy, which demonstrate our framework's ability to capture a human's intended\nspecification and the benefits of learning from demonstration approaches over\nhuman-provided specifications."}
{"id": "2508.19540", "pdf": "https://arxiv.org/pdf/2508.19540", "abs": "https://arxiv.org/abs/2508.19540", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Yuanwei Liu"], "title": "Pinching Antenna System for Integrated Sensing and Communications", "categories": ["eess.SP"], "comment": "13 pages, 8 figures", "summary": "Recently, the pinching antenna system (PASS) has attracted considerable\nattention due to their advantages in flexible deployment and reduction of\nsignal propagation loss. In this work, a multiple waveguide PASS assisted\nintegrated sensing and communication (ISAC) system is proposed, where the base\nstation (BS) is equipped with transmitting pinching antennas (PAs) and\nreceiving uniform linear array (ULA) antennas. The full-duplex (FD) BS\ntransmits the communication and sensing signals through the PAs on waveguides\nand collects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraint, and PA deployment constraints. The alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. In each iteration, the overall optimization problem is decomposed into\na digital beamforming sub-problem and a pinching beamforming sub-problem. The\nsensing covariance matrix and communication beamforming matrix at the BS are\noptimized by solving the digital beamforming sub-problem with semidefinite\nrelaxation (SDR). The PA deployment is updated by solving the pinching\nbeamforming sub-problem with the successive convex approximation (SCA) method,\npenalty method, and element-wise optimization. Simulation results show that the\nproposed PASS assisted ISAC framework achieves superior performance over\nbenchmark schemes, is less affected by stringent communication constraints\ncompared to conventional MIMO-ISAC, and benefits further from increasing the\nnumber of waveguides and PAs per waveguide."}
{"id": "2508.19380", "pdf": "https://arxiv.org/pdf/2508.19380", "abs": "https://arxiv.org/abs/2508.19380", "authors": ["Diancheng Li", "Nia Ralston", "Bastiaan Hagen", "Phoebe Tan", "Matthew A. Robertson"], "title": "FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain", "categories": ["cs.RO"], "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "This paper introduces FlipWalker, a novel underactuated robot locomotion\nsystem inspired by Jacob's Ladder illusion toy, designed to traverse\nchallenging terrains where wheeled robots often struggle. Like the Jacob's\nLadder toy, FlipWalker features two interconnected segments joined by flexible\ncables, enabling it to pivot and flip around singularities in a manner\nreminiscent of the toy's cascading motion. Actuation is provided by\nmotor-driven legs within each segment that push off either the ground or the\nopposing segment, depending on the robot's current configuration. A\nphysics-based model of the underactuated flipping dynamics is formulated to\nelucidate the critical design parameters governing forward motion and obstacle\nclearance or climbing. The untethered prototype weighs 0.78 kg, achieves a\nmaximum flipping speed of 0.2 body lengths per second. Experimental trials on\nartificial grass, river rocks, and snow demonstrate that FlipWalker's flipping\nstrategy, which relies on ground reaction forces applied normal to the surface,\noffers a promising alternative to traditional locomotion for navigating\nirregular outdoor terrain."}
{"id": "2508.19552", "pdf": "https://arxiv.org/pdf/2508.19552", "abs": "https://arxiv.org/abs/2508.19552", "authors": ["Shuo Chang", "Rui Sun", "Jiashuo He", "Sai Huang", "Kan Yu", "Zhiyong Feng"], "title": "CSRD2025: A Large-Scale Synthetic Radio Dataset for Spectrum Sensing in Wireless Communications", "categories": ["eess.SP"], "comment": null, "summary": "The development of Large AI Models (LAMs) for wireless communications,\nparticularly for complex tasks like spectrum sensing, is critically dependent\non the availability of vast, diverse, and realistic datasets. Addressing this\nneed, this paper introduces the ChangShuoRadioData (CSRD) framework, an\nopen-source, modular simulation platform designed for generating large-scale\nsynthetic radio frequency (RF) data. CSRD simulates the end-to-end transmission\nand reception process, incorporating an extensive range of modulation schemes\n(100 types, including analog, digital, OFDM, and OTFS), configurable channel\nmodels featuring both statistical fading and site-specific ray tracing using\nOpenStreetMap data, and detailed modeling of realistic RF front-end impairments\nfor various antenna configurations (SISO/MISO/MIMO). Using this framework, we\ncharacterize CSRD2025, a substantial dataset benchmark comprising over\n25,000,000 frames (approx. 200TB), which is approximately 10,000 times larger\nthan the widely used RML2018 dataset. CSRD2025 offers unprecedented signal\ndiversity and complexity, specifically engineered to bridge the Sim2Real gap.\nFurthermore, we provide processing pipelines to convert IQ data into\nspectrograms annotated in COCO format, facilitating object detection approaches\nfor time-frequency signal analysis. The dataset specification includes\nstandardized 8:1:1 training, validation, and test splits (via frame indices) to\nensure reproducible research. The CSRD framework is released at\nhttps://github.com/Singingkettle/ChangShuoRadioData to accelerate the\nadvancement of AI-driven spectrum sensing and management."}
{"id": "2508.19391", "pdf": "https://arxiv.org/pdf/2508.19391", "abs": "https://arxiv.org/abs/2508.19391", "authors": ["Chaoran Zhu", "Hengyi Wang", "Yik Lung Pang", "Changjae Oh"], "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Visual-textual understanding is essential for language-guided robot\nmanipulation. Recent works leverage pre-trained vision-language models to\nmeasure the similarity between encoded visual observations and textual\ninstructions, and then train a model to map this similarity to robot actions.\nHowever, this two-step approach limits the model to capture the relationship\nbetween visual observations and textual instructions, leading to reduced\nprecision in manipulation tasks. We propose to learn visual-textual\nassociations through a self-supervised pretext task: reconstructing a masked\ngoal image conditioned on an input image and textual instructions. This\nformulation allows the model to learn visual-action representations without\nrobot action supervision. The learned representations can then be fine-tuned\nfor manipulation tasks with only a few demonstrations. We also introduce the\n\\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot\ntabletop manipulation episodes, including 180 object classes and 3,200\ninstances with corresponding textual instructions. This dataset enables the\nmodel to acquire diverse object priors and allows for a more comprehensive\nevaluation of its generalisation capability across object instances.\nExperimental results on the five benchmarks, including both simulated and\nreal-robot validations, demonstrate that our method outperforms prior art."}
{"id": "2508.19566", "pdf": "https://arxiv.org/pdf/2508.19566", "abs": "https://arxiv.org/abs/2508.19566", "authors": ["Chen Shang", "Jiadong Yu", "Dinh Thai Hoang"], "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks", "categories": ["eess.SP", "cs.AI"], "comment": "6 pages, 4 figures, conference paper", "summary": "This work proposes an energy-efficient, learning-based beamforming scheme for\nintegrated sensing and communication (ISAC)-enabled V2X networks. Specifically,\nwe first model the dynamic and uncertain nature of V2X environments as a Markov\nDecision Process. This formulation allows the roadside unit to generate\nbeamforming decisions based solely on current sensing information, thereby\neliminating the need for frequent pilot transmissions and extensive channel\nstate information acquisition. We then develop a deep reinforcement learning\n(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring\nboth communication throughput and sensing accuracy in highly dynamic scenario.\nTo address the high energy demands of conventional learning-based schemes, we\nembed spiking neural networks (SNNs) into the DRL framework. Leveraging their\nevent-driven and sparsely activated architecture, SNNs significantly enhance\nenergy efficiency while maintaining robust performance. Simulation results\nconfirm that the proposed method achieves substantial energy savings and\nsuperior communication performance, demonstrating its potential to support\ngreen and sustainable connectivity in future V2X systems."}
{"id": "2508.19425", "pdf": "https://arxiv.org/pdf/2508.19425", "abs": "https://arxiv.org/abs/2508.19425", "authors": ["John M. Scanlon", "Timothy L McMurry", "Yin-Hsiu Chen", "Kristofer D. Kusano", "Trent Victor"], "title": "From Stoplights to On-Ramps: A Comprehensive Set of Crash Rate Benchmarks for Freeway and Surface Street ADS Evaluation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents crash rate benchmarks for evaluating US-based Automated\nDriving Systems (ADS) for multiple urban areas. The purpose of this study was\nto extend prior benchmarks focused only on surface streets to additionally\ncapture freeway crash risk for future ADS safety performance assessments. Using\npublicly available police-reported crash and vehicle miles traveled (VMT) data,\nthe methodology details the isolation of in-transport passenger vehicles, road\ntype classification, and crash typology. Key findings revealed that freeway\ncrash rates exhibit large geographic dependence variations with\nany-injury-reported crash rates being nearly 3.5 times higher in Atlanta (2.4\nIPMM; the highest) when compared to Phoenix (0.7 IPMM; the lowest). The results\nshow the critical need for location-specific benchmarks to avoid biased safety\nevaluations and provide insights into the vehicle miles traveled (VMT) required\nto achieve statistical significance for various safety impact levels. The\ndistribution of crash types depended on the outcome severity level. Higher\nseverity outcomes (e.g., fatal crashes) had a larger proportion of\nsingle-vehicle, vulnerable road users (VRU), and opposite-direction collisions\ncompared to lower severity (police-reported) crashes. Given heterogeneity in\ncrash types by severity, performance in low-severity scenarios may not be\npredictive of high-severity outcomes. These benchmarks are additionally used to\nquantify at the required mileage to show statistically significant deviations\nfrom human performance. This is the first paper to generate freeway-specific\nbenchmarks for ADS evaluation and provides a foundational framework for future\nADS benchmarking by evaluators and developers."}
{"id": "2508.19631", "pdf": "https://arxiv.org/pdf/2508.19631", "abs": "https://arxiv.org/abs/2508.19631", "authors": ["Yubeen Jo", "Geon Choi", "Yongjune Kim", "Namyoon Lee"], "title": "Code-Weight Sphere Decoding", "categories": ["eess.SP"], "comment": "5 pages, 6 figures", "summary": "Ultra-reliable low-latency communications (URLLC) demand high-performance\nerror-correcting codes and decoders in the finite blocklength regime. This\nletter introduces a novel two-stage near-maximum likelihood (near-ML) decoding\nframework applicable to any linear block code. Our approach first employs a\nlow-complexity initial decoder. If this initial stage fails a cyclic redundancy\ncheck, it triggers a second stage: the proposed code-weight sphere decoding\n(WSD). WSD iteratively refines the codeword estimate by exploring a localized\nsphere of candidates constructed from pre-computed low-weight codewords. This\nstrategy adaptively minimizes computational overhead at high signal-to-noise\nratios while achieving near-ML performance, especially for low-rate codes.\nExtensive simulations demonstrate that our two-stage decoder provides an\nexcellent trade-off between decoding reliability and complexity, establishing\nit as a promising solution for next-generation URLLC systems."}
{"id": "2508.19429", "pdf": "https://arxiv.org/pdf/2508.19429", "abs": "https://arxiv.org/abs/2508.19429", "authors": ["Gustavo A. Cardona", "Kaier Liang", "Cristian-Ioan Vasile"], "title": "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals", "categories": ["cs.RO", "cs.FL"], "comment": null, "summary": "This paper presents an iterative approach for heterogeneous multi-agent route\nplanning in environments with unknown resource distributions. We focus on a\nteam of robots with diverse capabilities tasked with executing missions\nspecified using Capability Temporal Logic (CaTL), a formal framework built on\nSignal Temporal Logic to handle spatial, temporal, capability, and resource\nconstraints. The key challenge arises from the uncertainty in the initial\ndistribution and quantity of resources in the environment. To address this, we\nintroduce an iterative algorithm that dynamically balances exploration and task\nfulfillment. Robots are guided to explore the environment, identifying resource\nlocations and quantities while progressively refining their understanding of\nthe resource landscape. At the same time, they aim to maximally satisfy the\nmission objectives based on the current information, adapting their strategies\nas new data is uncovered. This approach provides a robust solution for planning\nin dynamic, resource-constrained environments, enabling efficient coordination\nof heterogeneous teams even under conditions of uncertainty. Our method's\neffectiveness and performance are demonstrated through simulated case studies."}
{"id": "2508.19637", "pdf": "https://arxiv.org/pdf/2508.19637", "abs": "https://arxiv.org/abs/2508.19637", "authors": ["Maha Shatta", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Georgios Panagopoulos", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge", "categories": ["eess.SP", "cs.AI"], "comment": "Accepted at 2025 International Conference on Computer-Aided Design\n  (ICCAD)", "summary": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring."}
{"id": "2508.19476", "pdf": "https://arxiv.org/pdf/2508.19476", "abs": "https://arxiv.org/abs/2508.19476", "authors": ["Dane Brouwer", "Joshua Citron", "Heather Nolte", "Jeannette Bohg", "Mark Cutkosky"], "title": "Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning", "categories": ["cs.RO"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Dense collections of movable objects are common in everyday spaces -- from\ncabinets in a home to shelves in a warehouse. Safely retracting objects from\nsuch collections is difficult for robots, yet people do it easily, using\nnon-prehensile tactile sensing on the sides and backs of their hands and arms.\nWe investigate the role of such sensing for training robots to gently reach\ninto constrained clutter and extract objects. The available sensing modalities\nare (1) \"eye-in-hand\" vision, (2) proprioception, (3) non-prehensile triaxial\ntactile sensing, (4) contact wrenches estimated from joint torques, and (5) a\nmeasure of successful object acquisition obtained by monitoring the vacuum line\nof a suction cup. We use imitation learning to train policies from a set of\ndemonstrations on randomly generated scenes, then conduct an ablation study of\nwrench and tactile information. We evaluate each policy's performance across 40\nunseen environment configurations. Policies employing any force sensing show\nfewer excessive force failures, an increased overall success rate, and faster\ncompletion times. The best performance is achieved using both tactile and\nwrench information, producing an 80% improvement above the baseline without\nforce information."}
{"id": "2508.19657", "pdf": "https://arxiv.org/pdf/2508.19657", "abs": "https://arxiv.org/abs/2508.19657", "authors": ["Jorge L. González-Rios", "Liz Martínez Marrero", "Juan Duncan", "Luis M. Garcés-Socarrás", "Raudel Cuiman Marquez", "Juan A. Vásquez Peralvo", "Jevgenij Krivochiza", "Symeon Chatzinotas", "Björn Ottersten"], "title": "Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites", "categories": ["eess.SP", "cs.AR"], "comment": null, "summary": "The use of communication satellites in medium Earth orbit (MEO) is foreseen\nto provide quasi-global broadband Internet connectivity in the coming\nnetworking ecosystems. Multi-user multiple-input single-output (MU-MISO)\ndigital signal processing techniques, such as precoding, emerge as appealing\ntechnological enablers in the forward link of multi-beam satellite systems\noperating in full frequency reuse (FFR). However, the orbit dynamics of MEO\nsatellites pose additional challenges that must be carefully evaluated and\naddressed. This work presents the design of an in-lab testbed based on\nsoftware-defined radio (SDR) platforms and the corresponding adaptations\nrequired for efficient precoding in a MEO scenario. The setup incorporates a\nprecise orbit model and the radiation pattern of a custom-designed direct\nradiating array (DRA). We analyze the main impairments affecting precoding\nperformance, including Doppler shifts and payload phase noise, and propose a\nsynchronization loop to mitigate these effects. Preliminary experimental\nresults validate the feasibility and effectiveness of the proposed solution."}
{"id": "2508.19508", "pdf": "https://arxiv.org/pdf/2508.19508", "abs": "https://arxiv.org/abs/2508.19508", "authors": ["Tian Qiu", "Alan Zoubi", "Yiyuan Lin", "Ruiming Du", "Lailiang Cheng", "Yu Jiang"], "title": "DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Digital twin applications offered transformative potential by enabling\nreal-time monitoring and robotic simulation through accurate virtual replicas\nof physical assets. The key to these systems is 3D reconstruction with high\ngeometrical fidelity. However, existing methods struggled under field\nconditions, especially with sparse and occluded views. This study developed a\ntwo-stage framework (DATR) for the reconstruction of apple trees from sparse\nviews. The first stage leverages onboard sensors and foundation models to\nsemi-automatically generate tree masks from complex field images. Tree masks\nare used to filter out background information in multi-modal data for the\nsingle-image-to-3D reconstruction at the second stage. This stage consists of a\ndiffusion model and a large reconstruction model for respective multi view and\nimplicit neural field generation. The training of the diffusion model and LRM\nwas achieved by using realistic synthetic apple trees generated by a Real2Sim\ndata generator. The framework was evaluated on both field and synthetic\ndatasets. The field dataset includes six apple trees with field-measured ground\ntruth, while the synthetic dataset featured structurally diverse trees.\nEvaluation results showed that our DATR framework outperformed existing 3D\nreconstruction methods across both datasets and achieved domain-trait\nestimation comparable to industrial-grade stationary laser scanners while\nimproving the throughput by $\\sim$360 times, demonstrating strong potential for\nscalable agricultural digital twin systems."}
{"id": "2508.19660", "pdf": "https://arxiv.org/pdf/2508.19660", "abs": "https://arxiv.org/abs/2508.19660", "authors": ["Vojtech Mrazek", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Zdenek Vasicek", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation", "categories": ["eess.SP", "cs.AI", "cs.NE"], "comment": "Accepted at IEEE Transactions on Circuits and Systems for Artificial\n  Intelligence", "summary": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs."}
{"id": "2508.19595", "pdf": "https://arxiv.org/pdf/2508.19595", "abs": "https://arxiv.org/abs/2508.19595", "authors": ["Maryam Kazemi Eskeri", "Thomas Wiedemann", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "A Lightweight Crowd Model for Robot Social Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 6 figures, accepted in ECMR 2025", "summary": "Robots operating in human-populated environments must navigate safely and\nefficiently while minimizing social disruption. Achieving this requires\nestimating crowd movement to avoid congested areas in real-time. Traditional\nmicroscopic models struggle to scale in dense crowds due to high computational\ncost, while existing macroscopic crowd prediction models tend to be either\noverly simplistic or computationally intensive. In this work, we propose a\nlightweight, real-time macroscopic crowd prediction model tailored for human\nmotion, which balances prediction accuracy and computational efficiency. Our\napproach simplifies both spatial and temporal processing based on the inherent\ncharacteristics of pedestrian flow, enabling robust generalization without the\noverhead of complex architectures. We demonstrate a 3.6 times reduction in\ninference time, while improving prediction accuracy by 3.1 %. Integrated into a\nsocially aware planning framework, the model enables efficient and socially\ncompliant robot navigation in dynamic environments. This work highlights that\nefficient human crowd modeling enables robots to navigate dense environments\nwithout costly computations."}
{"id": "2508.19739", "pdf": "https://arxiv.org/pdf/2508.19739", "abs": "https://arxiv.org/abs/2508.19739", "authors": ["Sebastian Lotter", "Marco Seiter", "Maryam Pirmoradi", "Lukas Brand", "Dagmar Fischer", "Robert Schober"], "title": "MC for Gastroretentive Drug Delivery", "categories": ["eess.SP", "cs.ET"], "comment": "4 pages, 2 figures, This paper has been submitted to IEEE\n  Transactions on Molecular, Biological, and Multi-Scale Communications as\n  Transactions Letter", "summary": "Recently, bacterial nanocellulose (BNC), a biological material produced by\nnon-pathogenic bacteria that possesses excellent material properties for\nvarious medical applications, has received increased interest as a carrier\nsystem for drug delivery. However, the vast majority of existing studies on\ndrug release from BNC are feasibility studies with modeling and design aspects\nremaining largely unexplored. To narrow this research gap, this paper proposes\na novel model for the drug release from BNC. Specifically, the drug delivery\nsystem considered in this paper consists of a BNC fleece coated with a polymer.\nThe polymer coating is used as an additional diffusion barrier, enabling the\ncontrolled release of an active pharmaceutical ingredient. The proposed\nphysics-based model reflects the geometry of the BNC and incorporates the\nimpact of the polymer coating on the drug release. Hence, it can be useful for\ndesigning BNC-based drug delivery systems in the future. The accuracy of the\nmodel is validated with experimental data obtained in wet lab experiments."}
{"id": "2508.19607", "pdf": "https://arxiv.org/pdf/2508.19607", "abs": "https://arxiv.org/abs/2508.19607", "authors": ["Amin Berjaoui Tahmaz", "Ravi Prakash", "Jens Kober"], "title": "Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks", "categories": ["cs.RO"], "comment": "This article is accepted for publication in IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "This paper presents an Impedance Primitive-augmented hierarchical\nreinforcement learning framework for efficient robotic manipulation in\nsequential contact tasks. We leverage this hierarchical structure to\nsequentially execute behavior primitives with variable stiffness control\ncapabilities for contact tasks. Our proposed approach relies on three key\ncomponents: an action space enabling variable stiffness control, an adaptive\nstiffness controller for dynamic stiffness adjustments during primitive\nexecution, and affordance coupling for efficient exploration while encouraging\ncompliance. Through comprehensive training and evaluation, our framework learns\nefficient stiffness control capabilities and demonstrates improvements in\nlearning efficiency, compositionality in primitive selection, and success rates\ncompared to the state-of-the-art. The training environments include block\nlifting, door opening, object pushing, and surface cleaning. Real world\nevaluations further confirm the framework's sim2real capability. This work lays\nthe foundation for more adaptive and versatile robotic manipulation systems,\nwith potential applications in more complex contact-based tasks."}
{"id": "2508.19822", "pdf": "https://arxiv.org/pdf/2508.19822", "abs": "https://arxiv.org/abs/2508.19822", "authors": ["Chunxuan Shi", "Yongzhe Li", "Ran Tao"], "title": "On Minimization/Maximization of the Generalized Multi-Order Complex Quadratic Form With Constant-Modulus Constraints", "categories": ["eess.SP"], "comment": "14 pages, 3 figures (16 subfigures)", "summary": "In this paper, we study the generalized problem that minimizes or maximizes a\nmulti-order complex quadratic form with constant-modulus constraints on all\nelements of its optimization variable. Such a mathematical problem is commonly\nencountered in various applications of signal processing. We term it as the\nconstant-modulus multi-order complex quadratic programming (CMCQP) in this\npaper. In general, the CMCQP is non-convex and difficult to solve. Its\nobjective function typically relates to metrics such as signal-to-noise ratio,\nCram\\'er-Rao bound, integrated sidelobe level, etc., and constraints normally\ncorrespond to requirements on similarity to desired aspects,\npeak-to-average-power ratio, or constant-modulus property in practical\nscenarios. In order to find efficient solutions to the CMCQP, we first\nreformulate it into an unconstrained optimization problem with respect to phase\nvalues of the studied variable only. Then, we devise a steepest descent/ascent\nmethod with fast determinations on its optimal step sizes. Specifically, we\nconvert the step-size searching problem into a polynomial form that leads to\nclosed-form solutions of high accuracy, wherein the third-order Taylor\nexpansion of the search function is conducted. Our major contributions also lie\nin investigating the effect of the order and specific form of matrices embedded\nin the CMCQP, for which two representative cases are identified. Examples of\nrelated applications associated with the two cases are also provided for\ncompleteness. The proposed methods are summarized into algorithms, whose\nconvergence speeds are verified to be fast by comprehensive simulations and\ncomparisons to existing methods. The accuracy of our proposed fast step-size\ndetermination is also evaluated."}
{"id": "2508.19608", "pdf": "https://arxiv.org/pdf/2508.19608", "abs": "https://arxiv.org/abs/2508.19608", "authors": ["Dongjae Lee", "Byeongjun Kim", "H. Jin Kim"], "title": "Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning", "categories": ["cs.RO"], "comment": null, "summary": "Aerial manipulators based on conventional multirotors can conduct\nmanipulation only in small roll and pitch angles due to the underactuatedness\nof the multirotor base. If the multirotor base is capable of hovering at\narbitrary orientation, the robot can freely locate itself at any point in\n$\\mathsf{SE}(3)$, significantly extending its manipulation workspace and\nenabling a manipulation task that was originally not viable. In this work, we\npresent a geometric robust control and whole-body motion planning framework for\nan omnidirectional aerial manipulator (OAM). To maximize the strength of OAM,\nwe first propose a geometric robust controller for a floating base. Since the\nmotion of the robotic arm and the interaction forces during manipulation affect\nthe stability of the floating base, the base should be capable of mitigating\nthese adverse effects while controlling its 6D pose. We then design a two-step\noptimization-based whole-body motion planner, jointly considering the pose of\nthe floating base and the joint angles of the robotic arm to harness the entire\nconfiguration space. The devised two-step approach facilitates real-time\napplicability and enhances convergence of the optimization problem with\nnon-convex and non-Euclidean search space. The proposed approach enables the\nbase to be stationary at any 6D pose while autonomously carrying out\nsophisticated manipulation near obstacles without any collision. We demonstrate\nthe effectiveness of the proposed framework through experiments in which an OAM\nperforms grasping and pulling of an object in multiple scenarios, including\nnear $90^\\circ$ and even $180^\\circ$ pitch angles."}
{"id": "2508.19910", "pdf": "https://arxiv.org/pdf/2508.19910", "abs": "https://arxiv.org/abs/2508.19910", "authors": ["Sergio Hernandez", "Christophe Peucheret", "Francesco Da Ros", "Darko Zibar"], "title": "Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission", "categories": ["eess.SP", "cs.LG"], "comment": "10 pages, 10 figures, submitted to journal of lightwave technology", "summary": "Directly modulated lasers (DMLs) are an attractive technology for short-reach\nintensity modulation and direct detection communication systems. However, their\ncomplex nonlinear dynamics make the modeling and optimization of DML-based\nsystems challenging. In this paper, we study the end-to-end optimization of\nDML-based systems based on a data-driven surrogate model trained on\nexperimental data. The end-to-end optimization includes the pulse shaping and\nequalizer filters, the bias current and the modulation radio-frequency (RF)\npower applied to the laser. The performance of the end-to-end optimization\nscheme is tested on the experimental setup and compared to 4 different\nbenchmark schemes based on linear and nonlinear receiver-side equalization. The\nresults show that the proposed end-to-end scheme is able to deliver better\nperformance throughout the studied symbol rates and transmission distances\nwhile employing lower modulation RF power, fewer filter taps and utilizing a\nsmaller signal bandwidth."}
{"id": "2508.19684", "pdf": "https://arxiv.org/pdf/2508.19684", "abs": "https://arxiv.org/abs/2508.19684", "authors": ["Ghadeer Elmkaiel", "Syn Schmitt", "Michael Muehlebach"], "title": "Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control", "categories": ["cs.RO", "physics.app-ph"], "comment": null, "summary": "Achieving both agile maneuverability and high energy efficiency in aerial\nrobots, particularly in dynamic wind environments, remains challenging.\nConventional thruster-powered systems offer agility but suffer from high energy\nconsumption, while fixed-wing designs are efficient but lack hovering and\nmaneuvering capabilities. We present Floaty, a shape-changing robot that\novercomes these limitations by passively soaring, harnessing wind energy\nthrough intelligent morphological control inspired by birds. Floaty's design is\noptimized for passive stability, and its control policy is derived from an\nexperimentally learned aerodynamic model, enabling precise attitude and\nposition control without active propulsion. Wind tunnel experiments demonstrate\nFloaty's ability to hover, maneuver, and reject disturbances in vertical\nairflows up to 10 m/s. Crucially, Floaty achieves this with a specific power\nconsumption of 10 W/kg, an order of magnitude lower than thruster-powered\nsystems. This introduces a paradigm for energy-efficient aerial robotics,\nleveraging morphological intelligence and control to operate sustainably in\nchallenging wind conditions."}
{"id": "2508.19931", "pdf": "https://arxiv.org/pdf/2508.19931", "abs": "https://arxiv.org/abs/2508.19931", "authors": ["Isabella W. G. da Silva", "Zahra Mobini", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Cell-Free Massive MIMO-Based Physical-Layer Authentication", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we exploit the cell-free massive multiple-input\nmultiple-output (CF-mMIMO) architecture to design a physical-layer\nauthentication (PLA) framework that can simultaneously authenticate multiple\ndistributed users across the coverage area. Our proposed scheme remains\neffective even in the presence of active adversaries attempting impersonation\nattacks to disrupt the authentication process. Specifically, we introduce a\ntag-based PLA CFmMIMO system, wherein the access points (APs) first estimate\ntheir channels with the legitimate users during an uplink training phase.\nSubsequently, a unique secret key is generated and securely shared between each\nuser and the APs. We then formulate a hypothesis testing problem and derive a\nclosed-form expression for the probability of detection for each user in the\nnetwork. Numerical results validate the effectiveness of the proposed approach,\ndemonstrating that it maintains a high detection probability even as the number\nof users in the system increases."}
{"id": "2508.19731", "pdf": "https://arxiv.org/pdf/2508.19731", "abs": "https://arxiv.org/abs/2508.19731", "authors": ["Maryam Kazemi Eskeri", "Ville Kyrki", "Dominik Baumann", "Tomasz Piotr Kucner"], "title": "Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments", "categories": ["cs.RO"], "comment": "7 Pages, 4 Figures, Accepted in IROS2025", "summary": "Multi-robot systems are increasingly deployed in applications, such as\nintralogistics or autonomous delivery, where multiple robots collaborate to\ncomplete tasks efficiently. One of the key factors enabling their efficient\ncooperation is Multi-Robot Task Allocation (MRTA). Algorithms solving this\nproblem optimize task distribution among robots to minimize the overall\nexecution time. In shared environments, apart from the relative distance\nbetween the robots and the tasks, the execution time is also significantly\nimpacted by the delay caused by navigating around moving people. However, most\nexisting MRTA approaches are dynamics-agnostic, relying on static maps and\nneglecting human motion patterns, leading to inefficiencies and delays. In this\npaper, we introduce \\acrfull{method name}. This method leverages Maps of\nDynamics (MoDs), spatio-temporal queryable models designed to capture\nhistorical human movement patterns, to estimate the impact of humans on the\ntask execution time during deployment. \\acrshort{method name} utilizes a\nstochastic cost function that includes MoDs. Experimental results show that\nintegrating MoDs enhances task allocation performance, resulting in reduced\nmission completion times by up to $26\\%$ compared to the dynamics-agnostic\nmethod and up to $19\\%$ compared to the baseline. This work underscores the\nimportance of considering human dynamics in MRTA within shared environments and\npresents an efficient framework for deploying multi-robot systems in\nenvironments populated by humans."}
{"id": "2508.19994", "pdf": "https://arxiv.org/pdf/2508.19994", "abs": "https://arxiv.org/abs/2508.19994", "authors": ["Noah Shore"], "title": "The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-fin.MF"], "comment": "Submitted to International Symposium for Signal Processing 2025", "summary": "The Coherent Multiplex is formalized and validated as a scalable, real-time\nsystem for identifying, analyzing, and visualizing coherence among multiple\ntime series. Its architecture comprises a fast spectral similarity layer based\non cosine similarity metrics of Fourier-transformed signals, and a sparse\ntime-frequency layer for wavelet coherence. The system constructs and evolves a\nmultilayer graph representing inter-signal relationships, enabling low-latency\ninference and monitoring. A simulation prototype demonstrates functionality\nacross 8 synthetic channels with a high similarity threshold for further\ncomputation, with additional opportunities for scaling the architecture up to\nsupport thousands of input signals with constrained hardware. Applications\ndiscussed include neuroscience, finance, and biomedical signal analysis."}
{"id": "2508.19771", "pdf": "https://arxiv.org/pdf/2508.19771", "abs": "https://arxiv.org/abs/2508.19771", "authors": ["Liding Zhang", "Zhenshan Bing", "Yu Zhang", "Kuanqi Cai", "Lingyun Chen", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Elliptical K-Nearest Neighbors -- Path Optimization via Coulomb's Law and Invalid Vertices in C-space Obstacles", "categories": ["cs.RO"], "comment": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "Path planning has long been an important and active research area in\nrobotics. To address challenges in high-dimensional motion planning, this study\nintroduces the Force Direction Informed Trees (FDIT*), a sampling-based planner\ndesigned to enhance speed and cost-effectiveness in pathfinding. FDIT* builds\nupon the state-of-the-art informed sampling planner, the Effort Informed Trees\n(EIT*), by capitalizing on often-overlooked information in invalid vertices. It\nincorporates principles of physical force, particularly Coulomb's law. This\napproach proposes the elliptical $k$-nearest neighbors search method, enabling\nfast convergence navigation and avoiding high solution cost or infeasible paths\nby exploring more problem-specific search-worthy areas. It demonstrates\nbenefits in search efficiency and cost reduction, particularly in confined,\nhigh-dimensional environments. It can be viewed as an extension of nearest\nneighbors search techniques. Fusing invalid vertex data with physical dynamics\nfacilitates force-direction-based search regions, resulting in an improved\nconvergence rate to the optimum. FDIT* outperforms existing single-query,\nsampling-based planners on the tested problems in R^4 to R^16 and has been\ndemonstrated on a real-world mobile manipulation task."}
{"id": "2508.19495", "pdf": "https://arxiv.org/pdf/2508.19495", "abs": "https://arxiv.org/abs/2508.19495", "authors": ["Muhammad Ahmed Mohsin", "Junaid Ahmad", "Muhammad Hamza Nawaz", "Muhammad Ali Jamshed"], "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks", "categories": ["cs.DC", "cs.LG", "eess.SP"], "comment": "Submitted as a chapter to the book Ambient Intelligence for 6G", "summary": "Ambient intelligence (AmI) is a computing paradigm in which physical\nenvironments are embedded with sensing, computation, and communication so they\ncan perceive people and context, decide appropriate actions, and respond\nautonomously. Realizing AmI at global scale requires sixth generation (6G)\nwireless networks with capabilities for real time perception, reasoning, and\naction aligned with human behavior and mobility patterns. We argue that\nGenerative Artificial Intelligence (GenAI) is the creative core of such\nenvironments. Unlike traditional AI, GenAI learns data distributions and can\ngenerate realistic samples, making it well suited to close key AmI gaps,\nincluding generating synthetic sensor and channel data in under observed areas,\ntranslating user intent into compact, semantic messages, predicting future\nnetwork conditions for proactive control, and updating digital twins without\ncompromising privacy.\n  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,\nand generative transformers, and connects them to practical AmI use cases,\nincluding spectrum sharing, ultra reliable low latency communication,\nintelligent security, and context aware digital twins. We also examine how 6G\nenablers, such as edge and fog computing, IoT device swarms, intelligent\nreflecting surfaces (IRS), and non terrestrial networks, can host or accelerate\ndistributed GenAI. Finally, we outline open challenges in energy efficient on\ndevice training, trustworthy synthetic data, federated generative learning, and\nAmI specific standardization. We show that GenAI is not a peripheral addition,\nbut a foundational element for transforming 6G from a faster network into an\nambient intelligent ecosystem."}
{"id": "2508.19776", "pdf": "https://arxiv.org/pdf/2508.19776", "abs": "https://arxiv.org/abs/2508.19776", "authors": ["Liding Zhang", "Yao Ling", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Tree-Based Grafting Approach for Bidirectional Motion Planning with Local Subsets Optimization", "categories": ["cs.RO"], "comment": "IEEE Robotics and Automation Letters (also presented at IEEE-IROS\n  2025)", "summary": "Bidirectional motion planning often reduces planning time compared to its\nunidirectional counterparts. It requires connecting the forward and reverse\nsearch trees to form a continuous path. However, this process could fail and\nrestart the asymmetric bidirectional search due to the limitations of\nlazy-reverse search. To address this challenge, we propose Greedy GuILD\nGrafting Trees (G3T*), a novel path planner that grafts invalid edge\nconnections at both ends to re-establish tree-based connectivity, enabling\nrapid path convergence. G3T* employs a greedy approach using the minimum\nLebesgue measure of guided incremental local densification (GuILD) subsets to\noptimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling\ndistribution between the informed set and GuILD subsets based on historical and\ncurrent cost improvements, ensuring asymptotic optimality. These features\nenhance the forward search's growth towards the reverse tree, achieving faster\nconvergence and lower solution costs. Benchmark experiments across dimensions\nfrom R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior\nperformance compared to existing single-query sampling-based planners. A video\nshowcasing our experimental results is available at:\nhttps://youtu.be/3mfCRL5SQIU"}
{"id": "2508.20003", "pdf": "https://arxiv.org/pdf/2508.20003", "abs": "https://arxiv.org/abs/2508.20003", "authors": ["Ayten Gürbüz", "Giuseppe Caire", "Alexander Steingass"], "title": "On the Outage Probability of Multiuser Multiple Antenna Systems with Non-Orthogonal Multiple Access for Air-Ground Communications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "14 pages, 7 figures", "summary": "This paper explores multiuser multiple antenna systems as a means to enhance\nthe spectral efficiency of aeronautical communications systems. To this end,\nthe outage regime for a multiuser multiple antenna system is studied within a\nrealistic geometry-based stochastic air-ground (AG) channel model. In this\napplication, users (aircraft) transmit air traffic management data to the\nground station at a predefined target rate. Due to the nature of the AG\npropagation, we argue that the relevant performance metric in this context is\nthe information outage probability. We consider the outage probability under\nthree decoding approaches. The first is based on successive interference\ncancellation (SIC). The second extends the first approach by considering joint\ngroup decoding. The third is a version of the second that limits the size of\nthe jointly decoded user groups in order to lower the decoding complexity. The\nresults show that joint group decoding, even in groups of only two, can\nsignificantly increase the spectral efficiency in the AG channel by allowing a\nlarge number of aircraft to transmit over a non-orthogonal channel with very\nlow outage probabilities."}
{"id": "2508.19788", "pdf": "https://arxiv.org/pdf/2508.19788", "abs": "https://arxiv.org/abs/2508.19788", "authors": ["Sena Ishii", "Akash Chikhalikar", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, Accepted for IEEE RO-MAN 2025 Conference", "summary": "We present a novel framework for estimating accident-prone regions in\neveryday indoor scenes, aimed at improving real-time risk awareness in service\nrobots operating in human-centric environments. As robots become integrated\ninto daily life, particularly in homes, the ability to anticipate and respond\nto environmental hazards is crucial for ensuring user safety, trust, and\neffective human-robot interaction. Our approach models object-level risk and\ncontext through a semantic graph-based propagation algorithm. Each object is\nrepresented as a node with an associated risk score, and risk propagates\nasymmetrically from high-risk to low-risk objects based on spatial proximity\nand accident relationship. This enables the robot to infer potential hazards\neven when they are not explicitly visible or labeled. Designed for\ninterpretability and lightweight onboard deployment, our method is validated on\na dataset with human-annotated risk regions, achieving a binary risk detection\naccuracy of 75%. The system demonstrates strong alignment with human\nperception, particularly in scenes involving sharp or unstable objects. These\nresults underline the potential of context-aware risk reasoning to enhance\nrobotic scene understanding and proactive safety behaviors in shared\nhuman-robot spaces. This framework could serve as a foundation for future\nsystems that make context-driven safety decisions, provide real-time alerts, or\nautonomously assist users in avoiding or mitigating hazards within home\nenvironments."}
{"id": "2508.19790", "pdf": "https://arxiv.org/pdf/2508.19790", "abs": "https://arxiv.org/abs/2508.19790", "authors": ["Liding Zhang", "Sicheng Wang", "Kuanqi Cai", "Zhenshan Bing", "Fan Wu", "Chaoqun Wang", "Sami Haddadin", "Alois Knoll"], "title": "APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors", "categories": ["cs.RO"], "comment": null, "summary": "Optimal path planning aims to determine a sequence of states from a start to\na goal while accounting for planning objectives. Popular methods often\nintegrate fixed batch sizes and neglect information on obstacles, which is not\nproblem-specific. This study introduces Adaptively Prolated Trees (APT*), a\nnovel sampling-based motion planner that extends based on Force Direction\nInformed Trees (FDIT*), integrating adaptive batch-sizing and elliptical\n$r$-nearest neighbor modules to dynamically modulate the path searching process\nbased on environmental feedback. APT* adjusts batch sizes based on the\nhypervolume of the informed sets and considers vertices as electric charges\nthat obey Coulomb's law to define virtual forces via neighbor samples, thereby\nrefining the prolate nearest neighbor selection. These modules employ\nnon-linear prolate methods to adaptively adjust the electric charges of\nvertices for force definition, thereby improving the convergence rate with\nlower solution costs. Comparative analyses show that APT* outperforms existing\nsingle-query sampling-based planners in dimensions from $\\mathbb{R}^4$ to\n$\\mathbb{R}^{16}$, and it was further validated through a real-world robot\nmanipulation task. A video showcasing our experimental results is available at:\nhttps://youtu.be/gCcUr8LiEw4"}
{"id": "2508.19816", "pdf": "https://arxiv.org/pdf/2508.19816", "abs": "https://arxiv.org/abs/2508.19816", "authors": ["Ricardo J. Manríquez-Cisterna", "Ankit A. Ravankar", "Jose V. Salazar Luces", "Takuro Hatsukari", "Yasuhisa Hirata"], "title": "A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living", "categories": ["cs.RO"], "comment": "7 pages, accepted work for IEEE RO-MAN2025", "summary": "This paper presents a standing support mobility robot \"Moby\" developed to\nenhance independence and safety for elderly individuals during daily activities\nsuch as toilet transfers. Unlike conventional seated mobility aids, the robot\nmaintains users in an upright posture, reducing physical strain, supporting\nnatural social interaction at eye level, and fostering a greater sense of\nself-efficacy. Moby offers a novel alternative by functioning both passively\nand with mobility support, enabling users to perform daily tasks more\nindependently. Its main advantages include ease of use, lightweight design,\ncomfort, versatility, and effective sit-to-stand assistance. The robot\nleverages the Robot Operating System (ROS) for seamless control, featuring\nmanual and autonomous operation modes. A custom control system enables safe and\nintuitive interaction, while the integration with NAV2 and LiDAR allows for\nrobust navigation capabilities. This paper reviews existing mobility solutions\nand compares them to Moby, details the robot's design, and presents objective\nand subjective experimental results using the NASA-TLX method and time\ncomparisons to other methods to validate our design criteria and demonstrate\nthe advantages of our contribution."}
{"id": "2508.19926", "pdf": "https://arxiv.org/pdf/2508.19926", "abs": "https://arxiv.org/abs/2508.19926", "authors": ["Tan Jing", "Shiting Chen", "Yangfan Li", "Weisheng Xu", "Renjing Xu"], "title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control", "categories": ["cs.RO"], "comment": null, "summary": "Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM."}
{"id": "2508.19953", "pdf": "https://arxiv.org/pdf/2508.19953", "abs": "https://arxiv.org/abs/2508.19953", "authors": ["Rafael Cathomen", "Mayank Mittal", "Marin Vlastelica", "Marco Hutter"], "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors", "categories": ["cs.RO"], "comment": "Accepted to CoRL 2025. For code and videos, please check:\n  https://leggedrobotics.github.io/d3-skill-discovery/", "summary": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards."}
{"id": "2508.19958", "pdf": "https://arxiv.org/pdf/2508.19958", "abs": "https://arxiv.org/abs/2508.19958", "authors": ["Yiguo Fan", "Pengxiang Ding", "Shuanghao Bai", "Xinyang Tong", "Yuyang Zhu", "Hongchao Lu", "Fengqi Dai", "Wei Zhao", "Yang Liu", "Siteng Huang", "Zhaoxin Fan", "Badong Chen", "Donglin Wang"], "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation", "categories": ["cs.RO"], "comment": "Accepted to CoRL 2025; Github Page: https://long-vla.github.io", "summary": "Vision-Language-Action (VLA) models have become a cornerstone in robotic\npolicy learning, leveraging large-scale multimodal data for robust and scalable\ncontrol. However, existing VLA frameworks primarily address short-horizon\ntasks, and their effectiveness on long-horizon, multi-step robotic manipulation\nremains limited due to challenges in skill chaining and subtask dependencies.\nIn this work, we introduce Long-VLA, the first end-to-end VLA model\nspecifically designed for long-horizon robotic tasks. Our approach features a\nnovel phase-aware input masking strategy that adaptively segments each subtask\ninto moving and interaction phases, enabling the model to focus on\nphase-relevant sensory cues and enhancing subtask compatibility. This unified\nstrategy preserves the scalability and data efficiency of VLA training, and our\narchitecture-agnostic module can be seamlessly integrated into existing VLA\nmodels. We further propose the L-CALVIN benchmark to systematically evaluate\nlong-horizon manipulation. Extensive experiments on both simulated and\nreal-world tasks demonstrate that Long-VLA significantly outperforms prior\nstate-of-the-art methods, establishing a new baseline for long-horizon robotic\ncontrol."}
{"id": "2508.20037", "pdf": "https://arxiv.org/pdf/2508.20037", "abs": "https://arxiv.org/abs/2508.20037", "authors": ["Henk H. A. Jekel", "Alejandro Díaz Rosales", "Luka Peternel"], "title": "Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech", "categories": ["cs.RO"], "comment": null, "summary": "The paper presents a visio-verbal teleimpedance interface for commanding 3D\nstiffness ellipsoids to the remote robot with a combination of the operator's\ngaze and verbal interaction. The gaze is detected by an eye-tracker, allowing\nthe system to understand the context in terms of what the operator is currently\nlooking at in the scene. Along with verbal interaction, a Visual Language Model\n(VLM) processes this information, enabling the operator to communicate their\nintended action or provide corrections. Based on these inputs, the interface\ncan then generate appropriate stiffness matrices for different physical\ninteraction actions. To validate the proposed visio-verbal teleimpedance\ninterface, we conducted a series of experiments on a setup including a Force\nDimension Sigma.7 haptic device to control the motion of the remote Kuka LBR\niiwa robotic arm. The human operator's gaze is tracked by Tobii Pro Glasses 2,\nwhile human verbal commands are processed by a VLM using GPT-4o. The first\nexperiment explored the optimal prompt configuration for the interface. The\nsecond and third experiments demonstrated different functionalities of the\ninterface on a slide-in-the-groove task."}
{"id": "2508.20085", "pdf": "https://arxiv.org/pdf/2508.20085", "abs": "https://arxiv.org/abs/2508.20085", "authors": ["Zhecheng Yuan", "Tianming Wei", "Langzhe Gu", "Pu Hua", "Tianhai Liang", "Yuanpei Chen", "Huazhe Xu"], "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/."}
{"id": "2508.20095", "pdf": "https://arxiv.org/pdf/2508.20095", "abs": "https://arxiv.org/abs/2508.20095", "authors": ["Jinhao Liang", "Sven Koenig", "Ferdinando Fioretto"], "title": "Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates."}
{"id": "2508.19257", "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."}
{"id": "2508.20072", "pdf": "https://arxiv.org/pdf/2508.20072", "abs": "https://arxiv.org/abs/2508.20072", "authors": ["Zhixuan Liang", "Yizhuo Li", "Tianshuo Yang", "Chengyue Wu", "Sitong Mao", "Liuao Pei", "Xiaokang Yang", "Jiangmiao Pang", "Yao Mu", "Ping Luo"], "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "15 pages", "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets."}
