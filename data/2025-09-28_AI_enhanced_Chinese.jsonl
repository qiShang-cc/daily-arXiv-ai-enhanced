{"id": "2509.20500", "pdf": "https://arxiv.org/pdf/2509.20500", "abs": "https://arxiv.org/abs/2509.20500", "authors": ["Weijian Zhang", "Hashan K. Weerasooriya", "Prateek Chennuri", "Stanley H. Chan"], "title": "Real-Time Markov Modeling for Single-Photon LiDAR: $1000 \\times$ Acceleration and Convergence Analysis", "categories": ["eess.SP"], "comment": null, "summary": "Asynchronous single-photon LiDAR (SP-LiDAR) is an important imaging modality\nfor high-quality 3D applications and navigation, but the modeling of the\ntimestamp distributions of a SP-LiDAR in the presence of dead time remains a\nvery challenging open problem. Prior works have shown that timestamps form a\ndiscrete-time Markov chain, whose stationary distribution can be computed as\nthe leading left eigenvector of a large transition matrix. However,\nconstructing this matrix is known to be computationally expensive because of\nthe coupling between states and the dead time. This paper presents the first\nnon-sequential Markov modeling for the timestamp distribution. The key\ninnovation is an equivalent formulation that reparameterizes the integral\nbounds and separates the effect of dead time as a deterministic row permutation\nof a base matrix. This decoupling enables efficient vectorized matrix\nconstruction, yielding up to $1000 \\times$ acceleration over existing methods.\nThe new model produces a nearly exact stationary distribution when compared\nwith the gold standard Monte Carlo simulations, yet using a fraction of the\ntime. In addition, a new theoretical analysis reveals the impact of the\nmagnitude and phase of the second-largest eigenvalue, which are overlooked in\nthe literature but are critical to the convergence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u65f6\u5e8f\u9a6c\u5c14\u53ef\u592b\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u5149\u5b50LiDAR\u5728\u6b7b\u533a\u65f6\u95f4\u5b58\u5728\u4e0b\u65f6\u95f4\u6233\u5206\u5e03\u5efa\u6a21\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u8fbe1000\u500d\u7684\u52a0\u901f\uff0c\u5e76\u63ed\u793a\u4e86\u7b2c\u4e8c\u5927\u7279\u5f81\u503c\u7684\u5e45\u503c\u548c\u76f8\u4f4d\u5bf9\u6536\u655b\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u5355\u5149\u5b50LiDAR\u5728\u9ad8\u54c1\u8d283D\u5e94\u7528\u548c\u5bfc\u822a\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6b7b\u533a\u65f6\u95f4\u5b58\u5728\u4e0b\u5efa\u6a21\u65f6\u95f4\u6233\u5206\u5e03\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u6210\u4e3a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u65f6\u5e8f\u9a6c\u5c14\u53ef\u592b\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u79ef\u5206\u8fb9\u754c\u5e76\u5206\u79bb\u6b7b\u533a\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u9ad8\u6548\u5411\u91cf\u5316\u77e9\u9635\u6784\u9020\u3002", "result": "\u65b0\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1000\u500d\u7684\u52a0\u901f\uff0c\u5e76\u751f\u6210\u4e86\u4e0e\u8499\u7279\u5361\u6d1b\u6a21\u62df\u51e0\u4e4e\u5b8c\u5168\u4e00\u81f4\u7684\u7a33\u6001\u5206\u5e03\u3002", "conclusion": "\u8bba\u6587\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5efa\u6a21\u6548\u7387\u95ee\u9898\uff0c\u8fd8\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u7b2c\u4e8c\u5927\u7279\u5f81\u503c\u7684\u5e45\u503c\u548c\u76f8\u4f4d\u5bf9\u6536\u655b\u7684\u5173\u952e\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u6587\u732e\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.20908", "pdf": "https://arxiv.org/pdf/2509.20908", "abs": "https://arxiv.org/abs/2509.20908", "authors": ["Peng Liu", "Zesong Fei", "Meng Hua", "Guangji Chen", "Xinyi Wang", "Ruiqi Liu"], "title": "Wireless Powered MEC Systems via Discrete Pinching Antennas: TDMA versus NOMA", "categories": ["eess.SP"], "comment": "13 pages, 9 figures. Submitted to IEEE journals for possible\n  publication", "summary": "Pinching antennas (PAs), a new type of reconfigurable and flexible antenna\nstructures, have recently attracted significant research interest due to their\nability to create line-of-sight links and mitigate large-scale path loss. Owing\nto their potential benefits, integrating PAs into wireless powered mobile edge\ncomputing (MEC) systems is regarded as a viable solution to enhance both energy\ntransfer and task offloading efficiency. Unlike prior studies that assume ideal\ncontinuous PA placement along waveguides, this paper investigates a practical\ndiscrete PA-assisted wireless powered MEC framework, where devices first\nharvest energy from PA-emitted radio-frequency signals and then adopt a partial\noffloading mode, allocating part of the harvested energy to local computing and\nthe remainder to uplink offloading. The uplink phase considers both the\ntime-division multiple access (TDMA) and non-orthogonal multiple access (NOMA),\neach examined under three levels of PA activation flexibility. For each\nconfiguration, we formulate a joint optimization problem to maximize the total\ncomputational bits and conduct a theoretical performance comparison between the\nTDMA and NOMA schemes. To address the resulting mixed-integer nonlinear\nproblems, we develop a two-layer algorithm that combines closed-form solutions\nbased on Karush-Kuhn-Tucker (KKT) conditions with a cross-entropy-based\nlearning method. Numerical results validate the superiority of the proposed\ndesign in terms of the harvested energy and computation performance, revealing\nthat TDMA and NOMA achieve comparable performance under coarser PA activation\nlevels, whereas finer activation granularity enables TDMA to achieve superior\ncomputation performance over NOMA.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u5f0f\u634f\u5408\u5929\u7ebf\u8f85\u52a9\u7684\u65e0\u7ebf\u4f9b\u7535\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u6846\u67b6\uff0c\u8003\u8651\u4e86TDMA\u548cNOMA\u4e24\u79cd\u591a\u5740\u63a5\u5165\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u53cc\u5c42\u7b97\u6cd5\u4f18\u5316\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u6574\u5408\u634f\u5408\u5929\u7ebf\u5230\u65e0\u7ebf\u4f9b\u7535\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u80fd\u91cf\u4f20\u8f93\u548c\u4efb\u52a1\u5378\u8f7d\u6548\u7387\u3002", "method": "\u7814\u7a76\u79bb\u6563\u5f0f\u634f\u5408\u5929\u7ebf\u8f85\u52a9\u6846\u67b6\uff0c\u91c7\u7528\u90e8\u5206\u5378\u8f7d\u6a21\u5f0f\uff0c\u63d0\u51fa\u7ed3\u5408KKT\u6761\u4ef6\u548c\u4ea4\u53c9\u71b5\u5b66\u4e60\u7684\u53cc\u5c42\u7b97\u6cd5\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u5728\u80fd\u91cf\u6536\u96c6\u548c\u8ba1\u7b97\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u5728\u7cbe\u7ec6\u6fc0\u6d3b\u7c92\u5ea6\u4e0bTDMA\u4f18\u4e8eNOMA\u3002", "conclusion": "\u79bb\u6563\u5f0f\u634f\u5408\u5929\u7ebf\u6846\u67b6\u5728\u65e0\u7ebf\u4f9b\u7535\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e0d\u540c\u591a\u5740\u63a5\u5165\u65b9\u5f0f\u7684\u6027\u80fd\u4e0e\u6fc0\u6d3b\u7c92\u5ea6\u5bc6\u5207\u76f8\u5173\u3002"}}
{"id": "2509.20987", "pdf": "https://arxiv.org/pdf/2509.20987", "abs": "https://arxiv.org/abs/2509.20987", "authors": ["Changhao Liu", "Weidong Mei", "Zhi Chen", "Jun Fang", "Boyu Ning"], "title": "A General Optimization Framework for Movable Antenna Systems via Discrete Sampling", "categories": ["eess.SP"], "comment": null, "summary": "Movable antenna (MA) systems have attracted growing interest in wireless\ncommunications due to their ability to reshape wireless channels via local\nantenna movement within a confined region. However, optimizing antenna\npositions to enhance communication performance turns out to be challenging due\nto the highly nonlinear relationship between wireless channels and antenna\npositions. Existing approaches, such as gradient-based and heuristic\nalgorithms, often suffer from high computational complexity or undesired local\noptima. To address the above challenge, this letter proposes a general and\nlow-complexity optimization framework for MA position optimization.\nSpecifically, we discretize the antenna movement region into a set of sampling\npoints, thereby transforming the continuous optimization problem into a\ndiscrete point selection problem. Next, we sequentially update the optimal\nsampling point for each MA over multiple rounds. To avoid convergence to poor\nlocal optima, a Gibbs sampling (GS) phase is introduced between rounds to\nexplore adjacent and randomly generated candidate solutions. As a case study,\nwe investigate joint precoding and antenna position optimization for an\nMA-enhanced broadcast system by applying the proposed framework. Numerical\nresults demonstrate that the proposed algorithm achieves near-optimal\nperformance and significantly outperforms existing benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u5929\u7ebf\u79fb\u52a8\u533a\u57df\u5e76\u5f15\u5165\u5409\u5e03\u65af\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u4e2d\u4f4d\u7f6e\u4f18\u5316\u7684\u6311\u6218\u3002", "motivation": "\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7cfb\u7edf\u56e0\u5176\u80fd\u591f\u901a\u8fc7\u5c40\u90e8\u5929\u7ebf\u79fb\u52a8\u91cd\u5851\u65e0\u7ebf\u4fe1\u9053\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u56e0\u9ad8\u5ea6\u975e\u7ebf\u6027\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5c06\u5929\u7ebf\u79fb\u52a8\u533a\u57df\u79bb\u6563\u5316\u4e3a\u91c7\u6837\u70b9\uff0c\u8f6c\u5316\u4e3a\u79bb\u6563\u70b9\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5409\u5e03\u65af\u91c7\u6837\u907f\u514d\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MA\u7cfb\u7edf\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21032", "pdf": "https://arxiv.org/pdf/2509.21032", "abs": "https://arxiv.org/abs/2509.21032", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "title": "Shapley Features for Robust Signal Prediction in Tactile Internet", "categories": ["eess.SP"], "comment": null, "summary": "The Tactile Internet (TI) requires ultra-low latency and reliable haptic\nsignal transmission, yet packet loss and delay remain unresolved challenges. We\npresent a novel prediction framework that integrates Gaussian Processes (GP)\nwith a ResNet-based Neural Network, where GP acts as an oracle to recover\nsignals lost or heavily delayed. To further optimize performance, we introduce\nShapley Feature Values (SFV), a principled feature selection mechanism that\nisolates the most informative inputs for prediction. This GP+SFV framework\nachieves 95.72% accuracy, surpassing the state-of-the-art LeFo method by 11.1%,\nwhile simultaneously relaxing TI's rigid delay constraints. Beyond accuracy,\nSFV operates as a modular accelerator: when paired with LeFo, it reduces\ninference time by 27%, and when paired with GP, by 72%. These results establish\nGP+SFV as both a high-accuracy and high-efficiency solution, paving the way for\npractical and reliable haptic communications in TI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u548c\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08ResNet\uff09\u7684\u65b0\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u5f15\u5165Shapley\u7279\u5f81\u503c\uff08SFV\uff09\u4f18\u5316\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u4fe1\u53f7\u4f20\u8f93\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e6\u89c9\u4e92\u8054\u7f51\u5bf9\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u7684\u4fe1\u53f7\u4f20\u8f93\u6709\u4e25\u683c\u8981\u6c42\uff0c\u4f46\u6570\u636e\u5305\u4e22\u5931\u548c\u5ef6\u8fdf\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u548c\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08ResNet\uff09\u6784\u5efa\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u5229\u7528Shapley\u7279\u5f81\u503c\uff08SFV\uff09\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002", "result": "\u8be5\u65b9\u6cd5\u51c6\u786e\u7387\u8fbe\u523095.72%\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5LeFo\u63d0\u5347\u4e8611.1%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5ef6\u8fdf\u7ea6\u675f\uff1b\u4e0eLeFo\u6216GP\u7ed3\u5408\u65f6\uff0c\u63a8\u7406\u65f6\u95f4\u5206\u522b\u51cf\u5c1127%\u548c72%\u3002", "conclusion": "GP+SFV\u6846\u67b6\u4e3a\u89e6\u89c9\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u89e6\u89c9\u901a\u4fe1\u7684\u5b9e\u7528\u5316\u3002"}}
{"id": "2509.20486", "pdf": "https://arxiv.org/pdf/2509.20486", "abs": "https://arxiv.org/abs/2509.20486", "authors": ["Sven Ochs", "Philip Sch\u00f6rner", "Marc Ren\u00e9 Zofka", "J. Marius Z\u00f6llner"], "title": "Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation", "categories": ["cs.RO"], "comment": null, "summary": "Semantic segmentation of LiDAR data presents considerable challenges,\nparticularly when dealing with diverse sensor types and configurations.\nHowever, incorporating semantic information can significantly enhance the\naccuracy and robustness of LiDAR-based localization techniques for autonomous\nmobile systems. We propose an approach that integrates semantic camera data\nwith LiDAR segmentation to address this challenge. By projecting LiDAR points\ninto the semantic segmentation space of the camera, our method enhances the\nprecision and reliability of the LiDAR-based localization pipeline.\n  For validation, we utilize the CoCar NextGen platform from the FZI Research\nCenter for Information Technology, which offers diverse sensor modalities and\nconfigurations. The sensor setup of CoCar NextGen enables a thorough analysis\nof different sensor types. Our evaluation leverages the state-of-the-art\nDepth-Anything network for camera image segmentation and an adaptive\nsegmentation network for LiDAR segmentation. To establish a reliable ground\ntruth for LiDAR-based localization, we make us of a Global Navigation Satellite\nSystem (GNSS) solution with Real-Time Kinematic corrections (RTK).\nAdditionally, we conduct an extensive 55 km drive through the city of\nKarlsruhe, Germany, covering a variety of environments, including urban areas,\nmulti-lane roads, and rural highways. This multimodal approach paves the way\nfor more reliable and precise autonomous navigation systems, particularly in\ncomplex real-world environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8bed\u4e49\u76f8\u673a\u6570\u636e\u4e0eLiDAR\u5206\u5272\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8LiDAR\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u6837\u5316\u7684\u4f20\u611f\u5668\u7c7b\u578b\u548c\u914d\u7f6e\u7ed9LiDAR\u6570\u636e\u7684\u8bed\u4e49\u5206\u5272\u5e26\u6765\u4e86\u6311\u6218\uff0c\u4f46\u878d\u5165\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u5347LiDAR\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06LiDAR\u70b9\u6295\u5f71\u5230\u76f8\u673a\u8bed\u4e49\u5206\u5272\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528CoCar NextGen\u5e73\u53f0\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u5408Depth-Anything\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u5206\u5272\u7f51\u7edc\u3002", "result": "\u572855\u516c\u91cc\u7684\u590d\u6742\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LiDAR\u5b9a\u4f4d\u7684\u53ef\u9760\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u4e3a\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21118", "pdf": "https://arxiv.org/pdf/2509.21118", "abs": "https://arxiv.org/abs/2509.21118", "authors": ["Ziyi Wang", "Frederik Zumegen", "Christoph Studer"], "title": "Neural Integrated Sensing and Communication for the MIMO-OFDM Downlink", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "To appear in the IEEE Journal on Selected Areas in Communications", "summary": "The ongoing convergence of spectrum and hardware requirements for wireless\nsensing and communication applications has fueled the integrated sensing and\ncommunication (ISAC) paradigm in next-generation networks. Neural-network-based\nISAC leverages data-driven learning techniques to add sensing capabilities to\nexisting communication infrastructure. This paper presents a novel\nsignal-processing framework for such neural ISAC systems based on the\nmultiple-input multiple-output (MIMO) and orthogonal frequency-division\nmultiplexing (OFDM) downlink. Our approach enables generalized sensing\nfunctionality without modifying the MIMO-OFDM communication link. Specifically,\nour neural ISAC pipeline measures the backscattered communication signals to\ngenerate discrete map representations of spatial occupancy, formulated as\nmulticlass or multilabel classification problems, which can then be utilized by\nspecialized downstream tasks. To improve sensing performance in closed or\ncluttered environments, our neural ISAC pipeline relies on features\nspecifically designed to mitigate strong reflective paths. Extensive\nsimulations using ray-tracing models demonstrate that our neural ISAC framework\nreliably reconstructs scene maps without altering the MIMO-OFDM communication\npipeline or reducing data rates.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684ISAC\u4fe1\u53f7\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7MIMO-OFDM\u4e0b\u884c\u94fe\u8def\u5b9e\u73b0\u611f\u77e5\u529f\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u901a\u4fe1\u94fe\u8def\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u751f\u6210\u573a\u666f\u5730\u56fe\u3002", "motivation": "\u65e0\u7ebf\u611f\u77e5\u4e0e\u901a\u4fe1\u9700\u6c42\u7684\u878d\u5408\u63a8\u52a8\u4e86ISAC\u7684\u53d1\u5c55\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6280\u672f\u589e\u5f3a\u73b0\u6709\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u57fa\u4e8eMIMO-OFDM\u4e0b\u884c\u94fe\u8def\uff0c\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edcISAC\u6846\u67b6\uff0c\u5229\u7528\u53cd\u5c04\u4fe1\u53f7\u751f\u6210\u7a7a\u95f4\u5360\u7528\u7684\u79bb\u6563\u5730\u56fe\uff0c\u5e76\u901a\u8fc7\u4e13\u7528\u7279\u5f81\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u6027\u80fd\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u5728\u4e0d\u6539\u53d8\u901a\u4fe1\u94fe\u8def\u6216\u964d\u4f4e\u6570\u636e\u901f\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u9760\u91cd\u5efa\u573a\u666f\u5730\u56fe\u3002", "conclusion": "\u795e\u7ecfISAC\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20488", "pdf": "https://arxiv.org/pdf/2509.20488", "abs": "https://arxiv.org/abs/2509.20488", "authors": ["Atef Azaiez", "David A. Anisi", "Marie Farrell", "Matt Luckcuck"], "title": "Revisiting Formal Methods for Autonomous Robots: A Structured Survey", "categories": ["cs.RO"], "comment": "Appeal accepted: MOD-66548 This is an appeal request regarding our\n  submission MOD-65174 - 6681725", "summary": "This paper presents the initial results from our structured literature review\non applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We\ndescribe our structured survey methodology; including database selection and\nassociated search strings, search filters and collaborative review of\nidentified papers. We categorise and enumerate the FM approaches and formalisms\nthat have been used for specification and verification of RAS. We investigate\nFM in the context of sub-symbolic AI-enabled RAS and examine the evolution of\nhow FM is used over time in this field. This work complements a pre-existing\nsurvey in this area and we examine how this research area has matured over\ntime. Specifically, our survey demonstrates that some trends have persisted as\nobserved in a previous survey. Additionally, it recognized new trends that were\nnot considered previously including a noticeable increase in adopting Formal\nSynthesis approaches as well as Probabilistic Verification Techniques.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u5f62\u5f0f\u5316\u65b9\u6cd5\uff08FM\uff09\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\uff08RAS\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5206\u6790\u4e86FM\u7684\u5206\u7c7b\u3001\u5f62\u5f0f\u4e3b\u4e49\u53ca\u5176\u968f\u65f6\u95f4\u53d1\u5c55\u7684\u8d8b\u52bf\uff0c\u5e76\u8865\u5145\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u53ca\u5176\u968f\u65f6\u95f4\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u7efc\u8ff0\u7684\u4e0d\u8db3\uff0c\u8bc6\u522b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u6280\u672f\u8d8b\u52bf\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u5e93\u9009\u62e9\u3001\u641c\u7d22\u5b57\u7b26\u4e32\u8bbe\u8ba1\u3001\u7b5b\u9009\u8fc7\u6ee4\u548c\u534f\u4f5c\u8bc4\u5ba1\uff0c\u5bf9\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728RAS\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u7edf\u8ba1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u4e9b\u4e0e\u5148\u524d\u8c03\u67e5\u4e00\u81f4\u7684\u957f\u671f\u8d8b\u52bf\uff0c\u540c\u65f6\u4e5f\u8bc6\u522b\u4e86\u65b0\u7684\u8d8b\u52bf\uff0c\u5982\u5f62\u5f0f\u5316\u5408\u6210\u65b9\u6cd5\u548c\u6982\u7387\u9a8c\u8bc1\u6280\u672f\u7684\u91c7\u7528\u663e\u8457\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f62\u5f0f\u5316\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u9886\u57df\u4e0d\u65ad\u6269\u5c55\u548c\u6210\u719f\uff0c\u65b0\u7684\u6280\u672f\u548c\u65b9\u6cd5\u6b63\u5728\u88ab\u5f15\u5165\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.21162", "pdf": "https://arxiv.org/pdf/2509.21162", "abs": "https://arxiv.org/abs/2509.21162", "authors": ["Ali Khandan Boroujeni", "Hyeon Seok Rou", "Ghazal Bagheri", "Giuseppe Thadeu Freitas de Abreu", "Stefan K\u00f6psell", "Kuranage Roche Rayan Ranasinghe", "Rafael F. Schaefer"], "title": "A Secure ISAC Waveform Design Framework via Random Frequency and PRI Agility", "categories": ["eess.SP"], "comment": "Submitted to an IEEE conference", "summary": "This paper presents a novel framework for enhancing the security, data rate,\nand sensing performance of integrated sensing and communications (ISAC)\nsystems. We employ a random frequency and pulse repetition interval (PRI)\nagility (RFPA) method for the waveform design, where the necessary random\nsequences are governed by shared secrets. These secrets, which can be\npre-shared or generated via channel reciprocity, obfuscate critical radar\nparameters like Doppler frequency and pulse start times, thereby significantly\nimpeding the ability to perform reconnaissance from a passive adversary without\nthe secret key. To further introduce enhanced data throughput, we also\nintroduce a hybrid information embedding scheme that integrates amplitude shift\nkeying (ASK), phase shift keying (PSK), index modulation (IM), and spatial\nmodulation (SM), for which a low-complexity sparse-matched filter receiver is\nproposed for accurate decoding with practical complexity. Finally, the\nexcellent range-velocity resolution and clutter suppression of the proposed\nwaveform are analyzed via the ambiguity function (AF).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u9891\u7387\u548c\u8109\u51b2\u91cd\u590d\u95f4\u9694\uff08PRI\uff09\u654f\u6377\u6027\uff08RFPA\uff09\u65b9\u6cd5\u589e\u5f3a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u6570\u636e\u901f\u7387\u548c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u6570\u636e\u901f\u7387\u548c\u611f\u77e5\u6027\u80fd\uff0c\u62b5\u5fa1\u88ab\u52a8\u654c\u624b\u7684\u4fa6\u5bdf\u3002", "method": "\u91c7\u7528RFPA\u65b9\u6cd5\u8bbe\u8ba1\u6ce2\u5f62\uff0c\u7ed3\u5408\u5171\u4eab\u5bc6\u94a5\u751f\u6210\u968f\u673a\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u4fe1\u606f\u5d4c\u5165\u65b9\u6848\uff08ASK\u3001PSK\u3001IM\u548cSM\uff09\u63d0\u9ad8\u6570\u636e\u541e\u5410\u91cf\u3002", "result": "\u901a\u8fc7\u6a21\u7cca\u51fd\u6570\uff08AF\uff09\u5206\u6790\u8bc1\u5b9e\u4e86\u4f18\u5f02\u7684\u8ddd\u79bb-\u901f\u5ea6\u5206\u8fa8\u7387\u548c\u6742\u6ce2\u6291\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728ISAC\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.20499", "pdf": "https://arxiv.org/pdf/2509.20499", "abs": "https://arxiv.org/abs/2509.20499", "authors": ["Boqi Li", "Siyuan Li", "Weiyi Wang", "Anran Li", "Zhong Cao", "Henry X. Liu"], "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid progress of foundation models and robotics, vision-language\nnavigation (VLN) has emerged as a key task for embodied agents with broad\npractical applications. We address VLN in continuous environments, a\nparticularly challenging setting where an agent must jointly interpret natural\nlanguage instructions, perceive its surroundings, and plan low-level actions.\nWe propose a zero-shot framework that integrates a simplified yet effective\nwaypoint predictor with a multimodal large language model (MLLM). The predictor\noperates on an abstract obstacle map, producing linearly reachable waypoints,\nwhich are incorporated into a dynamically updated topological graph with\nexplicit visitation records. The graph and visitation information are encoded\ninto the prompt, enabling reasoning over both spatial structure and exploration\nhistory to encourage exploration and equip MLLM with local path planning for\nerror correction. Extensive experiments on R2R-CE and RxR-CE show that our\nmethod achieves state-of-the-art zero-shot performance, with success rates of\n41% and 36%, respectively, outperforming prior state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7ed3\u5408\u7b80\u5316\u7684\u8def\u5f84\u9884\u6d4b\u5668\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u6210\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fde\u7eed\u73af\u5883\u4e2d\u7684VLN\u95ee\u9898\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u9700\u540c\u65f6\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u611f\u77e5\u73af\u5883\u5e76\u89c4\u5212\u4f4e\u7ea7\u52a8\u4f5c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u62bd\u8c61\u969c\u788d\u5730\u56fe\u7684\u8def\u5f84\u9884\u6d4b\u5668\uff0c\u751f\u6210\u7ebf\u6027\u53ef\u8fbe\u7684\u8def\u6807\u70b9\uff0c\u5e76\u52a8\u6001\u66f4\u65b0\u62d3\u6251\u56fe\u548c\u8bbf\u95ee\u8bb0\u5f55\u3002\u901a\u8fc7\u5c06\u56fe\u548c\u8bbf\u95ee\u4fe1\u606f\u7f16\u7801\u5230\u63d0\u793a\u4e2d\uff0c\u7ed3\u5408MLLM\u8fdb\u884c\u63a8\u7406\uff0c\u652f\u6301\u9519\u8bef\u6821\u6b63\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u8fbe\u523041%\u548c36%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8def\u5f84\u9884\u6d4b\u5668\u548cMLLM\uff0c\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u4e3a\u96f6\u6837\u672cVLN\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.21171", "pdf": "https://arxiv.org/pdf/2509.21171", "abs": "https://arxiv.org/abs/2509.21171", "authors": ["Ali Khandan Boroujeni", "Ghazal Bagheri", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan K\u00f6psell", "Rafael F. Schaefer"], "title": "Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels", "categories": ["eess.SP"], "comment": "Submitted to an IEEE journal", "summary": "We propose an adversarially robust physical layer authentication (AR-PLA)\nframework tailored for non-stationary multiple-input multiple-output (MIMO)\nwireless channels. The framework integrates sequential Bayesian\ndecision-making, deep feature extraction via contrastive learning, and\ngenerative adversarial modeling to simulate adaptive spoofers. Unlike\nconventional methods that assume stationary channels or independent\nobservations, our approach explicitly accounts for temporal and spatial\ncorrelations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. A\ncomprehensive analytical characterization of the authentication performance\nusing both 2-state and 3-state hidden Markov models (HMMs) with moving-average\nonline adaptation is also provided, with closed-form recursions for\nloglikelihood ratios, detection probabilities, and steady-state approximations,\nwhich demonstrate significant robustness improvement over classical sequential\nauthentication schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u975e\u5e73\u7a33MIMO\u65e0\u7ebf\u4fe1\u9053\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u7269\u7406\u5c42\u8ba4\u8bc1\u6846\u67b6\uff08AR-PLA\uff09\uff0c\u6574\u5408\u4e86\u5e8f\u5217\u8d1d\u53f6\u65af\u51b3\u7b56\u3001\u5bf9\u6bd4\u5b66\u4e60\u7684\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u5bf9\u6297\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u52a8\u6001\u6b3a\u9a97\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u975e\u5e73\u7a33MIMO\u4fe1\u9053\u4e2d\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u4fe1\u9053\u5e73\u7a33\u6216\u89c2\u6d4b\u72ec\u7acb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u65f6\u7a7a\u76f8\u5173\u6027\u3001\u89c6\u8ddd\u963b\u585e\u548c\u52a8\u6001\u6b3a\u9a97\u7b56\u7565\u7684\u8ba4\u8bc1\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5e8f\u5217\u8d1d\u53f6\u65af\u51b3\u7b56\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u5bf9\u6297\u751f\u6210\u6a21\u578b\uff0c\u5229\u75282\u6001\u548c3\u6001\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u5728\u7ebf\u9002\u5e94\u7684\u79fb\u52a8\u5e73\u5747\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5bf9\u6570\u4f3c\u7136\u6bd4\u3001\u68c0\u6d4b\u6982\u7387\u548c\u7a33\u6001\u8fd1\u4f3c\u7684\u95ed\u5f0f\u9012\u63a8\u516c\u5f0f\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5e8f\u5217\u8ba4\u8bc1\u65b9\u6848\u5177\u6709\u663e\u8457\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002", "conclusion": "AR-PLA\u6846\u67b6\u5728\u52a8\u6001MIMO\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.20510", "pdf": "https://arxiv.org/pdf/2509.20510", "abs": "https://arxiv.org/abs/2509.20510", "authors": ["Petr Trunin", "Diana Cafiso", "Anderson Brazil Nardin", "Trevor Exley", "Lucia Beccai"], "title": "MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors", "categories": ["cs.RO"], "comment": "15 pages, 6 figures. SI 18 pages, 19 figures. Submitted to Wiley\n  Advanced Science", "summary": "The elephant trunk exemplifies a natural gripper where structure, actuation,\nand sensing are seamlessly integrated. Inspired by the distal morphology of the\nAfrican elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired\nGRipper with Optical Sensors, emphasizing sensing as an intrinsic,\nco-fabricated capability. Unlike multi-material or tendon-based approaches,\nMELEGROS directly integrates six optical waveguide sensors and five pneumatic\nchambers into a pneumatically actuated lattice structure (12.5 mm cell size)\nusing a single soft resin and one continuous 3D print. This eliminates\nmechanical mismatches between sensors, actuators, and body, reducing model\nuncertainty and enabling simulation-guided sensor design and placement. Only\nfour iterations were required to achieve the final prototype, which features a\ncontinuous structure capable of elongation, compression, and bending while\ndecoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than\ntwice its weight, performs bioinspired actions such as pinching, scooping, and\nreaching, and delicately grasps fragile items like grapes. The integrated\noptical sensors provide distinct responses to touch, bending, and chamber\ndeformation, enabling multifunctional perception. MELEGROS demonstrates a new\nparadigm for soft robotics where fully embedded sensing and continuous\nstructures inherently support versatile, bioinspired manipulation.", "AI": {"tldr": "MELEGROS\u662f\u4e00\u79cd\u53d7\u5927\u8c61\u9f3b\u5b50\u542f\u53d1\u7684\u5355\u7247\u5f0f\u8f6f\u4f53\u6293\u53d6\u5668\uff0c\u901a\u8fc7\u5149\u5b66\u4f20\u611f\u5668\u5b9e\u73b0\u591a\u529f\u80fd\u611f\u77e5\uff0c\u5177\u5907\u8fde\u7eed\u7ed3\u6784\u548c\u5b8c\u5168\u5d4c\u5165\u5f0f\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u53d7\u975e\u6d32\u5927\u8c61\u9f3b\u5b50\u7ed3\u6784\u4e0e\u529f\u80fd\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u96c6\u6210\u4f20\u611f\u3001\u9a71\u52a8\u548c\u7ed3\u6784\u7684\u8f6f\u4f53\u6293\u53d6\u5668\uff0c\u51cf\u5c11\u673a\u68b0\u4e0d\u5339\u914d\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5355\u79cd\u8f6f\u6811\u8102\u548c\u4e00\u6b21\u8fde\u7eed3D\u6253\u5370\uff0c\u5c06\u516d\u4e2a\u5149\u5b66\u6ce2\u5bfc\u4f20\u611f\u5668\u548c\u4e94\u4e2a\u6c14\u52a8\u8154\u96c6\u6210\u5230\u6c14\u52a8\u9a71\u52a8\u7684\u6676\u683c\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u4f20\u611f\u4e0e\u7ed3\u6784\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "MELEGROS\u91cd\u91cf132\u514b\uff0c\u53ef\u4e3e\u8d77\u8d85\u8fc7\u81ea\u8eab\u4e24\u500d\u7684\u91cd\u91cf\uff0c\u5b8c\u6210\u591a\u79cd\u4eff\u751f\u52a8\u4f5c\uff08\u5982\u634f\u53d6\u3001\u8200\u53d6\u548c\u4f38\u5c55\uff09\uff0c\u5e76\u80fd\u8f7b\u67d4\u6293\u53d6\u6613\u788e\u7269\u54c1\uff1b\u5149\u5b66\u4f20\u611f\u5668\u80fd\u591f\u533a\u5206\u89e6\u6478\u3001\u5f2f\u66f2\u548c\u6c14\u52a8\u8154\u53d8\u5f62\u4fe1\u53f7\u3002", "conclusion": "MELEGROS\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u9886\u57df\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5b8c\u5168\u5d4c\u5165\u5f0f\u4f20\u611f\u548c\u8fde\u7eed\u7ed3\u6784\u652f\u6301\u591a\u529f\u80fd\u4eff\u751f\u64cd\u4f5c\u3002"}}
{"id": "2509.21219", "pdf": "https://arxiv.org/pdf/2509.21219", "abs": "https://arxiv.org/abs/2509.21219", "authors": ["Amir Eshaghi Chaleshtori", "Abdollah Aghaie"], "title": "An enhanced statistical feature fusion approach using an improved distance evaluation algorithm and weighted K-nearest neighbor for bearing fault diagnosis", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "Bearings are among the most failure-prone components in rotating machinery,\nand their condition directly impacts overall performance. Therefore, accurately\ndiagnosing bearing faults is essential for ensuring system stability. However,\ndetecting such malfunctions in noisy environments, where data is collected from\nmultiple sensors, necessitates the extraction and selection of informative\nfeatures. This paper proposes an improved distance evaluation algorithm\ncombined with a weighted K-nearest neighbor (KNN) classifier for bearing fault\ndiagnosis. The process begins with extracting and integrating statistical\nfeatures of vibration across the time, frequency, and time-frequency domains.\nNext, the improved distance evaluation algorithm assigns weights to the\nextracted features, retaining only the most informative ones by eliminating\ninsensitive features. Finally, the selected features are used to train the\nweighted KNN classifier. To validate the proposed method, we employ bearing\ndata from the University of Ottawa. The results demonstrate the effectiveness\nof our approach in accurately identifying bearing faults.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6539\u8fdb\u8ddd\u79bb\u8bc4\u4f30\u7b97\u6cd5\u548c\u52a0\u6743KNN\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u73af\u5883\u4e2d\u51c6\u786e\u8bca\u65ad\u8f74\u627f\u6545\u969c\u3002", "motivation": "\u8f74\u627f\u662f\u65cb\u8f6c\u673a\u68b0\u4e2d\u6700\u6613\u635f\u574f\u7684\u90e8\u4ef6\uff0c\u5176\u72b6\u6001\u76f4\u63a5\u5f71\u54cd\u6574\u4f53\u6027\u80fd\uff0c\u56e0\u6b64\u5728\u566a\u58f0\u73af\u5883\u4e2d\u51c6\u786e\u8bca\u65ad\u8f74\u627f\u6545\u969c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u63d0\u53d6\u5e76\u6574\u5408\u65f6\u57df\u3001\u9891\u57df\u548c\u65f6\u9891\u57df\u7684\u632f\u52a8\u7edf\u8ba1\u7279\u5f81\uff0c\u7136\u540e\u7528\u6539\u8fdb\u7684\u8ddd\u79bb\u8bc4\u4f30\u7b97\u6cd5\u5bf9\u7279\u5f81\u52a0\u6743\u5e76\u7b5b\u9009\u51fa\u6700\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\uff0c\u6700\u540e\u7528\u52a0\u6743KNN\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u4f7f\u7528\u6e25\u592a\u534e\u5927\u5b66\u7684\u8f74\u627f\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u8f74\u627f\u6545\u969c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u80fd\u591f\u51c6\u786e\u8bca\u65ad\u8f74\u627f\u6545\u969c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.20516", "pdf": "https://arxiv.org/pdf/2509.20516", "abs": "https://arxiv.org/abs/2509.20516", "authors": ["Prasanna Sriganesh", "Barath Satheeshkumar", "Anushree Sabnis", "Matthew Travers"], "title": "Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation", "categories": ["cs.RO"], "comment": null, "summary": "For robots to operate autonomously in densely cluttered environments, they\nmust reason about and potentially physically interact with obstacles to clear a\npath. Safely clearing a path on challenging terrain, such as a cluttered\nstaircase, requires controlled interaction. For example, a quadrupedal robot\nthat pushes objects out of the way with one leg while maintaining a stable\nstance with its three other legs. However, tightly coupled physical actions,\nsuch as one-legged pushing, create new constraints on the system that can be\ndifficult to predict at design time. In this work, we present a new method that\naddresses one such constraint, wherein the object being pushed by a quadrupedal\nrobot with one of its legs becomes occluded from the robot's sensors during\nmanipulation. To address this challenge, we present a tightly coupled\nperception-action framework that enables the robot to perceive clutter, reason\nabout feasible push paths, and execute the clearing maneuver. Our core\ncontribution is an interaction-aware state estimation loop that uses\nproprioceptive feedback regarding foot contact and leg position to predict an\nobject's displacement during the occlusion. This prediction guides the\nperception system to robustly re-detect the object after the interaction,\nclosing the loop between action and sensing to enable accurate tracking even\nafter partial pushes. Using this feedback allows the robot to learn from\nphysical outcomes, reclassifying an object as immovable if a push fails due to\nit being too heavy. We present results of implementing our approach on a Boston\nDynamics Spot robot that show our interaction-aware approach achieves higher\ntask success rates and tracking accuracy in pushing objects on stairs compared\nto open-loop baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u5b89\u5168\u63a8\u52a8\u7269\u4f53\u7684\u611f\u77e5-\u884c\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u72b6\u6001\u4f30\u8ba1\u89e3\u51b3\u7269\u4f53\u88ab\u906e\u6321\u65f6\u7684\u8ddf\u8e2a\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u4e0e\u969c\u788d\u7269\u4e92\u52a8\u4ee5\u6e05\u7406\u8def\u5f84\uff0c\u4f46\u7269\u7406\u52a8\u4f5c\uff08\u5982\u5355\u817f\u63a8\uff09\u53ef\u80fd\u5f15\u53d1\u96be\u4ee5\u9884\u6d4b\u7684\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7269\u4f53\u5728\u63a8\u8fc7\u7a0b\u4e2d\u56e0\u906e\u6321\u800c\u65e0\u6cd5\u88ab\u4f20\u611f\u5668\u611f\u77e5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u5bc6\u8026\u5408\u7684\u611f\u77e5-\u884c\u52a8\u6846\u67b6\uff0c\u5229\u7528\u672c\u4f53\u611f\u77e5\u53cd\u9988\uff08\u5982\u811a\u90e8\u63a5\u89e6\u548c\u817f\u90e8\u4f4d\u7f6e\uff09\u9884\u6d4b\u88ab\u63a8\u7269\u4f53\u7684\u4f4d\u79fb\uff0c\u5e76\u5728\u7269\u4f53\u91cd\u65b0\u51fa\u73b0\u540e\u91cd\u65b0\u68c0\u6d4b\u4ee5\u95ed\u73af\u8ddf\u8e2a\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u52a8\u529bSpot\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u697c\u68af\u63a8\u7269\u4efb\u52a1\u4e2d\u6bd4\u5f00\u73af\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u72b6\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u53cd\u9988\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u63a8\u7269\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7269\u7406\u4ea4\u4e92\u4e2d\u611f\u77e5\u4e0e\u884c\u52a8\u7d27\u5bc6\u8026\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.21290", "pdf": "https://arxiv.org/pdf/2509.21290", "abs": "https://arxiv.org/abs/2509.21290", "authors": ["Tianqi Mao", "Jiayue Liu", "Weijie Liu", "Dezhi Zheng", "Zhaocheng Wang"], "title": "Vision-Intelligence-Enabled Beam Tracking for Cross-Interface Water-Air Optical Wireless Communications", "categories": ["eess.SP"], "comment": null, "summary": "The escalating development of oceanic applications like underwater\nsurveillance and mineral exploration, is motivating real-time wireless backhaul\nof the considerable observation data. Such prospects can be hardly realized by\nthe narrowband acoustic approach. Alternatively, optical wireless communication\n(OWC) has emerged as a promising solution for maritime and underwater\napplications due to its great potential for broadband underwater transmission.\nHowever, the implementations of water-air OWC can be rather challenging,\nespecially when penetrating the fluctuating interface, where the direction of\nrefracted signals changes dynamically, causing severe beam misalignment with\nairborne stations. This has necessitated real-time transceiver alignment\nadaptable to the sophisticated oceanic environment, which has yet to be\naddressed. Against this background, this paper establishes a mathematical\nchannel model for water-air optical wireless transmission across the\nfluctuating sea surface. Based on the model, we propose a vision-based beam\ntracking algorithm that leverages artificial intelligence (AI) methods for\ndynamic channel prediction. The proposed algorithm integrates a convolutional\nneural network (CNN) with bi-directional long short-term memory (Bi-LSTM),\nwhich further incorporates the attention mechanism to effectively extract\ncritical spatio-temporal features from the vision data. The numerical\nsimulation results show that the proposed algorithm can outperform its\nclassical counterparts in maintaining receiving signal strength and supressing\nthe vision noises, which demonstrates its robustness against the the harsh\nconditions of water-air OWC systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6c34-\u7a7a\u5149\u65e0\u7ebf\u901a\u4fe1\u4e2d\u56e0\u6ce2\u52a8\u6d77\u9762\u5bfc\u81f4\u7684\u5149\u675f\u5bf9\u51c6\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u89c6\u89c9\u5149\u675f\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u7ed3\u5408CNN\u548cBi-LSTM\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u53d6\u5173\u952e\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u53f7\u5f3a\u5ea6\u548c\u6297\u566a\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6d77\u6d0b\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff08\u5982\u6c34\u4e0b\u76d1\u6d4b\u548c\u77ff\u4ea7\u52d8\u63a2\uff09\uff0c\u5b9e\u65f6\u65e0\u7ebf\u56de\u4f20\u5927\u91cf\u89c2\u6d4b\u6570\u636e\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u7a84\u5e26\u58f0\u5b66\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\uff0c\u800c\u5149\u65e0\u7ebf\u901a\u4fe1\uff08OWC\uff09\u56e0\u5176\u5bbd\u5e26\u6f5c\u529b\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u4f46\u6c34-\u7a7a\u754c\u9762\u7684\u6ce2\u52a8\u5bfc\u81f4\u4fe1\u53f7\u52a8\u6001\u6298\u5c04\uff0c\u5f15\u53d1\u5149\u675f\u4e0d\u5bf9\u51c6\u95ee\u9898\uff0c\u4e9f\u9700\u80fd\u591f\u9002\u5e94\u590d\u6742\u6d77\u6d0b\u73af\u5883\u7684\u5b9e\u65f6\u5bf9\u51c6\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5efa\u7acb\u4e86\u6ce2\u52a8\u6d77\u9762\u4e0b\u6c34-\u7a7a\u5149\u65e0\u7ebf\u4f20\u8f93\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u89c6\u89c9\u5149\u675f\u8ddf\u8e2a\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u7ed3\u5408CNN\u548cBi-LSTM\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u9ad8\u6548\u63d0\u53d6\u5173\u952e\u65f6\u7a7a\u7279\u5f81\uff0c\u9884\u6d4b\u52a8\u6001\u4fe1\u9053\u53d8\u5316\u3002", "result": "\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4fdd\u6301\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u548c\u6291\u5236\u89c6\u89c9\u566a\u58f0\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u6c34-\u7a7aOWC\u7cfb\u7edf\u6076\u52a3\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6c34-\u7a7a\u5149\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5149\u675f\u5bf9\u51c6\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u52a8\u6001\u9884\u6d4b\u548c\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u6d77\u6d0b\u5e94\u7528\u7684\u5b9e\u65f6\u6570\u636e\u4f20\u8f93\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.20541", "pdf": "https://arxiv.org/pdf/2509.20541", "abs": "https://arxiv.org/abs/2509.20541", "authors": ["Anujith Muraleedharan", "Anamika J H"], "title": "Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning", "categories": ["cs.RO", "I.2.9; I.2.6; I.2.8"], "comment": "Preprint. 8 pages, 3 figures, 1 table, 1 algorithm. CoRL 2025 style\n  (preprint). Code/data to be released", "summary": "Human feedback can greatly accelerate robot learning, but in real-world\nsettings, such feedback is costly and limited. Existing human-in-the-loop\nreinforcement learning (HiL-RL) methods often assume abundant feedback,\nlimiting their practicality for physical robot deployment. In this work, we\nintroduce SPARQ, a progress-aware query policy that requests feedback only when\nlearning stagnates or worsens, thereby reducing unnecessary oracle calls. We\nevaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing\nagainst three baselines: no feedback, random querying, and always querying. Our\nexperiments show that SPARQ achieves near-perfect task success, matching the\nperformance of always querying while consuming about half the feedback budget.\nIt also provides more stable and efficient learning than random querying, and\nsignificantly improves over training without feedback. These findings suggest\nthat selective, progress-based query strategies can make HiL-RL more efficient\nand scalable for robots operating under realistic human effort constraints.", "AI": {"tldr": "SPARQ\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u8fdb\u5ea6\u7684\u67e5\u8be2\u7b56\u7565\uff0c\u901a\u8fc7\u4ec5\u5728\u5b66\u4e60\u505c\u6ede\u6216\u9000\u6b65\u65f6\u8bf7\u6c42\u53cd\u9988\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u53cd\u9988\u8c03\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4eba\u7c7b\u53cd\u9988\u6210\u672c\u9ad8\u6602\u4e14\u6709\u9650\uff0c\u73b0\u6709HiL-RL\u65b9\u6cd5\u5047\u8bbe\u53cd\u9988\u5145\u8db3\uff0c\u9650\u5236\u4e86\u5728\u7269\u7406\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165SPARQ\u7b56\u7565\uff0c\u4ec5\u5728\u5b66\u4e60\u505c\u6ede\u6216\u9000\u6b65\u65f6\u8bf7\u6c42\u53cd\u9988\uff1b\u5728\u6a21\u62dfUR5\u7acb\u65b9\u4f53\u62fe\u53d6\u4efb\u52a1\u4e2d\u4e0e\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff08\u65e0\u53cd\u9988\u3001\u968f\u673a\u67e5\u8be2\u3001\u59cb\u7ec8\u67e5\u8be2\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "SPARQ\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u63a5\u8fd1\u59cb\u7ec8\u67e5\u8be2\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u6d88\u8017\u7ea6\u4e00\u534a\u7684\u53cd\u9988\u9884\u7b97\uff1b\u76f8\u6bd4\u968f\u673a\u67e5\u8be2\u548c\u65e0\u53cd\u9988\uff0c\u5b66\u4e60\u66f4\u7a33\u5b9a\u9ad8\u6548\u3002", "conclusion": "\u9009\u62e9\u6027\u3001\u57fa\u4e8e\u8fdb\u5ea6\u7684\u67e5\u8be2\u7b56\u7565\u53ef\u4ee5\u63d0\u5347HiL-RL\u5728\u73b0\u5b9e\u4eba\u7c7b\u52aa\u529b\u7ea6\u675f\u4e0b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.21311", "pdf": "https://arxiv.org/pdf/2509.21311", "abs": "https://arxiv.org/abs/2509.21311", "authors": ["Orestis Kaparounakis", "Phillip Stanley-Marbell"], "title": "Efficient Digital Methods to Quantify Sensor Output Uncertainty", "categories": ["eess.SP", "I.4.1; G.3; E.4; H.3"], "comment": null, "summary": "Accurate characterization of sensor output uncertainty is important for\nreliable data interpretation in many applications. Here, we investigate the\nimpact of transducer-level measurement uncertainty on overall sensor\nmeasurement accuracy due to limited-precision information about sensor\ncomponents. We explain our method using thermopile-based sensors as an example\nclass of sensors. We show how sensor calibration and conversion equations,\nwhich are an essential part of all sensing systems, propagate uncertainties\nresulting from the quantization of calibration parameters, to the final,\ncompensated sensor output. The experimental results show that the epistemic\nuncertainty of calibration-related quantities leads to absolute error in the\nsensor output as high as 5.3 {\\deg}C (and relative error as high as 25.7%) for\none commonly-used thermopile sensor. In one instance of using the epistemic\nuncertainty information in edge detection, we show reduction of false-positives\nedges to zero for the conventional Canny operator, while maintaining accuracy.\nWe show these ideas are practical and possible on actual embedded sensor\nsystems by prototyping them on two commercially-available uncertainty tracking\nhardware platforms, one with average power dissipation 16.7 mW and 42.9x\nspeedup compared to the equal-confidence Monte Carlo computation (the status\nquo), and the other with average power dissipation 147.15 mW and 94.4x speedup,\npaving the way for use in real time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f20\u611f\u5668\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u8868\u5f81\u53ca\u5176\u5bf9\u6d4b\u91cf\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u6821\u51c6\u53c2\u6570\u91cf\u5316\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u3002\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u6821\u51c6\u76f8\u5173\u91cf\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5bf9\u4f20\u611f\u5668\u8f93\u51fa\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5d4c\u5165\u5f0f\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u5b9e\u9645\u5e94\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u51c6\u786e\u8868\u5f81\u4f20\u611f\u5668\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u5bf9\u6570\u636e\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u7531\u4e8e\u4f20\u611f\u5668\u7ec4\u4ef6\u7cbe\u5ea6\u9650\u5236\u5bfc\u81f4\u7684\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u5bf9\u6574\u4f53\u4f20\u611f\u5668\u6d4b\u91cf\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u4ee5\u70ed\u7535\u5806\u4f20\u611f\u5668\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u6821\u51c6\u548c\u8f6c\u6362\u65b9\u7a0b\u5728\u4f20\u64ad\u6821\u51c6\u53c2\u6570\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4e2d\u7684\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5728\u4e24\u79cd\u5546\u4e1a\u5316\u786c\u4ef6\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6821\u51c6\u76f8\u5173\u91cf\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u5bfc\u81f4\u9ad8\u8fbe5.3\u00b0C\u7684\u7edd\u5bf9\u8bef\u5dee\u548c25.7%\u7684\u76f8\u5bf9\u8bef\u5dee\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fb9\u7f18\u68c0\u6d4b\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u62a5\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u529f\u8017\u548c\u901f\u5ea6\u4f18\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u5e94\u7528\u4e2d\u4f20\u611f\u5668\u4e0d\u786e\u5b9a\u6027\u7684\u8ddf\u8e2a\u548c\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.20550", "pdf": "https://arxiv.org/pdf/2509.20550", "abs": "https://arxiv.org/abs/2509.20550", "authors": ["Srinidhi Kalgundi Srinivas", "Yash Shukla", "Adam Arnold", "Sachin Chitta"], "title": "GraspFactory: A Large Object-Centric Grasping Dataset", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic grasping is a crucial task in industrial automation, where robots are\nincreasingly expected to handle a wide range of objects. However, a significant\nchallenge arises when robot grasping models trained on limited datasets\nencounter novel objects. In real-world environments such as warehouses or\nmanufacturing plants, the diversity of objects can be vast, and grasping models\nneed to generalize to this diversity. Training large, generalizable\nrobot-grasping models requires geometrically diverse datasets. In this paper,\nwe introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps\ncollectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85\ngrippers (with 33,710 objects). GraspFactory is designed for training\ndata-intensive models, and we demonstrate the generalization capabilities of\none such model trained on a subset of GraspFactory in both simulated and\nreal-world settings. The dataset and tools are made available for download at\nhttps://graspfactory.github.io/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86GraspFactory\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc71.09\u4ebf\u4e2a6\u81ea\u7531\u5ea6\u6293\u53d6\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u901a\u7528\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u96c6\u8bad\u7ec3\u540e\u96be\u4ee5\u9002\u5e94\u65b0\u7269\u4f53\u7684\u95ee\u9898\u3002", "method": "\u521b\u5efa\u5927\u89c4\u6a21\u3001\u51e0\u4f55\u591a\u6837\u5316\u7684\u6570\u636e\u96c6GraspFactory\uff0c\u5e76\u8bad\u7ec3\u6570\u636e\u5bc6\u96c6\u578b\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GraspFactory\u6570\u636e\u96c6\u548c\u5de5\u5177\u516c\u5f00\u63d0\u4f9b\uff0c\u4fc3\u8fdb\u673a\u5668\u4eba\u6293\u53d6\u7814\u7a76\u3002"}}
{"id": "2509.20382", "pdf": "https://arxiv.org/pdf/2509.20382", "abs": "https://arxiv.org/abs/2509.20382", "authors": ["Dilli Hang Rai", "Sabin Kafley"], "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation", "categories": ["cs.CR", "cs.AI", "cs.LG", "eess.SP"], "comment": "5 pages, 7 figures, 5 tables", "summary": "ECG biometrics offer a unique, secure authentication method, yet their\ndeployment on wearable devices faces real-time processing, privacy, and\nspoofing vulnerability challenges. This paper proposes a lightweight deep\nlearning model (MobileNetV1+GRU) for ECG-based authentication, injection of\n20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and\nedge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving\naccuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,\n0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of\n0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,\n0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,\nwhile under FGSM adversarial attacks, accuracy drops from 96.82% to as low as\n0.80%. This paper highlights federated learning, adversarial testing, and the\nneed for diverse wearable physiological datasets to ensure secure and scalable\nbiometrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MobileNetV1+GRU\uff09\u7528\u4e8eECG\u751f\u7269\u8ba4\u8bc1\uff0c\u5728\u6a21\u62df\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5bf9\u6297\u6027\u653b\u51fb\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5fc3\u7535\u56fe\u751f\u7269\u8bc6\u522b\u6280\u672f\u5177\u6709\u72ec\u7279\u6027\uff0c\u4f46\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u9762\u4e34\u5b9e\u65f6\u6027\u3001\u9690\u79c1\u548c\u6b3a\u9a97\u6f0f\u6d1e\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528MobileNetV1+GRU\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u548c\u81ea\u5b9a\u4e49\u9884\u5904\u7406\uff0c\u6a21\u62df\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u73af\u5883\u548c\u8fb9\u7f18\u90e8\u7f72\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff08\u6700\u9ad899.34%\uff09\uff0c\u4f46\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u81f30.80%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8054\u90a6\u5b66\u4e60\u3001\u5bf9\u6297\u6027\u6d4b\u8bd5\u4ee5\u53ca\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u751f\u7269\u8bc6\u522b\u7684\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.20593", "pdf": "https://arxiv.org/pdf/2509.20593", "abs": "https://arxiv.org/abs/2509.20593", "authors": ["Song Ma", "Richard Bucknall", "Yuanchang Liu"], "title": "Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles", "categories": ["cs.RO"], "comment": "Accepted for presentation at Oceantech: Marine Robotics & Science\n  Workshop, IROS 2025", "summary": "This paper proposes an uncertainty-aware marine pollution source tracking\nframework for unmanned surface vehicles (USVs). By integrating high-fidelity\nmarine pollution dispersion simulation with informative path planning\ntechniques, we demonstrate effective identification of pollution sources in\nmarine environments. The proposed approach is implemented based on Robot\nOperating System (ROS), processing real-time sensor data to update\nprobabilistic source location estimates. The system progressively refines the\nestimation of source location while quantifying uncertainty levels in its\npredictions. Experiments conducted in simulated environments with varying\nsource locations, flow conditions, and starting positions demonstrate the\nframework's ability to localise pollution sources with high accuracy. Results\nshow that the proposed approach achieves reliable source localisation\nefficiently. This work contributes to the development of full autonomous\nenvironmental monitoring capabilities essential for rapid response to marine\npollution incidents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u6c34\u9762\u8247\uff08USV\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6d77\u6d0b\u6c61\u67d3\u6e90\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u4fdd\u771f\u6c61\u67d3\u6269\u6563\u6a21\u62df\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6d77\u6d0b\u6c61\u67d3\u6e90\u7684\u6709\u6548\u5b9a\u4f4d\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5168\u81ea\u4e3b\u7684\u73af\u5883\u76d1\u6d4b\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u6d77\u6d0b\u6c61\u67d3\u4e8b\u4ef6\u7684\u5feb\u901f\u54cd\u5e94\u3002", "method": "\u57fa\u4e8eROS\u5b9e\u73b0\uff0c\u6574\u5408\u9ad8\u4fdd\u771f\u6c61\u67d3\u6269\u6563\u6a21\u62df\u548c\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u6280\u672f\uff0c\u5b9e\u65f6\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u4ee5\u66f4\u65b0\u6c61\u67d3\u6e90\u4f4d\u7f6e\u7684\u6982\u7387\u4f30\u8ba1\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5b9a\u4f4d\u6c61\u67d3\u6e90\uff0c\u5e76\u63d0\u4f9b\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d77\u6d0b\u6c61\u67d3\u6e90\u7684\u5feb\u901f\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5168\u81ea\u4e3b\u73af\u5883\u76d1\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.20460", "pdf": "https://arxiv.org/pdf/2509.20460", "abs": "https://arxiv.org/abs/2509.20460", "authors": ["Andrew Campbell", "Anna Scaglione", "Hang Liu", "Victor Elvira", "Sean Peisert", "Daniel Arnold"], "title": "Differential Privacy of Network Parameters from a System Identification Perspective", "categories": ["cs.CR", "eess.SP"], "comment": null, "summary": "This paper addresses the problem of protecting network information from\nprivacy system identification (SI) attacks when sharing cyber-physical system\nsimulations. We model analyst observations of networked states as time-series\noutputs of a graph filter driven by differentially private (DP) nodal\nexcitations, with the analyst aiming to infer the underlying graph shift\noperator (GSO). Unlike traditional SI, which estimates system parameters, we\nstudy the inverse problem: what assumptions prevent adversaries from\nidentifying the GSO while preserving utility for legitimate analysis. We show\nthat applying DP mechanisms to inputs provides formal privacy guarantees for\nthe GSO, linking the $(\\epsilon,\\delta)$-DP bound to the spectral properties of\nthe graph filter and noise covariance. More precisely, for DP Gaussian signals,\nthe spectral characteristics of both the filter and noise covariance determine\nthe privacy bound, with smooth filters and low-condition-number covariance\nyielding greater privacy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7f51\u7edc\u4fe1\u606f\u5171\u4eab\u4e2d\u5982\u4f55\u4fdd\u62a4\u9690\u79c1\u514d\u53d7\u7cfb\u7edf\u8bc6\u522b\u653b\u51fb\uff0c\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u673a\u5236\u4e3a\u56fe\u6ee4\u6ce2\u5668\u63d0\u4f9b\u9690\u79c1\u4fdd\u969c\u3002", "motivation": "\u89e3\u51b3\u5728\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u4e2d\u5171\u4eab\u4fe1\u606f\u65f6\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u9632\u6b62\u7cfb\u7edf\u8bc6\u522b\u653b\u51fb\u63a8\u65ad\u51fa\u5e95\u5c42\u56fe\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u673a\u5236\u5bf9\u8282\u70b9\u6fc0\u52b1\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u5c06\u5176\u5efa\u6a21\u4e3a\u56fe\u6ee4\u6ce2\u5668\u7684\u65f6\u95f4\u5e8f\u5217\u8f93\u51fa\uff0c\u5206\u6790\u5176\u8c31\u7279\u6027\u5bf9\u9690\u79c1\u4fdd\u969c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u9690\u79c1\u754c\u9650\u4e0e\u56fe\u6ee4\u6ce2\u5668\u548c\u566a\u58f0\u534f\u65b9\u5dee\u7684\u8c31\u7279\u6027\u76f8\u5173\uff0c\u5e73\u6ed1\u6ee4\u6ce2\u5668\u548c\u4f4e\u6761\u4ef6\u6570\u534f\u65b9\u5dee\u53ef\u63d0\u4f9b\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u673a\u5236\u53ef\u4ee5\u6709\u6548\u4fdd\u62a4\u56fe\u7ed3\u6784\u7684\u9690\u79c1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4fe1\u606f\u7684\u5b9e\u7528\u6027\uff0c\u8c31\u7279\u6027\u662f\u9690\u79c1\u4fdd\u969c\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.20623", "pdf": "https://arxiv.org/pdf/2509.20623", "abs": "https://arxiv.org/abs/2509.20623", "authors": ["Satyajeet Das", "Darren Chiu", "Zhehui Huang", "Lars Lindemann", "Gaurav S. Sukhatme"], "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning has enabled significant progress in complex domains\nsuch as coordinating and navigating multiple quadrotors. However, even\nwell-trained policies remain vulnerable to collisions in obstacle-rich\nenvironments. Addressing these infrequent but critical safety failures through\nretraining or fine-tuning is costly and risks degrading previously learned\nskills. Inspired by activation steering in large language models and latent\nediting in computer vision, we introduce a framework for inference-time Latent\nActivation Editing (LAE) that refines the behavior of pre-trained policies\nwithout modifying their weights or architecture. The framework operates in two\nstages: (i) an online classifier monitors intermediate activations to detect\nstates associated with undesired behaviors, and (ii) an activation editing\nmodule that selectively modifies flagged activations to shift the policy\ntowards safer regimes. In this work, we focus on improving safety in\nmulti-quadrotor navigation. We hypothesize that amplifying a policy's internal\nperception of risk can induce safer behaviors. We instantiate this idea through\na latent collision world model trained to predict future pre-collision\nactivations, thereby prompting earlier and more cautious avoidance responses.\nExtensive simulations and real-world Crazyflie experiments demonstrate that LAE\nachieves statistically significant reduction in collisions (nearly 90% fewer\ncumulative collisions compared to the unedited baseline) and substantially\nincreases the fraction of collision-free trajectories, while preserving task\ncompletion. More broadly, our results establish LAE as a lightweight paradigm,\nfeasible on resource-constrained hardware, for post-deployment refinement of\nlearned robot policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAE\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\u7f16\u8f91\u6f5c\u5728\u6fc0\u6d3b\u6765\u63d0\u5347\u591a\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u78b0\u649e\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\u4ecd\u5bb9\u6613\u53d1\u751f\u78b0\u649e\u3002\u76f4\u63a5\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u5f71\u54cd\u5df2\u6709\u6280\u80fd\u3002\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u5bfc\u5411\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6f5c\u5728\u7f16\u8f91\u542f\u53d1\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63a8\u7406\u65f6\u95f4\u6f5c\u5728\u6fc0\u6d3b\u7f16\u8f91\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "LAE\u6846\u67b6\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u5728\u7ebf\u5206\u7c7b\u5668\u76d1\u6d4b\u6fc0\u6d3b\u72b6\u6001\u4ee5\u68c0\u6d4b\u5371\u9669\uff0c\u7f16\u8f91\u6a21\u5757\u9009\u62e9\u6027\u4fee\u6539\u5371\u9669\u72b6\u6001\u7684\u6fc0\u6d3b\u4ee5\u4fc3\u8fdb\u884c\u4e3a\u5b89\u5168\u3002\u7814\u7a76\u4e2d\u901a\u8fc7\u6f5c\u5728\u78b0\u649e\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u672a\u6765\u78b0\u649e\u524d\u7684\u6fc0\u6d3b\uff0c\u5f15\u5bfc\u66f4\u65e9\u89c4\u907f\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cLAE\u663e\u8457\u51cf\u5c11\u4e86\u8fd190%\u7684\u78b0\u649e\uff0c\u63d0\u9ad8\u4e86\u65e0\u78b0\u649e\u8f68\u8ff9\u6bd4\u4f8b\uff0c\u540c\u65f6\u4efb\u52a1\u5b8c\u6210\u7387\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "LAE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u4f18\u5316\u5df2\u90e8\u7f72\u673a\u5668\u4eba\u7b56\u7565\uff0c\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2509.20511", "pdf": "https://arxiv.org/pdf/2509.20511", "abs": "https://arxiv.org/abs/2509.20511", "authors": ["Oscar Leong", "Yann Traonmilin"], "title": "A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm", "categories": ["cs.LG", "eess.SP", "math.OC"], "comment": null, "summary": "Recovering high-dimensional signals from corrupted measurements is a central\nchallenge in inverse problems. Recent advances in generative diffusion models\nhave shown remarkable empirical success in providing strong data-driven priors,\nbut rigorous recovery guarantees remain limited. In this work, we develop a\ntheoretical framework for analyzing deterministic diffusion-based algorithms\nfor inverse problems, focusing on a deterministic version of the algorithm\nproposed by Kadkhodaie \\& Simoncelli \\cite{kadkhodaie2021stochastic}. First, we\nshow that when the underlying data distribution concentrates on a\nlow-dimensional model set, the associated noise-convolved scores can be\ninterpreted as time-varying projections onto such a set. This leads to\ninterpreting previous algorithms using diffusion priors for inverse problems as\ngeneralized projected gradient descent methods with varying projections. When\nthe sensing matrix satisfies a restricted isometry property over the model set,\nwe can derive quantitative convergence rates that depend explicitly on the\nnoise schedule. We apply our framework to two instructive data distributions:\nuniform distributions over low-dimensional compact, convex sets and low-rank\nGaussian mixture models. In the latter setting, we can establish global\nconvergence guarantees despite the nonconvexity of the underlying model set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8e\u786e\u5b9a\u6027\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4f4e\u7ef4\u6a21\u578b\u96c6\u4e0a\u7684\u6536\u655b\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u4f4e\u7ef4\u51f8\u96c6\u548c\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4e25\u683c\u7684\u6062\u590d\u4fdd\u8bc1\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6846\u67b6\u5206\u6790\u786e\u5b9a\u6027\u6269\u6563\u7b97\u6cd5\uff0c\u5c06\u5176\u89c6\u4e3a\u5e7f\u4e49\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u53d7\u9650\u7b49\u8ddd\u6027\u8d28\u4e0b\uff0c\u5b9a\u91cf\u6536\u655b\u901f\u7387\u4f9d\u8d56\u566a\u58f0\u8c03\u5ea6\uff1b\u5728\u4f4e\u7ef4\u51f8\u96c6\u548c\u975e\u51f8\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e2d\u5747\u80fd\u6536\u655b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u9002\u7528\u4e8e\u975e\u51f8\u95ee\u9898\u3002"}}
{"id": "2509.20635", "pdf": "https://arxiv.org/pdf/2509.20635", "abs": "https://arxiv.org/abs/2509.20635", "authors": ["Matheus P. Angarola", "Francisco Affonso", "Marcelo Becker"], "title": "Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Legged robots must exhibit robust and agile locomotion across diverse,\nunstructured terrains, a challenge exacerbated under blind locomotion settings\nwhere terrain information is unavailable. This work introduces a hierarchical\nreinforcement learning framework that leverages terrain-specialized policies\nand curriculum learning to enhance agility and tracking performance in complex\nenvironments. We validated our method on simulation, where our approach\noutperforms a generalist policy by up to 16% in success rate and achieves lower\ntracking errors as the velocity target increases, particularly on low-friction\nand discontinuous terrains, demonstrating superior adaptability and robustness\nacross mixed-terrain scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u5f62\u4e13\u7cbe\u7b56\u7565\u548c\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u589e\u5f3a\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u5730\u5f62\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5730\u5f62\u4e13\u7cbe\u7b56\u7565\u548c\u8bfe\u7a0b\u5b66\u4e60\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u901a\u7528\u7b56\u7565\u6210\u529f\u7387\u63d0\u9ad816%\uff0c\u5e76\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5c42\u6b21\u5316\u5f3a\u5316\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u6df7\u5408\u5730\u5f62\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.20963", "pdf": "https://arxiv.org/pdf/2509.20963", "abs": "https://arxiv.org/abs/2509.20963", "authors": ["Yuyang Hu", "Michael Brown", "Didem Dogan", "Mah\u00e9 Bulot", "Maxime Cheppe", "Guillaume Ferin", "Geert Leus", "Antonius F. W. van der Steen", "Pieter Kruizinga", "Johannes G. Bosch"], "title": "4D Computational Ultrasound Imaging of Carotid Artery Flow", "categories": ["physics.med-ph", "eess.SP"], "comment": "The submission consists of an 11-page manuscript with 5 figures,\n  followed by 2 pages of supplemental files", "summary": "Computational ultrasound imaging (cUSi) with few elements and spatial field\nencoding can provide high-resolution volumetric B-mode imaging. In this work,\nwe extend its application to 4D carotid artery (CA) flow imaging using a custom\nlarge-aperture 240-element matrix probe. We implemented a frequency band-based\nmatched filtering strategy that balances resolution and contrast. The system's\ninherent imaging capabilities were evaluated and validated in flow phantom and\nhuman CA experiments. In the phantom study, 3D/4D power Doppler image and\nspeckle-tracking analyses confirmed the system's ability to resolve flow\nstructures and hemodynamics. In the human study, the CA bifurcation flow\nstructure and its local pulsatile flow dynamics were successfully\nreconstructed. These results demonstrate the feasibility of using a\nlarge-footprint, few-element cUSi system for 4D CA flow assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u8ba1\u7b97\u8d85\u58f0\u6210\u50cf\uff08cUSi\uff09\u57284D\u9888\u52a8\u8109\u8840\u6d41\u6210\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528240\u5143\u7d20\u77e9\u9635\u63a2\u5934\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u6a21\u62df\u548c\u4eba\u4f53\u5b9e\u9a8c\u4e2d\u7684\u6210\u50cf\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5927\u5b54\u5f84\u3001\u5c11\u5143\u7d20\u7684\u8ba1\u7b97\u8d85\u58f0\u6210\u50cf\u7cfb\u7edf\u57284D\u9888\u52a8\u8109\u8840\u6d41\u8bc4\u4f30\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u9891\u7387\u5e26\u5339\u914d\u6ee4\u6ce2\u7b56\u7565\uff0c\u7ed3\u5408240\u5143\u7d20\u77e9\u9635\u63a2\u5934\uff0c\u8fdb\u884c\u6a21\u62df\u548c\u4eba\u4f53\u9888\u52a8\u8109\u5b9e\u9a8c\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u91cd\u5efa\u4e86\u9888\u52a8\u8109\u5206\u53c9\u5904\u7684\u8840\u6d41\u7ed3\u6784\u548c\u5c40\u90e8\u8109\u52a8\u52a8\u529b\u5b66\uff0c\u9a8c\u8bc1\u4e86\u5176\u6210\u50cf\u80fd\u529b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5c11\u5143\u7d20cUSi\u7cfb\u7edf\u57284D\u8840\u6d41\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.20646", "pdf": "https://arxiv.org/pdf/2509.20646", "abs": "https://arxiv.org/abs/2509.20646", "authors": ["Sun Zhaole", "Xiaofeng Mao", "Jihong Zhu", "Yuanlong Zhang", "Robert B. Fisher"], "title": "Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation", "categories": ["cs.RO"], "comment": "An IEEE conference paper currently under review", "summary": "Dexterous in-hand manipulation remains a foundational challenge in robotics,\nwith progress often constrained by the prevailing paradigm of imitating the\nhuman hand. This anthropomorphic approach creates two critical barriers: 1) it\nlimits robotic capabilities to tasks humans can already perform, and 2) it\nmakes data collection for learning-based methods exceedingly difficult. Both\nchallenges are caused by traditional force-closure which requires coordinating\ncomplex, multi-point contacts based on friction, normal force, and gravity to\ngrasp an object. This makes teleoperated demonstrations unstable and amplifies\nthe sim-to-real gap for reinforcement learning. In this work, we propose a\nparadigm shift: moving away from replicating human mechanics toward the design\nof novel robotic embodiments. We introduce the \\textbf{S}uction\n\\textbf{Leap}-Hand (SLeap Hand), a multi-fingered hand featuring integrated\nfingertip suction cups that realize a new form of suction-enabled dexterity. By\nreplacing complex force-closure grasps with stable, single-point adhesion, our\ndesign fundamentally simplifies in-hand teleoperation and facilitates the\ncollection of high-quality demonstration data. More importantly, this\nsuction-based embodiment unlocks a new class of dexterous skills that are\ndifficult or even impossible for the human hand, such as one-handed paper\ncutting and in-hand writing. Our work demonstrates that by moving beyond\nanthropomorphic constraints, novel embodiments can not only lower the barrier\nfor collecting robust manipulation data but also enable the stable,\nsingle-handed completion of tasks that would typically require two human hands.\nOur webpage is https://sites.google.com/view/sleaphand.", "AI": {"tldr": "\u793a\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u624b\u52bf\u8bbe\u8ba1SLeap Hand\uff0c\u901a\u8fc7\u5438\u76d8\u66ff\u4ee3\u4f20\u7edf\u529b\u95ed\u5408\u6280\u672f\uff0c\u57fa\u672c\u4e0a\u7b80\u5316\u4e86\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\uff0c\u5e76\u5f00\u542f\u4e86\u4eba\u624b\u65e0\u6cd5\u5b9e\u73b0\u7684\u65b0\u6280\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u624b\u4eff\u771f\u65b9\u5f0f\u5bfc\u81f4\u64cd\u4f5c\u4e0d\u7a33\u5b9a\u548c\u6570\u636e\u6536\u96c6\u56f0\u96be\uff0c\u6240\u4ee5\u9700\u8981\u65b0\u7684\u8bbe\u8ba1\u6765\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faSLeap Hand\uff0c\u91c7\u7528\u6302\u7ed3\u5438\u76d8\u66ff\u4ee3\u529b\u95ed\u5408\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "\u65b0\u8bbe\u8ba1\u7b80\u5316\u4e86\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\uff0c\u540c\u65f6\u5f00\u542f\u4e86\u4eba\u624b\u65e0\u6cd5\u5b9e\u73b0\u7684\u65b0\u6280\u80fd\u3002", "conclusion": "\u8d85\u8d8a\u4eba\u624b\u4eff\u771f\u7684\u8bbe\u8ba1\u65e2\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u95e8\u69db\uff0c\u53c8\u6269\u5c55\u4e86\u64cd\u4f5c\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.21105", "pdf": "https://arxiv.org/pdf/2509.21105", "abs": "https://arxiv.org/abs/2509.21105", "authors": ["Wenchao Liu", "Xuhui Zhang", "Jinke Ren", "Weijie Yuan", "Changsheng You", "Shuangyang Li"], "title": "UAV-Enabled ISAC Systems with Fluid Antennas", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Unmanned aerial vehicle (UAV)-enabled integrated sensing and communication\n(ISAC) is regarded as a key enabler for next-generation wireless systems.\nHowever, conventional fixed antenna arrays limit the ability of UAVs to fully\nexploit their inherent potential. To overcome this limitation, we propose a\nUAV-enabled ISAC framework equipped with fluid antenna (FA) arrays, where the\nmobility of antenna elements introduces additional spatial degrees of freedom\nto simultaneously enhance communication and sensing performance. A\nmulti-objective optimization problem is formulated to maximize the\ncommunication rates of multiple users while minimizing the Cram\\'er-Rao bound\n(CRB) for single-target angle estimation. Due to excessively frequent updates\nof FA positions may lead to response delays, a three-timescale optimization\nframework is developed to jointly design transmit beamforming, FA positions,\nand UAV trajectory based on their characteristics. To solve the non-convexity\nof the problem, an alternating optimization-based algorithm is developed to\nobtain a sub-optimal solution. Numerical results show that the proposed scheme\nsignificantly outperforms various benchmark schemes, validating the\neffectiveness of integrating the FA technology into the UAV-enabled ISAC\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\u9635\u5217\u7684\u65e0\u4eba\u673a\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u3001\u6ce2\u675f\u5f62\u6210\u548c\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u548c\u4f20\u611f\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u9635\u5217\u9650\u5236\u4e86\u65e0\u4eba\u673a\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u4e2d\u7684\u6f5c\u529b\uff0c\u800c\u6d41\u4f53\u5929\u7ebf\u6280\u672f\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u81ea\u7531\u5ea6\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u901a\u4fe1\u548c\u4f20\u611f\u8868\u73b0\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\u9635\u5217\u7684\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u6700\u5927\u5316\u901a\u4fe1\u901f\u7387\u548c\u6700\u5c0f\u5316\u89d2\u5ea6\u4f30\u8ba1\u8bef\u5dee\u3002\u91c7\u7528\u4e09\u65f6\u95f4\u5c3a\u5ea6\u4f18\u5316\u6846\u67b6\uff0c\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u57fa\u51c6\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u6d41\u4f53\u5929\u7ebf\u6280\u672f\u5bf9\u65e0\u4eba\u673a\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6d41\u4f53\u5929\u7ebf\u9635\u5217\u7684\u5f15\u5165\u4e3a\u65e0\u4eba\u673a\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.20653", "pdf": "https://arxiv.org/pdf/2509.20653", "abs": "https://arxiv.org/abs/2509.20653", "authors": ["Congkai Shen", "Siyuan Yu", "Yifan Weng", "Haoran Ma", "Chen Li", "Hiroshi Yasuda", "James Dallas", "Michael Thompson", "John Subosits", "Tulga Ersal"], "title": "Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "This study introduces a haptic shared control framework designed to teach\nhuman drivers advanced driving skills. In this context, shared control refers\nto a driving mode where the human driver collaborates with an autonomous\ndriving system to control the steering of a vehicle simultaneously. Advanced\ndriving skills are those necessary to safely push the vehicle to its handling\nlimits in high-performance driving such as racing and emergency obstacle\navoidance. Previous research has demonstrated the performance and safety\nbenefits of shared control schemes using both subjective and objective\nevaluations. However, these schemes have not been assessed for their impact on\nskill acquisition on complex and demanding tasks. Prior research on long-term\nskill acquisition either applies haptic shared control to simple tasks or\nemploys other feedback methods like visual and auditory aids. To bridge this\ngap, this study creates a cyber racing coach framework based on the haptic\nshared control paradigm and evaluates its performance in helping human drivers\nacquire high-performance driving skills. The framework introduces (1) an\nautonomous driving system that is capable of cooperating with humans in a\nhighly performant driving scenario; and (2) a haptic shared control mechanism\nalong with a fading scheme to gradually reduce the steering assistance from\nautonomy based on the human driver's performance during training. Two\nbenchmarks are considered: self-learning (no assistance) and full assistance\nduring training. Results from a human subject study indicate that the proposed\nframework helps human drivers develop superior racing skills compared to the\nbenchmarks, resulting in better performance and consistency.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6559\u6388\u4eba\u7c7b\u9a7e\u9a76\u5458\u9ad8\u7ea7\u9a7e\u9a76\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u4f18\u4e8e\u65e0\u8f85\u52a9\u548c\u5168\u8f85\u52a9\u8bad\u7ec3\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u5171\u4eab\u63a7\u5236\u65b9\u6848\u5728\u590d\u6742\u9ad8\u8981\u6c42\u4efb\u52a1\u6280\u80fd\u83b7\u53d6\u65b9\u9762\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u5728\u9ad8\u7ea7\u9a7e\u9a76\u6280\u80fd\u57f9\u8bad\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u7684\u8d5b\u8f66\u6559\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5408\u4f5c\u673a\u5236\u548c\u9010\u6e10\u51cf\u5c11\u8f85\u52a9\u7684\u892a\u8272\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5e2e\u52a9\u9a7e\u9a76\u5458\u5728\u9ad8\u6027\u80fd\u8d5b\u8f66\u6280\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u6027\u80fd\u548c\u4e00\u81f4\u6027\u5747\u4f18\u4e8e\u57fa\u51c6\u3002", "conclusion": "\u89e6\u89c9\u5171\u4eab\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4eba\u7c7b\u9a7e\u9a76\u5458\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6280\u80fd\u83b7\u53d6\u548c\u8868\u73b0\u3002"}}
{"id": "2509.21112", "pdf": "https://arxiv.org/pdf/2509.21112", "abs": "https://arxiv.org/abs/2509.21112", "authors": ["Bade Aksoy", "Do\u011fukan \u00d6zbayrak", "Ahmed Hareedy"], "title": "Adapt or Regress: Rate-Memory-Compatible Spatially-Coupled Codes", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "11 pages (double column), 4 figures, the short version is to be\n  submitted to the IEEE International Conference on Communications (ICC)", "summary": "Spatially-coupled (SC) codes are a class of low-density parity-check (LDPC)\ncodes that have excellent performance thanks to the degrees of freedom they\noffer. An SC code is designed by partitioning a base matrix into components,\nthe number of which implies the code memory, then coupling and lifting them. In\nthe same system, various error-correction coding schemes are typically needed.\nFor example, in wireless communication standards, several channel conditions\nand data rates should be supported. In storage and computing systems, stronger\ncodes should be adopted as the device ages. Adaptive code design enables\nswitching from one code to another when needed, ensuring reliability while\nreducing hardware cost. In this paper, we introduce a class of reconfigurable\nSC codes named rate-memory-compatible SC (RMC-SC) codes, which we design\nprobabilistically. In particular, rate compatibility in RMC-SC codes is\nachieved via increasing the SC code memory, which also makes the codes\nmemory-compatible and improves performance. We express the expected number of\nshort cycles in the SC code protograph as a function of the fixed probability\ndistribution characterizing the already-designed SC code as well as the unknown\ndistribution characterizing the additional components. We use the\ngradient-descent algorithm to find a locally-optimal distribution, in terms of\ncycle count, for the new components. The method can be recursively used to\ndesign any number of SC codes needed, and we show how to extend it to other\ncases. Next, we perform the finite-length optimization using a Markov chain\nMonte Carlo (MC$^2$) approach that we update to design the proposed RMC-SC\ncodes. Experimental results demonstrate significant reductions in cycle counts\nand remarkable performance gains achieved by RMC-SC codes compared with a\nliterature-based straightforward scheme.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMC-SC\u7684\u53ef\u91cd\u6784SC\u7801\uff0c\u901a\u8fc7\u6982\u7387\u8bbe\u8ba1\u548c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\u5176\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5faa\u73af\u8ba1\u6570\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u4e0d\u540c\u7684\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u652f\u6301\u591a\u79cd\u7ea0\u9519\u7f16\u7801\u65b9\u6848\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u4fe1\u9053\u6761\u4ef6\u548c\u6570\u636e\u901f\u7387\u3002\u81ea\u9002\u5e94\u7f16\u7801\u8bbe\u8ba1\u80fd\u591f\u5728\u9700\u8981\u65f6\u5207\u6362\u7f16\u7801\uff0c\u786e\u4fdd\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3002", "method": "\u901a\u8fc7\u589e\u52a0SC\u7801\u7684\u8bb0\u5fc6\u6df1\u5ea6\u5b9e\u73b0\u901f\u7387\u517c\u5bb9\u6027\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u627e\u5230\u65b0\u7ec4\u4ef6\u7684\u5c40\u90e8\u6700\u4f18\u5206\u5e03\u3002\u6b64\u5916\uff0c\u91c7\u7528MC$^2$\u65b9\u6cd5\u8fdb\u884c\u6709\u9650\u957f\u5ea6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6848\u76f8\u6bd4\uff0cRMC-SC\u7801\u663e\u8457\u51cf\u5c11\u4e86\u5faa\u73af\u8ba1\u6570\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RMC-SC\u7801\u901a\u8fc7\u81ea\u9002\u5e94\u8bbe\u8ba1\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u591a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.20656", "pdf": "https://arxiv.org/pdf/2509.20656", "abs": "https://arxiv.org/abs/2509.20656", "authors": ["Junzhe Wang", "Jiarui Xie", "Pengfei Hao", "Zheng Li", "Yi Cai"], "title": "EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation", "categories": ["cs.RO"], "comment": "8 pages, 14 figures, submitted to ICRA 2026", "summary": "Reliable brain-computer interface (BCI) control of robots provides an\nintuitive and accessible means of human-robot interaction, particularly\nvaluable for individuals with motor impairments. However, existing BCI-Robot\nsystems face major limitations: electroencephalography (EEG) signals are noisy\nand unstable, target selection is often predefined and inflexible, and most\nstudies remain restricted to simulation without closed-loop validation. These\nissues hinder real-world deployment in assistive scenarios. To address them, we\npropose a closed-loop BCI-AR-Robot system that integrates motor imagery\n(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic\ngrasping for zero-touch operation. A 14-channel EEG headset enabled\nindividualized MI calibration, a smartphone-based AR interface supported\nmulti-target navigation with direction-congruent feedback to enhance stability,\nand the robotic arm combined decision outputs with vision-based pose estimation\nfor autonomous grasping. Experiments are conducted to validate the framework:\nMI training achieved 93.1 percent accuracy with an average information transfer\nrate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained\ncontrol (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with\nstatic, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2\npercent success rate with good efficiency and strong user-reported control.\nThese results show that AR feedback substantially stabilizes EEG-based control\nand that the proposed framework enables robust zero-touch grasping, advancing\nassistive robotic applications and future modes of human-robot interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73afBCI-AR-\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408EEG\u89e3\u7801\u3001AR\u795e\u7ecf\u53cd\u9988\u548c\u673a\u5668\u4eba\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709BCI-\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u566a\u58f0\u3001\u4e0d\u7075\u6d3b\u548c\u7f3a\u4e4f\u95ed\u73af\u9a8c\u8bc1\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u7cfb\u7edf\u5728\u63a7\u5236\u7a33\u5b9a\u6027\u548c\u6293\u53d6\u6210\u529f\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709BCI-\u673a\u5668\u4eba\u7cfb\u7edf\u56e0EEG\u4fe1\u53f7\u566a\u58f0\u5927\u3001\u76ee\u6807\u9009\u62e9\u4e0d\u7075\u6d3b\u53ca\u7f3a\u4e4f\u95ed\u73af\u9a8c\u8bc1\u800c\u96be\u4ee5\u5728\u73b0\u5b9e\u8f85\u52a9\u573a\u666f\u4e2d\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u91c7\u752814\u901a\u9053EEG\u8033\u673a\u8fdb\u884c\u4e2a\u6027\u5316\u8fd0\u52a8\u60f3\u8c61(MI)\u6821\u51c6\uff0c\u667a\u80fd\u624b\u673aAR\u754c\u9762\u652f\u6301\u591a\u76ee\u6807\u5bfc\u822a\u5e76\u63d0\u4f9b\u65b9\u5411\u4e00\u81f4\u7684\u53cd\u9988\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u5b9e\u73b0\u81ea\u4e3b\u6293\u53d6\u3002", "result": "MI\u8bad\u7ec3\u51c6\u786e\u7387\u8fbe93.1%\uff0c\u5e73\u5747\u4fe1\u606f\u4f20\u8f93\u7387(ITR)\u4e3a14.8\u6bd4\u7279/\u5206\u949f\uff1bAR\u795e\u7ecf\u53cd\u9988\u663e\u8457\u63d0\u5347\u63a7\u5236\u7a33\u5b9a\u6027(SCI=0.210)\uff0c\u6700\u9ad8ITR\u8fbe21.3\u6bd4\u7279/\u5206\u949f\uff1b\u95ed\u73af\u6293\u53d6\u6210\u529f\u7387\u4e3a97.2%\u3002", "conclusion": "AR\u53cd\u9988\u663e\u8457\u589e\u5f3aEEG\u63a7\u5236\u7684\u7a33\u5b9a\u6027\uff0c\u6240\u63d0\u7cfb\u7edf\u4e3a\u8f85\u52a9\u673a\u5668\u4eba\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.20674", "pdf": "https://arxiv.org/pdf/2509.20674", "abs": "https://arxiv.org/abs/2509.20674", "authors": ["Zeyu Han", "Shuocheng Yang", "Minghan Zhu", "Fang Zhang", "Shaobing Xu", "Maani Ghaffari", "Jianqiang Wang"], "title": "Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous vehicles and robots rely on accurate odometry estimation in\nGPS-denied environments. While LiDARs and cameras struggle under extreme\nweather, 4D mmWave radar emerges as a robust alternative with all-weather\noperability and velocity measurement. In this paper, we introduce Equi-RO, an\nequivariant network-based framework for 4D radar odometry. Our algorithm\npre-processes Doppler velocity into invariant node and edge features in the\ngraph, and employs separate networks for equivariant and invariant feature\nprocessing. A graph-based architecture enhances feature aggregation in sparse\nradar data, improving inter-frame correspondence. Experiments on the\nopen-source dataset and self-collected dataset show Equi-RO outperforms\nstate-of-the-art algorithms in accuracy and robustness. Overall, our method\nachieves 10.7% and 20.0% relative improvements in translation and rotation\naccuracy, respectively, compared to the best baseline on the open-source\ndataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEqui-RO\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u57fa\u4e8e\u7b49\u53d8\u7f51\u7edc\uff0c\u901a\u8fc7\u9884\u5904\u7406\u591a\u666e\u52d2\u901f\u5ea6\u548c\u56fe\u7ed3\u6784\u63d0\u5347\u4e86\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u7684\u7279\u5f81\u805a\u5408\u548c\u5e27\u95f4\u5bf9\u5e94\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728GPS\u4fe1\u53f7\u7f3a\u5931\u7684\u6781\u7aef\u5929\u6c14\u73af\u5883\u4e2d\uff0cLiDAR\u548c\u76f8\u673a\u6027\u80fd\u53d7\u9650\uff0c\u800c4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5177\u6709\u5168\u5929\u5019\u5de5\u4f5c\u548c\u901f\u5ea6\u6d4b\u91cf\u7684\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEqui-RO\u6846\u67b6\uff0c\u5c06\u591a\u666e\u52d2\u901f\u5ea6\u9884\u5904\u7406\u4e3a\u56fe\u4e2d\u7684\u4e0d\u53d8\u8282\u70b9\u548c\u8fb9\u7279\u5f81\uff0c\u5e76\u5229\u7528\u72ec\u7acb\u7684\u7f51\u7edc\u5904\u7406\u7b49\u53d8\u548c\u4e0d\u53d8\u7279\u5f81\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u589e\u5f3a\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u7684\u7279\u5f81\u805a\u5408\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u91c7\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0cEqui-RO\u5728\u5e73\u79fb\u548c\u65cb\u8f6c\u7cbe\u5ea6\u4e0a\u5206\u522b\u6bd4\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8610.7%\u548c20.0%\u3002", "conclusion": "Equi-RO\u57284D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3aGPS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20681", "pdf": "https://arxiv.org/pdf/2509.20681", "abs": "https://arxiv.org/abs/2509.20681", "authors": ["Wei-Teng Chu", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Implicit representations have been widely applied in robotics for obstacle\navoidance and path planning. In this paper, we explore the problem of\nconstructing an implicit distance representation from a single image. Past\nmethods for implicit surface reconstruction, such as \\emph{NeuS} and its\nvariants generally require a large set of multi-view images as input, and\nrequire long training times. In this work, we propose Fast Image-to-Neural\nSurface (FINS), a lightweight framework that can reconstruct high-fidelity\nsurfaces and SDF fields based on a single or a small set of images. FINS\nintegrates a multi-resolution hash grid encoder with lightweight geometry and\ncolor heads, making the training via an approximate second-order optimizer\nhighly efficient and capable of converging within a few seconds. Additionally,\nwe achieve the construction of a neural surface requiring only a single RGB\nimage, by leveraging pre-trained foundation models to estimate the geometry\ninherent in the image. Our experiments demonstrate that under the same\nconditions, our method outperforms state-of-the-art baselines in both\nconvergence speed and accuracy on surface reconstruction and SDF field\nestimation. Moreover, we demonstrate the applicability of FINS for robot\nsurface following tasks and show its scalability to a variety of benchmark\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFINS\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u6216\u5c11\u91cf\u7684RGB\u56fe\u50cf\u5feb\u901f\u91cd\u5efa\u9ad8\u4fdd\u771f\u8868\u9762\u548cSDF\u573a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u63d0\u5347\u9690\u5f0f\u8ddd\u79bb\u8868\u793a\u4ece\u5355\u5f20\u56fe\u50cf\u7684\u6784\u5efa\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u9ad8\u8bad\u7ec3\u65f6\u95f4\u7684\u4f9d\u8d56\u3002", "method": "FINS\u6574\u5408\u4e86\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u51e0\u4f55\u4e0e\u989c\u8272\u5934\uff0c\u91c7\u7528\u8fd1\u4f3c\u4e8c\u9636\u4f18\u5316\u5668\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u4f30\u8ba1\u56fe\u50cf\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFINS\u5728\u6536\u655b\u901f\u5ea6\u548c\u8868\u9762\u91cd\u5efa/SDF\u573a\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8868\u9762\u8ddf\u968f\u4efb\u52a1\u3002", "conclusion": "FINS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u5355\u56fe\u50cf\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.20688", "pdf": "https://arxiv.org/pdf/2509.20688", "abs": "https://arxiv.org/abs/2509.20688", "authors": ["Shouren Mao", "Minghao Qin", "Wei Dong", "Huajian Liu", "Yongzhuo Gao"], "title": "RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks", "categories": ["cs.RO", "cs.CV"], "comment": "Joint first authors: Shouren Mao and Minghao Qin. Published in\n  IEEE/RSJ IROS 2024. This arXiv version adds a joint first-authorship note to\n  correct an omission in the IEEE Xplore version. No technical changes. Please\n  cite the IEEE version", "summary": "Neural architecture search (NAS) has shown great promise in automatically\ndesigning lightweight models. However, conventional approaches are insufficient\nin training the supernet and pay little attention to actual robot hardware\nresources. To meet such challenges, we propose RAM-NAS, a resource-aware\nmulti-objective NAS method that focuses on improving the supernet pretrain and\nresource-awareness on robot hardware devices. We introduce the concept of\nsubnets mutual distillation, which refers to mutually distilling all subnets\nsampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge\nDistillation (DKD) loss to enhance logits distillation performance. To expedite\nthe search process with consideration for hardware resources, we used data from\nthree types of robotic edge hardware to train Latency Surrogate predictors.\nThese predictors facilitated the estimation of hardware inference latency\nduring the search phase, enabling a unified multi-objective evolutionary search\nto balance model accuracy and latency trade-offs. Our discovered model family,\nRAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on\nImageNet. In addition, the resource-aware multi-objective NAS we employ\nsignificantly reduces the model's inference latency on edge hardware for\nrobots. We conducted experiments on downstream tasks to verify the scalability\nof our methods. The inference time for detection and segmentation is reduced on\nall three hardware types compared to MobileNetv3-based methods. Our work fills\nthe gap in NAS for robot hardware resource-aware.", "AI": {"tldr": "\u63d0\u51faRAM-NAS\uff0c\u4e00\u79cd\u8d44\u6e90\u611f\u77e5\u7684\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u8d85\u7f51\u7edc\u9884\u8bad\u7ec3\u548c\u673a\u5668\u4eba\u786c\u4ef6\u8d44\u6e90\u5229\u7528\uff0c\u901a\u8fc7\u5b50\u7f51\u4e92\u84b8\u998f\u548cDKD\u635f\u5931\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5229\u7528\u5ef6\u8fdf\u4ee3\u7406\u9884\u6d4b\u5668\u52a0\u901f\u641c\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u8fb9\u7f18\u786c\u4ef6\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfNAS\u65b9\u6cd5\u5728\u8d85\u7f51\u7edc\u8bad\u7ec3\u548c\u786c\u4ef6\u8d44\u6e90\u5229\u7528\u4e0a\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u673a\u5668\u4eba\u786c\u4ef6\u8d44\u6e90\u3002", "method": "\u5f15\u5165\u5b50\u7f51\u4e92\u84b8\u998f\u548cDKD\u635f\u5931\u4f18\u5316\u6027\u80fd\uff0c\u4f7f\u7528\u5ef6\u8fdf\u4ee3\u7406\u9884\u6d4b\u5668\u52a0\u901f\u591a\u76ee\u6807\u8fdb\u5316\u641c\u7d22\u3002", "result": "RAM-NAS\u6a21\u578b\u5728ImageNet\u4e0a\u51c6\u786e\u738776.7%-81.4%\uff0c\u663e\u8457\u964d\u4f4e\u8fb9\u7f18\u786c\u4ef6\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "RAM-NAS\u586b\u8865\u4e86NAS\u5728\u673a\u5668\u4eba\u786c\u4ef6\u8d44\u6e90\u611f\u77e5\u4e0a\u7684\u7a7a\u767d\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.20689", "pdf": "https://arxiv.org/pdf/2509.20689", "abs": "https://arxiv.org/abs/2509.20689", "authors": ["Chathura Semasinghe", "Siavash Rezazadeh"], "title": "Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking", "categories": ["cs.RO"], "comment": null, "summary": "This paper extends the forced-oscillation-based reduced-order model of\nwalking to a model with ankles and feet. A human-inspired paradigm was designed\nfor the ankle dynamics, which results in improved gait characteristics compared\nto the point-foot model. In addition, it was shown that while the proposed\nmodel can stabilize against large errors in initial conditions through\ncombination of foot placement and ankle strategies, the model is able to\nstabilize against small perturbations without relying on the foot placement\ncontrol and solely through the designed proprioceptive ankle scheme. This novel\nproperty, which is also observed in humans, can help in better understanding of\nanthropomorphic walking and its stabilization mechanisms.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u811a\u8e1d\u548c\u8db3\u7684\u529b\u5b66\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u5f3a\u5236\u632f\u8361\u7684\u884c\u8d70\u7b80\u5316\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u542f\u53d1\u7684\u811a\u8e1d\u52a8\u529b\u5b66\u8bbe\u8ba1\uff0c\u663e\u8457\u6539\u5584\u4e86\u6b65\u6001\u7279\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u811a\u8e1d\u548c\u8db3\u7684\u6a21\u578b\uff0c\u63d0\u9ad8\u884c\u8d70\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u81ea\u7136\u6027\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6a21\u62df\u548c\u7406\u89e3\u4eba\u7c7b\u884c\u8d70\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u542f\u53d1\u7684\u811a\u8e1d\u52a8\u529b\u5b66\u8303\u5f0f\uff0c\u5e76\u4e0e\u8db3\u90e8\u653e\u7f6e\u7b56\u7565\u7ed3\u5408\uff0c\u4ee5\u7a33\u5b9a\u884c\u8d70\u6a21\u578b\u3002", "result": "\u65b0\u6a21\u578b\u5728\u5927\u8303\u56f4\u521d\u59cb\u6761\u4ef6\u8bef\u5dee\u4e0b\u901a\u8fc7\u8db3\u90e8\u653e\u7f6e\u548c\u811a\u8e1d\u7b56\u7565\u7a33\u5b9a\u884c\u8d70\uff0c\u800c\u5c0f\u6270\u52a8\u4e0b\u4ec5\u901a\u8fc7\u811a\u8e1d\u65b9\u6848\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u7279\u6027\u4e0d\u4ec5\u6a21\u62df\u4e86\u4eba\u7c7b\u884c\u8d70\u7684\u7a33\u5b9a\u673a\u5236\uff0c\u8fd8\u4e3a\u7406\u89e3\u7c7b\u4f3c\u884c\u8d70\u7684\u7a33\u5b9a\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.20696", "pdf": "https://arxiv.org/pdf/2509.20696", "abs": "https://arxiv.org/abs/2509.20696", "authors": ["Qingpeng Li", "Chengrui Zhu", "Yanming Wu", "Xin Yuan", "Zhen Zhang", "Jian Yang", "Yong Liu"], "title": "RuN: Residual Policy for Natural Humanoid Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Enabling humanoid robots to achieve natural and dynamic locomotion across a\nwide range of speeds, including smooth transitions from walking to running,\npresents a significant challenge. Existing deep reinforcement learning methods\ntypically require the policy to directly track a reference motion, forcing a\nsingle policy to simultaneously learn motion imitation, velocity tracking, and\nstability maintenance. To address this, we introduce RuN, a novel decoupled\nresidual learning framework. RuN decomposes the control task by pairing a\npre-trained Conditional Motion Generator, which provides a kinematically\nnatural motion prior, with a reinforcement learning policy that learns a\nlightweight residual correction to handle dynamical interactions. Experiments\nin simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN\nachieves stable, natural gaits and smooth walk-run transitions across a broad\nvelocity range (0-2.5 m/s), outperforming state-of-the-art methods in both\ntraining efficiency and final performance.", "AI": {"tldr": "RuN\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u8fd0\u52a8\u751f\u6210\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u79cd\u901f\u5ea6\u4e0b\u7684\u7a33\u5b9a\u3001\u81ea\u7136\u6b65\u6001\u548c\u6b65\u6001\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u81ea\u7136\u52a8\u6001\u6b65\u6001\u548c\u901f\u5ea6\u8f6c\u6362\u65f6\uff0c\u9700\u540c\u65f6\u5b66\u4e60\u8fd0\u52a8\u6a21\u4eff\u3001\u901f\u5ea6\u8ddf\u8e2a\u548c\u7a33\u5b9a\u6027\u7ef4\u62a4\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faRuN\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6761\u4ef6\u8fd0\u52a8\u751f\u6210\u5668\u63d0\u4f9b\u8fd0\u52a8\u5148\u9a8c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u4fee\u6b63\uff0c\u5904\u7406\u52a8\u529b\u5b66\u4ea4\u4e92\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\uff0cRuN\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e860-2.5 m/s\u901f\u5ea6\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u81ea\u7136\u6b65\u6001\u548c\u6b65\u6001\u8f6c\u6362\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RuN\u6846\u67b6\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u52a8\u6001\u6b65\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20703", "pdf": "https://arxiv.org/pdf/2509.20703", "abs": "https://arxiv.org/abs/2509.20703", "authors": ["Xiaoxiang Dong", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Learning from human video demonstrations offers a scalable alternative to\nteleoperation or kinesthetic teaching, but poses challenges for robot\nmanipulators due to embodiment differences and joint feasibility constraints.\nWe address this problem by proposing the Joint Flow Trajectory Optimization\n(JFTO) framework for grasp pose generation and object trajectory imitation\nunder the video-based Learning-from-Demonstration (LfD) paradigm. Rather than\ndirectly imitating human hand motions, our method treats demonstrations as\nobject-centric guides, balancing three objectives: (i) selecting a feasible\ngrasp pose, (ii) generating object trajectories consistent with demonstrated\nmotions, and (iii) ensuring collision-free execution within robot kinematics.\nTo capture the multimodal nature of demonstrations, we extend flow matching to\n$\\SE(3)$ for probabilistic modeling of object trajectories, enabling\ndensity-aware imitation that avoids mode collapse. The resulting optimization\nintegrates grasp similarity, trajectory likelihood, and collision penalties\ninto a unified differentiable objective. We validate our approach in both\nsimulation and real-world experiments across diverse real-world manipulation\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJFTO\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6293\u53d6\u59ff\u52bf\u548c\u7269\u4f53\u8f68\u8ff9\u6a21\u4eff\uff0c\u89e3\u51b3\u4ece\u4eba\u7c7b\u89c6\u9891\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u9065\u64cd\u4f5c\u6216\u624b\u628a\u624b\u6559\u5b66\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u800c\u76f4\u63a5\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u5b58\u5728\u672c\u4f53\u5dee\u5f02\u548c\u5173\u8282\u53ef\u884c\u6027\u7ea6\u675f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faJFTO\u6846\u67b6\uff0c\u5c06\u6f14\u793a\u89c6\u4e3a\u7269\u4f53\u4e2d\u5fc3\u6307\u5bfc\uff0c\u5e73\u8861\u6293\u53d6\u59ff\u52bf\u53ef\u884c\u6027\u3001\u7269\u4f53\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u65e0\u78b0\u649e\u6267\u884c\uff0c\u5e76\u901a\u8fc7SE(3)\u6d41\u5339\u914d\u5b9e\u73b0\u6982\u7387\u5efa\u6a21\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "JFTO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u6a21\u4eff\u5b66\u4e60\u3002"}}
{"id": "2509.20705", "pdf": "https://arxiv.org/pdf/2509.20705", "abs": "https://arxiv.org/abs/2509.20705", "authors": ["Reza Akhavian", "Mani Amani", "Johannes Mootz", "Robert Ashe", "Behrad Beheshti"], "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework", "categories": ["cs.RO"], "comment": null, "summary": "The adoption of cyber-physical systems and jobsite intelligence that connects\ndesign models, real-time site sensing, and autonomous field operations can\ndramatically enhance digital management in the construction industry. This\npaper introduces BIM2RDT (Building Information Models to Robot-Ready Site\nDigital Twins), an agentic artificial intelligence (AI) framework designed to\ntransform static Building Information Modeling (BIM) into dynamic, robot-ready\ndigital twins (DTs) that prioritize safety during execution. The framework\nbridges the gap between pre-existing BIM data and real-time site conditions by\nintegrating three key data streams: geometric and semantic information from BIM\nmodels, activity data from IoT sensor networks, and visual-spatial data\ncollected by robots during site traversal. The methodology introduces\nSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that\nleverages large language model (LLM) reasoning. Unlike traditional methods,\nSG-ICP utilizes an LLM to infer object-specific, plausible orientation priors\nbased on BIM semantics, improving alignment accuracy by avoiding convergence on\nlocal minima. This creates a feedback loop where robot-collected data updates\nthe DT, which in turn optimizes paths for missions. The framework employs YOLOE\nobject detection and Shi-Tomasi corner detection to identify and track\nconstruction elements while using BIM geometry as a priori maps. The framework\nalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping\nsensor-detected safety events to the digital twin using IFC standards for\nintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,\nachieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with\noccluded features, ensuring plausible orientations. HAV integration triggers\nwarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BIM2RDT\u6846\u67b6\uff0c\u5c06\u9759\u6001BIM\u8f6c\u5316\u4e3a\u52a8\u6001\u3001\u673a\u5668\u4eba\u53ef\u7528\u7684\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u63d0\u5347\u65bd\u5de5\u884c\u4e1a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u7ed3\u5408BIM\u6570\u636e\u3001\u7269\u8054\u7f51\u4f20\u611f\u5668\u548c\u673a\u5668\u4eba\u91c7\u96c6\u7684\u89c6\u89c9\u7a7a\u95f4\u6570\u636e\uff0c\u4ee5\u53ca\u521b\u65b0\u7684Semantic-Gravity ICP\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u70b9\u4e91\u5bf9\u9f50\u7cbe\u5ea6\u3002", "motivation": "\u65bd\u5de5\u884c\u4e1a\u9700\u8981\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u7684\u6570\u5b57\u5316\u7ba1\u7406\u65b9\u6cd5\uff0c\u73b0\u6709\u7684BIM\u6570\u636e\u4e0e\u5b9e\u65f6\u73b0\u573a\u6761\u4ef6\u4e4b\u95f4\u5b58\u5728\u9e3f\u6c9f\uff0c\u8feb\u5207\u9700\u8981\u52a8\u6001\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4f18\u5316\u51b3\u7b56\u548c\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86BIM2RDT\u6846\u67b6\uff0c\u6574\u5408\u4e86BIM\u7684\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\u3001\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\u548c\u673a\u5668\u4eba\u89c6\u89c9\u6570\u636e\u3002\u521b\u65b0\u6027\u5730\u4f7f\u7528Semantic-Gravity ICP\u7b97\u6cd5\uff0c\u5229\u7528LLM\u63a8\u7406\u63d0\u5347\u70b9\u4e91\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u5e76\u7ed3\u5408YOLOE\u548cShi-Tomasi\u6280\u672f\u5b9e\u73b0\u5143\u7d20\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSG-ICP\u7b97\u6cd5\u76f8\u6bd4\u6807\u51c6ICP\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7cbe\u5ea6\uff08RMSE\u964d\u4f4e64.3%--88.3%\uff09\u3002\u5b9e\u65f6HAV\u76d1\u6d4b\u786e\u4fdd\u4e86\u5b89\u5168\u4e8b\u4ef6\u7684\u53ca\u65f6\u5e72\u9884\u3002", "conclusion": "BIM2RDT\u901a\u8fc7\u52a8\u6001\u6570\u5b57\u5b6a\u751f\u4f53\u548c\u667a\u80fd\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65bd\u5de5\u884c\u4e1a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5e94\u7528\u4e8e\u5176\u4ed6\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2509.20709", "pdf": "https://arxiv.org/pdf/2509.20709", "abs": "https://arxiv.org/abs/2509.20709", "authors": ["Mani Amani", "Reza Akhavian"], "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor", "categories": ["cs.RO"], "comment": null, "summary": "Integrating natural language (NL) prompts into robotic mission planning has\nattracted significant interest in recent years. In the construction domain,\nBuilding Information Models (BIM) encapsulate rich NL descriptions of the\nenvironment. We present a novel framework that fuses NL directives with\nBIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting\nthe LLM as a sensor: each obstacle's design-time repulsive coefficient is\ntreated as a Beta(alpha, beta) random variable and LLM-returned danger scores\nare incorporated as pseudo-counts to update alpha and beta. The resulting\nposterior mean yields a continuous, context-aware repulsive gain that augments\na Euclidean-distance-based potential field for cost heuristics. By adjusting\ngains based on sentiment and context inferred from user prompts, our method\nguides robots along safer, more context-aware paths. This provides a\nnumerically stable method that can chain multiple natural commands and prompts\nfrom construction workers and foreman to enable planning while giving\nflexibility to be integrated in any learned or classical AI framework.\nSimulation results demonstrate that this Beta-Bernoulli fusion yields both\nqualitative and quantitative improvements in path robustness and validity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0eBIM\u8bed\u4e49\u5730\u56fe\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Beta-Bernoulli\u8d1d\u53f6\u65af\u878d\u5408\u63d0\u5347\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u878d\u5165\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\uff0c\u5c24\u5176\u5728\u5efa\u7b51\u9886\u57df\uff0cBIM\u6a21\u578b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u73af\u5883\u63cf\u8ff0\u3002\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528Beta-Bernoulli\u8d1d\u53f6\u65af\u878d\u5408\uff0c\u5c06LLM\uff08\u8bed\u8a00\u6a21\u578b\uff09\u89c6\u4e3a\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u66f4\u65b0Beta\u5206\u5e03\u53c2\u6570\u6765\u52a8\u6001\u8c03\u6574\u969c\u788d\u7269\u7684\u6392\u65a5\u7cfb\u6570\uff0c\u7ed3\u5408\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u5206\u6790\u751f\u6210\u8fde\u7eed\u589e\u76ca\uff0c\u589e\u5f3a\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u7684\u52bf\u573a\u542f\u53d1\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u4e0a\u5b9e\u73b0\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u53cc\u91cd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u94fe\u5f0f\u5904\u7406\u63d0\u4f9b\u4e86\u6570\u503c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u7c7b\u5b66\u4e60\u6216\u7ecf\u5178AI\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.20717", "pdf": "https://arxiv.org/pdf/2509.20717", "abs": "https://arxiv.org/abs/2509.20717", "authors": ["Zhenguo Sun", "Yibo Peng", "Yuan Meng", "Xukun Li", "Bo-Sheng Huang", "Zhenshan Bing", "Xinlong Wang", "Alois Knoll"], "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle\nbecause absolute joint commands cannot compensate model-plant mismatch, leading\nto error accumulation. We propose RobotDancing, a simple, scalable framework\nthat predicts residual joint targets to explicitly correct dynamics\ndiscrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and\nzero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)\nsetup with a unified observation, reward, and hyperparameter configuration. We\nevaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and\nvalidate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy\nbehaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with\nhigh motion tracking quality.", "AI": {"tldr": "RobotDancing\u662f\u4e00\u4e2a\u7b80\u5316\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u6b8b\u4f59\u5173\u8282\u76ee\u6807\u6765\u7ea0\u6b63\u52a8\u529b\u5b66\u504f\u5dee\uff0c\u5b9e\u73b0\u957f\u65f6\u7a0b\u3001\u9ad8\u52a8\u6001\u7684\u8fd0\u52a8\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u7531\u4e8e\u6a21\u578b\u4e0e\u5b9e\u7269\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u8ddf\u8e2a\u7684\u7a33\u5065\u6027\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\uff0c\u5305\u62ec\u8bad\u7ec3\u3001\u6a21\u62df\u9a8c\u8bc1\u548c\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\uff0c\u7edf\u4e00\u7684\u89c2\u6d4b\u3001\u5956\u52b1\u548c\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728Unitree G1\u4e0a\u6210\u529f\u8ddf\u8e2a\u591a\u5206\u949f\u7684\u9ad8\u80fd\u91cf\u821e\u8e48\u52a8\u4f5c\uff08\u5982\u8df3\u8dc3\u3001\u65cb\u8f6c\uff09\uff0c\u5e76\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u786c\u4ef6\u4e0a\uff0c\u5c55\u73b0\u51fa\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u8ddf\u8e2a\u3002", "conclusion": "RobotDancing\u6709\u6548\u5730\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u8fd0\u52a8\u8ddf\u8e2a\u4e2d\u7684\u52a8\u529b\u5b66\u504f\u5dee\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u590d\u6742\u52a8\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.20739", "pdf": "https://arxiv.org/pdf/2509.20739", "abs": "https://arxiv.org/abs/2509.20739", "authors": ["Guoyang Zhao", "Yudong Li", "Weiqing Qi", "Kai Zhang", "Bonan Liu", "Kai Chen", "Haoang Li", "Jun Ma"], "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under\nrapid motion, calibration demands, and sensor drift, while offering limited\nsemantic reasoning for task-driven exploration. To deal with these issues, we\npropose a vision-only, SLAM-free navigation framework that replaces dense\ngeometry with semantic reasoning and lightweight topological representations. A\nhierarchical vision-language perception module fuses scene-level context with\nobject-level cues for robust semantic inference. And a semantic-probabilistic\ntopological map supports coarse-to-fine planning: LLM-based global reasoning\nfor subgoal selection and vision-based local planning for obstacle avoidance.\nIntegrated with reinforcement-learning locomotion controllers, the framework is\ndeployable across diverse legged robot platforms. Experiments in simulation and\nreal-world settings demonstrate consistent improvements in semantic accuracy,\nplanning quality, and navigation success, while ablation studies further\nshowcase the necessity of both hierarchical perception and fine local planning.\nThis work introduces a new paradigm for SLAM-free, vision-language-driven\nnavigation, shifting robotic exploration from geometry-centric mapping to\nsemantics-driven decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u63a8\u7406\u548c\u8f7b\u91cf\u62d3\u6251\u8868\u793a\u7684\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684SLAM\u5bc6\u96c6\u578b\u51e0\u4f55\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u5728\u5feb\u901f\u8fd0\u52a8\u3001\u6821\u51c6\u9700\u6c42\u548c\u9ad8\u4f20\u611f\u5668\u6f02\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u4e14\u7f3a\u4e4f\u4efb\u52a1\u9a71\u52a8\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u6a21\u5757\u548c\u8bed\u4e49\u6982\u7387\u62d3\u6251\u5730\u56fe\uff0c\u7ed3\u5408LLM\u5168\u5c40\u63a8\u7406\u548c\u89c6\u89c9\u5c40\u90e8\u89c4\u5212\uff0c\u5b9e\u73b0\u5206\u5c42\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u6846\u67b6\u5728\u8bed\u4e49\u51c6\u786e\u6027\u3001\u89c4\u5212\u8d28\u91cf\u548c\u5bfc\u822a\u6210\u529f\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u65e0SLAM\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c06\u673a\u5668\u4eba\u63a2\u7d22\u4ece\u51e0\u4f55\u6620\u5c04\u8f6c\u5411\u8bed\u4e49\u9a71\u52a8\u51b3\u7b56\u3002"}}
{"id": "2509.20757", "pdf": "https://arxiv.org/pdf/2509.20757", "abs": "https://arxiv.org/abs/2509.20757", "authors": ["Yuxuan Zhou", "Xingxing Li", "Shengyu Li", "Zhuohao Yan", "Chunxi Xia", "Shaoquan Feng"], "title": "MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual SLAM is a cornerstone technique in robotics, autonomous driving and\nextended reality (XR), yet classical systems often struggle with low-texture\nenvironments, scale ambiguity, and degraded performance under challenging\nvisual conditions. Recent advancements in feed-forward neural network-based\npointmap regression have demonstrated the potential to recover high-fidelity 3D\nscene geometry directly from images, leveraging learned spatial priors to\novercome limitations of traditional multi-view geometry methods. However, the\nwidely validated advantages of probabilistic multi-sensor information fusion\nare often discarded in these pipelines. In this work, we propose\nMASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly\nintegrates feed-forward pointmap regression with complementary sensor\ninformation, including inertial measurements and GNSS data. The system\nintroduces Sim(3)-based visualalignment constraints (in the Hessian form) into\na universal metric-scale SE(3) factor graph for effective information fusion. A\nhierarchical factor graph design is developed, which allows both real-time\nsliding-window optimization and global optimization with aggressive loop\nclosures, enabling real-time pose tracking, metric-scale structure perception\nand globally consistent mapping. We evaluate our approach on both public\nbenchmarks and self-collected datasets, demonstrating substantial improvements\nin accuracy and robustness over existing visual-centered multi-sensor SLAM\nsystems. The code will be released open-source to support reproducibility and\nfurther research (https://github.com/GREAT-WHU/MASt3R-Fusion).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MASt3R-Fusion\u6846\u67b6\uff0c\u7ed3\u5408\u524d\u9988\u70b9\u4e91\u56de\u5f52\u4e0e\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u63d0\u5347\u89c6\u89c9SLAM\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u548c\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9SLAM\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u548c\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u4f18\u52bf\u3002", "method": "\u5f15\u5165Sim(3)\u89c6\u89c9\u5bf9\u9f50\u7ea6\u675f\u548c\u5206\u5c42\u56e0\u5b50\u56fe\u8bbe\u8ba1\uff0c\u7ed3\u5408\u60ef\u6027\u6d4b\u91cf\u548cGNSS\u6570\u636e\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4f18\u5316\u4e0e\u5168\u5c40\u4e00\u81f4\u6027\u91cd\u5efa\u3002", "result": "\u5728\u516c\u5171\u548c\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MASt3R-Fusion\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u63d0\u5347\u4e86\u89c6\u89c9SLAM\u6027\u80fd\uff0c\u4ee3\u7801\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2509.20766", "pdf": "https://arxiv.org/pdf/2509.20766", "abs": "https://arxiv.org/abs/2509.20766", "authors": ["Gawon Lee", "Daesol Cho", "H. Jin Kim"], "title": "Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-task reinforcement learning (MTRL) offers a promising approach to\nimprove sample efficiency and generalization by training agents across multiple\ntasks, enabling knowledge sharing between them. However, applying MTRL to\nrobotics remains challenging due to the high cost of collecting diverse task\ndata. To address this, we propose MT-L\\'evy, a novel exploration strategy that\nenhances sample efficiency in MTRL environments by combining behavior sharing\nacross tasks with temporally extended exploration inspired by L\\'evy flight.\nMT-L\\'evy leverages policies trained on related tasks to guide exploration\ntowards key states, while dynamically adjusting exploration levels based on\ntask success ratios. This approach enables more efficient state-space coverage,\neven in complex robotics environments. Empirical results demonstrate that\nMT-L\\'evy significantly improves exploration and sample efficiency, supported\nby quantitative and qualitative analyses. Ablation studies further highlight\nthe contribution of each component, showing that combining behavior sharing\nwith adaptive exploration strategies can significantly improve the practicality\nof MTRL in robotics applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MT-L\\'evy\uff0c\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u5171\u4eab\u548cL\\'evy\u542f\u53d1\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff08MTRL\uff09\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u521b\u65b0\u63a2\u7d22\u7b56\u7565\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "MT-L\\'evy\u901a\u8fc7\u7ed3\u5408\u8de8\u4efb\u52a1\u7684\u884c\u4e3a\u5171\u4eab\u548c\u52a8\u6001\u8c03\u6574\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u4f18\u5316\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMT-L\\'evy\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u548c\u6837\u672c\u5229\u7528\u7387\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "\u7ed3\u5408\u884c\u4e3a\u5171\u4eab\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347MTRL\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.20839", "pdf": "https://arxiv.org/pdf/2509.20839", "abs": "https://arxiv.org/abs/2509.20839", "authors": ["Jiaxuan He", "Jiamei Ren", "Chongshang Yan", "Wenjie Song"], "title": "SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation", "categories": ["cs.RO"], "comment": null, "summary": "In target-driven navigation and autonomous exploration, reasonable prediction\nof unknown regions is crucial for efficient navigation and environment\nunderstanding. Existing methods mostly focus on single objects or geometric\noccupancy maps, lacking the ability to model room-level semantic structures. We\npropose SemSight, a probabilistic bird's-eye-view prediction model for\nmulti-level scene semantics. The model jointly infers structural layouts,\nglobal scene context, and target area distributions, completing semantic maps\nof unexplored areas while estimating probability maps for target categories. To\ntrain SemSight, we simulate frontier-driven exploration on 2,000 indoor layout\ngraphs, constructing a diverse dataset of 40,000 sequential egocentric\nobservations paired with complete semantic maps. We adopt an encoder-decoder\nnetwork as the core architecture and introduce a mask-constrained supervision\nstrategy. This strategy applies a binary mask of unexplored areas so that\nsupervision focuses only on unknown regions, forcing the model to infer\nsemantic structures from the observed context. Experimental results show that\nSemSight improves prediction performance for key functional categories in\nunexplored regions and outperforms non-mask-supervised approaches on metrics\nsuch as Structural Consistency (SC) and Region Recognition Accuracy (PA). It\nalso enhances navigation efficiency in closed-loop simulations, reducing the\nnumber of search steps when guiding robots toward target areas.", "AI": {"tldr": "SemSight\u662f\u4e00\u79cd\u591a\u7ea7\u573a\u666f\u8bed\u4e49\u7684\u6982\u7387\u9e1f\u77b0\u56fe\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u63a8\u65ad\u7ed3\u6784\u5e03\u5c40\u3001\u5168\u5c40\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u76ee\u6807\u533a\u57df\u5206\u5e03\uff0c\u63d0\u5347\u672a\u77e5\u533a\u57df\u7684\u8bed\u4e49\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u5bf9\u8c61\u6216\u51e0\u4f55\u5360\u636e\u56fe\uff0c\u7f3a\u4e4f\u5bf9\u623f\u95f4\u7ea7\u8bed\u4e49\u7ed3\u6784\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faSemSight\u6a21\u578b\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u7ea6\u675f\u76d1\u7763\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u672a\u77e5\u533a\u57df\u7684\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemSight\u5728\u672a\u63a2\u7d22\u533a\u57df\u7684\u5173\u952e\u529f\u80fd\u7c7b\u522b\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5728\u5bfc\u822a\u6548\u7387\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "SemSight\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bed\u4e49\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u5728\u5b9e\u9645\u5bfc\u822a\u4e2d\u51cf\u5c11\u4e86\u641c\u7d22\u6b65\u9aa4\u3002"}}
{"id": "2509.20841", "pdf": "https://arxiv.org/pdf/2509.20841", "abs": "https://arxiv.org/abs/2509.20841", "authors": ["Dekun Lu", "Wei Gao", "Kui Jia"], "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "First two authors contribute equally. Project page:\n  https://sites.google.com/view/imaginationpolicy", "summary": "End-to-end robot manipulation policies offer significant potential for\nenabling embodied agents to understand and interact with the world. Unlike\ntraditional modular pipelines, end-to-end learning mitigates key limitations\nsuch as information loss between modules and feature misalignment caused by\nisolated optimization targets. Despite these advantages, existing end-to-end\nneural networks for robotic manipulation--including those based on large\nVLM/VLA models--remain insufficiently performant for large-scale practical\ndeployment. In this paper, we take a step towards an end-to-end manipulation\npolicy that is generalizable, accurate and reliable. To achieve this goal, we\npropose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for\nrobotic manipulation. Our formulation is used as the action representation of a\nneural policy, which can be trained in an end-to-end fashion. Such an action\nrepresentation is general, as it extends the standard end-effector pose action\nrepresentation and supports a diverse set of manipulation tasks in a unified\nmanner. The oriented keypoint in our method enables natural generalization to\nobjects with different shapes and sizes, while achieving sub-centimeter\naccuracy. Moreover, our formulation can easily handle multi-stage tasks,\nmulti-modal robot behaviors, and deformable objects. Extensive simulated and\nhardware experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u94fe\u5f0f\u79fb\u52a8\u5bfc\u5411\u5173\u952e\u70b9\uff08CoMOK\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u901a\u7528\u6027\u3001\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u7aef\u5230\u7aef\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5177\u6709\u907f\u514d\u6a21\u5757\u95f4\u4fe1\u606f\u4e22\u5931\u548c\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86CoMOK\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5bfc\u5411\u5173\u952e\u70b9\u5b9e\u73b0\u81ea\u7136\u6cdb\u5316\u3002", "result": "\u5728\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u5e76\u5904\u7406\u591a\u6a21\u6001\u884c\u4e3a\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u3002", "conclusion": "CoMOK\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u64cd\u4f5c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u548c\u591a\u6837\u5316\u5bf9\u8c61\u3002"}}
{"id": "2509.20843", "pdf": "https://arxiv.org/pdf/2509.20843", "abs": "https://arxiv.org/abs/2509.20843", "authors": ["Ziang Luo", "Kangan Qian", "Jiahua Wang", "Yuechen Luo", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Sicong Jiang", "Zilin Huang", "Yifei Hu", "Yuhao Yang", "Hao Ye", "Mengmeng Yang", "Xiaojian Dong", "Kun Jiang", "Diange Yang"], "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Vision-Language Models(VLMs) have demonstrated significant potential for\nend-to-end autonomous driving, yet a substantial gap remains between their\ncurrent capabilities and the reliability necessary for real-world deployment. A\ncritical challenge is their fragility, characterized by hallucinations and poor\ngeneralization in out-of-distribution (OOD) scenarios. To bridge this gap, we\nintroduce MTRDrive, a novel framework that integrates procedural driving\nexperiences with a dynamic toolkit to enhance generalization and proactive\ndecision-making.\n  MTRDrive addresses these limitations through a closed-loop system that\ncombines a memory-based experience retrieval mechanism with dynamic toolkits.\nThis synergy enables the model to interact more effectively with its\nenvironment, improving both reasoning and decision-making capabilities with the\nhelp of our memory-tool synergistic reasoning. Additionally, we introduce a new\nbenchmark based on complex Roadwork construction scenarios to rigorously\nevaluate zero-shot generalization.\n  Extensive experiments demonstrate the superior effectiveness of our approach.\nOn the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an\nexceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art\nperformance bar on high-level planning, with a driving metric score of 79.8\\%\nand a planning accuracy of 82.6\\%. Rigorous zero-shot evaluation on the new\nRoadwork-VLM benchmark shows a strong ability to reason robustly in unseen\nscenarios, achieving a driving metric score of 80.2\\%. These results highlight\nMTRDrive's potential to advance autonomous driving toward safer and more\nreliable systems.", "AI": {"tldr": "MTRDrive\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bb0\u5fc6\u68c0\u7d22\u548c\u52a8\u6001\u5de5\u5177\u5305\uff0c\u63d0\u5347VLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709VLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u63d0\u5347\u5176\u5b9e\u7528\u6027\u3002", "method": "MTRDrive\u91c7\u7528\u95ed\u73af\u7cfb\u7edf\uff0c\u7ed3\u5408\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u548c\u52a8\u6001\u5de5\u5177\u5305\uff0c\u5b9e\u73b0\u534f\u540c\u63a8\u7406\u3002", "result": "\u5728NAVSIM\u548cRoadwork-VLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMTRDrive\u8868\u73b0\u5353\u8d8a\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u3002", "conclusion": "MTRDrive\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.20917", "pdf": "https://arxiv.org/pdf/2509.20917", "abs": "https://arxiv.org/abs/2509.20917", "authors": ["Xiaohan Ye", "Kui Wu", "Zherong Pan", "Taku Komura"], "title": "Efficient Differentiable Contact Model with Long-range Influence", "categories": ["cs.RO"], "comment": null, "summary": "With the maturation of differentiable physics, its role in various downstream\napplications: such as model predictive control, robotic design optimization,\nand neural PDE solvers, has become increasingly important. However, the\nderivative information provided by differentiable simulators can exhibit abrupt\nchanges or vanish altogether, impeding the convergence of gradient-based\noptimizers. In this work, we demonstrate that such erratic gradient behavior is\nclosely tied to the design of contact models. We further introduce a set of\nproperties that a contact model must satisfy to ensure well-behaved gradient\ninformation. Lastly, we present a practical contact model for differentiable\nrigid-body simulators that satisfies all of these properties while maintaining\ncomputational efficiency. Our experiments show that, even from simple\ninitializations, our contact model can discover complex, contact-rich control\nsignals, enabling the successful execution of a range of downstream locomotion\nand manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u5fae\u5206\u7269\u7406\u4e2d\u56e0\u63a5\u89e6\u6a21\u578b\u8bbe\u8ba1\u5bfc\u81f4\u68af\u5ea6\u4fe1\u606f\u5f02\u5e38\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6ee1\u8db3\u7279\u5b9a\u5c5e\u6027\u7684\u63a5\u89e6\u6a21\u578b\uff0c\u4ee5\u6539\u5584\u68af\u5ea6\u4fe1\u606f\u7684\u884c\u4e3a\uff0c\u6700\u7ec8\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53ef\u5fae\u5206\u7269\u7406\u5728\u591a\u9886\u57df\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u68af\u5ea6\u4fe1\u606f\u7684\u5f02\u5e38\u95ee\u9898\u963b\u788d\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5668\u7684\u6536\u655b\uff0c\u5c24\u5176\u662f\u63a5\u89e6\u6a21\u578b\u7684\u8bbe\u8ba1\u5bf9\u6b64\u6709\u663e\u8457\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u68af\u5ea6\u4fe1\u606f\u5f02\u5e38\u4e0e\u63a5\u89e6\u6a21\u578b\u8bbe\u8ba1\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u7ec4\u63a5\u89e6\u6a21\u578b\u9700\u6ee1\u8db3\u7684\u5c5e\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u63a5\u89e6\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u63a5\u89e6\u6a21\u578b\u80fd\u4ece\u7b80\u5355\u521d\u59cb\u5316\u4e2d\u751f\u6210\u590d\u6742\u7684\u63a5\u89e6\u4e30\u5bcc\u63a7\u5236\u4fe1\u53f7\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u548c\u63a7\u5236\u4efb\u52a1\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u63a5\u89e6\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u4fe1\u606f\u5f02\u5e38\u95ee\u9898\uff0c\u4e3a\u53ef\u5fae\u5206\u7269\u7406\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u57fa\u7840\u3002"}}
{"id": "2509.20938", "pdf": "https://arxiv.org/pdf/2509.20938", "abs": "https://arxiv.org/abs/2509.20938", "authors": ["Jianbo Zhao", "Taiyu Ban", "Xiangjie Li", "Xingtai Gui", "Hangning Zhou", "Lei Liu", "Hongwei Zhao", "Bin Li"], "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The inherent sequential modeling capabilities of autoregressive models make\nthem a formidable baseline for end-to-end planning in autonomous driving.\nNevertheless, their performance is constrained by a spatio-temporal\nmisalignment, as the planner must condition future actions on past sensory\ndata. This creates an inconsistent worldview, limiting the upper bound of\nperformance for an otherwise powerful approach. To address this, we propose a\nTime-Invariant Spatial Alignment (TISA) module that learns to project initial\nenvironmental features into a consistent ego-centric frame for each future time\nstep, effectively correcting the agent's worldview without explicit future\nscene prediction. In addition, we employ a kinematic action prediction head\n(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.\nFinally, we introduce a multi-objective post-training stage using Direct\nPreference Optimization (DPO) to move beyond pure imitation. Our approach\nprovides targeted feedback on specific driving behaviors, offering a more\nfine-grained learning signal than the single, overall objective used in\nstandard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM\ndataset among autoregressive models. The video document is available at\nhttps://tisa-dpo-e2e.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u4e0d\u53d8\u7a7a\u95f4\u5bf9\u9f50\uff08TISA\uff09\u6a21\u5757\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u52a8\u4f5c\u9884\u6d4b\u548c\u591a\u76ee\u6807\u8bad\u7ec3\uff0c\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65f6\u7a7a\u4e0d\u4e00\u81f4\u6027\u5bfc\u81f4\u7684\u4e16\u754c\u89c2\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u91c7\u7528TISA\u6a21\u5757\u5b66\u4e60\u5c06\u521d\u59cb\u73af\u5883\u7279\u5f81\u6295\u5f71\u5230\u4e00\u81f4\u7684\u81ea\u6211\u4e2d\u5fc3\u5e27\u4e2d\uff0c\u5e76\u7ed3\u5408\u8fd0\u52a8\u5b66\u52a8\u4f5c\u9884\u6d4b\u548c\u591a\u76ee\u6807DPO\u8bad\u7ec3\u3002", "result": "\u5728NAVSIM\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.8 PDMS\uff0c\u662f\u76ee\u524d\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "TISA\u6a21\u5757\u548c\u591a\u76ee\u6807\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u6027\u80fd\u3002"}}
{"id": "2509.20964", "pdf": "https://arxiv.org/pdf/2509.20964", "abs": "https://arxiv.org/abs/2509.20964", "authors": ["Rubaiyat Tasnim Chowdhury", "Nayan Bala", "Ronojoy Roy", "Tarek Mahmud"], "title": "BactoBot: A Low-Cost, Bacteria-Inspired Soft Underwater Robot for Marine Exploration", "categories": ["cs.RO"], "comment": "8 pages, 4 figures. Project repository available at\n  https://github.com/rubaiyattasnim/BactoBot", "summary": "Traditional rigid underwater vehicles pose risks to delicate marine\necosystems. This paper presents BactoBot, a low-cost, soft underwater robot\ndesigned for safe and gentle marine exploration. Inspired by bacterial\nflagellar propulsion, BactoBot features 12 flexible, silicone-based arms\narranged on a 3D-printed dodecahedral frame. The design provides inherent\ncompliance, redundancy, and the potential for omnidirectional movement. The\nprototype was fabricated using accessible DIY methods, including food-grade\nsilicone molding, 3D printing, and off-the-shelf microcontrollers.\nWaterproofing and buoyancy calibration protocols were developed, and the robot\nwas successfully tested in a controlled water tank, demonstrating forward\nmotion and turning. The results validate the feasibility of replicating complex\nbiological locomotion at low cost. The project lays a foundation for\nenvironmentally conscious robotic tools, particularly for marine science in\nresource-constrained settings, and identifies pathways toward autonomous\noperation and field deployment.", "AI": {"tldr": "BactoBot\u662f\u4e00\u6b3e\u4f4e\u6210\u672c\u3001\u67d4\u8f6f\u7684\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u65e8\u5728\u5b89\u5168\u63a2\u7d22\u6d77\u6d0b\u73af\u5883\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u7ec6\u83cc\u97ad\u6bdb\u63a8\u8fdb\u673a\u5236\uff0c\u5c55\u793a\u4e86\u53ef\u884c\u6027\u548c\u73af\u4fdd\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u6c34\u4e0b\u673a\u5668\u4eba\u5bf9\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u6709\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u752812\u6761\u67d4\u6027\u7845\u80f6\u81c2\u548c3D\u6253\u5370\u6846\u67b6\uff0c\u901a\u8fc7DIY\u65b9\u6cd5\u5236\u4f5c\u539f\u578b\uff0c\u5e76\u4f18\u5316\u9632\u6c34\u548c\u6d6e\u529b\u6821\u51c6\u3002", "result": "\u5728\u53d7\u63a7\u6c34\u7bb1\u4e2d\u6d4b\u8bd5\u6210\u529f\uff0c\u5b9e\u73b0\u4e86\u524d\u8fdb\u548c\u8f6c\u5411\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u590d\u73b0\u590d\u6742\u751f\u7269\u8fd0\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "BactoBot\u4e3a\u73af\u4fdd\u673a\u5668\u4eba\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u6d77\u6d0b\u79d1\u5b66\u7814\u7a76\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u81ea\u4e3b\u64cd\u4f5c\u548c\u5b9e\u5730\u90e8\u7f72\u3002"}}
{"id": "2509.21006", "pdf": "https://arxiv.org/pdf/2509.21006", "abs": "https://arxiv.org/abs/2509.21006", "authors": ["Konstantin Gubernatorov", "Artem Voronov", "Roman Voronov", "Sergei Pasynkov", "Stepan Perminov", "Ziang Guo", "Dzmitry Tsetserukou"], "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We address natural language pick-and-place in unseen, unpredictable indoor\nenvironments with AnywhereVLA, a modular framework for mobile manipulation. A\nuser text prompt serves as an entry point and is parsed into a structured task\ngraph that conditions classical SLAM with LiDAR and cameras, metric semantic\nmapping, and a task-aware frontier exploration policy. An approach planner then\nselects visibility and reachability aware pre grasp base poses. For\ninteraction, a compact SmolVLA manipulation head is fine tuned on platform pick\nand place trajectories for the SO-101 by TheRobotStudio, grounding local visual\ncontext and sub-goals into grasp and place proposals. The full system runs\nfully onboard on consumer-level hardware, with Jetson Orin NX for perception\nand VLA and an Intel NUC for SLAM, exploration, and control, sustaining\nreal-time operation. We evaluated AnywhereVLA in a multi-room lab under static\nscenes and normal human motion. In this setting, the system achieves a $46\\%$\noverall task success rate while maintaining throughput on embedded compute. By\ncombining a classical stack with a fine-tuned VLA manipulation, the system\ninherits the reliability of geometry-based navigation with the agility and task\ngeneralization of language-conditioned manipulation.", "AI": {"tldr": "AnywhereVLA \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u7684\u62fe\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684\u5bfc\u822a\u4e0e\u8bed\u8a00\u6761\u4ef6\u64cd\u7eb5\u7684\u4f18\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u53ef\u9760\u7684\u62fe\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u89e3\u6790\u7528\u6237\u6587\u672c\u4e3a\u4efb\u52a1\u56fe\u3001\u7ed3\u5408\u7ecf\u5178SLAM\u4e0eLiDAR\u3001\u8bed\u4e49\u6620\u5c04\u3001\u4efb\u52a1\u611f\u77e5\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5fae\u8c03\u7684\u7d27\u51d1\u64cd\u7eb5\u5934\uff08SmolVLA\uff09\u3002", "result": "\u7cfb\u7edf\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e8646%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u64cd\u4f5c\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7ecf\u5178\u5bfc\u822a\u4e0e\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u64cd\u7eb5\uff0c\u7cfb\u7edf\u65e2\u53ef\u9760\u53c8\u5177\u5907\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.21020", "pdf": "https://arxiv.org/pdf/2509.21020", "abs": "https://arxiv.org/abs/2509.21020", "authors": ["Abdelaziz Shaarawy", "Cansu Erdogan", "Rustam Stolkin", "Alireza Rastegarpanah"], "title": "Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting", "categories": ["cs.RO"], "comment": null, "summary": "Electric-vehicle (EV) battery disassembly requires precise multi-robot\ncoordination, short and reliable motions, and robust collision safety in\ncluttered, dynamic scenes. We propose a four-layer task-and-motion planning\n(TAMP) framework that couples symbolic task planning and cost- and\naccessibility-aware allocation with a TP-GMM-guided motion planner learned from\ndemonstrations. Stereo vision with YOLOv8 provides real-time component\nlocalization, while OctoMap-based 3D mapping and FCL(Flexible Collision\nLibrary) checks in MoveIt unify predictive digital-twin collision checking with\nreactive, vision-based avoidance. Validated on two UR10e robots across cable,\nbusbar, service plug, and three leaf-cell removals, the approach yields\nsubstantially more compact and safer motions than a default RRTConnect baseline\nunder identical perception and task assignments: average end-effector path\nlength drops by $-63.3\\%$ and makespan by $-8.1\\%$; per-arm swept volumes\nshrink (R1: $0.583\\rightarrow0.139\\,\\mathrm{m}^3$; R2:\n$0.696\\rightarrow0.252\\,\\mathrm{m}^3$), and mutual overlap decreases by $47\\%$\n($0.064\\rightarrow0.034\\,\\mathrm{m}^3$). These results highlight improved\nautonomy, precision, and safety for multi-robot EV battery disassembly in\nunstructured, dynamic environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u5c42\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u540c\u62c6\u5378\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\uff0c\u7ed3\u5408\u4e86\u7b26\u53f7\u4efb\u52a1\u89c4\u5212\u548c\u57fa\u4e8e\u6f14\u793a\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u8def\u5f84\u7d27\u51d1\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u62c6\u5378\u9700\u8981\u591a\u673a\u5668\u4eba\u7cbe\u786e\u534f\u540c\u3001\u77ed\u800c\u53ef\u9760\u7684\u8fd0\u52a8\u4ee5\u53ca\u5728\u6742\u4e71\u52a8\u6001\u573a\u666f\u4e2d\u7684\u78b0\u649e\u5b89\u5168\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u91c7\u7528\u56db\u5c42TAMP\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u4efb\u52a1\u89c4\u5212\u3001\u6210\u672c\u4e0e\u53ef\u8fbe\u6027\u5206\u914d\uff0c\u4ee5\u53ca\u57fa\u4e8eTP-GMM\u7684\u8fd0\u52a8\u89c4\u5212\uff1b\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u548cYOLOv8\u5b9e\u65f6\u5b9a\u4f4d\u7ec4\u4ef6\uff0c\u5229\u7528OctoMap\u548cFCL\u8fdb\u884c\u78b0\u649e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebfRRTConnect\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4f7f\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u957f\u5ea6\u5e73\u5747\u51cf\u5c1163.3%\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed8.1%\uff0c\u4e14\u673a\u5668\u4eba\u95f4\u7684\u78b0\u649e\u4f53\u79ef\u51cf\u5c1147%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u62c6\u5378\u7535\u6c60\u7684\u81ea\u4e3b\u6027\u3001\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.21027", "pdf": "https://arxiv.org/pdf/2509.21027", "abs": "https://arxiv.org/abs/2509.21027", "authors": ["Sibo Li", "Qianyue Hao", "Yu Shang", "Yong Li"], "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic world models are a promising paradigm for forecasting future\nenvironment states, yet their inference speed and the physical plausibility of\ngenerated trajectories remain critical bottlenecks, limiting their real-world\napplications. This stems from the redundancy of the prevailing frame-to-frame\ngeneration approach, where the model conducts costly computation on similar\nframes, as well as neglecting the semantic importance of key transitions. To\naddress this inefficiency, we propose KeyWorld, a framework that improves\ntext-conditioned robotic world models by concentrating transformers computation\non a few semantic key frames while employing a lightweight convolutional model\nto fill the intermediate frames. Specifically, KeyWorld first identifies\nsignificant transitions by iteratively simplifying the robot's motion\ntrajectories, obtaining the ground truth key frames. Then, a DiT model is\ntrained to reason and generate these physically meaningful key frames from\ntextual task descriptions. Finally, a lightweight interpolator efficiently\nreconstructs the full video by inpainting all intermediate frames. Evaluations\non the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$\nacceleration compared to the frame-to-frame generation baseline, and focusing\non the motion-aware key frames further contributes to the physical validity of\nthe generated videos, especially on complex tasks. Our approach highlights a\npractical path toward deploying world models in real-time robotic control and\nother domains requiring both efficient and effective world models. Code is\nreleased at https://anonymous.4open.science/r/Keyworld-E43D.", "AI": {"tldr": "KeyWorld\u901a\u8fc7\u4e13\u6ce8\u4e8e\u8bed\u4e49\u5173\u952e\u5e27\u548c\u8f7b\u91cf\u7ea7\u63d2\u503c\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u4e0e\u751f\u6210\u8f68\u8ff9\u7684\u7269\u7406\u5408\u7406\u6027\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faKeyWorld\u6846\u67b6\uff0c\u901a\u8fc7Transformer\u805a\u7126\u5173\u952e\u5e27\u4e0e\u8f7b\u91cf\u5377\u79ef\u6a21\u578b\u586b\u5145\u4e2d\u95f4\u5e27\uff0c\u4f18\u5316\u751f\u6210\u6548\u7387\u4e0e\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKeyWorld\u5b9e\u73b0\u4e865.68\u500d\u7684\u52a0\u901f\uff0c\u5e76\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u6709\u6548\u6027\u3002", "conclusion": "KeyWorld\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21045", "pdf": "https://arxiv.org/pdf/2509.21045", "abs": "https://arxiv.org/abs/2509.21045", "authors": ["Mahya Ramezani", "M. Amin Alandihallaj", "Bar\u0131\u015f Can Yal\u00e7\u0131n", "Miguel Angel Olivares Mendez", "Holger Voos"], "title": "MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation", "categories": ["cs.RO", "cs.LG"], "comment": "Pre-print version submitted to IEEE IROS", "summary": "This paper presents an integrated Reinforcement Learning (RL) and Model\nPredictive Control (MPC) framework for autonomous satellite docking with a\npartially filled fuel tank. Traditional docking control faces challenges due to\nfuel sloshing in microgravity, which induces unpredictable forces affecting\nstability. To address this, we integrate Proximal Policy Optimization (PPO) and\nSoft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive\ncapabilities to accelerate RL training and improve control robustness. The\nproposed approach is validated through Zero-G Lab of SnT experiments for planar\nstabilization and high-fidelity numerical simulations for 6-DOF docking with\nfuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves\nsuperior docking accuracy, higher success rates, and lower control effort,\noutperforming standalone RL and PPO-MPC methods. This study advances\nfuel-efficient and disturbance-resilient satellite docking, enhancing the\nfeasibility of on-orbit refueling and servicing missions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u586b\u5145\u71c3\u6599\u7bb1\u7684\u536b\u661f\u81ea\u4e3b\u5bf9\u63a5\u95ee\u9898\u3002\u901a\u8fc7\u6574\u5408PPO\u548cSAC\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0eMPC\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u536b\u661f\u5bf9\u63a5\u8fc7\u7a0b\u4e2d\u71c3\u6599\u6643\u52a8\u5728\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u529b\uff0c\u5f71\u54cd\u7a33\u5b9a\u6027\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7ed3\u5408Proximal Policy Optimization\uff08PPO\uff09\u548cSoft Actor-Critic\uff08SAC\uff09\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u5229\u7528MPC\u7684\u9884\u6d4b\u80fd\u529b\u52a0\u901fRL\u8bad\u7ec3\u5e76\u63d0\u5347\u63a7\u5236\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cSAC-MPC\u65b9\u6cd5\u5728\u5bf9\u63a5\u7cbe\u5ea6\u3001\u6210\u529f\u7387\u548c\u63a7\u5236\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u72ec\u7acb\u7684RL\u548cPPO-MPC\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u71c3\u6599\u9ad8\u6548\u548c\u6297\u5e72\u6270\u7684\u536b\u661f\u5bf9\u63a5\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u5728\u8f68\u71c3\u6599\u8865\u7ed9\u548c\u7ef4\u62a4\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.21073", "pdf": "https://arxiv.org/pdf/2509.21073", "abs": "https://arxiv.org/abs/2509.21073", "authors": ["Simon Kristoffersson Lind", "Jialong Li", "Maj Stenmark", "Volker Kr\u00fcger"], "title": "Normalizing Flows are Capable Visuomotor Policy Learning Models", "categories": ["cs.RO"], "comment": null, "summary": "The field of general purpose robotics has recently embraced powerful\nprobabilistic models, such as diffusion models, to model and learn complex\nbehaviors. However, these models often come with significant trade-offs, namely\nhigh computational costs for inference and a fundamental inability to quantify\noutput uncertainty. We argue that a model's trustworthiness, a critical factor\nfor reliable, general-purpose robotics, is inherently linked to its ability to\nprovide confidence measures.\n  In this work, we introduce Normalizing Flows Policy, a novel visuomotor\npolicy learning model based on Normalizing Flows. We show that Normalizing\nFlows are a natural and powerful alternative to diffusion models, providing\nboth a statistically sound measure of confidence and a highly efficient\ninference process. Through comprehensive experiments across four distinct\nsimulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves\nperformance comparable to, and often surpassing, Diffusion Policy, and it does\nso not only with improved sample efficiency but also with up to 30 times faster\ninference. Additionally, our ablation study validates several key architectural\nand training techniques that enable Normalizing Flows to perform well in this\ndomain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNormalizing Flows\u7684\u65b0\u578b\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u65e0\u6cd5\u91cf\u5316\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNormalizing Flows\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aNormalizing Flows Policy\u7684\u65b0\u6a21\u578b\uff0c\u5229\u7528Normalizing Flows\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e0a\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u548c\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cNormalizing Flows Policy\u6027\u80fd\u4f18\u4e8e\u6216\u6301\u5e73Diffusion Policy\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe30\u500d\u3002", "conclusion": "Normalizing Flows Policy\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u53ef\u9760\u3001\u901a\u7528\u7684\u673a\u5668\u4eba\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.21085", "pdf": "https://arxiv.org/pdf/2509.21085", "abs": "https://arxiv.org/abs/2509.21085", "authors": ["Chenyu Zhao", "Jingao Xu", "Ciyu Ruan", "Haoyang Wang", "Shengbo Wang", "Jiaqi Li", "Jirong Zha", "Weijie Hong", "Zheng Yang", "Yunhao Liu", "Xiao-Ping Zhang", "Xinlei Chen"], "title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection", "categories": ["cs.RO", "cs.NI"], "comment": null, "summary": "Drone-based rapid and accurate environmental edge detection is highly\nadvantageous for tasks such as disaster relief and autonomous navigation.\nCurrent methods, using radars or cameras, raise deployment costs and burden\nlightweight drones with high computational demands. In this paper, we propose\nAirTouch, a system that transforms the ground effect from a stability \"foe\" in\ntraditional flight control views, into a \"friend\" for accurate and efficient\nedge detection. Our key insight is that analyzing drone basic attitude sensor\nreadings and flight commands allows us to detect ground effect changes. Such\nchanges typically indicate the drone flying over a boundary of two materials,\nmaking this information valuable for edge detection. We approach this insight\nthrough theoretical analysis, algorithm design, and implementation, fully\nleveraging the ground effect as a new sensing modality without compromising\ndrone flight stability, thereby achieving accurate and efficient scene edge\ndetection. We also compare this new sensing modality with vision-based methods\nto clarify its exclusive advantages in resource efficiency and detection\ncapability. Extensive evaluations demonstrate that our system achieves a high\ndetection accuracy with mean detection distance errors of 0.051m, outperforming\nthe baseline method performance by 86%. With such detection performance, our\nsystem requires only 43 mW power consumption, contributing to this new sensing\nmodality for low-cost and highly efficient edge detection.", "AI": {"tldr": "AirTouch\u7cfb\u7edf\u5229\u7528\u5730\u9762\u6548\u5e94\u53d8\u5316\u8fdb\u884c\u9ad8\u6548\u8fb9\u7f18\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u80fd\u8017\u4f4e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u96f7\u8fbe\u6216\u76f8\u673a\uff09\u90e8\u7f72\u6210\u672c\u9ad8\u4e14\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u8f7b\u91cf\u5316\u65e0\u4eba\u673a\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u5206\u6790\u65e0\u4eba\u673a\u59ff\u6001\u4f20\u611f\u5668\u6570\u636e\u548c\u98de\u884c\u6307\u4ee4\uff0c\u68c0\u6d4b\u5730\u9762\u6548\u5e94\u53d8\u5316\uff0c\u5b9e\u73b0\u8fb9\u7f18\u68c0\u6d4b\u3002", "result": "\u5e73\u5747\u68c0\u6d4b\u8ddd\u79bb\u8bef\u5dee\u4e3a0.051m\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd586%\uff0c\u4ec5\u970043mW\u529f\u8017\u3002", "conclusion": "AirTouch\u4e3a\u4f4e\u6210\u672c\u9ad8\u6548\u8fb9\u7f18\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f20\u611f\u6a21\u5f0f\u3002"}}
{"id": "2509.21107", "pdf": "https://arxiv.org/pdf/2509.21107", "abs": "https://arxiv.org/abs/2509.21107", "authors": ["William Barron", "Xiaoxiang Dong", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Cross-Modal Instructions for Robot Motion Generation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Teaching robots novel behaviors typically requires motion demonstrations via\nteleoperation or kinaesthetic teaching, that is, physically guiding the robot.\nWhile recent work has explored using human sketches to specify desired\nbehaviors, data collection remains cumbersome, and demonstration datasets are\ndifficult to scale. In this paper, we introduce an alternative paradigm,\nLearning from Cross-Modal Instructions, where robots are shaped by\ndemonstrations in the form of rough annotations, which can contain free-form\ntext labels, and are used in lieu of physical motion. We introduce the\nCrossInstruct framework, which integrates cross-modal instructions as examples\ninto the context input to a foundational vision-language model (VLM). The VLM\nthen iteratively queries a smaller, fine-tuned model, and synthesizes the\ndesired motion over multiple 2D views. These are then subsequently fused into a\ncoherent distribution over 3D motion trajectories in the robot's workspace. By\nincorporating the reasoning of the large VLM with a fine-grained pointing\nmodel, CrossInstruct produces executable robot behaviors that generalize beyond\nthe environment of in the limited set of instruction examples. We then\nintroduce a downstream reinforcement learning pipeline that leverages\nCrossInstruct outputs to efficiently learn policies to complete fine-grained\ntasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and\nreal hardware, demonstrating effectiveness without additional fine-tuning and\nproviding a strong initialization for policies subsequently refined via\nreinforcement learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86CrossInstruct\u6846\u67b6\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6307\u4ee4\uff08\u5982\u6587\u672c\u6807\u7b7e\uff09\u4ee3\u66ff\u7269\u7406\u6f14\u793a\u6765\u6307\u5bfc\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u63a8\u7406\u4e0e\u5c0f\u6a21\u578b\u7cbe\u51c6\u6307\u5411\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u884c\u4e3a\u5b66\u4e60\u4f9d\u8d56\u7269\u7406\u6f14\u793a\uff0c\u6570\u636e\u6536\u96c6\u56f0\u96be\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u5bfb\u6c42\u66ff\u4ee3\u65b9\u5f0f\uff0c\u5373\u901a\u8fc7\u8de8\u6a21\u6001\u6307\u4ee4\uff08\u5982\u6587\u672c\u6807\u6ce8\uff09\u6765\u6307\u5bfc\u673a\u5668\u4eba\u3002", "method": "\u91c7\u7528CrossInstruct\u6846\u67b6\uff0c\u5c06\u8de8\u6a21\u6001\u6307\u4ee4\u8f93\u5165\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u7ed3\u5408\u5c0f\u6a21\u578b\u7cbe\u51c6\u6307\u5411\uff0c\u8fed\u4ee3\u751f\u62102D\u89c6\u56fe\u5e76\u878d\u5408\u4e3a3D\u8fd0\u52a8\u8f68\u8ff9\uff0c\u6700\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u786c\u4ef6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u80fd\u751f\u6210\u53ef\u6267\u884c\u7684\u6cdb\u5316\u884c\u4e3a\uff0c\u5e76\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002", "conclusion": "CrossInstruct\u901a\u8fc7\u8de8\u6a21\u6001\u6307\u4ee4\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u4eba\u884c\u4e3a\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u7269\u7406\u6f14\u793a\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.21122", "pdf": "https://arxiv.org/pdf/2509.21122", "abs": "https://arxiv.org/abs/2509.21122", "authors": ["Mingjiang Liu", "Hailong Huang"], "title": "Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study", "categories": ["cs.RO"], "comment": "Accepted for presentation at the Advancements in Aerial Physical\n  Interaction Workshop of the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2025", "summary": "This paper addresses a drone ball-balancing task, in which a drone stabilizes\na ball atop a movable beam through cable-based interaction. We propose a\nhierarchical control framework that decouples high-level balancing policy from\nlow-level drone control, and train a reinforcement learning (RL) policy to\nhandle the high-level decision-making. Simulation results show that the RL\npolicy achieves superior performance compared to carefully tuned PID\ncontrollers within the same hierarchical structure. Through systematic\ncomparative analysis, we demonstrate that RL's advantage stems not from\nimproved parameter tuning or inherent nonlinear mapping capabilities, but from\nits ability to effectively utilize richer state observations. These findings\nunderscore the critical role of comprehensive state representation in\nlearning-based systems and suggest that enhanced sensing could be instrumental\nin improving controller performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u901a\u8fc7\u7f06\u7ef3\u4ea4\u4e92\u5e73\u8861\u7403\u7684\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9ad8\u5c42\u7b56\u7565\u3002\u7ed3\u679c\u663e\u793a\u5f3a\u5316\u5b66\u4e60\u4f18\u4e8ePID\u63a7\u5236\u5668\uff0c\u4e3b\u8981\u5f97\u76ca\u4e8e\u5176\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u72b6\u6001\u89c2\u6d4b\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u65e0\u4eba\u673a\u5728\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5982\u4f55\u901a\u8fc7\u5206\u5c42\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5e73\u8861\u7403\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u9ad8\u5c42\u5e73\u8861\u7b56\u7565\u4e0e\u5e95\u5c42\u65e0\u4eba\u673a\u63a7\u5236\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9ad8\u5c42\u7b56\u7565\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u4f18\u4e8e\u7cbe\u5fc3\u8c03\u4f18\u7684PID\u63a7\u5236\u5668\uff0c\u5176\u4f18\u52bf\u6e90\u4e8e\u80fd\u591f\u6709\u6548\u5229\u7528\u66f4\u5168\u9762\u7684\u72b6\u6001\u89c2\u6d4b\u4fe1\u606f\uff0c\u800c\u975e\u975e\u7ebf\u6027\u6620\u5c04\u80fd\u529b\u6216\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5168\u9762\u7684\u72b6\u6001\u8868\u5f81\u5728\u5b66\u4e60\u578b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u5347\u611f\u77e5\u80fd\u529b\u53ef\u80fd\u663e\u8457\u6539\u5584\u63a7\u5236\u5668\u6027\u80fd\u3002"}}
{"id": "2509.21143", "pdf": "https://arxiv.org/pdf/2509.21143", "abs": "https://arxiv.org/abs/2509.21143", "authors": ["Junfeng Yan", "Biao Wu", "Meng Fang", "Ling Chen"], "title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems", "categories": ["cs.RO", "cs.CL", "F.2.2; I.2.7"], "comment": "10 pages, 5 figures,", "summary": "Multimodal agents have demonstrated strong performance in general GUI\ninteractions, but their application in automotive systems has been largely\nunexplored. In-vehicle GUIs present distinct challenges: drivers' limited\nattention, strict safety requirements, and complex location-based interaction\npatterns. To address these challenges, we introduce Automotive-ENV, the first\nhigh-fidelity benchmark and interaction environment tailored for vehicle GUIs.\nThis platform defines 185 parameterized tasks spanning explicit control,\nimplicit intent understanding, and safety-aware tasks, and provides structured\nmultimodal observations with precise programmatic checks for reproducible\nevaluation. Building on this benchmark, we propose ASURADA, a geo-aware\nmultimodal agent that integrates GPS-informed context to dynamically adjust\nactions based on location, environmental conditions, and regional driving\nnorms. Experiments show that geo-aware information significantly improves\nsuccess on safety-aware tasks, highlighting the importance of location-based\ncontext in automotive environments. We will release Automotive-ENV, complete\nwith all tasks and benchmarking tools, to further the development of safe and\nadaptive in-vehicle agents.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9488\u5bf9\u6c7d\u8f66GUI\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0Automotive-ENV\u548c\u4e00\u79cd\u540d\u4e3aASURADA\u7684\u5730\u7406\u611f\u77e5\u591a\u6a21\u6001\u4ee3\u7406\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u4f4d\u7f6e\u7684\u60c5\u5883\u5728\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u4ee3\u7406\u5728\u6c7d\u8f66\u7cfb\u7edf\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u8f66\u5185GUI\u9762\u4e34\u7684\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u6709\u9650\u3001\u5b89\u5168\u8981\u6c42\u4e25\u683c\u548c\u590d\u6742\u7684\u4f4d\u7f6e\u4ea4\u4e92\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Automotive-ENV\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b185\u9879\u53c2\u6570\u5316\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86ASURADA\u4ee3\u7406\uff0c\u5229\u7528GPS\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5730\u7406\u611f\u77e5\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "Automotive-ENV\u548cASURADA\u5c55\u793a\u4e86\u4f4d\u7f6e\u60c5\u5883\u5bf9\u6c7d\u8f66\u7cfb\u7edf\u4e2d\u591a\u6a21\u6001\u4ee3\u7406\u7684\u91cd\u8981\u6027\uff0c\u5e73\u53f0\u5c06\u88ab\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2509.21145", "pdf": "https://arxiv.org/pdf/2509.21145", "abs": "https://arxiv.org/abs/2509.21145", "authors": ["Md Faizal Karim", "Vignesh Vembar", "Keshab Patra", "Gaurav Singh", "K Madhava Krishna"], "title": "DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps", "categories": ["cs.RO"], "comment": null, "summary": "Reliable dual-arm grasping is essential for manipulating large and complex\nobjects but remains a challenging problem due to stability, collision, and\ngeneralization requirements. Prior methods typically decompose the task into\ntwo independent grasp proposals, relying on region priors or heuristics that\nlimit generalization and provide no principled guarantee of stability. We\npropose DAGDiff, an end-to-end framework that directly denoises to grasp pairs\nin the SE(3) x SE(3) space. Our key insight is that stability and collision can\nbe enforced more effectively by guiding the diffusion process with classifier\nsignals, rather than relying on explicit region detection or object priors. To\nthis end, DAGDiff integrates geometry-, stability-, and collision-aware\nguidance terms that steer the generative process toward grasps that are\nphysically valid and force-closure compliant. We comprehensively evaluate\nDAGDiff through analytical force-closure checks, collision analysis, and\nlarge-scale physics-based simulations, showing consistent improvements over\nprevious work on these metrics. Finally, we demonstrate that our framework\ngenerates dual-arm grasps directly on real-world point clouds of previously\nunseen objects, which are executed on a heterogeneous dual-arm setup where two\nmanipulators reliably grasp and lift them.", "AI": {"tldr": "DAGDiff\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u5728SE(3) x SE(3)\u7a7a\u95f4\u4e2d\u76f4\u63a5\u53bb\u566a\u751f\u6210\u53cc\u81c2\u6293\u53d6\u5bf9\uff0c\u5229\u7528\u5206\u7c7b\u5668\u4fe1\u53f7\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u6293\u53d6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u53cc\u81c2\u6293\u53d6\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u6293\u53d6\u63d0\u8bae\uff0c\u4f9d\u8d56\u533a\u57df\u5148\u9a8c\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u7a33\u5b9a\u6027\u3002", "method": "DAGDiff\u901a\u8fc7\u51e0\u4f55\u3001\u7a33\u5b9a\u6027\u548c\u78b0\u649e\u611f\u77e5\u7684\u5f15\u5bfc\u9879\uff0c\u76f4\u63a5\u5728SE(3) x SE(3)\u7a7a\u95f4\u4e2d\u751f\u6210\u6293\u53d6\u5bf9\u3002", "result": "DAGDiff\u5728\u529b\u95ed\u5408\u68c0\u67e5\u3001\u78b0\u649e\u5206\u6790\u548c\u5927\u89c4\u6a21\u7269\u7406\u4eff\u771f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DAGDiff\u80fd\u591f\u76f4\u63a5\u5728\u771f\u5b9e\u70b9\u4e91\u4e0a\u751f\u6210\u53cc\u81c2\u6293\u53d6\uff0c\u5e76\u5728\u5f02\u6784\u53cc\u81c2\u7cfb\u7edf\u4e2d\u6210\u529f\u6267\u884c\u6293\u53d6\u548c\u4e3e\u5347\u4efb\u52a1\u3002"}}
{"id": "2509.21189", "pdf": "https://arxiv.org/pdf/2509.21189", "abs": "https://arxiv.org/abs/2509.21189", "authors": ["Bhargav Chandaka", "Gloria X. Wang", "Haozhe Chen", "Henry Che", "Albert J. Zhai", "Shenlong Wang"], "title": "Human-like Navigation in a World Built for Humans", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CoRL 2025. Project website: https://reasonnav.github.io/", "summary": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.", "AI": {"tldr": "ReasonNav\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u4eff\u4eba\u7c7b\u5bfc\u822a\u884c\u4e3a\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u5927\u578b\u5efa\u7b51\u4e2d\u7684\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u5728\u964c\u751f\u73af\u5883\u4e2d\u7f3a\u4e4f\u4eba\u7c7b\u7684\u9ad8\u6548\u5bfc\u822a\u884c\u4e3a\uff08\u5982\u9605\u8bfb\u6807\u5fd7\u3001\u8be2\u95ee\u65b9\u5411\uff09\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5bfc\u822a\u5730\u6807\u7684\u7d27\u51d1\u8f93\u5165\u8f93\u51fa\u62bd\u8c61\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cReasonNav\u6210\u529f\u901a\u8fc7\u9ad8\u9636\u63a8\u7406\u5b9e\u73b0\u4e86\u9ad8\u6548\u5bfc\u822a\u3002", "conclusion": "ReasonNav\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u5bfc\u822a\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2509.21210", "pdf": "https://arxiv.org/pdf/2509.21210", "abs": "https://arxiv.org/abs/2509.21210", "authors": ["Ali Kafili Gavgani", "Amin Talaeizadeh", "Aria Alasty", "Hossein Nejat Pishkenari", "Esmaeil Najafi"], "title": "Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Conventional multi-rotors are under-actuated systems, hindering them from\nindependently controlling attitude from position. In this study, we present\nseveral distinct configurations that incorporate additional control inputs for\nmanipulating the angles of the propeller axes. This addresses the mentioned\nlimitations, making the systems \"omniorientational\". We comprehensively derived\ndetailed dynamic models for all introduced configurations and validated by a\nmethodology using Simscape Multibody simulations. Two controllers are designed:\na sliding mode controller for robust handling of disturbances and a novel\nPID-based controller with gravity compensation integrating linear and\nnon-linear allocators, designed for computational efficiency. A custom control\nallocation strategy is implemented to manage the input-non-affine nature of\nthese systems, seeking to maximize battery life by minimizing the \"Power\nConsumption Factor\" defined in this study. Moreover, the controllers\neffectively managed harsh disturbances and uncertainties. Simulations compare\nand analyze the proposed configurations and controllers, majorly considering\ntheir power consumption. Furthermore, we conduct a qualitative comparison to\nevaluate the impact of different types of uncertainties on the control system,\nhighlighting areas for potential model or hardware improvements. The analysis\nin this study provides a roadmap for future researchers to design\nomniorientational drones based on their design objectives, offering practical\ninsights into configuration selection and controller design. This research\naligns with the project SAC-1, one of the objectives of Sharif AgRoLab.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u591a\u79cd\u591a\u65cb\u7ffc\u914d\u7f6e\uff0c\u901a\u8fc7\u589e\u52a0\u63a7\u5236\u8f93\u5165\u5b9e\u73b0\u5168\u5411\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7cfb\u7edf\u7684\u6b20\u9a71\u52a8\u95ee\u9898\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6027\u80fd\u548c\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u65cb\u7ffc\u7cfb\u7edf\u65e0\u6cd5\u72ec\u7acb\u63a7\u5236\u59ff\u6001\u548c\u4f4d\u7f6e\u7684\u6b20\u9a71\u52a8\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u5168\u5411\u63a7\u5236\u3002", "method": "\u5f15\u5165\u591a\u79cd\u914d\u7f6e\uff0c\u5efa\u7acb\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u5668\u548c\u65b0\u578bPID\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7Simscape Multibody\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u63a7\u5236\u5668\u80fd\u6709\u6548\u5904\u7406\u5e72\u6270\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u914d\u7f6e\u548c\u63a7\u5236\u5668\u5728\u80fd\u8017\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5168\u5411\u65e0\u4eba\u673a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u914d\u7f6e\u9009\u62e9\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2509.21231", "pdf": "https://arxiv.org/pdf/2509.21231", "abs": "https://arxiv.org/abs/2509.21231", "authors": ["Jaehwi Jang", "Zhuoheng Wang", "Ziyi Zhou", "Feiyang Wu", "Ye Zhao"], "title": "SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation", "categories": ["cs.RO"], "comment": "9 pages, 5 figures", "summary": "Arm end-effector stabilization is essential for humanoid loco-manipulation\ntasks, yet it remains challenging due to the high degrees of freedom and\ninherent dynamic instability of bipedal robot structures. Previous model-based\ncontrollers achieve precise end-effector control but rely on precise dynamics\nmodeling and estimation, which often struggle to capture real-world factors\n(e.g., friction and backlash) and thus degrade in practice. On the other hand,\nlearning-based methods can better mitigate these factors via exploration and\ndomain randomization, and have shown potential in real-world use. However, they\noften overfit to training conditions, requiring retraining with the entire\nbody, and still struggle to adapt to unseen scenarios. To address these\nchallenges, we propose a novel stable end-effector control (SEEC) framework\nwith model-enhanced residual learning that learns to achieve precise and robust\nend-effector compensation for lower-body induced disturbances through\nmodel-guided reinforcement learning (RL) with a perturbation generator. This\ndesign allows the upper-body policy to achieve accurate end-effector\nstabilization as well as adapt to unseen locomotion controllers with no\nadditional training. We validate our framework in different simulators and\ntransfer trained policies to the Booster T1 humanoid robot. Experiments\ndemonstrate that our method consistently outperforms baselines and robustly\nhandles diverse and demanding loco-manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEEC\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u624b\u81c2\u672b\u7aef\u6267\u884c\u5668\u7684\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u53cc\u8db3\u673a\u5668\u4eba\u7531\u4e8e\u5176\u9ad8\u81ea\u7531\u5ea6\u548c\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\uff0c\u624b\u81c2\u672b\u7aef\u6267\u884c\u5668\u7684\u7a33\u5b9a\u4ecd\u5177\u6311\u6218\u6027\u3002\u4f20\u7edf\u6a21\u578b\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u5efa\u6a21\u4f46\u96be\u4ee5\u9002\u5e94\u5b9e\u9645\u56e0\u7d20\uff08\u5982\u6469\u64e6\u548c\u95f4\u9699\uff09\uff0c\u800c\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u7f13\u548c\u8fd9\u4e9b\u95ee\u9898\u5374\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002", "method": "SEEC\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u6270\u52a8\u751f\u6210\u5668\uff0c\u5b66\u4e60\u5bf9\u4e0b\u534a\u8eab\u6270\u52a8\u7684\u7cbe\u786e\u8865\u507f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u672a\u77e5\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u62df\u5668\u548cBooster T1\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEEC\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u7a33\u5065\u5904\u7406\u591a\u6837\u4e14\u9ad8\u8981\u6c42\u7684\u79fb\u52a8-\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "SEEC\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u672b\u7aef\u6267\u884c\u5668\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.21242", "pdf": "https://arxiv.org/pdf/2509.21242", "abs": "https://arxiv.org/abs/2509.21242", "authors": ["Yutong Li", "Jieyi Zhang", "Wenqiang Xu", "Tutian Tang", "Cewu Lu"], "title": "FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration", "categories": ["cs.RO"], "comment": "Presented at IROS 2025, details are available at\n  https://fsglove.robotflow.ai", "summary": "Accurate hand motion capture (MoCap) is vital for applications in robotics,\nvirtual reality, and biomechanics, yet existing systems face limitations in\ncapturing high-degree-of-freedom (DoF) joint kinematics and personalized hand\nshape. Commercial gloves offer up to 21 DoFs, which are insufficient for\ncomplex manipulations while neglecting shape variations that are critical for\ncontact-rich tasks. We present FSGlove, an inertial-based system that\nsimultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes\nvia DiffHCal, a novel calibration method. Each finger joint and the dorsum are\nequipped with IMUs, enabling high-resolution motion sensing. DiffHCal\nintegrates with the parametric MANO model through differentiable optimization,\nresolving joint kinematics, shape parameters, and sensor misalignment during a\nsingle streamlined calibration. The system achieves state-of-the-art accuracy,\nwith joint angle errors of less than 2.7 degree, and outperforms commercial\nalternatives in shape reconstruction and contact fidelity. FSGlove's\nopen-source hardware and software design ensures compatibility with current VR\nand robotics ecosystems, while its ability to capture subtle motions (e.g.,\nfingertip rubbing) bridges the gap between human dexterity and robotic\nimitation. Evaluated against Nokov optical MoCap, FSGlove advances hand\ntracking by unifying the kinematic and contact fidelity. Hardware design,\nsoftware, and more results are available at:\nhttps://sites.google.com/view/fsglove.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86FSGlove\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u6b3e\u57fa\u4e8e\u60ef\u6027\u7684\u624b\u52bf\u6355\u6349\u7cfb\u7edf\uff0c\u80fd\u540c\u65f6\u8ffd\u8e2a48\u4e2a\u81ea\u7531\u5ea6\u5e76\u91cd\u5efa\u4e2a\u6027\u5316\u624b\u90e8\u5f62\u72b6\uff0c\u901a\u8fc7DiffHCal\u6821\u51c6\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u6355\u6349\u3002", "motivation": "\u73b0\u6709\u5546\u4e1a\u624b\u5957\u5728\u6355\u6349\u9ad8\u81ea\u7531\u5ea6\u548c\u4e2a\u6027\u5316\u624b\u90e8\u5f62\u72b6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u64cd\u4f5c\u548c\u63a5\u89e6\u4efb\u52a1\u4e2d\u3002", "method": "FSGlove\u7ed3\u5408\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMUs\uff09\u548cDiffHCal\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u4e0eMANO\u6a21\u578b\u96c6\u6210\uff0c\u5b9e\u73b0\u5173\u8282\u8fd0\u52a8\u5b66\u3001\u5f62\u72b6\u53c2\u6570\u548c\u4f20\u611f\u5668\u504f\u5dee\u7684\u6821\u51c6\u3002", "result": "\u7cfb\u7edf\u5728\u5173\u8282\u89d2\u5ea6\u8bef\u5dee\uff08\u5c0f\u4e8e2.7\u5ea6\uff09\u3001\u5f62\u72b6\u91cd\u5efa\u548c\u63a5\u89e6\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5546\u4e1a\u4ea7\u54c1\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u8bbe\u8ba1\u517c\u5bb9\u73b0\u6709VR\u548c\u673a\u5668\u4eba\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "FSGlove\u5728\u624b\u90e8\u8ffd\u8e2a\u4e2d\u7edf\u4e00\u4e86\u8fd0\u52a8\u5b66\u548c\u63a5\u89e6\u4fdd\u771f\u5ea6\uff0c\u586b\u8865\u4e86\u4eba\u7c7b\u7075\u5de7\u6027\u4e0e\u673a\u5668\u4eba\u6a21\u4eff\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.21243", "pdf": "https://arxiv.org/pdf/2509.21243", "abs": "https://arxiv.org/abs/2509.21243", "authors": ["Jiyeon Koo", "Taewan Cho", "Hyunjoon Kang", "Eunseom Pyo", "Tae Gyun Oh", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Recent Vision-Language-Action (VLA) models demonstrate remarkable\ngeneralization in robotics but are restricted by their substantial size and\ncomputational cost, limiting real-world deployment. However, conventional\nlightweighting methods often sacrifice critical capabilities, particularly\nspatial reasoning. This creates a trade-off between efficiency and performance.\nTo address this challenge, our work reuses Register Tokens, which were\nintroduced for artifact removal in Vision Transformers but subsequently\ndiscarded. We suppose that these tokens contain essential spatial information\nand propose RetoVLA, a novel architecture that reuses them directly by\ninjecting them into the Action Expert.\n  RetoVLA maintains a lightweight structure while leveraging this repurposed\nspatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness\nthrough a series of comprehensive experiments. On our custom-built 7-DOF robot\narm, the model achieves a 17.1%p absolute improvement in success rates for\ncomplex manipulation tasks. Our results confirm that reusing Register Tokens\ndirectly enhances spatial reasoning, demonstrating that what was previously\ndiscarded as an artifact is in fact a valuable, unexplored resource for robotic\nintelligence. A video demonstration is available at:\nhttps://youtu.be/2CseBR-snZg", "AI": {"tldr": "RetoVLA\u901a\u8fc7\u91cd\u7528Vision Transformers\u4e2d\u7684Register Tokens\uff0c\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7VLA\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8617.1%\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u56e0\u4f53\u79ef\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff0c\u4e14\u4f20\u7edf\u7684\u8f7b\u91cf\u5316\u65b9\u6cd5\u727a\u7272\u4e86\u5173\u952e\u80fd\u529b\uff08\u5982\u7a7a\u95f4\u63a8\u7406\uff09\u3002RetoVLA\u8bd5\u56fe\u901a\u8fc7\u91cd\u7528Register Tokens\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "RetoVLA\u91cd\u7528Vision Transformers\u4e2d\u672c\u88ab\u4e22\u5f03\u7684Register Tokens\uff0c\u5c06\u5176\u6ce8\u5165Action Expert\u6a21\u5757\uff0c\u4ee5\u4fdd\u7559\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u57287-DOF\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0cRetoVLA\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u63d0\u5347\u4e8617.1%\u3002", "conclusion": "Register Tokens\u662f\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u8d44\u6e90\uff0cRetoVLA\u7684\u6210\u529f\u8868\u660e\u5176\u5728\u63d0\u5347\u8f7b\u91cf\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.21256", "pdf": "https://arxiv.org/pdf/2509.21256", "abs": "https://arxiv.org/abs/2509.21256", "authors": ["Huayi Zhou", "Kui Jia"], "title": "BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives", "categories": ["cs.RO"], "comment": "under review", "summary": "Non-prehensile manipulation, encompassing ungraspable actions such as\npushing, poking, and pivoting, represents a critical yet underexplored domain\nin robotics due to its contact-rich and analytically intractable nature. In\nthis work, we revisit this problem from two novel perspectives. First, we move\nbeyond the usual single-arm setup and the strong assumption of favorable\nexternal dexterity such as walls, ramps, or edges. Instead, we advocate a\ngeneralizable dual-arm configuration and establish a suite of Bimanual\nNon-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the\nprevailing RL-based paradigm and propose a three-stage, RL-free framework to\nlearn non-prehensile skills. Specifically, we begin by extracting bimanual hand\nmotion trajectories from video demonstrations. Due to visual inaccuracies and\nmorphological gaps, these coarse trajectories are difficult to transfer\ndirectly to robotic end-effectors. To address this, we propose a geometry-aware\npost-optimization algorithm that refines raw motions into executable\nmanipulation primitives that conform to specific motion patterns. Beyond\ninstance-level reproduction, we further enable category-level generalization by\nparameterizing the learned primitives with object-relevant geometric\nattributes, particularly size, resulting in adaptable and general parameterized\nmanipulation primitives. We validate BiNoMaP across a range of representative\nbimanual tasks and diverse object categories, demonstrating its effectiveness,\nefficiency, versatility, and superior generalization capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u673a\u68b0\u81c2\u975e\u6293\u53d6\u64cd\u4f5c\u6846\u67b6BiNoMaP\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u540e\u4f18\u5316\u7b97\u6cd5\uff0c\u4ece\u89c6\u9891\u6f14\u793a\u4e2d\u63d0\u53d6\u5e76\u4f18\u5316\u53ef\u6267\u884c\u7684\u64cd\u4f5c\u6280\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u901a\u7528\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u64cd\u4f5c\u3002", "motivation": "\u975e\u6293\u53d6\u64cd\u4f5c\u5728\u673a\u5668\u4eba\u9886\u57df\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u7814\u7a76\u8f83\u5c11\u7684\u9886\u57df\uff0c\u56e0\u4e3a\u5176\u63a5\u89e6\u591a\u4e14\u96be\u4ee5\u5206\u6790\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5355\u81c2\u548c\u5916\u90e8\u8f85\u52a9\u6761\u4ef6\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u53cc\u673a\u68b0\u81c2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684RL-free\u6846\u67b6\uff1a1)\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u53cc\u673a\u68b0\u81c2\u8fd0\u52a8\u8f68\u8ff9\uff1b2)\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u540e\u4f18\u5316\u7b97\u6cd5\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u64cd\u4f5c\u6280\u80fd\uff1b3)\u53c2\u6570\u5316\u8fd9\u4e9b\u6280\u80fd\u4ee5\u652f\u6301\u8de8\u7c7b\u522b\u6cdb\u5316\u3002", "result": "\u5728\u591a\u79cd\u53cc\u673a\u68b0\u81c2\u4efb\u52a1\u548c\u7269\u4f53\u7c7b\u522b\u4e0a\u9a8c\u8bc1\u4e86BiNoMaP\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BiNoMaP\u4e3a\u975e\u6293\u53d6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6446\u8131\u4e86\u5bf9RL\u548c\u5916\u90e8\u6761\u4ef6\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u8de8\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.21264", "pdf": "https://arxiv.org/pdf/2509.21264", "abs": "https://arxiv.org/abs/2509.21264", "authors": ["Babak Salamat", "Dominik Mattern", "Sebastian-Sven Olzem", "Gerhard Elsbacher", "Christian Seidel", "Andrea M. Tonello"], "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)", "categories": ["cs.RO"], "comment": null, "summary": "We propose $\\text{GMP}^{3}$, a multiphase global path planning framework that\ngenerates dynamically feasible three-dimensional trajectories for unmanned\naerial vehicles (UAVs) operating in cluttered environments. The framework\nextends traditional path planning from Euclidean position spaces to the Lie\ngroup $\\mathrm{SE}(3)$, allowing joint learning of translational motion and\nrotational dynamics. A modified Bellman-based operator is introduced to support\nreinforcement learning (RL) policy updates while leveraging prior trajectory\ninformation for improved convergence. $\\text{GMP}^{3}$ is designed as a\ndistributed framework in which agents influence each other and share policy\ninformation along the trajectory: each agent refines its assigned segment and\nshares with its neighbors via a consensus-based scheme, enabling cooperative\npolicy updates and convergence toward a path shaped globally even under\nkinematic constraints. We also propose DroneManager, a modular ground control\nsoftware that interfaces the planner with real UAV platforms via the MAVLink\nprotocol, supporting real-time deployment and feedback. Simulation studies and\nindoor flight experiments validate the effectiveness of the proposed method in\nconstrained 3D environments, demonstrating reliable obstacle avoidance and\nsmooth, feasible trajectories across both position and orientation. The\nopen-source implementation is available at\nhttps://github.com/Domattee/DroneManager", "AI": {"tldr": "\u2018\u200bGMP\u00b3\u2019\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u4e09\u7ef4\u8f68\u8ff9\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u5230SE(3)\u674e\u7fa4\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u7684Bellman\u7b97\u5b50\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u66f4\u65b0\u3002\u5206\u5e03\u5f0f\u6846\u67b6\u5141\u8bb8\u4ee3\u7406\u534f\u540c\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u901a\u8fc7DroneManager\u8f6f\u4ef6\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5982\u4f55\u540c\u65f6\u4f18\u5316\u5e73\u79fb\u8fd0\u52a8\u548c\u65cb\u8f6c\u52a8\u529b\u5b66\uff0c\u5e76\u5b9e\u73b0\u5206\u5e03\u5f0f\u534f\u540c\u89c4\u5212\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eSE(3)\u674e\u7fa4\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\u2018\u200bGMP\u00b3\u2019\uff0c\u6539\u8fdbBellman\u7b97\u5b50\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u4ee3\u7406\u5171\u4eab\u7b56\u7565\u4fe1\u606f\uff1b\u5f00\u53d1DroneManager\u8f6f\u4ef6\u5b9e\u73b0\u4e0e\u771f\u5b9e\u65e0\u4eba\u673a\u7684\u5b9e\u65f6\u4ea4\u4e92\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u53d7\u96503D\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u907f\u969c\u548c\u5e73\u6ed1\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u2018\u200bGMP\u00b3\u2019\u80fd\u591f\u9ad8\u6548\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u52a8\u6001\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.21281", "pdf": "https://arxiv.org/pdf/2509.21281", "abs": "https://arxiv.org/abs/2509.21281", "authors": ["Luis Augenstein", "No\u00e9mie Jaquier", "Tamim Asfour", "Leonel Rozo"], "title": "Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 6 figures, 1 table", "summary": "Human-like motion generation for robots often draws inspiration from\nbiomechanical studies, which often categorize complex human motions into\nhierarchical taxonomies. While these taxonomies provide rich structural\ninformation about how movements relate to one another, this information is\nfrequently overlooked in motion generation models, leading to a disconnect\nbetween the generated motions and their underlying hierarchical structure. This\npaper introduces the \\ac{gphdm}, a novel approach that learns latent\nrepresentations preserving both the hierarchical structure of motions and their\ntemporal dynamics to ensure physical consistency. Our model achieves this by\nextending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to\nthe hyperbolic manifold and integrating it with taxonomy-aware inductive\nbiases. Building on this geometry- and taxonomy-aware frameworks, we propose\nthree novel mechanisms for generating motions that are both\ntaxonomically-structured and physically-consistent: two probabilistic recursive\napproaches and a method based on pullback-metric geodesics. Experiments on\ngenerating realistic motion sequences on the hand grasping taxonomy show that\nthe proposed GPHDM faithfully encodes the underlying taxonomy and temporal\ndynamics, and generates novel physically-consistent trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPHDM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u52a8\u6001\u5148\u9a8c\u548c\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u751f\u6210\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u4e00\u81f4\u6027\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u5e38\u5ffd\u89c6\u8fd0\u52a8\u7684\u5c42\u6b21\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8fd0\u52a8\u4e0e\u5e95\u5c42\u7ed3\u6784\u8131\u8282\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6269\u5c55GPDM\u7684\u52a8\u6001\u5148\u9a8c\u5230\u53cc\u66f2\u6d41\u5f62\uff0c\u7ed3\u5408\u5206\u7c7b\u5b66\u611f\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u63d0\u51fa\u4e09\u79cd\u65b0\u673a\u5236\uff1a\u4e24\u79cd\u6982\u7387\u9012\u5f52\u65b9\u6cd5\u548c\u4e00\u79cd\u57fa\u4e8e\u62c9\u56de\u5ea6\u91cf\u6d4b\u5730\u7ebf\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u624b\u6293\u53d6\u5206\u7c7b\u5b66\u7684\u5b9e\u9a8c\u4e2d\uff0cGPHDM\u6210\u529f\u7f16\u7801\u4e86\u5e95\u5c42\u5206\u7c7b\u5b66\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u751f\u6210\u4e86\u7269\u7406\u4e00\u81f4\u7684\u65b0\u8fd0\u52a8\u8f68\u8ff9\u3002", "conclusion": "GPHDM\u6709\u6548\u7ed3\u5408\u4e86\u51e0\u4f55\u548c\u5206\u7c7b\u5b66\u611f\u77e5\u6846\u67b6\uff0c\u751f\u6210\u7684\u8fd0\u52a8\u65e2\u7b26\u5408\u5c42\u6b21\u7ed3\u6784\u53c8\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.20401", "pdf": "https://arxiv.org/pdf/2509.20401", "abs": "https://arxiv.org/abs/2509.20401", "authors": ["Binod Singh", "Sayan Deb Sarkar", "Iro Armeni"], "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Aligning 3D scene graphs is a crucial initial step for several applications\nin robot navigation and embodied perception. Current methods in 3D scene graph\nalignment often rely on single-modality point cloud data and struggle with\nincomplete or noisy input. We introduce SGAligner++, a cross-modal,\nlanguage-aided framework for 3D scene graph alignment. Our method addresses the\nchallenge of aligning partially overlapping scene observations across\nheterogeneous modalities by learning a unified joint embedding space, enabling\naccurate alignment even under low-overlap conditions and sensor noise. By\nemploying lightweight unimodal encoders and attention-based fusion, SGAligner++\nenhances scene understanding for tasks such as visual localization, 3D\nreconstruction, and navigation, while ensuring scalability and minimal\ncomputational overhead. Extensive evaluations on real-world datasets\ndemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%\non noisy real-world reconstructions, while enabling cross-modal generalization.", "AI": {"tldr": "SGAligner++\u662f\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u548c\u8bed\u8a00\u8f85\u52a9\u76843D\u573a\u666f\u56fe\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u89e3\u51b3\u5f02\u6784\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u573a\u666f\u56fe\u5bf9\u9f50\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u70b9\u4e91\u6570\u636e\u548c\u4e0d\u5b8c\u6574/\u566a\u58f0\u8f93\u5165\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5355\u6a21\u6001\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\u6280\u672f\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd540%\uff0c\u4e14\u5728\u4f4e\u91cd\u53e0\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u4ecd\u80fd\u51c6\u786e\u5bf9\u9f50\u3002", "conclusion": "SGAligner++\u4e3a\u89c6\u89c9\u5b9a\u4f4d\u30013D\u91cd\u5efa\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20414", "pdf": "https://arxiv.org/pdf/2509.20414", "abs": "https://arxiv.org/abs/2509.20414", "authors": ["Yandan Yang", "Baoxiong Jia", "Shujie Zhang", "Siyuan Huang"], "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted by NeurIPS 2025, 26 pages", "summary": "Indoor scene synthesis has become increasingly important with the rise of\nEmbodied AI, which requires 3D environments that are not only visually\nrealistic but also physically plausible and functionally diverse. While recent\napproaches have advanced visual fidelity, they often remain constrained to\nfixed scene categories, lack sufficient object-level detail and physical\nconsistency, and struggle to align with complex user instructions. In this\nwork, we present SceneWeaver, a reflective agentic framework that unifies\ndiverse scene synthesis paradigms through tool-based iterative refinement. At\nits core, SceneWeaver employs a language model-based planner to select from a\nsuite of extensible scene generation tools, ranging from data-driven generative\nmodels to visual- and LLM-based methods, guided by self-evaluation of physical\nplausibility, visual realism, and semantic alignment with user input. This\nclosed-loop reason-act-reflect design enables the agent to identify semantic\ninconsistencies, invoke targeted tools, and update the environment over\nsuccessive iterations. Extensive experiments on both common and open-vocabulary\nroom types demonstrate that SceneWeaver not only outperforms prior methods on\nphysical, visual, and semantic metrics, but also generalizes effectively to\ncomplex scenes with diverse instructions, marking a step toward general-purpose\n3D environment generation. Project website: https://scene-weaver.github.io/.", "AI": {"tldr": "SceneWeaver\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u8fed\u4ee3\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u548c\u591a\u5de5\u5177\u534f\u4f5c\uff0c\u63d0\u5347\u5ba4\u5185\u573a\u666f\u5408\u6210\u7684\u7269\u7406\u5408\u7406\u6027\u3001\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u573a\u666f\u5408\u6210\u65b9\u6cd5\u5728\u89c6\u89c9\u903c\u771f\u5ea6\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u590d\u6742\u7528\u6237\u6307\u4ee4\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u89c4\u5212\u5668\u9009\u62e9\u751f\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u95ed\u73af\u7684\u201c\u63a8\u7406-\u884c\u52a8-\u53cd\u601d\u201d\u8bbe\u8ba1\u5b9e\u73b0\u591a\u5de5\u5177\u534f\u540c\u4f18\u5316\u573a\u666f\u3002", "result": "\u5728\u7269\u7406\u3001\u89c6\u89c9\u548c\u8bed\u4e49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u590d\u6742\u573a\u666f\u548c\u591a\u6837\u6307\u4ee4\u3002", "conclusion": "SceneWeaver\u4e3a\u901a\u75283D\u73af\u5883\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.20579", "pdf": "https://arxiv.org/pdf/2509.20579", "abs": "https://arxiv.org/abs/2509.20579", "authors": ["Hanna Yurchyk", "Wei-Di Chang", "Gregory Dudek", "David Meger"], "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots", "summary": "We investigate the integration of attention maps from a pre-trained Vision\nTransformer into voxel representations to enhance bimanual robotic\nmanipulation. Specifically, we extract attention maps from DINOv2, a\nself-supervised ViT model, and interpret them as pixel-level saliency scores\nover RGB images. These maps are lifted into a 3D voxel grid, resulting in\nvoxel-level semantic cues that are incorporated into a behavior cloning policy.\nWhen integrated into a state-of-the-art voxel-based policy, our\nattention-guided featurization yields an average absolute improvement of 8.2%\nand a relative gain of 21.9% across all tasks in the RLBench bimanual\nbenchmark.", "AI": {"tldr": "\u5c06\u9884\u8bad\u7ec3Vision Transformer\u7684\u6ce8\u610f\u529b\u56fe\u6574\u5408\u5230\u4f53\u7d20\u8868\u793a\u4e2d\uff0c\u4ee5\u63d0\u5347\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u81ea\u76d1\u7763ViT\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u50cf\u7d20\u7ea7\u663e\u8457\u6027\u5206\u6570\uff0c\u4e3a3D\u4f53\u7d20\u7f51\u683c\u63d0\u4f9b\u8bed\u4e49\u7ebf\u7d22\uff0c\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u3002", "method": "\u4eceDINOv2\u63d0\u53d6\u6ce8\u610f\u529b\u56fe\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a3D\u4f53\u7d20\u7f51\u683c\u7684\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u6574\u5408\u5230\u884c\u4e3a\u514b\u9686\u7b56\u7565\u4e2d\u3002", "result": "\u5728RLBench\u53cc\u624b\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e868.2%\u7684\u7edd\u5bf9\u63d0\u5347\u548c21.9%\u7684\u76f8\u5bf9\u589e\u76ca\u3002", "conclusion": "\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u5316\u80fd\u591f\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u4f53\u7d20\u7684\u7b56\u7565\u6027\u80fd\u3002"}}
{"id": "2509.20648", "pdf": "https://arxiv.org/pdf/2509.20648", "abs": "https://arxiv.org/abs/2509.20648", "authors": ["Yiyuan Pan", "Zhe Liu", "Hesheng Wang"], "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Autonomous exploration in complex multi-agent reinforcement learning (MARL)\nwith sparse rewards critically depends on providing agents with effective\nintrinsic motivation. While artificial curiosity offers a powerful\nself-supervised signal, it often confuses environmental stochasticity with\nmeaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform\nnovelty bias, treating all unexpected observations equally. However, peer\nbehavior novelty, which encode latent task dynamics, are often overlooked,\nresulting in suboptimal exploration in decentralized, communication-free MARL\nsettings. To this end, inspired by how human children adaptively calibrate\ntheir own exploratory behaviors via observing peers, we propose a novel\napproach to enhance multi-agent exploration. We introduce CERMIC, a principled\nframework that empowers agents to robustly filter noisy surprise signals and\nguide exploration by dynamically calibrating their intrinsic curiosity with\ninferred multi-agent context. Additionally, CERMIC generates\ntheoretically-grounded intrinsic rewards, encouraging agents to explore state\ntransitions with high information gain. We evaluate CERMIC on benchmark suites\nincluding VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that\nexploration with CERMIC significantly outperforms SoTA algorithms in\nsparse-reward environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCERMIC\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u5185\u5728\u597d\u5947\u5fc3\u4e0e\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\uff0c\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u5185\u5728\u597d\u5947\u5fc3\u673a\u5236\u5728\u7a00\u758f\u5956\u52b1\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u6df7\u6dc6\u73af\u5883\u968f\u673a\u6027\u4e0e\u6709\u610f\u4e49\u7684\u65b0\u9896\u6027\uff0c\u4e14\u5ffd\u7565\u4e86\u540c\u4f34\u884c\u4e3a\u7684\u65b0\u9896\u6027\u3002", "method": "CERMIC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u5185\u5728\u597d\u5947\u5fc3\u4e0e\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\uff0c\u8fc7\u6ee4\u566a\u58f0\u4fe1\u53f7\u5e76\u751f\u6210\u7406\u8bba\u9a71\u52a8\u7684\u5185\u5728\u5956\u52b1\u3002", "result": "\u5728VMAS\u3001Meltingpot\u548cSMACv2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCERMIC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "CERMIC\u901a\u8fc7\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u548c\u7406\u8bba\u9a71\u52a8\u5956\u52b1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6027\u80fd\u3002"}}
{"id": "2509.20754", "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ .", "AI": {"tldr": "Meta-Memory\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u6a21\u6001\u8054\u5408\u63a8\u7406\u9ad8\u6548\u68c0\u7d22\u548c\u6574\u5408\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u56de\u7b54\u7a7a\u95f4\u67e5\u8be2\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u5b58\u50a8\u548c\u68c0\u7d22\u89c2\u5bdf\u8bb0\u5fc6\uff0c\u5e76\u7528\u4e8e\u56de\u7b54\u4eba\u7c7b\u7a7a\u95f4\u67e5\u8be2\u7684\u7814\u7a76\u6311\u6218\u3002", "method": "\u63d0\u51faMeta-Memory\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u9ad8\u5bc6\u5ea6\u73af\u5883\u8bb0\u5fc6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u6a21\u6001\u8054\u5408\u63a8\u7406\u5b9e\u73b0\u8bb0\u5fc6\u68c0\u7d22\u548c\u6574\u5408\u3002", "result": "\u5728SpaceLocQA\u548cNaVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "Meta-Memory\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u5347\u673a\u5668\u4eba\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u529b\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.20906", "pdf": "https://arxiv.org/pdf/2509.20906", "abs": "https://arxiv.org/abs/2509.20906", "authors": ["Julius Pesonen", "Arno Solin", "Eija Honkavaara"], "title": "Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences", "categories": ["cs.CV", "cs.RO", "I.4.8; I.4.9"], "comment": null, "summary": "3D object localisation based on a sequence of camera measurements is\nessential for safety-critical surveillance tasks, such as drone-based wildfire\nmonitoring. Localisation of objects detected with a camera can typically be\nsolved with dense depth estimation or 3D scene reconstruction. However, in the\ncontext of distant objects or tasks limited by the amount of available\ncomputational resources, neither solution is feasible. In this paper, we show\nthat the task can be solved using particle filters for both single and multiple\ntarget scenarios. The method was studied using a 3D simulation and a\ndrone-based image segmentation sequence with global navigation satellite system\n(GNSS)-based camera pose estimates. The results showed that a particle filter\ncan be used to solve practical localisation tasks based on camera poses and\nimage segments in these situations where other solutions fail. The particle\nfilter is independent of the detection method, making it flexible for new\ntasks. The study also demonstrates that drone-based wildfire monitoring can be\nconducted using the proposed method paired with a pre-existing image\nsegmentation model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u76843D\u7269\u4f53\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fdc\u8ddd\u79bb\u6216\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\uff0c\u5e76\u5728\u65e0\u4eba\u673a\u91ce\u706b\u76d1\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u8fdc\u8ddd\u79bb\u7269\u4f53\u6216\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u4efb\u52a1\uff0c\u4f20\u7edf\u7684\u5bc6\u96c6\u6df1\u5ea6\u4f30\u8ba1\u62163D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7c92\u5b50\u6ee4\u6ce2\u6280\u672f\uff0c\u7ed3\u5408\u76f8\u673a\u4f4d\u59ff\u548c\u56fe\u50cf\u5206\u5272\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u3002", "result": "\u901a\u8fc73D\u6a21\u62df\u548c\u65e0\u4eba\u673a\u56fe\u50cf\u5206\u5272\u5e8f\u5217\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7c92\u5b50\u6ee4\u6ce2\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u65e0\u4eba\u673a\u91ce\u706b\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\u5177\u6709\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u7279\u5b9a\u68c0\u6d4b\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u5b9e\u9645\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u65e0\u4eba\u673a\u76d1\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
