{"id": "2509.19304", "pdf": "https://arxiv.org/pdf/2509.19304", "abs": "https://arxiv.org/abs/2509.19304", "authors": ["M. Andrecut"], "title": "Raspberry Pi Pico as a Radio Transmitter", "categories": ["eess.SP", "cs.CR"], "comment": "13 pages, 3 figures", "summary": "In this paper we discuss several surprisingly simple methods for transforming\nthe Raspberry Pi Pico (RP2) microcontroller into a radio transmitter, by using\nonly cheap off the shelf electronic components, and open source software. While\ninitially this transformation may look as a harmless curiosity, in some extreme\ncases it can also pose security risks, since it can be used to open a large\nnumber of local stealth radio communication channels."}
{"id": "2509.19306", "pdf": "https://arxiv.org/pdf/2509.19306", "abs": "https://arxiv.org/abs/2509.19306", "authors": ["Jingyi Wang", "Zhongyuan Zhao", "Qingtian Wang", "Zexu Li", "Yue Wang", "Tony Q. S. Quek"], "title": "A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "Edge intelligence has emerged as a promising strategy to deliver low-latency\nand ubiquitous services for mobile devices. Recent advances in fine-tuning\nmechanisms of foundation models have enabled edge intelligence by integrating\nlow-rank adaptation (LoRA) with federated learning. However, in wireless\nnetworks, the device heterogeneity and resource constraints on edge devices\npose great threats to the performance of federated fine-tuning. To tackle these\nissues, we propose to optimize federated fine-tuning in heterogenous wireless\nnetworks via online learning. First, the framework of switching-based federated\nfine-tuning in wireless networks is provided. The edge devices switches to LoRA\nmodules dynamically for federated fine-tuning with base station to jointly\nmitigate the impact of device heterogeneity and transmission unreliability.\nSecond, a tractable upper bound on the inference risk gap is derived based on\ntheoretical analysis. To improve the generalization capability, we formulate a\nnon-convex mixed-integer programming problem with long-term constraints, and\ndecouple it into model switching, transmit power control, and bandwidth\nallocation subproblems. An online optimization algorithm is developed to solve\nthe problems with polynomial computational complexity. Finally, the simulation\nresults on the SST-2 and QNLI data sets demonstrate the performance gains in\ntest accuracy and energy efficiency."}
{"id": "2509.19307", "pdf": "https://arxiv.org/pdf/2509.19307", "abs": "https://arxiv.org/abs/2509.19307", "authors": ["Anthony LoPrete", "Johannes Burge"], "title": "Bandwidth of Gamma-Distribution-Shaped Functions via Lambert W Function", "categories": ["eess.SP", "math.PR", "60E05 (primary), 33E20 (secondary)"], "comment": null, "summary": "The full width at half maximum (FWHM) is a useful quantity for characterizing\nthe bandwidth of unimodal functions. However, a closed-form expression for the\nFWHM of gamma-shaped functions-i.e. functions that are shaped like the gamma\ndistribution probability density function (PDF)-is not widely available. Here,\nwe derive and present just such an expression. To do so, we use the Lambert W\nfunction to compute the inverse of the gamma PDF. We use this inverse to derive\nan exact analytic expression for the width of a gamma distribution at an\narbitrary proportion of the maximum, from which the FWHM follows trivially. (An\nexpression for the octave bandwidth of gamma-shaped functions is also\nprovided.) The FWHM is then compared to the Gaussian approximation of\ngamma-shaped functions. A few other related issues are discussed."}
{"id": "2509.19308", "pdf": "https://arxiv.org/pdf/2509.19308", "abs": "https://arxiv.org/abs/2509.19308", "authors": ["Chang Wang", "Ming Zhu", "Shahram Latifi", "Buddhadeb Dawn", "Shengjie Zhai"], "title": "Graph-Based Spatio-temporal Attention and Multi-Scale Fusion for Clinically Interpretable, High-Fidelity Fetal ECG Extraction", "categories": ["eess.SP", "cs.LG"], "comment": "6 pages, ACM BCB 2025", "summary": "Congenital Heart Disease (CHD) is the most common neonatal anomaly,\nhighlighting the urgent need for early detection to improve outcomes. Yet,\nfetal ECG (fECG) signals in abdominal ECG (aECG) are often masked by maternal\nECG and noise, challenging conventional methods under low signal-to-noise ratio\n(SNR) conditions. We propose FetalHealthNet (FHNet), a deep learning framework\nthat integrates Graph Neural Networks with a multi-scale enhanced transformer\nto dynamically model spatiotemporal inter-lead correlations and extract clean\nfECG signals. On benchmark aECG datasets, FHNet consistently outperforms long\nshort-term memory (LSTM) models, standard transformers, and state-of-the-art\nmodels, achieving R2>0.99 and RMSE = 0.015 even under severe noise.\nInterpretability analyses highlight physiologically meaningful temporal and\nlead contributions, supporting model transparency and clinical trust. FHNet\nillustrates the potential of AI-driven modeling to advance fetal monitoring and\nenable early CHD screening, underscoring the transformative impact of\nnext-generation biomedical signal processing."}
{"id": "2509.19452", "pdf": "https://arxiv.org/pdf/2509.19452", "abs": "https://arxiv.org/abs/2509.19452", "authors": ["Alessandro Saviolo", "Jeffrey Mao", "Giuseppe Loianno"], "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Search and rescue operations require unmanned aerial vehicles to both\ntraverse unknown unstructured environments at high speed and track targets once\ndetected. Achieving both capabilities under degraded sensing and without global\nlocalization remains an open challenge. Recent works on relative navigation\nhave shown robust tracking by anchoring planning and control to a visible\ndetected object, but cannot address navigation when no target is in the field\nof view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time\nframework that unifies traversal, acquisition, and tracking within a single\nrelative formulation. HUNT defines navigation objectives directly from onboard\ninstantaneous observables such as attitude, altitude, and velocity, enabling\nreactive high-speed flight during search. Once a target is detected, the same\nperception-control pipeline transitions seamlessly to tracking. Outdoor\nexperiments in dense forests, container compounds, and search-and-rescue\noperations with vehicles and mannequins demonstrate robust autonomy where\nglobal methods fail."}
{"id": "2509.19310", "pdf": "https://arxiv.org/pdf/2509.19310", "abs": "https://arxiv.org/abs/2509.19310", "authors": ["Mukul Chauhan", "Waseem Z. Lone", "Amit K. Verma"], "title": "A Novel Two-Dimensional Wigner Distribution Framework via the Quadratic Phase Fourier Transform with a Non-Separable Kernel", "categories": ["eess.SP", "cs.IT", "math.FA", "math.IT", "42B10, 42A38, 44A35, 65R10, 81S30"], "comment": null, "summary": "This paper introduces a novel time-frequency distribution, referred to as the\nTwo-Dimensional Non-Separable Quadratic Phase Wigner Distribution (2D-NSQPWD),\nformulated within the framework of the Two-Dimensional Non-Separable Quadratic\nPhase Fourier Transform (2D-NSQPFT). By replacing the classical Fourier kernel\nwith the NSQPFT kernel, the proposed distribution generalizes the classical\nWigner distribution and effectively captures complex, non-separable signal\nstructures. We rigorously establish several key properties of the 2D-NSQPWD,\nincluding time and frequency shift invariance, marginal behavior, conjugate\nsymmetry, convolution relations, and Moyal's identity. Furthermore, the\nconnection between the 2D-NSQPWD and the two-dimensional short-time Fourier\ntransform (2D-STFT) is explored. The distribution's effectiveness is\ndemonstrated through its application to single-, bi-, and tri-component\ntwo-dimensional linear frequency modulated (2D-LFM) signals, where it shows\nsuperior performance in cross-term suppression and signal localization."}
{"id": "2509.19454", "pdf": "https://arxiv.org/pdf/2509.19454", "abs": "https://arxiv.org/abs/2509.19454", "authors": ["Jason Chen", "I-Chun Arthur Liu", "Gaurav Sukhatme", "Daniel Seita"], "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Training robust bimanual manipulation policies via imitation learning\nrequires demonstration data with broad coverage over robot poses, contacts, and\nscene contexts. However, collecting diverse and precise real-world\ndemonstrations is costly and time-consuming, which hinders scalability. Prior\nworks have addressed this with data augmentation, typically for either\neye-in-hand (wrist camera) setups with RGB inputs or for generating novel\nimages without paired actions, leaving augmentation for eye-to-hand\n(third-person) RGB-D training with new action labels less explored. In this\npaper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data\nAugmentation (ROPA), an offline imitation learning data augmentation method\nthat fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D\nobservations of novel robot poses. Our approach simultaneously generates\ncorresponding joint-space action labels while employing constrained\noptimization to enforce physical consistency through appropriate\ngripper-to-object contact constraints in bimanual scenarios. We evaluate our\nmethod on 5 simulated and 3 real-world tasks. Our results across 2625\nsimulation trials and 300 real-world trials demonstrate that ROPA outperforms\nbaselines and ablations, showing its potential for scalable RGB and RGB-D data\naugmentation in eye-to-hand bimanual manipulation. Our project website is\navailable at: https://ropaaug.github.io/."}
{"id": "2509.19312", "pdf": "https://arxiv.org/pdf/2509.19312", "abs": "https://arxiv.org/abs/2509.19312", "authors": ["Minghui Wu", "Zhen Gao"], "title": "E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Massive multiple-input multiple-output (MIMO) promises high spectral\nefficiency but also leads to high-dimensional downlink channel state\ninformation (CSI), which complicates real-time channel acquisition and\nprecoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI\nfusion precoding network that jointly models downlink CSI reference signal\n(CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single\nE2E neural architecture. Concretely, a projection network built on the MAXIM\narchitecture takes uplink sounding reference signals (SRS) as input and outputs\nfrequency-, beam-, and port-domain projection matrices for designing downlink\nCSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS\nobservations and feeds back a compact representation. At the base station (BS),\ntwo complementary branches produce candidate precoders: one is a feedback-only\nprecoding network driven by quantized downlink observations, and the other is\nan SRS-only precoding network driven by uplink SRS. These candidate precoders\nare subsequently combined by a fusion precoding network to yield the final\ntransmit precoder. All the modules are trained with a\nspectral-efficiency-oriented loss under a three-stage schedule. Simulation\nresults show that the proposed approach effectively harnesses both SRS-derived\ninformation and UE feedback, achieving markedly better performance than\nconventional baselines."}
{"id": "2509.19460", "pdf": "https://arxiv.org/pdf/2509.19460", "abs": "https://arxiv.org/abs/2509.19460", "authors": ["Yifan Ye", "Jun Cen", "Jing Chen", "Zhihe Lu"], "title": "Self-evolved Imitation Learning in Simulated World", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning has been a trend recently, yet training a generalist agent\nacross multiple tasks still requires large-scale expert demonstrations, which\nare costly and labor-intensive to collect. To address the challenge of limited\nsupervision, we propose Self-Evolved Imitation Learning (SEIL), a framework\nthat progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasksin the simulator, from which successful\ntrajectories are collected as new demonstrations for iterative refinement. To\nenhance the diversity of these demonstrations, SEIL employs dual-level\naugmentation: (i) Model-level, using an Exponential Moving Average (EMA) model\nto collaborate with the primary model, and (ii) Environment-level, introducing\nslight variations in initial object positions. We further introduce a\nlightweight selector that filters complementary and informative trajectories\nfrom the generated pool to ensure demonstration quality. These curated samples\nenable the model to achieve competitive performance with far fewer training\nexamples. Extensive experiments on the LIBERO benchmark show that SEIL achieves\na new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git."}
{"id": "2509.19313", "pdf": "https://arxiv.org/pdf/2509.19313", "abs": "https://arxiv.org/abs/2509.19313", "authors": ["Huipeng Liu", "Zhichao Zhu", "Yuan Zhou", "Changlu Li"], "title": "STL-FFT-STFT-TCN-LSTM: An Effective Wave Height High Accuracy Prediction Model Fusing Time-Frequency Domain Features", "categories": ["eess.SP", "cs.LG"], "comment": "17 page, 13 figures; references added", "summary": "As the consumption of traditional energy sources intensifies and their\nadverse environmental impacts become more pronounced, wave energy stands out as\na highly promising member of the renewable energy family due to its high energy\ndensity, stability, widespread distribution, and environmental friendliness.\nThe key to its development lies in the precise prediction of Significant Wave\nHeight (WVHT). However, wave energy signals exhibit strong nonlinearity, abrupt\nchanges, multi-scale periodicity, data sparsity, and high-frequency noise\ninterference; additionally, physical models for wave energy prediction incur\nextremely high computational costs. To address these challenges, this study\nproposes a hybrid model combining STL-FFT-STFT-TCN-LSTM. This model exploits\nthe Seasonal-Trend Decomposition Procedure based on Loess (STL), Fast Fourier\nTransform (FFT), Short-Time Fourier Transform (STFT), Temporal Convolutional\nNetwork (TCN), and Long Short-Term Memory (LSTM) technologies. The model aims\nto optimize multi-scale feature fusion, capture extreme wave heights, and\naddress issues related to high-frequency noise and periodic signals, thereby\nachieving efficient and accurate prediction of significant wave height.\nExperiments were conducted using hourly data from NOAA Station 41008 and 41047\nspanning 2019 to 2022. The results showed that compared with other single\nmodels and hybrid models, the STL-FFT-STFT-TCN-LSTM model achieved\nsignificantly higher prediction accuracy in capturing extreme wave heights and\nsuppressing high-frequency noise, with MAE reduced by 15.8\\%-40.5\\%, SMAPE\nreduced by 8.3\\%-20.3\\%, and R increased by 1.31\\%-2.9\\%; in ablation\nexperiments, the model also demonstrated the indispensability of each component\nstep, validating its superiority in multi-scale feature fusion."}
{"id": "2509.19463", "pdf": "https://arxiv.org/pdf/2509.19463", "abs": "https://arxiv.org/abs/2509.19463", "authors": ["Doncey Albin", "Daniel McGann", "Miles Mena", "Annika Thomas", "Harel Biggie", "Xuefei Sun", "Steve McGuire", "Jonathan P. How", "Christoffer Heckman"], "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception", "categories": ["cs.RO"], "comment": "8 pages, 11 figures", "summary": "A central challenge for multi-robot systems is fusing independently gathered\nperception data into a unified representation. Despite progress in\nCollaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of\ndedicated multi-robot datasets. Many evaluations instead partition single-robot\ntrajectories, a practice that may only partially reflect true multi-robot\noperations and, more critically, lacks standardization, leading to results that\nare difficult to interpret or compare across studies. While several multi-robot\ndatasets have recently been introduced, they mostly contain short trajectories\nwith limited inter-robot overlap and sparse intra-robot loop closures. To\novercome these limitations, we introduce CU-Multi, a dataset collected over\nmultiple days at two large outdoor sites on the University of Colorado Boulder\ncampus. CU-Multi comprises four synchronized runs with aligned start times and\ncontrolled trajectory overlap, replicating the distinct perspectives of a robot\nteam. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined\nground-truth odometry. By combining overlap variation with dense semantic\nannotations, CU-Multi provides a strong foundation for reproducible evaluation\nin multi-robot collaborative perception tasks."}
{"id": "2509.19315", "pdf": "https://arxiv.org/pdf/2509.19315", "abs": "https://arxiv.org/abs/2509.19315", "authors": ["Yiqiao Chen", "Zijian Huang", "Zhenghui Feng"], "title": "Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "12pages, 10 figures", "summary": "Pediatric arrhythmias are a major risk factor for disability and sudden\ncardiac death, yet their automated classification remains challenging due to\nclass imbalance, few-shot categories, and complex signal characteristics, which\nseverely limit the efficiency and reliability of early screening and clinical\nintervention. To address this problem, we propose a multimodal end-to-end deep\nlearning framework that combines dual-branch convolutional encoders for ECG and\nIEGM, semantic attention for cross-modal feature alignment, and a lightweight\nTransformer encoder for global dependency modeling. In addition, we introduce a\nnew contrastive loss fucntion named Adaptive Global Class-Aware Contrastive\nLoss (AGCACL) to enhance intra-class compactness and inter-class separability\nthrough class prototypes and a global similarity matrix. To the best of our\nknowledge, this is the first systematic study based on the Leipzig Heart Center\npediatric/congenital ECG+IEGM dataset, for which we also provide a complete and\nreproducible preprocessing pipeline. Experimental results demonstrate that the\nproposed method achieves the overall best performance on this dataset,\nincluding 97.76\\% Top-1 Accuracy, 94.08\\% Macro Precision, 91.97\\% Macro\nRecall, 92.97\\% Macro F1, and 92.36\\% Macro F2, with improvements of +13.64,\n+15.96, +19.82, and +19.44 percentage points over the strongest baseline in\nMacro Precision/Recall/F1/F2, respectively. These findings indicate that the\nframework significantly improves the detectability and robustness for minority\narrhythmia classes, offering potential clinical value for rhythm screening,\npre-procedural assessment, and postoperative follow-up in pediatric and\ncongenital heart disease populations."}
{"id": "2509.19473", "pdf": "https://arxiv.org/pdf/2509.19473", "abs": "https://arxiv.org/abs/2509.19473", "authors": ["Adarsh Salagame", "Henry Noyes", "Alireza Ramezani", "Eric Sihite", "Arash Kalantari"], "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping\nstone for future missions to Mars and beyond. The discovery of water ice on the\nMoon's craters located in permanently shadowed regions, which can provide\ndrinking water, oxygen, and rocket fuel, is therefore of critical importance.\nHowever, current methods to access lunar ice deposits are limited. While rovers\nhave been used to explore the lunar surface for decades, they face significant\nchallenges in navigating harsh terrains, such as permanently shadowed craters,\ndue to the high risk of immobilization. This report introduces COBRA (Crater\nObserving Bio-inspired Rolling Articulator), a multi-modal snake-style robot\ndesigned to overcome mobility challenges in Shackleton Crater's rugged\nenvironment. COBRA combines slithering and tumbling locomotion to adapt to\nvarious crater terrains. In snake mode, it uses sidewinding to traverse flat or\nlow inclined surfaces, while in tumbling mode, it forms a circular barrel by\nlinking its head and tail, enabling rapid movement with minimal energy on steep\nslopes. Equipped with an onboard computer, stereo camera, inertial measurement\nunit, and joint encoders, COBRA facilitates real-time data collection and\nautonomous operation. This paper highlights COBRAs robustness and efficiency in\nnavigating extreme terrains through both simulations and experimental\nvalidation."}
{"id": "2509.19316", "pdf": "https://arxiv.org/pdf/2509.19316", "abs": "https://arxiv.org/abs/2509.19316", "authors": ["Ammar Kamoona", "Hui Song", "Ali Moradi Amani", "Mahdi Jalili", "Xinghuo Yu", "Peter McTaggart"], "title": "Electric Vehicle Identification from Behind Smart Meter Data", "categories": ["eess.SP", "cs.LG"], "comment": "27 pages,", "summary": "Electric vehicle (EV) charging loads identification from behind smart meter\nrecordings is an indispensable aspect that enables effective decision-making\nfor energy distributors to reach an informed and intelligent decision about the\npower grid's reliability. When EV charging happens behind the meter (BTM), the\ncharging occurs on the customer side of the meter, which measures the overall\nelectricity consumption. In other words, the charging of the EV is considered\npart of the customer's load and not separately measured by the Distribution\nNetwork Operators (DNOs). DNOs require complete knowledge about the EV presence\nin their network. Identifying the EV charging demand is essential to better\nplan and manage the distribution grid. Unlike supervised methods, this paper\naddresses the problem of EV charging load identification in a non-nonintrusive\nmanner from low-frequency smart meter using an unsupervised learning approach\nbased on anomaly detection technique. Our approach does not require prior\nknowledge of EV charging profiles. It only requires real power consumption data\nof non-EV users, which are abundant in practice. We propose a deep temporal\nconvolution encoding decoding (TAE) network. The TAE is applied to power\nconsumption from smart BTM from Victorian households in Australia, and the TAE\nshows superior performance in identifying households with EVs."}
{"id": "2509.19480", "pdf": "https://arxiv.org/pdf/2509.19480", "abs": "https://arxiv.org/abs/2509.19480", "authors": ["Noriaki Hirose", "Catherine Glossop", "Dhruv Shah", "Sergey Levine"], "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 7 figures, 6 tables", "summary": "Humans can flexibly interpret and compose different goal specifications, such\nas language instructions, spatial coordinates, or visual references, when\nnavigating to a destination. In contrast, most existing robotic navigation\npolicies are trained on a single modality, limiting their adaptability to\nreal-world scenarios where different forms of goal specification are natural\nand complementary. In this work, we present a training framework for robotic\nfoundation models that enables omni-modal goal conditioning for vision-based\nnavigation. Our approach leverages a high-capacity vision-language-action (VLA)\nbackbone and trains with three primary goal modalities: 2D poses, egocentric\nimages, and natural language, as well as their combinations, through a\nrandomized modality fusion strategy. This design not only expands the pool of\nusable datasets but also encourages the policy to develop richer geometric,\nsemantic, and visual representations. The resulting model, OmniVLA, achieves\nstrong generalization to unseen environments, robustness to scarce modalities,\nand the ability to follow novel natural language instructions. We demonstrate\nthat OmniVLA outperforms specialist baselines across modalities and offers a\nflexible foundation for fine-tuning to new modalities and tasks. We believe\nOmniVLA provides a step toward broadly generalizable and flexible navigation\npolicies, and a scalable path for building omni-modal robotic foundation\nmodels. We present videos showcasing OmniVLA performance and will release its\ncheckpoints and training code on our project page."}
{"id": "2509.19318", "pdf": "https://arxiv.org/pdf/2509.19318", "abs": "https://arxiv.org/abs/2509.19318", "authors": ["Yanbaihui Liu", "Erica Babusci", "Claudia K. Gunsch", "Boyuan Chen"], "title": "Scensory: Automated Real-Time Fungal Identification and Spatial Mapping", "categories": ["eess.SP", "cs.RO"], "comment": "Our project website is at: http://generalroboticslab.com/Scensory", "summary": "Indoor fungal contamination poses significant risks to public health, yet\nexisting detection methods are slow, costly, and lack spatial resolution.\nConventional approaches rely on laboratory analysis or high-concentration\nsampling, making them unsuitable for real-time monitoring and scalable\ndeployment. We introduce \\textbf{\\textit{Scensory}}, a robot-enabled olfactory\nsystem that simultaneously identifies fungal species and localizes their\nspatial origin using affordable volatile organic compound (VOC) sensor arrays\nand deep learning. Our key idea is that temporal VOC dynamics encode both\nchemical and spatial signatures, which we decode through neural architectures\ntrained on robot-automated data collection. We demonstrate two operational\nmodes: a passive multi-array configuration for environmental monitoring, and a\nmobile single-array configuration for active source tracking. Across five\nfungal species, our system achieves up to 89.85\\% accuracy in species detection\nand 87.31\\% accuracy in localization under ambient conditions, where each\nprediction only takes 3--7\\,s sensor inputs. Additionally, by computationally\nanalyzing model behavior, we can uncover key biochemical signatures without\nadditional laboratory experiments. Our approach enables real-time, spatially\naware fungal monitoring and establishes a scalable and affordable framework for\nautonomous environmental sensing."}
{"id": "2509.19486", "pdf": "https://arxiv.org/pdf/2509.19486", "abs": "https://arxiv.org/abs/2509.19486", "authors": ["Kieran S. Lachmansingh", "José R. González-Estrada", "Ryan E. Grant", "Matthew K. X. J. Pan"], "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots", "categories": ["cs.RO", "cs.DC"], "comment": "8 pages, 3 figures", "summary": "This paper presents SHARP (Supercomputing for High-speed Avoidance and\nReactive Planning), a proof-of-concept study demonstrating how high-performance\ncomputing (HPC) can enable millisecond-scale responsiveness in robotic control.\nWhile modern robots face increasing demands for reactivity in human--robot\nshared workspaces, onboard processors are constrained by size, power, and cost.\nOffloading to HPC offers massive parallelism for trajectory planning, but its\nfeasibility for real-time robotics remains uncertain due to network latency and\njitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator\nmust dodge high-speed foam projectiles. Using a parallelized multi-goal A*\nsearch implemented with MPI on both local and remote HPC clusters, the system\nachieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300\nkm away), with avoidance success rates of 84% and 88%, respectively. These\nresults show that when round-trip latency remains within the\ntens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,\nenabling avoidance well below human reaction times. The SHARP results motivate\nhybrid control architectures: low-level reflexes remain onboard for safety,\nwhile bursty, high-throughput planning tasks are offloaded to HPC for\nscalability. By reporting per-stage timing and success rates, this study\nprovides a reproducible template for assessing real-time feasibility of\nHPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable\npathway toward dependable, reactive robots in dynamic environments."}
{"id": "2509.19328", "pdf": "https://arxiv.org/pdf/2509.19328", "abs": "https://arxiv.org/abs/2509.19328", "authors": ["Sina Montazeri", "Waltenegus Dargie", "Yunhe Feng", "Kewei Sha"], "title": "Human Activity Recognition Based on Electrocardiogram Data Only", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "This is a preprint version. Content may change before final\n  publication", "summary": "Human activity recognition is critical for applications such as early\nintervention and health analytics. Traditional activity recognition relies on\ninertial measurement units (IMUs), which are resource intensive and require\ncalibration. Although electrocardiogram (ECG)-based methods have been explored,\nthese have typically served as supplements to IMUs or have been limited to\nbroad categorical classification such as fall detection or active vs. inactive\nin daily activities. In this paper, we advance the field by demonstrating, for\nthe first time, robust recognition of activity only with ECG in six distinct\nactivities, which is beyond the scope of previous work. We design and evaluate\nthree new deep learning models, including a CNN classifier with\nSqueeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet\nclassifier with dilated convolutions for multiscale temporal dependency\ncapture, and a novel CNNTransformer hybrid combining convolutional feature\nextraction with attention mechanisms for long-range temporal relationship\nmodeling. Tested on data from 54 subjects for six activities, all three models\nachieve over 94% accuracy for seen subjects, while CNNTransformer hybrid\nreaching the best accuracy of 72% for unseen subjects, a result that can be\nfurther improved by increasing the training population. This study demonstrates\nthe first successful ECG-only activity classification in multiple physical\nactivities, offering significant potential for developing next-generation\nwearables capable of simultaneous cardiac monitoring and activity recognition\nwithout additional motion sensors."}
{"id": "2509.19521", "pdf": "https://arxiv.org/pdf/2509.19521", "abs": "https://arxiv.org/abs/2509.19521", "authors": ["Najeeb Ahmed Bhuiyan", "M. Nasimul Huq", "Sakib H. Chowdhury", "Rahul Mangharam"], "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion", "categories": ["cs.RO"], "comment": "12 pages, 11 figures", "summary": "Gesture-based control for mobile manipulators faces persistent challenges in\nreliability, efficiency, and intuitiveness. This paper presents a dual-hand\ngesture interface that integrates TinyML, spectral analysis, and sensor fusion\nwithin a ROS framework to address these limitations. The system uses left-hand\ntilt and finger flexion, captured using accelerometer and flex sensors, for\nmobile base navigation, while right-hand IMU signals are processed through\nspectral analysis and classified by a lightweight neural network. This pipeline\nenables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3\nmanipulator. By supporting simultaneous navigation and manipulation, the\nframework improves efficiency and coordination compared to sequential methods.\nKey contributions include a bimanual control architecture, real-time low-power\ngesture recognition, robust multimodal sensor fusion, and a scalable ROS-based\nimplementation. The proposed approach advances Human-Robot Interaction (HRI)\nfor industrial automation, assistive robotics, and hazardous environments,\noffering a cost-effective, open-source solution with strong potential for\nreal-world deployment and further optimization."}
{"id": "2509.19330", "pdf": "https://arxiv.org/pdf/2509.19330", "abs": "https://arxiv.org/abs/2509.19330", "authors": ["Zejun Liu", "Yunshan Chen", "Chengxi Xie", "Huan Liu"], "title": "LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG", "cs.MM"], "comment": "5 pages, 2 figures", "summary": "EEG-based multimodal emotion recognition(EMER) has gained significant\nattention and witnessed notable advancements, the inherent complexity of human\nneural systems has motivated substantial efforts toward multimodal approaches.\nHowever, this field currently suffers from three critical limitations: (i) the\nabsence of open-source implementations. (ii) the lack of standardized and\ntransparent benchmarks for fair performance analysis. (iii) in-depth discussion\nregarding main challenges and promising research directions is a notable\nscarcity. To address these challenges, we introduce LibEMER, a unified\nevaluation framework that provides fully reproducible PyTorch implementations\nof curated deep learning methods alongside standardized protocols for data\npreprocessing, model realization, and experimental setups. This framework\nenables unbiased performance assessment on three widely-used public datasets\nacross two learning tasks. The open-source library is publicly accessible at:\nhttps://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384"}
{"id": "2509.19522", "pdf": "https://arxiv.org/pdf/2509.19522", "abs": "https://arxiv.org/abs/2509.19522", "authors": ["Fabio Coelho", "Joao Victor T. Borges", "Paulo Padrao", "Jose Fuentes", "Ramon R. Costa", "Liu Hsu", "Leonardo Bobadilla"], "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a\nbioinspired SLAM framework based on computational models of the rodent\nhippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based\nSLAM, suitable for GPS-denied environments. Our contributions include a\nROS2-based architecture, experimental results on new waterway datasets, and\ninsights into system parameter tuning. This work represents the first known\napplication of RatSLAM on USVs. The estimated trajectory was compared with\nground truth data using the Hausdorff distance. The results show that the\nalgorithm can generate a semimetric map with an error margin acceptable for\nmost robotic applications."}
{"id": "2509.19331", "pdf": "https://arxiv.org/pdf/2509.19331", "abs": "https://arxiv.org/abs/2509.19331", "authors": ["Enhao Huang", "Zhiyu Zhang", "Tianxiang Xu", "Chunshu Xia", "Kaichun Hu", "Yuchen Yang", "Tongtong Pan", "Dong Dong", "Zhan Qin"], "title": "Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Complex-valued signals encode both amplitude and phase, yet most deep models\ntreat attention as real-valued correlation, overlooking interference effects.\nWe introduce the Holographic Transformer, a physics-inspired architecture that\nincorporates wave interference principles into self-attention. Holographic\nattention modulates interactions by relative phase and coherently superimposes\nvalues, ensuring consistency between amplitude and phase. A dual-headed decoder\nsimultaneously reconstructs the input and predicts task outputs, preventing\nphase collapse when losses prioritize magnitude over phase. We demonstrate that\nholographic attention implements a discrete interference operator and maintains\nphase consistency under linear mixing. Experiments on PolSAR image\nclassification and wireless channel prediction show strong performance,\nachieving high classification accuracy and F1 scores, low regression error, and\nincreased robustness to phase perturbations. These results highlight that\nenforcing physical consistency in attention leads to generalizable improvements\nin complex-valued learning and provides a unified, physics-based framework for\ncoherent signal modeling. The code is available at\nhttps://github.com/EonHao/Holographic-Transformers."}
{"id": "2509.19525", "pdf": "https://arxiv.org/pdf/2509.19525", "abs": "https://arxiv.org/abs/2509.19525", "authors": ["James Avtges", "Jake Ketchum", "Millicent Schlafly", "Helena Young", "Taekyoung Kim", "Allison Pinosky", "Ryan L. Truby", "Todd D. Murphey"], "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot", "categories": ["cs.RO"], "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "Closed-loop control remains an open challenge in soft robotics. The nonlinear\nresponses of soft actuators under dynamic loading conditions limit the use of\nanalytic models for soft robot control. Traditional methods of controlling soft\nrobots underutilize their configuration spaces to avoid nonlinearity,\nhysteresis, large deformations, and the risk of actuator damage. Furthermore,\nepisodic data-driven control approaches such as reinforcement learning (RL) are\ntraditionally limited by sample efficiency and inconsistency across\ninitializations. In this work, we demonstrate RL for reliably learning control\npolicies for dynamic balancing tasks in real-time single-shot hardware\ndeployments. We use a deformable Stewart platform constructed using parallel,\n3D-printed soft actuators based on motorized handed shearing auxetic (HSA)\nstructures. By introducing a curriculum learning approach based on expanding\nneighborhoods of a known equilibrium, we achieve reliable single-deployment\nbalancing at arbitrary coordinates. In addition to benchmarking the performance\nof model-based and model-free methods, we demonstrate that in a single\ndeployment, Maximum Diffusion RL is capable of learning dynamic balancing after\nhalf of the actuators are effectively disabled, by inducing buckling and by\nbreaking actuators with bolt cutters. Training occurs with no prior data, in as\nfast as 15 minutes, with performance nearly identical to the fully-intact\nplatform. Single-shot learning on hardware facilitates soft robotic systems\nreliably learning in the real world and will enable more diverse and capable\nsoft robots."}
{"id": "2509.19334", "pdf": "https://arxiv.org/pdf/2509.19334", "abs": "https://arxiv.org/abs/2509.19334", "authors": ["Shangqing Yuan", "Wenshuang Zhai", "Shengwen Guo"], "title": "A Spatio-Temporal Feature Fusion EEG Virtual Channel Signal Generation Network and Its Application in Anxiety Assessment", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "To address the issue of limited channels and insufficient information\ncollection in portable EEG devices, this study explores an EEG virtual channel\nsignal generation network using a novel spatio-temporal feature fusion\nstrategy. Based on the EEG signals from four frontal lobe channels, the network\naims to generate virtual channel EEG signals for other 13 important brain\nregions. The architecture of the network is a two-dimensional convolutional\nneural network and it includes a parallel module for temporal and spatial\ndomain feature extraction, followed by a feature fusion module. The public\nPRED+CT database, which includes multi-channel EEG signals from 119 subjects,\nwas selected to verify the constructed network. The results showed that the\naverage correlation coefficient between the generated virtual channel EEG\nsignals and the original real signals was 0.6724, with an average absolute\nerror of 3.9470. Furthermore, the 13 virtual channel EEG signals were combined\nwith the original EEG signals of four brain regions and then used for anxiety\nclassification with a support vector machine. The results indicate that the\nvirtual EEG signals generated by the constructed network not only have a high\ndegree of consistency with the real channel EEG signals but also significantly\nenhance the performance of machine learning algorithms for anxiety\nclassification. This study effectively alleviates the problem of insufficient\ninformation acquisition by portable EEG devices with few channels."}
{"id": "2509.19541", "pdf": "https://arxiv.org/pdf/2509.19541", "abs": "https://arxiv.org/abs/2509.19541", "authors": ["Xuan Cao", "Yuxin Wu", "Michael L. Whittaker"], "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture", "categories": ["cs.RO"], "comment": null, "summary": "Despite the rapidly growing applications of robots in industry, the use of\nrobots to automate tasks in scientific laboratories is less prolific due to\nlack of generalized methodologies and high cost of hardware. This paper focuses\non the automation of characterization tasks necessary for reducing cost while\nmaintaining generalization, and proposes a software architecture for building\nrobotic systems in scientific laboratory environment. A dual-layer (Socket.IO\nand ROS) action server design is the basic building block, which facilitates\nthe implementation of a web-based front end for user-friendly operations and\nthe use of ROS Behavior Tree for convenient task planning and execution. A\nrobotic platform for automating mineral and material sample characterization is\nbuilt upon the architecture, with an open source, low-cost three-axis computer\nnumerical control gantry system serving as the main robot. A handheld laser\ninduced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed\nadapter, enabling automated 2D chemical mapping. We demonstrate the utility of\nautomated chemical mapping by scanning of the surface of a spodumene-bearing\npegmatite core sample with a 1071-point dense hyperspectral map acquired at a\nrate of 1520 bits per second. Automated LIBS scanning enables controlled\nchemical quantification in the laboratory that complements field-based\nmeasurements acquired with the same handheld device, linking resource\nexploration and processing steps in the supply chain for lithium-based battery\nmaterials."}
{"id": "2509.19335", "pdf": "https://arxiv.org/pdf/2509.19335", "abs": "https://arxiv.org/abs/2509.19335", "authors": ["Xudong Zhang", "Jingbo Tan", "Zhizhen Ren", "Jintao Wang", "Yihua Ma", "Jian Song"], "title": "CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems", "categories": ["eess.SP", "cs.AI"], "comment": "13 pages, 16 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "ISAC is regarded as a promising technology for next-generation communication\nsystems, enabling simultaneous data transmission and target sensing. Among\nvarious tasks in ISAC, scatter sensing plays a crucial role in exploiting the\nfull potential of ISAC and supporting applications such as autonomous driving\nand low-altitude economy. However, most existing methods rely on either\nwaveform and hardware modifications or traditional signal processing schemes,\nleading to poor compatibility with current communication systems and limited\nsensing accuracy. To address these challenges, we propose CSIYOLO, a framework\nthat performs scatter localization only using estimated CSI from a single base\nstation-user equipment pair. This framework comprises two main components:\nanchor-based scatter parameter detection and CSI-based scatter localization.\nFirst, by formulating scatter parameter extraction as an image detection\nproblem, we propose an anchor-based scatter parameter detection method inspired\nby You Only Look Once architectures. After that, a CSI-based localization\nalgorithm is derived to determine scatter locations with extracted parameters.\nMoreover, to improve localization accuracy and implementation efficiency, we\ndesign an extendable network structure with task-oriented optimizations,\nenabling multi-scale anchor detection and better adaptation to CSI\ncharacteristics. A noise injection training strategy is further designed to\nenhance robustness against channel estimation errors. Since the proposed\nframework operates solely on estimated CSI without modifying waveforms or\nsignal processing pipelines, it can be seamlessly integrated into existing\ncommunication systems as a plugin. Experiments show that our proposed method\ncan significantly outperform existing methods in scatter localization accuracy\nwith relatively low complexities under varying numbers of scatters and\nestimation errors."}
{"id": "2509.19545", "pdf": "https://arxiv.org/pdf/2509.19545", "abs": "https://arxiv.org/abs/2509.19545", "authors": ["Min Dai", "Aaron D. Ames"], "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "We present RoMoCo, an open-source C++ toolbox for the synthesis and\nevaluation of reduced-order model-based planners and whole-body controllers for\nbipedal and humanoid robots. RoMoCo's modular architecture unifies\nstate-of-the-art planners and whole-body locomotion controllers under a\nconsistent API, enabling rapid prototyping and reproducible benchmarking. By\nleveraging reduced-order models for platform-agnostic gait generation, RoMoCo\nenables flexible controller design across diverse robots. We demonstrate its\nversatility and performance through extensive simulations on the Cassie,\nUnitree H1, and G1 robots, and validate its real-world efficacy with hardware\nexperiments on the Cassie and G1 humanoids."}
{"id": "2509.19340", "pdf": "https://arxiv.org/pdf/2509.19340", "abs": "https://arxiv.org/abs/2509.19340", "authors": ["Ying Ju", "Mingdong Li", "Haoyu Wang", "Lei Liu", "Youyang Qu", "Mianxiong Dong", "Victor C. M. Leung", "Chau Yuen"], "title": "Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "With the emergence of fluid antenna (FA) in wireless communications, the\ncapability to dynamically adjust port positions offers substantial benefits in\nspatial diversity and spectrum efficiency, which are particularly valuable for\nmobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC\noffloading framework to minimize system delay. This framework faces two severe\nchallenges, which are the complexity of channel estimation due to dynamic port\nconfiguration and the inherent non-convexity of the joint optimization problem.\nFirstly, we propose Information Bottleneck Metric-enhanced Channel Compressed\nSensing (IBM-CCS), which advances FA channel estimation by integrating\ninformation relevance into the sensing process and capturing key features of FA\nchannels effectively. Secondly, to address the non-convex and high-dimensional\noptimization problem in FA-assisted MEC systems, which includes FA port\nselection, beamforming, power control, and resource allocation, we propose a\ngame theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA)\nbased offloading scheme, where the hierarchical structure effectively decouples\nand coordinates the optimization tasks between the user side and the base\nstation side. Crucially, the game theory effectively reduces the dimensionality\nof power control variables, allowing deep reinforcement learning (DRL) agents\nto achieve improved optimization efficiency. Numerical results confirm that the\nproposed scheme significantly reduces system delay and enhances offloading\nperformance, outperforming benchmarks. Additionally, the IBM-CCS channel\nestimation demonstrates superior accuracy and robustness under varying port\ndensities, contributing to efficient communication under imperfect CSI."}
{"id": "2509.19555", "pdf": "https://arxiv.org/pdf/2509.19555", "abs": "https://arxiv.org/abs/2509.19555", "authors": ["Sankalp Agrawal", "Junwon Seo", "Kensuke Nakamura", "Ran Tian", "Andrea Bajcsy"], "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recent works have shown that foundational safe control methods, such as\nHamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space\nof world models. While this enables the synthesis of latent safety filters for\nhard-to-model vision-based tasks, they assume that the safety constraint is\nknown a priori and remains fixed during deployment, limiting the safety\nfilter's adaptability across scenarios. To address this, we propose\nconstraint-parameterized latent safety filters that can adapt to user-specified\nsafety constraints at runtime. Our key idea is to define safety constraints by\nconditioning on an encoding of an image that represents a constraint, using a\nlatent-space similarity measure. The notion of similarity to failure is aligned\nin a principled way through conformal calibration, which controls how closely\nthe system may approach the constraint representation. The parameterized safety\nfilter is trained entirely within the world model's imagination, treating any\nimage seen by the model as a potential test-time constraint, thereby enabling\nruntime adaptation to arbitrary safety constraints. In simulation and hardware\nexperiments on vision-based control tasks with a Franka manipulator, we show\nthat our method adapts at runtime by conditioning on the encoding of\nuser-specified constraint images, without sacrificing performance. Video\nresults can be found on https://any-safe.github.io"}
{"id": "2509.19342", "pdf": "https://arxiv.org/pdf/2509.19342", "abs": "https://arxiv.org/abs/2509.19342", "authors": ["Xinyu Qin", "Ye Xue", "Qi Yan", "Shutao Zhang", "Bingsheng Peng", "Tsung-Hui Chang"], "title": "A Measurement Report Data-Driven Framework for Localized Statistical Channel Modeling", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Localized statistical channel modeling (LSCM) is crucial for effective\nperformance evaluation in digital twin-assisted network optimization. Solely\nrelying on the multi-beam reference signal receiving power (RSRP), LSCM aims to\nmodel the localized statistical propagation environment by estimating the\nchannel angular power spectrum (APS). However, existing methods rely heavily on\ndrive test data with high collection costs and limited spatial coverage. In\nthis paper, we propose a measurement report (MR) data-driven framework for\nLSCM, exploiting the low-cost and extensive collection of MR data. The\nframework comprises two novel modules. The MR localization module addresses the\nissue of missing locations in MR data by introducing a semi-supervised method\nbased on hypergraph neural networks, which exploits multi-modal information via\ndistance-aware hypergraph modeling and hypergraph convolution for location\nextraction. To enhance the computational efficiency and solution robustness,\nLSCM operates at the grid level. Compared to independently constructing\ngeographically uniform grids and estimating channel APS, the joint grid\nconstruction and channel APS estimation module enhances robustness in complex\nenvironments with spatially non-uniform data by exploiting their correlation.\nThis module alternately optimizes grid partitioning and APS estimation using\nclustering and improved sparse recovery for the ill-conditioned measurement\nmatrix and incomplete observations. Through comprehensive experiments on a\nreal-world MR dataset, we demonstrate the superior performance and robustness\nof our framework in localization and channel modeling."}
{"id": "2509.19571", "pdf": "https://arxiv.org/pdf/2509.19571", "abs": "https://arxiv.org/abs/2509.19571", "authors": ["Sacha Morin", "Kumaraditya Gupta", "Mahtab Sandhu", "Charlie Gauthier", "Francesco Argenziano", "Kirsty Ellis", "Liam Paull"], "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action", "categories": ["cs.RO", "cs.CV"], "comment": "Project page:\n  https://montrealrobotics.ca/agentic-scene-policies.github.io/", "summary": "Executing open-ended natural language queries is a core problem in robotics.\nWhile recent advances in imitation learning and vision-language-actions models\n(VLAs) have enabled promising end-to-end policies, these models struggle when\nfaced with complex instructions and new scenes. An alternative is to design an\nexplicit scene representation as a queryable interface between the robot and\nthe world, using query results to guide downstream motion planning. In this\nwork, we present Agentic Scene Policies (ASP), an agentic framework that\nleverages the advanced semantic, spatial, and affordance-based querying\ncapabilities of modern scene representations to implement a capable\nlanguage-conditioned robot policy. ASP can execute open-vocabulary queries in a\nzero-shot manner by explicitly reasoning about object affordances in the case\nof more complex skills. Through extensive experiments, we compare ASP with VLAs\non tabletop manipulation problems and showcase how ASP can tackle room-level\nqueries through affordance-guided navigation, and a scaled-up scene\nrepresentation. (Project page:\nhttps://montrealrobotics.ca/agentic-scene-policies.github.io/)"}
{"id": "2509.19367", "pdf": "https://arxiv.org/pdf/2509.19367", "abs": "https://arxiv.org/abs/2509.19367", "authors": ["Borhan Uddin Chowdhury", "Damian Valles", "Md Raf E Ul Shougat"], "title": "Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods", "categories": ["eess.SP", "cs.LG", "stat.ML"], "comment": "Copyright 2025 IEEE. This is the author's version of the work\n  accepted for publication in FMLDS 2025. The final version will be published\n  by IEEE and available via DOI (to be inserted when available). Accepted at\n  FMLDS 2025, to appear in IEEE Xplore. 8 pages, 17 figures, 3 tables", "summary": "We present a sensor-fusion framework for rapid, non-destructive\nclassification and quality control of organic substances, built on a standard\nArduino Mega 2560 microcontroller platform equipped with three commercial\nenvironmental and gas sensors. All data used in this study were generated\nin-house: sensor outputs for ten distinct classes - including fresh and expired\nsamples of apple juice, onion, garlic, and ginger, as well as cinnamon and\ncardamom - were systematically collected and labeled using this hardware setup,\nresulting in a unique, application-specific dataset. Correlation analysis was\nemployed as part of the preprocessing pipeline for feature selection. After\npreprocessing and dimensionality reduction (PCA/LDA), multiple supervised\nlearning models - including Support Vector Machine (SVM), Decision Tree (DT),\nand Random Forest (RF), each with hyperparameter tuning, as well as an\nArtificial Neural Network (ANN) and an ensemble voting classifier - were\ntrained and cross-validated on the collected dataset. The best-performing\nmodels, including tuned Random Forest, ensemble, and ANN, achieved test\naccuracies in the 93 to 94 percent range. These results demonstrate that\nlow-cost, multisensory platforms based on the Arduino Mega 2560, combined with\nadvanced machine learning and correlation-driven feature engineering, enable\nreliable identification and quality control of organic compounds."}
{"id": "2509.19573", "pdf": "https://arxiv.org/pdf/2509.19573", "abs": "https://arxiv.org/abs/2509.19573", "authors": ["Zachary Olkin", "Kejun Li", "William D. Compton", "Aaron D. Ames"], "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Achieving highly dynamic behaviors on humanoid robots, such as running,\nrequires controllers that are both robust and precise, and hence difficult to\ndesign. Classical control methods offer valuable insight into how such systems\ncan stabilize themselves, but synthesizing real-time controllers for nonlinear\nand hybrid dynamics remains challenging. Recently, reinforcement learning (RL)\nhas gained popularity for locomotion control due to its ability to handle these\ncomplex dynamics. In this work, we embed ideas from nonlinear control theory,\nspecifically control Lyapunov functions (CLFs), along with optimized dynamic\nreference trajectories into the reinforcement learning training process to\nshape the reward. This approach, CLF-RL, eliminates the need to handcraft and\ntune heuristic reward terms, while simultaneously encouraging certifiable\nstability and providing meaningful intermediate rewards to guide learning. By\ngrounding policy learning in dynamically feasible trajectories, we expand the\nrobot's dynamic capabilities and enable running that includes both flight and\nsingle support phases. The resulting policy operates reliably on a treadmill\nand in outdoor environments, demonstrating robustness to disturbances applied\nto the torso and feet. Moreover, it achieves accurate global reference tracking\nutilizing only on-board sensors, making a critical step toward integrating\nthese dynamic motions into a full autonomy stack."}
{"id": "2509.19374", "pdf": "https://arxiv.org/pdf/2509.19374", "abs": "https://arxiv.org/abs/2509.19374", "authors": ["Oscar A. Oviedo"], "title": "Short-Term Regional Electricity Demand Forecasting in Argentina Using LSTM Networks", "categories": ["eess.SP", "cs.LG"], "comment": "44 pages, 13 figures", "summary": "This study presents the development and optimization of a deep learning model\nbased on Long Short-Term Memory (LSTM) networks to predict short-term hourly\nelectricity demand in C\\'ordoba, Argentina. Integrating historical consumption\ndata with exogenous variables (climatic factors, temporal cycles, and\ndemographic statistics), the model achieved high predictive precision, with a\nmean absolute percentage error of 3.20\\% and a determination coefficient of\n0.95. The inclusion of periodic temporal encodings and weather variables proved\ncrucial to capture seasonal patterns and extreme consumption events, enhancing\nthe robustness and generalizability of the model. In addition to the design and\nhyperparameter optimization of the LSTM architecture, two complementary\nanalyses were carried out: (i) an interpretability study using Random Forest\nregression to quantify the relative importance of exogenous drivers, and (ii)\nan evaluation of model performance in predicting the timing of daily demand\nmaxima and minima, achieving exact-hour accuracy in more than two-thirds of the\ntest days and within abs(1) hour in over 90\\% of cases. Together, these results\nhighlight both the predictive accuracy and operational relevance of the\nproposed framework, providing valuable insights for grid operators seeking\noptimized planning and control strategies under diverse demand scenarios."}
{"id": "2509.19579", "pdf": "https://arxiv.org/pdf/2509.19579", "abs": "https://arxiv.org/abs/2509.19579", "authors": ["Chad R. Samuelson", "Abigail Austin", "Seth Knoop", "Blake Romrell", "Gabriel R. Slade", "Timothy W. McLain", "Joshua G. Mangelson"], "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Outdoor intelligent autonomous robotic operation relies on a sufficiently\nexpressive map of the environment. Classical geometric mapping methods retain\nessential structural environment information, but lack a semantic understanding\nand organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)\naddress this limitation by integrating geometric, topological, and semantic\nrelationships into a multi-level graph-based map. Outdoor autonomous operations\ncommonly rely on terrain information either due to task-dependence or the\ntraversability of the robotic platform. We propose a novel approach that\ncombines indoor 3DSG techniques with standard outdoor geometric mapping and\nterrain-aware reasoning, producing terrain-aware place nodes and hierarchically\norganized regions for outdoor environments. Our method generates a\ntask-agnostic metric-semantic sparse map and constructs a 3DSG from this map\nfor downstream planning tasks, all while remaining lightweight for autonomous\nrobotic operation. Our thorough evaluation demonstrates our 3DSG method\nperforms on par with state-of-the-art camera-based 3DSG methods in object\nretrieval and surpasses them in region classification while remaining memory\nefficient. We demonstrate its effectiveness in diverse robotic tasks of object\nretrieval and region monitoring in both simulation and real-world environments."}
{"id": "2509.19382", "pdf": "https://arxiv.org/pdf/2509.19382", "abs": "https://arxiv.org/abs/2509.19382", "authors": ["Xiaolong Li", "Zhi-qin John Xu", "Peiting You", "Yifei Zhu"], "title": "Neural Network Based Framework for Passive Intermodulation Cancellation in MIMO Systems", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Passive intermodulation (PIM) has emerged as a critical source of\nself-interference in modern MIMO-OFDM systems, especially under the stringent\nrequirements of 5G and beyond. Conventional cancellation methods often rely on\ncomplex nonlinear models with limited scalability and high computational cost.\nIn this work, we propose a lightweight deep learning framework for PIM\ncancellation that leverages depthwise separable convolutions and dilated\nconvolutions to efficiently capture nonlinear dependencies across antennas and\nsubcarriers. To further enhance convergence, we adopt a cyclic learning rate\nschedule and gradient clipping. In a controlled MIMO experimental setup, the\nmethod effectively suppresses third-order passive intermodulation (PIM)\ndistortion, achieving up to 29dB of average power error (APE) with only 11k\ntrainable parameters. These results highlight the potential of compact neural\narchitectures for scalable interference mitigation in future wireless\ncommunication systems."}
{"id": "2509.19597", "pdf": "https://arxiv.org/pdf/2509.19597", "abs": "https://arxiv.org/abs/2509.19597", "authors": ["Sander Tonkens", "Nikhil Uday Shinde", "Azra Begzadić", "Michael C. Yip", "Jorge Cortés", "Sylvia L. Herbert"], "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The first three authors contributed equally. This work has been\n  accepted for publication at the Conference on Robot Learning", "summary": "The widespread deployment of autonomous systems in safety-critical\nenvironments such as urban air mobility hinges on ensuring reliable,\nperformant, and safe operation under varying environmental conditions. One such\napproach, value function-based safety filters, minimally modifies a nominal\ncontroller to ensure safety. Recent advances leverage offline learned value\nfunctions to scale these safety filters to high-dimensional systems. However,\nthese methods assume detailed priors on all possible sources of model mismatch,\nin the form of disturbances in the environment -- information that is rarely\navailable in real world settings. Even in well-mapped environments like urban\ncanyons or industrial sites, drones encounter complex, spatially-varying\ndisturbances arising from payload-drone interaction, turbulent airflow, and\nother environmental factors. We introduce SPACE2TIME, which enables safe and\nadaptive deployment of offline-learned safety filters under unknown,\nspatially-varying disturbances. The key idea is to reparameterize spatial\nvariations in disturbance as temporal variations, enabling the use of\nprecomputed value functions during online operation. We validate SPACE2TIME on\na quadcopter through extensive simulations and hardware experiments,\ndemonstrating significant improvement over baselines."}
{"id": "2509.19383", "pdf": "https://arxiv.org/pdf/2509.19383", "abs": "https://arxiv.org/abs/2509.19383", "authors": ["Qianqian Li", "Hua Li", "Shiya Hao", "Lintao Li", "Xiaoming Dai"], "title": "Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs", "categories": ["eess.SP", "cs.IT", "cs.PF", "math.IT"], "comment": null, "summary": "This study evaluates the performance of an active reconfigurable intelligent\nsurface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing\nlow-precision analog-to-digital converters (ADCs). Analytical approximations\nfor the outage probability (OP) are derived, considering residual hardware\nimpairments (RHIs) and imperfect successive interference cancellation (ipSIC).\nAdditionally, we analyze the asymptotic OP, system throughput, and diversity\norder at high signal-to-noise ratios (SNRs). Simulation results demonstrate\nthat the proposed quantized ARIS-NOMA system outperforms its passive\ncounterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced\ntransmit power requirements and fewer reflecting elements. Moreover, the outage\nperformance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates\nsignificant improvement as the number of reflecting elements increases. The\nnegative impacts of low-precision ADCs can be effectively mitigated by\noptimizing transmit power and scaling the number of reflecting elements."}
{"id": "2509.19610", "pdf": "https://arxiv.org/pdf/2509.19610", "abs": "https://arxiv.org/abs/2509.19610", "authors": ["Qingxi Meng", "Emiliano Flores", "Carlos Quintero-Peña", "Peizhu Qian", "Zachary Kingston", "Shannan K. Hamlin", "Vaibhav Unhelkar", "Lydia E. Kavraki"], "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots", "categories": ["cs.RO"], "comment": "16 pages, 10 figures, under review", "summary": "In this work, we address the problem of planning robot motions for a\nhigh-degree-of-freedom (DoF) robot that effectively achieves a given perception\ntask while the robot and the perception target move in a dynamic environment.\nAchieving navigation and perception tasks simultaneously is challenging, as\nthese objectives often impose conflicting requirements. Existing methods that\ncompute motion under perception constraints fail to account for obstacles, are\ndesigned for low-DoF robots, or rely on simplified models of perception.\nFurthermore, in dynamic real-world environments, robots must replan and react\nquickly to changes and directly evaluating the quality of perception (e.g.,\nobject detection confidence) is often expensive or infeasible at runtime. This\nproblem is especially important in human-centered environments such as homes\nand hospitals, where effective perception is essential for safe and reliable\noperation. To address these challenges, we propose a GPU-parallelized\nperception-score-guided probabilistic roadmap planner with a neural surrogate\nmodel (PS-PRM). The planner explicitly incorporates the estimated quality of a\nperception task into motion planning for high-DoF robots. Our method uses a\nlearned model to approximate perception scores and leverages GPU parallelism to\nenable efficient online replanning in dynamic settings. We demonstrate that our\nplanner, evaluated on high-DoF robots, outperforms baseline methods in both\nstatic and dynamic environments in both simulation and real-robot experiments."}
{"id": "2509.19384", "pdf": "https://arxiv.org/pdf/2509.19384", "abs": "https://arxiv.org/abs/2509.19384", "authors": ["Hongyuan Shi", "Yilin Zhai", "Ping Dong", "Zaijin You", "Chao Zhan", "Qing Wang"], "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations", "categories": ["eess.SP", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Reconstructing high-resolution regional significant wave height fields from\nsparse and uneven buoy observations remains a core challenge for ocean\nmonitoring and risk-aware operations. We introduce AUWave, a hybrid deep\nlearning framework that fuses a station-wise sequence encoder (MLP) with a\nmulti-scale U-Net enhanced by a bottleneck self-attention layer to recover\n32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search\nwith Optuna identifies the learning rate as the dominant driver of\ngeneralization, followed by the scheduler decay and the latent dimension. Using\nNDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave\nattains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE\ndistribution. Spatial errors are lowest near observation sites and increase\nwith distance, reflecting identifiability limits under sparse sampling.\nSensitivity experiments show that AUWave consistently outperforms a\nrepresentative baseline in data-richer configurations, while the baseline is\nonly marginally competitive in the most underdetermined single-buoy cases. The\narchitecture's multi-scale and attention components translate into accuracy\ngains when minimal but non-trivial spatial anchoring is available. Error maps\nand buoy ablations reveal key anchor stations whose removal disproportionately\ndegrades performance, offering actionable guidance for network design. AUWave\nprovides a scalable pathway for gap filling, high-resolution priors for data\nassimilation, and contingency reconstruction."}
{"id": "2509.19626", "pdf": "https://arxiv.org/pdf/2509.19626", "abs": "https://arxiv.org/abs/2509.19626", "authors": ["Ryan Punamiya", "Dhruv Patel", "Patcharapong Aphiwetsa", "Pranav Kuppili", "Lawrence Y. Zhu", "Simar Kareer", "Judy Hoffman", "Danfei Xu"], "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)", "summary": "Egocentric human experience data presents a vast resource for scaling up\nend-to-end imitation learning for robotic manipulation. However, significant\ndomain gaps in visual appearance, sensor modalities, and kinematics between\nhuman and robot impede knowledge transfer. This paper presents EgoBridge, a\nunified co-training framework that explicitly aligns the policy latent spaces\nbetween human and robot data using domain adaptation. Through a measure of\ndiscrepancy on the joint policy latent features and actions based on Optimal\nTransport (OT), we learn observation representations that not only align\nbetween the human and robot domain but also preserve the action-relevant\ninformation critical for policy learning. EgoBridge achieves a significant\nabsolute policy success rate improvement by 44% over human-augmented\ncross-embodiment baselines in three real-world single-arm and bimanual\nmanipulation tasks. EgoBridge also generalizes to new objects, scenes, and\ntasks seen only in human data, where baselines fail entirely. Videos and\nadditional information can be found at https://ego-bridge.github.io"}
{"id": "2509.19385", "pdf": "https://arxiv.org/pdf/2509.19385", "abs": "https://arxiv.org/abs/2509.19385", "authors": ["Benjamin J. Choi", "Griffin Milsap", "Clara A. Scholl", "Francesco Tenore", "Mattson Ogg"], "title": "A Statistical Mixture-of-Experts Framework for EMG Artifact Removal in EEG: Empirical Insights and a Proof-of-Concept Application", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Effective control of neural interfaces is limited by poor signal quality.\nWhile neural network-based electroencephalography (EEG) denoising methods for\nelectromyogenic (EMG) artifacts have improved in recent years, current\nstate-of-the-art (SOTA) models perform suboptimally in settings with high\nnoise. To address the shortcomings of current machine learning (ML)-based\ndenoising algorithms, we present a signal filtration algorithm driven by a new\nmixture-of-experts (MoE) framework. Our algorithm leverages three new\nstatistical insights into the EEG-EMG denoising problem: (1) EMG artifacts can\nbe partitioned into quantifiable subtypes to aid downstream MoE classification,\n(2) local experts trained on narrower signal-to-noise ratio (SNR) ranges can\nachieve performance increases through specialization, and (3) correlation-based\nobjective functions, in conjunction with rescaling algorithms, can enable\nfaster convergence in a neural network-based denoising context. We empirically\ndemonstrate these three insights into EMG artifact removal and use our findings\nto create a new downstream MoE denoising algorithm consisting of convolutional\n(CNN) and recurrent (RNN) neural networks. We tested all results on a major\nbenchmark dataset (EEGdenoiseNet) collected from 67 subjects. We found that our\nMoE denoising model achieved competitive overall performance with SOTA ML\ndenoising algorithms and superior lower bound performance in high noise\nsettings. These preliminary results highlight the promise of our MoE framework\nfor enabling advances in EMG artifact removal for EEG processing, especially in\nhigh noise settings. Further research and development will be necessary to\nassess our MoE framework on a wider range of real-world test cases and explore\nits downstream potential to unlock more effective neural interfaces."}
{"id": "2509.19636", "pdf": "https://arxiv.org/pdf/2509.19636", "abs": "https://arxiv.org/abs/2509.19636", "authors": ["Mahmoud Ali", "Hassan Jardali", "Youwei Yu", "Durgakant Pushp", "Lantao Liu"], "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The data associated with this paper is available at\n  https://doi.org/10.5281/zenodo.17187680", "summary": "Autonomous racing has seen significant advancements, driven by competitions\nsuch as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing\nLeague (A2RL). However, developing an autonomous racing stack for a full-scale\ncar is often constrained by limited access to dedicated test tracks,\nrestricting opportunities for real-world validation. While previous work\ntypically requires extended development cycles and significant track time, this\npaper introduces a minimalistic autonomous racing stack for high-speed\ntime-trial racing that emphasizes rapid deployment and efficient system\nintegration with minimal on-track testing. The proposed stack was validated on\nreal speedways, achieving a top speed of 206 km/h within just 11 hours'\npractice run on the track with 325 km in total. Additionally, we present the\nsystem performance analysis, including tracking accuracy, vehicle dynamics, and\nsafety considerations, offering insights for teams seeking to rapidly develop\nand deploy an autonomous racing stack with limited track access."}
{"id": "2509.19387", "pdf": "https://arxiv.org/pdf/2509.19387", "abs": "https://arxiv.org/abs/2509.19387", "authors": ["Antonio Quintero Rincon", "Nicolas Masino", "Veronica Marsico", "Hadj Batatia"], "title": "Hybrid Pipeline SWD Detection in Long-Term EEG Signals", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ML"], "comment": "11 pages, 8 figures, 4 tables, SABI 2025 CLIC 2025", "summary": "Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of\nabsence epilepsy, yet their manual identification in multi-day recordings\nremains labour-intensive and error-prone. We present a lightweight hybrid\npipeline that couples analytical features with a shallow artificial neural\nnetwork (ANN) for accurate, patient-specific SWD detection in long-term,\nmonopolar EEG. A two-sided moving-average (MA) filter first suppresses the\nhigh-frequency components of normal background activity. The residual signal is\nthen summarised by the mean and the standard deviation of its normally\ndistributed samples, yielding a compact, two-dimensional feature vector for\nevery 20s window. These features are fed to a single-hidden-layer ANN trained\nvia back-propagation to classify each window as SWD or non-SWD. The method was\nevaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392\nannotated SWD events. It correctly detected 384 events (sensitivity: 98%) while\nachieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because\nfeature extraction is analytic, and the classifier is small, the pipeline runs\nin real-time and requires no manual threshold tuning. These results indicate\nthat normal-distribution descriptors combined with a modest ANN provide an\neffective and computationally inexpensive solution for automated SWD screening\nin extended EEG recordings."}
{"id": "2509.19658", "pdf": "https://arxiv.org/pdf/2509.19658", "abs": "https://arxiv.org/abs/2509.19658", "authors": ["Youngju Yoo", "Jiaheng Hu", "Yifeng Zhu", "Bo Liu", "Qiang Liu", "Roberto Martín-Martín", "Peter Stone"], "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 11 figures", "summary": "In-context imitation learning (ICIL) enables robots to learn tasks from\nprompts consisting of just a handful of demonstrations. By eliminating the need\nfor parameter updates at deployment time, this paradigm supports few-shot\nadaptation to novel tasks. However, recent ICIL methods rely on Transformers,\nwhich have computational limitations and tend to underperform when handling\nlonger prompts than those seen during training. In this work, we introduce\nRoboSSM, a scalable recipe for in-context imitation learning based on\nstate-space models (SSM). Specifically, RoboSSM replaces Transformers with\nLonghorn -- a state-of-the-art SSM that provides linear-time inference and\nstrong extrapolation capabilities, making it well-suited for long-context\nprompts. We evaluate our approach on the LIBERO benchmark and compare it\nagainst strong Transformer-based ICIL baselines. Experiments show that RoboSSM\nextrapolates effectively to varying numbers of in-context demonstrations,\nyields high performance on unseen tasks, and remains robust in long-horizon\nscenarios. These results highlight the potential of SSMs as an efficient and\nscalable backbone for ICIL. Our code is available at\nhttps://github.com/youngjuY/RoboSSM."}
{"id": "2509.19397", "pdf": "https://arxiv.org/pdf/2509.19397", "abs": "https://arxiv.org/abs/2509.19397", "authors": ["Jiarui Jin", "Xiaocheng Fang", "Haoyu Wang", "Jun Li", "Che Liu", "Donglin Xie", "Hongyan Li", "Shenda Hong"], "title": "Self-Alignment Learning to Improve Myocardial Infarction Detection from Single-Lead ECG", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Myocardial infarction is a critical manifestation of coronary artery disease,\nyet detecting it from single-lead electrocardiogram (ECG) remains challenging\ndue to limited spatial information. An intuitive idea is to convert single-lead\ninto multiple-lead ECG for classification by pre-trained models, but generative\nmethods optimized at the signal level in most cases leave a large latent space\ngap, ultimately degrading diagnostic performance. This naturally raises the\nquestion of whether latent space alignment could help. However, most prior ECG\nalignment methods focus on learning transformation invariance, which mismatches\nthe goal of single-lead detection. To address this issue, we propose SelfMIS, a\nsimple yet effective alignment learning framework to improve myocardial\ninfarction detection from single-lead ECG. Discarding manual data\naugmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead\nECG with their corresponding single-lead segments and directly align them in\nthe latent space. This design shifts the learning objective from pursuing\ntransformation invariance to enriching the single-lead representation,\nexplicitly driving the single-lead ECG encoder to learn a representation\ncapable of inferring global cardiac context from the local signal.\nExperimentally, SelfMIS achieves superior performance over baseline models\nacross nine myocardial infarction types while maintaining a simpler\narchitecture and lower computational overhead, thereby substantiating the\nefficacy of direct latent space alignment. Our code and checkpoint will be\npublicly available after acceptance."}
{"id": "2509.19672", "pdf": "https://arxiv.org/pdf/2509.19672", "abs": "https://arxiv.org/abs/2509.19672", "authors": ["Dongzhe Zheng", "Wenjie Mei"], "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains", "categories": ["cs.RO", "math.DS"], "comment": "Accepted by NeurIPS 2025", "summary": "Stochastic optimal control methods often struggle in complex non-convex\nlandscapes, frequently becoming trapped in local optima due to their inability\nto learn from historical trajectory data. This paper introduces\nMemory-Augmented Potential Field Theory, a unified mathematical framework that\nintegrates historical experience into stochastic optimal control. Our approach\ndynamically constructs memory-based potential fields that identify and encode\nkey topological features of the state space, enabling controllers to\nautomatically learn from past experiences and adapt their optimization\nstrategy. We provide a theoretical analysis showing that memory-augmented\npotential fields possess non-convex escape properties, asymptotic convergence\ncharacteristics, and computational efficiency. We implement this theoretical\nframework in a Memory-Augmented Model Predictive Path Integral (MPPI)\ncontroller that demonstrates significantly improved performance in challenging\nnon-convex environments. The framework represents a generalizable approach to\nexperience-based learning within control systems (especially robotic dynamics),\nenhancing their ability to navigate complex state spaces without requiring\nspecialized domain knowledge or extensive offline training."}
{"id": "2509.19401", "pdf": "https://arxiv.org/pdf/2509.19401", "abs": "https://arxiv.org/abs/2509.19401", "authors": ["Jiazhen Hong", "Geoff Mackellar", "Soheila Ghane"], "title": "SpellerSSL: Self-Supervised Learning with P300 Aggregation for Speller BCIs", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Electroencephalogram (EEG)-based P300 speller brain-computer interfaces\n(BCIs) face three main challenges: low signal-to-noise ratio (SNR), poor\ngeneralization, and time-consuming calibration. We propose SpellerSSL, a\nframework that combines self-supervised learning (SSL) with P300 aggregation to\naddress these issues. First, we introduce an aggregation strategy to enhance\nSNR. Second, to achieve generalization in training, we employ a customized 1D\nU-Net backbone and pretrain the model on both cross-domain and in-domain EEG\ndata. The pretrained model is subsequently fine-tuned with a lightweight\nERP-Head classifier for P300 detection, which adapts the learned\nrepresentations to subject-specific data. Our evaluations on calibration time\ndemonstrate that combining the aggregation strategy with SSL significantly\nreduces the calibration burden per subject and improves robustness across\nsubjects. Experimental results show that SSL learns effective EEG\nrepresentations in both in-domain and cross-domain, with in-domain achieving a\nstate-of-the-art character recognition rate of 94% with only 7 repetitions and\nthe highest information transfer rate (ITR) of 21.86 bits/min on the public\nII-B dataset. Moreover, in-domain SSL with P300 aggregation reduces the\nrequired calibration size by 60% while maintaining a comparable character\nrecognition rate. To the best of our knowledge, this is the first study to\napply SSL to P300 spellers, highlighting its potential to improve both\nefficiency and generalization in speller BCIs and paving the way toward an EEG\nfoundation model for P300 speller BCIs."}
{"id": "2509.19688", "pdf": "https://arxiv.org/pdf/2509.19688", "abs": "https://arxiv.org/abs/2509.19688", "authors": ["Devesh Nath", "Haoran Yin", "Glen Chou"], "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "10 pages, 12 figures", "summary": "We present a method for formal safety verification of learning-based\ngenerative motion planners. Generative motion planners (GMPs) offer advantages\nover traditional planners, but verifying the safety and dynamic feasibility of\ntheir outputs is difficult since neural network verification (NNV) tools scale\nonly to a few hundred neurons, while GMPs often contain millions. To preserve\nGMP expressiveness while enabling verification, our key insight is to imitate\nthe GMP by stabilizing references sampled from the GMP with a small neural\ntracking controller and then applying NNV to the closed-loop dynamics. This\nyields reachable sets that rigorously certify closed-loop safety, while the\ncontroller enforces dynamic feasibility. Building on this, we construct a\nlibrary of verified GMP references and deploy them online in a way that\nimitates the original GMP distribution whenever it is safe to do so, improving\nsafety without retraining. We evaluate across diverse planners, including\ndiffusion, flow matching, and vision-language models, improving safety in\nsimulation (on ground robots and quadcopters) and on hardware\n(differential-drive robot)."}
{"id": "2509.19403", "pdf": "https://arxiv.org/pdf/2509.19403", "abs": "https://arxiv.org/abs/2509.19403", "authors": ["Sheng-Bin Duan", "Jian-Long Hao", "Tian-Yu Xiang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Zeng-Guang Hou"], "title": "Online Adaptation via Dual-Stage Alignment and Self-Supervision for Fast-Calibration Brain-Computer Interfaces", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Individual differences in brain activity hinder the online application of\nelectroencephalogram (EEG)-based brain computer interface (BCI) systems. To\novercome this limitation, this study proposes an online adaptation algorithm\nfor unseen subjects via dual-stage alignment and self-supervision. The\nalignment process begins by applying Euclidean alignment in the EEG data space\nand then updates batch normalization statistics in the representation space.\nMoreover, a self-supervised loss is designed to update the decoder. The loss is\ncomputed by soft pseudo-labels derived from the decoder as a proxy for the\nunknown ground truth, and is calibrated by Shannon entropy to facilitate\nself-supervised training. Experiments across five public datasets and seven\ndecoders show the proposed algorithm can be integrated seamlessly regardless of\nBCI paradigm and decoder architecture. In each iteration, the decoder is\nupdated with a single online trial, which yields average accuracy gains of 4.9%\non steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery.\nThese results support fast-calibration operation and show that the proposed\nalgorithm has great potential for BCI applications."}
{"id": "2509.19696", "pdf": "https://arxiv.org/pdf/2509.19696", "abs": "https://arxiv.org/abs/2509.19696", "authors": ["Noah Geiger", "Tamim Asfour", "Neville Hogan", "Johannes Lachner"], "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "15 pages, 12 figures", "summary": "Learning methods excel at motion generation in the information domain but are\nnot primarily designed for physical interaction in the energy domain. Impedance\nControl shapes physical interaction but requires task-aware tuning by selecting\nfeasible impedance parameters. We present Diffusion-Based Impedance Learning, a\nframework that combines both domains. A Transformer-based Diffusion Model with\ncross-attention to external wrenches reconstructs a simulated Zero-Force\nTrajectory (sZFT). This captures both translational and rotational task-space\nbehavior. For rotations, we introduce a novel SLERP-based quaternion noise\nscheduler that ensures geometric consistency. The reconstructed sZFT is then\npassed to an energy-based estimator that updates stiffness and damping\nparameters. A directional rule is applied that reduces impedance along non task\naxes while preserving rigidity along task directions. Training data were\ncollected for a parkour scenario and robotic-assisted therapy tasks using\nteleoperation with Apple Vision Pro. With only tens of thousands of samples,\nthe model achieved sub-millimeter positional accuracy and sub-degree rotational\naccuracy. Its compact model size enabled real-time torque control and\nautonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller\nachieved smooth parkour traversal within force and velocity limits and 30/30\nsuccess rates for cylindrical, square, and star peg insertions without any\npeg-specific demonstrations in the training data set. All code for the\nTransformer-based Diffusion Model, the robot controller, and the Apple Vision\nPro telemanipulation framework is publicly available. These results mark an\nimportant step towards Physical AI, fusing model-based control for physical\ninteraction with learning-based methods for trajectory generation."}
{"id": "2509.19551", "pdf": "https://arxiv.org/pdf/2509.19551", "abs": "https://arxiv.org/abs/2509.19551", "authors": ["Jérôme Leclère", "Thyagaraja Marathe", "Tyler G. R. Reid"], "title": "Insights into Xona Pulsar LEO PNT: Constellation, Signals, and Receiver Design", "categories": ["eess.SP"], "comment": "ION GNSS+ 2025 Conference", "summary": "The landscape of global navigation satellite systems (GNSS) is expanding with\nthe emergence of low Earth orbit (LEO) constellations such as Pulsar, which are\nexpected to play a key role in the future of positioning, navigation, and\ntiming (PNT). LEO-based systems provide advantages including stronger signals\nfor greater robustness, faster dynamics that aid convergence and multipath\nmitigation, and shorter time to first fix (TTFF) enabled by high data rates.\nThese benefits, however, come with changes in signal behavior and constellation\ngeometry that require careful consideration in receiver design. This paper\ninvestigates Pulsar properties using a GNSS simulator, analyzing parameters\nsuch as satellite pass duration, elevation, Doppler shift, Doppler rate, range,\nand number of satellites in view. Comparisons with GPS highlight the\ndifferences introduced by LEO operation. The analysis examines temporal\nevolution, statistical distributions, and maximum and minimum values. Beyond\nthese statistical insights, the study explores interdependencies between\nparameters and differences across satellites, providing additional perspective.\nEvaluations are performed at multiple latitudes to ensure a worldwide\nperspective, and the impact of applying different elevation masks is discussed\nwhere relevant. Building on these findings, the paper assesses Pulsar's impact\non receiver design from two standpoints: design considerations, addressing\nexpanded Doppler ranges, higher Doppler rates, and unique constellation\nstructure; and design optimizations, exploiting parameter analyses and\ninterdependencies (e.g., Doppler rate vs Doppler) to refine acquisition\nstrategies and applying prediction and prioritization techniques to avoid\nunnecessary computations. Together, these optimizations can reduce acquisition\ntime and lower receiver power consumption."}
{"id": "2509.19712", "pdf": "https://arxiv.org/pdf/2509.19712", "abs": "https://arxiv.org/abs/2509.19712", "authors": ["Liquan Wang", "Jiangjie Bian", "Eric Heiden", "Animesh Garg"], "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation tasks involving cutting deformable objects remain\nchallenging due to complex topological behaviors, difficulties in perceiving\ndense object states, and the lack of efficient evaluation methods for cutting\noutcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for\nmulti-step robotic cutting tasks that integrates a cutting environment and\ngeneralized policy learning. TopoCut is built upon three core components: (1)\nWe introduce a high-fidelity simulation environment based on a particle-based\nelastoplastic solver with compliant von Mises constitutive models, augmented by\na novel damage-driven topology discovery mechanism that enables accurate\ntracking of multiple cutting pieces. (2) We develop a comprehensive reward\ndesign that integrates the topology discovery with a pose-invariant spectral\nreward model based on Laplace-Beltrami eigenanalysis, facilitating consistent\nand robust assessment of cutting quality. (3) We propose an integrated policy\nlearning pipeline, where a dynamics-informed perception module predicts\ntopological evolution and produces particle-wise, topology-aware embeddings to\nsupport PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for\ngoal-conditioned policy learning. Extensive experiments demonstrate that\nTopoCut supports trajectory generation, scalable learning, precise evaluation,\nand strong generalization across diverse object geometries, scales, poses, and\ncutting goals."}
{"id": "2509.19594", "pdf": "https://arxiv.org/pdf/2509.19594", "abs": "https://arxiv.org/abs/2509.19594", "authors": ["Mohammadhossein Karimi", "Yuanzhe Gong", "Tho Le-Ngoc"], "title": "DNN-Based Nulling Control Beam Focusing for Near-Field Multi-User Interference Mitigation", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, submitted to IEEE WCNC 2026", "summary": "This paper proposes a deep learning-based framework for near-field nulling\ncontrol beam focusing (NCBF) in extra-large MIMO (XL-MIMO) systems to mitigate\nmulti-user interference (MUI). A dual-estimator architecture comprising two\nfully connected deep neural networks (FCDNNs) is developed to separately\npredict the phase and magnitude components of NCBF weights, using locations of\nboth desired and interfering users. The models are trained on a large dataset\ngenerated via a Linearly Constrained Minimum Variance (LCMV) beamforming\nalgorithm to accommodate diverse user configurations, including both collinear\nand non-collinear scenarios. Illustrative results demonstrate that the proposed\nDNN models achieve high prediction accuracy, with test errors of only 0.067\nradians for phase estimation and 0.206 dB for magnitude estimation. Full-wave\nsimulations incorporating realistic element radiation patterns and\ninter-element coupling confirm the close agreement between the beam patterns\nproduced by the DNN-predicted and LCMV-based NCBF schemes under practical\ndeployment conditions. An average MUI suppression of 36.7 dB is achieved, with\ninterference mitigation exceeding 17.5 dB across all tested cases. The proposed\napproach enables scalable and real-time beam focusing with effective\ninterference suppression, offering a promising solution for future near-field\nmulti-user wireless communications."}
{"id": "2509.19725", "pdf": "https://arxiv.org/pdf/2509.19725", "abs": "https://arxiv.org/abs/2509.19725", "authors": ["Naveed D. Riaziat", "Joseph Chen", "Axel Krieger", "Jeremy D. Brown"], "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging", "categories": ["cs.RO"], "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Electrosurgery is a surgical technique that can improve tissue cutting by\nreducing cutting force and bleeding. However, electrosurgery adds a risk of\nthermal injury to surrounding tissue. Expert surgeons estimate desirable\ncutting velocities based on experience but have no quantifiable reference to\nindicate if a particular velocity is optimal. Furthermore, prior demonstrations\nof autonomous electrosurgery have primarily used constant tool velocity, which\nis not robust to changes in electrosurgical tissue characteristics, power\nsettings, or tool type. Thermal imaging feedback provides information that can\nbe used to reduce thermal injury while balancing cutting force by controlling\ntool velocity. We introduce Thermography for Electrosurgical Rate Modulation\nvia Optimization (ThERMO) to autonomously reduce thermal injury while balancing\ncutting force by intelligently controlling tool velocity. We demonstrate ThERMO\nin tissue phantoms and compare its performance to the constant velocity\napproach. Overall, ThERMO improves cut success rate by a factor of three and\ncan reduce peak cutting force by a factor of two. ThERMO responds to varying\nenvironmental disturbances, reduces damage to tissue, and completes cutting\ntasks that would otherwise result in catastrophic failure for the constant\nvelocity approach."}
{"id": "2509.19686", "pdf": "https://arxiv.org/pdf/2509.19686", "abs": "https://arxiv.org/abs/2509.19686", "authors": ["Gabriel J. Griswold", "Mark A. Griswold"], "title": "Non-locally averaged pruned reassigned spectrograms: a tool for glottal pulse visualization and analysis", "categories": ["eess.SP", "cs.SD", "eess.AS"], "comment": "Submitted to Speech Communications. 16 pages, 7 figs, 1 table", "summary": "Reassigned spectrograms have shown advantages in precise formant measuring\nand inter-speaker differentiation. However, reassigned spectrograms suffer from\ntheir inability to visualize larger amounts of data in an easily comprehensible\nand reproducible manner. Utilizing the techniques and tools developed by Fulop\nand Fitz, a variation of the reassigned spectrogram is proposed. Non-locally\nAveraged Pruned Reassigned Spectrograms (NAPReS) provide a simplified view into\nthe characteristics of a speaker's glottal pulsation patterns throughout the\ncentroid of a vowel through the stacking, summing, and pruning of large numbers\nof glottal pulses. In this exploratory study, NAPReS has been shown to display\na large amount of data in an easily comprehensible and quantifiable manner,\nwhile also making the observation of low-amplitude cyclical structures more\naccessible. NAPReS also allows for alternative formant fitting methods such as\nGaussian mixture modeling. In this study, NAPReS with GMM was compared against\nconventional LPC fitting of formant values and was shown to be more\nreproducible than conventional LPC fitting in high-noise situations."}
{"id": "2509.19732", "pdf": "https://arxiv.org/pdf/2509.19732", "abs": "https://arxiv.org/abs/2509.19732", "authors": ["Kyo Kutsuzawa", "Mitsuhiro Hayashibe"], "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering", "categories": ["cs.RO"], "comment": "Accepted to The International Journal of Robotics Research (IJRR)", "summary": "Estimating the contact state between a grasped tool and the environment is\nessential for performing contact tasks such as assembly and object\nmanipulation. Force signals are valuable for estimating the contact state, as\nthey can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using\nforce/torque signals; however, most methods require the geometry of the tool\nsurface to be known. Although several studies have proposed methods that do not\nrequire the tool shape, these methods require considerable time for estimation\nor are limited to tools with low-dimensional shape parameters. Here, we propose\na method for simultaneously estimating the contact position and tool shape,\nwhere the tool shape is represented by a grid, which is high-dimensional (more\nthan 1000 dimensional). The proposed method uses a particle filter in which\neach particle has individual tool shape parameters, thereby to avoid directly\nhandling a high-dimensional parameter space. The proposed method is evaluated\nthrough simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool\nsimultaneously with the contact positions, making the contact-position\nestimation more accurate."}
{"id": "2509.19754", "pdf": "https://arxiv.org/pdf/2509.19754", "abs": "https://arxiv.org/abs/2509.19754", "authors": ["Xiaolei Yang", "Zijing Wang", "Zhijin Qin", "Xiaoming Tao"], "title": "Timeliness-Aware Joint Source and Channel Coding for Adaptive Image Transmission", "categories": ["eess.SP"], "comment": "6 pages, 7 figures, accepted at IEEE GLOBECOM Workshops 2025", "summary": "Accurate and timely image transmission is critical for emerging\ntime-sensitive applications such as remote sensing in satellite-assisted\nInternet of Things. However, the bandwidth limitation poses a significant\nchallenge in existing wireless systems, making it difficult to fulfill the\nrequirements of both high-fidelity and low-latency image transmission. Semantic\ncommunication is expected to break through the performance bottleneck by\nfocusing on the transmission of goal-oriented semantic information rather than\nraw data. In this paper, we employ a new timeliness metric named the value of\ninformation (VoI) and propose an adaptive joint source and channel coding\n(JSCC) method for image transmission that simultaneously considers both\nreconstruction quality and timeliness. Specifically, we first design a JSCC\nframework for image transmission with adaptive code length. Next, we formulate\na VoI maximization problem by optimizing the transmission code length of the\nadaptive JSCC under the reconstruction quality constraint. Then, a deep\nreinforcement learning-based algorithm is proposed to solve the optimization\nproblem efficiently. Experimental results show that the proposed method\nsignificantly outperforms baseline schemes in terms of reconstruction quality\nand timeliness, particularly in low signal-to-noise ratio conditions, offering\na promising solution for efficient and robust image transmission in\ntime-sensitive wireless networks."}
{"id": "2509.19734", "pdf": "https://arxiv.org/pdf/2509.19734", "abs": "https://arxiv.org/abs/2509.19734", "authors": ["Akshay Jaitly", "Jon Arrizabalaga", "Guanrui Li"], "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions", "categories": ["cs.RO"], "comment": null, "summary": "Planning collision free trajectories in complex environments remains a core\nchallenge in robotics. Existing corridor based planners which rely on\ndecomposition of the free space into collision free subsets scale poorly with\nenvironmental complexity and require explicit allocations of time windows to\ntrajectory segments. We introduce a new trajectory parameterization that\nrepresents trajectories in a nonconvex collision free corridor as being in a\nconvex cartesian product of balls. This parameterization allows us to decouple\nproblem size from geometric complexity of the solution and naturally avoids\nexplicit time allocation by allowing trajectories to evolve continuously inside\nellipsoidal corridors. Building on this representation, we formulate the\nOrthogonal Trust Region Problem (Orth-TRP), a specialized convex program with\nseparable block constraints, and develop a solver that exploits this parallel\nstructure and the unique structure of each parallel subproblem for efficient\noptimization. Experiments on a quadrotor trajectory planning benchmark show\nthat our approach produces smoother trajectories and lower runtimes than\nstate-of-the-art corridor based planners, especially in highly complicated\nenvironments."}
{"id": "2509.19801", "pdf": "https://arxiv.org/pdf/2509.19801", "abs": "https://arxiv.org/abs/2509.19801", "authors": ["Ioannis Gavras", "George C. Alexandropoulos"], "title": "Electromagnetics-Compliant Optimization of Dynamic Metasurface Antennas for Bistatic Sensing", "categories": ["eess.SP"], "comment": "13 pages, 7 figures", "summary": "Dynamic Metasurface Antennas (DMAs) are recently attracting considerable\nresearch interests due to their potential to enable low-cost, reconfigurable,\nand highly scalable antenna array architectures for next generation wireless\nsystems. However, most of the existing literature relies on idealized models\nfor the DMA operation, often overlooking critical structural and physical\nconstraints inherent to their constituent metamaterials. In this paper,\nleveraging a recently proposed model for this antenna architecture\nincorporating physically consistent modeling of mutual coupling and waveguide\npropagation losses, we optimize DMA-based transmission for bistatic sensing. A\ntractable approximation for the DMA response is first presented, which enables\nefficient optimization of the dynamically reconfigurable Lorentzian-constrained\nresponses of the array's metamaterials. In particular, we formulate a robust\nbeamforming optimization problem with the objective to minimize the worst-case\nposition error bound, in the presence of spatial uncertainties for the\nenvironment's scatterers as well as synchronization uncertainties at the analog\ncombining multi-antenna receiver. To address the resulting high computational\ncomplexity due to the possibly excessive number of metamaterial-based antennas\nand their operation constraints, two low complexity beamforming design\napproaches are presented that perform offline searching over a novel beam\ncodebook. The accuracy of all presented DMA designs is assessed by means of\nMonte Carlo simulations for various system parameters, confirming that\naccurately modeling mutual coupling is essential for maintaining increased\nlocalization performance. It is also shown that, even under positioning and\nsynchronization uncertainties, the proposed designs yield accuracy comparable\nto their fully digital and analog counterparts, while adhering to the\nstructural DMA constraints."}
{"id": "2509.19752", "pdf": "https://arxiv.org/pdf/2509.19752", "abs": "https://arxiv.org/abs/2509.19752", "authors": ["Rushuai Yang", "Hangxing Wei", "Ran Zhang", "Zhiyuan Feng", "Xiaoyu Chen", "Tong Li", "Chuheng Zhang", "Li Zhao", "Jiang Bian", "Xiu Su", "Yi Chen"], "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization across\ntasks and embodiments; however, their reliance on large-scale human\ndemonstrations limits their scalability owing to the cost and effort of manual\ndata collection. Reinforcement learning (RL) offers a potential alternative to\ngenerate demonstrations autonomously, yet conventional RL algorithms often\nstruggle on long-horizon manipulation tasks with sparse rewards. In this paper,\nwe propose a modified diffusion policy optimization algorithm to generate\nhigh-quality and low-variance trajectories, which contributes to a diffusion\nRL-powered VLA training pipeline. Our algorithm benefits from not only the high\nexpressiveness of diffusion models to explore complex and diverse behaviors but\nalso the implicit regularization of the iterative denoising process to yield\nsmooth and consistent demonstrations. We evaluate our approach on the LIBERO\nbenchmark, which includes 130 long-horizon manipulation tasks, and show that\nthe generated trajectories are smoother and more consistent than both human\ndemonstrations and those from standard Gaussian RL policies. Further, training\na VLA model exclusively on the diffusion RL-generated data achieves an average\nsuccess rate of 81.9%, which outperforms the model trained on human data by\n+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight\nour diffusion RL as an effective alternative for generating abundant,\nhigh-quality, and low-variance demonstrations for VLA models."}
{"id": "2509.19900", "pdf": "https://arxiv.org/pdf/2509.19900", "abs": "https://arxiv.org/abs/2509.19900", "authors": ["Xinjue Wang", "Esa Ollila", "Sergiy A. Vorobyov", "Ammar Mian"], "title": "Generalized Nonnegative Structured Kruskal Tensor Regression", "categories": ["eess.SP", "stat.AP", "stat.ML"], "comment": null, "summary": "This paper introduces Generalized Nonnegative Structured Kruskal Tensor\nRegression (NS-KTR), a novel tensor regression framework that enhances\ninterpretability and performance through mode-specific hybrid regularization\nand nonnegativity constraints. Our approach accommodates both linear and\nlogistic regression formulations for diverse response variables while\naddressing the structural heterogeneity inherent in multidimensional tensor\ndata. We integrate fused LASSO, total variation, and ridge regularizers, each\ntailored to specific tensor modes, and develop an efficient alternating\ndirection method of multipliers (ADMM) based algorithm for parameter\nestimation. Comprehensive experiments on synthetic signals and real\nhyperspectral datasets demonstrate that NS-KTR consistently outperforms\nconventional tensor regression methods. The framework's ability to preserve\ndistinct structural characteristics across tensor dimensions while ensuring\nphysical interpretability makes it especially suitable for applications in\nsignal processing and hyperspectral image analysis."}
{"id": "2509.19804", "pdf": "https://arxiv.org/pdf/2509.19804", "abs": "https://arxiv.org/abs/2509.19804", "authors": ["Sowoo Lee", "Dongyun Kang", "Jaehyun Park", "Hae-Won Park"], "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations", "categories": ["cs.RO"], "comment": "8 pages", "summary": "This paper introduces DynaFlow, a novel framework that embeds a\ndifferentiable simulator directly into a flow matching model. By generating\ntrajectories in the action space and mapping them to dynamically feasible state\ntrajectories via the simulator, DynaFlow ensures all outputs are physically\nconsistent by construction. This end-to-end differentiable architecture enables\ntraining on state-only demonstrations, allowing the model to simultaneously\ngenerate physically consistent state trajectories while inferring the\nunderlying action sequences required to produce them. We demonstrate the\neffectiveness of our approach through quantitative evaluations and showcase its\nreal-world applicability by deploying the generated actions onto a physical Go1\nquadruped robot. The robot successfully reproduces diverse gait present in the\ndataset, executes long-horizon motions in open-loop control and translates\ninfeasible kinematic demonstrations into dynamically executable, stylistic\nbehaviors. These hardware experiments validate that DynaFlow produces\ndeployable, highly effective motions on real-world hardware from state-only\ndemonstrations, effectively bridging the gap between kinematic data and\nreal-world execution."}
{"id": "2509.19912", "pdf": "https://arxiv.org/pdf/2509.19912", "abs": "https://arxiv.org/abs/2509.19912", "authors": ["Xingxiang Peng", "Qingqing Wu", "Ziyuan Zheng", "Wen Chen", "Yanze Zhu", "Ying Gao"], "title": "Rotatable Antenna Enabled Spectrum Sharing: Joint Antenna Orientation and Beamforming Design", "categories": ["eess.SP"], "comment": "13 pages, 10 figures. Submitted to an IEEE journal for possible\n  publication", "summary": "Conventional antenna arrays rely primarily on digital beamforming for spatial\ncontrol. While adding more elements can narrow beamwidth and suppress\ninterference, such scaling incurs prohibitive hardware and power costs.\nRotatable antennas (RAs), which allow mechanical or electronic adjustment of\nelement orientations, introduce a new degree of freedom to exploit spatial\nflexibility without enlarging the array. By dynamically optimizing\norientations, RAs can substantially improve desired link alignment and\ninterference suppression. This paper investigates RA-enabled multiple-input\nsingle-output (MISO) interference channels under co-channel spectrum sharing\nand formulates a weighted sum-rate maximization problem that jointly optimizes\ntransmit beamforming and antenna orientations. To tackle this nonconvex\nproblem, we develop an alternating optimization (AO) framework that integrates\nweighted minimum mean-square error (WMMSE)-based beamforming with\nFrank-Wolfe-based orientation updates. To reduce complexity, we further study\norientation optimization under maximum-ratio transmission (MRT) and\nzero-forcing (ZF) beamforming schemes. For finite-resolution actuators, we\nconstruct spherical Fibonacci codebooks and design a cross-entropy method\n(CEM)-based algorithm for discrete orientation selection. Simulations show that\nintegrating RAs with conventional beamforming markedly increases weighted\nsum-rate, with gains rising with element directivity. Under discrete\norientation control, the proposed CEM algorithm consistently outperforms the\nnearest-projection baseline."}
{"id": "2509.19851", "pdf": "https://arxiv.org/pdf/2509.19851", "abs": "https://arxiv.org/abs/2509.19851", "authors": ["Benjamin Bogenberger", "Oliver Harrison", "Orrin Dahanaggamaarachchi", "Lukas Brunke", "Jingxing Qian", "Siqi Zhou", "Angela P. Schoellig"], "title": "Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments", "categories": ["cs.RO"], "comment": null, "summary": "Robots deployed in real-world environments, such as homes, must not only\nnavigate safely but also understand their surroundings and adapt to environment\nchanges. To perform tasks efficiently, they must build and maintain a semantic\nmap that accurately reflects the current state of the environment. Existing\nresearch on semantic exploration largely focuses on static scenes without\npersistent object-level instance tracking. A consistent map is, however,\ncrucial for real-world robotic applications where objects in the environment\ncan be removed, reintroduced, or shifted over time. In this work, to close this\ngap, we propose an open-vocabulary, semantic exploration system for semi-static\nenvironments. Our system maintains a consistent map by building a probabilistic\nmodel of object instance stationarity, systematically tracking semi-static\nchanges, and actively exploring areas that have not been visited for a\nprolonged period of time. In addition to active map maintenance, our approach\nleverages the map's semantic richness with LLM-based reasoning for\nopen-vocabulary object-goal navigation. This enables the robot to search more\nefficiently by prioritizing contextually relevant areas. We evaluate our\napproach across multiple real-world semi-static environments. Our system\ndetects 95% of map changes on average, improving efficiency by more than 29% as\ncompared to random and patrol baselines. Overall, our approach achieves a\nmapping precision within 2% of a fully rebuilt map while requiring\nsubstantially less exploration and further completes object goal navigation\ntasks about 14% faster than the next-best tested strategy (coverage\npatrolling). A video of our work can be found at\nhttp://tiny.cc/sem-explor-semi-static ."}
{"id": "2509.19974", "pdf": "https://arxiv.org/pdf/2509.19974", "abs": "https://arxiv.org/abs/2509.19974", "authors": ["Natsuki Ueno", "Ryotaro Sato", "Nobutaka Ono"], "title": "On the Invariance of Cross-Correlation Peak Positions Under Monotonic Signal Transformations, with Application to Fast Time Difference Estimation", "categories": ["eess.SP", "eess.AS"], "comment": null, "summary": "We present a theorem concerning the invariance of cross-correlation peak\npositions, which provides a foundation for a new method for time difference\nestimation that is potentially faster than the conventional fast Fourier\ntransform (FFT) approach for real/complex sequences. This theoretical result\nshows that the peak position of the cross-correlation function between two\nshifted discrete-time signals remains unchanged under arbitrary monotonic\ntransformations of the input signals. By exploiting this property, we design an\nefficient estimation algorithm based on the cross-correlation function between\nsignals quantized into low-bit integers. The proposed method requires only\ninteger arithmetic instead of real-valued operations, and further computational\nefficiency can be achieved through number-theoretic algorithms. Numerical\nexperiments demonstrate that the proposed method achieves a shorter processing\ntime than conventional FFT-based approaches."}
{"id": "2509.19853", "pdf": "https://arxiv.org/pdf/2509.19853", "abs": "https://arxiv.org/abs/2509.19853", "authors": ["BinXu Wu", "TengFei Zhang", "Chen Yang", "JiaHao Wen", "HaoCheng Li", "JingTian Ma", "Zhen Chen", "JingYuan Wang"], "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process", "categories": ["cs.RO"], "comment": null, "summary": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and\ncrucial in robotics. They often involve state ambiguity, where visually similar\nobservations correspond to different actions. We present SAGE, a state-aware\nguided imitation learning framework that models tasks as a Hidden Markov\nDecision Process (HMDP) to explicitly capture latent task stages and resolve\nambiguity. We instantiate the HMDP with a state transition network that infers\nhidden states, and a state-aware action policy that conditions on both\nobservations and hidden states to produce actions, thereby enabling\ndisambiguation across task stages. To reduce manual annotation effort, we\npropose a semi-automatic labeling pipeline combining active learning and soft\nlabel interpolation. In real-world experiments across multiple complex MSS\ntasks with state ambiguity, SAGE achieved 100% task success under the standard\nevaluation protocol, markedly surpassing the baselines. Ablation studies\nfurther show that such performance can be maintained with manual labeling for\nonly about 13% of the states, indicating its strong effectiveness."}
{"id": "2509.20026", "pdf": "https://arxiv.org/pdf/2509.20026", "abs": "https://arxiv.org/abs/2509.20026", "authors": ["Jiayi Lu", "Jiayi Zhang", "Hao Lei", "Huahua Xiao", "Bo Ai", "Derrick Wing Kwan Ng"], "title": "Near-field Spatial-domain Channel Extrapolation for XL-MIMO Systems", "categories": ["eess.SP"], "comment": "This paper has been accepted by IEEE Transactions on Wireless\n  Communications", "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are\npivotal to next-generation wireless communications, where dynamic RF chain\narchitectures offer enhanced performance. However, efficient precoding in such\nsystems requires accurate channel state information (CSI) obtained with low\ncomplexity. To address this challenge, spatial-domain channel extrapolation has\nattracted growing interest. Existing methods often overlook near-field\nspherical wavefronts or rely heavily on sparsity priors, leading to performance\ndegradation. In this paper, we propose an adaptive near-field channel\nextrapolation framework for multi-subcarrier XL-MIMO systems, leveraging a\nstrategically selected subset of antennas. Subsequently, we develop both\non-grid and off-grid algorithms, where the latter refines the former's\nestimates for improved accuracy. To further reduce complexity, a\ncross-validation (CV)-based scheme is introduced. Additionally, we analytically\nformulate the mutual coherence of the sensing matrix and propose a\ncoherence-minimizing-based random pattern to ensure robust extrapolation.\nNumerical results validate that the proposed algorithms significantly\noutperform existing methods in both extrapolation accuracy and achievable rate,\nwhile maintaining low computational complexity. In particular, our proposed CV\nratio offers a flexible trade-off between accuracy and efficiency, and the\ncorresponding off-grid algorithm achieves high accuracy with complexity\ncomparable to conventional on-grid methods."}
{"id": "2509.19892", "pdf": "https://arxiv.org/pdf/2509.19892", "abs": "https://arxiv.org/abs/2509.19892", "authors": ["Keyu Wang", "Bingcong Lu", "Zhengxue Cheng", "Hengdi Zhang", "Li Song"], "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects", "categories": ["cs.RO"], "comment": null, "summary": "Achieving diverse and stable dexterous grasping for general and deformable\nobjects remains a fundamental challenge in robotics, due to high-dimensional\naction spaces and uncertainty in perception. In this paper, we present D3Grasp,\na multimodal perception-guided reinforcement learning framework designed to\nenable Diverse and Deformable Dexterous Grasping. We firstly introduce a\nunified multimodal representation that integrates visual and tactile perception\nto robustly grasp common objects with diverse properties. Second, we propose an\nasymmetric reinforcement learning architecture that exploits privileged\ninformation during training while preserving deployment realism, enhancing both\ngeneralization and sample efficiency. Third, we meticulously design a training\nstrategy to synthesize contact-rich, penetration-free, and kinematically\nfeasible grasps with enhanced adaptability to deformable and contact-sensitive\nobjects. Extensive evaluations confirm that D3Grasp delivers highly robust\nperformance across large-scale and diverse object categories, and substantially\nadvances the state of the art in dexterous grasping for deformable and\ncompliant objects, even under perceptual uncertainty and real-world\ndisturbances. D3Grasp achieves an average success rate of 95.1% in real-world\ntrials,outperforming prior methods on both rigid and deformable objects\nbenchmarks."}
{"id": "2509.20030", "pdf": "https://arxiv.org/pdf/2509.20030", "abs": "https://arxiv.org/abs/2509.20030", "authors": ["Renzhi Yuan", "Zhixing Wang", "Shouye Miao", "Mufei Zhao", "Haifeng Yao", "Bin Cao", "Mugen Peng"], "title": "Multi-Stage CD-Kennedy Receiver for QPSK Modulated CV-QKD in Turbulent Channels", "categories": ["eess.SP"], "comment": "25pages,7 figures", "summary": "Continuous variable-quantum key distribution (CV-QKD) protocols attract\nincreasing attentions in recent years because they enjoy high secret key rate\n(SKR) and good compatibility with existing optical communication\ninfrastructure. Classical coherent receivers are widely employed in coherent\nstates based CV-QKD protocols, whose detection performance is bounded by the\nstandard quantum limit (SQL). Recently, quantum receivers based on displacement\noperators are experimentally demonstrated with detection performance\noutperforming the SQL in various practical conditions. However, potential\napplications of quantum receivers in CV-QKD protocols under turbulent channels\nare still not well explored, while practical CV-QKD protocols must survive from\nthe atmospheric turbulence in satellite-to-ground optical communication links.\nIn this paper, we consider the possibility of using a quantum receiver called\nmulti-stage CD-Kennedy receiver to enhance the SKR performance of a quadrature\nphase shift keying (QPSK) modulated CV-QKD protocol in turbulent channels. We\nfirst derive the error probability of the multi-stage CD-Kennedy receiver for\ndetecting QPSK signals in turbulent channels and further propose three types of\nmulti-stage CD-Kennedy receiver with different displacement choices, i.e., the\nType-I, Type-II, and Type-III receivers. Then we derive the SKR of a QPSK\nmodulated CV-QKD protocol using the multi-stage CD-Kennedy receiver and\npost-selection strategy in turbulent channels. Numerical results show that the\nmulti-stage CD-Kennedy receiver can outperform the classical coherent receiver\nin turbulent channels in terms of both error probability and SKR performance\nand the Type-II receiver can tolerate worse channel conditions compared with\nType-I and Type-III receivers in terms of error probability performance."}
{"id": "2509.19916", "pdf": "https://arxiv.org/pdf/2509.19916", "abs": "https://arxiv.org/abs/2509.19916", "authors": ["Zijun Che", "Yinghong Zhang", "Shengyi Liang", "Boyu Zhou", "Jun Ma", "Jinni Zhou"], "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous exploration in structured and complex indoor environments remains\na challenging task, as existing methods often struggle to appropriately model\nunobserved space and plan globally efficient paths. To address these\nlimitations, we propose GUIDE, a novel exploration framework that\nsynergistically combines global graph inference with diffusion-based\ndecision-making. We introduce a region-evaluation global graph representation\nthat integrates both observed environmental data and predictions of unexplored\nareas, enhanced by a region-level evaluation mechanism to prioritize reliable\nstructural inferences while discounting uncertain predictions. Building upon\nthis enriched representation, a diffusion policy network generates stable,\nforesighted action sequences with significantly reduced denoising steps.\nExtensive simulations and real-world deployments demonstrate that GUIDE\nconsistently outperforms state-of-the-art methods, achieving up to 18.3% faster\ncoverage completion and a 34.9% reduction in redundant movements."}
{"id": "2509.20034", "pdf": "https://arxiv.org/pdf/2509.20034", "abs": "https://arxiv.org/abs/2509.20034", "authors": ["Etienne Lasalle", "Barbara Pascal"], "title": "Reproduction Number and Spatial Connectivity Structure Estimation via Graph Sparsity-Promoting Penalized Functional", "categories": ["eess.SP", "math.OC", "stat.AP"], "comment": "11 pages, 3 figures", "summary": "During an epidemic outbreak, decision makers crucially need accurate and\nrobust tools to monitor the pathogen propagation. The effective reproduction\nnumber, defined as the expected number of secondary infections stemming from\none contaminated individual, is a state-of-the-art indicator quantifying the\nepidemic intensity. Numerous estimators have been developed to precisely track\nthe reproduction number temporal evolution. Yet, COVID-19 pandemic surveillance\nraised unprecedented challenges due to the poor quality of worldwide reported\ninfection counts. When monitoring the epidemic in different territories\nsimultaneously, leveraging the spatial structure of data significantly enhances\nboth the accuracy and robustness of reproduction number estimates. However,\nthis requires a good estimate of the spatial structure. To tackle this major\nlimitation, the present work proposes a joint estimator of the reproduction\nnumber and connectivity structure. The procedure is assessed through intensive\nnumerical simulations on carefully designed synthetic data and illustrated on\nreal COVID-19 spatiotemporal infection counts."}
{"id": "2509.19954", "pdf": "https://arxiv.org/pdf/2509.19954", "abs": "https://arxiv.org/abs/2509.19954", "authors": ["Pinhao Song", "Yurui Du", "Ophelie Saussus", "Sofie De Schrijver", "Irene Caprara", "Peter Janssen", "Renaud Detry"], "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation", "categories": ["cs.RO"], "comment": "26 pages, 20 figures", "summary": "We propose a probabilistic shared-control solution for navigation, called\nRobot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,\neffective assistance in human-robot interaction. RT-V2 jointly models a user's\nlong-term behavioral patterns and their noisy, low-dimensional control signals\nby combining a prior intent model with a posterior update that accounts for\nreal-time user input and environmental context. The prior captures the\nmultimodal and history-dependent nature of user intent using recurrent neural\nnetworks and conditional variational autoencoders, while the posterior\nintegrates this with uncertain user commands to infer desired actions. We\nconduct extensive experiments to validate RT-V2 across synthetic benchmarks,\nhuman-computer interaction studies with keyboard input, and brain-machine\ninterface experiments with non-human primates. Results show that RT-V2\noutperforms the state of the art in intent estimation, provides safe and\nefficient navigation support, and adequately balances user autonomy with\nassistive intervention. By unifying probabilistic modeling, reinforcement\nlearning, and safe optimization, RT-V2 offers a principled and generalizable\napproach to shared control for diverse assistive technologies."}
{"id": "2509.20046", "pdf": "https://arxiv.org/pdf/2509.20046", "abs": "https://arxiv.org/abs/2509.20046", "authors": ["Knut H. Grythe", "Jan Erik Håkegård"], "title": "A dual bistatic optical forward transceiver configuration for determining the position of an acoustic communication source detected by optical communication fibers", "categories": ["eess.SP"], "comment": "18 pages, 17 figures", "summary": "Optical fibers have long been employed as sensors in a wide range of\ncommercial systems. Distributed Acoustic Sensing (DAS) extends this concept by\nenabling the detection and localization of acoustic sources along the fiber,\nusing backscattered light from small segments to achieve spatial resolution on\nthe order of meters. Recently, DAS has also been explored as a component in\nunderwater acoustic communication systems. Emerging interest in bidirectional\nconfigurations where both transmitter and receiver are placed at opposite ends\nof the fiber has opened new possibilities. However, in such setups, source\nlocalization is not inherently integrated into the signal decoding process. For\nscenarios where source positioning is valuable, we propose an approach inspired\nby bi-static radar principles. This configuration utilizes acoustic signals\nreceived at both ends of the fiber to estimate source position based on\npropagation delay differences. Although the localization accuracy is lower than\nthat of DAS due to reduced sampling rates, the method offers a viable\nalternative for integrated communication and positioning. We present the system\ntopology and configuration for a dual-fiber layout, each end equipped with\noptical transmitters and receivers. The position estimation is derived from the\ntime difference of arrival (TDOA) between the two receivers. The Cram\\'er-Rao\nBound is derived to characterize the theoretical limits of localization\naccuracy, highlighting dependencies on system parameters such as optical power\nloss. Our analysis shows that increased acoustic bandwidth and higher carrier\nfrequencies enhance spatial resolution. We formulate the Cross Ambiguity\nFunction as a maximum likelihood estimator for TDOA and provide simulation\nresults illustrating its performance under varying system conditions. Finally,\nwe discuss key challenges that must be addressed for practical implementation."}
{"id": "2509.19958", "pdf": "https://arxiv.org/pdf/2509.19958", "abs": "https://arxiv.org/abs/2509.19958", "authors": ["Alexander Spiridonov", "Jan-Nico Zaech", "Nikolay Nikolov", "Luc Van Gool", "Danda Pani Paudel"], "title": "Generalist Robot Manipulation beyond Action Labeled Data", "categories": ["cs.RO"], "comment": "Accepted at Conference on Robot Learning 2025", "summary": "Recent advances in generalist robot manipulation leverage pre-trained\nVision-Language Models (VLMs) and large-scale robot demonstrations to tackle\ndiverse tasks in a zero-shot manner. A key challenge remains: scaling\nhigh-quality, action-labeled robot demonstration data, which existing methods\nrely on for robustness and generalization. To address this, we propose a method\nthat benefits from videos without action labels - featuring humans and/or\nrobots in action - enhancing open-vocabulary performance and enabling\ndata-efficient learning of new tasks. Our method extracts dense, dynamic 3D\npoint clouds at the hand or gripper location and uses a proposed 3D dynamics\npredictor for self-supervision. This predictor is then tuned to an action\npredictor using a smaller labeled dataset for action alignment. We show that\nour method not only learns from unlabeled human and robot demonstrations -\nimproving downstream generalist robot policies - but also enables robots to\nlearn new tasks without action labels (i.e., out-of-action generalization) in\nboth real-world and simulated settings."}
{"id": "2509.20059", "pdf": "https://arxiv.org/pdf/2509.20059", "abs": "https://arxiv.org/abs/2509.20059", "authors": ["Koki Kanzaki", "Koya Sato"], "title": "Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors", "categories": ["eess.SP", "cs.NI"], "comment": "6 pages, 8 figures. Accepted for presentation at 2025 IEEE GLOBECOM\n  Workshops: Workshop on The Interplay of Digital Twins and Pervasive\n  Artificial Intelligence for Next-Generation IoT", "summary": "This paper proposes a high-accuracy radio map construction method tailored\nfor environments where location information is affected by bursty errors. Radio\nmaps are an effective tool for visualizing wireless environments. Although\nextensive research has been conducted on accurate radio map construction, most\nexisting approaches assume noise-free location information during sensing. In\npractice, however, positioning errors ranging from a few to several tens of\nmeters can arise due to device-based positioning systems (e.g., GNSS). Ignoring\nsuch errors during inference can lead to significant degradation in radio map\naccuracy. This study highlights that these errors often tend to be biased when\nusing mobile devices as sensors. We introduce a novel framework that models\nthese errors together with spatial correlation in radio propagation by\nembedding them as tunable parameters in the marginal log-likelihood function.\nThis enables ex-post calibration of location uncertainty during radio map\nconstruction. Numerical results based on practical human mobility data\ndemonstrate that the proposed method can limit RMSE degradation to\napproximately 0.25-0.29 dB, compared with Gaussian process regression using\nnoise-free location data, whereas baseline methods suffer performance losses\nexceeding 1 dB."}
{"id": "2509.19972", "pdf": "https://arxiv.org/pdf/2509.19972", "abs": "https://arxiv.org/abs/2509.19972", "authors": ["Albina Klepach", "Egor E. Nuzhin", "Alexey A. Tsukanov", "Nikolay V. Brilliantov"], "title": "An effective control of large systems of active particles: An application to evacuation problem", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Manipulation of large systems of active particles is a serious challenge\nacross diverse domains, including crowd management, control of robotic swarms,\nand coordinated material transport. The development of advanced control\nstrategies for complex scenarios is hindered, however, by the lack of\nscalability and robustness of the existing methods, in particular, due to the\nneed of an individual control for each agent. One possible solution involves\ncontrolling a system through a leader or a group of leaders, which other agents\ntend to follow. Using such an approach we develop an effective control strategy\nfor a leader, combining reinforcement learning (RL) with artificial forces\nacting on the system. To describe the guidance of active particles by a leader\nwe introduce the generalized Vicsek model. This novel method is then applied to\nthe problem of the effective evacuation by a robot-rescuer (leader) of large\ngroups of people from hazardous places. We demonstrate, that while a\nstraightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation\nstrategy. The source code supporting this study is publicly available at:\nhttps://github.com/cinemere/evacuation."}
{"id": "2509.20246", "pdf": "https://arxiv.org/pdf/2509.20246", "abs": "https://arxiv.org/abs/2509.20246", "authors": ["Marko Fidanovski", "Iván Alexander Morales Sandoval", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "Emil Björnson"], "title": "Reciprocal Beyond-Diagonal Reconfigurable Intelligent Surface (BD-RIS): Scattering Matrix Design via Manifold Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) are emerging as\na transformative technology in wireless communications, enabling enhanced\nperformance and quality of service (QoS) of wireless systems in harsh urban\nenvironments due to their relatively low cost and advanced signal processing\ncapabilities. Generally, BD-RIS systems are employed to improve robustness,\nincrease achievable rates, and enhance energy efficiency of wireless systems in\nboth direct and indirect ways. The direct way is to produce a favorable\npropagation environment via the design of optimized scattering matrices, while\nthe indirect way is to reap additional improvements via the design of\nmultiple-input multiple-output (MIMO) beamformers that further exploit the\nlatter \"engineered\" medium. In this article, the problem of sum-rate\nmaximization via BD-RIS is examined, with a focus on feasibility, namely\nlow-complexity physical implementation, by enforcing reciprocity in the BD-RIS\ndesign. We begin by outlining the system model and formulating an optimization\nproblem that aims to enhance the system's sum-rate by designing a symmetric\nscattering matrix. In particular, the approach leverages a manifold\noptimization framework, where a penalty term is added to the objective function\nto ensure that the symmetry constraint is upheld, with reciprocity further\nenforced by projecting the obtained solution onto a set of feasible scattering\nmatrices. Simulation results demonstrate the effectiveness of the proposed\nmethod in outperforming current state-of-the-art (SotA) approaches in terms of\nsum-rate maximization."}
{"id": "2509.20009", "pdf": "https://arxiv.org/pdf/2509.20009", "abs": "https://arxiv.org/abs/2509.20009", "authors": ["Simon Schäfer", "Bassam Alrifaee", "Ehsan Hashemi"], "title": "Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure", "categories": ["cs.RO"], "comment": "21 pages, 9 figures, this work was submitted to Wileys'Advanced\n  Intelligent Systems for review", "summary": "This paper presents a lidar-only state estimation and tracking framework,\nalong with a roadside sensing unit for integration with existing urban\ninfrastructure. Urban deployments demand scalable, real-time tracking\nsolutions, yet traditional remote sensing remains costly and computationally\nintensive, especially under perceptually degraded conditions. Our sensor node\ncouples a single lidar with an edge computing unit and runs a computationally\nefficient, GPU-free observer that simultaneously estimates object state, class,\ndimensions, and existence probability. The pipeline performs: (i) state updates\nvia an extended Kalman filter, (ii) dimension estimation using a 1D\ngrid-map/Bayesian update, (iii) class updates via a lookup table driven by the\nmost probable footprint, and (iv) existence estimation from track age and\nbounding-box consistency. Experiments in dynamic urban-like scenes with diverse\ntraffic participants demonstrate real-time performance and high precision: The\ncomplete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for\n\\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is\nfurther confirmed under simulated wind and sensor vibration. These results\nindicate that reliable, real-time roadside tracking is feasible on CPU-only\nedge hardware, enabling scalable, privacy-friendly deployments within existing\ncity infrastructure. The framework integrates with existing poles, traffic\nlights, and buildings, reducing deployment costs and simplifying large-scale\nurban rollouts and maintenance efforts."}
{"id": "2509.20299", "pdf": "https://arxiv.org/pdf/2509.20299", "abs": "https://arxiv.org/abs/2509.20299", "authors": ["Chenguang Rao", "Kai-Kit Wong", "Mohd Hamza Naim Shaikh", "Hanjiang Hong", "Hyundong Shin", "Yangyang Zhang"], "title": "Geometric Port Selection in CUMA Systems", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Compact ultra-massive antenna-array (CUMA) is a novel multiple access\ntechnology built on the fluid antenna system (FAS) concept, offering an\nimproved scheme over fluid antenna multiple access (FAMA) that can support\nmassive connectivity on the same physical channel without the need of precoding\nand interference cancellation. By employing a simple port-selection mechanism\nthat leverages random channel superposition, CUMA can suppress inter-user\ninterference while keeping hardware costs low. Nevertheless, its ad-hoc\nport-selection strategy leaves considerable room for optimization. In this\nwork, we revisit CUMA and propose two adaptive single-RF port-selection schemes\nthat retain its simplicity while significantly enhancing performance. The first\none, referred to as exact optimal half-space (EOHS), dynamically selects the\nprojection direction that maximizes the instantaneous signal build-up across\nactive ports. To reduce complexity while preserving most of the gains, we\nfurthermore introduce a principal component analysis (PCA)-based scheme, which\naligns port partitioning with the dominant statistical direction of per-port\nchannel vectors. This method yields a closed-form low-complexity solution,\ncomplemented by a tractable analytical framework that provides a closed-form\nexpression for the signal-to-interference ratio (SIR) probability density\nfunction (PDF). Simulation results corroborate the analysis, demonstrating that\nboth EOHS and PCA consistently outperform conventional CUMA across diverse user\ndensities, port counts, and FAS aperture sizes. Notably, PCA achieves\nperformance close to EOHS at a fraction of the computational cost. The proposed\nschemes scale effectively to large-user regimes, offering a compelling\ncomplexity-performance trade-off for next-generation multiple access systems."}
{"id": "2509.20036", "pdf": "https://arxiv.org/pdf/2509.20036", "abs": "https://arxiv.org/abs/2509.20036", "authors": ["Yinzhao Dong", "Ji Ma", "Liu Zhao", "Wanyue Li", "Peng Lu"], "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have\ndemonstrated impressive performance on challenging terrains, allowing robots to\nexecute complex skills such as climbing, running, and jumping. However,\nexisting blind locomotion controllers often struggle to ensure safety and\nefficient traversal through risky gap terrains, which are typically highly\ncomplex, requiring robots to perceive terrain information and select\nappropriate footholds during locomotion accurately. Meanwhile, existing\nperception-based controllers still present several practical limitations,\nincluding a complex multi-sensor deployment system and expensive computing\nresource requirements. This paper proposes a DRL controller named MAstering\nRisky Gap Terrains (MARG), which integrates terrain maps and proprioception to\ndynamically adjust the action and enhance the robot's stability in these tasks.\nDuring the training phase, our controller accelerates policy optimization by\nselectively incorporating privileged information (e.g., center of mass,\nfriction coefficients) that are available in simulation but unmeasurable\ndirectly in real-world deployments due to sensor limitations. We also designed\nthree foot-related rewards to encourage the robot to explore safe footholds.\nMore importantly, a terrain map generation (TMG) model is proposed to reduce\nthe drift existing in mapping and provide accurate terrain maps using only one\nLiDAR, providing a foundation for zero-shot transfer of the learned policy. The\nexperimental results indicate that MARG maintains stability in various risky\nterrain tasks."}
{"id": "2509.19305", "pdf": "https://arxiv.org/pdf/2509.19305", "abs": "https://arxiv.org/abs/2509.19305", "authors": ["Yifu Luo", "Yongzhe Chang", "Xueqian Wang"], "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Diffusion probability models have shown significant promise in offline\nreinforcement learning by directly modeling trajectory sequences. However,\nexisting approaches primarily focus on time-domain features while overlooking\nfrequency-domain features, leading to frequency shift and degraded performance\naccording to our observation. In this paper, we investigate the RL problem from\na new perspective of the frequency domain. We first observe that\ntime-domain-only approaches inadvertently introduce shifts in the low-frequency\ncomponents of the frequency domain, which results in trajectory instability and\ndegraded performance. To address this issue, we propose Wavelet Fourier\nDiffuser (WFDiffuser), a novel diffusion-based RL framework that integrates\nDiscrete Wavelet Transform to decompose trajectories into low- and\nhigh-frequency components. To further enhance diffusion modeling for each\ncomponent, WFDiffuser employs Short-Time Fourier Transform and cross attention\nmechanisms to extract frequency-domain features and facilitate cross-frequency\ninteraction. Extensive experiment results on the D4RL benchmark demonstrate\nthat WFDiffuser effectively mitigates frequency shift, leading to smoother,\nmore stable trajectories and improved decision-making performance over existing\nmethods."}
{"id": "2509.20070", "pdf": "https://arxiv.org/pdf/2509.20070", "abs": "https://arxiv.org/abs/2509.20070", "authors": ["Abraham George", "Amir Barati Farimani"], "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs", "categories": ["cs.RO"], "comment": "9 pages, 5 figures, 4 tables. Submitted to ICRA 2026", "summary": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer"}
{"id": "2509.19378", "pdf": "https://arxiv.org/pdf/2509.19378", "abs": "https://arxiv.org/abs/2509.19378", "authors": ["Nelson Alves Ferreira Neto"], "title": "Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning", "categories": ["cs.CV", "cs.AR", "cs.LG", "eess.IV", "eess.SP"], "comment": "2022. 117p. Electrical Engineering PhD Thesis - Graduate Program in\n  Electrical and Computer Engineering, Federal University of Bahia, 40210-630,\n  Salvador, Brazil", "summary": "Low-latency intelligent systems are required for autonomous driving on\nnon-uniform terrain in open-pit mines and developing countries. This work\nproposes a perception system for autonomous vehicles on unpaved roads and\noff-road environments, capable of navigating rough terrain without a predefined\ntrail. The Configurable Modular Segmentation Network (CMSNet) framework is\nproposed, facilitating different architectural arrangements. CMSNet\nconfigurations were trained to segment obstacles and trafficable ground on new\nimages from unpaved/off-road scenarios with adverse conditions (night, rain,\ndust). We investigated applying deep learning to detect drivable regions\nwithout explicit track boundaries, studied algorithm behavior under visibility\nimpairment, and evaluated field tests with real-time semantic segmentation. A\nnew dataset, Kamino, is presented with almost 12,000 images from an operating\nvehicle with eight synchronized cameras. The Kamino dataset has a high number\nof labeled pixels compared to similar public collections and includes images\nfrom an off-road proving ground emulating a mine under adverse visibility. To\nachieve real-time inference, CMSNet CNN layers were methodically removed and\nfused using TensorRT, C++, and CUDA. Empirical experiments on two datasets\nvalidated the proposed system's effectiveness."}
{"id": "2509.20077", "pdf": "https://arxiv.org/pdf/2509.20077", "abs": "https://arxiv.org/abs/2509.20077", "authors": ["Xun Li", "Rodrigo Santa Cruz", "Mingze Xi", "Hu Zhang", "Madhawa Perera", "Ziwei Wang", "Ahalya Ravendran", "Brandon J. Matthews", "Feng Xu", "Matt Adcock", "Dadong Wang", "Jiajun Liu"], "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": null, "summary": "To enable robots to comprehend high-level human instructions and perform\ncomplex tasks, a key challenge lies in achieving comprehensive scene\nunderstanding: interpreting and interacting with the 3D environment in a\nmeaningful way. This requires a smart map that fuses accurate geometric\nstructure with rich, human-understandable semantics. To address this, we\nintroduce the 3D Queryable Scene Representation (3D QSR), a novel framework\nbuilt on multimedia data that unifies three complementary 3D representations:\n(1) 3D-consistent novel view rendering and segmentation from panoptic\nreconstruction, (2) precise geometry from 3D point clouds, and (3) structured,\nscalable organization via 3D scene graphs. Built on an object-centric design,\nthe framework integrates with large vision-language models to enable semantic\nqueryability by linking multimodal object embeddings, and supporting\nobject-level retrieval of geometric, visual, and semantic information. The\nretrieved data are then loaded into a robotic task planner for downstream\nexecution. We evaluate our approach through simulated robotic task planning\nscenarios in Unity, guided by abstract language instructions and using the\nindoor public dataset Replica. Furthermore, we apply it in a digital duplicate\nof a real wet lab environment to test QSR-supported robotic task planning for\nemergency response. The results demonstrate the framework's ability to\nfacilitate scene understanding and integrate spatial and semantic reasoning,\neffectively translating high-level human instructions into precise robotic task\nplanning in complex 3D environments."}
{"id": "2509.19774", "pdf": "https://arxiv.org/pdf/2509.19774", "abs": "https://arxiv.org/abs/2509.19774", "authors": ["Xiaocheng Fang", "Jiarui Jin", "Haoyu Wang", "Che Liu", "Jieyi Cai", "Guangkun Nie", "Jun Li", "Hongyan Li", "Shenda Hong"], "title": "PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "In clinical practice, electrocardiography (ECG) remains the gold standard for\ncardiac monitoring, providing crucial insights for diagnosing a wide range of\ncardiovascular diseases (CVDs). However, its reliance on specialized equipment\nand trained personnel limits feasibility for continuous routine monitoring.\nPhotoplethysmography (PPG) offers accessible, continuous monitoring but lacks\ndefinitive electrophysiological information, preventing conclusive diagnosis.\nGenerative models present a promising approach to translate PPG into clinically\nvaluable ECG signals, yet current methods face substantial challenges,\nincluding the misalignment of physiological semantics in generative models and\nthe complexity of modeling in high-dimensional signals. To this end, we propose\nPPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent\nspace via the CardioAlign Encoder and employs latent rectified flow to generate\nECGs with high fidelity and interpretability. To the best of our knowledge,\nthis is the first study to experiment on MCMED, a newly released clinical-grade\ndataset comprising over 10 million paired PPG-ECG samples from more than\n118,000 emergency department visits with expert-labeled cardiovascular disease\nannotations. Results demonstrate the effectiveness of our method for PPG-to-ECG\ntranslation and cardiovascular disease detection. Moreover, cardiologist-led\nevaluations confirm that the synthesized ECGs achieve high fidelity and improve\ndiagnostic reliability, underscoring our method's potential for real-world\ncardiovascular screening."}
{"id": "2509.20081", "pdf": "https://arxiv.org/pdf/2509.20081", "abs": "https://arxiv.org/abs/2509.20081", "authors": ["Jose E. Maese", "Luis Merino", "Fernando Caballero"], "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a high-efficiency, CPU-only volumetric mapping framework\nbased on a Truncated Signed Distance Field (TSDF). The system incrementally\nfuses raw LiDAR point-cloud data into a voxel grid using a directional\nbitmask-based integration scheme, producing dense and consistent TSDF\nrepresentations suitable for real-time 3D reconstruction. A key feature of the\napproach is that the processing time per point-cloud remains constant,\nregardless of the voxel grid resolution, enabling high resolution mapping\nwithout sacrificing runtime performance. In contrast to most recent TSDF/ESDF\nmethods that rely on GPU acceleration, our method operates entirely on CPU,\nachieving competitive results in speed. Experiments on real-world open datasets\ndemonstrate that the generated maps attain accuracy on par with contemporary\nmapping techniques."}
{"id": "2509.19886", "pdf": "https://arxiv.org/pdf/2509.19886", "abs": "https://arxiv.org/abs/2509.19886", "authors": ["Natsuki Akaishi", "Koki Yamada", "Kohei Yatabe"], "title": "Sparse Regularization by Smooth Non-separable Non-convex Penalty Function Based on Ultra-discretization Formula", "categories": ["math.OC", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In sparse optimization, the $\\ell_{1}$ norm is widely adopted for its\nconvexity, yet it often yields solutions with smaller magnitudes than expected.\nTo mitigate this drawback, various non-convex sparse penalties have been\nproposed. Some employ non-separability, with ordered weighting as an effective\nexample, to retain large components while suppressing small ones. Motivated by\nthese approaches, we propose ULPENS, a non-convex, non-separable\nsparsity-inducing penalty function that enables control over the suppression of\nelements. Derived from the ultra-discretization formula, ULPENS can\ncontinuously interpolate between the $\\ell_{1}$ norm and a non-convex selective\nsuppressing function by adjusting parameters inherent to the formula. With the\nformula, ULPENS is smooth, allowing the use of efficient gradient-based\noptimization algorithms. We establish key theoretical properties of ULPENS and\ndemonstrate its practical effectiveness through numerical experiments."}
{"id": "2509.20082", "pdf": "https://arxiv.org/pdf/2509.20082", "abs": "https://arxiv.org/abs/2509.20082", "authors": ["Surov Maksim"], "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate\ntime-desynchronization dynamics. To stabilize the resulting extended transverse\ndynamics, we employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group of six\nButterfly robots."}
{"id": "2509.20048", "pdf": "https://arxiv.org/pdf/2509.20048", "abs": "https://arxiv.org/abs/2509.20048", "authors": ["Rami Zewail"], "title": "Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Learning robust representations for biosignals is often hampered by the\nchallenge of designing effective data augmentations.Traditional methods can\nfail to capture the complex variations inherent in physiological data. Within\nthis context, we propose a novel hybrid framework, Diffusion-Augmented\nContrastive Learning (DACL), that fuses concepts from diffusion models and\nsupervised contrastive learning. The DACL framework operates on a latent space\ncreated by a lightweight Variational Autoencoder (VAE) trained on our novel\nScattering Transformer (ST) features [12]. It utilizes the diffusion forward\nprocess as a principled data augmentation technique to generate multiple noisy\nviews of these latent embeddings. A U-Net style encoder is then trained with a\nsupervised contrastive objective to learn a representation that balances class\ndiscrimination with robustness to noise across various diffusion time steps. We\nevaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,\nachieving a competitive AUROC of 0.7815. This work establishes a new paradigm\nfor representation learning by using the diffusion process itself to drive the\ncontrastive objective, creating noise-invariant embeddings that demonstrate a\nstrong foundation for class separability."}
{"id": "2509.20084", "pdf": "https://arxiv.org/pdf/2509.20084", "abs": "https://arxiv.org/abs/2509.20084", "authors": ["Guillermo Gil", "Jose Antonio Cobano", "Luis Merino", "Fernando Caballero"], "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields", "categories": ["cs.RO"], "comment": "9 pages, 5 figures, submitted to ICRA 2026", "summary": "This paper introduces a novel framework for continuous 3D trajectory\noptimization in cluttered environments, leveraging online neural Euclidean\nSigned Distance Fields (ESDFs). Unlike prior approaches that rely on\ndiscretized ESDF grids with interpolation, our method directly optimizes smooth\ntrajectories represented by fifth-order polynomials over a continuous neural\nESDF, ensuring precise gradient information throughout the entire trajectory.\nThe framework integrates a two-stage nonlinear optimization pipeline that\nbalances efficiency, safety and smoothness. Experimental results demonstrate\nthat C-3TO produces collision-aware and dynamically feasible trajectories.\nMoreover, its flexibility in defining local window sizes and optimization\nparameters enables straightforward adaptation to diverse user's needs without\ncompromising performance. By combining continuous trajectory parameterization\nwith a continuously updated neural ESDF, C-3TO establishes a robust and\ngeneralizable foundation for safe and efficient local replanning in aerial\nrobotics."}
{"id": "2509.20141", "pdf": "https://arxiv.org/pdf/2509.20141", "abs": "https://arxiv.org/abs/2509.20141", "authors": ["Davi Juvêncio Gomes de Sousa", "Caroline da Silva Morais Alves", "Valéria Loureiro da Silva", "Nelson Alves Ferreira Neto"], "title": "Digital Signal Processing from Classical Coherent Systems to Continuous-Variable QKD: A Review of Cross-Domain Techniques, Applications, and Challenges", "categories": ["quant-ph", "cs.AR", "cs.ET", "cs.IR", "eess.SP"], "comment": null, "summary": "This systematic review investigates the application of digital signal\nprocessing (DSP) techniques -- originally developed for coherent optical\ncommunication systems to continuous-variable quantum key distribution (CV-QKD).\nThe convergence of these domains has enabled significant advances in CV-QKD\nperformance, particularly in phase synchronization, polarization tracking, and\nexcess noise mitigation. To provide a comprehensive and reproducible synthesis\nof this emerging field, we employed the APISSER methodology, a task-oriented\nframework adapted from the PRISMA protocol. A structured search across IEEE\nXplore and Web of Science databases (2021-2025) yielded 220 relevant\npublications, which were screened, classified, and analyzed to address six\nresearch questions. Our findings highlight that many classical DSP algorithms,\nsuch as Kalman filtering, carrier recovery, adaptive equalization, and\nmachine-learning-assisted signal estimation, have been successfully adapted to\nthe quantum regime, often requiring modifications to meet security and noise\nconstraints. We also identify a range of recent DSP innovations in coherent\noptical communication systems with high potential for future CV-QKD\nintegration, including neural equalization, probabilistic shaping, and joint\nretiming-equalization filters. Despite these advances, challenges remain in\nachieving robust phase tracking under ultra-low Signal-to-Noise Ratio (SNR)\nconditions, real-time polarization compensation, and secure co-existence with\nclassical channels. This review maps current trends, technical barriers, and\nemerging opportunities at the intersection of signal processing for quantum and\nclassical communication, supporting the development of scalable and resilient\nCV-QKD systems."}
{"id": "2509.20093", "pdf": "https://arxiv.org/pdf/2509.20093", "abs": "https://arxiv.org/abs/2509.20093", "authors": ["Venkat Margapuri", "Garik Kazanjian", "Naren Kosaraju"], "title": "Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees", "categories": ["cs.RO"], "comment": null, "summary": "This study proposes a hybrid safety verification framework for closed-loop\nmulti-agent systems under bounded stochastic disturbances. The proposed\napproach augments control barrier functions with a novel $\\psi$-weighted\nformulation that encodes directional control alignment between agents into the\nsafety constraints. Deterministic admissibility is combined with empirical\nvalidation via Monte Carlo rollouts, and a PAC-style guarantee is derived based\non margin-aware safety violations to provide a probabilistic safety\ncertificate. The results from the experiments conducted under different bounded\nstochastic disturbances validate the feasibility of the proposed approach."}
{"id": "2509.20109", "pdf": "https://arxiv.org/pdf/2509.20109", "abs": "https://arxiv.org/abs/2509.20109", "authors": ["Pengxiang Li", "Yinan Zheng", "Yue Wang", "Huimin Wang", "Hang Zhao", "Jingjing Liu", "Xianyuan Zhan", "Kun Zhan", "Xianpeng Lang"], "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": null, "summary": "End-to-End (E2E) solutions have emerged as a mainstream approach for\nautonomous driving systems, with Vision-Language-Action (VLA) models\nrepresenting a new paradigm that leverages pre-trained multimodal knowledge\nfrom Vision-Language Models (VLMs) to interpret and interact with complex\nreal-world environments. However, these methods remain constrained by the\nlimitations of imitation learning, which struggles to inherently encode\nphysical rules during training. Existing approaches often rely on complex\nrule-based post-refinement, employ reinforcement learning that remains largely\nlimited to simulation, or utilize diffusion guidance that requires\ncomputationally expensive gradient calculations. To address these challenges,\nwe introduce ReflectDrive, a novel learning-based framework that integrates a\nreflection mechanism for safe trajectory generation via discrete diffusion. We\nfirst discretize the two-dimensional driving space to construct an action\ncodebook, enabling the use of pre-trained Diffusion Language Models for\nplanning tasks through fine-tuning. Central to our approach is a safety-aware\nreflection mechanism that performs iterative self-correction without gradient\ncomputation. Our method begins with goal-conditioned trajectory generation to\nmodel multi-modal driving behaviors. Based on this, we apply local search\nmethods to identify unsafe tokens and determine feasible solutions, which then\nserve as safe anchors for inpainting-based regeneration. Evaluated on the\nNAVSIM benchmark, ReflectDrive demonstrates significant advantages in\nsafety-critical trajectory generation, offering a scalable and reliable\nsolution for autonomous driving systems."}
{"id": "2509.20219", "pdf": "https://arxiv.org/pdf/2509.20219", "abs": "https://arxiv.org/abs/2509.20219", "authors": ["Sicong Liu", "Jianhui Liu", "Fang Chen", "Wenjian Yang", "Juan Yi", "Yu Zheng", "Zheng Wang", "Wanchao Chi", "Chaoyang Song"], "title": "A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering", "categories": ["cs.RO"], "comment": "20 pages, 11 figures, 4 tables. Submitted Under Review", "summary": "Robotic tails can enhance the stability and maneuverability of mobile robots,\nbut current designs face a trade-off between the power of rigid systems and the\nsafety of soft ones. Rigid tails generate large inertial effects but pose risks\nin unstructured environments, while soft tails lack sufficient speed and force.\nWe present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this\nchallenge through a compliant pneumatic body reinforced by a passively jointed\nvertebral column inspired by musculoskeletal structures. This hybrid design\ndecouples load-bearing and actuation, enabling high-pressure actuation (up to 6\nbar) for superior dynamics while preserving compliance. A dedicated kinematic\nand dynamic model incorporating vertebral constraints is developed and\nvalidated experimentally. The BVSR tail achieves angular velocities above\n670{\\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,\nindicating over 200% improvement compared to non-vertebraic designs.\nDemonstrations on rapid cart stabilization, obstacle negotiation, high-speed\nsteering, and quadruped integration confirm its versatility and practical\nutility for agile robotic platforms."}
{"id": "2509.20229", "pdf": "https://arxiv.org/pdf/2509.20229", "abs": "https://arxiv.org/abs/2509.20229", "authors": ["Angelos Plastropoulos", "Nicolas P. Avdelidis", "Argyrios Zolotas"], "title": "Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale", "categories": ["cs.RO"], "comment": null, "summary": "The accuracy, resilience, and affordability of localisation are fundamental\nto autonomous robotic inspection within aircraft maintenance and overhaul (MRO)\nhangars. Hangars typically feature tall ceilings and are often made of\nmaterials such as metal. Due to its nature, it is considered a GPS-denied\nenvironment, with extensive multipath effects and stringent operational\nconstraints that collectively create a uniquely challenging environment. This\npersistent gap highlights the need for domain-specific comparative studies,\nincluding rigorous cost, accuracy, and integration assessments, to inform a\nreliable and scalable deployment of a localisation system in the Smart Hangar.\nThis paper presents the first techno-economic roadmap that benchmarks motion\ncapture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network\nacross three operational scenarios: robot localisation, asset tracking, and\nsurface defect detection within a 40x50 m hangar bay. A dual-layer optimisation\nfor camera selection and positioning framework is introduced, which couples\nmarket-based camera-lens selection with an optimisation solver, producing\ncamera layouts that minimise hardware while meeting accuracy targets. The\nroadmap equips MRO planners with an actionable method to balance accuracy,\ncoverage, and budget, demonstrating that an optimised vision architecture has\nthe potential to unlock robust and cost-effective sensing for next-generation\nSmart Hangars."}
{"id": "2509.20253", "pdf": "https://arxiv.org/pdf/2509.20253", "abs": "https://arxiv.org/abs/2509.20253", "authors": ["Jinhao Chai", "Anqing Jiang", "Hao Jiang", "Shiyi Mu", "Zichong Gu", "Shugong Xu"], "title": "AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving", "categories": ["cs.RO", "cs.AI"], "comment": "IWACIII 2025", "summary": "End-to-end multi-modal planning has become a transformative paradigm in\nautonomous driving, effectively addressing behavioral multi-modality and the\ngeneralization challenge in long-tail scenarios. We propose AnchDrive, a\nframework for end-to-end driving that effectively bootstraps a diffusion policy\nto mitigate the high computational cost of traditional generative models.\nRather than denoising from pure noise, AnchDrive initializes its planner with a\nrich set of hybrid trajectory anchors. These anchors are derived from two\ncomplementary sources: a static vocabulary of general driving priors and a set\nof dynamic, context-aware trajectories. The dynamic trajectories are decoded in\nreal-time by a Transformer that processes dense and sparse perceptual features.\nThe diffusion model then learns to refine these anchors by predicting a\ndistribution of trajectory offsets, enabling fine-grained refinement. This\nanchor-based bootstrapping design allows for efficient generation of diverse,\nhigh-quality trajectories. Experiments on the NAVSIM benchmark confirm that\nAnchDrive sets a new state-of-the-art and shows strong gen?eralizability"}
{"id": "2509.20263", "pdf": "https://arxiv.org/pdf/2509.20263", "abs": "https://arxiv.org/abs/2509.20263", "authors": ["Bingjie Chen", "Zihan Wang", "Zhe Han", "Guoping Pan", "Yi Cheng", "Houde Liu"], "title": "HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms", "categories": ["cs.RO"], "comment": null, "summary": "Traditional IK methods for redundant humanoid manipulators emphasize\nend-effector (EE) tracking, frequently producing configurations that are valid\nmechanically but not human-like. We present Human-Like Inverse Kinematics\n(HL-IK), a lightweight IK framework that preserves EE tracking while shaping\nwhole-arm configurations to appear human-like, without full-body sensing at\nruntime. The key idea is a learned elbow prior: using large-scale human motion\ndata retargeted to the robot, we train a FiLM-modulated spatio-temporal\nattention network (FiSTA) to predict the next-step elbow pose from the EE\ntarget and a short history of EE-elbow states.This prediction is incorporated\nas a small residual alongside EE and smoothness terms in a standard\nLevenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK\nstacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and\ndirection error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the\nmost challenging trajectories. Hardware teleoperation on a robot distinct from\nsimulation further confirms the gains in anthropomorphism. HL-IK is simple to\nintegrate, adaptable across platforms via our pipeline, and adds minimal\ncomputation, enabling human-like motions for humanoid robots. Project page:\nhttps://hl-ik.github.io/"}
{"id": "2509.20286", "pdf": "https://arxiv.org/pdf/2509.20286", "abs": "https://arxiv.org/abs/2509.20286", "authors": ["Georgios Tziafas", "Jiayun Zhang", "Hamidreza Kasaei"], "title": "Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video", "categories": ["cs.RO"], "comment": null, "summary": "Learning visuomotor policies from expert demonstrations is an important\nfrontier in modern robotics research, however, most popular methods require\ncopious efforts for collecting teleoperation data and struggle to generalize\nout-ofdistribution. Scaling data collection has been explored through\nleveraging human videos, as well as demonstration augmentation techniques. The\nlatter approach typically requires expensive simulation rollouts and trains\npolicies with synthetic image data, therefore introducing a sim-to-real gap. In\nparallel, alternative state representations such as keypoints have shown great\npromise for category-level generalization. In this work, we bring these avenues\ntogether in a unified framework: PAD (Parse-AugmentDistill), for learning\ngeneralizable bimanual policies from a single human video. Our method relies on\nthree steps: (a) parsing a human video demo into a robot-executable\nkeypoint-action trajectory, (b) employing bimanual task-and-motion-planning to\naugment the demonstration at scale without simulators, and (c) distilling the\naugmented trajectories into a keypoint-conditioned policy. Empirically, we\nshowcase that PAD outperforms state-ofthe-art bimanual demonstration\naugmentation works relying on image policies with simulation rollouts, both in\nterms of success rate and sample/cost efficiency. We deploy our framework in\nsix diverse real-world bimanual tasks such as pouring drinks, cleaning trash\nand opening containers, producing one-shot policies that generalize in unseen\nspatial arrangements, object instances and background distractors.\nSupplementary material can be found in the project webpage\nhttps://gtziafas.github.io/PAD_project/."}
{"id": "2509.20297", "pdf": "https://arxiv.org/pdf/2509.20297", "abs": "https://arxiv.org/abs/2509.20297", "authors": ["Remo Steiner", "Alexander Millane", "David Tingdahl", "Clemens Volk", "Vikram Ramasamy", "Xinjie Yao", "Peter Du", "Soha Pouya", "Shiwei Sheng"], "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies", "categories": ["cs.RO"], "comment": "Accepted to CoRL 2025 Workshop RemembeRL", "summary": "End-to-end learning of robot control policies, structured as neural networks,\nhas emerged as a promising approach to robotic manipulation. To complete many\ncommon tasks, relevant objects are required to pass in and out of a robot's\nfield of view. In these settings, spatial memory - the ability to remember the\nspatial composition of the scene - is an important competency. However,\nbuilding such mechanisms into robot learning systems remains an open research\nproblem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D\nAction Policies), a 3D diffusion policy that generates robot trajectories based\non a semantic 3D reconstruction of the environment. We show in simulation\nexperiments that our approach is effective at solving tasks where\nstate-of-the-art approaches without memory mechanisms struggle. We release our\nreconstruction system, training code, and evaluation tasks to spur research in\nthis direction."}
{"id": "2509.20322", "pdf": "https://arxiv.org/pdf/2509.20322", "abs": "https://arxiv.org/abs/2509.20322", "authors": ["Shaofeng Yin", "Yanjie Ze", "Hong-Xing Yu", "C. Karen Liu", "Jiajun Wu"], "title": "VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Website: https://visualmimic.github.io", "summary": "Humanoid loco-manipulation in unstructured environments demands tight\nintegration of egocentric perception and whole-body control. However, existing\napproaches either depend on external motion capture systems or fail to\ngeneralize across diverse tasks. We introduce VisualMimic, a visual sim-to-real\nframework that unifies egocentric vision with hierarchical whole-body control\nfor humanoid robots. VisualMimic combines a task-agnostic low-level keypoint\ntracker -- trained from human motion data via a teacher-student scheme -- with\na task-specific high-level policy that generates keypoint commands from visual\nand proprioceptive input. To ensure stable training, we inject noise into the\nlow-level policy and clip high-level actions using human motion statistics.\nVisualMimic enables zero-shot transfer of visuomotor policies trained in\nsimulation to real humanoid robots, accomplishing a wide range of\nloco-manipulation tasks such as box lifting, pushing, football dribbling, and\nkicking. Beyond controlled laboratory settings, our policies also generalize\nrobustly to outdoor environments. Videos are available at:\nhttps://visualmimic.github.io ."}
{"id": "2509.20333", "pdf": "https://arxiv.org/pdf/2509.20333", "abs": "https://arxiv.org/abs/2509.20333", "authors": ["Srikrishna Bangalore Raghu", "Alessandro Roncone"], "title": "BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning", "categories": ["cs.RO"], "comment": "8 Pages, 7 Figures", "summary": "In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based\nmotion planner that consistently and quickly finds low-cost solutions in\nenvironments with varying obstacle clutter. The algorithm combines exploration\nand exploitation while relying on precomputed robot state traversals, resulting\nin efficient convergence towards the goal. Our key contributions include: i) a\nstrategy to navigate through obstacle-rich spaces by sorting and sequencing\npreprocessed forward propagations; and ii) BBoE, a robust bidirectional\nkinodynamic planner that utilizes this strategy to produce fast and feasible\nsolutions. The proposed framework reduces planning time, diminishes solution\ncost and increases success rate in comparison to previous approaches."}
{"id": "2509.19318", "pdf": "https://arxiv.org/pdf/2509.19318", "abs": "https://arxiv.org/abs/2509.19318", "authors": ["Yanbaihui Liu", "Erica Babusci", "Claudia K. Gunsch", "Boyuan Chen"], "title": "Scensory: Automated Real-Time Fungal Identification and Spatial Mapping", "categories": ["eess.SP", "cs.RO"], "comment": "Our project website is at: http://generalroboticslab.com/Scensory", "summary": "Indoor fungal contamination poses significant risks to public health, yet\nexisting detection methods are slow, costly, and lack spatial resolution.\nConventional approaches rely on laboratory analysis or high-concentration\nsampling, making them unsuitable for real-time monitoring and scalable\ndeployment. We introduce \\textbf{\\textit{Scensory}}, a robot-enabled olfactory\nsystem that simultaneously identifies fungal species and localizes their\nspatial origin using affordable volatile organic compound (VOC) sensor arrays\nand deep learning. Our key idea is that temporal VOC dynamics encode both\nchemical and spatial signatures, which we decode through neural architectures\ntrained on robot-automated data collection. We demonstrate two operational\nmodes: a passive multi-array configuration for environmental monitoring, and a\nmobile single-array configuration for active source tracking. Across five\nfungal species, our system achieves up to 89.85\\% accuracy in species detection\nand 87.31\\% accuracy in localization under ambient conditions, where each\nprediction only takes 3--7\\,s sensor inputs. Additionally, by computationally\nanalyzing model behavior, we can uncover key biochemical signatures without\nadditional laboratory experiments. Our approach enables real-time, spatially\naware fungal monitoring and establishes a scalable and affordable framework for\nautonomous environmental sensing."}
{"id": "2509.19379", "pdf": "https://arxiv.org/pdf/2509.19379", "abs": "https://arxiv.org/abs/2509.19379", "authors": ["Returaj Burnwal", "Hriday Mehta", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "Learning from Observation: A Survey of Recent Advances", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": null, "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent\nby mimicking an expert's behavior without requiring a reward function. IL\nalgorithms often necessitate access to state and action information from expert\ndemonstrations. Although expert actions can provide detailed guidance,\nrequiring such action information may prove impractical for real-world\napplications where expert actions are difficult to obtain. To address this\nlimitation, the concept of learning from observation (LfO) or state-only\nimitation learning (SOIL) has recently gained attention, wherein the imitator\nonly has access to expert state visitation information. In this paper, we\npresent a framework for LfO and use it to survey and classify existing LfO\nmethods in terms of their trajectory construction, assumptions and algorithm's\ndesign choices. This survey also draws connections between several related\nfields like offline RL, model-based RL and hierarchical RL. Finally, we use our\nframework to identify open problems and suggest future research directions."}
{"id": "2509.19477", "pdf": "https://arxiv.org/pdf/2509.19477", "abs": "https://arxiv.org/abs/2509.19477", "authors": ["Abhinav Sinha", "Rohit V. Nanavati"], "title": "Robust Near-Optimal Nonlinear Target Enclosing Guidance", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper proposes a nonlinear optimal guidance law that enables a pursuer\nto enclose a target within arbitrary geometric patterns, which extends beyond\nconventional circular encirclement. The design operates using only relative\nstate measurements and formulates a target enclosing guidance law in which the\nvehicle's lateral acceleration serves as the steering control, making it\nwell-suited for aerial vehicles with turning constraints. Our approach\ngeneralizes and extends existing guidance strategies that are limited to target\nencirclement and provides a degree of optimality. At the same time, the exact\ninformation of the target's maneuver is unnecessary during the design. The\nguidance law is developed within the framework of a state-dependent Riccati\nequation (SDRE), thereby providing a systematic way to handle nonlinear\ndynamics through a pseudo-linear representation to design locally optimal\nfeedback guidance commands through state-dependent weighting matrices. While\nSDRE ensures near-optimal performance in the absence of strong disturbances, we\nfurther augment the design to incorporate an integral sliding mode manifold to\ncompensate when disturbances push the system away from the nominal trajectory,\nand demonstrate that the design provides flexibility in the sense that the\n(possibly time-varying) stand-off curvature could also be treated as unknown.\nSimulations demonstrate the efficacy of the proposed approach."}
{"id": "2509.19524", "pdf": "https://arxiv.org/pdf/2509.19524", "abs": "https://arxiv.org/abs/2509.19524", "authors": ["Ramy ElMallah", "Krish Chhajer", "Chi-Guhn Lee"], "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop", "summary": "Robot learning papers typically report a single binary success rate (SR),\nwhich obscures where a policy succeeds or fails along a multi-step manipulation\ntask. We argue that subgoal-level reporting should become routine: for each\ntrajectory, a vector of per-subgoal SRs that makes partial competence visible\n(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware\nplug-in evaluation framework that utilizes vision-language models (VLMs) as\nautomated judges of subgoal outcomes from recorded images or videos. Rather\nthan proposing new benchmarks or APIs, our contribution is to outline design\nprinciples for a scalable, community-driven open-source project. In StepEval,\nthe primary artifact for policy evaluation is the per-subgoal SR vector;\nhowever, other quantities (e.g., latency or cost estimates) are also considered\nfor framework-optimization diagnostics to help the community tune evaluation\nefficiency and accuracy when ground-truth subgoal success labels are available.\nWe discuss how such a framework can remain model-agnostic, support single- or\nmulti-view inputs, and be lightweight enough to adopt across labs. The intended\ncontribution is a shared direction: a minimal, extensible seed that invites\nopen-source contributions, so that scoring the steps, not just the final goal,\nbecomes a standard and reproducible practice."}
{"id": "2509.19644", "pdf": "https://arxiv.org/pdf/2509.19644", "abs": "https://arxiv.org/abs/2509.19644", "authors": ["William L. Muckelroy III", "Mohammed Alsakabi", "John M. Dolan", "Ozan K. Tonguz"], "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR's dense, sharp point cloud (PC) representations of the surrounding\nenvironment enable accurate perception and significantly improve road safety by\noffering greater scene awareness and understanding. However, LiDAR's high cost\ncontinues to restrict the broad adoption of high-level Autonomous Driving (AD)\nsystems in commercially available vehicles. Prior research has shown progress\ntowards circumventing the need for LiDAR by training a neural network, using\nLiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds\nusing only 4D Radars. One of the best examples is a neural network created to\ntrain a more efficient radar target detector with a modular 2D convolutional\nneural network (CNN) backbone and a temporal coherence network at its core that\nuses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we\ninvestigate the impact of higher-capacity segmentation backbones on the quality\nof the produced point clouds. Our results show that while very high-capacity\nmodels may actually hurt performance, an optimal segmentation backbone can\nprovide a 23.7% improvement over the state-of-the-art (SOTA)."}
{"id": "2509.19713", "pdf": "https://arxiv.org/pdf/2509.19713", "abs": "https://arxiv.org/abs/2509.19713", "authors": ["Saimouli Katragadda", "Guoquan Huang"], "title": "VIMD: Monocular Visual-Inertial Motion and Depth Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate and efficient dense metric depth estimation is crucial for 3D visual\nperception in robotics and XR. In this paper, we develop a monocular\nvisual-inertial motion and depth (VIMD) learning framework to estimate dense\nmetric depth by leveraging accurate and efficient MSCKF-based monocular\nvisual-inertial motion tracking. At the core the proposed VIMD is to exploit\nmulti-view information to iteratively refine per-pixel scale, instead of\nglobally fitting an invariant affine model as in the prior work. The VIMD\nframework is highly modular, making it compatible with a variety of existing\ndepth estimation backbones. We conduct extensive evaluations on the TartanAir\nand VOID datasets and demonstrate its zero-shot generalization capabilities on\nthe AR Table dataset. Our results show that VIMD achieves exceptional accuracy\nand robustness, even with extremely sparse points as few as 10-20 metric depth\npoints per image. This makes the proposed VIMD a practical solution for\ndeployment in resource constrained settings, while its robust performance and\nstrong generalization capabilities offer significant potential across a wide\nrange of scenarios."}
{"id": "2509.19789", "pdf": "https://arxiv.org/pdf/2509.19789", "abs": "https://arxiv.org/abs/2509.19789", "authors": ["Carlo Bosio", "Greg Woelki", "Noureldin Hendy", "Nicholas Roy", "Byungsoo Kim"], "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "10 pages, 6 figures", "summary": "Human drivers focus only on a handful of agents at any one time. On the other\nhand, autonomous driving systems process complex scenes with numerous agents,\nregardless of whether they are pedestrians on a crosswalk or vehicles parked on\nthe side of the road. While attention mechanisms offer an implicit way to\nreduce the input to the elements that affect decisions, existing attention\nmechanisms for capturing agent interactions are quadratic, and generally\ncomputationally expensive. We propose RDAR, a strategy to learn per-agent\nrelevance -- how much each agent influences the behavior of the controlled\nvehicle -- by identifying which agents can be excluded from the input to a\npre-trained behavior model. We formulate the masking procedure as a Markov\nDecision Process where the action consists of a binary mask indicating agent\nselection. We evaluate RDAR on a large-scale driving dataset, and demonstrate\nits ability to learn an accurate numerical measure of relevance by achieving\ncomparable driving performance, in terms of overall progress, safety and\nperformance, while processing significantly fewer agents compared to a state of\nthe art behavior model."}
{"id": "2509.19843", "pdf": "https://arxiv.org/pdf/2509.19843", "abs": "https://arxiv.org/abs/2509.19843", "authors": ["Filippo Ziliotto", "Jelin Raphael Akkara", "Alessandro Daniele", "Lamberto Ballan", "Luciano Serafini", "Tommaso Campari"], "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent advances in Embodied AI have enabled agents to perform increasingly\ncomplex tasks and adapt to diverse environments. However, deploying such agents\nin realistic human-centered scenarios, such as domestic households, remains\nchallenging, particularly due to the difficulty of modeling individual human\npreferences and behaviors. In this work, we introduce PersONAL (PERSonalized\nObject Navigation And Localization, a comprehensive benchmark designed to study\npersonalization in Embodied AI. Agents must identify, retrieve, and navigate to\nobjects associated with specific users, responding to natural-language queries\nsuch as \"find Lily's backpack\". PersONAL comprises over 2,000 high-quality\nepisodes across 30+ photorealistic homes from the HM3D dataset. Each episode\nincludes a natural-language scene description with explicit associations\nbetween objects and their owners, requiring agents to reason over user-specific\nsemantics. The benchmark supports two evaluation modes: (1) active navigation\nin unseen environments, and (2) object grounding in previously mapped scenes.\nExperiments with state-of-the-art baselines reveal a substantial gap to human\nperformance, highlighting the need for embodied agents capable of perceiving,\nreasoning, and memorizing over personalized information; paving the way towards\nreal-world assistive robot."}
{"id": "2509.20021", "pdf": "https://arxiv.org/pdf/2509.20021", "abs": "https://arxiv.org/abs/2509.20021", "authors": ["Tongtong Feng", "Xin Wang", "Yu-Gang Jiang", "Wenwu Zhu"], "title": "Embodied AI: From LLMs to World Models", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "Accepted by IEEE CASM", "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation."}
{"id": "2509.20107", "pdf": "https://arxiv.org/pdf/2509.20107", "abs": "https://arxiv.org/abs/2509.20107", "authors": ["JuanaJuana Valeria Hurtado", "Rohit Mohan", "Abhinav Valada"], "title": "Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Hyperspectral imaging (HSI) captures spatial information along with dense\nspectral measurements across numerous narrow wavelength bands. This rich\nspectral content has the potential to facilitate robust robotic perception,\nparticularly in environments with complex material compositions, varying\nillumination, or other visually challenging conditions. However, current HSI\nsemantic segmentation methods underperform due to their reliance on\narchitectures and learning frameworks optimized for RGB inputs. In this work,\nwe propose a novel hyperspectral adapter that leverages pretrained vision\nfoundation models to effectively learn from hyperspectral data. Our\narchitecture incorporates a spectral transformer and a spectrum-aware spatial\nprior module to extract rich spatial-spectral features. Additionally, we\nintroduce a modality-aware interaction block that facilitates effective\nintegration of hyperspectral representations and frozen vision Transformer\nfeatures through dedicated extraction and injection mechanisms. Extensive\nevaluations on three benchmark autonomous driving datasets demonstrate that our\narchitecture achieves state-of-the-art semantic segmentation performance while\ndirectly using HSI inputs, outperforming both vision-based and hyperspectral\nsegmentation methods. We make the code available at\nhttps://hyperspectraladapter.cs.uni-freiburg.de."}
{"id": "2509.20314", "pdf": "https://arxiv.org/pdf/2509.20314", "abs": "https://arxiv.org/abs/2509.20314", "authors": ["Abhinav Sinha", "Dwaipayan Mukherjee", "Shashi Ranjan Kumar"], "title": "On Robustness of Consensus over Pseudo-Undirected Path Graphs", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.DS", "math.OC"], "comment": null, "summary": "Consensus over networked agents is typically studied using undirected or\ndirected communication graphs. Undirected graphs enforce symmetry in\ninformation exchange, leading to convergence to the average of initial states,\nwhile directed graphs permit asymmetry but make consensus dependent on root\nnodes and their influence. Both paradigms impose inherent restrictions on\nachievable consensus values and network robustness. This paper introduces a\ntheoretical framework for achieving consensus over a class of network\ntopologies, termed pseudo-undirected graphs, which retains bidirectional\nconnectivity between node pairs but allows the corresponding edge weights to\ndiffer, including the possibility of negative values under bounded conditions.\nThe resulting Laplacian is generally non-symmetric, yet it guarantees consensus\nunder connectivity assumptions, to expand the solution space, which enables the\nsystem to achieve a stable consensus value that can lie outside the convex hull\nof the initial state set. We derive admissibility bounds for negative weights\nfor a pseudo-undirected path graph, and show an application in the simultaneous\ninterception of a moving target."}
{"id": "2509.20328", "pdf": "https://arxiv.org/pdf/2509.20328", "abs": "https://arxiv.org/abs/2509.20328", "authors": ["Thaddäus Wiedemer", "Yuxuan Li", "Paul Vicol", "Shixiang Shane Gu", "Nick Matarese", "Kevin Swersky", "Been Kim", "Priyank Jaini", "Robert Geirhos"], "title": "Video models are zero-shot learners and reasoners", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://video-zero-shot.github.io/", "summary": "The remarkable zero-shot capabilities of Large Language Models (LLMs) have\npropelled natural language processing from task-specific models to unified,\ngeneralist foundation models. This transformation emerged from simple\nprimitives: large, generative models trained on web-scale data. Curiously, the\nsame primitives apply to today's generative video models. Could video models be\non a trajectory towards general-purpose vision understanding, much like LLMs\ndeveloped general-purpose language understanding? We demonstrate that Veo 3 can\nsolve a broad variety of tasks it wasn't explicitly trained for: segmenting\nobjects, detecting edges, editing images, understanding physical properties,\nrecognizing object affordances, simulating tool use, and more. These abilities\nto perceive, model, and manipulate the visual world enable early forms of\nvisual reasoning like maze and symmetry solving. Veo's emergent zero-shot\ncapabilities indicate that video models are on a path to becoming unified,\ngeneralist vision foundation models."}
