{"id": "2511.15812", "pdf": "https://arxiv.org/pdf/2511.15812", "abs": "https://arxiv.org/abs/2511.15812", "authors": ["Luke Dosiek", "Akaash Karn", "Frank Liu"], "title": "Rapid and Accurate Changepoint Detection of Power System Forced Oscillations", "categories": ["eess.SP", "eess.SY"], "comment": "Currently under review for the proceedings of the 2026 IEEE Power and Energy Society General Meeting (PESGM26)", "summary": "This paper describes a new approach for using changepoint detection (CPD) to estimate the starting and stopping times of a forced oscillation (FO) in measured power system data. As with a previous application of CPD to this problem, the pruned exact linear time (PELT) algorithm is used. However, instead of allowing PELT to automatically tune its penalty parameter, a method of manually providing it is presented that dramatically reduces computation time without sacrificing accuracy. Additionally, the new algorithm requires fewer input parameters and provides a formal, data-driven approach to setting the minimum FO segment length to consider as troublesome for an electromechanical mode meter. A low-order ARMAX representation of the minniWECC model is used to test the approach, where a 98\\% reduction in computation time is enjoyed with high estimation accuracy."}
{"id": "2511.15902", "pdf": "https://arxiv.org/pdf/2511.15902", "abs": "https://arxiv.org/abs/2511.15902", "authors": ["Roman Dolgopolyi", "Antonis Chatzipanagiotou"], "title": "EEG Emotion Recognition Through Deep Learning", "categories": ["eess.SP", "cs.LG"], "comment": "This version corresponds to the original manuscript submitted to the 22nd EMCIS conference prior to peer review. The peer-reviewed and accepted version will appear in the Springer conference proceedings", "summary": "An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions..."}
{"id": "2511.15947", "pdf": "https://arxiv.org/pdf/2511.15947", "abs": "https://arxiv.org/abs/2511.15947", "authors": ["Jeongju Jee", "Jeffrey G. Andrews"], "title": "Integrated Coexistence for Satellite and Terrestrial Networks with Multistatic ISAC", "categories": ["eess.SP"], "comment": null, "summary": "Tightly integrated low earth orbit (LEO) satellite communications and terrestrial integrated sensing and communication (ISAC) are expected to be key novel aspects of the 6G era. Spectrum sharing between satellite and terrestrial cellular networks may, however, cause severe interference. This paper introduces a cooperation framework for integrated coexistence between satellite and terrestrial networks where the terrestrial network also deploys multistatic ISAC. Unlike prior works that assume ideal channel state information (CSI) acquisition, the proposed approach develops a practical structure consisting of pre-optimization and refinement stages that leverages the predictability of satellite CSI. In addition, a co-design of terrestrial beamforming and satellite power allocation utilizing a weighted minimum mean-squared error algorithm is proposed, and a target-radar association method designed for multistatic ISAC is presented. Simulation results show that the proposed approach significantly enhances the performance of these integrated networks. Furthermore, it is confirmed that the overall performance approaches the interference-free benchmark as the number of spot beams and radar receivers increases, demonstrating the feasibility of spectral coexistence between the two networks."}
{"id": "2511.16000", "pdf": "https://arxiv.org/pdf/2511.16000", "abs": "https://arxiv.org/abs/2511.16000", "authors": ["Weijie Xiong", "Jingran Lin", "Zhiling Xiao", "Qiang Li", "Yuhan Zhang"], "title": "Joint Admission Control and Power Minimization in IRS-assisted Networks", "categories": ["eess.SP"], "comment": null, "summary": "Joint admission control and power minimization are critical challenges in intelligent reflecting surface (IRS)-assisted networks. Traditional methods often rely on \\( l_1 \\)-norm approximations and alternating optimization (AO) techniques, which suffer from high computational complexity and lack robust convergence guarantees. To address these limitations, we propose a sigmoid-based approximation of the \\( l_0 \\)-norm AC indicator, enabling a more efficient and tractable reformulation of the problem. Additionally, we introduce a penalty dual decomposition (PDD) algorithm to jointly optimize beamforming and admission control, ensuring convergence to a stationary solution. This approach reduces computational complexity and supports distributed implementation. Moreover, it outperforms existing methods by achieving lower power consumption, accommodating more users, and reducing computational time."}
{"id": "2511.15909", "pdf": "https://arxiv.org/pdf/2511.15909", "abs": "https://arxiv.org/abs/2511.15909", "authors": ["J. Cristobal", "A. Z. Zain Aldeen", "M. Izadi", "R. Faieghi"], "title": "Gimballed Rotor Mechanism for Omnidirectional Quadrotors", "categories": ["cs.RO"], "comment": "6 pages, 7 figures, CASE 2025", "summary": "This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach."}
{"id": "2511.16169", "pdf": "https://arxiv.org/pdf/2511.16169", "abs": "https://arxiv.org/abs/2511.16169", "authors": ["Zijian Wang", "Xiaoyu Bao", "Chenhao Zhao", "Jihui Zhang", "Sizhi Ai", "Yuanqing Li"], "title": "UT-OSANet: A Multimodal Deep Learning model for Evaluating and Classifying Obstructive Sleep Apnea", "categories": ["eess.SP"], "comment": "12 pages,8 figures", "summary": "Obstructive sleep apnea (OSA) is a highly prevalent sleep disorder that is associated with increased risks of cardiovascular morbidity and all-cause mortality. While existing diagnostic approaches can roughly classify OSA severity or detect isolated respiratory events, they lack the precision and comprehensiveness required for high resolution, event level diagnosis. Here, we present UT OSANet, a deep learning based model designed as a event level, multi scenario diagnostic tool for OSA. This model facilitates detailed identification of events associated with OSA, including apnea, hypopnea, oxygen desaturation, and arousal. Moreover, the model employs flexibly adjustable input modalities such as electroencephalography (EEG), airflow, and SpO 2. It utilizes a random masked modality combination training strategy, allowing it to comprehend cross-modal relationships while sustaining consistent performance across varying modality conditions. This model was trained and evaluated utilizing 9,021 polysomnography (PSG) recordings from five independent datasets. achieving sensitivities up to 0.93 and macro F1 scores of 0.84, 0.85 across home, clinical, and research scenarios. This model serves as an event-level, multi-scenario diagnostic instrument for real-world applications of OSA, while also establishing itself as a means to deepen the mechanistic comprehension of respiratory processes in sleep disorders and their extensive health implications."}
{"id": "2511.15914", "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "categories": ["cs.RO"], "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency."}
{"id": "2511.16260", "pdf": "https://arxiv.org/pdf/2511.16260", "abs": "https://arxiv.org/abs/2511.16260", "authors": ["Hao Wu", "Shanchi Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong"], "title": "Low-Complexity Rydberg Array Reuse: Modeling and Receiver Design for Sparse Channels", "categories": ["eess.SP"], "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency measurements and the high sensitivity to a large range of frequencies makes it attractive for communications reception. However, current implementations of Rydberg array antennas predominantly rely on simple stacking of multiple single-antenna units. While conceptually straightforward, this approach leads to substantial system bulkiness due to the unique requirements of atomic sensors, particularly the need for multiple spatially separated laser setups, rendering such designs both impractical for real-world applications and challenging to fabricate. This limitation underscores the critical need for developing multiplexed Rydberg sensor array architectures. In the domain of conventional RF array antennas, hybrid analog-digital beamforming has emerged as a pivotal architecture for large-scale millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems, as it substantially reduces the hardware complexity associated with fully-digital beamforming while closely approaching its performance. Drawing inspiration from this methodology, we conduct a systematic study in this work on the design principles, equivalent modeling, and precoding strategies for low-complexity multiplexed Rydberg array, an endeavor crucial to enabling practical and scalable quantum-enhanced communication systems."}
{"id": "2511.15956", "pdf": "https://arxiv.org/pdf/2511.15956", "abs": "https://arxiv.org/abs/2511.15956", "authors": ["Aliyah Smith", "Monroe Kennedy"], "title": "The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies."}
{"id": "2511.16277", "pdf": "https://arxiv.org/pdf/2511.16277", "abs": "https://arxiv.org/abs/2511.16277", "authors": ["Manjun Cui", "Ziqi Yan", "Yangfan He", "Zhichao Zhang"], "title": "Dynamic Multiple-Parameter Joint Time-Vertex Fractional Fourier Transform and its Intelligent Filtering Methods", "categories": ["eess.SP"], "comment": null, "summary": "Dynamic graph signal processing provides a principled framework for analyzing time-varying data defined on irregular graph domains. However, existing joint time-vertex transforms such as the joint time-vertex fractional Fourier transform assign only one fractional order to the spatial domain and another one to the temporal domain, thereby restricting their capacity to model the complex and continuously varying dynamics of graph signals. To address this limitation, we propose a novel dynamic multiple-parameter joint time-vertex fractional Fourier transform (DMPJFRFT) framework, which introduces time-varying fractional parameters to achieve adaptive spectral modeling of dynamic graph structures. By assigning distinct fractional orders to each time step, the proposed transform enables dynamic and flexible representation of spatio-temporal signal evolution in the joint time-vertex spectral domain. Theoretical properties of the DMPJFRFT are systematically analyzed, and two filtering approaches: a gradient descent-based method and a neural network-based method, are developed for dynamic signal restoration. Experimental results on dynamic graph and video datasets demonstrate that the proposed framework effectively captures temporal topology variations and achieves superior performance in denoising and deblurring tasks compared with some state-of-the-art graph-based transforms and neural networks."}
{"id": "2511.15995", "pdf": "https://arxiv.org/pdf/2511.15995", "abs": "https://arxiv.org/abs/2511.15995", "authors": ["Zili Tang", "Ying Zhang", "Meng Guo"], "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization", "categories": ["cs.RO"], "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025", "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing."}
{"id": "2511.16327", "pdf": "https://arxiv.org/pdf/2511.16327", "abs": "https://arxiv.org/abs/2511.16327", "authors": ["Deqiao Gan", "Xiaoxia Xu", "Xiaohu Ge", "Yuanwei Liu"], "title": "Revealing computation-communication trade-off in Segmented Pinching Antenna System (PASS)", "categories": ["eess.SP"], "comment": null, "summary": "A joint communication and computation (JCC) framework using segmented pinching antenna system (PASS) is proposed, where both the communication bit streams and computation data are simultaneously transmitted via uplink communications. The segmented PASS design is used to yield the tractable uplink transmission, and to mitigate large-scale path loss and in-waveguide loss. Based on three operating protocols, namely segment selection (SS), segment aggregation (SA), and segment multiplexing (SM), the joint transmit and receive beamforming problem is formulated: 1) The mean square error (MSE) minimization problem is formulated for computation-oriented cases. To address this problem, a low-complexity alternating optimization-minimum mean square error (AO-MMSE) algorithm is developed. This problem is decomposed into receiver-side and transmitter-side MSE subproblems that are iteratively optimized by MMSE receivers to obtain the closed-form solutions. It is mathematically proved that the segmented JCC-PASS framework significantly outperforms the conventional PASS for the average in-waveguide propagation gain. 2) The weighted sum rate (WSR) maximization problem is formulated for communication-oriented cases. To solve the decomposed receiver-side and transmitter-side MSE subproblems, the AO-weighted minimum mean square error (AO-WMMSE) algorithm is further developed. An auxiliary weight variable is introduced to linearize the WSR function and is alternatively optimized based on WMMSE to derive the closed-form solutions. Simulation results demonstrate that: i) The proposed JCC-PASS framework achieves up to 70.65% and 45.32% reductions in MSE compared with conventional MIMO and conventional PASS, and ii) it reaches 87.70% and 51.35% improvements in WSR compared with conventional MIMO and conventional PASS, respectively."}
{"id": "2511.16048", "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency."}
{"id": "2511.16346", "pdf": "https://arxiv.org/pdf/2511.16346", "abs": "https://arxiv.org/abs/2511.16346", "authors": ["Deniz Kasap", "Taraneh Aminosharieh Najafi", "Jérôme Paul Rémy Thevenot", "Jonathan Dan", "Stefano Albini", "David Atienza"], "title": "VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture", "categories": ["eess.SP", "cs.LG", "eess.SY"], "comment": "14 pages, 8 figures", "summary": "We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications."}
{"id": "2511.16050", "pdf": "https://arxiv.org/pdf/2511.16050", "abs": "https://arxiv.org/abs/2511.16050", "authors": ["Takeru Tsunoori", "Masato Kobayashi", "Yuki Uranishi"], "title": "Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers", "categories": ["cs.RO"], "comment": null, "summary": "Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua"}
{"id": "2511.16352", "pdf": "https://arxiv.org/pdf/2511.16352", "abs": "https://arxiv.org/abs/2511.16352", "authors": ["Till-Yannic Müller", "Frederik Zumegen", "Reinhard Wiesmayr", "Emre Gönültaş", "Christoph Studer"], "title": "Neural Positioning Without External Reference", "categories": ["eess.SP", "cs.IT"], "comment": "Submitted to a journal", "summary": "Channel state information (CSI)-based user equipment (UE) positioning with neural networks -- referred to as neural positioning -- is a promising approach for accurate off-device UE localization. Most existing methods train their neural networks with ground-truth position labels obtained from external reference positioning systems, which requires costly hardware and renders label acquisition difficult in large areas. In this work, we propose a novel neural positioning pipeline that avoids the need for any external reference positioning system. Our approach trains the positioning network only using CSI acquired off-device and relative displacement commands executed on commercial off-the-shelf (COTS) robot platforms, such as robotic vacuum cleaners -- such an approach enables inexpensive training of accurate neural positioning functions over large areas. We evaluate our method in three real-world scenarios, ranging from small line-of-sight (LoS) areas to larger non-line-of-sight (NLoS) environments, using CSI measurements acquired in IEEE 802.11 Wi-Fi and 5G New Radio (NR) systems. Our experiments demonstrate that the proposed neural positioning pipeline achieves UE localization accuracies close to state-of-the-art methods that require externally acquired high-precision ground-truth position labels for training."}
{"id": "2511.16158", "pdf": "https://arxiv.org/pdf/2511.16158", "abs": "https://arxiv.org/abs/2511.16158", "authors": ["Lara Bergmann", "Cedric Grothues", "Klaus Neumann"], "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/"}
{"id": "2511.16369", "pdf": "https://arxiv.org/pdf/2511.16369", "abs": "https://arxiv.org/abs/2511.16369", "authors": ["Jaron Fontaine", "Mohammad Cheraghinia", "John Strassner", "Adnan Shahid", "Eli De Poorter"], "title": "Reasoning Meets Representation: Envisioning Neuro-Symbolic Wireless Foundation Models", "categories": ["eess.SP", "cs.NI"], "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG)", "summary": "Recent advances in Wireless Physical Layer Foundation Models (WPFMs) promise a new paradigm of universal Radio Frequency (RF) representations. However, these models inherit critical limitations found in deep learning such as the lack of explainability, robustness, adaptability, and verifiable compliance with physical and regulatory constraints. In addition, the vision for an AI-native 6G network demands a level of intelligence that is deeply embedded into the systems and is trustworthy. In this vision paper, we argue that the neuro-symbolic paradigm, which integrates data-driven neural networks with rule- and logic-based symbolic reasoning, is essential for bridging this gap. We envision a novel Neuro-Symbolic framework that integrates universal RF embeddings with symbolic knowledge graphs and differentiable logic layers. This hybrid approach enables models to learn from large datasets while reasoning over explicit domain knowledge, enabling trustworthy, generalizable, and efficient wireless AI that can meet the demands of future networks."}
{"id": "2511.16200", "pdf": "https://arxiv.org/pdf/2511.16200", "abs": "https://arxiv.org/abs/2511.16200", "authors": ["Kewei Chen", "Yayu Long", "Mingsheng Shang"], "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems."}
{"id": "2511.16472", "pdf": "https://arxiv.org/pdf/2511.16472", "abs": "https://arxiv.org/abs/2511.16472", "authors": ["Niko Lindvall", "Mikko Heino", "Mikko Valkama"], "title": "3-20 GHz Wideband Tightly-Coupled Dual-Polarized Vivaldi Antenna Array", "categories": ["eess.SP"], "comment": null, "summary": "Very wideband apertures are needed in positioning, sensing, spectrum monitoring, and modern spread spectrum, e.g., frequency hopping systems. Vivaldi antennas are one of the prominent choices for the aforementioned systems due to their natural wideband characteristics. Furthermore, tightly-coupled antenna arrays have been researched in the recent years to extend the lower band edge of compact arrays by taking advantage of the strong mutual coupling between the elements especially with dipole elements, but not with dual-polarized Vivaldi antennas. This paper presents a novel tightly-coupled dual-polarized antipodal Vivaldi antenna (TC-AVA) with -6 dB impedance bandwidth of 3 to 20 GHz. The tight coupling by overlapping the Vivaldi leaves is shown to extend the lower band edge from 3.75 to 3 GHz and 2.75 GHz, an improvement of 20% to 25% for both polarizations, compared with an isolated antipodal Vivaldi element."}
{"id": "2511.16223", "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning."}
{"id": "2511.16627", "pdf": "https://arxiv.org/pdf/2511.16627", "abs": "https://arxiv.org/abs/2511.16627", "authors": ["Pengxin Li", "Yimin Zhou", "Jie Min", "Yirong Wang", "Wei Liang", "Wang Li"], "title": "TFCDiff: Robust ECG Denoising via Time-Frequency Complementary Diffusion", "categories": ["eess.SP"], "comment": null, "summary": "Ambulatory electrocardiogram (ECG) readings are prone to mixed noise from physical activities, including baseline wander (BW), muscle artifact (MA), and electrode motion artifact (EM). Developing a method to remove such complex noise and reconstruct high-fidelity signals is clinically valuable for diagnostic accuracy. However, denoising of multi-beat ECG segments remains understudied and poses technical challenges. To address this, we propose Time-Frequency Complementary Diffusion (TFCDiff), a novel approach that operates in the Discrete Cosine Transform (DCT) domain and uses the DCT coefficients of noisy signals as conditioning input. To refine waveform details, we incorporate Temporal Feature Enhancement Mechanism (TFEM) to reinforce temporal representations and preserve key physiological information. Comparative experiments on a synthesized dataset demonstrate that TFCDiff achieves state-of-the-art performance across five evaluation metrics. Furthermore, TFCDiff shows superior generalization on the unseen SimEMG Database, outperforming all benchmark models. Notably, TFCDiff processes raw 10-second sequences and maintains robustness under flexible random mixed noise (fRMN), enabling plug-and-play deployment in wearable ECG monitors for high-motion scenarios. Source code is available at https://github.com/Miroircivil/TFCDiff."}
{"id": "2511.16233", "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models."}
{"id": "2511.15766", "pdf": "https://arxiv.org/pdf/2511.15766", "abs": "https://arxiv.org/abs/2511.15766", "authors": ["Mohit Sharma", "Robbe Van Rompaey", "Wouter Lanneer", "Marc Moonen"], "title": "A Generalized Weighted Overlap-Add (WOLA) Filter Bank for Improved Subband System Identification", "categories": ["eess.AS", "eess.SP"], "comment": "For associated MatLab script: https://github.com/mohit-nith/GeneralizedWOLA-SystemIdentification.git", "summary": "This paper addresses the challenges in short-time Fourier transform (STFT) domain subband adaptive filtering, in particular, subband system identification. Previous studies in this area have primarily focused on setups with subband filtering at a downsampled rate, implemented using the weighted overlap-add (WOLA) filter bank, popular in audio and speech-processing for its reduced complexity. However, this traditional approach imposes constraints on the subband filters when transformed to their full-rate representation. This paper makes three key contributions. First, it introduces a generalized WOLA filter bank that repositions subband filters before the downsampling operation, eliminating the constraints on subband filters inherent in the conventional WOLA filter bank. Second, it investigates the mean square error (MSE) performance of the generalized WOLA filter bank for full-band system identification, establishing analytical ties between the order of subband filters, the full-band system impulse response length, the decimation factor, and the prototype filters. Third, to address the increased computational complexity of the generalized WOLA, the paper proposes a low-complexity implementation termed per-tone weighted overlap-add (PT-WOLA), which maintains computational complexity on par with conventional WOLA. Analytical and empirical evidence demonstrates that the proposed generalized WOLA filter bank significantly enhances the performance of subband system identification."}
{"id": "2511.16262", "pdf": "https://arxiv.org/pdf/2511.16262", "abs": "https://arxiv.org/abs/2511.16262", "authors": ["Oliver Bimber", "Karl Dietrich von Ellenrieder", "Michael Haller", "Rakesh John Amala Arokia Nathan", "Gianni Lunardi", "Marco Camurri", "Mohamed Youssef", "Santos Miguel Orozco Soto", "Jeremy E. Niven"], "title": "How Robot Dogs See the Unseeable", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments."}
{"id": "2511.15838", "pdf": "https://arxiv.org/pdf/2511.15838", "abs": "https://arxiv.org/abs/2511.15838", "authors": ["Meiyi Zhu", "Caili Guo", "Chunyan Feng", "Osvaldo Simeone"], "title": "Attention-Based Feature Online Conformal Prediction for Time Series", "categories": ["cs.LG", "cs.IT", "eess.SP"], "comment": "25 pages, 24 figures", "summary": "Online conformal prediction (OCP) wraps around any pre-trained predictor to produce prediction sets with coverage guarantees that hold irrespective of temporal dependencies or distribution shifts. However, standard OCP faces two key limitations: it operates in the output space using simple nonconformity (NC) scores, and it treats all historical observations uniformly when estimating quantiles. This paper introduces attention-based feature OCP (AFOCP), which addresses both limitations through two key innovations. First, AFOCP operates in the feature space of pre-trained neural networks, leveraging learned representations to construct more compact prediction sets by concentrating on task-relevant information while suppressing nuisance variation. Second, AFOCP incorporates an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively handling non-stationarity and distribution shifts. We provide theoretical guarantees showing that AFOCP maintains long-term coverage while provably achieving smaller prediction intervals than standard OCP under mild regularity conditions. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP consistently reduces the size of prediction intervals by as much as $88\\%$ as compared to OCP, while maintaining target coverage levels, validating the benefits of both feature-space calibration and attention-based adaptive weighting."}
{"id": "2511.16265", "pdf": "https://arxiv.org/pdf/2511.16265", "abs": "https://arxiv.org/abs/2511.16265", "authors": ["Haru Fukatsu", "Ryoji Yasuda", "Yuki Funabora", "Shinji Doki"], "title": "Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist", "categories": ["cs.RO"], "comment": "8 pages, 8 figures. This work has been submitted to the IEEE for possible publication", "summary": "This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications."}
{"id": "2511.16081", "pdf": "https://arxiv.org/pdf/2511.16081", "abs": "https://arxiv.org/abs/2511.16081", "authors": ["Huseyin Goksu"], "title": "L-JacobiNet and S-JacobiNet: An Analysis of Adaptive Generalization, Stabilization, and Spectral Domain Trade-offs in GNNs", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Spectral GNNs, like ChebyNet, are limited by heterophily and over-smoothing due to their static, low-pass filter design. This work investigates the \"Adaptive Orthogonal Polynomial Filter\" (AOPF) class as a solution. We introduce two models operating in the [-1, 1] domain: 1) `L-JacobiNet`, the adaptive generalization of `ChebyNet` with learnable alpha, beta shape parameters, and 2) `S-JacobiNet`, a novel baseline representing a LayerNorm-stabilized static `ChebyNet`. Our analysis, comparing these models against AOPFs in the [0, infty) domain (e.g., `LaguerreNet`), reveals critical, previously unknown trade-offs. We find that the [0, infty) domain is superior for modeling heterophily, while the [-1, 1] domain (Jacobi) provides superior numerical stability at high K (K>20). Most significantly, we discover that `ChebyNet`'s main flaw is stabilization, not its static nature. Our static `S-JacobiNet` (ChebyNet+LayerNorm) outperforms the adaptive `L-JacobiNet` on 4 out of 5 benchmark datasets, identifying `S-JacobiNet` as a powerful, overlooked baseline and suggesting that adaptation in the [-1, 1] domain can lead to overfitting."}
{"id": "2511.16306", "pdf": "https://arxiv.org/pdf/2511.16306", "abs": "https://arxiv.org/abs/2511.16306", "authors": ["Lasse Hohmeyer", "Mihaela Popescu", "Ivan Bergonzani", "Dennis Mronga", "Frank Kirchner"], "title": "InEKFormer: A Hybrid State Estimator for Humanoid Robots", "categories": ["cs.RO"], "comment": "Accepted at The 22nd International Conference on Advanced Robotics (ICAR 2025)", "summary": "Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems."}
{"id": "2511.16101", "pdf": "https://arxiv.org/pdf/2511.16101", "abs": "https://arxiv.org/abs/2511.16101", "authors": ["Huseyin Goksu"], "title": "HybSpecNet: A Critical Analysis of Architectural Instability in Hybrid-Domain Spectral GNNs", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Spectral Graph Neural Networks offer a principled approach to graph filtering but face a fundamental \"Stability-vs-Adaptivity\" trade-off. This trade-off is dictated by the choice of spectral domain. Filters in the finite [-1, 1] domain (e.g., ChebyNet) are numerically stable at high polynomial degrees (K) but are static and low-pass, causing them to fail on heterophilic graphs. Conversely, filters in the semi-infinite [0, infty) domain (e.g., KrawtchoukNet) are highly adaptive and achieve SOTA results on heterophily by learning non-low-pass responses. However, as we demonstrate, these adaptive filters can also suffer from numerical instability, leading to catastrophic performance collapse at high K. In this paper, we propose to resolve this trade-off by designing a hybrid-domain GNN, HybSpecNet, which combines a stable `ChebyNet` branch with an adaptive `KrawtchoukNet` branch. We first demonstrate that a \"naive\" hybrid architecture, which fuses the branches via concatenation, successfully unifies performance at low K, achieving strong results on both homophilic and heterophilic benchmarks. However, we then prove that this naive architecture fails the stability test. Our K-ablation experiments show that this architecture catastrophically collapses at K=25, exactly mirroring the collapse of its unstable `KrawtchoukNet` branch. We identify this critical finding as \"Instability Poisoning,\" where `NaN`/`Inf` gradients from the adaptive branch destroy the training of the model. Finally, we propose and validate an advanced architecture that uses \"Late Fusion\" to completely isolate the gradient pathways. We demonstrate that this successfully solves the instability problem, remaining perfectly stable up to K=30 while retaining its SOTA performance across all graph types. This work identifies a critical architectural pitfall in hybrid GNN design and provides the robust architectural solution."}
{"id": "2511.16330", "pdf": "https://arxiv.org/pdf/2511.16330", "abs": "https://arxiv.org/abs/2511.16330", "authors": ["Shreyas Kumar", "Ravi Prakash"], "title": "Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments."}
{"id": "2511.16126", "pdf": "https://arxiv.org/pdf/2511.16126", "abs": "https://arxiv.org/abs/2511.16126", "authors": ["Ryo Aihara", "Yoshiki Masuyama", "Francesco Paissan", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "SUNAC: Source-aware Unified Neural Audio Codec", "categories": ["eess.AS", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Neural audio codecs (NACs) provide compact representations that can be leveraged in many downstream applications, in particular large language models. Yet most NACs encode mixtures of multiple sources in an entangled manner, which may impede efficient downstream processing in applications that need access to only a subset of the sources (e.g., analysis of a particular type of sound, transcription of a given speaker, etc). To address this, we propose a source-aware codec that encodes individual sources directly from mixtures, conditioned on source type prompts. This enables user-driven selection of which source(s) to encode, including separately encoding multiple sources of the same type (e.g., multiple speech signals). Experiments show that our model achieves competitive resynthesis and separation quality relative to a cascade of source separation followed by a conventional NAC, with lower computational cost."}
{"id": "2511.16372", "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers."}
{"id": "2511.16597", "pdf": "https://arxiv.org/pdf/2511.16597", "abs": "https://arxiv.org/abs/2511.16597", "authors": ["Ivana Nikoloska", "Osvaldo Simeone"], "title": "Variational Quantum Integrated Sensing and Communication", "categories": ["quant-ph", "cs.IT", "cs.LG", "eess.SP"], "comment": "Submitted for publication", "summary": "The integration of sensing and communication functionalities within a common system is one of the main innovation drivers for next-generation networks. In this paper, we introduce a quantum integrated sensing and communication (QISAC) protocol that leverages entanglement in quantum carriers of information to enable both superdense coding and quantum sensing. The proposed approach adaptively optimizes encoding and quantum measurement via variational circuit learning, while employing classical machine learning-based decoders and estimators to process the measurement outcomes. Numerical results for qudit systems demonstrate that the proposed QISAC protocol can achieve a flexible trade-off between classical communication rate and accuracy of parameter estimation."}
{"id": "2511.16390", "pdf": "https://arxiv.org/pdf/2511.16390", "abs": "https://arxiv.org/abs/2511.16390", "authors": ["Ajith Anil Meera", "Poppy Collis", "Polina Arbuzova", "Abián Torres", "Paul F Kinghorn", "Ricardo Sanz", "Pablo Lanillos"], "title": "Robot Metacognition: Decision Making with Confidence for Tool Invention", "categories": ["cs.RO", "cs.AI"], "comment": "under review", "summary": "Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition."}
{"id": "2511.16406", "pdf": "https://arxiv.org/pdf/2511.16406", "abs": "https://arxiv.org/abs/2511.16406", "authors": ["Luis Luna", "Isaac Chairez", "Andrey Polyakov"], "title": "Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators", "categories": ["cs.RO", "nlin.AO"], "comment": null, "summary": "Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments."}
{"id": "2511.16407", "pdf": "https://arxiv.org/pdf/2511.16407", "abs": "https://arxiv.org/abs/2511.16407", "authors": ["Xizhou Bu", "Jiexi Lyu", "Fulei Sun", "Ruichen Yang", "Zhiqiang Ma", "Wei Li"], "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints", "categories": ["cs.RO"], "comment": "Code can be found at https://github.com/XizoB/LAOF", "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels."}
{"id": "2511.16434", "pdf": "https://arxiv.org/pdf/2511.16434", "abs": "https://arxiv.org/abs/2511.16434", "authors": ["Chenming Wu", "Xiaofan Li", "Chengkai Dai"], "title": "From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization", "categories": ["cs.RO"], "comment": "Technical report (7 pages)", "summary": "The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\\textit{\\underline{S}upport-\\underline{E}ffective \\underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices."}
{"id": "2511.16518", "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied."}
{"id": "2511.16651", "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "categories": ["cs.RO"], "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research."}
{"id": "2511.16661", "pdf": "https://arxiv.org/pdf/2511.16661", "abs": "https://arxiv.org/abs/2511.16661", "authors": ["Irmak Guzey", "Haozhi Qi", "Julen Urain", "Changhao Wang", "Jessica Yin", "Krishna Bodduluri", "Mike Lambeta", "Lerrel Pinto", "Akshara Rai", "Jitendra Malik", "Tingfan Wu", "Akash Sharma", "Homanga Bharadhwaj"], "title": "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io."}
{"id": "2511.16026", "pdf": "https://arxiv.org/pdf/2511.16026", "abs": "https://arxiv.org/abs/2511.16026", "authors": ["Mohamed Abdallah Salem", "Hamdy Ahmed Ashur", "Ahmed Elshinnawy"], "title": "Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing."}
{"id": "2511.16133", "pdf": "https://arxiv.org/pdf/2511.16133", "abs": "https://arxiv.org/abs/2511.16133", "authors": ["Taejun Kim", "Youngbo Aram Shim", "Geehyuk Lee"], "title": "Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display", "categories": ["cs.HC", "cs.RO"], "comment": "ACM CHI 2021", "summary": "Beyond a simple notification of incoming calls or messages, more complex information such as alphabets and digits can be delivered through spatiotemporal tactile patterns (STPs) on a wrist-worn tactile display (WTD) with multiple tactors. However, owing to the limited skin area and spatial acuity of the wrist, frequent confusions occur between closely located tactors, resulting in a low recognition accuracy. Furthermore, the accuracies reported in previous studies have mostly been measured for a specific posture and could further decrease with free arm postures in real life. Herein, we present Heterogeneous Stroke, a design concept for improving the recognition accuracy of STPs on a WTD. By assigning unique vibrotactile stimuli to each tactor, the confusion between tactors can be reduced. Through our implementation of Heterogeneous Stroke, the alphanumeric characters could be delivered with high accuracy (93.8% for 26 alphabets and 92.4% for 10 digits) across different arm postures."}
{"id": "2511.16144", "pdf": "https://arxiv.org/pdf/2511.16144", "abs": "https://arxiv.org/abs/2511.16144", "authors": ["Sibaek Lee", "Seongbo Ha", "Kyeongsu Kang", "Joonyeol Choi", "Seungjun Tak", "Hyeonwoo Yu"], "title": "LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM", "categories": ["cs.CV", "cs.RO"], "comment": "18 pages", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS."}
{"id": "2511.16347", "pdf": "https://arxiv.org/pdf/2511.16347", "abs": "https://arxiv.org/abs/2511.16347", "authors": ["Chunyang Li", "Zifeng Kang", "Junwei Zhang", "Zhuo Ma", "Anda Cheng", "Xinghua Li", "Jianfeng Ma"], "title": "The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks", "categories": ["cs.CR", "cs.CY", "cs.RO"], "comment": null, "summary": "The adoption of Vision-Language Models (VLMs) in embodied AI agents, while being effective, brings safety concerns such as jailbreaking. Prior work have explored the possibility of directly jailbreaking the embodied agents through elaborated multi-modal prompts. However, no prior work has studied or even reported indirect jailbreaks in embodied AI, where a black-box attacker induces a jailbreak without issuing direct prompts to the embodied agent. In this paper, we propose, for the first time, indirect environmental jailbreak (IEJ), a novel attack to jailbreak embodied AI via indirect prompt injected into the environment, such as malicious instructions written on a wall. Our key insight is that embodied AI does not ''think twice'' about the instructions provided by the environment -- a blind trust that attackers can exploit to jailbreak the embodied agent. We further design and implement open-source prototypes of two fully-automated frameworks: SHAWSHANK, the first automatic attack generation framework for the proposed attack IEJ; and SHAWSHANK-FORGE, the first automatic benchmark generation framework for IEJ. Then, using SHAWSHANK-FORGE, we automatically construct SHAWSHANK-BENCH, the first benchmark for indirectly jailbreaking embodied agents. Together, our two frameworks and one benchmark answer the questions of what content can be used for malicious IEJ instructions, where they should be placed, and how IEJ can be systematically evaluated. Evaluation results show that SHAWSHANK outperforms eleven existing methods across 3,957 task-scene combinations and compromises all six tested VLMs. Furthermore, current defenses only partially mitigate our attack, and we have responsibly disclosed our findings to all affected VLM vendors."}
{"id": "2511.16593", "pdf": "https://arxiv.org/pdf/2511.16593", "abs": "https://arxiv.org/abs/2511.16593", "authors": ["Diaeddin Rimawi"], "title": "Green Resilience of Cyber-Physical Systems: Doctoral Dissertation", "categories": ["cs.SE", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS."}
