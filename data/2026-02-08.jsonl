{"id": "2602.05034", "pdf": "https://arxiv.org/pdf/2602.05034", "abs": "https://arxiv.org/abs/2602.05034", "authors": ["Fatih Ayten", "Musa Furkan Keskin", "Akshay Jain", "Mehmet C. Ilter", "Ossi Kaltiokallio", "Jukka Talvitie", "Elena Simona Lohan", "Mikko Valkama"], "title": "Phase-Only Positioning in Distributed MIMO Under Phase Impairments: AP Selection Using Deep Learning", "categories": ["eess.SP", "eess.AS"], "comment": null, "summary": "Carrier phase positioning (CPP) can enable cm-level accuracy in next-generation wireless systems, while recent literature shows that accuracy remains high using phase-only measurements in distributed MIMO (D-MIMO). However, the impact of phase synchronization errors on such systems remains insufficiently explored. To address this gap, we first show that the proposed hyperbola intersection method achieves highly accurate positioning even in the presence of phase synchronization errors, when trained on appropriate data reflecting such impairments. We then introduce a deep learning (DL)-based D-MIMO antenna point (AP) selection framework that ensures high-precision localization under phase synchronization errors. Simulation results show that the proposed framework improves positioning accuracy compared to prior-art methods, while reducing inference complexity by approximately 19.7%."}
{"id": "2602.05010", "pdf": "https://arxiv.org/pdf/2602.05010", "abs": "https://arxiv.org/abs/2602.05010", "authors": ["Maia Stiber", "Sameer Khan", "Russell Taylor", "Chien-Ming Huang"], "title": "Signal or 'Noise': Human Reactions to Robot Errors in the Wild", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but \"noisy.\" We discuss lessons, benefits, and challenges for using social signals in real-world HRI."}
{"id": "2602.05209", "pdf": "https://arxiv.org/pdf/2602.05209", "abs": "https://arxiv.org/abs/2602.05209", "authors": ["Zhiyu Chen", "Ming-Min Zhao", "Songfu Cai", "Ming Lei", "Min-Jian Zhao"], "title": "Integrated Sensing, Communication, and Control for UAV-Assisted Mobile Target Tracking", "categories": ["eess.SP", "cs.IT"], "comment": "13 pages, 10 figures", "summary": "Unmanned aerial vehicles (UAVs) are increasingly deployed in mission-critical applications such as target tracking, where they must simultaneously sense dynamic environments, ensure reliable communication, and achieve precise control. A key challenge here is to jointly guarantee tracking accuracy, communication reliability, and control stability within a unified framework. To address this issue, we propose an integrated sensing, communication, and control (ISCC) framework for UAV-assisted target tracking, where the considered tracking system is modeled as a discrete-time linear control process, with the objective of driving the deviation between the UAV and target states toward zero. We formulate a stochastic model predictive control (MPC) optimization problem for joint control and beamforming design, which is highly non-convex and intractable in its original form. To overcome this difficulty, the target state is first estimated using an extended Kalman filter (EKF). Then, by deriving the closed-form optimal beamforming solution under a given control input, the original problem is equivalently reformulated into a tractable control-oriented form. Finally, we convexify the remaining non-convex constraints via a relaxation-based convex approximation, yielding a computationally tractable convex optimization problem that admits efficient global solution. Numerical results show that the proposed ISCC framework achieves tracking accuracy comparable to a non-causal benchmark while maintaining stable communication, and it significantly outperforms the conventional control and tracking method."}
{"id": "2602.05029", "pdf": "https://arxiv.org/pdf/2602.05029", "abs": "https://arxiv.org/abs/2602.05029", "authors": ["Octavio Arriaga", "Proneet Sharma", "Jichen Guo", "Marc Otto", "Siddhant Kadwe", "Rebecca Adam"], "title": "Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L) for review. This version includes the statement required by IEEE for preprints", "summary": "Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments."}
{"id": "2602.05308", "pdf": "https://arxiv.org/pdf/2602.05308", "abs": "https://arxiv.org/abs/2602.05308", "authors": ["Jiwei Qian", "Yee Hui Lee", "Kaixuan Cheng", "Qiqi Dai", "Arda Yalcinkaya", "Mohamed Lokman Mohd Yusof", "James Wang", "Abdulkadir C. Yucel"], "title": "A Migration-Assisted Deep Learning Scheme for Imaging Defects Inside Cylindrical Structures via GPR: A Case Study for Tree Trunks", "categories": ["eess.SP"], "comment": null, "summary": "Ground-penetrating radar (GPR) has emerged as a prominent tool for imaging internal defects in cylindrical structures, such as columns, utility poles, and tree trunks. However, accurately reconstructing both the shape and permittivity of the defects inside cylindrical structures remains challenging due to complex wave scattering phenomena and the limited accuracy of the existing signal processing and deep learning techniques. To address these issues, this study proposes a migration-assisted deep learning scheme for reconstructing the shape and permittivity of defects within cylindrical structures. The proposed scheme involves three stages of GPR data processing. First, a dual-permittivity estimation network extracts the permittivity values of the defect and the cylindrical structure, the latter of which is estimated with the help of a novel structural similarity index measure-based autofocusing technique. Second, a modified Kirchhoff migration incorporating the extracted permittivity of the cylindrical structure maps the signals reflected from the defect to the imaging domain. Third, a shape reconstruction network processes the migrated image to recover the precise shape of the defect. The image of the interior defect is finally obtained by combining the reconstructed shape and extracted permittivity of the defect. The proposed scheme is validated using both synthetic and experimental data from a laboratory trunk model and real tree trunk samples. Comparative results show superior performance over existing deep learning methods, while generalization tests on live trees confirm its feasibility for in-field deployment. The underlying principle can further be applied to other circumferential GPR imaging scenarios. The code and database are available at: https://github.com/jwqian54/Migration-Assisted-DL."}
{"id": "2602.05079", "pdf": "https://arxiv.org/pdf/2602.05079", "abs": "https://arxiv.org/abs/2602.05079", "authors": ["Vinal Asodia", "Iman Sharifi", "Saber Fallah"], "title": "Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking", "categories": ["cs.RO", "cs.LG"], "comment": "12 pages, 7 figures, 5 tables", "summary": "The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving."}
{"id": "2602.05342", "pdf": "https://arxiv.org/pdf/2602.05342", "abs": "https://arxiv.org/abs/2602.05342", "authors": ["Zitong Wang", "Cheng Zhang", "Wen Wang", "Shuigen Yang", "Haiming Wang", "Yongming Huang"], "title": "Joint Optimization of Latency and Accuracy for Split Federated Learning in User-Centric Cell-Free MIMO Networks", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a user-centric split federated learning (UCSFL) framework for user-centric cell-free multiple-input multiple-output (CF-MIMO) networks to support split federated learning (SFL). In the proposed UCSFL framework, users deploy split sub-models locally, while complete models are maintained and updated at access point (AP)-side distributed processing units (DPUs), followed by a two-level aggregation procedure across DPUs and the central processing unit (CPU). Under standard machine learning (ML) assumptions, we provide a theoretical convergence analysis for UCSFL, which reveals that the AP-cluster size is a key factor influencing model training accuracy. Motivated by this result, we introduce a new performance metric, termed the latency-to-accuracy ratio, defined as the ratio of a user's per-iteration training latency to the weighted size of its AP cluster. Based on this metric, we formulate a joint optimization problem to minimize the maximum latency-to-accuracy ratio by jointly optimizing uplink power control, downlink beamforming, model splitting, and AP clustering. The resulting problem is decomposed into two sub-problems operating on different time scales, for which dedicated algorithms are developed to handle the short-term and long-term optimizations, respectively. Simulation results verify the convergence of the proposed algorithms and demonstrate that UCSFL effectively reduces the latency-to-accuracy ratio of the VGG16 model compared with baseline schemes. Moreover, the proposed framework adaptively adjusts splitting and clustering strategies in response to varying communication and computation resources. An MNIST-based handwritten digit classification example further shows that UCSFL significantly accelerates the convergence of the VGG16 model."}
{"id": "2602.05092", "pdf": "https://arxiv.org/pdf/2602.05092", "abs": "https://arxiv.org/abs/2602.05092", "authors": ["Thomas Cohn", "Lihan Tang", "Alexandre Amice", "Russ Tedrake"], "title": "A Framework for Combining Optimization-Based and Analytic Inverse Kinematics", "categories": ["cs.RO"], "comment": "19 pages, 5 figures, 6 tables. Under submission", "summary": "Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability."}
{"id": "2602.05554", "pdf": "https://arxiv.org/pdf/2602.05554", "abs": "https://arxiv.org/abs/2602.05554", "authors": ["Mohammad Shamsesalehi", "Mahmoud Ahmadian Attari", "Mohammad Amin Maleki Sadr", "Benoit Champagne"], "title": "Beamformed Fingerprint-Based Transformer Network for Trajectory Estimation and Path Determination in Outdoor mmWave MIMO Systems", "categories": ["eess.SP"], "comment": "14 pages, 11 figures", "summary": "Radio transmissions in millimeter wave (mmWave) bands have gained significant interest for applications demanding precise device localization and trajectory estimation. This paper explores novel neural network (NN) architectures suitable for trajectory estimation and path determination in a mmWave multiple-input multiple-output (MIMO) outdoor system based on localization data from beamformed fingerprint (BFF). The NN architecture captures sequences of BFF signals from different users, and through the application of learning mechanisms, subsequently estimate their trajectories. In turn, this information is employed to find the shortest path to the target, thereby enabling more efficient navigation. Specifically, we propose a two-stage procedure for trajectory estimation and optimal path finding. In the first stage, a transformer network (TN) based on attention mechanisms is developed to predict trajectories of wireless devices using BFF sequences captured in a mmWave MIMO outdoor system. In the second stage, a novel algorithm based on Informed Rapidly-exploring Random Trees (iRRT*) is employed to determine the optimal path to target locations using trajectory estimates derived in the first stage. The effectiveness of the proposed schemes is validated through numerical experiments, using a comprehensive dataset of radio measurements, generated using ray tracing simulations to model outdoor propagation at 28 GHz. We show that our proposed TN-based trajectory estimator outperforms other methods from the recent literature and can successfully generalize to new trajectories outside the training set. Furthermore, our proposed iRRT* algorithm is able to consistently provide the shortest path to the target."}
{"id": "2602.05156", "pdf": "https://arxiv.org/pdf/2602.05156", "abs": "https://arxiv.org/abs/2602.05156", "authors": ["Dong Ho Kang", "Aaron Kim", "Mingyo Seo", "Kazuto Yokoyama", "Tetsuya Narita", "Luis Sentis"], "title": "PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation."}
{"id": "2602.05560", "pdf": "https://arxiv.org/pdf/2602.05560", "abs": "https://arxiv.org/abs/2602.05560", "authors": ["Yangjin Xu", "Wei Gao", "Xiaolei Li", "Qinghang Zeng"], "title": "Depth estimation of a monoharmonic source using a vertical linear array at fixed distance", "categories": ["eess.SP", "physics.ao-ph"], "comment": null, "summary": "Estimating the depth of a monoharmonic sound source at a fixed range using a vertical linear array (VLA) is challenging in the absence of seabed environmental parameters, and relevant research remains scarce. The orthogonality constrained modal search based depth estimation (OCMS-D) method is proposed in this paper, which enables the estimation of the depth of a monoharmonic source at a fixed range using a VLA under unknown seabed parameters. Using the sparsity of propagating normal modes and the orthogonality of mode depth functions, OCMS-D estimates the normal mode parameters under a fixed source-array distance at first. The estimated normal mode parameters are then used to estimate the source depth. To ensure the precision of the source depth estimation, the method utilizes information on both the amplitude distribution and the sign (positive/negative) patterns of the estimated mode depth functions at the inferred source depth. Numerical simulations evaluate the performance of OCMS-D under different conditions. The effectiveness of OCMS-D is also verified by the Yellow Sea experiment and the SWellEx-96 experiment. In the Yellow Sea experiment, the depth estimation absolute errors by OCMS-D with a 4-second time window are less than 2.4 m. And the depth estimation absolute errors in the SWellEx-96 experiment with a 10-second time window are less than 5.4 m for the shallow source and less than 10.8 m for the deep source."}
{"id": "2602.05198", "pdf": "https://arxiv.org/pdf/2602.05198", "abs": "https://arxiv.org/abs/2602.05198", "authors": ["Kalvik Jakkala", "Saurav Agarwal", "Jason O'Kane", "Srinivas Akella"], "title": "Informative Path Planning with Guaranteed Estimation Uncertainty", "categories": ["cs.RO"], "comment": "16 pages, 11 figures, preprint", "summary": "Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.\n  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility."}
{"id": "2602.05579", "pdf": "https://arxiv.org/pdf/2602.05579", "abs": "https://arxiv.org/abs/2602.05579", "authors": ["Mu Jia", "Hao Sun", "Junting Chen", "Pooi-Yuen Kam"], "title": "Physics-Aware Tensor Reconstruction for Radio Maps in Pixel-Based Fluid Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "The deployment of pixel-based antennas and fluid antenna systems (FAS) is hindered by prohibitive channel state information (CSI) acquisition overhead. While radio maps enable proactive mode selection, reconstructing high-fidelity maps from sparse measurements is challenging. Existing physics-agnostic or data-driven methods often fail to recover fine-grained shadowing details under extreme sparsity. We propose a Physics-Regularized Low-Rank Tensor Completion (PR-LRTC) framework for radio map reconstruction. By modeling the signal field as a three-way tensor, we integrate environmental low-rankness with deterministic antenna physics. Specifically, we leverage Effective Aerial Degrees-of-Freedom (EADoF) theory to derive a differential gain topology map as a physical prior for regularization. The resulting optimization problem is solved via an efficient Alternating Direction Method of Multipliers (ADMM)-based algorithm. Simulations show that PR-LRTC achieves a 4 dB gain over baselines at a 10% sampling ratio. It effectively preserves sharp shadowing edges, providing a robust, physics-compliant solution for low-overhead beam management."}
{"id": "2602.05233", "pdf": "https://arxiv.org/pdf/2602.05233", "abs": "https://arxiv.org/abs/2602.05233", "authors": ["Wenbo Wang", "Fangyun Wei", "QiXiu Li", "Xi Chen", "Yaobo Liang", "Chang Xu", "Jiaolong Yang", "Baining Guo"], "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."}
{"id": "2602.05581", "pdf": "https://arxiv.org/pdf/2602.05581", "abs": "https://arxiv.org/abs/2602.05581", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Xin Tong", "Zhaohui Yang", "Chongwen Huang"], "title": "Physics-Inspired Target Shape Detection and Reconstruction in mmWave Communication Systems", "categories": ["eess.SP"], "comment": "Accepted by GLOBECOM 2023", "summary": "The integration of sensing and communication (ISAC) is an essential function of future wireless systems. Due to its large available bandwidth, millimeter-wave (mmWave) ISAC systems are able to achieve high sensing accuracy. In this paper, we consider the multiple base-station (BS) collaborative sensing problem in a multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) mmWave communication system. Our aim is to sense a remote target shape with the collected signals which consist of both the reflection and scattering signals. We first characterize the mmWave's scattering and reflection effects based on the Lambertian scattering model. Then we apply the periodogram technique to obtain rough scattering point detection, and further incorporate the subspace method to achieve more precise scattering and reflection point detection. Based on these, a reconstruction algorithm based on Hough Transform and principal component analysis (PCA) is designed for a single convex polygon target scenario. To improve the accuracy and completeness of the reconstruction results, we propose a method to further fuse the scattering and reflection points. Extensive simulation results validate the effectiveness of the proposed algorithms."}
{"id": "2602.05265", "pdf": "https://arxiv.org/pdf/2602.05265", "abs": "https://arxiv.org/abs/2602.05265", "authors": ["Kalvik Jakkala", "Jason O'Kane"], "title": "Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments."}
{"id": "2602.05715", "pdf": "https://arxiv.org/pdf/2602.05715", "abs": "https://arxiv.org/abs/2602.05715", "authors": ["Yuyang Liu", "Johan Karlsson", "Filip Elvander"], "title": "Sound Field Estimation Using Optimal Transport Barycenters in the Presence of Phase Errors", "categories": ["eess.SP"], "comment": null, "summary": "This study introduces a novel approach for estimating plane-wave coefficients in sound field reconstruction, specifically addressing challenges posed by error-in-variable phase perturbations. Such systematic errors typically arise from sensor mis-calibration, including uncertainties in sensor positions and response characteristics, leading to measurement-induced phase shifts in plane wave coefficients. Traditional methods often result in biased estimates or non-convex solutions. To overcome these issues, we propose an optimal transport (OT) framework. This framework operates on a set of lifted non-negative measures that correspond to observation-dependent shifted coefficients relative to the unperturbed ones. By applying OT, the supports of the measures are transported toward an optimal average in the phase space, effectively morphing them into an indistinguishable state. This optimal average, known as barycenter, is linked to the estimated plane-wave coefficients using the same lifting rule. The framework addresses the ill-posed nature of the problem, due to the large number of plane waves, by adding a constant to the ground cost, ensuring the sparsity of the transport matrix. Convex consistency of the solution is maintained. Simulation results confirm that our proposed method provides more accurate coefficient estimations compared to baseline approaches in scenarios with both additive noise and phase perturbations."}
{"id": "2602.05273", "pdf": "https://arxiv.org/pdf/2602.05273", "abs": "https://arxiv.org/abs/2602.05273", "authors": ["Hengxuan Xu", "Fengbo Lan", "Zhixin Zhao", "Shengjie Wang", "Mengqiao Liu", "Jieqian Sun", "Yu Cheng", "Tao Zhang"], "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions", "categories": ["cs.RO"], "comment": "14 pages, 10 figures, 8 tables", "summary": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for \"I'm thirsty\") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."}
{"id": "2602.05724", "pdf": "https://arxiv.org/pdf/2602.05724", "abs": "https://arxiv.org/abs/2602.05724", "authors": ["Shoma Hara", "Takumi Takahashi", "Hiroki Iimori", "Hideki Ochiai", "Erik G. Larsson"], "title": "Reciprocity Calibration of Dual-Antenna Repeaters via MMSE Estimation", "categories": ["eess.SP", "cs.IT"], "comment": "13 pages, 9 figures", "summary": "This paper proposes a novel Bayesian reciprocity calibration method that consistently ensures uplink and downlink channel reciprocity in repeater-assisted multiple-input multiple-output (MIMO) systems. The proposed algorithm is formulated under the minimum mean-square error (MMSE) criterion. Its Bayesian framework incorporates complete statistical knowledge of the signal model, noise, and prior distributions, enabling a coherent design that achieves both low computational complexity and high calibration accuracy. To further enhance phase alignment accuracy, which is critical for calibration tasks, we develop a von Mises denoiser that exploits the fact that the target parameters lie on the circle in the complex plane. Simulation results demonstrate that the proposed MMSE algorithm achieves substantially improved estimation accuracy compared with conventional deterministic non-linear least-squares (NLS) methods, while maintaining comparable computational complexity. Furthermore, the proposed method exhibits remarkably fast convergence, making it well suited for practical implementation."}
{"id": "2602.05310", "pdf": "https://arxiv.org/pdf/2602.05310", "abs": "https://arxiv.org/abs/2602.05310", "authors": ["Jipeng Kong", "Xinzhe Liu", "Yuhang Lin", "Jinrui Han", "Sören Schwertfeger", "Chenjia Bai", "Xuelong Li"], "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework", "categories": ["cs.RO"], "comment": "13 pages, 9 figures, conference", "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/."}
{"id": "2602.05802", "pdf": "https://arxiv.org/pdf/2602.05802", "abs": "https://arxiv.org/abs/2602.05802", "authors": ["Niclas Führling", "Getuar Rexhepi", "Giuseppe Abreu"], "title": "Discrete Aware Tensor Completion via Convexized $\\ell_0$-Norm Approximation", "categories": ["eess.SP", "cs.IT"], "comment": null, "summary": "We consider a novel algorithm, for the completion of partially observed low-rank tensors, where each entry of the tensor can be chosen from a discrete finite alphabet set, such as in common image processing problems, where the entries represent the RGB values. The proposed low-rank tensor completion (TC) method builds on the conventional nuclear norm (NN) minimization-based low-rank TC paradigm, through the addition of a discrete-aware regularizer, which enforces discreteness in the objective of the problem, by an $\\ell_0$-norm regularizer that is approximated by a continuous and differentiable function normalized via fractional programming (FP) under a proximal gradient (PG) framework, in order to solve the proposed problem. Simulation results demonstrate the superior performance of the new method both in terms of normalized mean square error (NMSE) and convergence, compared to the conventional state of-the-art (SotA) techniques, including NN minimization approaches, as well as a mixture of the latter with a matrix factorization approach."}
{"id": "2602.05325", "pdf": "https://arxiv.org/pdf/2602.05325", "abs": "https://arxiv.org/abs/2602.05325", "authors": ["Jiacheng Fan", "Zhiyue Zhao", "Yiqian Zhang", "Chao Chen", "Peide Wang", "Hengdi Zhang", "Zhengxue Cheng"], "title": "RoboPaint: From Human Demonstration to Any Robot and Any View", "categories": ["cs.RO"], "comment": "17 pages", "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."}
{"id": "2602.05876", "pdf": "https://arxiv.org/pdf/2602.05876", "abs": "https://arxiv.org/abs/2602.05876", "authors": ["Chenyang Yan", "Mats Bengtsson"], "title": "IDSOR: Intensity- and Distance-Aware Statistical Outlier Removal for Weather-Robust LiDAR Point Clouds", "categories": ["eess.SP"], "comment": null, "summary": "LiDAR point clouds captured in rain or snow are often corrupted by weather-induced returns, which can degrade perception and safety-critical scene understanding. This paper proposes Intensity- and Distance-Aware Statistical Outlier Removal (IDSOR), a range-adaptive filtering method that jointly exploits intensity cues and neighborhood sparsity. By incorporating an empirical, range-dependent distribution of weather returns into the threshold design, IDSOR suppresses weather-induced points while preserving fine structural details without cumbersome manual parameter tuning. We also propose a variant that uses a previously proposed method to estimate the weather return distribution from data, and integrates it into IDSOR. Experiments on simulation-augmented level-crossing measurements and on the Winter Adverse Driving dataset (WADS) demonstrate that IDSOR achieves a favorable precision-recall trade-off, maintaining both precision and recall above 90% on WADS."}
{"id": "2602.05441", "pdf": "https://arxiv.org/pdf/2602.05441", "abs": "https://arxiv.org/abs/2602.05441", "authors": ["Dean Fortier", "Timothy Adamson", "Tess Hellebrekers", "Teresa LaScala", "Kofi Ennin", "Michael Murray", "Andrey Kolobov", "Galen Mullins"], "title": "Benchmarking Affordance Generalization with BusyBox", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.\n  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox."}
{"id": "2602.04971", "pdf": "https://arxiv.org/pdf/2602.04971", "abs": "https://arxiv.org/abs/2602.04971", "authors": ["Minjie Tang", "Photios A. Stavrou", "Marios Kountouris"], "title": "Multi-Sensor Scheduling for Remote State Estimation over Wireless MIMO Fading Channels with Semantic Over-the-Air Aggregation", "categories": ["eess.SY", "eess.SP"], "comment": null, "summary": "In this work, we study multi-sensor scheduling for remote state estimation over wireless multiple-input multiple-output (MIMO) fading channels using a novel semantic over-the-air (SemOTA) aggregation approach. We first revisit Kalman filtering with conventional over-the-air (OTA) aggregation and highlight its transmit power limitations. To balance power efficiency and estimation performance, we formulate the scheduling task as a finite-horizon dynamic programming (DP) problem. By analyzing the structure of the optimal Q-function, we show that the resulting scheduling policy exhibits a semantic structure that adapts online to the estimation error covariance and channel variations. To obtain a practical solution, we derive a tractable upper bound on the Q-function via a positive semidefinite (PSD) cone decomposition, which enables an efficient approximate scheduling policy and a low-complexity remote estimation algorithm. Numerical results confirm that the proposed scheme outperforms existing methods in both estimation accuracy and power efficiency."}
{"id": "2602.05456", "pdf": "https://arxiv.org/pdf/2602.05456", "abs": "https://arxiv.org/abs/2602.05456", "authors": ["Maksym Figat", "Ryan M. Mackey", "Michel D. Ingham"], "title": "Ontology-Driven Robotic Specification Synthesis", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "8 pages, 9 figures, 3 tables, journal", "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future."}
{"id": "2602.05517", "pdf": "https://arxiv.org/pdf/2602.05517", "abs": "https://arxiv.org/abs/2602.05517", "authors": ["Sergio Angulo Cosín", "Javier Junquera-Sánchez", "Carlos Hernando-Ramiro", "José-Antonio Gómez-Sánchez"], "title": "GNSS SpAmming: a spoofing-based GNSS denial-of-service attack", "categories": ["cs.CR", "eess.SP"], "comment": null, "summary": "GNSSs are vulnerable to attacks of two kinds: jamming (i.e. denying access to the signal) and spoofing (i.e. impersonating a legitimate satellite). These attacks have been extensively studied, and we have a myriad of countermeasures to mitigate them. In this paper we expose a new type of attack: SpAmming, which combines both approaches to achieve the same effects in a more subtle way.\n  Exploiting the CDMA multiplexing present in most GNSSs, and through a spoofing attack, this approach leads the receiver to lose access to the signal of a legitimate satellite, which would be equivalent to a denial of service; but in this case the existing countermeasures against jamming or spoofing would not allow safeguarding its effectiveness, as it is neither of them.\n  An experimental proof-of-concept is presented in which its impact is evaluated as a function of the previous state of the receiver. Using an SDR-based system developed at the Space Security Centre, the attack is executed against a cold-started receiver, a warm-started receiver, and a receiver that has already acquired the PVT solution and is navigating. Different attack configurations are also tested, starting from a raw emission of the false signal, to surgical Doppler effect configuration, code offset, etc. Although it is shown to be particularly successful against cold-started receivers, the results show that it is also effective in other scenarios, especially if accompanied by other attacks. We will conclude the article by outlining possible countermeasures to detect and, eventually, counteract it; and possible avenues of research to better understand its impact, especially for authenticated services such as OSNMA, and to characterize it in order to improve the response to similar attacks."}
{"id": "2602.05468", "pdf": "https://arxiv.org/pdf/2602.05468", "abs": "https://arxiv.org/abs/2602.05468", "authors": ["Pranav Ponnivalavan", "Satoshi Funabashi", "Alexander Schmitz", "Tetsuya Ogata", "Shigeki Sugano"], "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation", "categories": ["cs.RO"], "comment": "8 pages, 8 figures, 8 tables, ICRA2026 accepted", "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation."}
{"id": "2602.05666", "pdf": "https://arxiv.org/pdf/2602.05666", "abs": "https://arxiv.org/abs/2602.05666", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Li Chen", "Yi Gong", "Chengwen Xing"], "title": "Low-complexity Design for Beam Coverage in Near-field and Far-field: A Fourier Transform Approach", "categories": ["cs.IT", "eess.SP"], "comment": "13 pages, 7 figures, submitted to IEEE for possible publication", "summary": "In this paper, we study efficient beam coverage design for multi-antenna systems in both far-field and near-field cases. To reduce the computational complexity of existing sampling-based optimization methods, we propose a new low-complexity yet efficient beam coverage design. To this end, we first formulate a general beam coverage optimization problem to maximize the worst-case beamforming gain over a target region. For the far-field case, we show that the beam coverage design can be viewed as a spatial-frequency filtering problem, where angular coverage can be achieved by weight-shaping in the antenna domain via an inverse FT, yielding an infinite-length weighting sequence. Under the constraint of a finite number of antennas, a surrogate scheme is proposed by directly truncating this sequence, which inevitably introduces a roll-off effect at the angular boundaries, yielding degraded worst-case beamforming gain. To address this issue, we characterize the finite-antenna-induced roll-off effect, based on which a roll-off-aware design with a protective zoom is developed to ensure a flat beamforming-gain profile within the target angular region. Next, we extend the proposed method to the near-field case. Specifically, by applying a first-order Taylor approximation to the near-field channel steering vector (CSV), the two-dimensional (2D) beam coverage design (in both angle and inverse-range) can be transformed into a 2D inverse FT, leading to a low-complexity beamforming design. Furthermore, an inherent near-field range defocusing effect is observed, indicating that sufficiently wide angular coverage results in range-insensitive beam steering. Finally, numerical results demonstrate that the proposed FT-based approach achieves a comparable worst-case beamforming performance with that of conventional sampling-based optimization methods while significantly reducing the computational complexity."}
{"id": "2602.05513", "pdf": "https://arxiv.org/pdf/2602.05513", "abs": "https://arxiv.org/abs/2602.05513", "authors": ["Xukun Li", "Yu Sun", "Lei Zhang", "Bosheng Huang", "Yibo Peng", "Yuan Meng", "Haojun Jiang", "Shaoxuan Xie", "Guacai Yao", "Alois Knoll", "Zhenshan Bing", "Xinlong Wang", "Zhenguo Sun"], "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter", "categories": ["cs.RO", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories."}
{"id": "2602.05798", "pdf": "https://arxiv.org/pdf/2602.05798", "abs": "https://arxiv.org/abs/2602.05798", "authors": ["Arnau Vilella", "Jasin Machkour", "Michael Muma", "Daniel P. Palomar"], "title": "Learning False Discovery Rate Control via Model-Based Neural Networks", "categories": ["stat.ME", "cs.LG", "eess.SP", "stat.ML"], "comment": "Accepted to IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches."}
{"id": "2602.05516", "pdf": "https://arxiv.org/pdf/2602.05516", "abs": "https://arxiv.org/abs/2602.05516", "authors": ["Runxiao Liu", "Pengda Mao", "Xiangli Le", "Shuang Gu", "Yapeng Chen", "Quan Quan"], "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments", "categories": ["cs.RO"], "comment": "10 pages, 8 figures", "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions."}
{"id": "2602.05552", "pdf": "https://arxiv.org/pdf/2602.05552", "abs": "https://arxiv.org/abs/2602.05552", "authors": ["Bessie Dominguez-Dager", "Sergio Suescun-Ferrandiz", "Felix Escalona", "Francisco Gomez-Donoso", "Miguel Cazorla"], "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments."}
{"id": "2602.05596", "pdf": "https://arxiv.org/pdf/2602.05596", "abs": "https://arxiv.org/abs/2602.05596", "authors": ["Hokyun Lee", "Woo-Jeong Baek", "Junhyeok Cha", "Jaeheung Park"], "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards", "categories": ["cs.RO"], "comment": "Accepted for Publication at IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field."}
{"id": "2602.05608", "pdf": "https://arxiv.org/pdf/2602.05608", "abs": "https://arxiv.org/abs/2602.05608", "authors": ["Yufei Zhu", "Shih-Min Yang", "Martin Magnusson", "Allan Wang"], "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments", "categories": ["cs.RO"], "comment": "Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds."}
{"id": "2602.05683", "pdf": "https://arxiv.org/pdf/2602.05683", "abs": "https://arxiv.org/abs/2602.05683", "authors": ["Chuwei Wang", "Eduardo Sebastián", "Amanda Prorok", "Anastasia Bizyaeva"], "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform."}
{"id": "2602.05760", "pdf": "https://arxiv.org/pdf/2602.05760", "abs": "https://arxiv.org/abs/2602.05760", "authors": ["Andreea Tulbure", "Carmen Scheidemann", "Elias Steiner", "Marco Hutter"], "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers."}
{"id": "2602.05791", "pdf": "https://arxiv.org/pdf/2602.05791", "abs": "https://arxiv.org/abs/2602.05791", "authors": ["Yufei Xue", "YunFeng Lin", "Wentao Dong", "Yang Tang", "Jingbo Wang", "Jiangmiao Pang", "Ming Zhou", "Minghuan Liu", "Weinan Zhang"], "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller."}
{"id": "2602.05855", "pdf": "https://arxiv.org/pdf/2602.05855", "abs": "https://arxiv.org/abs/2602.05855", "authors": ["Dennis Bank", "Joost Cordes", "Thomas Seel", "Simon F. G. Ehlers"], "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift."}
{"id": "2602.05895", "pdf": "https://arxiv.org/pdf/2602.05895", "abs": "https://arxiv.org/abs/2602.05895", "authors": ["Qi Li", "Karsten Berns"], "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools", "categories": ["cs.RO"], "comment": "12 pages", "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone."}
{"id": "2602.05922", "pdf": "https://arxiv.org/pdf/2602.05922", "abs": "https://arxiv.org/abs/2602.05922", "authors": ["Aziz Mohamed Mili", "Louis Catar", "Paul Gérard", "Ilyass Tabiai", "David St-Onge"], "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits", "categories": ["cs.RO"], "comment": null, "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans."}
{"id": "2602.06001", "pdf": "https://arxiv.org/pdf/2602.06001", "abs": "https://arxiv.org/abs/2602.06001", "authors": ["Carolina Higuera", "Sergio Arnaud", "Byron Boots", "Mustafa Mukadam", "Francois Robert Hogan", "Franziska Meier"], "title": "Visuo-Tactile World Models", "categories": ["cs.RO"], "comment": "Preprint", "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations."}
{"id": "2602.06038", "pdf": "https://arxiv.org/pdf/2602.06038", "abs": "https://arxiv.org/abs/2602.06038", "authors": ["Xiaopan Zhang", "Zejin Wang", "Zhixu Li", "Jianpeng Yao", "Jiachen Li"], "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/", "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io."}
{"id": "2602.04992", "pdf": "https://arxiv.org/pdf/2602.04992", "abs": "https://arxiv.org/abs/2602.04992", "authors": ["Puqi Zhou", "Charles R. Twardy", "Cynthia Lum", "Myeong Lee", "David J. Porfirio", "Michael R. Hieb", "Chris Thomas", "Xuesu Xiao", "Sungsoo Ray Hong"], "title": "Applying Ground Robot Fleets in Urban Search: Understanding Professionals' Operational Challenges and Design Opportunities", "categories": ["cs.HC", "cs.RO"], "comment": "Under review", "summary": "Urban searches demand rapid, defensible decisions and sustained physical effort under high cognitive and situational load. Incident commanders must plan, coordinate, and document time-critical operations, while field searchers execute evolving tasks in uncertain environments. With recent advances in technology, ground-robot fleets paired with computer-vision-based situational awareness and LLM-powered interfaces offer the potential to ease these operational burdens. However, no dedicated studies have examined how public safety professionals perceive such technologies or envision their integration into existing practices, risking building technically sophisticated yet impractical solutions. To address this gap, we conducted focus-group sessions with eight police officers across five local departments in Virginia. Our findings show that ground robots could reduce professionals' reliance on paper references, mental calculations, and ad-hoc coordination, alleviating cognitive and physical strain in four key challenge areas: (1) partitioning the workforce across multiple search hypotheses, (2) retaining group awareness and situational awareness, (3) building route planning that fits the lost-person profile, and (4) managing cognitive and physical fatigue under uncertainty. We further identify four design opportunities and requirements for future ground-robot fleet integration in public-safety operations: (1) scalable multi-robot planning and control interfaces, (2) agency-specific route optimization, (3) real-time replanning informed by debrief updates, and (4) vision-assisted cueing that preserves operational trust while reducing cognitive workload. We conclude with design implications for deployable, accountable, and human-centered urban-search support systems"}
{"id": "2602.05049", "pdf": "https://arxiv.org/pdf/2602.05049", "abs": "https://arxiv.org/abs/2602.05049", "authors": ["Yiye Chen", "Yanan Jian", "Xiaoyi Dong", "Shuxin Cao", "Jing Wu", "Patricio Vela", "Benjamin E. Lundell", "Dongdong Chen"], "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "In submission. Project website: https://vista-vla.github.io/", "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ ."}
{"id": "2602.05051", "pdf": "https://arxiv.org/pdf/2602.05051", "abs": "https://arxiv.org/abs/2602.05051", "authors": ["Songyuan Zhang", "Oswin So", "H. M. Sabbir Ahmad", "Eric Yang Yu", "Matthew Cleaveland", "Mitchell Black", "Chuchu Fan"], "title": "ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "24 pages, 17 figures; Accepted by the fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves."}
{"id": "2602.05075", "pdf": "https://arxiv.org/pdf/2602.05075", "abs": "https://arxiv.org/abs/2602.05075", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance", "categories": ["cs.AI", "cs.LG", "cs.RO", "physics.space-ph"], "comment": "Accpeted at Conference: 15th IAA Symposium on Small Satellites for Earth System Observation At: Berlin", "summary": "As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.\n  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.\n  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.\n  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning."}
{"id": "2602.05089", "pdf": "https://arxiv.org/pdf/2602.05089", "abs": "https://arxiv.org/abs/2602.05089", "authors": ["Ethan Rathbun", "Wo Wei Lin", "Alina Oprea", "Christopher Amato"], "title": "Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning", "categories": ["cs.CR", "cs.LG", "cs.RO"], "comment": "10 pages main body, ICLR 2026", "summary": "Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks."}
{"id": "2602.05091", "pdf": "https://arxiv.org/pdf/2602.05091", "abs": "https://arxiv.org/abs/2602.05091", "authors": ["Agni Bandyopadhyay", "Günther Waxenegger-Wilfing"], "title": "Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal", "categories": ["cs.AI", "cs.LG", "cs.RO", "physics.space-ph"], "comment": "Presented at Conference: International Conference on Space Robotics (ISPARO,2025) At: Sendai,Japan", "summary": "Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners."}
{"id": "2602.05105", "pdf": "https://arxiv.org/pdf/2602.05105", "abs": "https://arxiv.org/abs/2602.05105", "authors": ["Rohan Patil", "Jai Malegaonkar", "Xiao Jiang", "Andre Dion", "Gaurav S. Sukhatme", "Henrik I. Christensen"], "title": "GAMMS: Graph based Adversarial Multiagent Modeling Simulator", "categories": ["cs.AI", "cs.RO", "cs.SE"], "comment": null, "summary": "As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/"}
{"id": "2602.05121", "pdf": "https://arxiv.org/pdf/2602.05121", "abs": "https://arxiv.org/abs/2602.05121", "authors": ["Farbod Younesi", "Walter Lucia", "Amr Youssef"], "title": "Trojan Attacks on Neural Network Controllers for Robotic Systems", "categories": ["eess.SY", "cs.RO"], "comment": "Paper submitted to the 2026 IEEE Conference on Control Technology and Applications (CCTA)", "summary": "Neural network controllers are increasingly deployed in robotic systems for tasks such as trajectory tracking and pose stabilization. However, their reliance on potentially untrusted training pipelines or supply chains introduces significant security vulnerabilities. This paper investigates backdoor (Trojan) attacks against neural controllers, using a differential-drive mobile robot platform as a case study. In particular, assuming that the robot's tracking controller is implemented as a neural network, we design a lightweight, parallel Trojan network that can be embedded within the controller. This malicious module remains dormant during normal operation but, upon detecting a highly specific trigger condition defined by the robot's pose and goal parameters, compromises the primary controller's wheel velocity commands, resulting in undesired and potentially unsafe robot behaviours. We provide a proof-of-concept implementation of the proposed Trojan network, which is validated through simulation under two different attack scenarios. The results confirm the effectiveness of the proposed attack and demonstrate that neural network-based robotic control systems are subject to potentially critical security threats."}
{"id": "2602.05142", "pdf": "https://arxiv.org/pdf/2602.05142", "abs": "https://arxiv.org/abs/2602.05142", "authors": ["Rulla Al-Haideri", "Bilal Farooq"], "title": "Modelling Pedestrian Behaviour in Autonomous Vehicle Encounters Using Naturalistic Dataset", "categories": ["physics.soc-ph", "cs.RO"], "comment": null, "summary": "Understanding how pedestrians adjust their movement when interacting with autonomous vehicles (AVs) is essential for improving safety in mixed traffic. This study examines micro-level pedestrian behaviour during midblock encounters in the NuScenes dataset using a hybrid discrete choice-machine learning framework based on the Residual Logit (ResLogit) model. The model incorporates temporal, spatial, kinematic, and perceptual indicators. These include relative speed, visual looming, remaining distance, and directional collision risk proximity (CRP) measures. Results suggest that some of these variables may meaningfully influence movement adjustments, although predictive performance remains moderate. Marginal effects and elasticities indicate strong directional asymmetries in risk perception, with frontal and rear CRP showing opposite influences. The remaining distance exhibits a possible mid-crossing threshold. Relative speed cues appear to have a comparatively less effect. These patterns may reflect multiple behavioural tendencies driven by both risk perception and movement efficiency."}
{"id": "2602.05257", "pdf": "https://arxiv.org/pdf/2602.05257", "abs": "https://arxiv.org/abs/2602.05257", "authors": ["Diya He", "Qingchen Liu", "Cong Zhang", "Jiahu Qin"], "title": "RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation", "categories": ["cs.CV", "cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting."}
{"id": "2602.05311", "pdf": "https://arxiv.org/pdf/2602.05311", "abs": "https://arxiv.org/abs/2602.05311", "authors": ["Chengxiao Wang", "Haoze Wu", "Gagandeep Singh"], "title": "Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY"], "comment": null, "summary": "Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \\emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics."}
{"id": "2602.05324", "pdf": "https://arxiv.org/pdf/2602.05324", "abs": "https://arxiv.org/abs/2602.05324", "authors": ["Mahdis Rabbani", "Navid Mojahed", "Shima Nazari"], "title": "A Data Driven Structural Decomposition of Dynamic Games via Best Response Maps", "categories": ["cs.GT", "cs.MA", "cs.RO", "eess.SY", "math.OC"], "comment": "11 pages, 6 figures, 5 tables, Submitted to RSS 2026", "summary": "Dynamic games are powerful tools to model multi-agent decision-making, yet computing Nash (generalized Nash) equilibria remains a central challenge in such settings. Complexity arises from tightly coupled optimality conditions, nested optimization structures, and poor numerical conditioning. Existing game-theoretic solvers address these challenges by directly solving the joint game, typically requiring explicit modeling of all agents' objective functions and constraints, while learning-based approaches often decouple interaction through prediction or policy approximation, sacrificing equilibrium consistency. This paper introduces a conceptually novel formulation for dynamic games by restructuring the equilibrium computation. Rather than solving a fully coupled game or decoupling agents through prediction or policy approximation, a data-driven structural reduction of the game is proposed that removes nested optimization layers and derivative coupling by embedding an offline-compiled best-response map as a feasibility constraint. Under standard regularity conditions, when the best-response operator is exact, any converged solution of the reduced problem corresponds to a local open-loop Nash (GNE) equilibrium of the original game; with a learned surrogate, the solution is approximately equilibrium-consistent up to the best-response approximation error. The proposed formulation is supported by mathematical proofs, accompanying a large-scale Monte Carlo study in a two-player open-loop dynamic game motivated by the autonomous racing problem. Comparisons are made against state-of-the-art joint game solvers, and results are reported on solution quality, computational cost, and constraint satisfaction."}
{"id": "2602.05467", "pdf": "https://arxiv.org/pdf/2602.05467", "abs": "https://arxiv.org/abs/2602.05467", "authors": ["Dekang Qi", "Shuang Zeng", "Xinyuan Chang", "Feng Xiong", "Shichao Xie", "Xiaolong Wu", "Mu Xu"], "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "9 pages, 2 figures, 5 tables, conference", "summary": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization."}
{"id": "2602.05555", "pdf": "https://arxiv.org/pdf/2602.05555", "abs": "https://arxiv.org/abs/2602.05555", "authors": ["Panagiotis Sapoutzoglou", "Orestis Vaggelis", "Athina Zacharia", "Evangelos Sartinas", "Maria Pateraki"], "title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools", "categories": ["cs.CV", "cs.RO"], "comment": "To appear in ICRA 2026", "summary": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes."}
{"id": "2602.05557", "pdf": "https://arxiv.org/pdf/2602.05557", "abs": "https://arxiv.org/abs/2602.05557", "authors": ["Michael Schwingshackl", "Fabio F. Oberweger", "Mario Niedermeyer", "Huemer Johannes", "Markus Murschitz"], "title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds", "categories": ["cs.CV", "cs.RO"], "comment": "8 Pages, 11 Figures, Accepted at 2026 IEEE International Conference on Robotics & Automation (ICRA) Vienna", "summary": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr."}
{"id": "2602.05827", "pdf": "https://arxiv.org/pdf/2602.05827", "abs": "https://arxiv.org/abs/2602.05827", "authors": ["Hai Zhang", "Siqi Liang", "Li Chen", "Yuxian Li", "Yukuan Xu", "Yichao Zhong", "Fu Zhang", "Hongyang Li"], "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes."}
{"id": "2602.05863", "pdf": "https://arxiv.org/pdf/2602.05863", "abs": "https://arxiv.org/abs/2602.05863", "authors": ["Roger Girgis", "Rodrigue de Schaetzen", "Luke Rowe", "Azalée Robitaille", "Christopher Pal", "Liam Paull"], "title": "Constrained Group Relative Policy Optimization", "categories": ["cs.LG", "cs.CL", "cs.RO"], "comment": "16 pages, 6 figures", "summary": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models."}
{"id": "2602.05948", "pdf": "https://arxiv.org/pdf/2602.05948", "abs": "https://arxiv.org/abs/2602.05948", "authors": ["Himani", "Supantha Pandit", "Gokarna Sharma"], "title": "Location-Aware Dispersion on Anonymous Graphs", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "comment": "3 tables, 2 figures, 6 pseudo-codes", "summary": "The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\\sf{col}(v) \\in C = \\{c_1, \\dots, c_t\\}, t\\leq n$. A set $R = \\{r_1, \\dots, r_k\\}$ of $k \\leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\\mathsf{col}(r_i) \\in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\\leq n$ without knowing $k,n$.\n  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION."}
{"id": "2602.06023", "pdf": "https://arxiv.org/pdf/2602.06023", "abs": "https://arxiv.org/abs/2602.06023", "authors": ["Christopher A. McClurg", "Alan R. Wagner"], "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments", "categories": ["cs.AI", "cs.RO"], "comment": "Preprint under review for conference publication. 9 pages, 4 figures, 4 tables", "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions."}
{"id": "2602.06035", "pdf": "https://arxiv.org/pdf/2602.06035", "abs": "https://arxiv.org/abs/2602.06035", "authors": ["Sirui Xu", "Samuel Schulter", "Morteza Ziyadi", "Xialin He", "Xiaohan Fei", "Yu-Xiong Wang", "Liangyan Gui"], "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Webpage: https://sirui-xu.github.io/InterPrior/", "summary": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment."}
