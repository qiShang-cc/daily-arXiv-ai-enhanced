{"id": "2602.21449", "pdf": "https://arxiv.org/pdf/2602.21449", "abs": "https://arxiv.org/abs/2602.21449", "authors": ["Van-Chung Luu", "Toan-Van Nguyen", "Nuria González-Prelcic", "Duy H. N. Nguyen"], "title": "Semi-Gridless Variational Bayes Channel Estimation in XL-MIMO: Near-Field Modeling and Inference", "categories": ["eess.SP"], "comment": "12 pages, 11 figures", "summary": "Extremely large antenna arrays and high-frequency operation are two key technologies that advance performance metrics such as higher data rates, lower latency, and wider coverage in sixth-generation communications. However, the adoption of these technologies fundamentally changes the characteristics of wavefronts, forcing communication systems to operate in the near-field region. The transition from planar far-field communications to spherical near-field propagation necessitates novel channel estimation algorithms to fully exploit the unique features of spherical wavefronts for advanced transceiver design. To this end, we propose a novel semi-gridless channel estimation approach based on a variational Bayesian (VB) inference framework. Specifically, we reformulate the near-field channel model for both uniform linear arrays and uniform planar arrays into separate direction-of-arrival (DoAs) and distance components. Building on these new representations, we employ a gridless approach for DoAs estimation using a von Mises distribution, and a coarse-to-fine grid search for distance estimation. We then develop a semi-gridless variational Bayesian (SG-VB) algorithm with efficient update rules that enables accurate channel reconstruction. Simulation results validate the effectiveness of the proposed SG-VB algorithm, demonstrating enhanced near-field channel reconstruction accuracy and superior estimation performance for both DoAs and distance components embedded in near-field channels."}
{"id": "2602.21573", "pdf": "https://arxiv.org/pdf/2602.21573", "abs": "https://arxiv.org/abs/2602.21573", "authors": ["Koji Yamamoto", "Katsuyuki Haneda"], "title": "Delay-Synchronous Wideband Channel Sounding Using Off-The-Shelf Multi-Antenna WiFi Devices", "categories": ["eess.SP"], "comment": "Author's version of a paper presented at EuCAP 2025; some figures are revised from the IEEE Xplore version", "summary": "It has been shown that WiFi devices enable sensing of environments and targets through their channel state information. However, the same devices have not been used for delay-synchronous channel sounding due to challenges related to the stability of synchronization and lack of reference power levels. Due to factors such as uncertainty in symbol reception timing, impulse responses are discontinuous across acquisitions. The present paper addresses the challenges to perform delay-synchronous channel sounding using off-the-shelf multiple-antenna IEEE 802.11ax WiFi devices, referred to as SoundiFi. Stable delay synchronization and power level reference are realized by remoting the antennas with coaxial cables and devoting one of the antennas as a reference channel, with which the gain and delay of other simultaneous channels are defined. Indoor experiments confirmed that the impulse response becomes continuous across successive acquisitions and provide the absolute delay. The impulse response has a noise level at -115 dB, indicating the maximum path gain value that can be measured with the devices. The impulse response also revealed the existence of long-delayed multipaths up to 132 m propagation distance in a reverberant 30-m-long corridor."}
{"id": "2602.21614", "pdf": "https://arxiv.org/pdf/2602.21614", "abs": "https://arxiv.org/abs/2602.21614", "authors": ["Shan Shan", "Chongjun Ouyang", "Yong Li", "Yuanwei Liu"], "title": "Pinching Antennas for Multiple Access in Multigroup Multicast Communications", "categories": ["eess.SP"], "comment": null, "summary": "This paper aims to design multiple access (MA) schemes to improve the max-min fairness (MMF) for pinching antennas (PAs)-based multigroup multicast communications, where PA placement and resource allocation are jointly optimized. Specifically, three MA schemes are considered to facilitate the multicast transmission: i) treating interference as noise (TIN), ii) non-orthogonal multiple access (NOMA), and iii) time-division multiple access (TDMA) with two PA reconfiguration protocols, namely pinching switching (PS) and pinching multiplexing (PM). i) For TIN, a closed-form solution is derived for optimal power allocation, while a sequential element-wise optimization (SEO) is developed for the PA placement. ii) For NOMA, a recursive power allocation framework incorporating a bisection search is developed, and a hierarchical objective evaluation (HOE) mechanism is incorporated to simplify the SEO process for PA location update. iii) For TDMA, the PS protocol allows the PA locations to be optimized separately using the SEO method, after which the time-power allocation is solved as a convex problem with a global optimum. Under the PM protocol, the PA locations are jointly optimized with the time-power resources through a Karush-Kuhn-Tucker (KKT)-based analytical solution. Numerical results demonstrate that: i) the pinching-antenna system (PASS) architecture significantly outperforms traditional fixed-antenna systems. ii) TDMA-PS achieves superior performance by fully leveraging the flexible PA reconfiguration and benefiting from interference-free transmission, whereas TIN serves as a practical lower-bound solution due to its simplicity despite its limited performance. iii) NOMA consistently outperforms TDMA-PM and, in high transmit power regimes with heterogeneous multicast group distributions, can even surpass the performance achieved by TDMA-PS."}
{"id": "2602.21654", "pdf": "https://arxiv.org/pdf/2602.21654", "abs": "https://arxiv.org/abs/2602.21654", "authors": ["Ruhao Zhang", "Yupeng Li", "Yitong Liu", "Shijian Gao", "Jing Jin", "Hongwen Yang", "Jiangzhou Wang"], "title": "Score-Based Conditional Flow Models for MIMO Receiver Design with Superimposed Pilots", "categories": ["eess.SP"], "comment": null, "summary": "Accurate channel state information (CSI) is vital for multiple-input multiple-output (MIMO) systems. However, superimposed pilots (SIP), which reduce overhead, introduce severe pilot contamination and data interference, complicating joint channel estimation and data detection. This paper proposes a conditional flow matching receiver (CFM-Rx), an unsupervised generative framework that learns directly from received signals, eliminating the need for labeled data and improving adaptability across diverse system settings. By leveraging flow-based generative modeling, CFM-Rx enables deterministic, low-latency inference and exploits model invertibility to capture the bidirectional nature of signal propagation. This framework unifies flow matching with score-based diffusion modeling via a moment-consistent ordinary differential equation (ODE), replacing stochastic differential equation (SDE) sampling with a deterministic and efficient process. Furthermore, it integrates receiver-side priors to ensure stable, data-consistent inference. Extensive simulation results across various MIMO configurations demonstrate that CFM-Rx consistently outperforms conventional estimators and state-of-the-art data-driven receivers, achieving notable gains in channel estimation accuracy and symbol detection robustness, particularly under severe pilot contamination."}
{"id": "2602.21259", "pdf": "https://arxiv.org/pdf/2602.21259", "abs": "https://arxiv.org/abs/2602.21259", "authors": ["Ricardo B. Grando", "Victor A. Kich", "Alisson H. Kolling", "Junior C. D. Jesus", "Rodrigo S. Guerra", "Paulo L. J. Drews-Jr"], "title": "Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles", "categories": ["cs.RO"], "comment": "Accepted to the Brazilian Conference on Robotics 2026", "summary": "Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles."}
{"id": "2602.21664", "pdf": "https://arxiv.org/pdf/2602.21664", "abs": "https://arxiv.org/abs/2602.21664", "authors": ["Weijie Jin", "Jing Zhang", "Hengtao He", "Chao-Kai Wen", "Xiao Li", "Shi Jin"], "title": "Deep Learning-based Low-Overhead Beam Alignment for mmWave Massive MIMO Systems", "categories": ["eess.SP"], "comment": "13 pages, 12 figures. This work has been submitted to the IEEE for possible publication", "summary": "Millimeter-wave massive multiple-input multiple-output systems employ highly directional beamforming to overcome severe path loss, and their performance critically depends on accurate beam alignment. Conventional codebook-based methods offer low training overhead but suffer from limited angular resolution and sensitivity to hardware impairments. To address these challenges, we propose a deep learning-enhanced super-resolution beam alignment framework with three key components. First, we design the Quaternary Search-based Super-Resolution (QSSR) algorithm, which leverages the monotonic power ratio property between two discrete Fourier transform (DFT) codebook beams to achieve super-resolution angle estimation without increasing measurement complexity relative to binary search. Second, we develop QSSR-Net, a gated recurrent unit-based neural network that exploits sequential multi-layer beam measurements to capture angular dependencies, thereby improving estimation accuracy, robustness to noise, and generalization across diverse propagation environments. Third, to mitigate the adverse effects of hardware impairments such as antenna position and phase errors, we propose a parametric self-calibration method that requires no additional hardware overhead and adapts compensation parameters in real time. Simulation results show that the proposed framework consistently outperforms binary search and even exhaustive search at high signal-to-noise ratios, achieving substantial performance gains while maintaining low overhead."}
{"id": "2602.21266", "pdf": "https://arxiv.org/pdf/2602.21266", "abs": "https://arxiv.org/abs/2602.21266", "authors": ["Mor Levenhar", "Itzik Klein"], "title": "Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints", "categories": ["cs.RO"], "comment": "12 pages, 5 figuers", "summary": "Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models."}
{"id": "2602.21744", "pdf": "https://arxiv.org/pdf/2602.21744", "abs": "https://arxiv.org/abs/2602.21744", "authors": ["Boxuan Xie", "Lauri Mela", "Alexis A. Dowhuszko", "Jiacheng Wang", "Kalle Ruttik", "Riku Jäntti"], "title": "Dual-Hop Joint Visible Light and Backscatter Communication Relaying under Finite Blocklength", "categories": ["eess.SP", "cs.NI"], "comment": "6 pages, 10 figures, 1 table", "summary": "This paper investigates a dual-hop joint visible light communication (VLC) and backscatter communication (BC) relaying framework under the finite blocklength (FBL) constraint, aiming at energy-neutral Ambient Internet of Things (A-IoT) deployments. In the proposed system, indoor LED access points are used to simultaneously provide illumination and transmit information over light to a backscatter device (BD), which harvests optical energy and backscatters the received messages to user equipments (UEs) equipped with radio frequency (RF) front ends. This forwarding of the information from VLC to RF channels is implemented without the need for carrier synthesizers and power amplifiers at the IoT node. By modeling the end-to-end communication link with short-packet IoT traffic and realistic levels of interference between adjacent VLC coverage areas, we analyze the outage performance and achievable data rate of the proposed system. Simulation results demonstrate that key factors, such as placement and orientation of the BD, as well as the selected code rate of the system affect reliability and data rate that can be achieved for communication purposes. The insights gained from this study pave the way for ambient power-enabled IoT solutions and future hybrid VLC/RF network designs."}
{"id": "2602.21302", "pdf": "https://arxiv.org/pdf/2602.21302", "abs": "https://arxiv.org/abs/2602.21302", "authors": ["Krishna Suresh", "Chris Atkeson"], "title": "Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control", "categories": ["cs.RO"], "comment": "Project website: https://flying-knots.github.io", "summary": "Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io"}
{"id": "2602.21793", "pdf": "https://arxiv.org/pdf/2602.21793", "abs": "https://arxiv.org/abs/2602.21793", "authors": ["Teng Wu", "Jiandong Li", "Junyu Liu", "Min Sheng", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Availability of Aerial Heterogeneous Networks for Reliable Emergency Communications", "categories": ["eess.SP"], "comment": null, "summary": "We investigate network availability (NA) in aerial heterogeneous networks (AHetNets) for effective emergency rescue, where diverse delay-constrained communication services must be provided to user equipments (UEs) with varying mobility. The heterogeneity in delay constraints and UE mobility introduces resource allocation conflicts and imbalances, which undermine communication reliability and challenge NA. Although unified resource allocation (URA) can mitigate these issues, it remains unclear whether NA can be sustained under such diverse conditions. To address this, we derive expressions for the lower bound (LB) on NA in AHetNets under URA. Our analysis reveals that extended heterogeneity significantly degrades the LB due to resource limitations-even when the heterogeneity stems from additional services under less stringent delay constraints (LSDC) or from UEs with lower mobility. To overcome this degradation, we formulate and solve a joint optimization problem for the number of UEs sharing time-frequency resources ($K$) and pilot length ($ξ$), aiming to enhance the LB by improving spatial, frequency, and temporal resource efficiency. Simulation results validate our analysis and demonstrate that jointly optimizing $K$ and $ξ$ enables AHetNets to achieve the target NA under greater heterogeneity, outperforming existing resource allocation policies."}
{"id": "2602.21316", "pdf": "https://arxiv.org/pdf/2602.21316", "abs": "https://arxiv.org/abs/2602.21316", "authors": ["Milad Azizkhani", "Yue Chen"], "title": "Unified Complementarity-Based Contact Modeling and Planning for Soft Robots", "categories": ["cs.RO"], "comment": "9 pages, 4 figures", "summary": "Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics."}
{"id": "2602.21801", "pdf": "https://arxiv.org/pdf/2602.21801", "abs": "https://arxiv.org/abs/2602.21801", "authors": ["Mauro Marchese", "Pietro Savazzi"], "title": "Cross-Pilot Superposition for Fractional Parameter Estimation in DoA-Aided OTFS Receivers", "categories": ["eess.SP"], "comment": "Submitted", "summary": "In this letter, a novel superimposed pilot scheme is proposed for channel estimation in multi-antenna orthogonal time frequency space (OTFS) receivers. Under the assumption of a large uniform linear array (ULA) size at the receiver, the multipath components are separated directly in the angular domain. It is then shown that the proposed superimposed pilot scheme enables the computation of integrated delay and Doppler profiles by averaging the received delay-Doppler matrix across the Doppler and delay axes, respectively. This procedure helps reduce data-to-pilot interference through data averaging. Moreover, it is demonstrated that fractional delays and Dopplers of the multipath components can be estimated by correlating the integrated delay and Doppler profiles with the corresponding delay/Doppler terms. Simulation results show that the proposed approach outperforms existing OTFS superimposed pilot schemes, achieving a lower bit error rate (BER) while exhibiting a trade-off between peak-to-average power ratio (PAPR) and communication performance."}
{"id": "2602.21331", "pdf": "https://arxiv.org/pdf/2602.21331", "abs": "https://arxiv.org/abs/2602.21331", "authors": ["Nelson Chen", "William R. Johnson", "Rebecca Kramer-Bottiglio", "Kostas Bekris", "Mridul Aanjaneya"], "title": "CableRobotGraphSim: A Graph Neural Network for Modeling Partially Observable Cable-Driven Robot Dynamics", "categories": ["cs.RO"], "comment": null, "summary": "General-purpose simulators have accelerated the development of robots. Traditional simulators based on first-principles, however, typically require full-state observability or depend on parameter search for system identification. This work presents \\texttt{CableRobotGraphSim}, a novel Graph Neural Network (GNN) model for cable-driven robots that aims to address shortcomings of prior simulation solutions. By representing cable-driven robots as graphs, with the rigid-bodies as nodes and the cables and contacts as edges, this model can quickly and accurately match the properties of other simulation models and real robots, while ingesting only partially observable inputs. Accompanying the GNN model is a sim-and-real co-training procedure that promotes generalization and robustness to noisy real data. This model is further integrated with a Model Predictive Path Integral (MPPI) controller for closed-loop navigation, which showcases the model's speed and accuracy."}
{"id": "2602.21856", "pdf": "https://arxiv.org/pdf/2602.21856", "abs": "https://arxiv.org/abs/2602.21856", "authors": ["Kaidi Wang", "Zhiguo Ding", "Daniel K. C. So"], "title": "Leaky Coaxial Cable based Generalized Pinching-Antenna Systems with Dual-Port Feeding", "categories": ["eess.SP"], "comment": null, "summary": "By leveraging the distributed leakage radiation of leaky coaxial cables (LCXs), the concept of pinching antennas can be generalized from the conventional high-frequency waveguide based architectures to cable based structures in lower-frequency scenarios. This paper investigates an LCX based generalized pinching-antenna system with dual-port feeding. By enabling bidirectional excitation along each cable, the proposed design significantly enhances spatial degrees of freedom. A comprehensive channel model is developed to characterize intra-cable attenuation, bidirectional phase progression, slot based radiation, and wireless propagation. Based on this model, both analog and hybrid beamforming frameworks are studied with the objective of maximizing the minimum achievable data rate. For analog transmission, slot activation, port selection, and power allocation are jointly optimized using matching theory, coalitional games, and bisection based power control. For hybrid transmission, zero-forcing (ZF) digital precoding is incorporated to eliminate inter-user interference, thereby simplifying slot activation and enabling closed-form optimal power allocation. Simulation results demonstrate that dual-port feeding provides notable performance gains over single-port LCX systems and fixed-antenna benchmarks, validating the effectiveness of the proposed beamforming and resource allocation designs under various transmit power levels and cable parameters."}
{"id": "2602.21366", "pdf": "https://arxiv.org/pdf/2602.21366", "abs": "https://arxiv.org/abs/2602.21366", "authors": ["Y. Deemo Chen", "Arion Zimmermann", "Thomas A. Berrueta", "Soon-Jo Chung"], "title": "Environment-Aware Learning of Smooth GNSS Covariance Dynamics for Autonomous Racing", "categories": ["cs.RO"], "comment": "8 pages, Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "Ensuring accurate and stable state estimation is a challenging task crucial to safety-critical domains such as high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. In this work, we develop a learning-based framework, LACE, capable of directly modeling the temporal dynamics of GNSS measurement covariance. We model the covariance evolution as an exponentially stable dynamical system where a deep neural network (DNN) learns to predict the system's process noise from environmental features through an attention mechanism. By using contraction-based stability and systematically imposing spectral constraints, we formally provide guarantees of exponential stability and smoothness for the resulting covariance dynamics. We validate our approach on an AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. Our results highlight the promise of dynamically modeling the perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity."}
{"id": "2602.21909", "pdf": "https://arxiv.org/pdf/2602.21909", "abs": "https://arxiv.org/abs/2602.21909", "authors": ["Jiali He", "Yamei Dai", "Sheng Shen", "Jiamin Wu", "Zheng Xu"], "title": "Modeling of Human Body-coupled Electric Field Interference in Unshielded Ultra-Low Field MRI", "categories": ["eess.SP", "physics.med-ph"], "comment": null, "summary": "Portable ultra-low field MRI (ULF-MRI) systems operated in unshielded environments are susceptible to electromagnetic interference (EMI). Subject presence in the imaging region will lead to substantial noise increases, yet the dominant coupling mechanism remains insufficiently characterized. We develop a lumped-parameter circuit model of the coupled environment-body-receiver system. The model indicates that ambient time-varying electric fields induce a body common-mode potential, which is converted into differential-mode noise through capacitive imbalance between the head and the receive-coil terminals, yielding strong dependence on subject position and geometry. Circuit analysis, simulations, and controlled experiments support the model, with predicted imbalance consistent with measured noise variations. Guided by this mechanism, we implement a capacitive low-impedance bypass to clamp the body potential, achieving an approximately 3.5-fold SNR improvement on a 50 mT prototype. The proposed model offers a compact circuit-based tool for analyzing and mitigating human body-coupled electric-field interference in portable ULF-MRI."}
{"id": "2602.21389", "pdf": "https://arxiv.org/pdf/2602.21389", "abs": "https://arxiv.org/abs/2602.21389", "authors": ["Zach J. Patterson", "Emily Sologuren", "Levi Cai", "Daniel Kim", "Alaa Maalouf", "Pascal Spino", "Daniela Rus"], "title": "Autonomous Sea Turtle Robot for Marine Fieldwork", "categories": ["cs.RO"], "comment": "22 pages, 3 figures, 1 table, 5 supplementary figures, 1 supplementary table. Submitted for review", "summary": "Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems."}
{"id": "2602.21918", "pdf": "https://arxiv.org/pdf/2602.21918", "abs": "https://arxiv.org/abs/2602.21918", "authors": ["Merijn Floren", "Jan Swevers"], "title": "A sliding-window approach for latent restoring force modeling", "categories": ["eess.SP"], "comment": "Preprint submitted to Mechanical Systems and Signal Processing", "summary": "Restoring force surface (RFS) methods offer an attractive nonparametric framework for identifying nonlinear restoring forces directly from data, but their reliance on complete kinematic measurements at each degree of freedom limits scalability to multidimensional systems. The aim of this paper is to overcome these measurement limitations by proposing an identification framework with relaxed sensing requirements that exploits periodic multisine excitation. Starting from an initial linear model, a sliding-window feedback approach reconstructs latent states and nonlinear restoring forces nonparametrically, enabling identification of the nonlinear component through linear-in-parameters regression instead of highly non-convex optimization. Validation on synthetic and experimental datasets demonstrates high simulation accuracy and reliable recovery of physical parameters under partial sensing and noisy conditions."}
{"id": "2602.21418", "pdf": "https://arxiv.org/pdf/2602.21418", "abs": "https://arxiv.org/abs/2602.21418", "authors": ["Mohammadsaleh Razmi", "Iman Shojaei"], "title": "Event-Driven On-Sensor Locomotion Mode Recognition Using a Shank-Mounted IMU with Embedded Machine Learning for Exoskeleton Control", "categories": ["cs.RO"], "comment": "10 pages, 6 figures. Sensor-level HAR using embedded IMU machine learning for wearable robotics", "summary": "This work presents a wearable human activity recognition (HAR) system that performs real-time inference directly inside a shank-mounted inertial measurement unit (IMU) to support low-latency control of a lower-limb exoskeleton. Unlike conventional approaches that continuously stream raw inertial data to a microcontroller for classification, the proposed system executes activity recognition at the sensor level using the embedded Machine Learning Core (MLC) of the STMicroelectronics LSM6DSV16X IMU, allowing the host microcontroller to remain in a low-power state and read only the recognized activity label from IMU registers. While the system generalizes to multiple human activities, this paper focuses on three representative locomotion modes - stance, level walking, and stair ascent - using data collected from adult participants. A lightweight decision-tree model was configured and deployed for on-sensor execution using ST MEMS Studio, enabling continuous operation without custom machine learning code on the microcontroller. During operation, the IMU asserts an interrupt when motion or a new classification is detected; the microcontroller wakes, reads the MLC output registers, and forwards the inferred mode to the exoskeleton controller. This interrupt-driven, on-sensor inference architecture reduces computation and communication overhead while preserving battery energy and improving robustness in distinguishing level walking from stair ascent for torque-assist control."}
{"id": "2602.21927", "pdf": "https://arxiv.org/pdf/2602.21927", "abs": "https://arxiv.org/abs/2602.21927", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Emil Björnson", "Ahmed M. Eltawil"], "title": "Analyzing URA Geometry for Enhanced Near-Field Beamfocusing and Spatial Degrees of Freedom", "categories": ["eess.SP"], "comment": null, "summary": "With the deployment of large antenna arrays at high-frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused on a spatial region with a finite depth, enabling spatial multiplexing in the range dimension. Moreover, in the line-of-sight MIMO near-field, multiple spatial degrees of freedom (DoF) are accessible, akin to a scattering- rich environment. In this paper, we derive the beamdepth for a generalized uniform rectangular array (URA) and investigate how the array geometry influences near-field beamdepth and its limits. We define the effective beamfocusing Rayleigh distance (EBRD), to present a near-field boundary with respect to beamfocusing and spatial multiplexing gains for the generalized URA. Our results demonstrate that under a fixed element count constraint, the array geometry has a strong impact on beamdepth, whereas this effect diminishes under a fixed aperture length constraint. Moreover, compared to uniform square arrays, elongated configurations such as uniform linear arrays (ULAs) yield narrower beamdepth and extend the effective near-field region defined by the EBRD. Building on these insights, we design a polar codebook for compressed-sensing-based channel estimation that leverages our findings. Simulation results show that the proposed polar codebook achieves a 2 dB NMSE improvement over state-of-the-art methods. Additionally, we present an analytical expression to quantify the effective spatial DoF in the near-field, revealing that they are also constrained by the EBRD. Notably, the maximum spatial DoF is achieved with a ULA configuration, outperforming a square URA in this regard."}
{"id": "2602.21445", "pdf": "https://arxiv.org/pdf/2602.21445", "abs": "https://arxiv.org/abs/2602.21445", "authors": ["Haoxuan Wang", "Gengyu Zhang", "Yan Yan", "Ramana Rao Kompella", "Gaowen Liu"], "title": "VLA Knows Its Limits", "categories": ["cs.RO"], "comment": "Project page at https://hatchetproject.github.io/autohorizon/", "summary": "Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models."}
{"id": "2602.21945", "pdf": "https://arxiv.org/pdf/2602.21945", "abs": "https://arxiv.org/abs/2602.21945", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Ahmed Nasser", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Spatial Degrees of Freedom in Near Field MIMO: Experimental Validation of Beamspace Perspective", "categories": ["eess.SP"], "comment": null, "summary": "Conventional far-field multiple-input multiple-output (MIMO) channels are limited to a single spatial degree of freedom (DoF) under a line-of-sight (LoS) condition. In contrast, the radiative near field (NF) supports multiple spatial DoF, enabled by spherical wavefronts and the reduced spatial footprint at short ranges. While recent research indicates that the effective DoF (EDoF) increases in NF, experimental validation and clear identification of the transition distances remain limited. In this letter, we develop an intuitive framework for characterizing the EDoF of a ULA-based MIMO system and derive two complementary analytical expressions: a closed-form formulation that relates the EDoF to the physical transmit beamwidth and receive aperture, and a discrete formulation based on the discrete Fourier transform (DFT) domain angular decomposition of the NF spherical wavefront, which is well suited for experimental evaluation. We further introduce the effective MIMO Rayleigh distance (EMRD) and the maximum spatial multiplexing distance (MSMD), which mark the distances where the EDoF reduces to one and attains its maximum, respectively. Experimental measurements using widely spaced phased arrays closely match the theoretical EDoF trends and validate the proposed distance metrics."}
{"id": "2602.21450", "pdf": "https://arxiv.org/pdf/2602.21450", "abs": "https://arxiv.org/abs/2602.21450", "authors": ["Felipe Bartelt", "Vinicius M. Gonçalves", "Luciano C. A. Pimenta"], "title": "Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness."}
{"id": "2602.21973", "pdf": "https://arxiv.org/pdf/2602.21973", "abs": "https://arxiv.org/abs/2602.21973", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Emil Björnson", "Ahmed M. Eltawil"], "title": "Sparse Array Design for Near-Field MU-MIMO: Reconfigurable Array Thinning Approach", "categories": ["eess.SP"], "comment": null, "summary": "Future wireless networks, deploying thousands of antenna elements, may operate in the radiative near-field (NF), enabling spatial multiplexing across both angle and range domains. Sparse arrays have the potential to achieve comparable performance with fewer antenna elements. However, fixed sparse array designs are generally suboptimal under dynamic user distributions, while movable antenna architectures rely on mechanically reconfigurable elements, introducing latency and increased hardware complexity. To address these limitations, we propose a reconfigurable array thinning approach that selectively activates a subset of antennas to form a flexible sparse array design without physical repositioning. We first analyze grating lobes for uniform sparse arrays in the angle and range domains, showing their absence along the range dimension. Based on the analysis, we develop two particle swarm optimization-based strategies: a grating-lobe-based thinned array (GTA) for grating- lobe suppression and a sum-rate-based thinned array (STA) for multiuser sum-rate maximization. Simulation results demonstrate that GTA outperforms conventional uniform sparse arrays, while STA achieves performance comparable to movable antennas, thereby offering a practical and efficient array deployment strategy without the associated mechanical complexity."}
{"id": "2602.21531", "pdf": "https://arxiv.org/pdf/2602.21531", "abs": "https://arxiv.org/abs/2602.21531", "authors": ["Yue Yang", "Shuo Cheng", "Yu Fang", "Homanga Bharadhwaj", "Mingyu Ding", "Gedas Bertasius", "Daniel Szafir"], "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "eess.SY"], "comment": null, "summary": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/."}
{"id": "2602.22087", "pdf": "https://arxiv.org/pdf/2602.22087", "abs": "https://arxiv.org/abs/2602.22087", "authors": ["Yuan Ai", "Xidong Mu", "Pengbo Si", "Yuanwei Liu"], "title": "Transmission Delay Minimization for NOMA-Based F-RANs", "categories": ["eess.SP"], "comment": "Accepted by IEEE Transactions on Wireless Communications", "summary": "A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework significantly outperforms orthogonal multiple access-based F-RANs systems in terms of average transmission delay in different scenarios."}
{"id": "2602.21583", "pdf": "https://arxiv.org/pdf/2602.21583", "abs": "https://arxiv.org/abs/2602.21583", "authors": ["Wentao Zhang", "Zhaoqi Ma", "Jinjie Li", "Huayi Wang", "Haokun Liu", "Junichiro Sugihara", "Chen Chen", "Yicheng Chen", "Moju Zhao"], "title": "Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors", "categories": ["cs.RO"], "comment": null, "summary": "Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware."}
{"id": "2602.21403", "pdf": "https://arxiv.org/pdf/2602.21403", "abs": "https://arxiv.org/abs/2602.21403", "authors": ["Luca Martino", "Eduardo Morgado", "Roberto San Millán-Castillo"], "title": "An index of effective number of variables for uncertainty and reliability analysis in model selection problems", "categories": ["stat.ME", "cs.CE", "eess.SP", "stat.CO"], "comment": null, "summary": "An index of an effective number of variables (ENV) is introduced for model selection in nested models. This is the case, for instance, when we have to decide the order of a polynomial function or the number of bases in a nonlinear regression, choose the number of clusters in a clustering problem, or the number of features in a variable selection application (to name few examples). It is inspired by the idea of the maximum area under the curve (AUC). The interpretation of the ENV index is identical to the effective sample size (ESS) indices concerning a set of samples. The ENV index improves {drawbacks of} the elbow detectors described in the literature and introduces different confidence measures of the proposed solution. These novel measures can be also employed jointly with the use of different information criteria, such as the well-known AIC and BIC, or any other model selection procedures. Comparisons with classical and recent schemes are provided in different experiments involving real datasets. Related Matlab code is given."}
{"id": "2602.21595", "pdf": "https://arxiv.org/pdf/2602.21595", "abs": "https://arxiv.org/abs/2602.21595", "authors": ["Hyungmin Kim", "Hobeom Jeon", "Dohyung Kim", "Minsu Jang", "Jeahong Kim"], "title": "SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints", "categories": ["cs.RO"], "comment": "Accepted to IEEE ICASSP 2026", "summary": "Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC"}
{"id": "2602.21476", "pdf": "https://arxiv.org/pdf/2602.21476", "abs": "https://arxiv.org/abs/2602.21476", "authors": ["Chun-wei Ho", "Sabato Marco Siniscalchi", "Kai Li", "Chin-Hui Lee"], "title": "A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "We propose a knowledge-driven, model-based approach to segmenting audio into single-category and mixed-category chunks with applications to source separation. \"Knowledge\" here denotes information associated with the data, such as music scores. \"Model\" here refers to tool that can be used for audio segmentation and recognition, such as hidden Markov models. In contrast to conventional learning that often relies on annotated data with given segment categories and their corresponding boundaries to guide the learning process, the proposed framework does not depend on any pre-segmented training data and learns directly from the input audio and its related knowledge sources to build all necessary models autonomously. Evaluation on simulation data shows that score-guided learning achieves very good music segmentation and separation results. Tested on movie track data for cinematic audio source separation also shows that utilizing sound category knowledge achieves better separation results than those obtained with data-driven techniques without using such information."}
{"id": "2602.21599", "pdf": "https://arxiv.org/pdf/2602.21599", "abs": "https://arxiv.org/abs/2602.21599", "authors": ["Weisheng Xu", "Qiwei Wu", "Jiaxi Zhang", "Tan Jing", "Yangfan Li", "Yuetong Fang", "Jiaqi Xiong", "Kai Wu", "Rong Ou", "Renjing Xu"], "title": "Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework."}
{"id": "2602.21672", "pdf": "https://arxiv.org/pdf/2602.21672", "abs": "https://arxiv.org/abs/2602.21672", "authors": ["Keke Ying", "Zhen Gao", "Tingting Yang", "Jianhua Zhang", "Xiang Cheng", "Tony Q. S. Quek", "H. Vincent Poor"], "title": "From Specialist to Large Models: A Paradigm Evolution Towards Semantic-Aware MIMO", "categories": ["cs.IT", "eess.SP"], "comment": "This article has been accepted by IEEE Communications Magazine", "summary": "The sixth generation (6G) network is expected to deploy larger multiple-input multiple-output (MIMO) arrays to support massive connectivity, which will increase overhead and latency at the physical layer. Meanwhile, emerging 6G demands such as immersive communications and environmental sensing pose challenges to traditional signal processing. To address these issues, we propose the ``semantic-aware MIMO'' paradigm, which leverages specialist models and large models to perceive, utilize, and fuse the inherent semantics of channels and sources for improved performance. Moreover, for representative MIMO physical-layer tasks, e.g., random access activity detection, channel feedback, and precoding, we design specialist models that exploit channel and source semantics for better performance. Additionally, in view of the more diversified functions of 6G MIMO, we further explore large models as a scalable solution for multi-task semantic-aware MIMO and review recent advances along with their advantages and limitations. Finally, we discuss the challenges, insights, and prospects of the evolution of specialist models and large models empowered semantic-aware MIMO paradigms."}
{"id": "2602.21612", "pdf": "https://arxiv.org/pdf/2602.21612", "abs": "https://arxiv.org/abs/2602.21612", "authors": ["Xuanqi Zeng", "Lingwei Zhang", "Linzhu Yue", "Zhitao Song", "Hongbo Zhang", "Tianlin Zhang", "Yun-Hui Liu"], "title": "Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization", "categories": ["cs.RO"], "comment": "8 pages, 12 figures", "summary": "Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m."}
{"id": "2602.21752", "pdf": "https://arxiv.org/pdf/2602.21752", "abs": "https://arxiv.org/abs/2602.21752", "authors": ["Minjie Tang", "Zunqi Li", "Photios A. Stavrou", "Marios Kountouris"], "title": "Pilot-Free Optimal Control over Wireless Networks: A Control-Aided Channel Prediction Approach", "categories": ["eess.SY", "eess.SP"], "comment": null, "summary": "A recurring theme in optimal controller design for wireless networked control systems (WNCS) is the reliance on real-time channel state information (CSI). However, acquiring accurate CSI a priori is notoriously challenging due to the time-varying nature of wireless channels. In this work, we propose a pilot-free framework for optimal control over wireless channels in which control commands are generated from plant states together with control-aided channel prediction. For linear plants operating over an orthogonal frequency-division multiplexing (OFDM) architecture, channel prediction is performed via a Kalman filter (KF), and the optimal control policy is derived from the Bellman principle. To alleviate the curse of dimensionality in computing the optimal control policy, we approximate the solution using a coupled algebraic Riccati equation (CARE), which can be computed efficiently via a stochastic approximation (SA) algorithm. Rigorous performance guarantees are established by proving the stability of both the channel predictor and the closed-loop system under the resulting control policy, providing sufficient conditions for the existence and uniqueness of a stabilizing approximate CARE solution, and establishing convergence of the SA-based control algorithm. The framework is further extended to nonlinear plants under general wireless architectures by combining a KalmanNet-based predictor with a Markov-modulated deep deterministic policy gradient (MM-DDPG) controller. Numerical results show that the proposed pilot-free approach outperforms benchmark schemes in both control performance and channel prediction accuracy for linear and nonlinear scenarios."}
{"id": "2602.21622", "pdf": "https://arxiv.org/pdf/2602.21622", "abs": "https://arxiv.org/abs/2602.21622", "authors": ["Enyi Wang", "Wen Fan", "Dandan Zhang"], "title": "ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation", "categories": ["cs.RO"], "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA 2026)", "summary": "Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios."}
{"id": "2602.21625", "pdf": "https://arxiv.org/pdf/2602.21625", "abs": "https://arxiv.org/abs/2602.21625", "authors": ["Lei Su", "Zhijie Peng", "Renyuan Ren", "Shengping Mao", "Juan Du", "Kaifeng Zhang", "Xuezhou Zhu"], "title": "Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap's deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot."}
{"id": "2602.21633", "pdf": "https://arxiv.org/pdf/2602.21633", "abs": "https://arxiv.org/abs/2602.21633", "authors": ["Chenyv Liu", "Wentao Tan", "Lei Zhu", "Fengling Li", "Jingjing Li", "Guoli Yang", "Heng Tao Shen"], "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA."}
{"id": "2602.21644", "pdf": "https://arxiv.org/pdf/2602.21644", "abs": "https://arxiv.org/abs/2602.21644", "authors": ["Li Zhang", "Yu-An Liu", "Xijia Jiang", "Conghao Huang", "Danyang Li", "Yanyong Zhang"], "title": "DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling", "categories": ["cs.RO"], "comment": null, "summary": "Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment."}
{"id": "2602.21666", "pdf": "https://arxiv.org/pdf/2602.21666", "abs": "https://arxiv.org/abs/2602.21666", "authors": ["Luying Feng", "Yaochu Jin", "Hanze Hu", "Wei Chen"], "title": "Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits", "categories": ["cs.RO"], "comment": null, "summary": "It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper."}
{"id": "2602.21670", "pdf": "https://arxiv.org/pdf/2602.21670", "abs": "https://arxiv.org/abs/2602.21670", "authors": ["Tomoya Kawabe", "Rin Takano"], "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Accepted to ICRA 2026. 8 pages, 2 figures", "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate."}
{"id": "2602.21682", "pdf": "https://arxiv.org/pdf/2602.21682", "abs": "https://arxiv.org/abs/2602.21682", "authors": ["Jishu Miao", "Han Chen", "Jiankun Zhai", "Qi Liu", "Tsubasa Hirakawa", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "title": "SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This \"dimensionality deficiency\" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers."}
{"id": "2602.21684", "pdf": "https://arxiv.org/pdf/2602.21684", "abs": "https://arxiv.org/abs/2602.21684", "authors": ["Xiaohan Lei", "Min Wang", "Wengang Zhou", "Xingyu Lu", "Houqiang Li"], "title": "Primary-Fine Decoupling for Action Generation in Robotic Imitation", "categories": ["cs.RO", "cs.LG"], "comment": "The Fourteenth International Conference on Learning Representations (ICLR), 2026", "summary": "Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation."}
{"id": "2602.21691", "pdf": "https://arxiv.org/pdf/2602.21691", "abs": "https://arxiv.org/abs/2602.21691", "authors": ["Yuting Zeng", "Manping Fan", "You Zhou", "Yongbin Yu", "Zhiwen Zheng", "Jingtao Zhang", "Liyong Ren", "Zhenglin Yang"], "title": "Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios", "categories": ["cs.RO"], "comment": "9 pages, 7 figures", "summary": "Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner."}
{"id": "2602.21696", "pdf": "https://arxiv.org/pdf/2602.21696", "abs": "https://arxiv.org/abs/2602.21696", "authors": ["Xiaorui Wang", "Hongwu Wang", "Yue Fan", "Hao Cheng", "Feitian Zhang"], "title": "Dual-Regime Hybrid Aerodynamic Modeling of Winged Blimps With Neural Mixing", "categories": ["cs.RO"], "comment": null, "summary": "Winged blimps operate across distinct aerodynamic regimes that cannot be adequately captured by a single model. At high speeds and small angles of attack, their dynamics exhibit strong coupling between lift and attitude, resembling fixed-wing aircraft behavior. At low speeds or large angles of attack, viscous effects and flow separation dominate, leading to drag-driven and damping-dominated dynamics. Accurately representing transitions between these regimes remains a fundamental challenge. This paper presents a hybrid aerodynamic modeling framework that integrates a fixed-wing Aerodynamic Coupling Model (ACM) and a Generalized Drag Model (GDM) using a learned neural network mixer with explicit physics-based regularization. The mixer enables smooth transitions between regimes while retaining explicit, physics-based aerodynamic representation. Model parameters are identified through a structured three-phase pipeline tailored for hybrid aerodynamic modeling. The proposed approach is validated on the RGBlimp platform through a large-scale experimental campaign comprising 1,320 real-world flight trajectories across 330 thruster and moving mass configurations, spanning a wide range of speeds and angles of attack. Experimental results demonstrate that the proposed hybrid model consistently outperforms single-model and predefined-mixer baselines, establishing a practical and robust aerodynamic modeling solution for winged blimps."}
{"id": "2602.21723", "pdf": "https://arxiv.org/pdf/2602.21723", "abs": "https://arxiv.org/abs/2602.21723", "authors": ["Yutang Lin", "Jieming Cui", "Yixuan Li", "Baoxiong Jia", "Yixin Zhu", "Siyuan Huang"], "title": "LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments."}
{"id": "2602.21736", "pdf": "https://arxiv.org/pdf/2602.21736", "abs": "https://arxiv.org/abs/2602.21736", "authors": ["Hao Luo", "Ye Wang", "Wanpeng Zhang", "Haoqi Yuan", "Yicheng Feng", "Haiweng Xu", "Sipeng Zheng", "Zongqing Lu"], "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild", "categories": ["cs.RO"], "comment": "CVPR2026", "summary": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data."}
{"id": "2602.21783", "pdf": "https://arxiv.org/pdf/2602.21783", "abs": "https://arxiv.org/abs/2602.21783", "authors": ["Beatrice Luciani", "Alex van den Berg", "Matti Lang", "Alexandre L. Ratschat", "Laura Marchal-Crespo"], "title": "Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control", "categories": ["cs.RO", "cs.LG"], "comment": "14 pages, 5 figures, 3 tables", "summary": "Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy."}
{"id": "2602.21811", "pdf": "https://arxiv.org/pdf/2602.21811", "abs": "https://arxiv.org/abs/2602.21811", "authors": ["Qingtao Liu", "Zhengnan Sun", "Yu Cui", "Haoming Li", "Gaofeng Li", "Lin Shao", "Jiming Chen", "Qi Ye"], "title": "DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations", "categories": ["cs.RO"], "comment": "Accepted by IEEE Transactions on Robotics (T-RO), 2026", "summary": "Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap."}
{"id": "2602.21816", "pdf": "https://arxiv.org/pdf/2602.21816", "abs": "https://arxiv.org/abs/2602.21816", "authors": ["Zhaowei Liang", "Song Wang", "Zhao Jin", "Shirui Wu", "Dan Wu"], "title": "Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects", "categories": ["cs.RO"], "comment": null, "summary": "Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/"}
{"id": "2602.21899", "pdf": "https://arxiv.org/pdf/2602.21899", "abs": "https://arxiv.org/abs/2602.21899", "authors": ["Arnau Romero", "Carmen Delgado", "Jana Baguer", "Raúl Suárez", "Xavier Costa-Pérez"], "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios", "categories": ["cs.RO", "cs.NI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2403.09177", "summary": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations."}
{"id": "2602.21967", "pdf": "https://arxiv.org/pdf/2602.21967", "abs": "https://arxiv.org/abs/2602.21967", "authors": ["Xiangqi Meng", "Pengxu Hou", "Zhenjun Zhao", "Javier Civera", "Daniel Cremers", "Hesheng Wang", "Haoang Li"], "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance."}
{"id": "2602.21983", "pdf": "https://arxiv.org/pdf/2602.21983", "abs": "https://arxiv.org/abs/2602.21983", "authors": ["Jingchao Wei", "Jingkai Qin", "Yuxiao Cao", "Jingcheng Huang", "Xiangrui Zeng", "Min Li", "Zhouping Yin"], "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots", "categories": ["cs.RO"], "comment": "submitted to AIM 2026", "summary": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions."}
{"id": "2602.22001", "pdf": "https://arxiv.org/pdf/2602.22001", "abs": "https://arxiv.org/abs/2602.22001", "authors": ["Freek Stulp", "Samuel Bustamante", "João Silvério", "Alin Albu-Schäffer", "Jeannette Bohg", "Shuran Song"], "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?", "categories": ["cs.RO"], "comment": "12 pages, 4 figures", "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology."}
{"id": "2602.22006", "pdf": "https://arxiv.org/pdf/2602.22006", "abs": "https://arxiv.org/abs/2602.22006", "authors": ["Jiadong Lu", "Zhehan Li", "Tao Han", "Miao Xu", "Chao Xu", "Yanjun Cao"], "title": "Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines", "categories": ["cs.RO"], "comment": "26 pages, 23 figures", "summary": "Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 °. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion."}
{"id": "2602.22010", "pdf": "https://arxiv.org/pdf/2602.22010", "abs": "https://arxiv.org/abs/2602.22010", "authors": ["Yue Su", "Sijin Chen", "Haixin Shi", "Mingyu Liu", "Zhengshen Zhang", "Ningyuan Huang", "Weiheng Zhong", "Zhengbang Zhu", "Yuxiao Liu", "Xihui Liu"], "title": "World Guidance: World Modeling in Condition Space for Action Generation", "categories": ["cs.RO", "cs.CV"], "comment": "Project Page: https://selen-suyue.github.io/WoGNet/", "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/"}
{"id": "2602.22056", "pdf": "https://arxiv.org/pdf/2602.22056", "abs": "https://arxiv.org/abs/2602.22056", "authors": ["Edgar Welte", "Yitian Shi", "Rosa Wolf", "Maximillian Gilles", "Rania Rayyes"], "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics."}
{"id": "2602.22088", "pdf": "https://arxiv.org/pdf/2602.22088", "abs": "https://arxiv.org/abs/2602.22088", "authors": ["Hongjie Fang", "Shirun Tang", "Mingyu Mei", "Haoxiang Qin", "Zihao He", "Jingjing Chen", "Ying Feng", "Chenxi Wang", "Wanxi Liu", "Zaixing He", "Cewu Lu", "Shiquan Wang"], "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/"}
{"id": "2602.22100", "pdf": "https://arxiv.org/pdf/2602.22100", "abs": "https://arxiv.org/abs/2602.22100", "authors": ["Andreas Kernbach", "Daniel Bargmann", "Werner Kraus", "Marco F. Huber"], "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %."}
{"id": "2602.22118", "pdf": "https://arxiv.org/pdf/2602.22118", "abs": "https://arxiv.org/abs/2602.22118", "authors": ["Benjamin Bokser", "Daniel Gonzalez", "Surya Singh", "Aaron Preston", "Alex Bahner", "Annika Wollschläger", "Arianna Ilvonen", "Asa Eckert-Erdheim", "Ashwin Khadke", "Bilal Hammoud", "Dean Molinaro", "Fabian Jenelten", "Henry Mayne", "Howie Choset", "Igor Bogoslavskyi", "Itic Tinman", "James Tigue", "Jan Preisig", "Kaiyu Zheng", "Kenny Sharma", "Kim Ang", "Laura Lee", "Liana Margolese", "Nicole Lin", "Oscar Frias", "Paul Drews", "Ravi Boggavarapu", "Rick Burnham", "Samuel Zapolsky", "Sangbae Kim", "Scott Biddlestone", "Sean Mayorga", "Shamel Fahmi", "Tyler McCollum", "Velin Dimitrov", "William Moyne", "Yu-Ming Chen", "Farbod Farshidian", "Marco Hutter", "David Perry", "Al Rizzi", "Gabe Nelson"], "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot", "categories": ["cs.RO"], "comment": "19 Pages, 11 figures, 3 movies, 2 tables", "summary": "Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height)."}
{"id": "2602.22154", "pdf": "https://arxiv.org/pdf/2602.22154", "abs": "https://arxiv.org/abs/2602.22154", "authors": ["Hossein B. Jond", "Veli Bakırcıoğlu", "Logan E. Beaver", "Nejat Tükenmez", "Adel Akbarimajd", "Martin Saska"], "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing", "categories": ["cs.RO"], "comment": null, "summary": "Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented."}
{"id": "2602.21319", "pdf": "https://arxiv.org/pdf/2602.21319", "abs": "https://arxiv.org/abs/2602.21319", "authors": ["Marion Neumeier", "Niklas Roßberg", "Michael Botsch", "Wolfgang Utschick"], "title": "Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "Accepted as a conference paper in IEEE Intelligent Vehicles Symposium (IV) 2026, Detroit, MI, United States", "summary": "Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.\n  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction."}
{"id": "2602.21904", "pdf": "https://arxiv.org/pdf/2602.21904", "abs": "https://arxiv.org/abs/2602.21904", "authors": ["Mariia Baidachna", "James Carty", "Aidan Ferguson", "Joseph Agrane", "Varad Kulkarni", "Aubrey Agub", "Michael Baxendale", "Aaron David", "Rachel Horton", "Elliott Atkinson"], "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 9 figures. Accepted to ICCV End-to-End 3D Learning Workshop 2025 and presented as a poster; not included in the final proceedings due to a conference administrative error", "summary": "Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estimation and the potential for color prediction. Our model achieves substantial improvements in keypoint accuracy over conventional methods. Furthermore, we leverage our predicted keypoints in the perception pipeline and evaluate the end-to-end autonomous system. Our results show high-quality performance across all metrics, highlighting the effectiveness of this approach and its potential for adoption in competitive autonomous racing systems."}
{"id": "2602.21954", "pdf": "https://arxiv.org/pdf/2602.21954", "abs": "https://arxiv.org/abs/2602.21954", "authors": ["Xinkai Ji", "Pan Liu", "Yu Han"], "title": "The Swarm Intelligence Freeway-Urban Trajectories (SWIFTraj) Dataset - Part II: A Graph-Based Approach for Trajectory Connection", "categories": ["physics.soc-ph", "cs.RO"], "comment": null, "summary": "In Part I of this companion paper series, we introduced SWIFTraj, a new open-source vehicle trajectory dataset collected using a unmanned aerial vehicle (UAV) swarm. The dataset has two distinctive features. First, by connecting trajectories across consecutive UAV videos, it provides long-distance continuous trajectories, with the longest exceeding 4.5 km. Second, it covers an integrated traffic network consisting of both freeways and their connected urban roads. Obtaining such long-distance continuous trajectories from a UAV swarm is challenging, due to the need for accurate time alignment across multiple videos and the irregular spatial distribution of UAVs. To address these challenges, this paper proposes a novel graph-based approach for connecting vehicle trajectories captured by a UAV swarm. An undirected graph is constructed to represent flexible UAV layouts, and an automatic time alignment method based on trajectory matching cost minimization is developed to estimate optimal time offsets across videos. To associate trajectories of the same vehicle observed in different videos, a vehicle matching table is established using the Hungarian algorithm. The proposed approach is evaluated using both simulated and real-world data. Results from real-world experiments show that the time alignment error is within three video frames, corresponding to approximately 0.1 s, and that the vehicle matching achieves an F1-score of about 0.99. These results demonstrate the effectiveness of the proposed method in addressing key challenges in UAV-based trajectory connection and highlight its potential for large-scale vehicle trajectory collection."}
