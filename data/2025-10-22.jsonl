{"id": "2510.17808", "pdf": "https://arxiv.org/pdf/2510.17808", "abs": "https://arxiv.org/abs/2510.17808", "authors": ["Amirhesam Aghanouri", "Mohamed Sabry", "Joshua Cherian Varughese", "Cristina Olaverri-Monreal"], "title": "Machine Learning-Based Performance Evaluation of a Solar-Powered Hydrogen Fuel Cell Hybrid in a Radio-Controlled Electric Vehicle", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents an experimental investigation and performance evaluation\nof a hybrid electric radio-controlled car powered by a Nickel-Metal Hydride\nbattery combined with a renewable Proton Exchange Membrane Fuel Cell system.\nThe study evaluates the performance of the system under various load-carrying\nscenarios and varying environmental conditions, simulating real-world operating\nconditions including throttle operation. In order to build a predictive model,\ngather operational insights, and detect anomalies, data-driven analyses using\nsignal processing and modern machine learning techniques were employed.\nSpecifically, machine learning techniques were used to distinguish throttle\nlevels with high precision based on the operational data. Anomaly and change\npoint detection methods enhanced voltage stability, resulting in fewer critical\nfaults in the hybrid system compared to battery-only operation. Temporal\nConvolutional Networks were effectively employed to predict voltage behavior,\ndemonstrating potential for use in planning the locations of fueling or\ncharging stations. Moreover, integration with a solar-powered electrolyzer\nconfirmed the system's potential for off-grid, renewable hydrogen use. The\nresults indicate that integrating a Proton Exchange Membrane Fuel Cell with\nNickel-Metal Hydride batteries significantly improves electrical performance\nand reliability for small electric vehicles, and these findings can be a\npotential baseline for scaling up to larger vehicles."}
{"id": "2510.17809", "pdf": "https://arxiv.org/pdf/2510.17809", "abs": "https://arxiv.org/abs/2510.17809", "authors": ["Massimo Capurso", "Luciano Afferrante"], "title": "In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning", "categories": ["eess.SP", "cs.LG", "I.2"], "comment": "20 pages, 17 figures, 3 tables, 33 references", "summary": "In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)\nrequirements demand high-precision finishing operations such as power honing.\nConventional quality control strategies rely on post-process inspections and\nStatistical Process Control (SPC), which fail to capture transient machining\nanomalies and cannot ensure real-time defect detection. This study proposes a\nnovel, data-driven framework for in-process monitoring of gear power honing\nusing vibration signal analysis and machine learning. Our proposed methodology\ninvolves continuous data acquisition via accelerometers, followed by\ntime-frequency signal analysis. We investigate and compare the efficacy of\nthree subspace learning methods for features extraction: (1) Principal\nComponent Analysis (PCA) for dimensionality reduction; (2) a two-stage\nframework combining PCA with Linear Discriminant Analysis (LDA) for enhanced\nclass separation; and (3) Uncorrelated Multilinear Discriminant Analysis with\nRegularization (R-UMLDA), adapted for tensor data, which enforces feature\ndecorrelation and includes regularization for small sample sizes. These\nextracted features are then fed into a Support Vector Machine (SVM) classifier\nto predict four distinct gear quality categories, established through rigorous\ngeometrical inspections and test bench results of assembled gearboxes. The\nmodels are trained and validated on an experimental dataset collected in an\nindustrial context during gear power-honing operations, with gears classified\ninto four different quality categories. The proposed framework achieves high\nclassification accuracy (up to 100%) in an industrial setting. The approach\noffers interpretable spectral features that correlate with process dynamics,\nenabling practical integration into real-time monitoring and predictive\nmaintenance systems."}
{"id": "2510.17810", "pdf": "https://arxiv.org/pdf/2510.17810", "abs": "https://arxiv.org/abs/2510.17810", "authors": ["Camilo Quiceno Quintero", "Sandip Varkey George"], "title": "Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification", "categories": ["eess.SP", "cs.LG", "nlin.CD", "physics.data-an"], "comment": "Version submitted to NODYCON 2025", "summary": "The complex dynamics of the heart are reflected in its electrical activity,\ncaptured through electrocardiograms (ECGs). In this study we use nonlinear time\nseries analysis to understand how ECG complexity varies with cardiac pathology.\nUsing the large PTB-XL dataset, we extracted nonlinear measures from lead II\nECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations\nand mutual information. Significant differences between diseased and healthy\nindividuals were found in almost all measures between healthy and diseased\nclasses, and between 5 diagnostic superclasses ($p<.001$). Moreover,\nincorporating these complexity quantifiers into machine learning models\nsubstantially improved classification accuracy measured using area under the\nROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90\n(including cross-time series metrics)."}
{"id": "2510.17811", "pdf": "https://arxiv.org/pdf/2510.17811", "abs": "https://arxiv.org/abs/2510.17811", "authors": ["Zhixing Wang", "Renzhi Yuan", "Haifeng Yao", "Chuang Yang", "Mugen Peng"], "title": "Channel Modeling of Satellite-to-Underwater Laser Communication Links: An Analytical-Monte Carlo Hybrid Approach", "categories": ["eess.SP", "physics.ao-ph"], "comment": null, "summary": "Channel modeling for satellite-to-underwater laser communication (StULC)\nlinks remains challenging due to long distances and the diversity of the\nchannel constituents. The StULC channel is typically segmented into three\nisolated channels: the atmospheric channel, the air-water interface channel,\nand the underwater channel. Previous studies involving StULC channel modeling\neither focused on separated channels or neglected the combined effects of\nparticles and turbulence on laser propagation. In this paper, we established a\ncomprehensive StULC channel model by an analytical-Monte Carlo hybrid approach,\ntaking into account the effects of both particles and turbulence. We first\nobtained the intensity distribution of the transmitted laser beam after passing\nthrough the turbulent atmosphere based on the extended Huygens-Fresnel\nprinciple. Then we derived a closed-form probability density function of the\nphoton propagating direction after passing through the air-water interface,\nwhich greatly simplified the modeling of StULC links. At last, we employed a\nMonte Carlo method to model the underwater links and obtained the power\ndistribution at the receiving plane. Based on the proposed StULC channel model,\nwe analyzed the bit error rate and the outage probability under different\nenvironmental conditions. Numerical results demonstrated that, the influence of\nunderwater particle concentration on the communication performance is much\npronounced than those of both the atmospheric turbulence and the underwater\nturbulence. Notably, increasing the wind speed at the air-water interface does\nnot significantly worsen the communication performance of the StULC links."}
{"id": "2510.17948", "pdf": "https://arxiv.org/pdf/2510.17948", "abs": "https://arxiv.org/abs/2510.17948", "authors": ["Christopher A McClurg", "Alan R Wagner"], "title": "Studying the Effects of Robot Intervention on School Shooters in Virtual Reality", "categories": ["cs.RO", "cs.AI", "cs.CY", "cs.HC"], "comment": "Preprint under review for conference publication. 10 pages, 9\n  figures, 3 tables (including 1-page appendix)", "summary": "We advance the understanding of robotic intervention in high-risk scenarios\nby examining their potential to distract and impede a school shooter. To\nevaluate this concept, we conducted a virtual reality study with 150 university\nparticipants role-playing as a school shooter. Within the simulation, an\nautonomous robot predicted the shooter's movements and positioned itself\nstrategically to interfere and distract. The strategy the robot used to\napproach the shooter was manipulated -- either moving directly in front of the\nshooter (aggressive) or maintaining distance (passive) -- and the distraction\nmethod, ranging from no additional cues (low), to siren and lights (medium), to\nsiren, lights, and smoke to impair visibility (high). An aggressive,\nhigh-distraction robot reduced the number of victims by 46.6% relative to a\nno-robot control. This outcome underscores both the potential of robotic\nintervention to enhance safety and the pressing ethical questions surrounding\ntheir use in school environments."}
{"id": "2510.17816", "pdf": "https://arxiv.org/pdf/2510.17816", "abs": "https://arxiv.org/abs/2510.17816", "authors": ["Xin Li", "Jingzhi Hu", "Yinghui He", "Hongbo Wang", "Jin Gan", "Jun Luo"], "title": "Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Wi-Fi-based human activity recognition (HAR) provides substantial convenience\nand has emerged as a thriving research field, yet the coarse spatial resolution\ninherent to Wi-Fi significantly hinders its ability to distinguish multiple\nsubjects. By exploiting the near-field domination effect, establishing a\ndedicated sensing link for each subject through their personal Wi-Fi device\noffers a promising solution for multi-person HAR under native traffic. However,\ndue to the subject-specific characteristics and irregular patterns of\nnear-field signals, HAR neural network models require fine-tuning (FT) for\ncross-domain adaptation, which becomes particularly challenging with certain\ncategories unavailable. In this paper, we propose WiAnchor, a novel training\nframework for efficient cross-domain adaptation in the presence of incomplete\nactivity categories. This framework processes Wi-Fi signals embedded with\nirregular time information in three steps: during pre-training, we enlarge\ninter-class feature margins to enhance the separability of activities; in the\nFT stage, we innovate an anchor matching mechanism for cross-domain adaptation,\nfiltering subject-specific interference informed by incomplete activity\ncategories, rather than attempting to extract complete features from them;\nfinally, the recognition of input samples is further improved based on their\nfeature-level similarity with anchors. We construct a comprehensive dataset to\nthoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with\nabsent activity categories."}
{"id": "2510.17950", "pdf": "https://arxiv.org/pdf/2510.17950", "abs": "https://arxiv.org/abs/2510.17950", "authors": ["Adina Yakefu", "Bin Xie", "Chongyang Xu", "Enwen Zhang", "Erjin Zhou", "Fan Jia", "Haitao Yang", "Haoqiang Fan", "Haowei Zhang", "Hongyang Peng", "Jing Tan", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Qinglun Zhang", "Ruitao Zhang", "Saike Huang", "Shen Cheng", "Shuaicheng Liu", "Tiancai Wang", "Tiezhen Wang", "Wei Sun", "Wenbin Tang", "Yajun Wei", "Yang Chen", "Youqiang Gui", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yunhuan Yang", "Yutong Guo", "Ze Chen", "Zhengyuan Du", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan"], "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies", "categories": ["cs.RO"], "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://robochallenge.ai", "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30."}
{"id": "2510.17818", "pdf": "https://arxiv.org/pdf/2510.17818", "abs": "https://arxiv.org/abs/2510.17818", "authors": ["Salar Nouri"], "title": "Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "This paper tackles the challenging problem of gridless two-dimensional (2D)\ndirection-of-arrival (DOA) estimation for a uniform circular array (UCA) from a\nsingle snapshot of data. Conventional gridless methods often fail in this\nscenario due to prohibitive computational costs or a lack of robustness. We\npropose a novel framework that overcomes these limitations by jointly\nestimating a manifold transformation matrix and the source azimuth-elevation\npairs within a single, unified optimization problem. This problem is solved\nefficiently using an inexact Augmented Lagrangian Method (iALM), which\ncompletely circumvents the need for semidefinite programming. By unifying the\nobjectives of data fidelity and transformation robustness, our approach is\nuniquely suited for the demanding single-snapshot case. Simulation results\nconfirm that the proposed iALM framework provides robust and high-resolution,\ngridless 2D-DOA estimates, establishing its efficacy for challenging array\nsignal processing applications."}
{"id": "2510.18002", "pdf": "https://arxiv.org/pdf/2510.18002", "abs": "https://arxiv.org/abs/2510.18002", "authors": ["Junli Ren", "Junfeng Long", "Tao Huang", "Huayi Wang", "Zirui Wang", "Feiyu Jia", "Wentao Zhang", "Jingbo Wang", "Ping Luo", "Jiangmiao Pang"], "title": "Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints", "categories": ["cs.RO"], "comment": null, "summary": "We present a reinforcement learning framework for autonomous goalkeeping with\nhumanoid robots in real-world scenarios. While prior work has demonstrated\nsimilar capabilities on quadrupedal platforms, humanoid goalkeeping introduces\ntwo critical challenges: (1) generating natural, human-like whole-body motions,\nand (2) covering a wider guarding range with an equivalent response time.\nUnlike existing approaches that rely on separate teleoperation or fixed motion\ntracking for whole-body control, our method learns a single end-to-end RL\npolicy, enabling fully autonomous, highly dynamic, and human-like robot-object\ninteractions. To achieve this, we integrate multiple human motion priors\nconditioned on perceptual inputs into the RL training via an adversarial\nscheme. We demonstrate the effectiveness of our method through real-world\nexperiments, where the humanoid robot successfully performs agile, autonomous,\nand naturalistic interceptions of fast-moving balls. In addition to\ngoalkeeping, we demonstrate the generalization of our approach through tasks\nsuch as ball escaping and grabbing. Our work presents a practical and scalable\nsolution for enabling highly dynamic interactions between robots and moving\nobjects, advancing the field toward more adaptive and lifelike robotic\nbehaviors."}
{"id": "2510.17821", "pdf": "https://arxiv.org/pdf/2510.17821", "abs": "https://arxiv.org/abs/2510.17821", "authors": ["Long Lin", "Pablo Peiro-Corbacho", "Pablo Ávila", "Alejandro Carta-Bergaz", "Ángel Arenal", "Gonzalo R. Ríos-Muñoz", "Carlos Sevilla-Salcedo"], "title": "CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Intracavitary atrial electrograms (EGMs) provide high-resolution insights\ninto cardiac electrophysiology but are often contaminated by noise and remain\nhigh-dimensional, limiting real-time analysis. We introduce CLARAE\n(CLArity-preserving Reconstruction AutoEncoder), a one-dimensional\nencoder--decoder designed for atrial EGMs, which achieves both high-fidelity\nreconstruction and a compact 64-dimensional latent representation. CLARAE is\ndesigned to preserve waveform morphology, mitigate reconstruction artifacts,\nand produce interpretable embeddings through three principles: downsampling\nwith pooling, a hybrid interpolation--convolution upsampling path, and a\nbounded latent space.\n  We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29\npatients across three rhythm types (AF, SR300, SR600). Performance was\nbenchmarked against six state-of-the-art autoencoders using reconstruction\nmetrics, rhythm classification, and robustness across signal-to-noise ratios\nfrom -5 to 15 dB. In downstream rhythm classification, CLARAE achieved\nF1-scores above 0.97 for all rhythm types, and its latent space showed clear\nclustering by rhythm. In denoising tasks, it consistently ranked among the top\nperformers for both unipolar and bipolar signals.\n  In order to promote reproducibility and enhance accessibility, we offer an\ninteractive web-based application. This platform enables users to explore\npre-trained CLARAE models, visualize the reconstructions, and compute metrics\nin real time. Overall, CLARAE combines robust denoising with compact,\ndiscriminative representations, offering a practical foundation for clinical\nworkflows such as rhythm discrimination, signal quality assessment, and\nreal-time mapping."}
{"id": "2510.18063", "pdf": "https://arxiv.org/pdf/2510.18063", "abs": "https://arxiv.org/abs/2510.18063", "authors": ["Bin-Bin Hu", "Weijia Yao", "Ming Cao"], "title": "MOFM-Nav: On-Manifold Ordering-Flexible Multi-Robot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "This paper addresses the problem of multi-robot navigation where robots\nmaneuver on a desired \\(m\\)-dimensional (i.e., \\(m\\)-D) manifold in the\n$n$-dimensional Euclidean space, and maintain a {\\it flexible spatial\nordering}. We consider $ m\\geq 2$, and the multi-robot coordination is achieved\nvia non-Euclidean metrics. However, since the $m$-D manifold can be\ncharacterized by the zero-level sets of $n$ implicit functions, the last $m$\nentries of the GVF propagation term become {\\it strongly coupled} with the\npartial derivatives of these functions if the auxiliary vectors are not\nappropriately chosen. These couplings not only influence the on-manifold\nmaneuvering of robots, but also pose significant challenges to the further\ndesign of the ordering-flexible coordination via non-Euclidean metrics.\n  To tackle this issue, we first identify a feasible solution of auxiliary\nvectors such that the last $m$ entries of the propagation term are effectively\ndecoupled to be the same constant. Then, we redesign the coordinated GVF (CGVF)\nalgorithm to {\\it boost} the advantages of singularities elimination and global\nconvergence by treating $m$ manifold parameters as additional $m$ virtual\ncoordinates. Furthermore, we enable the on-manifold ordering-flexible motion\ncoordination by allowing each robot to share $m$ virtual coordinates with its\ntime-varying neighbors and a virtual target robot, which {\\it circumvents} the\npossible complex calculation if Euclidean metrics were used instead. Finally,\nwe showcase the proposed algorithm's flexibility, adaptability, and robustness\nthrough extensive simulations with different initial positions,\nhigher-dimensional manifolds, and robot breakdown, respectively."}
{"id": "2510.17823", "pdf": "https://arxiv.org/pdf/2510.17823", "abs": "https://arxiv.org/abs/2510.17823", "authors": ["Saeed Mohammadzadeh", "Rodrigo C. de Lamare", "Yuriy Zakharov"], "title": "Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": "13 figures, 14 pages", "summary": "This work proposes an efficient, robust adaptive beamforming technique to\ndeal with steering vector (SV) estimation mismatches and data covariance matrix\nreconstruction problems. In particular, the direction-of-arrival(DoA) of\ninterfering sources is estimated with available snapshots in which the angular\nsectors of the interfering signals are computed adaptively. Then, we utilize\nthe well-known general linear combination algorithm to reconstruct the\ninterference-plus-noise covariance (IPNC) matrix using preprocessing-based\nspatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be\nreplaced by the sample covariance matrix (SCM) in the shrinkage method. A power\nspectrum sampling strategy is then devised based on a preprocessing matrix\ncomputed with the estimated angular sectors' information. Moreover, the\ncovariance matrix for the signal is formed for the angular sector of the\nsignal-of-interest (SOI), which allows for calculating an SV for the SOI using\nthe power method. An analysis of the array beampattern in the proposed PPBSS\ntechnique is carried out, and a study of the computational cost of competing\napproaches is conducted. Simulation results show the proposed method's\neffectiveness compared to existing approaches."}
{"id": "2510.18085", "pdf": "https://arxiv.org/pdf/2510.18085", "abs": "https://arxiv.org/abs/2510.18085", "authors": ["Connor Mattson", "Varun Raveendra", "Ellen Novoseller", "Nicholas Waytowich", "Vernon J. Lawhern", "Daniel S. Brown"], "title": "R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "9 pages, 6 figures", "summary": "Imitation Learning (IL) is a natural way for humans to teach robots,\nparticularly when high-quality demonstrations are easy to obtain. While IL has\nbeen widely applied to single-robot settings, relatively few studies have\naddressed the extension of these methods to multi-agent systems, especially in\nsettings where a single human must provide demonstrations to a team of\ncollaborating robots. In this paper, we introduce and study Round-Robin\nBehavior Cloning (R2BC), a method that enables a single human operator to\neffectively train multi-robot systems through sequential, single-agent\ndemonstrations. Our approach allows the human to teleoperate one agent at a\ntime and incrementally teach multi-agent behavior to the entire system, without\nrequiring demonstrations in the joint multi-agent action space. We show that\nR2BC methods match, and in some cases surpass, the performance of an oracle\nbehavior cloning approach trained on privileged synchronized demonstrations\nacross four multi-agent simulated tasks. Finally, we deploy R2BC on two\nphysical robot tasks trained using real human demonstrations."}
{"id": "2510.17825", "pdf": "https://arxiv.org/pdf/2510.17825", "abs": "https://arxiv.org/abs/2510.17825", "authors": ["Shumaila Javaid", "Nasir Saeed"], "title": "Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin", "categories": ["eess.SP", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as\nkey enablers of 6G, providing global connectivity for applications such as\nautonomous transportation, Industrial IoT, and disaster response. Their\nlarge-scale deployment, however, risks unsustainable energy use and carbon\nemissions. This work advances prior energy-aware studies by proposing a\ncarbon-aware orchestration framework for ISATNs that leverages Digital Twin\n(DT) technology. The framework adopts grams of CO$_2$-equivalent per bit\n(gCO$_2$/bit) as a primary sustainability metric and implements a multi\ntimescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting\nwith real-time adaptive optimization. ISATN-specific control knobs, including\ncarbon-aware handovers, UAV duty cycling, and renewable-aware edge placement,\nare exploited to reduce emissions. Simulation results with real carbon\nintensity data show up to 29\\% lower gCO$_2$/bit than QoS-only orchestration,\nwhile improving renewable utilization and resilience under adverse events."}
{"id": "2510.18127", "pdf": "https://arxiv.org/pdf/2510.18127", "abs": "https://arxiv.org/abs/2510.18127", "authors": ["Dharmik Patel", "Antonio Rafael Vazquez Pantoja", "Jiuzhou Lei", "Kiju Lee", "Xiao Liang", "Minghui Zheng"], "title": "ANGEL: A Novel Gripper for Versatile and Light-touch Fruit Harvesting", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Fruit harvesting remains predominantly a labor-intensive process, motivating\nthe development of research for robotic grippers. Conventional rigid or\nvacuum-driven grippers require complex mechanical design or high energy\nconsumption. Current enveloping-based fruit harvesting grippers lack\nadaptability to fruits of different sizes. This paper introduces a\ndrawstring-inspired, cable-driven soft gripper for versatile and gentle fruit\nharvesting. The design employs 3D-printed Thermoplastic Polyurethane (TPU)\npockets with integrated steel wires that constrict around the fruit when\nactuated, distributing pressure uniformly to minimize bruising and allow\nversatility to fruits of varying sizes. The lightweight structure, which\nrequires few components, reduces mechanical complexity and cost compared to\nother grippers. Actuation is achieved through servo-driven cable control, while\nmotor feedback provides autonomous grip adjustment with tunable grip strength.\nExperimental validation shows that, for tomatoes within the gripper's effective\nsize range, harvesting was achieved with a 0% immediate damage rate and a\nbruising rate of less than 9% after five days, reinforcing the gripper's\nsuitability for fruit harvesting."}
{"id": "2510.17832", "pdf": "https://arxiv.org/pdf/2510.17832", "abs": "https://arxiv.org/abs/2510.17832", "authors": ["Henrique de Lima Alexandre", "Clodoaldo Aparecido de Moraes Lima"], "title": "Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "15 pages, BRACIS", "summary": "Electroencephalography (EEG) is a widely used, non-invasive method for\ncapturing brain activity, and is particularly relevant for applications in\nBrain-Computer Interfaces (BCI). However, collecting high-quality EEG data\nremains a major challenge due to sensor costs, acquisition time, and\ninter-subject variability. To address these limitations, this study proposes a\nmethodology for generating synthetic EEG signals associated with motor imagery\nbrain tasks using Diffusion Probabilistic Models (DDPM). The approach involves\npreprocessing real EEG data, training a diffusion model to reconstruct EEG\nchannels from noise, and evaluating the quality of the generated signals\nthrough both signal-level and task-level metrics. For validation, we employed\nclassifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks\n(CNN), and U-Net to compare the performance of synthetic data against real data\nin classification tasks. The generated data achieved classification accuracies\nabove 95%, with low mean squared error and high correlation with real signals.\n  Our results demonstrate that synthetic EEG signals produced by diffusion\nmodels can effectively complement datasets, improving classification\nperformance in EEG-based BCIs and addressing data scarcity."}
{"id": "2510.18137", "pdf": "https://arxiv.org/pdf/2510.18137", "abs": "https://arxiv.org/abs/2510.18137", "authors": ["Hrishikesh Sathyanarayan", "Victor Vantilborgh", "Ian Abraham"], "title": "Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we investigate the utility of datasets and whether more data\nor the 'right' data is advantageous for robot learning. In particular, we are\ninterested on quantifying the utility of contact-based data as contact holds\nsignificant information for robot learning. Our approach derives a\ncontact-aware objective function for learning object dynamics and shape from\npose and contact data. We show that the contact-aware Fisher-information metric\ncan be used to rank and curate contact-data based on how informative data is\nfor learning. In addition, we find that selecting a reduced dataset based on\nthis ranking improves the learning task while also making learning a\ndeterministic process. Interestingly, our results show that more data is not\nnecessarily advantageous, and rather, less but informative data can accelerate\nlearning, especially depending on the contact interactions. Last, we show how\nour metric can be used to provide initial guidance on data curation for\ncontact-based robot learning."}
{"id": "2510.17836", "pdf": "https://arxiv.org/pdf/2510.17836", "abs": "https://arxiv.org/abs/2510.17836", "authors": ["G. Messa", "G. Acconciaioco", "S. Ripani", "L. Bozzelli", "A. Simone", "O. Giustolisi"], "title": "Two Phases Leakage Detection Strategy Supported by DMAs", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "The present work proposes a novel two phases model-based strategy for leakage\ndetection. The two phases are: the identification of the district metering area\n(DMA) and the pipe pre-localization into the identified DMA. The strategy is\nbased on detecting and pre-localizing the punctual leakage as anomaly with\nrespect to the normal working conditions. A further novelty is the fact that\nthe pre-localization phase returns the sequence of pipes to inspect, which\nmakes the strategy attractive for water utilities, whose aim is to identify the\nanomaly at DMA level and, successively, to localize it with the minimum\ninspection cost. Furthermore, a random database is useful to test the\nperformance of the strategy with respect to the configuration of DMAs and the\npressure metering system. Consequently, a novel strategy to design the location\nof pressure meters is also proposed. It is demonstrated that the entire\nstrategy limits false positives during the DMA identification phase by using\nthe recently proposed index named Asset Management Support Indicator (AMSI).\nAMSI is invariant with respect to the deterioration, i.e., it is sensitive to\nits increase causing punctual leakage. The strategy is studied and discussed\nusing two real Apulian WDNs managed by Acquedotto Pugliese."}
{"id": "2510.18316", "pdf": "https://arxiv.org/pdf/2510.18316", "abs": "https://arxiv.org/abs/2510.18316", "authors": ["Chengshu Li", "Mengdi Xu", "Arpit Bahety", "Hang Yin", "Yunfan Jiang", "Huang Huang", "Josiah Wong", "Sujay Garlanka", "Cem Gokmen", "Ruohan Zhang", "Weiyu Liu", "Jiajun Wu", "Roberto Martín-Martín", "Li Fei-Fei"], "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project website: momagen.github.io. The first four authors contribute\n  equally", "summary": "Imitation learning from large-scale, diverse human demonstrations has proven\neffective for training robots, but collecting such data is costly and\ntime-consuming. This challenge is amplified for multi-step bimanual mobile\nmanipulation, where humans must teleoperate both a mobile base and two\nhigh-degree-of-freedom arms. Prior automated data generation frameworks have\naddressed static bimanual manipulation by augmenting a few human demonstrations\nin simulation, but they fall short for mobile settings due to two key\nchallenges: (1) determining base placement to ensure reachability, and (2)\npositioning the camera to provide sufficient visibility for visuomotor\npolicies. To address these issues, we introduce MoMaGen, which formulates data\ngeneration as a constrained optimization problem that enforces hard constraints\n(e.g., reachability) while balancing soft constraints (e.g., visibility during\nnavigation). This formulation generalizes prior approaches and provides a\nprincipled foundation for future methods. We evaluate MoMaGen on four\nmulti-step bimanual mobile manipulation tasks and show that it generates\nsignificantly more diverse datasets than existing methods. Leveraging this\ndiversity, MoMaGen can train successful imitation learning policies from a\nsingle source demonstration, and these policies can be fine-tuned with as few\nas 40 real-world demonstrations to achieve deployment on physical robotic\nhardware. More details are available at our project page: momagen.github.io."}
{"id": "2510.18008", "pdf": "https://arxiv.org/pdf/2510.18008", "abs": "https://arxiv.org/abs/2510.18008", "authors": ["Henrik Hellström", "Jiwon Jeong", "Ayfer Özgür", "Viktoria Fodor", "Carlo Fischione"], "title": "Majority Vote Compressed Sensing", "categories": ["eess.SP"], "comment": null, "summary": "We consider the problem of non-coherent over-the-air computation (AirComp),\nwhere $n$ devices carry high-dimensional data vectors\n$\\mathbf{x}_i\\in\\mathbb{R}^d$ of sparsity $\\lVert\\mathbf{x}_i\\rVert_0\\leq k$\nwhose sum has to be computed at a receiver. Previous results on non-coherent\nAirComp require more than $d$ channel uses to compute functions of\n$\\mathbf{x}_i$, where the extra redundancy is used to combat non-coherent\nsignal aggregation. However, if the data vectors are sparse, sparsity can be\nexploited to offer significantly cheaper communication. In this paper, we\npropose to use random transforms to transmit lower-dimensional projections\n$\\mathbf{s}_i\\in\\mathbb{R}^T$ of the data vectors. These projected vectors are\ncommunicated to the receiver using a majority vote (MV)-AirComp scheme, which\nestimates the bit-vector corresponding to the signs of the aggregated\nprojections, i.e., $\\mathbf{y} = \\text{sign}(\\sum_i\\mathbf{s}_i)$. By\nleveraging 1-bit compressed sensing (1bCS) at the receiver, the real-valued and\nhigh-dimensional aggregate $\\sum_i\\mathbf{x}_i$ can be recovered from\n$\\mathbf{y}$. We prove analytically that the proposed MVCS scheme estimates the\naggregated data vector $\\sum_i \\mathbf{x}_i$ with $\\ell_2$-norm error\n$\\epsilon$ in $T=\\mathcal{O}(kn\\log(d)/\\epsilon^2)$ channel uses. Moreover, we\nspecify algorithms that leverage MVCS for histogram estimation and distributed\nmachine learning. Finally, we provide numerical evaluations that reveal the\nadvantage of MVCS compared to the state-of-the-art."}
{"id": "2510.18337", "pdf": "https://arxiv.org/pdf/2510.18337", "abs": "https://arxiv.org/abs/2510.18337", "authors": ["Wenhui Huang", "Changhe Chen", "Han Qi", "Chen Lv", "Yilun Du", "Heng Yang"], "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning", "categories": ["cs.RO"], "comment": null, "summary": "Integrating visual-language instructions into visuomotor policies is gaining\nmomentum in robot learning for enhancing open-world generalization. Despite\npromising advances, existing approaches face two challenges: limited language\nsteerability when no generated reasoning is used as a condition, or significant\ninference latency when reasoning is incorporated.In this work, we introduce\nMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)\nmodel that integrates fast-slow unified reasoning with behavior policy\nlearning. MoTVLA preserves the general intelligence of pre-trained VLMs\n(serving as the generalist) for tasks such as perception, scene understanding,\nand semantic planning, while incorporating a domain expert, a second\ntransformer that shares knowledge with the pretrained VLM, to generate\ndomain-specific fast reasoning (e.g., robot motion decomposition), thereby\nimproving policy execution efficiency. By conditioning the action expert on\ndecomposed motion instructions, MoTVLA can learn diverse behaviors and\nsubstantially improve language steerability. Extensive evaluations across\nnatural language processing benchmarks, robotic simulation environments, and\nreal-world experiments confirm the superiority of MoTVLA in both fast-slow\nreasoning and manipulation task performance."}
{"id": "2510.18336", "pdf": "https://arxiv.org/pdf/2510.18336", "abs": "https://arxiv.org/abs/2510.18336", "authors": ["Wangye Jiang", "Haoming Yang", "Xinyu Lu", "Mingyuan Wang", "Huimei Sun", "Jingya Zhang"], "title": "MCANet: A Coherent Multimodal Collaborative Attention Network for Advanced Modulation Recognition in Adverse Noisy Environments", "categories": ["eess.SP"], "comment": null, "summary": "As wireless communication systems evolve, automatic modulation recognition\n(AMR) plays a key role in improving spectrum efficiency, especially in\ncognitive radio systems. Traditional AMR methods face challenges in complex,\nnoisy environments, particularly in low signal-to-noise ratio (SNR) conditions.\nThis paper introduces MCANet (Multimodal Collaborative Attention Network), a\nmultimodal deep learning framework designed to address these challenges. MCANet\nemploys refined feature extraction and global modeling to support its fusion\nstrategy.Experimental results across multiple benchmark datasets show that\nMCANet outperforms mainstream AMR models, offering better robustness in low-SNR\nconditions."}
{"id": "2510.18347", "pdf": "https://arxiv.org/pdf/2510.18347", "abs": "https://arxiv.org/abs/2510.18347", "authors": ["Muhammad Hanif", "Reiji Terunuma", "Takumi Sumino", "Kelvin Cheng", "Takeshi Hatanaka"], "title": "Coverage-Recon: Coordinated Multi-Drone Image Sampling with Online Map Feedback", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": "Submitted to IEEE Transactions on Control Systems Technology (under\n  review). Project page: https://htnk-lab.github.io/coverage-recon/", "summary": "This article addresses collaborative 3D map reconstruction using multiple\ndrones. Achieving high-quality reconstruction requires capturing images of\nkeypoints within the target scene from diverse viewing angles, and coverage\ncontrol offers an effective framework to meet this requirement. Meanwhile,\nrecent advances in real-time 3D reconstruction algorithms make it possible to\nrender an evolving map during flight, enabling immediate feedback to guide\ndrone motion. Building on this, we present Coverage-Recon, a novel coordinated\nimage sampling algorithm that integrates online map feedback to improve\nreconstruction quality on-the-fly. In Coverage-Recon, the coordinated motion of\ndrones is governed by a Quadratic Programming (QP)-based angle-aware coverage\ncontroller, which ensures multi-viewpoint image capture while enforcing safety\nconstraints. The captured images are processed in real time by the NeuralRecon\nalgorithm to generate an evolving 3D mesh. Mesh changes across the scene are\ninterpreted as indicators of reconstruction uncertainty and serve as feedback\nto update the importance index of the coverage control as the map evolves. The\neffectiveness of Coverage-Recon is validated through simulation and\nexperiments, demonstrating both qualitatively and quantitatively that\nincorporating online map feedback yields more complete and accurate 3D\nreconstructions than conventional methods. Project page:\nhttps://htnk-lab.github.io/coverage-recon/"}
{"id": "2510.18422", "pdf": "https://arxiv.org/pdf/2510.18422", "abs": "https://arxiv.org/abs/2510.18422", "authors": ["Yizhen Jia", "Siyao Xiao", "Wenkai Jia", "Hui Chen", "Wen-Qin Wang"], "title": "AWSPNet: Attention-based Dual-Tree Wavelet Scattering Prototypical Network for MIMO Radar Target Recognition and Jamming Suppression", "categories": ["eess.SP"], "comment": "13 pages, 10 figures, The code is available in\n  https://github.com/jiaxuanzhi/AwspNet", "summary": "The increasing of digital radio frequency memory based electronic\ncountermeasures poses a significant threat to the survivability and\neffectiveness of radar systems. These jammers can generate a multitude of\ndeceptive false targets, overwhelming the radar's processing capabilities and\nmasking targets. Consequently, the ability to robustly discriminate between\ntrue targets and complex jamming signals, especially in low signal-to-noise\nratio (SNR) environments, is of importance. This paper introduces the\nattention-based dual-tree wavelet scattering prototypical network (AWSPNet), a\ndeep learning framework designed for simultaneous radar target recognition and\njamming suppression. The core of AWSPNet is the encoder that leverages the\ndual-tree complex wavelet transform to extract features that are inherently\nrobust to noise and signal translations. These features are further refined by\nan attention mechanism and a pre-trained backbone network. To address the\nchallenge of limited labeled data and enhance generalization, we employ a\nsupervised contrastive learning strategy during the training phase. The\nclassification is performed by a prototypical network, which is particularly\neffective in few-shot learning scenarios, enabling rapid adaptation to new\nsignal types. We demonstrate the efficacy of our approach through extensive\nexperiments. The results show that AWSPNet achieves 90.45\\% accuracy at -6 dB\nSNR. Furthermore, we provide a physical interpretation of the network's inner\nworkings through t-SNE visualizations, which analyze the feature separability\nat different stages of the model. Finally, by integrating AWSPNet with a\ntime-domain sliding window approach, we present a complete algorithm capable of\nnot only identifying but also effectively suppressing various types of jamming,\nthereby validating its potential for practical application in complex\nelectromagnetic environments."}
{"id": "2510.18348", "pdf": "https://arxiv.org/pdf/2510.18348", "abs": "https://arxiv.org/abs/2510.18348", "authors": ["Alexandros Ntagkas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages, 9 figures, 2 tables", "summary": "State-of-the-art perceptive Reinforcement Learning controllers for legged\nrobots either (i) impose oscillator or IK-based gait priors that constrain the\naction space, add bias to the policy optimization and reduce adaptability\nacross robot morphologies, or (ii) operate \"blind\", which struggle to\nanticipate hind-leg terrain, and are brittle to noise. In this paper, we\npropose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL\napproach that overcomes these limitations by enforcing gait structure purely\nthrough reward shaping, thereby reducing inductive bias in policy learning\ncompared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase\nas a cubic Hermite spline that adapts swing height to local heightmap\nstatistics and adds a swing-phase contact penalty, while the policy acts\ndirectly in joint space supporting morphology-agnostic deployment. Trained in\nMuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and\ndomain randomization, PGTT achieves the highest success under push disturbances\n(median +7.5% vs. the next best method) and on discrete obstacles (+9%), with\ncomparable velocity tracking, and converging to an effective policy roughly 2x\nfaster than strong end-to-end baselines. We validate PGTT on a Unitree Go2\nusing a real-time LiDAR elevation-to-heightmap pipeline, and we report\npreliminary results on ANYmal-C obtained with the same hyperparameters. These\nfindings indicate that terrain-adaptive, phase-guided reward shaping is a\nsimple and general mechanism for robust perceptive locomotion across platforms."}
{"id": "2510.18501", "pdf": "https://arxiv.org/pdf/2510.18501", "abs": "https://arxiv.org/abs/2510.18501", "authors": ["Tung-Anh Nguyen", "Van-Phuc Bui", "Shashi Raj Pandey", "Kim Hue Ta", "Nguyen H. Tran", "Petar Popovski"], "title": "Microsecond Federated SVD on Grassmann Manifold for Real-time IoT Intrusion Detection", "categories": ["eess.SP"], "comment": null, "summary": "This paper introduces FedSVD, a novel unsupervised federated learning\nframework for real-time anomaly detection in IoT networks. By leveraging\nSingular Value Decomposition (SVD) and optimization on the Grassmann manifolds,\nFedSVD enables accurate detection of both known and unknown intrusions without\nrelying on labeled data or centralized data sharing. Tailored for deployment on\nlow-power devices like the NVIDIA Jetson AGX Orin, the proposed method\nsignificantly reduces communication overhead and computational cost.\nExperimental results show that FedSVD achieves performance comparable to deep\nlearning baselines while reducing inference latency by over 10x, making it\nsuitable for latency-sensitive IoT applications."}
{"id": "2510.18371", "pdf": "https://arxiv.org/pdf/2510.18371", "abs": "https://arxiv.org/abs/2510.18371", "authors": ["Mingxin Li", "Haibo Hu", "Jinghuai Deng", "Yuchen Xi", "Xinhong Chen", "Jianping Wang"], "title": "MMRHP: A Miniature Mixed-Reality HIL Platform for Auditable Closed-Loop Evaluation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Validation of autonomous driving systems requires a trade-off between test\nfidelity, cost, and scalability. While miniaturized hardware-in-the-loop (HIL)\nplatforms have emerged as a promising solution, a systematic framework\nsupporting rigorous quantitative analysis is generally lacking, limiting their\nvalue as scientific evaluation tools. To address this challenge, we propose\nMMRHP, a miniature mixed-reality HIL platform that elevates miniaturized\ntesting from functional demonstration to rigorous, reproducible quantitative\nanalysis. The core contributions are threefold. First, we propose a systematic\nthree-phase testing process oriented toward the Safety of the Intended\nFunctionality(SOTIF)standard, providing actionable guidance for identifying the\nperformance limits and triggering conditions of otherwise correctly functioning\nsystems. Second, we design and implement a HIL platform centered around a\nunified spatiotemporal measurement core to support this process, ensuring\nconsistent and traceable quantification of physical motion and system timing.\nFinally, we demonstrate the effectiveness of this solution through\ncomprehensive experiments. The platform itself was first validated, achieving a\nspatial accuracy of 10.27 mm RMSE and a stable closed-loop latency baseline of\napproximately 45 ms. Subsequently, an in-depth Autoware case study leveraged\nthis validated platform to quantify its performance baseline and identify a\ncritical performance cliff at an injected latency of 40 ms. This work shows\nthat a structured process, combined with a platform offering a unified\nspatio-temporal benchmark, enables reproducible, interpretable, and\nquantitative closed-loop evaluation of autonomous driving systems."}
{"id": "2510.18604", "pdf": "https://arxiv.org/pdf/2510.18604", "abs": "https://arxiv.org/abs/2510.18604", "authors": ["Zian Meng", "Qiang Li", "Wenqian Tang", "Mingdie Yan", "Xiaohu Ge"], "title": "Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels", "categories": ["eess.SP", "cs.LG", "eess.IV"], "comment": "12 pages, 8 figures", "summary": "Deep learning-based semantic communication has largely relied on analog or\nsemi-digital transmission, which limits compatibility with modern digital\ncommunication infrastructures. Recent studies have employed vector quantization\n(VQ) to enable discrete semantic transmission, yet existing methods neglect\nchannel state information during codebook optimization, leading to suboptimal\nrobustness. To bridge this gap, we propose a channel-aware vector quantization\n(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed\nVQJSCC, established on a discrete memoryless channel. In this framework,\nsemantic features are discretized and directly mapped to modulation\nconstellation symbols, while CAVQ integrates channel transition probabilities\ninto the quantization process, aligning easily confused symbols with\nsemantically similar codewords. A multi-codebook alignment mechanism is further\nintroduced to handle mismatches between codebook order and modulation order by\ndecomposing the transmission stream into multiple independently optimized\nsubchannels. Experimental results demonstrate that VQJSCC effectively mitigates\nthe digital cliff effect, achieves superior reconstruction quality across\nvarious modulation schemes, and outperforms state-of-the-art digital semantic\ncommunication baselines in both robustness and efficiency."}
{"id": "2510.18373", "pdf": "https://arxiv.org/pdf/2510.18373", "abs": "https://arxiv.org/abs/2510.18373", "authors": ["Wanchen Li", "Kahina Chalabi", "Sabbah Maxime", "Thomas Bousquet", "Robin Passama", "Sofiane Ramdani", "Andrea Cherubini", "Vincent Bonnet"], "title": "Biomechanically consistent real-time action recognition for human-robot interaction", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel framework for real-time human action recognition\nin industrial contexts, using standard 2D cameras. We introduce a complete\npipeline for robust and real-time estimation of human joint kinematics, input\nto a temporally smoothed Transformer-based network, for action recognition. We\nrely on a new dataset including 11 subjects performing various actions, to\nevaluate our approach. Unlike most of the literature that relies on joint\ncenter positions (JCP) and is offline, ours uses biomechanical prior, eg. joint\nangles, for fast and robust real-time recognition. Besides, joint angles make\nthe proposed method agnostic to sensor and subject poses as well as to\nanthropometric differences, and ensure robustness across environments and\nsubjects. Our proposed learning model outperforms the best baseline model,\nrunning also in real-time, along various metrics. It achieves 88% accuracy and\nshows great generalization ability, for subjects not facing the cameras.\nFinally, we demonstrate the robustness and usefulness of our technique, through\nan online interaction experiment, with a simulated robot controlled in\nreal-time via the recognized actions."}
{"id": "2510.18646", "pdf": "https://arxiv.org/pdf/2510.18646", "abs": "https://arxiv.org/abs/2510.18646", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Indrakshi Dey"], "title": "Delay Management Using Packet Fragmentation in Wireless Industrial Automation Systems", "categories": ["eess.SP"], "comment": "21st Int. Conference on Networking and Services (ICNS 2025). Lisbon,\n  Portugal", "summary": "Managing delay is one of the core requirements of industrial automation\napplications due to the high risk associated for equipment and human lives.\nUsing efficient Media Access Control (MAC) schemes guarantees the timely\ntransmission of critical data, particularly in the industrial environments\nwhere heterogeneous data is inherently expected. This paper compares the\nperformance of Fragmentation based MAC (FROG-MAC) against Fuzzy Priority\nScheduling based MAC (FPS-MAC), both of which have been designed to optimize\nthe performance of heterogenous wireless networks. Contiki has been used as a\nsimulation platform and a single hop star topology has been assumed to resemble\nthe industrial environment. It has been shown that FROG-MAC has the potential\nto outperform FPS-MAC in terms of energy efficiency and delay both, due to its\ninherent feature of interrupting ongoing lower priority transmission on the\nchannel."}
{"id": "2510.18402", "pdf": "https://arxiv.org/pdf/2510.18402", "abs": "https://arxiv.org/abs/2510.18402", "authors": ["Matthias Lorenzen", "Teodoro Alamo", "Martina Mammarella", "Fabrizio Dabbene"], "title": "MPC-based motion planning for non-holonomic systems in non-convex domains", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Preprint of ECC 2025 submission", "summary": "Motivated by the application of using model predictive control (MPC) for\nmotion planning of autonomous mobile robots, a form of output tracking MPC for\nnon-holonomic systems and with non-convex constraints is studied. Although the\nadvantages of using MPC for motion planning have been demonstrated in several\npapers, in most of the available fundamental literature on output tracking MPC\nit is assumed, often implicitly, that the model is holonomic and generally the\nstate or output constraints must be convex. Thus, in application-oriented\npublications, empirical results dominate and the topic of proving completeness,\nin particular under which assumptions the target is always reached, has\nreceived comparatively little attention. To address this gap, we present a\nnovel MPC formulation that guarantees convergence to the desired target under\nrealistic assumptions, which can be verified in relevant real-world scenarios."}
{"id": "2510.18662", "pdf": "https://arxiv.org/pdf/2510.18662", "abs": "https://arxiv.org/abs/2510.18662", "authors": ["Shama Siddiqui", "Anwar Ahmed Khan", "Indrakshi Dey"], "title": "A Comparative Analysis of High-Level vs. Low-Level Simulations for Dynamic MAC Protocols in Wireless Sensor Networks", "categories": ["eess.SP"], "comment": "21st Int. Conference on Networking and Services (ICNS 2025). Lisbon,\n  Portugal", "summary": "Simulation studies are conducted at different levels of details for assessing\nthe performance of Media Access Control (MAC) protocols in Wireless Sensor\nNetworks (WSN). In the present-day scenario where hundreds of MAC protocols\nhave been proposed, it is important to assess the quality of performance\nevaluation being conducted for each of the proposed protocols. It therefore\nbecomes crucial to compare the results of high-level theoretical simulations\nwith the detailed implementation results before any network protocol could be\ndeployed for a real-world scenario. In this work, we present a comparison of\nhigh-level theoretical and detailed implementation results for Adaptive and\nDynamic Polling-MAC (ADP-MAC). MATLAB has been used for conducting initial\ntheoretical simulations and TinyOS has been used to develop the detailed\nimplementation of protocol for Mica2 platform. Performance evaluation of\nADP-MAC using the two levels of simulation has been conducted based on energy\nand delay. In the high-level implementation, energy consumption was found to be\ndecreasing whereas delay was found to be increasing for increasing channel\npolling intervals. On the other hand, when detailed implementation was\ndeveloped, it was observed that both energy consumption and delay revealed an\nincreasing trend with the increasing polling intervals. Therefore, it has been\nshown that the trends for high- and low-level simulations for ADP-MAC are\nsignificantly different, due to the lack of realistic assumptions in the\nhigher-level study."}
{"id": "2510.18518", "pdf": "https://arxiv.org/pdf/2510.18518", "abs": "https://arxiv.org/abs/2510.18518", "authors": ["Fang Nan", "Hao Ma", "Qinghua Guan", "Josie Hughes", "Michael Muehlebach", "Marco Hutter"], "title": "Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning", "categories": ["cs.RO"], "comment": null, "summary": "We present an online model-based reinforcement learning algorithm suitable\nfor controlling complex robotic systems directly in the real world. Unlike\nprevailing sim-to-real pipelines that rely on extensive offline simulation and\nmodel-free policy optimization, our method builds a dynamics model from\nreal-time interaction data and performs policy updates guided by the learned\ndynamics model. This efficient model-based reinforcement learning scheme\nsignificantly reduces the number of samples to train control policies, enabling\ndirect training on real-world rollout data. This significantly reduces the\ninfluence of bias in the simulated data, and facilitates the search for\nhigh-performance control policies. We adopt online learning analysis to derive\nsublinear regret bounds under standard stochastic online optimization\nassumptions, providing formal guarantees on performance improvement as more\ninteraction data are collected. Experimental evaluations were performed on a\nhydraulic excavator arm and a soft robot arm, where the algorithm demonstrates\nstrong sample efficiency compared to model-free reinforcement learning methods,\nreaching comparable performance within hours. Robust adaptation to shifting\ndynamics was also observed when the payload condition was randomized. Our\napproach paves the way toward efficient and reliable on-robot learning for a\nbroad class of challenging control tasks."}
{"id": "2510.18729", "pdf": "https://arxiv.org/pdf/2510.18729", "abs": "https://arxiv.org/abs/2510.18729", "authors": ["Yhonatan Kvich", "Rotem Arie", "Hana Hasan", "Shaik Basheeruddin Shah", "Yonina C. Eldar"], "title": "mSQUID: Model-Based Leanred Modulo Recovery at Low Sampling Rates", "categories": ["eess.SP"], "comment": null, "summary": "Modulo sampling enables acquisition of signals with unlimited dynamic range\nby folding the input into a bounded interval prior to sampling, thus\neliminating the risk of signal clipping and preserving information without\nrequiring highresolution ADCs. While this enables low-cost hardware, the\nnonlinear distortion introduced by folding presents recovery challenges,\nparticularly under noise and quantization. We propose a model-based deep\nunfolding network tailored to this setting, combining the interpretability of\nclassical compress sensing (CS) solvers with the flexibility of learning. A key\ninnovation is a soft-quantization module that encodes the modulo prior by\nguiding the solution toward discrete multiples of the folding range in a\ndifferentiable and learnable way. Our method, modulo soft-quantized unfolded\niterative decoder (mSQUID), achieves superior reconstruction performance at low\nsampling rates under additive Gaussian noise. We further demonstrate its\nutility in a challenging case where signals with vastly different amplitudes\nand disjoint frequency bands are acquired simultaneously and quantized. In this\nscenario, classical sampling often struggles due to weak signal distortion or\nstrong signal clipping, while our approach is able to recover the input\nsignals. Our method also offers significantly reduced runtimes, making it\nsuitable for real-time, resource-limited systems."}
{"id": "2510.18546", "pdf": "https://arxiv.org/pdf/2510.18546", "abs": "https://arxiv.org/abs/2510.18546", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval", "categories": ["cs.RO", "cs.AI"], "comment": "NeurIPS 2025", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."}
{"id": "2510.18743", "pdf": "https://arxiv.org/pdf/2510.18743", "abs": "https://arxiv.org/abs/2510.18743", "authors": ["Kasun R. Wijewardhana", "Animesh Yadav", "Ming Zeng", "Mohamed Elsayed", "Octavia A. Dobre", "Zhiguo Ding"], "title": "Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless Networks", "categories": ["eess.SP"], "comment": "14 pages, 5 figures, For Potential Publication in the IEEE\n  Communications Magazine", "summary": "Waveguide-based pinching-antenna systems (PASS) have recently emerged as a\npromising solution to mitigate severe propagation losses in millimeter-wave and\nterahertz bands by intelligently and flexibly establishing line-of-sight links.\nHowever, their reliance on wire-based feeding confines deployment to areas near\nthe base station (BS), limiting installation flexibility and making them\ncost-ineffective for serving distant users or regions. To overcome this\nchallenge, this article proposes wireless-fed pinchingantenna systems\n(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer\na practical and cost-efficient means to extend coverage beyond the BS vicinity.\nSeveral indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.\nNumerical results further show that Wi-PASS deliver higher data rates than\nconventional fixed-antenna systems, confirming the superior feasibility and\nperformance of Wi-PASS. Key future research directions are also discussed to\nadvance Wi-PASS deployment."}
{"id": "2510.18558", "pdf": "https://arxiv.org/pdf/2510.18558", "abs": "https://arxiv.org/abs/2510.18558", "authors": ["Yue Wang", "Lixian Zhang", "Yimin Zhu", "Yangguang Liu", "Xuwei Yang"], "title": "Flexbee: A Grasping and Perching UAV Based on Soft Vector-Propulsion Nozzle", "categories": ["cs.RO"], "comment": "11 pages, 17 figures", "summary": "The aim of this paper is to design a new type of grasping and perching\nunmanned aerial vehicle (UAV), called Flexbee, which features a soft\nvector-propulsion nozzle (SVPN). Compared to previous UAVs, Flexbee integrates\nflight, grasping, and perching functionalities into the four SVPNs. This\nintegration offers advantages including decoupled position and attitude\ncontrol, high structural reuse, and strong adaptability strong adaptability for\ngrasping and perching. A dynamics model of Flexbee has been developed, and the\nnonlinear coupling issue of the moment has been resolved through linearization\nof the equivalent moment model. A hierarchical control strategy was used to\ndesign controllers for the two operational modes of Flexbee. Finally, flight,\ngrasping, and perching experiments were conducted to validate Flexbee's\nkinematic capabilities and the effectiveness of the control strategy."}
{"id": "2510.18760", "pdf": "https://arxiv.org/pdf/2510.18760", "abs": "https://arxiv.org/abs/2510.18760", "authors": ["Mouna Gharbi", "Silvia Villa", "Emilie Chouzenoux", "Jean-Christophe Pesquet", "Laurent Duval"], "title": "Analyse comparative d'algorithmes de restauration en architecture dépliée pour des signaux chromatographiques parcimonieux", "categories": ["eess.SP", "cs.LG", "physics.chem-ph"], "comment": "4 pages, in French, GRETSI Symposium on Signal and Image Processing,\n  Strasbourg, France, August 2025", "summary": "Data restoration from degraded observations, of sparsity hypotheses, is an\nactive field of study. Traditional iterative optimization methods are now\ncomplemented by deep learning techniques. The development of unfolded methods\nbenefits from both families. We carry out a comparative study of three\narchitectures on parameterized chromatographic signal databases, highlighting\nthe performance of these approaches, especially when employing metrics adapted\nto physico-chemical peak signal characterization."}
{"id": "2510.18600", "pdf": "https://arxiv.org/pdf/2510.18600", "abs": "https://arxiv.org/abs/2510.18600", "authors": ["Shubham Vyas", "Franek Stark", "Rohit Kumar", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "Quadrupeds for Planetary Exploration: Field Testing Control Algorithms on an Active Volcano", "categories": ["cs.RO"], "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Missions such as the Ingenuity helicopter have shown the advantages of using\nnovel locomotion modes to increase the scientific return of planetary\nexploration missions. Legged robots can further expand the reach and capability\nof future planetary missions by traversing more difficult terrain than wheeled\nrovers, such as jumping over cracks on the ground or traversing rugged terrain\nwith boulders. To develop and test algorithms for using quadruped robots, the\nAAPLE project was carried out at DFKI. As part of the project, we conducted a\nseries of field experiments on the Volcano on the Aeolian island of Vulcano, an\nactive stratovolcano near Sicily, Italy. The experiments focused on validating\nnewly developed state-of-the-art adaptive optimal control algorithms for\nquadrupedal locomotion in a high-fidelity analog environment for Lunar and\nMartian surfaces. This paper presents the technical approach, test plan,\nsoftware architecture, field deployment strategy, and evaluation results from\nthe Vulcano campaign."}
{"id": "2510.18827", "pdf": "https://arxiv.org/pdf/2510.18827", "abs": "https://arxiv.org/abs/2510.18827", "authors": ["Michael Fraiman", "Paulina Hoyos", "Tamir Bendory", "Joe Kileel", "Oscar Mickelin", "Nir Sharon", "Amit Singer"], "title": "SO(3)-invariant PCA with application to molecular data", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Principal component analysis (PCA) is a fundamental technique for\ndimensionality reduction and denoising; however, its application to\nthree-dimensional data with arbitrary orientations -- common in structural\nbiology -- presents significant challenges. A naive approach requires\naugmenting the dataset with many rotated copies of each sample, incurring\nprohibitive computational costs. In this paper, we extend PCA to 3D volumetric\ndatasets with unknown orientations by developing an efficient and principled\nframework for SO(3)-invariant PCA that implicitly accounts for all rotations\nwithout explicit data augmentation. By exploiting underlying algebraic\nstructure, we demonstrate that the computation involves only the square root of\nthe total number of covariance entries, resulting in a substantial reduction in\ncomplexity. We validate the method on real-world molecular datasets,\ndemonstrating its effectiveness and opening up new possibilities for\nlarge-scale, high-dimensional reconstruction problems."}
{"id": "2510.18608", "pdf": "https://arxiv.org/pdf/2510.18608", "abs": "https://arxiv.org/abs/2510.18608", "authors": ["Luigi Quarantiello", "Elia Piccoli", "Jack Bell", "Malio Li", "Giacomo Carfì", "Eric Nuertey Coleman", "Gerlando Gramaglia", "Lanpei Li", "Mauro Madeddu", "Irene Testa", "Vincenzo Lomonaco"], "title": "A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The birth of Foundation Models brought unprecedented results in a wide range\nof tasks, from language to vision, to robotic control. These models are able to\nprocess huge quantities of data, and can extract and develop rich\nrepresentations, which can be employed across different domains and modalities.\nHowever, they still have issues in adapting to dynamic, real-world scenarios\nwithout retraining the entire model from scratch. In this work, we propose the\napplication of Continual Learning and Compositionality principles to foster the\ndevelopment of more flexible, efficient and smart AI solutions."}
{"id": "2510.18206", "pdf": "https://arxiv.org/pdf/2510.18206", "abs": "https://arxiv.org/abs/2510.18206", "authors": ["Hanyu Meng", "Vidhyasaharan Sethu", "Eliathamby Ambikairajah", "Qiquan Zhang", "Haizhou Li"], "title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "Submitted to ICASSP2026", "summary": "In audio signal processing, learnable front-ends have shown strong\nperformance across diverse tasks by optimizing task-specific representation.\nHowever, their parameters remain fixed once trained, lacking flexibility during\ninference and limiting robustness under dynamic complex acoustic environments.\nIn this paper, we introduce a novel adaptive paradigm for audio front-ends that\nreplaces static parameterization with a closed-loop neural controller.\nSpecifically, we simplify the learnable front-end LEAF architecture and\nintegrate a neural controller for adaptive representation via dynamically\ntuning Per-Channel Energy Normalization. The neural controller leverages both\nthe current and the buffered past subband energies to enable input-dependent\nadaptation during inference. Experimental results on multiple audio\nclassification tasks demonstrate that the proposed adaptive front-end\nconsistently outperforms prior fixed and learnable front-ends under both clean\nand complex acoustic conditions. These results highlight neural adaptability as\na promising direction for the next generation of audio front-ends."}
{"id": "2510.18643", "pdf": "https://arxiv.org/pdf/2510.18643", "abs": "https://arxiv.org/abs/2510.18643", "authors": ["Mattias Trende", "Petter Ögren"], "title": "Least Restrictive Hyperplane Control Barrier Functions", "categories": ["cs.RO"], "comment": null, "summary": "Control Barrier Functions (CBFs) can provide provable safety guarantees for\ndynamic systems. However, finding a valid CBF for a system of interest is often\nnon-trivial, especially if the shape of the unsafe region is complex and the\nCBFs are of higher order. A common solution to this problem is to make a\nconservative approximation of the unsafe region in the form of a\nline/hyperplane, and use the corresponding conservative Hyperplane-CBF when\ndeciding on safe control actions. In this letter, we note that conservative\nconstraints are only a problem if they prevent us from doing what we want.\nThus, instead of first choosing a CBF and then choosing a safe control with\nrespect to the CBF, we optimize over a combination of CBFs and safe controls to\nget as close as possible to our desired control, while still having the safety\nguarantee provided by the CBF. We call the corresponding CBF the least\nrestrictive Hyperplane-CBF. Finally, we also provide a way of creating a smooth\nparameterization of the CBF-family for the optimization, and illustrate the\napproach on a double integrator dynamical system with acceleration constraints,\nmoving through a group of arbitrarily shaped static and moving obstacles."}
{"id": "2510.18273", "pdf": "https://arxiv.org/pdf/2510.18273", "abs": "https://arxiv.org/abs/2510.18273", "authors": ["Mohammadreza Doostmohammadian", "Sergio Pequito"], "title": "Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure", "categories": ["eess.SY", "cs.DC", "cs.MA", "cs.SY", "eess.SP", "math.OC"], "comment": "European Journal of Control", "summary": "Distributed resource allocation (DRA) is fundamental to modern networked\nsystems, spanning applications from economic dispatch in smart grids to CPU\nscheduling in data centers. Conventional DRA approaches require reliable\ncommunication, yet real-world networks frequently suffer from link failures,\npacket drops, and communication delays due to environmental conditions, network\ncongestion, and security threats.\n  We introduce a novel resilient DRA algorithm that addresses these critical\nchallenges, and our main contributions are as follows: (1) guaranteed\nconstraint feasibility at all times, ensuring resource-demand balance even\nduring algorithm termination or network disruption; (2) robust convergence\ndespite sector-bound nonlinearities at nodes/links, accommodating practical\nconstraints like quantization and saturation; and (3) optimal performance under\nmerely uniformly-connected networks, eliminating the need for continuous\nconnectivity.\n  Unlike existing approaches that require persistent network connectivity and\nprovide only asymptotic feasibility, our graph-theoretic solution leverages\nnetwork percolation theory to maintain performance during intermittent\ndisconnections. This makes it particularly valuable for mobile multi-agent\nsystems where nodes frequently move out of communication range. Theoretical\nanalysis and simulations demonstrate that our algorithm converges to optimal\nsolutions despite heterogeneous time delays and substantial link failures,\nsignificantly advancing the reliability of distributed resource allocation in\npractical network environments."}
{"id": "2510.18678", "pdf": "https://arxiv.org/pdf/2510.18678", "abs": "https://arxiv.org/abs/2510.18678", "authors": ["Alberto Sanchez-Delgado", "João Carlos Virgolino Soares", "David Omar Al Tawil", "Alessia Li Noce", "Matteo Villa", "Victor Barasuol", "Paolo Arena", "Claudio Semini"], "title": "Towards An Adaptive Locomotion Strategy For Quadruped Rovers: Quantifying When To Slide Or Walk On Planetary Slopes", "categories": ["cs.RO"], "comment": "Published at the 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA 2025)", "summary": "Legged rovers provide enhanced mobility compared to wheeled platforms,\nenabling navigation on steep and irregular planetary terrains. However,\ntraditional legged locomotion might be energetically inefficient and\npotentially dangerous to the rover on loose and inclined surfaces, such as\ncrater walls and cave slopes. This paper introduces a preliminary study that\ncompares the Cost of Transport (CoT) of walking and torso-based sliding\nlocomotion for quadruped robots across different slopes, friction conditions\nand speed levels. By identifying intersections between walking and sliding CoT\ncurves, we aim to define threshold conditions that may trigger transitions\nbetween the two strategies. The methodology combines physics-based simulations\nin Isaac Sim with particle interaction validation in ANSYS-Rocky. Our results\nrepresent an initial step towards adaptive locomotion strategies for planetary\nlegged rovers."}
{"id": "2510.18387", "pdf": "https://arxiv.org/pdf/2510.18387", "abs": "https://arxiv.org/abs/2510.18387", "authors": ["Silvère Ségaud", "Charlie Budd", "Matthew Elliot", "Graeme Stasiuk", "Jonathan Shapey", "Yijing Xie", "Tom Vercauteren"], "title": "Quantification of dual-state 5-ALA-induced PpIX fluorescence: Methodology and validation in tissue-mimicking phantoms", "categories": ["physics.med-ph", "eess.IV", "eess.SP", "q-bio.QM"], "comment": null, "summary": "Quantification of protoporphyrin IX (PpIX) fluorescence in human brain\ntumours has the potential to significantly improve patient outcomes in\nneuro-oncology, but represents a formidable imaging challenge. Protoporphyrin\nis a biological molecule which interacts with the tissue micro-environment to\nform two photochemical states in glioma. Each exhibits markedly different\nquantum efficiencies, with distinct but overlapping emission spectra that also\noverlap with tissue autofluorescence. Fluorescence emission is known to be\ndistorted by the intrinsic optical properties of tissue, coupled with marked\nintra-tumoural heterogeneity as a hallmark of glioma tumours. Existing\nquantitative fluorescence systems are developed and validated using simplified\nphantoms that do not simultaneously mimic the complex interactions between\nfluorophores and tissue optical properties or micro-environment. Consequently,\nexisting systems risk introducing systematic errors into PpIX quantification\nwhen used in tissue. In this work, we introduce a novel pipeline for\nquantification of PpIX in glioma, which robustly differentiates both emission\nstates from background autofluorescence without reliance on a priori spectral\ninformation, and accounts for variations in their quantum efficiency. Unmixed\nPpIX emission forms are then corrected for wavelength-dependent optical\ndistortions and weighted for accurate quantification. Significantly, this\npipeline is developed and validated using novel tissue-mimicking phantoms\nreplicating the optical properties of glioma tissues and photochemical\nvariability of PpIX fluorescence in glioma. Our workflow achieves strong\ncorrelation with ground-truth PpIX concentrations (R2 = 0.918+-0.002),\ndemonstrating its potential for robust, quantitative PpIX fluorescence imaging\nin clinical settings."}
{"id": "2510.18697", "pdf": "https://arxiv.org/pdf/2510.18697", "abs": "https://arxiv.org/abs/2510.18697", "authors": ["Phuoc Nguyen", "Francesco Verdoja", "Ville Kyrki"], "title": "Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations", "categories": ["cs.RO"], "comment": "Submitted to RA-L", "summary": "A fundamental aspect for building intelligent autonomous robots that can\nassist humans in their daily lives is the construction of rich environmental\nrepresentations. While advances in semantic scene representations have enriched\nrobotic scene understanding, current approaches lack a connection between\nspatial features and dynamic events; e.g., connecting the blue mug to the event\nwashing a mug. In this work, we introduce the event-grounding graph (EGG), a\nframework grounding event interactions to spatial features of a scene. This\nrepresentation allows robots to perceive, reason, and respond to complex\nspatio-temporal queries. Experiments using real robotic data demonstrate EGG's\ncapability to retrieve relevant information and respond accurately to human\ninquiries concerning the environment and events within. Furthermore, the EGG\nframework's source code and evaluation dataset are released as open-source at:\nhttps://github.com/aalto-intelligent-robotics/EGG."}
{"id": "2510.18766", "pdf": "https://arxiv.org/pdf/2510.18766", "abs": "https://arxiv.org/abs/2510.18766", "authors": ["Alexander Krawciw", "Sven Lilge", "Luka Antonyshyn", "Timothy D. Barfoot"], "title": "Sharing the Load: Distributed Model-Predictive Control for Precise Multi-Rover Cargo Transport", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions."}
{"id": "2510.18776", "pdf": "https://arxiv.org/pdf/2510.18776", "abs": "https://arxiv.org/abs/2510.18776", "authors": ["Emad Razavi", "Angelo Bratta", "João Carlos Virgolino Soares", "Carmine Recchiuto", "Claudio Semini"], "title": "Online Object-Level Semantic Mapping for Quadrupeds in Real-World Environments", "categories": ["cs.RO"], "comment": "Published at the Italian Conference on Robotics and Intelligent\n  Machines (I-RIM) 3D, 2025", "summary": "We present an online semantic object mapping system for a quadruped robot\noperating in real indoor environments, turning sensor detections into named\nobjects in a global map. During a run, the mapper integrates range geometry\nwith camera detections, merges co-located detections within a frame, and\nassociates repeated detections into persistent object instances across frames.\nObjects remain in the map when they are out of view, and repeated sightings\nupdate the same instance rather than creating duplicates. The output is a\ncompact object layer that can be queried (class, pose, and confidence), is\nintegrated with the occupancy map and readable by a planner. In on-robot tests,\nthe layer remained stable across viewpoint changes."}
{"id": "2510.18845", "pdf": "https://arxiv.org/pdf/2510.18845", "abs": "https://arxiv.org/abs/2510.18845", "authors": ["Ryan Teoh", "Sander Tonkens", "William Sharpless", "Aijia Yang", "Zeyuan Feng", "Somil Bansal", "Sylvia Herbert"], "title": "MADR: MPC-guided Adversarial DeepReach", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, under review", "summary": "Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe\nvalue functions and policies in the face of adversarial disturbance, but is\nlimited by the curse of dimensionality. Physics-informed deep learning is able\nto overcome this infeasibility, but itself suffers from slow and inaccurate\nconvergence, primarily due to weak PDE gradients and the complexity of\nself-supervised learning. A few works, recently, have demonstrated that\nenriching the self-supervision process with regular supervision (based on the\nnature of the optimal control problem), greatly accelerates convergence and\nsolution quality, however, these have been limited to single player problems\nand simple games. In this work, we introduce MADR: MPC-guided Adversarial\nDeepReach, a general framework to robustly approximate the two-player, zero-sum\ndifferential game value function. In doing so, MADR yields the corresponding\noptimal strategies for both players in zero-sum games as well as safe policies\nfor worst-case robustness. We test MADR on a multitude of high-dimensional\nsimulated and real robotic agents with varying dynamics and games, finding that\nour approach significantly out-performs state-of-the-art baselines in\nsimulation and produces impressive results in hardware."}
{"id": "2510.17863", "pdf": "https://arxiv.org/pdf/2510.17863", "abs": "https://arxiv.org/abs/2510.17863", "authors": ["Demetrious T. Kutzke", "Ying-Kun Wu", "Elizabeth Terveen", "Junaed Sattar"], "title": "Robotic Classification of Divers' Swimming States using Visual Pose Keypoints as IMUs", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Traditional human activity recognition uses either direct image analysis or\ndata from wearable inertial measurement units (IMUs), but can be ineffective in\nchallenging underwater environments. We introduce a novel hybrid approach that\nbridges this gap to monitor scuba diver safety. Our method leverages computer\nvision to generate high-fidelity motion data, effectively creating a\n``pseudo-IMU'' from a stream of 3D human joint keypoints. This technique\ncircumvents the critical problem of wireless signal attenuation in water, which\nplagues conventional diver-worn sensors communicating with an Autonomous\nUnderwater Vehicle (AUV). We apply this system to the vital task of identifying\nanomalous scuba diver behavior that signals the onset of a medical emergency\nsuch as cardiac arrest -- a leading cause of scuba diving fatalities. By\nintegrating our classifier onboard an AUV and conducting experiments with\nsimulated distress scenarios, we demonstrate the utility and effectiveness of\nour method for advancing robotic monitoring and diver safety."}
{"id": "2510.18034", "pdf": "https://arxiv.org/pdf/2510.18034", "abs": "https://arxiv.org/abs/2510.18034", "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"], "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.9; I.4.8"], "comment": "8 pages, 5 figures", "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of\nrare, out-of-distribution scenarios with semantic anomalies. While Vision\nLanguage Models (VLMs) offer promising reasoning capabilities, naive prompting\napproaches yield unreliable performance and depend on expensive proprietary\nmodels, limiting practical deployment. We introduce SAVANT (Semantic Analysis\nwith Vision-Augmented Anomaly deTection), a structured reasoning framework that\nachieves high accuracy and recall in detecting anomalous driving scenarios from\ninput images through layered scene analysis and a two-phase pipeline:\nstructured scene description extraction followed by multi-modal evaluation. Our\napproach transforms VLM reasoning from ad-hoc prompting to systematic analysis\nacross four semantic layers: Street, Infrastructure, Movable Objects, and\nEnvironment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world\ndriving scenarios, significantly outperforming unstructured baselines. More\nimportantly, we demonstrate that our structured framework enables a fine-tuned\n7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%\naccuracy - surpassing all models evaluated while enabling local deployment at\nnear-zero cost. By automatically labeling over 9,640 real-world images with\nhigh accuracy, SAVANT addresses the critical data scarcity problem in anomaly\ndetection and provides a practical path toward reliable, accessible semantic\nmonitoring for autonomous systems."}
{"id": "2510.18060", "pdf": "https://arxiv.org/pdf/2510.18060", "abs": "https://arxiv.org/abs/2510.18060", "authors": ["Wei-Jer Chang", "Akshay Rangesh", "Kevin Joseph", "Matthew Strong", "Masayoshi Tomizuka", "Yihan Hu", "Wei Zhan"], "title": "SPACeR: Self-Play Anchoring with Centralized Reference Models", "categories": ["cs.LG", "cs.AI", "cs.RO", "I.2.9; I.2.6"], "comment": "Project page: https://spacer-ai.github.io/", "summary": "Developing autonomous vehicles (AVs) requires not only safety and efficiency,\nbut also realistic, human-like behaviors that are socially aware and\npredictable. Achieving this requires sim agent policies that are human-like,\nfast, and scalable in multi-agent settings. Recent progress in imitation\nlearning with large diffusion-based or tokenized models has shown that\nbehaviors can be captured directly from human driving data, producing realistic\npolicies. However, these models are computationally expensive, slow during\ninference, and struggle to adapt in reactive, closed-loop scenarios. In\ncontrast, self-play reinforcement learning (RL) scales efficiently and\nnaturally captures multi-agent interactions, but it often relies on heuristics\nand reward shaping, and the resulting policies can diverge from human norms. We\npropose SPACeR, a framework that leverages a pretrained tokenized\nautoregressive motion model as a centralized reference policy to guide\ndecentralized self-play. The reference model provides likelihood rewards and KL\ndivergence, anchoring policies to the human driving distribution while\npreserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our\nmethod achieves competitive performance with imitation-learned policies while\nbeing up to 10x faster at inference and 50x smaller in parameter size than\nlarge generative models. In addition, we demonstrate in closed-loop ego\nplanning evaluation tasks that our sim agents can effectively measure planner\nquality with fast and scalable traffic simulation, establishing a new paradigm\nfor testing autonomous driving policies."}
{"id": "2510.18082", "pdf": "https://arxiv.org/pdf/2510.18082", "abs": "https://arxiv.org/abs/2510.18082", "authors": ["Donggeon David Oh", "Duy P. Nguyen", "Haimin Hu", "Jaime F. Fisac"], "title": "Provably Optimal Reinforcement Learning under Safety Filtering", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": "17 pages, 3 figures", "summary": "Recent advances in reinforcement learning (RL) enable its use on increasingly\ncomplex tasks, but the lack of formal safety guarantees still limits its\napplication in safety-critical settings. A common practical approach is to\naugment the RL policy with a safety filter that overrides unsafe actions to\nprevent failures during both training and deployment. However, safety filtering\nis often perceived as sacrificing performance and hindering the learning\nprocess. We show that this perceived safety-performance tradeoff is not\ninherent and prove, for the first time, that enforcing safety with a\nsufficiently permissive safety filter does not degrade asymptotic performance.\nWe formalize RL safety with a safety-critical Markov decision process (SC-MDP),\nwhich requires categorical, rather than high-probability, avoidance of\ncatastrophic failure states. Additionally, we define an associated filtered MDP\nin which all actions result in safe effects, thanks to a safety filter that is\nconsidered to be a part of the environment. Our main theorem establishes that\n(i) learning in the filtered MDP is safe categorically, (ii) standard RL\nconvergence carries over to the filtered MDP, and (iii) any policy that is\noptimal in the filtered MDP-when executed through the same filter-achieves the\nsame asymptotic return as the best safe policy in the SC-MDP, yielding a\ncomplete separation between safety enforcement and performance optimization. We\nvalidate the theory on Safety Gymnasium with representative tasks and\nconstraints, observing zero violations during training and final performance\nmatching or exceeding unfiltered baselines. Together, these results shed light\non a long-standing question in safety-filtered learning and provide a simple,\nprincipled recipe for safe RL: train and deploy RL policies with the most\npermissive safety filter that is available."}
{"id": "2510.18123", "pdf": "https://arxiv.org/pdf/2510.18123", "abs": "https://arxiv.org/abs/2510.18123", "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"], "title": "SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Collaborative driving systems leverage vehicle-to-everything (V2X)\ncommunication across multiple agents to enhance driving safety and efficiency.\nTraditional V2X systems take raw sensor data, neural features, or perception\nresults as communication media, which face persistent challenges, including\nhigh bandwidth demands, semantic loss, and interoperability issues. Recent\nadvances investigate natural language as a promising medium, which can provide\nsemantic richness, decision-level reasoning, and human-machine interoperability\nat significantly lower bandwidth. Despite great promise, this paradigm shift\nalso introduces new vulnerabilities within language communication, including\nmessage loss, hallucinations, semantic manipulation, and adversarial attacks.\nIn this work, we present the first systematic study of full-stack safety and\nsecurity issues in natural-language-based collaborative driving. Specifically,\nwe develop a comprehensive taxonomy of attack strategies, including connection\ndisruption, relay/replay interference, content spoofing, and multi-connection\nforgery. To mitigate these risks, we introduce an agentic defense pipeline,\nwhich we call SafeCoop, that integrates a semantic firewall,\nlanguage-perception consistency checks, and multi-source consensus, enabled by\nan agentic transformation function for cross-frame spatial alignment. We\nsystematically evaluate SafeCoop in closed-loop CARLA simulation across 32\ncritical scenarios, achieving 69.15% driving score improvement under malicious\nattacks and up to 67.32% F1 score for malicious detection. This study provides\nguidance for advancing research on safe, secure, and trustworthy\nlanguage-driven collaboration in transportation systems. Our project page is\nhttps://xiangbogaobarry.github.io/SafeCoop."}
{"id": "2510.18636", "pdf": "https://arxiv.org/pdf/2510.18636", "abs": "https://arxiv.org/abs/2510.18636", "authors": ["Baptiste Bauvin", "Loïc Baret", "Ola Ahmad"], "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "10 pages, BMVC2025", "summary": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub."}
