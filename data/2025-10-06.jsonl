{"id": "2510.02646", "pdf": "https://arxiv.org/pdf/2510.02646", "abs": "https://arxiv.org/abs/2510.02646", "authors": ["Jinsung Park", "Junyong Shin", "Yongjeong Oh", "Jihun Park", "Yo-Seb Jeon"], "title": "Rate-Adaptive Semantic Communication via Multi-Stage Vector Quantization", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a novel framework for rate-adaptive semantic\ncommunication based on multi-stage vector quantization (VQ), termed\n\\textit{MSVQ-SC}. Unlike conventional single-stage VQ approaches, which require\nexponentially larger codebooks to achieve higher fidelity, the proposed\nframework decomposes the quantization process into multiple stages and\ndynamically activates both stages and individual VQ modules. This design\nenables fine-grained rate adaptation under varying bit constraints while\nmitigating computational complexity and the codebook collapse problem. To\noptimize performance, we formulate a module selection problem that minimizes\ntask loss subject to a rate constraint and solve it using an incremental\nallocation algorithm. Furthermore, we extend the framework by incorporating\nentropy coding to exploit non-uniform codeword distributions, further reducing\ncommunication overhead. Simulation results on the CIFAR-10 dataset demonstrate\nthat the proposed framework outperforms existing digital semantic communication\nmethods, achieving superior semantic fidelity with lower complexity while\nproviding flexible and fine-grained rate control."}
{"id": "2510.02696", "pdf": "https://arxiv.org/pdf/2510.02696", "abs": "https://arxiv.org/abs/2510.02696", "authors": ["Anish Pradhan", "Lingjia Liu", "Harpreet S. Dhillon"], "title": "Mutual Information-Driven Visualization and Clustering for Core KPI Selection in O-RAN Testing", "categories": ["eess.SP", "stat.AP"], "comment": null, "summary": "O-RAN testing is becoming increasingly difficult with the exponentially\ngrowing number of performance measurements as the system grows more complex,\nwith additional units, interfaces, applications, and possible implementations\nand configurations. To simplify the testing procedure and improve system design\nfor O-RAN systems, it is important to identify the dependencies among various\nperformance measurements, which are inherently time-series and can be modeled\nas realizations of random processes. While information theory can be utilized\nas a principled foundation for mapping these dependencies, the robust\nestimation of such measures for random processes from real-world data remains\nchallenging. This paper introduces AMIF-MDS, which employs aggregate mutual\nInformation in frequency (AMIF), a practical proxy for directed information\n(DI), to quantify similarity and visualize inter-series dependencies with\nmultidimensional scaling (MDS). The proposed quantile-based AMIF estimator is\napplied to O-RAN time-series testing data to identify dependencies among\nvarious performance measures so that we can focus on a set of ``core''\nperformance measures. Applying density-based spatial clustering of applications\nwith noise (DBSCAN) to the MDS embedding groups mutually informative metrics,\norganically reveals the link-adaptation indicators among other clusters, and\nyields a ``core'' performance measure set for future learning-driven O-RAN\ntesting."}
{"id": "2510.02744", "pdf": "https://arxiv.org/pdf/2510.02744", "abs": "https://arxiv.org/abs/2510.02744", "authors": ["Yupeng Li", "Ruhao Zhang", "Yitong Liu", "Chunju Shao", "Jing Jin", "Shijian Gao"], "title": "Denoising and Augmentation: A Dual Use of Diffusion Model for Enhanced CSI Recovery", "categories": ["eess.SP"], "comment": "This paper is formatted for an IEEE conference. It contains 4 figures\n  and 2 tables. The source code is available at\n  https://github.com/fhghwericge/Diffusion-Model-for-Enhanced-CSI-Recovery", "summary": "This letter introduces a dual application of denoising diffusion\nprobabilistic model (DDPM)-based channel estimation algorithm integrating data\ndenoising and augmentation. Denoising addresses the severe noise in raw signals\nat pilot locations, which can impair channel estimation accuracy. An\nunsupervised structure is proposed to clean field data without prior knowledge\nof pure channel information. Data augmentation is crucial due to the\ndata-intensive nature of training deep learning (DL) networks for channel state\ninformation (CSI) estimation. The network generates new channel data by\nadjusting reverse steps, enriching the training dataset. To manage varying\nsignal-to-noise ratios (SNRs) in communication data, a piecewise forward\nstrategy is proposed to enhance the DDPM convergence precision. The link-level\nsimulations indicate that the proposed scheme achieves a superior tradeoff\nbetween precision and computational cost compared to existing benchmarks."}
{"id": "2510.02785", "pdf": "https://arxiv.org/pdf/2510.02785", "abs": "https://arxiv.org/abs/2510.02785", "authors": ["Shanglin Yang", "Jean-Marie Gorce", "Muhammad Jehangir Khan", "Dinh-Thuy Phan-Huy", "Guillaume Villemaud"], "title": "Neyman Pearson Detector for Multiple Ambient Backscatter Zero-Energy-Devices Beacons using Near-Perfect Code", "categories": ["eess.SP"], "comment": "11 pages", "summary": "Recently, a novel ultra-low-power indoor localization system based on\nZero-Energy Devices (ZEDs) has shown promising results in ambient backscatter\ncommunication. In this paper, we study detection of multiple coexisting ZEDs in\nambient backscatter systems under interference and synchronization uncertainty.\nBuilding on a Neyman-Pearson (NP) formulation previously applied to single-tag\ndetection, we introduce a detector tailored to multi-tag scenarios. The core\nidea is to use a Near-Perfect Code (NPC) as the synchronization sequence, which\nsubstantially improves the peak-to-sidelobe (PSL) ratio and thus separability\namong concurrent tags. The proposed scheme replaces dual band-pass filtering\nwith dual correlators, enabling an explicit Bayesian detector and tight control\nof the false-alarm rate; we further incorporate a contrast metric and\nmulti-frequency combining to reveal secondary tags. Experiments on the\nCorteXlab testbed (part of the SLICES-EU infrastructure) confirm robustness at\nlow SNR, with observed PSL improvements from about 11 dB to about 22 dB. These\nresults advance scalable, reliable ambient backscatter localization in\npractical multi-tag environments."}
{"id": "2510.02464", "pdf": "https://arxiv.org/pdf/2510.02464", "abs": "https://arxiv.org/abs/2510.02464", "authors": ["Isaac Ngui", "Courtney McBeth", "Andr√© Santos", "Grace He", "Katherine J. Mimnaugh", "James D. Motes", "Luciano Soares", "Marco Morales", "Nancy M. Amato"], "title": "ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "We propose the Extended Reality Universal Planning Toolkit (ERUPT), an\nextended reality (XR) system for interactive motion planning. Our system allows\nusers to create and dynamically reconfigure environments while they plan robot\npaths. In immersive three-dimensional XR environments, users gain a greater\nspatial understanding. XR also unlocks a broader range of natural interaction\ncapabilities, allowing users to grab and adjust objects in the environment\nsimilarly to the real world, rather than using a mouse and keyboard with the\nscene projected onto a two-dimensional computer screen. Our system integrates\nwith MoveIt, a manipulation planning framework, allowing users to send motion\nplanning requests and visualize the resulting robot paths in virtual or\naugmented reality. We provide a broad range of interaction modalities, allowing\nusers to modify objects in the environment and interact with a virtual robot.\nOur system allows operators to visualize robot motions, ensuring desired\nbehavior as it moves throughout the environment, without risk of collisions\nwithin a virtual space, and to then deploy planned paths on physical robots in\nthe real world."}
{"id": "2510.02793", "pdf": "https://arxiv.org/pdf/2510.02793", "abs": "https://arxiv.org/abs/2510.02793", "authors": ["Jiachen Tian", "Yu Han", "Zhengtao Jin", "Xi Yang", "Jie Yang", "Wankai Tang", "Xiao Li", "Wenjin Wang", "Shi Jin"], "title": "Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "The mid-band frequency range, combined with extra large-scale multiple-input\nmultiple-output (XL-MIMO), is emerging as a key enabler for future\ncommunication systems. Thanks to the advent of new spectrum resources and\ndegrees of freedom brought by the near-field propagation, the mid-band XL-MIMO\nsystem is expected to significantly enhance throughput and inherently support\nadvanced functionalities such as integrated sensing and communication. Although\ntheoretical studies have highlighted the benefits of mid-band XL-MIMO systems,\nthe promised performance gains have yet to be validated in practical systems,\nposing a major challenge to the standardization. In this paper, preliminaries\nare first discussed, followed by an analysis of key challenges in constructing\na real-time prototype system. Subsequently, the design and implementation of a\nreal-time mid-band XL-MIMO prototype system are presented. Benefiting from the\nnovel architecture, the proposed prototype system supports metrics aligned with\nstandardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,\nand up to 256 transceiver chains. Operating in time-division duplexing (TDD)\nmode, the prototype enables multiuser communication with support for up to 12\nusers, while retaining standard communication procedures. Built on\nsoftware-defined radio (SDR) platforms, the system is programmable and allows\nfor flexible deployment of advanced algorithms. Moreover, the modular\narchitecture ensures high scalability, making the system adaptable to various\nconfigurations, including distributed deployments and decentralized signal\nprocessing. Experimental results with the proposed prototype system demonstrate\nreal-time digital sample processing at 1167.85 Gbps, a peak data throughput of\n15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80\nbit/s/Hz."}
{"id": "2510.02469", "pdf": "https://arxiv.org/pdf/2510.02469", "abs": "https://arxiv.org/abs/2510.02469", "authors": ["Sung-Yeon Park", "Adam Lee", "Juanwu Lu", "Can Cui", "Luyang Jiang", "Rohit Gupta", "Kyungtae Han", "Ahmadreza Moradipari", "Ziran Wang"], "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Driving scene manipulation with sensor data is emerging as a promising\nalternative to traditional virtual driving simulators. However, existing\nframeworks struggle to generate realistic scenarios efficiently due to limited\nediting capabilities. To address these challenges, we present SIMSplat, a\npredictive driving scene editor with language-aligned Gaussian splatting. As a\nlanguage-controlled editor, SIMSplat enables intuitive manipulation using\nnatural language prompts. By aligning language with Gaussian-reconstructed\nscenes, it further supports direct querying of road objects, allowing precise\nand flexible editing. Our method provides detailed object-level editing,\nincluding adding new objects and modifying the trajectories of both vehicles\nand pedestrians, while also incorporating predictive path refinement through\nmulti-agent motion prediction to generate realistic interactions among all\nagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's\nextensive editing capabilities and adaptability across a wide range of\nscenarios. Project page: https://sungyeonparkk.github.io/simsplat/"}
{"id": "2510.02939", "pdf": "https://arxiv.org/pdf/2510.02939", "abs": "https://arxiv.org/abs/2510.02939", "authors": ["Xin Tong", "Zhaoyang Zhang", "Yuzhi Yang", "Yu Ge", "Zhaohui Yang", "Henk Wymeersch", "M√©rouane Debbah"], "title": "Integrated Sensing, Communication, and Positioning in Cellular Vehicular Networks", "categories": ["eess.SP"], "comment": "This paper is accepted by IEEE Transactions on Vehicular Technology", "summary": "In this correspondence, a novel integrated sensing and communication (ISAC)\nframework is proposed to accomplish data communication, vehicle positioning,\nand environment sensing simultaneously in a cellular vehicular network. By\nincorporating the vehicle positioning problem with the existing\ncomputational-imaging-based ISAC models, we formulate a special integrated\nsensing, communication, and positioning problem in which the unknowns are\nhighly coupled. To mitigate the rank deficiency and make it solvable, we\ndiscretize the region of interest (ROI) into sensing and positioning pixels\nrespectively, and exploit both the line-of-sight and non-line-of-sight\npropagation of the vehicles' uplink access signals. The resultant problem is\nshown to be a polynomial bilinear compressed sensing (CS) reconstruction\nproblem, which is then solved by the alternating optimization (AO) algorithm to\niteratively achieve symbol detection, vehicle positioning and environment\nsensing. Performance analysis and numerical results demonstrate the\neffectiveness of the proposed method."}
{"id": "2510.02526", "pdf": "https://arxiv.org/pdf/2510.02526", "abs": "https://arxiv.org/abs/2510.02526", "authors": ["Anamika J H", "Anujith Muraleedharan"], "title": "U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation", "categories": ["cs.RO"], "comment": "8 pages, 5 figures. Accepted to the IROS 2025 Workshop on Perception\n  and Planning for Mobile Manipulation in Changing Environments", "summary": "Robots manipulating in changing environments must act on percepts that are\nlate, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer\nthat leaves the low-level controller unchanged while re-aiming task goals\n(pre-contact, contact, post) as new observations arrive. Unlike motion\nretargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming\nas a first-class, pluggable module between perception and control. Our main\ntechnical contribution is UAR-PF, an uncertainty-aware retargeter that\nmaintains a distribution over object pose under sensing lag and selects goals\nthat maximize expected progress. We instantiate a reproducible Shift x Lag\nstress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,\nwhere the object undergoes abrupt in-plane shifts while synthetic perception\nlag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,\nUAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving\nhigher success with modest end-effector travel and fewer aborts; simple\noperational safeguards further improve stability. Contributions: (1) UAR-PF for\nlag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting\ninterface; and (3) a reproducible Shift x Lag benchmark with evaluation on\npick, push, stacking, and peg insertion."}
{"id": "2510.02979", "pdf": "https://arxiv.org/pdf/2510.02979", "abs": "https://arxiv.org/abs/2510.02979", "authors": ["Jonathan Baum", "Chamot-Nonin Manon", "Oppelt Vera", "David Guiraud", "Christine Azevedo Coste", "Thomas Guiho"], "title": "Towards Electrophysiological and Histological Mapping of Upper Limb Nerves in Pigs Using Epineural Stimulation", "categories": ["eess.SP", "q-bio.QM"], "comment": "FESWS 2025 - 15th Vienna International Workshop on Functional\n  Electrical Stimulation & 30 years IFESS Anniversary, IFESS, Sep 2025, Vienna,\n  Austria", "summary": "Understanding the relationship between nerve anatomy and the functional\noutcomes of electrical stimulation is critical for optimizing neural interface\ndesign. In this study, we conducted acute experiments on four pigs in which\nepineural cuff electrodes with multiple contacts were placed around upper limb\nnerves. A subset of electrical stimulation configurations -- previously\nidentified via computational study -- was applied, and the resulting evoked\nelectromyographic (EMG) responses were recorded from target muscles. Muscle\nrecruitment curves were extracted and analysed offline to quantify activation\npatterns. Following the electrophysiological experiments, the stimulated nerves\nwere harvested and processed for histological analysis to visualize fascicular\norganization and distribution. This work presents preliminary results from the\ncombined analysis of muscle activation profiles and fascicle anatomy in one\nanimal. Our findings aim to inform the design of stimulation strategies by\nlinking electrode configuration to selective muscle recruitment, ultimately\ncontributing to more effective neuromodulation and neuroprosthetic\napplications."}
{"id": "2510.02538", "pdf": "https://arxiv.org/pdf/2510.02538", "abs": "https://arxiv.org/abs/2510.02538", "authors": ["Yilin Wang", "Shangzhe Li", "Haoyi Niu", "Zhiao Huang", "Weitong Zhang", "Hao Su"], "title": "A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models", "categories": ["cs.RO"], "comment": null, "summary": "We are interested in solving the problem of imitation learning with a limited\namount of real-world expert data. Existing offline imitation methods often\nstruggle with poor data coverage and severe performance degradation. We propose\na solution that leverages robot simulators to achieve online imitation\nlearning. Our sim-to-real framework is based on world models and combines\nonline imitation pretraining with offline finetuning. By leveraging online\ninteractions, our approach alleviates the data coverage limitations of offline\nmethods, leading to improved robustness and reduced performance degradation\nduring finetuning. It also enhances generalization during domain transfer. Our\nempirical results demonstrate its effectiveness, improving success rates by at\nleast 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over\nexisting offline imitation learning baselines."}
{"id": "2510.03019", "pdf": "https://arxiv.org/pdf/2510.03019", "abs": "https://arxiv.org/abs/2510.03019", "authors": ["Yang Zhou", "Haochang Wu", "Yunxi Mu", "Hao Qin", "Xinyue Zhang", "Xingqi Zhang"], "title": "Physics-Constrained Inc-GAN for Tunnel Propagation Modeling from Sparse Line Measurements", "categories": ["eess.SP"], "comment": null, "summary": "High-speed railway tunnel communication systems require reliable radio wave\npropagation prediction to ensure operational safety. However, conventional\nsimulation methods face challenges of high computational complexity and\ninability to effectively process sparse measurement data collected during\nactual railway operations. This letter proposes an inception-enhanced\ngenerative adversarial network (Inc-GAN) that can reconstruct complete electric\nfield distributions across tunnel cross-sections using sparse value lines\nmeasured during actual train operations as input. This directly addresses\npractical railway measurement constraints. Through an inception-based generator\narchitecture and progressive training strategy, the method achieves robust\nreconstruction from single measurement signal lines to complete field\ndistributions. Numerical simulation validation demonstrates that Inc-GAN can\naccurately predict electric fields based on measured data collected during\nactual train operations, with significantly improved computational efficiency\ncompared to traditional methods, providing a novel solution for railway\ncommunication system optimization based on real operational data."}
{"id": "2510.02584", "pdf": "https://arxiv.org/pdf/2510.02584", "abs": "https://arxiv.org/abs/2510.02584", "authors": ["Mohammad Abtahi", "Navid Mojahed", "Shima Nazari"], "title": "Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.DS", "math.OC"], "comment": "This work has been submitted to the ACC2026 conference", "summary": "This paper presents a data-driven model predictive control framework for\nmobile robots navigating in dynamic environments, leveraging Koopman operator\ntheory. Unlike the conventional Koopman-based approaches that focus on the\nlinearization of system dynamics only, our work focuses on finding a global\nlinear representation for the optimal path planning problem that includes both\nthe nonlinear robot dynamics and collision-avoidance constraints. We deploy\nextended dynamic mode decomposition to identify linear and bilinear Koopman\nrealizations from input-state data. Our open-loop analysis demonstrates that\nonly the bilinear Koopman model can accurately capture nonlinear state-input\ncouplings and quadratic terms essential for collision avoidance, whereas linear\nrealizations fail to do so. We formulate a quadratic program for the robot path\nplanning in the presence of moving obstacles in the lifted space and determine\nthe optimal robot action in an MPC framework. Our approach is capable of\nfinding the safe optimal action 320 times faster than a nonlinear MPC\ncounterpart that solves the path planning problem in the original state space.\nOur work highlights the potential of bilinear Koopman realizations for\nlinearization of highly nonlinear optimal control problems subject to nonlinear\nstate and input constraints to achieve computational efficiency similar to\nlinear problems."}
{"id": "2510.03055", "pdf": "https://arxiv.org/pdf/2510.03055", "abs": "https://arxiv.org/abs/2510.03055", "authors": ["Dexin Wang", "Isha Jariwala", "Ahmad Bazzi", "Sundeep Rangan", "Theodore S. Rappaport", "Marwa Chafii"], "title": "Compressed Multiband Sensing in FR3 Using Alternating Direction Method of Multipliers", "categories": ["eess.SP"], "comment": "submitted to IEEE Wireless Communications and Networking Conference\n  (WCNC) 2026", "summary": "Joint detection and localization of users and scatterers in multipath-rich\nchannels on multiple bands is critical for integrated sensing and communication\n(ISAC) in 6G. Existing multiband sensing methods are limited by classical\nbeamforming or computationally expensive approaches. This paper introduces\nalternating direction method of multipliers (ADMM)-assisted compressed\nmultiband sensing (CMS), hereafter referred to as ADMM-CMS, which is a novel\nframework for multiband sensing using uplink QAM-modulated pilot symbols. To\nsolve the CMS problem, we develop an adaptive ADMM algorithm that adjusts to\nnoise and ensures automatic stopping if converged. ADMM combines the\ndecomposability of dual ascent with the robustness of augmented Lagrangian\nmethods, making it suitable for large-scale structured optimization.\nSimulations show that ADMM-CMS achieves higher spatial resolution and improved\ndenoising compared to Bartlett-type beamforming, yielding a 34 dB gain in\nper-antenna transmit power for achieving a 0.9 successful recovery probability\n(SRP). Moreover, compared to performing compressed sensing separately on the\nconstituent 7 GHz and 10 GHz sub-bands, ADMM-CMS achieves reductions in delay\nroot mean squared error of 35% and 38.1%, respectively, at -41 dBm per-antenna\ntransmit power, while also yielding improved SRP. Our findings demonstrate\nADMM-CMS as an efficient enabler of ISAC in frequency range 3 (FR3, 7-24 GHz)\nfor 6G systems."}
{"id": "2510.02594", "pdf": "https://arxiv.org/pdf/2510.02594", "abs": "https://arxiv.org/abs/2510.02594", "authors": ["Ruo Chen", "David Blow", "Adnan Abdullah", "Md Jahidul Islam"], "title": "SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics", "categories": ["cs.RO"], "comment": "Presented at the OCEANS 2025 Great Lakes Conference", "summary": "This paper investigates the integration of haptic feedback and virtual\nreality (VR) control interfaces to enhance teleoperation and telemanipulation\nof underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation\nrelies on low-resolution 2D camera feeds and lacks immersive and sensory\nfeedback, which diminishes situational awareness in complex subsea\nenvironments. We propose SubSense -- a novel VR-Haptic framework incorporating\na non-invasive feedback interface to an otherwise 1-DOF (degree of freedom)\nmanipulator, which is paired with the teleoperator's glove to provide haptic\nfeedback and grasp status. Additionally, our framework integrates end-to-end\nsoftware for managing control inputs and displaying immersive camera views\nthrough a VR platform. We validate the system through comprehensive experiments\nand user studies, demonstrating its effectiveness over conventional\nteleoperation interfaces, particularly for delicate manipulation tasks. Our\nresults highlight the potential of multisensory feedback in immersive virtual\nenvironments to significantly improve remote situational awareness and mission\nperformance, offering more intuitive and accessible ROV operations in the\nfield."}
{"id": "2510.03069", "pdf": "https://arxiv.org/pdf/2510.03069", "abs": "https://arxiv.org/abs/2510.03069", "authors": ["Rom Hirsch", "Ziv Aharoni", "Henry D. Pfister", "Haim H. Permuter"], "title": "A Study of Neural Polar Decoders for Communication", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for\nend-to-end communication systems. While prior work demonstrated the\neffectiveness of NPDs on synthetic channels, this study extends the NPD to\nreal-world communication systems. The NPD was adapted to complete OFDM and\nsingle-carrier communication systems. To satisfy practical system requirements,\nthe NPD is extended to support any code length via rate matching, higher-order\nmodulations, and robustness across diverse channel conditions. The NPD operates\ndirectly on channels with memory, exploiting their structure to achieve higher\ndata rates without requiring pilots and a cyclic prefix. Although NPD entails\nhigher computational complexity than the standard 5G polar decoder, its neural\nnetwork architecture enables an efficient representation of channel statistics,\nresulting in manageable complexity suitable for practical systems. Experimental\nresults over 5G channels demonstrate that the NPD consistently outperforms the\n5G polar decoder in terms of BER, BLER, and throughput. These improvements are\nparticularly significant for low-rate and short-block configurations, which are\nprevalent in 5G control channels. Furthermore, NPDs applied to single-carrier\nsystems offer performance comparable to OFDM with lower PAPR, enabling\neffective single-carrier transmission over 5G channels. These results position\nthe NPD as a high-performance, pilotless, and robust decoding solution."}
{"id": "2510.02614", "pdf": "https://arxiv.org/pdf/2510.02614", "abs": "https://arxiv.org/abs/2510.02614", "authors": ["Harsh Gupta", "Xiaofeng Guo", "Huy Ha", "Chuer Pan", "Muqing Cao", "Dongjae Lee", "Sebastian Sherer", "Shuran Song", "Guanya Shi"], "title": "UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies", "categories": ["cs.RO"], "comment": "Result videos can be found at umi-on-air.github.io", "summary": "We introduce UMI-on-Air, a framework for embodiment-aware deployment of\nembodiment-agnostic manipulation policies. Our approach leverages diverse,\nunconstrained human demonstrations collected with a handheld gripper (UMI) to\ntrain generalizable visuomotor policies. A central challenge in transferring\nthese policies to constrained robotic embodiments-such as aerial\nmanipulators-is the mismatch in control and robot dynamics, which often leads\nto out-of-distribution behaviors and poor execution. To address this, we\npropose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level\nUMI policy with a low-level embodiment-specific controller at inference time.\nBy integrating gradient feedback from the controller's tracking cost into the\ndiffusion sampling process, our method steers trajectory generation towards\ndynamically feasible modes tailored to the deployment embodiment. This enables\nplug-and-play, embodiment-aware trajectory adaptation at test time. We validate\nour approach on multiple long-horizon and high-precision aerial manipulation\ntasks, showing improved success rates, efficiency, and robustness under\ndisturbances compared to unguided diffusion baselines. Finally, we demonstrate\ndeployment in previously unseen environments, using UMI demonstrations\ncollected in the wild, highlighting a practical pathway for scaling\ngeneralizable manipulation skills across diverse-and even highly\nconstrained-embodiments. All code, data, and checkpoints will be publicly\nreleased after acceptance. Result videos can be found at umi-on-air.github.io."}
{"id": "2510.02355", "pdf": "https://arxiv.org/pdf/2510.02355", "abs": "https://arxiv.org/abs/2510.02355", "authors": ["Yubo Zhang", "Jeremy Johnston", "Xiaodong Wang"], "title": "An Encoder-Decoder Network for Beamforming over Sparse Large-Scale MIMO Channels", "categories": ["eess.SY", "cs.IT", "cs.LG", "cs.SY", "eess.SP", "math.IT"], "comment": "13 pages, 9 figures, submitted to TCOM and is waiting for reviews", "summary": "We develop an end-to-end deep learning framework for downlink beamforming in\nlarge-scale sparse MIMO channels. The core is a deep EDN architecture with\nthree modules: (i) an encoder NN, deployed at each user end, that compresses\nestimated downlink channels into low-dimensional latent vectors. The latent\nvector from each user is compressed and then fed back to the BS. (ii) a\nbeamformer decoder NN at the BS that maps recovered latent vectors to\nbeamformers, and (iii) a channel decoder NN at the BS that reconstructs\ndownlink channels from recovered latent vectors to further refine the\nbeamformers. The training of EDN leverages two key strategies: (a)\nsemi-amortized learning, where the beamformer decoder NN contains an analytical\ngradient ascent during both training and inference stages, and (b) knowledge\ndistillation, where the loss function consists of a supervised term and an\nunsupervised term, and starting from supervised training with MMSE beamformers,\nover the epochs, the model training gradually shifts toward unsupervised using\nthe sum-rate objective. The proposed EDN beamforming framework is extended to\nboth far-field and near-field hybrid beamforming scenarios. Extensive\nsimulations validate its effectiveness under diverse network and channel\nconditions."}
{"id": "2510.02616", "pdf": "https://arxiv.org/pdf/2510.02616", "abs": "https://arxiv.org/abs/2510.02616", "authors": ["Mobin Habibpour", "Alireza Nemati", "Ali Meghdari", "Alireza Taheri", "Shima Nazari"], "title": "RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments", "categories": ["cs.RO"], "comment": "Proceedings of SAI Intelligent Systems Conference 2023", "summary": "Simultaneous Localization and Mapping (SLAM) plays an important role in many\nrobotics fields, including social robots. Many of the available visual SLAM\nmethods are based on the assumption of a static world and struggle in dynamic\nenvironments. In the current study, we introduce a real-time semantic RGBD SLAM\napproach designed specifically for dynamic environments. Our proposed system\ncan effectively detect moving objects and maintain a static map to ensure\nrobust camera tracking. The key innovation of our approach is the incorporation\nof deep learning-based semantic information into SLAM systems to mitigate the\nimpact of dynamic objects. Additionally, we enhance the semantic segmentation\nprocess by integrating an Extended Kalman filter to identify dynamic objects\nthat may be temporarily idle. We have also implemented a generative network to\nfill in the missing regions of input images belonging to dynamic objects. This\nhighly modular framework has been implemented on the ROS platform and can\nachieve around 22 fps on a GTX1080. Benchmarking the developed pipeline on\ndynamic sequences from the TUM dataset suggests that the proposed approach\ndelivers competitive localization error in comparison with the state-of-the-art\nmethods, all while operating in near real-time. The source code is publicly\navailable."}
{"id": "2510.02514", "pdf": "https://arxiv.org/pdf/2510.02514", "abs": "https://arxiv.org/abs/2510.02514", "authors": ["Guy Ohayon", "Pierre-Etienne H. Fiquet", "Florentin Guth", "Jona Ball√©", "Eero P. Simoncelli"], "title": "Learning a distance measure from the information-estimation geometry of data", "categories": ["eess.IV", "cs.CV", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "comment": "Code available at\n  https://github.com/ohayonguy/information-estimation-metric", "summary": "We introduce the Information-Estimation Metric (IEM), a novel form of\ndistance function derived from an underlying continuous probability density\nover a domain of signals. The IEM is rooted in a fundamental relationship\nbetween information theory and estimation theory, which links the\nlog-probability of a signal with the errors of an optimal denoiser, applied to\nnoisy observations of the signal. In particular, the IEM between a pair of\nsignals is obtained by comparing their denoising error vectors over a range of\nnoise amplitudes. Geometrically, this amounts to comparing the score vector\nfields of the blurred density around the signals over a range of blur levels.\nWe prove that the IEM is a valid global metric and derive a closed-form\nexpression for its local second-order approximation, which yields a Riemannian\nmetric. For Gaussian-distributed signals, the IEM coincides with the\nMahalanobis distance. But for more complex distributions, it adapts, both\nlocally and globally, to the geometry of the distribution. In practice, the IEM\ncan be computed using a learned denoiser (analogous to generative diffusion\nmodels) and solving a one-dimensional integral. To demonstrate the value of our\nframework, we learn an IEM on the ImageNet database. Experiments show that this\nIEM is competitive with or outperforms state-of-the-art supervised image\nquality metrics in predicting human perceptual judgments."}
{"id": "2510.02623", "pdf": "https://arxiv.org/pdf/2510.02623", "abs": "https://arxiv.org/abs/2510.02623", "authors": ["Taha Shafa", "Yiming Meng", "Melkior Ornik"], "title": "Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes an algorithm capable of driving a system to follow a\npiecewise linear trajectory without prior knowledge of the system dynamics.\nMotivated by a critical failure scenario in which a system can experience an\nabrupt change in its dynamics, we demonstrate that it is possible to follow a\nset of waypoints comprised of states analytically proven to be reachable\ndespite not knowing the system dynamics. The proposed algorithm first applies\nsmall perturbations to locally learn the system dynamics around the current\nstate, then computes the set of states that are provably reachable using the\nlocally learned dynamics and their corresponding maximum growth-rate bounds,\nand finally synthesizes a control action that navigates the system to a\nguaranteed reachable state."}
{"id": "2510.02556", "pdf": "https://arxiv.org/pdf/2510.02556", "abs": "https://arxiv.org/abs/2510.02556", "authors": ["Klaus Br√ºmann", "Simon Doclo"], "title": "Multi-Source Position and Direction-of-Arrival Estimation Based on Euclidean Distance Matrices", "categories": ["eess.AS", "eess.SP"], "comment": "13 pages, 6 figures, submitted to IEEE Transactions on Audio, Speech\n  and Language Processing (awaiting review)", "summary": "A popular method to estimate the positions or directions-of-arrival (DOAs) of\nmultiple sound sources using an array of microphones is based on\nsteered-response power (SRP) beamforming. For a three-dimensional scenario,\nSRP-based methods need to jointly optimize three continuous variables for\nposition estimation or two continuous variables for DOA estimation, which can\nbe computationally expensive. In this paper, we propose novel methods for\nmulti-source position and DOA estimation by exploiting properties of Euclidean\ndistance matrices (EDMs) and their respective Gram matrices. In the proposed\nmulti-source position estimation method only a single continuous variable,\nrepresenting the distance between each source and a reference microphone, needs\nto be optimized. For each source, the optimal continuous distance variable and\nset of candidate time-difference of arrival (TDOA) estimates are determined by\nminimizing a cost function that is defined using the eigenvalues of the Gram\nmatrix. The estimated relative source positions are then mapped to estimated\nabsolute source positions by solving an orthogonal Procrustes problem for each\nsource. The proposed multi-source DOA estimation method entirely eliminates the\nneed for continuous variable optimization by defining a relative coordinate\nsystem per source such that one of its coordinate axes is aligned with the\nrespective source DOA. The optimal set of candidate TDOA estimates is\ndetermined by minimizing a cost function that is defined using the eigenvalues\nof a rank-reduced Gram matrix. The computational cost of the proposed EDM-based\nmethods is significantly reduced compared to the SRP-based methods.\nExperimental results for different source and microphone configurations show\nthat the proposed EDM-based method consistently outperforms the SRP-based\nmethod in terms of two-source position and DOA estimation accuracy."}
{"id": "2510.02624", "pdf": "https://arxiv.org/pdf/2510.02624", "abs": "https://arxiv.org/abs/2510.02624", "authors": ["Qun Yang", "Soung Chang Liew"], "title": "Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization", "categories": ["cs.RO"], "comment": null, "summary": "Rigid-formation navigation of multiple robots is essential for applications\nsuch as cooperative transportation. This process involves a team of\ncollaborative robots maintaining a predefined geometric configuration, such as\na square, while in motion. For untethered collaborative motion, inter-robot\ncommunication must be conducted through a wireless network. Notably, few\nexisting works offer a comprehensive solution for multi-robot formation\nnavigation executable on microprocessor platforms via wireless networks,\nparticularly for formations that must traverse complex curvilinear paths. To\naddress this gap, we introduce a novel \"hold-and-hit\" communication-control\nframework designed to work seamlessly with the widely-used Robotic Operating\nSystem (ROS) platform. The hold-and-hit framework synchronizes robot movements\nin a manner robust against wireless network delays and packet loss. It operates\nover discrete-time communication-control cycles, making it suitable for\nimplementation on contemporary microprocessors. Complementary to hold-and-hit,\nwe propose an intra-cycle optimization approach that enables rigid formations\nto closely follow desired curvilinear paths, even under the nonholonomic\nmovement constraints inherent to most vehicular robots. The combination of\nhold-and-hit and intra-cycle optimization ensures precise and reliable\nnavigation even in challenging scenarios. Simulations in a virtual environment\ndemonstrate the superiority of our method in maintaining a four-robot square\nformation along an S-shaped path, outperforming two existing approaches.\nFurthermore, real-world experiments validate the effectiveness of our\nframework: the robots maintained an inter-distance error within $\\pm 0.069m$\nand an inter-angular orientation error within $\\pm19.15^{\\circ}$ while\nnavigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$."}
{"id": "2510.02640", "pdf": "https://arxiv.org/pdf/2510.02640", "abs": "https://arxiv.org/abs/2510.02640", "authors": ["Jaewon Yun", "Joohyuk Park", "Yo-Seb Jeon"], "title": "Anti-Jamming Modulation for OFDM Systems under Jamming Attacks", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "In this paper, we propose an anti-jamming communication framework for\northogonal frequency-division multiplexing (OFDM) systems under jamming\nattacks. To this end, we first develop an anti-jamming modulation scheme that\nuses a spreading matrix to distribute each symbol across multiple subcarriers,\nenhancing robustness against jamming. For optimal demodulation at a receiver,\nwe devise a maximum likelihood detection (MLD) method and its low-complexity\nvariant tailored to our anti-jamming modulation scheme in scenarios with known\njamming variance. We analyze the bit error rate (BER) of our modulation scheme\nto optimize its modulation order according to a jamming scenario. To adapt to\ndynamic and unknown jamming environments, we present a jamming-adaptive\ncommunication framework consisting of two phases: (i) a jamming-noncoherent\nphase and (ii) a jamming-coherent phase. In the jamming-noncoherent phase, we\ndevelop an approximate MLD method that operates without prior knowledge of\njamming variance and enables the estimation of jamming parameters. In the\njamming-coherent phase, we use these estimated parameters to optimize the\nproposed modulation scheme while employing the low-complexity MLD method.\nSimulation results demonstrate the superior BER performance of the proposed\nanti-jamming framework compared to existing OFDM communication frameworks\nacross a wide range of communication and jamming scenarios."}
{"id": "2510.02627", "pdf": "https://arxiv.org/pdf/2510.02627", "abs": "https://arxiv.org/abs/2510.02627", "authors": ["Ruining Yang", "Yi Xu", "Yixiao Chen", "Yun Fu", "Lili Su"], "title": "A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate trajectory prediction is fundamental to autonomous driving, as it\nunderpins safe motion planning and collision avoidance in complex environments.\nHowever, existing benchmark datasets suffer from a pronounced long-tail\ndistribution problem, with most samples drawn from low-density scenarios and\nsimple straight-driving behaviors. This underrepresentation of high-density\nscenarios and safety critical maneuvers such as lane changes, overtaking and\nturning is an obstacle to model generalization and leads to overly optimistic\nevaluations. To address these challenges, we propose a novel trajectory\ngeneration framework that simultaneously enhances scenarios density and\nenriches behavioral diversity. Specifically, our approach converts continuous\nroad environments into a structured grid representation that supports\nfine-grained path planning, explicit conflict detection, and multi-agent\ncoordination. Built upon this representation, we introduce behavior-aware\ngeneration mechanisms that combine rule-based decision triggers with\nFrenet-based trajectory smoothing and dynamic feasibility constraints. This\ndesign allows us to synthesize realistic high-density scenarios and rare\nbehaviors with complex interactions that are often missing in real data.\nExtensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets\ndemonstrate that our method significantly improves both agent density and\nbehavior diversity, while preserving motion realism and scenario-level safety.\nOur synthetic data also benefits downstream trajectory prediction models and\nenhances performance in challenging high-density scenarios."}
{"id": "2510.02700", "pdf": "https://arxiv.org/pdf/2510.02700", "abs": "https://arxiv.org/abs/2510.02700", "authors": ["Sagar Lekhak", "Emmett J. Ientilucci", "Jasper Baur", "Susmita Ghosh"], "title": "A UAV-Based VNIR Hyperspectral Benchmark Dataset for Landmine and UXO Detection", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "This work has been accepted and will be presented at the Indian\n  Geoscience and Remote Sensing Symposium (InGARSS) 2025 in India and will\n  appear in the IEEE InGARSS 2025 Proceedings", "summary": "This paper introduces a novel benchmark dataset of Visible and Near-Infrared\n(VNIR) hyperspectral imagery acquired via an unmanned aerial vehicle (UAV)\nplatform for landmine and unexploded ordnance (UXO) detection research. The\ndataset was collected over a controlled test field seeded with 143 realistic\nsurrogate landmine and UXO targets, including surface, partially buried, and\nfully buried configurations. Data acquisition was performed using a Headwall\nNano-Hyperspec sensor mounted on a multi-sensor drone platform, flown at an\naltitude of approximately 20.6 m, capturing 270 contiguous spectral bands\nspanning 398-1002 nm. Radiometric calibration, orthorectification, and\nmosaicking were performed followed by reflectance retrieval using a two-point\nEmpirical Line Method (ELM), with reference spectra acquired using an SVC\nspectroradiometer. Cross-validation against six reference objects yielded RMSE\nvalues below 1.0 and SAM values between 1 and 6 degrees in the 400-900 nm\nrange, demonstrating high spectral fidelity. The dataset is released alongside\nraw radiance cubes, GCP/AeroPoint data, and reference spectra to support\nreproducible research. This contribution fills a critical gap in open-access\nUAV-based hyperspectral data for landmine detection and offers a multi-sensor\nbenchmark when combined with previously published drone-based electromagnetic\ninduction (EMI) data from the same test field."}
{"id": "2510.02716", "pdf": "https://arxiv.org/pdf/2510.02716", "abs": "https://arxiv.org/abs/2510.02716", "authors": ["Junlin Zeng", "Xin Zhang", "Xiang Zhao", "Yan Pan"], "title": "A $1000\\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Path planning in grid maps, arising from various applications, has garnered\nsignificant attention. Existing methods, such as A*, Dijkstra, and their\nvariants, work well for small-scale maps but fail to address large-scale ones\ndue to high search time and memory consumption. Recently, Large Language Models\n(LLMs) have shown remarkable performance in path planning but still suffer from\nspatial illusion and poor planning performance. Among all the works, LLM-A*\n\\cite{meng2024llm} leverages LLM to generate a series of waypoints and then\nuses A* to plan the paths between the neighboring waypoints. In this way, the\ncomplete path is constructed. However, LLM-A* still suffers from high\ncomputational time for large-scale maps. To fill this gap, we conducted a deep\ninvestigation into LLM-A* and found its bottleneck, resulting in limited\nperformance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.\nas iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the\noptimization of A*, an incremental learning method for LLM to generate\nhigh-quality waypoints, and the selection of the appropriate waypoints for A*\nfor path planning. Finally, a comprehensive evaluation on various grid maps\nshows that, compared with LLM-A*, iLLM-A* \\textbf{1) achieves more than\n$1000\\times$ speedup on average, and up to $2349.5\\times$ speedup in the\nextreme case, 2) saves up to $58.6\\%$ of the memory cost, 3) achieves both\nobviously shorter path length and lower path length standard deviation.}"}
{"id": "2510.02800", "pdf": "https://arxiv.org/pdf/2510.02800", "abs": "https://arxiv.org/abs/2510.02800", "authors": ["Rohith Reddy Vennam", "Maiyun Zhang", "Raghav Subbaraman", "Deepak Vashist", "Dinesh Bharadia"], "title": "FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways", "categories": ["cs.NI", "eess.SP"], "comment": "14 pages, 19 figures", "summary": "The proliferation of Low Earth Orbit (LEO) satellites for universal IoT\napplications and the growing use of drones in emergency services, agriculture,\nand military operations highlight the transformative potential of\nnon-terrestrial networks (NTN). However, these networks face two key\nchallenges: (1) large coverage footprints that create frequent collisions and\n(2) moving gateways that cause dynamic links and demand synchronization-free,\nlink-aware transmissions. Existing random access schemes such as ALOHA, CSMA,\nand BSMA fail in this setting, suffering from high collision rates, hidden\nterminals, or excessive gateway energy overhead. We propose Free Signal\nMultiple Access (FSMA), a gateway-controlled protocol that introduces a\nlightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes\ntransmit only when the channel is idle and when links are reliable, thereby\nreducing collisions and enabling link-aware access without the need for\nsynchronization or complex scheduling. We evaluate FSMA using 25 commercial\nLoRa devices with a drone-mounted moving gateway and demonstrate up to 2x\nhigher throughput, 2x to 5x better packet reception ratio, and 5x improved\nenergy efficiency compared to the baselines. Large-scale simulations with a\ncustom Satellite IoT Simulator further show that FSMA scales to 5000+ devices\nper satellite pass. These results establish FSMA as a practical step toward\nscalable, energy-efficient, and reliable NTN IoT networks."}
{"id": "2510.02728", "pdf": "https://arxiv.org/pdf/2510.02728", "abs": "https://arxiv.org/abs/2510.02728", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Yuchen Zhang", "Haoxiang Fu", "Ruibin Hu", "Yanbiao Ma", "Wenbo Ding", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4", "categories": ["cs.RO"], "comment": null, "summary": "Cross-modal drone navigation remains a challenging task in robotics,\nrequiring efficient retrieval of relevant images from large-scale databases\nbased on natural language descriptions. The RoboSense 2025 Track 4 challenge\naddresses this challenge, focusing on robust, natural language-guided\ncross-view image retrieval across multiple platforms (drones, satellites, and\nground cameras). Current baseline methods, while effective for initial\nretrieval, often struggle to achieve fine-grained semantic matching between\ntext queries and visual content, especially in complex aerial scenes. To\naddress this challenge, we propose a two-stage retrieval refinement method:\nCaption-Guided Retrieval System (CGRS) that enhances the baseline coarse\nranking through intelligent reranking. Our method first leverages a baseline\nmodel to obtain an initial coarse ranking of the top 20 most relevant images\nfor each query. We then use Vision-Language-Model (VLM) to generate detailed\ncaptions for these candidate images, capturing rich semantic descriptions of\ntheir visual content. These generated captions are then used in a multimodal\nsimilarity computation framework to perform fine-grained reranking of the\noriginal text query, effectively building a semantic bridge between the visual\ncontent and natural language descriptions. Our approach significantly improves\nupon the baseline, achieving a consistent 5\\% improvement across all key\nmetrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the\nchallenge, demonstrating the practical value of our semantic refinement\nstrategy in real-world robotic navigation scenarios."}
{"id": "2510.02889", "pdf": "https://arxiv.org/pdf/2510.02889", "abs": "https://arxiv.org/abs/2510.02889", "authors": ["Mohammadreza Doostmohammadian", "Narahari Kasagatta Ramesh", "Alireza Aghasi"], "title": "Delay-Tolerant Augmented-Consensus-based Distributed Directed Optimization", "categories": ["eess.SY", "cs.MA", "cs.SI", "cs.SY", "eess.SP", "math.OC"], "comment": "Systems & Control Letters", "summary": "Distributed optimization finds applications in large-scale machine learning,\ndata processing and classification over multi-agent networks. In real-world\nscenarios, the communication network of agents may encounter latency that may\naffect the convergence of the optimization protocol. This paper addresses the\ncase where the information exchange among the agents (computing nodes) over\ndata-transmission channels (links) might be subject to communication\ntime-delays, which is not well addressed in the existing literature. Our\nproposed algorithm improves the state-of-the-art by handling heterogeneous and\narbitrary but bounded and fixed (time-invariant) delays over general\nstrongly-connected directed networks. Arguments from matrix theory, algebraic\ngraph theory, and augmented consensus formulation are applied to prove the\nconvergence to the optimal value. Simulations are provided to verify the\nresults and compare the performance with some existing delay-free algorithms."}
{"id": "2510.02738", "pdf": "https://arxiv.org/pdf/2510.02738", "abs": "https://arxiv.org/abs/2510.02738", "authors": ["Tianyu Li", "Yihan Li", "Zizhe Zhang", "Nadia Figueroa"], "title": "Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "While visuomotor policy has made advancements in recent years, contact-rich\ntasks still remain a challenge. Robotic manipulation tasks that require\ncontinuous contact demand explicit handling of compliance and force. However,\nmost visuomotor policies ignore compliance, overlooking the importance of\nphysical interaction with the real world, often leading to excessive contact\nforces or fragile behavior under uncertainty. Introducing force information\ninto vision-based imitation learning could help improve awareness of contacts,\nbut could also require a lot of data to perform well. One remedy for data\nscarcity is to generate data in simulation, yet computationally taxing\nprocesses are required to generate data good enough not to suffer from the\nSim2Real gap. In this work, we introduce a framework for generating\nforce-informed data in simulation, instantiated by a single human\ndemonstration, and show how coupling with a compliant policy improves the\nperformance of a visuomotor policy learned from synthetic data. We validate our\napproach on real-robot tasks, including non-prehensile block flipping and a\nbi-manual object moving, where the learned policy exhibits reliable contact\nmaintenance and adaptation to novel conditions. Project Website:\nhttps://flow-with-the-force-field.github.io/webpage/"}
{"id": "2510.02803", "pdf": "https://arxiv.org/pdf/2510.02803", "abs": "https://arxiv.org/abs/2510.02803", "authors": ["Yifan Liao", "Zhen Sun", "Xiaoyun Qiu", "Zixiao Zhao", "Wenbing Tang", "Xinlei He", "Xinhu Zheng", "Tianwei Zhang", "Xinyi Huang", "Xingshuo Han"], "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages,5 figures", "summary": "Visual Language Models (VLMs), with powerful multimodal reasoning\ncapabilities, are gradually integrated into autonomous driving by several\nautomobile manufacturers to enhance planning capability in challenging\nenvironments. However, the trajectory planning capability of VLMs in work\nzones, which often include irregular layouts, temporary traffic control, and\ndynamically changing geometric structures, is still unexplored. To bridge this\ngap, we conduct the \\textit{first} systematic study of VLMs for work zone\ntrajectory planning, revealing that mainstream VLMs fail to generate correct\ntrajectories in $68.0%$ of cases. To better understand these failures, we first\nidentify candidate patterns via subgraph mining and clustering analysis, and\nthen confirm the validity of $8$ common failure patterns through human\nverification. Building on these findings, we propose REACT-Drive, a trajectory\nplanning framework that integrates VLMs with Retrieval-Augmented Generation\n(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases\ninto constraint rules and executable trajectory planning code, while RAG\nretrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a\nreduction of around $3\\times$ in average displacement error relative to VLM\nbaselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the\nlowest inference time ($0.58$s) compared with other methods such as fine-tuning\n($17.90$s). We further conduct experiments using a real vehicle in 15 work zone\nscenarios in the physical world, demonstrating the strong practicality of\nREACT-Drive."}
{"id": "2510.02808", "pdf": "https://arxiv.org/pdf/2510.02808", "abs": "https://arxiv.org/abs/2510.02808", "authors": ["Andreas Christou", "Elliot Lister", "Georgia Andreopoulou", "Don Mahad", "Sethu Vijayakumar"], "title": "Assist-as-needed Control for FES in Foot Drop Management", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Foot drop is commonly managed using Functional Electrical Stimulation (FES),\ntypically delivered via open-loop controllers with fixed stimulation\nintensities. While users may manually adjust the intensity through external\ncontrols, this approach risks overstimulation, leading to muscle fatigue and\ndiscomfort, or understimulation, which compromises dorsiflexion and increases\nfall risk. In this study, we propose a novel closed-loop FES controller that\ndynamically adjusts the stimulation intensity based on real-time toe clearance,\nproviding \"assistance as needed\". We evaluate this system by inducing foot drop\nin healthy participants and comparing the effects of the closed-loop controller\nwith a traditional open-loop controller across various walking conditions,\nincluding different speeds and surface inclinations. Kinematic data reveal that\nour closed-loop controller maintains adequate toe clearance without\nsignificantly affecting the joint angles of the hips, the knees, and the\nankles, and while using significantly lower stimulation intensities compared to\nthe open-loop controller. These findings suggest that the proposed method not\nonly matches the effectiveness of existing systems but also offers the\npotential for reduced muscle fatigue and improved long-term user comfort and\nadherence."}
{"id": "2510.02851", "pdf": "https://arxiv.org/pdf/2510.02851", "abs": "https://arxiv.org/abs/2510.02851", "authors": ["Jeyoung Park", "Yeonsub Lim", "Seungeun Oh", "Jihong Park", "Jinho Choi", "Seong-Lyun Kim"], "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots", "categories": ["cs.RO", "cs.DC"], "comment": null, "summary": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML, connecting\ndistributed computational resources in edge and cloud over hyper-reliable\nlow-latency communication (HRLLC). In this setting, speculative decoding can\nfacilitate collaborative inference of models distributively deployed: an\non-device draft model locally generates drafts and a remote server-based target\nmodel verifies and corrects them, resulting lower latency. However, unlike\nautoregressive text generation, behavior cloning policies, typically used for\nembodied AI applications like robot manipulation and autonomous driving, cannot\nparallelize verification and correction for multiple drafts as each action\ndepends on observation which needs to be updated by a previous action. To this\nend, we propose Action Deviation-Aware Hybrid Inference, wherein the draft\nmodel estimates an action's need for verification and correction by the target\nmodel and selectively skips communication and computation for server\noperations. Action deviation shows a strong correlation with action's rejection\nprobability by the target model, enabling selective skipping. We derive the\npath deviation threshold that balances the transmission rate and the inference\nperformance, and we empirically show that action deviation-aware hybrid\ninference reduces uplink transmission and server operation by 40%, while\nlowering end-to-end latency by 33.32% relative to hybrid inference without\nskipping and achieving task success rate up to 97.03% of that of target model\nonly inference."}
{"id": "2510.02874", "pdf": "https://arxiv.org/pdf/2510.02874", "abs": "https://arxiv.org/abs/2510.02874", "authors": ["Charith Premachandra", "U-Xuan Tan"], "title": "Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping", "categories": ["cs.RO"], "comment": "Accepted and presented at the 15th International Conference on Indoor\n  Positioning and Indoor Navigation (IPIN) 2025, see\n  https://ipin-conference.org/2025/", "summary": "Traditional exteroceptive sensors in mobile robots, such as LiDARs and\ncameras often struggle to perceive the environment in poor visibility\nconditions. Recently, radar technologies, such as ultra-wideband (UWB) have\nemerged as potential alternatives due to their ability to see through adverse\nenvironmental conditions (e.g. dust, smoke and rain). However, due to the small\napertures with low directivity, the UWB radars cannot reconstruct a detailed\nimage of its field of view (FOV) using a single scan. Hence, a virtual large\naperture is synthesized by moving the radar along a mobile robot path. The\nresulting synthetic aperture radar (SAR) image is a high-definition\nrepresentation of the surrounding environment. Hence, this paper proposes a\npipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an\nunknown environment. Finally, we evaluated the performance of classical feature\ndetectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB\nSAR images. The experiments were conducted emulating adverse environmental\nconditions. The results demonstrate the viability and effectiveness of UWB SAR\nimaging for high-resolution environmental mapping and loop closure detection\ntoward more robust and reliable robotic perception systems."}
{"id": "2510.02885", "pdf": "https://arxiv.org/pdf/2510.02885", "abs": "https://arxiv.org/abs/2510.02885", "authors": ["Faduo Liang", "Yunfeng Yang", "Shi-Lu Dai"], "title": "Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots", "categories": ["cs.RO"], "comment": "8 pages, 8 figures, accepted to IROS2025", "summary": "In this work, we propose a novel motion planning algorithm to facilitate\nsafety-critical navigation for autonomous mobile robots. The proposed algorithm\nintegrates a real-time dynamic obstacle tracking and mapping system that\ncategorizes point clouds into dynamic and static components. For dynamic point\nclouds, the Kalman filter is employed to estimate and predict their motion\nstates. Based on these predictions, we extrapolate the future states of dynamic\npoint clouds, which are subsequently merged with static point clouds to\nconstruct the forward-time-domain (FTD) map. By combining control barrier\nfunctions (CBFs) with nonlinear model predictive control, the proposed\nalgorithm enables the robot to effectively avoid both static and dynamic\nobstacles. The CBF constraints are formulated based on risk points identified\nthrough collision detection between the predicted future states and the FTD\nmap. Experimental results from both simulated and real-world scenarios\ndemonstrate the efficacy of the proposed algorithm in complex environments. In\nsimulation experiments, the proposed algorithm is compared with two baseline\napproaches, showing superior performance in terms of safety and robustness in\nobstacle avoidance. The source code is released for the reference of the\nrobotics community."}
{"id": "2510.02941", "pdf": "https://arxiv.org/pdf/2510.02941", "abs": "https://arxiv.org/abs/2510.02941", "authors": ["Stefano Trepella", "Mauro Martini", "No√© P√©rez-Higueras", "Andrea Ostuni", "Fernando Caballero", "Luis Merino", "Marcello Chiaberge"], "title": "Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis", "categories": ["cs.RO"], "comment": null, "summary": "Social, also called human-aware, navigation is a key challenge for the\nintegration of mobile robots into human environments. The evaluation of such\nsystems is complex, as factors such as comfort, safety, and legibility must be\nconsidered. Human-centered assessments, typically conducted through surveys,\nprovide reliable insights but are costly, resource-intensive, and difficult to\nreproduce or compare across systems. Alternatively, numerical social navigation\nmetrics are easy to compute and facilitate comparisons, yet the community lacks\nconsensus on a standard set of metrics.\n  This work explores the relationship between numerical metrics and\nhuman-centered evaluations to identify potential correlations. If specific\nquantitative measures align with human perceptions, they could serve as\nstandardized evaluation tools, reducing the dependency on surveys. Our results\nindicate that while current metrics capture some aspects of robot navigation\nbehavior, important subjective factors remain insufficiently represented and\nnew metrics are necessary."}
{"id": "2510.02946", "pdf": "https://arxiv.org/pdf/2510.02946", "abs": "https://arxiv.org/abs/2510.02946", "authors": ["Juraj Lieskovsk√Ω", "Hijiri Akahane", "Aoto Osawa", "Jaroslav Bu≈°ek", "Ikuo Mizuuchi", "Tom√°≈° Vyhl√≠dal"], "title": "Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases", "categories": ["cs.RO", "cs.SY", "eess.SY", "93D25", "I.6.5; F.2.1"], "comment": "11 pages, 13 figures, 1 table, Accepted 27 July 2025, Available\n  online 16 Sept 2025, Version of Record 28 Sept 2025", "summary": "A complete mechatronic design of a minimal configuration brachiation robot is\npresented. The robot consists of a single rigid rod with gripper mechanisms\nattached to both ends. The grippers are used to hang the robot on a horizontal\nbar on which it swings or rotates. The motion is imposed by repositioning the\nrobot's center of mass, which is performed using a crank-slide mechanism. Based\non a non-linear model, an optimal control strategy is proposed, for\nrepositioning the center of mass in a bang-bang manner. Consequently, utilizing\nthe concept of input-output linearization, a continuous control strategy is\nproposed that takes into account the limited torque of the crank-slide\nmechanism and its geometry. An increased attention is paid to energy\naccumulation towards the subsequent jump stage of the brachiation. These two\nstrategies are validated and compared in simulations. The continuous control\nstrategy is then also implemented within a low-cost STM32-based control system,\nand both the swing and rotation stages of the brachiation motion are\nexperimentally validated."}
{"id": "2510.02968", "pdf": "https://arxiv.org/pdf/2510.02968", "abs": "https://arxiv.org/abs/2510.02968", "authors": ["Amir Habel", "Fawad Mehboob", "Jeffrin Sam", "Clement Fortin", "Dzmitry Tsetserukou"], "title": "YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control", "categories": ["cs.RO"], "comment": null, "summary": "Achieving precise lateral motion modeling and decoupled control in hover\nremains a significant challenge for tail-sitter Unmanned Aerial Vehicles\n(UAVs), primarily due to complex aerodynamic couplings and the absence of\nwelldefined lateral dynamics. This paper presents a novel modeling and control\nstrategy that enhances yaw authority and lateral motion by introducing a\nsideslip force model derived from differential propeller slipstream effects\nacting on the fuselage under differential thrust. The resulting lateral force\nalong the body y-axis enables yaw-based lateral position control without\ninducing roll coupling. The control framework employs a YXZ Euler rotation\nformulation to accurately represent attitude and incorporate gravitational\ncomponents while directly controlling yaw in the yaxis, thereby improving\nlateral dynamic behavior and avoiding singularities. The proposed approach is\nvalidated through trajectory-tracking simulations conducted in a Unity-based\nenvironment. Tests on both rectangular and circular paths in hover mode\ndemonstrate stable performance, with low mean absolute position errors and yaw\ndeviations constrained within 5.688 degrees. These results confirm the\neffectiveness of the proposed lateral force generation model and provide a\nfoundation for the development of agile, hover-capable tail-sitter UAVs."}
{"id": "2510.02975", "pdf": "https://arxiv.org/pdf/2510.02975", "abs": "https://arxiv.org/abs/2510.02975", "authors": ["Amir Hossein Barjini", "Jouni Mattila"], "title": "AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel framework for estimating the position and\norientation of flexible manipulators undergoing vertical motion using multiple\ninertial measurement units (IMUs), optimized and calibrated with ground truth\ndata. The flexible links are modeled as a series of rigid segments, with joint\nangles estimated from accelerometer and gyroscope measurements acquired by\ncost-effective IMUs. A complementary filter is employed to fuse the\nmeasurements, with its parameters optimized through particle swarm optimization\n(PSO) to mitigate noise and delay. To further improve estimation accuracy,\nresidual errors in position and orientation are compensated using radial basis\nfunction neural networks (RBFNN). Experimental results validate the\neffectiveness of the proposed intelligent multi-IMU kinematic estimation\nmethod, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and\n0.00024~rad for $y$, $z$, and $\\theta$, respectively."}
{"id": "2510.02976", "pdf": "https://arxiv.org/pdf/2510.02976", "abs": "https://arxiv.org/abs/2510.02976", "authors": ["Alvaro Paz", "Pauli Mustalahti", "Mohammad Dastranj", "Jouni Mattila"], "title": "Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a framework for real-time optimal controlling of a\nheavy-duty skid-steered mobile platform for trajectory tracking. The importance\nof accurate real-time performance of the controller lies in safety\nconsiderations of situations where the dynamic system under control is affected\nby uncertainties and disturbances, and the controller should compensate for\nsuch phenomena in order to provide stable performance. A multiple-shooting\nnonlinear model-predictive control framework is proposed in this paper. This\nframework benefits from suitable algorithm along with readings from various\nsensors for genuine real-time performance with extremely high accuracy. The\ncontroller is then tested for tracking different trajectories where it\ndemonstrates highly desirable performance in terms of both speed and accuracy.\nThis controller shows remarkable improvement when compared to existing\nnonlinear model-predictive controllers in the literature that were implemented\non skid-steered mobile platforms."}
{"id": "2510.03011", "pdf": "https://arxiv.org/pdf/2510.03011", "abs": "https://arxiv.org/abs/2510.03011", "authors": ["Chenyuan Chen", "Haoran Ding", "Ran Ding", "Tianyu Liu", "Zewen He", "Anqing Duan", "Dezhen Song", "Xiaodan Liang", "Yoshihiko Nakamura"], "title": "3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning", "categories": ["cs.RO"], "comment": null, "summary": "Diffusion models, as a class of deep generative models, have recently emerged\nas powerful tools for robot skills by enabling stable training with reliable\nconvergence. In this paper, we present an end-to-end framework for generating\nlong, smooth trajectories that explicitly target high surface coverage across\nvarious industrial tasks, including polishing, robotic painting, and spray\ncoating. The conventional methods are always fundamentally constrained by their\npredefined functional forms, which limit the shapes of the trajectories they\ncan represent and make it difficult to handle complex and diverse tasks.\nMoreover, their generalization is poor, often requiring manual redesign or\nextensive parameter tuning when applied to new scenarios. These limitations\nhighlight the need for more expressive generative models, making\ndiffusion-based approaches a compelling choice for trajectory generation. By\niteratively denoising trajectories with carefully learned noise schedules and\nconditioning mechanisms, diffusion models not only ensure smooth and consistent\nmotion but also flexibly adapt to the task context. In experiments, our method\nimproves trajectory continuity, maintains high coverage, and generalizes to\nunseen shapes, paving the way for unified end-to-end trajectory learning across\nindustrial surface-processing tasks without category-specific models. On\naverage, our approach improves Point-wise Chamfer Distance by 98.2\\% and\nsmoothness by 97.0\\%, while increasing surface coverage by 61\\% compared to\nprior methods. The link to our code can be found\n\\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}."}
{"id": "2510.03022", "pdf": "https://arxiv.org/pdf/2510.03022", "abs": "https://arxiv.org/abs/2510.03022", "authors": ["Rui Zhong", "Yizhe Sun", "Junjie Wen", "Jinming Li", "Chuang Cheng", "Wei Dai", "Zhiwen Zeng", "Huimin Lu", "Yichen Zhu", "Yi Xu"], "title": "HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton", "categories": ["cs.RO"], "comment": null, "summary": "A significant bottleneck in humanoid policy learning is the acquisition of\nlarge-scale, diverse datasets, as collecting reliable real-world data remains\nboth difficult and cost-prohibitive. To address this limitation, we introduce\nHumanoidExo, a novel system that transfers human motion to whole-body humanoid\ndata. HumanoidExo offers a high-efficiency solution that minimizes the\nembodiment gap between the human demonstrator and the robot, thereby tackling\nthe scarcity of whole-body humanoid data. By facilitating the collection of\nmore voluminous and diverse datasets, our approach significantly enhances the\nperformance of humanoid robots in dynamic, real-world scenarios. We evaluated\nour method across three challenging real-world tasks: table-top manipulation,\nmanipulation integrated with stand-squat motions, and whole-body manipulation.\nOur results empirically demonstrate that HumanoidExo is a crucial addition to\nreal-robot data, as it enables the humanoid policy to generalize to novel\nenvironments, learn complex whole-body control from only five real-robot\ndemonstrations, and even acquire new skills (i.e., walking) solely from\nHumanoidExo data."}
{"id": "2510.03031", "pdf": "https://arxiv.org/pdf/2510.03031", "abs": "https://arxiv.org/abs/2510.03031", "authors": ["Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics", "categories": ["cs.RO"], "comment": "IEEE Robotics and Automation Letters", "summary": "Long-term human motion prediction (LHMP) is important for the safe and\nefficient operation of autonomous robots and vehicles in environments shared\nwith humans. Accurate predictions are important for applications including\nmotion planning, tracking, human-robot interaction, and safety monitoring. In\nthis paper, we exploit Maps of Dynamics (MoDs), which encode spatial or\nspatio-temporal motion patterns as environment features, to achieve LHMP for\nhorizons of up to 60 seconds. We propose an MoD-informed LHMP framework that\nsupports various types of MoDs and includes a ranking method to output the most\nlikely predicted trajectory, improving practical utility in robotics. Further,\na time-conditioned MoD is introduced to capture motion patterns that vary\nacross different times of day. We evaluate MoD-LHMP instantiated with three\ntypes of MoDs. Experiments on two real-world datasets show that MoD-informed\nmethod outperforms learning-based ones, with up to 50\\% improvement in average\ndisplacement error, and the time-conditioned variant achieves the highest\naccuracy overall. Project code is available at\nhttps://github.com/test-bai-cpu/LHMP-with-MoDs.git"}
{"id": "2510.03081", "pdf": "https://arxiv.org/pdf/2510.03081", "abs": "https://arxiv.org/abs/2510.03081", "authors": ["Guiliang Liu", "Bo Yue", "Yi Jin Kim", "Kui Jia"], "title": "Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid robots, as general-purpose physical agents, must integrate both\nintelligent control and adaptive morphology to operate effectively in diverse\nreal-world environments. While recent research has focused primarily on\noptimizing control policies for fixed robot structures, this position paper\nargues for evolving both control strategies and humanoid robots' physical\nstructure under a co-design mechanism. Inspired by biological evolution, this\napproach enables robots to iteratively adapt both their form and behavior to\noptimize performance within task-specific and resource-constrained contexts.\nDespite its promise, co-design in humanoid robotics remains a relatively\nunderexplored domain, raising fundamental questions about its feasibility and\nnecessity in achieving true embodied intelligence. To address these challenges,\nwe propose practical co-design methodologies grounded in strategic exploration,\nSim2Real transfer, and meta-policy learning. We further argue for the essential\nrole of co-design by analyzing it from methodological, application-driven, and\ncommunity-oriented perspectives. Striving to guide and inspire future studies,\nwe present open research questions, spanning from short-term innovations to\nlong-term goals. This work positions co-design as a cornerstone for developing\nthe next generation of intelligent and adaptable humanoid agents."}
{"id": "2510.03119", "pdf": "https://arxiv.org/pdf/2510.03119", "abs": "https://arxiv.org/abs/2510.03119", "authors": ["Chaoxiang Ye", "Guido de Croon", "Salua Hamaza"], "title": "Whisker-based Tactile Flight for Tiny Drones", "categories": ["cs.RO"], "comment": null, "summary": "Tiny flying robots hold great potential for search-and-rescue, safety\ninspections, and environmental monitoring, but their small size limits\nconventional sensing-especially with poor-lighting, smoke, dust or reflective\nobstacles. Inspired by nature, we propose a lightweight, 3.2-gram,\nwhisker-based tactile sensing apparatus for tiny drones, enabling them to\nnavigate and explore through gentle physical interaction. Just as rats and\nmoles use whiskers to perceive surroundings, our system equips drones with\ntactile perception in flight, allowing obstacle sensing even in pitch-dark\nconditions. The apparatus uses barometer-based whisker sensors to detect\nobstacle locations while minimising destabilisation. To address sensor noise\nand drift, we develop a tactile depth estimation method achieving sub-6 mm\naccuracy. This enables drones to navigate, contour obstacles, and explore\nconfined spaces solely through touch-even in total darkness along both soft and\nrigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system\nsupports autonomous tactile flight and is validated in both simulation and\nreal-world tests. Our bio-inspired approach redefines vision-free navigation,\nopening new possibilities for micro aerial vehicles in extreme environments."}
{"id": "2510.03123", "pdf": "https://arxiv.org/pdf/2510.03123", "abs": "https://arxiv.org/abs/2510.03123", "authors": ["Zhe Shen"], "title": "Learning Stability Certificate for Robotics in Real-World Environments", "categories": ["cs.RO"], "comment": null, "summary": "Stability certificates play a critical role in ensuring the safety and\nreliability of robotic systems. However, deriving these certificates for\ncomplex, unknown systems has traditionally required explicit knowledge of\nsystem dynamics, often making it a daunting task. This work introduces a novel\nframework that learns a Lyapunov function directly from trajectory data,\nenabling the certification of stability for autonomous systems without needing\ndetailed system models. By parameterizing the Lyapunov candidate using a neural\nnetwork and ensuring positive definiteness through Cholesky factorization, our\napproach automatically identifies whether the system is stable under the given\ntrajectory. To address the challenges posed by noisy, real-world data, we allow\nfor controlled violations of the stability condition, focusing on maintaining\nhigh confidence in the stability certification process. Our results demonstrate\nthat this framework can provide data-driven stability guarantees, offering a\nrobust method for certifying the safety of robotic systems in dynamic,\nreal-world environments. This approach works without access to the internal\ncontrol algorithms, making it applicable even in situations where system\nbehavior is opaque or proprietary. The tool for learning the stability proof is\nopen-sourced by this research: https://github.com/HansOersted/stability."}
{"id": "2510.03142", "pdf": "https://arxiv.org/pdf/2510.03142", "abs": "https://arxiv.org/abs/2510.03142", "authors": ["Tianyu Xu", "Jiawei Chen", "Jiazhao Zhang", "Wenyao Zhang", "Zekun Qi", "Minghan Li", "Zhizheng Zhang", "He Wang"], "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://pku-epic.github.io/MM-Nav-Web/", "summary": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method."}
{"id": "2510.03169", "pdf": "https://arxiv.org/pdf/2510.03169", "abs": "https://arxiv.org/abs/2510.03169", "authors": ["Duanjiao Li", "Yun Chen", "Ying Zhang", "Junwen Yao", "Dongyue Huang", "Jianguo Zhang", "Ning Ding"], "title": "Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment", "categories": ["cs.RO"], "comment": "This paper has been accepted for publication in the 44th Chinese\n  Control Conference, 2025. Please cite the paper using appropriate formats", "summary": "For typical applications of UAVs in power grid scenarios, we construct the\nproblem as planning UAV trajectories for coverage in cluttered environments. In\nthis paper, we propose an optimal smooth coverage trajectory planning\nalgorithm. The algorithm consists of two stages. In the front-end, a Genetic\nAlgorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for\nPoints of Interest (POIs), generating an initial sequence of optimized visiting\npoints. In the back-end, the sequence is further optimized by considering\ntrajectory smoothness, time consumption, and obstacle avoidance. This is\nformulated as a nonlinear least squares problem and solved to produce a smooth\ncoverage trajectory that satisfies these constraints. Numerical simulations\nvalidate the effectiveness of the proposed algorithm, ensuring UAVs can\nsmoothly cover all POIs in cluttered environments."}
{"id": "2510.03182", "pdf": "https://arxiv.org/pdf/2510.03182", "abs": "https://arxiv.org/abs/2510.03182", "authors": ["Yilun Hao", "Yongchao Chen", "Chuchu Fan", "Yang Zhang"], "title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.SC"], "comment": "30 pages, 5 figures, 5 tables", "summary": "Vision Language Models (VLMs) show strong potential for visual planning but\nstruggle with precise spatial and long-horizon reasoning. In contrast, Planning\nDomain Definition Language (PDDL) planners excel at long-horizon formal\nplanning, but cannot interpret visual inputs. Recent works combine these\ncomplementary advantages by enabling VLMs to turn visual planning problems into\nPDDL files for formal planning. However, while VLMs can generate PDDL problem\nfiles satisfactorily, they struggle to accurately generate the PDDL domain\nfiles, which describe all the planning rules. As a result, prior methods rely\non human experts to predefine domain files or on constant environment access\nfor refinement. We propose VLMFP, a Dual-VLM-guided framework that can\nautonomously generate both PDDL problem and domain files for formal visual\nplanning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A\nSimVLM that simulates action consequences based on input rule descriptions, and\na GenVLM that generates and iteratively refines PDDL files by comparing the\nPDDL and SimVLM execution results. VLMFP unleashes multiple levels of\ngeneralizability: The same generated PDDL domain file works for all the\ndifferent instances under the same problem, and VLMs generalize to different\nproblems with varied appearances and rules. We evaluate VLMFP with 6 grid-world\ndomains and test its generalization to unseen instances, appearance, and game\nrules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,\nsimulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal\nreaching for seen and unseen appearances, respectively. With the guidance of\nSimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for\nunseen instances in seen and unseen appearances, respectively. Project page:\nhttps://sites.google.com/view/vlmfp."}
{"id": "2510.02364", "pdf": "https://arxiv.org/pdf/2510.02364", "abs": "https://arxiv.org/abs/2510.02364", "authors": ["Tianyi Li", "Tianyu Liu", "Yicheng Yang"], "title": "Conceptualizing and Modeling Communication-Based Cyberattacks on Automated Vehicles", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Adaptive Cruise Control (ACC) is rapidly proliferating across electric\nvehicles (EVs) and internal combustion engine (ICE) vehicles, enhancing traffic\nflow while simultaneously expanding the attack surface for communication-based\ncyberattacks. Because the two powertrains translate control inputs into motion\ndifferently, their cyber-resilience remains unquantified. Therefore, we\nformalize six novel message-level attack vectors and implement them in a\nring-road simulation that systematically varies the ACC market penetration\nrates (MPRs) and the spatial pattern of compromised vehicles. A three-tier risk\ntaxonomy converts disturbance metrics into actionable defense priorities for\npractitioners. Across all simulation scenarios, EV platoons exhibit lower\nvelocity standard deviation, reduced spacing oscillations, and faster\npost-attack recovery compared to ICE counterparts, revealing an inherent\nstability advantage. These findings clarify how controller-to-powertrain\ncoupling influences vulnerability and offer quantitative guidance for the\ndetection and mitigation of attacks in mixed automated traffic."}
{"id": "2510.02769", "pdf": "https://arxiv.org/pdf/2510.02769", "abs": "https://arxiv.org/abs/2510.02769", "authors": ["Chidre Shravista Kashyap", "Karnan A", "Pushpak Jagtap", "Jishnu Keshavan"], "title": "Periodic Event-Triggered Prescribed Time Control of Euler-Lagrange Systems under State and Input Constraints", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "This article proposes a periodic event-triggered adaptive barrier control\npolicy for the trajectory tracking problem of perturbed Euler-Lagrangian\nsystems with state, input, and temporal (SIT) constraints. In particular, an\napproximation-free adaptive-barrier control architecture is designed to ensure\nprescribed-time convergence of the tracking error to a prescribed bound while\nrejecting exogenous disturbances. In contrast to existing approaches that\nnecessitate continuous real-time control action, the proposed controller\ngenerates event-based updates through periodic evaluation of the triggering\ncondition. Additionally, we derive an upper bound on the monitoring period by\nanalysing the performance degradation of the filtered tracking error to\nfacilitate periodic evaluation of the event-triggered strategy. To this end, a\ntime-varying threshold function is considered in the triggering mechanism to\nreduce the number of triggers during the transient phase of system behaviour.\nNotably, the proposed design avoids Zeno behaviour and precludes the need for\ncontinuous monitoring of the triggering condition. A simulation and\nexperimental study is undertaken to demonstrate the efficacy of the proposed\ncontrol scheme."}
{"id": "2510.02791", "pdf": "https://arxiv.org/pdf/2510.02791", "abs": "https://arxiv.org/abs/2510.02791", "authors": ["Patrick Sandoz", "Antoine N. Andr√©", "Guillaume J. Laurent"], "title": "VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Pose estimation is still a challenge at the small scales. Few solutions exist\nto capture the 6 degrees of freedom of an object with nanometric and\nmicroradians resolutions over relatively large ranges. Over the years, we have\nproposed several fiducial marker and pattern designs to achieve reliable\nperformance for various microscopy applications. Centimeter ranges are possible\nusing pattern encoding methods, while nanometer resolutions can be achieved\nusing phase processing of the periodic frames. This paper presents VERNIER, an\nopen source phase processing software designed to provide fast and reliable\npose measurement based on pseudo-periodic patterns. Thanks to a phase-based\nlocal thresholding algorithm, the software has proven to be particularly robust\nto noise, defocus and occlusion. The successive steps of the phase processing\nare presented, as well as the different types of patterns that address\ndifferent application needs. The implementation procedure is illustrated with\nsynthetic and experimental images. Finally, guidelines are given for selecting\nthe appropriate pattern design and microscope magnification lenses as a\nfunction of the desired performance."}
{"id": "2510.03100", "pdf": "https://arxiv.org/pdf/2510.03100", "abs": "https://arxiv.org/abs/2510.03100", "authors": ["Tianhua Gao"], "title": "A Dimension-Decomposed Learning Framework for Online Disturbance Identification in Quadrotor SE(3) Control", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "Quadrotor stability under complex dynamic disturbances and model\nuncertainties poses significant challenges. One of them remains the\nunderfitting problem in high-dimensional features, which limits the\nidentification capability of current learning-based methods. To address this,\nwe introduce a new perspective: Dimension-Decomposed Learning (DiD-L), from\nwhich we develop the Sliced Adaptive-Neuro Mapping (SANM) approach for\ngeometric control. Specifically, the high-dimensional mapping for\nidentification is axially ``sliced\" into multiple low-dimensional submappings\n(``slices\"). In this way, the complex high-dimensional problem is decomposed\ninto a set of simple low-dimensional tasks addressed by shallow neural networks\nand adaptive laws. These neural networks and adaptive laws are updated online\nvia Lyapunov-based adaptation without any pre-training or persistent excitation\n(PE) condition. To enhance the interpretability of the proposed approach, we\nprove that the full-state closed-loop system exhibits arbitrarily close to\nexponential stability despite multi-dimensional time-varying disturbances and\nmodel uncertainties. This result is novel as it demonstrates exponential\nconvergence without requiring pre-training for unknown disturbances and\nspecific knowledge of the model."}
{"id": "2510.03104", "pdf": "https://arxiv.org/pdf/2510.03104", "abs": "https://arxiv.org/abs/2510.03104", "authors": ["Zhiting Mei", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Semantic distillation in radiance fields has spurred significant advances in\nopen-vocabulary robot policies, e.g., in manipulation and navigation, founded\non pretrained semantics from large vision models. While prior work has\ndemonstrated the effectiveness of visual-only semantic features (e.g., DINO and\nCLIP) in Gaussian Splatting and neural radiance fields, the potential benefit\nof geometry-grounding in distilled fields remains an open question. In\nprinciple, visual-geometry features seem very promising for spatial tasks such\nas pose estimation, prompting the question: Do geometry-grounded semantic\nfeatures offer an edge in distilled fields? Specifically, we ask three critical\nquestions: First, does spatial-grounding produce higher-fidelity geometry-aware\nsemantic features? We find that image features from geometry-grounded backbones\ncontain finer structural details compared to their counterparts. Secondly, does\ngeometry-grounding improve semantic object localization? We observe no\nsignificant difference in this task. Thirdly, does geometry-grounding enable\nhigher-accuracy radiance field inversion? Given the limitations of prior work\nand their lack of semantics integration, we propose a novel framework SPINE for\ninverting radiance fields without an initial guess, consisting of two core\ncomponents: coarse inversion using distilled semantics, and fine inversion\nusing photometric-based optimization. Surprisingly, we find that the pose\nestimation accuracy decreases with geometry-grounded features. Our results\nsuggest that visual-only features offer greater versatility for a broader range\nof downstream tasks, although geometry-grounded features contain more geometric\ndetail. Notably, our findings underscore the necessity of future research on\neffective strategies for geometry-grounding that augment the versatility and\nperformance of pretrained semantic features."}
{"id": "2510.03135", "pdf": "https://arxiv.org/pdf/2510.03135", "abs": "https://arxiv.org/abs/2510.03135", "authors": ["Gen Li", "Bo Zhao", "Jianfei Yang", "Laura Sevilla-Lara"], "title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://reagan1311.github.io/mask2iv", "summary": "Generating interaction-centric videos, such as those depicting humans or\nrobots interacting with objects, is crucial for embodied intelligence, as they\nprovide rich and diverse visual priors for robot learning, manipulation policy\ntraining, and affordance reasoning. However, existing methods often struggle to\nmodel such complex and dynamic interactions. While recent studies show that\nmasks can serve as effective control signals and enhance generation quality,\nobtaining dense and precise mask annotations remains a major challenge for\nreal-world use. To overcome this limitation, we introduce Mask2IV, a novel\nframework specifically designed for interaction-centric video generation. It\nadopts a decoupled two-stage pipeline that first predicts plausible motion\ntrajectories for both actor and object, then generates a video conditioned on\nthese trajectories. This design eliminates the need for dense mask inputs from\nusers while preserving the flexibility to manipulate the interaction process.\nFurthermore, Mask2IV supports versatile and intuitive control, allowing users\nto specify the target object of interaction and guide the motion trajectory\nthrough action descriptions or spatial position cues. To support systematic\ntraining and evaluation, we curate two benchmarks covering diverse action and\nobject categories across both human-object interaction and robotic manipulation\nscenarios. Extensive experiments demonstrate that our method achieves superior\nvisual realism and controllability compared to existing baselines."}
{"id": "2510.03153", "pdf": "https://arxiv.org/pdf/2510.03153", "abs": "https://arxiv.org/abs/2510.03153", "authors": ["Hima Jacob Leven Suprabha", "Laxmi Nag Laxminarayan Nagesh", "Ajith Nair", "Alvin Reuben Amal Selvaster", "Ayan Khan", "Raghuram Damarla", "Sanju Hannah Samuel", "Sreenithi Saravana Perumal", "Titouan Puech", "Venkataramireddy Marella", "Vishal Sonar", "Alessandro Suglia", "Oliver Lemon"], "title": "Improving Cooperation in Collaborative Embodied AI", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": "In proceedings of UKCI 2025", "summary": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations."}
