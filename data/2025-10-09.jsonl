{"id": "2510.06429", "pdf": "https://arxiv.org/pdf/2510.06429", "abs": "https://arxiv.org/abs/2510.06429", "authors": ["Linlin Mao", "Zeping Sui", "Michail Matthaiou", "Hongbin Li"], "title": "Distributed Detection and Bandwidth Allocation with Hybrid Quantized and Full-Precision Observations over Multiplicative Fading Channels", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, submitted to IEEE TVT", "summary": "A hybrid detector that fuses both quantized and full-precision observations\nis proposed for weak signal detection under additive and multiplicative\nGaussian noise. We first derive a locally most powerful test (LMPT)--based\nhybrid detector from the composite probability distribution of the compound\nobservations received by the fusion center, and then analyze its asymptotic\ndetection performance. Subsequently, we optimize the sensor-wise quantization\nthresholds to achieve near-optimal asymptotic performance at the local sensor\nlevel. Moreover, we propose a mixed-integer linear programming approach to\nsolve the optimization problem of transmission bandwidth allocation accounting\nfor bandwidth constraints and error-prone channels. Finally, simulation results\ndemonstrate the superiority of the proposed hybrid detector and the bandwidth\nallocation strategy, especially in challenging error-prone channel conditions."}
{"id": "2510.06476", "pdf": "https://arxiv.org/pdf/2510.06476", "abs": "https://arxiv.org/abs/2510.06476", "authors": ["Nishant Gadde", "Yoshua Alexander", "Sarvesh Parthasarthy", "Arman Allidina"], "title": "Optimized SVR Framework for Electric Load Forecasting", "categories": ["eess.SP"], "comment": "10 pages, 11 figures", "summary": "Load forecasting has always been a challenge for grid operators due to the\ngrowing complexity of power systems. The increase in extreme weather and the\nneed for energy from customers has led to load forecasting sometimes failing.\nThis research presents a Support Vector Regression (SVR) framework for electric\nload forecasting that outperforms the industry standard. The SVR model\ndemonstrates better accuracy across all evaluation metrics that are important\nfor power system operations. The model has a 54.2\\% reduction in Mean Squared\nError (31.91 vs. 69.63), a 33.5\\% improvement in Mean Absolute Error, and\nperformance benefits across other metrics. These improvements show significant\nbenefits when integrated with power forecasting tools and show that the\napproach provides an additional tool for accuracy checking for system planning\nand resource allocation in times of need for resource allocation in electric\npower systems."}
{"id": "2510.06654", "pdf": "https://arxiv.org/pdf/2510.06654", "abs": "https://arxiv.org/abs/2510.06654", "authors": ["Yan Yang", "Zhendong Li", "Jianwei Zhao", "Qingqing Wu", "Zhiqing Wei", "Wen Chen", "Weimin Jia"], "title": "Cooperative Multi-Static ISAC Networks: A Unified Design Framework for Active and Passive Sensing", "categories": ["eess.SP"], "comment": "13 pages, 12 figures", "summary": "Multi-static cooperative sensing emerges as a promising technology for\nadvancing integrated sensing and communication (ISAC), enhancing sensing\naccuracy and range. In this paper, we develop a unified design framework for\njoint active and passive sensing (JAPS). In particular, we consider a JAPSbased\ncooperative multi-static ISAC system for coexisting downlink (DL) and uplink\n(UL) communications. An optimization problem is formulated for maximizing the\nsum rate of both the DL and UL transmissions via jointly optimizing\nbeamforming, receive filters and power allocation, while guaranteeing the\nsensing requirements and transmission power constraints. However, the\nformulated problem is a non-convex optimization problem that is challenging to\nsolve directly due to the tight coupling among optimization variables. To\ntackle this complicated issue, we employ an efficient algorithm architecture\nleveraging alternating optimization (AO). Specifically, with the given receive\nfilters and transmission power for UL communication, the transmit beamforming\nsubproblem is addressed by successive convex approximation (SCA)-based and\npenalty-based algorithms. A fractional programming (FP)-based algorithm is\ndeveloped to tackle the receive filters and transmission power for UL\ncommunication optimization subproblem. Extensive numerical results validate the\nperformance improvement of our proposed JAPS scheme and demonstrate the\neffectiveness of our proposed algorithms."}
{"id": "2510.06709", "pdf": "https://arxiv.org/pdf/2510.06709", "abs": "https://arxiv.org/abs/2510.06709", "authors": ["Zhou Ni", "Sravan Reddy Chintareddy", "Peiyuan Guan", "Morteza Hashemi"], "title": "Personalized Federated Learning-Driven Beamforming Optimization for Integrated Sensing and Communication Systems", "categories": ["eess.SP"], "comment": "6 pages, 5 figures, accepted by IEEE Consumer Communications and\n  Networking Conference (CCNC) 2026", "summary": "In this paper, we propose an Expectation-Maximization-based (EM) Personalized\nFederated Learning (PFL) framework for multi-objective optimization (MOO) in\nIntegrated Sensing and Communication (ISAC) systems. In contrast to standard\nfederated learning (FL) methods that handle all clients uniformly, the proposed\napproach enables each base station (BS) to adaptively determine its aggregation\nweight with the EM algorithm. Specifically, an EM posterior is computed at each\nBS to quantify the relative suitability between the global and each local\nmodel, based on the losses of models on their respective datasets. The proposed\nmethod is especially valuable in scenarios with competing communication and\nsensing objectives, as it enables BSs to dynamically adapt to\napplication-specific trade-offs. To assess the effectiveness of the proposed\napproach, we conduct simulation studies under both objective-wise homogeneous\nand heterogeneous conditions. The results demonstrate that our approach\noutperforms existing PFL baselines, such as FedPer and pFedMe, achieving faster\nconvergence and better multi-objective performance."}
{"id": "2510.06339", "pdf": "https://arxiv.org/pdf/2510.06339", "abs": "https://arxiv.org/abs/2510.06339", "authors": ["Leiyao Cui", "Zihang Zhao", "Sirui Xie", "Wenhuan Zhang", "Zhi Han", "Yixin Zhu"], "title": "Vi-TacMan: Articulated Object Manipulation via Vision and Touch", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous manipulation of articulated objects remains a fundamental\nchallenge for robots in human environments. Vision-based methods can infer\nhidden kinematics but can yield imprecise estimates on unfamiliar objects.\nTactile approaches achieve robust control through contact feedback but require\naccurate initialization. This suggests a natural synergy: vision for global\nguidance, touch for local precision. Yet no framework systematically exploits\nthis complementarity for generalized articulated manipulation. Here we present\nVi-TacMan, which uses vision to propose grasps and coarse directions that seed\na tactile controller for precise execution. By incorporating surface normals as\ngeometric priors and modeling directions via von Mises-Fisher distributions,\nour approach achieves significant gains over baselines (all p<0.0001).\nCritically, manipulation succeeds without explicit kinematic models -- the\ntactile controller refines coarse visual estimates through real-time contact\nregulation. Tests on more than 50,000 simulated and diverse real-world objects\nconfirm robust cross-category generalization. This work establishes that coarse\nvisual cues suffice for reliable manipulation when coupled with tactile\nfeedback, offering a scalable paradigm for autonomous systems in unstructured\nenvironments."}
{"id": "2510.06768", "pdf": "https://arxiv.org/pdf/2510.06768", "abs": "https://arxiv.org/abs/2510.06768", "authors": ["Di Zhang", "Yinglei Yang", "Zhilong Liu", "Shaobo Jia", "Kyungchun Lee", "Zhirong Zhang"], "title": "Low Complexity Weight Flexible Decoding Schemes of Linear Block Code for 6G xURLLC", "categories": ["eess.SP"], "comment": null, "summary": "Low complexity error correction code is a key enabler for next generation\nultra-reliable low-latency communications (xURLLC) in six generation (6G).\nAgainst this background, this paper proposes a decoding scheme for linear block\ncode by leveraging certain interesting properties of dual codewords. It is\nfound that dual codewords with flexible weights can provide useful decoding\ninformation for the locations and magnitudes of error bits, which yielding\nhigher reliability performance. In addition, two decoding schemes are proposed,\nin which one directly utilizes intrinsic information for iterative decoding,\nand the other combines prior channel information with intrinsic information for\ndecoding. Both schemes are implemented using vector multiplication and\nreal-number comparisons, making them easy to implement in hardware. Simulation\nresults demonstrate the validness of our study."}
{"id": "2510.06351", "pdf": "https://arxiv.org/pdf/2510.06351", "abs": "https://arxiv.org/abs/2510.06351", "authors": ["Kaleb Ben Naveed", "Devansh R. Agrawal", "Dimitra Panagou"], "title": "A Formal gatekeeper Framework for Safe Dual Control with Active Exploration", "categories": ["cs.RO"], "comment": "Submitted to American Control Conference (ACC) 2026", "summary": "Planning safe trajectories under model uncertainty is a fundamental\nchallenge. Robust planning ensures safety by considering worst-case\nrealizations, yet ignores uncertainty reduction and leads to overly\nconservative behavior. Actively reducing uncertainty on-the-fly during a\nnominal mission defines the dual control problem. Most approaches address this\nby adding a weighted exploration term to the cost, tuned to trade off the\nnominal objective and uncertainty reduction, but without formal consideration\nof when exploration is beneficial. Moreover, safety is enforced in some methods\nbut not in others. We propose a framework that integrates robust planning with\nactive exploration under formal guarantees as follows: The key innovation and\ncontribution is that exploration is pursued only when it provides a verifiable\nimprovement without compromising safety. To achieve this, we utilize our\nearlier work on gatekeeper as an architecture for safety verification, and\nextend it so that it generates both safe and informative trajectories that\nreduce uncertainty and the cost of the mission, or keep it within a\nuser-defined budget. The methodology is evaluated via simulation case studies\non the online dual control of a quadrotor under parametric uncertainty."}
{"id": "2510.06861", "pdf": "https://arxiv.org/pdf/2510.06861", "abs": "https://arxiv.org/abs/2510.06861", "authors": ["Abidemi Orimogunje", "Kyeong-Ju Cha", "Hyunwoo Park", "Abdulahi A. Badrudeen", "Sunwoo Kim", "Dejan Vukobratovic"], "title": "Mobility-Aware Localization in mmWave Channel: Adaptive Hybrid Filtering Approach", "categories": ["eess.SP"], "comment": "6 pages", "summary": "Precise user localization and tracking enhances energy-efficient and\nultra-reliable low latency applications in the next generation wireless\nnetworks. In addition to computational complexity and data association\nchallenges with Kalman-filter localization techniques, estimation errors tend\nto grow as the user's trajectory speed increases. By exploiting mmWave signals\nfor joint sensing and communication, our approach dispenses with additional\nsensors adopted in most techniques while retaining high resolution spatial\ncues. We present a hybrid mobility-aware adaptive framework that selects the\nExtended Kalman filter at pedestrian speed and the Unscented Kalman filter at\nvehicular speed. The scheme mitigates data-association problem and estimation\nerrors through adaptive noise scaling, chi-square gating, Rauch-Tung-Striebel\nsmoothing. Evaluations using Absolute Trajectory Error, Relative Pose Error,\nNormalized Estimated Error Squared and Root Mean Square Error metrics\ndemonstrate roughly 30-60% improvement in their respective regimes indicating a\nclear advantage over existing approaches tailored to either indoor or static\nsettings."}
{"id": "2510.06357", "pdf": "https://arxiv.org/pdf/2510.06357", "abs": "https://arxiv.org/abs/2510.06357", "authors": ["Grayson Byrd", "Corban Rivera", "Bethany Kemp", "Meghan Booker", "Aurora Schmidt", "Celso M de Melo", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Constrained Natural Language Action Planning for Resilient Embodied Systems", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Replicating human-level intelligence in the execution of embodied tasks\nremains challenging due to the unconstrained nature of real-world environments.\nNovel use of large language models (LLMs) for task planning seeks to address\nthe previously intractable state/action space of complex planning tasks, but\nhallucinations limit their reliability, and thus, viability beyond a research\ncontext. Additionally, the prompt engineering required to achieve adequate\nsystem performance lacks transparency, and thus, repeatability. In contrast to\nLLM planning, symbolic planning methods offer strong reliability and\nrepeatability guarantees, but struggle to scale to the complexity and ambiguity\nof real-world tasks. We introduce a new robotic planning method that augments\nLLM planners with symbolic planning oversight to improve reliability and\nrepeatability, and provide a transparent approach to defining hard constraints\nwith considerably stronger clarity than traditional prompt engineering.\nImportantly, these augmentations preserve the reasoning capabilities of LLMs\nand retain impressive generalization in open-world environments. We demonstrate\nour approach in simulated and real-world environments. On the ALFWorld planning\nbenchmark, our approach outperforms current state-of-the-art methods, achieving\na near-perfect 99% success rate. Deployment of our method to a real-world\nquadruped robot resulted in 100% task success compared to 50% and 30% for pure\nLLM and symbolic planners across embodied pick and place tasks. Our approach\npresents an effective strategy to enhance the reliability, repeatability and\ntransparency of LLM-based robot planners while retaining their key strengths:\nflexibility and generalizability to complex real-world environments. We hope\nthat this work will contribute to the broad goal of building resilient embodied\nintelligent systems."}
{"id": "2510.06884", "pdf": "https://arxiv.org/pdf/2510.06884", "abs": "https://arxiv.org/abs/2510.06884", "authors": ["Rahul Gulia", "Amlan Ganguly", "Michael E. Kuhl", "Ehsan Rashedi", "Clark Hochgraf"], "title": "Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments", "categories": ["eess.SP", "cs.NI"], "comment": null, "summary": "Accurate and real-time prediction of wireless channel conditions,\nparticularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a\nfoundational requirement for enabling Ultra-Reliable Low-Latency Communication\n(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based\nor statistical models fail to cope with the spatio-temporal complexities\nintroduced by mobile obstacles and transient interference inherent to smart\nwarehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless\nInfrastructure for Smart Warehouse using VAE), a novel synergistic deep\nlearning architecture that functions as a lightweight 2D predictive digital\ntwin of the radio environment. Evo-WISVA integrates a memory-augmented\nVariational Autoencoder (VAE) featuring an Attention-driven Latent Memory\nModule (LMM) for robust, context-aware spatial feature extraction, with a\nConvolutional Long Short-Term Memory (ConvLSTM) network for precise temporal\nforecasting and sequential refinement. The entire pipeline is optimized\nend-to-end via a joint loss function, ensuring optimal feature alignment\nbetween the generative and predictive components. Rigorous experimental\nevaluation conducted on a high-fidelity ns-3-generated industrial warehouse\ndataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art\nbaselines, achieving up to a 47.6\\% reduction in average reconstruction error.\nCrucially, the model exhibits exceptional generalization capacity to unseen\nenvironments with vastly increased dynamic complexity (up to ten simultaneously\nmoving obstacles) while maintaining amortized computational efficiency\nessential for real-time deployment. Evo-WISVA establishes a foundational\ntechnology for proactive wireless resource management, enabling autonomous\noptimization and advancing the realization of predictive digital twins in\nindustrial communication networks."}
{"id": "2510.06481", "pdf": "https://arxiv.org/pdf/2510.06481", "abs": "https://arxiv.org/abs/2510.06481", "authors": ["Amirhossein Mollaei Khass", "Guangyi Liu", "Vivek Pandey", "Wen Jiang", "Boshu Lei", "Kostas Daniilidis", "Nader Motee"], "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Safe navigation in uncertain environments requires planning methods that\nintegrate risk aversion with active perception. In this work, we present a\nunified framework that refines a coarse reference path by constructing\ntail-sensitive risk maps from Average Value-at-Risk statistics on an\nonline-updated 3D Gaussian-splat Radiance Field. These maps enable the\ngeneration of locally safe and feasible trajectories. In parallel, we formulate\nNext-Best-View (NBV) selection as an optimization problem on the SE(3) pose\nmanifold, where Riemannian gradient descent maximizes an expected information\ngain objective to reduce uncertainty most critical for imminent motion. Our\napproach advances the state-of-the-art by coupling risk-averse path refinement\nwith NBV planning, while introducing scalable gradient decompositions that\nsupport efficient online updates in complex environments. We demonstrate the\neffectiveness of the proposed framework through extensive computational\nstudies."}
{"id": "2510.06936", "pdf": "https://arxiv.org/pdf/2510.06936", "abs": "https://arxiv.org/abs/2510.06936", "authors": ["Eren Berk Kama", "Murat Babek Salman", "Isaac Skog", "Emil Björnson"], "title": "Sensing Management for Pilot-Free Predictive Beamforming in Cell-Free Massive MIMO Systems", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "This paper introduces a sensing management method for integrated sensing and\ncommunications (ISAC) in cell-free massive multiple-input multiple-output\n(MIMO) systems. Conventional communication systems employ channel estimation\nprocedures that impose significant overhead during data transmission, consuming\nresources that could otherwise be utilized for data. To address this challenge,\nwe propose a state-based approach that leverages sensing capabilities to track\nthe user when there is no communication request. Upon receiving a communication\nrequest, predictive beamforming is employed based on the tracked user position,\nthereby reducing the need for channel estimation. Our framework incorporates an\nextended Kalman filter (EKF) based tracking algorithm with adaptive sensing\nmanagement to perform sensing operations only when necessary to maintain high\ntracking accuracy. The simulation results demonstrate that our proposed sensing\nmanagement approach provides uniform downlink communication rates that are\nhigher than with existing methods by achieving overhead-free predictive\nbeamforming."}
{"id": "2510.06492", "pdf": "https://arxiv.org/pdf/2510.06492", "abs": "https://arxiv.org/abs/2510.06492", "authors": ["Matthew Kim", "Kensuke Nakamura", "Andrea Bajcsy"], "title": "What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?", "categories": ["cs.RO"], "comment": "8 tables 6 figures", "summary": "Safe control techniques, such as Hamilton-Jacobi reachability, provide\nprincipled methods for synthesizing safety-preserving robot policies but\ntypically assume hand-designed state spaces and full observability. Recent work\nhas relaxed these assumptions via latent-space safe control, where state\nrepresentations and dynamics are learned jointly through world models that\nreconstruct future high-dimensional observations (e.g., RGB images) from\ncurrent observations and actions. This enables safety constraints that are\ndifficult to specify analytically (e.g., spilling) to be framed as\nclassification problems in latent space, allowing controllers to operate\ndirectly from raw observations. However, these methods assume that\nsafety-critical features are observable in the learned latent state. We ask:\nwhen are latent state spaces sufficient for safe control? To study this, we\nexamine temperature-based failures, comparable to overheating in cooking or\nmanufacturing tasks, and find that RGB-only observations can produce myopic\nsafety behaviors, e.g., avoiding seeing failure states rather than preventing\nfailure itself. To predict such behaviors, we introduce a mutual\ninformation-based measure that identifies when observations fail to capture\nsafety-relevant features. Finally, we propose a multimodal-supervised training\nstrategy that shapes the latent state with additional sensory inputs during\ntraining, but requires no extra modalities at deployment, and validate our\napproach in simulation and on hardware with a Franka Research 3 manipulator\npreventing a pot of wax from overheating."}
{"id": "2510.06937", "pdf": "https://arxiv.org/pdf/2510.06937", "abs": "https://arxiv.org/abs/2510.06937", "authors": ["He Huang", "Zilong Liu", "Zeping Sui", "Wei Huang", "Md. Noor-A-Rahim", "Haishi Wang", "Zhiheng Hu"], "title": "Optimal Real-time Communication in 6G Ultra-Massive V2X Mobile Networks", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "6 pages, 5 figures, accepted by IEEE VTC-fall 2025", "summary": "This paper introduces a novel cooperative vehicular communication algorithm\ntailored for future 6G ultra-massive vehicle-to-everything (V2X) networks\nleveraging integrated space-air-ground communication systems. Specifically, we\naddress the challenge of real-time information exchange among rapidly moving\nvehicles. We demonstrate the existence of an upper bound on channel capacity\ngiven a fixed number of relays, and propose a low-complexity relay selection\nheuristic algorithm. Simulation results verify that our proposed algorithm\nachieves superior channel capacities compared to existing cooperative vehicular\ncommunication approaches."}
{"id": "2510.06518", "pdf": "https://arxiv.org/pdf/2510.06518", "abs": "https://arxiv.org/abs/2510.06518", "authors": ["Malakhi Hopkins", "Varun Murali", "Vijay Kumar", "Camillo J Taylor"], "title": "Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": "8 pages, 8 figures, submitted to ICRA 2026", "summary": "Autonomous aerial robots are increasingly being deployed in real-world\nscenarios, where transparent obstacles present significant challenges to\nreliable navigation and mapping. These materials pose a unique problem for\ntraditional perception systems because they lack discernible features and can\ncause conventional depth sensors to fail, leading to inaccurate maps and\npotential collisions. To ensure safe navigation, robots must be able to\naccurately detect and map these transparent obstacles. Existing methods often\nrely on large, expensive sensors or algorithms that impose high computational\nburdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.\nIn this work, we propose a novel and computationally efficient framework for\ndetecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our\nmethod fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor\nwith a custom, lightweight 2D convolution model. This specialized approach\naccurately detects specular reflections and propagates their depth into\ncorresponding empty regions of the depth map, effectively rendering transparent\nobstacles visible. The entire pipeline operates in real-time, utilizing only a\nsmall fraction of a CPU core on an embedded processor. We validate our system\nthrough a series of experiments in both controlled and real-world environments,\ndemonstrating the utility of our method through experiments where the robot\nmaps indoor environments containing glass. Our work is, to our knowledge, the\nfirst of its kind to demonstrate a real-time, onboard transparent obstacle\nmapping system on a low-SWaP quadrotor using only the CPU."}
{"id": "2510.06946", "pdf": "https://arxiv.org/pdf/2510.06946", "abs": "https://arxiv.org/abs/2510.06946", "authors": ["Ruifeng Gao", "Hao Zhang", "Jue Wang", "Ye Li", "Yingdong Hu", "Qiuming Zhu", "Shu Sun", "Meixia Tao"], "title": "Maritime Communication in Evaporation Duct Environment with Ship Trajectory Optimization", "categories": ["eess.SP"], "comment": null, "summary": "In maritime wireless networks, the evaporation duct effect has been known as\na preferable condition for long-range transmissions. However, how to\neffectively utilize the duct effect for efficient communication design is still\nopen for investigation. In this paper, we consider a typical scenario of\nship-to-shore data transmission, where a ship collects data from multiple\noceanographic buoys, sails from one to another, and transmits the collected\ndata back to a terrestrial base station during its voyage. A novel framework,\nwhich exploits priori information of the channel gain map in the presence of\nevaporation duct, is proposed to minimize the data transmission time and the\nsailing time by optimizing the ship's trajectory. To this end, a\nmulti-objective optimization problem is formulated and is further solved by a\ndynamic population PSO-integrated NSGA-II algorithm. Through simulations, it is\ndemonstrated that, compared to the benchmark scheme which ignores useful\ninformation of the evaporation duct, the proposed scheme can effectively reduce\nboth the data transmission time and the sailing time."}
{"id": "2510.06546", "pdf": "https://arxiv.org/pdf/2510.06546", "abs": "https://arxiv.org/abs/2510.06546", "authors": ["Mohammad Nazeri", "Sheldon Mei", "Jeffrey Watchorn", "Alex Zhang", "Erin Ng", "Tao Wen", "Abhijoy Mandal", "Kevin Golovin", "Alan Aspuru-Guzik", "Frank Gu"], "title": "RAISE: A self-driving laboratory for interfacial property formulation discovery", "categories": ["cs.RO"], "comment": "Mohammad Nazeri, Sheldon Mei, and Jeffrey Watchorn contributed\n  equally to this work. *Corresponding author: Frank Gu (f.gu@utoronto.ca)", "summary": "Surface wettability is a critical design parameter for biomedical devices,\ncoatings, and textiles. Contact angle measurements quantify liquid-surface\ninteractions, which depend strongly on liquid formulation. Herein, we present\nthe Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,\nself-driving laboratory that is capable of linking liquid formulation\noptimization with surface wettability assessment. RAISE comprises a full\nexperimental orchestrator with the ability of mixing liquid ingredients to\ncreate varying formulation cocktails, transferring droplets of prepared\nformulations to a high-throughput stage, and using a pick-and-place camera tool\nfor automated droplet image capture. The system also includes an automated\nimage processing pipeline to measure contact angles. This closed loop\nexperiment orchestrator is integrated with a Bayesian Optimization (BO) client,\nwhich enables iterative exploration of new formulations based on previous\ncontact angle measurements to meet user-defined objectives. The system operates\nin a high-throughput manner and can achieve a measurement rate of approximately\n1 contact angle measurement per minute. Here we demonstrate RAISE can be used\nto explore surfactant wettability and how surfactant combinations create\ntunable formulations that compensate for purity-related variations.\nFurthermore, multi-objective BO demonstrates how precise and optimal\nformulations can be reached based on application-specific goals. The\noptimization is guided by a desirability score, which prioritizes formulations\nthat are within target contact angle ranges, minimize surfactant usage and\nreduce cost. This work demonstrates the capabilities of RAISE to autonomously\nlink liquid formulations to contact angle measurements in a closed-loop system,\nusing multi-objective BO to efficiently identify optimal formulations aligned\nwith researcher-defined criteria."}
{"id": "2510.07120", "pdf": "https://arxiv.org/pdf/2510.07120", "abs": "https://arxiv.org/abs/2510.07120", "authors": ["Yinong Chen", "Wenchi Cheng", "Jingqing Wang", "Xiao Zheng", "Jiangzhou Wang"], "title": "Towards Reliable Emergency Wireless Communications over SAGINs: A Composite Fading and QoS-Centric Perspective", "categories": ["eess.SP"], "comment": "13 pages", "summary": "In emergency wireless communications (EWC) scenarios, ensuring reliable,\nflexible, and high-rate transmission while simultaneously maintaining seamless\ncoverage and rapid response capabilities presents a critical technical\nchallenge. To this end, satellite-aerial-ground integrated network (SAGIN) has\nemerged as a promising solution due to its comprehensive three-dimensional\ncoverage and capability to meet stringent, multi-faceted quality-of-service\n(QoS) requirements. Nevertheless, most existing studies either neglected the\ninherent characteristics of the complex channel conditions due to the terrain\nchanges or analyzed the performance in the absence of QoS constraints,\nresulting in a mismatch between theoretical analysis and practical performance.\nTo remedy such deficiencies, in this paper we establish a performance modeling\nframework for SAGIN employing the Fisher-Snedecor $\\mathcal{F}$ composite\nfading model to characterize the air-ground link. In specific, the proposed\n$\\mathcal{F}$ composite fading channel is adopted to accurately describe both\nmultipath fading and shadowing in harsh ground environments. The exact\ndistribution of end-to-end signal-to-noise (SNR) statistics for space-air and\nair-ground links is developed, enabling theoretical analysis of cascaded\nchannels with fixed-gain amplify-and-forward (AF) and decode-and-forward (DF)\nrelaying protocols, respectively. Furthermore, asymptotic expressions of the\nderived results are provided to offer concise representations and demonstrate\nclose alignment with theoretical predictions in the high-SNR regime. Finally,\nthe insightful closed-form and asymptotic expressions of effective capacity\nwith QoS provisioning, outage probability, and $\\epsilon$-outage capacity are\ninvestigated, respectively, followed by both field measurements and Monte Carlo\nsimulations to verify the effectiveness."}
{"id": "2510.06566", "pdf": "https://arxiv.org/pdf/2510.06566", "abs": "https://arxiv.org/abs/2510.06566", "authors": ["Vincent Lam", "Robin Chhabra"], "title": "Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "The objective of this study is to develop a model-free workspace trajectory\nplanner for space manipulators using a Twin Delayed Deep Deterministic Policy\nGradient (TD3) agent to enable safe and reliable debris capture. A local\ncontrol strategy with singularity avoidance and manipulability enhancement is\nemployed to ensure stable execution. The manipulator must simultaneously track\na capture point on a non-cooperative target, avoid self-collisions, and prevent\nunintended contact with the target. To address these challenges, we propose a\ncurriculum-based multi-critic network where one critic emphasizes accurate\ntracking and the other enforces collision avoidance. A prioritized experience\nreplay buffer is also used to accelerate convergence and improve policy\nrobustness. The framework is evaluated on a simulated seven-degree-of-freedom\nKUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating\nsafe and adaptive trajectory generation for debris removal missions."}
{"id": "2510.07199", "pdf": "https://arxiv.org/pdf/2510.07199", "abs": "https://arxiv.org/abs/2510.07199", "authors": ["Shirin Shoushtari", "Edward P. Chandler", "Ulugbek S. Kamilov"], "title": "Moments Matter: Posterior Recovery in Poisson Denoising via Log-Networks", "categories": ["eess.SP"], "comment": null, "summary": "Poisson denoising plays a central role in photon-limited imaging applications\nsuch as microscopy, astronomy, and medical imaging. It is common to train deep\nlearning models for denoising using the mean-squared error (MSE) loss, which\ncorresponds to computing the posterior mean $\\mathbb{E}[x \\mid y]$. When the\nnoise is Gaussian, Tweedie's formula enables approximation of the posterior\ndistribution through its higher-order moments. However, this connection no\nlonger holds for Poisson denoising: while $ \\mathbb{E}[x \\mid y] $ still\nminimizes MSE, it fails to capture posterior uncertainty. We propose a new\nstrategy for Poisson denoising based on training a log-network. Instead of\npredicting the posterior mean $ \\mathbb{E}[x \\mid y] $, the log-network is\ntrained to learn $\\mathbb{E}[\\log x \\mid y]$, leveraging the logarithm as a\nconvenient parameterization for the Poisson distribution. We provide a\ntheoretical proof that the proposed log-network enables recovery of\nhigher-order posterior moments and thus supports posterior approximation.\nExperiments on simulated data show that our method matches the denoising\nperformance of standard MMSE models while providing access to the posterior."}
{"id": "2510.06633", "pdf": "https://arxiv.org/pdf/2510.06633", "abs": "https://arxiv.org/abs/2510.06633", "authors": ["Kruthika Gangaraju", "Tanmayi Inaparthy", "Jiaqi Yang", "Yihao Zheng", "Fengpei Yuan"], "title": "Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care", "categories": ["cs.RO"], "comment": null, "summary": "People living with dementia (PLWDs) face progressively declining abilities in\nmedication management-from simple forgetfulness to complete task breakdown-yet\nmost assistive technologies fail to adapt to these changing needs. This\none-size-fits-all approach undermines autonomy, accelerates dependence, and\nincreases caregiver burden. Occupational therapy principles emphasize matching\nassistance levels to individual capabilities: minimal reminders for those who\nmerely forget, spatial guidance for those who misplace items, and comprehensive\nmultimodal support for those requiring step-by-step instruction. However,\nexisting robotic systems lack this adaptive, graduated response framework\nessential for maintaining PLWD independence. We present an adaptive multimodal\nrobotic framework using the Pepper robot that dynamically adjusts assistance\nbased on real-time assessment of user needs. Our system implements a\nhierarchical intervention model progressing from (1) simple verbal reminders,\nto (2) verbal + gestural cues, to (3) full multimodal guidance combining\nphysical navigation to medication locations with step-by-step verbal and\ngestural instructions. Powered by LLM-driven interaction strategies and\nmultimodal sensing, the system continuously evaluates task states to provide\njust-enough assistance-preserving autonomy while ensuring medication adherence.\nWe conducted a preliminary study with healthy adults and dementia care\nstakeholders in a controlled lab setting, evaluating the system's usability,\ncomprehensibility, and appropriateness of adaptive feedback mechanisms. This\nwork contributes: (1) a theoretically grounded adaptive assistance framework\ntranslating occupational therapy principles into HRI design, (2) a multimodal\nrobotic implementation that preserves PLWD dignity through graduated support,\nand (3) empirical insights into stakeholder perceptions of adaptive robotic\ncare."}
{"id": "2510.06355", "pdf": "https://arxiv.org/pdf/2510.06355", "abs": "https://arxiv.org/abs/2510.06355", "authors": ["Kürşat Tekbıyık", "Güneş Karabulut Kurt", "Antoine Lesage-Landry"], "title": "PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) communications demand accurate yet\ninterpretable air-to-ground (A2G) channel models that can adapt to\nnonstationary propagation environments. While deterministic models offer\ninterpretability and deep learning (DL) models provide accuracy, both\napproaches suffer from either rigidity or a lack of explainability. To bridge\nthis gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)\nthat embeds physical principles (e.g., free-space path loss, two-ray\nreflections) into the learning process. Unlike physics-informed neural networks\n(PINNs), PIKAN is more flexible for applying physical information because it\nintroduces them as flexible inductive biases. Thus, it enables a more flexible\ntraining process. Experiments on UAV A2G measurement data show that PIKAN\nachieves comparable accuracy to DL models while providing symbolic and\nexplainable expressions aligned with propagation laws. Remarkably, PIKAN\nachieves this performance with only 232 parameters, making it up to 37 times\nlighter than multilayer perceptron (MLP) baselines with thousands of\nparameters, without sacrificing correlation with measurements and also\nproviding symbolic expressions. These results highlight PIKAN as an efficient,\ninterpretable, and scalable solution for UAV channel modelling in beyond-5G and\n6G networks."}
{"id": "2510.06710", "pdf": "https://arxiv.org/pdf/2510.06710", "abs": "https://arxiv.org/abs/2510.06710", "authors": ["Hongzhi Zang", "Mingjie Wei", "Si Xu", "Yongji Wu", "Zhen Guo", "Yuanqing Wang", "Hao Lin", "Liangzhi Shi", "Yuqing Xie", "Zhexuan Xu", "Zhihao Liu", "Kang Chen", "Wenhao Tang", "Quanlu Zhang", "Weinan Zhang", "Chao Yu", "Yu Wang"], "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training", "categories": ["cs.RO"], "comment": "This is the technical report of the RLinf Team, focusing on the\n  algorithm side. For the system-level design, please refer to\n  arXiv:2509.15965. The open-sourced code link: https://github.com/RLinf/RLinf", "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence."}
{"id": "2510.06632", "pdf": "https://arxiv.org/pdf/2510.06632", "abs": "https://arxiv.org/abs/2510.06632", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Non-Negative Matrix Factorization (NMF) is an unsupervised learning method\noffering low-rank representations across various domains such as audio\nprocessing, biomedical signal analysis, and image recognition. The\nincorporation of $\\alpha$-divergence in NMF formulations enhances flexibility\nin optimization, yet extending these methods to multi-layer architectures\npresents challenges in ensuring convergence. To address this, we introduce a\nnovel approach inspired by the Boltzmann probability of the energy barriers in\nchemical reactions to theoretically perform convergence analysis. We introduce\na novel method, called Chem-NMF, with a bounding factor which stabilizes\nconvergence. To our knowledge, this is the first study to apply a physical\nchemistry perspective to rigorously analyze the convergence behaviour of the\nNMF algorithm. We start from mathematically proven asymptotic convergence\nresults and then show how they apply to real data. Experimental results\ndemonstrate that the proposed algorithm improves clustering accuracy by 5.6%\n$\\pm$ 2.7% on biomedical signals and 11.1% $\\pm$ 7.2% on face images (mean\n$\\pm$ std)."}
{"id": "2510.06717", "pdf": "https://arxiv.org/pdf/2510.06717", "abs": "https://arxiv.org/abs/2510.06717", "authors": ["Yuanfei Lin", "Sebastian Illing", "Matthias Althoff"], "title": "SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis", "categories": ["cs.RO"], "comment": "@2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Large language models have been widely applied to knowledge-driven\ndecision-making for automated vehicles due to their strong generalization and\nreasoning capabilities. However, the safety of the resulting decisions cannot\nbe ensured due to possible hallucinations and the lack of integrated vehicle\ndynamics. To address this issue, we propose SanDRA, the first safe\nlarge-language-model-based decision making framework for automated vehicles\nusing reachability analysis. Our approach starts with a comprehensive\ndescription of the driving scenario to prompt large language models to generate\nand rank feasible driving actions. These actions are translated into temporal\nlogic formulas that incorporate formalized traffic rules, and are subsequently\nintegrated into reachability analysis to eliminate unsafe actions. We validate\nour approach in both open-loop and closed-loop driving environments using\noff-the-shelf and finetuned large language models, showing that it can provide\nprovably safe and, where possible, legally compliant driving actions, even\nunder high-density traffic conditions. To ensure transparency and facilitate\nfuture research, all code and experimental setups are publicly available at\ngithub.com/CommonRoad/SanDRA."}
{"id": "2510.06734", "pdf": "https://arxiv.org/pdf/2510.06734", "abs": "https://arxiv.org/abs/2510.06734", "authors": ["Fabian Göttsch", "Max Franke", "Arash Pourdamghani", "Giuseppe Caire", "Stefan Schmid"], "title": "Optimizing Fronthaul Quantization for Flexible User Load in Cell-Free Massive MIMO", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "We investigate the physical layer (PHY) spectral efficiency and fronthaul\nnetwork load of a scalable user-centric cell-free massive MIMO system. Each\nuser-centric cluster processor responsible for cluster-level signal processing\nis located at one of multiple decentralized units (DUs). Thus, the radio units\nin the cluster must exchange data with the corresponding DU over the fronthaul.\nBecause the fronthaul links have limited capacity, this data must be quantized\nbefore it is sent over the fronthaul. We consider a routed fronthaul network,\nwhere the cluster processor placement and fronthaul traffic routing are jointly\noptimized with a mixed-integer linear program. For different numbers of users\nin the network, we investigate the effect of fronthaul quantization rates, a\nsystem parameter computed based on rate-distortion theory. Our results show\nthat with optimized quantization rates, the fronthaul load is quite stable for\na wide range of user loads without significant PHY performance loss. This\ndemonstrates that the cell-free massive MIMO PHY and fronthaul network are\nresilient to varying user densities."}
{"id": "2510.06754", "pdf": "https://arxiv.org/pdf/2510.06754", "abs": "https://arxiv.org/abs/2510.06754", "authors": ["Christian Maurer", "Snehal Jauhri", "Sophie Lueth", "Georgia Chalvatzaki"], "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene", "categories": ["cs.RO", "cs.CV"], "comment": "Project website: https://sites.google.com/view/uniffield", "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is\ncrucial for successful execution of robotic tasks, especially in unstructured\nand complex environments. Additionally, to make robust decisions, it is\nnecessary for the robot to evaluate the reliability of perceived information.\nWhile recent advances in 3D neural feature fields have enabled robots to\nleverage features from pretrained foundation models for tasks such as\nlanguage-guided manipulation and navigation, existing methods suffer from two\ncritical limitations: (i) they are typically scene-specific, and (ii) they lack\nthe ability to model uncertainty in their predictions. We present UniFField, a\nunified uncertainty-aware neural feature field that combines visual, semantic,\nand geometric features in a single generalizable representation while also\npredicting uncertainty in each modality. Our approach, which can be applied\nzero shot to any new environment, incrementally integrates RGB-D images into\nour voxel-based feature representation as the robot explores the scene,\nsimultaneously updating uncertainty estimation. We evaluate our uncertainty\nestimations to accurately describe the model prediction errors in scene\nreconstruction and semantic feature prediction. Furthermore, we successfully\nleverage our feature predictions and their respective uncertainty for an active\nobject search task using a mobile manipulator robot, demonstrating the\ncapability for robust decision-making."}
{"id": "2510.06836", "pdf": "https://arxiv.org/pdf/2510.06836", "abs": "https://arxiv.org/abs/2510.06836", "authors": ["Jesús Bautista", "Héctor García de Marina"], "title": "Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms", "categories": ["cs.RO"], "comment": "7 pages, 3 figures. Submitted for presentation at the IFAC World\n  Congress 2026", "summary": "This paper presents a geometric control framework on the Lie group SO(3) for\n3D source-seeking by robots with first-order attitude dynamics and constant\ntranslational speed. By working directly on SO(3), the approach avoids\nEuler-angle singularities and quaternion ambiguities, providing a unique,\nintrinsic representation of orientation. We design a proportional feed-forward\ncontroller that ensures exponential alignment of each agent to an estimated\nascending direction toward a 3D scalar field source. The controller adapts to\nbounded unknown variations and preserves well-posed swarm formations. Numerical\nsimulations demonstrate the effectiveness of the method, with all code provided\nopen source for reproducibility."}
{"id": "2510.07027", "pdf": "https://arxiv.org/pdf/2510.07027", "abs": "https://arxiv.org/abs/2510.07027", "authors": ["Saravana Prashanth Murali Babu", "Aida Parvaresh", "Ahmad Rafsanjani"], "title": "Tailoring materials into kirigami robots", "categories": ["cs.RO", "cond-mat.soft"], "comment": null, "summary": "Kirigami, the traditional paper-cutting craft, holds immense potential for\nrevolutionizing robotics by providing multifunctional, lightweight, and\nadaptable solutions. Kirigami structures, characterized by their\nbending-dominated deformation, offer resilience to tensile forces and\nfacilitate shape morphing under small actuation forces. Kirigami components\nsuch as actuators, sensors, batteries, controllers, and body structures can be\ntailored to specific robotic applications by optimizing cut patterns. Actuators\nbased on kirigami principles exhibit complex motions programmable through\nvarious energy sources, while kirigami sensors bridge the gap between\nelectrical conductivity and compliance. Kirigami-integrated batteries enable\nenergy storage directly within robot structures, enhancing flexibility and\ncompactness. Kirigami-controlled mechanisms mimic mechanical computations,\nenabling advanced functionalities such as shape morphing and memory functions.\nApplications of kirigami-enabled robots include grasping, locomotion, and\nwearables, showcasing their adaptability to diverse environments and tasks.\nDespite promising opportunities, challenges remain in the design of cut\npatterns for a given function and streamlining fabrication techniques."}
{"id": "2510.07028", "pdf": "https://arxiv.org/pdf/2510.07028", "abs": "https://arxiv.org/abs/2510.07028", "authors": ["Sicong Pan", "Xuying Huang", "Maren Bennewitz"], "title": "Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction", "categories": ["cs.RO"], "comment": "Accepted to the Active Perception Workshop at IROS 2025", "summary": "Periodic 3D reconstruction is essential for crop monitoring, but costly when\neach cycle restarts from scratch, wasting resources and ignoring information\nfrom previous captures. We propose temporal-prior-guided view planning for\nperiodic plant reconstruction, in which a previously reconstructed model of the\nsame plant is non-rigidly aligned to a new partial observation to form an\napproximation of the current geometry. To accommodate plant growth, we inflate\nthis approximation and solve a set covering optimization problem to compute a\nminimal set of views. We integrated this method into a complete pipeline that\nacquires one additional next-best view before registration for robustness and\nthen plans a globally shortest path to connect the planned set of views and\noutputs the best view sequence. Experiments on maize and tomato under\nhemisphere and sphere view spaces show that our system maintains or improves\nsurface coverage while requiring fewer views and comparable movement cost\ncompared to state-of-the-art baselines."}
{"id": "2510.07030", "pdf": "https://arxiv.org/pdf/2510.07030", "abs": "https://arxiv.org/abs/2510.07030", "authors": ["Abhinav Kumar", "Fan Yang", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Multi-fingered hands are emerging as powerful platforms for performing fine\nmanipulation tasks, including tool use. However, environmental perturbations or\nexecution errors can impede task performance, motivating the use of recovery\nbehaviors that enable normal task execution to resume. In this work, we take\nadvantage of recent advances in diffusion models to construct a framework that\nautonomously identifies when recovery is necessary and optimizes contact-rich\ntrajectories to recover. We use a diffusion model trained on the task to\nestimate when states are not conducive to task execution, framed as an\nout-of-distribution detection problem. We then use diffusion sampling to\nproject these states in-distribution and use trajectory optimization to plan\ncontact-rich recovery trajectories. We also propose a novel diffusion-based\napproach that distills this process to efficiently diffuse the full\nparameterization, including constraints, goal state, and initialization, of the\nrecovery trajectory optimization problem, saving time during online execution.\nWe compare our method to a reinforcement learning baseline and other methods\nthat do not explicitly plan contact interactions, including on a hardware\nscrewdriver-turning task where we show that recovering using our method\nimproves task performance by 96% and that ours is the only method evaluated\nthat can attempt recovery without causing catastrophic task failure. Videos can\nbe found at https://dtourrecovery.github.io/."}
{"id": "2510.07067", "pdf": "https://arxiv.org/pdf/2510.07067", "abs": "https://arxiv.org/abs/2510.07067", "authors": ["Daria Pugacheva", "Andrey Moskalenko", "Denis Shepelev", "Andrey Kuznetsov", "Vlad Shakhuro", "Elena Tutubalina"], "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models", "categories": ["cs.RO"], "comment": null, "summary": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions."}
{"id": "2510.07077", "pdf": "https://arxiv.org/pdf/2510.07077", "abs": "https://arxiv.org/abs/2510.07077", "authors": ["Kento Kawaharazuka", "Jihoon Oh", "Jun Yamada", "Ingmar Posner", "Yuke Zhu"], "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE Access, website: https://vla-survey.github.io", "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io ."}
{"id": "2510.07094", "pdf": "https://arxiv.org/pdf/2510.07094", "abs": "https://arxiv.org/abs/2510.07094", "authors": ["David Rytz", "Kim Tien Ly", "Ioannis Havoutis"], "title": "Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies", "categories": ["cs.RO"], "comment": null, "summary": "This work focuses on sampling strategies of configuration variations for\ngenerating robust universal locomotion policies for quadrupedal robots. We\ninvestigate the effects of sampling physical robot parameters and joint\nproportional-derivative gains to enable training a single reinforcement\nlearning policy that generalizes to multiple parameter configurations. Three\nfundamental joint gain sampling strategies are compared: parameter sampling\nwith (1) linear and polynomial function mappings of mass-to-gains, (2)\nperformance-based adaptive filtering, and (3) uniform random sampling. We\nimprove the robustness of the policy by biasing the configurations using\nnominal priors and reference models. All training was conducted on RaiSim,\ntested in simulation on a range of diverse quadrupeds, and zero-shot deployed\nonto hardware using the ANYmal quadruped robot. Compared to multiple baseline\nimplementations, our results demonstrate the need for significant joint\ncontroller gains randomization for robust closing of the sim-to-real gap."}
{"id": "2510.07133", "pdf": "https://arxiv.org/pdf/2510.07133", "abs": "https://arxiv.org/abs/2510.07133", "authors": ["Tony Zhang", "Burak Kantarci", "Umair Siddique"], "title": "A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Ensuring the safety of self-driving cars remains a major challenge due to the\ncomplexity and unpredictability of real-world driving environments. Traditional\ntesting methods face significant limitations, such as the oracle problem, which\nmakes it difficult to determine whether a system's behavior is correct, and the\ninability to cover the full range of scenarios an autonomous vehicle may\nencounter. In this paper, we introduce a digital twin-driven metamorphic\ntesting framework that addresses these challenges by creating a virtual replica\nof the self-driving system and its operating environment. By combining digital\ntwin technology with AI-based image generative models such as Stable Diffusion,\nour approach enables the systematic generation of realistic and diverse driving\nscenes. This includes variations in weather, road topology, and environmental\nfeatures, all while maintaining the core semantics of the original scenario.\nThe digital twin provides a synchronized simulation environment where changes\ncan be tested in a controlled and repeatable manner. Within this environment,\nwe define three metamorphic relations inspired by real-world traffic rules and\nvehicle behavior. We validate our framework in the Udacity self-driving\nsimulator and demonstrate that it significantly enhances test coverage and\neffectiveness. Our method achieves the highest true positive rate (0.719), F1\nscore (0.689), and precision (0.662) compared to baseline approaches. This\npaper highlights the value of integrating digital twins with AI-powered\nscenario generation to create a scalable, automated, and high-fidelity testing\nsolution for autonomous vehicle safety."}
{"id": "2510.07134", "pdf": "https://arxiv.org/pdf/2510.07134", "abs": "https://arxiv.org/abs/2510.07134", "authors": ["Jiahang Liu", "Yunpeng Qi", "Jiazhao Zhang", "Minghan Li", "Shaoan Wang", "Kui Wu", "Hanjing Ye", "Hong Zhang", "Zhibo Chen", "Fangwei Zhong", "Zhizheng Zhang", "He Wang"], "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://pku-epic.github.io/TrackVLA-plus-plus-Web/", "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios."}
{"id": "2510.07152", "pdf": "https://arxiv.org/pdf/2510.07152", "abs": "https://arxiv.org/abs/2510.07152", "authors": ["Jingkai Sun", "Gang Han", "Pihai Sun", "Wen Zhao", "Jiahang Cao", "Jiaxu Wang", "Yijie Guo", "Qiang Zhang"], "title": "DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "Recent advancements in legged robot perceptive locomotion have shown\npromising progress. However, terrain-aware humanoid locomotion remains largely\nconstrained to two paradigms: depth image-based end-to-end learning and\nelevation map-based methods. The former suffers from limited training\nefficiency and a significant sim-to-real gap in depth perception, while the\nlatter depends heavily on multiple vision sensors and localization systems,\nresulting in latency and reduced robustness. To overcome these challenges, we\npropose a novel framework that tightly integrates three key components: (1)\nTerrain-Aware Locomotion Policy with a Blind Backbone, which leverages\npre-trained elevation map-based perception to guide reinforcement learning with\nminimal visual input; (2) Multi-Modality Cross-Attention Transformer, which\nreconstructs structured terrain representations from noisy depth images; (3)\nRealistic Depth Images Synthetic Method, which employs self-occlusion-aware ray\ncasting and noise-aware modeling to synthesize realistic depth observations,\nachieving over 30\\% reduction in terrain reconstruction error. This combination\nenables efficient policy training with limited data and hardware resources,\nwhile preserving critical terrain features essential for generalization. We\nvalidate our framework on a full-sized humanoid robot, demonstrating agile and\nadaptive locomotion across diverse and challenging terrains."}
{"id": "2510.07160", "pdf": "https://arxiv.org/pdf/2510.07160", "abs": "https://arxiv.org/abs/2510.07160", "authors": ["Fengze Xie", "Xiaozhou Fan", "Jacob Schuster", "Yisong Yue", "Morteza Gharib"], "title": "A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft", "categories": ["cs.RO"], "comment": null, "summary": "Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but\nlack low-speed agility due to highly coupled dynamics. We present an end-to-end\nsensing-to-control pipeline that combines bio-inspired hardware,\nphysics-informed dynamics learning, and convex control allocation. Measuring\nairflow on a small airframe is difficult because near-body aerodynamics,\npropeller slipstream, control-surface actuation, and ambient gusts distort\npressure signals. Inspired by the narwhal's protruding tusk, we mount in-house\nmulti-hole probes far upstream and complement them with sparse, carefully\nplaced wing pressure sensors for local flow measurement. A data-driven\ncalibration maps probe pressures to airspeed and flow angles. We then learn a\ncontrol-affine dynamics model using the estimated airspeed/angles and sparse\nsensors. A soft left/right symmetry regularizer improves identifiability under\npartial observability and limits confounding between wing pressures and\nflaperon inputs. Desired wrenches (forces and moments) are realized by a\nregularized least-squares allocator that yields smooth, trimmed actuation.\nWind-tunnel studies across a wide operating range show that adding wing\npressures reduces force-estimation error by 25-30%, the proposed model degrades\nless under distribution shift (about 12% versus 44% for an unstructured\nbaseline), and force tracking improves with smoother inputs, including a 27%\nreduction in normal-force RMSE versus a plain affine model and 34% versus an\nunstructured baseline."}
{"id": "2510.07181", "pdf": "https://arxiv.org/pdf/2510.07181", "abs": "https://arxiv.org/abs/2510.07181", "authors": ["Yi Han", "Cheng Chi", "Enshen Zhou", "Shanyu Rong", "Jingkun An", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks."}
{"id": "2510.07197", "pdf": "https://arxiv.org/pdf/2510.07197", "abs": "https://arxiv.org/abs/2510.07197", "authors": ["Aman Singh", "Deepak Kapa", "Suryank Joshi", "Shishir Kolathaya"], "title": "COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators", "categories": ["cs.RO"], "comment": "8 pages, 9 Figures, 2 tables, first two authors contributed equally", "summary": "The optimal design of robotic actuators is a critical area of research, yet\nlimited attention has been given to optimizing gearbox parameters and\nautomating actuator CAD. This paper introduces COMPAct: Computational\nOptimization and Automated Modular Design of Planetary Actuators, a framework\nthat systematically identifies optimal gearbox parameters for a given motor\nacross four gearbox types, single-stage planetary gearbox (SSPG), compound\nplanetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage\nplanetary gearbox (DSPG). The framework minimizes mass and actuator width while\nmaximizing efficiency, and further automates actuator CAD generation to enable\ndirect 3D printing without manual redesign. Using this framework, optimal\ngearbox designs are explored over a wide range of gear ratios, providing\ninsights into the suitability of different gearbox types across various gear\nratio ranges. In addition, the framework is used to generate CAD models of all\nfour gearbox types with varying gear ratios and motors. Two actuator types are\nfabricated and experimentally evaluated through power efficiency, no-load\nbacklash, and transmission stiffness tests. Experimental results indicate that\nthe SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load\nbacklash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the\nCPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of\n201.6 Nm/rad. Code available at:\nhttps://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:\nhttps://youtu.be/99zOKgxsDho"}
{"id": "2510.07210", "pdf": "https://arxiv.org/pdf/2510.07210", "abs": "https://arxiv.org/abs/2510.07210", "authors": ["Donald Pfaffmann", "Matthias Klusch", "Marcel Steinmetz"], "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present a novel hybrid learning-assisted planning method, named HyPlan,\nfor solving the collision-free navigation problem for self-driving cars in\npartially observable traffic environments. HyPlan combines methods for\nmulti-agent behavior prediction, deep reinforcement learning with proximal\npolicy optimization and approximated online POMDP planning with heuristic\nconfidence-based vertical pruning to reduce its execution time without\ncompromising safety of driving. Our experimental performance analysis on the\nCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed\nthat HyPlan may navigate safer than selected relevant baselines and perform\nsignificantly faster than considered alternative online POMDP planners."}
{"id": "2510.06394", "pdf": "https://arxiv.org/pdf/2510.06394", "abs": "https://arxiv.org/abs/2510.06394", "authors": ["Praveen Kumar Ranjan", "Abhinav Sinha", "Yongcan Cao"], "title": "Three-dimensional Integrated Guidance and Control for Leader-Follower Flexible Formation of Fixed Wing UAVs", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.DS"], "comment": null, "summary": "This paper presents a nonlinear integrated guidance and control (IGC)\napproach for flexible leader-follower formation flight of fixed-wing unmanned\naerial vehicles (UAVs) while accounting for high-fidelity aerodynamics and\nthrust dynamics. Unlike conventional leader-follower schemes that fix the\nfollower's position relative to the leader, the follower is steered to maintain\nrange and bearing angles (which is the angle between its velocity vector and\nits line-of-sight (LOS) with respect to the leader) arbitrarily close to the\nprescribed values, enabling the follower to maintain formation on a\nhemispherical region behind the leader. The proposed IGC framework directly\nmaps leader-follower relative range dynamics to throttle commands, and the\nfollower's velocity orientation relative to the LOS to aerodynamic control\nsurface deflections. This enables synergism between guidance and control\nsubsystems. The control design uses a dynamic surface control-based\nbackstepping approach to achieve convergence to the desired formation set,\nwhere Lyapunov barrier functions are incorporated to ensure the follower's\nbearing angle is constrained within specified bounds. Rigorous stability\nanalysis guarantees uniform ultimate boundedness of all error states and strict\nconstraint satisfaction in the presence of aerodynamic nonlinearities. The\nproposed flexible formation scheme allows the follower to have an orientation\nmismatch relative to the leader to execute anticipatory reconfiguration by\ntransitioning between the relative positions in the admissible formation set\nwhen the leader aggressively maneuvers. The proposed IGC law relies only on\nrelative information and onboard sensors without the information about the\nleader's maneuver, making it suitable for GPS-denied or non-cooperative\nscenarios. Finally, we present simulation results to vindicate the\neffectiveness and robustness of our approach."}
{"id": "2510.06470", "pdf": "https://arxiv.org/pdf/2510.06470", "abs": "https://arxiv.org/abs/2510.06470", "authors": ["Abdülbaki Şanlan", "Fatih Erol", "Murad Abu-Khalaf", "Emre Koyuncu"], "title": "Terrain-Aided Navigation Using a Point Cloud Measurement Sensor", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "We investigate the use of a point cloud measurement in terrain-aided\nnavigation. Our goal is to aid an inertial navigation system, by exploring ways\nto generate a useful measurement innovation error for effective nonlinear state\nestimation. We compare two such measurement models that involve the scanning of\na digital terrain elevation model: a) one that is based on typical ray-casting\nfrom a given pose, that returns the predicted point cloud measurement from that\npose, and b) another computationally less intensive one that does not require\nraycasting and we refer to herein as a sliding grid. Besides requiring a pose,\nit requires the pattern of the point cloud measurement itself and returns a\npredicted point cloud measurement. We further investigate the observability\nproperties of the altitude for both measurement models. As a baseline, we\ncompare the use of a point cloud measurement performance to the use of a radar\naltimeter and show the gains in accuracy. We conclude by showing that a point\ncloud measurement outperforms the use of a radar altimeter, and the point cloud\nmeasurement model to use depends on the computational resources"}
{"id": "2510.06582", "pdf": "https://arxiv.org/pdf/2510.06582", "abs": "https://arxiv.org/abs/2510.06582", "authors": ["Fei Zhang", "Rob Chancia", "Josie Clapp", "Amirhossein Hassanzadeh", "Dimah Dera", "Richard MacKenzie", "Jan van Aardt"], "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point\nclouds is limited by costly manual annotation. We propose a semi-automated,\nuncertainty-aware pipeline that integrates spherical projection, feature\nenrichment, ensemble learning, and targeted annotation to reduce labeling\neffort, while sustaining high accuracy. Our approach projects 3D points to a 2D\nspherical grid, enriches pixels with multi-source features, and trains an\nensemble of segmentation networks to produce pseudo-labels and uncertainty\nmaps, the latter guiding annotation of ambiguous regions. The 2D outputs are\nback-projected to 3D, yielding densely annotated point clouds supported by a\nthree-tier visualization suite (2D feature maps, 3D colorized point clouds, and\ncompact virtual spheres) for rapid triage and reviewer guidance. Using this\npipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove\nforests. We further evaluate data efficiency and feature importance to address\ntwo key questions: (1) how much annotated data are needed and (2) which\nfeatures matter most. Results show that performance saturates after ~12\nannotated scans, geometric features contribute the most, and compact\nnine-channel stacks capture nearly all discriminative power, with the mean\nIntersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm\nthe generalization of our feature-enrichment strategy through cross-dataset\ntests on ForestSemantic and Semantic3D.\n  Our contributions include: (i) a robust, uncertainty-aware TLS annotation\npipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)\nempirical guidance on data efficiency and feature importance, thus enabling\nscalable, high-quality segmentation of TLS point clouds for ecological\nmonitoring and beyond. The dataset and processing scripts are publicly\navailable at https://fz-rit.github.io/through-the-lidars-eye/."}
{"id": "2510.06876", "pdf": "https://arxiv.org/pdf/2510.06876", "abs": "https://arxiv.org/abs/2510.06876", "authors": ["Samir Abou Haidar", "Alexandre Chariot", "Mehdi Darouich", "Cyril Joly", "Jean-Emmanuel Deschaud"], "title": "HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "summary": "LiDAR semantic segmentation is crucial for autonomous vehicles and mobile\nrobots, requiring high accuracy and real-time processing, especially on\nresource-constrained embedded systems. Previous state-of-the-art methods often\nface a trade-off between accuracy and speed. Point-based and sparse\nconvolution-based methods are accurate but slow due to the complexity of\nneighbor searching and 3D convolutions. Projection-based methods are faster but\nlose critical geometric information during the 2D projection. Additionally,\nmany recent methods rely on test-time augmentation (TTA) to improve\nperformance, which further slows the inference. Moreover, the pre-processing\nphase across all methods increases execution time and is demanding on embedded\nplatforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR\nsemantic segmentation network. We first propose a novel pre-processing\nmethodology that significantly reduces computational overhead. Then, we design\nthe Conv-SE-NeXt feature extraction block to efficiently capture\nrepresentations without deep layer stacking per network stage. We also employ a\nmulti-scale range-point fusion backbone that leverages information at multiple\nabstraction levels to preserve essential geometric details, thereby enhancing\naccuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that\nHARP-NeXt achieves a superior speed-accuracy trade-off compared to all\nstate-of-the-art methods, and, without relying on ensemble models or TTA, is\ncomparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is\navailable at https://github.com/SamirAbouHaidar/HARP-NeXt"}
{"id": "2510.06913", "pdf": "https://arxiv.org/pdf/2510.06913", "abs": "https://arxiv.org/abs/2510.06913", "authors": ["Ke Guo", "Haochen Liu", "Xiaojun Wu", "Chen Lv"], "title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Realistic traffic simulation is critical for the development of autonomous\ndriving systems and urban mobility planning, yet existing imitation learning\napproaches often fail to model realistic traffic behaviors. Behavior cloning\nsuffers from covariate shift, while Generative Adversarial Imitation Learning\n(GAIL) is notoriously unstable in multi-agent settings. We identify a key\nsource of this instability: irrelevant interaction misguidance, where a\ndiscriminator penalizes an ego vehicle's realistic behavior due to unrealistic\ninteractions among its neighbors. To address this, we propose Decomposed\nMulti-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map\nand ego-neighbor components, filtering out misleading neighbor: neighbor and\nneighbor: map interactions. We further introduce a social PPO objective that\naugments ego rewards with distance-weighted neighborhood rewards, encouraging\noverall realism across agents. Integrated into a lightweight SMART-based\nbackbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim\nAgents 2025 benchmark."}
{"id": "2510.07053", "pdf": "https://arxiv.org/pdf/2510.07053", "abs": "https://arxiv.org/abs/2510.07053", "authors": ["Manshika Charvi Bissessur", "Efimia Panagiotaki", "Daniele De Martini"], "title": "Introspection in Learned Semantic Scene Graph Localisation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "I.2.10; I.2.9; I.4.8; I.5.2; I.5.1"], "comment": "IEEE IROS 2025 Workshop FAST", "summary": "This work investigates how semantics influence localisation performance and\nrobustness in a learned self-supervised, contrastive semantic localisation\nframework. After training a localisation network on both original and perturbed\nmaps, we conduct a thorough post-hoc introspection analysis to probe whether\nthe model filters environmental noise and prioritises distinctive landmarks\nover routine clutter. We validate various interpretability methods and present\na comparative reliability analysis. Integrated gradients and Attention Weights\nconsistently emerge as the most reliable probes of learned behaviour. A\nsemantic class ablation further reveals an implicit weighting in which frequent\nobjects are often down-weighted. Overall, the results indicate that the model\nlearns noise-robust, semantically salient relations about place definition,\nthereby enabling explainable registration under challenging visual and\nstructural variations."}
{"id": "2510.07063", "pdf": "https://arxiv.org/pdf/2510.07063", "abs": "https://arxiv.org/abs/2510.07063", "authors": ["Francesca Cocchella", "Nilay Roy Choudhury", "Eric Chen", "Patrícia Alves-Oliveira"], "title": "Artists' Views on Robotics Involvement in Painting Productions", "categories": ["cs.HC", "cs.RO"], "comment": "10 pages, 9 figures, submitted to RAM special issue: Arts and\n  Robotics", "summary": "As robotic technologies evolve, their potential in artistic creation becomes\nan increasingly relevant topic of inquiry. This study explores how professional\nabstract artists perceive and experience co-creative interactions with an\nautonomous painting robotic arm. Eight artists engaged in six painting sessions\n-- three with a human partner, followed by three with the robot -- and\nsubsequently participated in semi-structured interviews analyzed through\nreflexive thematic analysis. Human-human interactions were described as\nintuitive, dialogic, and emotionally engaging, whereas human-robot sessions\nfelt more playful and reflective, offering greater autonomy and prompting for\nnovel strategies to overcome the system's limitations. This work offers one of\nthe first empirical investigations into artists' lived experiences with a\nrobot, highlighting the value of long-term engagement and a multidisciplinary\napproach to human-robot co-creation."}
{"id": "2510.07092", "pdf": "https://arxiv.org/pdf/2510.07092", "abs": "https://arxiv.org/abs/2510.07092", "authors": ["Riccardo Mereu", "Aidan Scannell", "Yuxin Hou", "Yi Zhao", "Aditya Jitta", "Antonio Dominguez", "Luigi Acerbi", "Amos Storkey", "Paul Chang"], "title": "Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "6 pages, 3 figures, 1X world model challenge technical report", "summary": "World models are a powerful paradigm in AI and robotics, enabling agents to\nreason about the future by predicting visual observations or compact latent\nstates. The 1X World Model Challenge introduces an open-source benchmark of\nreal-world humanoid interaction, with two complementary tracks: sampling,\nfocused on forecasting future image frames, and compression, focused on\npredicting future discrete latent codes. For the sampling track, we adapt the\nvideo generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned\nfuture frame prediction. We condition the video generation on robot states\nusing AdaLN-Zero, and further post-train the model using LoRA. For the\ncompression track, we train a Spatio-Temporal Transformer model from scratch.\nOur models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386\nin the compression task, securing 1st place in both challenges."}
{"id": "2510.07151", "pdf": "https://arxiv.org/pdf/2510.07151", "abs": "https://arxiv.org/abs/2510.07151", "authors": ["Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "22 pages, 7 figures", "summary": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability."}
{"id": "2510.07313", "pdf": "https://arxiv.org/pdf/2510.07313", "abs": "https://arxiv.org/abs/2510.07313", "authors": ["Zezhong Qian", "Xiaowei Chi", "Yuming Li", "Shizun Wang", "Zhiyuan Qin", "Xiaozhu Ju", "Sirui Han", "Shanghang Zhang"], "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap."}
