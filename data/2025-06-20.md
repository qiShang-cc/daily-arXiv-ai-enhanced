<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder](https://arxiv.org/abs/2506.14906)
*Robert Czupryniak,Abhishek Chakraborty,Andrew N. Jordan,John C. Howell*

Main category: eess.SP

TL;DR: 使用机器学习方法（去噪自编码器）在雷达探测中实现超分辨率测距，信号设计对性能起关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究如何在亚波长范围内通过机器学习提高雷达测距的分辨率。

Method: 采用去噪自编码器，通过带宽受限的波形训练模型，分析瓶颈层与散射体间距的关联。

Result: Bessel信号表现最佳，三角形波次之，sinc信号最差，信号设计对分辨率影响显著。

Conclusion: 机器学习结合优化信号设计可显著提升雷达测距的分辨能力。

Abstract: We apply machine learning methods to demonstrate range superresolution in remote sensing radar detection. Specifically, we implement a denoising autoencoder to estimate the distance between two equal intensity scatterers in the subwavelength regime. The machine learning models are trained on waveforms subject to a bandlimit constraint such that ranges much smaller than the inverse bandlimit are optimized in their precision. The autoencoder achieves effective dimensionality reduction, with the bottleneck layer exhibiting a strong and consistent correlation with the true scatterer separation. We confirm reproducibility across different training sessions and network initializations by analyzing the scaled encoder outputs and their robustness to noise. We investigate the behavior of the bottleneck layer for the following types of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse, and a theoretically near-optimal pulse created from a spherical Bessel function basis. The Bessel signal performs best, followed by the triangle wave, with the sinc signal performing worst, highlighting the crucial role of signal design in the success of machine-learning-based range resolution.

</details>


### [2] [Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization](https://arxiv.org/abs/2506.14985)
*Kuranage Roche Rayan Ranasinghe,Hyeon Seok Rou,Iván Alexander Morales Sandoval,Giuseppe Thadeu Freitas de Abreu,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 该论文提出了一种新的基于超表面的双色散（MPDD）信道模型，该模型集成了任意数量的RIS，并在发射和接收端引入SIM，用于优化DD环境中的波形性能。


<details>
  <summary>Details</summary>
Motivation: 扩展双色散（DD）框架到MIMO系统，尤其是在RIS和SIM增强的环境中，是一个尚未解决的挑战。

Method: 引入了MPDD信道模型，集成RIS和SIM，并应用于OFDM、OTFS和AFDM等波形优化。

Result: 通过应用展示，模型显示出在SIM辅助的无线系统中优化波形性能的潜力。

Conclusion: MPDD模型为动态环境中无线通信系统的信道建模和波形优化提供了新思路。

Abstract: The doubly-dispersive (DD) channel structure has played a pivotal role in wireless communications, particularly in high-mobility scenarios and integrated sensing and communications (ISAC), due to its ability to capture the key fading effects experienced by a transmitted signal as it propagates through a dynamic medium. However, extending the DD framework to multiple-input multiple-output (MIMO) systems, especially in environments artificially enhanced by reconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces (SIM), remains a challenging open problem. In this chapter, a novel metasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary number of RISs, while also incorporating SIM at both the transmitter and receiver is introduced. Next, the application of this model to some key waveforms optimized for DD environments -- namely orthogonal frequency division multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine frequency division multiplexing (AFDM) -- is discussed. Finally, the programmability of the proposed model is highlighted through an illustrative application, demonstrating its potential for enhancing waveform performance in SIM-assisted wireless systems.

</details>


### [3] [Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks](https://arxiv.org/abs/2506.14992)
*Zhihao Tao,Athina P. Petropulu*

Main category: eess.SP

TL;DR: 提出了一种基于生成式AI的时间调制智能反射面（TM-IRS）设计方法，用于OFDM系统中的定向调制，以提升多用户场景下的安全性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有TM-IRS设计方法仅适用于单用户场景，无法满足多用户需求。本文提出一种新方法以支持多用户场景的安全通信。

Method: 采用生成式AI（GFlowNets）将TM-IRS参数选择建模为确定性MDP，通过概率采样最大化授权方向的总和速率。

Result: 实验表明，该方法显著提升了TM-IRS-OFDM系统的安全性，且GFlowNets在训练仅需极少量配置的情况下即可收敛。

Conclusion: 提出的方法在多用户场景中表现出高效性和安全性，为TM-IRS设计提供了新思路。

Abstract: We propose a novel directional modulation (DM) design for OFDM transmitters aided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS is configured to preserve the integrity of transmitted signals toward multiple legitimate users while scrambling the signal in all other directions. Existing TM-IRS design methods typically target a single user direction and follow predefined rule-based procedures, making them unsuitable for multi-user scenarios. Here, we propose a generative AI-based approach to design good sets of TM-IRS parameters out of a set of all possible quantized ranges of parameters. The design objective is to maximize the sum rate across the authorized directions. We model the TM-IRS parameter selection as a deterministic Markov decision process (MDP), where each terminal state corresponds to a specific configuration of TM-IRS parameters. GFlowNets are employed to learn a stochastic policy that samples TM-IRS parameter sets with probability proportional to their associated sum rate reward. Experimental results demonstrate that the proposed method effectively enhances the security of the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size of the TM-IRS configuration space, the GFlowNet is able to converge after training on fewer than 0.000001% of all possible configurations, demonstrating remarkable efficiency compared to exhaustive combinatorial search. Implementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to facilitate reproducibility.

</details>


### [4] [Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks](https://arxiv.org/abs/2506.15125)
*Linlin Wang,Wei Wang,Dezhao Wang,Shanwen Wang*

Main category: eess.SP

TL;DR: 提出了一种基于混合深度学习网络的信号去噪算法（HDLNet），用于光纤分布式声学传感系统（DAS），并在智能交通系统（ITS）中实现高效信号处理。


<details>
  <summary>Details</summary>
Motivation: 随着光纤DAS系统在智能交通系统中的广泛应用，需要高效的信号处理和分析方法以推动其普及。

Method: 提出了一种结合自编码器（DAE）和长短期记忆网络（LSTM）的自监督混合深度学习网络（HDLNet），无需标注数据，并引入了逐行匹配算法用于车辆检测与跟踪。

Result: 在自建的真实高速公路隧道数据集上的实验表明，HDLNet的去噪性能优于空间域DAE。

Conclusion: 该混合网络能够有效实现信号去噪和特征提取的完整处理，为DAS系统在ITS中的应用提供了有力支持。

Abstract: With the applicability of optical fiber-based distributed acoustic sensing (DAS) systems, effective signal processing and analysis approaches are needed to promote its popularization in the field of intelligent transportation systems (ITS). This paper presents a signal denoising algorithm using a hybrid deep-learning network (HDLNet). Without annotated data and time-consuming labeling, this self-supervised network runs in parallel, combining an autoencoder for denoising (DAE) and a long short-term memory (LSTM) for sequential processing. Additionally, a line-by-line matching algorithm for vehicle detection and tracking is introduced, thus realizing the complete processing of fiber signal denoising and feature extraction. Experiments were carried out on a self-established real highway tunnel dataset, showing that our proposed hybrid network yields more satisfactory denoising performance than Spatial-domain DAE.

</details>


### [5] [Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead](https://arxiv.org/abs/2506.15136)
*Kehui Li,Binggui Zhou,Jiajia Guo,Feifei Gao,Guanghua Yang,Shaodan Ma*

Main category: eess.SP

TL;DR: 提出一种基于视觉和位置协同的OOB模态（OMS）方案，用于多用户毫米波通信中的波束预测和基站选择，实现高效多基站协调。


<details>
  <summary>Details</summary>
Motivation: 解决多基站系统中因协调开销大导致的波束跟踪和基站选择效率低的问题。

Method: 结合视觉和位置两种OOB模态，通过空间对齐和时间序列跟踪用户，并设计BEM-GBPN网络预测波束增益和最优波束。

Result: 仿真显示方案实现91%最优传输速率，零导频开销，显著提升多基站协调效率。

Conclusion: OMS方案有效减少协调开销，提升多基站系统和移动用户性能。

Abstract: Multi-user millimeter-wave communication relies on narrow beams and dense cell deployments to ensure reliable connectivity. However, tracking optimal beams for multiple mobile users across multiple base stations (BSs) results in significant signaling overhead. Recent works have explored the capability of out-of-band (OOB) modalities in obtaining spatial characteristics of wireless channels and reducing pilot overhead in single-BS single-user/multi-user systems. However, applying OOB modalities for multi-BS selection towards dense cell deployments leads to high coordination overhead, i.e, excessive computing overhead and high latency in data exchange. How to leverage OOB modalities to eliminate pilot overhead and achieve efficient multi-BS coordination in multi-BS systems remains largely unexplored. In this paper, we propose a novel OOB modality synergy (OMS) based mobility management scheme to realize multi-user beam prediction and proactive BS selection by synergizing two OOB modalities, i.e., vision and location. Specifically, mobile users are initially identified via spatial alignment of visual sensing and location feedback, and then tracked according to the temporal correlation in image sequence. Subsequently, a binary encoding map based gain and beam prediction network (BEM-GBPN) is designed to predict beamforming gains and optimal beams for mobile users at each BS, such that a central unit can control the BSs to perform user handoff and beam switching. Simulation results indicate that the proposed OMS-based mobility management scheme enhances beam prediction and BS selection accuracy and enables users to achieve 91% transmission rates of the optimal with zero pilot overhead and significantly improve multi-BS coordination efficiency compared to existing methods.

</details>


### [6] [Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation](https://arxiv.org/abs/2506.15148)
*Yuxuan Xia,Ángel F. García-Fernández,Johan Karlsson,Yu Ge,Lennart Svensson,Ting Yuan*

Main category: eess.SP

TL;DR: 本文提出了一种广义化的轨迹最优子模式分配（GOSPA）度量方法，用于评估提供轨迹不确定性估计的多目标跟踪算法，结合存在和状态估计不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前的多目标跟踪算法需要更全面的评估标准，以考虑轨迹估计中的不确定性，尤其是在存在和状态估计方面。

Method: 通过扩展概率GOSPA度量，将其构建为一个多维分配问题，并使用线性规划松弛方法在多时间内计算。

Result: 新度量方法保留了TGOSPA的可解释性，并通过模拟研究验证了其有效性。

Conclusion: 提出的度量方法能够更全面地评估多目标跟踪算法，特别是在考虑不确定性时。

Abstract: This paper presents a generalization of the trajectory general optimal sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking algorithms that provide trajectory estimates with track-level uncertainties. This metric builds on the recently introduced probabilistic GOSPA metric to account for both the existence and state estimation uncertainties of individual object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a multidimensional assignment problem, and its linear programming relaxation--also a valid metric--is computable in polynomial time. Additionally, this metric retains the interpretability of TGOSPA, and we show that its decomposition yields intuitive costs terms associated to expected localization error and existence probability mismatch error for properly detected objects, expected missed and false detection error, and track switch error. The effectiveness of the proposed metric is demonstrated through a simulation study.

</details>


### [7] [Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data](https://arxiv.org/abs/2506.15235)
*Taewon Kang,Seunghyeon Park,Pyo-Woong Son,Jiwon Seo*

Main category: eess.SP

TL;DR: 研究提出了一种结合加权线性回归和各向异性广义回归神经网络的模型（WLR-AGRNN），用于提高eLoran/GPS时间差（TD）估计精度，实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）易受信号干扰，需要补充的定位、导航和时间（PNT）系统。eLoran作为补充方案，但其信号传播延迟（ASF）导致误差，需通过气象因素改善TD估计。

Method: 使用时间间隔计数器测量GPS和eLoran的TD，分析其与11种气象因素的相关性。提出WLR-AGRNN模型，结合线性回归和非线性神经网络，并引入地形高程加权气象数据。

Result: 基于四个月数据的实验表明，WLR-AGRNN模型的TD估计精度优于其他方法。

Conclusion: WLR-AGRNN模型能有效提高eLoran/GPS TD估计精度，为eLoran提供与GPS相当的时间同步能力。

Abstract: The vulnerabilities of global navigation satellite systems (GNSS) to signal interference have increased the demand for complementary positioning, navigation, and timing (PNT) systems. To address this, South Korea has decided to deploy an enhanced long-range navigation (eLoran) system as a complementary PNT solution. Similar to GNSS, eLoran provides highly accurate timing information, which is essential for applications such as telecommunications, financial systems, and power distribution. However, the primary sources of error for GNSS and eLoran differ. For eLoran, the main source of error is signal propagation delay over land, known as the additional secondary factor (ASF). This delay, influenced by ground conductivity and weather conditions along the signal path, is challenging to predict and mitigate. In this paper, we measure the time difference (TD) between GPS and eLoran using a time interval counter and analyze the correlations between eLoran/GPS TD and eleven meteorological factors. Accurate estimation of eLoran/GPS TD could enable eLoran to achieve timing accuracy comparable to that of GPS. We propose two estimation models for eLoran/GPS TD and compare their performance with existing TD estimation methods. The proposed WLR-AGRNN model captures the linear relationships between meteorological factors and eLoran/GPS TD using weighted linear regression (WLR) and models nonlinear relationships between outputs from expert networks through an anisotropic general regression neural network (AGRNN). The model incorporates terrain elevation to appropriately weight meteorological data, as elevation influences signal propagation delay. Experimental results based on four months of data demonstrate that the WLR-AGRNN model outperforms other models, highlighting its effectiveness in improving eLoran/GPS TD estimation accuracy.

</details>


### [8] [Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access](https://arxiv.org/abs/2506.15273)
*Anup Mishra,Čedomir Stefanović,Xiuqiang Xu,Petar Popovski,Israel Leyva-Mayorga*

Main category: eess.SP

TL;DR: 论文研究了无线网络中延迟受限的物联网设备与宽带用户共存的资源分配问题，提出了基于双Q学习的强化学习方法优化物联网传输策略，结果表明该方法显著改善了物联网用户的延迟性能。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要灵活的异构服务资源共享，物联网设备与宽带用户的共存是一个关键挑战，需要高效的资源管理方法。

Method: 采用无授权接入框架，结合正交RAN切片或共享接入；为物联网用户设计了基于双Q学习的强化学习算法，优化其重复传输策略。

Result: 提出的RL策略显著提升了物联网用户的延迟性能，同时保持了宽带用户的吞吐量和能效；低流量下共享接入更节能，高流量下切片接入更优。

Conclusion: 通过强化学习方法可以在不同流量条件下优化资源分配，为未来无线网络的异构服务共享提供有效解决方案。

Abstract: Flexible and efficient wireless resource sharing across heterogeneous services is a key objective for future wireless networks. In this context, we investigate the performance of a system where latency-constrained internet-of-things (IoT) devices coexist with a broadband user. The base station adopts a grant-free access framework to manage resource allocation, either through orthogonal radio access network (RAN) slicing or by allowing shared access between services. For the IoT users, we propose a reinforcement learning (RL) approach based on double Q-Learning (QL) to optimise their repetition-based transmission strategy, allowing them to adapt to varying levels of interference and meet a predefined latency target. We evaluate the system's performance in terms of the cumulative distribution function of IoT users' latency, as well as the broadband user's throughput and energy efficiency (EE). Our results show that the proposed RL-based access policies significantly enhance the latency performance of IoT users in both RAN Slicing and RAN Sharing scenarios, while preserving desirable broadband throughput and EE. Furthermore, the proposed policies enable RAN Sharing to be energy-efficient at low IoT traffic levels, and RAN Slicing to be favourable under high IoT traffic.

</details>


### [9] [Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry](https://arxiv.org/abs/2506.15338)
*Islam M. Tanash,Ayush Kumar Dwivedi,Taneli Riihonen*

Main category: eess.SP

TL;DR: 论文研究了一种由可重构智能表面（RIS）支持的高空平台（HAP）网络，通过建模和分析实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究高空平台网络在城市场景中的连通性和数据卸载能力，并解决由建筑物遮挡和其他HAP干扰带来的挑战。

Method: 采用泊松点过程建模HAP和RIS的不规则分布，布尔矩形模型模拟建筑物遮挡；提出基于广义Beta素数分布的统计信道表征方法，推导覆盖概率和遍历容量解析表达式。

Result: 通过蒙特卡洛仿真验证了理论分析的准确性，系统性能显著提升，且建筑物遮挡有助于抑制其他可见HAP的干扰。

Conclusion: 所提系统能有效增强城市场景的连通性并为数据卸载提供支持，展示了RIS在高空平台网络中的潜力。

Abstract: This paper studies a high-altitude platform (HAP) network supported by reconfigurable intelligent surfaces (RISs). The practical irregular placement of HAPs and RISs is modeled using homogeneous Poisson point processes, while buildings that cause blockages in urban areas are modeled as a Boolean scheme of rectangles. We introduce a novel approach to characterize the statistical channel based on generalized Beta prime distribution. Analytical expressions for coverage probability and ergodic capacity in an interference-limited system are derived and validated through Monte Carlo simulations. The findings show notable performance improvements and reveal the impact of various system parameters, including blockages effect which contribute in mitigating interference from the other visible HAPs. This proposed system could enhance connectivity and enable effective data offloading in urban environments.

</details>


### [10] [Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array](https://arxiv.org/abs/2506.15463)
*Shweta Pal,Arun Kumar,Monika Agrawal*

Main category: eess.SP

TL;DR: 本文研究了信号量化对一维一阶差分麦克风阵列（DMA）性能的影响，发现量化位数主要影响零陷深度（ND），而波束图形状、方向性因子（DF）和前向与后向比（FBR）保持不变。


<details>
  <summary>Details</summary>
Motivation: 量化是数据采集中关键的一环，但现有研究未探讨其对一维一阶差分麦克风阵列性能的影响。

Method: 通过建立一维一阶DMA的量化波束输出表达式，分析量化对波束图、DF、FBR和ND的影响。

Result: 量化位数仅影响ND，且其随量化位数增加而改善，而其他性能指标保持不变。ND在频率上独立，但随零陷靠近主波束方向而降低。

Conclusion: 量化对DMA性能的影响主要集中在ND上，量化位数的增加能提升干扰抑制能力，但零陷位置靠近主波束方向会降低ND。

Abstract: In practical systems, recorded analog signals must be digitized for processing, introducing quantization as a critical aspect of data acquisition. While prior studies have examined quantization effects in various signal processing contexts, its impact on differential microphone arrays (DMAs), particularly in one-dimensional (1D) first-order configurations, remains unexplored. This paper investigates the influence of signal quantization on performance of first-order 1D DMAs across various beampatterns. An analytical expression for quantized beamformed output for a first-order 1D DMA has been formulated. The effect of signal quantization has been studied on array performance measures such as the Beampattern, Directivity Factor (DF), Front-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that beampattern shape remains structurally invariant across quantization bit depths, with quantization primarily affecting ND. DF and FBR remain constant with the varying number of quantization bits. Additionally, ND is shown to be frequency-independent; however, it increases with increasing quantization bit depths, enhancing interference suppression. The study also examines the effect of steering nulls across the azimuthal range, showing that ND degrades as the null moves closer to the source look direction, indicating reduced interference suppression.

</details>


### [11] [Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage](https://arxiv.org/abs/2506.15470)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: 该论文研究了高频率带大型天线阵列部署下的近场波束聚焦与空间复用问题，推导了广义均匀矩形阵列（URA）的波束深度，并分析了几何形状对波束深度和近场边界的影响。结果表明，方形URA的波束深度最窄，但宽或高的URA在有效波束聚焦瑞利距离（EBRD）和空间复用性能上表现更优，仿真显示其多用户和速率是方形URA的3.5倍。


<details>
  <summary>Details</summary>
Motivation: 随着高频率带大型天线阵列的部署，未来无线通信系统可能工作在辐射近场区。近场波束聚焦可实现有限深度空间区域内的空间复用，但需要研究阵列几何形状对波束深度和近场边界的影响。

Method: 推导广义均匀矩形阵列（URA）的波束深度，定义有效波束聚焦瑞利距离（EBRD）来表征近场边界，分析不同几何形状URA的性能差异。

Result: 方形URA波束深度最窄，但宽或高的URA在EBRD和空间复用性能上更优，仿真显示其多用户和速率是方形URA的3.5倍。

Conclusion: 阵列几何形状对近场波束聚焦和空间复用性能有显著影响，宽或高的URA在多用户通信中性能更佳。

Abstract: With the deployment of large antenna arrays at high frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused within a spatial region of finite depth, enabling spatial multiplexing in both the angular and range dimensions. This paper derives the beamdepth for a generalized uniform rectangular array (URA) and investigates how array geometry influences the near-field beamdepth and the limits where near-field beamfocusing is achievable. To characterize the near-field boundary in terms of beamfocusing and spatial multiplexing gains, we define the effective beamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis reveals that while a square URA achieves the narrowest beamdepth, the EBRD is maximized for a wide or tall URA. However, despite its narrow beamdepth, a square URA may experience a reduction in multiuser sum rate due to its severely constrained EBRD. Simulation results confirm that a wide or tall URA achieves a sum rate of 3.5 X more than that of a square URA, benefiting from the extended EBRD and improved spatial multiplexing capabilities.

</details>


### [12] [Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward](https://arxiv.org/abs/2506.15670)
*Özlem Tugfe Demir,Mustafa Ozger,Ferdi Kara,Woong-Hee Lee,Emil Björnson*

Main category: eess.SP

TL;DR: 探讨了将SWIPT与gMIMO技术结合在7-24 GHz频段的应用，提出了能量高效、高容量的通信系统方案。


<details>
  <summary>Details</summary>
Motivation: 满足6G网络需求，推动能量自主的物联网和智能工厂网络发展。

Method: 利用球形波传播和近场效应，采用波束聚焦和大规模空间复用技术，结合高级信道估计和动态阵列配置。

Result: 通过案例研究证明了在密集动态环境中优化能量收集和数据吞吐的可行性。

Conclusion: 该研究为下一代无线技术的能量自主应用提供了理论支持和实践解决方案。

Abstract: This paper explores the integration of simultaneous wireless information and power transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO) technology operating in the upper mid-band frequency range (7-24 GHz). The near-field propagation achieved by gMIMO introduces unique opportunities for energy-efficient, high-capacity communication systems that cater to the demands of 6G wireless networks. Exploiting spherical wave propagation, near-field SWIPT with gMIMO enables precise energy and data delivery, enhancing spectral efficiency through beamfocusing and massive spatial multiplexing. This paper discusses theoretical principles, design challenges, and enabling solutions, including advanced channel estimation techniques, precoding strategies, and dynamic array configurations such as sparse and modular arrays. Through analytical insights and a case study, this paper demonstrates the feasibility of achieving optimized energy harvesting and data throughput in dense and dynamic environments. These findings contribute to advancing energy-autonomous Internet-of-Everything (IoE) deployments, smart factory networks, and other energy-autonomous applications aligned with the goals of next-generation wireless technologies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.AI

TL;DR: 本文提出了一种基于无人机和船舶协作的海上物联网(MIoT)计算卸载和资源分配框架，通过Lyapunov优化和马尔可夫博弈解决不确定任务带来的挑战，并使用异质智能体软演员评论家方法有效解决问题。


<details>
  <summary>Details</summary>
Motivation: 海上物联网的计算需求快速增长，但不确定的任务导致计算卸载和资源分配效率低下，无人机与船舶协作的多接入边缘计算(MEC)可以满足这些需求。

Method: 提出协作MEC框架，利用Lyapunov优化处理不确定任务，将长期约束转化为短期问题，并通过马尔可夫博弈和异质智能体软演员评论家方法解决。

Result: 仿真验证了所提方法在计算卸载和资源分配中的有效性。

Conclusion: 该框架成功解决了海上物联网中由不确定任务引起的计算卸载和资源分配问题，为实际应用提供了有效方案。

Abstract: The computation demands from the maritime Internet of Things (MIoT) increase rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels based multi-access edge computing (MEC) can fulfill these MIoT requirements. However, the uncertain maritime tasks present significant challenges of inefficient computation offloading and resource allocation. In this paper, we focus on the maritime computation offloading and resource allocation through the cooperation of UAVs and vessels, with consideration of uncertain tasks. Specifically, we propose a cooperative MEC framework for computation offloading and resource allocation, including MIoT devices, UAVs and vessels. Then, we formulate the optimization problem to minimize the total execution time. As for the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the unpredictable task arrivals and varying computational resource availability. 
By converting the long-term constraints into short-term constraints, we obtain a set of small-scale optimization problems. Further, considering the heterogeneity of actions and resources of UAVs and vessels, we reformulate the small-scale optimization problem into a Markov game (MG). Moreover, a heterogeneous-agent soft actor-critic is proposed to sequentially update various neural networks and effectively solve the MG problem. 
Finally, simulations are conducted to verify the effectiveness in addressing computational offloading and resource allocation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models](https://arxiv.org/abs/2506.15530)
*Teysir Baoueb,Xiaoyu Bie,Xi Wang,Gaël Richard*

Main category: cs.SD

TL;DR: 本文探讨了如何利用已有的文本到音乐扩散模型进行乐器编辑，通过选择合适的时间步长，在保留原始音频内容的同时实现音色修改。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到音乐生成模型在控制生成结果方面仍面临挑战，尤其是在编辑乐器时如何保留原始内容。

Method: 利用预训练的文本到音乐扩散模型，通过选择中间时间步长（由乐器分类器确定）来平衡内容保留与音色修改。

Result: 该方法无需额外训练模型，也不会影响生成速度，成功实现了乐器编辑。

Conclusion: 提供了一种高效的乐器编辑方法，为音乐创作提供了新的工具。

Abstract: Breakthroughs in text-to-music generation models are transforming the creative landscape, equipping musicians with innovative tools for composition and experimentation like never before. However, controlling the generation process to achieve a specific desired outcome remains a significant challenge. Even a minor change in the text prompt, combined with the same random seed, can drastically alter the generated piece. In this paper, we explore the application of existing text-to-music diffusion models for instrument editing. Specifically, for an existing audio track, we aim to leverage a pretrained text-to-music diffusion model to edit the instrument while preserving the underlying content. Based on the insight that the model first focuses on the overall structure or content of the audio, then adds instrument information, and finally refines the quality, we show that selecting a well-chosen intermediate timestep, identified through an instrument classifier, yields a balance between preserving the original piece's content and achieving the desired timbre. Our method does not require additional training of the text-to-music diffusion model, nor does it compromise the generation process's speed.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [An Integrated Sensing and Communication System for Time-Sensitive Targets with Random Arrivals](https://arxiv.org/abs/2506.15045)
*Homa Nikbakht,Yonina C. Eldar,H. Vincent Poor*

Main category: cs.IT

TL;DR: 这篇论文提出了一种基于双态MIMO的ISAC系统，用于优先处理URLLC消息，通过脏纸编码技术降低干扰，并在有限块长体制下优化了速率、可靠性和检测的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 6G网络中，ISAC是实现联合感知与通信的关键技术，而新兴应用如智能城市和自动驾驶对URLLC有严格要求。因此，设计一种能够优先处理URLLC消息的ISAC系统具有重要意义。

Method: 论文提出了一种双态MIMO ISAC系统，利用脏纸编码（DPC）技术降低eMBB和感知信号的干扰。系统通过部署感知接收器来检测目标，并在需要时触发URLLC消息的传输。评估了两种处理URLLC干扰的方法：将干扰视为噪声和连续干扰消除。

Result: 数值分析表明，所提出的基于DPC的ISAC方案显著优于功率共享和传统时间共享方案，能够在满足URLLC和感知约束的同时实现更高的eMBB传输速率。

Conclusion: 该方案为6G网络中ISAC和URLLC的联合设计提供了有效的解决方案，满足了未来应用对超可靠和低延迟通信的需求。

Abstract: In 6G networks, integrated sensing and communication (ISAC) is envisioned as a key technology that enables wireless systems to perform joint sensing and communication using shared hardware, antennas and spectrum. ISAC designs facilitate emerging applications such as smart cities and autonomous driving. Such applications also demand ultra-reliable and low-latency communication (URLLC). Thus, an ISAC-enabled URLLC system can prioritize time-sensitive targets and ensure information delivery under strict latency and reliability constraints. We propose a bi-static MIMO ISAC system to detect the arrival of URLLC messages and prioritize their delivery. In this system, a base station (BS) communicates with a user equipment (UE) and a sensing receiver (SR) is deployed to collect echo signals reflected from a target of interest. The BS regularly transmits messages of enhanced mobile broadband (eMBB) services to the UE. During each eMBB transmission, if the SR senses the presence of a target of interest, it immediately triggers the transmission of an additional URLLC message. To reinforce URLLC transmissions, we propose a dirty-paper coding (DPC)-based technique that mitigates the interference of both eMBB and sensing signals. To decode the eMBB message, we consider two approaches for handling the URLLC interference: treating interference as noise and successive interference cancellation. For this system, we formulate the rate-reliability-detection trade-off in the finite blocklength (FBL) regime by evaluating the communication rate of the eMBB transmissions, the reliability of the URLLC transmissions and the probability of the target detection. Our numerical analysis show that our proposed DPC-based ISAC scheme significantly outperforms power-sharing and traditional time-sharing schemes. In particular, it achieves higher eMBB transmission rate while satisfying both URLLC and sensing constraints.

</details>


### [16] [MIMO Systems Aided by Microwave Linear Analog Computers: Capacity-Achieving Architectures with Reduced Circuit Complexity](https://arxiv.org/abs/2506.15052)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 为应对未来无线网络需求，提出了一种基于图论的微波线性模拟计算机（MiLAC）模型，设计了一种低复杂度的MiLAC架构（stem-connected MiLACs），在保持性能的同时显著降低电路复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决巨型MIMO系统中硬件和计算成本过高的问题，尤其是完全连接的MiLACs电路复杂度随天线数量二次方增长的限制。

Method: 提出基于图论的MiLAC模型，设计stem-connected MiLACs架构，并通过封闭形式的解决方案优化其性能。

Result: 理论分析和数值模拟表明，stem-connected MiLACs在保持性能的同时，电路复杂度与天线数量线性增长，适用于可扩展的巨型MIMO系统。

Conclusion: stem-connected MiLACs为高性能、可扩展的巨型MIMO提供了一种实用的解决方案。

Abstract: To meet the demands of future wireless networks, antenna arrays must scale from massive multiple-input multiple-output (MIMO) to gigantic MIMO, involving even larger numbers of antennas. To address the hardware and computational cost of gigantic MIMO, several strategies are available that shift processing from the digital to the analog domain. Among them, microwave linear analog computers (MiLACs) offer a compelling solution by enabling fully analog beamforming through reconfigurable microwave networks. Prior work has focused on fully-connected MiLACs, whose ports are all interconnected to each other via tunable impedance components. Although such MiLACs are capacity-achieving, their circuit complexity, given by the number of required impedance components, scales quadratically with the number of antennas, limiting their practicality. To solve this issue, in this paper, we propose a graph theoretical model of MiLAC facilitating the systematic design of lower-complexity MiLAC architectures. Leveraging this model, we propose stem-connected MiLACs as a family of MiLAC architectures maintaining capacity-achieving performance while drastically reducing the circuit complexity. Besides, we optimize stem-connected MiLACs with a closed-form capacity-achieving solution. Our theoretical analysis, confirmed by numerical simulations, shows that stem-connected MiLACs are capacity-achieving, but with circuit complexity that scales linearly with the number of antennas, enabling high-performance, scalable, gigantic MIMO.

</details>


### [17] [In-Context Learning for Gradient-Free Receiver Adaptation: Principles, Applications, and Theory](https://arxiv.org/abs/2506.15176)
*Matteo Zecchin,Tomer Raviv,Dileep Kalathil,Krishna Narayanan,Nir Shlezinger,Osvaldo Simeone*

Main category: cs.IT

TL;DR: 论文提出了一种基于上下文学习（ICL）的无梯度适应技术，用于优化深度学习的无线接收器，使其能动态适应不同信道环境，无需在线重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统模型设计和现有适应策略在灵活性和效率上存在局限，需要一种无需显式优化的实时适应方法。

Method: 利用基于Transformer和结构化状态空间模型（SSM）的ICL架构，结合理论和实验分析。

Result: ICL能够通过导频信号和上下文信息实现高效的实时接收器适应。

Conclusion: ICL为无线接收器的动态适应提供了一种原理性且高效的方法。

Abstract: In recent years, deep learning has facilitated the creation of wireless receivers capable of functioning effectively in conditions that challenge traditional model-based designs. Leveraging programmable hardware architectures, deep learning-based receivers offer the potential to dynamically adapt to varying channel environments. However, current adaptation strategies, including joint training, hypernetwork-based methods, and meta-learning, either demonstrate limited flexibility or necessitate explicit optimization through gradient descent. This paper presents gradient-free adaptation techniques rooted in the emerging paradigm of in-context learning (ICL). We review architectural frameworks for ICL based on Transformer models and structured state-space models (SSMs), alongside theoretical insights into how sequence models effectively learn adaptation from contextual information. Further, we explore the application of ICL to cell-free massive MIMO networks, providing both theoretical analyses and empirical evidence. Our findings indicate that ICL represents a principled and efficient approach to real-time receiver adaptation using pilot signals and auxiliary contextual information-without requiring online retraining.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [18] [Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations](https://arxiv.org/abs/2506.14795)
*Batuhan Hangun,Emine Akpinar,Oguz Altun,Onder Eyecioglu*

Main category: quant-ph

TL;DR: 本篇论文探讨了量子机器学习（QML）的优势，通过评估量子神经网络（QNNs）在风力能源数据集上的表现，证明了QNNs在预测任务中优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）是一个新兴领域，结合了量子计算和机器学习，旨在通过量子力学原理（如纠缠和叠加）提升经典机器学习方法。但由于当前嘈杂中型量子（NISQ）设备的限制，其实际优势备受质疑。

Method: 研究系统地构建和评估了12种不同的QNN配置，结合两种量子特征映射和六种纠缠策略设计ansatz。实验在风力能源数据集上进行，预测风力发电输出。

Result: 实验显示，采用Z特征映射的QNNs在仅使用四个输入参数时，预测准确率高达93%，且QNNs在预测任务中表现优于经典方法。

Conclusion: 研究结果表明QML在实际应用中具有潜力，QNNs能有效提升预测任务的性能。

Abstract: Quantum Machine Learning (QML) is an emerging field at the intersection of quantum computing and machine learning, aiming to enhance classical machine learning methods by leveraging quantum mechanics principles such as entanglement and superposition. However, skepticism persists regarding the practical advantages of QML, mainly due to the current limitations of noisy intermediate-scale quantum (NISQ) devices. This study addresses these concerns by extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired counterparts of Artificial Neural Networks (ANNs), demonstrating their effectiveness compared to classical methods. We systematically construct and evaluate twelve distinct QNN configurations, utilizing two unique quantum feature maps combined with six different entanglement strategies for ansatz design. Experiments conducted on a wind energy dataset reveal that QNNs employing the Z feature map achieve up to 93% prediction accuracy when forecasting wind power output using only four input parameters. Our findings show that QNNs outperform classical methods in predictive tasks, underscoring the potential of QML in real-world applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [19] [Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels](https://arxiv.org/abs/2506.15105)
*David Nozadze,Zurab Kiguradze,Amendra Koul,Mike Sapozhnikov*

Main category: eess.SY

TL;DR: 论文提出了两种新指标SILD和FOM_SILD，用于量化高速互连中的P/N skew影响，并验证了其与BER趋势的相关性。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的增加和数据中心需求增长，需要超高速互连（超过200 Gb/s），而传统方法无法准确量化P/N skew的影响。

Method: 提出两种新指标：Skew-Induced Insertion Loss Deviation (SILD)及其辅助指标Figure of Merit (FOM_SILD)，通过实测S参数和224G PAM4 SerDes仿真验证。

Result: 实验证实FOM_SILD的互易性，仿真显示其与BER趋势高度相关。

Conclusion: 该方法为分析下一代超高速互连中的skew提供了可靠的框架。

Abstract: The rise of AI workloads and growing data center demands have driven the need for ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI) shrink, even a few picoseconds of P/N skew can degrade serializer-deserializer (SerDes) performance. Traditional methods for quantifying skew fall short in capturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion Loss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD), analytically developed to assess P/N skew effects. Measured S-parameters confirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong correlation with bit error rate (BER) trends. This approach offers a robust framework for analyzing skew in next-generation ultra-high-speed interconnects.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [20] [CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets](https://arxiv.org/abs/2506.15075)
*Samhita Kuili,Mohammadreza Amini,Burak Kantarci*

Main category: cs.CR

TL;DR: 该论文研究了5G-NR网络中通过生成对抗网络（GAN）增强数据的自动化编码器（CAE）检测信号干扰的方法，并展示了优于基准模型的性能。


<details>
  <summary>Details</summary>
Motivation: 5G-NR网络中干扰攻击频发，影响了信号质量，需要一种有效的检测方法。

Method: 利用卷积自编码器（CAE）结合条件Wasserstein GAN（CWGAN-GP）对不平衡数据集进行增强，并与去噪自编码器（CDAE）和稀疏自编码器（CSAE）进行比较。

Result: CAE在干扰检测中表现出色，平均精确率97.33%、召回率91.33%、F1分数94.08%及准确率94.35%。

Conclusion: CAE在复杂异构数据下仍具鲁棒性，证明了其在干扰检测中的有效性。

Abstract: In the ever-expanding domain of 5G-NR wireless cellular networks, over-the-air jamming attacks are prevalent as security attacks, compromising the quality of the received signal. We simulate a jamming environment by incorporating additive white Gaussian noise (AWGN) into the real-world In-phase and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is exploited to implement a jamming detection over various characteristics such as heterogenous I/Q datasets; extracting relevant information on Synchronization Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance. Given the characteristics of datasets, balanced datasets are acquired by employing a Conv1D conditional Wasserstein Generative Adversarial Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB observations. Additionally, we compare the performance and detection ability of the proposed CAE model on augmented datasets with benchmark models: Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder (CSAE). Despite the complexity of data heterogeneity involved across all datasets, CAE depicts the robustness in detection performance of jammed signal by achieving average values of 97.33% precision, 91.33% recall, 94.08% F1-score, and 94.35% accuracy over CDAE and CSAE.

</details>
