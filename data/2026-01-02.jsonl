{"id": "2512.23725", "pdf": "https://arxiv.org/pdf/2512.23725", "abs": "https://arxiv.org/abs/2512.23725", "authors": ["Sel Ly", "Rufan Yang", "Ninad Dixit", "Hung Dinh Nguyen"], "title": "RUL-QMoE: Multiple Non-crossing Quantile Mixture-of-Experts for Probabilistic Remaining Useful Life Predictions of Varying Battery Materials", "categories": ["eess.SP"], "comment": "This is an extended version of the conference paper at the 38th Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-26)", "summary": "Lithium-ion batteries are the major type of battery used in a variety of everyday applications, including electric vehicles (EVs), mobile devices, and energy storage systems. Predicting the Remaining Useful Life (RUL) of Li-ion batteries is crucial for ensuring their reliability, safety, and cost-effectiveness in battery-powered systems. The materials used for the battery cathodes and their designs play a significant role in determining the degradation rates and RUL, as they lead to distinct electrochemical reactions. Unfortunately, RUL prediction models often overlook the cathode materials and designs to simplify the model-building process, ignoring the effects of these electrochemical reactions. Other reasons are that specifications related to battery materials may not always be readily available, and a battery might consist of a mix of different materials. As a result, the predictive models that are developed often lack generalizability. To tackle these challenges, this paper proposes a novel material-based Mixture-of-Experts (MoE) approach for predicting the RUL of batteries, specifically addressing the complexities associated with heterogeneous battery chemistries. The MoE is integrated into a probabilistic framework, called Multiple Non-crossing Quantile Mixture-of-Experts for Probabilistic Prediction (RUL-QMoE), which accommodates battery operational conditions and enables uncertainty quantification. The RUL-QMoE model integrates specialized expert networks for five battery types: LFP, NCA, NMC, LCO, and NMC-LCO, within a gating mechanism that dynamically assigns relevance based on the battery's input features. Furthermore, by leveraging non-crossing quantile regression, the proposed RUL-QMoE produces coherent and interpretable predictive distributions of the battery's RUL, enabling robust uncertainty quantification in the battery's RUL prediction."}
{"id": "2512.23902", "pdf": "https://arxiv.org/pdf/2512.23902", "abs": "https://arxiv.org/abs/2512.23902", "authors": ["Hesam Khoshkbari", "Georges Kaddoum", "Omid Abbasi", "Bassant Selim", "Halim Yanikomeroglu"], "title": "Beamforming for Massive MIMO Aerial Communications: A Robust and Scalable DRL Approach", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents a distributed beamforming framework for a constellation of airborne platform stations (APSs) in a massive Multiple-Input and Multiple-Output (MIMO) non-terrestrial network (NTN) that targets the downlink sum-rate maximization under imperfect local channel state information (CSI). We propose a novel entropy-based multi-agent deep reinforcement learning (DRL) approach where each non-terrestrial base station (NTBS) independently computes its beamforming vector using a Fourier Neural Operator (FNO) to capture long-range dependencies in the frequency domain. To ensure scalability and robustness, the proposed framework integrates transfer learning based on a conjugate prior mechanism and a low-rank decomposition (LRD) technique, thus enabling efficient support for large-scale user deployments and aerial layers. Our simulation results demonstrate the superiority of the proposed method over baseline schemes including WMMSE, ZF, MRT, CNN-based DRL, and the deep deterministic policy gradient (DDPG) method in terms of average sum rate, robustness to CSI imperfection, user mobility, and scalability across varying network sizes and user densities. Furthermore, we show that the proposed method achieves significant computational efficiency compared to CNN-based and WMMSE methods, while reducing communication overhead in comparison with shared-critic DRL approaches."}
{"id": "2512.23906", "pdf": "https://arxiv.org/pdf/2512.23906", "abs": "https://arxiv.org/abs/2512.23906", "authors": ["Wendong Yao", "Binhua Huang", "Soumyabrata Dev"], "title": "A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "submitted to ISPRS Journal of Photogrammetry and Remote Sensing for review", "summary": "Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a multimodal patch-based Transformer for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the multimodal Transformer clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and multimodal STGCN when all models receive the same multimodal inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies."}
{"id": "2512.24090", "pdf": "https://arxiv.org/pdf/2512.24090", "abs": "https://arxiv.org/abs/2512.24090", "authors": ["Dong Wang", "Weidong Mei", "Zhi Chen", "Boyu Ning"], "title": "Movable Antenna Enhanced Multi-Region Beam Coverage: A Multi-Notch-Filter-Inspired Design", "categories": ["eess.SP"], "comment": "5 pages, 5 figures", "summary": "Movable antenna (MA) has emerged as a promising technology to enhance wireless communication performance by exploiting the new degree of freedom (DoF) via antenna position optimization. In this letter, we investigate the MA-enhanced wide beam coverage over multiple subregions in the spatial domain. Specifically, we aim to maximize the minimum beam gain over the desired subregions by jointly optimizing the transmit beamforming and antenna position vector (APV). Although this problem is non-convex, we propose an efficient algorithm to solve it by leveraging the similarity between the considered multi-region coverage and classical multi-notch filter (MNF) design. In particular, we construct a spatial MNF-based transmit beamforming vector by assuming a continuous amplitude and phase-shift profile within the antenna movement region. Based on this continuous profile, we propose a sequential update algorithm to select an optimal subset of MA positions for multi-region coverage, jointly with a Gibbs sampling (GS) procedure to avoid undesired local optimum. Numerical results show that our proposed algorithm can significantly outperform conventional fixed position antennas (FPAs) and achieve a comparable performance to the alternating optimization (AO) algorithm with dramatically lower complexity."}
{"id": "2512.23856", "pdf": "https://arxiv.org/pdf/2512.23856", "abs": "https://arxiv.org/abs/2512.23856", "authors": ["Mark Van der Merwe", "Kei Ota", "Dmitry Berenson", "Nima Fazeli", "Devesh K. Jha"], "title": "Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing", "categories": ["cs.RO"], "comment": "8 pages. IEEE Robotics and Automation Letters, 2026", "summary": "Prehensile autonomous manipulation, such as peg insertion, tool use, or assembly, require precise in-hand understanding of the object pose and the extrinsic contacts made during interactions. Providing accurate estimation of pose and contacts is challenging. Tactile sensors can provide local geometry at the sensor and force information about the grasp, but the locality of sensing means resolving poses and contacts from tactile alone is often an ill-posed problem, as multiple configurations can be consistent with the observations. Adding visual feedback can help resolve ambiguities, but can suffer from noise and occlusions. In this work, we propose a method that pairs local observations from sensing with the physical constraints of contact. We propose a set of factors that ensure local consistency with tactile observations as well as enforcing physical plausibility, namely, that the estimated pose and contacts must respect the kinematic and force constraints of quasi-static rigid body interactions. We formalize our problem as a factor graph, allowing for efficient estimation. In our experiments, we demonstrate that our method outperforms existing geometric and contact-informed estimation pipelines, especially when only tactile information is available. Video results can be found at https://tacgraph.github.io/."}
{"id": "2512.24155", "pdf": "https://arxiv.org/pdf/2512.24155", "abs": "https://arxiv.org/abs/2512.24155", "authors": ["Ashish Patwari", "Sanjeeva Reddy S", "G Ramachandra Reddy"], "title": "Discovering Optimal Robust Minimum Redundancy Arrays (RMRAs) through Exhaustive Search and Algebraic Formulation of a New Sub-Optimal RMRA", "categories": ["eess.SP", "eess.SY"], "comment": "8 Pages, 2 Figures, IEEE Journal Format", "summary": "Modern sparse arrays are maximally economic in that they retain just as many sensors required to provide a specific aperture while maintaining a hole-free difference coarray. As a result, these are susceptible to the failure of even a single sensor. Contrarily, two-fold redundant sparse arrays (TFRSAs) and robust minimum redundancy arrays (RMRAs) ensure robustness against single-sensor failures due to their inherent redundancy in their coarrays. At present, optimal RMRA configurations are known only for arrays with sensor counts N=6 to N=10. To this end, this paper proposes two objectives: (i) developing a systematic algorithm to discover optimal RMRAs for N>10, and (ii) obtaining a new family of near-/sub-optimal RMRA that can be completely specified using closed-form expressions (CFEs). We solve the combinatorial optimization problem of finding RMRAs using an exhaustive search technique implemented in MATLAB. Optimal RMRAs for N = 11 to 14 were successfully found and near/sub-optimal arrays for N = 15 to 20 were determined using the proposed technique. As a byproduct of the exhaustive search, a large catalogue of valid near- and sub-optimal RMRAs was also obtained. In the second stage, CFEs for a new TFRSA were obtained by applying pattern mining and algebraic generalizations to the arrays obtained through exhaustive search. The proposed family enjoys CFEs for sensor positions, available aperture, and achievable degrees of freedom (DOFs). The CFEs have been thoroughly validated using MATLAB and are found to be valid for $N\\geq8$. Hence, it can be concluded that the novelty of this work is two-fold: extending the catalogue of known optimal RMRAs and formulating a sub-optimal RMRA that abides by CFEs."}
{"id": "2512.23864", "pdf": "https://arxiv.org/pdf/2512.23864", "abs": "https://arxiv.org/abs/2512.23864", "authors": ["Guo Ye", "Zexi Zhang", "Xu Zhao", "Shang Wu", "Haoran Lu", "Shihan Lu", "Han Liu"], "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents."}
{"id": "2512.24250", "pdf": "https://arxiv.org/pdf/2512.24250", "abs": "https://arxiv.org/abs/2512.24250", "authors": ["Wenchao Li", "Xuezhi Wang", "Qiang Sun", "Allison N. Kealy", "Andrew D. Greentree"], "title": "Quantifying the advantage of vector over scalar magnetic sensor networks for undersea surveillance", "categories": ["eess.SP", "quant-ph"], "comment": null, "summary": "Magnetic monitoring of maritime environments is an important problem for monitoring and optimising shipping, as well as national security. New developments in compact, fibre-coupled quantum magnetometers have led to the opportunity to critically evaluate how best to create such a sensor network. Here we explore various magnetic sensor network architectures for target identification. Our modelling compares networks of scalar vs vector magnetometers. We implement an unscented Kalman filter approach to perform target tracking, and we find that vector networks provide a significant improvement in target tracking, specifically tracking accuracy and resilience compared with scalar networks."}
{"id": "2512.23972", "pdf": "https://arxiv.org/pdf/2512.23972", "abs": "https://arxiv.org/abs/2512.23972", "authors": ["Liangtao Feng", "Zhenchang Liu", "Feng Zhang", "Xuefeng Ren"], "title": "SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community."}
{"id": "2512.24334", "pdf": "https://arxiv.org/pdf/2512.24334", "abs": "https://arxiv.org/abs/2512.24334", "authors": ["Anbang Zhang", "Chenyuan Feng", "Wai Ho Mow", "Jia Ye", "Shuaishuai Guo", "Geyong Min", "Tony Q. S. Quek"], "title": "OptiVote: Non-Coherent FSO Over-the-Air Majority Vote for Communication-Efficient Distributed Federated Learning in Space Data Centers", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The rapid deployment of mega-constellations is driving the long-term vision of space data centers (SDCs), where interconnected satellites form in-orbit distributed computing and learning infrastructures. Enabling distributed federated learning in such systems is challenging because iterative training requires frequent aggregation over inter-satellite links that are bandwidth- and energy-constrained, and the link conditions can be highly dynamic. In this work, we exploit over-the-air computation (AirComp) as an in-network aggregation primitive. However, conventional coherent AirComp relies on stringent phase alignment, which is difficult to maintain in space environments due to satellite jitter and Doppler effects. To overcome this limitation, we propose OptiVote, a robust and communication-efficient non-coherent free-space optical (FSO) AirComp framework for federated learning toward Space Data Centers. OptiVote integrates sign stochastic gradient descent (signSGD) with a majority-vote (MV) aggregation principle and pulse-position modulation (PPM), where each satellite conveys local gradient signs by activating orthogonal PPM time slots. The aggregation node performs MV detection via non-coherent energy accumulation, transforming phase-sensitive field superposition into phase-agnostic optical intensity combining, thereby eliminating the need for precise phase synchronization and improving resilience under dynamic impairments. To mitigate aggregation bias induced by heterogeneous FSO channels, we further develop an importance-aware, channel state information (CSI)-free dynamic power control scheme that balances received energies without additional signaling. We provide theoretical analysis by characterizing the aggregate error probability under statistical FSO channels and establishing convergence guarantees for non-convex objectives."}
{"id": "2512.24029", "pdf": "https://arxiv.org/pdf/2512.24029", "abs": "https://arxiv.org/abs/2512.24029", "authors": ["Takashi Yamamoto", "Hiroaki Yaguchi", "Shohei Kato", "Hiroyuki Okada"], "title": "Evaluation of Impression Difference of a Domestic Mobile Manipulator with Autonomous and/or Remote Control in Fetch-and-Carry Tasks", "categories": ["cs.RO", "cs.HC"], "comment": "Published in Advanced Robotics (2020). This paper defines an autonomous remote-control setting that makes the User-Robot-Operator triadic interaction explicit in physical tasks, and reports empirical differences in affinity and perceived security across autonomous, remote, and hybrid modes. Please cite: Advanced Robotics 34(20):1291-1308, 2020. DOI:10.1080/01691864.2020.1780152", "summary": "A single service robot can present two distinct agencies: its onboard autonomy and an operator-mediated agency, yet users experience them through one physical body. We formalize this dual-agency structure as a User-Robot-Operator triad in an autonomous remote-control setting that combines autonomous execution with remote human support. Prior to the recent surge of language-based and multimodal interfaces, we developed and evaluated an early-stage prototype in 2020 that combined natural-language text chat with freehand sketch annotations over the robot's live camera view to support remote intervention. We evaluated three modes - autonomous, remote, and hybrid - in controlled fetch-and-carry tasks using a domestic mobile manipulator (HSR) on a World Robot Summit 2020 rule-compliant test field. The results show systematic mode-dependent differences in user-rated affinity and additional insights on perceived security, indicating that switching or blending agency within one robot measurably shapes human impressions. These findings provide empirical guidance for designing human-in-the-loop mobile manipulation in domestic physical tasks."}
{"id": "2512.24412", "pdf": "https://arxiv.org/pdf/2512.24412", "abs": "https://arxiv.org/abs/2512.24412", "authors": ["Javier Giménez", "José A. Cortés", "Luis Díez"], "title": "Low-complexity spectral shaping method for OFDM signals with dynamically adaptive emission mask", "categories": ["eess.SP"], "comment": "12 pages", "summary": "Orthogonal frequency division multiplexing (OFDM) signals with rectangular pulses exhibit low spectral confinement. Shaping their power spectral density (PSD) is imperative in the increasingly overcrowded spectrum to benefit from the cognitive radio (CR) paradigm. However, since the available spectrum is non-contiguous and its occupancy changes with time, the spectral shaping solution has to be dynamically adapted. This work proposes a framework that allows using a reduced set of preoptimized pulses to shape the spectrum of OFDM signals, irrespective of its spectral width and location, by means of simple transformations. The employed pulses combine active interference cancellation (AIC) and adaptive symbol transition (AST) terms in a transparent way to the receiver. They can be easily adapted online by the communication device to changes in the location or width of the transmission band, which contrasts with existing methods of the same type that require solving NP-hard optimization problems."}
{"id": "2512.24112", "pdf": "https://arxiv.org/pdf/2512.24112", "abs": "https://arxiv.org/abs/2512.24112", "authors": ["Zonghan Li", "Tianwen Tao", "Rao Fu", "Liang Wang", "Dongyuan Zhang", "Quan Quan"], "title": "RflyUT-Sim: A Simulation Platform for Development and Testing of Complex Low-Altitude Traffic Control", "categories": ["cs.RO"], "comment": null, "summary": "Significant challenges are posed by simulation and testing in the field of low-altitude unmanned aerial vehicle (UAV) traffic due to the high costs associated with large-scale UAV testing and the complexity of establishing low-altitude traffic test scenarios. Stringent safety requirements make high fidelity one of the key metrics for simulation platforms. Despite advancements in simulation platforms for low-altitude UAVs, there is still a shortage of platforms that feature rich traffic scenarios, high-precision UAV and scenario simulators, and comprehensive testing capabilities for low-altitude traffic. Therefore, this paper introduces an integrated high-fidelity simulation platform for low-altitude UAV traffic. This platform simulates all components of the UAV traffic network, including the control system, the traffic management system, the UAV system, the communication network , the anomaly and fault modules, etc. Furthermore, it integrates RflySim/AirSim and Unreal Engine 5 to develop full-state models of UAVs and 3D maps that model the real world using the oblique photogrammetry technique. Additionally, the platform offers a wide range of interfaces, and all models and scenarios can be customized with a high degree of flexibility. The platform's source code has been released, making it easier to conduct research related to low-altitude traffic."}
{"id": "2512.24488", "pdf": "https://arxiv.org/pdf/2512.24488", "abs": "https://arxiv.org/abs/2512.24488", "authors": ["Erik Lentz", "Emily Ellwein", "Bill Kay", "Audun Myers", "Cameron Mackenzie"], "title": "The Wigner-Ville Transform as an Information Theoretic Tool in Radio-frequency Signal Analysis", "categories": ["eess.SP", "cs.IT", "quant-ph"], "comment": "12 pages, 11 figures", "summary": "This paper presents novel interpretations to the field of classical signal processing of the Wigner-Ville transform as an information measurement tool. The transform's utility in detecting and localizing information-laden signals amidst noisy and cluttered backgrounds, and further providing measure of their information volumes, are detailed herein using Tsallis' entropy and information and related functionals. Example use cases in radio frequency communications are given, where Wigner-Ville-based detection measures can be seen to provide significant sensitivity advantage, for some shown contexts greater than 15~dB advantage, over energy-based measures and without extensive training routines. Such an advantage is particularly significant for applications which have limitations on observation resources including time/space integration pressures and transient and/or feeble signals, where Wigner-Ville-based methods would improve sensing effectiveness by multiple orders of magnitude. The potential for advancement of several such applications is discussed."}
{"id": "2512.24125", "pdf": "https://arxiv.org/pdf/2512.24125", "abs": "https://arxiv.org/abs/2512.24125", "authors": ["Yi Liu", "Sukai Wang", "Dafeng Wei", "Xiaowei Cai", "Linqing Zhong", "Jiange Yang", "Guanghui Ren", "Jinyu Zhang", "Maoqing Yao", "Chuankang Li", "Xindong He", "Liliang Chen", "Jianlan Luo"], "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation."}
{"id": "2512.24573", "pdf": "https://arxiv.org/pdf/2512.24573", "abs": "https://arxiv.org/abs/2512.24573", "authors": ["Lei Li", "Yanqing Xu", "Tenghao Cai", "Tsung-Hui Chang"], "title": "Power Minimization in Pinching-Antenna Systems under Probabilistic LoS Blockage", "categories": ["eess.SP"], "comment": null, "summary": "With great flexibility to adjust antenna positions, pinching antennas (PAs) are promising for alleviating large-scale attenuation in wireless networks. In this work, we investigate the antenna positioning and beamforming (AP-BF) design in a multi-PA multi-user system under probabilistic light-of-sight (LoS) blockage and formulate a power minimization problem subject to per-user signal-to-noise ratio (SNR) constraints. For a single PA, we prove the convexity of the simplified problem and obtain its global optimum. For multiple PAs, we derive closed-form BF structures and develop an efficient first-order algorithm to achieve high-quality local solutions. Extensive numerical results validate the efficacy of our proposed designs and the substantial performance advantage of PA systems compared with conventional fixed-antenna systems in a term of power saving."}
{"id": "2512.24129", "pdf": "https://arxiv.org/pdf/2512.24129", "abs": "https://arxiv.org/abs/2512.24129", "authors": ["Manuel Bied", "John Arockiasamy", "Andy Comeca", "Maximilian Schrapel", "Victoria Yang", "Alexey Rolich", "Barbara Bruno", "Maike Schwammberger", "Dieter Fiems", "Alexey Vinel"], "title": "ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike."}
{"id": "2512.24583", "pdf": "https://arxiv.org/pdf/2512.24583", "abs": "https://arxiv.org/abs/2512.24583", "authors": ["Rahul Gulia", "Ashish Sheikh", "Feyisayo Favour Popoola", "Serisha Vadlamudi"], "title": "Resource Allocation via Backscatter-Aware Transmit Antenna Selection for Low-PAPR and Ultra-Reliable WSNs", "categories": ["eess.SP"], "comment": null, "summary": "This paper addresses a fundamental physical layer conflict in hybrid Wireless Sensor Networks (WSNs) between high-throughput primary communication and the stringent power envelope requirements of passive backscatter sensors. We propose a Backscatter-Constrained Transmit Antenna Selection (BC-TAS) framework, a per-subcarrier selection strategy for multi-antenna illuminators operating within a Multi-Dimensional Orthogonal Frequency Division Multiplexing (MD-OFDM) architecture. Unlike conventional signal-to-noise ratio (SNR) centric selection schemes, BC-TAS employs a multi-objective cost function that jointly maximizes desired link reliability, stabilizes the incident RF energy envelope at passive Surface Acoustic Wave (SAW) sensors, and suppresses interference toward coexisting victim receivers. By exploiting the inherent sparsity of MD-OFDM, the proposed framework enables dual-envelope regulation, simultaneously reducing the transmitter Peak-to-Average Power Ratio (PAPR) and the Backscatter Crest Factor (BCF) observed at the tag. To enhance robustness under imperfect Channel State Information (CSI), a Kalman-based channel smoothing mechanism is incorporated to maintain selection stability in low-SNR regimes. Numerical results using IEEE 802.11be dispersive channel models and a nonlinear Rapp power amplifier demonstrate that BC-TAS achieves orders-of-magnitude improvement in outage probability and significant gains in energy efficiency compared to conventional MU-MIMO baselines, while ensuring spectral mask compliance under reduced power amplifier back-off. These results establish BC-TAS as an effective illuminator-side control mechanism for enabling reliable and energy-stable sensing and communication coexistence in dense, power-constrained wireless environments."}
{"id": "2512.24210", "pdf": "https://arxiv.org/pdf/2512.24210", "abs": "https://arxiv.org/abs/2512.24210", "authors": ["Ruoshi Wen", "Guangzeng Chen", "Zhongren Cui", "Min Du", "Yang Gou", "Zhigang Han", "Liqun Huang", "Mingyu Lei", "Yunfei Li", "Zhuohang Li", "Wenlei Liu", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Yutao Ouyang", "Zeyu Ren", "Haixin Shi", "Wei Xu", "Haoxiang Zhang", "Jiajun Zhang", "Xiao Zhang", "Liwei Zheng", "Weiheng Zhong", "Yifei Zhou", "Zhengming Zhu", "Hang Li"], "title": "GR-Dexter Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation."}
{"id": "2512.24624", "pdf": "https://arxiv.org/pdf/2512.24624", "abs": "https://arxiv.org/abs/2512.24624", "authors": ["Borui Du", "Yumeng Zhang", "Christos Masouros", "Bruno Clerckx"], "title": "A Uniform Pilot and Data Payload Optimization Framework for OTFS-Based ISAC", "categories": ["eess.SP", "cs.IT"], "comment": null, "summary": "The orthogonal time frequency space (OTFS) signal is considered a promising solution for high-mobility wireless environments. It manages Doppler effects by utilizing delay-Doppler (DD) domain processing. However, the relatively long OTFS frame duration could introduce considerable sensing or communication latency when radar and communication are performed separately. By operating in a dual-functional radar and communication (DFRC) mode, the OTFS system performs sensing and data transmission simultaneously, thereby reducing the resulting latency. Nevertheless, the optimal OTFS DFRC signal strategy remains insufficiently explored. This paper investigates the optimal signal design for OTFS DFRC systems, focusing on pilot symbol design and data symbol power allocation. Specifically, we derive a channel capacity lower bound metric for communication that considers channel estimation errors in OTFS. For sensing, we derive an integrated sidelobe level (ISL), accounting for the randomness of the data symbols alongside the deterministic pilot symbols. Leveraging the above metrics, we formulate an optimization problem that balances radar and communication performance, and then solve it using an alternating optimization framework. We validate the proposed signal through numerical analysis and Monte Carlo simulations. Our analysis shows that OTFS DFRC enforces a deterministic pilot signal that is characterized by a concentrated peak in the DD domain, which furnishes a common structure in the DD domain facilitating sensing and channel estimation, with data multiplexed in other DD grids, thereby unifying sensing and communication within a single OTFS signal. Compared with conventional OTFS signals, the proposed OTFS DFRC signal expands the achievable sensing-communication performance region, delivering at least a 9.45 dB ISL suppression for sensing and a 4.82 dB SINR ratio gain for communication."}
{"id": "2512.24212", "pdf": "https://arxiv.org/pdf/2512.24212", "abs": "https://arxiv.org/abs/2512.24212", "authors": ["Ming-Ming Yu", "Yi Chen", "Börje F. Karlsson", "Wenjun Wu"], "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required."}
{"id": "2512.24727", "pdf": "https://arxiv.org/pdf/2512.24727", "abs": "https://arxiv.org/abs/2512.24727", "authors": ["Jaehong Jo", "Jihun Park", "Yo-Seb Jeon", "H. Vincent Poor"], "title": "Beam-Squint-Aided Hierarchical Sensing for Integrated Sensing and Communications with Uniform Planar Arrays", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we propose a novel hierarchical sensing framework for wideband integrated sensing and communications with uniform planar arrays (UPAs). Leveraging the beam-squint effect inherent in wideband orthogonal frequency-division multiplexing (OFDM) systems, the proposed framework enables efficient two-dimensional angle estimation through a structured multi-stage sensing process. Specifically, the sensing procedure first searches over the elevation angle domain, followed by a dedicated search over the azimuth angle domain given the estimated elevation angles. In each stage, true-time-delay lines and phase shifters of the UPA are jointly configured to cover multiple grid points simultaneously across OFDM subcarriers. To enable accurate and efficient target localization, we formulate the angle estimation problem as a sparse signal recovery problem and develop a modified matching pursuit algorithm tailored to the hierarchical sensing architecture. Additionally, we design power allocation strategies that minimize total transmit power while meeting performance requirements for both sensing and communication. Numerical results demonstrate that the proposed framework achieves superior performance over conventional sensing methods with reduced sensing power."}
{"id": "2512.24249", "pdf": "https://arxiv.org/pdf/2512.24249", "abs": "https://arxiv.org/abs/2512.24249", "authors": ["Fuqiang Gu", "Jiangshan Ai", "Xu Lu", "Xianlei Long", "Yan Li", "Tao Jiang", "Chao Chen", "Huidong Liu"], "title": "Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking", "categories": ["cs.RO"], "comment": "Accepted by IROS 2025 (2025 IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Unmanned Aerial Vehicles (UAVs) play an important role in various applications, where precise trajectory tracking is crucial. However, conventional control algorithms for trajectory tracking often exhibit limited performance due to the underactuated, nonlinear, and highly coupled dynamics of quadrotor systems. To address these challenges, we propose HBO-PID, a novel control algorithm that integrates the Heteroscedastic Bayesian Optimization (HBO) framework with the classical PID controller to achieve accurate and robust trajectory tracking. By explicitly modeling input-dependent noise variance, the proposed method can better adapt to dynamic and complex environments, and therefore improve the accuracy and robustness of trajectory tracking. To accelerate the convergence of optimization, we adopt a two-stage optimization strategy that allow us to more efficiently find the optimal controller parameters. Through experiments in both simulation and real-world scenarios, we demonstrate that the proposed method significantly outperforms state-of-the-art (SOTA) methods. Compared to SOTA methods, it improves the position accuracy by 24.7% to 42.9%, and the angular accuracy by 40.9% to 78.4%."}
{"id": "2512.24788", "pdf": "https://arxiv.org/pdf/2512.24788", "abs": "https://arxiv.org/abs/2512.24788", "authors": ["Zhixu Wang", "Jiacheng Yao", "Wei Xu", "Wei Shi", "Kaibin Huang"], "title": "Digitalizing Over-the-Air Computation via The Novel Complement Coded Modulation", "categories": ["eess.SP"], "comment": null, "summary": "To overcome inherent limitations of analog signals in over-the-air computation (AirComp), this letter proposes a two's complement-based coding scheme for the AirComp implementation with compatible digital modulations. Specifically, quantized discrete values are encoded into binary sequences using the two's complement and transmitted over multiple subcarriers. At the receiver, we design a decoder that constructs a functional mapping between the superimposed digital modulation signals and the target of computational results, theoretically ensuring asymptotic error free computation with the minimal codeword length. To further mitigate the adverse effects of channel fading, we adopt a truncated inversion strategy for pre-processing. Benefiting from the unified symbol distribution after the proposed encoding, we derive the optimal linear minimum mean squared error (LMMSE) detector in closed form and propose a low complexity algorithm seeking for the optimal truncation selection. Furthermore, the inherent importance differences among the coded outputs motivate an uneven power allocation strategy across subcarriers to improve computational accuracy. Numerical results validate the superiority of the proposed scheme over existing digital AirComp approaches, especially at low signal to-noise ratio (SNR) regimes."}
{"id": "2512.24272", "pdf": "https://arxiv.org/pdf/2512.24272", "abs": "https://arxiv.org/abs/2512.24272", "authors": ["Jiawei Zhang", "Chengchao Bai", "Wei Pan", "Tianhang Liu", "Jifeng Guo"], "title": "Local Path Optimization in The Latent Space Using Learned Distance Gradient", "categories": ["cs.RO"], "comment": "This paper has been published in IROS 2025", "summary": "Constrained motion planning is a common but challenging problem in robotic manipulation. In recent years, data-driven constrained motion planning algorithms have shown impressive planning speed and success rate. Among them, the latent motion method based on manifold approximation is the most efficient planning algorithm. Due to errors in manifold approximation and the difficulty in accurately identifying collision conflicts within the latent space, time-consuming path validity checks and path replanning are required. In this paper, we propose a method that trains a neural network to predict the minimum distance between the robot and obstacles using latent vectors as inputs. The learned distance gradient is then used to calculate the direction of movement in the latent space to move the robot away from obstacles. Based on this, a local path optimization algorithm in the latent space is proposed, and it is integrated with the path validity checking process to reduce the time of replanning. The proposed method is compared with state-of-the-art algorithms in multiple planning scenarios, demonstrating the fastest planning speed"}
{"id": "2512.24815", "pdf": "https://arxiv.org/pdf/2512.24815", "abs": "https://arxiv.org/abs/2512.24815", "authors": ["Boyao Li", "Qinwei He", "Boao Zhang", "Xiaopeng Yuan", "Anke Schmeink"], "title": "Efficient Joint Resource Allocation for Wireless Powered ISAC with Target Localization", "categories": ["eess.SP"], "comment": null, "summary": "Wireless powered integrated sensing and communication (ISAC) faces a fundamental tradeoff between energy supply, communication throughput, and sensing accuracy. This paper investigates a wireless powered ISAC system with target localization requirements, where users harvest energy from wireless power transfer (WPT) and then conduct ISAC transmissions in a time-division manner. In addition to energy supply, the WPT signal also contributes to target sensing, and the localization accuracy is characterized by Cramér-Rao bound (CRB) constraints. Under this setting, we formulate a max-min throughput maximization problem by jointly allocating the WPT duration, ISAC transmission time allocation, and transmit power. Due to the nonconvexity of the resulting problem, a suitable reformulation is developed by exploiting variable substitutions and the monotonicity of logarithmic functions, based on which an efficient successive convex approximation (SCA)-based iterative algorithm is proposed. Simulation results demonstrate convergence and significant performance gains over benchmark schemes, highlighting the importance of coordinated time-power optimization in balancing sensing accuracy and communication performance in wireless powered ISAC systems."}
{"id": "2512.24284", "pdf": "https://arxiv.org/pdf/2512.24284", "abs": "https://arxiv.org/abs/2512.24284", "authors": ["Ruitong Li", "Lin Zhang", "Yuenan Zhao", "Chengxin Liu", "Ran Song", "Wei Zhang"], "title": "DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Deep reinforcement learning (DRL) methods have demonstrated potential for autonomous navigation and obstacle avoidance of unmanned ground vehicles (UGVs) in crowded environments. Most existing approaches rely on single-frame observation and employ simple concatenation for multi-modal fusion, which limits their ability to capture temporal context and hinders dynamic adaptability. To address these challenges, we propose a DRL-based navigation framework, DRL-TH, which leverages temporal graph attention and hierarchical graph pooling to integrate historical observations and adaptively fuse multi-modal information. Specifically, we introduce a temporal-guided graph attention network (TG-GAT) that incorporates temporal weights into attention scores to capture correlations between consecutive frames, thereby enabling the implicit estimation of scene evolution. In addition, we design a graph hierarchical abstraction module (GHAM) that applies hierarchical pooling and learnable weighted fusion to dynamically integrate RGB and LiDAR features, achieving balanced representation across multiple scales. Extensive experiments demonstrate that our DRL-TH outperforms existing methods in various crowded environments. We also implemented DRL-TH control policy on a real UGV and showed that it performed well in real world scenarios."}
{"id": "2512.24923", "pdf": "https://arxiv.org/pdf/2512.24923", "abs": "https://arxiv.org/abs/2512.24923", "authors": ["Haojin Li", "Dongzhe Li", "Anbang Zhang", "Wenqi Zhang", "Chen Sun", "Haijun Zhang"], "title": "No Vision, No Wearables: 5G-based 2D Human Pose Recognition with Integrated Sensing and Communications", "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "With the increasing maturity of contactless human pose recognition (HPR) technology, indoor interactive applications have raised higher demands for natural, controller-free interaction methods. However, current mainstream HPR solutions relying on vision or radio-frequency (RF) (including WiFi, radar) still face various challenges in practical deployment, such as privacy concerns, susceptibility to occlusion, dedicated equipment and functions, and limited sensing resolution and range. 5G-based integrated sensing and communication (ISAC) technology, by merging communication and sensing functions, offers a new approach to address these challenges in contactless HPR. We propose a practical 5G-based ISAC system capable of inferring 2D HPR from uplink sounding reference signals (SRS). Specifically, rich features are extracted from multiple domains and employ an encoder to achieve unified alignment and representation in a latent space. Subsequently, low-dimensional features are fused to output the human pose state. Experimental results demonstrate that in typical indoor environments, our proposed 5G-based ISAC HPR system significantly outperforms current mainstream baseline solutions in HPR performance, providing a solid technical foundation for universal human-computer interaction."}
{"id": "2512.24288", "pdf": "https://arxiv.org/pdf/2512.24288", "abs": "https://arxiv.org/abs/2512.24288", "authors": ["Yinuo Zhao", "Huiqian Jin", "Lechun Jiang", "Xinyi Zhang", "Kun Wu", "Pei Ren", "Zhiyuan Xu", "Zhengping Che", "Lei Sun", "Dapeng Wu", "Chi Harold Liu", "Jian Tang"], "title": "Real-world Reinforcement Learning from Suboptimal Interventions", "categories": ["cs.RO"], "comment": null, "summary": "Real-world reinforcement learning (RL) offers a promising approach to training precise and dexterous robotic manipulation policies in an online manner, enabling robots to learn from their own experience while gradually reducing human labor. However, prior real-world RL methods often assume that human interventions are optimal across the entire state space, overlooking the fact that even expert operators cannot consistently provide optimal actions in all states or completely avoid mistakes. Indiscriminately mixing intervention data with robot-collected data inherits the sample inefficiency of RL, while purely imitating intervention data can ultimately degrade the final performance achievable by RL. The question of how to leverage potentially suboptimal and noisy human interventions to accelerate learning without being constrained by them thus remains open. To address this challenge, we propose SiLRI, a state-wise Lagrangian reinforcement learning algorithm for real-world robot manipulation tasks. Specifically, we formulate the online manipulation problem as a constrained RL optimization, where the constraint bound at each state is determined by the uncertainty of human interventions. We then introduce a state-wise Lagrange multiplier and solve the problem via a min-max optimization, jointly optimizing the policy and the Lagrange multiplier to reach a saddle point. Built upon a human-as-copilot teleoperation system, our algorithm is evaluated through real-world experiments on diverse manipulation tasks. Experimental results show that SiLRI effectively exploits human suboptimal interventions, reducing the time required to reach a 90% success rate by at least 50% compared with the state-of-the-art RL method HIL-SERL, and achieving a 100% success rate on long-horizon manipulation tasks where other RL methods struggle to succeed. Project website: https://silri-rl.github.io/."}
{"id": "2512.24958", "pdf": "https://arxiv.org/pdf/2512.24958", "abs": "https://arxiv.org/abs/2512.24958", "authors": ["Tong Wei", "Kumar Vijay Mishra", "Bhavani Shankar M. R.", "Björn Ottersten"], "title": "Fundamental Limits for Near-Field Sensing -- Part I: Narrow-Band Systems", "categories": ["eess.SP"], "comment": null, "summary": "Extremely large-scale antenna arrays (ELAAs) envisioned for 6G enable high-resolution sensing. However, the ELAAs worked in extremely high frequency will push operation into the near-field region, where spherical wavefronts invalidate classical far-field models and alter fundamental estimation limits. The purpose of this and the companion paper (Part II) is to develop the theory of fundamental limits for near-field sensing systems in detail. In this paper (Part I), we develop a unified narrow-band near-field signal model for joint parameter sensing of moving targets using the ELAAs. Leveraging the Slepian--Bangs formulation, we derive closed-form Cram'er--Rao bounds (CRBs) for joint estimation of target position, velocity, and radar cross-section (RCS) under the slow-time sampling model. To obtain interpretable insights, we further establish explicit far-field and near-field approximations that reveal how the bounds scale with array aperture, target range, carrier wavelength, and coherent integration length. The resulting expressions expose the roles of self-information terms and their cross terms, clarifying when Fresnel corrections become non-negligible and providing beamformer and algorithm design guidelines for near-field sensing with ELAAs. Simulation results validate the derived CRBs and their far-field and near-field approximations, demonstrating accurate agreement with the analytical scaling laws across representative array sizes and target ranges."}
{"id": "2512.24310", "pdf": "https://arxiv.org/pdf/2512.24310", "abs": "https://arxiv.org/abs/2512.24310", "authors": ["TARS Robotics", "Yuhang Zheng", "Jichao Peng", "Weize Li", "Yupeng Zheng", "Xiang Li", "Yujie Jin", "Julong Wei", "Guanhua Zhang", "Ruiling Zheng", "Ming Cao", "Songen Gu", "Zhenhong Zou", "Kaige Li", "Ke Wu", "Mingmin Yang", "Jiahao Liu", "Pengfei Li", "Hengjie Si", "Feiyu Zhu", "Wang Fu", "Likun Wang", "Ruiwen Yao", "Jieru Zhao", "Yilun Chen", "Wenchao Din"], "title": "World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild", "categories": ["cs.RO"], "comment": null, "summary": "Large-scale pre-training is fundamental for generalization in language and vision models, but data for dexterous hand manipulation remains limited in scale and diversity, hindering policy generalization. Limited scenario diversity, misaligned modalities, and insufficient benchmarking constrain current human manipulation datasets. To address these gaps, we introduce World In Your Hands (WiYH), a large-scale open-source ecosystem for human-centric manipulation learning. WiYH includes (1) the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture; (2) the WiYH Dataset, featuring over 1,000 hours of multi-modal manipulation data across hundreds of skills in diverse real-world scenarios; and (3) extensive annotations and benchmarks supporting tasks from perception to action. Furthermore, experiments based on the WiYH ecosystem show that integrating WiYH's human-centric data significantly enhances the generalization and robustness of dexterous hand policies in tabletop manipulation tasks. We believe that World In Your Hands will bring new insights into human-centric data collection and policy learning to the community."}
{"id": "2512.24962", "pdf": "https://arxiv.org/pdf/2512.24962", "abs": "https://arxiv.org/abs/2512.24962", "authors": ["Tong Wei", "Kumar Vijay Mishra", "Bhavani Shankar M. R.", "Björn Ottersten"], "title": "Fundamental Limits for Near-Field Sensing -- Part II: Wide-Band Systems", "categories": ["eess.SP"], "comment": null, "summary": "Near-field sensing with extremely large-scale antenna arrays (ELAAs) in practical 6G systems is expected to operate over broad bandwidths, where delay, Doppler, and spatial effects become tightly coupled across frequency. The purpose of this and the companion paper (Part I) is to develop the unified Cram'er--Rao bounds (CRBs) for sensing systems spanning from far-field to near-field, and narrow-band to wide-band. This paper (Part II) derives fundamental estimation limits for a wide-band near-field sensing systems employing orthogonal frequency-division multiplexing signaling over a coherent processing interval. We establish an exact near-field wide-band signal model that captures frequency-dependent propagation, spherical-wave geometry, and the intrinsic coupling between target location and motion parameters across subcarriers and slow time. Similar as Part I using the Slepian--Bangs formulation, we derive the wide-band Fisher information matrix and the CRBs for joint estimation of target position, velocity, and radar cross-section, and we show how wide-band information aggregates across orthogonal subcarriers. We further develop tractable far-field and near-field approximations which provide design-level insights into the roles of bandwidth, coherent integration length, and array aperture, and clarify when wide-band effects. Simulation results validate the derived CRBs and its approximations, demonstrating close agreement with the analytical scaling laws across representative ranges, bandwidths, and array configurations."}
{"id": "2512.24326", "pdf": "https://arxiv.org/pdf/2512.24326", "abs": "https://arxiv.org/abs/2512.24326", "authors": ["Camron Alexander Hirst", "Chris Reale", "Eric Frew"], "title": "3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second."}
{"id": "2407.03898", "pdf": "https://arxiv.org/pdf/2407.03898", "abs": "https://arxiv.org/abs/2407.03898", "authors": ["Shunqi Huang", "Lei Liu", "Brian M. Kurkoski"], "title": "Overflow-Avoiding Memory AMP", "categories": ["cs.IT", "eess.SP", "math.ST"], "comment": null, "summary": "Approximate Message Passing (AMP) type algorithms are widely used for signal recovery in high-dimensional noisy linear systems. Recently, a principle called Memory AMP (MAMP) was proposed. Leveraging this principle, the gradient descent MAMP (GD-MAMP) algorithm was designed, inheriting the strengths of AMP and OAMP/VAMP. In this paper, we first provide an overflow-avoiding GD-MAMP (OA-GD-MAMP) to address the overflow problem that arises from some intermediate variables exceeding the range of floating point numbers. Second, we develop a complexity-reduced GD-MAMP (CR-GD-MAMP) to reduce the number of matrix-vector products per iteration by 1/3 (from 3 to 2) with little to no impact on the convergence speed."}
{"id": "2512.24384", "pdf": "https://arxiv.org/pdf/2512.24384", "abs": "https://arxiv.org/abs/2512.24384", "authors": ["Yanlong Ma", "Nakul S. Joshi", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "Geometric Multi-Session Map Merging with Learned Local Descriptors", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation."}
{"id": "2508.08099", "pdf": "https://arxiv.org/pdf/2508.08099", "abs": "https://arxiv.org/abs/2508.08099", "authors": ["Lei Liu", "Yuhao Chi", "Shunqi Huang"], "title": "Random Modulation: Achieving Asymptotic Replica Optimality over Arbitrary Norm-Bounded and Spectrally Convergent Channel Matrices", "categories": ["cs.IT", "eess.SP", "math.ST"], "comment": null, "summary": "This paper introduces a random modulation technique that is decoupled from the channel matrix, allowing it to be applied to arbitrary norm-bounded and spectrally convergent channel matrices. The proposed random modulation constructs an equivalent dense and random channel matrix, ensuring that the signals undergo sufficient statistical channel fading. It also guarantees the asymptotic replica maximum a posteriori (MAP) bit-error rate (BER) optimality of approximate message passing (AMP)-type detectors for linear systems with arbitrary norm-bounded and spectrally convergent channel matrices when their state evolution has a unique fixed point. Then, a low-complexity cross-domain memory approximate message passing (CD-MAMP) detector is proposed for random modulation, leveraging the sparsity of the time-domain channel and the randomness of the random transform-domain channel. Furthermore, the optimal power allocation schemes are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random-modulated linear systems, assuming the availability of channel state information (CSI) at the transceiver. Numerical results show that the proposed random modulation can achieve BER and block-error rate (BLER) performance gains of up to 2 - 3 dB compared to existing OFDM/OTFS/AFDM with 5G-NR LDPC codes, under both average and optimized power allocation."}
{"id": "2512.24402", "pdf": "https://arxiv.org/pdf/2512.24402", "abs": "https://arxiv.org/abs/2512.24402", "authors": ["Giovanni Lambertini", "Matteo Pini", "Eugenio Mascaro", "Francesco Moretti", "Ayoub Raji", "Marko Bertogna"], "title": "Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack", "categories": ["cs.RO", "cs.AI", "cs.SE", "eess.SY"], "comment": "Accepted to the 2026 IEEE/SICE International Symposium on System Integration (SII 2026)", "summary": "In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis."}
{"id": "2512.24039", "pdf": "https://arxiv.org/pdf/2512.24039", "abs": "https://arxiv.org/abs/2512.24039", "authors": ["Shengsong Luo", "Ruilin Wu", "Chongbin Xu", "Junjie Ma", "Xiaojun Yuan", "Xin Wang"], "title": "Continuous Angular Power Spectrum Recovery From Channel Covariance via Chebyshev Polynomials", "categories": ["cs.IT", "eess.SP"], "comment": "14 pages", "summary": "This paper proposes a Chebyshev polynomial expansion framework for the recovery of a continuous angular power spectrum (APS) from channel covariance. By exploiting the orthogonality of Chebyshev polynomials in a transformed domain, we derive an exact series representation of the covariance and reformulate the inherently ill-posed APS inversion as a finite-dimensional linear regression problem via truncation. The associated approximation error is directly controlled by the tail of the APS's Chebyshev series and decays rapidly with increasing angular smoothness. Building on this representation, we derive an exact semidefinite characterization of nonnegative APS and introduce a derivative-based regularizer that promotes smoothly varying APS profiles while preserving transitions of clusters. Simulation results show that the proposed Chebyshev-based framework yields accurate APS reconstruction, and enables reliable downlink (DL) covariance prediction from uplink (UL) measurements in a frequency division duplex (FDD) setting. These findings indicate that jointly exploiting smoothness and nonnegativity in a Chebyshev domain provides an effective tool for covariance-domain processing in multi-antenna systems."}
{"id": "2512.24426", "pdf": "https://arxiv.org/pdf/2512.24426", "abs": "https://arxiv.org/abs/2512.24426", "authors": ["Zhenghao \"Mark\" Peng", "Wenhao Ding", "Yurong You", "Yuxiao Chen", "Wenjie Luo", "Thomas Tian", "Yulong Cao", "Apoorva Sharma", "Danfei Xu", "Boris Ivanovic", "Boyi Li", "Bolei Zhou", "Yan Wang", "Marco Pavone"], "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning", "categories": ["cs.RO"], "comment": null, "summary": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act."}
{"id": "2512.24087", "pdf": "https://arxiv.org/pdf/2512.24087", "abs": "https://arxiv.org/abs/2512.24087", "authors": ["Lei Liu", "Yuhao Chi", "Shunqi Huang", "Zhaoyang Zhang"], "title": "Random Multiplexing", "categories": ["cs.IT", "cs.AI", "cs.LG", "eess.SP", "math.ST"], "comment": null, "summary": "As wireless communication applications evolve from traditional multipath environments to high-mobility scenarios like unmanned aerial vehicles, multiplexing techniques have advanced accordingly. Traditional single-carrier frequency-domain equalization (SC-FDE) and orthogonal frequency-division multiplexing (OFDM) have given way to emerging orthogonal time-frequency space (OTFS) and affine frequency-division multiplexing (AFDM). These approaches exploit specific channel structures to diagonalize or sparsify the effective channel, thereby enabling low-complexity detection. However, their reliance on these structures significantly limits their robustness in dynamic, real-world environments. To address these challenges, this paper studies a random multiplexing technique that is decoupled from the physical channels, enabling its application to arbitrary norm-bounded and spectrally convergent channel matrices. Random multiplexing achieves statistical fading-channel ergodicity for transmitted signals by constructing an equivalent input-isotropic channel matrix in the random transform domain. It guarantees the asymptotic replica MAP bit-error rate (BER) optimality of AMP-type detectors for linear systems with arbitrary norm-bounded, spectrally convergent channel matrices and signaling configurations, under the unique fixed point assumption. A low-complexity cross-domain memory AMP (CD-MAMP) detector is considered, leveraging the sparsity of the time-domain channel and the randomness of the equivalent channel. Optimal power allocations are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random multiplexing systems. The optimal coding principle and replica constrained-capacity optimality of CD-MAMP detector are investigated for random multiplexing systems. Additionally, the versatility of random multiplexing in diverse wireless applications is explored."}
{"id": "2512.24428", "pdf": "https://arxiv.org/pdf/2512.24428", "abs": "https://arxiv.org/abs/2512.24428", "authors": ["Qian Wang", "Omar Abdellall", "Tony Gao", "Xiatao Sun", "Daniel Rakita"], "title": "Subsecond 3D Mesh Generation for Robot Manipulation", "categories": ["cs.RO"], "comment": "In submission", "summary": "3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning."}
{"id": "2512.24564", "pdf": "https://arxiv.org/pdf/2512.24564", "abs": "https://arxiv.org/abs/2512.24564", "authors": ["Shunbo Jia", "Caizhi Liao"], "title": "CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability."}
{"id": "2512.24470", "pdf": "https://arxiv.org/pdf/2512.24470", "abs": "https://arxiv.org/abs/2512.24470", "authors": ["Kim Alexander Christensen", "Andreas Gudahl Tufte", "Alexey Gusev", "Rohan Sinha", "Milan Ganai", "Ole Andreas Alsos", "Marco Pavoned", "Martin Steinert"], "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models", "categories": ["cs.RO", "cs.AI"], "comment": "17 pages without bibliography or appendix. The main paper has 16 figures", "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning."}
{"id": "2512.24619", "pdf": "https://arxiv.org/pdf/2512.24619", "abs": "https://arxiv.org/abs/2512.24619", "authors": ["Yunian Pan", "Jun Li", "Lifan Xu", "Shunqiao Sun", "Quanyan Zhu"], "title": "Decentralized No-Regret Frequency-Time Scheduling for FMCW Radar Interference Avoidance", "categories": ["eess.SY", "eess.SP"], "comment": null, "summary": "Automotive FMCW radars are indispensable to modern ADAS and autonomous-driving systems, but their increasing density has intensified the risk of mutual interference. Existing mitigation techniques, including reactive receiver-side suppression, proactive waveform design, and cooperative scheduling, often face limitations in scalability, reliance on side-channel communication, or degradation of range-Doppler resolution. Building on our earlier work on decentralized Frequency-Domain No-Regret hopping, this paper introduces a unified time-frequency game-theoretic framework that enables radars to adapt across both spectral and temporal resources. We formulate the interference-avoidance problem as a repeated anti-coordination game, in which each radar autonomously updates a mixed strategy over frequency subbands and chirp-level time offsets using regret-minimization dynamics. We show that the proposed Time-Frequency No-Regret Hopping algorithm achieves vanishing external and swap regret, and that the induced empirical play converges to an $\\varepsilon$-coarse correlated equilibrium or a correlated equilibrium. Theoretical analysis provides regret bounds in the joint domain, revealing how temporal adaptation implicitly regularizes frequency selection and enhances robustness against asynchronous interference. Numerical experiments with multi-radar scenarios demonstrate substantial improvements in SINR, collision rate, and range-Doppler quality compared with time-frequency random hopping and centralized Nash-based benchmarks."}
{"id": "2512.24550", "pdf": "https://arxiv.org/pdf/2512.24550", "abs": "https://arxiv.org/abs/2512.24550", "authors": ["Tomoya Yamanokuchi", "Alberto Bacchin", "Emilio Olivastri", "Ryotaro Arifuku", "Takamitsu Matsubara", "Emanuele Menegatti"], "title": "DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass", "categories": ["cs.RO"], "comment": "48 pages", "summary": "In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/"}
{"id": "2512.24679", "pdf": "https://arxiv.org/pdf/2512.24679", "abs": "https://arxiv.org/abs/2512.24679", "authors": ["Pengcheng Xia", "Yixiang Huang", "Chengjin Qin", "Chengliang Liu"], "title": "Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions", "categories": ["cs.AI", "eess.SP"], "comment": "21 pages, 8 figures", "summary": "Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG."}
{"id": "2512.24638", "pdf": "https://arxiv.org/pdf/2512.24638", "abs": "https://arxiv.org/abs/2512.24638", "authors": ["Qingda Hu", "Ziheng Qiu", "Zijun Xu", "Kaizhao Zhang", "Xizhou Bu", "Zuolei Sun", "Bo Zhang", "Jieru Zhao", "Zhongxue Gan", "Wenchao Ding"], "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding", "categories": ["cs.RO"], "comment": null, "summary": "State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/"}
{"id": "2512.24773", "pdf": "https://arxiv.org/pdf/2512.24773", "abs": "https://arxiv.org/abs/2512.24773", "authors": ["Anas K. Saeed", "Mahmoud M. Salim", "Ali Arshad Nasir", "Ali H. Muqaibel"], "title": "Throughput Optimization in UAV-Mounted RIS under Jittering and Imperfect CSI via DRL", "categories": ["cs.IT", "eess.SP"], "comment": null, "summary": "Reconfigurable intelligent surfaces (RISs) mounted on unmanned aerial vehicles (UAVs) can reshape wireless propagation on-demand. However, their performance is sensitive to UAV jitter and cascaded channel uncertainty. This paper investigates a downlink multiple-input single-output UAV-mounted RIS system in which a ground multiple-antenna base station (BS) serves multiple single-antenna users under practical impairments. Our goal is to maximize the expected throughput under stochastic three-dimensional UAV jitter and imperfect cascaded channel state information (CSI) based only on the available channel estimates. This leads to a stochastic nonconvex optimization problem subject to a BS transmit power constraint and strict unit-modulus constraints on all RIS elements. To address this problem, we design a model-free deep reinforcement learning (DRL) framework with a contextual bandit formulation. A differentiable feasibility layer is utilized to map continuous actions to feasible solutions, while the reward is a Monte Carlo estimate of the expected throughput. We instantiate this framework with constrained variants of deep deterministic policy gradient (DDPG) and twin delayed deep deterministic policy gradient (TD3) that do not use target networks. Simulations show that the proposed algorithms yield higher throughput than conventional alternating optimization-based weighted minimum mean-square error (AO-WMMSE) baselines under severe jitter and low CSI quality. Across different scenarios, the proposed methods achieve performance that is either comparable to or slightly below the AO-WMMSE benchmark, based on sample average approximation (SAA) with a relative gap ranging from 0-12%. Moreover, the proposed DRL controllers achieve online inference times of 0.6 ms per decision versus roughly 370-550 ms for AO-WMMSE solvers."}
{"id": "2512.24651", "pdf": "https://arxiv.org/pdf/2512.24651", "abs": "https://arxiv.org/abs/2512.24651", "authors": ["Yury Kolomeytsev", "Dmitry Golembiovsky"], "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "22 pages, 4 figures", "summary": "Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings."}
{"id": "2512.24803", "pdf": "https://arxiv.org/pdf/2512.24803", "abs": "https://arxiv.org/abs/2512.24803", "authors": ["Yuan Gao", "Guangjin Pan", "Zhiyong Zhong", "Zhengyu Jin", "Yichen Hu", "Yifei Jin", "Shugong Xu"], "title": "Sidelink Positioning: Standardization Advancements, Challenges and Opportunities", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "With the integration of cellular networks in vertical industries that demand precise location information, such as vehicle-to-everything (V2X), public safety, and Industrial Internet of Things (IIoT), positioning has become an imperative component for future wireless networks. By exploiting a wider spectrum, multiple antennas and flexible architectures, cellular positioning achieves ever-increasing positioning accuracy. Still, it faces fundamental performance degradation when the distance between user equipment (UE) and the base station (BS) is large or in non-line-of-sight (NLoS) scenarios. To this end, the 3rd generation partnership project (3GPP) Rel-18 proposes to standardize sidelink (SL) positioning, which provides unique opportunities to extend the positioning coverage via direct positioning signaling between UEs. Despite the standardization advancements, the capability of SL positioning is controversial, especially how much spectrum is required to achieve the positioning accuracy defined in 3GPP. To this end, this article summarizes the latest standardization advancements of 3GPP on SL positioning comprehensively, covering a) network architecture; b) positioning types; and c) performance requirements. The capability of SL positioning using various positioning methods under different imperfect factors is evaluated and discussed in-depth. Finally, according to the evolution of SL in 3GPP Rel-19, we discuss the possible research directions and challenges of SL positioning."}
{"id": "2512.24653", "pdf": "https://arxiv.org/pdf/2512.24653", "abs": "https://arxiv.org/abs/2512.24653", "authors": ["Chengkai Hou", "Kun Wu", "Jiaming Liu", "Zhengping Che", "Di Wu", "Fei Liao", "Guangrun Li", "Jingyang He", "Qiuxuan Feng", "Zhao Jin", "Chenyang Gu", "Zhuoyang Liu", "Nuowei Han", "Xiangju Mi", "Yaoxu Lv", "Yankai Fu", "Gaole Dai", "Langzhe Gu", "Tao Li", "Yuheng Zhang", "Yixue Zhang", "Xinhua Wang", "Shichao Fan", "Meng Li", "Zhen Zhao", "Ning Liu", "Zhiyuan Xu", "Pei Ren", "Junjie Ji", "Haonan Liu", "Kuan Cheng", "Shanghang Zhang", "Jian Tang"], "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence", "categories": ["cs.RO"], "comment": null, "summary": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions."}
{"id": "2512.24657", "pdf": "https://arxiv.org/pdf/2512.24657", "abs": "https://arxiv.org/abs/2512.24657", "authors": ["Sungjae Min", "Hyungjoo Kim", "David Hyunchul Shim"], "title": "Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids", "categories": ["cs.RO"], "comment": "Preprint", "summary": "Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations."}
{"id": "2512.24673", "pdf": "https://arxiv.org/pdf/2512.24673", "abs": "https://arxiv.org/abs/2512.24673", "authors": ["Yongsheng Zhao", "Lei Zhao", "Baoping Cheng", "Gongxin Yao", "Xuanzhang Wen", "Han Gao"], "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models."}
{"id": "2512.24680", "pdf": "https://arxiv.org/pdf/2512.24680", "abs": "https://arxiv.org/abs/2512.24680", "authors": ["Kangjie Zhou", "Zhaoyang Li", "Han Gao", "Yao Su", "Hangxin Liu", "Junzhi Yu", "Chang Liu"], "title": "ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments", "categories": ["cs.RO"], "comment": "17 pages, 12 figures, accepted to IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency."}
{"id": "2512.24688", "pdf": "https://arxiv.org/pdf/2512.24688", "abs": "https://arxiv.org/abs/2512.24688", "authors": ["Zhehan Li", "Zheng Wang", "Jiadong Lu", "Qi Liu", "Zhiren Xun", "Yue Wang", "Fei Gao", "Chao Xu", "Yanjun Cao"], "title": "CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System", "categories": ["cs.RO"], "comment": "21 pages, 23 figures, journal", "summary": "Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817° in real-world datasets."}
{"id": "2512.24698", "pdf": "https://arxiv.org/pdf/2512.24698", "abs": "https://arxiv.org/abs/2512.24698", "authors": ["Dongyun Kang", "Min-Gyu Kim", "Tae-Gyu Song", "Hajun Kim", "Sehoon Ha", "Hae-Won Park"], "title": "Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer", "categories": ["cs.RO"], "comment": "8 pages. Submitted to the IEEE for possible publication", "summary": "Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot."}
{"id": "2512.24712", "pdf": "https://arxiv.org/pdf/2512.24712", "abs": "https://arxiv.org/abs/2512.24712", "authors": ["Qian Cheng", "Weitao Zhou", "Cheng Jing", "Nanshan Deng", "Junze Wen", "Zhaoyang Liu", "Kun Jiang", "Diange Yang"], "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving."}
{"id": "2512.24740", "pdf": "https://arxiv.org/pdf/2512.24740", "abs": "https://arxiv.org/abs/2512.24740", "authors": ["Yichen Liu", "Kesava Viswanadha", "Zhongyu Li", "Nelson Lojo", "Kristofer S. J. Pister"], "title": "Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints", "categories": ["cs.RO", "eess.SY"], "comment": "9 pages, 10 figures", "summary": "An important function of autonomous microrobots is the ability to perform robust movement over terrain. This paper explores an edge ML approach to microrobot locomotion, allowing for on-device, lower latency control under compute, memory, and power constraints. This paper explores the locomotion of a sub-centimeter quadrupedal microrobot via reinforcement learning (RL) and deploys the resulting controller on an ultra-small system-on-chip (SoC), SC$μ$M-3C, featuring an ARM Cortex-M0 microcontroller running at 5 MHz. We train a compact FP32 multilayer perceptron (MLP) policy with two hidden layers ($[128, 64]$) in a massively parallel GPU simulation and enhance robustness by utilizing domain randomization over simulation parameters. We then study integer (Int8) quantization (per-tensor and per-feature) to allow for higher inference update rates on our resource-limited hardware, and we connect hardware power budgets to achievable update frequency via a cycles-per-update model for inference on our Cortex-M0. We propose a resource-aware gait scheduling viewpoint: given a device power budget, we can select the gait mode (trot/intermediate/gallop) that maximizes expected RL reward at a corresponding feasible update frequency. Finally, we deploy our MLP policy on a real-world large-scale robot on uneven terrain, qualitatively noting that domain-randomized training can improve out-of-distribution stability. We do not claim real-world large-robot empirical zero-shot transfer in this work."}
{"id": "2512.24766", "pdf": "https://arxiv.org/pdf/2512.24766", "abs": "https://arxiv.org/abs/2512.24766", "authors": ["Karthik Dharmarajan", "Wenlong Huang", "Jiajun Wu", "Li Fei-Fei", "Ruohan Zhang"], "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://dream2flow.github.io/", "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/."}
{"id": "2512.24845", "pdf": "https://arxiv.org/pdf/2512.24845", "abs": "https://arxiv.org/abs/2512.24845", "authors": ["Qiuyi Gu", "Yuze Sheng", "Jincheng Yu", "Jiahao Tang", "Xiaolong Shan", "Zhaoyang Shen", "Tinghao Yi", "Xiaodan Liang", "Xinlei Chen", "Yu Wang"], "title": "ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects."}
{"id": "2512.24974", "pdf": "https://arxiv.org/pdf/2512.24974", "abs": "https://arxiv.org/abs/2512.24974", "authors": ["Yunxi Tang", "Tianqi Yang", "Jing Huang", "Xiangyu Chu", "Kwok Wai Samuel Au"], "title": "Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments", "categories": ["cs.RO"], "comment": null, "summary": "Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks."}
{"id": "2512.25072", "pdf": "https://arxiv.org/pdf/2512.25072", "abs": "https://arxiv.org/abs/2512.25072", "authors": ["Haozhi Qi", "Yen-Jen Wang", "Toru Lin", "Brent Yi", "Yi Ma", "Koushil Sreenath", "Jitendra Malik"], "title": "Coordinated Humanoid Manipulation with Choice Policies", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Code and Website: https://choice-policy.github.io/", "summary": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments."}
{"id": "2512.23739", "pdf": "https://arxiv.org/pdf/2512.23739", "abs": "https://arxiv.org/abs/2512.23739", "authors": ["Michaela Levi-Richter", "Reuth Mirsky", "Oren Glickman"], "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": "Poster presented at the Israeli Seminar on Computational Linguistics 2025", "summary": "``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots' cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.\n  Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants' kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.\n  To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.\n  We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments."}
{"id": "2512.23786", "pdf": "https://arxiv.org/pdf/2512.23786", "abs": "https://arxiv.org/abs/2512.23786", "authors": ["Ankan Aich", "Yangming Lee"], "title": "Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting."}
{"id": "2512.24111", "pdf": "https://arxiv.org/pdf/2512.24111", "abs": "https://arxiv.org/abs/2512.24111", "authors": ["Yongtao Chen", "Yanbo Wang", "Wentao Zhao", "Guole Shen", "Tianchen Deng", "Jingchuan Wang"], "title": "Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Monocular Depth Estimation (MDE) serves as a core perception module in autonomous driving systems, but it remains highly susceptible to adversarial attacks. Errors in depth estimation may propagate through downstream decision making and influence overall traffic safety. Existing physical attacks primarily rely on texture-based patches, which impose strict placement constraints and exhibit limited realism, thereby reducing their effectiveness in complex driving environments. To overcome these limitations, this work introduces a training-free generative adversarial attack framework that generates naturalistic, scene-consistent adversarial objects via a diffusion-based conditional generation process. The framework incorporates a Salient Region Selection module that identifies regions most influential to MDE and a Jacobian Vector Product Guidance mechanism that steers adversarial gradients toward update directions supported by the pre-trained diffusion model. This formulation enables the generation of physically plausible adversarial objects capable of inducing substantial adversarial depth shifts. Extensive digital and physical experiments demonstrate that our method significantly outperforms existing attacks in effectiveness, stealthiness, and physical deployability, underscoring its strong practical implications for autonomous driving safety assessment."}
{"id": "2512.24281", "pdf": "https://arxiv.org/pdf/2512.24281", "abs": "https://arxiv.org/abs/2512.24281", "authors": ["Spyridon Syntakas", "Kostas Vlachos"], "title": "Safe Sliding Mode Control for Marine Vessels Using High-Order Control Barrier Functions and Fast Projection", "categories": ["eess.SY", "cs.RO", "math.DS"], "comment": null, "summary": "This paper presents a novel safe control framework that integrates Sliding Mode Control (SMC), High-Order Control Barrier Functions (HOCBFs) with state-dependent adaptiveness and a lightweight projection for collision-free navigation of an over-actuated 3-DOF marine surface vessel subjected to strong environmental disturbances (wind, waves, and current). SMC provides robustness to matched disturbances common in marine operations, while HOCBFs enforce forward invariance of obstacle-avoidance constraints. A fast half-space projection method adjusts the SMC control only when needed, preserving robustness and minimizing chattering. The approach is evaluated on a nonlinear marine platform model that includes added mass, hydrodynamic damping, and full thruster allocation. Simulation results show robust navigation, guaranteed obstacle avoidance, and computational efficiency suitable for real-time embedded use. For small marine robots and surface vessels with limited onboard computational resources-where execution speed and computational efficiency are critical-the SMC-HOCBF framework constitutes a strong candidate for safety-critical control."}
{"id": "2512.24321", "pdf": "https://arxiv.org/pdf/2512.24321", "abs": "https://arxiv.org/abs/2512.24321", "authors": ["Nan Jiang", "Zimo He", "Wanhe Yu", "Lexi Pang", "Yunhao Li", "Hongjie Li", "Jieming Cui", "Yuhan Li", "Yizhou Wang", "Yixin Zhu", "Siyuan Huang"], "title": "UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://jnnan.github.io/uniact/", "summary": "A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control."}
{"id": "2512.24377", "pdf": "https://arxiv.org/pdf/2512.24377", "abs": "https://arxiv.org/abs/2512.24377", "authors": ["Brett T. Lopez"], "title": "New Insights into Cascaded Geometric Flight Control: From Performance Guarantees to Practical Pitfalls", "categories": ["eess.SY", "cs.RO"], "comment": "V1", "summary": "We present a new stability proof for cascaded geometric control used by aerial vehicles tracking time-varying position trajectories. Our approach uses sliding variables and a recently proposed quaternion-based sliding controller to demonstrate that exponentially convergent position trajectory tracking is theoretically possible. Notably, our analysis reveals new aspects of the control strategy, including how tracking error in the attitude loop influences the position loop, how model uncertainties affect the closed-loop system, and the practical pitfalls of the control architecture."}
{"id": "2512.24385", "pdf": "https://arxiv.org/pdf/2512.24385", "abs": "https://arxiv.org/abs/2512.24385", "authors": ["Song Wang", "Lingdong Kong", "Xiaolu Liu", "Hao Shi", "Wentong Li", "Jianke Zhu", "Steven C. H. Hoi"], "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint; 38 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-spatial-intelligence", "summary": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment."}
{"id": "2512.24493", "pdf": "https://arxiv.org/pdf/2512.24493", "abs": "https://arxiv.org/abs/2512.24493", "authors": ["Chi Ho Leung", "Philip E. Paré"], "title": "Energy-Aware Bayesian Control Barrier Functions for Physics-Informed Gaussian Process Dynamics", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We study safe control for dynamical systems whose continuous-time dynamics are learned with Gaussian processes (GPs), focusing on mechanical and port-Hamiltonian systems where safety is naturally expressed via energy constraints. The availability of a GP Hamiltonian posterior naturally raises the question of how to systematically exploit this structure to design an energy-aware control barrier function with high-probability safety guarantees. We address this problem by developing a Bayesian-CBF framework and instantiating it with energy-aware Bayesian-CBFs (EB-CBFs) that construct conservative energy-based barriers directly from the Hamiltonian and vector-field posteriors, yielding safety filters that minimally modify a nominal controller while providing probabilistic energy safety guarantees. Numerical simulations on a mass-spring system demonstrate that the proposed EB-CBFs achieve high-probability safety under noisy sampled GP-learned dynamics."}
{"id": "2512.24497", "pdf": "https://arxiv.org/pdf/2512.24497", "abs": "https://arxiv.org/abs/2512.24497", "authors": ["Basile Terver", "Tsung-Yen Yang", "Jean Ponce", "Adrien Bardes", "Yann LeCun"], "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "comment": null, "summary": "A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms."}
{"id": "2512.24829", "pdf": "https://arxiv.org/pdf/2512.24829", "abs": "https://arxiv.org/abs/2512.24829", "authors": ["Emmanuel Fashae", "Michael Burke", "Leimin Tian", "Lingheng Meng", "Pamela Carreno-Medrano"], "title": "Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted to the 2026 ACM/IEEE International Conference on Human-Robot Interaction (HRI '26)", "summary": "Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning."}
{"id": "2512.24838", "pdf": "https://arxiv.org/pdf/2512.24838", "abs": "https://arxiv.org/abs/2512.24838", "authors": ["Md Ahmed Al Muzaddid", "Jordan A. James", "William J. Beksi"], "title": "CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 5 figures, and 3 tables", "summary": "Multiple-object tracking (MOT) in agricultural environments presents major challenges due to repetitive patterns, similar object appearances, sudden illumination changes, and frequent occlusions. Contemporary trackers in this domain rely on the motion of objects rather than appearance for association. Nevertheless, they struggle to maintain object identities when targets undergo frequent and strong occlusions. The high similarity of object appearances makes integrating appearance-based association nontrivial for agricultural scenarios. To solve this problem we propose CropTrack, a novel MOT framework based on the combination of appearance and motion information. CropTrack integrates a reranking-enhanced appearance association, a one-to-many association with appearance-based conflict resolution strategy, and an exponential moving average prototype feature bank to improve appearance-based association. Evaluated on publicly available agricultural MOT datasets, CropTrack demonstrates consistent identity preservation, outperforming traditional motion-based tracking methods. Compared to the state of the art, CropTrack achieves significant gains in identification F1 and association accuracy scores with a lower number of identity switches."}
{"id": "2512.24851", "pdf": "https://arxiv.org/pdf/2512.24851", "abs": "https://arxiv.org/abs/2512.24851", "authors": ["Xunyi Zhao", "Gengze Zhou", "Qi Wu"], "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents."}
{"id": "2512.24955", "pdf": "https://arxiv.org/pdf/2512.24955", "abs": "https://arxiv.org/abs/2512.24955", "authors": ["Yongwei Zhang", "Yuanzhe Xing", "Quan Quan", "Zhikun She"], "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY"], "comment": null, "summary": "Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available."}
{"id": "2512.24985", "pdf": "https://arxiv.org/pdf/2512.24985", "abs": "https://arxiv.org/abs/2512.24985", "authors": ["Yohan Park", "Hyunwoo Ha", "Wonjun Jo", "Tae-Hyun Oh"], "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance."}
{"id": "2512.25075", "pdf": "https://arxiv.org/pdf/2512.25075", "abs": "https://arxiv.org/abs/2512.25075", "authors": ["Zhening Huang", "Hyeonho Jeong", "Xuelin Chen", "Yulia Gryaditskaya", "Tuanfeng Y. Wang", "Joan Lasenby", "Chun-Hao Huang"], "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot", "summary": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot"}
