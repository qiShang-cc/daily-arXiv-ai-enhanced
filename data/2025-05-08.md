<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [cs.CV](#cs.CV) [Total: 68]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.GR](#cs.GR) [Total: 5]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Device-Free Localization Using Multi-Link MIMO Channels in Distributed Antenna Networks](https://arxiv.org/abs/2505.04085)
*Minseok Kim,Gesi Teng,Keita Nishi,Togo Ikegami,Masamune Sato*

Main category: eess.SP

TL;DR: 本文提出了一种基于分布式天线网络的设备无关定位框架，通过多链路MIMO信道的时空多样性提高定位精度，并在室内环境中验证了其亚米级精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对未来6G网络中集成感知与通信的需求，探索一种无需设备即可实现高精度定位的方法。

Method: 利用分布式天线网络中的多链路MIMO信道进行无线电断层成像，并通过贝叶斯优化调整关键参数以提升重建质量。

Result: 在多种场景下实现亚米级定位精度，并在复杂多径环境中表现稳健。

Conclusion: 该方法证实了基于分布式天线网络的设备无关定位系统在高精度、鲁棒性和可扩展性方面的可行性。

Abstract: This paper presented a novel device-free localization (DFL) framework based
on distributed antenna networks (DANs), targeting integrated sensing and
communication (ISAC) in future 6G radio access networks (RANs). In the proposed
approach, radio tomographic imaging (RTI) leverages the spatial and temporal
diversity of multi-link multiple-input multiple-output (MIMO) channels in DANs
to improve localization accuracy. Furthermore, a prototype system was developed
using software-defined radios (SDRs) operating in the sub-6 GHz band, and
comprehensive evaluations were conducted under indoor conditions involving
varying node densities and target types. The results demonstrate that the
framework achieves sub-meter localization accuracy in most scenarios and
maintains robust performance under complex multipath environments. In addition,
the use of Bayesian optimization to fine-tune key parameters, such as sparsity
and path thickness, led to significant improvements in image reconstruction
quality and target estimation accuracy. These results demonstrate the
feasibility and effectiveness of DAN-based DFL systems for accurate, robust,
and scalable localization.

</details>


### [2] [UX-aware Rate Allocation for Real-Time Media](https://arxiv.org/abs/2505.04114)
*Belal Korany,Peerapol Tinnakornsrisuphap,Saadallah Kassir,Prashanth Hande,Hyun Yong Lee,Thomas Stockhammer*

Main category: eess.SP

TL;DR: 论文提出了一种新框架，通过应用服务器与网络的实时信息共享，6G网络可以基于用户体验(QoE)提供保障，而非仅依赖传统网络性能指标(QoS)，显著提升了网络的用户体验容量。


<details>
  <summary>Details</summary>
Motivation: 现有4G/5G网络的QoS框架和拥塞控制算法（如L4S）主要关注网络性能指标（如速率、延迟），但这些指标无法直接保证用户体验（QoE），因为用户体验还受媒体复杂性和编码格式等因素影响。

Method: 提出一种框架，通过应用服务器向网络实时共享用户体验与数据速率的关系，网络利用这些信息优化基于用户体验的效用函数。这种框架利用了应用服务器边缘化的行业趋势，实现了服务器与6G系统的紧密协同。

Result: 仿真结果表明，相比传统速率控制算法，该框架显著提高了网络的用户体验容量（即达到特定用户体验阈值的用户数量）。

Conclusion: 该框架通过将用户体验直接纳入网络优化目标，弥补了传统QoS方法的不足，为6G沉浸式通信提供了更可靠的QoE保障。

Abstract: Immersive communications is a key use case for 6G where applications require
reliable latency-bound media traffic at a certain data rate to deliver an
acceptable User Experience (UX) or Quality-of-Experience (QoE). The
Quality-of-Service (QoS) framework of current cellular systems (4G and 5G) and
prevalent network congestion control algorithms for latency-bound traffic like
L4S typically target network-related Key Performance Indicators (KPIs) such as
data rates and latencies. Network capacity is based on the number of users that
attain these KPIs. However, the UX of an immersive application for a given data
rate and latency is not the same across users, since it depends on other
factors such as the complexity of the media being transmitted and the encoder
format. This implies that guarantees on network KPIs do not necessarily
translate to guarantees on the UX.
  In this paper, we propose a framework in which the communication network can
provide guarantees on the UX. The framework requires application servers to
share real-time information on UX dependency on data rate to the network, which
in turn, uses this information to maximize a UX-based network utility function.
Our framework is motivated by the recent industry trends of increasing
application awareness at the network, and pushing application servers towards
the edge, allowing for tighter coordination between the servers and the 6G
system. Our simulation results show that the proposed framework substantially
improves the UX capacity of the network, which is the number of users above a
certain UX threshold, compared to conventional rate control algorithms.

</details>


### [3] [Energy Efficient RSMA-Based LEO Satellite Communications Assisted by UAV-Mounted BD-Active RIS: A DRL Approach](https://arxiv.org/abs/2505.04148)
*Rahman Saadat Yeganeh,Hamid Behroozi*

Main category: eess.SP

TL;DR: 论文提出了一种将RSMA与无人机载BD-ARIS结合的通信架构，通过优化卫星波束成形、无人机定位等参数提升能效，采用三种DRL算法（TRPO、TD3、A3C）求解问题。结果显示TRPO表现最佳，尤其在高效能和抗干扰性方面，RSMA-BD-ARIS框架显著优于传统RIS方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决非地面网络中6G和大规模物联网应用面临的能效和可扩展性问题，论文提出结合RSMA和BD-ARIS的方案，旨在通过动态优化提升通信性能。

Method: 提出RSMA-BD-ARIS架构，优化卫星波束成形、无人机位置、功率分配和速率分配；使用TRPO、TD3和A3C三种DRL算法求解高维非凸问题；考虑无人机和BD-ARIS的功耗模型。

Result: TRPO在高功率和复杂场景中表现最优（能效和总速率），TD3收敛快但适合中等条件，A3C因高方差表现不稳定。RSMA-BD-ARIS框架显著优于传统RIS设计。

Conclusion: RSMA-BD-ARIS为6G非地面网络提供了高效、可扩展的解决方案，TRPO的鲁棒性尤其适合实际部署，未来可扩展到更复杂场景。

Abstract: This paper proposes an advanced non-terrestrial communication architecture
that integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal
Active Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the
coverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a
group-connected structure to enhance signal amplification and adaptability,
while RSMA enables efficient multi-user access by dividing messages into common
and private components. The system jointly optimizes satellite beamforming, UAV
positioning, power allocation, and rate-splitting ratios to maximize the
overall energy efficiency (EE). To solve the resulting non-convex and
high-dimensional problem, we employ three state-of-the-art deep reinforcement
learning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin
Delayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage
Actor-Critic (A3C). Moreover, realistic models for the power consumption of
both the UAV and the BD-ARIS are considered. Simulation results reveal that
TRPO consistently achieves the best performance in terms of EE and sum rate,
especially under high transmit powers and challenging deployment scenarios. TD3
converges faster and performs competitively in moderate settings, while A3C
suffers from instability due to its high variance. Additionally, the robustness
of each algorithm under channel state information (CSI) uncertainty is
evaluated, confirming TRPO resilience to imperfect observations. Overall, the
proposed RSMA-BD-ARIS framework significantly outperforms conventional
RIS-assisted designs and provides a scalable, energy-efficient solution for 6G
and massive IoT applications in non-terrestrial networks.

</details>


### [4] [The stability of generalized phase retrieval problem over compact groups](https://arxiv.org/abs/2505.04190)
*Tal Amir,Tamir Bendory,Nadav Dym,Dan Edidin*

Main category: eess.SP

TL;DR: 广义相位恢复问题在紧群上试图从Gram矩阵中恢复未知信号的矩阵表示，利用信号的结构先验知识。该问题扩展了经典相位恢复问题，适用于非交换紧群，并证明了在有噪声情况下问题的唯一性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机源于电子显微镜中对生物大分子3D结构的高噪声观测实际需求，同时结合机器学习与信号处理中的现代结构假设。

Method: 该方法将信号建模为几类广泛的结构族（如线性子空间、稀疏表示、ReLU神经网络输出或低维流形），并在紧群框架下分析广义相位恢复问题。

Result: 研究结果表明，在温和条件下，广义相位恢复问题不仅存在唯一解（考虑群对称性），还满足双Lipschitz性质，对噪声和模型失配具有鲁棒性。

Conclusion: 该研究为高噪声环境下的科学问题提供了理论支持，并为开发鲁棒算法奠定了坚实基础。

Abstract: The generalized phase retrieval problem over compact groups aims to recover a
set of matrices, representing an unknown signal, from their associated Gram
matrices, leveraging prior structural knowledge about the signal. This
framework generalizes the classical phase retrieval problem, which reconstructs
a signal from the magnitudes of its Fourier transform, to a richer setting
involving non-abelian compact groups. In this broader context, the unknown
phases in Fourier space are replaced by unknown orthogonal matrices that arise
from the action of a compact group on a finite-dimensional vector space. This
problem is primarily motivated by advances in electron microscopy to
determining the 3D structure of biological macromolecules from highly noisy
observations. To capture realistic assumptions from machine learning and signal
processing, we model the signal as belonging to one of several broad structural
families: a generic linear subspace, a sparse representation in a generic
basis, the output of a generic ReLU neural network, or a generic
low-dimensional manifold. Our main result shows that, under mild conditions,
the generalized phase retrieval problem not only admits a unique solution (up
to inherent group symmetries), but also satisfies a bi-Lipschitz property. This
implies robustness to both noise and model mismatch, an essential requirement
for practical use, especially when measurements are severely corrupted by
noise. These findings provide theoretical support for a wide class of
scientific problems under modern structural assumptions, and they offer strong
foundations for developing robust algorithms in high-noise regimes.

</details>


### [5] [Model-based learning for joint channel estimationand hybrid MIMO precoding](https://arxiv.org/abs/2505.04255)
*Nay Klaimi,Amira Bedoui,Clément Elvira,Philippe Mary,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 该论文提出了一种基于模型的全端架构，用于联合信道估计和混合预编码，适用于具有硬件损伤的现实系统。


<details>
  <summary>Details</summary>
Motivation: 混合预编码是大规模MIMO系统成本效益的关键，但在多用户联合优化及信道知识不足、硬件损伤的现实条件下存在挑战。

Method: 采用模型驱动的轻量级神经网络，通过展开匹配追踪（信道估计）和展开投影梯度上升（预编码）实现端到端学习。

Result: 在现实合成信道上的实验验证了该方法的有效性。

Conclusion: 所提方法显著提升了硬件损伤环境下的混合预编码性能，且具有参数少、可解释性强等优势。

Abstract: Hybrid precoding is a key ingredient of cost-effective massive multiple-input
multiple-output transceivers. However, setting jointly digital and analog
precoders to optimally serve multiple users is a difficult optimization
problem. Moreover, it relies heavily on precise knowledge of the channels,
which is difficult to obtain, especially when considering realistic systems
comprising hardware impairments. In this paper, a joint channel estimation and
hybrid precoding method is proposed, which consists in an end-to-end
architecture taking received pilots as inputs and outputting precoders. The
resulting neural network is fully model-based, making it lightweight and
interpretable with very few learnable parameters. The channel estimation step
is performed using the unfolded matching pursuit algorithm, accounting for
imperfect knowledge of the antenna system, while the precoding step is done via
unfolded projected gradient ascent. The great potential of the proposed method
is empirically demonstrated on realistic synthetic channels.

</details>


### [6] [Near-Field MIMO Channel Acquisition: Geometry-Aided Feedback and Transmission Design](https://arxiv.org/abs/2505.04305)
*Shima Eslami,Bikshapathi Gouda,Antti Tölli*

Main category: eess.SP

TL;DR: 该论文提出了一种新的近场视距MIMO信道参数化方法，通过两个出发角和相对旋转角简化信道获取，并设计了低开销的多用户CSI获取方案。通过两阶段预编码和流分配策略，提高了系统鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 近场视距MIMO系统中，传统信道参数化需要精细的距离网格，导致复杂度高。此外，多用户场景下上行链路训练开销大。本文旨在通过简化参数化和优化训练策略，降低开销并提升性能。

Method: 引入双出发角和相对旋转角的紧凑参数化；采用固定下行链路训练集减少开销；设计两阶段预编码和双向OTA训练以提高鲁棒性；提出两步流分配策略优化计算效率。

Result: 仿真表明，该方法仅需少量OTA迭代即可接近完美CSI性能，显著降低了训练和计算开销。CRB分析证实增大天线间距可提升估计精度。

Conclusion: 论文的方案在近场视距MIMO系统中实现了高效信道获取和鲁棒通信，为多用户场景提供了低开销、高性能的解决方案。

Abstract: Near-field (NF) line-of-sight (LoS) MIMO systems enable efficient channel
state information (CSI) acquisition and precoding by exploiting known antenna
geometries at both the base station (BS) and user equipment (UE). This paper
introduces a compact parameterization of the NF LoS MIMO channel using two
angles of departure (AoDs) and a BS-UE relative rotation angle. The inclusion
of the second AoD removes the need for fine-grained distance grids imposed by
conventional NF channel parametrization. To address the user-specific uplink
pilot overhead in multiuser NF CSI acquisition, we propose a scheme that uses a
fixed, UE-independent set of downlink pilots transmitted from a carefully
selected subset of BS antennas. In dominant LoS conditions, as few as four
pilots suffice, with Cram\'er-Rao bound (CRB) analysis confirming that
increased antenna spacing improves estimation accuracy. Each UE estimates and
quantizes its angular parameters and feeds them back to the BS for
geometry-based CSI reconstruction, eliminating the need for full channel
feedback. To enhance robustness against noise, quantization errors, and
non-line-of-sight (NLoS) components, we introduce a two-stage precoding method.
The initial precoding is computed from estimated LoS CSI and refined through
bidirectional over-the-air (OTA) training. Furthermore, a two-step stream
allocation strategy reduces pilot and computational overhead. Simulations
demonstrate that the proposed approach achieves high data rates with
significantly fewer OTA iterations, approaching the performance of perfect CSI.

</details>


### [7] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TL;DR: 采用启发式集成深度强化学习（DRL）框架优化大规模可重构智能表面（RIS）的离散相移，结合DDQN和贪婪算法（GA）提升控制精度。


<details>
  <summary>Details</summary>
Motivation: 由于大规模RIS的离散相移优化问题具有非凸和非线性特性，传统方法难以有效解决，因此提出一种新的优化框架。

Method: 1. 在双深度Q网络（DDQN）中利用多步累积动作进行RIS列控制；2. 在每一步DRL中集成贪婪算法（GA）进行细粒度元素级优化。

Result: 该方法在小DRL动作空间内有效优化了大规模RIS的相移配置。

Conclusion: 提出的启发式集成DRL框架能高效解决大规模RIS的离散相移优化问题。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>


### [8] [SwinLSTM Autoencoder for Temporal-Spatial-Frequency Domain CSI Compression in Massive MIMO Systems](https://arxiv.org/abs/2505.04432)
*Aakash Saini,Yunchou Xing,Jee Hyun Kim,Amir Ahmadian Tehrani,Wolfgang Gerstacker*

Main category: eess.SP

TL;DR: 论文提出了一种参数轻量、低复杂度的AI/ML模型，通过联合利用时域、空域和频域（TSF）相关性，优化无线系统中的信道状态信息（CSI）反馈。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅利用空频域（SF）相关性，而大规模多输入多输出（mMIMO）系统在低移动场景中表现出强烈的时域相关性，因此需要一种能够联合利用TSF相关性的方法。

Method: 提出了一种新型的、参数轻量、低复杂度的AI/ML循环自动编码器架构，用于在UE端压缩CSI并在NW端重构，同时最小化CSI反馈开销。

Result: 该方法有效利用了TSF域的相关性，减少了参数数量和计算复杂度。

Conclusion: 该研究提供了一种高效且低复杂度的解决方案，适用于需要联合利用时域、空域和频域相关性的CSI反馈场景。

Abstract: This study presents a parameter-light, low-complexity artificial
intelligence/machine learning (AI/ML) model that enhances channel state
information (CSI) feedback in wireless systems by jointly exploiting temporal,
spatial, and frequency (TSF) domain correlations. While traditional frameworks
use autoencoders for CSI compression at the user equipment (UE) and
reconstruction at the network (NW) side in spatial-frequency (SF), massive
multiple-input multiple-output (mMIMO) systems in low mobility scenarios
exhibit strong temporal correlation alongside frequency and spatial
correlations. An autoencoder architecture alone is insufficient to exploit the
TSF domain correlation in CSI; a recurrent element is also required. To address
the vanishing gradients problem, researchers in recent works have proposed
state-of-the-art TSF domain CSI compression architectures that combine
recurrent networks for temporal correlation exploitation with deep pre-trained
autoencoder that handle SF domain CSI compression. However, this approach
increases the number of parameters and computational complexity. To jointly
utilize correlations across the TSF domain, we propose a novel,
parameter-light, low-complexity AI/ML-based recurrent autoencoder architecture
to compress CSI at the UE side and reconstruct it on the NW side while
minimizing CSI feedback overhead.

</details>


### [9] [Phase Shift Information Compression in IRS-aided Wireless Systems: Challenges and Opportunities](https://arxiv.org/abs/2505.04449)
*Xianhua Yu,Dong Li*

Main category: eess.SP

TL;DR: 本文提出了一种基于任务提示和元学习的智能反射面 (IRS) 相位信息压缩框架，以解决大规模或快速变化环境中的相位信息传输开销问题。


<details>
  <summary>Details</summary>
Motivation: IRS在6G网络中具有重要潜力，但相位信息配置的高传输开销成为关键瓶颈，尤其是在大规模或动态环境中。本文旨在通过高效压缩方法解决这一问题。

Method: 提出了一种基于任务感知提示和元学习的即时相位信息压缩框架，能够在多样化条件下实现实时高效传输。

Result: 仿真结果表明，与基线方法相比，该框架在重构精度和鲁棒性上均有显著提升。

Conclusion: 该研究为IRS相位信息压缩提供了高效解决方案，并提出了未来研究的潜在方向。

Abstract: Intelligent reflecting surfaces (IRS) have emerged as a promising technology
for future 6G wireless networks, offering programmable control of the wireless
environment by adjusting the phase shifts of reflecting elements. However, IRS
performance relies on accurately configuring the phase shifts of reflecting
elements, which introduces substantial phase shift information (PSI) delivery
overhead, especially in large-scale or rapidly changing environments. This
paper first introduces the architecture of IRS-assisted systems and highlights
real-world use cases where PSI delivery becomes a critical bottleneck. It then
reviews current PSI compression approaches, outlining their limitations in
adaptability and scalability. To address these gaps, we propose a prompt-guided
PSI compression framework that leverages task-aware prompts and meta-learning
to achieve efficient and real-time PSI delivery under diverse conditions.
Simulation results show improved reconstruction accuracy and robustness
compared to the baseline method. Finally, we discuss open challenges and
outline promising directions for future research.

</details>


### [10] [Meta-Learning Driven Lightweight Phase Shift Compression for IRS-Assisted Wireless Systems](https://arxiv.org/abs/2505.04453)
*Xianhua Yu,Dong Li,Bowen Gu,Xiaoye Jing,Wen Wu,Tuo Wu,Kan Yu*

Main category: eess.SP

TL;DR: MCRNet是一个轻量级的相位信息压缩框架，通过元学习和深度卷积门控模块，显著降低了计算成本和推理延迟，适用于动态IRS辅助无线系统。


<details>
  <summary>Details</summary>
Motivation: 相位信息开销是动态和资源受限条件下实时智能反射面（IRS）辅助无线系统的关键挑战。

Method: 提出了MCRNet框架，结合元学习（MAML）和深度卷积门控模块，实现快速泛化和高效解码。

Result: 在不同压缩比下，MCRNet实现了与前沿基线相当的性能，同时显著降低了模型大小和推理延迟。

Conclusion: MCRNet的非对称架构证明了其在动态IRS辅助无线部署中的实际可扩展性和实时适用性。

Abstract: The phase shift information (PSI) overhead poses a critical challenge to
enabling real-time intelligent reflecting surface (IRS)-assisted wireless
systems, particularly under dynamic and resource-constrained conditions. In
this paper, we propose a lightweight PSI compression framework, termed
meta-learning-driven compression and reconstruction network (MCRNet). By
leveraging a few-shot adaptation strategy via model-agnostic meta-learning
(MAML), MCRNet enables rapid generalization across diverse IRS configurations
with minimal retraining overhead. Furthermore, a novel depthwise convolutional
gating (DWCG) module is incorporated into the decoder to achieve adaptive local
feature modulation with low computational cost, significantly improving
decoding efficiency. Extensive simulations demonstrate that MCRNet achieves
competitive normalized mean square error performance compared to
state-of-the-art baselines across various compression ratios, while
substantially reducing model size and inference latency. These results validate
the effectiveness of the proposed asymmetric architecture and highlight the
practical scalability and real-time applicability of MCRNet for dynamic
IRS-assisted wireless deployments.

</details>


### [11] [Image Steganography For Securing Intellicise Wireless Networks: "Invisible Encryption" Against Eavesdroppers](https://arxiv.org/abs/2505.04467)
*Bizhu Wang,Song Gao,Rui Meng,Haixiao Gao,Xiaodong Xu,Mengying Sun,Chen Dong,Ping Zhang,Dusit Niyato*

Main category: eess.SP

TL;DR: 该论文探讨了在语义通信（SemCom）中集成图像隐写术以增强安全性，评估了现有加密技术并提出了多种JSCC模型和训练策略。


<details>
  <summary>Details</summary>
Motivation: 语义通信易受智能窃听攻击，而图像隐写术能够隐藏秘密信息，为SemCom提供隐形加密。

Method: 论文回顾了SemCom的现有加密技术，评估图像隐写术潜力，并提出多种JSCC模型和训练策略。

Result: 案例研究表明无掩体隐写术SemCom的有效性，并提出了未来研究方向。

Conclusion: 图像隐写术能有效提升SemCom安全性，未来需进一步研究优化其实际应用。

Abstract: As one of the most promising technologies for intellicise (intelligent and
consice) wireless networks, Semantic Communication (SemCom) significantly
improves communication efficiency by extracting, transmitting, and recovering
semantic information, while reducing transmission delay. However, an
integration of communication and artificial intelligence (AI) also exposes
SemCom to security and privacy threats posed by intelligent eavesdroppers. To
address this challenge, image steganography in SemCom embeds secret semantic
features within cover semantic features, allowing intelligent eavesdroppers to
decode only the cover image. This technique offers a form of "invisible
encryption" for SemCom. Motivated by these advancements, this paper conducts a
comprehensive exploration of integrating image steganography into SemCom.
Firstly, we review existing encryption techniques in SemCom and assess the
potential of image steganography in enhancing its security. Secondly, we delve
into various image steganographic paradigms designed to secure SemCom,
encompassing three categories of joint source-channel coding (JSCC) models
tailored for image steganography SemCom, along with multiple training
strategies. Thirdly, we present a case study to illustrate the effectiveness of
coverless steganography SemCom. Finally, we propose future research directions
for image steganography SemCom.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLM）在视觉视角任务中的表现，发现其在场景理解上表现优异，但在空间推理和视角推理上表现较差，建议未来开发中引入明确的几何表示和定制训练协议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估VLM在视觉认知任务（如场景理解、空间推理和视角推理）中的能力，揭示其在复杂视觉任务中的局限性，为未来模型改进提供方向。

Method: 通过设计144个独特的视觉任务（基于人偶与物体的空间配置变化），搭配7个诊断问题，评估了GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct和Claude Sonnet变体的表现。

Result: 模型在场景理解上表现良好，但在空间推理和视角推理上表现显著下降。

Conclusion: 当前VLM在复杂视觉任务中存在明显不足，未来需整合几何表示和定制化训练以提高性能。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [13] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TL;DR: 研究提出了一种基于机器学习（ML）的非接触式、原位蚀刻深度预测框架，通过人工神经网络（ANN）和贝叶斯神经网络（BNN）分别实现高精度预测和不确定性量化，并验证了数字图像比色法（DIC）数据的可行性，为半导体制造提供了实时、低成本的非侵入式监测方案。


<details>
  <summary>Details</summary>
Motivation: 传统的外部分析方法存在时间延迟和污染风险，无法满足半导体制造中对蚀刻深度和绝缘材料厚度的高精度实时监测需求。

Method: 研究分两种场景：1）使用ANN从工艺参数预测平均蚀刻深度；2）引入BNN量化测量变异性，并验证DIC的RGB数据作为输入的可行性。

Result: ANN显著降低了均方误差（MSE），BNN能可靠估计不确定性，DIC数据即使无明确工艺参数也表现优异。

Conclusion: 结合DIC和ML的方案为等离子蚀刻工艺提供了实时、原位、非侵入式监测的有效替代方法，提升了工艺稳定性和制造效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [14] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: 这篇综述分析了针对视频大语言模型（VideoLLMs）的基准与评估方法，讨论了现有视频理解基准的特点、评估协议和局限性，总结了当前最佳模型的性能趋势，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，视频理解技术也取得了显著进展。然而，如何有效评估视频大语言模型的性能仍存在挑战，因此需要对相关基准和评估方法进行全面梳理，以指导未来研究。

Method: 本文通过分析现有视频理解基准（包括封闭集、开放集和专门任务评估）及评估协议，总结了不同方法的优缺点，并探讨了时空理解任务的表现趋势。

Result: 研究发现当前评估框架存在局限性，如多样性不足和缺乏可解释性。同时，总结了领先模型在各基准上的性能表现。

Conclusion: 未来研究方向包括设计更多样化、多模态和注重可解释性的基准，以推动视频大语言模型评估技术的发展，从而促进视频理解领域的进步。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [15] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: 论文综述了现有的视频伪造检测技术，探讨了其在确保监控录像真实性方面的有效性，并强调了开发更强大取证技术的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的普及，视频篡改变得容易，威胁到监控录像的真实性，进而影响司法公正。

Method: 研究分析了多种技术，包括基于压缩的分析、帧复制检测和基于机器学习的方法。

Result: 研究结果表明需要更强大的取证技术以应对不断演变的伪造方法。

Conclusion: 强化视频取证能力对于确保监控录像的可信度和法律证据效力至关重要。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [16] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TL;DR: PointExplainer是一种可解释的诊断策略，用于分析手绘信号以早期诊断帕金森病，通过量化手绘片段对模型决策的贡献，提供直观解释且不影响诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病诊断方法缺乏可解释性，影响临床信任。为解决这一问题，提出了PointExplainer，旨在提供可解释的诊断策略。

Method: PointExplainer包括诊断模块（将手绘信号编码为3D点云）和解释模块（训练可解释的替代模型近似黑盒模型行为），并引入一致性度量确保解释的准确性。

Result: 在两个基准数据集和新构建的数据集上的实验表明，PointExplainer能提供直观解释且诊断性能无下降。

Conclusion: PointExplainer为帕金森病早期诊断提供了可解释且高效的解决方案，有助于提升临床信任。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [17] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Scaled Directed Divergence (SDD)的可解释面部识别方法，通过精细定位相关面部特征来提高深度学习模型的透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的面部识别系统缺乏必要的解释性和透明度，导致用户对其信任度不足。为了解决这一问题，作者提出了一种新的可解释方法。

Method: 利用Scaled Directed Divergence (SDD)这一基于Class Activation Mapping (CAM)的判别定位技术，对与深度学习模型决策相关的面部特征进行精细定位。

Result: 实验表明，SDD CAM相比传统CAM能更准确、更精细地突出相关面部特征，从而提升系统的可解释性。

Conclusion: 通过SDD CAM提供的视觉解释和特征定位，可以显著增强基于深度学习的面部识别系统的透明度和用户信任度。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [18] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAME的图增强多模态编码器，用于从短视频中进行人格分析。该方法结合了视觉、听觉和文本特征，并通过注意力机制实现多模态融合，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 短视频中的人格分析面临视觉、听觉和文本特征的复杂交互挑战，现有方法在多模态融合上表现不足，因此需要一种更鲁棒的模型来有效整合多源特征。

Method: GAME采用图卷积网络（GCN）和卷积神经网络（CNN）的双分支Geo Two-Stream Network处理面部特征，结合BiGRU和注意力机制捕获时序动态。音频和文本特征分别通过VGGish和XLM-Roberta提取，并通过通道注意力融合模块实现多模态集成，最终通过MLP回归头预测人格特质。

Result: 在多个基准测试中，GAME的表现优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME通过多模态特征融合和注意力机制，显著提升了短视频人格分析的性能，为未来研究提供了新方向。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [19] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TL;DR: 该论文提出了一种结合深度拓扑数据分析、自监督学习和迁移学习的先进聚类框架，用于无监督半导体图像聚类，提高缺陷识别和工艺优化能力。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中的图像数据量大且复杂，传统聚类方法难以有效处理高维无标签数据，亟需更高效的解决方案。

Method: 通过深度拓扑数据分析（TDA）捕获拓扑特征，结合自监督学习和迁移学习提取数据表征并增强框架的适应性和扩展性。

Result: 在合成和真实半导体图像数据集上验证了框架的有效性，成功识别了缺陷和工艺变化的聚类。

Conclusion: 该框架为半导体制造等大规模图像数据处理提供了可扩展的解决方案，展现了结合多种技术的潜力。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [20] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TL;DR: 本文提出了一种基于主动推理的视觉注意模型，通过动态优化感官精度来最小化自由能，实现了隐性和显性视觉注意的调控。模型在Posner提示任务和简单目标聚焦任务中表现出与人类相似的行为，如外源性提示和有效提示通常反应更快。


<details>
  <summary>Details</summary>
Motivation: 为了构建一个能够选择性关注相关刺激并过滤干扰的智能体模型，研究者在复杂高维感官输入处理中引入了主动推理框架。

Method: 通过动态优化感官精度，模型结合当前环境信念和感官输入来决定视觉感官精度，从而调控隐性和显性视觉注意的分配。模型在2D视觉数据中测试，分析Posner提示任务和目标聚焦任务中的行为表现。

Result: 结果表明外源性和有效提示通常反应更快，模型还表现出类似抑制返回的行为，即之前注意过的位置在一定延迟后会被抑制。此外，不自主的反射性眼动比自主眼动更快但适应性较差。

Conclusion: 该模型成功模拟了人类视觉注意的典型行为，证明了主动推理框架在注意机制研究中的有效性。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [21] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TL;DR: 论文提出了一种新型的视网膜血管分割方法AttUKAN，结合注意力机制和标签引导的像素对比损失，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜血管分割方法主要关注解码器输出与标签的差异，而忽视了编码器特征级别的细粒度表示，导致了性能瓶颈。

Method: 使用注意力门增强Kolmogorov-Arnold网络的灵敏度，并设计标签引导的像素对比损失以提取更具判别性的特征。

Result: 在多个公开及私有数据集上，AttUKAN的F1和MIoU分数均优于现有11种网络，达到SOTA性能。

Conclusion: AttUKAN通过结合注意力机制和新型损失函数，显著提升了视网膜血管分割的精度和鲁棒性。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [22] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: 该论文提出一个结合CNN和ESPCNN的框架，用于高效处理无人机拍摄的低分辨率基础设施图像，分类正负病害并提升分辨率，从而减少计算成本和误报。


<details>
  <summary>Details</summary>
Motivation: 当前无人机等数据采集平台用于基础设施管理时，图像分辨率低且存在误报问题，传统超分辨率技术计算成本高且效果不佳。

Method: 使用CNN准确分类图像的病害类别（正/负），再用轻量级ESPCNN对正病害图像进行超分辨率处理。

Result: ESPCNN在超分辨率任务中优于双三次插值，且组合框架能有效减少计算成本和误报，视觉检测显示其能捕捉复杂裂纹特征。

Conclusion: 该框架有助于高速公路管理机构精准检测病害并优化资产管理。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [23] [Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges](https://arxiv.org/abs/2505.03991)
*Hao Xu,Arbind Agrahari Baniya,Sam Well,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

TL;DR: 这篇论文综述了视频事件检测在体育分析中的应用，涵盖TAL、AS和PES三种任务，讨论了方法演进、数据集、评估指标及多模态技术，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 体育视频事件检测对提升分析效率、观众参与和转播效果至关重要。现有技术虽进步显著，但仍存在泛化性和鲁棒性等挑战，需系统性总结和方向指引。

Method: 论文通过文献综述，分类梳理技术方法（如CNN、Transformer、多模态融合等）、数据集和评估指标，并分析现有技术的优缺点。

Result: 研究发现多模态、自监督学习和知识蒸馏等技术有效提升了事件检测性能，但泛化性和效率仍是待解决问题。

Conclusion: 体育事件检测需进一步发展通用、高效、鲁棒的框架。未来研究应聚焦多模态融合、跨体育泛化和自监督学习等方向。

Abstract: Video event detection has become an essential component of sports analytics,
enabling automated identification of key moments and enhancing performance
analysis, viewer engagement, and broadcast efficiency. Recent advancements in
deep learning, particularly Convolutional Neural Networks (CNNs) and
Transformers, have significantly improved accuracy and efficiency in Temporal
Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting
(PES). This survey provides a comprehensive overview of these three key tasks,
emphasizing their differences, applications, and the evolution of
methodological approaches. We thoroughly review and categorize existing
datasets and evaluation metrics specifically tailored for sports contexts,
highlighting the strengths and limitations of each. Furthermore, we analyze
state-of-the-art techniques, including multi-modal approaches that integrate
audio and visual information, methods utilizing self-supervised learning and
knowledge distillation, and approaches aimed at generalizing across multiple
sports. Finally, we discuss critical open challenges and outline promising
research directions toward developing more generalized, efficient, and robust
event detection frameworks applicable to diverse sports. This survey serves as
a foundation for future research on efficient, generalizable, and multi-modal
sports event detection.

</details>


### [24] [The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics](https://arxiv.org/abs/2505.04006)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 该论文探讨了视网膜成像技术在疾病早期检测和监测中的应用，尤其是结合人工智能（AI）的发展，推动了“眼组学”（oculomics）这一新兴领域的发展，并提出了当前的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 人类眼睛独特的血管化结构为健康监测提供了窗口，尤其是在早期疾病检测和干预中。随着成像技术的进步和AI的应用，进一步推动了从眼部获取全身健康信息的能力。

Method: 通过回顾视网膜成像技术的演变，分析AI驱动分析的整合需求，以及从传统技术向眼组学的转变，讨论了相关研究进展。

Result: 眼组学作为新兴领域，覆盖了眼部和全身疾病，为无创标记和早期干预提供了新途径，但也面临技术、数据和研究缺口等挑战。

Conclusion: 眼组学具有巨大潜力，但仍需克服技术难题和研究不足，未来应关注多学科合作和数据标准化以推动发展。

Abstract: The unique vascularized anatomy of the human eye, encased in the retina,
provides an opportunity to act as a window for human health. The retinal
structure assists in assessing the early detection, monitoring of disease
progression and intervention for both ocular and non-ocular diseases. The
advancement in imaging technology leveraging Artificial Intelligence has seized
this opportunity to bridge the gap between the eye and human health. This track
paves the way for unveiling systemic health insight from the ocular system and
surrogating non-invasive markers for timely intervention and identification.
The new frontiers of oculomics in ophthalmology cover both ocular and systemic
diseases, and getting more attention to explore them. In this survey paper, we
explore the evolution of retinal imaging techniques, the dire need for the
integration of AI-driven analysis, and the shift of retinal imaging from
classical techniques to oculomics. We also discuss some hurdles that may be
faced in the progression of oculomics, highlighting the research gaps and
future directions.

</details>


### [25] [FoodTrack: Estimating Handheld Food Portions with Egocentric Video](https://arxiv.org/abs/2505.04055)
*Ervin Wang,Yuhao Chen*

Main category: cs.CV

TL;DR: FoodTrack框架通过第一人称视频直接估计食物体积，解决传统方法对手部遮挡和相机角度敏感的问题，实现了7.01%的绝对百分比误差。


<details>
  <summary>Details</summary>
Motivation: 传统食物追踪方法依赖特定摄像头角度、无遮挡图像或手势识别，无法直接测量食物体积。FoodTrack旨在提供一种更准确、适应性强的方法。

Method: 使用第一人称视频，无需依赖进食手势或固定咬合大小假设，直接测量手持食物的体积。

Result: 在手持食物对象上实现了约7.01%的绝对百分比误差，优于之前方法在最佳情况下的16.40%误差。

Conclusion: FoodTrack提供了一种更灵活、准确的食物体积测量方法，适用于实际场景中的食物消费追踪。

Abstract: Accurately tracking food consumption is crucial for nutrition and health
monitoring. Traditional approaches typically require specific camera angles,
non-occluded images, or rely on gesture recognition to estimate intake, making
assumptions about bite size rather than directly measuring food volume. We
propose the FoodTrack framework for tracking and measuring the volume of
hand-held food items using egocentric video which is robust to hand occlusions
and flexible with varying camera and object poses. FoodTrack estimates food
volume directly, without relying on intake gestures or fixed assumptions about
bite size, offering a more accurate and adaptable solution for tracking food
consumption. We achieve absolute percentage loss of approximately 7.01% on a
handheld food object, improving upon a previous approach that achieved a 16.40%
mean absolute percentage error in its best case, under less flexible
conditions.

</details>


### [26] [AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding](https://arxiv.org/abs/2505.04058)
*Feng Xiao,Hongbin Xu,Guocan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的2D辅助3D视觉定位框架，通过构建语义-空间场景图来增强关系感知，利用2D预训练属性指导多模态对象编码，并通过图注意力实现跨模态信息融合，实验表明该方法在多个相似干扰物场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D视觉定位的主要挑战在于如何通过自然语言描述的物体在3D场景中进行精确定位，尤其是在存在多个相似干扰物的情况下。当前方法常忽略被提及物体的感知，导致跨模态理解的局限性。

Method: 提出了一个2D辅助的3D视觉定位框架，包含双分支视觉编码器（利用2D预训练属性指导多模态对象编码）和跨模态交互模块（通过图注意力实现关系导向信息融合），并构建语义-空间场景图增强关系感知。

Result: 在主流基准测试中表现优于现有方法，特别是在处理多个相似干扰物时表现突出。

Conclusion: 通过增强对象表示和迭代关系学习，该方法有效建立了3D视觉与引用描述之间的对齐，为复杂场景中的3D视觉定位提供了新思路。

Abstract: 3D visual grounding aims to localize the unique target described by natural
languages in 3D scenes. The significant gap between 3D and language modalities
makes it a notable challenge to distinguish multiple similar objects through
the described spatial relationships. Current methods attempt to achieve
cross-modal understanding in complex scenes via a target-centered learning
mechanism, ignoring the perception of referred objects. We propose a novel
2D-assisted 3D visual grounding framework that constructs semantic-spatial
scene graphs with referred object discrimination for relationship perception.
The framework incorporates a dual-branch visual encoder that utilizes 2D
pre-trained attributes to guide the multi-modal object encoding. Furthermore,
our cross-modal interaction module uses graph attention to facilitate
relationship-oriented information fusion. The enhanced object representation
and iterative relational learning enable the model to establish effective
alignment between 3D vision and referential descriptions. Experimental results
on the popular benchmarks demonstrate our superior performance compared to
state-of-the-art methods, especially in addressing the challenges of multiple
similar distractors.

</details>


### [27] [SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation](https://arxiv.org/abs/2505.04087)
*Zixuan Hu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了一种名为SEVA的新型测试时适应方法，通过单步集成邻域增强技术，在不增加计算负担的情况下有效利用可靠样本提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法通常依赖单轮熵训练，无法充分利用可靠样本，而增强策略虽能释放样本潜力，却因计算成本高难以实时应用。

Method: SEVA通过理论框架探索多轮增强对模型适应的影响，提出优化熵损失上界，将多轮增强效果集成到单步中，并结合选择策略过滤有害样本。

Result: 在多种网络架构和测试场景下的实验表明，SEVA在性能和适应性上表现优越。

Conclusion: SEVA通过高效损失函数和选择策略，提升了可靠样本的潜力并满足实时性要求，具有广泛适应性。

Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against
distribution shifts through rapid model adaptation during inference. While
existing TTA methods often rely on entropy-based unsupervised training and
achieve promising results, the common practice of a single round of entropy
training is typically unable to adequately utilize reliable samples, hindering
adaptation efficiency. In this paper, we discover augmentation strategies can
effectively unleash the potential of reliable samples, but the rapidly growing
computational cost impedes their real-time application. To address this
limitation, we propose a novel TTA approach named Single-step Ensemble of
Vicinal Augmentations (SEVA), which can take advantage of data augmentations
without increasing the computational burden. Specifically, instead of
explicitly utilizing the augmentation strategy to generate new data, SEVA
develops a theoretical framework to explore the impacts of multiple
augmentations on model adaptation and proposes to optimize an upper bound of
the entropy loss to integrate the effects of multiple rounds of augmentation
training into a single step. Furthermore, we discover and verify that using the
upper bound as the loss is more conducive to the selection mechanism, as it can
effectively filter out harmful samples that confuse the model. Combining these
two key advantages, the proposed efficient loss and a complementary selection
strategy can simultaneously boost the potential of reliable samples and meet
the stringent time requirements of TTA. The comprehensive experiments on
various network architectures across challenging testing scenarios demonstrate
impressive performances and the broad adaptability of SEVA. The code will be
publicly available.

</details>


### [28] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/abs/2505.04088)
*Shang Zhang,Huanbin Zhang,Dali Feng,Yujie Cui,Ruoyan Xiong,Cen He*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的Siamese Motion Mamba Tracker (SMMT)，结合双向状态空间模型和自注意力机制，解决TIR目标跟踪中的遮挡、运动模糊和背景干扰问题，并在多个基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: TIR目标跟踪常因目标遮挡、运动模糊和背景干扰等问题导致性能下降，SMMT旨在通过新方法解决这些挑战。

Method: SMMT集成了双向状态空间模型和自注意力机制，引入Motion Mamba模块以提取运动特征并恢复边缘细节，同时采用Siamese参数共享策略减少计算冗余。另设计运动边缘感知回归损失提升跟踪精度。

Result: 在LSOTB-TIR、PTB-TIR、VOT-TIR2015和VOT-TIR2017四个TIR跟踪基准测试中，SMMT表现出优越性能。

Conclusion: SMMT通过结合双向建模和注意力机制，有效提升了TIR目标跟踪的准确性，尤其在处理运动模糊目标时表现突出。

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [29] [MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction](https://arxiv.org/abs/2505.04105)
*Andrew Zhang,Hao Wang,Shuchang Ye,Michael Fulham,Jinman Kim*

Main category: cs.CV

TL;DR: 该论文提出了MAISY方法，通过动态学习空间模式和引入VS-SSIM损失函数，有效解决了现有GAN方法在处理局部特征和像素强度变化时的不足，显著提升了图像去运动伪影的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的图像去运动伪影方法虽然能有效恢复全局结构，但忽视了局部特征和像素强度变化的影响，导致关键病理信息丢失。

Method: MAISY通过结合Segment Anything Model（SAM）动态学习空间模式，并引入VS-SSIM损失函数，自适应地处理高方差区域，从而更精准地校正运动伪影。

Result: 在胸部和头部CT数据集上，MAISY的PSNR提升了40%，SSIM提升了10%，Dice系数提高了16%，显著优于现有方法。

Conclusion: MAISY通过结合动态空间学习和自适应损失函数，显著提升了图像去运动伪影的效果，尤其在保留局部细节方面表现优异。

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging.Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>


### [30] [One2Any: One-Reference 6D Pose Estimation for Any Object](https://arxiv.org/abs/2505.04109)
*Mengya Liu,Siyuan Li,Ajad Chhatkuli,Prune Truong,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: 论文提出了一种名为One2Any的新方法，通过单参考-单查询的RGB-D图像估计6自由度物体姿态，无需3D模型、多视图数据或类别限制。该方法将姿态估计视为编码-解码过程，通过ROPE编码物体形状、方向和纹理，利用U-Net解码模块生成新视图的ROC，实现快速精确的估计。实验表明，该方法在新物体上泛化能力强，计算效率高，性能媲美需要多视图或CAD输入的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的6D物体姿态估计方法依赖完整的3D模型、多视图图像或特定类别的训练数据，难以泛化到未知物体。本文旨在解决这些限制，提出一种仅需单参考-单查询图像的方法。

Method: One2Any方法通过单参考视图生成ROPE（参考物体姿态嵌入），编码物体的形状、方向和纹理。随后，U-Net解码模块利用ROPE为新视图生成参考物体坐标（ROC），实现姿态估计。该方法支持大规模训练，无需3D模型或多视图数据。

Result: 在多个基准数据集上的实验表明，One2Any在新物体上表现出色，计算高效，性能与依赖多视图或CAD输入的方法相当，甚至更优。

Conclusion: One2Any提供了一种简单高效的6D姿态估计框架，无需先验3D模型或多视图数据，泛化能力强，计算成本低，为实际应用提供了可行解决方案。

Abstract: 6D object pose estimation remains challenging for many applications due to
dependencies on complete 3D models, multi-view images, or training limited to
specific object categories. These requirements make generalization to novel
objects difficult for which neither 3D models nor multi-view images may be
available. To address this, we propose a novel method One2Any that estimates
the relative 6-degrees of freedom (DOF) object pose using only a single
reference-single query RGB-D image, without prior knowledge of its 3D model,
multi-view data, or category constraints. We treat object pose estimation as an
encoding-decoding process, first, we obtain a comprehensive Reference Object
Pose Embedding (ROPE) that encodes an object shape, orientation, and texture
from a single reference view. Using this embedding, a U-Net-based pose decoding
module produces Reference Object Coordinate (ROC) for new views, enabling fast
and accurate pose estimation. This simple encoding-decoding framework allows
our model to be trained on any pair-wise pose data, enabling large-scale
training and demonstrating great scalability. Experiments on multiple benchmark
datasets demonstrate that our model generalizes well to novel objects,
achieving state-of-the-art accuracy and robustness even rivaling methods that
require multi-view or CAD inputs, at a fraction of compute.

</details>


### [31] [GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model](https://arxiv.org/abs/2505.04119)
*Zixiang Ai,Zichen Liu,Yuanhang Lei,Zhenyu Cui,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 该论文提出了一种基于几何感知的点云提示方法（GAPrompt），旨在以参数高效的方式提升3D视觉模型在点云数据上的适应能力，避免全微调的高成本问题。通过点提示、点移提示器和提示传播机制，该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于全微调3D视觉模型的计算和存储成本高昂，而现有的参数高效微调方法在捕捉点云几何信息方面表现不足，因此需要一种更高效的几何感知方法。

Method: 提出GAPrompt方法，包括点提示（辅助输入捕捉几何细节）、点移提示器（提取全局形状信息）和提示传播机制（将形状信息融入特征提取过程）。

Result: 在多个基准测试中，GAPrompt显著优于现有PEFT方法，且仅需2.19%的可训练参数即可达到与全微调竞争的性能。

Conclusion: GAPrompt通过几何感知的提示机制，实现了高效的点云任务适应，为3D视觉模型的参数高效微调提供了新思路。

Abstract: Pre-trained 3D vision models have gained significant attention for their
promising performance on point cloud data. However, fully fine-tuning these
models for downstream tasks is computationally expensive and storage-intensive.
Existing parameter-efficient fine-tuning (PEFT) approaches, which focus
primarily on input token prompting, struggle to achieve competitive performance
due to their limited ability to capture the geometric information inherent in
point clouds. To address this challenge, we propose a novel Geometry-Aware
Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the
adaptability of 3D vision models. First, we introduce a Point Prompt that
serves as an auxiliary input alongside the original point cloud, explicitly
guiding the model to capture fine-grained geometric details. Additionally, we
present a Point Shift Prompter designed to extract global shape information
from the point cloud, enabling instance-specific geometric adjustments at the
input level. Moreover, our proposed Prompt Propagation mechanism incorporates
the shape information into the model's feature extraction process, further
strengthening its ability to capture essential geometric characteristics.
Extensive experiments demonstrate that GAPrompt significantly outperforms
state-of-the-art PEFT methods and achieves competitive results compared to full
fine-tuning on various benchmarks, while utilizing only 2.19% of trainable
parameters. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [32] [Vision Graph Prompting via Semantic Low-Rank Decomposition](https://arxiv.org/abs/2505.04121)
*Zixiang Ai,Zichen Liu,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 提出了一种称为Vision Graph Prompting (VGP)的新框架，专为视觉图结构设计，利用语义低秩提示技术优化ViG在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法主要是为基于Transformer的模型设计的，忽略了图结构中节点和边的拓扑关系，难以捕捉复杂的语义信息。

Method: 通过分析发现图中的语义连接组件具有低秩特性，提出了一种语义低秩提示方法，融合全局结构模式和细粒度语义依赖。

Result: 实验表明VGP显著提升了ViG在下游任务中的迁移性能，效果接近全微调，同时保持参数高效性。

Conclusion: VGP为视觉图结构提供了一种有效的参数高效微调方法，填补了现有方法的不足。

Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as
graph structures, providing a more natural way to capture irregular semantic
patterns beyond traditional grid or sequence-based representations. To
efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning
techniques like visual prompting become increasingly essential. However,
existing prompting methods are primarily designed for Transformer-based models,
neglecting the rich topological relationships among nodes and edges in
graph-based representations, limiting their capacity to model complex
semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel
framework tailored for vision graph structures. Our core insight reveals that
semantically connected components in the graph exhibit low-rank properties.
Building on this observation, we introduce a semantic low-rank prompting method
that decomposes low-rank semantic features and integrates them with prompts on
vision graph topologies, capturing both global structural patterns and
fine-grained semantic dependencies. Extensive experiments demonstrate our
method significantly improves ViG's transfer performance on diverse downstream
tasks, achieving results comparable to full fine-tuning while maintaining
parameter efficiency. Our code is available at
https://github.com/zhoujiahuan1991/ICML2025-VGP.

</details>


### [33] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TL;DR: 该论文提出了一个名为R^3-VQA的高质量视频数据集，用于评估复杂社交场景中的社交推理能力，并发现现有的大型视觉语言模型在社交推理上与人类水平仍存在差距，但通过心理理论提示可以提升其表现。


<details>
  <summary>Details</summary>
Motivation: 现有的社交推理任务和数据集过于简单，无法反映现实社交互动的复杂性。为了填补这一空白，研究团队创建了一个包含精细标注社交事件和心理状态的视频数据集，以推动更复杂的社交推理研究。

Method: 研究团队构建了R^3-VQA数据集，包含复杂社交场景中的精细标注（如信念、意图、欲望和情绪）及对应的社交因果链。此外，还设计了三项任务：社交事件理解、心理状态估计和社交因果推理，并评估了当前最先进的大型视觉语言模型的表现。

Result: 实验表明：(1) 大型视觉语言模型在复杂社交场景中的推理能力与人类水平仍有较大差距；(2) 使用心理理论（ToM）提示可以有效提升模型的社交推理表现。

Conclusion: 该研究为社交推理领域提供了高质量的数据集和基准，揭示了现有模型的局限性，并提出心理理论提示作为一种改进方向，为未来研究提供了重要参考。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [34] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TL;DR: 本文提出了一种名为OSLSP的方法，通过利用相似性比例损失和序数类别信息，改进了肌肉组织恢复阶段的自动分类，优于现有的大规模预训练和微调模型。


<details>
  <summary>Details</summary>
Motivation: 传统肌肉组织再生评估依赖人工视觉检查，缺乏定量化和客观性。现有弱监督学习方法（LLP）无法适应肌肉组织特征提取且忽略序数类别信息，因此需要改进。

Method: 提出OSLSP方法，利用相似性比例损失和序数类别注意力机制，通过两袋组合优化特征提取器。

Result: OSLSP模型在骨骼肌恢复阶段分类任务中表现优于大规模预训练和微调模型。

Conclusion: OSLSP通过结合序数类别信息和弱监督学习，为肌肉组织再生评估提供了更高效的自动化解决方案。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [35] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 提出了一种新的端到端文本识别框架，结合ResNet和Vision Transformer，采用Deformable Convolutions、Retrieval-Augmented Generation和CRF等方法，显著提升了OCR性能。


<details>
  <summary>Details</summary>
Motivation: 自然图像中的文本识别是一个重要但具有挑战性的任务，应用于计算机视觉和自然语言处理。当前方法在复杂场景下表现不佳，因此需要更鲁棒的解决方案。

Method: 提出一种端到端框架，结合ResNet和Vision Transformer，使用Deformable Convolutions替换标准卷积层，引入自适应dropout和CRF进行序列建模。

Result: 在六个基准数据集上测试，达到了新的SOTA性能，平均准确率为77.77%。

Conclusion: 该方法在多样且具有挑战性的数据集上表现出色，证明了其鲁棒性和先进性。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [36] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TL;DR: S3D是一个新颖的框架，能够将简单的手绘草图转换为详细的3D模型，利用U-Net架构和风格对齐损失提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 由于草图数据的模糊性和稀疏性，从2D草图生成高质量3D模型具有挑战性。

Method: 采用U-Net编码器-解码器架构生成面部分割掩码，再转化为3D表示，并引入风格对齐损失和草图增强技术。

Result: S3D能够从草图输入生成高质量的3D模型。

Conclusion: S3D框架有效解决了草图到3D模型的转换问题，代码已开源。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [37] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA是首个整合单切片图像、自动提取关键帧视频和手动分割病理视频的多模态模型，模拟病理学家的诊断过程，通过VideoPath-Instruct数据集训练，提升了病理视频分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高质量病理数据稀缺且标注耗时，需通过多模态整合模拟病理学家的自然诊断流程，提升诊断推理能力。

Method: 利用VideoPath-Instruct数据集（4278对视频与诊断链式指令），结合单图像指令数据集的知识迁移，先训练弱标注关键帧视频，再微调手动分割视频。

Result: 模型在病理视频分析中设定了新基准，为支持临床决策的视觉与诊断推理一体化AI系统奠定基础。

Conclusion: VideoPath-LLaVA通过多模态整合和数据高效利用，推动了病理学AI的发展，代码和数据已开源。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [38] [SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios](https://arxiv.org/abs/2505.04201)
*Ning Cheng,Jinan Xu,Jialing Chen,Wenjuan Han*

Main category: cs.CV

TL;DR: 该论文提出了一个自适应的触觉-语言框架SToLa，用于解决触觉感知在智能系统中的模态差异和数据稀缺问题，并在开放式触觉常识推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决触觉感知在智能系统中的两个主要挑战：模态差异和触觉数据的稀缺性与多样性不足，以推进开放式物理世界的常识推理能力。

Method: 引入SToLa框架，采用Mixture of Experts (MoE)动态处理和统一触觉与语言模态，并构建了一个全面的触觉常识推理数据集。

Result: SToLa在PhysiCLeAR基准测试和自建数据集上表现优异，证明了MoE架构在多模态管理和开放式触觉常识推理任务中的有效性。

Conclusion: SToLa框架通过MoE架构成功解决了触觉模态的挑战，显著提升了开放式触觉常识推理任务的性能。

Abstract: This paper explores the challenges of integrating tactile sensing into
intelligent systems for multimodal reasoning, particularly in enabling
commonsense reasoning about the open-ended physical world. We identify two key
challenges: modality discrepancy, where existing large touch-language models
often treat touch as a mere sub-modality of language, and open-ended tactile
data scarcity, where current datasets lack the diversity, open-endness and
complexity needed for reasoning. To overcome these challenges, we introduce
SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of
Experts (MoE) to dynamically process, unify, and manage tactile and language
modalities, capturing their unique characteristics. Crucially, we also present
a comprehensive tactile commonsense reasoning dataset and benchmark featuring
free-form questions and responses, 8 physical properties, 4 interactive
characteristics, and diverse commonsense knowledge. Experiments show SToLa
exhibits competitive performance compared to existing models on the PhysiCLeAR
benchmark and self-constructed datasets, proving the effectiveness of the
Mixture of Experts architecture in multimodal management and the performance
advantages for open-scenario tactile commonsense reasoning tasks.

</details>


### [39] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TL;DR: 该论文提出了一种基于YOLOv8改进的模型，用于坑洼检测及其物理特征分析，通过RGB-D数据集（PothRGBD）和结构改进（DSConv、SimAM、GELU），显著提升了检测精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 坑洼会导致车辆损坏和交通事故，现有的基于2D RGB图像的方法无法准确分析其物理特征，因此需要更精确的检测与分析方法。

Method: 使用Intel RealSense D415深度相机采集RGB-D数据，构建PothRGBD数据集；基于YOLOv8n-seg架构改进模型，引入DSConv、SimAM和GELU模块。

Result: 改进后的模型在精度（93.7%）、召回率（90.4%）和mAP@50（93.8%）上分别提升了1.96%、6.13%和2.07%，并能高精度测量坑洼周长和深度。

Conclusion: 该模型轻量高效，适合实时应用，为基于深度学习的智能交通解决方案提供了有效工具。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [40] [CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models](https://arxiv.org/abs/2505.04214)
*Fabian Wolf,Oliver Tüselmann,Arthur Matei,Lukas Hennies,Christoph Rass,Gernot A. Fink*

Main category: cs.CV

TL;DR: 论文介绍了针对手写文档中键值信息提取的新数据集CM1，用于评估大视觉语言模型（LVLM）的少样本能力，并与传统全页提取模型对比，结果显示在少量训练样本时LVLM表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决手写文档信息提取的挑战，特别是在标注数据稀缺的场景下，利用LVLM的潜力提升少样本学习能力。

Method: 方法包括创建CM1数据集（历史表格），设计三个提取任务（姓名、出生日期等），并在不同训练集规模下对比LVLM与传统模型的性能。

Result: 结果显示，传统全页模型在充足数据时表现优异，但LVLM在少量样本时凭借预训练优势超越传统方法。

Conclusion: 结论是LVLM在少样本场景下具有潜力，为文档数字化提供了新思路。

Abstract: The automatic extraction of key-value information from handwritten documents
is a key challenge in document analysis. A reliable extraction is a
prerequisite for the mass digitization efforts of many archives. Large Vision
Language Models (LVLM) are a promising technology to tackle this problem
especially in scenarios where little annotated training data is available. In
this work, we present a novel dataset specifically designed to evaluate the
few-shot capabilities of LVLMs. The CM1 documents are a historic collection of
forms with handwritten entries created in Europe to administer the Care and
Maintenance program after World War Two. The dataset establishes three
benchmarks on extracting name and birthdate information and, furthermore,
considers different training set sizes. We provide baseline results for two
different LVLMs and compare performances to an established full-page extraction
model. While the traditional full-page model achieves highly competitive
performances, our experiments show that when only a few training samples are
available the considered LVLMs benefit from their size and heavy pretraining
and outperform the classical approach.

</details>


### [41] [A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation](https://arxiv.org/abs/2505.04229)
*Theophilus Aidoo,Till Koebe,Akansh Maurya,Hewan Shrestha,Ingmar Weber*

Main category: cs.CV

TL;DR: 该研究提出了一种弱监督框架，利用3米分辨率的卫星图像和粗略时间标签（假设德国超市和五金店停车场在周六通常满、周日通常空），训练了一个成对比较模型，在大型停车场上的AUC达到0.92。该方法降低了对高分辨率图像的依赖，适合扩展应用于城市流动性分析和资源分配评估。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像标记数据稀缺且成本高，特别是在低收入地区。研究旨在通过弱监督方法解决这一问题，为城市流动性分析和资源分配提供数据支持。

Method: 采用弱监督框架，利用粗略时间标签（基于德国超市和五金店停车场使用模式的假设），通过3米分辨率卫星图像训练成对比较模型。

Result: 模型在大型停车场上的AUC为0.92，验证了方法的有效性。该方法减少了对高分辨率图像的依赖，并展示了在脆弱社区中评估交通模式和资源分配的潜力。

Conclusion: 该弱监督方法为高成本数据场景提供了一种可行的替代方案，有望广泛应用于城市流动性分析和社会资源分配优化，尤其适用于资源有限地区。

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>


### [42] [Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting](https://arxiv.org/abs/2505.04262)
*Feng Yang,Wenliang Qian,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TL;DR: 一种新方法Coupled Score Distillation (CSD)改进了Score Distillation Sampling (SDS)，通过耦合多视角联合分布先验，提升3D生成几何一致性并减少多面伪影。


<details>
  <summary>Details</summary>
Motivation: 现有SDS方法依赖预训练的2D扩散模型生成3D内容时忽略多视角相关性，导致几何不一致和多面伪影。

Method: 提出CSD框架，将优化重新构造为多视角联合优化问题，并直接优化3D高斯泼溅(3D-GS)从随机初始化生成几何一致3D内容。

Result: 定量与定性实验表明，CSD方法能高效生成高质量3D资产，且几何一致性显著提升。

Conclusion: CSD通过耦合多视角先验直接优化3D-GS，实现了更稳定和几何一致的3D内容生成。

Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.

</details>


### [43] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TL;DR: OSGNet通过提取视频中的对象信息和分析镜头运动特征，提出了一种增强的自上而下的视频定位方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在自中心视频定位时忽略了视频的关键特性和查询的细节信息，因此需要更有效的解决方案。

Method: 提出了OSGNet，通过提取对象信息和分析镜头动态特征来丰富视频表示，并增强模态对齐能力。

Result: 在三个数据集上的实验证明了OSGNet在性能上达到了最先进的水平。

Conclusion: OSGNet通过有效结合对象信息和镜头动态特征，显著提升了自中心视频定位任务的性能。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [44] [HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation](https://arxiv.org/abs/2505.04276)
*Yajie Fu,Chaorui Huang,Junwei Li,Hui Kong,Yibin Tian,Huakang Li,Zhiyuan Zhang*

Main category: cs.CV

TL;DR: HDiffTG是一种新颖的3D人体姿态估计方法，结合Transformer、图卷积网络和扩散模型，提升准确性和鲁棒性，同时保持轻量级设计。


<details>
  <summary>Details</summary>
Motivation: 通过融合多种技术来解决3D姿态估计中全局与局部特征的平衡问题，并应对遮挡和复杂场景的挑战。

Method: 整合Transformer捕获全局时空依赖，GCN建模局部骨骼结构，扩散模型进行逐步优化，同时引入轻量级优化和目标函数设计。

Result: 在Human3.6M和MPI-INF-3DHP数据集上达到SOTA性能，尤其在噪声和遮挡环境下表现出色。

Conclusion: HDiffTG在准确性、效率和鲁棒性上均表现卓越，为3D姿态估计提供了新思路。

Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that
integrates Transformer, Graph Convolutional Network (GCN), and diffusion model
into a unified framework. HDiffTG leverages the strengths of these techniques
to significantly improve pose estimation accuracy and robustness while
maintaining a lightweight design. The Transformer captures global
spatiotemporal dependencies, the GCN models local skeletal structures, and the
diffusion model provides step-by-step optimization for fine-tuning, achieving a
complementary balance between global and local features. This integration
enhances the model's ability to handle pose estimation under occlusions and in
complex scenarios. Furthermore, we introduce lightweight optimizations to the
integrated model and refine the objective function design to reduce
computational overhead without compromising performance. Evaluation results on
the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves
state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling
in both accuracy and computational efficiency. Additionally, the model exhibits
exceptional robustness in noisy and occluded environments. Source codes and
models are available at https://github.com/CirceJie/HDiffTG

</details>


### [45] [TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement](https://arxiv.org/abs/2505.04281)
*Yi Li,Zhiyuan Zhang,Jiangnan Xia,Jianghan Cheng,Qilong Wu,Junwei Li,Yibin Tian,Hui Kong*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的两阶段扩散模型TS-Diff，用于增强极低光RAW图像。通过虚拟相机噪声空间预训练和特定目标优化，结合颜色校正器和QID数据集，模型在去噪、泛化和颜色一致性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法无法解决极端光照条件下的噪声和颜色偏移问题，且缺乏针对不同相机的通用性。TS-Diff旨在通过两阶段扩散模型和虚拟相机噪声空间技术提升性能。

Method: 分预训练和对齐两阶段：预训练阶段通过噪声空间构建虚拟相机并学习通用特征；对齐阶段用少量真实数据微调目标相机特征。引入颜色校正器和结构重参数化技术优化部署效率。

Result: TS-Diff在QID、SID和ELD数据集上达到最先进效果，显著提升去噪能力、跨相机泛化性和颜色一致性，且部署效率高。

Conclusion: TS-Diff通过两阶段噪声建模和动态颜色校正，为极低光成像提供了高效、鲁棒的解决方案，代码已开源。

Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing
extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes
noisy images by constructing multiple virtual cameras based on a noise space.
Camera Feature Integration (CFI) modules are then designed to enable the model
to learn generalizable features across diverse virtual cameras. During the
aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is
fine-tuned using a small amount of real RAW data to adapt to the noise
characteristics of specific cameras. A structural reparameterization technique
further simplifies CFI$^T$ for efficient deployment. To address color shifts
during the diffusion process, a color corrector is introduced to ensure color
consistency by dynamically adjusting global color distributions. Additionally,
a novel dataset, QID, is constructed, featuring quantifiable illumination
levels and a wide dynamic range, providing a comprehensive benchmark for
training and evaluation under extreme low-light conditions. Experimental
results demonstrate that TS-Diff achieves state-of-the-art performance on
multiple datasets, including QID, SID, and ELD, excelling in denoising,
generalization, and color consistency across various cameras and illumination
levels. These findings highlight the robustness and versatility of TS-Diff,
making it a practical solution for low-light imaging applications. Source codes
and models are available at https://github.com/CircccleK/TS-Diff

</details>


### [46] [MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition](https://arxiv.org/abs/2505.04306)
*Qiannan Fan,Zhuoyang Li,Jitong Li,Chenyang Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散专家混合模型（MoDE）的遮挡人脸识别方法，通过身份门控网络整合多重建人脸信息，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有遮挡人脸识别算法缺乏对遮挡的先验知识，导致在实际应用中性能不佳，影响了日常生活的便捷性。

Method: 采用身份门控的扩散专家混合模型（MoDE），每个扩散生成专家估计一种可能的完整人脸图像，并通过身份门控网络评估其对身份的贡献。

Result: 在三个公开人脸数据集和两个真实场景数据集上验证了该方法对各种遮挡的先进性能。

Conclusion: MoDE作为一种即插即用模块，显著提升了现有模型的遮挡人脸识别能力。

Abstract: With the continuous impact of epidemics, people have become accustomed to
wearing masks. However, most current occluded face recognition (OFR) algorithms
lack prior knowledge of occlusions, resulting in poor performance when dealing
with occluded faces of varying types and severity in reality. Recognizing
occluded faces is still a significant challenge, which greatly affects the
convenience of people's daily lives. In this paper, we propose an
identity-gated mixture of diffusion experts (MoDE) for OFR. Each
diffusion-based generative expert estimates one possible complete image for
occluded faces. Considering the random sampling process of the diffusion model,
which introduces inevitable differences and variations between the inpainted
faces and the real ones. To ensemble effective information from
multi-reconstructed faces, we introduce an identity-gating network to evaluate
the contribution of each reconstructed face to the identity and adaptively
integrate the predictions in the decision space. Moreover, our MoDE is a
plug-and-play module for most existing face recognition models. Extensive
experiments on three public face datasets and two datasets in the wild validate
our advanced performance for various occlusions in comparison with the
competing methods.

</details>


### [47] [Multi-turn Consistent Image Editing](https://arxiv.org/abs/2505.04320)
*Zijun Zhou,Yingying Deng,Xiangyu He,Weiming Dong,Fan Tang*

Main category: cs.CV

TL;DR: 提出了一种多轮图像编辑框架，通过迭代优化编辑步骤，解决现有方法在模糊用户意图或复杂转换中的不足，显著提升了编辑成功率和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法多为单步操作，难以处理用户模糊意图或复杂编辑需求，导致效果不稳定或不满足预期，需要一种支持迭代优化的方法。

Method: 结合流匹配（flow matching）实现精准图像反演，采用双目标线性二次调节器（LQR）稳定采样，并通过自适应注意力增强机制提升多轮编辑的一致性。

Result: 实验表明，该框架在编辑成功率和视觉质量上显著优于现有方法。

Conclusion: 迭代式多轮编辑框架有效解决了单步方法的局限性，平衡了编辑灵活性与结果稳定性。

Abstract: Many real-world applications, such as interactive photo retouching, artistic
content creation, and product design, require flexible and iterative image
editing. However, existing image editing methods primarily focus on achieving
the desired modifications in a single step, which often struggles with
ambiguous user intent, complex transformations, or the need for progressive
refinements. As a result, these methods frequently produce inconsistent
outcomes or fail to meet user expectations. To address these challenges, we
propose a multi-turn image editing framework that enables users to iteratively
refine their edits, progressively achieving more satisfactory results. Our
approach leverages flow matching for accurate image inversion and a
dual-objective Linear Quadratic Regulators (LQR) for stable sampling,
effectively mitigating error accumulation. Additionally, by analyzing the
layer-wise roles of transformers, we introduce a adaptive attention
highlighting method that enhances editability while preserving multi-turn
coherence. Extensive experiments demonstrate that our framework significantly
improves edit success rates and visual fidelity compared to existing methods.

</details>


### [48] [CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion](https://arxiv.org/abs/2505.04347)
*Yanyu Li,Pencheng Wan,Liang Han,Yaowei Wang,Liqiang Nie,Min Zhang*

Main category: cs.CV

TL;DR: CountDiffusion是一种无需训练的方法，通过两阶段修正注意力图来提升扩散模型生成图像中物体数量的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型在准确生成指定物体数量时面临计算成本高和抽象概念学习的挑战，因此需要一种无需额外训练的方法。

Method: CountDiffusion分为两阶段：首先生成中间去噪结果并计数，随后通过修正注意力图调整物体数量。

Result: 实验表明，该方法显著提升了文本生成图像模型在物体数量准确性上的表现。

Conclusion: CountDiffusion无需额外训练即可有效改善扩散模型生成图像中物体数量的准确性。

Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to
generate images with accurate object quantity is still difficult due to the
high computational cost and the challenge of teaching models the abstract
concept of quantity. In this paper, we propose CountDiffusion, a training-free
framework aiming at generating images with correct object quantity from textual
descriptions. CountDiffusion consists of two stages. In the first stage, an
intermediate denoising result is generated by the diffusion model to predict
the final synthesized image with one-step denoising, and a counting model is
used to count the number of objects in this image. In the second stage, a
correction module is used to correct the object quantity by changing the
attention map of the object with universal guidance. The proposed
CountDiffusion can be plugged into any diffusion-based text-to-image (T2I)
generation models without further training. Experiment results demonstrate the
superiority of our proposed CountDiffusion, which improves the accurate object
quantity generation ability of T2I models by a large margin.

</details>


### [49] [WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing](https://arxiv.org/abs/2505.04369)
*Jie Sun,Heng Liu,Yongzhen Wang,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种基于小波变换的新型去雾框架WDMamba，通过低频恢复和细节增强两阶段处理，结合Mamba块和自对比正则化，显著提升了去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法未能充分利用雾化信息的频域分布特性，作者通过小波分析发现雾信息主要集中于低频，从而提出分阶段处理的框架以优化去雾效果。

Method: 采用两阶段策略：1) 使用Mamba块进行低频恢复（线性复杂度），初步去雾；2) 细节增强阶段补充高频信息。训练中引入自对比正则化，以粗糙恢复结果作为负样本提升模型判别力。

Result: 在公开去雾基准测试中，WDMamba在定性和定量上均优于现有方法，代码已开源。

Conclusion: 通过频域分解与两阶段设计，WDMamba有效结合全局结构重建与细节恢复，为图像去雾任务提供了高效且性能优异的解决方案。

Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior
observed through wavelet transform analysis, which shows that haze-related
information predominantly resides in low-frequency components. Exploiting this
insight, we propose a novel dehazing framework, WDMamba, which decomposes the
image dehazing task into two sequential stages: low-frequency restoration
followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to
effectively capture features specific to each stage of the dehazing process,
resulting in high-quality restored images. Specifically, in the low-frequency
restoration stage, we integrate Mamba blocks to reconstruct global structures
with linear complexity, efficiently removing overall haze and producing a
coarse restored image. Thereafter, the detail enhancement stage reinstates
fine-grained information that may have been overlooked during the previous
phase, culminating in the final dehazed output. Furthermore, to enhance detail
retention and achieve more natural dehazing, we introduce a self-guided
contrastive regularization during network training. By utilizing the coarse
restored output as a hard negative example, our model learns more
discriminative representations, substantially boosting the overall dehazing
performance. Extensive evaluations on public dehazing benchmarks demonstrate
that our method surpasses state-of-the-art approaches both qualitatively and
quantitatively. Code is available at https://github.com/SunJ000/WDMamba.

</details>


### [50] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TL;DR: 该研究探讨了不同规模视觉Transformer在标签噪声下的性能表现，发现大模型（如ViTl32）在准确性和校准上表现更优，而Swin Transformer的鲁棒性较弱。较小的patch尺寸并不总能提升性能，基于信息的主动学习策略仅在中等噪声率下有效。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的卷积神经网络在ImageNet上微调已很成熟，但视觉Transformer在标签噪声下的表现仍缺乏研究。鉴于Transformer的实用性和适应性，该研究旨在评估其在低预算和噪声标签场景下的实用性。

Method: 研究评估了四种视觉Transformer配置（Base和Large，16x16和32x32 patch尺寸）和三种Swin Transformer配置（Tiny、Small、Base），在CIFAR10和CIFAR100数据集上，施加不同标签噪声率，测试分类准确性和校准表现。

Result: 结果表明，较大的ViT模型（如ViTl32）在准确性和校准上均优于小模型，而Swin Transformer在所有噪声水平下表现较弱。较小patch尺寸不一定更好，ViTl16表现不如ViTl32且计算成本更高。主动学习策略仅在中等噪声率下有效，在高噪声率下校准更差。

Conclusion: 研究为在资源受限环境中部署视觉Transformer提供了实用建议：需权衡模型复杂度、标签噪声和计算效率。大模型（如ViTl32）更可靠，而主动学习策略需谨慎使用。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [51] [Label-efficient Single Photon Images Classification via Active Learning](https://arxiv.org/abs/2505.04376)
*Zili Zhang,Ziting Wen,Yiheng Qiang,Hongzhou Dong,Wenle Dong,Xinyang Li,Xiaofan Wang,Xiaoqiang Ren*

Main category: cs.CV

TL;DR: 提出首个单光子图像分类的主动学习框架，通过成像条件感知采样策略和合成增强，以极少标注样本达到高分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注稀疏光子事件的3D重建，而单光子图像的语义解释因标注成本高和低效标注策略而未被充分探索。

Method: 采用成像条件感知采样策略，结合合成增强建模成像条件变化，选择最不确定且对条件敏感的样本进行标注。

Result: 在合成数据上仅用1.5%标注样本达到97%准确率；真实数据上以8%标注样本实现90.63%准确率（比基线高4.51%）。

Conclusion: 主动学习使单光子图像分类性能媲美传统图像，为单光子数据的大规模应用铺平道路。

Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme
environments through quantum-level photon detection technology. Current
research primarily focuses on reconstructing 3D scenes from sparse photon
events, whereas the semantic interpretation of single-photon images remains
underexplored, due to high annotation costs and inefficient labeling
strategies. This paper presents the first active learning framework for
single-photon image classification. The core contribution is an imaging
condition-aware sampling strategy that integrates synthetic augmentation to
model variability across imaging conditions. By identifying samples where the
model is both uncertain and sensitive to these conditions, the proposed method
selectively annotates only the most informative examples. Experiments on both
synthetic and real-world datasets show that our approach outperforms all
baselines and achieves high classification accuracy with significantly fewer
labeled samples. Specifically, our approach achieves 97% accuracy on synthetic
single-photon data using only 1.5% labeled samples. On real-world data, we
maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher
than the best-performing baseline. This illustrates that active learning
enables the same level of classification performance on single-photon images as
on classical images, opening doors to large-scale integration of single-photon
data in real-world applications.

</details>


### [52] [Tetrahedron-Net for Medical Image Registration](https://arxiv.org/abs/2505.04380)
*Jinhai Xiang,Shuai Guo,Qianru Han,Dantong Shi,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: 论文提出了Tetrahedron-Net架构，通过增加一个解码器与原始编码器和解码器交互，提升医学图像配准的表现，实验证明其在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net-like结构虽通过跳连提升表征能力，但未充分挖掘单编码器-解码器架构的交互潜力。本文旨在通过增加一个解码器增强特征复用和交互，提升配准精度。

Method: 提出Tetrahedron-Net，在单编码器基础上增加一个与原始解码器交互的新解码器，形成“四面体”结构。设计三种新解码器变体，验证其泛化性。

Result: 在多个医学图像配准基准测试中表现优异，且能无缝集成到VoxelMorph、ViT-V-Net等流行架构中，带来一致性能提升。

Conclusion: Tetrahedron-Net通过简洁的“四面体”设计有效增强特征交互，为医学图像配准提供了高效且通用的解决方案。

Abstract: Medical image registration plays a vital role in medical image processing.
Extracting expressive representations for medical images is crucial for
improving the registration quality. One common practice for this end is
constructing a convolutional backbone to enable interactions with skip
connections among feature extraction layers. The de facto structure, U-Net-like
networks, has attempted to design skip connections such as nested or full-scale
ones to connect one single encoder and one single decoder to improve its
representation capacity. Despite being effective, it still does not fully
explore interactions with a single encoder and decoder architectures. In this
paper, we embrace this observation and introduce a simple yet effective
alternative strategy to enhance the representations for registrations by
appending one additional decoder. The new decoder is designed to interact with
both the original encoder and decoder. In this way, it not only reuses feature
presentation from corresponding layers in the encoder but also interacts with
the original decoder to corporately give more accurate registration results.
The new architecture is concise yet generalized, with only one encoder and two
decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.
Three instantiations of Tetrahedron-Net are further constructed regarding the
different structures of the appended decoder. Our extensive experiments prove
that superior performance can be obtained on several representative benchmarks
of medical image registration. Finally, such a ``Tetrahedron'' design can also
be easily integrated into popular U-Net-like architectures including
VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.

</details>


### [53] [DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution](https://arxiv.org/abs/2505.04384)
*Ming-Hui Liu,Xiao-Qian Liu,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 提出了一种名为DATA的新框架，通过多解耦对比学习提升开放世界半监督深度伪造溯源任务的泛化能力，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度伪造溯源（DFA）需要区分不同的面部伪造技术，但现有方法易过拟合且忽略共同伪造特征，尤其在开放世界场景下难以处理新类别。

Method: 提出DATA框架，引入‘正交深度伪造基’概念解耦方法特定特征，结合增强记忆机制和对比学习优化新类别发现与分类边界清晰度。

Result: 在OSS-DFA基准测试中表现最优，相比现有方法精度提升2.55%至5.7%。

Conclusion: DATA通过解耦设计与对比学习有效提升深度伪造溯源的泛化能力，尤其在开放世界场景中表现卓越。

Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different
facial manipulation techniques, thereby mitigating the detrimental effects of
forgery content on the social order and personal reputations. However, previous
methods focus only on method-specific clues, which easily lead to overfitting,
while overlooking the crucial role of common forgery features. Additionally,
they struggle to distinguish between uncertain novel classes in more practical
open-world scenarios. To address these issues, in this paper we propose an
innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to
enhance the generalization ability on novel classes for the open-world
semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all
generation techniques can be abstracted into a similar architecture, DATA
defines the concept of 'Orthonormal Deepfake Basis' for the first time and
utilizes it to disentangle method-specific features, thereby reducing the
overfitting on forgery-irrelevant information. Furthermore, an augmented-memory
mechanism is designed to assist in novel class discovery and contrastive
learning, which aims to obtain clear class boundaries for the novel classes
through instance-level disentanglements. Additionally, to enhance the
standardization and discrimination of features, DATA uses bases contrastive
loss and center contrastive loss as auxiliaries for the aforementioned modules.
Extensive experimental evaluations show that DATA achieves state-of-the-art
performance on the OSS-DFA benchmark, e.g., there are notable accuracy
improvements in 2.55% / 5.7% under different settings, compared with the
existing methods.

</details>


### [54] [Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle](https://arxiv.org/abs/2505.04392)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TL;DR: 提出了一种通过视觉跟踪前车来检测道路异常的新方法，适用于低能见度或密集交通场景，可预测性地检测多种道路异常，并通过补偿相机俯仰旋转提高准确性。实验证明该方法在实时性和可靠性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的道路异常检测方法多依赖直接视觉检测，但在低能见度或前车遮挡情况下效果不佳。本文旨在通过跟踪前车的运动来预测性地检测道路异常，以提升驾驶安全性和舒适性。

Method: 通过视觉跟踪前车运动，结合迭代鲁棒估计器补偿相机俯仰旋转的干扰，实现道路异常的预测性检测。

Result: 实验表明，即使在复杂路况下，该方法也能可靠地远距离检测道路异常，并在标准硬件上实现实时运行。

Conclusion: 该方法有效解决了低能见度和遮挡条件下的道路异常检测问题，具有实时性和鲁棒性，适用于自动驾驶或车辆底盘预配置场景。

Abstract: A novel approach to detect road surface anomalies by visual tracking of a
preceding vehicle is proposed. The method is versatile, predicting any kind of
road anomalies, such as potholes, bumps, debris, etc., unlike direct
observation methods that rely on training visual detectors of those cases. The
method operates in low visibility conditions or in dense traffic where the
anomaly is occluded by a preceding vehicle. Anomalies are detected
predictively, i.e., before a vehicle encounters them, which allows to
pre-configure low-level vehicle systems (such as chassis) or to plan an
avoidance maneuver in case of autonomous driving. A challenge is that the
signal coming from camera-based tracking of a preceding vehicle may be weak and
disturbed by camera ego motion due to vibrations affecting the ego vehicle.
Therefore, we propose an efficient method to compensate camera pitch rotation
by an iterative robust estimator. Our experiments on both controlled setup and
normal traffic conditions show that road anomalies can be detected reliably at
a distance even in challenging cases where the ego vehicle traverses imperfect
road surfaces. The method is effective and performs in real time on standard
consumer hardware.

</details>


### [55] [SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer](https://arxiv.org/abs/2505.04394)
*Young-Hu Park,Rae-Hong Park,Hyung-Min Park*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视觉语音编码器SwinLip，结合Swin Transformer的分层结构和窗口自注意力机制，显著降低了计算复杂度并提升了唇读性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ResNet的唇读方法计算复杂度高，难以高效捕获时空信息，且在多模态任务中可能拖慢整体网络速度。本文旨在通过改进的视觉语音编码器克服这些限制。

Method: 采用Swin Transformer的分层结构和窗口自注意力机制，设计轻量级架构SwinLip，整合改进的Conformer时序嵌入与空间嵌入，优化计算效率。

Result: 在英语LRW和汉语LRW-1000数据集上验证了性能提升与推理速度优化，尤其在汉语数据集上以更低计算量达到SOTA。

Conclusion: SwinLip通过高效建模时空特征，为唇读任务提供了计算友好且性能优越的解决方案。

Abstract: This paper presents an efficient visual speech encoder for lip reading. While
most recent lip reading studies have been based on the ResNet architecture and
have achieved significant success, they are not sufficiently suitable for
efficiently capturing lip reading features due to high computational complexity
in modeling spatio-temporal information. Additionally, using a complex visual
model not only increases the complexity of lip reading models but also induces
delays in the overall network for multi-modal studies (e.g., audio-visual
speech recognition, speech enhancement, and speech separation). To overcome the
limitations of Convolutional Neural Network (CNN)-based models, we apply the
hierarchical structure and window self-attention of the Swin Transformer to lip
reading. We configure a new lightweight scale of the Swin Transformer suitable
for processing lip reading data and present the SwinLip visual speech encoder,
which efficiently reduces computational load by integrating modified
Convolution-augmented Transformer (Conformer) temporal embeddings with
conventional spatial embeddings in the hierarchical structure. Through
extensive experiments, we have validated that our SwinLip successfully improves
the performance and inference speed of the lip reading network when applied to
various backbones for word and sentence recognition, reducing computational
load. In particular, our SwinLip demonstrated robust performance in both
English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art
performance on the Mandarin LRW-1000 dataset with less computation compared to
the existing state-of-the-art model.

</details>


### [56] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TL;DR: 提出的深度乘积单元残差神经网络（PURe）通过将乘积单元集成到残差块中，提升深度卷积网络的表达能力和参数效率。PURe在多个基准数据集上表现优异，超越更深层的ResNet，同时参数更少、收敛更快、鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 传统的求和神经元只能进行简单的特征交互，而乘积单元可以实现乘法特征交互，从而更有效地表示复杂模式。PURe旨在通过改进残差块结构，提升模型的表达能力和效率。

Method: PURe将每个残差块的第二层替换为2D乘积单元，并去除非线性激活函数以保留结构信息。这种方法结合了乘积单元的乘法交互优势，同时保留了残差网络的优化特性。

Result: 在Galaxy10 DECaLS、ImageNet和CIFAR-10数据集上，PURe均显著优于传统ResNet。例如，PURe34在ImageNet上达到80.27%的top-1准确率，超越ResNet50/101，且参数量和计算资源更少。

Conclusion: PURe在准确性、效率和鲁棒性之间取得了良好平衡，展示了基于乘积单元的结构在计算机视觉任务中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [57] [MFSeg: Efficient Multi-frame 3D Semantic Segmentation](https://arxiv.org/abs/2505.04408)
*Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: MFSeg是一种高效的多帧3D语义分割框架，通过特征级点云序列聚合和轻量级MLP解码器，在保持高精度的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义分割方法在处理点云序列时存在计算冗余和效率低下的问题，MFSeg旨在通过优化特征提取和聚合过程解决这些问题。

Method: MFSeg采用特征级点云序列聚合，并利用轻量级MLP解码器避免对过去帧冗余点的上采样。

Result: 在nuScenes和Waymo数据集上的实验表明，MFSeg在精度和效率上均优于现有方法。

Conclusion: MFSeg通过高效的特征聚合和轻量设计，实现了高性能的3D语义分割，为实时应用提供了可行方案。

Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation
framework. By aggregating point cloud sequences at the feature level and
regularizing the feature extraction and aggregation process, MFSeg reduces
computational overhead while maintaining high accuracy. Moreover, by employing
a lightweight MLP-based point decoder, our method eliminates the need to
upsample redundant points from past frames. Experiments on the nuScenes and
Waymo datasets show that MFSeg outperforms existing methods, demonstrating its
effectiveness and efficiency.

</details>


### [58] [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/abs/2505.04410)
*Junjie Wang,Bin Chen,Yulin Li,Bin Kang,Yichi Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP 框架通过解耦 CLIP 的自注意力模块来提高密集视觉预测任务的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在密集预测任务中表现不佳，主要是由于其局部特征表示不足。

Method: 提出 DeCLIP，通过解耦自注意力模块分别获取‘内容’和‘上下文’特征，利用图像裁剪表示和视觉基础模型优化性能。

Result: DeCLIP 在多项开放词汇密集预测任务（如目标检测和语义分割）中表现显著优于现有方法。

Conclusion: DeCLIP 有效解决了密集预测任务中的局部特征表示问题，为开放词汇场景提供了高效解决方案。

Abstract: Dense visual prediction tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense prediction often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. The ``content'' features are aligned with image crop
representations to improve local discriminability, while ``context'' features
learn to retain the spatial correlations under the guidance of vision
foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP
significantly outperforms existing methods across multiple open-vocabulary
dense prediction tasks, including object detection and semantic segmentation.
Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.

</details>


### [59] [RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation](https://arxiv.org/abs/2505.04424)
*Jing Hu,Chengming Feng,Shu Hu,Ming-Ching Chang,Xin Li,Xi Wu,Xin Wang*

Main category: cs.CV

TL;DR: RLMiniStyler提出一种基于强化学习的轻量级任意风格迁移框架，通过迭代优化和不确定性感知的多任务学习策略，高效生成多样化的艺术图像序列。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在任意风格迁移中计算成本高且难以生成多样化结果，因此提出了基于强化学习的轻量级解决方案。

Method: 采用强化学习策略迭代引导风格迁移过程，并结合不确定性感知的多任务学习策略自动调整损失权重。

Result: 实验表明，RLMiniStyler在低计算成本下能生成高质量、多样化的艺术图像序列，优于现有方法。

Conclusion: RLMiniStyler在风格迁移任务中实现了高效与轻量化的平衡，具有广泛的应用潜力。

Abstract: Arbitrary style transfer aims to apply the style of any given artistic image
to another content image. Still, existing deep learning-based methods often
require significant computational costs to generate diverse stylized results.
Motivated by this, we propose a novel reinforcement learning-based framework
for arbitrary style transfer RLMiniStyler. This framework leverages a unified
reinforcement learning policy to iteratively guide the style transfer process
by exploring and exploiting stylization feedback, generating smooth sequences
of stylized results while achieving model lightweight. Furthermore, we
introduce an uncertainty-aware multi-task learning strategy that automatically
adjusts loss weights to adapt to the content and style balance requirements at
different training stages, thereby accelerating model convergence. Through a
series of experiments across image various resolutions, we have validated the
advantages of RLMiniStyler over other state-of-the-art methods in generating
high-quality, diverse artistic image sequences at a lower cost. Codes are
available at https://github.com/fengxiaoming520/RLMiniStyler.

</details>


### [60] [Learning Real Facial Concepts for Independent Deepfake Detection](https://arxiv.org/abs/2505.04460)
*Ming-Hui Liu,Harry Cheng,Tianyi Wang,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为RealID的新方法，通过独立评估真实和伪造类别的概率来提升深度伪造检测模型的泛化能力，实验表明其性能优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测模型因过度依赖伪造痕迹和对真实面部理解不足，导致在未见数据集上泛化能力差，常将真实实例误判为伪造。

Method: RealID方法包含Real Concept Capture Module (RealC2)和Independent Dual-Decision Classifier (IDC)两个模块：RealC2通过学习真实面部的多样原型捕捉全面概念，IDC则基于真实类别概念和伪造痕迹独立决策。

Result: 在五个广泛使用的数据集上，RealID表现优于现有方法，平均准确率提升1.74%。

Conclusion: RealID通过结合RealC2和IDC模块，有效减轻了伪造无关模式的影响，显著提升了模型的泛化性能。

Abstract: Deepfake detection models often struggle with generalization to unseen
datasets, manifesting as misclassifying real instances as fake in target
domains. This is primarily due to an overreliance on forgery artifacts and a
limited understanding of real faces. To address this challenge, we propose a
novel approach RealID to enhance generalization by learning a comprehensive
concept of real faces while assessing the probabilities of belonging to the
real and fake classes independently. RealID comprises two key modules: the Real
Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier
(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various
prototypes for real faces, allowing the model to capture a comprehensive
concept of real class. Meanwhile, IDC redefines the classification strategy by
making independent decisions based on the concept of the real class and the
presence of forgery artifacts. Through the combined effect of the above
modules, the influence of forgery-irrelevant patterns is alleviated, and
extensive experiments on five widely used datasets demonstrate that RealID
significantly outperforms existing state-of-the-art methods, achieving a 1.74%
improvement in average accuracy.

</details>


### [61] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/abs/2505.04481)
*Jiahao Li,Weijian Ma,Xueyang Li,Yunzhong Lou,Guichun Zhou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）生成计算机辅助设计（CAD）模型的参数序列，提出了CAD-Llama框架，通过层次化标注和自适应预训练方法提升LLMs在3D CAD参数序列生成上的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在通用文本生成上已取得显著成功，但其在专业领域（如CAD参数序列生成）的应用仍具挑战性，因为LLMs在预训练阶段未接触此类数据且缺乏对3D结构的直接认知。

Method: 提出了一个层次化标注流程和类代码格式（SPCC）来转换CAD参数序列，并结合自适应预训练和指令微调方法，赋予LLMs空间知识。

Result: 实验表明，该框架在生成3D CAD参数序列上的表现显著优于现有自回归方法和LLM基线。

Conclusion: CAD-Llama框架为LLMs在专业领域的参数化生成提供了有效的解决方案，验证了其在3D CAD模型生成中的潜力。

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>


### [62] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/abs/2505.04485)
*Ali Alawieh,Alexandru P. Condurache*

Main category: cs.CV

TL;DR: FA-KPConv是一种基于KPConv的新型神经网络架构，通过帧平均实现精确的平移、旋转和反射等变换的不变性和/或等变性，适用于3D点云分析。特别是在训练数据不足或测试数据随机旋转等挑战性场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: KPConv在3D点云分析中虽广泛使用，但其对欧几里得变换的不变性和等变性仅能通过大数据训练或数据增强近似实现。FA-KPConv旨在通过帧平均精确实现这些性质，无需增加参数或损失输入信息。

Method: FA-KPConv通过帧平均（Frame Averaging）方法包装现有KPConv网络，使其具备对平移、旋转和反射的精确不变性/等变性，同时保留可学习参数数量和输入信息完整性。

Result: 实验表明，FA-KPConv在点云分类和配准任务中表现优越，尤其在训练数据稀缺或测试数据随机旋转等挑战性场景中效果显著。

Conclusion: FA-KPConv通过嵌入几何先验知识，提升了KPConv网络的性能，尤其在变换鲁棒性要求高的任务中展现了优势，且无需额外参数或信息损失。

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>


### [63] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TL;DR: Flow matching models, like Latent-CFM, improve image generation efficiency by leveraging latent structures in data, reducing training time and enhancing quality.


<details>
  <summary>Details</summary>
Motivation: High-dimensional real-world datasets often reside in low-dimensional manifolds, but current flow matching models inefficiently learn these structures, leading to suboptimal performance.

Method: Latent-CFM simplifies training by using pretrained deep latent variable models to incorporate multi-modal data structures.

Result: Latent-CFM shows improved generation quality with 50% less training time and better performance on synthetic and real-world datasets.

Conclusion: Latent-CFM effectively integrates latent structures into flow matching models, enhancing efficiency and generation quality.

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [64] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TL;DR: 通过构建基准数据集VisAssistDaily和SafeVid，以及引入轮询机制，评估了VideoLLMs在辅助视障人士动态环境中的效果，发现GPT-4o表现最佳但仍存在潜在风险感知不足问题。


<details>
  <summary>Details</summary>
Motivation: 视障人群在动态复杂环境中缺乏有效的实时感知辅助工具，现有研究多关注静态内容，无法满足其日常活动需求。

Method: 构建VisAssistDaily基准数据集（覆盖基础技能、家庭及社交任务），评估VideoLLMs（以GPT-4o为主）；通过用户研究分析闭/开放场景表现；提出SafeVid数据集和轮询机制以优化环境风险感知。

Result: GPT-4o任务成功率最高，但当前模型在动态环境中感知潜在风险的能力仍不足。轮询机制可主动检测环境风险。

Conclusion: 为解决视障人士实时辅助需求，需结合高级视觉理解技术并优化动态环境风险感知，此研究为未来方向提供了依据。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [65] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: 论文研究了生成AI模型的创造力，并提出了可帮助用户选择适合任务的AI模型的量化指标。


<details>
  <summary>Details</summary>
Motivation: 解决生成AI模型创造力评估的缺乏定量方法问题，帮助用户更科学地选择模型。

Method: 提出了量化指标，并在多个流行的图像生成模型上进行了评估。

Result: 实验结果表明，提出的指标符合人类直觉。

Conclusion: 量化指标能有效评估生成AI的创造力，为用户选择模型提供依据。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [66] [Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition](https://arxiv.org/abs/2505.04502)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 该论文提出了一种在边缘GPU上并发利用多个硬件引擎的方法，以提高视频人脸检测和识别的效率，同时降低功耗。


<details>
  <summary>Details</summary>
Motivation: 在公共场合的视频人脸检测和识别需求日益增长，但现有方法通常无法充分利用边缘GPU的所有硬件引擎，限制了性能。

Method: 通过并发和流水线化任务（如视频解码、人脸检测和识别），结合NVIDIA Orin GPU的多引擎能力，提升了处理效率。

Result: 实验显示，该方法在实时性能约束下提高了吞吐量，并节省了约5%的功耗（300 mW）。

Conclusion: 论文提出了硬件改进建议，以进一步提升性能，同时证明了多引擎并发策略的有效性。

Abstract: Video face detection and recognition in public places at the edge is required
in several applications, such as security reinforcement and contactless access
to authorized venues. This paper aims to maximize the simultaneous usage of
hardware engines available in edge GPUs nowadays by leveraging the concurrency
and pipelining of tasks required for face detection and recognition. This also
includes the video decoding task, which is required in most face monitoring
applications as the video streams are usually carried via Gbps Ethernet
network. This constitutes an improvement over previous works where the tasks
are usually allocated to a single engine due to the lack of a unified and
automated framework that simultaneously explores all hardware engines. In
addition, previously, the input faces were usually embedded in still images or
within raw video streams that overlook the burst delay caused by the decoding
stage. The results on real-life video streams suggest that simultaneously using
all the hardware engines available in the recent NVIDIA edge Orin GPU, higher
throughput, and a slight saving of power consumption of around 300 mW,
accounting for around 5%, have been achieved while satisfying the real-time
performance constraint. The performance gets even higher by considering several
video streams simultaneously. Further performance improvement could have been
obtained if the number of shuffle layers that were created by the tensor RT
framework for the face recognition task was lower. Thus, the paper suggests
some hardware improvements to the existing edge GPU processors to enhance their
performance even higher.

</details>


### [67] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/abs/2505.04512)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Sen Liang,Yuan Zhou,Qin Lin,Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanCustom提出了一种多模态定制视频生成框架，解决了现有方法在身份一致性和输入模态上的局限性，通过多模态条件注入和身份增强模块显著提升了视频生成的ID一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有定制视频生成方法在身份一致性和输入模态支持上存在不足，HunyuanCustom旨在通过多模态条件注入和身份增强策略解决这些问题。

Method: 基于HunyuanVideo框架，引入文本-图像融合模块和图像ID增强模块，同时设计了AudioNet和视频驱动注入模块以支持音频和视频条件生成。

Result: 实验表明，HunyuanCustom在ID一致性、真实性和文本-视频对齐上显著优于现有方法，并验证了其在音频和视频驱动生成任务中的鲁棒性。

Conclusion: 多模态条件和身份保持策略有效推动了可控视频生成的进步，HunyuanCustom展示了其在多样化应用中的潜力。

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>


### [68] [Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model](https://arxiv.org/abs/2505.04522)
*Pengfei Guo,Can Zhao,Dong Yang,Yufan He,Vishwesh Nath,Ziyue Xu,Pedro R. A. S. Bassi,Zongwei Zhou,Benjamin D. Simon,Stephanie Anne Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: 本文介绍了Text2CT，一种基于扩散模型从自由文本描述生成3D CT扫描的新方法，展示了在诊断和数据增强中的潜力。


<details>
  <summary>Details</summary>
Motivation: 从自由文本生成3D CT扫描为诊断和研究带来变革性机会，现有方法依赖固定格式文本输入，无法处理多样化描述。

Method: Text2CT通过扩散模型将医学文本编码为潜在表示，并解码为高分辨率3D CT扫描，实现了语义文本到体积表示的转换。

Result: 该方法在保持解剖学保真度和捕捉复杂结构方面表现优异，达到了最先进的性能。

Conclusion: Text2CT在诊断和数据增强中具有广阔的应用前景。

Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a
transformative opportunity in diagnostics and research. In this paper, we
introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual
descriptions using the diffusion model. Unlike previous methods that rely on
fixed-format text input, Text2CT employs a novel prompt formulation that
enables generation from diverse, free-text descriptions. The proposed framework
encodes medical text into latent representations and decodes them into
high-resolution 3D CT scans, effectively bridging the gap between semantic text
inputs and detailed volumetric representations in a unified 3D framework. Our
method demonstrates superior performance in preserving anatomical fidelity and
capturing intricate structures as described in the input text. Extensive
evaluations show that our approach achieves state-of-the-art results, offering
promising potential applications in diagnostics, and data augmentation.

</details>


### [69] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 该论文提出了一种结合硬件和软件的优化方法，在NVIDIA Jetson AGX Orin边缘GPU上提升人脸检测与识别系统的效率和功耗表现。通过同时利用GPU的所有硬件引擎和集成人脸跟踪模块，系统在1920x1080分辨率下达到了290 FPS的高吞吐量，并节省了约800 mW的功耗。


<details>
  <summary>Details</summary>
Motivation: 公共场合的实时、准确人脸检测与识别需要高性价比的机器视觉系统，尽管现有专用硬件（如边缘或云AI加速器）性能较高，但在吞吐量和功耗方面仍有改进空间。

Method: 论文提出一种硬件-软件协同设计方法：1) 充分利用NVIDIA Jetson AGX Orin GPU的所有硬件引擎并行处理任务；2) 集成人脸跟踪模块，避免对每帧重复运行识别算法，仅在新人脸出现时触发。

Result: 实验结果表明，该方法在1920x1080分辨率（平均每帧6张人脸）下实现了290 FPS的高吞吐量，相比仅使用CPU/GPU引擎且未集成跟踪模块的方案，功耗降低约800 mW。

Conclusion: 该硬件-软件协同设计方法为边缘高性能机器视觉系统提供了可行路径，尤其适用于公共场合多摄像头视频监控场景。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [70] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种名为DFVO的网络，通过多任务级联方式解决可见光与红外图像融合中因光照退化导致的图像模糊问题，实现了更清晰的融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在可见光图像光照较差时，融合结果模糊且暗淡，无法满足自动驾驶等高层次视觉任务的需求。

Method: 采用级联多任务策略，构建潜在共同特征提取器（LCFE），设计细节提取模块（DEM）和高交叉注意力模块（HCAM），并通过损失函数优化整体网络学习。

Result: 在LLVIP数据集上取得了63.258 dB的PSNR和0.724的CC，融合结果更清晰、信息更丰富且光照更均匀。

Conclusion: DFVO在黑暗环境中表现优异，为高层次视觉任务提供了更有效的信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [71] [RAFT: Robust Augmentation of FeaTures for Image Segmentation](https://arxiv.org/abs/2505.04529)
*Edward Humes,Xiaomin Lin,Uttej Kallakuri,Tinoosh Mohsenin*

Main category: cs.CV

TL;DR: 论文提出了RAFT框架，旨在通过少量标注的真实数据和增强技术提升图像分割模型的性能，解决了合成数据训练模型在真实场景中的适应问题。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据训练模型在真实世界部署中的性能下降问题（Syn2Real问题），减少对大量高质量标注数据的依赖。

Method: 提出RAFT框架，结合数据和特征增强以及主动学习，利用少量标注真实数据进行模型适应。

Result: 在多个合成到真实和真实到真实的基准测试（如SYNTHIA->Cityscapes、GTAV->Cityscapes等）中超越了现有方法HALO，mIoU显著提升。

Conclusion: RAFT有效提升了模型在真实场景中的性能，验证了其在数据适应和模型迁移中的优势。

Abstract: Image segmentation is a powerful computer vision technique for scene
understanding. However, real-world deployment is stymied by the need for
high-quality, meticulously labeled datasets. Synthetic data provides
high-quality labels while reducing the need for manual data collection and
annotation. However, deep neural networks trained on synthetic data often face
the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a
novel framework for adapting image segmentation models using minimal labeled
real-world data through data and feature augmentations, as well as active
learning. To validate RAFT, we perform experiments on the synthetic-to-real
"SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass
the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an
improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes
experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach
on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO,
with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the
effect of the allocated annotation budget and various components of RAFT upon
the final transfer mIoU.

</details>


### [72] [Registration of 3D Point Sets Using Exponential-based Similarity Matrix](https://arxiv.org/abs/2505.04540)
*Ashutosh Singandhupe,Sanket Lokhande,Hung Manh La*

Main category: cs.CV

TL;DR: 本文提出了一种改进的迭代最近点（ICP）算法——ESM-ICP，通过引入高斯启发的指数加权方案，动态调整相似性矩阵，有效解决了大旋转差异和噪声数据下的点云配准问题，表现出优于传统方法和现有学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前点云配准技术在存在大旋转差异或严重噪声时表现不佳，导致配准错误和重建失真。为克服这些限制，本文提出了一种更鲁棒的ICP改进方法。

Method: 本文提出的ESM-ICP算法通过动态生成指数加权的相似性矩阵，结合高斯分布特性，在迭代过程中优化旋转和平移参数的估计，从而提升配准精度。

Result: 实验表明，ESM-ICP在大旋转差异和非高斯噪声环境下均优于传统几何配准方法和部分最新学习方法，表现出更高的鲁棒性。

Conclusion: ESM-ICP为复杂场景下的点云配准提供了高效解决方案，其公开的实现代码支持进一步研究和应用发展。

Abstract: Point cloud registration is a fundamental problem in computer vision and
robotics, involving the alignment of 3D point sets captured from varying
viewpoints using depth sensors such as LiDAR or structured light. In modern
robotic systems, especially those focused on mapping, it is essential to merge
multiple views of the same environment accurately. However, state-of-the-art
registration techniques often struggle when large rotational differences exist
between point sets or when the data is significantly corrupted by sensor noise.
These challenges can lead to misalignments and, consequently, to inaccurate or
distorted 3D reconstructions. In this work, we address both these limitations
by proposing a robust modification to the classic Iterative Closest Point (ICP)
algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),
integrates a Gaussian-inspired exponential weighting scheme to construct a
similarity matrix that dynamically adapts across iterations. This matrix
facilitates improved estimation of both rotational and translational components
during alignment. We demonstrate the robustness of ESM-ICP in two challenging
scenarios: (i) large rotational discrepancies between the source and target
point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show
that ESM-ICP outperforms traditional geometric registration techniques as well
as several recent learning-based methods. To encourage reproducibility and
community engagement, our full implementation is made publicly available on
GitHub. https://github.com/aralab-unr/ESM_ICP

</details>


### [73] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 该论文提出了KA-Prompt方法，通过组件感知的提示知识对齐改进领域增量学习（DIL），解决了现有提示方法中因组件不对齐导致的知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的DIL方法在多领域知识存储和跨域提示融合方面表现优异，但存在组件不对齐问题，导致知识冲突和性能下降。论文旨在解决这一问题。

Method: KA-Prompt分为两阶段：(1)初始组件结构配置，通过贪婪搜索挖掘与新区相关的旧提示，初始化新提示以实现知识转移和对齐；(2)在线对齐保持，动态识别目标旧提示并施加自适应组件一致性约束。

Result: 在DIL基准测试中，KA-Prompt显著提升了模型的学习和推理能力。

Conclusion: KA-Prompt通过组件对齐有效解决了跨域知识集成中的冲突问题，为DIL任务提供了更高效的解决方案。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [74] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 该论文提出了一种多目标强化学习框架，用于从欠采样的k空间数据中进行全面、连续的诊断评估，旨在降低MRI的成本和复杂度，使其更适用于即时诊断（PoC）。


<details>
  <summary>Details</summary>
Motivation: 尽管MRI具有卓越的诊断能力，但其高成本和复杂性限制了其作为即时诊断设备的使用。为了通过降低磁场强度实现这一目标，改进采样策略是关键。

Method: 采用一种新型的多目标强化学习框架，通过逐步加权奖励函数识别对每个诊断目标贡献最大的样本，并在推理过程中主动适应连续决策以优化采样。

Result: 在膝关节病理评估任务中（如ACL扭伤检测和软骨厚度损失评估），该方法在疾病检测、严重程度量化和整体连续诊断方面表现优异，同时显著减少了k空间样本的需求。

Conclusion: 该方法为MRI作为全面且经济的即时诊断设备的未来发展铺平了道路。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


### [75] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/abs/2505.04594)
*Zhihao Zhang,Abhinav Kumar,Girish Chandar Ganesan,Xiaoming Liu*

Main category: cs.CV

TL;DR: 该论文提出MonoCoP方法，通过Chain-of-Prediction（CoP）序列化预测3D属性，解决单目3D目标检测中深度估计的模糊性问题，并在多个数据集上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了3D属性之间的内在关联性，导致深度预测的精度和稳定性受限。受大型语言模型中Chain-of-Thought启发，作者提出通过序列化预测条件化其他3D属性以提高准确性。

Method: MonoCoP采用三步设计：1) 为每个3D属性设计轻量级AttributeNet（AN）学习属性特征；2) 构建显式链条传播特征；3) 使用残差连接汇总特征，确保后续属性预测基于先前属性。

Result: MonoCoP在KITTI排行榜上达到最优性能，无需额外数据，并在Waymo和nuScenes数据集上超越现有方法。

Conclusion: 通过条件化序列预测3D属性，MonoCoP有效解决了单目3D检测中的深度模糊性问题，显著提升了准确性和稳定性。

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [76] [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/abs/2505.04601)
*Xianhang Li,Yanqing Liu,Haoqin Tu,Hongru Zhu,Cihang Xie*

Main category: cs.CV

TL;DR: 这篇论文介绍了OpenVision，一个完全开放且高效的视觉编码器家族，性能媲美或超越OpenAI的CLIP，并提供了从5.9M到632.1M参数的不同规模模型。


<details>
  <summary>Details</summary>
Motivation: 填补现有开放视觉编码器的空白，提供成本效益高的替代方案，同时提升多模态模型的性能。

Method: 基于现有工作（如CLIPS训练框架和Recap-DataComp-1B训练数据），揭示了增强编码器质量的关键见解。

Result: OpenVision在不同参数规模下均表现出色，较大模型提升了多模态性能，较小模型适用于轻量级边缘部署。

Conclusion: OpenVision为多模态模型提供了灵活的选择，平衡了性能与效率，推动了开放多模态研究的进展。

Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of
vision encoder for building multimodal foundation models. Although recent
alternatives such as SigLIP have begun to challenge this status quo, to our
knowledge none are fully open: their training data remains proprietary and/or
their training recipes are not released. This paper fills this gap with
OpenVision, a fully-open, cost-effective family of vision encoders that match
or surpass the performance of OpenAI's CLIP when integrated into multimodal
frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for
training framework and Recap-DataComp-1B for training data -- while revealing
multiple key insights in enhancing encoder quality and showcasing practical
benefits in advancing multimodal models. By releasing vision encoders spanning
from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible
trade-off between capacity and efficiency in building multimodal models: larger
models deliver enhanced multimodal performance, while smaller versions enable
lightweight, edge-ready multimodal deployments.

</details>


### [77] [FastMap: Revisiting Dense and Scalable Structure from Motion](https://arxiv.org/abs/2505.04612)
*Jiahao Li,Haochen Wang,Muhammad Zubair Irshad,Igor Vasiljevic,Matthew R. Walter,Vitor Campagnolo Guizilini,Greg Shakhnarovich*

Main category: cs.CV

TL;DR: FastMap is a scalable, GPU-friendly structure from motion (SfM) 方法，比COLMAP和GLOMAP快一到两个数量级，同时保持相当的相机姿态精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如COLMAP和GLOMAP）虽能高精度估计相机姿态，但在处理大规模匹配关键点对时扩展性差，主要因并行性不足和优化步骤计算成本高。

Method: 设计了一个完全基于GPU友好操作的SfM框架，优化步骤的时间仅与图像对数量线性相关，不依赖关键点对或3D点数量。

Result: 在大规模场景中，FastMap比COLMAP和GLOMAP快一到两个数量级，且姿态精度相当。

Conclusion: FastMap通过高效并行化和线性优化步骤，显著提升SfM的扩展性和速度，适用于大规模场景。

Abstract: We propose FastMap, a new global structure from motion method focused on
speed and simplicity. Previous methods like COLMAP and GLOMAP are able to
estimate high-precision camera poses, but suffer from poor scalability when the
number of matched keypoint pairs becomes large. We identify two key factors
leading to this problem: poor parallelization and computationally expensive
optimization steps. To overcome these issues, we design an SfM framework that
relies entirely on GPU-friendly operations, making it easily parallelizable.
Moreover, each optimization step runs in time linear to the number of image
pairs, independent of keypoint pairs or 3D points. Through extensive
experiments, we show that FastMap is one to two orders of magnitude faster than
COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.

</details>


### [78] [Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait](https://arxiv.org/abs/2505.04616)
*Feng Liu,Nicholas Chimitt,Lanqing Guo,Jitesh Jain,Aditya Kane,Minchul Kim,Wes Robbins,Yiyang Su,Dingqiang Ye,Xingguang Zhang,Jie Zhu,Siddharth Satyakam,Christopher Perry,Stanley H. Chan,Arun Ross,Humphrey Shi,Zhangyang Wang,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: FarSight是一个用于在远距离和复杂环境下进行全身人物识别的端到端系统，结合了面部、步态和体型多模态生物特征，显著提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 解决在远距离、高仰角及恶劣大气条件下（如湍流和高风速）的生物特征识别问题，尤其在IARPA BRIAR项目中这类监控场景下的需求。

Method: FarSight系统整合了四个核心模块：多目标检测与跟踪、识别感知的视频恢复、模态特定的生物特征编码、以及质量引导的多模态融合。

Result: 在BRIAR数据集上，FarSight在1:1验证准确率上提高了34.1%，闭集识别率（Rank-20）提升了17.8%，开集识别错误率降低了34.3%。

Conclusion: FarSight在复杂真实条件下展现了先进的生物特征识别能力，成为该领域的领先解决方案。

Abstract: We address the problem of whole-body person recognition in unconstrained
environments. This problem arises in surveillance scenarios such as those in
the IARPA Biometric Recognition and Identification at Altitude and Range
(BRIAR) program, where biometric data is captured at long standoff distances,
elevated viewing angles, and under adverse atmospheric conditions (e.g.,
turbulence and high wind velocity). To this end, we propose FarSight, a unified
end-to-end system for person recognition that integrates complementary
biometric cues across face, gait, and body shape modalities. FarSight
incorporates novel algorithms across four core modules: multi-subject detection
and tracking, recognition-aware video restoration, modality-specific biometric
feature encoding, and quality-guided multi-modal fusion. These components are
designed to work cohesively under degraded image conditions, large pose and
scale variations, and cross-domain gaps. Extensive experiments on the BRIAR
dataset, one of the most comprehensive benchmarks for long-range, multi-modal
biometric recognition, demonstrate the effectiveness of FarSight. Compared to
our preliminary system, this system achieves a 34.1% absolute gain in 1:1
verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set
identification (Rank-20), and a 34.3% reduction in open-set identification
errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE
Face in Video Evaluation (FIVE), which conducts standardized face recognition
testing on the BRIAR dataset. These results establish FarSight as a
state-of-the-art solution for operational biometric recognition in challenging
real-world conditions.

</details>


### [79] [On Path to Multimodal Generalist: General-Level and General-Bench](https://arxiv.org/abs/2505.04620)
*Hao Fei,Yuan Zhou,Juncheng Li,Xiangtai Li,Qingshan Xu,Bobo Li,Shengqiong Wu,Yaoting Wang,Junbao Zhou,Jiahao Meng,Qingyu Shi,Zhiyuan Zhou,Liangtao Shi,Minghe Gao,Daoan Zhang,Zhiqi Ge,Weiming Wu,Siliang Tang,Kaihang Pan,Yaobo Ye,Haobo Yuan,Tao Zhang,Tianjie Ju,Zixiang Meng,Shilin Xu,Liyu Jia,Wentao Hu,Meng Luo,Jiebo Luo,Tat-Seng Chua,Shuicheng Yan,Hanwang Zhang*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLM）正快速发展，但现有评测方法无法全面衡量其能力。本文提出General-Level评估框架和General-Bench数据集，用于更科学地评估MLLM的性能和通用性，助力AGI研究。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测方法简单以任务表现衡量模型能力，无法反映其真正通用性和多模态协同能力，阻碍了AGI的进展。

Method: 提出5级General-Level评估框架，引入Synergy指标衡量模型在多模态理解与生成中的一致性；构建包含700任务、32万实例的General-Bench数据集。

Result: 评测100多个SOTA MLLM后发现：当前模型离真正通用AI仍有差距，且任务表现与通用性不完全正相关，验证了框架的科学性。

Conclusion: General-Level框架和评测基准为下一代多模态基础模型研究指明方向，有望加速AGI实现。

Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid
growth, driven by the advanced capabilities of LLMs. Unlike earlier
specialists, existing MLLMs are evolving towards a Multimodal Generalist
paradigm. Initially limited to understanding multiple modalities, these models
have advanced to not only comprehend but also generate across modalities. Their
capabilities have expanded from coarse-grained to fine-grained multimodal
understanding and from supporting limited modalities to arbitrary ones. While
many benchmarks exist to assess MLLMs, a critical question arises: Can we
simply assume that higher performance across tasks indicates a stronger MLLM
capability, bringing us closer to human-level AI? We argue that the answer is
not as straightforward as it seems. This project introduces General-Level, an
evaluation framework that defines 5-scale levels of MLLM performance and
generality, offering a methodology to compare MLLMs and gauge the progress of
existing systems towards more robust multimodal generalists and, ultimately,
towards AGI. At the core of the framework is the concept of Synergy, which
measures whether models maintain consistent capabilities across comprehension
and generation, and across multiple modalities. To support this evaluation, we
present General-Bench, which encompasses a broader spectrum of skills,
modalities, formats, and capabilities, including over 700 tasks and 325,800
instances. The evaluation results that involve over 100 existing
state-of-the-art MLLMs uncover the capability rankings of generalists,
highlighting the challenges in reaching genuine AI. We expect this project to
pave the way for future research on next-generation multimodal foundation
models, providing a robust infrastructure to accelerate the realization of AGI.
Project page: https://generalist.top/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [80] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac是一个基于AI的4D心脏图像自动分割与疾病分类的web平台，通过深度学习模型在ACDC数据集上训练，实现了92.6%的分割准确率和98%的分类准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心脏成像数据的精确处理对心血管疾病的诊断和管理至关重要。现有方法在分割和分类的整合上表现不足，需要通过更高效、准确的AI工具优化临床决策。

Method: 结合深度学习分割模型与两步分类流程：分割模块标注心脏结构，分类模块基于分割特征将图像分类为五种疾病类别。

Result: 分割准确率92.6%，分类准确率98%，优于当前同类方法，支持实时可视化和临床工作流。

Conclusion: IntelliCardiac作为一个可扩展的AI工具，在心脏成像诊断中展现了高准确性和临床应用潜力。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [81] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TL;DR: 论文提出了一种利用空间条件技术将卫星SAR图像转化为机载SAR表示的新方法，解决了高分辨率SAR图像获取成本高、开源数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 由于高分辨率机载SAR图像获取成本高且开源数据集稀缺，限制了现有基础模型在遥感应用中的使用，因此需要一种方法来扩充稀缺数据。

Method: 利用ONERA长达15年的机载SAR数据，构建了包含11万张图像的数据集，并基于35亿参数的预训练潜在扩散模型，采用空间条件技术将卫星SAR图像转化为机载SAR表示。

Result: 该方法成功将卫星SAR图像转化为机载SAR表示，并提高了基于物理模拟器生成的仿真图像的真实性。

Conclusion: 该研究为SAR成像技术的进步提供了一种创新的AI应用方法，填补了相关领域的空白。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [82] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TL;DR: 利用深度学习模型（ViViT、Video Swin Tiny和3D CNN-LSTM）通过面部视频分析评估帕金森病患者抑郁症状的存在和严重程度，其中Video Swin Tiny表现最佳，分类准确率高达94%。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）患者中抑郁症状普遍且常被漏诊，尤其是因为与运动症状（如面部表情减少）重叠。研究旨在通过视频分析更准确地识别抑郁症状。

Method: 采用ViViT、Video Swin Tiny和3D CNN-LSTM（带注意力层）三种深度学习模型，基于1,875个视频数据集，分析患者面部表情，评估抑郁症状（使用老年抑郁量表GDS），并分ON/OFF药物状态进行二次分析。

Result: Video Swin Tiny在二元分类（抑郁症状存在与否）中表现最佳（94%准确率，93.7% F1分数），在多分类任务（无、轻度或重度抑郁）中达87.1%准确率和85.4% F1分数。

Conclusion: Video Swin Tiny模型能有效识别PD患者的抑郁症状，药物状态对结果无显著影响，为临床抑郁筛查提供了非侵入性工具。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [83] [Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification](https://arxiv.org/abs/2505.04003)
*Feng Gao,Sheng Liu,Chuanzheng Gong,Xiaowei Zhou,Jiayi Wang,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: 该论文提出了一种基于HSI和SAR/LiDAR数据的原型信息补偿网络（PICNet），用于解决多源遥感数据联合分类中的特征耦合和互补信息不一致问题。通过频次交互模块和原型补偿模块，显著提高了分类精度。


<details>
  <summary>Details</summary>
Motivation: 解决多源遥感数据联合分类中存在的频次间特征耦合不足和互补信息探索不一致问题，提升土地覆盖分类的准确性和可靠性。

Method: 设计了频次交互模块和原型信息补偿模块，通过解耦和重耦合多源特征，并利用可学习模态原型实现跨模态特征整合与对齐。

Result: 在三个公开数据集上的实验表明，PICNet显著优于现有最先进方法。

Conclusion: PICNet通过频次交互和原型补偿，有效提升了多源遥感数据的分类性能，代码已开源。

Abstract: Multi-source remote sensing data joint classification aims to provide
accuracy and reliability of land cover classification by leveraging the
complementary information from multiple data sources. Existing methods confront
two challenges: inter-frequency multi-source feature coupling and inconsistency
of complementary information exploration. To solve these issues, we present a
Prototype-based Information Compensation Network (PICNet) for land cover
classification based on HSI and SAR/LiDAR data. Specifically, we first design a
frequency interaction module to enhance the inter-frequency coupling in
multi-source feature extraction. The multi-source features are first decoupled
into high- and low-frequency components. Then, these features are recoupled to
achieve efficient inter-frequency communication. Afterward, we design a
prototype-based information compensation module to model the global
multi-source complementary information. Two sets of learnable modality
prototypes are introduced to represent the global modality information of
multi-source data. Subsequently, cross-modal feature integration and alignment
are achieved through cross-attention computation between the modality-specific
prototype vectors and the raw feature representations. Extensive experiments on
three public datasets demonstrate the significant superiority of our PICNet
over state-of-the-art methods. The codes are available at
https://github.com/oucailab/PICNet.

</details>


### [84] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TL;DR: 本文开发了一个3D卷积神经网络，用于将T1加权脑部MRI扫描分类为健康或阿尔茨海默病，通过噪声注入和交叉验证，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在通过简单的数据增强方法提升3D MRI分类的准确性，并探索未来更先进的增强方法和架构。

Method: 采用3D卷积、池化、批归一化、ReLU层和Sigmoid输出的网络结构，结合噪声注入和五折交叉验证。

Result: 测试集准确率为0.912，ROC曲线下面积为0.961，灵敏度和特异性均超过0.90。

Conclusion: 证明了简单数据增强在3D MRI分类中的有效性，并鼓励未来探索更先进的增强方法和架构。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [85] [TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models](https://arxiv.org/abs/2505.04050)
*Kazuki Higo,Toshiki Kanai,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 提出了一种基于潜在扩散模型的联合生成地形高度图和纹理的方法，通过无监督训练和有监督适配器实现用户手绘草图控制，实验证明能直观生成地形并保持高度图与纹理的关联性。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多独立生成地形高度图或纹理，未能充分捕捉二者之间的固有关联，而3D地形模型中表面颜色与几何形状的关联对真实感至关重要。

Method: 采用潜在扩散模型，先通过无监督学习随机生成配对的高度图和纹理，再通过有监督学习的外部适配器实现手绘草图控制的用户交互。

Result: 实验表明，该方法能直观地生成地形，并有效保持高度图与纹理的相关性。

Conclusion: 所提方法通过联合生成高度图和纹理，结合用户控制，为3D地形建模提供了兼具真实感和灵活性的解决方案。

Abstract: 3D terrain models are essential in fields such as video game development and
film production. Since surface color often correlates with terrain geometry,
capturing this relationship is crucial to achieving realism. However, most
existing methods generate either a heightmap or a texture, without sufficiently
accounting for the inherent correlation. In this paper, we propose a method
that jointly generates terrain heightmaps and textures using a latent diffusion
model. First, we train the model in an unsupervised manner to randomly generate
paired heightmaps and textures. Then, we perform supervised learning of an
external adapter to enable user control via hand-drawn sketches. Experiments
show that our approach allows intuitive terrain generation while preserving the
correlation between heightmaps and textures.

</details>


### [86] [Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control](https://arxiv.org/abs/2505.04052)
*Shun Masuda,Yuki Endo,Yoshihiro Kanamori*

Main category: cs.GR

TL;DR: 本文提出两种方法，用于更自然地将人物合成到场景图像中，处理遮挡问题并提供姿势控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理插入人物的遮挡和深度问题，且姿势控制有限，因此提出新方法以改善。

Method: 第一种方法是两阶段法，先通过监督学习生成场景深度图，再合成人物；第二种方法直接从输入数据学习遮挡并合成人物。

Result: 定量和定性评估显示，两种方法在场景一致性和遮挡处理上优于现有方法。

Conclusion: 提出的方法能更自然地合成人物，有效处理遮挡并提供姿势控制，优于现有技术。

Abstract: Compositing human figures into scene images has broad applications in areas
such as entertainment and advertising. However, existing methods often cannot
handle occlusion of the inserted person by foreground objects and unnaturally
place the person in the frontmost layer. Moreover, they offer limited control
over the inserted person's pose. To address these challenges, we propose two
methods. Both allow explicit pose control via a 3D body model and leverage
latent diffusion models to synthesize the person at a contextually appropriate
depth, naturally handling occlusions without requiring occlusion masks. The
first is a two-stage approach: the model first learns a depth map of the scene
with the person through supervised learning, and then synthesizes the person
accordingly. The second method learns occlusion implicitly and synthesizes the
person directly from input data without explicit depth supervision.
Quantitative and qualitative evaluations show that both methods outperform
existing approaches by better preserving scene consistency while accurately
reflecting occlusions and user-specified poses.

</details>


### [87] [Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control](https://arxiv.org/abs/2505.04387)
*Amin Fadaeinejad,Abdallah Dib,Luiz Gustavo Hafemann,Emeline Got,Trevor Anderson,Amaury Depierre,Nikolaus F. Troje,Marcus A. Brubaker,Marc-André Carbonneau*

Main category: cs.GR

TL;DR: 提出了一种通过几何感知纹理合成简化3D头部建模的框架，提供三层次艺术控制，从整体几何到皮肤细节编辑，实验验证了其多样性及实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D头部资产创作过程繁琐，需简化并提升艺术家的控制能力。

Method: 采用几何感知纹理合成管道，学习头部几何与皮肤纹理的关联，支持多层次编辑。

Result: 生成多样且几何干净的结果，展示了皮肤色调调整和细节编辑的实际应用。

Conclusion: 该方法整合了艺术家工作流程，提升了虚拟角色创作的效率和直观性。

Abstract: Creating realistic 3D head assets for virtual characters that match a precise
artistic vision remains labor-intensive. We present a novel framework that
streamlines this process by providing artists with intuitive control over
generated 3D heads. Our approach uses a geometry-aware texture synthesis
pipeline that learns correlations between head geometry and skin texture maps
across different demographics. The framework offers three levels of artistic
control: manipulation of overall head geometry, adjustment of skin tone while
preserving facial characteristics, and fine-grained editing of details such as
wrinkles or facial hair. Our pipeline allows artists to make edits to a single
texture map using familiar tools, with our system automatically propagating
these changes coherently across the remaining texture maps needed for realistic
rendering. Experiments demonstrate that our method produces diverse results
with clean geometries. We showcase practical applications focusing on intuitive
control for artists, including skin tone adjustments and simplified editing
workflows for adding age-related details or removing unwanted features from
scanned models. This integrated approach aims to streamline the artistic
workflow in virtual character creation.

</details>


### [88] [TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization](https://arxiv.org/abs/2505.04590)
*Alexandre Binninger,Ruben Wiersma,Philipp Herholz,Olga Sorkine-Hornung*

Main category: cs.GR

TL;DR: 这篇论文提出了TetWeave，一种用于基于梯度的网格优化的新型等值面表示方法，结合了四面体网格和定向符号距离的联合优化，灵活性高且能保证提取的网格是水密、二维流形且无交叉的。该方法在计算机图形和视觉的多种任务中表现出色，并显著提升内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统的等值面表示方法通常使用预定义网格，缺乏灵活性且内存效率低。为了解决这些问题，作者提出了TetWeave，以实现更灵活的网格优化和更高的内存效率。

Method: TetWeave通过Delaunay三角剖分动态生成四面体网格，并联合优化网格位置和定向符号距离。该方法引入了重采样策略，在高误差区域放置新点，同时保持网格的公平性。

Result: 实验结果表明，TetWeave能在多种计算机图形和视觉任务（如多视角3D重建、网格压缩和几何纹理生成）中生成高质量的自适应网格，且内存使用接近线性增长，显著优于预定义网格方法。

Conclusion: TetWeave提供了一种灵活且高效的等值面表示方法，优化了网格的质量和内存使用，适用于广泛的图形和视觉任务。

Abstract: We introduce TetWeave, a novel isosurface representation for gradient-based
mesh optimization that jointly optimizes the placement of a tetrahedral grid
used for Marching Tetrahedra and a novel directional signed distance at each
point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay
triangulation, enabling increased flexibility compared to predefined grids. The
extracted meshes are guaranteed to be watertight, two-manifold and
intersection-free. The flexibility of TetWeave enables a resampling strategy
that places new points where reconstruction error is high and allows to
encourage mesh fairness without compromising on reconstruction error. This
leads to high-quality, adaptive meshes that require minimal memory usage and
few parameters to optimize. Consequently, TetWeave exhibits near-linear memory
scaling relative to the vertex count of the output mesh - a substantial
improvement over predefined grids. We demonstrate the applicability of TetWeave
to a broad range of challenging tasks in computer graphics and vision, such as
multi-view 3D reconstruction, mesh compression and geometric texture
generation.

</details>


### [89] [PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer](https://arxiv.org/abs/2505.04622)
*Jingwen Ye,Yuze He,Yanning Zhou,Yiqin Zhu,Kaiwen Xiao,Yong-Jin Liu,Wei Yang,Xiao Han*

Main category: cs.GR

TL;DR: 论文提出了PrimitiveAnything框架，将形状基元抽象重新定义为基元组装生成任务，通过自回归生成和无歧义参数化方案，从大规模人工标注中学习基元组装过程，生成更符合人类感知的高质量基元组合。


<details>
  <summary>Details</summary>
Motivation: 现有基元抽象方法要么依赖几何优化且语义理解有限，要么局限于小规模、类别特定的数据集，难以泛化到多样形状类别。本文旨在解决这些问题，提出更通用的基元抽象方法。

Method: 提出PrimitiveAnything框架，包括形状条件基元变换器（自回归生成）和无歧义参数化方案，统一表示多种基元类型，并从大规模人工标注数据中学习基元组装过程。

Result: 实验表明，PrimitiveAnything能生成高质量基元组合，既符合人类感知，又保持几何保真度，适用于多样形状类别，并有望支持游戏中的基元用户生成内容（UGC）。

Conclusion: PrimitiveAnything通过重新定义基元抽象任务并引入统一表示和学习框架，显著提升了基元抽象的质量和泛化能力，为3D应用开辟了新方向。

Abstract: Shape primitive abstraction, which decomposes complex 3D shapes into simple
geometric elements, plays a crucial role in human visual cognition and has
broad applications in computer vision and graphics. While recent advances in 3D
content generation have shown remarkable progress, existing primitive
abstraction methods either rely on geometric optimization with limited semantic
understanding or learn from small-scale, category-specific datasets, struggling
to generalize across diverse shape categories. We present PrimitiveAnything, a
novel framework that reformulates shape primitive abstraction as a primitive
assembly generation task. PrimitiveAnything includes a shape-conditioned
primitive transformer for auto-regressive generation and an ambiguity-free
parameterization scheme to represent multiple types of primitives in a unified
manner. The proposed framework directly learns the process of primitive
assembly from large-scale human-crafted abstractions, enabling it to capture
how humans decompose complex shapes into primitive elements. Through extensive
experiments, we demonstrate that PrimitiveAnything can generate high-quality
primitive assemblies that better align with human perception while maintaining
geometric fidelity across diverse shape categories. It benefits various 3D
applications and shows potential for enabling primitive-based user-generated
content (UGC) in games. Project page: https://primitiveanything.github.io

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [90] [Aliasing Reduction in Neural Amp Modeling by Smoothing Activations](https://arxiv.org/abs/2505.04082)
*Ryota Sato,Julius O. Smith III*

Main category: eess.AS

TL;DR: 论文研究了改进神经网络激活函数以减少模拟音频硬件数字仿真中的混叠问题，并提出了新的混叠评估指标ASR。实验表明，平滑的激活函数能有效降低混叠且不影响建模精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络建模模拟音频硬件时非线性激活函数引起的混叠问题，作者探索了新的激活函数，旨在提升模型质量。

Method: 作者提出了Aliasing-to-Signal Ratio (ASR)作为混叠评估指标，并结合传统Error-to-Signal Ratio (ESR)，测试了多种激活函数及其拉伸因子的效果。

Result: 实验发现，曲线更平滑的激活函数能显著降低ASR，且ESR未显著增加，表明能同时实现高精度建模和低混叠。

Conclusion: 改进激活函数可有效减少神经放大器模型的混叠问题，为高质量音频硬件仿真提供了可行方案。

Abstract: The increasing demand for high-quality digital emulations of analog audio
hardware such as vintage guitar amplifiers has led to numerous works in
neural-network-based black-box modeling, with deep learning architectures like
WaveNet showing promising results. However, a key limitation in all of these
models is the aliasing artifacts that arise from the use of nonlinear
activation functions in neural networks. In this paper, we investigate novel
and modified activation functions aimed at mitigating aliasing within neural
amplifier models. Supporting this, we introduce a novel metric, the
Aliasing-to-Signal Ratio (ASR), which quantitatively assesses the level of
aliasing with high accuracy. Measuring also the conventional Error-to-Signal
Ratio (ESR), we conducted studies on a range of preexisting and modern
activation functions with varying stretch factors. Our findings confirmed that
activation functions with smoother curves tend to achieve lower ASR values,
indicating a noticeable reduction in aliasing. Notably, this improvement in
aliasing reduction was achievable without a substantial increase in ESR,
demonstrating the potential for high modeling accuracy with reduced aliasing in
neural amp models.

</details>


### [91] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TL;DR: 该论文提出了EchoInk-R1，一个基于强化学习的框架，用于提升多模态大语言模型（MLLMs）在音频和视觉信号结合时的跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本、视觉和音频感知方面取得了进展，但在结构化跨模态推理（尤其是音频与视觉信号的整合）方面仍存在局限。

Method: 基于Qwen2.5-Omni-7B模型，采用Group Relative Policy Optimization（GRPO）优化，利用AVQA-R1-6K数据集进行多选问答训练。

Result: EchoInk-R1在验证集上达到了85.77%的准确率，优于基础模型的80.53%，且仅需562步强化学习训练。

Conclusion: 轻量级强化学习微调能有效提升MLLMs的跨模态推理能力，EchoInk-R1首次通过强化学习实现了音频、视觉和文本模态的统一推理。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [92] [Pseudo Random Number Generator using Internet-of-Things Techniques on Portable Field-Programmable-Gate-Array Platform](https://arxiv.org/abs/2505.03741)
*Tee Hui Teo*

Main category: cs.CR

TL;DR: 论文比较了三种基于IoT的PRNG模型在FPGA平台上的性能，包括Logistic Map、Double Pendulum和Multi-LFSR，重点评估了随机性、延迟、功耗等指标。Logistic Map和Double Pendulum适合高安全性应用，但资源需求高；Multi-LFSR则更适合嵌入式或实时应用，具有更高的能效。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为IoT环境下的FPGA平台选择适合的PRNG模型，评估其在不同应用场景下的性能差异和适用性。

Method: 方法包括在FPGA平台上实现三种PRNG模型，并比较它们在随机性、延迟、功耗等关键性能指标上的表现。

Result: 结果显示Logistic Map和Double Pendulum在安全性方面表现优异，但资源消耗高；Multi-LFSR在能效和实时性方面更具优势。

Conclusion: 研究为选择PRNG模型提供了决策支持，强调了不同模型在不同应用场景下的适用性和权衡。

Abstract: This paper conducts a comparative study of three IoT-based PRNG models,
including Logistic Map, Double Pendulum, and Multi-LFSR, implemented on an FPGA
platform. Comparisons are made across key performance metrics like randomness,
latency, power consumption, hardware resource usage, energy efficiency,
scalability, and application suitability. Compared to Multi-LFSR, Logistic Map,
and Double Pendulum Models provide perfect quality randomness, which is quite
apt for high-security grade applications; however, the requirements of these
models concerning power and hardware resources are also considerably high. By
contrast, the Multi-LFSR comes into its own due to its lower latency, power
consumption, and resource-efficient design. It is, therefore, suited for
embedded or real-time applications. Furthermore, environmental sensors will
also be introduced as entropy sources for the PRNGs to enhance the randomness
of the systems, particularly in IoT-enabled battery-powered FPGA platforms. The
experimental results confirm that the Multi-LFSR model has the highest energy
efficiency, while the Logistic Map and Double Pendulum outperform in generating
numbers with very high security. The study thus provides a deeper insight into
decision-making for selecting PRNG models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [93] [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912)
*Can Cui,Pengxiang Ding,Wenxuan Song,Shuanghao Bai,Xinyang Tong,Zirui Ge,Runze Suo,Wanqi Zhou,Yang Liu,Bofang Jia,Han Zhao,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: 本文总结了现有双系统VLA（视觉-语言-动作）架构的结构设计，并通过系统实验评估其核心设计元素，最终提供了一个低成本开源模型供进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现有的双系统VLA架构在具身智能研究中很热门，但缺乏足够的开源工作以支持进一步性能分析和优化。

Method: 总结并比较现有双系统架构的结构设计，对其核心设计元素进行系统实验评估。

Result: 提供了一个低成本的开源模型，并计划持续更新更多实验结论和性能改进的开源模型。

Conclusion: 通过系统分析和开源模型的提供，为双系统VLA架构的研究和优化奠定了基础，并承诺持续更新资源。

Abstract: Dual-system VLA (Vision-Language-Action) architectures have become a hot
topic in embodied intelligence research, but there is a lack of sufficient
open-source work for further performance analysis and optimization. To address
this problem, this paper will summarize and compare the structural designs of
existing dual-system architectures, and conduct systematic empirical
evaluations on the core design elements of existing dual-system architectures.
Ultimately, it will provide a low-cost open-source model for further
exploration. Of course, this project will continue to update with more
experimental conclusions and open-source models with improved performance for
everyone to choose from. Project page: https://openhelix-robot.github.io/.

</details>


### [94] [Scalable Aerial GNSS Localization for Marine Robots](https://arxiv.org/abs/2505.04095)
*Shuo Wen,Edwin Meriaux,Mariana Sosa Guzmán,Charlotte Morissette,Chloe Si,Bobak Baghi,Gregory Dudek*

Main category: cs.RO

TL;DR: 传统水机器人定位方法受限于信号反射和高成本，本文提出利用无人机GNSS定位水面附近水机器人，实现高效、准确单/多机器人定位。


<details>
  <summary>Details</summary>
Motivation: 解决水机器人精确定位难题，克服GNSS信号反射、高成本及现有方法误差累积、计算复杂等问题。

Method: 采用配备GNSS的无人机，近距离追踪并定位水面水机器人。

Result: 新方法可实现单/多水机器人的高精度定位。

Conclusion: 无人机辅助GNSS定位为水机器人提供了一种高效、可扩展的解决方案。

Abstract: Accurate localization is crucial for water robotics, yet traditional onboard
Global Navigation Satellite System (GNSS) approaches are difficult or
ineffective due to signal reflection on the water's surface and its high cost
of aquatic GNSS receivers. Existing approaches, such as inertial navigation,
Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face
challenges like error accumulation and high computational complexity.
Therefore, a more efficient and scalable solution remains necessary. This paper
proposes an alternative approach that leverages an aerial drone equipped with
GNSS localization to track and localize a marine robot once it is near the
surface of the water. Our results show that this novel adaptation enables
accurate single and multi-robot marine robot localization.

</details>


### [95] [RGB-Event Fusion with Self-Attention for Collision Prediction](https://arxiv.org/abs/2505.04258)
*Pietro Bonazzi,Christian Vogt,Michael Jost,Haotong Qin,Lyes Khacef,Federico Paredes-Valles,Michele Magno*

Main category: cs.RO

TL;DR: 该论文提出了一种神经网络框架，通过结合RGB和事件相机数据来预测无人机与动态物体的碰撞时间和位置，融合模型在准确性上优于单模态方法，但计算成本更高。事件相机模型在性能和计算效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为提升无人机在动态环境中的避障能力，研究结合RGB和事件相机的多模态感知方法，以优化碰撞预测的准确性和实时性。

Method: 使用双编码器分支分别处理RGB和事件相机数据，通过自注意力机制融合，并在ABCD数据集上对比单模态与融合模型的性能。

Result: 融合模型在50Hz的预测频率下，准确性平均提升1%，远距离（>0.5m）提升10%，但内存和计算量分别增加71%和105%。事件相机模型在相似计算成本下，位置和时间误差分别减少4%和26%。量化为1-8位后，事件模型的性能与效率权衡明显。

Conclusion: 多模态融合能提升预测精度，但需权衡计算成本；事件相机在效率和性能上更具优势，量化可进一步优化部署。

Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe
operation of autonomous robots in dynamic, real-world environments. This paper
proposes a neural network framework for predicting the time and collision
position of an unmanned aerial vehicle with a dynamic object, using RGB and
event-based vision sensors. The proposed architecture consists of two separate
encoder branches, one for each modality, followed by fusion by self-attention
to improve prediction accuracy. To facilitate benchmarking, we leverage the
ABCD [8] dataset collected that enables detailed comparisons of single-modality
and fusion-based approaches. At the same prediction throughput of 50Hz, the
experimental results show that the fusion-based model offers an improvement in
prediction accuracy over single-modality approaches of 1% on average and 10%
for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%
in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for
position and 26% for time error at a similar computational cost, making it a
competitive alternative. Additionally, we evaluate quantized versions of the
event-based models, applying 1- to 8-bit quantization to assess the trade-offs
between predictive performance and computational efficiency. These findings
highlight the trade-offs of multi-modal perception using RGB and event-based
cameras in robotic applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [96] [Particle Gibbs without the Gibbs bit](https://arxiv.org/abs/2505.04611)
*Adrien Corenflos*

Main category: stat.CO

TL;DR: 文章提出了PGibbs改良方法，避免传统PGibbs因参数与状态轨迹高相关性导致的混合速度问题，类似PMMH的方式对轨迹分布边缘化处理，并通过CSMC算法在联合分布上实现。


<details>
  <summary>Details</summary>
Motivation: 解决现有PGibbs方法中参数与状态轨迹强相关性导致的混合速度慢的问题，同时在理论及实验上优于PIMH。

Method: 提出改良的PGibbs方法，通过CSMC算法在参数提案与当前参数的联合分布上实现，绕过Gibbs步骤，边缘化轨迹分布。

Result: 在PMMH处理困难的示例中展示了该方法优于传统方法的优势。

Conclusion: 改良的PGibbs方法解决了原方法的混合问题，且在实际应用中表现优于PMMH。

Abstract: Exact parameter and trajectory inference in state-space models is typically
achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or
particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly
proposes a new trajectory and parameter, and accepts or rejects both at once.
PGibbs instead alternates between sampling from the trajectory, using an
algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter
in a Hastings-within-Gibbs fashion. While particle independent Metropolis
Hastings (PIMH), the parameter-free version of PMMH, is known to be
statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter
and the state trajectory are very correlated. This has made PMMH the method of
choice for many practitioners, despite theory and experiments favouring CSMC
over PIMH for the parameter-free problem. In this article, we describe a
formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing
over the trajectory distribution in a fashion similar to PMMH. This is achieved
by considering the implementation of a CSMC algortihm for the state-space model
integrated over the joint distribution of the current parameter and the
parameter proposal. We illustrate the benefits of method on a simple example
known to be challenging for PMMH.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: 该课程设计开发了一个手写矩阵识别与分步可视化计算系统，通过AI和可视化技术帮助学生学习数学，使用Mamba骨干网络和YOLO模型实现精确识别与矩阵重构，结合CoordAttention提升空间定位，并通过Manim动画引擎分步展示计算过程。


<details>
  <summary>Details</summary>
Motivation: 解决学生在数学学习中因抽象公式和复杂计算步骤导致的难以理解的问题，通过创新的人机交互方式提升学习效果。

Method: 结合Mamba骨干网络和YOLO模型进行手写矩阵识别与重构，利用CoordAttention机制优化空间定位，并通过Manim动画引擎分步展示计算过程。

Result: 系统实现了高模块化和灵活性，能实时生成多种数学运算示例，帮助学生直观理解计算逻辑，提升学习效果。

Conclusion: 该系统具有扩展性和交互性，是一种直观、用户友好且高效的教育辅助工具，实现了“每一步都理解”的学习体验。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [98] [Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers](https://arxiv.org/abs/2505.04018)
*Xudong Jian,Kiran Bacsa,Gregory Duthé,Eleni Chatzi*

Main category: cs.CE

TL;DR: 本文提出了一种结合图神经网络（GNN）、变压器和物理信息损失函数的深度学习框架，用于模态分解和识别，无需标注数据即可有效提取结构动态响应的模态属性。


<details>
  <summary>Details</summary>
Motivation: 模态识别对结构健康监测和控制至关重要，但传统方法依赖于标注数据或特定假设。本文旨在开发一种无需标注、能适应多种结构和外部负载变化的通用方法。

Method: 通过变压器模块将多自由度结构动态测量分解为单自由度模态响应，并利用GNN捕捉结构配置和模态形状。模型采用物理信息损失函数和无监督训练，基于模态分解理论和模态独立性指导学习。

Result: 数值模拟和实验验证表明，模型能准确分解动态响应并识别模态属性（如自然频率、阻尼比和模态形状），且性能优于现有技术。

Conclusion: 该框架为基于群体的结构健康监测提供了高效、通用的解决方案，尤其在数据稀疏或负载多变场景中表现出色。

Abstract: Modal identification is crucial for structural health monitoring and
structural control, providing critical insights into structural dynamics and
performance. This study presents a novel deep learning framework that
integrates graph neural networks (GNNs), transformers, and a physics-informed
loss function to achieve modal decomposition and identification across a
population of structures. The transformer module decomposes
multi-degrees-of-freedom (MDOF) structural dynamic measurements into
single-degree-of-freedom (SDOF) modal responses, facilitating the
identification of natural frequencies and damping ratios. Concurrently, the GNN
captures the structural configurations and identifies mode shapes corresponding
to the decomposed SDOF modal responses. The proposed model is trained in a
purely physics-informed and unsupervised manner, leveraging modal decomposition
theory and the independence of structural modes to guide learning without the
need for labeled data. Validation through numerical simulations and laboratory
experiments demonstrates its effectiveness in accurately decomposing dynamic
responses and identifying modal properties from sparse structural dynamic
measurements, regardless of variations in external loads or structural
configurations. Comparative analyses against established modal identification
techniques and model variations further underscore its superior performance,
positioning it as a favorable approach for population-based structural health
monitoring.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [99] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TL;DR: 本文提出了一种渐进式甲骨文复制品发现框架，结合无监督低层关键点匹配与高层以文本为中心的内容匹配，显著提升了识别效率和准确性，发现了60多对新复制品。


<details>
  <summary>Details</summary>
Motivation: 甲骨文复制品的识别是甲骨文研究的基础问题，传统方法效率低且准确性不足，需要一种更有效的方法。

Method: 采用渐进式框架，结合无监督低层关键点匹配和高层以文本为中心的内容匹配，优化并排序候选复制品。

Result: 与现有方法相比，本方法在召回率和排名得分上表现优异，计算效率显著提升，并实际发现了60多对新复制品。

Conclusion: 该框架为甲骨文复制品识别提供了高效且有解释性的解决方案，推动了甲骨文研究的进展。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [100] [Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems](https://arxiv.org/abs/2505.04596)
*Mohammad Merati,David Castañón*

Main category: math.OC

TL;DR: 该论文提出了一种新颖的方法，通过结合卡尔曼滤波和动态网络流模型，优化PTZ相机的调度与控制，以提高动态监控环境中的实时捕获效率。


<details>
  <summary>Details</summary>
Motivation: 在动态监控环境中，传统的主从相机系统往往无法高效调度，导致覆盖不足或事件遗漏。本文旨在通过预测和优化技术提升系统的实时性和适应性。

Method: 方法结合了卡尔曼滤波（用于预测目标位置）和动态网络流模型（优化调度任务），并引入基于价值的优先级系统和分组跟踪节点以减少冗余监控。

Result: 仿真结果表明，该方法显著提高了覆盖率，减少了平均等待时间，并降低了事件遗漏率，优于传统系统。

Conclusion: 该研究为动态监控提供了一种高效、可扩展的解决方案，特别适用于拥挤或高动态环境。

Abstract: This paper presents a novel approach for optimizing the scheduling and
control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.
The proposed method integrates Kalman filters for motion prediction with a
dynamic network flow model to enhance real-time video capture efficiency. By
assigning Kalman filters to tracked objects, the system predicts future
locations, enabling precise scheduling of camera tasks. This prediction-driven
approach is formulated as a network flow optimization, ensuring scalability and
adaptability to various surveillance scenarios. To further reduce redundant
monitoring, we also incorporate group-tracking nodes, allowing multiple objects
to be captured within a single camera focus when appropriate. In addition, a
value-based system is introduced to prioritize camera actions, focusing on the
timely capture of critical events. By adjusting the decay rates of these values
over time, the system ensures prompt responses to tasks with imminent
deadlines. Extensive simulations demonstrate that this approach improves
coverage, reduces average wait times, and minimizes missed events compared to
traditional master-slave camera systems. Overall, our method significantly
enhances the efficiency, scalability, and effectiveness of surveillance
systems, particularly in dynamic and crowded environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [101] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TL;DR: 提出一种新颖的多模态大语言模型（LLM）不确定性校准方法，利用跨模态一致性改善校准效果，通过温度缩放技术优化校准过程，在多项任务中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法在多模态LLM中因一致性错误导致校准不佳，需改进校准效果以提升模型可靠性。

Method: 结合跨模态一致性（文本响应与视觉输入对齐）并应用温度缩放校准接地模型的置信度。

Result: 在Slake和VQAv2等任务中，校准效果显著优于现有方法。

Conclusion: 跨模态一致性及温度缩放有效提升多模态LLM的校准性能，增强模型可靠性。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [102] [Coverage Biases in High-Resolution Satellite Imagery](https://arxiv.org/abs/2505.03842)
*Vadim Musienko,Axel Jacquet,Ingmar Weber,Till Koebe*

Main category: cs.CY

TL;DR: 卫星影像在全球覆盖存在偏差，远离赤道的地区和发达地区更容易获得高分辨率影像，而欠发达地区和冲突区域的影像可用性较低。


<details>
  <summary>Details</summary>
Motivation: 研究卫星影像在不同地区的覆盖偏差，揭示信息获取的不平等问题，特别是因物理、社会经济和地缘政治因素导致的差异。

Method: 通过分析卫星轨道路径估算30天内的重访频率，并基于卫星影像提供商的元数据评估历史影像可用性；结合三个冲突地区的案例研究。

Result: 远离赤道的地区重访频率更高；欠发达和人口稀少地区历史影像较少；冲突区域的影像可用性受地缘政治影响。

Conclusion: 卫星影像的信息红利在全球分布不均，需关注覆盖偏差背后的物理和社会经济因素。

Abstract: Satellite imagery is increasingly used to complement traditional data
collection approaches such as surveys and censuses across scientific
disciplines. However, we ask: Do all places on earth benefit equally from this
new wealth of information? In this study, we investigate coverage bias of major
satellite constellations that provide optical satellite imagery with a ground
sampling distance below 10 meters, evaluating both the future on-demand tasking
opportunities as well as the availability of historic images across the globe.
Specifically, forward-looking, we estimate how often different places are
revisited during a window of 30 days based on the satellites' orbital paths,
thus investigating potential coverage biases caused by physical factors. We
find that locations farther away from the equator are generally revisited more
frequently by the constellations under study. Backward-looking, we show that
historic satellite image availability -- based on metadata collected from major
satellite imagery providers -- is influenced by socio-economic factors on the
ground: less developed, less populated places have less satellite images
available. Furthermore, in three small case studies on recent conflict regions
in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical
events play an important role in satellite image availability, hinting at
underlying business model decisions. These insights lay bare that the digital
dividend yielded by satellite imagery is not equally distributed across our
planet.

</details>


### [103] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TL;DR: 摘要概述了文本到图像（T2I）模型带来的深度伪造（deepfake）风险，通过分析Hugging Face和Civitai上的公开模型，发现大量可下载的深度伪造模型及其广泛传播的情况。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示深度伪造模型的普及性和潜在危害，尤其是非自愿亲密图像（NCII）的生成风险。

Method: 方法是通过对Hugging Face和Civitai上的数千个公开模型进行元数据分析，量化深度伪造模型的可访问性和传播情况。

Result: 结果显示近35,000个可公开下载的深度伪造模型，其中96%针对女性，并存在大量NCII意图。这些模型主要使用Stable Diffusion和Flux技术，通过LoRA微调技术极低成本生成。

Conclusion: 结论强调了尽管平台条款和法律禁止，深度伪造模型仍广泛传播，亟需采取更严厉的措施应对其危害。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [104] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TL;DR: 论文提出了一种基于用户意图的交互式视频故事系统，利用视觉语言模型和多智能体系统实现个性化体验，并以《哈利波特》为例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频故事交互方法局限于用户选择或预设叙事，缺乏定制化。为突破这些限制，研究旨在通过用户意图驱动的系统提升交互体验。

Method: 1) 使用视觉语言模型（VLM）理解视频故事；2) 结合检索增强生成（RAG）和多智能体系统（MAS）分三阶段处理：故事处理、多空间对话生成、场景定制化。

Result: 系统在《哈利波特》中成功展现了角色社交行为与成长的涌现特性，显著提升了交互体验。

Conclusion: 基于用户意图的交互系统通过VLM和MAS技术，实现了高度定制化的视频故事探索，为未来叙事交互提供了新方向。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [105] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TL;DR: 该论文提出了一种利用设备端大型语言模型（LLM）进行跨层无线漫游优化的方法，通过应用层的高层推理实时控制物理/ MAC层操作，显著提升漫游稳定性和信号质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值或启发式的无线漫游方案在动态移动环境中表现不佳，导致粘性切换或过度切换问题。为解决这一问题，作者探索了利用LLM在应用层进行智能决策的潜力。

Method: 论文提出了一个跨层框架，LLM完成两项任务：（1）基于上下文（如位置、时间）选择最佳AP；（2）动态调整漫游阈值。通过思维链提示、参数高效微调和量化等技术优化模型，以适配边缘硬件的低延迟和资源限制。

Result: 实验表明，该方法在室内外场景下均优于传统启发式和深度强化学习基线，实现了漫游稳定性与信号质量的平衡。

Conclusion: 该研究展示了应用层LLM推理在未来边缘系统底层无线控制中的潜力，为智能漫游提供了新思路。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [106] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: 该研究提出了一种结合开源遥感数据和AI模型的高效方法，用于检测有害藻华，整合了Sentinel-2影像、DEM和气候数据，并通过集成树模型和神经网络提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对水质和公共健康构成威胁，亟需高效、准确且低成本检测方法。

Method: 使用Sentinel-2光学影像、DEM和NOAA气候数据，通过GEE和MPC平台获取数据，结合树模型和神经网络构建集成分类模型。

Result: 树模型表现优异，加入神经网络后模型更鲁棒，能有效利用多样化遥感输入，适用于全球范围监测。

Conclusion: 该方法展示了遥感数据与AI结合解决环境问题的潜力，代码开源可供进一步应用。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [107] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的在线数据训练框架，首次将动态数据选择和增强技术统一，实现了训练效率与性能提升。通过估计样本的局部密度和多模态语义一致性联合分布，该方法能够有针对性地选择适合增强的样本，避免噪声或模糊数据，从而显著减少数据集规模而不影响模型泛化性。实验证明，该方法在多个基准数据集和架构上优于现有技术，例如在ImageNet-1k上减少50%训练成本且性能无损。


<details>
  <summary>Details</summary>
Motivation: 动态数据选择旨在加速训练且保持性能无损，但减少训练数据会限制数据多样性，从而影响泛化能力。虽然数据增强能提升多样性，但其与选择技术的结合尚未优化，无法充分发挥协同效应。为此，作者提出统一二者的新框架。

Method: 提出在线数据训练框架，通过估计样本的局部密度和多模态语义一致性联合分布，有针对性地选择适合增强的样本并抑制噪声或模糊数据的引入。

Result: 实验结果表明，该方法在多个基准数据集和架构上优于现有技术，例如在ImageNet-1k上减少50%训练成本且性能无损，同时增强了噪声抵抗能力和模型鲁棒性。

Conclusion: 该框架首次统一动态数据选择与增强技术，实现了高效训练与性能提升，展现了在实际场景中的实用价值。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [108] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAD的算法，通过多视角融合与单视角特征增强，提高样本效率并实现轻量级部署，展示了在Meta-World和ManiSkill3上的高效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为提高视觉伺服的鲁棒性，需使用多摄像头扩展视野，但计算复杂且部署成本高。

Method: 引入Merge And Disentanglement (MAD)算法，高效融合多视角以提升样本效率，同时增强单视角特征以实现轻量部署。

Result: 在Meta-World和ManiSkill3上验证了MAD算法的高效性和鲁棒性。

Conclusion: MAD算法在保持高效样本利用的同时，实现了低成本部署和鲁棒策略。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [109] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TL;DR: 提出了一种基于残差的神经校正策略，通过神经网络学习初始几何变换后的系统性失真，从而降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统坐标变换模型无法处理非线性且空间依赖的失真，导致地理空间应用中存在显著残差误差。

Method: 使用神经网络仅建模初始变换后的残差模式，减少模型复杂度。

Result: 在模拟和实际图像地理参考任务中，该方法比直接神经网络坐标转换和经典变换模型更准确稳定。

Conclusion: 残差建模是提升坐标变换准确性的轻量且稳健的替代方案。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>
