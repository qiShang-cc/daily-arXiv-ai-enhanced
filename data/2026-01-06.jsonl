{"id": "2601.00819", "pdf": "https://arxiv.org/pdf/2601.00819", "abs": "https://arxiv.org/abs/2601.00819", "authors": ["Houtianfu Wang", "Ozgur Akan"], "title": "AFDM for LEO Inter-Satellite Links: Path-Level CSI Prediction and CRLB-Guided Pre-Equalization", "categories": ["eess.SP", "eess.SY"], "comment": null, "summary": "Low-Earth-orbit (LEO) inter-satellite links must cope with strongly doubly selective channels and aged channel state information (CSI). In this paper, the term ``sensing'' refers to the receiver-side identifiability of a small set of dominant delay--Doppler path parameters, quantified via CRLB-type proxies, rather than a full-fledged target-sensing pipeline. Affine frequency division multiplexing (AFDM) provides a sparse delay--Doppler (DD) representation well suited to such channels, yet most existing AFDM designs assume ideal CSI, operate on grid-based channel coefficients, and optimize only communication performance. This paper proposes a two-stage AFDM-based ISAC framework for mobile LEO ISLs that explicitly operates under predicted CSI. In Stage~I, we model the channel by a small number of dominant specular paths and perform sequence prediction directly on their complex gains, delays, and Dopplers, from which we reconstruct the AFDM DD-domain kernel used as the sole instantaneous CSI at the transmitter. In Stage~II, we design a sensing-aware AFDM pre-equalizer by augmenting the classical minimum mean-square error (MMSE) solution with a term obtained from Cramér--Rao-type sensitivity measures evaluated under the predicted channel model, leading to a first-order surrogate of a CRLB-regularized pre-equalizer with a single tuning parameter that controls the communication--sensing tradeoff. Simulation results for representative LEO ISL trajectories show that the proposed path-level predictor improves effective-kernel reconstruction over AFDM-unaware baselines, and that, under predicted CSI, the sensing-aware pre-equalizer significantly improves sensing-oriented metrics over outdated-CSI baselines while keeping symbol error rates close to a communication-oriented MMSE design with only modest additional complexity."}
{"id": "2601.00820", "pdf": "https://arxiv.org/pdf/2601.00820", "abs": "https://arxiv.org/abs/2601.00820", "authors": ["Houtianfu Wang", "Haofan Dong", "Hanlin Cai", "Ozgur B. Akan"], "title": "Environment-to-Link ISAC with Space-Weather Sensing for Ka-Band LEO Downlinks", "categories": ["eess.SP", "eess.SY"], "comment": null, "summary": "Ka-band low-Earth-orbit (LEO) downlinks can suffer second-scale reliability collapses during flare-driven ionospheric disturbances, where fixed fade margins and reactive adaptive coding and modulation (ACM) are either overly conservative or too slow. This paper presents a GNSS-free, link-internal predictive controller that senses the same downlink via a geometry-free dual-carrier phase observable at 10~Hz: a high-pass filter and template-based onset detector, followed by a four-state nearly-constant-velocity Kalman filter, estimate $Δ$VTEC and its rate, and a short look-ahead (60~s) yields an endpoint outage probability used as a risk gate to trigger one-step discrete MCS down-switch and pilot-time update with hysteresis. Evaluation uses physics-informed log replay driven by real GOES X-ray flare morphologies under a disjoint-day frozen-calibration protocol, with uncertainty reported via paired moving-block bootstrap. Across stressed 60~s windows, the controller reduces peak BLER by 25--30\\% and increases goodput by 0.10--0.15~bps/Hz versus no-adaptation baselines under a unified link-level abstraction. The loop runs in $\\mathcal{O}(1)$ per 0.1~s epoch (about 0.042~ms measured), making on-board implementation feasible, and scope and deployment considerations for dispersion-dominated events are discussed."}
{"id": "2601.00999", "pdf": "https://arxiv.org/pdf/2601.00999", "abs": "https://arxiv.org/abs/2601.00999", "authors": ["Marcin Kolakowski", "Vitomir Djaja-Josko"], "title": "Dynamic Accuracy Estimation in a Wi-Fi-based Positioning System", "categories": ["eess.SP", "cs.LG"], "comment": "Originally presented at 2025 33rd Telecommunications Forum (TELFOR), Belgrade, Serbia", "summary": "The paper presents a concept of a dynamic accuracy estimation method, in which the localization errors are derived based on the measurement results used by the positioning algorithm. The concept was verified experimentally in a Wi\\nobreakdash-Fi based indoor positioning system, where several regression methods were tested (linear regression, random forest, k-nearest neighbors, and neural networks). The highest positioning error estimation accuracy was achieved for random forest regression, with a mean absolute error of 0.72 m."}
{"id": "2601.01033", "pdf": "https://arxiv.org/pdf/2601.01033", "abs": "https://arxiv.org/abs/2601.01033", "authors": ["Abidemi Orimogunje", "Hyunwoo Park", "Igbafe Orikumhi", "Sunwoo Kim", "Dejan Vukobratovic"], "title": "System-Level Comparison of Multimodal and In-Band mmWave Sensing for Beam Prediction in 6G ISAC", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) can reduce beam-training overhead in mmWave vehicle-to-infrastructure (V2I) links by enabling in-band sensing-based beam prediction, while exteroceptive sensors can further enhance the prediction accuracy. This work develop a system-level framework that evaluates camera, LiDAR, radar, GPS, and in-band mmWave power, both individually and in multimodal fusion using the DeepSense-6G Scenario-33 dataset. A latency-aware neural network composed of lightweight convolutional (CNN) and multilayer-perceptron (MLP) encoders predict a 64-beam index. We assess performance using Top-k accuracy alongside spectral-efficiency (SE) gap, signal-to-noise-ratio (SNR) gap, rate loss, and end-to-end latency. Results show that the mmWave power vector is a strong standalone predictor, and fusing exteroceptive sensors with it preserves high performance: mmWave alone and mmWave+LiDAR/GPS/Radar achieve 98% Top-5 accuracy, while mmWave+camera achieves 94% Top-5 accuracy. The proposed framework establishes calibrated baselines for 6G ISAC-assisted beam prediction in V2I systems."}
{"id": "2601.00969", "pdf": "https://arxiv.org/pdf/2601.00969", "abs": "https://arxiv.org/abs/2601.00969", "authors": ["Ali Salamatian", "Ke", "Ren", "Kieran Pattison", "Cyrus Neary"], "title": "Value Vision-Language-Action Planning & Search", "categories": ["cs.RO", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior."}
{"id": "2601.01152", "pdf": "https://arxiv.org/pdf/2601.01152", "abs": "https://arxiv.org/abs/2601.01152", "authors": ["Haojin Li", "Kaiqian Qu", "Chen Sun", "Anbang Zhang", "Xiaoxue Wang", "Wenqi Zhang", "Haijun Zhang"], "title": "Towards a Theoretical Framework for Robust Node Deployment in Cooperative ISAC Networks", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates node deployment strategies for robust multi-node cooperative localization in integrated sensing and communication (ISAC) networks.We first analyze how steering vector correlation across different positions affects localization performance and introduce a novel distance-weighted correlation metric to characterize this effect. Building upon this insight, we propose a deployment optimization framework that minimizes the maximum weighted steering vector correlation by optimizing simultaneously node positions and array orientations, thereby enhancing worst-case network robustness. Then, a genetic algorithm (GA) is developed to solve this min-max optimization, yielding optimized node positions and array orientations. Extensive simulations using both multiple signal classification (MUSIC) and neural-network (NN)-based localization validate the effectiveness of the proposed methods, demonstrating significant improvements in robust localization performance."}
{"id": "2601.00978", "pdf": "https://arxiv.org/pdf/2601.00978", "abs": "https://arxiv.org/abs/2601.00978", "authors": ["Yanyi Chen", "Min Deng"], "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly", "categories": ["cs.RO"], "comment": null, "summary": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions."}
{"id": "2601.01229", "pdf": "https://arxiv.org/pdf/2601.01229", "abs": "https://arxiv.org/abs/2601.01229", "authors": ["Furkan Genç", "Boran İsmet Macun", "Sait Sarper Özaslan", "Emine U. Saritas", "Tolga Çukur"], "title": "NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods."}
{"id": "2601.00981", "pdf": "https://arxiv.org/pdf/2601.00981", "abs": "https://arxiv.org/abs/2601.00981", "authors": ["Wenhui Chu", "Khang Tran", "Nikolaos V. Tsekos"], "title": "Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions", "categories": ["cs.RO", "cs.CV", "eess.SY"], "comment": "9 pages, 8 figures, published in ICBBB 2022", "summary": "Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms."}
{"id": "2601.01232", "pdf": "https://arxiv.org/pdf/2601.01232", "abs": "https://arxiv.org/abs/2601.01232", "authors": ["Yu Xue", "Kwantae Kim"], "title": "A 2.5 $μ$W 30 nV/$\\surd$Hz Instrumentation Amplifier for Bioimpedance Sensors with Source Degenerated Current Mirror and DTMOS Transistor", "categories": ["eess.SP"], "comment": "11 pages, 14 figures", "summary": "This paper proposes a low-power and low-noise instrumentation amplifier (IA) tailored for bioimpedance sensing applications. The design originates from a gain-boosted flipped voltage follower (FVF) transconductance (TC) stage and integrates two complementary circuit techniques to improve the noise performance. To achieve an optimal balance between input-referred noise and available voltage headroom, a source-degenerated current mirror (SDCM) is adopted, resulting in reducing the input-referred noise by 7.95% compared with a conventional current mirror structure. In addition, a dynamic threshold MOSFET (DTMOS) scheme is employed to enhance the effective transconductance, leading to a further 11.66% reduction in input-referred noise. Simulated in a 28 nm CMOS process demonstrate that the proposed IA achieves an input-referred noise floor of 30 nV/$\\surd$Hz and a bandwidth of 1.44 MHz, while consuming only 2.5 $μ$W from a 0.8 V supply. Compared to the baseline design, the proposed approach achieves a 32.4% reduction in power consumption without degrading noise performance. The complete design parameters are open-sourced in this paper, to ensure reproducibility and facilitate future developments."}
{"id": "2601.01067", "pdf": "https://arxiv.org/pdf/2601.01067", "abs": "https://arxiv.org/abs/2601.01067", "authors": ["Wenzheng Zhang", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Topological Mapping and Navigation using a Monocular Camera based on AnyLoc", "categories": ["cs.RO"], "comment": "Published in Proc. IEEE CASE 2025. 7 pages, 11 figures", "summary": "This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios."}
{"id": "2601.01277", "pdf": "https://arxiv.org/pdf/2601.01277", "abs": "https://arxiv.org/abs/2601.01277", "authors": ["Ximing Xie", "Fang Fang", "Zhiguo Ding", "Xianbin Wang"], "title": "Pinching Antennas in Blockage-Aware Environments: Modeling, Design, and Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Pinching-antenna (PA) systems have recently emerged as a promising member of the flexible-antenna family due to their ability to dynamically establish line-of-sight (LoS) links. While most existing studies assume ideal environments without obstacles, practical indoor deployments are often obstacle-rich, where LoS blockage significantly degrades performance. This paper investigates pinching-antenna systems in blockage-aware environments by developing a deterministic model for cylinder-shaped obstacles that precisely characterizes LoS conditions without relying on stochastic approximations. Based on this model, a special case is first studied where each PA serves a single user and can only be deployed at discrete positions along the waveguide. In this case, the waveguide-user assignment is obtained via the Hungarian algorithm, and PA positions are refined using a surrogate-assisted block-coordinate search. Then, a general case is considered where each PA serves all users and can be continuously placed along the waveguide. In this case, beamforming and PA positions are jointly optimized by a weighted minimum mean square error integrated deep deterministic policy gradient (WMMSE-DDPG) approach to address non-smooth LoS transitions. Simulation results demonstrate that the proposed algorithms significantly improve system throughput and LoS connectivity compared with benchmark methods. Moreover, the results reveal that pinching-antenna systems can effectively leverage obstacles to suppress co-channel interference, converting potential blockages into performance gains."}
{"id": "2601.01106", "pdf": "https://arxiv.org/pdf/2601.01106", "abs": "https://arxiv.org/abs/2601.01106", "authors": ["Michele Grimaldi", "Yosaku Maeda", "Hitoshi Kakami", "Ignacio Carlucho", "Yvan Petillot", "Tomoya Inoue"], "title": "Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment."}
{"id": "2601.01598", "pdf": "https://arxiv.org/pdf/2601.01598", "abs": "https://arxiv.org/abs/2601.01598", "authors": ["Anthony Joseph Perre", "Parker Huggins", "Alphan Sahin"], "title": "KAN-AE with Non-Linearity Score and Symbolic Regression for Energy-Efficient Channel Coding", "categories": ["eess.SP"], "comment": "IEEE Consumer Communications & Networking Conference 2026 (IEEE CCNC 2026), 9-12 January 2026, Las Vegas, NV, USA", "summary": "In this paper, we investigate Kolmogorov-Arnold network-based autoencoders (KAN-AEs) with symbolic regression (SR) for energy-efficient channel coding. By using SR, we convert KAN-AEs into symbolic expressions, which enables low-complexity implementation and improved energy efficiency at the radios. To further enhance the efficiency, we introduce a new non-linearity score term in the SR process to help select lower-complexity equations when possible. Through numerical simulations, we demonstrate that KAN-AEs achieve competitive BLER performance while improving energy efficiency when paired with SR. We score the energy efficiency of a KAN-AE implementation using the proposed non-linearity metric and compare it to a multi-layer perceptron-based autoencoder (MLP-AE). Our experiment shows that the KAN-AE paired with SR uses 1.38 times less energy than the MLP-AE, supporting that KAN-AEs are a promising choice for energy-efficient deep learning-based channel coding."}
{"id": "2601.01139", "pdf": "https://arxiv.org/pdf/2601.01139", "abs": "https://arxiv.org/abs/2601.01139", "authors": ["Sriram Rajasekar", "Ashwini Ratnoo"], "title": "Latent Space Reinforcement Learning for Multi-Robot Exploration", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings."}
{"id": "2601.01773", "pdf": "https://arxiv.org/pdf/2601.01773", "abs": "https://arxiv.org/abs/2601.01773", "authors": ["Chengwang Ji", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "title": "Joint Sparsity and Beamforming Design for RDARS-Aided Systems", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable distributed antennas and reflecting surface (RDARS) has emerged as a promising architecture for communication and sensing performance enhancement. In particular, the new selection gain can be achieved by leveraging the dynamic working mode selection between connection and reflection modes, whereas low-complexity element configuration remains an open issue. In this paper, we consider a RDARS-assisted communication system, where the connected elements are formed as a uniform sparse array for simplified mode configuration while achieving enlarged physical array aperture. The sum rate maximization problem is then formulated by jointly optimizing the active and passive beamforming matrices and sparsity of connected element array. For the special cases of a single user equipment (UE) and two UEs, the optimal sparsity designs are derived in closed-form. Then, for an arbitrary number of UEs, a weighted minimum mean-square error-based alternating optimization (AO) algorithm is proposed to tackle the non-convex optimization problem. Numerical results demonstrate the importance of optimizing the sparsity and the effectiveness of low-complexity sparsity optimization."}
{"id": "2601.01144", "pdf": "https://arxiv.org/pdf/2601.01144", "abs": "https://arxiv.org/abs/2601.01144", "authors": ["Shu Pan", "Simon Archieri", "Ahmet Cinar", "Jonatan Scharff Willners", "Ignacio Carlucho", "Yvan Petillot"], "title": "VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method."}
{"id": "2601.01791", "pdf": "https://arxiv.org/pdf/2601.01791", "abs": "https://arxiv.org/abs/2601.01791", "authors": ["Shunpu Tang", "Yuanyuan Jia", "Zijiu Yang", "Qianqian Yang", "Ruichen Zhang", "Jun Du", "Jihong Park", "Zhiguo Shi", "Khaled B. Letaief"], "title": "Rethinking Secure Semantic Communications in the Age of Generative and Agentic AI: Threats and Opportunities", "categories": ["eess.SP", "cs.IT", "cs.NI"], "comment": null, "summary": "Semantic communication (SemCom) improves communication efficiency by transmitting task-relevant information instead of raw bits and is expected to be a key technology for 6G networks. Recent advances in generative AI (GenAI) further enhance SemCom by enabling robust semantic encoding and decoding under limited channel conditions. However, these efficiency gains also introduce new security and privacy vulnerabilities. Due to the broadcast nature of wireless channels, eavesdroppers can also use powerful GenAI-based semantic decoders to recover private information from intercepted signals. Moreover, rapid advances in agentic AI enable eavesdroppers to perform long-term and adaptive inference through the integration of memory, external knowledge, and reasoning capabilities. This allows eavesdroppers to further infer user private behavior and intent beyond the transmitted content. Motivated by these emerging challenges, this paper comprehensively rethinks the security and privacy of SemCom systems in the age of generative and agentic AI. We first present a systematic taxonomy of eavesdropping threat models in SemCom systems. Then, we provide insights into how GenAI and agentic AI can enhance eavesdropping threats. Meanwhile, we also highlight potential opportunities for leveraging GenAI and agentic AI to design privacy-preserving SemCom systems."}
{"id": "2601.01155", "pdf": "https://arxiv.org/pdf/2601.01155", "abs": "https://arxiv.org/abs/2601.01155", "authors": ["Zhang Shizhe", "Liang Jingsong", "Zhou Zhitao", "Ye Shuhan", "Wang Yizhuo", "Tan Ming Siang Derek", "Chiun Jimmy", "Cao Yuhong", "Sartoretti Guillaume"], "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."}
{"id": "2601.01834", "pdf": "https://arxiv.org/pdf/2601.01834", "abs": "https://arxiv.org/abs/2601.01834", "authors": ["Tianyu Fang", "Xiaohua Zhou", "Yijie Mao"], "title": "On the Performance of Lossless Reciprocal MiLAC Architectures in Multi-User Networks", "categories": ["eess.SP"], "comment": null, "summary": "Microwave linear analog computer (MiLAC)-aided beamforming, which processes the transmitted symbols fully in the analog domain, has recently emerged as a promising alternative to fully digital and hybrid beamforming architectures for multiple-input multiple-output (MIMO) systems. While prior studies have shown that lossless and reciprocal MiLAC can achieve the same capacity as digital beamforming in a single-user MIMO network, its performance in multi-user scenarios remains unknown. To answer this question, in this work, we establish a downlink multi-user multiple-input single-output (MU-MISO) network with a MiLAC-aided transmitter, and investigate its sum-rate performance. Based on the microwave network theory, we first prove that lossless and reciprocal MiLAC cannot achieve the same performance as digital beamforming in a general MU-MISO network. Then, we formulate a sum-rate maximization problem and develop an efficient optimization framework to jointly optimize the power allocation and the scattering matrix for MiLAC. Numerical results validate our theoretical analysis and demonstrate that MiLAC is a promising architecture for future extremely large-scale MIMO systems."}
{"id": "2601.01188", "pdf": "https://arxiv.org/pdf/2601.01188", "abs": "https://arxiv.org/abs/2601.01188", "authors": ["Zhiwei Huang", "Yanwei Fu", "Yi Zhou", "Xieyuanli Chen", "Qijun Chen", "Rui Fan"], "title": "DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability."}
{"id": "2601.01956", "pdf": "https://arxiv.org/pdf/2601.01956", "abs": "https://arxiv.org/abs/2601.01956", "authors": ["Tang Shuntian", "Wu Xiaomei", "Wang Xinyi", "Zhao Le", "Yang Guang", "Liu Zilong", "Liu Fan", "Fei Zesong"], "title": "Doppler-Resilient LEO Satellite OFDM Transmission with Affine Frequency Domain Pilot", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, submitted to 2026 ICC Workshops", "summary": "Orthogonal frequency division multiplexing (OFDM) based low Earth orbit (LEO) satellite communication system suffers from severe Doppler shifts, while {the Doppler-resilient affine frequency-division multiplexing (AFDM) transmission suffers from significantly high processing complexity in data detection}. In this paper, we explore the channel estimation gain of affine frequency (AF) domain pilot to enhance the OFDM transmission under high mobility. Specifically, we propose a novel AF domain pilot embedding scheme for satellite-ground downlink OFDM systems for capturing the channel characteristics. By exploiting the autoregressive (AR) property of adjacent channels, a long short-term memory (LSTM) based predictor is designed to replace conventional interpolation operation in OFDM channel estimation. Simulation results show that the proposed transmission scheme significantly outperforms conventional OFDM scheme in terms of bit error rate (BER) under high Doppler scenarios, thus paving a new way for the design of next generation non-terrestrial network (NTN) communication systems."}
{"id": "2601.01196", "pdf": "https://arxiv.org/pdf/2601.01196", "abs": "https://arxiv.org/abs/2601.01196", "authors": ["Shenqi Lu", "Liangwei Zhang"], "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners", "categories": ["cs.RO"], "comment": null, "summary": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests."}
{"id": "2601.02219", "pdf": "https://arxiv.org/pdf/2601.02219", "abs": "https://arxiv.org/abs/2601.02219", "authors": ["Zihao Zhou", "Zhaolin Wang", "Yuanwei Liu"], "title": "Beam-Brainstorm: A Generative Site-Specific Beamforming Approach", "categories": ["eess.SP"], "comment": null, "summary": "Accurately understanding the propagation environment is a fundamental challenge in site-specific beamforming (SSBF). This paper proposes a novel generative SSBF (GenSSBF) solution, which represents a paradigm shift from conventional unstructured prediction to joint-structure modeling. First, considering the fundamental differences between beam generation and conventional image synthesis, a unified GenSSBF framework is proposed, which includes a site profile, a wireless prompting module, and a generator. Second, a beam-brainstorm (BBS) solution is proposed as an instantiation of this GenSSBF framework. Specifically, the site profile is configured by transforming channel data from spatial domain to a reversible latent space via discrete Fourier transform (DFT). To facilitate practical deployment, the wireless prompt is constructed from the reference signal received power (RSRP) measured using a small number of DFT-beams. Finally, the generator is developed using a customized conditional diffusion model. Rather than relying on a meticulously designed global codebook, BBS directly generates diverse and high-fidelity user-specific beams guided by the wireless prompts. Simulation results on accurate ray-tracing datasets demonstrate that BBS can achieve near-optimal beamforming gain while drastically reducing the beam sweeping overhead, even in low signal-to-noise ratio (SNR) environments."}
{"id": "2601.01282", "pdf": "https://arxiv.org/pdf/2601.01282", "abs": "https://arxiv.org/abs/2601.01282", "authors": ["Fang Nan", "Meher Malladi", "Qingqing Li", "Fan Yang", "Joonas Juola", "Tiziano Guadagnino", "Jens Behley", "Cesar Cadena", "Cyrill Stachniss", "Marco Hutter"], "title": "SAHA: Supervised Autonomous HArvester for selective forest thinning", "categories": ["cs.RO"], "comment": null, "summary": "Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management."}
{"id": "2601.02225", "pdf": "https://arxiv.org/pdf/2601.02225", "abs": "https://arxiv.org/abs/2601.02225", "authors": ["Yunping Mu", "Gongpu Wang", "Ruisi He", "Theodoros A. Tsiftsis", "Saman Atapattu", "Chintha Tellambura"], "title": "Backscatter-Assisted High-Speed Rail Communications in Straight Tunnel Environments: Effects of Tag Number and Phase Control", "categories": ["eess.SP"], "comment": "6 pages, 5 figures", "summary": "Backscatter communication is a promising technology to enhance the signal strength received by the receiver in straight tunnel environments. The impact of the number of tags and their phase adjustment on system performance remains a challenging issue though. Therefore, in this paper, we investigate the channel gain of backscatter-assisted communication with multiple tags in straight tunnels. In particular, we derive the probabilities that the backscatter link gain is greater than the direct link under adjustable and random phase assumptions by applying the Gaussian and Gamma approximations to derive tractable expressions. The simulation results show that phaseadjustable tags significantly improve the channel gain of the backscatter links compared to the random phase case. Moreover, the number of tags has an upper threshold for an effective tag deployment pattern. These insights provide valuable guidelines for the efficient design of backscatter communication systems in tunnel environments."}
{"id": "2601.01438", "pdf": "https://arxiv.org/pdf/2601.01438", "abs": "https://arxiv.org/abs/2601.01438", "authors": ["Russell Buchanan", "Adrian Röfer", "João Moura", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Online Estimation and Manipulation of Articulated Objects", "categories": ["cs.RO", "cs.AI"], "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Autonomous Robots, and is available online at [Link will be updated when available]", "summary": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects."}
{"id": "2601.02227", "pdf": "https://arxiv.org/pdf/2601.02227", "abs": "https://arxiv.org/abs/2601.02227", "authors": ["Hanyeol Ryu", "Sangkil Kim"], "title": "Ultra-low-power Monostatic Backscatter Platform with Phase-Aware Channel Estimation and System-Level Validation", "categories": ["eess.SP"], "comment": "19 pages, 18 figures", "summary": "This paper presents a novel channel-estimation (CE) method that mitigates residual phase drifts in backscatter links and a full hardware and signal-processing pipeline for a single-antenna monostatic system. The platform comprises a semi-passive tag, a software-defined radio (SDR) reader, and a 2x1 planar Yagi-Uda array (7 dBi with higher than 30 dB isolation) operating at 2.4 ~ 2.5 GHz. The developed backscatter fading model accounts for round-trip propagation and temporal correlation, and employs an analytically derived resource-optimal pilot allocation strategy. At the receiver, optimized least square (LS) and linear minimum mean square error (LMMSE) CE with pilot-aided carrier frequency offset (CFO) compensation feed a zero-forcing (ZF) equalizer to suppress ISI. The prototype delivers 500 kbps at 1 m with power of 158 uW (SDR baseband) and 10 uW (RF switch), yielding 320 pJ/bit. OOK and BPSK modulations achieve measured EVMs of 2.97 % and 4.02 %, respectively. Performance is validated by BER measurements and successful reconstruction of a full-color image in an over-the-air experiment. The results demonstrate an ultra-low-power, multimedia-capable backscatter IoT link and provide practical hardware-software co-design guidance for scalable deployments."}
{"id": "2601.01561", "pdf": "https://arxiv.org/pdf/2601.01561", "abs": "https://arxiv.org/abs/2601.01561", "authors": ["Yujian Qiu", "Yuqiu Mu", "Wen Yang", "Hao Zhu"], "title": "AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization", "categories": ["cs.RO"], "comment": null, "summary": "This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches."}
{"id": "2601.00973", "pdf": "https://arxiv.org/pdf/2601.00973", "abs": "https://arxiv.org/abs/2601.00973", "authors": ["William Consagra", "Eardi Lila"], "title": "Learned Hemodynamic Coupling Inference in Resting-State Functional MRI", "categories": ["eess.IV", "eess.SP", "stat.AP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Functional magnetic resonance imaging (fMRI) provides an indirect measurement of neuronal activity via hemodynamic responses that vary across brain regions and individuals. Ignoring this hemodynamic variability can bias downstream connectivity estimates. Furthermore, the hemodynamic parameters themselves may serve as important imaging biomarkers. Estimating spatially varying hemodynamics from resting-state fMRI (rsfMRI) is therefore an important but challenging blind inverse problem, since both the latent neural activity and the hemodynamic coupling are unknown. In this work, we propose a methodology for inferring hemodynamic coupling on the cortical surface from rsfMRI. Our approach avoids the highly unstable joint recovery of neural activity and hemodynamics by marginalizing out the latent neural signal and basing inference on the resulting marginal likelihood. To enable scalable, high-resolution estimation, we employ a deep neural network combined with conditional normalizing flows to accurately approximate this intractable marginal likelihood, while enforcing spatial coherence through priors defined on the cortical surface that admit sparse representations. The proposed approach is extensively validated using synthetic data and real fMRI datasets, demonstrating clear improvements over current methods for hemodynamic estimation and downstream connectivity analysis."}
{"id": "2601.01577", "pdf": "https://arxiv.org/pdf/2601.01577", "abs": "https://arxiv.org/abs/2601.01577", "authors": ["Tran Tien Dat", "Nguyen Hai An", "Nguyen Khanh Viet Dung", "Nguyen Duy Duc"], "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines"}
{"id": "2601.01023", "pdf": "https://arxiv.org/pdf/2601.01023", "abs": "https://arxiv.org/abs/2601.01023", "authors": ["João Morais", "Sadjad Alikhani", "Akshay Malhotra", "Shahab Hamidi-Rad", "Ahmed Alkhateeb"], "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning", "categories": ["cs.LG", "eess.SP"], "comment": "resources available in: https://www.wi-lab.net/research/dataset-similarity", "summary": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets."}
{"id": "2601.01618", "pdf": "https://arxiv.org/pdf/2601.01618", "abs": "https://arxiv.org/abs/2601.01618", "authors": ["Huajie Tan", "Peterson Co", "Yijie Xu", "Shanyu Rong", "Yuheng Ji", "Cheng Chi", "Xiansheng Chen", "Qiongyu Zhang", "Zhongxia Zhao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation", "categories": ["cs.RO"], "comment": "26 pages, 14 figures", "summary": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io"}
{"id": "2601.01065", "pdf": "https://arxiv.org/pdf/2601.01065", "abs": "https://arxiv.org/abs/2601.01065", "authors": ["Achraf Hsain", "Yahya Zaki", "Othman Abaakil", "Hibat-allah Bekkar", "Yousra Chtouki"], "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco", "categories": ["cs.LG", "eess.SP", "eess.SY"], "comment": "Published in IEEE GCAIoT 2024", "summary": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices."}
{"id": "2601.01651", "pdf": "https://arxiv.org/pdf/2601.01651", "abs": "https://arxiv.org/abs/2601.01651", "authors": ["Yucheng Xu", "Xiaofeng Mao", "Elle Miller", "Xinyu Yi", "Yang Li", "Zhibin Li", "Robert B. Fisher"], "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos", "categories": ["cs.RO"], "comment": null, "summary": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos."}
{"id": "2601.01194", "pdf": "https://arxiv.org/pdf/2601.01194", "abs": "https://arxiv.org/abs/2601.01194", "authors": ["Ozgur Ercetin", "Mohaned Chraiti"], "title": "On the Structure of the Optimal Detector for Sub-THz Multi-Hop Relays with Unknown Prior: Over-the-Air Diffusion", "categories": ["cs.IT", "eess.SP"], "comment": null, "summary": "Amplify and forward (AF) relaying is a viable strategy to extend the coverage of sub-terahertz (sub-THz) links, but inevitably propagates noise, leading to cumulative degradation across multiple hops. At the receiver, optimal decoding is desirable, yet challenging under non-Gaussian input distributions (video, voice, etc), for which neither the Minimum Mean Square Error (MMSE) estimator nor the mutual information admits a closed form. A further open question is whether knowledge of Channel State Information (CSI) and noise statistics at the intermediate relays is necessary for optimal detection. Aiming for an optimal decoder, this paper introduces a new framework that interprets the AF relay chain as a variance-preserving diffusion process and employs denoising diffusion implicit models (DDIMs) for signal recovery. We show that each AF hop is mathematically equivalent to a diffusion step with hop-dependent attenuation and noise injection. Consequently, the entire multi-hop chain collapses to an equivalent Gaussian channel fully described by only three real scalars per block: the cumulative complex gain and the effective noise variance. At the receiver, these end-to-end sufficient statistics define a matched reverse schedule that guides the DDIM-based denoiser, enabling near-optimal Bayesian decoding without per-hop CSI. We establish the information-theoretic foundation of this equivalence, proving that decoding performance depends solely on the final effective Signal-to-Noise-Ratio (SNR), regardless of intermediate noise/channel allocation or prior distribution. Simulations under AWGN and Rician fading confirm that the proposed AF-DDIM decoder reduces mean-squared error, symbol error rate, and bit error rate, particularly at moderate SNRs and for higher-order constellations."}
{"id": "2601.01675", "pdf": "https://arxiv.org/pdf/2601.01675", "abs": "https://arxiv.org/abs/2601.01675", "authors": ["Snehal s. Dikhale", "Karankumar Patel", "Daksh Dhingra", "Itoshi Naramura", "Akinobu Hayashi", "Soshi Iba", "Nawid Jamali"], "title": "VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data", "categories": ["cs.RO"], "comment": "Accepted for publication in IEEE Robotics and Automation Letters (RA-L), January 2022. Presented at ICRA 2022. This is the author's version of the manuscript", "summary": "Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots."}
{"id": "2601.01257", "pdf": "https://arxiv.org/pdf/2601.01257", "abs": "https://arxiv.org/abs/2601.01257", "authors": ["Gaetane Lorna N. Tchana", "Damaris Belle M. Fotso", "Antonio Hendricks", "Christophe Bobda"], "title": "Seamlessly Natural: Image Stitching with Natural Appearance Preservation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.GR", "eess.SP"], "comment": null, "summary": "This paper introduces SENA (SEamlessly NAtural), a geometry-driven image stitching approach that prioritizes structural fidelity in challenging real-world scenes characterized by parallax and depth variation. Conventional image stitching relies on homographic alignment, but this rigid planar assumption often fails in dual-camera setups with significant scene depth, leading to distortions such as visible warps and spherical bulging. SENA addresses these fundamental limitations through three key contributions. First, we propose a hierarchical affine-based warping strategy, combining global affine initialization with local affine refinement and smooth free-form deformation. This design preserves local shape, parallelism, and aspect ratios, thereby avoiding the hallucinated structural distortions commonly introduced by homography-based models. Second, we introduce a geometry-driven adequate zone detection mechanism that identifies parallax-minimized regions directly from the disparity consistency of RANSAC-filtered feature correspondences, without relying on semantic segmentation. Third, building upon this adequate zone, we perform anchor-based seamline cutting and segmentation, enforcing a one-to-one geometric correspondence across image pairs by construction, which effectively eliminates ghosting, duplication, and smearing artifacts in the final panorama.\n  Extensive experiments conducted on challenging datasets demonstrate that SENA achieves alignment accuracy comparable to leading homography-based methods, while significantly outperforming them in critical visual metrics such as shape preservation, texture integrity, and overall visual realism."}
{"id": "2601.01705", "pdf": "https://arxiv.org/pdf/2601.01705", "abs": "https://arxiv.org/abs/2601.01705", "authors": ["Kenneth Kwok", "Basura Fernando", "Qianli Xu", "Vigneshwaran Subbaraju", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Explicit World Models for Reliable Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations."}
{"id": "2601.01541", "pdf": "https://arxiv.org/pdf/2601.01541", "abs": "https://arxiv.org/abs/2601.01541", "authors": ["Antoine De Paepe", "Pascal Nguyen", "Michael Mabelle", "Cédric Saleun", "Antoine Jouadé", "Jean-Christophe Louvigne"], "title": "Sim2Real SAR Image Restoration: Metadata-Driven Models for Joint Despeckling and Sidelobes Reduction", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted at the Conference on Artificial Intelligence for Defense (CAID), 2025, Rennes, France", "summary": "Synthetic aperture radar (SAR) provides valuable information about the Earth's surface under all weather and illumination conditions. However, the inherent phenomenon of speckle and the presence of sidelobes around bright targets pose challenges for accurate interpretation of SAR imagery. Most existing SAR image restoration methods address despeckling and sidelobes reduction as separate tasks. In this paper, we propose a unified framework that jointly performs both tasks using neural networks (NNs) trained on a realistic SAR simulated dataset generated with MOCEM. Inference can then be performed on real SAR images, demonstrating effective simulation to real (Sim2Real) transferability. Additionally, we incorporate acquisition metadata as auxiliary input to the NNs, demonstrating improved restoration performance."}
{"id": "2601.01726", "pdf": "https://arxiv.org/pdf/2601.01726", "abs": "https://arxiv.org/abs/2601.01726", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions", "categories": ["cs.RO", "eess.SY"], "comment": "10 pages, 7 figures", "summary": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures."}
{"id": "2601.01616", "pdf": "https://arxiv.org/pdf/2601.01616", "abs": "https://arxiv.org/abs/2601.01616", "authors": ["Md Istiauk Hossain Rifat", "Moin Khan", "Mohammad Zunaed"], "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry", "categories": ["cs.LG", "eess.SP"], "comment": "9 pages, 9 figures", "summary": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads."}
{"id": "2601.01762", "pdf": "https://arxiv.org/pdf/2601.01762", "abs": "https://arxiv.org/abs/2601.01762", "authors": ["Yanhao Wu", "Haoyang Zhang", "Fei He", "Rui Wu", "Congpei Qiu", "Liang Gao", "Wei Ke", "Tong Zhang"], "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "underreview", "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety"}
{"id": "2601.01805", "pdf": "https://arxiv.org/pdf/2601.01805", "abs": "https://arxiv.org/abs/2601.01805", "authors": ["Masahiro Kurisaki"], "title": "Pathwise Representation of the Smoothing Distribution in Continuous-Time Linear Gaussian Models", "categories": ["math.ST", "eess.SP", "math.PR"], "comment": null, "summary": "We study the filtering and smoothing problem for continuous-time linear Gaussian systems. While classical approaches such as the Kalman-Bucy filter and the Rauch-Tung-Striebel (RTS) smoother provide recursive formulas for the conditional mean and covariance, we present a pathwise perspective that characterizes the smoothing error dynamics as an Ornstein-Uhlenbeck process. As an application, we show that standard filtering and smoothing equations can be uniformly derived as corollaries of our main theorem. In particular, we provide the first mathematically rigorous derivation of the Bryson-Frazier smoother in the continuous-time setting. Beyond offering a more transparent understanding of the smoothing distribution, our formulation enables pathwise sampling from it, which facilitates Monte Carlo methods for evaluating nonlinear functionals."}
{"id": "2601.01822", "pdf": "https://arxiv.org/pdf/2601.01822", "abs": "https://arxiv.org/abs/2601.01822", "authors": ["Shiyong Meng", "Tao Zou", "Bolei Chen", "Chaoxu Mu", "Jianxin Wang"], "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 4 figures", "summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy."}
{"id": "2601.02298", "pdf": "https://arxiv.org/pdf/2601.02298", "abs": "https://arxiv.org/abs/2601.02298", "authors": ["Mahmoud Elgenedy"], "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)", "categories": ["cs.CL", "eess.SP"], "comment": null, "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision."}
{"id": "2601.01872", "pdf": "https://arxiv.org/pdf/2601.01872", "abs": "https://arxiv.org/abs/2601.01872", "authors": ["Hongbo Duan", "Shangyi Luo", "Zhiyuan Deng", "Yanbo Chen", "Yuanhao Chiang", "Yi Liu", "Fangming Liu", "Xueqian Wang"], "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios", "categories": ["cs.RO"], "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency."}
{"id": "2601.01946", "pdf": "https://arxiv.org/pdf/2601.01946", "abs": "https://arxiv.org/abs/2601.01946", "authors": ["Sichao Song", "Yuki Okafuji", "Takuya Iwamoto", "Jun Baba", "Hiroshi Ishiguro"], "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail."}
{"id": "2601.01948", "pdf": "https://arxiv.org/pdf/2601.01948", "abs": "https://arxiv.org/abs/2601.01948", "authors": ["Zhihao Gu", "Ming Yang", "Difan Zou", "Dong Xu"], "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation", "categories": ["cs.RO"], "comment": "Accepted to AAAI2026", "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies."}
{"id": "2601.01969", "pdf": "https://arxiv.org/pdf/2601.01969", "abs": "https://arxiv.org/abs/2601.01969", "authors": ["Sichao Song", "Yuki Okafuji", "Kaito Ariu", "Amy Koike"], "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI", "categories": ["cs.RO"], "comment": null, "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings."}
{"id": "2601.01971", "pdf": "https://arxiv.org/pdf/2601.01971", "abs": "https://arxiv.org/abs/2601.01971", "authors": ["Aditya Singh", "Rajpal Singh", "Jishnu Keshavan"], "title": "Deep Robust Koopman Learning from Noisy Data", "categories": ["cs.RO"], "comment": null, "summary": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting."}
{"id": "2601.02078", "pdf": "https://arxiv.org/pdf/2601.02078", "abs": "https://arxiv.org/abs/2601.02078", "authors": ["Chenghao Yin", "Da Huang", "Di Yang", "Jichao Wang", "Nanshu Zhao", "Chen Xu", "Wenjun Sun", "Linjie Hou", "Zhijun Li", "Junhui Wu", "Zhaobo Liu", "Zhen Xiao", "Sheng Zhang", "Lei Bao", "Rui Feng", "Zhenquan Pang", "Jiayu Li", "Qian Wang", "Maoqing Yao"], "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot", "categories": ["cs.RO"], "comment": null, "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim."}
{"id": "2601.02085", "pdf": "https://arxiv.org/pdf/2601.02085", "abs": "https://arxiv.org/abs/2601.02085", "authors": ["Meili Sun", "Chunjiang Zhao", "Lichao Yang", "Hao Liu", "Shimin Hu", "Ya Xiong"], "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS."}
{"id": "2601.02125", "pdf": "https://arxiv.org/pdf/2601.02125", "abs": "https://arxiv.org/abs/2601.02125", "authors": ["Zhuoxiong Xu", "Xuanchen Li", "Yuhao Cheng", "Fei Xu", "Yichao Yan", "Xiaokang Yang"], "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches."}
{"id": "2601.02184", "pdf": "https://arxiv.org/pdf/2601.02184", "abs": "https://arxiv.org/abs/2601.02184", "authors": ["Yuhang Zhang", "Sören Schwertfeger"], "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors", "categories": ["cs.RO"], "comment": null, "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric."}
{"id": "2601.02295", "pdf": "https://arxiv.org/pdf/2601.02295", "abs": "https://arxiv.org/abs/2601.02295", "authors": ["Chenyang Ma", "Guangyu Yang", "Kai Lu", "Shitong Xu", "Bill Byrne", "Niki Trigoni", "Andrew Markham"], "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding", "categories": ["cs.RO"], "comment": "Project Page: https://dannymcy.github.io/cyclevla/", "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/"}
{"id": "2601.00844", "pdf": "https://arxiv.org/pdf/2601.00844", "abs": "https://arxiv.org/abs/2601.00844", "authors": ["Matthieu Destrade", "Oumayma Bounou", "Quentin Le Lidec", "Jean Ponce", "Yann LeCun"], "title": "Value-guided action planning with JEPA world models", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Presented as a poster at the World Modeling Workshop 2026, Mila", "summary": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks."}
{"id": "2601.00898", "pdf": "https://arxiv.org/pdf/2601.00898", "abs": "https://arxiv.org/abs/2601.00898", "authors": ["Ruiming Liang", "Yinan Zheng", "Kexin Zheng", "Tianyi Tan", "Jianxiong Li", "Liyuan Mao", "Zhihao Wang", "Guang Chen", "Hangjun Ye", "Jingjing Liu", "Jinqiao Wang", "Xianyuan Zhan"], "title": "Dichotomous Diffusion Policy Optimization", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications."}
{"id": "2601.00928", "pdf": "https://arxiv.org/pdf/2601.00928", "abs": "https://arxiv.org/abs/2601.00928", "authors": ["Luis Yoichi Morales", "Francesco Zanlungo", "David M. Woollard"], "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios."}
{"id": "2601.01003", "pdf": "https://arxiv.org/pdf/2601.01003", "abs": "https://arxiv.org/abs/2601.01003", "authors": ["Amin Abyaneh", "Charlotte Morissette", "Mohamad H. Danesh", "Anas El Houssaini", "David Meger", "Gregory Dudek", "Hsiu-Chin Lin"], "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations", "categories": ["cs.LG", "cs.RO"], "comment": "Under review at ICLR 2026", "summary": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity."}
{"id": "2601.01076", "pdf": "https://arxiv.org/pdf/2601.01076", "abs": "https://arxiv.org/abs/2601.01076", "authors": ["Devesh Nath", "Haoran Yin", "Glen Chou"], "title": "Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.RO", "math.OC"], "comment": "Under review, 28 pages, 12 figures", "summary": "We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods."}
{"id": "2601.01210", "pdf": "https://arxiv.org/pdf/2601.01210", "abs": "https://arxiv.org/abs/2601.01210", "authors": ["Kazuhiko Murasaki", "Shunsuke Konagai", "Masakatsu Aoki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts."}
{"id": "2601.01218", "pdf": "https://arxiv.org/pdf/2601.01218", "abs": "https://arxiv.org/abs/2601.01218", "authors": ["Ka Yan Fung", "Tze Leung Rick Lui", "Yuxing Tao", "Kuen Fung Sin"], "title": "MotiBo: The Impact of Interactive Digital Storytelling Robots on Student Motivation through Self-Determination Theory", "categories": ["cs.HC", "cs.MM", "cs.RO"], "comment": null, "summary": "Creativity is increasingly recognized as an important skill in education, and storytelling can enhance motivation and engagement among students. However, conventional storytelling methods often lack the interactive elements necessary to engage students. To this end, this study examines the impact of an interactive digital storytelling system incorporating a human-like robot on student engagement and creativity. The study aims to compare engagement levels across three modalities: paper-based, PowerPoint, and robot-assisted storytelling, MotiBo. Utilizing a quasi-experimental design, this work involves three groups of students who interact with the storytelling system over a five-day learning. Findings reveal that students using MotiBo exhibit statistically significant improvement in behavioural and cognitive engagement compared to those using traditional methods. These results suggest that the integration of novel technologies can effectively enhance the learning experience, ultimately promoting creativity and self-learning ability in educational settings. Future research will investigate the long-term effects of these technologies on learning outcomes and explore their potential for broader applications in diverse educational contexts."}
{"id": "2601.01227", "pdf": "https://arxiv.org/pdf/2601.01227", "abs": "https://arxiv.org/abs/2601.01227", "authors": ["Ka Yan Fung", "Kwong Chiu Fung", "Yuxing Tao", "Tze Leung Rick Lui", "Kuen Fung Sin"], "title": "LiveBo: Empowering Non-Chinese Speaking Students through AI-Driven Real-Life Scenarios in Cantonese", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Language learning is a multifaceted process. Insufficient vocabulary can hinder communication and lead to demotivation. For non-Chinese speaking (NCS) students, learning Traditional Chinese (Cantonese) poses distinct challenges, particularly due to the complexity of converting spoken and written forms. To address this issue, this study examines the effectiveness of real-life scenario simulations integrated with interactive social robots in enhancing NCS student engagement and language acquisition. The research employs a quasi-experimental design involving NCS students who interact with an AI-driven, robot-assisted language learning system, LiveBo. The study aims to assess the impact of this innovative approach on active participation and motivation. Data are collected through proficiency tests, questionnaires and semi-structured interviews. Findings indicate that NCS students experience positive improvements in behavioural and emotional engagement, motivation and learning outcomes, highlighting the potential of integrating novel technologies in language education. We plan to compare with the control group in the future. This study highlights the significance of interactive and immersive learning experiences in promoting motivation and enhancing language acquisition among NCS students."}
{"id": "2601.01234", "pdf": "https://arxiv.org/pdf/2601.01234", "abs": "https://arxiv.org/abs/2601.01234", "authors": ["Ka-Yan Fung", "Yuxing Tao", "Tze-Leung", "Rick Lui", "Kuen-Fung Sin"], "title": "Bridging Language Gaps: Utilizing Interactive Robots to Teach Cantonese in Real-Life Contexts for Newly-Arrived Children", "categories": ["cs.ET", "cs.HC", "cs.RO"], "comment": null, "summary": "Hong Kong's education system is notably multicultural, including local, non-Chinese-speaking, and newly arrived students (NAS) (Mandarine Chinese-speaking). NAS can guess the meaning of vocabulary but cannot speak out, presenting unique challenges for them, particularly language barriers and cultural differences. These challenges hinder their academic success and social integration, leading to feelings of isolation and demotivation. Current resources often fail to address the emotional well-being of these students and predominantly focus on English language acquisition, leaving a gap in support for learning Cantonese and navigating the local cultural landscape. This study explores the effectiveness of an interactive robot, Boon Boon, in teaching Cantonese through real-life contexts to enhance NAS children learning engagement and motivation. The research questions are: (1) How does interactive robot-empowered scenario learning influence the learning engagement and motivation of NAS in learning Cantonese? and (2) What is the impact of a robot-empowered scenario learning system on the Cantonese language proficiency of NAS? Fourteen children are invited to participate in a four-day learning program with Boon Boon. The preliminary result indicated that Boon Boon drove students' attention to learning and academic achievement. Future research will focus on long-term assessments of robot-empowered learning's effectiveness and explore the scalability of this approach across diverse educational settings and cultural backgrounds."}
{"id": "2601.01288", "pdf": "https://arxiv.org/pdf/2601.01288", "abs": "https://arxiv.org/abs/2601.01288", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Benjamin Ultan Cowley"], "title": "PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS", "categories": ["cs.GR", "cs.AI", "cs.PF", "cs.RO"], "comment": null, "summary": "Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender."}
{"id": "2601.01409", "pdf": "https://arxiv.org/pdf/2601.01409", "abs": "https://arxiv.org/abs/2601.01409", "authors": ["Chuyuan Tao", "Fanxin Wang", "Haolong Jiang", "Jia He", "Yiyang Chen", "Qinglei Bu"], "title": "Sampling Strategy Design for Model Predictive Path Integral Control on Legged Robot Locomotion", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has emerged as a powerful sampling-based optimal control method for complex, nonlinear, and high-dimensional systems. However, directly applying MPPI to legged robotic systems presents several challenges. This paper systematically investigates the role of sampling strategy design within the MPPI framework for legged robot locomotion. Based upon the idea of structured control parameterization, we explore and compare multiple sampling strategies within the framework, including both unstructured and spline-based approaches. Through extensive simulations on a quadruped robot platform, we evaluate how different sampling strategies affect control smoothness, task performance, robustness, and sample efficiency. The results provide new insights into the practical implications of sampling design for deploying MPPI on complex legged systems."}
{"id": "2601.01528", "pdf": "https://arxiv.org/pdf/2601.01528", "abs": "https://arxiv.org/abs/2601.01528", "authors": ["Yang Zhou", "Hao Shao", "Letian Wang", "Zhuofan Zong", "Hongsheng Li", "Steven L. Waslander"], "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "10 pages, 4 figures; Project Website: https://drivinggen-bench.github.io/", "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making."}
{"id": "2601.01696", "pdf": "https://arxiv.org/pdf/2601.01696", "abs": "https://arxiv.org/abs/2601.01696", "authors": ["Yian Liu", "Xiong Wang", "Ping Xu", "Lei Zhu", "Ming Yan", "Linyun Xue"], "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems."}
{"id": "2601.01989", "pdf": "https://arxiv.org/pdf/2601.01989", "abs": "https://arxiv.org/abs/2601.01989", "authors": ["Aly R. Elkammar", "Karim M. Gamaleldin", "Catherine M. Elias"], "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies."}
{"id": "2601.02082", "pdf": "https://arxiv.org/pdf/2601.02082", "abs": "https://arxiv.org/abs/2601.02082", "authors": ["Yueyang Wang", "Mehmet Dogar", "Gustav Markkula"], "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation."}
