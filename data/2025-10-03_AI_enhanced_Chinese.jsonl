{"id": "2510.01213", "pdf": "https://arxiv.org/pdf/2510.01213", "abs": "https://arxiv.org/abs/2510.01213", "authors": ["Tao Han", "Ang Li", "Qinyu Chen", "Chang Gao"], "title": "JaneEye: A 12-nm 2K-FPS 18.9-$\u03bc$J/Frame Event-based Eye Tracking Accelerator", "categories": ["eess.SP", "cs.AR", "cs.CV", "cs.HC", "eess.IV"], "comment": "Accepted to 2026 IEEE 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC) 2026", "summary": "Eye tracking has become a key technology for gaze-based interactions in\nExtended Reality (XR). However, conventional frame-based eye-tracking systems\noften fall short of XR's stringent requirements for high accuracy, low latency,\nand energy efficiency. Event cameras present a compelling alternative, offering\nultra-high temporal resolution and low power consumption. In this paper, we\npresent JaneEye, an energy-efficient event-based eye-tracking hardware\naccelerator designed specifically for wearable devices, leveraging sparse,\nhigh-temporal-resolution event data. We introduce an ultra-lightweight neural\nnetwork architecture featuring a novel ConvJANET layer, which simplifies the\ntraditional ConvLSTM by retaining only the forget gate, thereby halving\ncomputational complexity without sacrificing temporal modeling capability. Our\nproposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+\ndataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To\nfurther enhance hardware efficiency, we employ custom linear approximations of\nactivation functions (hardsigmoid and hardtanh) and fixed-point quantization.\nThrough software-hardware co-design, our 12-nm ASIC implementation operates at\n400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames\nPer Second (FPS)) at an energy efficiency of 18.9 $\\mu$J/frame. JaneEye sets a\nnew benchmark in low-power, high-performance eye-tracking solutions suitable\nfor integration into next-generation XR wearables.", "AI": {"tldr": "JaneEye\u662f\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u9ad8\u6548\u773c\u7403\u8ffd\u8e2a\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u4e13\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\u548c\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u4f4e\u529f\u8017\u3002", "motivation": "\u4f20\u7edf\u7684\u5e27\u57fa\u773c\u7403\u8ffd\u8e2a\u7cfb\u7edf\u5728XR\u5e94\u7528\u4e2d\u96be\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u529f\u8017\u7684\u8981\u6c42\uff0c\u4e8b\u4ef6\u6444\u50cf\u5934\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u65f6\u95f4\u548c\u80fd\u91cf\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edcConvJANET\uff0c\u7b80\u5316\u4e86ConvLSTM\u7ed3\u6784\u5e76\u4f7f\u7528\u81ea\u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570\u548c\u5b9a\u70b9\u91cf\u5316\u3002\u786c\u4ef6\u4e0a\u91c7\u7528\u4e8612nm ASIC\u8bbe\u8ba1\u3002", "result": "\u57283ET+\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e862.45\u50cf\u7d20\u8bef\u5dee\uff0c1250Hz\u4e8b\u4ef6\u5e27\u7387\uff0c0.5ms\u5ef6\u8fdf\uff0c\u80fd\u6548\u4e3a18.9\u03bcJ/\u5e27\u3002", "conclusion": "JaneEye\u4e3a\u4e0b\u4e00\u4ee3XR\u53ef\u7a7f\u6234\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u773c\u7403\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01408", "pdf": "https://arxiv.org/pdf/2510.01408", "abs": "https://arxiv.org/abs/2510.01408", "authors": ["Jeong Min Kong", "Ian P. Roberts"], "title": "Satellite Assignment Policy Learning for Coexistence in LEO Networks", "categories": ["eess.SP"], "comment": null, "summary": "Unlike in terrestrial cellular networks, certain frequency bands for\nlow-earth orbit (LEO) satellite systems have thus far been allocated on a\nnon-exclusive basis. In this context, systems that launch their satellites\nearlier (referred to as primary systems) are given spectrum access priority\nover those that launch later, known as secondary systems. For a secondary\nsystem to function, it is expected to either coordinate with primary systems or\nensure that it does not cause excessive interference to primary ground users.\nReliably meeting this interference constraint requires real-time knowledge of\nthe receive beams of primary users, which in turn depends on the primary\nsatellite-to-primary user associations. However, in practice, primary systems\nhave thus far not publicly disclosed their satellite assignment policies;\ntherefore, it becomes essential for secondary systems to develop methods to\ninfer such policies. Assuming there is limited historical data indicating which\nprimary satellites have served which primary users, we propose an end-to-end\ngraph structure learning-based algorithm for learning highest elevation primary\nsatellite assignment policies, that, upon deployment, can directly map the\nprimary satellite coordinates into assignment decisions for the primary users.\nSimulation results show that our method can outperform the best baseline,\nachieving approximately a 15% improvement in prediction accuracy.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7cfb\u7edf\u4e2d\u9891\u6bb5\u975e\u72ec\u5360\u5206\u914d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u56fe\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u63a8\u65ad\u4e3b\u8981\u536b\u661f\u5206\u914d\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u4e3b\u8981\u7cfb\u7edf\u672a\u516c\u5f00\u5176\u536b\u661f\u5206\u914d\u7b56\u7565\uff0c\u6b21\u8981\u7cfb\u7edf\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u63a8\u65ad\u8fd9\u4e9b\u7b56\u7565\u4ee5\u786e\u4fdd\u9891\u6bb5\u5171\u4eab\u65f6\u7684\u5e72\u6270\u63a7\u5236\u3002", "method": "\u57fa\u4e8e\u6709\u9650\u7684\u5386\u53f2\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u5750\u6807\u76f4\u63a5\u6620\u5c04\u4e3a\u4e3b\u8981\u7528\u6237\u5206\u914d\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e86\u7ea615%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6b21\u8981\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e3b\u8981\u536b\u661f\u5206\u914d\u7b56\u7565\u63a8\u65ad\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u9891\u6bb5\u5171\u4eab\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.01411", "pdf": "https://arxiv.org/pdf/2510.01411", "abs": "https://arxiv.org/abs/2510.01411", "authors": ["Hibatallah Alwazani", "Omran Abbas", "Loic Markley", "Anas Chaaban"], "title": "Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities", "categories": ["eess.SP"], "comment": "7 pages, 3 figures", "summary": "Stacked intelligent surfaces (SIS)s have been proposed recently as an\nenabling technology for Holographic Multiple Input Multiple Output (HMIMO) and\nUltra-massive MIMO (umMIMO) technologies. Their utility can extend beyond\nspatial wave-domain processing of signals if they are enhanced with\nstrategically-tuned symbol-duration level delays to enable temporal processing\nas well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).\nWe shed light on the feasibility of realizing delay units in an SIS. Then, we\ndiscuss the relevance of the proposed DA-SIS and present a use case that\nillustrates its potential, wherein the DA-SIS serves as an analog equalizer\nthat aids in eliminating multi-path-induced inter-symbol-interference (ISI). We\nshow how the number of elements affect the equalization process using the bit\nerror rate (BER) as a metric, and demonstrate the potential of the DA-SIS in\nequalization via comparing with digital equalizers as a benchmark. Finally, we\npresent opportunities and future research directions that can be undertaken to\nbring this idea to fruition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5ef6\u8fdf\u589e\u5f3a\u578b\u5806\u53e0\u667a\u80fd\u8868\u9762(DA-SIS)\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u589e\u52a0\u65f6\u95f4\u5ef6\u8fdf\u5904\u7406\u529f\u80fd\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6d88\u9664\u591a\u5f84\u5e72\u6270\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u667a\u80fd\u8868\u9762\u5728\u7a7a\u95f4\u6ce2\u57df\u4fe1\u53f7\u5904\u7406\u5916\u7684\u529f\u80fd\uff0c\u5f15\u5165\u65f6\u95f4\u5ef6\u8fdf\u5904\u7406\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76DA-SIS\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5747\u8861\u5668\u7684\u5e94\u7528\u6848\u4f8b\u5c55\u793a\u5176\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u5143\u7d20\u6570\u91cf\u5bf9\u5747\u8861\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "DA-SIS\u5728\u6d88\u9664\u591a\u5f84\u5e72\u6270\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7BER\u6307\u6807\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e0e\u6570\u5b57\u5747\u8861\u5668\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "DA-SIS\u4e3a\u667a\u80fd\u8868\u9762\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5e94\u7528\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.01417", "pdf": "https://arxiv.org/pdf/2510.01417", "abs": "https://arxiv.org/abs/2510.01417", "authors": ["Alex Paul Hoffmann", "Matthew G. Finley", "Eftyhia Zesta", "Mark B. Moldwin", "Lauro V. Ojeda"], "title": "A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection", "categories": ["eess.SP", "physics.ins-det"], "comment": "13 pages, 5 figures", "summary": "Landmines have been extensively used in conflict zones as an indiscriminate\nweapon to control military movements, often remaining active long after\nhostilities have ended. Their presence poses a persistent danger to civilians,\nhindering post-war recovery efforts, causing injuries or death, and restricting\naccess to essential land for agriculture and infrastructure. Unmanned aerial\nvehicles (UAV) equipped with magnetometers are commonly used to detect remnant\nhidden landmines but come with significant technical challenges due to magnetic\nfield interference from UAV electronics such as motors. We propose the use of a\nframe-mounted UAV-borne two-magnetometer payload to perform a two-step\nautomated interference removal and landmine detection analysis. The first step\nremoves interference via the Wavelet-Adaptive Interference Cancellation for\nUnderdetermined Platform (WAIC-UP) method designed for spaceflight\nmagnetometers. The second method uses the Rapid Unsupervised Detection of\nEvents (RUDE) algorithm to detect landmine signatures. This two-step\nWAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity\nordinance detection at a low computational cost and simplifies the design of\nmagnetic survey payloads. We validate the method through a Monte Carlo\nsimulation of randomized landmine placements in a 10 x 10 m square grid and\ndrone motor interference. Additionally, we assess the efficacy of the algorithm\nby varying the drone's altitude, examining its performance at different heights\nabove the ground.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u642d\u8f7d\u53cc\u78c1\u529b\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u81ea\u52a8\u5316\u65b9\u6cd5\uff08WAIC-UP\u548cRUDE\u7b97\u6cd5\uff09\u53bb\u9664\u5e72\u6270\u5e76\u68c0\u6d4b\u5730\u96f7\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5730\u96f7\u5728\u51b2\u7a81\u5730\u533a\u957f\u671f\u5a01\u80c1\u5e73\u6c11\u5b89\u5168\u5e76\u963b\u788d\u6218\u540e\u6062\u590d\uff0c\u73b0\u6709\u65e0\u4eba\u673a\u78c1\u529b\u8ba1\u68c0\u6d4b\u6280\u672f\u56e0\u7535\u5b50\u5e72\u6270\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1aWAIC-UP\u53bb\u9664\u65e0\u4eba\u673a\u7535\u5b50\u5e72\u6270\uff0cRUDE\u7b97\u6cd5\u68c0\u6d4b\u5730\u96f7\u4fe1\u53f7\uff1b\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u572810x10\u7c73\u7f51\u683c\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5bf9\u65e0\u4eba\u673a\u9ad8\u5ea6\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u78c1\u529b\u8ba1\u6846\u67b6\u548c\u4e24\u6b65\u6cd5\u4e3a\u5730\u96f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01348", "pdf": "https://arxiv.org/pdf/2510.01348", "abs": "https://arxiv.org/abs/2510.01348", "authors": ["Michal Werner", "David \u010capek", "Tom\u00e1\u0161 Musil", "Ond\u0159ej Fran\u011bk", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge", "categories": ["cs.RO", "x", "I.2.9"], "comment": "8 pages", "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied\nenvironments is challenging: integrating odometry leads to drift, loop closures\nare unavailable in previously unseen areas and embedded platforms provide\nlimited computational power. We present a fully onboard UAV system developed\nfor the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km\nlong-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS\nor prior dense mapping. The system integrates perception, mapping, planning,\nand control with a lightweight drift-correction method that matches\nLiDAR-derived local heightmaps to a prior geo-data heightmap via\ngradient-template matching and fuses the evidence with odometry in a clustered\nparticle filter. Deployed during the competition, the system executed\nkilometer-scale flights across urban, forest, and open-field terrain and\nreduced drift substantially relative to raw odometry, while running in real\ntime on CPU-only hardware. We describe the system architecture, the\nlocalization pipeline, and the competition evaluation, and we report practical\ninsights from field deployment that inform the design of GNSS-denied UAV\nautonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GNSS\u7f3a\u5931\u73af\u5883\u4e0b\u5b9e\u73b0\u65e0\u4eba\u673a\u957f\u8ddd\u79bb\u53ef\u9760\u98de\u884c\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\u548c\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u663e\u8457\u51cf\u5c11\u6f02\u79fb\u5e76\u5728CPU\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u5728GNSS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u957f\u8ddd\u79bb\u98de\u884c\u9762\u4e34\u6f02\u79fb\u3001\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7b49\u6311\u6218\uff0c\u9700\u8981\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u611f\u77e5\u3001\u5efa\u56fe\u3001\u89c4\u5212\u548c\u63a7\u5236\uff0c\u91c7\u7528\u68af\u5ea6\u6a21\u677f\u5339\u914d\u548c\u805a\u7c7b\u7c92\u5b50\u6ee4\u6ce2\u5668\u8fdb\u884c\u6f02\u79fb\u6821\u6b63\u3002", "result": "\u5728\u6bd4\u8d5b\u4e2d\uff0c\u7cfb\u7edf\u5728\u57ce\u5e02\u3001\u68ee\u6797\u548c\u5f00\u9614\u5730\u5e26\u5b9e\u73b0\u516c\u91cc\u7ea7\u98de\u884c\uff0c\u6f02\u79fb\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aGNSS\u7f3a\u5931\u4e0b\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u53c2\u8003\u3002"}}
{"id": "2510.01437", "pdf": "https://arxiv.org/pdf/2510.01437", "abs": "https://arxiv.org/abs/2510.01437", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates a full-duplex (FD) scenario where a base station (BS)\nequipped with movable antennas (MAs) simultaneously provides communication\nservices to a set of downlink (DL) and uplink (UL) users while also enabling\nsensing functionalities for target detection, thereby supporting integrated\nsensing and communication (ISAC) technology. Additionally, a receiving BS, also\nequipped with MAs (denoted as BS R), is responsible for capturing the reflected\necho. To optimize this setup, we formulate an optimization problem aimed at\nmaximizing the signal-to-noise and interference ratio (SINR) of the captured\necho. This is achieved by jointly optimizing the transmit beamforming vectors\nat the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the\nUL users' transmit power, and the MAs' positions at both BSs, all while\nsatisfying the quality-of-service (QoS) requirements for both sensing and\ncommunication. Given the non-convex nature of the problem and the high coupling\nbetween the variables, we employ a gradient-based meta-learning (GML) approach\ntailored for large-scale optimization. Numerical results demonstrate the\neffectiveness of the proposed meta-learning approach, achieving results within\n99% of the optimal solution. Furthermore, the MA-based scheme outperforms\nseveral benchmark approaches, highlighting its advantages in practical ISAC\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MAs\uff09\u7684\u5168\u53cc\u5de5\uff08FD\uff09\u57fa\u7ad9\uff08BS\uff09\u573a\u666f\uff0c\u540c\u65f6\u652f\u6301\u901a\u4fe1\u670d\u52a1\u548c\u76ee\u6807\u68c0\u6d4b\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u9879\u53c2\u6570\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\u7684\u9ad8\u6548\u5e94\u7528\uff0c\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MAs\uff09\u4f18\u5316\u5168\u53cc\u5de5\u57fa\u7ad9\uff08BS\uff09\u7684\u6027\u80fd\uff0c\u4ee5\u540c\u65f6\u6ee1\u8db3\u901a\u4fe1\u670d\u52a1\u9700\u6c42\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u77e2\u91cf\u3001\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u77e2\u91cf\u3001\u7528\u6237\u53d1\u5c04\u529f\u7387\u548c\u5929\u7ebf\u4f4d\u7f6e\uff0c\u63d0\u51fa\u68af\u5ea6\u5143\u5b66\u4e60\u65b9\u6cd5\uff08GML\uff09\uff0c\u4ee5\u89e3\u51b3\u975e\u51f8\u548c\u9ad8\u8026\u5408\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u63a5\u8fd1\u6700\u4f18\u89e3\uff0899%\uff09\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\u65b9\u6848\u7684\u5b9e\u9645\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\u7684\u65b9\u6848\u53ef\u4ee5\u6709\u6548\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u68af\u5ea6\u5143\u5b66\u4e60\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01357", "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "categories": ["cs.RO"], "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u7528\u4e8e\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u5185\u6cb3\u6c34\u9053\u7684\u5b9e\u65f6\u5bfc\u822a\u3002", "motivation": "\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u6c34\u9053\u4e2d\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u6216\u8fc7\u4e8e\u4fdd\u5b88\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u53d8\u5316\u7684\u81a8\u80c0\u692d\u5706\u969c\u788d\u7269\u8868\u793a\uff0c\u6839\u636e\u8239\u8236\u4e0e\u969c\u788d\u7269\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u8c03\u6574\u81a8\u80c0\u534a\u5f84\uff0c\u7ed3\u5408MPC\u548cCBFs\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u65f6\u5bfc\u822a\u5e76\u89e3\u51b3\u6b7b\u9501\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u81a8\u80c0\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u4fdd\u5b88\u6027\uff0c\u9002\u7528\u4e8e\u72ed\u7a84\u7a7a\u95f4\u7684\u81ea\u4e3b\u8239\u8236\u5bfc\u822a\u3002"}}
{"id": "2510.01605", "pdf": "https://arxiv.org/pdf/2510.01605", "abs": "https://arxiv.org/abs/2510.01605", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong"], "title": "The Analysis and Performance of LODC-OFDM Signal in Nonlinear Rydberg Atomic Sensor", "categories": ["eess.SP"], "comment": null, "summary": "Rydberg atomic sensors have been seen as novel radio frequency (RF)\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, the signal sensing process in\nRydberg system involves sequential transduction from electromagnetic waves to\noptical signals and finally to electrical signals. The unipolar characteristic\nof the optical interface inherently restricts conventional OFDM reception.\nTherefore, adopting unipolar OFDM schemes, inspired by optical communication\nsystems, becomes essential for compatible signal transmission. In this work, we\ninvestigate the amplitude modulation-to-amplitude modulation (AM-AM)\ncharacteristics of Rydberg atomic sensors, establishing an empirical\napproximation function. Building on the direct current-biased optical\northogonal frequency division multiplexing (DCO-OFDM) framework, we propose a\nnovel local oscillator direct current-biased OFDM (LODC-OFDM) scheme\nspecifically optimized for Rydberg-based sensing, effectively addressing the\nbroadband OFDM reception challenge. Then, we adopt Bussgang theorem to analyze\nthe nonlinear distortion of LODC-OFDM signals and the results in closed-form\nsolutions are derived for AM/AM curves approximated by Taylor series expansion\nand for the ideal pre-distortion case. In real experiments, the experimental\nand theoretical results fit well.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Rydberg\u539f\u5b50\u4f20\u611f\u5668\u7684AM-AM\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684LODC-OFDM\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5bbd\u5e26OFDM\u63a5\u6536\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "Rydberg\u539f\u5b50\u4f20\u611f\u5668\u5728\u5c04\u9891\u6d4b\u91cf\u4e2d\u8868\u73b0\u51fa\u9ad8\u7075\u654f\u5ea6\uff0c\u4f46\u5176\u5149\u5b66\u63a5\u53e3\u7684\u5355\u6781\u6027\u7279\u6027\u9650\u5236\u4e86\u4f20\u7edfOFDM\u63a5\u6536\uff0c\u56e0\u6b64\u9700\u8981\u517c\u5bb9\u7684\u4fe1\u53f7\u4f20\u8f93\u65b9\u6848\u3002", "method": "\u7814\u7a76AM-AM\u7279\u6027\u5e76\u5efa\u7acb\u7ecf\u9a8c\u8fd1\u4f3c\u51fd\u6570\uff0c\u57fa\u4e8eDCO-OFDM\u6846\u67b6\u63d0\u51faLODC-OFDM\u65b9\u6848\uff0c\u91c7\u7528Bussgang\u5b9a\u7406\u5206\u6790\u975e\u7ebf\u6027\u5931\u771f\u3002", "result": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u543b\u5408\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4f18\u5316\u7684LODC-OFDM\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86Rydberg\u539f\u5b50\u4f20\u611f\u5668\u4e2d\u7684\u5bbd\u5e26OFDM\u63a5\u6536\u95ee\u9898\u3002"}}
{"id": "2510.01381", "pdf": "https://arxiv.org/pdf/2510.01381", "abs": "https://arxiv.org/abs/2510.01381", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots", "categories": ["cs.RO"], "comment": "17 pages, 11 figures. Submitted to IEEE Transactions on Robotics", "summary": "State estimation techniques for continuum robots (CRs) typically involve\nusing computationally complex dynamic models, simplistic shape approximations,\nor are limited to quasi-static methods. These limitations can be sensitive to\nunmodelled disturbances acting on the robot. Inspired by a factor-graph\noptimization paradigm, this work introduces a continuous-time stochastic state\nestimation framework for continuum robots. We introduce factors based on\ncontinuous-time kinematics that are corrupted by a white noise Gaussian process\n(GP). By using a simple robot model paired with high-rate sensing, we show\nadaptability to unmodelled external forces and data dropout. The result\ncontains an estimate of the mean and covariance for the robot's pose, velocity,\nand strain, each of which can be interpolated continuously in time or space.\nThis same interpolation scheme can be used during estimation, allowing for\ninclusion of measurements on states that are not explicitly estimated. Our\nmethod's inherent sparsity leads to a linear solve complexity with respect to\ntime and interpolation queries in constant time. We demonstrate our method on a\nCR with gyroscope and pose sensors, highlighting its versatility in real-world\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u4f18\u5316\u7684\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08CRs\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u6a21\u578b\u548c\u9ad8\u9891\u4f20\u611f\u5e94\u5bf9\u672a\u5efa\u6a21\u5e72\u6270\u548c\u6570\u636e\u4e22\u5931\u3002", "motivation": "\u73b0\u6709CRs\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u3001\u6a21\u578b\u7b80\u5316\u6216\u9650\u4e8e\u51c6\u9759\u6001\u7684\u5c40\u9650\uff0c\u6613\u53d7\u672a\u5efa\u6a21\u5e72\u6270\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u8fd0\u52a8\u5b66\u56e0\u5b50\u548c\u767d\u8272\u566a\u58f0\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\uff0c\u7ed3\u5408\u7b80\u5355\u6a21\u578b\u548c\u9ad8\u9891\u4f20\u611f\u3002", "result": "\u65b9\u6cd5\u80fd\u4f30\u8ba1\u673a\u5668\u4eba\u4f4d\u59ff\u3001\u901f\u5ea6\u548c\u5e94\u53d8\u7684\u5747\u503c\u4e0e\u534f\u65b9\u5dee\uff0c\u652f\u6301\u65f6\u95f4\u548c\u7a7a\u95f4\u8fde\u7eed\u63d2\u503c\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "conclusion": "\u65b9\u6cd5\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\u5c55\u793a\u4e86\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2510.01707", "pdf": "https://arxiv.org/pdf/2510.01707", "abs": "https://arxiv.org/abs/2510.01707", "authors": ["Amila Ravinath", "Minhua Ding", "Bikshapathi Gouda", "Italo Atzeni", "Antti T\u00f6lli"], "title": "SEP Analysis of 1-Bit Quantized SIMO Systems with QPSK over Fading Channels", "categories": ["eess.SP"], "comment": "Accepted to Asilomar Conference on Signals, Systems, and Computers\n  2025", "summary": "The average symbol error probability (SEP) of a 1-bit quantized single-input\nmultiple-output (SIMO) system is analyzed under Rayleigh fading channels and\nquadrature phase-shift keying (QPSK) modulation. Previous studies have\npartially characterized the diversity gain for selection combining (SC). In\nthis paper, leveraging a novel analytical method, an exact analytical SEP\nexpression is derived for a 1-bit quantized SIMO system employing QPSK\nmodulation at the transmitter and maximum ratio combining (MRC) at the\nreceiver. The corresponding diversity and coding gains of a SIMO-MRC system are\nalso determined. Furthermore, the diversity and coding gains of a 1-bit\nquantized SIMO-SC system are quantified for an arbitrary number of receive\nantennas, thereby extending and complementing prior results.", "AI": {"tldr": "\u5206\u6790\u4e861\u6bd4\u7279\u91cf\u5316SIMO\u7cfb\u7edf\u5728\u745e\u5229\u8870\u843d\u4fe1\u9053\u548cQPSK\u8c03\u5236\u4e0b\u7684\u5e73\u5747\u7b26\u53f7\u9519\u8bef\u6982\u7387\uff08SEP\uff09\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u6790\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e86MRC\u63a5\u6536\u5668\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\uff0c\u5e76\u786e\u5b9a\u4e86\u76f8\u5e94\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u5bf9\u9009\u62e9\u5408\u5e76\uff08SC\uff09\u7684\u5206\u96c6\u589e\u76ca\u8fdb\u884c\u4e86\u90e8\u5206\u7814\u7a76\uff0c\u4f46\u57281\u6bd4\u7279\u91cf\u5316SIMO\u7cfb\u7edf\u4e2d\uff0cMRC\u63a5\u6536\u5668\u7684SEP\u53ca\u5176\u5206\u96c6\u4e0e\u7f16\u7801\u589e\u76ca\u5c1a\u672a\u5b8c\u5168\u89e3\u6790\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u6790\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e861\u6bd4\u7279\u91cf\u5316SIMO\u7cfb\u7edf\u5728QPSK\u8c03\u5236\u548cMRC\u63a5\u6536\u5668\u4e0b\u7684\u7cbe\u786eSEP\u8868\u8fbe\u5f0f\u3002", "result": "\u786e\u5b9a\u4e86SIMO-MRC\u7cfb\u7edf\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\uff0c\u5e76\u91cf\u5316\u4e86SIMO-SC\u7cfb\u7edf\u5728\u4efb\u610f\u63a5\u6536\u5929\u7ebf\u6570\u91cf\u4e0b\u7684\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\uff0c\u8865\u5145\u4e86\u4e4b\u524d\u7684\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u5bf91\u6bd4\u7279\u91cf\u5316SIMO\u7cfb\u7edf\u6027\u80fd\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u5206\u96c6\u548c\u7f16\u7801\u589e\u76ca\u65b9\u9762\uff0c\u586b\u8865\u4e86\u4e4b\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.01388", "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "AI": {"tldr": "VENTURA\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89c9\u8def\u5f84\uff0c\u907f\u5f00\u4e86\u76f4\u63a5\u9884\u6d4b\u52a8\u4f5c\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u548c\u9884\u8bad\u7ec3\u76ee\u6807\u5dee\u5f02\u5bfc\u81f4\u7684\u96be\u8fc1\u79fb\u95ee\u9898\u3002", "method": "VENTURA\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u7a7a\u95f4\u7684\u8def\u5f84\u63a9\u7801\uff08\u89c6\u89c9\u8ba1\u5212\uff09\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u884c\u4e3a\u514b\u9686\u7b56\u7565\uff0c\u5c06\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u8f68\u8ff9\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5229\u7528\u81ea\u76d1\u7763\u8ddf\u8e2a\u6a21\u578b\u548cVLM\u589e\u5f3a\u7684\u6807\u6ce8\u6570\u636e\uff0c\u907f\u514d\u4e86\u4eba\u5de5\u6807\u6ce8\u3002", "result": "VENTURA\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u6210\u529f\u7387\u548c\u907f\u969c\u80fd\u529b\u5206\u522b\u63d0\u5347\u4e8633%\u548c54%\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u65b0\u4efb\u52a1\u7ec4\u5408\u7684\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "VENTURA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.01748", "pdf": "https://arxiv.org/pdf/2510.01748", "abs": "https://arxiv.org/abs/2510.01748", "authors": ["Hadi Zayyani", "Felipe A. P. de Figueiredo", "Mohammad Salman", "Rausley A. A. de Souza"], "title": "3D 8-Ary Noise Modulation Using Bayesian- and Kurtosis-based Detectors", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents a novel three-dimensional (3D) 8-ary noise modulation\nscheme that introduces a new dimension: the mixture probability of a Mixture of\nGaussian (MoG) distribution. This proposed approach utilizes the dimensions of\nmean and variance, in addition to the new probability dimension. Within this\nframework, each transmitted symbol carries three bits, each corresponding to a\ndistinct sub-channel. For detection, a combination of specialized detectors is\nemployed: a simple threshold based detector for the first sub-channel bit\n(modulated by the mean), a Maximum-Likelihood (ML) detector for the second\nsub-channel bit (modulated by the variance), a Kurtosis-based, Jarque-Bera (JB)\ntest, and Bayesian Hypothesis (BHT)-based detectors for the third bit\n(modulated by the MoG probability). The Kurtosis- and JB-based detectors\nspecifically distinguish between Gaussian (or near-Gaussian) and non-Gaussian\nMoG distributions by leveraging higher-order statistical measures. The Bit\nError Probabilities (BEPs) are derived for the threshold-, Kurtosis-, and\nBHT-based detectors. The optimum threshold for the Kurtosis-based detector is\nalso derived in a tractable manner. Simulation results demonstrate that a\ncomparably low BEP is achieved for the third sub-channel bit relative to\nexisting two-dimensional (2D) schemes. Simultaneously, the proposed scheme\nincreases the data rate by a factor of 1.5 and 3 compared to the Generalized\nQuadratic noise modulator and the classical binary KLJN noise modulator,\nrespectively. Furthermore, the Kurtosis-based detector offers a low-complexity\nsolution, achieving an acceptable BEP of approximately 0.06.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u7ef48\u8fdb\u5236\u566a\u58f0\u8c03\u5236\u65b9\u6848\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u6982\u7387\u7ef4\u5ea6\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u4f20\u8f93\u901f\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8bef\u7801\u7387\u3002", "motivation": "\u73b0\u6709\u566a\u58f0\u8c03\u5236\u65b9\u6848\u7684\u6570\u636e\u4f20\u8f93\u901f\u7387\u6709\u9650\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u7ef4\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7ef4\u5ea6\uff08\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u6982\u7387\uff09\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ef48\u8fdb\u5236\u566a\u58f0\u8c03\u5236\u65b9\u6848\uff0c\u5229\u7528\u5747\u503c\u3001\u65b9\u5dee\u548c\u65b0\u7684\u6982\u7387\u7ef4\u5ea6\u4f20\u8f93\u7b26\u53f7\u3002\u901a\u8fc7\u9608\u503c\u68c0\u6d4b\u5668\u3001\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u5668\u3001\u5cf0\u5ea6\u68c0\u6d4b\u5668\u548c\u8d1d\u53f6\u65af\u5047\u8bbe\u68c0\u6d4b\u5668\u76f8\u7ed3\u5408\u8fdb\u884c\u89e3\u8c03\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u7b2c\u4e09\u5b50\u4fe1\u9053\u4e2d\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8bef\u7801\u7387\uff0c\u6570\u636e\u901f\u7387\u6bd4\u73b0\u6709\u65b9\u6848\u63d0\u9ad8\u4e861.5\u500d\u81f33\u500d\uff0c\u5cf0\u5ea6\u68c0\u6d4b\u5668\u7684\u590d\u6742\u5ea6\u4f4e\u4e14\u8bef\u7801\u7387\u7ea6\u4e3a0.06\u3002", "conclusion": "\u8be5\u4e09\u7ef4\u566a\u58f0\u8c03\u5236\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u4f20\u8f93\u901f\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01389", "pdf": "https://arxiv.org/pdf/2510.01389", "abs": "https://arxiv.org/abs/2510.01389", "authors": ["Ulas Berk Karli", "Ziyao Shangguan", "Tesca FItzgerald"], "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.", "AI": {"tldr": "INSIGHT\u6846\u67b6\u901a\u8fc7\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u9884\u6d4b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f55\u65f6\u9700\u8981\u8bf7\u6c42\u5e2e\u52a9\uff0c\u6bd4\u8f83\u4e86\u5f3a\u76d1\u7763\u548c\u5f31\u76d1\u7763\u7684\u6548\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u65f6\u5e8f\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u9884\u6d4b\u5931\u8d25\u5e76\u8bf7\u6c42\u4eba\u7c7b\u5e2e\u52a9\u7684\u673a\u5236\u3002", "method": "\u5229\u7528\u03c0\u2080-FAST\u6a21\u578b\u63d0\u53d6\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff08\u5982\u71b5\u3001\u5bf9\u6570\u6982\u7387\u7b49\uff09\uff0c\u8bad\u7ec3\u7d27\u51d1\u7684Transformer\u5206\u7c7b\u5668\u3002", "result": "\u5f3a\u76d1\u7763\u53ef\u6355\u6349\u7cbe\u7ec6\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\uff0c\u5f31\u76d1\u7763\u5728\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4e00\u81f4\u65f6\u4ecd\u6709\u7ade\u4e89\u529b\uff1b\u65f6\u5e8f\u5efa\u6a21\u4f18\u4e8e\u9759\u6001\u8bc4\u5206\u3002", "conclusion": "INSIGHT\u4e3a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684VLA\u81ea\u7701\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u4e3a\u4e3b\u52a8\u5b66\u4e60\u548c\u5b9e\u65f6\u9519\u8bef\u7f13\u89e3\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.01763", "pdf": "https://arxiv.org/pdf/2510.01763", "abs": "https://arxiv.org/abs/2510.01763", "authors": ["Xiao Ding", "Enbin Song", "Dunbiao Niu", "Zhujun Cao", "Qingjiang Shi"], "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large", "categories": ["eess.SP", "math.OC", "math.ST", "stat.TH"], "comment": null, "summary": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728Wasserstein\u8ddd\u79bb\u7ea6\u675f\u4e0b\u7684\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u4f30\u8ba1\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u65e0\u9650\u7ef4\u975e\u51f8\u6781\u5c0f\u6781\u5927\u95ee\u9898\u5b58\u5728\u978d\u70b9\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u5e76\u7ed9\u51fa\u4e86\u9a8c\u8bc1\u65b9\u6cd5\u53ca\u7b80\u5316\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76\u5728Wasserstein\u8ddd\u79bb\u7ea6\u675f\u4e0b\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u4e2d\u7684\u9c81\u68d2\u4f30\u8ba1\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u9650\u7ef4\u975e\u51f8\u6781\u5c0f\u6781\u5927\u95ee\u9898\u4e2d\u978d\u70b9\u5b58\u5728\u7684\u6761\u4ef6\u53ca\u5176\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u65e0\u9650\u7ef4\u4e0e\u6709\u9650\u7ef4\u95ee\u9898\u7684\u978d\u70b9\u7b49\u4ef7\u6027\uff0c\u63d0\u51fa\u53ef\u9a8c\u8bc1\u7684\u5fc5\u8981\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u7b80\u5316\u6761\u4ef6\uff1b\u5728\u65e0\u978d\u70b9\u65f6\u6c42\u89e3\u6709\u9650\u7ef4\u975e\u51f8\u6781\u5c0f\u6781\u5927\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86\u7b49\u4ef7\u6027\u6761\u4ef6\uff0c\u5e76\u7ed9\u51fa\u4e86\u978d\u70b9\u5b58\u5728\u7684\u7b80\u5316\u6761\u4ef6\uff1b\u5728\u65e0\u978d\u70b9\u65f6\u63d0\u51fa\u4e86\u9c81\u68d2\u7ebf\u6027\u4f30\u8ba1\u5668\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u4e3a\u89e3\u51b3Wasserstein\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002"}}
{"id": "2510.01402", "pdf": "https://arxiv.org/pdf/2510.01402", "abs": "https://arxiv.org/abs/2510.01402", "authors": ["Hun Kuk Park", "Taekyung Kim", "Dimitra Panagou"], "title": "Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The first two authors contributed equally to this work. Project page:\n  https://www.taekyung.me/dpcbf", "summary": "Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety\nof autonomous systems, yet applying them to nonholonomic robots in cluttered,\ndynamic environments remains an open challenge. State-of-the-art methods often\nrely on collision-cone or velocity-obstacle constraints which, by only\nconsidering the angle of the relative velocity, are inherently conservative and\ncan render the CBF-based quadratic program infeasible, particularly in dense\nscenarios. To address this issue, we propose a Dynamic Parabolic Control\nBarrier Function (DPCBF) that defines the safe set using a parabolic boundary.\nThe parabola's vertex and curvature dynamically adapt based on both the\ndistance to an obstacle and the magnitude of the relative velocity, creating a\nless restrictive safety constraint. We prove that the proposed DPCBF is valid\nfor a kinematic bicycle model subject to input constraints. Extensive\ncomparative simulations demonstrate that our DPCBF-based controller\nsignificantly enhances navigation success rates and QP feasibility compared to\nbaseline methods. Our approach successfully navigates through dense\nenvironments with up to 100 dynamic obstacles, scenarios where collision\ncone-based methods fail due to infeasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08DPCBF\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u629b\u7269\u7ebf\u8fb9\u754c\u6765\u51cf\u5c11\u5b89\u5168\u7ea6\u675f\u7684\u9650\u5236\u6027\uff0c\u63d0\u5347\u975e\u5b8c\u6574\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u5b8c\u6574\u673a\u5668\u4eba\u7684\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u5bb9\u6613\u5bfc\u81f4\u4e8c\u6b21\u89c4\u5212\u4e0d\u53ef\u884c\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u5b89\u5168\u7ea6\u675f\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08DPCBF\uff09\uff0c\u5176\u629b\u7269\u7ebf\u7684\u9876\u70b9\u548c\u66f2\u7387\u6839\u636e\u969c\u788d\u7269\u8ddd\u79bb\u548c\u76f8\u5bf9\u901f\u5ea6\u52a8\u6001\u8c03\u6574\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cDPCBF\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u5904\u7406\u591a\u8fbe100\u4e2a\u52a8\u6001\u969c\u788d\u7269\u3002", "conclusion": "DPCBF\u4e3a\u975e\u5b8c\u6574\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.01776", "pdf": "https://arxiv.org/pdf/2510.01776", "abs": "https://arxiv.org/abs/2510.01776", "authors": ["Hadi Zayyani", "Mohammad Salman", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "Composite Generalized Quadratic Noise Modulation via Signal Addition: Towards Higher Dimensional Noise Modulations", "categories": ["eess.SP"], "comment": null, "summary": "This letter proposes superposing two Generalized Quadratic Noise Modulators\n(GQNM) by simply adding their outputs. It creates a 16-ary noise modulator that\nresembles QAM modulators in classical communication. It modulates the\ninformation bits on four different means and four different variances. It could\nalso be applied to reach higher-order modulations than 16-ary schemes by adding\nthe outputs of more than two modulators, which is not discussed in detail in\nthis letter and left for future work. By selecting the parameters necessary for\nsatisfying the theoretical distinguishability conditions provided in the paper,\nwe can reach better performances in comparison to the Kirchhoff-Law Johnson\nNoise (KLJN) modulator and the GQNM modulator, which is verified by the\nsimulations. The better result in terms of smaller Bit Error Probability (BEP)\nis achieved by increasing the complexity in the modulator, the transmitter, and\nthe detectors in the receiver.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53e0\u52a0\u4e24\u4e2a\u5e7f\u4e49\u4e8c\u6b21\u566a\u58f0\u8c03\u5236\u5668\uff08GQNM\uff09\u8f93\u51fa\u751f\u621016\u8fdb\u5236\u566a\u58f0\u8c03\u5236\u5668\u7684\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u4e8e\u7ecf\u5178\u901a\u4fe1\u4e2d\u7684QAM\u8c03\u5236\u5668\u3002\u901a\u8fc7\u9009\u62e9\u6ee1\u8db3\u7406\u8bba\u53ef\u533a\u5206\u6761\u4ef6\u7684\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u4e8eKLJN\u548cGQNM\u8c03\u5236\u5668\uff0c\u9a8c\u8bc1\u4e86\u66f4\u4f4e\u7684\u8bef\u7801\u6982\u7387\uff08BEP\uff09\uff0c\u4f46\u589e\u52a0\u4e86\u8c03\u5236\u5668\u3001\u53d1\u5c04\u673a\u548c\u63a5\u6536\u673a\u7684\u590d\u6742\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u566a\u58f0\u8c03\u5236\u5668\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u53e0\u52a0GQNM\u8c03\u5236\u5668\u8f93\u51fa\u5b9e\u73b0\u66f4\u9ad8\u9636\u8c03\u5236\uff0c\u540c\u65f6\u9a8c\u8bc1\u5176\u5728\u8bef\u7801\u6982\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "method": "\u53e0\u52a0\u4e24\u4e2aGQNM\u8c03\u5236\u5668\u7684\u8f93\u51fa\uff0c\u751f\u621016\u8fdb\u5236\u8c03\u5236\u5668\uff1b\u901a\u8fc7\u9009\u62e9\u6ee1\u8db3\u7406\u8bba\u53ef\u533a\u5206\u6761\u4ef6\u7684\u53c2\u6570\u4f18\u5316\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bef\u7801\u6982\u7387\uff08BEP\uff09\u4e0a\u4f18\u4e8eKLJN\u548cGQNM\u8c03\u5236\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u589e\u52a0\u4e86\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u66f4\u9ad8\u9636\u8c03\u5236\u65b9\u6848\u6709\u5f85\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.01404", "pdf": "https://arxiv.org/pdf/2510.01404", "abs": "https://arxiv.org/abs/2510.01404", "authors": ["Lexi Foland", "Thomas Cohn", "Adam Wei", "Nicholas Pfaff", "Boyuan Chen", "Russ Tedrake"], "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?", "categories": ["cs.RO"], "comment": "Under review. 8 pages, 3 figures, 3 tables. Additional results\n  available at https://diffusion-learns-kinematic.github.io", "summary": "Diffusion policies have shown impressive results in robot imitation learning,\neven for tasks that require satisfaction of kinematic equality constraints.\nHowever, task performance alone is not a reliable indicator of the policy's\nability to precisely learn constraints in the training data. To investigate, we\nanalyze how well diffusion policies discover these manifolds with a case study\non a bimanual pick-and-place task that encourages fulfillment of a kinematic\nconstraint for success. We study how three factors affect trained policies:\ndataset size, dataset quality, and manifold curvature. Our experiments show\ndiffusion policies learn a coarse approximation of the constraint manifold with\nlearning affected negatively by decreases in both dataset size and quality. On\nthe other hand, the curvature of the constraint manifold showed inconclusive\ncorrelations with both constraint satisfaction and task success. A hardware\nevaluation verifies the applicability of our results in the real world. Project\nwebsite with additional results and visuals:\nhttps://diffusion-learns-kinematic.github.io", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u80fd\u5927\u81f4\u5b66\u4e60\u7ea6\u675f\u6d41\u5f62\uff0c\u4f46\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8d28\u91cf\u4e0b\u964d\u4f1a\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff1b\u7ea6\u675f\u6d41\u5f62\u66f2\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\u548c\u4efb\u52a1\u6210\u529f\u7684\u5173\u7cfb\u4e0d\u660e\u663e\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u7b56\u7565\u662f\u5426\u80fd\u7cbe\u786e\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7ea6\u675f\u6d41\u5f62\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6ee1\u8db3\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u53cc\u624b\u673a\u5668\u4eba\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u6570\u636e\u96c6\u89c4\u6a21\u3001\u8d28\u91cf\u548c\u7ea6\u675f\u6d41\u5f62\u66f2\u7387\u5bf9\u6269\u6563\u7b56\u7565\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6269\u6563\u7b56\u7565\u80fd\u7c97\u7565\u5b66\u4e60\u7ea6\u675f\u6d41\u5f62\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8d28\u91cf\u4e0b\u964d\u4f1a\u964d\u4f4e\u5b66\u4e60\u6548\u679c\uff0c\u66f2\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\u548c\u4efb\u52a1\u6210\u529f\u7684\u76f8\u5173\u6027\u4e0d\u660e\u786e\u3002", "conclusion": "\u6269\u6563\u7b56\u7565\u5728\u5b9e\u9645\u786c\u4ef6\u4efb\u52a1\u4e2d\u5177\u6709\u4e00\u5b9a\u9002\u7528\u6027\uff0c\u4f46\u9700\u6ce8\u610f\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u89c4\u6a21\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.01778", "pdf": "https://arxiv.org/pdf/2510.01778", "abs": "https://arxiv.org/abs/2510.01778", "authors": ["Samaneh Motie", "Hadi Zayyani", "Mohammad Salman", "Hasan Abu Hilal"], "title": "Closed-form Single UAV-aided Emitter Localization and Trajectory Design Using Doppler and TOA Measurements", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, a single Unmanned-Aerial-Vehicle (UAV)-aided localization\nalgorithm which uses both Doppler and Time of Arrival (ToA) measurements is\npresented. In contrast to Doppler-based localization algorithms which are based\non non-convex functions, exploiting ToA measurements in a Least-Square (LS)\nDoppler-based cost function, leads to a quadratic convex function whose\nminimizer lies on a line. Utilizing the ToA measurements in addition to the\nlinear equation of minimizer, a closed form solution is obtained for the\nemitter location using a constrained LS optimization. In addition, a trajectory\ndesign of the UAV is provided which has also closed-form solution. Simulation\nexperiments demonstrate the effectiveness of the proposed algorithm in\ncomparison to some others in the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u666e\u52d2\u548c\u5230\u8fbe\u65f6\u95f4\uff08ToA\uff09\u6d4b\u91cf\u7684\u65e0\u4eba\u673a\u8f85\u52a9\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u548c\u7ea6\u675f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5b9e\u73b0\u95ed\u5408\u89e3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65e0\u4eba\u673a\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u666e\u52d2\u7684\u5b9a\u4f4d\u7b97\u6cd5\u4f9d\u8d56\u975e\u51f8\u51fd\u6570\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u672c\u6587\u65e8\u5728\u5229\u7528ToA\u6d4b\u91cf\u548c\u591a\u666e\u52d2\u4fe1\u606f\uff0c\u7b80\u5316\u5b9a\u4f4d\u95ee\u9898\u5e76\u5f97\u5230\u95ed\u5408\u89e3\u3002", "method": "\u7ed3\u5408\u591a\u666e\u52d2\u548cToA\u6d4b\u91cf\uff0c\u6784\u5efa\u51f8\u4f18\u5316\u7684\u6700\u5c0f\u4e8c\u4e58\u6210\u672c\u51fd\u6570\uff0c\u5229\u7528\u7ea6\u675f\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u83b7\u5f97\u53d1\u5c04\u6e90\u4f4d\u7f6e\u7684\u95ed\u5408\u89e3\uff0c\u5e76\u8bbe\u8ba1\u65e0\u4eba\u673a\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u51f8\u4f18\u5316\u548c\u7ea6\u675f\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5b9a\u4f4d\uff0c\u65e0\u4eba\u673a\u8f68\u8ff9\u8bbe\u8ba1\u4e5f\u6709\u95ed\u5408\u89e3\u3002"}}
{"id": "2510.01433", "pdf": "https://arxiv.org/pdf/2510.01433", "abs": "https://arxiv.org/abs/2510.01433", "authors": ["Anukriti Singh", "Kasra Torshizi", "Khuzema Habib", "Kelin Yu", "Ruohan Gao", "Pratap Tokekar"], "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.", "AI": {"tldr": "AFFORD2ACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u63d0\u53d6\u548c\u8f6c\u6362\u5668\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u91cf\u5927\u3001\u80cc\u666f\u5e72\u6270\u6216\u8bed\u4e49\u7406\u89e3\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u4f7f\u7528\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u8fc7\u6ee4\u65e0\u5173\u7279\u5f81\uff0c\u6784\u5efa\u5173\u952e\u70b9\uff0c\u5e76\u901a\u8fc7\u8f6c\u6362\u5668\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u8bad\u7ec315\u5206\u949f\u768438\u7ef4\u7b56\u7565\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8fbe\u523082%\u6210\u529f\u7387\u3002", "conclusion": "AFFORD2ACT\u5728\u6570\u636e\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.01789", "pdf": "https://arxiv.org/pdf/2510.01789", "abs": "https://arxiv.org/abs/2510.01789", "authors": ["Ruixi Feng", "Weidong Mei", "Lele Lu", "Xin Wei", "Zhi Chen", "Zhen Gao", "Boyu Ning"], "title": "Performance Optimization for Movable Antenna Enhanced MISO-OFDM Systems", "categories": ["eess.SP"], "comment": "Accepted to IEEE GLOBECOM 2025 Workshop", "summary": "Movable antenna (MA) technology offers a flexible approach to enhancing\nwireless channel conditions by adjusting antenna positions within a designated\nregion. While most existing works focus on narrowband MA systems, this paper\ninvestigates MA position optimization for an MA-enhanced multiple-input\nsingle-output (MISO) orthogonal frequency-division multiplexing (OFDM) system.\nThis problem appears to be particularly challenging due to the frequency-flat\nnature of MA positioning, which should accommodate the channel conditions\nacross different subcarriers. To overcome this challenge, we discretize the\nmovement region into a multitude of sampling points, thereby converting the\ncontinuous position optimization problem into a discrete point selection\nproblem. Although this problem is combinatorial, we develop an efficient\npartial enumeration algorithm to find the optimal solution using a\nbranch-and-bound framework, where a graph-theoretic method is incorporated to\neffectively prune suboptimal solutions. In the low signal-to-noise ratio (SNR)\nregime, a simplified graph-based algorithm is also proposed to obtain the\noptimal MA positions without the need for enumeration. Simulation results\nreveal that the proposed algorithm outperforms conventional fixed-position\nantennas (FPAs), while narrowband-based antenna position optimization can\nachieve near-optimal performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9MA\u589e\u5f3a\u578bMISO-OFDM\u7cfb\u7edf\u7684\u9ad8\u6548\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u548c\u56fe\u8bba\u65b9\u6cd5\u89e3\u51b3\u4e86\u9891\u7387\u5e73\u5766\u7279\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u4eff\u771f\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u7a84\u5e26MA\u7cfb\u7edf\uff0c\u800cMA\u4f4d\u7f6e\u4f18\u5316\u5728\u5bbd\u5e26OFDM\u7cfb\u7edf\u4e2d\u9762\u4e34\u9891\u7387\u5e73\u5766\u7279\u6027\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u8fde\u7eed\u4f4d\u7f6e\u4f18\u5316\u95ee\u9898\u79bb\u6563\u5316\u4e3a\u91c7\u6837\u70b9\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u5206\u652f\u5b9a\u754c\u6846\u67b6\u4e0b\u7684\u90e8\u5206\u679a\u4e3e\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u56fe\u8bba\u65b9\u6cd5\u526a\u679d\uff1b\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u8bbe\u8ba1\u7b80\u5316\u56fe\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u65b9\u6848\uff0c\u7a84\u5e26\u4f18\u5316\u65b9\u6cd5\u4e5f\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MA\u4f4d\u7f6e\u4f18\u5316\u5728\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u6240\u63d0\u7b97\u6cd5\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u5c24\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2510.01438", "pdf": "https://arxiv.org/pdf/2510.01438", "abs": "https://arxiv.org/abs/2510.01438", "authors": ["Minglun Wei", "Xintong Yang", "Yu-Kun Lai", "Ze Ji"], "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation", "categories": ["cs.RO"], "comment": "4 pages", "summary": "Robotic automation is accelerating scientific discovery by reducing manual\neffort in laboratory workflows. However, precise manipulation of powders\nremains challenging, particularly in tasks such as transport that demand\naccuracy and stability. We propose a trajectory optimisation framework for\npowder transport in laboratory settings, which integrates differentiable\nphysics simulation for accurate modelling of granular dynamics, low-dimensional\nskill-space parameterisation to reduce optimisation complexity, and a\ncurriculum-based strategy that progressively refines task competence over long\nhorizons. This formulation enables end-to-end optimisation of contact-rich\nrobot trajectories while maintaining stability and convergence efficiency.\nExperimental results demonstrate that the proposed method achieves superior\ntask success rates and stability compared to the reinforcement learning\nbaseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u9a8c\u5ba4\u7c89\u672b\u8fd0\u8f93\u4efb\u52a1\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u7269\u7406\u4eff\u771f\u548c\u4f4e\u7ef4\u6280\u80fd\u7a7a\u95f4\u53c2\u6570\u5316\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u7c89\u672b\u8fd0\u8f93\u4e2d\u7cbe\u786e\u6027\u548c\u7a33\u5b9a\u6027\u7684\u6311\u6218\uff0c\u52a0\u901f\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u7269\u7406\u4eff\u771f\u5efa\u6a21\u9897\u7c92\u52a8\u529b\u5b66\uff0c\u4f4e\u7ef4\u6280\u80fd\u7a7a\u95f4\u53c2\u6570\u5316\u964d\u4f4e\u4f18\u5316\u590d\u6742\u5ea6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6e10\u8fdb\u5f0f\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.01850", "pdf": "https://arxiv.org/pdf/2510.01850", "abs": "https://arxiv.org/abs/2510.01850", "authors": ["Ying-Ren Chien", "Po-Heng Chou", "You-Jie Peng", "Chun-Yuan Huang", "Hen-Wai Tsao", "Yu Tsao"], "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT", "68T07, 94A12, 62M10", "I.2.6; I.5.4; C.2.1"], "comment": "16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, Vol. 74, 2025", "summary": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive\nnoise is a critical issue in enhancing impulse noise processing for narrowband\npowerline communication (NB-PLC) transceivers. However, existing mathematical\nnoise generative models capture only some of the characteristics of additive\nnoise. Therefore, we propose a generative adversarial network (GAN), called the\nnoise-generation GAN (NGGAN), that learns the complicated characteristics of\npractically measured noise samples for data augmentation. To closely match the\nstatistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise\nvia the analog coupling and bandpass filtering circuits of a commercial NB-PLC\nmodem to build a realistic dataset. Specifically, the NGGAN design approaches\nbased on the practically measured dataset are as follows: (i) we design the\nlength of input signals that the NGGAN model can fit to facilitate\ncyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss\nfunction to enhance the similarity between the generated noise and the training\ndataset and ensure that the sample diversity is sufficient for various\napplications. (iii) To measure the similarity performance of the GAN-based\nmodels based on mathematical and practically measured datasets, we perform\nquantitative and qualitative analyses. The training datasets include (1) a\npiecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a\nfrequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC\nsystems. Simulation results demonstrate that the proposed NGGAN trained using\nwaveform characteristics is closer to the practically measured dataset in terms\nof the quality of the generated noise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNGGAN\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u7528\u4e8e\u6355\u6349\u7a84\u5e26\u7535\u529b\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u975e\u5468\u671f\u6027\u5f02\u6b65\u8109\u51b2\u566a\u58f0\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u751f\u6210\u7684\u566a\u58f0\u66f4\u63a5\u8fd1\u771f\u5b9e\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u6a21\u578b\u4ec5\u80fd\u6355\u6349\u90e8\u5206\u566a\u58f0\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u7edf\u8ba1\u975e\u5468\u671f\u6027\u5f02\u6b65\u8109\u51b2\u566a\u58f0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u6570\u636e\u5904\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528Wasserstein\u8ddd\u79bb\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u7684NGGAN\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u7684\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u8f93\u5165\u4fe1\u53f7\u957f\u5ea6\u4ee5\u62df\u5408\u566a\u58f0\u751f\u6210\u3002", "result": "NGGAN\u751f\u6210\u7684\u566a\u58f0\u5728\u8d28\u91cf\u4e0a\u66f4\u63a5\u8fd1\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6570\u5b66\u6a21\u578b\u3002", "conclusion": "NGGAN\u901a\u8fc7\u5229\u7528\u6ce2\u5f62\u7279\u5f81\u8bad\u7ec3\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6a21\u62df\u590d\u6742\u566a\u58f0\uff0c\u9002\u7528\u4e8e\u7a84\u5e26\u7535\u529b\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u6570\u636e\u5904\u7406\u3002"}}
{"id": "2510.01452", "pdf": "https://arxiv.org/pdf/2510.01452", "abs": "https://arxiv.org/abs/2510.01452", "authors": ["Laura Connolly", "Tamas Ungi", "Adnan Munawar", "Anton Deguet", "Chris Yeung", "Russell H. Taylor", "Parvin Mousavi", "Gabor Fichtinger Keyvan Hashtrudi-Zaad"], "title": "Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Purpose: Delineating tumor boundaries during breast-conserving surgery is\nchallenging as tumors are often highly mobile, non-palpable, and have\nirregularly shaped borders. To address these challenges, we introduce a\ncooperative robotic guidance system that applies haptic feedback for tumor\nlocalization. In this pilot study, we aim to assess if and how this system can\nbe successfully integrated into breast cancer care.\n  Methods: A small haptic robot is retrofitted with an electrocautery blade to\noperate as a cooperatively controlled surgical tool. Ultrasound and\nelectromagnetic navigation are used to identify the tumor boundaries and\nposition. A forbidden region virtual fixture is imposed when the surgical tool\ncollides with the tumor boundary. We conducted a study where users were asked\nto resect tumors from breast simulants both with and without the haptic\nguidance. We then assess the results of these simulated resections both\nqualitatively and quantitatively.\n  Results: Virtual fixture guidance is shown to improve resection margins. On\naverage, users find the task to be less mentally demanding, frustrating, and\neffort intensive when haptic feedback is available. We also discovered some\nunanticipated impacts on surgical workflow that will guide design adjustments\nand training protocol moving forward.\n  Conclusion: Our results suggest that virtual fixtures can help localize tumor\nboundaries in simulated breast-conserving surgery. Future work will include an\nextensive user study to further validate these results and fine-tune our\nguidance system.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u89e6\u89c9\u53cd\u9988\u7684\u534f\u4f5c\u673a\u5668\u4eba\u5f15\u5bfc\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u624b\u672f\u4e2d\u80bf\u7624\u8fb9\u754c\u5b9a\u4f4d\uff0c\u5e76\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u663e\u793a\u51fa\u6539\u5584\u5207\u9664\u8fb9\u7f18\u548c\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u7684\u6548\u679c\u3002", "motivation": "\u4e73\u817a\u764c\u624b\u672f\u4e2d\uff0c\u80bf\u7624\u8fb9\u754c\u96be\u4ee5\u754c\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\uff0c\u56e0\u6b64\u5f15\u5165\u89e6\u89c9\u53cd\u9988\u673a\u5668\u4eba\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u624b\u672f\u7cbe\u51c6\u5ea6\u3002", "method": "\u6539\u88c5\u5c0f\u578b\u89e6\u89c9\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u8d85\u58f0\u548c\u7535\u78c1\u5bfc\u822a\u5b9a\u4f4d\u80bf\u7624\u8fb9\u754c\uff0c\u901a\u8fc7\u865a\u62df\u5939\u5177\u63d0\u4f9b\u89e6\u89c9\u53cd\u9988\uff0c\u5e76\u5bf9\u6bd4\u6709/\u65e0\u5f15\u5bfc\u7684\u6a21\u62df\u5207\u9664\u6548\u679c\u3002", "result": "\u89e6\u89c9\u53cd\u9988\u6539\u5584\u4e86\u5207\u9664\u8fb9\u7f18\uff0c\u964d\u4f4e\u4e86\u533b\u751f\u7684\u5fc3\u7406\u8d1f\u62c5\u548c\u64cd\u4f5c\u96be\u5ea6\uff0c\u4f46\u4e5f\u66b4\u9732\u4e86\u4e00\u4e9b\u5bf9\u624b\u672f\u6d41\u7a0b\u7684\u672a\u9884\u671f\u5f71\u54cd\u3002", "conclusion": "\u865a\u62df\u5939\u5177\u5728\u6a21\u62df\u624b\u672f\u4e2d\u6709\u6548\u8f85\u52a9\u80bf\u7624\u5b9a\u4f4d\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9a8c\u8bc1\u5e76\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2510.02000", "pdf": "https://arxiv.org/pdf/2510.02000", "abs": "https://arxiv.org/abs/2510.02000", "authors": ["Giusy Spacone", "Sebastian Frey", "Mattia Orlandi", "Pierangelo Maria Rapa", "Victor Kartsch", "Simone Benatti", "Luca Benini", "Andrea Cossettini"], "title": "Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist Kinematic Tracking", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "Hand gesture recognition based on biosignals has shown strong potential for\ndeveloping intuitive human-machine interaction strategies that closely mimic\nnatural human behavior. In particular, sensor fusion approaches have gained\nattention for combining complementary information and overcoming the\nlimitations of individual sensing modalities, thereby enabling more robust and\nreliable systems. Among them, the fusion of surface electromyography (EMG) and\nA-mode ultrasound (US) is very promising. However, prior solutions rely on\npower-hungry platforms unsuitable for multi-day use and are limited to discrete\ngesture classification. In this work, we present an ultra-low-power (sub-50 mW)\nsystem for concurrent acquisition of 8-channel EMG and 4-channel A-mode US\nsignals, integrating two state-of-the-art platforms into fully wearable,\ndry-contact armbands. We propose a framework for continuous tracking of 23\ndegrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a\nkinematic glove for ground-truth labeling. Our method employs lightweight\nencoder-decoder architectures with multi-task learning to simultaneously\nestimate hand and wrist joint angles. Experimental results under realistic\nsensor repositioning conditions demonstrate that EMG-US fusion achieves a root\nmean squared error of $10.6^\\circ\\pm2.0^\\circ$, compared to\n$12.0^\\circ\\pm1^\\circ$ for EMG and $13.1^\\circ\\pm2.6^\\circ$ for US, and a R$^2$\nscore of $0.61\\pm0.1$, with $0.54\\pm0.03$ for EMG and $0.38\\pm0.20$ for US.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u4f4e\u529f\u8017\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408EMG\u548cA-mode US\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e8623\u4e2a\u81ea\u7531\u5ea6\u7684\u8fde\u7eed\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u786e\u5ea6\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u529f\u8017\u9ad8\u4e14\u4ec5\u9002\u7528\u4e8e\u79bb\u6563\u624b\u52bf\u5206\u7c7b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u65e5\u4f7f\u7528\u9700\u6c42\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8d85\u4f4e\u529f\u8017\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u8fde\u7eed\u624b\u52bf\u8ddf\u8e2a\uff0c\u5e76\u878d\u5408EMG\u548cUS\u4fe1\u53f7\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8d85\u4f4e\u529f\u8017\u7cfb\u7edf\uff0c\u96c6\u62108\u901a\u9053EMG\u548c4\u901a\u9053A-mode US\u4fe1\u53f7\u91c7\u96c6\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u4f30\u8ba1\u624b\u548c\u8155\u5173\u8282\u89d2\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cEMG-US\u878d\u5408\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a10.6\u00b0\u00b12.0\u00b0\uff0c\u4f18\u4e8e\u5355\u72ec\u7684EMG\u548cUS\uff1bR\u00b2\u5f97\u5206\u4e3a0.61\u00b10.1\uff0c\u4e5f\u663e\u8457\u9ad8\u4e8e\u5355\u4e00\u6a21\u6001\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u7684\u8fde\u7eed\u624b\u52bf\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.01483", "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "AI": {"tldr": "VL-KnG\u7cfb\u7edf\u901a\u8fc7\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u548c\u9ad8\u6548\u67e5\u8be2\u5904\u7406\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bfc\u822a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5e94\u7528\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5b58\u5728\u573a\u666f\u8bb0\u5fc6\u7f3a\u5931\u3001\u7a7a\u95f4\u63a8\u7406\u6709\u9650\u548c\u5b9e\u65f6\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u89c6\u9891\u5904\u7406\u6784\u5efa\u6301\u4e45\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u9ad8\u6548\u67e5\u8be2\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVL-KnG\u6210\u529f\u7387\u8fbe77.27%\uff0c\u56de\u7b54\u51c6\u786e\u738776.92%\uff0c\u6027\u80fd\u5ab2\u7f8eGemini 2.5 Pro\u3002", "conclusion": "VL-KnG\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u5b9e\u7528\u7684\u7279\u70b9\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2510.02012", "pdf": "https://arxiv.org/pdf/2510.02012", "abs": "https://arxiv.org/abs/2510.02012", "authors": ["Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "David Gonz\u00e1lez G.", "Carlo Fischione"], "title": "Computing on Dirty Paper: Interference-Free Integrated Communication and Computing", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "Submitted to an IEEE conference", "summary": "Inspired by Costa's pioneering work on dirty paper coding (DPC), this paper\nproposes a novel scheme for integrated communication and computing (ICC), named\nComputing on Dirty Paper, whereby the transmission of discrete data symbols for\ncommunication, and over-the-air computation (AirComp) of nomographic functions\ncan be achieved simultaneously over common multiple-access channels. In\nparticular, the proposed scheme allows for the integration of communication and\ncomputation in a manner that is asymptotically interference-free, by\nprecanceling the computing symbols at the transmitters (TXs) using DPC\nprinciples. A simulation-based assessment of the proposed ICC scheme under a\nsingle-input multiple-output (SIMO) setup is also offered, including the\nevaluation of performance for data detection, and of mean-squared-error (MSE)\nperformance for function computation, over a block of symbols. The results\nvalidate the proposed method and demonstrate its ability to significantly\noutperform state-of-the-art (SotA) ICC schemes in terms of both bit error rate\n(BER) and MSE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018Computing on Dirty Paper\u2019\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u810f\u7eb8\u7f16\u7801\uff08DPC\uff09\u539f\u7406\u5728\u5171\u6709\u591a\u5740\u4fe1\u9053\u4e0a\u5b9e\u73b0\u901a\u4fe1\u4e0e\u8ba1\u7b97\u7684\u96c6\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u53d7Costa\u810f\u7eb8\u7f16\u7801\uff08DPC\uff09\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u901a\u4fe1\u4e0e\u8ba1\u7b97\uff08ICC\uff09\u7684\u96c6\u6210\uff0c\u4ee5\u5728\u9ad8\u5e72\u6270\u73af\u5883\u4e0b\u5b9e\u73b0\u5e72\u6270\u81ea\u7531\u7684\u6570\u636e\u4f20\u8f93\u4e0e\u51fd\u6570\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u810f\u7eb8\u7f16\u7801\uff08DPC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53d1\u9001\u7aef\uff08TXs\uff09\u9884\u6d88\u9664\u8ba1\u7b97\u7b26\u53f7\uff0c\u5b9e\u73b0\u901a\u4fe1\u548c\u8ba1\u7b97\u7684\u6e10\u8fdb\u5e72\u6270\u81ea\u7531\u96c6\u6210\u3002", "result": "\u5728\u5355\u8f93\u5165\u591a\u8f93\u51fa\uff08SIMO\uff09\u914d\u7f6e\u4e0b\u7684\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u68c0\u6d4b\u548c\u51fd\u6570\u8ba1\u7b97\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff08SotA\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u2018Computing on Dirty Paper\u2019\u65b9\u6cd5\u901a\u8fc7\u810f\u7eb8\u7f16\u7801\u539f\u7406\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u4fe1\u4e0e\u8ba1\u7b97\u7684\u9ad8\u6548\u96c6\u6210\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728BER\u548cMSE\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.01485", "pdf": "https://arxiv.org/pdf/2510.01485", "abs": "https://arxiv.org/abs/2510.01485", "authors": ["Nicholas B. Andrews", "Yanhao Yang", "Sofya Akhetova", "Kristi A. Morgansen", "Ross L. Hatton"], "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 8 figures, 3 tables", "summary": "This work demonstrates pose (position and shape) estimation for a\nfree-floating, bioinspired multi-link robot with unactuated joints,\nlink-mounted thrusters for control, and a single gyroscope per link, resulting\nin an underactuated, minimally sensed platform. Through a proof-of-concept\nhardware experiment and offline Kalman filter analysis, we show that the\nrobot's pose can be reliably estimated. State estimation is performed using an\nunscented Kalman filter augmented with Gaussian process residual learning to\ncompensate for non-zero-mean, non-Gaussian noise. We further show that a filter\ntrained on a multi-gait dataset (forward, backward, left, right, and turning)\nperforms comparably to one trained on a larger forward-gait-only dataset when\nboth are evaluated on the same forward-gait test trajectory. These results\nreveal overlap in the gait input space, which can be exploited to reduce\ntraining data requirements while enhancing the filter's generalizability across\nmultiple gaits.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u65e0\u4f20\u611f\u5668\u53cd\u9988\u7684\u5e73\u53f0\u4f30\u8ba1\u81ea\u7531\u6d6e\u52a8\u3001\u4eff\u751f\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u59ff\u6001\uff08\u4f4d\u7f6e\u548c\u5f62\u72b6\uff09\uff0c\u5e76\u7ed3\u5408Kalman\u6ee4\u6ce2\u5668\u548c\u9ad8\u65af\u8fc7\u7a0b\u6b8b\u5dee\u5b66\u4e60\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65e0\u4f20\u611f\u5668\u53cd\u9988\u548c\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7684\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408Kalman\u6ee4\u6ce2\u5668\u548c\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u8865\u507f\u566a\u58f0\u3002", "method": "\u4f7f\u7528\u65e0\u8ff9Kalman\u6ee4\u6ce2\u5668\u548c\u9ad8\u65af\u8fc7\u7a0b\u6b8b\u5dee\u5b66\u4e60\u8865\u507f\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u591a\u6b65\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u6ee4\u6ce2\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u673a\u5668\u4eba\u7684\u59ff\u6001\u53ef\u4ee5\u53ef\u9760\u4f30\u8ba1\uff0c\u591a\u6b65\u6001\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u4e0e\u4ec5\u524d\u8fdb\u6b65\u6001\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u6b65\u6001\u8f93\u5165\u7a7a\u95f4\u5b58\u5728\u91cd\u53e0\uff0c\u53ef\u4ee5\u5229\u7528\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u6ee4\u6ce2\u5668\u5728\u591a\u6b65\u6001\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.02021", "pdf": "https://arxiv.org/pdf/2510.02021", "abs": "https://arxiv.org/abs/2510.02021", "authors": ["Gian Marti", "Christoph Studer"], "title": "Joint Jammer Mitigation and Data Detection", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "This work has not been submitted to the IEEE for possible\n  publication. The copyright remains with the authors, and this version will\n  remain publicly accessible", "summary": "Multi-antenna (or MIMO) processing is a promising solution to the problem of\njammer mitigation. Existing methods mitigate the jammer based on an estimate of\nits spatial signature that is acquired through a dedicated training phase. This\nstrategy has two main drawbacks: (i) it reduces the communication rate since no\ndata can be transmitted during the training phase and (ii) it can be evaded by\nsmart or multi-antenna jammers that do not transmit during the training phase\nor that dynamically change their subspace through time-varying beamforming. To\naddress these drawbacks, we propose Joint jammer Mitigation and data Detection\n(JMD), a novel paradigm for MIMO jammer mitigation. The core idea of JMD is to\nestimate and remove the jammer interference subspace jointly with detecting the\nlegitimate transmit data over multiple time slots. Doing so removes the need\nfor a dedicated and rate-reducing training period while being able to mitigate\nsmart and dynamic multi-antenna jammers. We provide two JMD-type algorithms,\nSANDMAN and MAED, that differ in the way they estimate the channels of the\nlegitimate transmitters and achieve different complexity-performance tradeoffs.\nExtensive simulations demonstrate the efficacy of JMD for jammer mitigation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MIMO\u6297\u5e72\u6270\u65b9\u6cd5JMD\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u548c\u6d88\u9664\u5e72\u6270\u5b50\u7a7a\u95f4\u4e0e\u6570\u636e\u4f20\u8f93\u68c0\u6d4b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u7528\u8bad\u7ec3\u9636\u6bb5\u7684\u7f3a\u70b9\u3002", "motivation": "\u4f20\u7edfMIMO\u6297\u5e72\u6270\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u8bad\u7ec3\u9636\u6bb5\u4f30\u8ba1\u5e72\u6270\u7279\u5f81\uff0c\u5b58\u5728\u901a\u4fe1\u901f\u7387\u964d\u4f4e\u548c\u65e0\u6cd5\u5e94\u5bf9\u667a\u80fd\u6216\u52a8\u6001\u5e72\u6270\u7684\u7f3a\u70b9\u3002", "method": "\u63d0\u51faJMD\u8303\u5f0f\uff0c\u8054\u5408\u4f30\u8ba1\u5e72\u6270\u5b50\u7a7a\u95f4\u548c\u68c0\u6d4b\u5408\u6cd5\u6570\u636e\uff0c\u65e0\u9700\u4e13\u7528\u8bad\u7ec3\u9636\u6bb5\uff1b\u5177\u4f53\u5b9e\u73b0\u5305\u62ecSANDMAN\u548cMAED\u4e24\u79cd\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e0d\u540c\u7684\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u6298\u8877\u3002", "result": "\u4eff\u771f\u8868\u660eJMD\u5728\u6297\u5e72\u6270\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "JMD\u901a\u8fc7\u6d88\u9664\u4e13\u7528\u8bad\u7ec3\u9636\u6bb5\u548c\u52a8\u6001\u6297\u5e72\u6270\u80fd\u529b\uff0c\u63d0\u5347\u4e86MIMO\u7cfb\u7edf\u7684\u6297\u5e72\u6270\u6027\u80fd\u3002"}}
{"id": "2510.01519", "pdf": "https://arxiv.org/pdf/2510.01519", "abs": "https://arxiv.org/abs/2510.01519", "authors": ["Wei Han Chen", "Yuchen Liu", "Alexiy Buynitsky", "Ahmed H. Qureshi"], "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "Robot navigation in large, complex, and unknown indoor environments is a\nchallenging problem. The existing approaches, such as traditional\nsampling-based methods, struggle with resolution control and scalability, while\nimitation learning-based methods require a large amount of demonstration data.\nActive Neural Time Fields (ANTFields) have recently emerged as a promising\nsolution by using local observations to learn cost-to-go functions without\nrelying on demonstrations. Despite their potential, these methods are hampered\nby challenges such as spectral bias and catastrophic forgetting, which diminish\ntheir effectiveness in complex scenarios. To address these issues, our approach\ndecomposes the planning problem into a hierarchical structure. At the high\nlevel, a sparse graph captures the environment's global connectivity, while at\nthe low level, a planner based on neural fields navigates local obstacles by\nsolving the Eikonal PDE. This physics-informed strategy overcomes common\npitfalls like spectral bias and neural field fitting difficulties, resulting in\na smooth and precise representation of the cost landscape. We validate our\nframework in large-scale environments, demonstrating its enhanced adaptability\nand precision compared to previous methods, and highlighting its potential for\nonline exploration, mapping, and real-world navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u56fe\u6355\u6349\u5168\u5c40\u8fde\u901a\u6027\u548c\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u5c40\u90e8\u89c4\u5212\u5668\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u590d\u6742\u3001\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5206\u8fa8\u7387\u63a7\u5236\u3001\u6269\u5c55\u6027\u4e0d\u8db3\u4ee5\u53ca\u5bf9\u5927\u91cf\u6f14\u793a\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u9ad8\u5c42\u901a\u8fc7\u7a00\u758f\u56fe\u8868\u793a\u5168\u5c40\u8fde\u901a\u6027\uff0c\u4f4e\u5c42\u4f7f\u7528\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u89c4\u5212\u5668\u89e3\u51b3Eikonal PDE\uff0c\u5b9e\u73b0\u5c40\u90e8\u907f\u969c\u548c\u7cbe\u786e\u5bfc\u822a\u3002", "result": "\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9002\u5e94\u6027\u3001\u7cbe\u786e\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u5206\u5c42\u89c4\u5212\u6709\u6548\u514b\u670d\u4e86\u8c31\u504f\u5dee\u548c\u707e\u96be\u6027\u9057\u5fd8\u7b49\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u5728\u7ebf\u63a2\u7d22\u3001\u5730\u56fe\u6784\u5efa\u548c\u5b9e\u9645\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02023", "pdf": "https://arxiv.org/pdf/2510.02023", "abs": "https://arxiv.org/abs/2510.02023", "authors": ["Ping Wang", "Zulin Wang", "Yuanhan Ni", "Qu Luo", "Yuanfang Ma", "Xiaosi Tian", "Pei Xiao"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "categories": ["eess.SP"], "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has garnered significant\nattention due to its superior performance in high-mobility scenarios, coupled\nwith multiple waveform parameters that provide greater degrees of freedom for\nsystem design. This paper introduces a novel secure affine frequency division\nmultiplexing (SE-AFDM) system, which advances prior designs by dynamically\nvarying an AFDM pre-chirp parameter to enhance physical-layer security. In the\nSE-AFDM system, the pre-chirp parameter is dynamically generated from a\ncodebook controlled by a long-period pseudo-noise (LPPN) sequence. Instead of\napplying spreading in the data domain, our parameter-domain spreading approach\nprovides additional security while maintaining reliability and high spectrum\nefficiency. We also propose a synchronization framework to solve the problem of\nreliably and rapidly synchronizing the time-varying parameter in fast\ntime-varying channels. The theoretical derivations prove that unsynchronized\neavesdroppers cannot eliminate the nonlinear impact of the time-varying\nparameter and further provide useful guidance for codebook design. Simulation\nresults demonstrate the security advantages of the proposed SE-AFDM system in\nhigh-mobility scenarios, while our hardware prototype validates the\neffectiveness of the proposed synchronization framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5b89\u5168\u4eff\u5c04\u9891\u5206\u590d\u7528\u7cfb\u7edf\uff08SE-AFDM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u53d8\u5316\u7684\u9884\u5541\u557e\u53c2\u6570\u589e\u5f3a\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u5e76\u5728\u5feb\u901f\u65f6\u53d8\u4fe1\u9053\u4e2d\u5b9e\u73b0\u9ad8\u6548\u540c\u6b65\u3002", "motivation": "\u4e3a\u89e3\u51b3\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684\u901a\u4fe1\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51faSE-AFDM\u7cfb\u7edf\uff0c\u5229\u7528\u53c2\u6570\u57df\u6269\u5c55\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001\u751f\u6210\u9884\u5541\u557e\u53c2\u6570\u7684\u7801\u672c\uff0c\u5e76\u8bbe\u8ba1\u540c\u6b65\u6846\u67b6\u4ee5\u9002\u5e94\u5feb\u901f\u65f6\u53d8\u4fe1\u9053\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u548c\u4eff\u771f\u8868\u660eSE-AFDM\u5728\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u9891\u8c31\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u786c\u4ef6\u539f\u578b\u9a8c\u8bc1\u4e86\u540c\u6b65\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "SE-AFDM\u7cfb\u7edf\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.01592", "pdf": "https://arxiv.org/pdf/2510.01592", "abs": "https://arxiv.org/abs/2510.01592", "authors": ["Shun Niijima", "Ryoichi Tsuzaki", "Noriaki Takasugi", "Masaya Kinoshita"], "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion", "categories": ["cs.RO"], "comment": "8 pages, 12 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transfered without notice, after which\n  this version may no longer be accessible", "summary": "This paper proposes a real-time multi-plane segmentation method based on\nGPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.\nExisting online planar mapping approaches struggle to balance accuracy and\ncomputational efficiency: direct depth image segmentation from specific sensors\nsuffers from poor temporal integration, height map-based methods cannot\nrepresent complex 3D structures like overhangs, and voxel-based plane\nsegmentation remains unexplored for real-time applications. To address these\nlimitations, we develop a novel framework that integrates vertex-based\nconnected component labeling with random sample consensus based plane detection\nand convex hull, leveraging GPU parallel computing to rapidly extract planar\nregions from point clouds accumulated in high-resolution 3D voxel maps.\nExperimental results demonstrate that the proposed method achieves fast and\naccurate 3D multi-plane segmentation at over 30 Hz update rate even at a\nresolution of 0.01 m, enabling the detected planes to be utilized in real time\nfor locomotion tasks. Furthermore, we validate the effectiveness of our\napproach through experiments in both simulated environments and physical legged\nrobot platforms, confirming robust locomotion performance when considering 3D\nplanar structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u5b9e\u65f6\u591a\u5e73\u9762\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u5e73\u9762\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u590d\u67423D\u7ed3\u6784\u7684\u8868\u793a\u548c\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u9ad8\u6548\u5904\u7406\u3002", "method": "\u7ed3\u5408\u9876\u70b9\u8fde\u901a\u6027\u6807\u8bb0\u3001RANSAC\u5e73\u9762\u68c0\u6d4b\u548c\u51f8\u5305\u7b97\u6cd5\uff0c\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u4ece\u9ad8\u5206\u8fa8\u73873D\u4f53\u7d20\u56fe\u4e2d\u5feb\u901f\u63d0\u53d6\u5e73\u9762\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57280.01\u7c73\u5206\u8fa8\u7387\u4e0b\u80fd\u4ee5\u8d85\u8fc730Hz\u7684\u901f\u7387\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u76843D\u591a\u5e73\u9762\u5206\u5272\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f63D\u591a\u5e73\u9762\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2510.02029", "pdf": "https://arxiv.org/pdf/2510.02029", "abs": "https://arxiv.org/abs/2510.02029", "authors": ["Haonan Si", "Zhaolin Wang", "Xiansheng Guo", "Jin Zhang", "Yuanwei Liu"], "title": "Joint DOA and Attitude Sensing Based on Tri-Polarized Continuous Aperture Array", "categories": ["eess.SP"], "comment": "13 pages, 10 figures", "summary": "This paper investigates joint direction-of-arrival (DOA) and attitude sensing\nusing tri-polarized continuous aperture arrays (CAPAs). By employing\nelectromagnetic (EM) information theory, the spatially continuous received\nsignals in tri-polarized CAPA are modeled, thereby enabling accurate DOA and\nattitude estimation. To facilitate subspace decomposition for continuous\noperators, an equivalent continuous-discrete transformation technique is\ndeveloped. Moreover, both self- and cross-covariances of tri-polarized signals\nare exploited to construct a tri-polarized spectrum, significantly enhancing\nDOA estimation performance. Theoretical analyses reveal that the\nidentifiability of attitude information fundamentally depends on the\navailability of prior target snapshots. Accordingly, two attitude estimation\nalgorithms are proposed: one capable of estimating partial attitude information\nwithout prior knowledge, and the other achieving full attitude estimation when\nsuch knowledge is available. Numerical results demonstrate the feasibility and\nsuperiority of the proposed framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u4e09\u6781\u5316\u8fde\u7eed\u5b54\u5f84\u9635\u5217\uff08CAPAs\uff09\u8fdb\u884c\u8054\u5408\u65b9\u5411\u5230\u8fbe\uff08DOA\uff09\u548c\u59ff\u6001\u611f\u77e5\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7535\u78c1\u4fe1\u606f\u7406\u8bba\u5efa\u6a21\u4fe1\u53f7\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u4e09\u6781\u5316CAPA\u548c\u7535\u78c1\u4fe1\u606f\u7406\u8bba\uff0c\u63d0\u5347DOA\u548c\u59ff\u6001\u611f\u77e5\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u8fde\u7eed-\u79bb\u6563\u53d8\u6362\u6280\u672f\uff0c\u5e76\u5229\u7528\u4e09\u6781\u5316\u4fe1\u53f7\u7684\u81ea\u534f\u65b9\u5dee\u548c\u4e92\u534f\u65b9\u5dee\u6784\u5efa\u4e09\u6781\u5316\u9891\u8c31\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u59ff\u6001\u4fe1\u606f\u7684\u53ef\u8bc6\u522b\u6027\u4f9d\u8d56\u4e8e\u76ee\u6807\u5feb\u7167\u7684\u53ef\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\u3002", "conclusion": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.01603", "pdf": "https://arxiv.org/pdf/2510.01603", "abs": "https://arxiv.org/abs/2510.01603", "authors": ["Sharfin Islam", "Zewen Chen", "Zhanpeng He", "Swapneel Bhatt", "Andres Permuy", "Brock Taylor", "James Vickery", "Pedro Piacenza", "Cheng Zhang", "Matei Ciocarlie"], "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity", "categories": ["cs.RO"], "comment": null, "summary": "Bimanual robot manipulators can achieve impressive dexterity, but typically\nrely on two full six- or seven- degree-of-freedom arms so that paired grippers\ncan coordinate effectively. This traditional framework increases system\ncomplexity while only exploiting a fraction of the overall workspace for\ndexterous interaction. We introduce the MiniBEE (Miniature Bimanual\nEnd-effector), a compact system in which two reduced-mobility arms (3+ DOF\neach) are coupled into a kinematic chain that preserves full relative\npositioning between grippers. To guide our design, we formulate a kinematic\ndexterity metric that enlarges the dexterous workspace while keeping the\nmechanism lightweight and wearable. The resulting system supports two\ncomplementary modes: (i) wearable kinesthetic data collection with self-tracked\ngripper poses, and (ii) deployment on a standard robot arm, extending dexterity\nacross its entire workspace. We present kinematic analysis and design\noptimization methods for maximizing dexterous range, and demonstrate an\nend-to-end pipeline in which wearable demonstrations train imitation learning\npolicies that perform robust, real-world bimanual manipulation.", "AI": {"tldr": "MiniBEE\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4e24\u4e2a\u4f4e\u81ea\u7531\u5ea6\u624b\u81c2\u8026\u5408\u4e3a\u8fd0\u52a8\u94fe\uff0c\u63d0\u9ad8\u7075\u5de7\u6027\u3002\u8bbe\u8ba1\u91c7\u7528\u7075\u5de7\u6027\u5ea6\u91cf\uff0c\u652f\u6301\u7a7f\u6234\u5f0f\u6570\u636e\u6536\u96c6\u548c\u673a\u5668\u4eba\u81c2\u90e8\u7f72\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u7a33\u5065\u7684\u53cc\u81c2\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u7684\u53cc\u81c2\u673a\u5668\u4eba\u4f9d\u8d56\u4e24\u4e2a\u9ad8\u81ea\u7531\u5ea6\u624b\u81c2\uff0c\u7cfb\u7edf\u590d\u6742\u4e14\u7075\u5de7\u5de5\u4f5c\u7a7a\u95f4\u6709\u9650\u3002MiniBEE\u65e8\u5728\u901a\u8fc7\u4f4e\u81ea\u7531\u5ea6\u624b\u81c2\u8026\u5408\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u7075\u5de7\u6027\u5e76\u7b80\u5316\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1MiniBEE\u7cfb\u7edf\uff0c\u5c06\u4e24\u4e2a3+\u81ea\u7531\u5ea6\u624b\u81c2\u8026\u5408\u4e3a\u8fd0\u52a8\u94fe\uff0c\u4fdd\u7559\u5939\u6301\u5668\u4e4b\u95f4\u7684\u5168\u76f8\u5bf9\u5b9a\u4f4d\u3002\u63d0\u51fa\u7075\u5de7\u6027\u5ea6\u91cf\u6307\u5bfc\u8bbe\u8ba1\uff0c\u652f\u6301\u4e24\u79cd\u4e92\u8865\u6a21\u5f0f\uff1a\u7a7f\u6234\u5f0f\u6570\u636e\u6536\u96c6\u548c\u673a\u5668\u4eba\u81c2\u90e8\u7f72\u3002", "result": "MiniBEE\u7cfb\u7edf\u5b9e\u73b0\u4e86\u7d27\u51d1\u8bbe\u8ba1\u548c\u6269\u5c55\u7684\u7075\u5de7\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5c55\u793a\u4e86\u7a33\u5065\u7684\u53cc\u81c2\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "MiniBEE\u901a\u8fc7\u4f4e\u81ea\u7531\u5ea6\u624b\u81c2\u8026\u5408\u548c\u7075\u5de7\u6027\u5ea6\u91cf\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u53cc\u81c2\u673a\u5668\u4eba\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.02103", "pdf": "https://arxiv.org/pdf/2510.02103", "abs": "https://arxiv.org/abs/2510.02103", "authors": ["Kawon Han", "Kaitao Meng", "Christos Masouros"], "title": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing Unauthorized Sensing", "categories": ["eess.SP"], "comment": "15 pages, 12 figures, accepted to IEEE Transactions on Wireless\n  Communications", "summary": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u611f\u77e5\u7684ISAC\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4fe1\u53f7\u4e2d\u5f15\u5165\u4eba\u5de5\u7f3a\u9677\u4ee5\u6df7\u6dc6\u975e\u6cd5\u611f\u77e5\uff0c\u540c\u65f6\u786e\u4fdd\u5408\u6cd5\u7cfb\u7edf\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ISAC\u7cfb\u7edf\u4e2d\uff0c\u611f\u77e5\u53c2\u6570\u5bb9\u6613\u88ab\u975e\u6cd5\u96f7\u8fbe\u7a83\u53d6\uff0c\u4e9f\u9700\u5f00\u53d1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4fe1\u53f7\u7684\u81ea\u76f8\u5173\u51fd\u6570\u548c\u5f15\u5165\u4eba\u5de5\u76ee\u6807\uff0c\u5e72\u6270\u975e\u6cd5\u611f\u77e5\u7684\u8303\u56f4\u4f30\u8ba1\uff0c\u540c\u65f6\u5408\u6cd5\u7cfb\u7edf\u4f7f\u7528\u5931\u914d\u6ee4\u6ce2\u6291\u5236\u5e72\u6270\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u975e\u6cd5\u611f\u77e5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u6cd5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u5e73\u8861\u3002"}}
{"id": "2510.01607", "pdf": "https://arxiv.org/pdf/2510.01607", "abs": "https://arxiv.org/abs/2510.01607", "authors": ["Qiyuan Zeng", "Chengmeng Li", "Jude St. John", "Zhongyi Zhou", "Junjie Wen", "Guorui Feng", "Yichen Zhu", "Yi Xu"], "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations", "categories": ["cs.RO", "cs.CV"], "comment": "technique report. The website is available at\n  https://activeumi.github.io", "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.", "AI": {"tldr": "ActiveUMI\u662f\u4e00\u4e2a\u4fbf\u643a\u5f0fVR\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u59ff\u6001\u5bf9\u9f50\u548c\u4e3b\u52a8\u611f\u77e5\u8bb0\u5f55\uff0c\u5c06\u4eba\u7c7b\u64cd\u4f5c\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u4e0a\uff0c\u5b9e\u73b0\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91ce\u5916\u4eba\u7c7b\u793a\u8303\u6570\u636e\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u4e0a\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u64cd\u4f5c\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u4fbf\u643a\u5f0fVR\u9065\u64cd\u4f5c\u5957\u4ef6\u548c\u4f20\u611f\u5668\u5316\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6c89\u6d78\u5f0f3D\u6e32\u67d3\u3001\u72ec\u7acb\u7a7f\u6234\u8ba1\u7b97\u673a\u548c\u9ad8\u6548\u6821\u51c6\u65b9\u6cd5\uff0c\u6355\u6349\u4e3b\u52a8\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u3002", "result": "\u5728\u516d\u4e2a\u590d\u6742\u53cc\u624b\u4efb\u52a1\u4e2d\uff0c\u7cfb\u7edf\u5e73\u5747\u6210\u529f\u7387\u8fbe70%\uff0c\u5e76\u5bf9\u65b0\u7269\u4f53\u548c\u73af\u5883\u5c55\u73b0\u51fa56%\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4fbf\u643a\u5f0f\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u7ed3\u5408\u4e3b\u52a8\u611f\u77e5\u5b66\u4e60\uff0c\u4e3a\u673a\u5668\u4eba\u7684\u901a\u7528\u80fd\u529b\u63d0\u4f9b\u9ad8\u6548\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2510.02108", "pdf": "https://arxiv.org/pdf/2510.02108", "abs": "https://arxiv.org/abs/2510.02108", "authors": ["Jinshuo Zhang", "Yafei Wang", "Xinping Yi", "Wenjin Wang", "Shi Jin", "Symeon Chatzinotas", "Bj\u00f6rn Ottersten"], "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network", "categories": ["eess.SP", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u590d\u6742\u5ea6\u7b26\u53f7\u7ea7\u9884\u7f16\u7801\u6846\u67b6\uff0c\u5229\u7528\u95ed\u5f0f\u89e3\u548c\u5f20\u91cf\u7b49\u53d8\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u4e0d\u5b8c\u7f8eCSI\u573a\u666f\u4e0b\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u7b26\u53f7\u7ea7\u9884\u7f16\u7801(SLP)\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u662f\u5176\u5e94\u7528\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e0e\u95ed\u5f0f\u89e3\uff0c\u8bbe\u8ba1\u5177\u6709\u5f20\u91cf\u7b49\u53d8\u6027\u7684\u6620\u5c04\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u663e\u793a\u6240\u63d0\u6846\u67b6\u5728\u6027\u80fd\u63a5\u8fd1\u6700\u4f18SLP\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e86\u7ea680\u500d\uff0c\u4e14\u5bf9\u4e0d\u540c\u7528\u6237\u6570\u548c\u7b26\u53f7\u5757\u957f\u5ea6\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86SLP\u7684\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.01642", "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "categories": ["cs.RO"], "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "AI": {"tldr": "FailSafe\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5931\u8d25\u751f\u6210\u4e0e\u6062\u590d\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u5931\u8d25\u6848\u4f8b\u53ca\u53ef\u6267\u884c\u6062\u590d\u52a8\u4f5c\uff0c\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u901a\u5e38\u53ea\u63d0\u4f9b\u771f\u5b9e\u8f68\u8ff9\u6216\u6587\u672c\u89e3\u91ca\uff0c\u96be\u4ee5\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u76f4\u63a5\u6709\u6548\u7684\u5931\u8d25\u6062\u590d\u652f\u6301\u3002", "method": "\u63d0\u51faFailSafe\u7cfb\u7edf\uff0c\u81ea\u52a8\u751f\u6210\u5931\u8d25\u6848\u4f8b\u548c\u6062\u590d\u52a8\u4f5c\uff0c\u5e76\u57fa\u4e8eLLaVa-OV-7B\u5fae\u8c03\u6784\u5efaFailSafe-VLM\u3002", "result": "FailSafe-VLM\u663e\u8457\u63d0\u5347\u4e86\u4e09\u79cdVLA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e73\u5747\u6539\u8fdb\u8fbe22.6%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u573a\u666f\u548c\u673a\u5668\u4eba\u5f62\u6001\u3002", "conclusion": "FailSafe\u586b\u8865\u4e86\u673a\u5668\u4eba\u5931\u8d25\u6062\u590d\u6570\u636e\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01364", "pdf": "https://arxiv.org/pdf/2510.01364", "abs": "https://arxiv.org/abs/2510.01364", "authors": ["Jonathan Gornet", "Yilin Mo", "Bruno Sinopoli"], "title": "A Control Theory inspired Exploration Method for a Linear Bandit driven by a Linear Gaussian Dynamical System", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "The paper introduces a linear bandit environment where the reward is the\noutput of a known Linear Gaussian Dynamical System (LGDS). In this environment,\nwe address the fundamental challenge of balancing exploration -- gathering\ninformation about the environment -- and exploitation -- selecting to the\naction with the highest predicted reward. We propose two algorithms, Kalman\nfilter Upper Confidence Bound (Kalman-UCB) and Information filter Directed\nExploration Action-selection (IDEA). Kalman-UCB uses the principle of optimism\nin the face of uncertainty. IDEA selects actions that maximize the combination\nof the predicted reward and a term that quantifies how much an action minimizes\nthe error of the Kalman filter state prediction, which depends on the LGDS\nproperty called observability. IDEA is motivated by applications such as\nhyperparameter optimization in machine learning. A major problem encountered in\nhyperparameter optimization is the large action spaces, which hinder the\nperformance of methods inspired by principle of optimism in the face of\nuncertainty as they need to explore each action to lower reward prediction\nuncertainty. To predict if either Kalman-UCB or IDEA will perform better, a\nmetric based on the LGDS properties is provided. This metric is validated with\nnumerical results across a variety of randomly generated environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff08Kalman-UCB\u548cIDEA\uff09\u5728\u7ebf\u6027\u9ad8\u65af\u52a8\u6001\u7cfb\u7edf\u73af\u5883\u4e0b\u89e3\u51b3\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u901a\u8fc7LGDS\u5c5e\u6027\u63d0\u4f9b\u6027\u80fd\u9884\u6d4b\u6307\u6807\u3002", "motivation": "\u5728\u8d85\u53c2\u6570\u4f18\u5316\u7b49\u5e94\u7528\u4e2d\uff0c\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u9650\u5236\u4e86\u57fa\u4e8e\u4e50\u89c2\u4e0d\u786e\u5b9a\u6027\u539f\u5219\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u597d\u7684\u63a2\u7d22\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86Kalman-UCB\uff08\u57fa\u4e8e\u4e50\u89c2\u4e0d\u786e\u5b9a\u6027\uff09\u548cIDEA\uff08\u7ed3\u5408\u5956\u52b1\u9884\u6d4b\u548c\u6700\u5c0f\u5316\u72b6\u6001\u9884\u6d4b\u8bef\u5dee\uff09\u4e24\u79cd\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u968f\u673a\u751f\u6210\u73af\u5883\u9a8c\u8bc1\u4e86\u57fa\u4e8eLGDS\u5c5e\u6027\u7684\u6027\u80fd\u9884\u6d4b\u6307\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "IDEA\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f18\u4e8eKalman-UCB\uff0c\u4e3a\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u7684\u63a2\u7d22-\u5229\u7528\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.01648", "pdf": "https://arxiv.org/pdf/2510.01648", "abs": "https://arxiv.org/abs/2510.01648", "authors": ["Seungwon Choi", "Donggyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation", "categories": ["cs.RO"], "comment": null, "summary": "A fundamental challenge in robust visual-inertial odometry (VIO) is to\ndynamically assess the reliability of sensor measurements. This assessment is\ncrucial for properly weighting the contribution of each measurement to the\nstate estimate. Conventional methods often simplify this by assuming a static,\nuniform uncertainty for all measurements. This heuristic, however, may be\nlimited in its ability to capture the dynamic error characteristics inherent in\nreal-world data. To improve this limitation, we present a statistical framework\nthat learns measurement reliability assessment online, directly from sensor\ndata and optimization results. Our approach leverages multi-view geometric\nconsistency as a form of self-supervision. This enables the system to infer\nlandmark uncertainty and adaptively weight visual measurements during\noptimization. We evaluated our method on the public EuRoC dataset,\ndemonstrating improvements in tracking accuracy with average reductions of\napproximately 24\\% in translation error and 42\\% in rotation error compared to\nbaseline methods with fixed uncertainty parameters. The resulting framework\noperates in real time while showing enhanced accuracy and robustness. To\nfacilitate reproducibility and encourage further research, the source code will\nbe made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8bc4\u4f30\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u6d4b\u91cf\u53ef\u9760\u6027\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u4f5c\u4e3a\u81ea\u76d1\u7763\uff0c\u5b66\u4e60\u5b9e\u65f6\u6d4b\u91cf\u53ef\u9760\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfVIO\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u662f\u9759\u6001\u4e14\u5747\u5300\u7684\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u8bef\u5dee\u7279\u6027\u3002\u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e00\u5c40\u9650\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u5229\u7528\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u4f5c\u4e3a\u81ea\u76d1\u7763\u4fe1\u53f7\uff0c\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u6570\u636e\u548c\u4f18\u5316\u7ed3\u679c\u4e2d\u5728\u7ebf\u5b66\u4e60\u6d4b\u91cf\u53ef\u9760\u6027\uff0c\u5e76\u6839\u636e\u5730\u6807\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u5730\u52a0\u6743\u89c6\u89c9\u6d4b\u91cf\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u56fa\u5b9a\u4e0d\u786e\u5b9a\u6027\u53c2\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51cf\u5c11\u4e8624%\u7684\u5e73\u79fb\u8bef\u5dee\u548c42%\u7684\u65cb\u8f6c\u8bef\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u65f6\u8fd0\u884c\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.01558", "pdf": "https://arxiv.org/pdf/2510.01558", "abs": "https://arxiv.org/abs/2510.01558", "authors": ["Zhengyang Shen", "Xuehao Zhai", "Hua Tu", "Mayue Shi"], "title": "CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal Chagas Disease Detection", "categories": ["cs.CE", "cs.LG", "eess.SP"], "comment": "4 pages, 2 figures. Accepted for oral presentation at the 52nd\n  international Computing in Cardiology Conference (CinC2025)", "summary": "Chagas disease affects nearly 6 million people worldwide, with Chagas\ncardiomyopathy representing its most severe complication. In regions where\nserological testing capacity is limited, AI-enhanced electrocardiogram (ECG)\nscreening provides a critical diagnostic alternative. However, existing machine\nlearning approaches face challenges such as limited accuracy, reliance on large\nlabeled datasets, and more importantly, weak integration with evidence-based\nclinical diagnostic indicators. We propose a retrieval-augmented generation\nframework, CardioRAG, integrating large language models with interpretable\nECG-based clinical features, including right bundle branch block, left anterior\nfascicular block, and heart rate variability metrics. The framework uses\nvariational autoencoder-learned representations for semantic case retrieval,\nproviding contextual cases to guide clinical reasoning. Evaluation demonstrated\nhigh recall performance of 89.80%, with a maximum F1 score of 0.68 for\neffective identification of positive cases requiring prioritized serological\ntesting. CardioRAG provides an interpretable, clinical evidence-based approach\nparticularly valuable for resource-limited settings, demonstrating a pathway\nfor embedding clinical indicators into trustworthy medical AI systems.", "AI": {"tldr": "CardioRAG\u662f\u4e00\u79cd\u57fa\u4e8eAI\u7684\u53ef\u89e3\u91caECG\u7b5b\u67e5\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4e34\u5e8a\u7279\u5f81\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u67e5\u52a0\u65af\u75c5\u5fc3\u808c\u75c5\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u67e5\u52a0\u65af\u75c5\u5fc3\u808c\u75c5\u7684\u8bca\u65ad\u5728\u8d44\u6e90\u6709\u9650\u5730\u533a\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709AI\u65b9\u6cd5\u51c6\u786e\u6027\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u4e34\u5e8a\u6307\u6807\u7684\u6574\u5408\u3002", "method": "\u63d0\u51faCardioRAG\u6846\u67b6\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u4e34\u5e8a\u7279\u5f81\uff08\u5982\u53f3\u675f\u652f\u4f20\u5bfc\u963b\u6ede\uff09\uff0c\u901a\u8fc7\u6848\u4f8b\u68c0\u7d22\u589e\u5f3a\u8bca\u65ad\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\u9ad8\u53ec\u56de\u7387\uff0889.80%\uff09\u548c\u6700\u9ad8F1\u5206\u65700.68\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u9700\u4f18\u5148\u8840\u6e05\u68c0\u6d4b\u7684\u75c5\u4f8b\u3002", "conclusion": "CardioRAG\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684AI\u533b\u7597\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u4e34\u5e8a\u6307\u6807\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.01661", "pdf": "https://arxiv.org/pdf/2510.01661", "abs": "https://arxiv.org/abs/2510.01661", "authors": ["Yifei Simon Shao", "Yuchen Zheng", "Sunan Sun", "Pratik Chaudhari", "Vijay Kumar", "Nadia Figueroa"], "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation", "categories": ["cs.RO"], "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP)\n  Workshop Best Paper Award (https://sites.google.com/view/symskill)", "summary": "Multi-step manipulation in dynamic environments remains challenging. Two\nmajor families of methods fail in distinct ways: (i) imitation learning (IL) is\nreactive but lacks compositional generalization, as monolithic policies do not\ndecide which skill to reuse when scenes change; (ii) classical task-and-motion\nplanning (TAMP) offers compositionality but has prohibitive planning latency,\npreventing real-time failure recovery. We introduce SymSkill, a unified\nlearning framework that combines the benefits of IL and TAMP, allowing\ncompositional generalization and failure recovery in real-time. Offline,\nSymSkill jointly learns predicates, operators, and skills directly from\nunlabeled and unsegmented demonstrations. At execution time, upon specifying a\nconjunction of one or more learned predicates, SymSkill uses a symbolic planner\nto compose and reorder learned skills to achieve the symbolic goals, while\nperforming recovery at both the motion and symbolic levels in real time.\nCoupled with a compliant controller, SymSkill enables safe and uninterrupted\nexecution under human and environmental disturbances. In RoboCasa simulation,\nSymSkill can execute 12 single-step tasks with 85% success rate. Without\nadditional data, it composes these skills into multi-step plans requiring up to\n6 skill recompositions, recovering robustly from execution failures. On a real\nFranka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented\nand unlabeled play data, is capable of performing multiple tasks simply by goal\nspecifications. The source code and additional analysis can be found on\nhttps://sites.google.com/view/symskill.", "AI": {"tldr": "SymSkill\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7ec4\u5408\u6cdb\u5316\u548c\u5931\u8d25\u6062\u590d\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7f3a\u4e4f\u7ec4\u5408\u6cdb\u5316\u548c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u5b9e\u65f6\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "SymSkill\u6846\u67b6\u79bb\u7ebf\u5b66\u4e60\u8c13\u8bcd\u3001\u64cd\u4f5c\u7b26\u548c\u6280\u80fd\uff0c\u5728\u7ebf\u4f7f\u7528\u7b26\u53f7\u89c4\u5212\u5b9e\u65f6\u7ec4\u5408\u6280\u80fd\u4e0e\u6062\u590d\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u6210\u529f\u738785%\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b66\u4e605\u5206\u949f\u6570\u636e\u5373\u53ef\u6267\u884c\u591a\u4efb\u52a1\u3002", "conclusion": "SymSkill\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u7684\u52a8\u6001\u73af\u5883\u64cd\u4f5c\u6846\u67b6\u3002"}}
{"id": "2510.01608", "pdf": "https://arxiv.org/pdf/2510.01608", "abs": "https://arxiv.org/abs/2510.01608", "authors": ["Roman Jacome", "Romario Gualdr\u00f3n-Hurtado", "Leon Suarez", "Henry Arguello"], "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems", "categories": ["cs.CV", "eess.SP", "math.OC"], "comment": "25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025", "summary": "Imaging inverse problems aims to recover high-dimensional signals from\nundersampled, noisy measurements, a fundamentally ill-posed task with infinite\nsolutions in the null-space of the sensing operator. To resolve this ambiguity,\nprior information is typically incorporated through handcrafted regularizers or\nlearned models that constrain the solution space. However, these priors\ntypically ignore the task-specific structure of that null-space. In this work,\nwe propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel\nclass of regularization that, instead of enforcing structural constraints in\nthe image domain, promotes solutions that lie in a low-dimensional projection\nof the sensing matrix's null-space with a neural network. Our approach has two\nkey advantages: (1) Interpretability: by focusing on the structure of the\nnull-space, we design sensing-matrix-specific priors that capture information\northogonal to the signal components that are fundamentally blind to the sensing\nprocess. (2) Flexibility: NPN is adaptable to various inverse problems,\ncompatible with existing reconstruction frameworks, and complementary to\nconventional image-domain priors. We provide theoretical guarantees on\nconvergence and reconstruction accuracy when used within plug-and-play methods.\nEmpirical results across diverse sensing matrices demonstrate that NPN priors\nconsistently enhance reconstruction fidelity in various imaging inverse\nproblems, such as compressive sensing, deblurring, super-resolution, computed\ntomography, and magnetic resonance imaging, with plug-and-play methods,\nunrolling networks, deep image prior, and diffusion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5NPN\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7684\u4f4e\u7ef4\u6295\u5f71\u6765\u63d0\u5347\u6210\u50cf\u53cd\u95ee\u9898\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5177\u6709\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u611f\u77e5\u77e9\u9635\u96f6\u7a7a\u95f4\u7684\u4efb\u52a1\u7279\u5b9a\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u96f6\u7a7a\u95f4\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027\u7684\u96f6\u7a7a\u95f4\u6295\u5f71\uff08NPN\uff09\uff0c\u901a\u8fc7\u5728\u96f6\u7a7a\u95f4\u7684\u4f4e\u7ef4\u6295\u5f71\u4e2d\u7ea6\u675f\u89e3\uff0c\u5e76\u8bbe\u8ba1\u611f\u77e5\u77e9\u9635\u7279\u5b9a\u7684\u5148\u9a8c\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u4e86\u6536\u655b\u6027\u548c\u91cd\u5efa\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aNPN\u5728\u5404\u79cd\u6210\u50cf\u53cd\u95ee\u9898\u4e2d\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "NPN\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u4e92\u8865\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u53cd\u95ee\u9898\uff0c\u5e76\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2510.01675", "pdf": "https://arxiv.org/pdf/2510.01675", "abs": "https://arxiv.org/abs/2510.01675", "authors": ["Jaewoo Lee", "Dongjae Lee", "Jinwoo Lee", "Hyungyu Lee", "Yeonjoon Kim", "H. Jin Kim"], "title": "Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This work presents a geometric backstepping controller for a variable-tilt\nomnidirectional multirotor that explicitly accounts for both servo and rotor\ndynamics. Considering actuator dynamics is essential for more effective and\nreliable operation, particularly during aggressive flight maneuvers or recovery\nfrom sudden disturbances. While prior studies have investigated actuator-aware\ncontrol for conventional and fixed-tilt multirotors, these approaches rely on\nlinear relationships between actuator input and wrench, which cannot capture\nthe nonlinearities induced by variable tilt angles. In this work, we exploit\nthe cascade structure between the rigid-body dynamics of the multirotor and its\nnonlinear actuator dynamics to design the proposed backstepping controller and\nestablish exponential stability of the overall system. Furthermore, we reveal\nparametric uncertainty in the actuator model through experiments, and we\ndemonstrate that the proposed controller remains robust against such\nuncertainty. The controller was compared against a baseline that does not\naccount for actuator dynamics across three experimental scenarios: fast\ntranslational tracking, rapid rotational tracking, and recovery from sudden\ndisturbance. The proposed method consistently achieved better tracking\nperformance, and notably, while the baseline diverged and crashed during the\nfastest translational trajectory tracking and the recovery experiment, the\nproposed controller maintained stability and successfully completed the tasks,\nthereby demonstrating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u53cd\u6b65\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u53ef\u53d8\u503e\u659c\u5168\u5411\u591a\u65cb\u7ffc\u98de\u884c\u5668\uff0c\u663e\u5f0f\u8003\u8651\u4e86\u4f3a\u670d\u548c\u8f6c\u5b50\u52a8\u529b\u5b66\uff0c\u589e\u5f3a\u4e86\u98de\u884c\u63a7\u5236\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u7ebf\u6027\u6267\u884c\u5668\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u53ef\u53d8\u503e\u659c\u89d2\u5f15\u5165\u7684\u975e\u7ebf\u6027\uff0c\u9650\u5236\u4e86\u63a7\u5236\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6fc0\u70c8\u673a\u52a8\u6216\u6297\u5e72\u6270\u65f6\u3002", "method": "\u5229\u7528\u591a\u65cb\u7ffc\u521a\u4f53\u52a8\u529b\u5b66\u4e0e\u975e\u7ebf\u6027\u6267\u884c\u5668\u52a8\u529b\u5b66\u7684\u7ea7\u8054\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e86\u53cd\u6b65\u63a7\u5236\u5668\uff0c\u5e76\u8bc1\u660e\u4e86\u7cfb\u7edf\u6307\u6570\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63a7\u5236\u5668\u5bf9\u6267\u884c\u5668\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u5feb\u901f\u5e73\u79fb\u8ddf\u8e2a\u3001\u5feb\u901f\u65cb\u8f6c\u8ddf\u8e2a\u548c\u6297\u5e72\u6270\u5b9e\u9a8c\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u5668\u663e\u8457\u63d0\u5347\u4e86\u53ef\u53d8\u503e\u659c\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u63a7\u5236\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.01636", "pdf": "https://arxiv.org/pdf/2510.01636", "abs": "https://arxiv.org/abs/2510.01636", "authors": ["Xingyu Zhou", "Le Liang", "Jing Zhang", "Chao-Kai Wen", "Shi Jin"], "title": "Next-Generation AI-Native Wireless Communications: MCMC-Based Receiver Architectures for Unified Processing", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "7 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "The multiple-input multiple-output (MIMO) receiver processing is a key\ntechnology for current and next-generation wireless communications. However, it\nfaces significant challenges related to complexity and scalability as the\nnumber of antennas increases. Artificial intelligence (AI), a cornerstone of\nnext-generation wireless networks, offers considerable potential for addressing\nthese challenges. This paper proposes an AI-driven, universal MIMO receiver\narchitecture based on Markov chain Monte Carlo (MCMC) techniques. Unlike\nexisting AI-based methods that treat receiver processing as a black box, our\nMCMC-based approach functions as a generic Bayesian computing engine applicable\nto various processing tasks, including channel estimation, symbol detection,\nand channel decoding. This method enhances the interpretability, scalability,\nand flexibility of receivers in diverse scenarios. Furthermore, the proposed\napproach integrates these tasks into a unified probabilistic framework, thereby\nenabling overall performance optimization. This unified framework can also be\nseamlessly combined with data-driven learning methods to facilitate the\ndevelopment of fully intelligent communication receivers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eMCMC\u7684AI\u9a71\u52a8MIMO\u63a5\u6536\u5668\u67b6\u6784\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u7edf\u4e00\u591a\u79cd\u5904\u7406\u4efb\u52a1\u4e8e\u4e00\u4e2a\u6982\u7387\u6846\u67b6\u4e2d\u3002", "motivation": "\u89e3\u51b3MIMO\u63a5\u6536\u5668\u5728\u590d\u6742\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528MCMC\u6280\u672f\u4f5c\u4e3a\u901a\u7528\u8d1d\u53f6\u65af\u8ba1\u7b97\u5f15\u64ce\uff0c\u96c6\u6210\u4fe1\u9053\u4f30\u8ba1\u3001\u7b26\u53f7\u68c0\u6d4b\u548c\u4fe1\u9053\u89e3\u7801\u7b49\u4efb\u52a1\u3002", "result": "\u589e\u5f3a\u4e86\u63a5\u6536\u5668\u7684\u6027\u80fd\uff0c\u5e76\u652f\u6301\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u65b9\u6cd5\u7684\u7ed3\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u901a\u4fe1\u63a5\u6536\u5668\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6982\u7387\u6846\u67b6\u3002"}}
{"id": "2510.01708", "pdf": "https://arxiv.org/pdf/2510.01708", "abs": "https://arxiv.org/abs/2510.01708", "authors": ["Zixing Lei", "Zibo Zhou", "Sheng Yin", "Yueru Chen", "Qingyao Xu", "Weixin Li", "Yunhong Wang", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.", "AI": {"tldr": "PolySim\u901a\u8fc7\u5728\u591a\u4eff\u771f\u5668\u4e2d\u8054\u5408\u8bad\u7ec3\u964d\u4f4e\u4eff\u771f\u5668\u5f52\u7eb3\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u63a7\u5236\u7b56\u7565\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u5668\u5f52\u7eb3\u504f\u5dee\u5bfc\u81f4\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u5728\u591a\u4eff\u771f\u5668\u5e73\u53f0\u4e0a\u5e76\u884c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u52a8\u6001\u7ea7\u522b\u7684\u57df\u968f\u673a\u5316\u3002", "result": "\u5728\u4eff\u771f\u5230\u4eff\u771f\u8bc4\u4f30\u4e2d\u51cf\u5c11\u52a8\u4f5c\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5728MuJoCo\u4e0a\u6210\u529f\u7387\u63d0\u534752.8%\uff1b\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u90e8\u7f72\u3002", "conclusion": "PolySim\u80fd\u6709\u6548\u964d\u4f4e\u4eff\u771f\u5668\u504f\u5dee\uff0c\u63d0\u5347\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2510.01914", "pdf": "https://arxiv.org/pdf/2510.01914", "abs": "https://arxiv.org/abs/2510.01914", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Yen-Ting Liu"], "title": "Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP", "68T07, 68U10", "I.4.8; I.2.10"], "comment": "12 pages, 16 figures, 7 tables, and published in IEEE Sensors Journal", "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u50cf\u751f\u6210\u7684\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u53cc\u5217\u76f4\u63d2\u5c01\u88c5\uff08DIP\uff09\u7684\u7f3a\u9677\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u90e8\u4ef6\u7684\u7f3a\u9677\u68c0\u6d4b\u8017\u65f6\u8017\u529b\uff0c\u7ed9\u8d28\u68c0\u4eba\u5458\u5e26\u6765\u8d1f\u62c5\u4e14\u96be\u4ee5\u7ba1\u7406\u4ea7\u54c1\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6570\u5b57\u76f8\u673a\u5149\u5b66\u6280\u672f\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7ed3\u5408ConSinGAN\u751f\u6210\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cdYOLO\u6a21\u578b\uff08v3\u3001v4\u3001v7\u3001v9\uff09\u3002", "result": "YOLOv7\u7ed3\u5408ConSinGAN\u5728\u7cbe\u5ea6\uff0895.50%\uff09\u3001\u68c0\u6d4b\u65f6\u95f4\uff08285 ms\uff09\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u8fdc\u8d85\u4f20\u7edf\u9608\u503c\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u6269\u5c55\u7684\u81ea\u52a8\u5316\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7f3a\u9677\u7c7b\u578b\u6216\u6570\u636e\u4e0d\u8db3\u7684\u573a\u666f\u3002"}}
{"id": "2510.01711", "pdf": "https://arxiv.org/pdf/2510.01711", "abs": "https://arxiv.org/abs/2510.01711", "authors": ["Taeyoung Kim", "Jimin Lee", "Myungkyu Koo", "Dongyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Younggyo Seo", "Jinwoo Shin"], "title": "Contrastive Representation Regularization for Vision-Language-Action Models", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 12 figures", "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.", "AI": {"tldr": "RS-CL\u901a\u8fc7\u673a\u5668\u4eba\u72b6\u6001\u611f\u77e5\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u4f18\u5316\u4e86VLA\u6a21\u578b\u7684\u8868\u73b0\uff0c\u63d0\u5347\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u673a\u5668\u4eba\u4fe1\u53f7\uff08\u5982\u63a7\u5236\u52a8\u4f5c\u548c\u672c\u4f53\u611f\u77e5\u72b6\u6001\uff09\u7684\u654f\u611f\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86Robot State-aware Contrastive Loss (RS-CL)\uff0c\u5229\u7528\u673a\u5668\u4eba\u72b6\u6001\u7684\u76f8\u5bf9\u8ddd\u79bb\u4f5c\u4e3a\u8f6f\u76d1\u7763\uff0c\u589e\u5f3a\u4e86VLA\u6a21\u578b\u7684\u63a7\u5236\u76f8\u5173\u8868\u793a\u5b66\u4e60\u3002", "result": "RS-CL\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u64cd\u63a7\u6027\u80fd\uff0c\u5728RoboCasa-Kitchen\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4ece30.8%\u63d0\u9ad8\u523041.5%\uff0c\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4ece45.0%\u63d0\u9ad8\u523058.3%\u3002", "conclusion": "RS-CL\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.02048", "pdf": "https://arxiv.org/pdf/2510.02048", "abs": "https://arxiv.org/abs/2510.02048", "authors": ["Xinyang Li", "Vlad C. Andrei", "Peter J. Gu", "Yiqi Chen", "Ullrich J. M\u00f6nich", "Holger Boche"], "title": "Variational Secret Common Randomness Extraction", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "This paper studies the problem of extracting common randomness (CR) or secret\nkeys from correlated random sources observed by two legitimate parties, Alice\nand Bob, through public discussion in the presence of an eavesdropper, Eve. We\npropose a practical two-stage CR extraction framework. In the first stage, the\nvariational probabilistic quantization (VPQ) step is introduced, where Alice\nand Bob employ probabilistic neural network (NN) encoders to map their\nobservations into discrete, nearly uniform random variables (RVs) with high\nagreement probability while minimizing information leakage to Eve. This is\nrealized through a variational learning objective combined with adversarial\ntraining. In the second stage, a secure sketch using code-offset construction\nreconciles the encoder outputs into identical secret keys, whose secrecy is\nguaranteed by the VPQ objective. As a representative application, we study\nphysical layer key (PLK) generation. Beyond the traditional methods, which rely\non the channel reciprocity principle and require two-way channel probing, thus\nsuffering from large protocol overhead and being unsuitable in high mobility\nscenarios, we propose a sensing-based PLK generation method for integrated\nsensing and communications (ISAC) systems, where paired range-angle (RA) maps\nmeasured at Alice and Bob serve as correlated sources. The idea is verified\nthrough both end-to-end simulations and real-world software-defined radio (SDR)\nmeasurements, including scenarios where Eve has partial knowledge about Bob's\nposition. The results demonstrate the feasibility and convincing performance of\nboth the proposed CR extraction framework and sensing-based PLK generation\nmethod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u5171\u540c\u968f\u673a\u6027\u63d0\u53d6\u6846\u67b6\uff0c\u7528\u4e8e\u4eceAlice\u548cBob\u7684\u76f8\u5173\u968f\u673a\u6e90\u4e2d\u63d0\u53d6\u79d8\u5bc6\u5bc6\u94a5\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u8ba8\u8bba\u5728\u7a83\u542c\u8005Eve\u7684\u5b58\u5728\u4e0b\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u534f\u8bae\u5f00\u9500\u5927\u548c\u4e0d\u9002\u7528\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u611f\u77e5\u7684\u7269\u7406\u5c42\u5bc6\u94a5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u53d8\u5206\u6982\u7387\u91cf\u5316\uff08VPQ\uff09\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5b89\u5168\u8349\u56fe\u6280\u672f\u3002\u5e94\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u4e2d\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u91cf\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u7684\u5bc6\u94a5\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u5728\u5171\u540c\u968f\u673a\u6027\u63d0\u53d6\u548c\u5bc6\u94a5\u751f\u6210\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728ISAC\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.01761", "pdf": "https://arxiv.org/pdf/2510.01761", "abs": "https://arxiv.org/abs/2510.01761", "authors": ["Wendu Zhang", "Heng Wang", "Shuangyi Wang", "Yuanrui Huang"], "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 3 figures, under review of ICRA 2026", "summary": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through\ntortuous anatomical channels, yet axially magnetized designs have largely been\nlimited to bending-only motion. To expand deformation capabilities, this paper\npresents a simple assembly that embeds permanent magnets radially within the\ncatheter wall, allowing a single externally steered permanent magnet to\nindependently induce either bending or torsion. A physics-based formulation\ntogether with finite-element analysis establishes the actuation principles, and\nbenchtop experiments validate decoupled mode control under practical fields.\nBuilding on this, a dual-layer blockage mechanism consisting of outer grooves\nand inner plates leverages torsional shear to achieve on-demand drug release.\nFinally, an in-phantom intervention experiment demonstrates end-to-end\noperation: lumen following by bending for target approach, followed by\ntwist-activated release at the site. The resulting compact, cable-free platform\ncombines versatile deformation with precise payload delivery, indicating strong\npotential for next-generation, site-specific therapies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u78c1\u6027\u8fde\u7eed\u673a\u5668\u4eba\uff08MCR\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5f84\u5411\u5d4c\u5165\u6c38\u78c1\u4f53\u5b9e\u73b0\u4e86\u72ec\u7acb\u5f2f\u66f2\u548c\u626d\u8f6c\u7684\u53cc\u6a21\u6001\u63a7\u5236\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9776\u5411\u836f\u7269\u9012\u9001\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u8f74\u5411\u78c1\u5316\u7684MCR\u4e3b\u8981\u5c40\u9650\u4e8e\u5f2f\u66f2\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u89e3\u5256\u901a\u9053\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55MCR\u7684\u53d8\u5f62\u80fd\u529b\uff0c\u901a\u8fc7\u65b0\u578b\u8bbe\u8ba1\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u7cbe\u51c6\u6cbb\u7597\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u7ec4\u88c5\u7ed3\u6784\uff0c\u5c06\u6c38\u78c1\u4f53\u5f84\u5411\u5d4c\u5165\u5bfc\u7ba1\u58c1\uff0c\u5229\u7528\u5916\u90e8\u6c38\u78c1\u4f53\u72ec\u7acb\u8bf1\u5bfc\u5f2f\u66f2\u6216\u626d\u8f6c\u3002\u7ed3\u5408\u7269\u7406\u6a21\u578b\u3001\u6709\u9650\u5143\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5f00\u53d1\u4e86\u53cc\u5c42\u963b\u585e\u673a\u5236\u4ee5\u5b9e\u73b0\u6309\u9700\u836f\u7269\u91ca\u653e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53cc\u6a21\u6001\u7684\u72ec\u7acb\u63a7\u5236\uff0c\u5e76\u5728\u6a21\u578b\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u4ece\u7ba1\u8154\u8ffd\u8e2a\u5230\u9776\u70b9\u91ca\u653e\u7684\u5168\u7a0b\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7d27\u51d1\u3001\u65e0\u7f06\u7684\u5e73\u53f0\u7ed3\u5408\u4e86\u591a\u6837\u5316\u53d8\u5f62\u548c\u7cbe\u51c6\u836f\u7269\u9012\u9001\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u4e0b\u4e00\u4ee3\u9776\u5411\u6cbb\u7597\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02191", "pdf": "https://arxiv.org/pdf/2510.02191", "abs": "https://arxiv.org/abs/2510.02191", "authors": ["Mateus P. Mota", "Mattia Merluzzi", "Emilio Calvanese Strinati"], "title": "Joint Channel and Semantic-aware Grouping for Effective Collaborative Edge Inference", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "Accepted in IEEE SPAWC 2025", "summary": "We focus on collaborative edge inference over wireless, which enables\nmultiple devices to cooperate to improve inference performance in the presence\nof corrupted data. Exploiting a key-query mechanism for selective information\nexchange (or, group formation for collaboration), we recall the effect of\nwireless channel impairments in feature communication. We argue and show that a\ndisjoint approach, which only considers either the semantic relevance or\nchannel state between devices, performs poorly, especially in harsh propagation\nconditions. Based on these findings, we propose a joint approach that takes\ninto account semantic information relevance and channel states when grouping\ndevices for collaboration, by making the general attention weights dependent of\nthe channel information. Numerical simulations show the superiority of the\njoint approach against local inference on corrupted data, as well as compared\nto collaborative inference with disjoint decisions that either consider\napplication or physical layer parameters when forming groups.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u5173\u6027\u548c\u901a\u9053\u72b6\u6001\u8fdb\u884c\u8bbe\u5907\u534f\u4f5c\u63a8\u65ad\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u8bbe\u5907\u95f4\u534f\u4f5c\u63a8\u65ad\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8bed\u4e49\u6216\u901a\u9053\u72b6\u6001\uff0c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5bc6\u94a5\u67e5\u8be2\u673a\u5236\u9009\u62e9\u6027\u4ea4\u6362\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u8054\u5408\u65b9\u6cd5\uff0c\u5c06\u901a\u9053\u4fe1\u606f\u878d\u5165\u6ce8\u610f\u529b\u6743\u91cd\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8054\u5408\u65b9\u6cd5\u5728\u6570\u636e\u635f\u574f\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5c40\u90e8\u63a8\u65ad\u548c\u4f20\u7edf\u534f\u4f5c\u63a8\u65ad\u3002", "conclusion": "\u8054\u5408\u8003\u8651\u8bed\u4e49\u548c\u901a\u9053\u72b6\u6001\u7684\u534f\u4f5c\u65b9\u6cd5\u5728\u6076\u52a3\u4f20\u64ad\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2510.01770", "pdf": "https://arxiv.org/pdf/2510.01770", "abs": "https://arxiv.org/abs/2510.01770", "authors": ["Christopher Leet", "Aidan Sciortino", "Sven Koenig"], "title": "An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory", "categories": ["cs.RO"], "comment": null, "summary": "Modern automated factories increasingly run manufacturing procedures using a\nmatrix of programmable machines, such as 3D printers, interconnected by a\nprogrammable transport system, such as a fleet of tabletop robots. To embed a\nmanufacturing procedure into a smart factory, an operator must: (a) assign each\nof its processes to a machine and (b) specify how agents should transport parts\nbetween machines. The problem of embedding a manufacturing process into a smart\nfactory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art\nSFE solvers can only scale to factories containing a couple dozen machines.\nModern smart factories, however, may contain hundreds of machines. We fill this\nhole by introducing the first highly scalable solution to the SFE, TS-ACES, the\nTraffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is\ncomplete and can scale to SFE instances based on real industrial scenarios with\nmore than a hundred machines.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TS-ACES\uff0c\u4e00\u79cd\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u667a\u80fd\u5de5\u5382\u5d4c\u5165\uff08SFE\uff09\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5de5\u5382\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SFE\u6c42\u89e3\u5668\u65e0\u6cd5\u6269\u5c55\u5230\u6570\u767e\u53f0\u673a\u5668\u7684\u73b0\u4ee3\u667a\u80fd\u5de5\u5382\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTS-ACES\uff08\u57fa\u4e8e\u4ea4\u901a\u7cfb\u7edf\u7684\u968f\u65f6\u5faa\u73af\u5d4c\u5165\u6c42\u89e3\u5668\uff09\u3002", "result": "TS-ACES\u5177\u6709\u5b8c\u5907\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u57fa\u4e8e\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u7684SFE\u5b9e\u4f8b\uff08\u8d85\u8fc7\u767e\u53f0\u673a\u5668\uff09\u3002", "conclusion": "TS-ACES\u586b\u8865\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u5de5\u5382\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.02196", "pdf": "https://arxiv.org/pdf/2510.02196", "abs": "https://arxiv.org/abs/2510.02196", "authors": ["Jason Anderson"], "title": "Authentication Security of PRF GNSS Ranging", "categories": ["cs.CR", "eess.SP"], "comment": null, "summary": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u4f2a\u968f\u673a\u51fd\u6570\uff08PRF\uff09\u7684GNSS\u6d4b\u8ddd\u5728\u591a\u79cd\u6b3a\u9a97\u6a21\u578b\u4e0b\u7684\u8ba4\u8bc1\u5b89\u5168\u6027\uff0c\u5e76\u5e94\u7528\u4e8eGalileo\u7cfb\u7edf\u7684\u4fe1\u53f7\u8ba4\u8bc1\u670d\u52a1\uff08SAS\uff09\u3002", "motivation": "\u7814\u7a76PRF-GNSS\u6d4b\u8ddd\u7684\u5b89\u5168\u6027\u80fd\uff0c\u4ee5\u5bf9\u6297GNSS\u6b3a\u9a97\u653b\u51fb\uff0c\u786e\u4fdd\u63a5\u6536\u673a\u7684\u4f4d\u7f6e\u3001\u5bfc\u822a\u548c\u65f6\u95f4\uff08PNT\uff09\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "method": "\u63a8\u5bfcPRF-GNSS\u6d4b\u8ddd\u7684\u5b89\u5168\u6027\uff0c\u5206\u6790\u4e0d\u540c\u6b3a\u9a97\u6a21\u578b\uff08\u5982SCER\uff09\u4e0b\u7684\u653b\u51fb\u6548\u679c\uff0c\u5e76\u5e94\u7528\u4e8eGalileo E6-C\u4fe1\u53f7\u7684\u8ba4\u8bc1\u670d\u52a1\u3002", "result": "\u5728\u975eSCER\u6a21\u578b\u4e0b\uff0c\u4ec5\u9700400ms\u7684Galileo E6-C\u6570\u636e\u5373\u53ef\u5b9e\u73b0128\u4f4d\u8ba4\u8bc1\u5b89\u5168\u6027\uff1b\u5bf9SCER\u653b\u51fb\u8005\uff0c\u9884\u6d4b\u4e86\u5176\u6240\u9700\u7684\u63a5\u6536\u8bbe\u5907\u3002", "conclusion": "\u8be5\u7814\u7a76\u53ef\u7528\u4e8e\u8bbe\u8ba1\u6ee1\u8db3\u8ba4\u8bc1\u5b89\u5168\u8981\u6c42\u7684PRF-GNSS\u6d4b\u8ddd\u534f\u8bae\uff0c\u5e76\u8ba1\u7b97\u6f0f\u68c0\u6982\u7387\u3002"}}
{"id": "2510.01795", "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "AI": {"tldr": "Nav-EE\u6846\u67b6\u901a\u8fc7\u5bfc\u822a\u5148\u9a8c\u52a8\u6001\u9009\u62e9\u63d0\u524d\u9000\u51fa\u5c42\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "motivation": "\u7531\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5bfc\u822a\u7cfb\u7edf\u53ef\u4ee5\u9884\u77e5\u5373\u5c06\u5230\u6765\u7684\u573a\u666f\uff08\u5982\u4ea4\u53c9\u53e3\u3001\u4ea4\u901a\u706f\uff09\uff0c\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u9884\u89c1\u6027\u4f18\u5316\u65e9\u671f\u9000\u51fa\u7b56\u7565\uff0c\u51cf\u5c11\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86Nav-EE\u6846\u67b6\uff0c\u79bb\u7ebf\u9884\u8ba1\u7b97\u4efb\u52a1\u7279\u5b9a\u9000\u51fa\u5c42\uff0c\u5728\u7ebf\u6839\u636e\u5bfc\u822a\u5148\u9a8c\u52a8\u6001\u5e94\u7528\u8fd9\u4e9b\u5c42\u3002", "result": "\u5728CODA\u3001Waymo\u548cBOSCH\u6570\u636e\u96c6\u4e0a\uff0cNav-EE\u5728\u4e0d\u635f\u5931\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u6700\u9ad8\u51cf\u5c1163.9%\u7684\u5ef6\u8fdf\uff1b\u771f\u5b9e\u8f66\u8f86\u90e8\u7f72\u4e2d\u5c06\u63a8\u7406\u5ef6\u8fdf\u4ece600ms\u964d\u81f3300ms\u3002", "conclusion": "\u7ed3\u5408\u5bfc\u822a\u9884\u89c1\u6027\u548c\u65e9\u671f\u9000\u51fa\u7b56\u7565\uff0c\u4e3a\u5927\u578b\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.02222", "pdf": "https://arxiv.org/pdf/2510.02222", "abs": "https://arxiv.org/abs/2510.02222", "authors": ["Mateus P. Mota", "Mattia Merluzzi", "Emilio Calvanese Strinati"], "title": "Collaborative Edge Inference via Semantic Grouping under Wireless Channel Constraints", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "5 pages, 5 figures. Accepted at 33rd European Signal Processing\n  Conference (EUSIPCO 2025)", "summary": "In this paper, we study the framework of collaborative inference, or edge\nensembles. This framework enables multiple edge devices to improve\nclassification accuracy by exchanging intermediate features rather than raw\nobservations. However, efficient communication strategies are essential to\nbalance accuracy and bandwidth limitations. Building upon a key-query mechanism\nfor selective information exchange, this work extends collaborative inference\nby studying the impact of channel noise in feature communication, the choice of\nintermediate collaboration points, and the communication-accuracy trade-off\nacross tasks. By analyzing how different collaboration points affect\nperformance and exploring communication pruning, we show that it is possible to\noptimize accuracy while minimizing resource usage. We show that the\nintermediate collaboration approach is robust to channel errors and that the\nquery transmission needs a higher degree of reliability than the data\ntransmission itself.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff08\u8fb9\u7f18\u96c6\u6210\uff09\uff0c\u901a\u8fc7\u4ea4\u6362\u4e2d\u95f4\u7279\u5f81\u800c\u975e\u539f\u59cb\u6570\u636e\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u63a2\u8ba8\u4e86\u4fe1\u9053\u566a\u58f0\u3001\u534f\u4f5c\u70b9\u9009\u62e9\u548c\u901a\u4fe1-\u51c6\u786e\u6027\u6743\u8861\uff0c\u8868\u660e\u4e2d\u95f4\u534f\u4f5c\u65b9\u6cd5\u5bf9\u4fe1\u9053\u9519\u8bef\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u67e5\u8be2\u4f20\u8f93\u6bd4\u6570\u636e\u4f20\u8f93\u9700\u8981\u66f4\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u8fb9\u7f18\u8bbe\u5907\u95f4\u9ad8\u6548\u4ea4\u6362\u4e2d\u95f4\u7279\u5f81\u4ee5\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u548c\u4fe1\u9053\u566a\u58f0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8e\u952e\u67e5\u8be2\u673a\u5236\u7684\u9009\u62e9\u6027\u4fe1\u606f\u4ea4\u6362\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u4fe1\u9053\u566a\u58f0\u5bf9\u4e0d\u540c\u534f\u4f5c\u70b9\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u901a\u4fe1\u526a\u679d\u6280\u672f\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u4e2d\u95f4\u534f\u4f5c\u65b9\u6cd5\u5bf9\u4fe1\u9053\u9519\u8bef\u5177\u6709\u9c81\u68d2\u6027\uff0c\u67e5\u8be2\u4f20\u8f93\u9700\u8981\u66f4\u9ad8\u7684\u53ef\u9760\u6027\uff0c\u4f18\u5316\u534f\u4f5c\u70b9\u53ef\u4ee5\u5e73\u8861\u8d44\u6e90\u4f7f\u7528\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u534f\u4f5c\u63a8\u7406\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u901a\u8fc7\u4f18\u5316\u4e2d\u95f4\u7279\u5f81\u4ea4\u6362\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5206\u7c7b\u3002"}}
{"id": "2510.01830", "pdf": "https://arxiv.org/pdf/2510.01830", "abs": "https://arxiv.org/abs/2510.01830", "authors": ["Hongze Wang", "Boyang Sun", "Jiaxu Xing", "Fan Yang", "Marco Hutter", "Dhruv Shah", "Davide Scaramuzza", "Marc Pollefeys"], "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework", "categories": ["cs.RO"], "comment": null, "summary": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying\nmobile robots in everyday, uncontrolled environments such as homes, schools,\nand workplaces. In this context, a robot must locate target objects in\npreviously unseen environments using only its onboard perception. Success\nrequires the integration of semantic understanding, spatial reasoning, and\nlong-horizon planning, which is a combination that remains extremely\nchallenging. While reinforcement learning (RL) has become the dominant\nparadigm, progress has spanned a wide range of design choices, yet the field\nstill lacks a unifying analysis to determine which components truly drive\nperformance. In this work, we conduct a large-scale empirical study of modular\nRL-based ObjectNav systems, decomposing them into three key components:\nperception, policy, and test-time enhancement. Through extensive controlled\nexperiments, we isolate the contribution of each and uncover clear trends:\nperception quality and test-time strategies are decisive drivers of\nperformance, whereas policy improvements with current methods yield only\nmarginal gains. Building on these insights, we propose practical design\nguidelines and demonstrate an enhanced modular system that surpasses\nState-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We\nalso introduce a human baseline under identical conditions, where experts\nachieve an average 98% success, underscoring the gap between RL agents and\nhuman-level navigation. Our study not only sets the SotA performance but also\nprovides principled guidance for future ObjectNav development and evaluation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\uff08\u611f\u77e5\u3001\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff09\uff0c\u53d1\u73b0\u611f\u77e5\u8d28\u91cf\u548c\u6d4b\u8bd5\u65f6\u7b56\u7565\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5357\u5e76\u63d0\u51fa\u4e86\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u7cfb\u7edf\u3002", "motivation": "\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u662f\u79fb\u52a8\u673a\u5668\u4eba\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u5206\u6790\uff0c\u96be\u4ee5\u786e\u5b9a\u54ea\u4e9b\u7ec4\u4ef6\u771f\u6b63\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u7cfb\u7edf\u4e3a\u611f\u77e5\u3001\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u7684\u5bf9\u7167\u5b9e\u9a8c\uff0c\u5206\u6790\u5404\u7ec4\u4ef6\u5bf9\u6027\u80fd\u7684\u8d21\u732e\u3002", "result": "\u611f\u77e5\u8d28\u91cf\u548c\u6d4b\u8bd5\u65f6\u7b56\u7565\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u800c\u7b56\u7565\u6539\u8fdb\u6548\u679c\u6709\u9650\uff1b\u63d0\u51fa\u7684\u65b0\u7cfb\u7edf\u5728SPL\u548c\u6210\u529f\u7387\u4e0a\u5206\u522b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd56.6%\u548c2.7%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u8fd8\u4e3a\u672a\u6765\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.02265", "pdf": "https://arxiv.org/pdf/2510.02265", "abs": "https://arxiv.org/abs/2510.02265", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Kemal Davaslioglu", "Sastry Kompella"], "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "comment": null, "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u52a8\u6001\u9002\u5e94\u53d1\u5c04\u529f\u7387\u3001\u8c03\u5236\u548c\u4fe1\u9053\u9009\u62e9\uff0c\u4ee5\u5e94\u5bf9\u53cd\u5e94\u5f0f\u5e72\u6270\u7684\u7b56\u7565\uff0c\u7ed3\u679c\u8868\u660eRL\u80fd\u5feb\u901f\u9002\u5e94\u9891\u8c31\u52a8\u6001\u5e76\u4fdd\u6301\u9ad8\u4f20\u8f93\u901f\u7387\u3002", "motivation": "\u7531\u4e8e\u5e72\u6270\u8005\u91c7\u7528\u52a8\u6001\u7b56\u7565\u9009\u62e9\u548c\u611f\u77e5\u4fe1\u9053\u9608\u503c\u6765\u5e72\u6270\u4f20\u8f93\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u907f\u514d\u5e72\u6270\u5e76\u4f18\u5316\u541e\u5410\u91cf\u3002", "method": "\u4f7f\u7528Q-learning\u5904\u7406\u79bb\u6563\u7684\u5e72\u6270\u4e8b\u4ef6\u72b6\u6001\uff0cDeep Q-Networks\uff08DQN\uff09\u5904\u7406\u57fa\u4e8e\u63a5\u6536\u529f\u7387\u7684\u8fde\u7eed\u72b6\u6001\u3002", "result": "\u4e0d\u540c\u5956\u52b1\u51fd\u6570\u548c\u52a8\u4f5c\u96c6\u4e0b\uff0cRL\u80fd\u5feb\u901f\u9002\u5e94\u9891\u8c31\u52a8\u6001\uff0c\u5e76\u5728\u4fe1\u9053\u548c\u5e72\u6270\u7b56\u7565\u53d8\u5316\u65f6\u4fdd\u6301\u9ad8\u4f20\u8f93\u901f\u7387\u3002", "conclusion": "RL\u65b9\u6cd5\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u5e72\u6270\uff0c\u9002\u7528\u4e8e\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u7684\u573a\u666f\u3002"}}
{"id": "2510.01843", "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "AI": {"tldr": "\u672c\u6587\u521b\u65b0\u5730\u5c06\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5e94\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8e22\u7403\u4efb\u52a1\u4e2d\u5bf9\u817f\u6446\u52a8\u8fc7\u7a0b\u7684\u7b80\u5316\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u8f68\u8ff9\u751f\u6210\u548c\u65f6\u95f4\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u7684\u4f4d\u7f6e\u63a7\u5236\u65b9\u6cd5\u6216\u5f3a\u5316\u5b66\u4e60\u5728\u53cc\u8db3\u673a\u5668\u4eba\u8e22\u7403\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5bf9\u817f\u6446\u52a8\u8fc7\u7a0b\u7684\u7b80\u5316\u9650\u5236\u4e86\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u81ea\u4e3b\u751f\u6210\u6ee1\u8db3\u76ee\u6807\u8e22\u7403\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u8db3\u90e8\u8f68\u8ff9\uff0c\u5e76\u4f18\u5316\u6446\u52a8\u76f8\u6301\u7eed\u65f6\u95f4\u3002", "result": "\u4f18\u5316\u7684\u8f68\u8ff9\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u8e22\u7403\u52a8\u4f5c\uff08\u5305\u62ec\u540e\u6446\u52a8\u4f5c\uff09\uff0c\u89c4\u5212\u65f6\u95f4\u4f4e\u4e8e1\u6beb\u79d2\uff0c\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\u5728\u76ee\u6807\u8303\u56f4\u5185\u63a5\u8fd1100%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u53ef\u9760\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u8e22\u7403\u4efb\u52a1\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2510.01848", "pdf": "https://arxiv.org/pdf/2510.01848", "abs": "https://arxiv.org/abs/2510.01848", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Simulating greenhouse environments is critical for developing and evaluating\nrobotic systems for agriculture, yet existing approaches rely on simplistic or\nsynthetic assets that limit simulation-to-real transfer. Recent advances in\nradiance field methods, such as Gaussian splatting, enable photorealistic\nreconstruction but have so far been restricted to individual plants or\ncontrolled laboratory conditions. In this work, we introduce GreenhouseSplat, a\nframework and dataset for generating photorealistic greenhouse assets directly\nfrom inexpensive RGB images. The resulting assets are integrated into a\nROS-based simulation with support for camera and LiDAR rendering, enabling\ntasks such as localization with fiducial markers. We provide a dataset of 82\ncucumber plants across multiple row configurations and demonstrate its utility\nfor robotics evaluation. GreenhouseSplat represents the first step toward\ngreenhouse-scale radiance-field simulation and offers a foundation for future\nresearch in agricultural robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGreenhouseSplat\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4f4e\u6210\u672cRGB\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u6e29\u5ba4\u8d44\u4ea7\uff0c\u652f\u6301ROS\u4eff\u771f\uff0c\u9996\u6b21\u5b9e\u73b0\u6e29\u5ba4\u5c3a\u5ea6\u7684\u8f90\u5c04\u573a\u6a21\u62df\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u6e29\u5ba4\u73af\u5883\u6a21\u62df\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u6216\u5408\u6210\u8d44\u4ea7\uff0c\u9650\u5236\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002\u63d0\u51faGreenhouseSplat\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u8f90\u5c04\u573a\u6280\u672f\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u6e29\u5ba4\u6a21\u62df\u3002", "method": "\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u7b49\u8f90\u5c04\u573a\u65b9\u6cd5\uff0c\u4ece\u4f4e\u6210\u672cRGB\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u6e29\u5ba4\u8d44\u4ea7\uff0c\u5e76\u96c6\u6210\u5230ROS\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u652f\u6301\u76f8\u673a\u548cLiDAR\u6e32\u67d3\u3002", "result": "\u63d0\u4f9b\u4e86\u5305\u542b82\u9897\u9ec4\u74dc\u690d\u7269\u7684\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u5b9a\u4f4d\u7b49\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "GreenhouseSplat\u662f\u9996\u4e2a\u6e29\u5ba4\u5c3a\u5ea6\u7684\u8f90\u5c04\u573a\u6a21\u62df\u6846\u67b6\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.01869", "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "AI": {"tldr": "TACOS\u662f\u4e00\u4e2a\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b9e\u73b0\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9ad8\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3001\u667a\u80fd\u534f\u8c03\u5668\u548c\u81ea\u4e3b\u6267\u884c\u4ee3\u7406\uff0c\u51cf\u5c11\u4e86\u98de\u884c\u5458\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u5355\u98de\u884c\u5458\u7ba1\u7406\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u9700\u8981\u7075\u6d3b\u7684\u5171\u4eab\u81ea\u6cbb\u6a21\u5f0f\uff0c\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u8fd9\u4e9b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u81ea\u7136\u7684\u4ea4\u4e92\u57fa\u7840\u3002", "method": "\u63d0\u51faTACOS\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3001\u4efb\u52a1\u8ba1\u5212\u7ffb\u8bd1\u5668\u548c\u81ea\u4e3b\u6267\u884c\u4ee3\u7406\uff0c\u5229\u7528LLM\u4e0e\u53ef\u6267\u884cAPI\u5e93\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86TACOS\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u5404\u6a21\u5757\u7684\u8d21\u732e\u3002", "conclusion": "TACOS\u8bc1\u660e\u4e86LLM\u5728\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5c42\u6b21\u4efb\u52a1\u59d4\u6d3e\u548c\u5b9e\u65f6\u534f\u8c03\u3002"}}
{"id": "2510.01984", "pdf": "https://arxiv.org/pdf/2510.01984", "abs": "https://arxiv.org/abs/2510.01984", "authors": ["Yue Wang"], "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module\nthat combines revolute (pitch) and prismatic (axial) motion with programmable\ntask-space impedance for quadruped robots. The system integrates three\ntorque-controlled actuators, a custom 1 kHz control board, and a protected\npower unit in a 1.26 kg package, enabling closed-loop stiffness and damping\nshaping along x, z, and theta. We develop an RNEA-based computed-acceleration\ncontroller with smooth Stribeck friction compensation to render spring-damper\nbehavior without explicit inertia shaping. Bench experiments validate the\napproach. Quasi-static push-pull tests show linear force-displacement\ncharacteristics with commanded horizontal stiffness spanning 300-700 N/m and <=\n1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic\ndisplace-and-release trials confirm mass-spring-damper responses over multiple\ndamping settings, with small, interpretable phase deviations due to\nconfiguration-dependent inertia and low-speed friction effects. A task-space PD\ncontroller produces roughly linear stiffness but with greater variability and\ncoupling sensitivity. SPARC provides a portable platform for systematic studies\nof spine compliance in legged locomotion and will be released with complete\nhardware and firmware resources.", "AI": {"tldr": "SPARC\u662f\u4e00\u79cd\u7d27\u51d1\u3001\u5f00\u6e90\u76843\u81ea\u7531\u5ea6\u810a\u67f1\u6a21\u5757\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u65cb\u8f6c\u548c\u7ebf\u6027\u8fd0\u52a8\uff0c\u5e76\u5177\u6709\u53ef\u7f16\u7a0b\u7684\u4efb\u52a1\u7a7a\u95f4\u963b\u6297\u3002", "motivation": "\u65e8\u5728\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e00\u79cd\u4fbf\u643a\u5f0f\u5e73\u53f0\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u810a\u67f1\u67d4\u987a\u6027\u5728\u817f\u5f0f locomotion\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u96c6\u6210\u4e86\u4e09\u4e2a\u626d\u77e9\u63a7\u5236\u6267\u884c\u5668\u3001\u81ea\u5b9a\u4e491 kHz\u63a7\u5236\u677f\u548c\u53d7\u4fdd\u62a4\u7684\u7535\u6e90\u5355\u5143\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eRNEA\u7684\u8ba1\u7b97\u52a0\u901f\u5ea6\u63a7\u5236\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6c34\u5e73\u521a\u5ea6\u53ef\u63a7\u5236\u5728300-700 N/m\u8303\u56f4\u5185\uff0c\u8bef\u5dee\u4f4e\u4e8e1.5%\uff0c\u52a8\u6001\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u54cd\u5e94\u3002", "conclusion": "SPARC\u4e3a\u810a\u67f1\u67d4\u987a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u5e73\u53f0\uff0c\u5e76\u5c06\u5f00\u6e90\u786c\u4ef6\u548c\u56fa\u4ef6\u8d44\u6e90\u3002"}}
{"id": "2510.01986", "pdf": "https://arxiv.org/pdf/2510.01986", "abs": "https://arxiv.org/abs/2510.01986", "authors": ["Varun Kotian", "Vishrut Jain", "Andrea Michelle Rios Lazcano", "Daan Marinus Pool", "Riender Happee", "Barys Shyrokau"], "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "math.OC", "q-bio.NC", "I.6"], "comment": null, "summary": "Driving simulators are increasingly used in research and development.\nHowever, simulators often cause motion sickness due to downscaled motion and\nunscaled veridical visuals. In this paper, a motion cueing algorithm is\nproposed that reduces motion sickness as predicted by the subjective vertical\nconflict (SVC) model using model predictive control (MPC). Both sensory\nconflict and specific force errors are penalised in the cost function, allowing\nthe algorithm to jointly optimise fidelity and comfort.\n  Human-in-the-loop experiments were conducted to compare four simulator motion\nsettings: two variations of our MPC-based algorithm, one focused on pure\nspecific force tracking and the second compromising specific force tracking and\nmotion sickness minimisation, as well as reference adaptive washout and no\nmotion cases. The experiments were performed on a hexapod driving simulator\nwith participants exposed to passive driving.\n  Experimental motion sickness results closely matched the sickness model\npredictions. As predicted by the model, the no motion condition yielded the\nlowest sickness levels. However, it was rated lowest in terms of fidelity. The\ncompromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)\ncompared to adaptive washout and the algorithm focusing on specific force\ntracking, without any significant reduction in fidelity rating.\n  The proposed approach for developing MCA that takes into account both the\nsimulator dynamics and time evolution of motion sickness offers a significant\nadvancement in achieving an optimal control of motion sickness and specific\nforce recreation in driving simulators, supporting broader simulator use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMPC\u7684\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6210\u672c\u548c\u51cf\u5c11\u8fd0\u52a8\u75c5\u75c7\u72b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u6a21\u62df\u5668\u7684\u8212\u9002\u5ea6\u548c\u903c\u771f\u5ea6\u3002", "motivation": "\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u7814\u7a76\u548c\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5e38\u56e0\u8fd0\u52a8\u7f29\u653e\u548c\u89c6\u89c9\u4e0d\u5339\u914d\u5bfc\u81f4\u8fd0\u52a8\u75c5\uff0c\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u5e94\u7528\u8303\u56f4\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e3b\u89c2\u5782\u76f4\u51b2\u7a81\uff08SVC\uff09\u6a21\u578b\uff0c\u4f18\u5316\u6210\u672c\u548c\u51cf\u5c11\u8fd0\u52a8\u75c5\u75c7\u72b6\u3002\u901a\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u56db\u79cd\u8fd0\u52a8\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u59a5\u534f\u65b9\u6848\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u52a8\u75c5\uff08\u964d\u4f4e50%\u4ee5\u4e0a\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u903c\u771f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u7684\u8fd0\u52a8\u75c5\u63a7\u5236\u548c\u52a8\u6001\u903c\u771f\u5ea6\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u652f\u6301\u6a21\u62df\u5668\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2510.02080", "pdf": "https://arxiv.org/pdf/2510.02080", "abs": "https://arxiv.org/abs/2510.02080", "authors": ["Lingxiang Hu", "Naima Ait Oufroukh", "Fabien Bonardi", "Raymond Ghandour"], "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "The application of monocular dense Simultaneous Localization and Mapping\n(SLAM) is often hindered by high latency, large GPU memory consumption, and\nreliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,\na novel calibration-free monocular dense SLAM framework that jointly achieves\nhigh localization and mapping accuracy, low latency, and low GPU memory\nconsumption. This enables the framework to achieve efficiency through the\ncoupling of a tracking module, which maintains a sparse map of feature points,\nand a mapping module based on a feed-forward 3D reconstruction model that\nsimultaneously estimates camera intrinsics. In addition, both local and global\nloop closures are incorporated to ensure mid-term and long-term data\nassociation, enforcing multi-view consistency and thereby enhancing the overall\naccuracy and robustness of the system. Experiments across multiple benchmarks\nshow that EC3R-SLAM achieves competitive performance compared to\nstate-of-the-art methods, while being faster and more memory-efficient.\nMoreover, it runs effectively even on resource-constrained platforms such as\nlaptops and Jetson Orin NX, highlighting its potential for real-world robotics\napplications.", "AI": {"tldr": "EC3R-SLAM\u662f\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u7684\u5355\u76ee\u5bc6\u96c6SLAM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8ddf\u8e2a\u6a21\u5757\u548c3D\u91cd\u5efa\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4eGPU\u5185\u5b58\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u76ee\u5bc6\u96c6SLAM\u7684\u9ad8\u5ef6\u8fdf\u3001\u5927\u5185\u5b58\u6d88\u8017\u548c\u4f9d\u8d56\u76f8\u673a\u6821\u51c6\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u7279\u5f81\u70b9\u8ddf\u8e2a\u6a21\u5757\u548c\u57fa\u4e8e\u524d\u99883D\u91cd\u5efa\u7684\u6620\u5c04\u6a21\u5757\uff0c\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\uff0c\u5e76\u5f15\u5165\u5c40\u90e8\u548c\u5168\u5c40\u95ed\u73af\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\u3001\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "EC3R-SLAM\u5177\u6709\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.02104", "pdf": "https://arxiv.org/pdf/2510.02104", "abs": "https://arxiv.org/abs/2510.02104", "authors": ["Yunhan Lin", "Wenqi Wu", "Zhijie Zhang", "Huasong Min"], "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.", "AI": {"tldr": "LangGrasp\u662f\u4e00\u4e2a\u65b0\u578b\u8bed\u8a00\u4ea4\u4e92\u673a\u5668\u4eba\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6765\u63a8\u65ad\u9690\u542b\u610f\u56fe\uff0c\u5e76\u7ed3\u5408\u70b9\u4e91\u5b9a\u4f4d\u6a21\u5757\u5b9e\u73b0\u4ece\u5bf9\u8c61\u7ea7\u522b\u5230\u90e8\u4ef6\u7ea7\u522b\u7684\u7cbe\u7ec6\u6293\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5904\u7406\u5305\u542b\u9690\u542b\u610f\u56fe\u7684\u6a21\u7cca\u6307\u4ee4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63a8\u65ad\u8fd9\u4e9b\u610f\u56fe\u5e76\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u64cd\u4f5c\u7684\u65b9\u6cd5\u3002", "method": "LangGrasp\u6846\u67b6\u6574\u5408\u4e86\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u63a8\u65ad\u9690\u542b\u610f\u56fe\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e2D\u90e8\u4ef6\u5206\u5272\u7684\u70b9\u4e91\u5b9a\u4f4d\u6a21\u5757\uff0c\u652f\u6301\u4ece\u5bf9\u8c61\u5230\u90e8\u4ef6\u7684\u7cbe\u7ec6\u6293\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLangGrasp\u80fd\u51c6\u786e\u89e3\u6790\u6a21\u7cca\u6307\u4ee4\u4e2d\u7684\u9690\u542b\u610f\u56fe\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f73\u6293\u53d6\u59ff\u6001\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "conclusion": "LangGrasp\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u4ea4\u4e92\u548c\u7cbe\u7ec6\u6293\u53d6\u80fd\u529b\u7684\u7ed3\u5408\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02129", "pdf": "https://arxiv.org/pdf/2510.02129", "abs": "https://arxiv.org/abs/2510.02129", "authors": ["Philip Reichenberg", "Tim Laue"], "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control", "categories": ["cs.RO"], "comment": null, "summary": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot\nincapable of standing up by itself is removed from the game for some time. In\nthis paper, we present our stand-up motions for the NAO robot. Our approach\ndates back to 2019 and has been evaluated and slightly expanded over the past\nsix years. We claim that the main reason for failed stand-up attempts are large\nerrors in the executed joint positions. By addressing such problems by either\nexecuting special motions to free up stuck limbs such as the arms, or by\ncompensating large errors with other joints, we significantly increased the\noverall success rate of our stand-up routine. The motions presented in this\npaper are also used by several other teams in the Standard Platform League,\nwhich thereby achieve similar success rates, as shown in an analysis of videos\nfrom multiple tournaments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3aNAO\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u7ad9\u7acb\u52a8\u4f5c\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u88ab\u591a\u4e2a\u56e2\u961f\u91c7\u7528\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u6bd4\u8d5b\u4e2d\uff0c\u673a\u5668\u4eba\u65e0\u6cd5\u81ea\u884c\u7ad9\u7acb\u4f1a\u88ab\u79fb\u9664\u6bd4\u8d5b\uff0c\u56e0\u6b64\u8bbe\u8ba1\u9ad8\u6548\u7684\u7ad9\u7acb\u52a8\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6267\u884c\u7279\u6b8a\u52a8\u4f5c\u91ca\u653e\u5361\u4f4f\u7684\u80a2\u4f53\uff08\u5982\u624b\u81c2\uff09\u6216\u901a\u8fc7\u5176\u4ed6\u5173\u8282\u8865\u507f\u5927\u8bef\u5dee\uff0c\u51cf\u5c11\u4e86\u7ad9\u7acb\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u2014\u2014\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7ad9\u7acb\u52a8\u4f5c\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6bd4\u8d5b\u4e2d\u88ab\u5176\u4ed6\u56e2\u961f\u91c7\u7528\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ad9\u7acb\u52a8\u4f5c\u89e3\u51b3\u4e86\u5173\u8282\u8bef\u5dee\u95ee\u9898\uff0c\u6210\u529f\u7387\u9ad8\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.02164", "pdf": "https://arxiv.org/pdf/2510.02164", "abs": "https://arxiv.org/abs/2510.02164", "authors": ["Nathaniel Hanson", "Austin Allison", "Charles DiMarzio", "Ta\u015fk\u0131n Pad\u0131r", "Kristen L. Dorsey"], "title": "SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics & Automation Letters Special Issue on\n  Interdisciplinarity and Widening Horizons in Soft Robotics", "summary": "We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,\nelectronics-free, fluidically actuated soft manipulator capable of assessing\nthe spectral properties of objects either in hand or through pre-touch caging.\nThis platform offers a wider spectral sensing capability than previous soft\nrobotic counterparts. We perform a material analysis to explore optimal soft\nsubstrates for spectral sensing, and evaluate both pre-touch and in-hand\nperformance. Experiments demonstrate explainable, statistical separation across\ndiverse object classes and sizes (metal, wood, plastic, organic, paper, foam),\nwith large spectral angle differences between items. Through linear\ndiscriminant analysis, we show that sensitivity in the near-infrared\nwavelengths is critical to distinguishing visually similar objects. These\ncapabilities advance the potential of optics as a multi-functional sensory\nmodality for soft robots. The complete parts list, assembly guidelines, and\nprocessing code for the SCANS gripper are accessible at:\nhttps://parses-lab.github.io/scans/.", "AI": {"tldr": "SCANS\u7cfb\u7edf\u662f\u4e00\u79cd\u7535\u5b50\u514d\u9a71\u52a8\u7684\u67d4\u6027\u64cd\u7eb5\u5668\uff0c\u901a\u8fc7\u6d41\u4f53\u9a71\u52a8\u5b9e\u73b0\u5149\u8c31\u4f20\u611f\uff0c\u80fd\u591f\u533a\u5206\u591a\u79cd\u7269\u4f53\u7c7b\u522b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u591a\u529f\u80fd\u3001\u7535\u5b50\u514d\u9a71\u52a8\u7684\u67d4\u6027\u64cd\u7eb5\u5668\uff0c\u4ee5\u63d0\u9ad8\u8f6f\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u5149\u8c31\u7279\u6027\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6750\u6599\u5206\u6790\u9009\u62e9\u6700\u4f73\u67d4\u6027\u57fa\u5e95\uff0c\u8bc4\u4f30\u9884\u89e6\u548c\u63e1\u6301\u6027\u80fd\uff0c\u5229\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790\u786e\u5b9a\u5173\u952e\u6ce2\u957f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSCANS\u80fd\u663e\u8457\u533a\u5206\u591a\u79cd\u7269\u4f53\u7c7b\u522b\uff0c\u7279\u522b\u662f\u5728\u8fd1\u7ea2\u5916\u6ce2\u957f\u4e0b\u7684\u654f\u611f\u6027\u3002", "conclusion": "SCANS\u7cfb\u7edf\u589e\u5f3a\u4e86\u5149\u5b66\u4f5c\u4e3a\u8f6f\u673a\u5668\u4eba\u591a\u529f\u80fd\u611f\u77e5\u6a21\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02167", "pdf": "https://arxiv.org/pdf/2510.02167", "abs": "https://arxiv.org/abs/2510.02167", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication. 6\n  pages, 4 figures", "summary": "In the context of the circular economy, products in their end-of-life phase\nshould be either remanufactured or recycled. Both of these processes are\ncrucial for sustainability and environmental conservation. However,\nmanufacturers often do not support these processes enough by not sharing\nrelevant data. This paper proposes use of a digital twin technology, which is\ncapable to help optimizing the disassembly processes to reduce ecological\nimpact and enhance sustainability. The proposed approach is demonstrated\nthrough a disassembly use-case of the product digital twin of an electric\nvehicle battery. By utilizing product digital twins, challenges associated with\nthe disassembly of electric vehicle batteries can be solved flexibly and\nefficiently for various battery types. As a backbone for the product digital\ntwin representation, the paper uses the paradigm of product-process-resource\nasset networks (PAN). Such networks enable to model relevant relationships\nacross products, production resources, manufacturing processes, and specific\nproduction operations that have to be done in the manufacturing phase of a\nproduct. This paper introduces a Bi-Flow Product-Process-Resource Asset Network\n(Bi-PAN) representation, which extends the PAN paradigm to cover not only the\nmanufacturing, but also the remanufacturing/recycling phase.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4f18\u5316\u4ea7\u54c1\u62c6\u5378\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u751f\u6001\u5f71\u54cd\u5e76\u63d0\u5347\u53ef\u6301\u7eed\u6027\uff0c\u7279\u522b\u662f\u5728\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u7684\u62c6\u5378\u6848\u4f8b\u4e2d\u3002", "motivation": "\u5728\u5faa\u73af\u7ecf\u6d4e\u80cc\u666f\u4e0b\uff0c\u5236\u9020\u5546\u7f3a\u4e4f\u8db3\u591f\u652f\u6301\u4ea7\u54c1\u7684\u518d\u5236\u9020\u6216\u56de\u6536\u8fc7\u7a0b\u7684\u76f8\u5173\u6570\u636e\u5171\u4eab\uff0c\u5f71\u54cd\u4e86\u53ef\u6301\u7eed\u6027\u548c\u73af\u5883\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u53cc\u5411\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08Bi-PAN\uff09\u8868\u793a\u6cd5\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684PAN\u8303\u5f0f\uff0c\u8986\u76d6\u5236\u9020\u548c\u518d\u5236\u9020/\u56de\u6536\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u7684\u6570\u5b57\u5b6a\u751f\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u9ad8\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u7c7b\u578b\u7535\u6c60\u62c6\u5378\u7684\u6311\u6218\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u548cBi-PAN\u8868\u793a\u4e3a\u4f18\u5316\u4ea7\u54c1\u62c6\u5378\u548c\u63d0\u5347\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.02178", "pdf": "https://arxiv.org/pdf/2510.02178", "abs": "https://arxiv.org/abs/2510.02178", "authors": ["Jialin Gao", "Donghao Zhou", "Mingjian Liang", "Lihao Liu", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.", "AI": {"tldr": "DisCo-Layout\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u534f\u8c03\u7269\u7406\u4e0e\u8bed\u4e49\u7ec6\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u8fd1\u671fLLM/VLM\u65b9\u6cd5\u57283D\u5ba4\u5185\u5e03\u5c40\u5408\u6210\u4e2d\u7684\u4e0d\u8db3\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u56e0\u56fa\u5b9a\u6570\u636e\u96c6\u96be\u4ee5\u6cdb\u5316\uff0c\u800c\u73b0\u6709LLM/VLM\u65b9\u6cd5\u867d\u7136\u8bed\u4e49\u4e30\u5bcc\uff0c\u4f46\u7f3a\u4e4f\u7075\u6d3b\u4e14\u5f3a\u5065\u7684\u7ec6\u5316\u80fd\u529b\u3002", "method": "DisCo-Layout\u5305\u542b\u72ec\u7acb\u7ec6\u5316\u7684\u8bed\u4e49\u7ec6\u5316\u5de5\u5177\uff08SRT\uff09\u548c\u7269\u7406\u7ec6\u5316\u5de5\u5177\uff08PRT\uff09\uff0c\u4ee5\u53ca\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u534f\u540c\u5de5\u4f5c\u7684\u89c4\u5212\u3001\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDisCo-Layout\u80fd\u751f\u6210\u903c\u771f\u3001\u4e00\u81f4\u4e14\u6cdb\u5316\u6027\u5f3a\u76843D\u5ba4\u5185\u5e03\u5c40\uff0c\u6027\u80fd\u9886\u5148\u3002", "conclusion": "DisCo-Layout\u901a\u8fc7\u534f\u8c03\u7269\u7406\u4e0e\u8bed\u4e49\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5ba4\u5185\u5e03\u5c40\u5408\u6210\u7684\u8d28\u91cf\u4e0e\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.02248", "pdf": "https://arxiv.org/pdf/2510.02248", "abs": "https://arxiv.org/abs/2510.02248", "authors": ["Yan Miao", "Ege Yuceel", "Georgios Fainekos", "Bardh Hoxha", "Hideki Okamoto", "Sayan Mitra"], "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0", "categories": ["cs.RO"], "comment": null, "summary": "Visual policy design is crucial for aerial navigation. However,\nstate-of-the-art visual policies often overfit to a single track and their\nperformance degrades when track geometry changes. We develop FalconGym 2.0, a\nphotorealistic simulation framework built on Gaussian Splatting (GSplat) with\nan Edit API that programmatically generates diverse static and dynamic tracks\nin milliseconds. Leveraging FalconGym 2.0's editability, we propose a\nPerformance-Guided Refinement (PGR) algorithm, which concentrates visual\npolicy's training on challenging tracks while iteratively improving its\nperformance. Across two case studies (fixed-wing UAVs and quadrotors) with\ndistinct dynamics and environments, we show that a single visual policy trained\nwith PGR in FalconGym 2.0 outperforms state-of-the-art baselines in\ngeneralization and robustness: it generalizes to three unseen tracks with 100%\nsuccess without per-track retraining and maintains higher success rates under\ngate-pose perturbations. Finally, we demonstrate that the visual policy trained\nwith PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a\nquadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30\ntrials spanning two three-gate tracks and a moving-gate track.", "AI": {"tldr": "FalconGym 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u55b7\u6d12\uff08GSplat\uff09\u7684\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7Performance-Guided Refinement\uff08PGR\uff09\u7b97\u6cd5\u8bad\u7ec3\u89c6\u89c9\u7b56\u7565\uff0c\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u548c\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u53ef\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u73b0\u5b9e\u786c\u4ef6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u7b56\u7565\u5728\u822a\u8ff9\u51e0\u4f55\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u9002\u5e94\u591a\u6837\u573a\u666f\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1FalconGym 2.0\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u63d0\u51faPGR\u7b97\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6311\u6218\u6027\u822a\u8ff9\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u79cd\u98de\u884c\u5668\u6848\u4f8b\u4e2d\uff0cPGR\u8bad\u7ec3\u7684\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u6210\u529f\u7387\u9ad8\u8fbe98.6%\u3002", "conclusion": "FalconGym 2.0\u7ed3\u5408PGR\u7b97\u6cd5\uff0c\u4e3a\u89c6\u89c9\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u6cdb\u5316\u65b9\u6848\u3002"}}
{"id": "2510.02252", "pdf": "https://arxiv.org/pdf/2510.02252", "abs": "https://arxiv.org/abs/2510.02252", "authors": ["Joao Pedro Araujo", "Yanjie Ze", "Pei Xu", "Jiajun Wu", "C. Karen Liu"], "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u4f53\u8fd0\u52a8\u91cd\u5b9a\u5411\u8d28\u91cf\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5GMR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4eba\u4f53\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5b9a\u5411\u4e2d\u5f15\u5165\u7684\u7f3a\u9677\u5f71\u54cd\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faGMR\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5f00\u6e90\u53ca\u95ed\u6e90\u91cd\u5b9a\u5411\u5de5\u5177\u5bf9\u6bd4\uff0c\u4f7f\u7528BeyondMimic\u8bad\u7ec3\u7b56\u7565\u3002", "result": "GMR\u5728\u8ddf\u8e2a\u6027\u80fd\u548c\u5bf9\u6e90\u8fd0\u52a8\u7684\u5fe0\u5b9e\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u65b9\u6cd5\uff0c\u63a5\u8fd1\u95ed\u6e90\u57fa\u7ebf\u3002", "conclusion": "GMR\u80fd\u6709\u6548\u51cf\u5c11\u91cd\u5b9a\u5411\u4e2d\u7684\u7f3a\u9677\uff0c\u63d0\u5347\u7b56\u7565\u9c81\u68d2\u6027\u548c\u8ddf\u8e2a\u6548\u679c\u3002"}}
{"id": "2510.02268", "pdf": "https://arxiv.org/pdf/2510.02268", "abs": "https://arxiv.org/abs/2510.02268", "authors": ["Tianchong Jiang", "Jingtian Ji", "Xiangshan Tan", "Jiading Fang", "Anand Bhattad", "Vitor Guizilini", "Matthew R. Walter"], "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning", "categories": ["cs.RO", "cs.CV"], "comment": "Code and project materials are available at\n  ripl.github.io/know_your_camera", "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u7b56\u7565\u4e0e\u76f8\u673a\u5916\u53c2\u5173\u8054\u6765\u5b9e\u73b0\u89c6\u89d2\u4e0d\u53d8\u7684\u6a21\u4eff\u5b66\u4e60\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u514b\u9686\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u653f\u7b56\u5728\u9762\u5bf9\u89c6\u89d2\u53d8\u5316\u65f6\u4f9d\u8d56\u9759\u6001\u80cc\u666f\u89c6\u89c9\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u5c06\u7b56\u7565\u4e0e\u76f8\u673a\u5916\u53c2\u5173\u8054\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Plucker\u5d4c\u5165\u6280\u672f\u5c06\u6bcf\u4e2a\u50cf\u7d20\u7684\u5149\u7ebf\u4e0e\u76f8\u673a\u5916\u53c2\u5173\u8054\uff0c\u5e76\u5728\u591a\u4e2a\u6807\u51c6\u884c\u4e3a\u514b\u9686\u7b56\u7565\uff08\u5982ACT\u3001Diffusion Policy\u548cSmolVLA\uff09\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672a\u5173\u8054\u76f8\u673a\u5916\u53c2\u7684\u7b56\u7565\u5728\u9762\u5bf9\u89c6\u89d2\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5173\u8054\u540e\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u663e\u5f0f\u5173\u8054\u76f8\u673a\u5916\u53c2\u662f\u4e00\u79cd\u6709\u6548\u7684RGB\u63a7\u5236\u65b9\u6cd5\uff0c\u65e0\u9700\u6df1\u5ea6\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0\u9c81\u68d2\u7684\u89c6\u89d2\u4e0d\u53d8\u6a21\u4eff\u5b66\u4e60\u3002"}}
{"id": "2510.02298", "pdf": "https://arxiv.org/pdf/2510.02298", "abs": "https://arxiv.org/abs/2510.02298", "authors": ["Wenye Yu", "Jun Lv", "Zixi Ying", "Yang Jin", "Chuan Wen", "Cewu Lu"], "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ARMADA\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u7c7b\u53c2\u4e0e\u7684\u5171\u4eab\u63a7\u5236\u548c\u81ea\u4e3b\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5FLOAT\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5e76\u9ad8\u6548\u83b7\u53d6\u9886\u57df\u6570\u636e\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u7684\u90e8\u7f72\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5904\u7406\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9884\u8bad\u7ec3\u7b56\u7565\u5728\u7f3a\u4e4f\u8db3\u591f\u9886\u57df\u6570\u636e\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u4eba\u5de5\u6536\u96c6\u7684\u6570\u636e\u8d39\u65f6\u4e14\u8d28\u91cf\u4e0d\u4e00\u3002\u4eba\u7c7b\u53c2\u4e0e\u7684\u7cfb\u7edf\u867d\u80fd\u63d0\u4f9b\u9886\u57df\u6570\u636e\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5168\u7a0b\u76d1\u7763\u3002", "method": "\u8bbe\u8ba1\u4e86ARMADA\u7cfb\u7edf\uff0c\u91c7\u7528\u591a\u673a\u5668\u4eba\u90e8\u7f72\u548c\u4eba\u7c7b\u53c2\u4e0e\u7684\u5171\u4eab\u63a7\u5236\uff0c\u5f15\u5165FLOAT\u65b9\u6cd5\u81ea\u4e3b\u68c0\u6d4b\u5728\u7ebf\u6545\u969c\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8bf7\u6c42\u4eba\u7c7b\u5e72\u9884\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cFLOAT\u7684\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523095%\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd520%\u4ee5\u4e0a\uff1bARMADA\u7684\u6210\u529f\u7387\u63d0\u53474\u500d\u4ee5\u4e0a\uff0c\u4eba\u7c7b\u5e72\u9884\u7387\u964d\u4f4e2\u500d\u4ee5\u4e0a\u3002", "conclusion": "ARMADA\u901a\u8fc7FLOAT\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u636e\u83b7\u53d6\u548c\u7b56\u7565\u90e8\u7f72\uff0c\u51cf\u5c11\u4e86\u4eba\u7c7b\u76d1\u7763\u9700\u6c42\uff0c\u4e3a\u65b0\u573a\u666f\u7684\u5feb\u901f\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.01192", "pdf": "https://arxiv.org/pdf/2510.01192", "abs": "https://arxiv.org/abs/2510.01192", "authors": ["Isabel Pedersen", "Andrea Slane"], "title": "Better Than \"Better Than Nothing\": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults", "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": "26 pages, 6 figures, version submitted to journal", "summary": "The paper asserts that emulating empathy in human-robot interaction is a key\ncomponent to achieve satisfying social, trustworthy, and ethical robot\ninteraction with older people. Following comments from older adult study\nparticipants, the paper identifies a gap. Despite the acceptance of robot care\nscenarios, participants expressed the poor quality of the social aspect.\nCurrent human-robot designs, to a certain extent, neglect to include empathy as\na theorized design pathway. Using rhetorical theory, this paper defines the\nsocio-cultural expectations for convincing empathetic relationships. It\nanalyzes and then summarizes how society understands, values, and negotiates\nempathic interaction between human companions in discursive exchanges, wherein\nempathy acts as a societal value system. Using two public research collections\non robots, with one geared specifically to gerontechnology for older people, it\nsubstantiates the lack of attention to empathy in public materials produced by\nrobot companies. This paper contends that using an empathetic care vocabulary\nas a design pathway is a productive underlying foundation for designing\nhumanoid social robots that aim to support older people's goals of\naging-in-place. It argues that the integration of affective AI into the\nsociotechnical assemblages of human-socially assistive robot interaction ought\nto be scrutinized to ensure it is based on genuine cultural values involving\nempathetic qualities.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u6a21\u62df\u5171\u60c5\u662f\u5b9e\u73b0\u8001\u5e74\u4eba\u6ee1\u610f\u3001\u53ef\u4fe1\u548c\u4f26\u7406\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u5173\u952e\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u8bbe\u8ba1\u7f3a\u4e4f\u5171\u60c5\uff0c\u63d0\u51fa\u5e94\u5c06\u5171\u60c5\u8bcd\u6c47\u4f5c\u4e3a\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u8001\u5e74\u4eba\u62a4\u7406\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7f3a\u4e4f\u5171\u60c5\u7684\u7a7a\u767d\uff0c\u4ee5\u63d0\u5347\u793e\u4f1a\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4fee\u8f9e\u7406\u8bba\u5206\u6790\u793e\u4f1a\u6587\u5316\u5bf9\u5171\u60c5\u7684\u671f\u671b\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u673a\u5668\u4eba\u7814\u7a76\u6570\u636e\u5e93\u9a8c\u8bc1\u8bbe\u8ba1\u4e2d\u7684\u5171\u60c5\u7f3a\u5931\u3002", "result": "\u53d1\u73b0\u673a\u5668\u4eba\u516c\u53f8\u7684\u516c\u5171\u6750\u6599\u4e2d\u5ffd\u89c6\u5171\u60c5\uff0c\u63d0\u51fa\u5171\u60c5\u8bcd\u6c47\u4f5c\u4e3a\u8bbe\u8ba1\u8def\u5f84\u3002", "conclusion": "\u5f3a\u8c03\u9700\u57fa\u4e8e\u771f\u5b9e\u6587\u5316\u4ef7\u503c\u89c2\u8bbe\u8ba1\u5171\u60c5\u578b\u673a\u5668\u4eba\uff0c\u786e\u4fdd\u60c5\u611fAI\u7684\u6574\u5408\u7b26\u5408\u4f26\u7406\u548c\u793e\u4f1a\u9700\u6c42\u3002"}}
{"id": "2510.01264", "pdf": "https://arxiv.org/pdf/2510.01264", "abs": "https://arxiv.org/abs/2510.01264", "authors": ["Isaac Peterson", "Christopher Allred", "Jacob Morrey", "Mario Harper"], "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab", "categories": ["cs.LG", "cs.RO", "68T42 (Primary) 68T40, 68T05 (Secondary)"], "comment": "8 page, 9 figures, code https://github.com/DIRECTLab/IsaacLab-HARL", "summary": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems\ncooperating in dynamic environments. While prior work has focused on these\ncollaborative settings, adversarial interactions are equally critical for\nreal-world applications such as pursuit-evasion, security, and competitive\nmanipulation. In this work, we extend the IsaacLab framework to support\nscalable training of adversarial policies in high-fidelity physics simulations.\nWe introduce a suite of adversarial MARL environments featuring heterogeneous\nagents with asymmetric goals and capabilities. Our platform integrates a\ncompetitive variant of Heterogeneous Agent Reinforcement Learning with Proximal\nPolicy Optimization (HAPPO), enabling efficient training and evaluation under\nadversarial dynamics. Experiments across several benchmark scenarios\ndemonstrate the framework's ability to model and train robust policies for\nmorphologically diverse multi-agent competition while maintaining high\nthroughput and simulation realism. Code and benchmarks are available at:\nhttps://github.com/DIRECTLab/IsaacLab-HARL .", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86IsaacLab\u6846\u67b6\uff0c\u652f\u6301\u5728\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u4e2d\u8bad\u7ec3\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u5f02\u6784\u667a\u80fd\u4f53\u7684\u5bf9\u6297\u6027MARL\u73af\u5883\u5957\u4ef6\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5728\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u8ffd\u6355-\u9003\u907f\u3001\u5b89\u5168\u548c\u7ade\u4e89\u6027\u64cd\u4f5c\uff09\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u6269\u5c55IsaacLab\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u4e00\u79cd\u7ade\u4e89\u6027\u5f02\u6784\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08HAPPO\uff09\uff0c\u652f\u6301\u9ad8\u6548\u8bad\u7ec3\u548c\u8bc4\u4f30\u5bf9\u6297\u6027\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u591a\u6837\u5f62\u6001\u7684\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u548c\u6a21\u62df\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5bf9\u6297\u6027MARL\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2510.01489", "pdf": "https://arxiv.org/pdf/2510.01489", "abs": "https://arxiv.org/abs/2510.01489", "authors": ["Xinyuan Liang", "Longhao Qian", "Yi Lok Lo", "Hugh H. T. Liu"], "title": "A Robust Neural Control Design for Multi-drone Slung Payload Manipulation with Control Contraction Metrics", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "Submit to the 2026 American Control Conference (ACC)", "summary": "This paper presents a robust neural control design for a three-drone slung\npayload transportation system to track a reference path under external\ndisturbances. The control contraction metric (CCM) is used to generate a neural\nexponentially converging baseline controller while complying with control input\nsaturation constraints. We also incorporate the uncertainty and disturbance\nestimator (UDE) technique to dynamically compensate for persistent\ndisturbances. The proposed framework yields a modularized design, allowing the\ncontroller and estimator to perform their individual tasks and achieve a zero\ntrajectory tracking error if the disturbances meet certain assumptions. The\nstability and robustness of the complete system, incorporating both the CCM\ncontroller and the UDE compensator, are presented. Simulations are conducted to\ndemonstrate the capability of the proposed control design to follow complicated\ntrajectories under external disturbances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e09\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\u7684\u9c81\u68d2\u795e\u7ecf\u63a7\u5236\u8bbe\u8ba1\uff0c\u4ee5\u5728\u5916\u90e8\u5e72\u6270\u4e0b\u8ddf\u8e2a\u53c2\u8003\u8def\u5f84\u3002\u7ed3\u5408\u63a7\u5236\u6536\u7f29\u5ea6\u91cf\uff08CCM\uff09\u548c\u4e0d\u786e\u5b9a\u6027\u53ca\u5e72\u6270\u4f30\u8ba1\u5668\uff08UDE\uff09\uff0c\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u786e\u4fdd\u4e86\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u5728\u5916\u90e8\u5e72\u6270\u4e0b\u7a33\u5b9a\u8ddf\u8e2a\u590d\u6742\u8def\u5f84\u7684\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u63a7\u5236\u6536\u7f29\u5ea6\u91cf\uff08CCM\uff09\u751f\u6210\u57fa\u7ebf\u63a7\u5236\u5668\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u53ca\u5e72\u6270\u4f30\u8ba1\u5668\uff08UDE\uff09\u52a8\u6001\u8865\u507f\u6301\u7eed\u5e72\u6270\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u63a7\u5236\u8bbe\u8ba1\u80fd\u591f\u5728\u5916\u90e8\u5e72\u6270\u4e0b\u8ddf\u8e2a\u590d\u6742\u8f68\u8ff9\uff0c\u5b9e\u73b0\u96f6\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "\u7ed3\u5408CCM\u548cUDE\u7684\u6a21\u5757\u5316\u63a7\u5236\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.01531", "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InfoSeeker\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fe1\u606f\u5bfb\u6c42\u4e0e\u4efb\u52a1\u89c4\u5212\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u5185\u90e8\u52a8\u6001\u4e0e\u5b9e\u9645\u73af\u5883\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u5728\u4fe1\u606f\u4e0d\u5b8c\u5168\u548c\u52a8\u6001\u566a\u58f0\u7684\u73af\u5883\u4e2d\u89e3\u51b3\u95ee\u9898\u65f6\uff0c\u4f1a\u4e3b\u52a8\u5bfb\u6c42\u4fe1\u606f\u4ee5\u66f4\u65b0\u8ba4\u77e5\u5e76\u6307\u5bfc\u51b3\u7b56\u3002\u7136\u800c\u73b0\u6709\u7684LLM\u89c4\u5212\u4ee3\u7406\u5e38\u5ffd\u7565\u5185\u90e8\u52a8\u6001\u4e0e\u5b9e\u9645\u73af\u5883\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u51b3\u7b56\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faInfoSeeker\u6846\u67b6\uff0c\u8ba9LLM\u4e3b\u52a8\u89c4\u5212\u884c\u52a8\u4ee5\u9a8c\u8bc1\u7406\u89e3\u3001\u68c0\u6d4b\u73af\u5883\u53d8\u5316\u6216\u6d4b\u8bd5\u5047\u8bbe\uff0c\u4ece\u800c\u751f\u6210\u6216\u4fee\u8ba2\u4efb\u52a1\u5bfc\u5411\u7684\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cInfoSeeker\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u6027\u80fd\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u534774%\uff0c\u4e14\u5728\u4e0d\u540cLLM\u548c\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u7d27\u5bc6\u7ed3\u5408\u89c4\u5212\u548c\u4fe1\u606f\u5bfb\u6c42\u7684\u91cd\u8981\u6027\uff0cInfoSeeker\u4e3a\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2510.01545", "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPredictive Preference Learning from Human Interventions (PPL)\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u5e72\u9884\u4fe1\u53f7\u6269\u5c55\u5230\u672a\u6765\u72b6\u6001\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4fee\u6b63\u5f53\u524d\u72b6\u6001\u7684\u884c\u4e3a\uff0c\u672a\u8003\u8651\u672a\u6765\u72b6\u6001\u53ef\u80fd\u7684\u98ce\u9669\u3002PPL\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u5e72\u9884\u7684\u9690\u542b\u504f\u597d\u4fe1\u53f7\uff0c\u4f18\u5316\u672a\u6765\u72b6\u6001\u7684\u884c\u4e3a\u3002", "method": "PPL\u5c06\u6bcf\u6b21\u4eba\u7c7b\u5e72\u9884\u6269\u5c55\u5230L\u4e2a\u672a\u6765\u65f6\u95f4\u6b65\uff08\u504f\u597d\u8303\u56f4\uff09\uff0c\u5047\u8bbe\u4ee3\u7406\u5728\u8fd9\u4e9b\u6b65\u9aa4\u4e2d\u91c7\u53d6\u76f8\u540c\u884c\u4e3a\u5e76\u63a5\u53d7\u76f8\u540c\u5e72\u9884\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u672a\u6765\u72b6\u6001\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePPL\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u663e\u8457\u51cf\u5c11\u6240\u9700\u7684\u4eba\u7c7b\u6f14\u793a\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684\u504f\u597d\u8303\u56f4\u80fd\u5e73\u8861\u98ce\u9669\u72b6\u6001\u8986\u76d6\u548c\u6807\u7b7e\u6b63\u786e\u6027\u3002", "conclusion": "PPL\u901a\u8fc7\u6269\u5c55\u4eba\u7c7b\u5e72\u9884\u5230\u672a\u6765\u72b6\u6001\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.01623", "pdf": "https://arxiv.org/pdf/2510.01623", "abs": "https://arxiv.org/abs/2510.01623", "authors": ["Angen Ye", "Zeyu Zhang", "Boyuan Wang", "Xiaofeng Wang", "Dapeng Zhang", "Zheng Zhu"], "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.", "AI": {"tldr": "VLA-R1\u662f\u4e00\u79cd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684VLA\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408RLVR\u548cGRPO\u4f18\u5316\u63a8\u7406\u548c\u6267\u884c\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6VLA-CoT-13K\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u8868\u73b0\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8bad\u7ec3\u65b9\u6cd5\u5ffd\u89c6\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u51e0\u4f55\u5173\u7cfb\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5176\u5728\u5d4c\u5165\u5f0fAI\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86VLA-R1\u6a21\u578b\uff0c\u6574\u5408\u4e86RLVR\u548cGRPO\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8eRLVR\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6VLA-CoT-13K\u7528\u4e8e\u94fe\u5f0f\u601d\u7ef4\u76d1\u7763\u3002", "result": "VLA-R1\u5728\u57df\u5185\u3001\u57df\u5916\u3001\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "VLA-R1\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\u63a8\u7406\u548c\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u4e3a\u5d4c\u5165\u5f0fAI\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01665", "pdf": "https://arxiv.org/pdf/2510.01665", "abs": "https://arxiv.org/abs/2510.01665", "authors": ["Yongbo Chen", "Yanhao Zhang", "Shaifali Parashar", "Liang Zhao", "Shoudong Huang"], "title": "Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing\nthe mapping challenges in monocular visual deformable simultaneous localization\nand mapping (SLAM), has attracted growing attention. We introduce a novel\nmethod, called Con-NRSfM, for NRSfM under conformal deformations, encompassing\nisometric deformations as a subset. Our approach performs point-wise\nreconstruction using 2D selected image warps optimized through a graph-based\nframework. Unlike existing methods that rely on strict assumptions, such as\nlocally planar surfaces or locally linear deformations, and fail to recover the\nconformal scale, our method eliminates these constraints and accurately\ncomputes the local conformal scale. Additionally, our framework decouples\nconstraints on depth and conformal scale, which are inseparable in other\napproaches, enabling more precise depth estimation. To address the sensitivity\nof the formulated problem, we employ a parallel separable iterative\noptimization strategy. Furthermore, a self-supervised learning framework,\nutilizing an encoder-decoder network, is incorporated to generate dense 3D\npoint clouds with texture. Simulation and experimental results using both\nsynthetic and real datasets demonstrate that our method surpasses existing\napproaches in terms of reconstruction accuracy and robustness. The code for the\nproposed method will be made publicly available on the project website:\nhttps://sites.google.com/view/con-nrsfm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCon-NRSfM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5171\u5f62\u53d8\u5f62\u4e0b\u7684\u975e\u521a\u6027\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08NRSfM\uff09\uff0c\u4f18\u5316\u4e86\u56fe\u50cf\u53d8\u5f62\u548c\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u89c6\u89c9\u53ef\u53d8\u5f62SLAM\u4e2d\u7684\u6620\u5c04\u6311\u6218\uff0c\u7279\u522b\u662f\u73b0\u6709\u65b9\u6cd5\u5728\u5171\u5f62\u5c3a\u5ea6\u6062\u590d\u548c\u6df1\u5ea6\u4f30\u8ba1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\u4f18\u53162D\u56fe\u50cf\u53d8\u5f62\uff0c\u89e3\u8026\u6df1\u5ea6\u548c\u5171\u5f62\u5c3a\u5ea6\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u5e76\u884c\u53ef\u5206\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u53ca\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u751f\u6210\u5bc6\u96c63D\u70b9\u4e91\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Con-NRSfM\u4e0d\u4ec5\u6d88\u9664\u4e86\u4e25\u683c\u5047\u8bbe\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u5171\u5f62\u5c3a\u5ea6\u548c\u6df1\u5ea6\u7684\u7cbe\u786e\u8ba1\u7b97\uff0c\u4e3aNRSfM\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02087", "pdf": "https://arxiv.org/pdf/2510.02087", "abs": "https://arxiv.org/abs/2510.02087", "authors": ["Shivam Bajpai", "Abhinav Sinha", "Shashi Ranjan Kumar"], "title": "Cooperative Guidance for Aerial Defense in Multiagent Systems", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.DS"], "comment": null, "summary": "This paper addresses a critical aerial defense challenge in contested\nairspace, involving three autonomous aerial vehicles -- a hostile drone (the\npursuer), a high-value drone (the evader), and a protective drone (the\ndefender). We present a cooperative guidance framework for the evader-defender\nteam that guarantees interception of the pursuer before it can capture the\nevader, even under highly dynamic and uncertain engagement conditions. Unlike\ntraditional heuristic, optimal control, or differential game-based methods, we\napproach the problem within a time-constrained guidance framework, leveraging\ntrue proportional navigation based approach that ensures robust and guaranteed\nsolutions to the aerial defense problem. The proposed strategy is\ncomputationally lightweight, scalable to a large number of agent\nconfigurations, and does not require knowledge of the pursuer's strategy or\ncontrol laws. From arbitrary initial geometries, our method guarantees that key\nengagement errors are driven to zero within a fixed time, leading to a\nsuccessful mission. Extensive simulations across diverse and adversarial\nscenarios confirm the effectiveness of the proposed strategy and its relevance\nfor real-time autonomous defense in contested airspace environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ade\u4e89\u7a7a\u57df\u4e2d\u4e09\u67b6\u81ea\u4e3b\u65e0\u4eba\u673a\u7684\u534f\u540c\u5236\u5bfc\u6846\u67b6\uff0c\u786e\u4fdd\u9632\u5fa1\u65e0\u4eba\u673a\u5728\u654c\u65e0\u4eba\u673a\u6355\u83b7\u9ad8\u4ef7\u503c\u65e0\u4eba\u673a\u524d\u5b8c\u6210\u62e6\u622a\u3002", "motivation": "\u89e3\u51b3\u7ade\u4e89\u7a7a\u57df\u4e2d\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u65e0\u4eba\u673a\u9632\u5fa1\u6311\u6218\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u7ea6\u675f\u7684\u5236\u5bfc\u6846\u67b6\u548c\u771f\u5b9e\u6bd4\u4f8b\u5bfc\u822a\u65b9\u6cd5\uff0c\u65e0\u9700\u4e86\u89e3\u654c\u65e0\u4eba\u673a\u7684\u7b56\u7565\u6216\u63a7\u5236\u89c4\u5f8b\u3002", "result": "\u5728\u4e0d\u540c\u5bf9\u6297\u573a\u666f\u7684\u4eff\u771f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u5173\u952e\u4ea4\u6218\u8bef\u5dee\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u964d\u81f3\u96f6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u8f7b\u91cf\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u81ea\u4e3b\u9632\u5fa1\u4efb\u52a1\u3002"}}
