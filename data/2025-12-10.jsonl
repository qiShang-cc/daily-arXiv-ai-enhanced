{"id": "2512.07851", "pdf": "https://arxiv.org/pdf/2512.07851", "abs": "https://arxiv.org/abs/2512.07851", "authors": ["Sansrit Paudel"], "title": "Signal and Noise Classification in Bio-Signals via unsupervised Machine Learning", "categories": ["eess.SP", "eess.SY"], "comment": null, "summary": "Real-world biosignal data is frequently corrupted by various types of noise, such as motion artifacts, and baseline wander. Although digital signal processing techniques exist to process such signals; however, heavily degraded signals cannot be recovered. In this study, we aim to classify two things: first, a binary classification of noisy and clean biosignals, and next, to categorize various kinds of noise such as motion artifacts, sensor failure, etc. We implemented K-means clustering, and our results indicate that the algorithm can most reliably group clean segments from noisy ones, particularly strong performance in identifying clean data compared to various categories of noise. This approach enables the selection of only high-quality bio-signal segments and provides accurate results for feature engineering that may enhance the precision of machine learning models trained on biosignals."}
{"id": "2512.08069", "pdf": "https://arxiv.org/pdf/2512.08069", "abs": "https://arxiv.org/abs/2512.08069", "authors": ["Florian Muralter", "Fabian Muralter", "Hugo Landaluce", "Asier Perallos"], "title": "Polarization-Diversity-Based Rotation Sensing Methodology Using COTS UHF RFID Tags", "categories": ["eess.SP"], "comment": null, "summary": "Phase-based sensing using ultra-high frequency (UHF) radio-frequency identification (RFID) has, in recent years, yielded numerous additions to the Internet of Things (IoT). This work presents a polarization diversity-based rotation sensing methodology using common-off-the-shelf (COTS) UHF RFID tags identified with a software-defined radio (SDR) UHF RFID reader. The proposed methodology uses the tag-to-reader message after fully coherent demodulation to calculate a difference signal of the backscatter load modulation states. This sequence is then used to compute the rotation speed by evaluating its phase change over time. Experimental results are used to validate the theoretical model and to evaluate the performance and limitations of the proposed system."}
{"id": "2512.08162", "pdf": "https://arxiv.org/pdf/2512.08162", "abs": "https://arxiv.org/abs/2512.08162", "authors": ["Benjamin W. Domae", "Ibrahim Pehlivan", "Danijela Cabric"], "title": "Millimeter-Wave True-Time Delay Array Beamforming with Robustness to Mobility", "categories": ["eess.SP"], "comment": "Presented at the 2025 Asilomar Conference on Signals, Systems, and Computers; 6 pages, 9 figures", "summary": "Ultra-reliable and low-latency connectivity is required for real-time and latency-sensitive applications, like wireless augmented and virtual reality streaming. Millimeter-wave (mmW) networks have enabled extremely high data rates through large available bandwidths but struggle to maintain continuous connectivity with mobile users. Achieving the required beamforming gain from large antenna arrays with minimal disruption is particularly challenging with fast-moving users and practical analog mmW array architectures. In this work, we propose frequency-dependent slanted beams from true-time delay (TTD) analog arrays to achieve robust beamforming in wideband, multi-user downlink scenarios. Novel beams with linear angle-frequency relationships for different users and sub-bands provide a trade-off between instantaneous capacity and angular coverage. Compared to alternative analog array beamforming designs, slanted beams provide higher reliability to angle offsets and greater adaptability to varied user movement statistics."}
{"id": "2512.08208", "pdf": "https://arxiv.org/pdf/2512.08208", "abs": "https://arxiv.org/abs/2512.08208", "authors": ["Mingyi Li", "Jiawen Xu", "Hanting Zhao", "Xu Zhao", "Yan Jin Chen", "Tie Jun Cui", "Vincenzo Galdi", "Lianlin Li"], "title": "Metasurfaces Enable Active-Like Passive Radar", "categories": ["eess.SP"], "comment": null, "summary": "Passive radars (PRs) provide a low-cost and energy-efficient approach to object detection by reusing existing wireless transmissions instead of emitting dedicated probing signals. Yet, conventional passive systems require prior knowledge of non-cooperative source waveforms, are vulnerable to strong interference, and rely on Doppler signatures, limiting their ability to detect subtle or slow-moving targets. Here, we introduce a metasurface-enabled PR (MEPR) concept that integrates a space-time-coding programmable metasurface to imprint distinct spatiotemporal tags onto ambient wireless wavefields. This mechanism transforms a PR into an active-like sensing platform without the need for source control, enabling interference suppression, signal enhancement, and accurate target localization and tracking in cluttered environments. A proof-of-concept implementation operating at 5.48 GHz confirms real-time imaging and tracking of unmanned aerial vehicles under interference-rich conditions, with performance comparable to active radar systems. These results establish MEPR as a solid foundation for scalable, adaptive, and energy-efficient next-generation integrated sensing and communication systems."}
{"id": "2512.07969", "pdf": "https://arxiv.org/pdf/2512.07969", "abs": "https://arxiv.org/abs/2512.07969", "authors": ["Alan Papalia", "Nikolas Sanderson", "Haoyu Han", "Heng Yang", "Hanumant Singh", "Michael Everett"], "title": "Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, submitted for review", "summary": "Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \\emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \\textbf{2$\\times$--35$\\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments."}
{"id": "2512.08244", "pdf": "https://arxiv.org/pdf/2512.08244", "abs": "https://arxiv.org/abs/2512.08244", "authors": ["Ye Ke", "Zhengnan Fu", "Junyi Yang", "Hongyang Shang", "Arindam Basu"], "title": "1024-Channel 0.8V 23.9-nW/Channel Event-based Compute In-memory Neural Spike Detector", "categories": ["eess.SP"], "comment": null, "summary": "The increasing data rate has become a major issue confronting next-generation intracortical brain-machine interfaces (iBMIs). The scaling number of recording sites requires complex analog wiring and lead to huge digitization power consumption. Compressive event-based neural frontends have been used in high-density neural implants to support the simultaneous recording of more channels. Event-based frontends (EBF) convert recorded signals into asynchronous digital events via delta modulation and can inherently achieve considerable compression. But EBFs are prone to false events that do not correspond to neural spikes. Spike detection (SPD) is a key process in the iBMI pipeline to detect neural spikes and further reduce the data rate. However, conventional digital SPD suffers from the increasing buffer size and frequent memory access power, and conventional spike emphasizers are not compatible with EBFs. In this work we introduced an event-based spike detection (Ev-SPD) algorithm for scalable compressive EBFs. To implement the algorithm effectively, we proposed a novel low-power 10-T eDRAM-SRAM hybrid random-access memory in-memory computing bitcell for event processing. We fabricated the proposed 1024-channel IMC SPD macro in a 65nm process and tested the macro with both synthetic dataset and Neuropixel recordings. The proposed macro achieved a high spike detection accuracy of 96.06% on a synthetic dataset and 95.08% similarity and 0.05 firing pattern MAE on Neuropixel recordings. Our event-based IMC SPD macro achieved a high per channel spike detection energy efficiency of 23.9 nW per channel and an area efficiency of 375 um^2 per channel. Our work presented a SPD scheme compatible with compressive EBFs for high-density iBMIs, achieving ultra-low power consumption with an IMC architecture while maintaining considerable accuracy."}
{"id": "2512.07976", "pdf": "https://arxiv.org/pdf/2512.07976", "abs": "https://arxiv.org/abs/2512.07976", "authors": ["Lazar Milikic", "Manthan Patel", "Jonas Frey"], "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies."}
{"id": "2512.08263", "pdf": "https://arxiv.org/pdf/2512.08263", "abs": "https://arxiv.org/abs/2512.08263", "authors": ["Jijia Tian", "Wangqian Chen", "Junting Chen", "Pooi-Yuen Kam"], "title": "Geometry-Aligned Differential Privacy for Location-Safe Federated Radio Map Construction", "categories": ["eess.SP"], "comment": null, "summary": "Radio maps that describe spatial variations in wireless signal strength are widely used to optimize networks and support aerial platforms. Their construction requires location-labeled signal measurements from distributed users, raising fundamental concerns about location privacy. Even when raw data are kept local, the shared model updates can reveal user locations through their spatial structure, while naive noise injection either fails to hide this leakage or degrades model accuracy. This work analyzes how location leakage arises from gradients in a virtual-environment radio map model and proposes a geometry-aligned differential privacy mechanism with heterogeneous noise tailored to both confuse localization and cover gradient spatial patterns. The approach is theoretically supported with a convergence guarantee linking privacy strength to learning accuracy. Numerical experiments show the approach increases attacker localization error from 30 m to over 180 m, with only 0.2 dB increase in radio map construction error compared to a uniform-noise baseline."}
{"id": "2512.07998", "pdf": "https://arxiv.org/pdf/2512.07998", "abs": "https://arxiv.org/abs/2512.07998", "authors": ["Mostafa Kamali Tabrizi", "Mingshi Chi", "Bir Bikram Dey", "Yu Qing Yuan", "Markus D. Solbach", "Yiqian Liu", "Michael Jenkin", "John K. Tsotsos"], "title": "DIJIT: A Robotic Head for an Active Observer", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy."}
{"id": "2512.08386", "pdf": "https://arxiv.org/pdf/2512.08386", "abs": "https://arxiv.org/abs/2512.08386", "authors": ["Yixuan Guo", "Mingliang Xiong", "Qingwen Liu"], "title": "Self-Alignment Resonant Beam Empowers Beamforming without Estimation and Control for 6G IoT", "categories": ["eess.SP"], "comment": null, "summary": "The integration of communication, sensing, and wireless power transfer (WPT) is a cornerstone of 6G intelligent IoT. However, relying on traditional beamforming imposes prohibitive overheads due to complex channel state information (CSI) estimation and active beam scanning, particularly in dynamic environments. This paper presents a comprehensive review of the radio frequency resonant beam system (RF-RBS), a native physical-layer paradigm that circumvents these limitations. By deploying retro-directive antenna arrays (RAA) at transceivers, RF-RBS establishes a self-sustaining cyclic electromagnetic loop. This mechanism inherently enables self-aligning, high-gain beamforming through positive feedback, eliminating the reliance on digital CSI processing. We analyze the system's architecture and its capability to support high-efficiency WPT, robust communication, and millimeter-level passive positioning. Finally, we evaluate the implementation challenges and strategic value of RF-RBS in latency-sensitive 6G scenarios, including unmanned systems and industrial automation."}
{"id": "2512.08028", "pdf": "https://arxiv.org/pdf/2512.08028", "abs": "https://arxiv.org/abs/2512.08028", "authors": ["Lampis Papakostas", "Aristeidis Geladaris", "Athanasios Mastrogeorgiou", "Jim Sharples", "Gautier Hattenberger", "Panagiotis Chatzakos", "Panagiotis Polygerinos"], "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles."}
{"id": "2512.08419", "pdf": "https://arxiv.org/pdf/2512.08419", "abs": "https://arxiv.org/abs/2512.08419", "authors": ["F. Philibert Andriniriniaimalaza", "Nour Mohammad Murad", "George Balan", "Habachi Bilal", "Nirilalaina Randriatefison", "Abdel Khoodaruth", "Charles Bernard Andrianirina", "Blaise Ravelo"], "title": "Hybrid Fuzzy Logic and Shading-Aware Particle Swarm Optimization for Dynamic Photovoltaic Shading Faults Mitigation", "categories": ["eess.SP"], "comment": null, "summary": "Shading faults remain one of the most critical challenges affecting photovoltaic (PV) system efficiency, as they not only reduce power generation but also disturb maximum power point tracking (MPPT). To address this issue, this study introduces a hybrid optimization framework that combines Fuzzy Logic Control (FLC) with a Shading-Aware Particle Swarm Optimization (SA-PSO) method. The proposed scheme is designed to adapt dynamically to both partial shading (20%-80%) and complete shading events, ensuring reliable global maximum power point (GMPP) detection. In this approach, the fuzzy controller provides rapid decision support based on shading patterns, while SA-PSO accelerates the search process and prevents the system from becoming trapped in local minima. A comparative performance assessment with the conventional Perturb and Observe (P\\&O) algorithm highlights the advantages of the hybrid model, showing up to an 11.8% improvement in power output and a 62% reduction in tracking time. These results indicate that integrating intelligent control with shading-aware optimization can significantly enhance the resilience and energy yield of PV systems operating under complex real-world conditions."}
{"id": "2512.08052", "pdf": "https://arxiv.org/pdf/2512.08052", "abs": "https://arxiv.org/abs/2512.08052", "authors": ["Pedro Santana"], "title": "An Introduction to Deep Reinforcement and Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL."}
{"id": "2512.08469", "pdf": "https://arxiv.org/pdf/2512.08469", "abs": "https://arxiv.org/abs/2512.08469", "authors": ["Gilles Monnoyer", "Jérôme Louveaux", "Laurence Defraigne", "Baptiste Sambon", "Luc vandendorpe"], "title": "Aliasing in Near-Field Array Ambiguity Functions: a Spatial Frequency-Domain Framework", "categories": ["eess.SP"], "comment": "17 pages, 14 figures", "summary": "Next-generation communication and localization systems increasingly rely on extremely large-scale arrays (XL-arrays), which promise unprecedented spatial resolution and new functionalities. These gains arise from their inherent operation in the near field (NF) regime, where the spherical nature of the wavefront can no longer be ignored; consequently, characterizing the ambiguity function -- which amounts to the matched beam pattern -- is considerably more challenging. Implementing very wide apertures with half-wavelength element spacing is costly and complex. This motivates thinning the array (removing elements), which introduces intricate aliasing structures, i.e., grating lobes. Whereas prior work has addressed this challenge using approximations tailored to specific array geometries, this paper develops a general framework that reveals the fundamental origins and geometric behavior of grating lobes in near-field ambiguity functions. Using a local spatial-frequency analysis of steering signals, we derive a systematic methodology to model NF grating lobes as aliasing artifacts, quantifying their structure on the AF, and providing design guidelines for XL-arrays that operate within aliasing-safe regions. We further connect our framework to established far-field principles. Finally, we demonstrate the practical value of the approach by deriving closed-form expressions for aliasing-free regions in canonical uniform linear arrays and uniform circular arrays."}
{"id": "2512.08145", "pdf": "https://arxiv.org/pdf/2512.08145", "abs": "https://arxiv.org/abs/2512.08145", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs."}
{"id": "2512.08509", "pdf": "https://arxiv.org/pdf/2512.08509", "abs": "https://arxiv.org/abs/2512.08509", "authors": ["Ashutosh Prajapati", "Prathapasinghe Dharmawansa", "Marco Di Renzo", "Italo Atzeni"], "title": "LoS+NLoS Holographic MIMO: Analysis and Application of Wavenumber-Division Multiplexing", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Holographic multiple-input multiple-output (MIMO) enables electrically large continuous apertures, overcoming the physical scaling limits of conventional MIMO architectures with half-wavelength spacing. Their near-field operating regime requires channel models that jointly capture line-of-sight (LoS) and non-line-of-sight (NLoS) components in a physically consistent manner. Existing studies typically treat these components separately or rely on environment-specific multipath models. In this work, we develop a unified LoS+NLoS channel representation for holographic lines that integrates spatial-sampling-based and expansion-based formulations. Building on this model, we extend the wavenumber-division multiplexing (WDM) framework, originally introduced for purely LoS channels, to the LoS+NLoS scenario. Applying WDM to the NLoS component yields its angular-domain representation, enabling direct characterization through the power spectral factor and power spectral density. We further derive closed-form characterizations for isotropic and non-isotropic scattering, with the former recovering Jakes' isotropic model. Lastly, we evaluate the resulting degrees of freedom and ergodic capacity, showing that incorporating the NLoS component substantially improves the performance relative to the purely LoS case."}
{"id": "2512.08170", "pdf": "https://arxiv.org/pdf/2512.08170", "abs": "https://arxiv.org/abs/2512.08170", "authors": ["Haoxin Zhang", "Shuaixin Li", "Xiaozhou Zhu", "Hongbo Chen", "Wen Yao"], "title": "RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community."}
{"id": "2512.08516", "pdf": "https://arxiv.org/pdf/2512.08516", "abs": "https://arxiv.org/abs/2512.08516", "authors": ["Ainna Yue Moreno-Locubiche", "Josep Vidal"], "title": "Beyond Diagonal RIS-assisted MIMO Transmission: Beamforming Gain and Capacity Optimization", "categories": ["eess.SP"], "comment": "6 pages, 7 figures", "summary": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology in wireless communications, offering unprecedented control over signal propagation. This study focuses on passive beyond diagonal reconfigurable intelligent surface (BD-RIS), which has been proposed to generalize conventional diagonal RIS, in Multiple-Input Multiple-Output (MIMO) downlink (DL) communication systems. We compare the performance of transmit beamforming (TxBF) and MIMO capacity transmission with waterfilling power allocation in the millimeter wave (mmWave) band, where propagation primarily occurs under line-of-sight (LOS) conditions. In the lack of closed-form expressions for the optimal RIS elements in either case, our approach adopts a gradient-based optimization approach requiring lower complexity than the solution in arXiv:2406.02170. Numerical results reveal that BD-RIS significantly outperforms traditional diagonal RIS in terms of spectral efficiency and coverage"}
{"id": "2512.08186", "pdf": "https://arxiv.org/pdf/2512.08186", "abs": "https://arxiv.org/abs/2512.08186", "authors": ["Meng Wei", "Chenyang Wan", "Jiaqi Peng", "Xiqian Yu", "Yuqiang Yang", "Delin Feng", "Wenzhe Cai", "Chenming Zhu", "Tai Wang", "Jiangmiao Pang", "Xihui Liu"], "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments."}
{"id": "2512.08556", "pdf": "https://arxiv.org/pdf/2512.08556", "abs": "https://arxiv.org/abs/2512.08556", "authors": ["Ainna Yue Moreno-Locubiche", "Josep Vidal", "Olga Muñoz-Medina", "Margarita Cabrera-Bean"], "title": "Contextual Bandits and Reconfigurable Intelligent Surfaces for Predictive LTM Handover Decisions", "categories": ["eess.SP"], "comment": "6 pages, 3 figures", "summary": "This article addresses the challenge of optimizing handover (HO) in next-generation wireless networks by integrating Reconfigurable Intelligent Surfaces (RIS), predicting received signal power, and utilizing learning-based decision-making. A conventional reactive HO mechanism, such as lower-layer triggered mobility (LTM), is enhanced through linear prediction to anticipate link degradation. Additionally, the use of RIS helps to mitigate signal blockage and extend coverage. An online trained non-linear Contextual Multi-Armed Bandit (CMAB) agent selects target gNBs based on context features, which reduces unnecessary HO and signaling overhead. Extensive simulations evaluate eight combinations of these techniques under realistic mobility and channel conditions. Results show that CMAB and RSRP prediction consistently reduce the number of HO, ping-pong rate and cell preparations, while RIS improves link reliability."}
{"id": "2512.08188", "pdf": "https://arxiv.org/pdf/2512.08188", "abs": "https://arxiv.org/abs/2512.08188", "authors": ["Wenjiang Xu", "Cindy Wang", "Rui Fang", "Mingkang Zhang", "Lusong Li", "Jing Xu", "Jiayuan Gu", "Zecui Zeng", "Rui Chen"], "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Website at https://embodied-tree-of-thoughts.github.io", "summary": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io ."}
{"id": "2512.08717", "pdf": "https://arxiv.org/pdf/2512.08717", "abs": "https://arxiv.org/abs/2512.08717", "authors": ["Oscar Romero", "Néstor Thome"], "title": "Applications of Singular Entropy to Signals and Singular Smoothness to Images", "categories": ["eess.SP", "math.NA"], "comment": "13 pages, 6 figures", "summary": "This paper explores signal and image analysis by using the Singular Value Decomposition (SVD) and its extension, the Generalized Singular Value Decomposition (GSVD). A key strength of SVD lies in its ability to separate information into orthogonal subspaces. While SVD is a well-established tool in ECG analysis, particularly for source separation, this work proposes a refined method for selecting a threshold to distinguish between maternal and fetal components more effectively. In the first part of the paper, the focus is onmedical signal analysis,where the concepts of Energy Gap Variation (EGV) and Singular Energy are introduced to isolate fetal and maternal ECG signals, improving the known ones. Furthermore, the approach is significantly enhanced by the application of GSVD, which provides additional discriminative power for more accurate signal separation. The second part introduces a novel technique called Singular Smoothness, developed for image analysis. This method incorporates Singular Entropy and the Frobenius normto evaluate information density, and is applied to the detection of natural anomalies such asmountain fractures and burned forest regions. Numerical experiments are presented to demonstrate the effectiveness of the proposed approaches."}
{"id": "2512.08206", "pdf": "https://arxiv.org/pdf/2512.08206", "abs": "https://arxiv.org/abs/2512.08206", "authors": ["Duo Zhang", "Junshan Huang", "Jingjin Yu"], "title": "High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement", "categories": ["cs.RO"], "comment": "ICRA 2026 Submission", "summary": "We propose Synchronous Dual-Arm Rearrangement Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal configurations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR-M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state-of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware."}
{"id": "2512.08746", "pdf": "https://arxiv.org/pdf/2512.08746", "abs": "https://arxiv.org/abs/2512.08746", "authors": ["Federica Fieramosca", "Vittorio Rampa", "Michele D'Amico", "Stefano Savazzi"], "title": "RF sensing with dense IoT network graphs: An EM-informed analysis", "categories": ["eess.SP"], "comment": "accepted to IEEE Internet of Things Journal", "summary": "Radio Frequency (RF) sensing is attracting interest in research, standardization, and industry, especially for its potential in Internet of Things (IoT) applications. By leveraging the properties of the ElectroMagnetic (EM) waves used in wireless networks, RF sensing captures environmental information such as the presence and movement of people and objects, enabling passive localization and vision applications. This paper investigates the theoretical bounds on accuracy and resolution for RF sensing systems within dense networks. It employs an EM model to predict the effects of body blockage in various scenarios. To detect human movements, the paper proposes a deep graph neural network, trained on Received Signal Strength (RSS) samples generated from the EM model. These samples are structured as dense graphs, with nodes representing antennas and edges as radio links. Focusing on the problem of identifying the number of human subjects co-present in a monitored area over time, the paper analyzes the theoretical limits on the number of distinguishable subjects, exploring how these limits depend on factors such as the number of radio links, the size of the monitored area and the subjects physical dimensions. These bounds enable the prediction of the system performance during network pre-deployment stages. The paper also presents the results of an indoor case study, which demonstrate the effectiveness of the approach and confirm the model's predictive potential in the network design stages."}
{"id": "2512.08233", "pdf": "https://arxiv.org/pdf/2512.08233", "abs": "https://arxiv.org/abs/2512.08233", "authors": ["Timothy Chen", "Marcus Dominguez-Kuhne", "Aiden Swann", "Xu Liu", "Mac Schwager"], "title": "Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior", "categories": ["cs.RO"], "comment": null, "summary": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/."}
{"id": "2512.08779", "pdf": "https://arxiv.org/pdf/2512.08779", "abs": "https://arxiv.org/abs/2512.08779", "authors": ["Emre Havazli", "Shadi Oveisgharan", "Michael Denbina", "Brian Hawkins"], "title": "Evaluating the Deformation Measurement Accuracy Using Low-SNR Radars for Future InSAR Missions", "categories": ["eess.SP"], "comment": null, "summary": "Interferometric Synthetic Aperture Radar (InSAR) is a powerful tool for monitoring surface deformation with high precision. However, low Signal-to-Noise Ratio (SNR) conditions, common in regions with low backscatter, can degrade phase coherence and compromise displacement accuracy. In this study, we quantify the impact of low-SNR conditions on InSAR-derived displacement using L-band UAVSAR data collected over the San Andreas Fault and Greenland ice sheet. We simulate low-SNR conditions by degrading the Noise-Equivalent Sigma Zero (NESZ) to $-15~\\mathrm{dB}$ and assess the resulting effects on interferometric coherence, phase unwrapping, and time series inversion. The displacement accuracy of 4mm in single interferogram can be achieved by taking looks for the signal decorrelation of 0.6 and SNR between -9dB to -10dB. Our findings indicate that even under low-SNR conditions, a velocity precision of $0.5~\\mathrm{cm/yr}$ can be achieved in comparison to high-SNR conditions. By applying multilooking with an 8x8 window, we significantly improve coherence and eliminate this bias, demonstrating that low-SNR systems can achieve comparable precision to high-SNR systems at the expense of spatial resolution. These results have important implications for the design of future cost-effective SAR missions, such as Surface Deformation and Change (SDC), and the optimization of InSAR processing techniques in challenging environments."}
{"id": "2512.08248", "pdf": "https://arxiv.org/pdf/2512.08248", "abs": "https://arxiv.org/abs/2512.08248", "authors": ["Ahan Basu", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments."}
{"id": "2512.08799", "pdf": "https://arxiv.org/pdf/2512.08799", "abs": "https://arxiv.org/abs/2512.08799", "authors": ["Boxuan Wen", "Junyu Luo"], "title": "Delay-Oriented Distributed Scheduling with TransGNN", "categories": ["eess.SP", "cs.NI"], "comment": "10 pages, 3 figures", "summary": "Minimizing transmission delay in wireless multi-hop networks is a fundamental yet challenging task due to the complex coupling among interference, queue dynamics, and distributed control. Traditional scheduling algorithms, such as max-weight or queue-length-based policies, primarily aim to optimize throughput but often suffer from high latency, especially in heterogeneous or dynamically changing topologies. Recent learning-based approaches, particularly those employing Graph Neural Networks (GNNs), have shown promise in capturing spatial interference structures. However, conventional Graph Convolutional Networks (GCNs) remain limited by their local aggregation mechanism and their inability to model long-range dependencies within the conflict graph. To address these challenges, this paper proposes a delay-oriented distributed scheduling framework based on Transformer GNN. The proposed model employs an attention-based graph encoder to generate adaptive per-link utility scores that reflect both queue backlog and interference intensity. A Local Greedy Solver (LGS) then utilizes these utilities to construct a feasible independent set of links for transmission, ensuring distributed and conflict-free scheduling."}
{"id": "2512.08271", "pdf": "https://arxiv.org/pdf/2512.08271", "abs": "https://arxiv.org/abs/2512.08271", "authors": ["Srijan Dokania", "Dharini Raghavan"], "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation", "categories": ["cs.RO", "cs.CV", "cs.LG", "eess.IV"], "comment": "Published and Presented at 3rd Workshop on Human-Centric Multilateral Teleoperation in ICRA 2025", "summary": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup."}
{"id": "2512.08887", "pdf": "https://arxiv.org/pdf/2512.08887", "abs": "https://arxiv.org/abs/2512.08887", "authors": ["Nakul Singh", "Coleman DeLude", "Mark Davenport", "Justin Romberg"], "title": "A Fast Broadband Beamspace Transformation", "categories": ["eess.SP"], "comment": null, "summary": "We present a new computationally efficient method for multi-beamforming in the broadband setting. Our \"fast beamspace transformation\" forms $B$ beams from $M$ sensor outputs using a number of operations per sample that scales linearly (to within logarithmic factors) with $M$ when $B\\sim M$. While the narrowband version of this transformation can be performed efficiently with a spatial fast Fourier transform, the broadband setting requires coherent processing of multiple array snapshots simultaneously. Our algorithm works by taking $N$ samples off of each of $M$ sensors and encoding the sensor outputs into a set of coefficients using a special non-uniform spaced Fourier transform. From these coefficients, each beam is formed by solving a small system of equations that has Toeplitz structure. The total runtime complexity is $\\mathcal{O}(M\\log N+B\\log N)$ operations per sample, exhibiting essentially the same scaling as in the narrowband case and vastly outperforming broadband beamformers based on delay and sum whose computations scale as $\\mathcal{O}(MB)$. Alongside a careful mathematical formulation and analysis of our fast broadband beamspace transform, we provide a host of numerical experiments demonstrating the algorithm's favorable computational scaling and high accuracy. Finally, we demonstrate how tasks such as interpolating to ``off-grid\" angles and nulling an interferer are more computationally efficient when performed directly in beamspace."}
{"id": "2512.08280", "pdf": "https://arxiv.org/pdf/2512.08280", "abs": "https://arxiv.org/abs/2512.08280", "authors": ["Haldun Balim", "Na Li", "Yilun Du"], "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": null, "summary": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control."}
{"id": "2512.08903", "pdf": "https://arxiv.org/pdf/2512.08903", "abs": "https://arxiv.org/abs/2512.08903", "authors": ["Ramin Babaee", "Shahab Oveis Gharan", "Martin Bouchard"], "title": "Timing-Error Optimized Architecture for Current-Steering DACs", "categories": ["eess.SP"], "comment": null, "summary": "We propose a novel digital-to-analog converter (DAC) weighting architecture that statistically minimizes the distortion caused by random timing mismatches among current sources. To decode the DAC input codewords into corresponding DAC switches, we present three algorithms with varying computational complexities. We perform high-level Matlab simulations to illustrate the dynamic performance improvement over the segmented structure."}
{"id": "2512.08333", "pdf": "https://arxiv.org/pdf/2512.08333", "abs": "https://arxiv.org/abs/2512.08333", "authors": ["Yajat Yadav", "Zhiyuan Zhou", "Andrew Wagenmaker", "Karl Pertsch", "Sergey Levine"], "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities."}
{"id": "2512.08909", "pdf": "https://arxiv.org/pdf/2512.08909", "abs": "https://arxiv.org/abs/2512.08909", "authors": ["Ramin Babaee", "Shahab Oveis Gharan", "Martin Bouchard"], "title": "Architecture Design for Rise/Fall Asymmetry Glitch Minimization in Current-Steering DACs", "categories": ["eess.SP"], "comment": null, "summary": "Current-steering digital-to-analog converter (DAC) is a prominent architecture that is commonly used in high-speed applications such as optical communications. One of the shortcomings of this architecture is the output glitches that are input dependent and degrade the dynamic performance of the DAC. We investigate DAC glitches that arise from asymmetry in the fall/rise response of DAC switches. We formulate a glitch metric that defines the overall DAC performance, which is then used to find a novel DAC weighting scheme. Numerical simulations show that the proposed architecture can potentially provide a significant performance advantage compared to the segmented structure."}
{"id": "2512.08405", "pdf": "https://arxiv.org/pdf/2512.08405", "abs": "https://arxiv.org/abs/2512.08405", "authors": ["Fan Zhang", "Michael Gienger"], "title": "Learning Robot Manipulation from Audio World Models", "categories": ["cs.RO"], "comment": null, "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns."}
{"id": "2512.07872", "pdf": "https://arxiv.org/pdf/2512.07872", "abs": "https://arxiv.org/abs/2512.07872", "authors": ["Ishaan Kunwar", "Henry Cantor", "Tyler Rizzo", "Ayaan Qayyum"], "title": "LocaGen: Sub-Sample Time-Delay Learning for Beam Localization", "categories": ["cs.SD", "eess.AS", "eess.SP"], "comment": "7 pages", "summary": "The goal of LocaGen is to improve the localization performance of audio signals in the 2-D beam localization problem. LocaGen reduces sampling quantization errors through machine learning models trained on realistic synthetic data generated by a simulation. The system increases the accuracy of both direction-of-arrival (DOA) and precise location estimation of an audio beam from an array of three microphones. We demonstrate LocaGen's efficacy on a low-powered embedded system with an increased localization accuracy with a minimal increase in real-time resource usage. LocaGen was demonstrated to reduce DOA error by approximately 67% even with a microphone array of only 10 kHz in audio processing."}
{"id": "2512.08476", "pdf": "https://arxiv.org/pdf/2512.08476", "abs": "https://arxiv.org/abs/2512.08476", "authors": ["Po-An Shih", "Shao-Hua Wang", "Yung-Che Li", "Chia-Heng Tu", "Chih-Han Chang"], "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems", "categories": ["cs.RO"], "comment": null, "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems."}
{"id": "2512.08034", "pdf": "https://arxiv.org/pdf/2512.08034", "abs": "https://arxiv.org/abs/2512.08034", "authors": ["Zilu Zhao", "Fangqing Xiao", "Dirk Slock"], "title": "Expectations in Expectation Propagation", "categories": ["cs.IT", "eess.SP", "stat.CO"], "comment": "9 pages, 2 figures, will be submitted to asilomar25", "summary": "Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have \"negative\" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.\n  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.\n  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values."}
{"id": "2512.08481", "pdf": "https://arxiv.org/pdf/2512.08481", "abs": "https://arxiv.org/abs/2512.08481", "authors": ["Yixiang Lin", "Tiancheng Yang", "Jonathan Eden", "Ying Tan"], "title": "Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\\% to 90\\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a \"trade-off\" group that modulated their physical responses according to disturbance likelihood, and an \"always-compensate\" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers."}
{"id": "2512.08157", "pdf": "https://arxiv.org/pdf/2512.08157", "abs": "https://arxiv.org/abs/2512.08157", "authors": ["Lei Xie", "Hengtao He", "Yifeng Xiong", "Fan Liu", "Shi Jin"], "title": "Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments", "categories": ["cs.IT", "eess.SP"], "comment": null, "summary": "This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods."}
{"id": "2512.08518", "pdf": "https://arxiv.org/pdf/2512.08518", "abs": "https://arxiv.org/abs/2512.08518", "authors": ["Nadezhda Kushina", "Ko Watanabe", "Aarthi Kannan", "Ashita Ashok", "Andreas Dengel", "Karsten Berns"], "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic."}
{"id": "2512.08352", "pdf": "https://arxiv.org/pdf/2512.08352", "abs": "https://arxiv.org/abs/2512.08352", "authors": ["Ying Zhang", "Fan Liu", "Yifeng Xiong", "Weijie Yuan", "Shuangyang Li", "Le Zheng", "Tony Xiao Han", "Christos Masouros", "Shi Jin"], "title": "On Discrete Ambiguity Functions of Random Communication Waveforms", "categories": ["cs.IT", "eess.SP"], "comment": "18 pages, 2 figures", "summary": "This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples."}
{"id": "2512.08541", "pdf": "https://arxiv.org/pdf/2512.08541", "abs": "https://arxiv.org/abs/2512.08541", "authors": ["Nils Gehrke", "David Brecht", "Dominik Kulmer", "Dheer Patel", "Frank Diermeyer"], "title": "vEDGAR -- Can CARLA Do HiL?", "categories": ["cs.RO"], "comment": null, "summary": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR"}
{"id": "2512.08416", "pdf": "https://arxiv.org/pdf/2512.08416", "abs": "https://arxiv.org/abs/2512.08416", "authors": ["Fanambinantsoa Philibert Andriniriniaimalaza", "Nour Murad", "Randriamaitso Telesphore", "Bilal Habachi", "Randriatefison Nirilalaina", "Manasina Ruffin", "Andrianirina Charles Bernard", "Ravelo Blaise"], "title": "Improvement and Stabilization of Output Voltages in a Vertical Tidal Turbine Using Intelligent Control Strategies", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "This article investigates on the improvement and stabilization of alternating current (AC) and direct current (DC) output voltages in a Permanent Magnet Synchronous Generator (PMSG) driven by a vertical-axis tidal turbine using advanced control strategies. The research integrates artificial intelligence (AI)-based techniques to enhance voltage stability and efficiency. Initially, the Maximum Power Point Tracking (MPPT) approach based on Tip Speed Ratio (TSR) and Artificial Neural Network (ANN) Fuzzy logic controllers is explored. To further optimize the performance, Particle Swarm Optimization (PSO) and a hybrid ANN-PSO methodology are implemented. These strategies aim to refine the reference rotational speed of the turbine while minimizing deviations from optimal power extraction conditions. The simulation results of a tidal turbine operating at a water flow velocity of 1.5 m/s demonstrate that the PSO-based control approach significantly enhances the voltage stability compared to conventional MPPT-TSR and ANN-Fuzzy controllers. The hybrid ANN-PSO technique improves the voltage regulation by dynamically adapting to system variations and providing real-time reference speed adjustments. This research highlights the AI-based hybrid optimization benefit to stabilize the output voltage of tidal energy systems, thereby increasing reliability and efficiency in renewable energy applications."}
{"id": "2512.08548", "pdf": "https://arxiv.org/pdf/2512.08548", "abs": "https://arxiv.org/abs/2512.08548", "authors": ["Yuchi Zhang", "Churui Sun", "Shiqi Liang", "Diyuan Liu", "Chao Ji", "Wei-Nan Zhang", "Ting Liu"], "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks."}
{"id": "2512.08574", "pdf": "https://arxiv.org/pdf/2512.08574", "abs": "https://arxiv.org/abs/2512.08574", "authors": ["Vit Kratky", "Robert Penicka", "Parakh M. Gupta", "Ondrej Prochazka", "Martin Saska"], "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials."}
{"id": "2512.08580", "pdf": "https://arxiv.org/pdf/2512.08580", "abs": "https://arxiv.org/abs/2512.08580", "authors": ["Peijun Tang", "Shangjin Xie", "Binyan Sun", "Baifu Huang", "Kuncheng Luo", "Haotian Yang", "Weiqi Jin", "Jianan Wang"], "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "49 pages, 25 figures", "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space."}
{"id": "2512.08630", "pdf": "https://arxiv.org/pdf/2512.08630", "abs": "https://arxiv.org/abs/2512.08630", "authors": ["Marta Manzoni", "Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach."}
{"id": "2512.08653", "pdf": "https://arxiv.org/pdf/2512.08653", "abs": "https://arxiv.org/abs/2512.08653", "authors": ["Doumegna Mawuto Koudjo Felix", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation", "categories": ["cs.RO"], "comment": null, "summary": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios."}
{"id": "2512.08656", "pdf": "https://arxiv.org/pdf/2512.08656", "abs": "https://arxiv.org/abs/2512.08656", "authors": ["Lauritz Rismark Fosso", "Herman Biørn Amundsen", "Marios Xanthidis", "Sveinung Johan Ohrem"], "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes", "categories": ["cs.RO"], "comment": "7 pages, 4 figures", "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions."}
{"id": "2512.08661", "pdf": "https://arxiv.org/pdf/2512.08661", "abs": "https://arxiv.org/abs/2512.08661", "authors": ["Ziyue Zheng", "Yongce Liu", "Hesheng Wang", "Zhongqiang Ren"], "title": "Ergodic Trajectory Planning with Dynamic Sensor Footprints", "categories": ["cs.RO"], "comment": "12 figures", "summary": "This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space."}
{"id": "2512.08688", "pdf": "https://arxiv.org/pdf/2512.08688", "abs": "https://arxiv.org/abs/2512.08688", "authors": ["Mark Pustilnik", "Francesco Borrelli"], "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme."}
{"id": "2512.08754", "pdf": "https://arxiv.org/pdf/2512.08754", "abs": "https://arxiv.org/abs/2512.08754", "authors": ["Jason Hughes", "Marcel Hussing", "Edward Zhang", "Shenbagaraj Kannapiran", "Joshua Caswell", "Kenneth Chaney", "Ruichen Deng", "Michaela Feehery", "Agelos Kratimenos", "Yi Fan Li", "Britny Major", "Ethan Sanchez", "Sumukh Shrote", "Youkang Wang", "Jeremy Wang", "Daudi Zein", "Luying Zhang", "Ruijun Zhang", "Alex Zhou", "Tenzi Zhouga", "Jeremy Cannon", "Zaffir Qasim", "Jay Yelon", "Fernando Cladera", "Kostas Daniilidis", "Camillo J. Taylor", "Eric Eaton"], "title": "A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models", "categories": ["cs.RO"], "comment": "Technical Report for the DARPA Triage Challenge PRONTO team", "summary": "This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved."}
{"id": "2512.08767", "pdf": "https://arxiv.org/pdf/2512.08767", "abs": "https://arxiv.org/abs/2512.08767", "authors": ["Mohammed Elseiagy", "Tsige Tadesse Alemayoh", "Ranulfo Bezerra", "Shotaro Kojima", "Kazunori Ohno"], "title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est", "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems"}
{"id": "2512.08813", "pdf": "https://arxiv.org/pdf/2512.08813", "abs": "https://arxiv.org/abs/2512.08813", "authors": ["Connor York", "Zachary R Madin", "Paul O'Dowd", "Edmund R Hunt"], "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks", "categories": ["cs.RO"], "comment": "Accepted to SAC '26. To appear, DOI: https://doi.org/10.1145/3748522.3779970", "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task."}
{"id": "2512.08877", "pdf": "https://arxiv.org/pdf/2512.08877", "abs": "https://arxiv.org/abs/2512.08877", "authors": ["Ryan LeRoy", "Jack Kolb"], "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams", "categories": ["cs.RO"], "comment": "4 pages, 3 figures, appendix", "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve."}
{"id": "2512.08920", "pdf": "https://arxiv.org/pdf/2512.08920", "abs": "https://arxiv.org/abs/2512.08920", "authors": ["Jessica Yin", "Haozhi Qi", "Youngsun Wi", "Sayantan Kundu", "Mike Lambeta", "William Yang", "Changhao Wang", "Tingfan Wu", "Jitendra Malik", "Tess Hellebrekers"], "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "categories": ["cs.RO", "cs.LG"], "comment": "Project website: https://jessicayin.github.io/osmo_tactile_glove/", "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption."}
{"id": "2512.08229", "pdf": "https://arxiv.org/pdf/2512.08229", "abs": "https://arxiv.org/abs/2512.08229", "authors": ["Tony Salloom", "Dandi Zhou", "Xinhai Sun"], "title": "Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior."}
{"id": "2512.08262", "pdf": "https://arxiv.org/pdf/2512.08262", "abs": "https://arxiv.org/abs/2512.08262", "authors": ["Hafeez Husain Cholakkal", "Stefano Arrigoni", "Francesco Braghin"], "title": "RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach."}
{"id": "2512.08411", "pdf": "https://arxiv.org/pdf/2512.08411", "abs": "https://arxiv.org/abs/2512.08411", "authors": ["Mingwei Li", "Xiaoyuan Zhang", "Chengwei Yang", "Zilong Zheng", "Yaodong Yang"], "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents."}
{"id": "2512.08430", "pdf": "https://arxiv.org/pdf/2512.08430", "abs": "https://arxiv.org/abs/2512.08430", "authors": ["Nico Leuze", "Maximilian Hoh", "Samed Doğan", "Nicolas R. -Peña", "Alfred Schoettl"], "title": "SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to WACV 2026. Preprint version", "summary": "Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios."}
{"id": "2512.08463", "pdf": "https://arxiv.org/pdf/2512.08463", "abs": "https://arxiv.org/abs/2512.08463", "authors": ["Antonio Terpin", "Raffaello D'Andrea"], "title": "Using reinforcement learning to probe the role of feedback in skill acquisition", "categories": ["cs.AI", "cs.LG", "cs.RO", "eess.SY"], "comment": "Website: https://antonioterpin.com/fluids-control", "summary": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity."}
{"id": "2512.08577", "pdf": "https://arxiv.org/pdf/2512.08577", "abs": "https://arxiv.org/abs/2512.08577", "authors": ["Yuna Kato", "Shohei Mori", "Hideo Saito", "Yoshifumi Takatsume", "Hiroki Kajita", "Mariko Isogawa"], "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option."}
{"id": "2512.08607", "pdf": "https://arxiv.org/pdf/2512.08607", "abs": "https://arxiv.org/abs/2512.08607", "authors": ["Adrian Wiltz", "Dimos V. Dimarogonas"], "title": "Decoupled Design of Time-Varying Control Barrier Functions via Equivariances", "categories": ["eess.SY", "cs.RO"], "comment": "7 pages, 3 figures", "summary": "This article presents a systematic method for designing time-varying Control Barrier Functions (CBF) composed of a time-invariant component and multiple time-dependent components, leveraging structural properties of the system dynamics. The method involves the construction of a specific class of time-invariant CBFs that encode the system's dynamic capabilities with respect to a given constraint, and augments them subsequently with appropriately designed time-dependent transformations. While transformations uniformly varying the time-invariant CBF can be applied to arbitrary systems, transformations exploiting structural properties in the dynamics - equivariances in particular - enable the handling of a broader and more expressive class of time-varying constraints. The article shows how to leverage such properties in the design of time-varying CBFs. The proposed method decouples the design of time variations from the computationally expensive construction of the underlying CBFs, thereby providing a computationally attractive method to the design of time-varying CBFs. The method accounts for input constraints and under-actuation, and requires only qualitative knowledge on the time-variation of the constraints making it suitable to the application in uncertain environments."}
{"id": "2512.08888", "pdf": "https://arxiv.org/pdf/2512.08888", "abs": "https://arxiv.org/abs/2512.08888", "authors": ["Manduhu Manduhu", "Alexander Dow", "Gerard Dooly", "James Riordan"], "title": "Accelerated Rotation-Invariant Convolution for UAV Image Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.\n  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\\(\\times\\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\\(\\times\\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks."}
{"id": "2512.08912", "pdf": "https://arxiv.org/pdf/2512.08912", "abs": "https://arxiv.org/abs/2512.08912", "authors": ["Simon de Moreau", "Andrei Bursuc", "Hafid El-Idrissi", "Fabien Moutarde"], "title": "LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint. 12 pages, 9 figures. Project page: https://simondemoreau.github.io/LiDAS/", "summary": "Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception."}
