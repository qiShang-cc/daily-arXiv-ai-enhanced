{"id": "2508.14204", "pdf": "https://arxiv.org/pdf/2508.14204", "abs": "https://arxiv.org/abs/2508.14204", "authors": ["Xingyu Chen", "Jianrong Ding", "Kai Zheng", "Xinmin Fang", "Xinyu Zhang", "Chris Xiaoxuan Lu", "Zhengxiong Li"], "title": "InverTwin: Solving Inverse Problems via Differentiable Radio Frequency Digital Twin", "categories": ["eess.SP"], "comment": null, "summary": "Digital twins (DTs), virtual simulated replicas of physical scenes, are\ntransforming various industries. However, their potential in radio frequency\n(RF) sensing applications has been limited by the unidirectional nature of\nconventional RF simulators. In this paper, we present InverTwin, an\noptimization-driven framework that creates RF digital twins by enabling\nbidirectional interaction between virtual and physical realms. InverTwin\novercomes the fundamental differentiability challenges of RF optimization\nproblems through novel design components, including path-space differentiation\nto address discontinuity in complex simulation functions, and a radar surrogate\nmodel to mitigate local non-convexity caused by RF signal periodicity. These\ntechniques enable smooth gradient propagation and robust optimization of the DT\nmodel. Our implementation and experiments demonstrate InverTwin's versatility\nand effectiveness in augmenting both data-driven and model-driven RF sensing\nsystems for DT reconstruction.", "AI": {"tldr": "InverTwin \u662f\u4e00\u4e2a\u4f18\u5316\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u63d0\u5347\u5c04\u9891\u6570\u5b57\u5b6a\u751f\uff08RF DT\uff09\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u5c04\u9891\u6a21\u62df\u5668\u7684\u5355\u5411\u6027\u9650\u5236\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u5c04\u9891\u4f20\u611f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8def\u5f84\u7a7a\u95f4\u5fae\u5206\u548c\u96f7\u8fbe\u66ff\u4ee3\u6a21\u578b\u89e3\u51b3\u5c04\u9891\u4f18\u5316\u7684\u4e0d\u53ef\u5fae\u6027\u548c\u5c40\u90e8\u975e\u51f8\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u68af\u5ea6\u7684\u5e73\u6ed1\u4f20\u64ad\u548c\u9c81\u68d2\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e InverTwin \u5728\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\u7684\u5c04\u9891\u4f20\u611f\u7cfb\u7edf\u4e2d\u5177\u6709\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "InverTwin \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53cc\u5411\u4ea4\u4e92\u65b9\u6cd5\uff0c\u4e3a\u5c04\u9891\u6570\u5b57\u5b6a\u751f\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.14438", "pdf": "https://arxiv.org/pdf/2508.14438", "abs": "https://arxiv.org/abs/2508.14438", "authors": ["Akash Prabakar", "Abhishek Shreekant Bhandiwad", "Abijith Jagannath Kamath", "Chandra Sekhar Seelamantula"], "title": "Weakly-Convex Regularization for Magnetic Resonance Image Denoising", "categories": ["eess.SP"], "comment": "Presented in ISCS25", "summary": "Regularization for denoising in magnetic resonance imaging (MRI) is typically\nachieved using convex regularization functions. Recently, deep learning\ntechniques have been shown to provide superior denoising performance. However,\nthis comes at the price of lack of explainability, interpretability and\nstability, which are all crucial to MRI. In this work, we present a\nconstructive approach for designing weakly-convex regularization functions for\nMR image denoising. We show that our technique performs on par with\nstate-of-the-art denoisers for diffusion-weighted MR image denoising. Our\ntechnique can be applied to design weakly-convex convolutional neural networks\nwith prototype activation functions that impart interpretability and are\nprovably convergent. We also show that our technique exhibits fewer denoising\nartifacts by demonstrating its effect on brain microstructure modelling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u9020\u6027\u65b9\u6cd5\u8bbe\u8ba1\u5f31\u51f8\u6b63\u5219\u5316\u51fd\u6570\u7528\u4e8eMRI\u53bb\u566a\uff0c\u6027\u80fd\u4e0e\u5148\u8fdb\u53bb\u566a\u6280\u672f\u76f8\u5f53\uff0c\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edfMRI\u53bb\u566a\u4f7f\u7528\u51f8\u6b63\u5219\u5316\u51fd\u6570\uff0c\u6df1\u5ea6\u5b66\u4e60\u867d\u6027\u80fd\u4f18\u8d8a\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6784\u9020\u6027\u65b9\u6cd5\u8bbe\u8ba1\u5f31\u51f8\u6b63\u5219\u5316\u51fd\u6570\uff0c\u5e76\u5e94\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u53ef\u89e3\u91ca\u6fc0\u6d3b\u51fd\u6570\u7684\u5f31\u51f8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u6280\u672f\u5728\u6269\u6563\u52a0\u6743MRI\u53bb\u566a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u51cf\u5c11\u4e86\u53bb\u566a\u4f2a\u5f71\uff0c\u5bf9\u8111\u5fae\u7ed3\u6784\u5efa\u6a21\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8eMRI\u53bb\u566a\u3002"}}
{"id": "2508.14458", "pdf": "https://arxiv.org/pdf/2508.14458", "abs": "https://arxiv.org/abs/2508.14458", "authors": ["Jingjing Zhao", "Haowen Song", "Xidong Mu", "Kaiquan Cai", "Yanbo Zhu", "Yuanwei Liu"], "title": "Pinching-Antenna Systems-Enabled Multi-User Communications: Transmission Structures and Beamforming Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Pinching-antenna systems (PASS) represent an innovative advancement in\nflexible-antenna technologies, aimed at significantly improving wireless\ncommunications by ensuring reliable line-of-sight connections and dynamic\nantenna array reconfigurations. To employ multi-waveguide PASS in multi-user\ncommunications, three practical transmission structures are proposed, namely\nwaveguide multiplexing (WM), waveguide division (WD), and waveguide switching\n(WS). Based on the proposed structures, the joint baseband signal processing\nand pinching beamforming design is studied for a general multi-group multicast\ncommunication system, with the unicast communication encompassed as a special\ncase. A max-min fairness problem is formulated for each proposed transmission\nstructure, subject to the maximum transmit power constraint. For WM, to solve\nthe highly-coupled and non-convex MMF problem with complex exponential and\nfractional expressions, a penalty dual decomposition (PDD)-based algorithm is\ninvoked for obtaining locally optimal solutions. Specifically, the augmented\nLagrangian relaxation is first applied to alleviate the stringent coupling\nconstraints, which is followed by the block decomposition over the resulting\naugmented Lagrangian function. Then, the proposed PDD-based algorithm is\nextended to solve the MMF problem for both WD and WS. Furthermore, a\nlow-complexity algorithm is proposed for the unicast case employing the WS\nstructure, by simultaneously aligning the signal phases and minimizing the\nlarge-scale path loss at each user. Finally, numerical results reveal that: 1)\nthe MMF performance is significantly improved by employing the PASS compared to\nconventional fixed-position antenna systems; 2) WS and WM are suitable for\nunicast and multicast communications, respectively; 3) the performance gap\nbetween WD and WM can be significantly alleviated when the users are\ngeographically isolated.", "AI": {"tldr": "Pinching-antenna systems (PASS) enhance wireless communications through dynamic reconfigurations, proposing three transmission structures (WM, WD, WS) and solving max-min fairness problems with PDD-based algorithms, showing significant performance improvements over conventional systems.", "motivation": "Improve wireless communications by enabling reliable line-of-sight and dynamic antenna reconfigurations in multi-user scenarios.", "method": "Propose three transmission structures (WM, WD, WS) and design joint baseband signal processing and pinching beamforming, using PDD-based algorithms for solving max-min fairness problems.", "result": "PASS outperforms conventional systems; WS suits unicast, WM suits multicast, and WD's performance gap with WM reduces with geographically isolated users.", "conclusion": "PASS significantly enhances wireless communication performance, with specific structures optimal for different communication scenarios."}}
{"id": "2508.14611", "pdf": "https://arxiv.org/pdf/2508.14611", "abs": "https://arxiv.org/abs/2508.14611", "authors": ["Jinkun Yang"], "title": "FPGA Design and Implementation of Fixed-Point Fast Divider Using Goldschmidt Division Algorithm and Mitchell Multiplication Algorithm", "categories": ["eess.SP"], "comment": "7 pages,9 figures", "summary": "This paper presents a variable bit-width fixed-point fast divider using\nGoldschmidt division algorithm and Mitchell multiplication algorithm. Described\nusing Verilog HDL and implemented on a Xilinx XC7Z020-2CLG400I FPGA, the\nproposed divider achieves over 99% computational accuracy with a minimum\nlatency of 99.1 ns, which is 31.7 ns faster than existing single-precision\ndividers. Compared with a Goldschmidt divider using a Vedic multiplier, the\nproposed design reduces Slice Registers by 46.68%, Slice LUTs by 4.93%, and\nSlices by 11.85%, with less than 1% accuracy loss and only 24.1 ns additional\ndelay. These results demonstrate an improved balance between computational\nspeed and resource utilization, making the divider well-suited for\nhigh-performance FPGA-based systems with strict resource constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGoldschmidt\u9664\u6cd5\u7b97\u6cd5\u548cMitchell\u4e58\u6cd5\u7b97\u6cd5\u7684\u53ef\u53d8\u4f4d\u5bbd\u5b9a\u70b9\u5feb\u901f\u9664\u6cd5\u5668\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\uff0c\u8ba1\u7b97\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\uff0c\u8d44\u6e90\u5229\u7528\u7387\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u7cbe\u5ea6\u9664\u6cd5\u5668\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u5728FPGA\u9ad8\u6027\u80fd\u7cfb\u7edf\u4e2d\u65e2\u80fd\u6ee1\u8db3\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u53c8\u80fd\u4fdd\u8bc1\u8ba1\u7b97\u901f\u5ea6\u548c\u51c6\u786e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Goldschmidt\u9664\u6cd5\u7b97\u6cd5\u7ed3\u5408Mitchell\u4e58\u6cd5\u7b97\u6cd5\uff0c\u901a\u8fc7Verilog HDL\u63cf\u8ff0\u5e76\u5728Xilinx XC7Z020-2CLG400I FPGA\u4e0a\u5b9e\u73b0\u3002", "result": "\u4e0e\u73b0\u6709\u5355\u7cbe\u5ea6\u9664\u6cd5\u5668\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u51cf\u5c11\u4e8631.7 ns\uff1b\u4e0e\u4f7f\u7528Vedic\u4e58\u6cd5\u5668\u7684Goldschmidt\u9664\u6cd5\u5668\u76f8\u6bd4\uff0c\u8d44\u6e90\u5360\u7528\u663e\u8457\u51cf\u5c11\uff0c\u4ec5\u589e\u52a024.1 ns\u5ef6\u8fdf\u4e14\u51c6\u786e\u7387\u635f\u5931\u4f4e\u4e8e1%\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u9ad8\u6027\u80fdFPGA\u7cfb\u7edf\u3002"}}
{"id": "2508.14096", "pdf": "https://arxiv.org/pdf/2508.14096", "abs": "https://arxiv.org/abs/2508.14096", "authors": ["Zhanxi Xie", "Baili Lu", "Yanzhao Gu", "Zikun Li", "Junhao Wei", "Ngai Cheong"], "title": "Research on UAV Applications in Public Administration: Based on an Improved RRT Algorithm", "categories": ["cs.RO"], "comment": null, "summary": "This study investigates the application of unmanned aerial vehicles (UAVs) in\npublic management, focusing on optimizing path planning to address challenges\nsuch as energy consumption, obstacle avoidance, and airspace constraints. As\nUAVs transition from 'technical tools' to 'governance infrastructure', driven\nby advancements in low-altitude economy policies and smart city demands,\nefficient path planning becomes critical. The research proposes an enhanced\nRapidly-exploring Random Tree algorithm (dRRT), incorporating four strategies:\nTarget Bias (to accelerate convergence), Dynamic Step Size (to balance\nexploration and obstacle navigation), Detour Priority (to prioritize horizontal\ndetours over vertical ascents), and B-spline smoothing (to enhance path\nsmoothness). Simulations in a 500 m3 urban environment with randomized\nbuildings demonstrate dRRT's superiority over traditional RRT, A*, and Ant\nColony Optimization (ACO). Results show dRRT achieves a 100\\% success rate with\nan average runtime of 0.01468s, shorter path lengths, fewer waypoints, and\nsmoother trajectories (maximum yaw angles <45{\\deg}). Despite improvements,\nlimitations include increased computational overhead from added mechanisms and\npotential local optima due to goal biasing. The study highlights dRRT's\npotential for efficient UAV deployment in public management scenarios like\nemergency response and traffic monitoring, while underscoring the need for\nintegration with real-time obstacle avoidance frameworks. This work contributes\nto interdisciplinary advancements in urban governance, robotics, and\ncomputational optimization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u5728\u516c\u5171\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u6539\u8fdb\u7684RRT\u7b97\u6cd5(dRRT)\uff0c\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u65e0\u4eba\u673a\u4ece\u6280\u672f\u5de5\u5177\u8f6c\u53d8\u4e3a\u6cbb\u7406\u57fa\u7840\u8bbe\u65bd\uff0c\u9700\u89e3\u51b3\u80fd\u8017\u3001\u907f\u969c\u548c\u7a7a\u57df\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fadRRT\u7b97\u6cd5\uff0c\u878d\u5408\u76ee\u6807\u504f\u5411\u3001\u52a8\u6001\u6b65\u957f\u3001\u7ed5\u884c\u4f18\u5148\u548cB\u6837\u6761\u5e73\u6ed1\u7b56\u7565\u3002", "result": "dRRT\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387100%\uff0c\u8def\u5f84\u66f4\u77ed\u3001\u66f4\u5e73\u6ed1\u3002", "conclusion": "dRRT\u9002\u5408\u7d27\u6025\u54cd\u5e94\u7b49\u573a\u666f\uff0c\u4f46\u9700\u7ed3\u5408\u5b9e\u65f6\u907f\u969c\u6846\u67b6\uff0c\u63a8\u52a8\u57ce\u5e02\u6cbb\u7406\u548c\u673a\u5668\u4eba\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2508.14637", "pdf": "https://arxiv.org/pdf/2508.14637", "abs": "https://arxiv.org/abs/2508.14637", "authors": ["Jinkun Yang", "Pengbin Xu"], "title": "Design of a Gm-C Dynamic Amplifier with High Linearity and High Temperature and Power Supply Voltage Stability", "categories": ["eess.SP"], "comment": "5 pages, 15 figures", "summary": "This paper presents a Gm-C dynamic amplifier with high linearity and high\ntemperature and power supply voltage stability. The main part of the amplifier\nemploys two asymmetric differential pairs to enhance transconductance\nlinearity. The amplifier maintains a nearly constant gain within a differential\ninput range of -40 mV to 40 mV, and achieves a total harmonic distortion (THD)\nof 70.5 dB. The bias part of the amplifier adopts a constant-gm bias circuit,\nwhich improves the temperature and supply voltage stability of the amplifier's\ntransconductance and gain. When the differential input is 1 mV, the power\nsupply voltage fluctuates by $\\pm$10%, and the temperature varies between\n-40$\\mathrm{^\\circ C}$ and 120$\\mathrm{^\\circ C}$, the standard deviation of\nthe gain distribution is 262m, and the distribution range is from 15.1 to 16.3.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7ebf\u6027\u5ea6\u3001\u9ad8\u6e29\u5ea6\u4e0e\u7535\u6e90\u7535\u538b\u7a33\u5b9a\u6027\u7684Gm-C\u52a8\u6001\u653e\u5927\u5668\uff0c\u901a\u8fc7\u91c7\u7528\u4e0d\u5bf9\u79f0\u5dee\u5206\u5bf9\u63d0\u5347\u7ebf\u6027\u5ea6\uff0c\u5e76\u5b9e\u73b0\u7a33\u5b9a\u7684\u589e\u76ca\u4e0e\u4f4e\u8c10\u6ce2\u5931\u771f\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u5728\u5bbd\u6e29\u5ea6\u8303\u56f4\u548c\u7535\u6e90\u7535\u538b\u6ce2\u52a8\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7ebf\u6027\u5ea6\u548c\u7a33\u5b9a\u589e\u76ca\u7684\u653e\u5927\u5668\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u4e0d\u5bf9\u79f0\u5dee\u5206\u5bf9\u63d0\u5347\u8de8\u5bfc\u7ebf\u6027\u5ea6\uff0c\u5e76\u4f7f\u7528\u6052\u5b9a\u8de8\u5bfc\u504f\u7f6e\u7535\u8def\u63d0\u9ad8\u6e29\u5ea6\u548c\u7535\u6e90\u7535\u538b\u7a33\u5b9a\u6027\u3002", "result": "\u5728-40 mV\u81f340 mV\u7684\u5dee\u5206\u8f93\u5165\u8303\u56f4\u5185\u4fdd\u6301\u7a33\u5b9a\u589e\u76ca\uff0cTHD\u4e3a70.5 dB\uff1b\u5728\u6e29\u5ea6-40\u00b0C\u81f3120\u00b0C\u548c\u7535\u6e90\u7535\u538b\u00b110%\u6ce2\u52a8\u4e0b\uff0c\u589e\u76ca\u6807\u51c6\u5dee\u4e3a262m\u3002", "conclusion": "\u8be5\u653e\u5927\u5668\u5728\u6076\u52a3\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u7ebf\u6027\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u5408\u9ad8\u7cbe\u5ea6\u5e94\u7528\u3002"}}
{"id": "2508.14098", "pdf": "https://arxiv.org/pdf/2508.14098", "abs": "https://arxiv.org/abs/2508.14098", "authors": ["Pranay Dugar", "Mohitvishnu S. Gadde", "Jonah Siekmann", "Yesh Godse", "Aayam Shrestha", "Alan Fern"], "title": "No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Humanoids operating in real-world workspaces must frequently execute\ntask-driven, short-range movements to SE(2) target poses. To be practical,\nthese transitions must be fast, robust, and energy efficient. While\nlearning-based locomotion has made significant progress, most existing methods\noptimize for velocity-tracking rather than direct pose reaching, resulting in\ninefficient, marching-style behavior when applied to short-range tasks. In this\nwork, we develop a reinforcement learning approach that directly optimizes\nhumanoid locomotion for SE(2) targets. Central to this approach is a new\nconstellation-based reward function that encourages natural and efficient\ntarget-oriented movement. To evaluate performance, we introduce a benchmarking\nframework that measures energy consumption, time-to-target, and footstep count\non a distribution of SE(2) goals. Our results show that the proposed approach\nconsistently outperforms standard methods and enables successful transfer from\nsimulation to hardware, highlighting the importance of targeted reward design\nfor practical short-range humanoid locomotion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f18\u5316\u4eba\u5f62\u673a\u5668\u4eba\u5728SE(2)\u76ee\u6807\u4e0b\u7684\u8fd0\u52a8\u8868\u73b0\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684\u5956\u52b1\u51fd\u6570\u63d0\u5347\u4e86\u6548\u7387\u548c\u81ea\u7136\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5b9e\u9645\u5de5\u4f5c\u4e2d\u9700\u9891\u7e41\u6267\u884c\u77ed\u8ddd\u79bb\u8fd0\u52a8\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u901f\u5ea6\u8ddf\u8e2a\u800c\u975e\u76f4\u63a5\u5230\u8fbe\u76ee\u6807\u4f4d\u59ff\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u661f\u5ea7\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u7136\u9ad8\u6548\u7684\u9762\u5411\u76ee\u6807\u7684\u8fd0\u52a8\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u80fd\u91cf\u6d88\u8017\u3001\u5230\u8fbe\u65f6\u95f4\u548c\u6b65\u6570\u4e0a\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u4ece\u6a21\u62df\u8fc1\u79fb\u5230\u786c\u4ef6\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5956\u52b1\u8bbe\u8ba1\u5bf9\u5b9e\u7528\u77ed\u8ddd\u79bb\u4eba\u5f62\u8fd0\u52a8\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.14739", "pdf": "https://arxiv.org/pdf/2508.14739", "abs": "https://arxiv.org/abs/2508.14739", "authors": ["Fatih Ayten", "Mehmet C. Ilter", "Akshay Jain", "Ossi Kaltiokallio", "Jukka Talvitie", "Elena Simona Lohan", "Henk Wymeersch", "Mikko Valkama"], "title": "Failure Tolerant Phase-Only Indoor Positioning via Deep Learning", "categories": ["eess.SP"], "comment": null, "summary": "High-precision localization turns into a crucial added value and asset for\nnext-generation wireless systems. Carrier phase positioning (CPP) enables\nsub-meter to centimeter-level accuracy and is gaining interest in 5G-Advanced\nstandardization. While CPP typically complements time-of-arrival (ToA)\nmeasurements, recent literature has introduced a phase-only positioning\napproach in a distributed antenna/MIMO system context with minimal bandwidth\nrequirements, using deep learning (DL) when operating under ideal hardware\nassumptions. In more practical scenarios, however, antenna failures can largely\ndegrade the performance. In this paper, we address the challenging phase-only\npositioning task, and propose a new DL-based localization approach harnessing\nthe so-called hyperbola intersection principle, clearly outperforming the\nprevious methods. Additionally, we consider and propose a processing and\nlearning mechanism that is robust to antenna element failures. Our results show\nthat the proposed DL model achieves robust and accurate positioning despite\nantenna impairments, demonstrating the viability of data-driven,\nimpairment-tolerant phase-only positioning mechanisms. Comprehensive set of\nnumerical results demonstrates large improvements in localization accuracy\nagainst the prior art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76f8\u4f4d\u5b9a\u4f4d\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u53cc\u66f2\u7ebf\u76f8\u4ea4\u539f\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u5728\u5929\u7ebf\u6545\u969c\u65f6\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "motivation": "\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u5bf9\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u6280\u672f\u5728\u975e\u7406\u60f3\u786c\u4ef6\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u76f8\u4f4d\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u66f2\u7ebf\u76f8\u4ea4\u539f\u7406\u548c\u9c81\u68d2\u7684\u5b66\u4e60\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u5929\u7ebf\u6545\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u5929\u7ebf\u6545\u969c\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u7684\u76f8\u4f4d\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5177\u6709\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2508.14099", "pdf": "https://arxiv.org/pdf/2508.14099", "abs": "https://arxiv.org/abs/2508.14099", "authors": ["Michal Ciebielski", "Victor Dh\u00e9din", "Majid Khadiv"], "title": "Task and Motion Planning for Humanoid Loco-manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This work presents an optimization-based task and motion planning (TAMP)\nframework that unifies planning for locomotion and manipulation through a\nshared representation of contact modes. We define symbolic actions as contact\nmode changes, grounding high-level planning in low-level motion. This enables a\nunified search that spans task, contact, and motion planning while\nincorporating whole-body dynamics, as well as all constraints between the\nrobot, the manipulated object, and the environment. Results on a humanoid\nplatform show that our method can generate a broad range of physically\nconsistent loco-manipulation behaviors over long action sequences requiring\ncomplex reasoning. To the best of our knowledge, this is the first work that\nenables the resolution of an integrated TAMP formulation with fully acyclic\nplanning and whole body dynamics with actuation constraints for the humanoid\nloco-manipulation problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u63a5\u89e6\u6a21\u5f0f\u7684\u5171\u4eab\u8868\u793a\u7edf\u4e00\u4e86\u79fb\u52a8\u4e0e\u64cd\u4f5c\u4efb\u52a1\u7684\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u5c06\u9ad8\u5c42\u6b21\u7684\u89c4\u5212\u4e0e\u4f4e\u5c42\u6b21\u7684\u8fd0\u52a8\u901a\u8fc7\u63a5\u89e6\u6a21\u5f0f\u53d8\u5316\u5173\u8054\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u641c\u7d22\uff0c\u6db5\u76d6\u4efb\u52a1\u3001\u63a5\u89e6\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u540c\u65f6\u8003\u8651\u5168\u8eab\u52a8\u529b\u5b66\u548c\u6240\u6709\u7ea6\u675f\u3002", "method": "\u5b9a\u4e49\u4e86\u63a5\u89e6\u6a21\u5f0f\u53d8\u5316\u7684\u7b26\u53f7\u52a8\u4f5c\uff0c\u5c06\u9ad8\u5c42\u6b21\u7684\u89c4\u5212\u4e0e\u4f4e\u5c42\u6b21\u8fd0\u52a8\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u641c\u7d22\u3002", "result": "\u5728\u4eff\u4eba\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u590d\u6742\u7684\u79fb\u52a8\u4e0e\u64cd\u4f5c\u884c\u4e3a\uff0c\u4e14\u7269\u7406\u4e00\u81f4\u6027\u9ad8\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u89e3\u51b3\u4eff\u4eba\u673a\u5668\u4eba\u79fb\u52a8\u4e0e\u64cd\u4f5c\u95ee\u9898\u7684\u5b8c\u6574TAMP\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u65e0\u73af\u89c4\u5212\u4e0e\u5168\u8eab\u52a8\u529b\u5b66\u7ea6\u675f\u7684\u7ed3\u5408\u3002"}}
{"id": "2508.14753", "pdf": "https://arxiv.org/pdf/2508.14753", "abs": "https://arxiv.org/abs/2508.14753", "authors": ["Ahsan Nazar", "Zhambyl Shaikhanov", "Sennur Ulukus"], "title": "Full-Duplex Beamforming Optimization for Near-Field ISAC", "categories": ["eess.SP"], "comment": null, "summary": "Integrated Sensing and Communications (ISAC) is a promising technology for\nfuture wireless networks, enabling simultaneous communication and sensing using\nshared resources. This paper investigates the performance of full-duplex (FD)\ncommunication in near-field ISAC systems, where spherical-wave propagation\nintroduces unique beam-focusing capabilities. We propose a joint optimization\nframework for transmit and receive beamforming at the base station to minimize\ntransmit power while satisfying rate constraints for multi-user downlink\ntransmission, multi-user uplink reception, and multi-target sensing. Our\napproach employs alternating optimization combined with semidefinite relaxation\nand Rayleigh quotient techniques to address the non-convexity of the problem.\nSimulation results demonstrate that FD-enabled near-field ISAC achieves\nsuperior power efficiency compared to half-duplex and far-field benchmarks,\neffectively detecting targets at identical angles while meeting communication\nrequirements.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u5168\u53cc\u5de5\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u6846\u67b6\u4ee5\u5b9e\u73b0\u4f4e\u529f\u8017\u548c\u9ad8\u6548\u7387\u7684\u901a\u4fe1\u4e0e\u611f\u77e5\u3002", "motivation": "\u63a2\u7d22\u5168\u53cc\u5de5\u901a\u4fe1\u5728\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u591a\u7528\u6237\u901a\u4fe1\u548c\u591a\u76ee\u6807\u611f\u77e5\u7684\u8d44\u6e90\u5171\u4eab\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u3001\u534a\u5b9a\u677e\u5f1b\u548cRayleigh\u5546\u6280\u672f\uff0c\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u53d1\u5c04\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5168\u53cc\u5de5\u8fd1\u573aISAC\u5728\u529f\u7387\u6548\u7387\u548c\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u534a\u53cc\u5de5\u548c\u8fdc\u573a\u7cfb\u7edf\u3002", "conclusion": "\u5168\u53cc\u5de5\u8fd1\u573aISAC\u80fd\u591f\u9ad8\u6548\u5b9e\u73b0\u901a\u4fe1\u4e0e\u611f\u77e5\u76ee\u6807\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.14100", "pdf": "https://arxiv.org/pdf/2508.14100", "abs": "https://arxiv.org/abs/2508.14100", "authors": ["Nilay Kushawaha", "Carlo Alessi", "Lorenzo Fruzzetti", "Egidio Falotico"], "title": "Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted at IEEE International Conference on Robotic Systems and\n  Applications", "summary": "Deep learning provides a powerful method for modeling the dynamics of soft\nrobots, offering advantages over traditional analytical approaches that require\nprecise knowledge of the robot's structure, material properties, and other\nphysical characteristics. Given the inherent complexity and non-linearity of\nthese systems, extracting such details can be challenging. The mappings learned\nin one domain cannot be directly transferred to another domain with different\nphysical properties. This challenge is particularly relevant for soft robots,\nas their materials gradually degrade over time. In this paper, we introduce a\ndomain translation framework based on a conditional cycle generative\nadversarial network (CCGAN) to enable knowledge transfer from a source domain\nto a target domain. Specifically, we employ a dynamic learning approach to\nadapt a pose controller trained in a standard simulation environment to a\ndomain with tenfold increased viscosity. Our model learns from input pressure\nsignals conditioned on corresponding end-effector positions and orientations in\nboth domains. We evaluate our approach through trajectory-tracking experiments\nacross five distinct shapes and further assess its robustness under noise\nperturbations and periodicity tests. The results demonstrate that CCGAN-GP\neffectively facilitates cross-domain skill transfer, paving the way for more\nadaptable and generalizable soft robotic controllers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5faa\u73af\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08CCGAN\uff09\u7684\u9886\u57df\u7ffb\u8bd1\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u9700\u8981\u7cbe\u786e\u7684\u673a\u5668\u4eba\u7ed3\u6784\u3001\u6750\u6599\u7279\u6027\u7b49\u7269\u7406\u7279\u5f81\uff0c\u800c\u8f6f\u673a\u5668\u4eba\u7684\u975e\u7ebf\u6027\u548c\u6750\u6599\u968f\u65f6\u95f4\u9000\u5316\u4f7f\u5f97\u8fd9\u7c7b\u65b9\u6cd5\u53d7\u9650\u3002\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u9886\u57df\u95f4\u76f4\u63a5\u8fc1\u79fb\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528CCGAN\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u65b9\u6cd5\u5c06\u59ff\u6001\u63a7\u5236\u5668\u4ece\u6807\u51c6\u6a21\u62df\u73af\u5883\u8fc1\u79fb\u81f3\u7c98\u5ea6\u589e\u52a0\u5341\u500d\u7684\u9886\u57df\u3002\u6a21\u578b\u901a\u8fc7\u8f93\u5165\u538b\u529b\u4fe1\u53f7\u5e76\u7ed3\u5408\u672b\u7aef\u6267\u884c\u5668\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u79cd\u4e0d\u540c\u5f62\u72b6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u5728\u566a\u58f0\u5e72\u6270\u548c\u5468\u671f\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7ed3\u679c\u8868\u660eCCGAN-GP\u80fd\u6709\u6548\u4fc3\u8fdb\u8de8\u9886\u57df\u6280\u80fd\u8fc1\u79fb\u3002", "conclusion": "CCGAN-GP\u4e3a\u8f6f\u673a\u5668\u4eba\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.14884", "pdf": "https://arxiv.org/pdf/2508.14884", "abs": "https://arxiv.org/abs/2508.14884", "authors": ["Brian Kim", "Justin H. Kong", "Terrence J. Moore", "Fikadu T. Dagefu"], "title": "Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks", "categories": ["eess.SP"], "comment": null, "summary": "Routing in multi-hop wireless networks is a complex problem, especially in\nheterogeneous networks where multiple wireless communication technologies\ncoexist. Reinforcement learning (RL) methods, such as Q-learning, have been\nintroduced for decentralized routing by allowing nodes to make decisions based\non local observations. However, Q-learning suffers from scalability issues and\npoor generalization due to the difficulty in managing the Q-table in large or\ndynamic network topologies, especially in heterogeneous networks (HetNets) with\ndiverse channel characteristics. Thus, in this paper, we propose a novel deep\nQ-network (DQN)-based routing framework for heterogeneous multi-hop wireless\nnetworks to maximize the end-to-end rate of the route by improving scalability\nand adaptability, where each node uses a deep neural network (DNN) to estimate\nthe Q-values and jointly select the next-hop relay and a communication\ntechnology for transmission. To achieve better performance with the DNN,\nselecting which nodes to exchange information is critical, as it not only\ndefines the state and action spaces but also determines the input to the DNN.\nTo this end, we propose neighbor node selection strategies based on channel\ngain and rate between nodes rather than a simple distance-based approach for an\nimproved set of states and actions for DQN-based routing. During training, the\nmodel experiences diverse network topologies to ensure generalization and\nrobustness, and simulation results show that the proposed neighbor node\nselection outperforms simple distance-based selection. Further, we observe that\nthe DQN-based approach outperforms various benchmark schemes and performs\ncomparably to the optimal approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u591a\u8df3\u65e0\u7ebf\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u90bb\u5c45\u8282\u70b9\u9009\u62e9\u7b56\u7565\u548cDNN\u4f30\u8ba1Q\u503c\uff0c\u63d0\u9ad8\u8def\u7531\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "Q\u5b66\u4e60\u5728\u591a\u8df3\u5f02\u6784\u7f51\u7edc\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8def\u7531\u65b9\u6cd5\u3002", "method": "\u91c7\u7528DQN\u6846\u67b6\uff0c\u7ed3\u5408DNN\u4f30\u8ba1Q\u503c\uff0c\u5e76\u57fa\u4e8e\u4fe1\u9053\u589e\u76ca\u548c\u901f\u7387\u9009\u62e9\u90bb\u5c45\u8282\u70b9\u3002", "result": "DQN\u65b9\u6cd5\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u5f02\u6784\u7f51\u7edc\u4e2d\u5177\u6709\u66f4\u597d\u7684\u8def\u7531\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.14105", "pdf": "https://arxiv.org/pdf/2508.14105", "abs": "https://arxiv.org/abs/2508.14105", "authors": ["Jahid Chowdhury Choton", "John Woods", "William Hsu"], "title": "Efficient Environment Design for Multi-Robot Navigation via Continuous Control", "categories": ["cs.RO"], "comment": "12 pages, 3 figures, conference", "summary": "Multi-robot navigation and path planning in continuous state and action\nspaces with uncertain environments remains an open challenge. Deep\nReinforcement Learning (RL) is one of the most popular paradigms for solving\nthis task, but its real-world application has been limited due to sample\ninefficiency and long training periods. Moreover, the existing works using RL\nfor multi-robot navigation lack formal guarantees while designing the\nenvironment. In this paper, we introduce an efficient and highly customizable\nenvironment for continuous-control multi-robot navigation, where the robots\nmust visit a set of regions of interest (ROIs) by following the shortest paths.\nThe task is formally modeled as a Markov Decision Process (MDP). We describe\nthe multi-robot navigation task as an optimization problem and relate it to\nfinding an optimal policy for the MDP. We crafted several variations of the\nenvironment and measured the performance using both gradient and non-gradient\nbased RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world\napplicability, we deployed our environment to a 3-D agricultural field with\nuncertainties using the CoppeliaSim robot simulator and measured the robustness\nby running inference on the learned models. We believe our work will guide the\nresearchers on how to develop MDP-based environments that are applicable to\nreal-world systems and solve them using the existing state-of-the-art RL\nmethods with limited resources and within reasonable time periods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u5b9a\u5236\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\u73af\u5883\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u79cdRL\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5bfc\u822a\u4e0e\u8def\u5f84\u89c4\u5212\u7684\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edfRL\u65b9\u6cd5\u7684\u6837\u672c\u4f4e\u6548\u548c\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eMDP\u7684\u5bfc\u822a\u4efb\u52a1\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u5229\u7528A2C\u3001PPO\u3001TRPO\u7b49\u68af\u5ea6\u4e0e\u975e\u68af\u5ea6RL\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u73af\u5883\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u9002\u7528\u4e8e\u5b9e\u9645\u519c\u4e1a\u573a\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u7cfb\u7edf\u7684MDP\u73af\u5883\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6709\u9650\u8d44\u6e90\u548c\u5408\u7406\u65f6\u95f4\u5185\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.14060", "pdf": "https://arxiv.org/pdf/2508.14060", "abs": "https://arxiv.org/abs/2508.14060", "authors": ["Kartik Pandey", "Arun Balasubramanian", "Debasis Samanta"], "title": "Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach", "categories": ["q-bio.NC", "cs.CV", "cs.HC", "cs.LG", "eess.SP"], "comment": null, "summary": "Electroencephalogram (EEG) signals have gained widespread adoption in\nbrain-computer interface (BCI) applications due to their non-invasive,\nlow-cost, and relatively simple acquisition process. The demand for higher\nspatial resolution, particularly in clinical settings, has led to the\ndevelopment of high-density electrode arrays. However, increasing the number of\nchannels introduces challenges such as cross-channel interference and\ncomputational overhead. To address these issues, modern BCI systems often\nemploy channel selection algorithms. Existing methods, however, are typically\ntask-specific and require re-optimization for each new application. This work\nproposes a task-agnostic channel selection method, Activity Coefficient-based\nChannel Selection (ACCS), which uses a novel metric called the Channel Activity\nCoefficient (CAC) to quantify channel utility based on activity levels. By\nselecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97%\nimprovement in multi-class classification accuracy. Unlike traditional\napproaches, ACCS identifies a reusable set of informative channels independent\nof the downstream task or model, making it highly adaptable for diverse\nEEG-based applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u8111\u7535\u56fe\uff08EEG\uff09\u901a\u9053\u9009\u62e9\u65b9\u6cd5ACCS\uff0c\u901a\u8fc7\u65b0\u9896\u7684CAC\u6307\u6807\u91cf\u5316\u901a\u9053\u6548\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7c7b\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u7535\u6781\u9635\u5217\u7684\u9700\u6c42\u5e26\u6765\u4e86\u8de8\u901a\u9053\u5e72\u6270\u548c\u8ba1\u7b97\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u65b0\u4efb\u52a1\u91cd\u65b0\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u901a\u9053\u6d3b\u52a8\u7cfb\u6570\uff08CAC\uff09\u7684\u4efb\u52a1\u65e0\u5173\u65b9\u6cd5ACCS\uff0c\u901a\u8fc7\u9009\u62e9CAC\u6392\u540d\u524d16\u7684\u901a\u9053\u6765\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "ACCS\u65b9\u6cd5\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe34.97%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "ACCS\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u53ef\u91cd\u7528\u7684\u4fe1\u606f\u901a\u9053\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684EEG\u5e94\u7528\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u9002\u5e94\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.14120", "pdf": "https://arxiv.org/pdf/2508.14120", "abs": "https://arxiv.org/abs/2508.14120", "authors": ["Yuhang Lin", "Yijia Xie", "Jiahong Xie", "Yuehao Huang", "Ruoyu Wang", "Jiajun Lv", "Yukai Ma", "Xingxing Zuo"], "title": "SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generating physically realistic humanoid-object interactions (HOI) is a\nfundamental challenge in robotics. Existing HOI generation approaches, such as\ndiffusion-based models, often suffer from artifacts such as implausible\ncontacts, penetrations, and unrealistic whole-body actions, which hinder\nsuccessful execution in physical environments. To address these challenges, we\nintroduce SimGenHOI, a unified framework that combines the strengths of\ngenerative modeling and reinforcement learning to produce controllable and\nphysically plausible HOI. Our HOI generative model, based on Diffusion\nTransformers (DiT), predicts a set of key actions conditioned on text prompts,\nobject geometry, sparse object waypoints, and the initial humanoid pose. These\nkey actions capture essential interaction dynamics and are interpolated into\nsmooth motion trajectories, naturally supporting long-horizon generation. To\nensure physical realism, we design a contact-aware whole-body control policy\ntrained with reinforcement learning, which tracks the generated motions while\ncorrecting artifacts such as penetration and foot sliding. Furthermore, we\nintroduce a mutual fine-tuning strategy, where the generative model and the\ncontrol policy iteratively refine each other, improving both motion realism and\ntracking robustness. Extensive experiments demonstrate that SimGenHOI generates\nrealistic, diverse, and physically plausible humanoid-object interactions,\nachieving significantly higher tracking success rates in simulation and\nenabling long-horizon manipulation tasks. Code will be released upon acceptance\non our project page: https://xingxingzuo.github.io/simgen_hoi.", "AI": {"tldr": "SimGenHOI\u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7269\u7406\u53ef\u4fe1\u7684\u4eba\u673a\u4ea4\u4e92\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u4e0d\u5408\u7406\u63a5\u89e6\u548c\u7a7f\u900f\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u751f\u6210\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff09\u5b58\u5728\u4e0d\u5408\u7406\u7684\u63a5\u89e6\u3001\u7a7f\u900f\u548c\u5168\u8eab\u52a8\u4f5c\u4e0d\u771f\u5b9e\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u6210\u529f\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86SimGenHOI\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u751f\u6210\u7684\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u3001\u5bf9\u8c61\u51e0\u4f55\u548c\u7a00\u758f\u8def\u5f84\u7684\u5173\u952e\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a5\u89e6\u611f\u77e5\u7684\u5168\u8eab\u63a7\u5236\u7b56\u7565\u3002", "result": "SimGenHOI\u80fd\u591f\u751f\u6210\u771f\u5b9e\u3001\u591a\u6837\u4e14\u7269\u7406\u53ef\u4fe1\u7684\u4eba\u673a\u4ea4\u4e92\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u6210\u529f\u7387\uff0c\u5e76\u652f\u6301\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "SimGenHOI\u901a\u8fc7\u751f\u6210\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u52a8\u4f5c\u7684\u771f\u5b9e\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\u3002"}}
{"id": "2508.14069", "pdf": "https://arxiv.org/pdf/2508.14069", "abs": "https://arxiv.org/abs/2508.14069", "authors": ["Chinmoy Biswas", "Nafis Faisal", "Vivek Chowdhury", "Abrar Al-Shadid Abir", "Sabir Mahmud", "Mithon Rahman", "Shaikh Anowarul Fattah", "Hafiz Imtiaz"], "title": "Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation", "categories": ["cs.LG", "eess.SP"], "comment": "Under review in Elsevier Electric Power Systems Research", "summary": "Sparsity, defined as the presence of missing or zero values in a dataset,\noften poses a major challenge while operating on real-life datasets. Sparsity\nin features or target data of the training dataset can be handled using various\ninterpolation methods, such as linear or polynomial interpolation, spline,\nmoving average, or can be simply imputed. Interpolation methods usually perform\nwell with Strict Sense Stationary (SSS) data. In this study, we show that an\napproximately 62\\% sparse dataset with hourly load data of a power plant can be\nutilized for load forecasting assuming the data is Wide Sense Stationary (WSS),\nif augmented with Gaussian interpolation. More specifically, we perform\nstatistical analysis on the data, and train multiple machine learning and deep\nlearning models on the dataset. By comparing the performance of these models,\nwe empirically demonstrate that Gaussian interpolation is a suitable option for\ndealing with load forecasting problems. Additionally, we demonstrate that Long\nShort-term Memory (LSTM)-based neural network model offers the best performance\namong a diverse set of classical and neural network-based models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7a00\u758f\u6570\u636e\uff08\u7ea662%\u7f3a\u5931\uff09\u5728\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u9ad8\u65af\u63d2\u503c\u5904\u7406\u7a00\u758f\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660eLSTM\u6a21\u578b\u5728\u591a\u79cd\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u7684\u7a00\u758f\u6027\uff08\u7f3a\u5931\u6216\u96f6\u503c\uff09\u5bf9\u6570\u636e\u5904\u7406\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u5728\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u9002\u5408\u7a00\u758f\u6570\u636e\u7684\u63d2\u503c\u65b9\u6cd5\u53ca\u6700\u4f18\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u5bf9\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff0c\u4f7f\u7528\u9ad8\u65af\u63d2\u503c\u5904\u7406\u7a00\u758f\u6027\uff0c\u5e76\u8bad\u7ec3\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecLSTM\uff09\u8fdb\u884c\u8d1f\u8377\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8\u65af\u63d2\u503c\u9002\u7528\u4e8e\u7a00\u758f\u6570\u636e\uff0c\u4e14LSTM\u6a21\u578b\u5728\u5404\u7c7b\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u9ad8\u65af\u63d2\u503c\u7ed3\u5408LSTM\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u7535\u529b\u8d1f\u8377\u9884\u6d4b\u4e2d\u7684\u7a00\u758f\u6570\u636e\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.14185", "pdf": "https://arxiv.org/pdf/2508.14185", "abs": "https://arxiv.org/abs/2508.14185", "authors": ["Evanns Morales-Cuadrado", "Luke Baird", "Yorai Wardi", "Samuel Coogan"], "title": "Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We investigate the performance of a lightweight tracking controller, based on\na flow version of the Newton-Raphson method, applied to a miniature blimp and a\nmid-size quadrotor. This tracking technique has been shown to enjoy theoretical\nguarantees of performance and has been applied with success in simulation\nstudies and on mobile robots with simple motion models. This paper investigates\nthe technique through real-world flight experiments on aerial hardware\nplatforms subject to realistic deployment and onboard computational\nconstraints. The technique's performance is assessed in comparison with the\nestablished control frameworks of feedback linearization for the blimp, and\nnonlinear model predictive control for both quadrotor and blimp. The\nperformance metrics under consideration are (i) root mean square error of\nflight trajectories with respect to target trajectories, (ii) algorithms'\ncomputation times, and (iii) CPU energy consumption associated with the control\nalgorithms. The experimental findings show that the Newton-Raphson flow-based\ntracking controller achieves comparable or superior tracking performance to the\nbaseline methods with substantially reduced computation time and energy\nexpenditure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u725b\u987f-\u62c9\u592b\u68ee\u6d41\u7684\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5728\u8ff7\u4f60\u98de\u8247\u548c\u4e2d\u578b\u56db\u65cb\u7ffc\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u5176\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u80fd\u8017\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u63a7\u5236\u5668\u5728\u771f\u5b9e\u98de\u884c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u9650\u5236\u3002", "method": "\u91c7\u7528\u725b\u987f-\u62c9\u592b\u68ee\u6d41\u65b9\u6cd5\u8bbe\u8ba1\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5e76\u4e0e\u53cd\u9988\u7ebf\u6027\u5316\uff08\u98de\u8247\uff09\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08\u56db\u65cb\u7ffc\u53ca\u98de\u8247\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8ddf\u8e2a\u6027\u80fd\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u548c\u80fd\u8017\u3002", "conclusion": "\u725b\u987f-\u62c9\u592b\u68ee\u6d41\u8ddf\u8e2a\u63a7\u5236\u5668\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8282\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u53d7\u9650\u7684\u98de\u884c\u5e73\u53f0\u3002"}}
{"id": "2508.14115", "pdf": "https://arxiv.org/pdf/2508.14115", "abs": "https://arxiv.org/abs/2508.14115", "authors": ["Taous Iatariene", "Alexandre Gu\u00e9rin", "Romain Serizel"], "title": "Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": null, "summary": "Speaker embeddings are promising identity-related features that can enhance\nthe identity assignment performance of a tracking system by leveraging its\nspatial predictions, i.e, by performing identity reassignment. Common speaker\nembedding extractors usually struggle with short temporal contexts and\noverlapping speech, which imposes long-term identity reassignment to exploit\nlonger temporal contexts. However, this increases the probability of tracking\nsystem errors, which in turn impacts negatively on identity reassignment. To\naddress this, we propose a Knowledge Distillation (KD) based training approach\nfor short context speaker embedding extraction from two speaker mixtures. We\nleverage the spatial information of the speaker of interest using beamforming\nto reduce overlap. We study the feasibility of performing identity reassignment\nover blocks of fixed size, i.e., blockwise identity reassignment, to go towards\na low-latency speaker embedding based tracking system. Results demonstrate that\nour distilled models are effective at short-context embedding extraction and\nmore robust to overlap. Although, blockwise reassignment results indicate that\nfurther work is needed to handle simultaneous speech more effectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u7684\u77ed\u4e0a\u4e0b\u6587\u8bf4\u8bdd\u4eba\u5d4c\u5165\u63d0\u53d6\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8bf4\u8bdd\u4eba\u5d4c\u5165\u63d0\u53d6\u5668\u5728\u77ed\u65f6\u4e0a\u4e0b\u6587\u548c\u91cd\u53e0\u8bed\u97f3\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u51cf\u5c11\u91cd\u53e0\uff0c\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e86\u5757\u72b6\u8eab\u4efd\u91cd\u5206\u914d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5728\u63d0\u5347\u8ddf\u8e2a\u7cfb\u7edf\u7684\u8eab\u4efd\u5206\u914d\u6027\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u63d0\u53d6\u5668\u5728\u77ed\u65f6\u4e0a\u4e0b\u6587\u548c\u91cd\u53e0\u8bed\u97f3\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8eab\u4efd\u91cd\u5206\u914d\u6548\u679c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u8bf4\u8bdd\u4eba\u6df7\u5408\u4e2d\u63d0\u53d6\u77ed\u4e0a\u4e0b\u6587\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u5e76\u5229\u7528\u6ce2\u675f\u6210\u5f62\u51cf\u5c11\u91cd\u53e0\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u5757\u72b6\u8eab\u4efd\u91cd\u5206\u914d\u7684\u53ef\u884c\u6027\u3002", "result": "\u84b8\u998f\u6a21\u578b\u5728\u77ed\u4e0a\u4e0b\u6587\u5d4c\u5165\u63d0\u53d6\u4e2d\u8868\u73b0\u826f\u597d\u4e14\u5bf9\u91cd\u53e0\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u5757\u72b6\u91cd\u5206\u914d\u7ed3\u679c\u663e\u793a\u4ecd\u9700\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u5904\u7406\u540c\u65f6\u8bed\u97f3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u77ed\u4e0a\u4e0b\u6587\u5d4c\u5165\u63d0\u53d6\u548c\u51cf\u5c11\u91cd\u53e0\u65b9\u9762\u6709\u6548\uff0c\u4f46\u5757\u72b6\u91cd\u5206\u914d\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.14235", "pdf": "https://arxiv.org/pdf/2508.14235", "abs": "https://arxiv.org/abs/2508.14235", "authors": ["Omar Mostafa", "Nikolaos Evangeliou", "Anthony Tzes"], "title": "SLAM-based Safe Indoor Exploration Strategy", "categories": ["cs.RO"], "comment": "5 pages, 8 figures. Published in the 2025 11th International\n  Conference on Automation, Robotics, and Applications (ICARA)", "summary": "This paper suggests a 2D exploration strategy for a planar space cluttered\nwith obstacles. Rather than using point robots capable of adjusting their\nposition and altitude instantly, this research is tailored to classical agents\nwith circular footprints that cannot control instantly their pose. Inhere, a\nself-balanced dual-wheeled differential drive system is used to explore the\nplace. The system is equipped with linear accelerometers and angular\ngyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs\nRTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop\nclosures. The mobile agent explores the planar space using a safe skeleton\napproach that places the agent as far as possible from the static obstacles.\nDuring the exploration strategy, the heading is towards any offered openings of\nthe space. This space exploration strategy has as its highest priority the\nagent's safety in avoiding the obstacles followed by the exploration of\nundetected space. Experimental studies with a ROS-enabled mobile agent are\npresented indicating the path planning strategy while exploring the space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e73\u9762\u969c\u788d\u7269\u7a7a\u95f4\u76842D\u63a2\u7d22\u7b56\u7565\uff0c\u5229\u7528\u53cc\u8f6e\u5dee\u901f\u9a71\u52a8\u7cfb\u7edf\u8fdb\u884c\u63a2\u7d22\uff0c\u4f18\u5148\u8003\u8651\u907f\u969c\u5b89\u5168\u6027\u548c\u672a\u63a2\u6d4b\u7a7a\u95f4\u7684\u63a2\u7d22\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u5706\u5f62\u8db3\u8ff9\u673a\u5668\u4eba\u65e0\u6cd5\u5373\u65f6\u8c03\u6574\u59ff\u6001\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u79cd\u5728\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u53cc\u8f6e\u5dee\u901f\u9a71\u52a8\u7cfb\u7edf\uff0c\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\uff08IMU\u30013D-LiDAR\u3001RGB-D\u76f8\u673a\uff09\uff0c\u91c7\u7528RTAB-SLAM\u548c\u9aa8\u9abc\u5316\u5b89\u5168\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u591f\u6709\u6548\u89c4\u5212\u8def\u5f84\uff0c\u786e\u4fdd\u907f\u969c\u5b89\u5168\u5e76\u63a2\u7d22\u672a\u63a2\u6d4b\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u969c\u788d\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14224", "pdf": "https://arxiv.org/pdf/2508.14224", "abs": "https://arxiv.org/abs/2508.14224", "authors": ["Christoph Sachs", "Martin Neuburger"], "title": "A Data-Based Review of Battery Electric Vehicle and Traction Inverter Trends", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "8 pages, 9 figures, 2025 IECON 51st Annual Conference of the IEEE IES", "summary": "Battery electric vehicles (BEVs) have advanced significantly during the past\ndecade, yet drivetrain energy losses continue to restrict practical range and\nelevate cost. A dataset comprising more than 1000 European-market BEVs (model\nyears 2010-2025) is combined with detailed inverter-motor co-simulation to\nchart technology progress for and quantify the efficiency and cost-saving\npotential of partial-load optimised multi-level inverter (MLI) for 2030.\nAverage drive-cycle range has climbed from 135 km to 455 km, while\nfleet-average energy consumption has remained virtually constant. Three\ninverter topologies are assessed to evaluate future efficiency and cost\nenhancements: a conventional two-level (2L) six halfbridge (B6) inverter with\nsilicon (Si) and silicon carbide (SiC) devices, and two three-level (3L) T-type\nneutral point clamped (TNPC) and active neutral point clamped (ANPC) inverters\ntailored for partial-load operation. The 3L-TNPC inverter, realised with only\n30% additional SiC chip area, lowers drive-cycle drivetrain losses by 0.67\nkWh/100 km relative to a SiC 2L-B6 baseline. These results identify\npartial-load optimised MLIs as a cost-effective route to further reduce BEV\nenergy consumption and total system cost.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86BEV\u7684\u80fd\u6548\u635f\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u7535\u5e73\u9006\u53d8\u5668\uff08MLI\uff09\u4f5c\u4e3a\u964d\u4f4e\u80fd\u8017\u548c\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "BEV\u7684\u9a71\u52a8\u7cfb\u7edf\u80fd\u91cf\u635f\u5931\u9650\u5236\u4e86\u5b9e\u9645\u7eed\u822a\u91cc\u7a0b\u5e76\u589e\u52a0\u4e86\u6210\u672c\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u54081000\u591a\u8f86\u6b27\u6d32\u5e02\u573aBEV\u7684\u6570\u636e\u548c\u9006\u53d8\u5668-\u7535\u673a\u534f\u540c\u4eff\u771f\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u9006\u53d8\u5668\u62d3\u6251\u7ed3\u6784\u5728\u80fd\u6548\u548c\u6210\u672c\u4e0a\u7684\u6f5c\u529b\u3002", "result": "3L-TNPC\u9006\u53d8\u5668\u53ef\u964d\u4f4e\u9a71\u52a8\u5faa\u73af\u635f\u80170.67 kWh/100 km\uff0c\u4e14\u53ea\u970030%\u989d\u5916\u7684SiC\u82af\u7247\u9762\u79ef\u3002", "conclusion": "\u90e8\u5206\u8d1f\u8f7d\u4f18\u5316\u7684MLI\u662f\u964d\u4f4eBEV\u80fd\u8017\u548c\u7cfb\u7edf\u6210\u672c\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.14258", "pdf": "https://arxiv.org/pdf/2508.14258", "abs": "https://arxiv.org/abs/2508.14258", "authors": ["Daegyun Choi", "Alhim Vera", "Donghoon Kim"], "title": "Adapting Biological Reflexes for Dynamic Reorientation in Space Manipulator Systems", "categories": ["cs.RO", "physics.bio-ph"], "comment": "18 pages, 11 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "Robotic arms mounted on spacecraft, known as space manipulator systems\n(SMSs), are critical for enabling on-orbit assembly, satellite servicing, and\ndebris removal. However, controlling these systems in microgravity remains a\nsignificant challenge due to the dynamic coupling between the manipulator and\nthe spacecraft base. This study explores the potential of using biological\ninspiration to address this issue, focusing on animals, particularly lizards,\nthat exhibit mid-air righting reflexes. Based on similarities between SMSs and\nthese animals in terms of behavior, morphology, and environment, their\nair-righting motion trajectories are extracted from high-speed video recordings\nusing computer vision techniques. These trajectories are analyzed within a\nmulti-objective optimization framework to identify the key behavioral goals and\nassess their relative importance. The resulting motion profiles are then\napplied as reference trajectories for SMS control, with baseline controllers\nused to track them. The findings provide a step toward translating evolved\nanimal behaviors into interpretable, adaptive control strategies for space\nrobotics, with implications for improving maneuverability and robustness in\nfuture missions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u7269\uff08\u5982\u8725\u8734\uff09\u7684\u7a7a\u4e2d\u8c03\u6574\u53cd\u5c04\u6765\u6539\u8fdb\u592a\u7a7a\u673a\u68b0\u81c2\u7684\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u7a7a\u95f4\u673a\u68b0\u81c2\u7cfb\u7edf\uff08SMS\uff09\u63a7\u5236\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u673a\u68b0\u81c2\u4e0e\u822a\u5929\u5668\u57fa\u5ea7\u4e4b\u95f4\u7684\u52a8\u6001\u8026\u5408\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u63d0\u53d6\u8725\u8734\u7684\u7a7a\u4e2d\u8c03\u6574\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u5728\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u4e2d\u5206\u6790\u8fd9\u4e9b\u8f68\u8ff9\u7684\u5173\u952e\u884c\u4e3a\u76ee\u6807\u53ca\u5176\u91cd\u8981\u6027\u3002\u968f\u540e\u5c06\u8fd9\u4e9b\u8f68\u8ff9\u4f5c\u4e3aSMS\u63a7\u5236\u7684\u53c2\u8003\u8f68\u8ff9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u751f\u7269\u884c\u4e3a\u53ef\u4ee5\u4e3a\u7a7a\u95f4\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u81ea\u9002\u5e94\u7684\u63a7\u5236\u7b56\u7565\uff0c\u6709\u671b\u63d0\u5347\u672a\u6765\u4efb\u52a1\u7684\u673a\u52a8\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u751f\u7269\u7075\u611f\u4e3a\u592a\u7a7a\u673a\u68b0\u81c2\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u4efb\u52a1\u7684\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.14600", "pdf": "https://arxiv.org/pdf/2508.14600", "abs": "https://arxiv.org/abs/2508.14600", "authors": ["Xudong Wang", "Guoming Tang", "Junyu Xue", "Srinivasan Keshav", "Tongxin Li", "Chris Ding"], "title": "DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning", "categories": ["cs.LG", "eess.SP", "I.2.6; J.7; I.5.4"], "comment": "Preprint", "summary": "Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain\nfine-grained appliance-level energy consumption in smart homes and building\napplications. However, the increasing adoption of behind-the-meter energy\nsources, such as solar panels and battery storage, poses new challenges for\nconventional NILM methods that rely solely on at-the-meter data. The injected\nenergy from the behind-the-meter sources can obscure the power signatures of\nindividual appliances, leading to a significant decline in NILM performance. To\naddress this challenge, we present DualNILM, a deep multi-task learning\nframework designed for the dual tasks of appliance state recognition and\ninjected energy identification in NILM. By integrating sequence-to-point and\nsequence-to-sequence strategies within a Transformer-based architecture,\nDualNILM can effectively capture multi-scale temporal dependencies in the\naggregate power consumption patterns, allowing for accurate appliance state\nrecognition and energy injection identification. We conduct validation of\nDualNILM using both self-collected and synthesized open NILM datasets that\ninclude both appliance-level energy consumption and energy injection. Extensive\nexperimental results demonstrate that DualNILM maintains an excellent\nperformance for the dual tasks in NILM, much outperforming conventional\nmethods.", "AI": {"tldr": "DualNILM\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u4e2d\u7531\u4e8e\u5206\u5e03\u5f0f\u80fd\u6e90\u6ce8\u5165\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u80fd\u6e90\uff08\u5982\u592a\u9633\u80fd\u677f\u548c\u7535\u6c60\u5b58\u50a8\uff09\u7684\u666e\u53ca\uff0c\u4f20\u7edfNILM\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7535\u8868\u6570\u636e\u7684\u95ee\u9898\u51f8\u663e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDualNILM\u6846\u67b6\uff0c\u7ed3\u5408\u5e8f\u5217\u5230\u70b9\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u7b56\u7565\uff0c\u57fa\u4e8eTransformer\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u8bc6\u522b\u7535\u5668\u72b6\u6001\u548c\u6ce8\u5165\u80fd\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDualNILM\u5728\u53cc\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "DualNILM\u4e3a\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u73af\u5883\u4e0b\u7684NILM\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.14355", "pdf": "https://arxiv.org/pdf/2508.14355", "abs": "https://arxiv.org/abs/2508.14355", "authors": ["Guodong Yao", "Hao Wang", "Qing Chang"], "title": "D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy", "categories": ["cs.RO"], "comment": "7 page, 2 figures", "summary": "LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate\nlocalization and mapping, especially in complex environments. However, the\npresence of LiDAR feature degeneracy poses a major challenge to reliable state\nestimation. To overcome this issue, we propose an enhanced LIO framework that\nintegrates adaptive outlier-tolerant correspondence with a scan-to-submap\nregistration strategy. The core contribution lies in an adaptive outlier\nremoval threshold, which dynamically adjusts based on point-to-sensor distance\nand the motion amplitude of platform. This mechanism improves the robustness of\nfeature matching in varying conditions. Moreover, we introduce a flexible\nscan-to-submap registration method that leverages IMU data to refine pose\nestimation, particularly in degenerate geometric configurations. To further\nenhance localization accuracy, we design a novel weighting matrix that fuses\nIMU preintegration covariance with a degeneration metric derived from the\nscan-to-submap process. Extensive experiments conducted in both indoor and\noutdoor environments-characterized by sparse or degenerate features-demonstrate\nthat our method consistently outperforms state-of-the-art approaches in terms\nof both robustness and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5f02\u5e38\u503c\u5bb9\u5fcd\u548c\u626b\u63cf\u5230\u5b50\u56fe\u914d\u51c6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86LiDAR\u7279\u5f81\u9000\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u548c\u5efa\u56fe\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0cLiDAR\u7279\u5f81\u9000\u5316\u5bf9\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u5f02\u5e38\u503c\u79fb\u9664\u9608\u503c\u548c\u626b\u63cf\u5230\u5b50\u56fe\u914d\u51c6\u7b56\u7565\uff0c\u5229\u7528IMU\u6570\u636e\u4f18\u5316\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u52a0\u6743\u77e9\u9635\u878d\u5408\u9884\u79ef\u5206\u534f\u65b9\u5dee\u548c\u9000\u5316\u5ea6\u91cf\u3002", "result": "\u5728\u7a00\u758f\u6216\u9000\u5316\u7279\u5f81\u7684\u5ba4\u5185\u5916\u73af\u5883\u4e2d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LiDAR\u7279\u5f81\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.14798", "pdf": "https://arxiv.org/pdf/2508.14798", "abs": "https://arxiv.org/abs/2508.14798", "authors": ["Soumyo Bhattacharjee", "Federico Villani", "Christian Vogt", "Andrea Cossettini", "Luca Benini"], "title": "ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems", "categories": ["cs.AR", "eess.SP"], "comment": "This work has been accepted for publication in IEEE IWASI Conference\n  proceedings. The final published version will be available via IEEE Xplore", "summary": "The demand for hundreds of tightly synchronized channels operating at tens of\nMSPS in ultrasound systems exceeds conventional low-voltage differential\nsignaling links' bandwidth, pin count, and latency. Although the JESD204B\nserial interface mitigates these limitations, commercial FPGA IP cores are\nproprietary, costly, and resource-intensive. We present ListenToJESD204B, an\nopen-source receiver IP core released under a permissive Solderpad 0.51 license\nfor AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable\nSystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides\ncycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It\noccupies only 107 configurable logic blocks (approximately 437 LUTs),\nrepresenting a 79\\% reduction compared to comparable commercially available IP.\nA modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC\ngeneration, and optional LFSR descrambling facilitates scaling to high lane\ncounts. We verified protocol compliance through simulation against the Xilinx\nJESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block\nstability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s\nlinks for 30 minutes with no errors.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u5f00\u6e90\u7684JESD204B\u63a5\u6536\u5668IP\u6838\u5fc3ListenToJESD204B\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u540c\u6b65\u6570\u636e\u4f20\u8f93\uff0c\u76f8\u6bd4\u5546\u4e1aIP\u66f4\u8282\u7701\u8d44\u6e90\u3002", "motivation": "\u4f20\u7edf\u5dee\u5206\u4fe1\u53f7\u94fe\u8def\u5728\u8d85\u58f0\u6ce2\u7cfb\u7edf\u4e2d\u5e26\u5bbd\u548c\u5ef6\u8fdf\u4e0d\u8db3\uff0c\u5546\u7528FPGA IP\u6838\u5fc3\u6210\u672c\u9ad8\u4e14\u8d44\u6e90\u5360\u7528\u5927\uff0c\u9700\u8981\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u53ef\u7efc\u5408\u7684SystemVerilog\u5b9e\u73b0\uff0c\u652f\u6301\u56db\u901a\u905312.8 Gb/s\u4f20\u8f93\uff0c\u91c7\u7528\u6a21\u5757\u5316\u6570\u636e\u8def\u5f84\u548c\u5f39\u6027\u7f13\u51b2\u5668\u7b49\u6280\u672f\u3002", "result": "\u8d44\u6e90\u5360\u7528\u4ec5\u4e3a107\u4e2a\u903b\u8f91\u5757\uff0c\u6bd4\u5546\u4e1aIP\u51cf\u5c1179%\uff0c\u9a8c\u8bc1\u4e86\u534f\u8bae\u517c\u5bb9\u6027\u548c30\u5206\u949f\u65e0\u9519\u8bef\u4f20\u8f93\u3002", "conclusion": "ListenToJESD204B\u662f\u4e00\u6b3e\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u5f00\u6e90\u7684\u9009\u62e9\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8d85\u58f0\u6ce2\u7cfb\u7edf\u3002"}}
{"id": "2508.14379", "pdf": "https://arxiv.org/pdf/2508.14379", "abs": "https://arxiv.org/abs/2508.14379", "authors": ["Chia-Han Yeh", "Tse-Sheng Nan", "Risto Vuorio", "Wei Hung", "Hung-Yen Wu", "Shao-Hua Sun", "Ping-Chun Hsieh"], "title": "Action-Constrained Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": "Published in ICML 2025", "summary": "Policy learning under action constraints plays a central role in ensuring\nsafe behaviors in various robot control and resource allocation applications.\nIn this paper, we study a new problem setting termed Action-Constrained\nImitation Learning (ACIL), where an action-constrained imitator aims to learn\nfrom a demonstrative expert with larger action space. The fundamental challenge\nof ACIL lies in the unavoidable mismatch of occupancy measure between the\nexpert and the imitator caused by the action constraints. We tackle this\nmismatch through \\textit{trajectory alignment} and propose DTWIL, which\nreplaces the original expert demonstrations with a surrogate dataset that\nfollows similar state trajectories while adhering to the action constraints.\nSpecifically, we recast trajectory alignment as a planning problem and solve it\nvia Model Predictive Control, which aligns the surrogate trajectories with the\nexpert trajectories based on the Dynamic Time Warping (DTW) distance. Through\nextensive experiments, we demonstrate that learning from the dataset generated\nby DTWIL significantly enhances performance across multiple robot control tasks\nand outperforms various benchmark imitation learning algorithms in terms of\nsample efficiency. Our code is publicly available at\nhttps://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u52a8\u4f5c\u7ea6\u675f\u6a21\u4eff\u5b66\u4e60\uff08ACIL\uff09\u201d\u7684\u65b0\u95ee\u9898\u8bbe\u7f6e\uff0c\u65e8\u5728\u901a\u8fc7\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3\u4e13\u5bb6\u4e0e\u6a21\u4eff\u8005\u4e4b\u95f4\u7684\u52a8\u4f5c\u7ea6\u675f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86DTWIL\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u8ddd\u79bb\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u8f68\u8ff9\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDTWIL\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u4f18\u4e8e\u5176\u4ed6\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u63a7\u5236\u548c\u8d44\u6e90\u5206\u914d\u7b49\u5e94\u7528\u4e2d\uff0c\u52a8\u4f5c\u7ea6\u675f\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u5bf9\u786e\u4fdd\u5b89\u5168\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76ACIL\u95ee\u9898\u662f\u4e3a\u4e86\u89e3\u51b3\u52a8\u4f5c\u7ea6\u675f\u5bfc\u81f4\u7684\u4e13\u5bb6\u4e0e\u6a21\u4eff\u8005\u4e4b\u95f4\u7684\u5360\u7528\u5ea6\u91cf\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51faDTWIL\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u5bf9\u9f50\u5c06\u4e13\u5bb6\u6f14\u793a\u8f6c\u6362\u4e3a\u9075\u5b88\u52a8\u4f5c\u7ea6\u675f\u7684\u66ff\u4ee3\u6570\u636e\u96c6\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5c06\u8f68\u8ff9\u5bf9\u9f50\u8f6c\u5316\u4e3a\u89c4\u5212\u95ee\u9898\uff1b2\uff09\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548cDTW\u8ddd\u79bb\u5bf9\u9f50\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eDTWIL\u751f\u6210\u7684\u6570\u636e\u96c6\u5b66\u4e60\u7684\u6a21\u578b\u5728\u591a\u4e2a\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u4e14\u6837\u672c\u6548\u7387\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "DTWIL\u901a\u8fc7\u8f68\u8ff9\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u7ea6\u675f\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14380", "pdf": "https://arxiv.org/pdf/2508.14380", "abs": "https://arxiv.org/abs/2508.14380", "authors": ["Nicole Fronda", "Phil Smith", "Bardh Hoxha", "Yash Pant", "Houssam Abbas"], "title": "Fair-CoPlan: Negotiated Flight Planning with Fair Deconfliction for Urban Air Mobility", "categories": ["cs.RO"], "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Urban Air Mobility (UAM) is an emerging transportation paradigm in which\nUncrewed Aerial Systems (UAS) autonomously transport passengers and goods in\ncities. The UAS have different operators with different, sometimes competing\ngoals, yet must share the airspace. We propose a negotiated, semi-distributed\nflight planner that optimizes UAS' flight lengths {\\em in a fair manner}.\nCurrent flight planners might result in some UAS being given disproportionately\nshorter flight paths at the expense of others. We introduce Fair-CoPlan, a\nplanner in which operators and a Provider of Service to the UAM (PSU) together\ncompute \\emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU\nconstrains take-off and landing choices for flights based on capacity at and\naround vertiports. Then, operators plan independently under these constraints.\nFinally, the PSU resolves any conflicting paths, optimizing for path length\nfairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages\nwider participation in UAM, ensures safety of the airspace and the areas below\nit, and promotes greater operator flexibility. We demonstrate Fair-CoPlan\nthrough simulation experiments and find fairer outcomes than a non-fair planner\nwith minor delays as a trade-off.", "AI": {"tldr": "Fair-CoPlan\u662f\u4e00\u79cd\u534a\u5206\u5e03\u5f0f\u98de\u884c\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u534f\u5546\u4f18\u5316\u65e0\u4eba\u822a\u7a7a\u7cfb\u7edf\uff08UAS\uff09\u7684\u98de\u884c\u8def\u5f84\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u98de\u884c\u89c4\u5212\u5668\u53ef\u80fd\u5bfc\u81f4\u90e8\u5206UAS\u98de\u884c\u8def\u5f84\u4e0d\u516c\u5e73\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Fair-CoPlan\uff0c\u4ee5\u516c\u5e73\u5206\u914d\u98de\u884c\u6210\u672c\u5e76\u4fc3\u8fdb\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u7684\u5e7f\u6cdb\u53c2\u4e0e\u3002", "method": "Fair-CoPlan\u5206\u4e3a\u4e09\u6b65\uff1a1\uff09PSU\u6839\u636e\u673a\u573a\u53ca\u5468\u8fb9\u5bb9\u91cf\u7ea6\u675f\u8d77\u98de\u548c\u7740\u9646\u9009\u62e9\uff1b2\uff09\u8fd0\u8425\u5546\u5728\u7ea6\u675f\u4e0b\u72ec\u7acb\u89c4\u5212\u8def\u5f84\uff1b3\uff09PSU\u89e3\u51b3\u8def\u5f84\u51b2\u7a81\u5e76\u4f18\u5316\u516c\u5e73\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cFair-CoPlan\u6bd4\u975e\u516c\u5e73\u89c4\u5212\u5668\u66f4\u516c\u5e73\uff0c\u4ec5\u4ee5\u8f7b\u5fae\u5ef6\u8fdf\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "Fair-CoPlan\u80fd\u63d0\u5347UAM\u5b89\u5168\u6027\u3001\u516c\u5e73\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u8fd0\u8425\u5546\u53c2\u4e0e\u3002"}}
{"id": "2508.14381", "pdf": "https://arxiv.org/pdf/2508.14381", "abs": "https://arxiv.org/abs/2508.14381", "authors": ["Nicole Fronda", "Bardh Hoxha", "Houssam Abbas"], "title": "FiReFly: Fair Distributed Receding Horizon Planning for Multiple UAVs", "categories": ["cs.RO"], "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "We propose injecting notions of fairness into multi-robot motion planning.\nWhen robots have competing interests, it is important to optimize for some kind\nof fairness in their usage of resources. In this work, we explore how the\nrobots' energy expenditures might be fairly distributed among them, while\nmaintaining mission success. We formulate a distributed fair motion planner and\nintegrate it with safe controllers in a algorithm called FiReFly. For simulated\nreach-avoid missions, FiReFly produces fairer trajectories and improves mission\nsuccess rates over a non-fair planner. We find that real-time performance is\nachievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with\ntrade-offs between runtime and fairness improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u516c\u5e73\u6027\u6ce8\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u516c\u5e73\u8fd0\u52a8\u89c4\u5212\u5668FiReFly\uff0c\u4f18\u5316\u673a\u5668\u4eba\u80fd\u91cf\u6d88\u8017\u7684\u516c\u5e73\u5206\u914d\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u673a\u5668\u4eba\u4e4b\u95f4\u5b58\u5728\u8d44\u6e90\u7ade\u4e89\uff0c\u9700\u4f18\u5316\u516c\u5e73\u6027\u4ee5\u786e\u4fdd\u4efb\u52a1\u6210\u529f\u548c\u8d44\u6e90\u5408\u7406\u5206\u914d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u516c\u5e73\u8fd0\u52a8\u89c4\u5212\u5668FiReFly\uff0c\u7ed3\u5408\u5b89\u5168\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u6a21\u62df\u7684\u907f\u969c\u4efb\u52a1\u3002", "result": "FiReFly\u751f\u6210\u7684\u8f68\u8ff9\u66f4\u516c\u5e73\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff1b15\u4e2a\u65e0\u4eba\u673a\u53ef\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c50\u4e2a\u65e0\u4eba\u673a\u9700\u6743\u8861\u8fd0\u884c\u65f6\u95f4\u548c\u516c\u5e73\u6027\u63d0\u5347\u3002", "conclusion": "\u516c\u5e73\u6027\u6ce8\u5165\u5728\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u6709\u6548\uff0cFiReFly\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.14383", "pdf": "https://arxiv.org/pdf/2508.14383", "abs": "https://arxiv.org/abs/2508.14383", "authors": ["Haitong Ma", "Bo Dai", "Zhaolin Ren", "Yebin Wang", "Na Li"], "title": "Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 5 figures", "summary": "Limited data has become a major bottleneck in scaling up offline imitation\nlearning (IL). In this paper, we propose enhancing IL performance under limited\nexpert data by introducing a pre-training stage that learns dynamics\nrepresentations, derived from factorizations of the transition dynamics. We\nfirst theoretically justify that the optimal decision variable of offline IL\nlies in the representation space, significantly reducing the parameters to\nlearn in the downstream IL. Moreover, the dynamics representations can be\nlearned from arbitrary data collected with the same dynamics, allowing the\nreuse of massive non-expert data and mitigating the limited data issues. We\npresent a tractable loss function inspired by noise contrastive estimation to\nlearn the dynamics representations at the pre-training stage. Experiments on\nMuJoCo demonstrate that our proposed algorithm can mimic expert policies with\nas few as a single trajectory. Experiments on real quadrupeds show that we can\nleverage pre-trained dynamics representations from simulator data to learn to\nwalk from a few real-world demonstrations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9884\u8bad\u7ec3\u52a8\u6001\u8868\u793a\u6765\u63d0\u5347\u6709\u9650\u4e13\u5bb6\u6570\u636e\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u6700\u4f18\u51b3\u7b56\u53d8\u91cf\u5b58\u5728\u4e8e\u8868\u793a\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6709\u9650\u6570\u636e\u662f\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u6269\u5c55\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u9636\u6bb5\u4ece\u52a8\u6001\u5206\u89e3\u4e2d\u5b66\u4e60\u8868\u793a\uff0c\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u7684\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u52a8\u6001\u8868\u793a\uff1b\u8fd9\u4e9b\u8868\u793a\u53ef\u4ee5\u4ece\u975e\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u51cf\u5c11\u4e0b\u6e38IL\u7684\u53c2\u6570\u9700\u6c42\u3002", "result": "\u5728MuJoCo\u5b9e\u9a8c\u4e2d\uff0c\u4ec5\u9700\u5355\u6761\u8f68\u8ff9\u5373\u53ef\u6a21\u4eff\u4e13\u5bb6\u7b56\u7565\uff1b\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u4eff\u771f\u6570\u636e\u9884\u8bad\u7ec3\u7684\u8868\u793a\u6210\u529f\u5b66\u4e60\u4e86\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u7684\u884c\u8d70\u7b56\u7565\u3002", "conclusion": "\u52a8\u6001\u8868\u793a\u9884\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u6709\u9650\u6570\u636e\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\uff0c\u4e14\u53ef\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.14387", "pdf": "https://arxiv.org/pdf/2508.14387", "abs": "https://arxiv.org/abs/2508.14387", "authors": ["Yuxiao Zhu", "Junfeng Chen", "Xintong Zhang", "Meng Guo", "Zhongkui Li"], "title": "DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models", "categories": ["cs.RO"], "comment": "submitted to IROS 2025", "summary": "Online coordination of multi-robot systems in open and unknown environments\nfaces significant challenges, particularly when semantic features detected\nduring operation dynamically trigger new tasks. Recent large language model\n(LLMs)-based approaches for scene reasoning and planning primarily focus on\none-shot, end-to-end solutions in known environments, lacking both dynamic\nadaptation capabilities for online operation and explainability in the\nprocesses of planning. To address these issues, a novel framework (DEXTER-LLM)\nfor dynamic task planning in unknown environments, integrates four modules: (i)\na mission comprehension module that resolves partial ordering of tasks\nspecified by natural languages or linear temporal logic formulas (LTL); (ii) an\nonline subtask generator based on LLMs that improves the accuracy and\nexplainability of task decomposition via multi-stage reasoning; (iii) an\noptimal subtask assigner and scheduler that allocates subtasks to robots via\nsearch-based optimization; and (iv) a dynamic adaptation and human-in-the-loop\nverification module that implements multi-rate, event-based updates for both\nsubtasks and their assignments, to cope with new features and tasks detected\nonline. The framework effectively combines LLMs' open-world reasoning\ncapabilities with the optimality of model-based assignment methods,\nsimultaneously addressing the critical issue of online adaptability and\nexplainability. Experimental evaluations demonstrate exceptional performances,\nwith 100% success rates across all scenarios, 160 tasks and 480 subtasks\ncompleted on average (3 times the baselines), 62% less queries to LLMs during\nadaptation, and superior plan quality (2 times higher) for compound tasks.\nProject page at https://tcxm.github.io/DEXTER-LLM/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEXTER-LLM\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u73af\u5883\u4e2d\u52a8\u6001\u4efb\u52a1\u89c4\u5212\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u5757\u534f\u4f5c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u5353\u8d8a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8bed\u4e49\u7279\u5f81\u89e6\u53d1\u65b0\u4efb\u52a1\u65f6\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u56db\u4e2a\u6a21\u5757\uff1a\u4efb\u52a1\u7406\u89e3\u3001\u5728\u7ebf\u5b50\u4efb\u52a1\u751f\u6210\u3001\u6700\u4f18\u5b50\u4efb\u52a1\u5206\u914d\u4e0e\u8c03\u5ea6\u3001\u52a8\u6001\u9002\u5e94\u4e0e\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\uff0c\u5c06LLM\u7684\u5f00\u73af\u4e16\u754c\u63a8\u7406\u80fd\u529b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u6027\u80fd\uff1a100%\u6210\u529f\u7387\u3001\u5e73\u5747\u5b8c\u6210160\u4e3b\u4efb\u52a1\u548c480\u5b50\u4efb\u52a1\uff08\u57fa\u7ebf3\u500d\uff09\u3001\u9002\u5e94\u9636\u6bb5LLM\u67e5\u8be2\u51cf\u5c1162%\u3001\u590d\u6742\u4efb\u52a1\u8ba1\u5212\u8d28\u91cf\u63d0\u53472\u500d\u3002", "conclusion": "DEXTER-LLM\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.14441", "pdf": "https://arxiv.org/pdf/2508.14441", "abs": "https://arxiv.org/abs/2508.14441", "authors": ["Yijin Chen", "Wenqiang Xu", "Zhenjun Yu", "Tutian Tang", "Yutong Li", "Siqiong Yao", "Cewu Lu"], "title": "FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous in-hand manipulation is a long-standing challenge in robotics due\nto complex contact dynamics and partial observability. While humans synergize\nvision and touch for such tasks, robotic approaches often prioritize one\nmodality, therefore limiting adaptability. This paper introduces Flow Before\nImitation (FBI), a visuotactile imitation learning framework that dynamically\nfuses tactile interactions with visual observations through motion dynamics.\nUnlike prior static fusion methods, FBI establishes a causal link between\ntactile signals and object motion via a dynamics-aware latent model. FBI\nemploys a transformer-based interaction module to fuse flow-derived tactile\nfeatures with visual inputs, training a one-step diffusion policy for real-time\nexecution. Extensive experiments demonstrate that the proposed method\noutperforms the baseline methods in both simulation and the real world on two\ncustomized in-hand manipulation tasks and three standard dexterous manipulation\ntasks. Code, models, and more results are available in the website\nhttps://sites.google.com/view/dex-fbi.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFBI\u7684\u89c6\u89c9\u89e6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u878d\u5408\u89e6\u89c9\u4e0e\u89c6\u89c9\u89c2\u5bdf\uff0c\u901a\u8fc7\u8fd0\u52a8\u52a8\u529b\u5b66\u89e3\u51b3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u590d\u6742\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u63a5\u89e6\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\uff0c\u7075\u5de7\u7684\u624b\u5185\u64cd\u4f5c\u4e00\u76f4\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u957f\u671f\u6311\u6218\u3002\u4eba\u7c7b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u80fd\u591f\u534f\u540c\u89c6\u89c9\u548c\u89e6\u89c9\uff0c\u4f46\u73b0\u6709\u673a\u5668\u4eba\u65b9\u6cd5\u5f80\u5f80\u53ea\u4f18\u5148\u8003\u8651\u4e00\u79cd\u6a21\u6001\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002", "method": "FBI\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u6f5c\u5728\u6a21\u578b\u5efa\u7acb\u4e86\u89e6\u89c9\u4fe1\u53f7\u4e0e\u7269\u4f53\u8fd0\u52a8\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u4ea4\u4e92\u6a21\u5757\u878d\u5408\u89e6\u89c9\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u8bad\u7ec3\u4e00\u6b65\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u5b9e\u65f6\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFBI\u5728\u4e24\u4e2a\u81ea\u5b9a\u4e49\u7684\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\u548c\u4e09\u4e2a\u6807\u51c6\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u5747\u5982\u6b64\uff09\u3002", "conclusion": "FBI\u901a\u8fc7\u52a8\u6001\u878d\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u534f\u540c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.14542", "pdf": "https://arxiv.org/pdf/2508.14542", "abs": "https://arxiv.org/abs/2508.14542", "authors": ["Weize Li", "Zhengxiao Han", "Lixin Xu", "Xiangyu Chen", "Harrison Bounds", "Chenrui Zhang", "Yifan Xu"], "title": "Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation", "categories": ["cs.RO"], "comment": "Technical report of First-place/Champion solution at IEEE ICRA 2025\n  What Bimanuals Can Do (WBCD) Challenge - Table Services Track", "summary": "This technical report presents the champion solution of the Table Service\nTrack in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a\nseries of demanding tasks under strict requirements for speed, precision, and\nreliability: unfolding a tablecloth (deformable-object manipulation), placing a\npizza onto the table (pick-and-place), and opening and closing a food container\nwith the lid. Our solution combines VR-based teleoperation and Learning from\nDemonstrations (LfD) to balance robustness and autonomy. Most subtasks were\nexecuted through high-fidelity remote teleoperation, while the pizza placement\nwas handled by an ACT-based policy trained from 100 in-person teleoperated\ndemonstrations with randomized initial configurations. By carefully integrating\nscoring rules, task characteristics, and current technical capabilities, our\napproach achieved both high efficiency and reliability, ultimately securing the\nfirst place in the competition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ICRA 2025 WBCD\u7ade\u8d5b\u4e2dTable Service Track\u7684\u51a0\u519b\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86VR\u8fdc\u7a0b\u64cd\u4f5c\u548c\u5b66\u4e60\u6f14\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u53ef\u9760\u7684\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u5728\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u4e25\u683c\u8981\u6c42\u7684\u4efb\u52a1\uff08\u5982\u5c55\u5f00\u684c\u5e03\u3001\u653e\u7f6e\u62ab\u8428\u548c\u5f00\u5173\u98df\u54c1\u5bb9\u5668\uff09\uff0c\u5e73\u8861\u9c81\u68d2\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u7ed3\u5408VR\u8fdc\u7a0b\u64cd\u4f5c\u548c\u57fa\u4e8eACT\u7b56\u7565\u7684\u5b66\u4e60\u6f14\u793a\uff08LfD\uff09\uff0c\u4efb\u52a1\u901a\u8fc7\u9ad8\u4fdd\u771f\u8fdc\u7a0b\u64cd\u4f5c\u6267\u884c\uff0c\u62ab\u8428\u653e\u7f6e\u901a\u8fc7100\u4e2a\u968f\u673a\u521d\u59cb\u914d\u7f6e\u7684\u6f14\u793a\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u6574\u5408\u8bc4\u5206\u89c4\u5219\u3001\u4efb\u52a1\u7279\u70b9\u548c\u6280\u672f\u80fd\u529b\uff0c\u65b9\u6848\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u6700\u7ec8\u83b7\u5f97\u7ade\u8d5b\u7b2c\u4e00\u540d\u3002", "conclusion": "\u7efc\u5408\u8fdc\u7a0b\u64cd\u4f5c\u548c\u5b66\u4e60\u6f14\u793a\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.14554", "pdf": "https://arxiv.org/pdf/2508.14554", "abs": "https://arxiv.org/abs/2508.14554", "authors": ["Xinkai Liang", "Yigu Ge", "Yangxi Shi", "Haoyu Yang", "Xu Cao", "Hao Fang"], "title": "EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR", "categories": ["cs.RO"], "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). This work has been submitted to the IEEE for\n  possible publication", "summary": "To address the challenges of localization drift and perception-planning\ncoupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios\n(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel\nframework with a downward-mounted tilted LiDAR configuration (20{\\deg}\ninclination), integrating a LiDAR-Inertial Odometry (LIO) system and a\nhierarchical trajectory-yaw optimization algorithm. The hardware innovation\nenables constraint enhancement via dense ground point cloud acquisition and\nforward environmental awareness for dynamic obstacle detection. A\ntightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter\n(IESKF) with dynamic motion compensation, achieves high level 6-DoF\nlocalization accuracy in feature-sparse environments. The planner, augmented by\nenvironment, balancing environmental exploration, target tracking precision,\nand energy efficiency. Physical experiments demonstrate 81% tracking error\nreduction, 22% improvement in perceptual coverage, and near-zero vertical drift\nacross indoor maze and 60-meter-scale outdoor scenarios. This work proposes a\nhardware-algorithm co-design paradigm, offering a robust solution for UAV\nautonomy in post-disaster search and rescue missions. We will release our\nsoftware and hardware as an open-source package for the community. Video:\nhttps://youtu.be/7av2ueLSiYw.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEAROL\u6846\u67b6\uff0c\u901a\u8fc7\u503e\u659cLiDAR\u914d\u7f6e\u548c\u5206\u5c42\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u5f00\u653e\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u6f02\u79fb\u548c\u611f\u77e5-\u89c4\u5212\u8026\u5408\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u5f00\u653e\u573a\u666f\uff08\u5982\u5012\u584c\u5efa\u7b51\uff09\u4e2d\u5b9a\u4f4d\u6f02\u79fb\u548c\u611f\u77e5-\u89c4\u5212\u8026\u5408\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5411\u4e0b\u503e\u659cLiDAR\u7684\u786c\u4ef6\u521b\u65b0\u548c\u5206\u5c42\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u4f7f\u7528\u7d27\u5bc6\u8026\u5408\u7684LIO\u7cfb\u7edf\u548cIESKF\u8fdb\u884c\u52a8\u6001\u8fd0\u52a8\u8865\u507f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c1181%\uff0c\u611f\u77e5\u8986\u76d6\u7387\u63d0\u534722%\uff0c\u5782\u76f4\u6f02\u79fb\u63a5\u8fd1\u96f6\u3002", "conclusion": "\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u4e3a\u707e\u540e\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14610", "pdf": "https://arxiv.org/pdf/2508.14610", "abs": "https://arxiv.org/abs/2508.14610", "authors": ["Junzhi Li", "Teng Long", "Jingliang Sun", "Jianxin Zhong"], "title": "TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with Uncertain Obstacle Spatial-temporal Avoidance", "categories": ["cs.RO"], "comment": null, "summary": "Despite extensive developments in motion planning of autonomous aerial\nvehicles (AAVs), existing frameworks faces the challenges of local minima and\ndeadlock in complex dynamic environments, leading to increased collision risks.\nTo address these challenges, we present TRUST-Planner, a topology-guided\nhierarchical planning framework for robust spatial-temporal obstacle avoidance.\nIn the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is\nproposed to rapidly explore topological paths for global guidance. The backend\nutilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and\ndynamic distance field (DDF) to enable efficient predictive obstacle avoidance\nand fast parallel computation. Furthermore, an incremental multi-branch\ntrajectory management framework is introduced to enable spatio-temporal\ntopological decision-making, while efficiently leveraging historical\ninformation to reduce replanning time. Simulation results show that\nTRUST-Planner outperforms baseline competitors, achieving a 96\\% success rate\nand millisecond-level computation efficiency in tested complex environments.\nReal-world experiments further validate the feasibility and practicality of the\nproposed method.", "AI": {"tldr": "TRUST-Planner \u662f\u4e00\u79cd\u62d3\u6251\u5f15\u5bfc\u7684\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3AAV\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5168\u5c40\u548c\u5c40\u90e8\u89c4\u5212\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709AAV\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u78b0\u649e\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6TRUST-Planner\uff0c\u524d\u7aef\u4f7f\u7528DEV-PRM\u5feb\u901f\u63a2\u7d22\u62d3\u6251\u8def\u5f84\uff0c\u540e\u7aef\u91c7\u7528UTF-MINCO\u548cDDF\u5b9e\u73b0\u9ad8\u6548\u907f\u969c\u548c\u5e76\u884c\u8ba1\u7b97\uff0c\u540c\u65f6\u5f15\u5165\u589e\u91cf\u591a\u5206\u652f\u8f68\u8ff9\u7ba1\u7406\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cTRUST-Planner\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fbe\u523096%\u7684\u6210\u529f\u7387\u548c\u6beb\u79d2\u7ea7\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "TRUST-Planner\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u548c\u5206\u5c42\u89c4\u5212\u6709\u6548\u89e3\u51b3\u4e86AAV\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.14635", "pdf": "https://arxiv.org/pdf/2508.14635", "abs": "https://arxiv.org/abs/2508.14635", "authors": ["Jo\u00e3o Vitor de Carvalho Silva", "Douglas G. Macharet"], "title": "Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The ability to coordinate actions across multiple agents is critical for\nsolving complex, real-world problems. Large Language Models (LLMs) have shown\nstrong capabilities in communication, planning, and reasoning, raising the\nquestion of whether they can also support effective collaboration in\nmulti-agent settings. In this work, we investigate the use of LLM agents to\nsolve a structured victim rescue task that requires division of labor,\nprioritization, and cooperative planning. Agents operate in a fully known\ngraph-based environment and must allocate resources to victims with varying\nneeds and urgency levels. We systematically evaluate their performance using a\nsuite of coordination-sensitive metrics, including task success rate, redundant\nactions, room conflicts, and urgency-weighted efficiency. This study offers new\ninsights into the strengths and failure modes of LLMs in physically grounded\nmulti-agent collaboration tasks, contributing to future benchmarks and\narchitectural improvements.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u5728\u591a\u667a\u80fd\u4f53\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u5206\u5de5\u548c\u4f18\u5148\u7ea7\u89c4\u5212\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u95ee\u9898\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1LLM\u5728\u8be5\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "LLM\u667a\u80fd\u4f53\u5728\u5df2\u77e5\u56fe\u73af\u5883\u4e2d\u534f\u4f5c\u6551\u63f4\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5305\u62ec\u4efb\u52a1\u6210\u529f\u7387\u3001\u5197\u4f59\u884c\u4e3a\u7b49\u6307\u6807\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u7269\u7406\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u4e3a\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u548c\u67b6\u6784\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2508.14636", "pdf": "https://arxiv.org/pdf/2508.14636", "abs": "https://arxiv.org/abs/2508.14636", "authors": ["Sanjeev Ramkumar Sudha", "Marija Popovi\u0107", "Erlend M. Coates"], "title": "An Informative Planning Framework for Target Tracking and Active Mapping in Dynamic Environments with ASVs", "categories": ["cs.RO"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Mobile robot platforms are increasingly being used to automate information\ngathering tasks such as environmental monitoring. Efficient target tracking in\ndynamic environments is critical for applications such as search and rescue and\npollutant cleanups. In this letter, we study active mapping of floating targets\nthat drift due to environmental disturbances such as wind and currents. This is\na challenging problem as it involves predicting both spatial and temporal\nvariations in the map due to changing conditions. We propose an informative\npath planning framework to map an arbitrary number of moving targets with\ninitially unknown positions in dynamic environments. A key component of our\napproach is a spatiotemporal prediction network that predicts target position\ndistributions over time. We propose an adaptive planning objective for target\ntracking that leverages these predictions. Simulation experiments show that our\nproposed planning objective improves target tracking performance compared to\nexisting methods that consider only entropy reduction as the planning\nobjective. Finally, we validate our approach in field tests using an autonomous\nsurface vehicle, showcasing its ability to track targets in real-world\nmonitoring scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u79fb\u52a8\u76ee\u6807\u4e3b\u52a8\u6620\u5c04\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u7a7a\u9884\u6d4b\u7f51\u7edc\u6539\u8fdb\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u6f02\u6d6e\u76ee\u6807\u56e0\u98ce\u548c\u6c34\u6d41\u7b49\u73af\u5883\u5e72\u6270\u5bfc\u81f4\u7684\u65f6\u7a7a\u53d8\u5316\u6620\u5c04\u96be\u9898\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u9884\u6d4b\u7f51\u7edc\u9884\u6d4b\u76ee\u6807\u4f4d\u7f6e\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u89c4\u5212\u76ee\u6807\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u76ee\u6807\u8ffd\u8e2a\u6027\u80fd\u4e0a\u4f18\u4e8e\u4ec5\u8003\u8651\u71b5\u51cf\u5c11\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u79fb\u52a8\u76ee\u6807\u7684\u8ffd\u8e2a\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u76d1\u6d4b\u573a\u666f\u3002"}}
{"id": "2508.14661", "pdf": "https://arxiv.org/pdf/2508.14661", "abs": "https://arxiv.org/abs/2508.14661", "authors": ["Alexander Raab", "Stephan Weiss", "Alessandro Fornasier", "Christian Brommer", "Abdalrahman Ibrahim"], "title": "Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds", "categories": ["cs.RO"], "comment": null, "summary": "Aiming to enhance the consistency and thus long-term accuracy of Extended\nKalman Filters for terrestrial vehicle localization, this paper introduces the\nManifold Error State Extended Kalman Filter (M-ESEKF). By representing the\nrobot's pose in a space with reduced dimensionality, the approach ensures\nfeasible estimates on generic smooth surfaces, without introducing artificial\nconstraints or simplifications that may degrade a filter's performance. The\naccompanying measurement models are compatible with common loosely- and\ntightly-coupled sensor modalities and also implicitly account for the ground\ngeometry. We extend the formulation by introducing a novel correction scheme\nthat embeds additional domain knowledge into the sensor data, giving more\naccurate uncertainty approximations and further enhancing filter consistency.\nThe proposed estimator is seamlessly integrated into a validated modular state\nestimation framework, demonstrating compatibility with existing\nimplementations. Extensive Monte Carlo simulations across diverse scenarios and\ndynamic sensor configurations show that the M-ESEKF outperforms classical\nfilter formulations in terms of consistency and stability. Moreover, it\neliminates the need for scenario-specific parameter tuning, enabling its\napplication in a variety of real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u63d0\u5347\u5730\u9762\u8f66\u8f86\u5b9a\u4f4d\u7cbe\u5ea6\u7684M-ESEKF\u65b9\u6cd5\uff0c\u901a\u8fc7\u964d\u4f4e\u7ef4\u5ea6\u7a7a\u95f4\u8868\u793a\u673a\u5668\u4eba\u4f4d\u59ff\uff0c\u63d0\u9ad8\u4e86\u6ee4\u6ce2\u5668\u7684\u957f\u671f\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u5728\u5730\u9762\u8f66\u8f86\u5b9a\u4f4d\u4e2d\u53ef\u80fd\u56e0\u7ef4\u5ea6\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u63d0\u5347\u5176\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6d41\u5f62\u8bef\u5dee\u72b6\u6001\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08M-ESEKF\uff09\uff0c\u5728\u964d\u7ef4\u7a7a\u95f4\u4e2d\u8868\u793a\u673a\u5668\u4eba\u4f4d\u59ff\uff0c\u5e76\u7ed3\u5408\u65b0\u9896\u7684\u6821\u6b63\u65b9\u6848\u5d4c\u5165\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u548c\u52a8\u6001\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\uff0cM-ESEKF\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6ee4\u6ce2\u5668\uff0c\u4e14\u65e0\u9700\u7279\u5b9a\u573a\u666f\u53c2\u6570\u8c03\u6574\u3002", "conclusion": "M-ESEKF\u663e\u8457\u63d0\u5347\u4e86\u6ee4\u6ce2\u5668\u7684\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.14763", "pdf": "https://arxiv.org/pdf/2508.14763", "abs": "https://arxiv.org/abs/2508.14763", "authors": ["Sagar Parekh", "Casey Grothoff", "Ryan Wright", "Robin White", "Dylan P. Losey"], "title": "Safe and Transparent Robots for Human-in-the-Loop Meat Processing", "categories": ["cs.RO"], "comment": null, "summary": "Labor shortages have severely affected the meat processing sector. Automated\ntechnology has the potential to support the meat industry, assist workers, and\nenhance job quality. However, existing automation in meat processing is highly\nspecialized, inflexible, and cost intensive. Instead of forcing manufacturers\nto buy a separate device for each step of the process, our objective is to\ndevelop general-purpose robotic systems that work alongside humans to perform\nmultiple meat processing tasks. Through a recently conducted survey of industry\nexperts, we identified two main challenges associated with integrating these\ncollaborative robots alongside human workers. First, there must be measures to\nensure the safety of human coworkers; second, the coworkers need to understand\nwhat the robot is doing. This paper addresses both challenges by introducing a\nsafety and transparency framework for general-purpose meat processing robots.\nFor safety, we implement a hand-detection system that continuously monitors\nnearby humans. This system can halt the robot in situations where the human\ncomes into close proximity of the operating robot. We also develop an\ninstrumented knife equipped with a force sensor that can differentiate contact\nbetween objects such as meat, bone, or fixtures. For transparency, we introduce\na method that detects the robot's uncertainty about its performance and uses an\nLED interface to communicate that uncertainty to the human. Additionally, we\ndesign a graphical interface that displays the robot's plans and allows the\nhuman to provide feedback on the planned cut. Overall, our framework can ensure\nsafe operation while keeping human workers in-the-loop about the robot's\nactions which we validate through a user study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u8089\u7c7b\u52a0\u5de5\u673a\u5668\u4eba\u7684\u5b89\u5168\u548c\u900f\u660e\u5ea6\u6846\u67b6\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u548c\u900f\u660e\u5ea6\u95ee\u9898\u3002", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u5f71\u54cd\u4e86\u8089\u7c7b\u52a0\u5de5\u4e1a\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u6280\u672f\u6602\u8d35\u4e14\u4e0d\u591f\u7075\u6d3b\uff0c\u9700\u5f00\u53d1\u80fd\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u901a\u7528\u673a\u5668\u4eba\u3002", "method": "\u901a\u8fc7\u624b\u90e8\u68c0\u6d4b\u7cfb\u7edf\u548c\u914d\u5907\u529b\u4f20\u611f\u5668\u7684\u5200\u5177\u786e\u4fdd\u5b89\u5168\uff1b\u4f7f\u7528LED\u548c\u56fe\u5f62\u754c\u9762\u63d0\u5347\u673a\u5668\u4eba\u7684\u900f\u660e\u5ea6\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0c\u80fd\u591f\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u5e76\u63d0\u5347\u5de5\u4eba\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8089\u7c7b\u52a0\u5de5\u884c\u4e1a\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u5b89\u5168\u548c\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14102", "pdf": "https://arxiv.org/pdf/2508.14102", "abs": "https://arxiv.org/abs/2508.14102", "authors": ["Thomas Gallien"], "title": "Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Trust region-based optimization methods have become foundational\nreinforcement learning algorithms that offer stability and strong empirical\nperformance in continuous control tasks. Growing interest in scalable and\nreusable control policies translate also in a demand for morphological\ngeneralization, the ability of control policies to cope with different\nkinematic structures. Graph-based policy architectures provide a natural and\neffective mechanism to encode such structural differences. However, while these\narchitectures accommodate variable morphologies, the behavior of trust region\nmethods under varying action space dimensionality remains poorly understood. To\nthis end, we conduct a theoretical analysis of trust region-based policy\noptimization methods, focusing on both Trust Region Policy Optimization (TRPO)\nand its widely used first-order approximation, Proximal Policy Optimization\n(PPO). The goal is to demonstrate how varying action space dimensionality\ninfluence the optimization landscape, particularly under the constraints\nimposed by KL-divergence or policy clipping penalties. Complementing the\ntheoretical insights, an empirical evaluation under morphological variation is\ncarried out using the Gymnasium Swimmer environment. This benchmark offers a\nsystematically controlled setting for varying the kinematic structure without\naltering the underlying task, making it particularly well-suited to study\nmorphological generalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u4fe1\u4efb\u533a\u57df\u7684\u4f18\u5316\u65b9\u6cd5\uff08TRPO\u548cPPO\uff09\u5728\u5f62\u6001\u5b66\u6cdb\u5316\u4e2d\u7684\u884c\u4e3a\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u8fd0\u52a8\u5b66\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u5bf9\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u7528\u63a7\u5236\u7b56\u7565\u9700\u6c42\u7684\u589e\u52a0\uff0c\u7814\u7a76\u5f62\u6001\u5b66\u6cdb\u5316\u80fd\u529b\u6210\u4e3a\u91cd\u8981\u8bfe\u9898\u3002\u8bba\u6587\u65e8\u5728\u7406\u89e3\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\u5728\u53d8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff08TRPO\u548cPPO\uff09\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff08Gymnasium Swimmer\u73af\u5883\uff09\uff0c\u7814\u7a76\u4e86\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u53d8\u5316\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\u5728\u5f62\u6001\u5b66\u53d8\u5316\u4e0b\u7684\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f62\u6001\u5b66\u6cdb\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4fe1\u4efb\u533a\u57df\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.14160", "pdf": "https://arxiv.org/pdf/2508.14160", "abs": "https://arxiv.org/abs/2508.14160", "authors": ["Ronghao Dang", "Yuqian Yuan", "Yunxuan Mao", "Kehan Li", "Jiangpin Liu", "Zhikai Wang", "Xin Li", "Fan Wang", "Deli Zhao"], "title": "RynnEC: Bringing MLLMs into Embodied World", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "The technical report of RynnEC, an embodied cognition MLLM", "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC", "AI": {"tldr": "RynnEC\u662f\u4e00\u79cd\u7528\u4e8e\u5177\u8eab\u8ba4\u77e5\u7684\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u533a\u57df\u7f16\u7801\u5668\u548c\u63a9\u7801\u89e3\u7801\u5668\uff0c\u6027\u80fd\u4f18\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u7cbe\u7ec6\u7684\u7269\u7406\u4e16\u754c\u611f\u77e5\u548c\u66f4\u7cbe\u786e\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u533a\u57df\u7f16\u7801\u5668\u548c\u63a9\u7801\u89e3\u7801\u5668\uff0c\u63d0\u51fa\u57fa\u4e8e\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u5177\u8eab\u8ba4\u77e5\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u5bf9\u8c61\u5c5e\u6027\u7406\u89e3\u3001\u5bf9\u8c61\u5206\u5272\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "RynnEC\u6709\u671b\u63a8\u52a8\u5177\u8eab\u4ee3\u7406\u901a\u7528\u8ba4\u77e5\u6838\u5fc3\u7684\u53d1\u5c55\uff0c\u5e76\u4fc3\u8fdb\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u3002"}}
{"id": "2508.14181", "pdf": "https://arxiv.org/pdf/2508.14181", "abs": "https://arxiv.org/abs/2508.14181", "authors": ["Jordan Peper", "Yan Miao", "Sayan Mitra", "Ivan Ruchkin"], "title": "Towards Unified Probabilistic Verification and Validation of Vision-Based Autonomy", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "Accepted by the 23rd International Symposium on Automated Technology\n  for Verification and Analysis (ATVA'25)", "summary": "Precise and comprehensive situational awareness is a critical capability of\nmodern autonomous systems. Deep neural networks that perceive task-critical\ndetails from rich sensory signals have become ubiquitous; however, their\nblack-box behavior and sensitivity to environmental uncertainty and\ndistribution shifts make them challenging to verify formally. Abstraction-based\nverification techniques for vision-based autonomy produce safety guarantees\ncontingent on rigid assumptions, such as bounded errors or known unique\ndistributions. Such overly restrictive and inflexible assumptions limit the\nvalidity of the guarantees, especially in diverse and uncertain test-time\nenvironments. We propose a methodology that unifies the verification models of\nperception with their offline validation. Our methodology leverages interval\nMDPs and provides a flexible end-to-end guarantee that adapts directly to the\nout-of-distribution test-time conditions. We evaluate our methodology on a\nsynthetic perception Markov chain with well-defined state estimation\ndistributions and a mountain car benchmark. Our findings reveal that we can\nguarantee tight yet rigorous bounds on overall system safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u611f\u77e5\u9a8c\u8bc1\u6a21\u578b\u548c\u79bb\u7ebf\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u533a\u95f4MDP\u63d0\u4f9b\u7075\u6d3b\u7aef\u5230\u7aef\u7684\u4fdd\u969c\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u6d4b\u8bd5\u6761\u4ef6\u3002", "motivation": "\u73b0\u4ee3\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u7cbe\u786e\u5168\u9762\u7684\u60c5\u5883\u611f\u77e5\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u201c\u9ed1\u76d2\u201d\u884c\u4e3a\u53ca\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u654f\u611f\u6027\u4f7f\u5176\u96be\u4ee5\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002\u73b0\u6709\u9a8c\u8bc1\u6280\u672f\u4f9d\u8d56\u4e8e\u8fc7\u4e8e\u4e25\u683c\u7684\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u4fdd\u969c\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u611f\u77e5\u9a8c\u8bc1\u6a21\u578b\u4e0e\u79bb\u7ebf\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u533a\u95f4MDP\u63d0\u4f9b\u7aef\u5230\u7aef\u4fdd\u969c\uff0c\u80fd\u591f\u76f4\u63a5\u9002\u5e94\u5206\u5e03\u5916\u7684\u6d4b\u8bd5\u6761\u4ef6\u3002", "result": "\u5728\u5408\u6210\u611f\u77e5\u9a6c\u5c14\u53ef\u592b\u94fe\u548c\u201c\u5c71\u5730\u8f66\u201d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u7d27\u5bc6\u4e14\u4e25\u683c\u7684\u7cfb\u7edf\u5b89\u5168\u6027\u8fb9\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14358", "pdf": "https://arxiv.org/pdf/2508.14358", "abs": "https://arxiv.org/abs/2508.14358", "authors": ["Zhujun Li", "Shuo Zhang", "Ioannis Stamos"], "title": "Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted by ICCV 2025 Workshop on Recovering 6D Object Pose (R6D)", "summary": "Category-level object pose estimation aims to predict the 6D pose and 3D size\nof objects within given categories. Existing approaches for this task rely\nsolely on 6D poses as supervisory signals without explicitly capturing the\nintrinsic continuity of poses, leading to inconsistencies in predictions and\nreduced generalization to unseen poses. To address this limitation, we propose\nHRC-Pose, a novel depth-only framework for category-level object pose\nestimation, which leverages contrastive learning to learn point cloud\nrepresentations that preserve the continuity of 6D poses. HRC-Pose decouples\nobject pose into rotation and translation components, which are separately\nencoded and leveraged throughout the network. Specifically, we introduce a\ncontrastive learning strategy for multi-task, multi-category scenarios based on\nour 6D pose-aware hierarchical ranking scheme, which contrasts point clouds\nfrom multiple categories by considering rotational and translational\ndifferences as well as categorical information. We further design pose\nestimation modules that separately process the learned rotation-aware and\ntranslation-aware embeddings. Our experiments demonstrate that HRC-Pose\nsuccessfully learns continuous feature spaces. Results on REAL275 and CAMERA25\nbenchmarks show that our method consistently outperforms existing depth-only\nstate-of-the-art methods and runs in real-time, demonstrating its effectiveness\nand potential for real-world applications. Our code is at\nhttps://github.com/zhujunli1993/HRC-Pose.", "AI": {"tldr": "HRC-Pose is a novel depth-only\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6355\u63496D\u59ff\u6001\u7684\u8fde\u7eed\u6027\uff0c\u5206\u79bb\u7f16\u7801\u65cb\u8f6c\u548c\u5e73\u79fb\u5206\u91cf\uff0c\u5728\u591a\u4efb\u52a1\u591a\u7c7b\u522b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d566D\u59ff\u6001\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u59ff\u6001\u7684\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u4e00\u81f4\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u5c42\u6392\u5e8f\u65b9\u6848\uff0c\u5206\u79bb\u65cb\u8f6c\u548c\u5e73\u79fb\u7684\u7f16\u7801\uff0c\u5e76\u8bbe\u8ba1\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u3002", "result": "\u5728REAL275\u548cCAMERA25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "HRC-Pose\u901a\u8fc7\u5b66\u4e60\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.14561", "pdf": "https://arxiv.org/pdf/2508.14561", "abs": "https://arxiv.org/abs/2508.14561", "authors": ["Sukhyun Jeong", "Hong-Gi Shin", "Yong-Hoon Choi"], "title": "Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent progress in text-to-motion has advanced both 3D human motion\ngeneration and text-based motion control. Controllable motion generation\n(CoMo), which enables intuitive control, typically relies on pose code\nrepresentations, but discrete pose codes alone cannot capture fine-grained\nmotion details, limiting expressiveness. To overcome this, we propose a method\nthat augments pose code-based latent representations with continuous motion\nfeatures using residual vector quantization (RVQ). This design preserves the\ninterpretability and manipulability of pose codes while effectively capturing\nsubtle motion characteristics such as high-frequency details. Experiments on\nthe HumanML3D dataset show that our model reduces Frechet inception distance\n(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.\nQualitative analysis of pairwise direction similarity between pose codes\nfurther confirms the model's controllability for motion editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u59ff\u6001\u7801\u548c\u8fde\u7eed\u8fd0\u52a8\u7279\u5f81\u7684\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u63d0\u5347\u7ec6\u8282\u8868\u8fbe\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u4e86FID\u5e76\u63d0\u9ad8\u4e86R-Precision\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u79bb\u6563\u59ff\u6001\u7801\u7684\u53ef\u63a7\u8fd0\u52a8\u751f\u6210\uff08CoMo\uff09\u65b9\u6cd5\u5728\u6355\u6349\u7cbe\u7ec6\u8fd0\u52a8\u7ec6\u8282\uff08\u5982\u9ad8\u9891\u7ec6\u8282\uff09\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u5c06\u8fde\u7eed\u8fd0\u52a8\u7279\u5f81\u4e0e\u79bb\u6563\u59ff\u6001\u7801\u7ed3\u5408\uff0c\u4fdd\u7559\u59ff\u6001\u7801\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u64cd\u63a7\u6027\u7684\u540c\u65f6\u589e\u5f3a\u7ec6\u8282\u8868\u73b0\u3002", "result": "\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\uff0cFID\u4ece0.041\u964d\u81f30.015\uff0cTop-1 R-Precision\u4ece0.508\u63d0\u5347\u81f30.510\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u751f\u6210\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u8fd0\u52a8\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u624b\u6bb5\u3002"}}
{"id": "2508.14767", "pdf": "https://arxiv.org/pdf/2508.14767", "abs": "https://arxiv.org/abs/2508.14767", "authors": ["Fabian Holst", "Emre G\u00fclsoylu", "Simone Frintrop"], "title": "Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels", "categories": ["cs.CV", "cs.RO"], "comment": "Author version of the submission to the IEEE Journal of Oceanic\n  Engineering", "summary": "The paper presents a novel technique for creating a 6D pose estimation\ndataset for marine vessels by fusing monocular RGB images with Automatic\nIdentification System (AIS) data. The proposed technique addresses the\nlimitations of relying purely on AIS for location information, caused by issues\nlike equipment reliability, data manipulation, and transmission delays. By\ncombining vessel detections from monocular RGB images, obtained using an object\ndetection network (YOLOX-X), with AIS messages, the technique generates 3D\nbounding boxes that represent the vessels' 6D poses, i.e. spatial and\nrotational dimensions. The paper evaluates different object detection models to\nlocate vessels in image space. We also compare two transformation methods\n(homography and Perspective-n-Point) for aligning AIS data with image\ncoordinates. The results of our work demonstrate that the Perspective-n-Point\n(PnP) method achieves a significantly lower projection error compared to\nhomography-based approaches used before, and the YOLOX-X model achieves a mean\nAverage Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold\nof 0.5 for relevant vessel classes. We show indication that our approach allows\nthe creation of a 6D pose estimation dataset without needing manual annotation.\nAdditionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a\npublicly available dataset comprising 3753 images with 3D bounding box\nannotations for pose estimation, created by our data fusion approach. This\ndataset can be used for training and evaluating 6D pose estimation networks. In\naddition we introduce a set of 1000 images with 2D bounding box annotations for\nship detection from the same scene.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u878d\u5408\u5355\u76eeRGB\u56fe\u50cf\u548cAIS\u6570\u636e\u751f\u62106D\u4f4d\u59ff\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7eafAIS\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8eAIS\u6570\u636e\u7684\u53ef\u9760\u6027\u3001\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u5355\u7eaf\u4f9d\u8d56AIS\u6570\u636e\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u8239\u8236\u76846D\u4f4d\u59ff\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u56fe\u50cf\u6570\u636e\u6539\u8fdb\u3002", "method": "\u7ed3\u5408YOLOX-X\u7f51\u7edc\u68c0\u6d4b\u7684\u8239\u8236\u56fe\u50cf\u4e0eAIS\u6570\u636e\uff0c\u751f\u62103D\u8fb9\u754c\u6846\u8868\u793a6D\u4f4d\u59ff\uff0c\u5e76\u6bd4\u8f83\u4e86Homography\u548cPnP\u4e24\u79cd\u5750\u6807\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "PnP\u65b9\u6cd5\u7684\u6295\u5f71\u8bef\u5dee\u663e\u8457\u4f4e\u4e8eHomography\uff0cYOLOX-X\u6a21\u578b\u7684mAP\u4e3a0.80\uff0c\u5e76\u751f\u6210\u4e86\u516c\u5f00\u6570\u636e\u96c6BONK-pose\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u751f\u62106D\u4f4d\u59ff\u6570\u636e\u96c6\uff0c\u4e3a\u8239\u8236\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2508.14893", "pdf": "https://arxiv.org/pdf/2508.14893", "abs": "https://arxiv.org/abs/2508.14893", "authors": ["Qinhong Zhou", "Hongxin Zhang", "Xiangye Lin", "Zheyuan Zhang", "Yutian Chen", "Wenjun Liu", "Zunzhe Zhang", "Sunli Chen", "Lixing Fang", "Qiushi Lyu", "Xinyu Sun", "Jincheng Yang", "Zeyuan Wang", "Bao Chi Dang", "Zhehuan Chen", "Daksha Ladia", "Jiageng Liu", "Chuang Gan"], "title": "Virtual Community: An Open World for Humans, Robots, and Society", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "website https://virtual-community-ai.github.io/", "summary": "The rapid progress in AI and Robotics may lead to a profound societal\ntransformation, as humans and robots begin to coexist within shared\ncommunities, introducing both opportunities and challenges. To explore this\nfuture, we present Virtual Community-an open-world platform for humans, robots,\nand society-built on a universal physics engine and grounded in real-world 3D\nscenes. With Virtual Community, we aim to study embodied social intelligence at\nscale: 1) How robots can intelligently cooperate or compete; 2) How humans\ndevelop social relations and build community; 3) More importantly, how\nintelligent robots and humans can co-exist in an open world. To support these,\nVirtual Community features: 1) An open-source multi-agent physics simulator\nthat supports robots, humans, and their interactions within a society; 2) A\nlarge-scale, real-world aligned community generation pipeline, including vast\noutdoor space, diverse indoor scenes, and a community of grounded agents with\nrich characters and appearances. Leveraging Virtual Community, we propose two\nnovel challenges. The Community Planning Challenge evaluates multi-agent\nreasoning and planning ability in open-world settings, such as cooperating to\nhelp agents with daily activities and efficiently connecting other agents. The\nCommunity Robot Challenge requires multiple heterogeneous robots to collaborate\nin solving complex open-world tasks. We evaluate various baselines on these\ntasks and demonstrate the challenges in both high-level open-world task\nplanning and low-level cooperation controls. We hope that Virtual Community\nwill unlock further study of human-robot coexistence within open-world\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVirtual Community\u7684\u5f00\u653e\u4e16\u754c\u5e73\u53f0\uff0c\u7528\u4e8e\u7814\u7a76\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5171\u5b58\u7684\u793e\u4ea4\u667a\u80fd\uff0c\u5305\u62ec\u5408\u4f5c\u3001\u7ade\u4e89\u53ca\u793e\u4f1a\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6311\u6218\u3002", "motivation": "\u63a2\u7d22AI\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5982\u4f55\u5f15\u53d1\u793e\u4f1a\u53d8\u9769\uff0c\u7814\u7a76\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u5171\u5b58\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u901a\u7528\u7269\u7406\u5f15\u64ce\u548c\u771f\u5b9e\u4e16\u754c3D\u573a\u666f\u6784\u5efaVirtual Community\u5e73\u53f0\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u5927\u89c4\u6a21\u793e\u533a\u751f\u6210\u3002", "result": "\u63d0\u51fa\u4e86Community Planning Challenge\u548cCommunity Robot Challenge\u4e24\u4e2a\u65b0\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u4efb\u52a1\u89c4\u5212\u548c\u534f\u4f5c\u63a7\u5236\u7684\u6311\u6218\u3002", "conclusion": "Virtual Community\u5e73\u53f0\u4e3a\u672a\u6765\u7814\u7a76\u4eba\u673a\u5171\u5b58\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
