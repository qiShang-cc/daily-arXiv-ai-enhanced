<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 16]
- [math.FA](#math.FA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Two-Stage Prony-Based Estimation of Fractional Delay and Doppler Shifts in OTFS Modulation](https://arxiv.org/abs/2506.17599)
*Yutaka Jitsumatsu,Liangchen Sun*

Main category: eess.SP

TL;DR: 本文提出了一种基于Prony技术的两阶段估计方法，用于在高移动性环境中准确估计多路径信道的分数延迟和多普勒频移，解决了传统方法在噪声环境下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信（ISAC）系统在高移动性环境中需要准确估计分数延迟和多普勒频移，以补偿多路径信道引起的双选择性衰落。现有方法在分数延迟和多普勒分量下性能受限。

Method: 采用两阶段估计方法：第一阶段通过联合求解M个耦合Prony方程估计多普勒频率；第二阶段对每个多普勒分量应用DFT和Prony方法估计延迟。

Result: 在无噪声条件下，该方法可准确估计最多N-1个延迟-多普勒参数；在噪声环境中，采用启发式模型阶数选择优化性能。

Conclusion: 数值模拟表明，该方法具有高估计精度，适用于未来ISAC系统。

Abstract: This paper addresses the estimation of fractional delay and Doppler shifts in
multipath channels that cause doubly selective fading-an essential task for
integrated sensing and communication (ISAC) systems in high-mobility
environments. Orthogonal Time Frequency Space (OTFS) modulation enables simple
and robust channel compensation under such conditions. However, fractional
delay and Doppler components introduce inter-path interference, degrading
estimation accuracy. We propose a two-stage estimation method based on Prony's
technique using OTFS pilot signals with M subchannels and N pilot repetitions.
In the first stage, Doppler frequencies are estimated by jointly solving M
coupled Prony equations, exploiting the periodicity of the pilot signal. In the
second stage, delays are estimated by applying the discrete Fourier transform
(DFT) and Prony's method to each Doppler component obtained in the first stage.
The proposed method can accurately estimate up to N-1 delay-Doppler parameters
under noiseless conditions. In noisy environments, conventional information
criteria such as AIC and BIC yield suboptimal performance; thus, a heuristic
model order selection is adopted. Numerical simulations confirm that the
proposed method achieves high estimation accuracy, highlighting its potential
for future ISAC frameworks.

</details>


### [2] [Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis](https://arxiv.org/abs/2506.17740)
*Pengyu Han,Zeyi Liu,Shijin Chen,Dongliang Zou,Xiao He*

Main category: eess.SP

TL;DR: 该论文研究多工况故障诊断，提出了一种两阶段诊断框架，结合域泛化编码器和再训练策略，提升了在显著工况影响下的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 多工况故障诊断在工业系统中普遍存在，但传统方法难以处理不同工况下的数据分布差异，且现有域泛化方法在工况显著影响故障特征时可能失效。

Method: 通过实验评估现有端到端域泛化方法在变速和变载工况下的表现，并提出一个两阶段诊断框架，结合域泛化编码器和再训练策略。

Result: 实验验证了所提框架在真实齿轮箱数据集上的有效性，能够提取工况不变的故障特征并减轻对源域的过拟合。

Conclusion: 该研究为显著工况影响下的多工况故障诊断提供了一种有效的解决方案，并通过实验验证了其优越性。

Abstract: Multi-condition fault diagnosis is prevalent in industrial systems and
presents substantial challenges for conventional diagnostic approaches. The
discrepancy in data distributions across different operating conditions
degrades model performance when a model trained under one condition is applied
to others. With the recent advancements in deep learning, transfer learning has
been introduced to the fault diagnosis field as a paradigm for addressing
multi-condition fault diagnosis. Among these methods, domain generalization
approaches can handle complex scenarios by extracting condition-invariant fault
features. Although many studies have considered fault diagnosis in specific
multi-condition scenarios, the extent to which operating conditions affect
fault information has been scarcely studied, which is crucial. However, the
extent to which operating conditions affect fault information has been scarcely
studied, which is crucial. When operating conditions have a significant impact
on fault features, directly applying domain generalization methods may lead the
model to learn condition-specific information, thereby reducing its overall
generalization ability. This paper investigates the performance of existing
end-to-end domain generalization methods under varying conditions, specifically
in variable-speed and variable-load scenarios, using multiple experiments on a
real-world gearbox. Additionally, a two-stage diagnostic framework is proposed,
aiming to improve fault diagnosis performance under scenarios with significant
operating condition impacts. By incorporating a domain-generalized encoder with
a retraining strategy, the framework is able to extract condition-invariant
fault features while simultaneously alleviating potential overfitting to the
source domain. Several experiments on a real-world gearbox dataset are
conducted to validate the effectiveness of the proposed approach.

</details>


### [3] [Machine Learning-Based Near-Field Localization in Mixed LoS/NLoS Scenarios](https://arxiv.org/abs/2506.17810)
*Parisa Ramezani,Seyed Jalaleddin Mousavirad,Mattias O'Nils,Emil Björnson*

Main category: eess.SP

TL;DR: 提出了一种基于机器学习的3D近场源定位方法，取代了传统MUSIC算法的高计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 传统MUSIC算法在近场场景中因3D网格搜索导致计算复杂度高，尤其是在混合视距/非视距场景下效率低下。

Method: 利用卷积神经网络（CNN）学习接收信号协方差矩阵特征向量与源3D位置之间的映射关系。

Result: 数值模拟验证了CNN方法的有效性和时间效率。

Conclusion: CNN方法在近场混合场景中提供了一种高效解决方案。

Abstract: The conventional MUltiple SIgnal Classification (MUSIC) algorithm is
effective for angle-of-arrival estimation in the far-field and can be extended
for full source localization in the near-field. However, it suffers from high
computational complexity, which becomes especially prohibitive in near-field
scenarios due to the need for exhaustive 3D grid searches. This paper presents
a machine learning-based approach for 3D localization of near-field sources in
mixed line-of-sight (LoS)/non-LoS scenarios. A convolutional neural network
(CNN) learns the mapping between the eigenvectors of the received signal's
covariance matrix at the anchor node and the sources' 3D locations. The
detailed description of the proposed CNN model is provided. The effectiveness
and time efficiency of the proposed CNN-based localization approach is
corroborated via numerical simulations.

</details>


### [4] [Near-Field Propagation and Spatial Non-Stationarity Channel Model for 6-24 GHz (FR3) Extremely Large-Scale MIMO: Adopted by 3GPP for 6G](https://arxiv.org/abs/2506.17887)
*Huixin Xu,Jianhua Zhang,Pan Tang,Hongbo Xing,Haiyang Miao,Nan Zhang,Jian Li,Jianming Wu,Wenfei Yang,Zhening Zhang,Wei Jiang,Zijian He,Afshin Haghighat,Qixing Wang,Guangyi Liu*

Main category: eess.SP

TL;DR: 本文提出了一个适用于6-24 GHz频段XL-MIMO系统的信道建模框架，结合了近场传播和空间非平稳性特征，并被3GPP采纳。新模型通过球面波源距离和随机方法建模近场和SNS效应，验证了其优于传统远场模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3GPP信道模型在超大规模天线阵列（XL-MIMO）和高频段（6-24 GHz）下，远场和空间平稳性假设失效，需解决近场传播和SNS（空间非平稳性）的新特征建模问题。

Method: 1. 采用球面波源距离建模近场传播，表征相位和角度的非线性变化；2. 提出基于随机方法的SNS建模（能见度概率和区域）和物理遮挡法；3. 在3GPP现有框架内集成仿真。

Result: 近场模型显示更高的信道容量潜力；SNS模型导致更显著的传播衰落（耦合损耗结果）。

Conclusion: 新框架成功整合了近场和SNS特征，验证了其在XL-MIMO系统中的有效性，为未来6G标准化提供了实用工具。

Abstract: Next generation cellular deployments are expected to exploit the 6-24 GHz
frequency range 3 (FR3) and extremely large-scale multiple-input
multiple-output (XL-MIMO) to enable ultra-high data rates and reliability.
However, the significantly enlarged antenna apertures and higher carrier
frequencies render the far-field and spatial stationarity assumptions in the
existing 3rd generation partnership project (3GPP) channel models invalid,
giving rise to new features such as near-field propagation and spatial
non-stationarity (SNS). Despite extensive prior research, incorporating these
new features within the standardized channel modeling framework remains an open
issue. To address this, this paper presents a channel modeling framework for
XL-MIMO systems that incorporates both near-field and SNS features, adopted by
3GPP. For the near-field propagation feature, the framework models the
distances from the base station (BS) and user equipment to the spherical-wave
sources associated with clusters. These distances are used to characterize
element-wise variations of path parameters, such as nonlinear changes in phase
and angle. To capture the effect of SNS at the BS side, a stochastic-based
approach is proposed to model SNS caused by incomplete scattering, by
establishing power attenuation factors from visibility probability and
visibility region to characterize antenna element-wise path power variation. In
addition, a physical blocker-based approach is introduced to model SNS effects
caused by partial blockage. Finally, a simulation framework for near-field and
SNS is developed within the structure of the existing 3GPP channel model.
Performance evaluations demonstrate that the near-field model captures higher
channel capacity potential compared to the far-field model. Coupling loss
results indicate that SNS leads to more pronounced propagation fading relative
to the spatial stationary model.

</details>


### [5] [ISAC Network Planning: Sensing Coverage Analysis and 3-D BS Deployment Optimization](https://arxiv.org/abs/2506.18009)
*Kaitao Meng,Kawon Han,Christos Masouros,Lajos Hanzo*

Main category: eess.SP

TL;DR: 论文研究了集成感知与通信（ISAC）网络中基站部署对目标定位覆盖与通信性能的影响，提出了优化部署策略以同时实现高精度定位和通信服务。


<details>
  <summary>Details</summary>
Motivation: 为了在ISAC网络中实现高精度目标定位和高吞吐量数据服务的平衡，需要从基站部署的角度研究感知与通信之间的权衡问题。

Method: 通过分析时间飞行（ToF）定位的性能覆盖问题，提出最小化区域Cramer-Rao下界（A-CRLB）的策略，确保服务区域内定位精度的均匀性，并推导了基站部署与A-CRLB的近似比例关系。

Result: 研究发现，当服务区域按比例扩大时，最优A-CRLB与路径损耗指数的平方成正比；合作基站可扩展覆盖范围，但随着感知区域维度的增加，A-CRLB的改善效果有限。

Conclusion: 论文通过理论分析为ISAC网络中基站部署提供了优化依据，为实现均匀高精度定位和高效通信奠定了理论基础。

Abstract: Integrated sensing and communication (ISAC) networks strive to deliver both
high-precision target localization and high-throughput data services across the
entire coverage area. In this work, we examine the fundamental trade-off
between sensing and communication from the perspective of base station (BS)
deployment. Furthermore, we conceive a design that simultaneously maximizes the
target localization coverage, while guaranteeing the desired communication
performance. In contrast to existing schemes optimized for a single target, an
effective network-level approach has to ensure consistent localization accuracy
throughout the entire service area. While employing time-of-flight (ToF) based
localization, we first analyze the deployment problem from a
localization-performance coverage perspective, aiming for minimizing the area
Cramer-Rao Lower Bound (A-CRLB) to ensure uniformly high positioning accuracy
across the service area. We prove that for a fixed number of BSs, uniformly
scaling the service area by a factor \kappa increases the optimal A-CRLB in
proportion to \kappa^{2\beta}, where \beta is the BS-to-target pathloss
exponent. Based on this, we derive an approximate scaling law that links the
achievable A-CRLB across the area of interest to the dimensionality of the
sensing area. We also show that cooperative BSs extends the coverage but yields
marginal A-CRLB improvement as the dimensionality of the sensing area grows.

</details>


### [6] [Cooperative Bistatic ISAC Systems for Low-Altitude Economy](https://arxiv.org/abs/2506.18067)
*Zhenkun Zhang,Yining Xu,Cunhua Pan,Hong Ren,Yiming Yu,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 论文提出了一种在MIMO-OFDM蜂窝网络中基于协作双基地ISAC框架的低复杂度算法，用于低空经济应用中的多目标定位和速度估计。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速发展需要能够在硬件和覆盖限制下实现高精度多目标定位和速度估计的ISAC系统。

Method: 采用CP张量分解和最小生成树（MST）方法，开发了一种低复杂度参数提取算法和鲁棒融合方案。

Result: 仿真结果表明，该框架在低空场景下具有计算高效性和优越的感知性能。

Conclusion: 提出的框架通过标准化5G NR基础设施，能够为低空经济应用提供强大的感知服务。

Abstract: The burgeoning low-altitude economy (LAE) necessitates integrated sensing and
communication (ISAC) systems capable of high-accuracy multi-target localization
and velocity estimation under hardware and coverage constraints inherent in
conventional ISAC architectures. This paper addresses these challenges by
proposing a cooperative bistatic ISAC framework within MIMO-OFDM cellular
networks, enabling robust sensing services for LAE applications through
standardized 5G New Radio (NR) infrastructure. We first develop a
low-complexity parameter extraction algorithm employing CANDECOMP/PARAFAC (CP)
tensor decomposition, which exploits the inherent Vandermonde structure in
delay-related factor matrices to efficiently recover bistatic ranges, Doppler
velocities, and angles-of-arrival (AoA) from multi-dimensional received signal
tensors. To resolve data association ambiguity across distributed
transmitter-receiver pairs and mitigate erroneous estimates, we further design
a robust fusion scheme based on the minimum spanning tree (MST) method,
enabling joint 3D position and velocity reconstruction. Comprehensive
simulation results validate the framework's superiority in computational
efficiency and sensing performance for low-altitude scenarios.

</details>


### [7] [Coherent Track-Before-Detect](https://arxiv.org/abs/2506.18177)
*Mingchao Liang,Florian Meyer*

Main category: eess.SP

TL;DR: 本文提出了一种基于全面信号模型的相干TBD方法，用于精确跟踪复杂环境中的多对象，克服了传统方法的局限，并在实验中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中准确跟踪未知且随时间变化的对象数量是许多应用中的基本需求，但传统方法存在信息丢失和性能不足的问题。

Method: 提出了一种新的相干TBD方法，通过全面的信号模型和因子图表示，开发了可扩展的信念传播算法进行高效贝叶斯推理。

Result: 实验结果表明，该方法在合成和真实数据上均优于传统多对象跟踪方法。

Conclusion: 相干TBD方法能够更准确地表示数据生成过程的物理特性，适用于多种应用场景。

Abstract: Accurately tracking an unknown and time-varying number of objects in complex
environments is a significant challenge but a fundamental capability in a
variety of applications, including applied ocean sciences, surveillance,
autonomous driving, and wireless communications. Conventional Bayesian
multiobject tracking (MOT) methods typically employ a detect-then-track (DTT)
approach, where a frontend detector preprocesses raw sensor data to extract
measurements for MOT. The irreversible nature of this preprocessing step can
discard valuable object-related information, particularly impairing the ability
to resolve weak or closely spaced objects. The track-before-detect (TBD)
paradigm offers an alternative by operating directly on sensor data. However,
existing TBD approaches introduce simplifications to facilitate the development
of inference methods, such as assuming known signal amplitudes or conditional
independence between sensor measurements given object states. These assumptions
can lead to suboptimal performance and limit the applicability of the resulting
TBD methods in realistic scenarios.
  This paper introduces coherent TBD based on a comprehensive signal model for
sensor data. The new model accounts for sensor data correlations and amplitude
fluctuations, enabling the accurate representation of the physics of the
data-generating process in TBD. Coherent TBD is suitable for a wide range of
problems in active and passive radar, active and passive sonar, as well as
integrated sensing and communication systems. Based on a factor graph
representation of the new measurement model, a scalable belief propagation (BP)
method is developed to perform efficient Bayesian inference. Experimental
results, performed with both synthetic and real data, demonstrate that the
proposed method outperforms state-of-the-art conventional MOT methods.

</details>


### [8] [Multimodal Visual Image Based User Association and Beamforming Using Graph Neural Networks](https://arxiv.org/abs/2506.18218)
*Yinghan Li,Yiming Liu,Wei Yu*

Main category: eess.SP

TL;DR: 该论文提出一种多模态数据集成方法，结合视觉图像和射频导频优化下行无线蜂窝网络中的用户关联和波束成形，以提高公平性并减少开销。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖精确的信道状态信息（CSI），导致高开销和延迟，且用户关联和波束成形是非凸优化问题，难以解析求解。

Method: 结合视觉图像和射频导频，通过检测神经网络定位用户，随后用两个图神经网络（GNN）提取特征，最后用多模态GNN集成特征进行联合优化。

Result: 模拟结果显示该方法性能优越，计算复杂度低，且具有可解释性和通用性。

Conclusion: 相比仅依赖射频导频的传统方法，该多模态集成方法更有效。

Abstract: This paper proposes an approach that leverages multimodal data by integrating
visual images with radio frequency (RF) pilots to optimize user association and
beamforming in a downlink wireless cellular network under a max-min fairness
criterion. Traditional methods typically optimize wireless system parameters
based on channel state information (CSI). However, obtaining accurate CSI
requires extensive pilot transmissions, which lead to increased overhead and
latency. Moreover, the optimization of user association and beamforming is a
discrete and non-convex optimization problem, which is challenging to solve
analytically. In this paper, we propose to incorporate visual camera data in
addition to the RF pilots to perform the joint optimization of user association
and beamforming. The visual image data helps enhance channel awareness, thereby
reducing the dependency on extensive pilot transmissions for system
optimization. We employ a learning-based approach based on using first a
detection neural network that estimates user locations from images, and
subsequently two graph neural networks (GNNs) that extract features for system
optimization based on the location information and the received pilots,
respectively. Then, a multimodal GNN is constructed to integrate the features
for the joint optimization user association and beamforming. Simulation results
demonstrate that the proposed method achieves superior performance, while
having low computational complexity and being interpretable and generalizable,
making it an effective solution as compared to traditional methods based only
on RF pilots.

</details>


### [9] [LLM-Integrated Digital Twins for Hierarchical Resource Allocation in 6G Networks](https://arxiv.org/abs/2506.18293)
*Majumder Haider,Imtiaz Ahmed,Zoheb Hassan,Kamrul Hasan,H. Vincent Poor*

Main category: eess.SP

TL;DR: 摘要提出了一种名为LLM-DTNet的分层框架，结合数字孪生（DTs）和大语言模型（LLMs），用于下一代无线网络的实时资源管理优化。讨论了其设计、效果和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要智能、可扩展且上下文感知的无线电资源管理（RRM），以应对超密集部署、多样化服务需求和动态网络条件。

Method: 提出了LLM-DTNet，一个分层框架，通过多层的DT架构和基于LLM的编排实现自适应、实时的RRM。

Result: LLM-DTNet在主动和情境感知的网络管理中表现出有效性，适用于地面和非地面应用。

Conclusion: 文章强调了未来研究方向，包括可扩展DT建模、安全的LLM-DT集成、能效实现和多模态数据处理。

Abstract: Next-generation (NextG) wireless networks are expected to require
intelligent, scalable, and context-aware radio resource management (RRM) to
support ultra-dense deployments, diverse service requirements, and dynamic
network conditions. Digital twins (DTs) offer a powerful tool for network
management by creating high-fidelity virtual replicas that model real-time
network behavior, while large language models (LLMs) enhance decision-making
through their advanced generalization and contextual reasoning capabilities.
This article proposes LLM-driven DTs for network optimization (LLM-DTNet), a
hierarchical framework that integrates multi-layer DT architectures with
LLM-based orchestration to enable adaptive, real-time RRM in heterogeneous
NextG networks. We present the fundamentals and design considerations of
LLM-DTNet while discussing its effectiveness in proactive and situation-aware
network management across terrestrial and non-terrestrial applications.
Furthermore, we highlight key challenges, including scalable DT modeling,
secure LLM-DT integration, energy-efficient implementations, and multimodal
data processing, shaping future advancements in NextG intelligent wireless
networks.

</details>


### [10] [ARSAR-Net: Adaptively Regularized SAR Imaging Network via Non-matrix-inversion ADMM](https://arxiv.org/abs/2506.18324)
*Shiping Fu,Yufan Chen,Zhe Zhang,Xiaolan Qiu,Qixiang Ye*

Main category: eess.SP

TL;DR: 本文提出了一种可学习正则化的SAR成像网络（ARSAR-Net），通过引入非矩阵逆ADMM算法提升了通用性和计算效率，实现了更快的成像速度和更高的成像质量。


<details>
  <summary>Details</summary>
Motivation: 传统展开网络缺乏跨场景的泛化能力，本文旨在通过可学习正则化和改进算法提升SAR成像的性能和适应性。

Method: 提出ARSAR-Net，结合可学习正则化和非矩阵逆ADMM算法，改进2D信号处理的效率。

Result: 实验表明，ARSAR-Net比现有方法快50%，成像质量提升2.0 dB PSNR，适应复杂异构场景。

Conclusion: ARSAR-Net为高效且通用的SAR成像系统设立了新标杆。

Abstract: Deep unfolding networks have constituted an emerging method for synthetic
aperture radar (SAR) imaging. However, baseline unfolding networks, when
unfolded from the alternating direction method of multipliers (ADMM), lack
generalization capability across scenes as the regularizers are empirically
designed. In this study, we introduce a learnable regularizer to the unfolding
network and create an adaptively regularized SAR imaging network (ARSAR-Net),
which pursues generalization capability across scenes, including varying
sparsity levels and heterogeneous scenes with offshore ships, islands, urban
areas, and mountainous terrain. In practice, the vanilla ARSAR-Net suffers from
inherent structural limitations in 2D signal processing, primarily due to its
reliance on matrix inversion. To conquer this challenge, we introduce the
non-matrix-inversion ADMM algorithm, which replaces inefficient matrix
inversion with efficient linear approximations. Extensive validation through
simulated and real-data experiments (including GF-3 satellite echoes)
demonstrates ARSAR-Net's triple advantage: (1) 50\% faster imaging speed
compared to existing unfolding networks, (2) up to 2.0 dB PSNR improvement in
imaging quality, and (3) enhanced adaptability to complex heterogeneous scenes.
These advancements establish a new paradigm for computationally efficient and
generalizable SAR imaging systems.

</details>


### [11] [Generative Diffusion Receivers: Achieving Pilot-Efficient MIMO-OFDM Communications](https://arxiv.org/abs/2506.18419)
*Yuzhi Yang,Omar Alhussein,Atefeh Arani,Zhaoyang Zhang,Mérouane Debbah*

Main category: eess.SP

TL;DR: 论文提出一种基于扩散模型的MIMO-OFDM接收器，通过结合生成模型与传统信号估计方法，显著降低了信道重构误差，尤其在低导频条件下表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统无线接收器在信道矩阵表征方面存在不足，而神经网络虽具潜力，但与传统方法结合时面临挑战。扩散模型因其对噪声的鲁棒性受到关注。

Method: 采用扩散模型重新评估MIMO-OFDM接收器，引入想象力筛选策略指导扩散过程，并利用同一神经网络适应不同噪声水平和导频方案。

Result: 在4-6导频/64子载波和SNR为-4 dB至0 dB条件下，信道重构误差降低至领先深度学习模型的一半，低导频条件下提升最显著。

Conclusion: 扩散模型在MIMO-OFDM接收器中表现出色，显著提升了性能，尤其在低导频和高噪声环境中，且通过想象力大小可进一步优化性能。

Abstract: This paper focuses on wireless multiple-input multiple-output
(MIMO)-orthogonal frequency division multiplex (OFDM) receivers. Traditional
wireless receivers have relied on mathematical modeling and Bayesian inference,
achieving remarkable success in most areas but falling short in their ability
to characterize channel matrices. Neural networks (NNs) have demonstrated
significant potential in this aspect. Nevertheless, integrating traditional
inference methods with NNs presents challenges, particularly in tracking the
error progression. Given the inevitable presence of noise in wireless systems,
generative models that are more resilient to noise are garnering increased
attention. In this paper, we propose re-evaluating the MIMO-OFDM receiver using
diffusion models, which is a common generative approach. With diffusion models,
we can effectively leverage prior knowledge of channel matrices and incorporate
traditional signal estimation components. Specifically, we explore the
diffusion system and introduce an imagination-screening strategy to guide the
diffusion process. Furthermore, diffusion models enable adaptation to varying
noise levels and pilot schemes using the same NN, significantly reducing
training and deployment costs. Simulated results reveal that, for pilot
densities ranging from 4-6 pilots per 64-subcarrier block and signal-to-noise
ratios (SNRs) from -4 dB to 0 dB, our proposed receiver reduces
channel-reconstruction error by up to two times compared to leading
deep-learning models, with the most pronounced improvements observed in
low-pilot conditions. Additionally, performance enhancements can be achieved
with a larger imagination size, despite increased computational complexity.

</details>


### [12] [A New Pathway to Integrated Learning and Communication (ILAC): Large AI Model and Hyperdimensional Computing for Communication](https://arxiv.org/abs/2506.18432)
*Wei Xu,Zhaohui Yang,Derrick Wing Kwan Ng,Robert Schober,H. Vincent Poor,Zhaoyang Zhang,Xiaohu You*

Main category: eess.SP

TL;DR: 6G网络需要AI与无线通信的无缝集成以实现高效通信和强大学习性能，提出了集成学习与通信（ILAC）框架，但实践中面临通信限制和AI学习动态的挑战。通过案例研究展示了如何在性能与成本之间找到最优平衡。


<details>
  <summary>Details</summary>
Motivation: 6G网络的迅速发展要求AI与无线通信的集成，以支持需要高效通信和强大学习性能的智能应用。

Method: 提出了集成学习与通信（ILAC）框架，并采用Dinkelbach和交替优化算法解决任务分配、模型选择、带宽分配和功率控制的联合优化问题。

Result: 通过优化算法，实现了学习性能与通信约束之间的最佳平衡。

Conclusion: ILAC框架为解决6G网络中AI与通信的集成问题提供了有效方案，但需进一步优化以应对实际挑战。

Abstract: The rapid evolution of forthcoming sixth-generation (6G) wireless networks
necessitates the seamless integration of artificial intelligence (AI) with
wireless communications to support emerging intelligent applications that
demand both efficient communication and robust learning performance. This dual
requirement calls for a unified framework of integrated learning and
communication (ILAC), where AI enhances communication through intelligent
signal processing and adaptive resource management, while wireless networks
support AI model deployment by enabling efficient and reliable data exchanges.
However, achieving this integration presents significant challenges in
practice. Communication constraints, such as limited bandwidth and fluctuating
channels, hinder learning accuracy and convergence. Simultaneously, AI-driven
learning dynamics, including model updates and task-driven inference, introduce
excessive burdens on communication systems, necessitating flexible,
context-aware transmission strategies. Finally, we present a case study on a
cost-to-performance optimization problem, where task assignments, model size
selection, bandwidth allocation, and transmission power control are jointly
optimized, considering computational cost, communication efficiency, and
inference accuracy. Leveraging the Dinkelbach and alternating optimization
algorithms, we offer a practical and effective solution to achieve an optimal
balance between learning performance and communication constraints.

</details>


### [13] [Sizing Antenna Arrays for Near-field Communication and Sensing](https://arxiv.org/abs/2506.18465)
*Marcin Wachowiak,André Bourdoux,Sofie Pollin*

Main category: eess.SP

TL;DR: 本文分析了近场通信与传感系统的性能指标随天线阵列孔径的变化规律，提供了设计大天线阵列的理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究近场通信与传感系统的性能指标随天线阵列孔径的变化规律，为系统设计提供理论指导。

Method: 通过分析不同标准阵列几何结构的近场聚焦行为，推导出最小波束深度、近场区域范围、可分辨波束数等性能指标的解析表达式。

Result: 发现最小波束深度随阵列孔径增加快速饱和，近场区域范围与孔径成二次关系，可分辨波束数与孔径成线性关系。此外，通道的显著奇异值数量与孔径呈幂律关系。

Conclusion: 提出的解析表达式为评估近场通信与传感应用中的孔径需求提供了实用设计指南。

Abstract: This paper presents key performance metrics for near-field communication and
sensing systems with a focus on their scaling behavior as a function of the
antenna array aperture. Analytical expressions are derived for several standard
array geometries to enable the design of the large antenna arrays for given
system requirements. First, the near-field beam focusing is analyzed and the
minimum beamdepth is observed to rapidly saturate to a low asymptotic limit as
the array aperture increases. In contrast, the near-field region span is shown
to scale quadratically with the array aperture. Based on these two metrics, the
maximum number of resolvable beamspots at 3 dB separation is derived
analytically, exhibiting a linear dependence on the array aperture. Finally,
the number of significant singular values of a channel observed at the array's
broadside is estimated, showing a power-law dependence on the aperture. The
resulting expressions provide practical design guidelines for evaluating
aperture requirements in near-field communication and sensing applications.

</details>


### [14] [Fast State-Augmented Learning for Wireless Resource Allocation with Dual Variable Regression](https://arxiv.org/abs/2506.18748)
*Yigit Berkay Uslu,Navid NaderiAlizadeh,Mark Eisen,Alejandro Ribeiro*

Main category: eess.SP

TL;DR: 本文提出了一种用于多用户无线网络资源分配的图神经网络方法，通过状态增强和动态对偶变量输入改进传统对偶次梯度方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统对偶次梯度方法在多用户无线网络资源分配中存在局限，需要一种更高效的优化方法。

Method: 采用状态增强的图神经网络（GNN）参数化资源分配策略，将对偶变量作为动态输入，并通过离线学习和在线推断优化性能。

Result: 数值实验验证了所提方法在传输功率控制中的优越性能，并提供了对偶函数收敛性理论证明。

Conclusion: 该方法显著提升资源分配效率，并具备理论保障。

Abstract: We consider resource allocation problems in multi-user wireless networks,
where the goal is to optimize a network-wide utility function subject to
constraints on the ergodic average performance of users. We demonstrate how a
state-augmented graph neural network (GNN) parametrization for the resource
allocation policy circumvents the drawbacks of the ubiquitous dual subgradient
methods by representing the network configurations (or states) as graphs and
viewing dual variables as dynamic inputs to the model, viewed as graph signals
supported over the graphs. Lagrangian maximizing state-augmented policies are
learned during the offline training phase, and the dual variables evolve
through gradient updates while executing the learned state-augmented policies
during the inference phase. Our main contributions are to illustrate how
near-optimal initialization of dual multipliers for faster inference can be
accomplished with dual variable regression, leveraging a secondary GNN
parametrization, and how maximization of the Lagrangian over the multipliers
sampled from the dual descent dynamics substantially improves the training of
state-augmented models. We demonstrate the superior performance of the proposed
algorithm with extensive numerical experiments in a case study of transmit
power control. Finally, we prove a convergence result and an exponential
probability bound on the excursions of the dual function (iterate) optimality
gaps.

</details>


### [15] [Variational Bayesian Channel Estimation and Data Detection for Cell-Free Massive MIMO with Low-Resolution Quantized Fronthaul Links](https://arxiv.org/abs/2506.18863)
*Sajjad Nassirpour,Toan-Van Nguyen,Hien Q. Ngo,Le-Nam Tran,Tharmalingam Ratnarajah,Duy H. N. Nguyen*

Main category: eess.SP

TL;DR: 论文提出了一种基于变分贝叶斯推断的方法，用于解决无蜂窝大规模MIMO网络中的联合信道估计与数据检测问题，并通过两种量化策略（Q-E和E-Q）验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO网络中，接入点与中央处理单元之间的前传链路带宽有限，限制了系统的可扩展性，尤其是在用户数量不断增加的情况下。因此，需要一种高效的方法来解决联合信道估计与数据检测的挑战。

Method: 提出了一种基于变分贝叶斯（VB）推断的方法，通过两种策略（Q-E和E-Q）在前传链路上传输低分辨率量化信号。

Result: 数值结果表明，VB(Q-E)和VB(E-Q)方法在符号错误率、信道估计误差、计算复杂性和前传信令开销方面均优于传统的LMMSE方法，其中VB(Q-E)表现尤为突出。

Conclusion: 基于VB的方法能够有效解决带宽限制问题，尤其是VB(Q-E)策略，因其避免了本地信道估计过程中的误差，展现了更优的性能。

Abstract: We study the joint channel estimation and data detection (JED) problem in a
cell-free massive multiple-input multiple-output (CF-mMIMO) network, where
access points (APs) communicate with a central processing unit (CPU) over
fronthaul links. However, the bandwidth of these links is limited, and thus,
presents challenges to the applicability of CF-mMIMO, especially with an
ever-increasing number of users. To address this, we propose a method based on
variational Bayesian (VB) inference for performing the JED process, where the
APs forward low-resolution quantized versions of the signals to the CPU. We
consider two approaches: \emph{quantization-and-estimation} (Q-E) and
\emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a
low-bit quantizer to quantize the signal before forwarding it to the CPU, while
in the E-Q approach, each AP first performs local channel estimation and then
sends a low-bit quantized version of the estimated channel to the CPU. We
evaluate the performance of our VB-based approach under perfect fronthaul link
(PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error
rate (SER), normalized mean square error (NMSE) of the channel estimation,
computational complexity, and fronthaul signaling overhead. We also compare
these results with those of the linear minimum mean squared error (LMMSE)
method under the PFL scenario. Our numerical results show that both the VB(Q-E)
and VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL),
benefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E)
method outperforms VB(E-Q) due to errors in the local channel estimation
process at the APs within the VB(E-Q) approach.

</details>


### [16] [Achieving 70 Gb/s Over A VCSEL-Based Optical Wireless Link Using A Multi-Mode Fiber-Coupled Receiver](https://arxiv.org/abs/2506.18864)
*Hossein Kazemi,Isaac N. O. Osahon,Nikolay Ledentsov Jr.,Ilya Titkov,Nikolay Ledentsov,Harald Haas*

Main category: eess.SP

TL;DR: 该论文展示了一种基于激光的光无线通信系统，使用940nm单模VCSEL和多模光纤耦合接收器，实现了超过70Gb/s的记录数据速率，且发射功率低于5mW。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证低成本、低功耗的VCSEL在超高速室内光无线通信系统中的可行性，为下一代LiFi连接提供新方案。

Method: 采用940nm单模VCSEL与多模光纤耦合接收器组合，利用高速光纤光电接收器避免带宽限制。

Result: 系统实现了超过70Gb/s的数据传输速率，且发射功率低于5mW，表现出超高容量和能效。

Conclusion: 实验验证了低成本VCSEL在超高速LiFi系统中的可行性，为下一代光无线通信应用提供了可能。

Abstract: In this paper, we demonstrate a laser-based optical wireless communication
(OWC) system employing a 940 nm single-mode (SM) vertical cavity surface
emitting laser (VCSEL) and a multi-mode (MM) fiber-coupled receiver, achieving
a record data rate beyond 70 Gb/s, while the optical transmit power is below 5
mW. The use of a high speed fiber-optic photoreceiver avoids limiting the
communication bandwidth by the receiver, enabling ultra-high capacity and
energy-efficient light fidelity (LiFi) links to unlock new applications. This
work experimentally validates the feasibility of ultra-high speed indoor OWC
systems using a single low-power and low-cost VCSEL for next-generation LiFi
connectivity.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [17] [A Selection of Distributions and Their Fourier Transforms with Applications in Magnetic Resonance Imaging](https://arxiv.org/abs/2506.18638)
*Kaibo Tang*

Main category: math.FA

TL;DR: 本文以数学方法介绍了信号处理中常见的分布及其傅里叶变换，特别强调MRI中的应用。


<details>
  <summary>Details</summary>
Motivation: 填补MRI教材中数学基础不足的空白，明确分布和傅里叶变换的拓扑空间定义。

Method: 采用严格的数学方法，包括泊松求和公式和ODE论证高斯函数的傅里叶变换。

Result: 提供了关键结果，如泊松求和公式和傅里叶变换的严格定义。

Conclusion: 本文为具有泛函分析和分布理论背景的读者提供了自洽的数学工具。

Abstract: This note presents a rigorous introduction to a selection of distributions
along with their Fourier transforms, which are commonly encountered in signal
processing and, in particular, magnetic resonance imaging (MRI). In contrast to
many textbooks on the principles of MRI, which place more emphasis on the
signal processing aspect, this note will take a more mathematical approach. In
particular, we will make explicit the underlying topological space of interest
and clarify the exact sense in which these distributions and their Fourier
transforms are defined. Key results presented in this note involve the Poisson
summation formula and the Fourier transform of a Gaussian function via an
ordinary differential equation (ODE) argument, etc. Although the readers are
expected to have prior exposure to functional analysis and distribution theory,
this note is intended to be self-contained.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: 该论文提出了一种基于纠错码的局部差分隐私协议，用于构建简洁直方图，利用极性码和高斯扰动提高低频项目的频率估计准确性。


<details>
  <summary>Details</summary>
Motivation: 在大规模、隐私敏感的机器学习应用中，简洁直方图对捕获高频项及其频率至关重要。为了在保证隐私的同时保持数据效用，需要一种高效的局部差分隐私（LDP）方法。

Method: 通过极性码及其逐次取消列表（SCL）解码算法，结合高斯扰动，设计了一种(ε,δ)-LDP协议。

Result: 实验表明，该方法在低频项目上优于现有方法，同时保持了相似的频率估计准确性。

Conclusion: 该协议为隐私保护下的简洁直方图构建提供了一种实用且高效的解决方案。

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [19] [Quantizing for Noisy Flash Memory Channels](https://arxiv.org/abs/2506.17646)
*Juyun Oh,Taewoo Park,Jiwoong Im,Yuval Cassuto,Yongjune Kim*

Main category: cs.IT

TL;DR: 本文提出了一种联合优化量化和验证级别的框架，以最小化均方误差，显著提高了基于闪存的PIM系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的验证级别优化方法未能满足基于闪存的PIM系统的独特需求，尤其是在MSE和PSNR等关键指标方面。

Method: 开发了一种迭代算法，联合优化量化和验证级别，同时考虑量化和闪存信道误差。

Result: 实验结果表明，该方法在量化图像和SwinIR模型参数存储方面显著提升了可靠性。

Conclusion: 该框架有效解决了高密度多级闪存中的噪声问题，为PIM系统的可靠性提供了优化方案。

Abstract: Flash memory-based processing-in-memory (flash-based PIM) offers high storage
capacity and computational efficiency but faces significant reliability
challenges due to noise in high-density multi-level cell (MLC) flash memories.
Existing verify level optimization methods are designed for general storage
scenarios and fail to address the unique requirements of flash-based PIM
systems, where metrics such as mean squared error (MSE) and peak
signal-to-noise ratio (PSNR) are critical. This paper introduces an integrated
framework that jointly optimizes quantization and verify levels to minimize the
MSE, considering both quantization and flash memory channel errors. We develop
an iterative algorithm to solve the joint optimization problem. Experimental
results on quantized images and SwinIR model parameters stored in flash memory
show that the proposed method significantly improves the reliability of
flash-based PIM systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [20] [Derandomizing Simultaneous Confidence Regions for Band-Limited Functions by Improved Norm Bounds and Majority-Voting Schemes](https://arxiv.org/abs/2506.17764)
*Balázs Csanád Csáji,Bálint Horváth*

Main category: stat.ML

TL;DR: 该论文通过改进带限函数的置信区域构建方法，结合核范数边界和多数投票，提升了统计稳定性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 带限函数在系统理论和信号处理中应用广泛，但其噪声输入输出的置信区域构建需要更精确的非参数、非渐进方法。

Method: 在Paley-Wiener再生核希尔伯特空间中，通过改进核范数边界（小样本用Hoeffding不等式，大样本用经验Bernstein界）和多数投票聚合子样本置信集。

Result: 实验验证了改进方法的有效性，证明即使每个输入聚合的区间仍保持同步覆盖保证。

Conclusion: 论文提出的改进方法在带限函数的置信区域构建中表现出更高的稳定性和覆盖能力。

Abstract: Band-limited functions are fundamental objects that are widely used in
systems theory and signal processing. In this paper we refine a recent
nonparametric, nonasymptotic method for constructing simultaneous confidence
regions for band-limited functions from noisy input-output measurements, by
working in a Paley-Wiener reproducing kernel Hilbert space. Kernel norm bounds
are tightened using a uniformly-randomized Hoeffding's inequality for small
samples and an empirical Bernstein bound for larger ones. We derive an
approximate threshold, based on the sample size and how informative the inputs
are, that governs which bound to deploy. Finally, we apply majority voting to
aggregate confidence sets from random subsamples, boosting both stability and
region size. We prove that even per-input aggregated intervals retain their
simultaneous coverage guarantee. These refinements are also validated through
numerical experiments.

</details>


### [21] [Trustworthy Prediction with Gaussian Process Knowledge Scores](https://arxiv.org/abs/2506.18630)
*Kurt Butler,Guanchao Feng,Tong Chen,Petar Djuric*

Main category: stat.ML

TL;DR: 提出一种用于高斯过程回归模型的知识评分，量化数据对预测不确定性的减少，实验表明其在异常检测、外推和缺失数据填补等任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在无观测数据的区域进行预测时，缺乏衡量预测是否可靠的方法。

Method: 提出知识评分，量化数据减少预测不确定性的程度，并展示其在高斯过程回归中的应用。

Result: 知识评分能有效预测GPR模型的准确性，并提升异常检测等任务的性能。

Conclusion: 知识评分是一种可解释、自然有界的工具，适用于评估预测的可靠性。

Abstract: Probabilistic models are often used to make predictions in regions of the
data space where no observations are available, but it is not always clear
whether such predictions are well-informed by previously seen data. In this
paper, we propose a knowledge score for predictions from Gaussian process
regression (GPR) models that quantifies the extent to which observing data have
reduced our uncertainty about a prediction. The knowledge score is
interpretable and naturally bounded between 0 and 1. We demonstrate in several
experiments that the knowledge score can anticipate when predictions from a GPR
model are accurate, and that this anticipation improves performance in tasks
such as anomaly detection, extrapolation, and missing data imputation. Source
code for this project is available online at
https://github.com/KurtButler/GP-knowledge.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [22] [A family of graph GOSPA metrics for graphs with different sizes](https://arxiv.org/abs/2506.17316)
*Jinhao Gu,Ángel F. García-Fernández,Robert E. Firth,Lennart Svensson*

Main category: cs.SI

TL;DR: 本文提出了一组用于测量不同大小图之间距离的图度量家族，扩展了图GOSPA度量并证明了其度量性质。


<details>
  <summary>Details</summary>
Motivation: 为了更灵活地处理图之间的差异，尤其是在节点属性和边缘不匹配方面，提出了更通用的度量家族。

Method: 通过定义图GOSPA度量家族的一般形式，并使用线性编程近似计算该度量。

Result: 仿真实验展示了度量家族在不同超参数下的特性，并在真实数据集上证明了其在分类任务中的优势。

Conclusion: 提出的图GOSPA度量家族提供了更灵活的边缘惩罚机制，适用于实际应用中的图比较任务。

Abstract: This paper proposes a family of graph metrics for measuring distances between
graphs of different sizes. The proposed metric family defines a general form of
the graph generalised optimal sub-pattern assignment (GOSPA) metric and is also
proved to satisfy the metric properties. Similarly to the graph GOSPA metric,
the proposed graph GOSPA metric family also penalises the node attribute costs
for assigned nodes between the two graphs, and the number of unassigned nodes.
However, the proposed family of metrics provides more general penalties for
edge mismatches than the graph GOSPA metric. This paper also shows that the
graph GOSPA metric family can be approximately computed using linear
programming. Simulation experiments are performed to illustrate the
characteristics of the proposed graph GOSPA metric family with different
choices of hyperparameters. The benefits of the proposed graph GOSPA metric
family for classification tasks are also shown on real-world datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models](https://arxiv.org/abs/2506.18124)
*Shaoxiu Wei,Mingchao Liang,Florian Meyer*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合模型驱动和数据驱动的多目标跟踪（MOT）混合方法，通过神经网络增强统计模型中的简化部分，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型驱动方法通用性强但性能有限，数据驱动方法在数据丰富时表现优异，作者希望结合两者优势。

Method: 使用神经网络改进贝叶斯MOT中的统计模型简化部分，结合置信传播和序贯蒙特卡洛方法降低计算复杂度。

Result: 在nuScenes数据集上验证，表现优于现有方法。

Conclusion: 混合方法兼具模型驱动的灵活性和数据驱动的学习能力，达到了先进性能。

Abstract: Multiobject tracking (MOT) is an important task in applications including
autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT
methods are model-based and combine sequential Bayesian estimation with data
association and an object birth model. More recent methods are fully
data-driven and rely on the training of neural networks. Both approaches offer
distinct advantages in specific settings. In particular, model-based methods
are generally applicable across a wide range of scenarios, whereas data-driven
MOT achieves superior performance in scenarios where abundant labeled data for
training is available. A natural thought is whether a general framework can
integrate the two approaches. This paper introduces a hybrid method that
utilizes neural networks to enhance specific aspects of the statistical model
in Bayesian MOT that have been identified as overly simplistic. By doing so,
the performance of the prediction and update steps of Bayesian MOT is improved.
To ensure tractable computation, our framework uses belief propagation to avoid
high-dimensional operations combined with sequential Monte Carlo methods to
perform low-dimensional operations efficiently. The resulting method combines
the flexibility and robustness of model-based approaches with the capability to
learn complex information from data of neural networks. We evaluate the
performance of the proposed method based on the nuScenes autonomous driving
dataset and demonstrate that it has state-of-the-art performance

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [24] [Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation](https://arxiv.org/abs/2506.17409)
*Quoc Thinh Vo,Joe Woods,Priontu Chowdhury,David K. Han*

Main category: cs.SD

TL;DR: 提出多分支网络架构，结合CNN和Conformer，通过自适应增益控制层提升性能，水下声源定位任务表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 海洋环境复杂多变，传统方法在噪声、几何形状和声学特性变化下定位困难，需创新解决方案。

Method: 使用CNN提取空间特征，Conformer捕捉时间依赖，输入为log-mel谱和GCC-PHAT特征，引入AGC层调整特征幅度。

Result: 在跨域测试中仅需少量数据微调，超越现有方法，为水下定位设新标准。

Conclusion: 多分支网络架构结合自适应技术有效提升复杂环境下的声源定位精度，具有泛化能力。

Abstract: Localizing acoustic sound sources in the ocean is a challenging task due to
the complex and dynamic nature of the environment. Factors such as high
background noise, irregular underwater geometries, and varying acoustic
properties make accurate localization difficult. To address these obstacles, we
propose a multi-branch network architecture designed to accurately predict the
distance between a moving acoustic source and a receiver, tested on real-world
underwater signal arrays. The network leverages Convolutional Neural Networks
(CNNs) for robust spatial feature extraction and integrates Conformers with
self-attention mechanism to effectively capture temporal dependencies. Log-mel
spectrogram and generalized cross-correlation with phase transform (GCC-PHAT)
features are employed as input representations. To further enhance the model
performance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively
adjusts the amplitude of input features, ensuring consistent energy levels
across varying ranges, signal strengths, and noise conditions. We assess the
model's generalization capability by training it in one domain and testing it
in a different domain, using only a limited amount of data from the test domain
for fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)
approaches in similar settings, establishing new benchmarks for underwater
sound localization.

</details>
