<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 18]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Access Probability Optimization in RACH: A Multi-Armed Bandits Approach](https://arxiv.org/abs/2504.14085)
*Ahmed O. Elmeligy,Ioannis Psaromiligkos,Au Minh*

Main category: eess.SP

TL;DR: 论文提出了一种两优先级随机接入信道模型，通过非均匀前导选择和强化学习优化高优先级设备的接入成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器类型通信中随机接入信道的拥塞和过载问题。

Method: 建立两优先级RACH模型，提出非均匀前导选择方案，并采用强化学习优化接入概率。

Result: 实现了高优先级设备接入成功率的近最优提升，同时限制了低优先级设备的影响。

Conclusion: 该方法有效缓解了mMTC场景下的RACH拥塞，且无需基站知道网络中的设备数量。

Abstract: The use of cellular networks for massive machine-type communications (mMTC)
is an appealing solution due to the availability of the existing
infrastructure. However, the massive number of user equipments (UEs) poses a
significant challenge to the cellular network's random access channel (RACH)
regarding congestion and overloading. To mitigate this problem, we first
present a novel approach to model a two-priority RACH, which allows us to
define access patterns that describe the random access behavior of UEs as
observed by the base station (BS). A non-uniform preamble selection scheme is
proposed, offering increased flexibility in resource allocation for different
UE priority classes. Then, we formulate an allocation model that finds the
optimal access probabilities to maximize the success rate of high-priority UEs
while constraining low-priority UEs. Finally, we develop a reinforcement
learning approach to solving the optimization problem using multi-armed
bandits, which provides a near-optimal but scalable solution and does not
require the BS to know the number of UEs in the network.

</details>


### [2] [6G WavesFM: A Foundation Model for Sensing, Communication, and Localization](https://arxiv.org/abs/2504.14100)
*Ahmed Aboulfotouh,Elsayed Mohammed,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 论文提出了WavesFM，一种新型无线基础模型框架，支持多种通信、感知和定位任务，通过共享ViT主干和任务特定MLP头部结合LoRA实现了高效参数共享，并在多个任务中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多任务无线应用中参数冗余和计算资源浪费的问题，通过基础模型实现高效、统一的解决方案。

Method: 结合ViT主干、任务特定MLP头部和LoRA技术，支持图像类无线模态（如频谱图、CSI）和IQ信号（OFDM资源网格）的处理。

Result: 在5G定位、MIMO-OFDM信道估计、人体活动感知和RF信号分类等任务中表现优于单独训练的基线模型，参数共享率达80%，且预训练可提升性能并加速5倍收敛。

Conclusion: WavesFM展示了基础模型在无线领域的潜力，为未来6G网络中的AI原生范式提供了高效、高性能的解决方案。

Abstract: This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.

</details>


### [3] [An Artificial Intelligence Enabled Signature Estimation of Dual Wideband Systems in Ultra-Low Signal-to-Noise Ratio](https://arxiv.org/abs/2504.14226)
*Chandrashekhar Rai,Debarati Sen*

Main category: eess.SP

TL;DR: 论文提出了一种基于AI的毫米波大规模MIMO系统信道特征估计框架，利用去噪卷积神经网络和局部引力聚类算法在高噪声环境中有效恢复信道响应。


<details>
  <summary>Details</summary>
Motivation: 毫米波大规模MIMO系统在用户通信信道中存在空间宽带扩展和时域宽带效应，需准确估计信道特征以设计高效波束成形收发器。

Method: 采用带残差学习的去噪卷积神经网络恢复信道响应，结合局部引力聚类算法推断物理传播路径数量及其支持区域。

Result: 方法在低至-20 dB的信噪比下成功恢复时空分集分支，并通过OFDM和QPSK调制验证了其鲁棒性。

Conclusion: 所提框架在超低信噪比场景下表现出必要性和鲁棒性，为高噪声环境中的信道估计提供了有效解决方案。

Abstract: Millimeter-wave (mmWave) massive Multiple Input Multiple Output (MIMO)
systems encounter both spatial wideband spreading and temporal wideband effects
in the communication channels of individual users. Accurate estimation of a
user's channel signature -- specifically, the direction of arrival and time of
arrival -- is crucial for designing efficient beamforming transceivers,
especially under noisy observations. In this work, we propose an Artificial
Intelligence (AI)-enabled framework for estimating the channel signature of a
user's location in mmWave massive MIMO systems. Our approach explicitly
accounts for spatial wideband spreading, finite basis leakage effects, and
significant unknown receiver noise. We demonstrate the effectiveness of a
denoising convolutional neural network with residual learning for recovering
channel responses, even when channel gains are of extremely low amplitude and
embedded in ultra-high receiver noise environments. Notably, our method
successfully recovers spatio-temporal diversity branches at signal-to-noise
ratios as low as -20 dB. Furthermore, we introduce a local gravitation-based
clustering algorithm to infer the number of physical propagation paths (unknown
a priori) and to identify their respective support in the delay-angle domain of
the denoised response. To complement our approach, we design tailored metrics
for evaluating denoising and clustering performance within the context of
wireless communications. We validate our framework through system-level
simulations using Orthogonal Frequency Division Multiplexing (OFDM) with a
Quadrature Phase Shift Keying (QPSK) modulation scheme over mmWave fading
channels, highlighting the necessity and robustness of the proposed methods in
ultra-low SNR scenarios.

</details>


### [4] [A Real-time and Hardware Efficient Artfecat-free Spike Sorting Using Deep Spike Detection](https://arxiv.org/abs/2504.14279)
*Xiaoyu Jiang,Tao Fang,Majid Zamani*

Main category: eess.SP

TL;DR: 该论文提出一种低成本、实时的深度尖峰检测框架，通过1-D CNN模型优化计算复杂性和模型大小，在FPGA平台上实现高效分类和低功耗。


<details>
  <summary>Details</summary>
Motivation: 为提升脑机接口研究中尖峰分类的效率和准确性，解决噪声和伪影干扰问题，开发低成本、实时的检测方法。

Method: 框架包含两个1-D CNN模型（通道选择和伪影去除），通过结构化剪枝、网络投影和量化优化模型，硬件层采用自定义MAC引擎和融合层等技术。

Result: 在FPGA平台上实现16.8微秒的分类延迟，准确率97.14%，功耗2.67mW；双CNN模型下准确率95.05%，功耗5.6mW。

Conclusion: 优化后的1-D CNN在保持高精度的同时显著降低计算和存储需求，适用于低功耗实时脑机接口应用。

Abstract: Spike sorting is a valuable tool in understanding brain regions. It assigns
detected spike waveforms to their origins, helping to research the mechanism of
the human brain and the development of implantable brain-machine interfaces
(iBMIs). The presence of noise and artefacts will adversely affect the efficacy
of spike sorting. This paper proposes a framework for low-cost and real-time
implementation of deep spike detection, which consists of two one-dimensional
(1-D) convolutional neural network (CNN) model for channel selection and
artefact removal. The framework utilizes simulation and hardware layers, and it
applies several low-power techniques to optimise the implementation cost of a
1-D CNN model. A compact CNN model with 210 bytes memory size is achieved using
structured pruning, network projection and quantization in the simulation
layer. The hardware layer also accommodates various techniques including a
customized multiply-accumulate (MAC) engine, novel fused layers in the
convolution pipeline and proposing flexible resource allocation for a
power-efficient and low-delay design. The optimized 1-D CNN significantly
decreases both computational complexity and model size, with only a minimal
reduction in accuracy. Classification of 1-D CNN on the Cyclone V 5CSEMA5F31C6
FPGA evaluation platform is accomplished in just 16.8 microseconds at a
frequency of 2.5 MHz. The FPGA prototype achieves an accuracy rate of 97.14% on
a standard dataset and operates with a power consumption of 2.67mW from a
supply voltage of 1.1 volts. An accuracy of 95.05% is achieved with a power of
5.6mW when deep spike detection is implemented using two optimized 1-D CNNs on
an FPGA board.

</details>


### [5] [Exploiting Symmetric Non-Convexity for Multi-Objective Symbol-Level DFRC Signal Design](https://arxiv.org/abs/2504.14281)
*Ly V. Nguyen,Rang Liu,Nhan Thanh Nguyen,Markku Juntti,Björn Ottersten,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 该论文提出了一种基于符号级预编码（SLP）的双功能雷达-通信（DFRC）信号设计方法，通过优化雷达性能并满足通信约束，利用对称旋转不变性和非凸性开发了低复杂度的DFRC算法。


<details>
  <summary>Details</summary>
Motivation: 解决双功能雷达-通信（DFRC）信号设计中的固有干扰问题，同时保证雷达和通信性能。

Method: 提出基于对称非凸性（SNC）的DFRC算法，利用雷达感知度量的对称性和非凸性，离线构造雷达解决方案以降低计算复杂度。

Result: 算法在雷达感知（如波束相似性、SINR、CRLB）和通信性能上优于现有方法，同时显著降低计算复杂度。

Conclusion: 提出的SNC算法具有通用性和高效性，适用于多种雷达感知度量，并通过离线计算进一步优化性能。

Abstract: Symbol-level precoding (SLP) is a promising solution for addressing the
inherent interference problem in dual-functional radar-communication (DFRC)
signal designs. This paper considers an SLP-DFRC signal design problem which
optimizes the radar performance under communication performance constraints. We
show that a common phase modulation applied to the transmit signals from an
antenna array does not affect the performance of different radar sensing
metrics, including beampattern similarity, signal-to-interference-plus-noise
ratio (SINR), and Cram\'er-Rao lower bound (CRLB). We refer to this as
symmetric-rotation invariance, upon which we develop low-complexity yet
efficient DFRC signal design algorithms. More specifically, we propose a
symmetric non-convexity (SNC)-based DFRC algorithm that relies on the
non-convexity of the radar sensing metrics to identify a set of radar-only
solutions. Based on these solutions, we further exploit the symmetry property
of the radar sensing metrics to efficiently design the DFRC signal. We show
that the proposed SNC-based algorithm is versatile in the sense that it can be
applied to the DFRC signal optimization of all three sensing metrics mentioned
above (beampattern, SINR, and CRLB). In addition, since the radar sensing
metrics are independent of the communication channel and data symbols, the set
of radar-only solutions can be constructed offline, thereby reducing the
computational complexity. We also develop an accelerated SNC-based algorithm
that further reduces the complexity. Finally, we numerically demonstrate the
superiority of the proposed algorithms compared to existing methods in terms of
sensing and communication performance as well as computational requirements.

</details>


### [6] [Iterative Polynomial Approximation Algorithms for Inverse Graph Filters](https://arxiv.org/abs/2504.14341)
*Cheng Cheng,Qiyu Sun,Cong Zheng*

Main category: eess.SP

TL;DR: 该论文基于切比雪夫插值多项式逼近提出了一种分布式实现多项式图滤波器逆滤波的迭代算法，具有指数收敛性，适用于计算和存储能力有限的分布式网络。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过切比雪夫插值多项式逼近，设计一种分布式算法，实现在计算和存储能力有限的网络中对多项式图滤波器逆滤波的高效计算，并验证其收敛速度优于现有方法。

Method: 提出基于切比雪夫插值多项式逼近的迭代多项式逼近算法，利用图中可交换移位操作的分布式实现，结合有限数据存储和单跳通信子系统。

Result: 仿真结果表明，所提出的算法在收敛速度上优于切比雪夫多项式逼近算法和传统梯度下降算法。

Conclusion: 本研究提出的分布式多项式逼近算法在保证指数收敛性的同时，适用于资源受限的网络环境，为高效实现逆滤波提供了可行方案。

Abstract: Chebyshev interpolation polynomials exhibit the exponential approximation
property to analytic functions on a cube. Based on the Chebyshev interpolation
polynomial approximation, we propose
  iterative polynomial approximation algorithms to implement the inverse filter
with a polynomial graph filter of commutative graph shifts in a distributed
manner. The proposed algorithms exhibit exponential convergence properties, and
they can be implemented on distributed networks in which agents are equipped
with a data processing subsystem for limited data storage and computation
power, and with a one-hop communication subsystem for direct data exchange only
with their adjacent agents. Our simulations show that the proposed polynomial
approximation algorithms may converge faster than the Chebyshev polynomial
approximation algorithm
  and the conventional gradient descent algorithm
  do.

</details>


### [7] [Beamforming Design and Association Scheme for Multi-RIS Multi-User mmWave Systems Through Graph Neural Networks](https://arxiv.org/abs/2504.14464)
*Mengbing Liu,Chongwen Huang,Ahmed Alhammadi,Marco Di Renzo,Merouane Debbah,Chau Yuen*

Main category: eess.SP

TL;DR: 本文提出了一种基于异构图神经网络（GNN）的方法，用于优化多RIS辅助通信系统中的RIS关联和波束成形设计，显著提升了系统容量。


<details>
  <summary>Details</summary>
Motivation: 多RIS系统能有效解决基站与用户间信号阻塞问题，但其非凸优化问题复杂，需采用学习型方法以充分发挥其潜力。

Method: 通过异构GNN建模无线通信环境的图拓扑，设计RIS关联方案并迭代优化波束成形，直至GNN收敛。

Result: 仿真结果表明，所提方法性能接近高复杂度交替优化算法，且优于其他基准方案，系统容量提升约30%。

Conclusion: 异构GNN在多RIS毫米波通信系统中具有高效性能，结合RIS关联方案能显著提升通信质量。

Abstract: Reconfigurable intelligent surface (RIS) is emerging as a promising
technology for next-generation wireless communication networks, offering a
variety of merits such as the ability to tailor the communication environment.
Moreover, deploying multiple RISs helps mitigate severe signal blocking between
the base station (BS) and users, providing a practical and efficient solution
to enhance the service coverage. However, fully reaping the potential of a
multi-RIS aided communication system requires solving a non-convex optimization
problem. This challenge motivates the adoption of learning-based methods for
determining the optimal policy. In this paper, we introduce a novel
heterogeneous graph neural network (GNN) to effectively leverage the graph
topology of a wireless communication environment. Specifically, we design an
association scheme that selects a suitable RIS for each user. Then, we maximize
the weighted sum rate (WSR) of all the users by iteratively optimizing the RIS
association scheme, and beamforming designs until the considered heterogeneous
GNN converges. Based on the proposed approach, each user is associated with the
best RIS, which is shown to significantly improve the system capacity in
multi-RIS multi-user millimeter wave (mmWave) communications. Specifically,
simulation results demonstrate that the proposed heterogeneous GNN closely
approaches the performance of the high-complexity alternating optimization (AO)
algorithm in the considered multi-RIS aided communication system, and it
outperforms other benchmark schemes. Moreover, the performance improvement
achieved through the RIS association scheme is shown to be of the order of 30%.

</details>


### [8] [Max-Min Fairness for Stacked Intelligent Metasurface-Assisted Multi-User MISO Systems](https://arxiv.org/abs/2504.14584)
*Nipuni Ginige,Prathapasinghe Dharmawansa,Arthur Sousa de Sena,Nurul Huda Mahmood,Nandana Rajatheva,Matti Latva-aho*

Main category: eess.SP

TL;DR: 该论文提出了两种基于即时和统计CSI的最大最小公平算法，用于优化SIM辅助多用户MISO系统中的波束成形和功率分配。


<details>
  <summary>Details</summary>
Motivation: 研究目的是在多用户MISO系统中实现公平可靠的服务质量，通过优化波束成形和功率分配提升最低可达速率。

Method: 针对即时CSI，采用交替优化算法结合几何规划和梯度下降-上升法；针对统计CSI，通过推导紧上界并利用几何规划和梯度下降算法解决随机优化问题。

Result: 仿真结果显示，所提算法显著提升了最低可达速率，尤其在即时CSI中波束成形的优化效果更显著；统计CSI下的上界在低信噪比时紧致。

Conclusion: 提出的算法有效改善了SIM辅助系统的公平性和性能，波束成形优化对系统性能影响更大。

Abstract: Stacked intelligent metasurface (SIM) is an emerging technology that uses
multiple reconfigurable surface layers to enable flexible wave-based
beamforming. In this paper, we focus on an \ac{SIM}-assisted multi-user
multiple-input single-output system, where it is essential to ensure that all
users receive a fair and reliable service level. To this end, we develop two
max-min fairness algorithms based on instantaneous channel state information
(CSI) and statistical CSI. For the instantaneous CSI case, we propose an
alternating optimization algorithm that jointly optimizes power allocation
using geometric programming and wave-based beamforming coefficients using the
gradient descent-ascent method. For the statistical CSI case, since deriving an
exact expression for the average minimum achievable rate is analytically
intractable, we derive a tight upper bound and thereby formulate a stochastic
optimization problem. This problem is then solved, capitalizing on an
alternating approach combining geometric programming and gradient descent
algorithms, to obtain the optimal policies. Our numerical results show
significant improvements in the minimum achievable rate compared to the
benchmark schemes. In particular, for the instantaneous CSI scenario, the
individual impact of the optimal wave-based beamforming is significantly higher
than that of the power allocation strategy. Moreover, the proposed upper bound
is shown to be tight in the low signal-to-noise ratio regime under the
statistical CSI.

</details>


### [9] [Markovian Continuity of the MMSE](https://arxiv.org/abs/2504.14659)
*Elad Domanovitz,Anatoly Khina*

Main category: eess.SP

TL;DR: 本文讨论了最小均方误差（MMSE）估计的连续性及其在实际应用中的稳健性，并提出了一种新的随机收敛概念——马尔可夫收敛，证明了MMSE在这种收敛方式下是连续的。


<details>
  <summary>Details</summary>
Motivation: 尽管MMSE估计在信号处理等领域广泛应用，但其连续性在某些情况下存在问题。本文旨在分析这些问题并提出一种更适合实际应用场景的连续性概念。

Method: 文章回顾了MMSE连续性的已知反例，观察到连续性问题源于收敛序列中的某些元素提供了比极限更多的信息。为此，作者引入并分析了马尔可夫收敛概念，并在这一新框架下证明了MMSE的连续性。

Result: 研究表明，MMSE在马尔可夫收敛下是连续的，且在特定条件下具有半连续性和连续性保证。

Conclusion: 本文提出的马尔可夫收敛为MMSE的连续性分析提供了新的理论基础，有助于其在实际应用中的稳健性和可靠性。

Abstract: Minimum mean square error (MMSE) estimation is widely used in signal
processing and related fields. While it is known to be non-continuous with
respect to all standard notions of stochastic convergence, it remains robust in
practical applications. In this work, we review the known counterexamples to
the continuity of the MMSE. We observe that, in these counterexamples, the
discontinuity arises from an element in the converging measurement sequence
providing more information about the estimand than the limit of the measurement
sequence. We argue that this behavior is uncharacteristic of real-world
applications and introduce a new stochastic convergence notion, termed
Markovian convergence, to address this issue. We prove that the MMSE is, in
fact, continuous under this new notion. We supplement this result with
semi-continuity and continuity guarantees of the MMSE in other settings and
prove the continuity of the MMSE under linear estimation.

</details>


### [10] [Delay-Angle Information Spoofing for Channel State Information-Free Location-Privacy Enhancement](https://arxiv.org/abs/2504.14780)
*Jianxiu Li,Urbashi Mitra*

Main category: eess.SP

TL;DR: 该论文提出了一种延迟-角度信息欺骗（DAIS）策略，通过人为改变无线信道的延迟和角度来增强物理层的位置隐私，使窃听者定位错误。设计的前置编码器在无需信道状态信息的情况下实现DAIS，合法定位者可通过安全接收少量信息恢复真实位置。分析了窃听者定位误差下界及参数设计，并通过仿真验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为增强无线通信中的位置隐私，避免窃听者通过信道信息准确获取用户位置。

Method: 提出DAIS策略，设计前置编码器人为改变延迟和角度，无需发射端信道状态信息，合法用户可通过安全信道恢复真实位置。

Result: 理论分析显示DAIS显著增加窃听者定位误差（15 dB以上），仿真验证其隐私保护效果优于现有技术。

Conclusion: DAIS策略通过延迟-角度欺骗有效增强位置隐私，其设计参数可优化性能，适用于多种窃听场景。

Abstract: In this paper, a delay-angle information spoofing (DAIS) strategy is proposed
to enhance the location privacy at the physical layer. More precisely, the
location-relevant delays and angles are artificially shifted without the aid of
channel state information (CSI) at the transmitter, such that the location
perceived by the eavesdropper is incorrect and distinct from the true one. By
leveraging the intrinsic structure of the wireless channel, a precoder is
designed to achieve DAIS while the legitimate localizer can remove the
obfuscation via securely receiving a modest amount of information, i.e., the
delay-angle shifts. A lower bound on eavesdropper's localization error is
derived, revealing that location privacy is enhanced not only due to estimation
error, but also by the geometric mismatch introduced by DAIS. Furthermore, the
lower bound is explicitly expressed as a function of the delay-angle shifts,
characterizing performance trends and providing the appropriate design of these
shift parameters. The statistical hardness of maliciously inferring the
delay-angle shifts by a single-antenna eavesdropper as well as the challenges
for a multi-antenna eavesdropper are investigated to assess the robustness of
the proposed DAIS strategy. Numerical results show that the proposed DAIS
strategy results in more than 15 dB performance degradation for the
eavesdropper as compared with that for the legitimate localizer at high
signal-to-noise ratios, and provides more effective location-privacy
enhancement than the prior art.

</details>


### [11] [Aligning Beam with Imbalanced Multi-modality: A Generative Federated Learning Approach](https://arxiv.org/abs/2504.14835)
*Jiahui Liang,Miaowen Wen,Shuoyao Wang,Yuxuan Liang,Shijian Gao*

Main category: eess.SP

TL;DR: 论文提出了一种生成式联邦学习（GFL4BS）方法，用于解决V2X通信中的标签不平衡和多模态差异问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着车辆智能化的提升，多模态感知辅助通信成为可靠V2X连接的关键，但集中式学习存在隐私和通信开销问题，联邦学习面临标签不平衡和多模态差异的挑战。

Method: 提出GFL4BS，包含自适应零样本多模态生成器和混合训练范式，通过合成数据和特征融合优化解决上述问题。

Result: 实验表明，GFL4BS在标签不平衡条件下比现有技术水平准确率高16.2%，且在缺少LiDAR和RGB输入时成功率仍超70%。

Conclusion: GFL4BS通过生成式联邦学习有效解决了V2X中的标签不平衡和多模态差异问题，提升了模型性能和训练稳定性。

Abstract: As vehicle intelligence advances, multi-modal sensing-aided communication
emerges as a key enabler for reliable Vehicle-to-Everything (V2X) connectivity
through precise environmental characterization. As centralized learning may
suffer from data privacy, model heterogeneity and communication overhead
issues, federated learning (FL) has been introduced to support V2X. However,
the practical deployment of FL faces critical challenges: model performance
degradation from label imbalance across vehicles and training instability
induced by modality disparities in sensor-equipped agents. To overcome these
limitations, we propose a generative FL approach for beam selection (GFL4BS).
Our solution features two core innovations: 1) An adaptive zero-shot
multi-modal generator coupled with spectral-regularized loss functions to
enhance the expressiveness of synthetic data compensating for both label
scarcity and missing modalities; 2) A hybrid training paradigm integrating
feature fusion with decentralized optimization to ensure training resilience
while minimizing communication costs. Experimental evaluations demonstrate
significant improvements over baselines achieving 16.2% higher accuracy than
the current state-of-the-art under severe label imbalance conditions while
maintaining over 70% successful rate even when two agents lack both LiDAR and
RGB camera inputs.

</details>


### [12] [Radar Code Design for the Joint Optimization of Detection Performance and Measurement Accuracy in Track Maintenance](https://arxiv.org/abs/2504.14885)
*Tao Fan,Augusto Aubry,Vincenzo Carotenuto,Antonio De Maio,Xianxiang Yu,Guolong Cui*

Main category: eess.SP

TL;DR: 该论文设计了一种慢时间编码波形，联合优化检测概率与测量精度，以应对有色高斯干扰下的跟踪问题。通过SINR和CRB指标，结合功率和相似性约束，提出了一种多项式时间算法以实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 解决有色高斯干扰环境下雷达信号检测与测量精度之间的联合优化问题，以满足跟踪维护的需求。

Method: 采用基于张量松弛和标量化方法的多项式时间算法，结合MBI框架迭代求解非凸多目标优化问题。

Result: 数值结果表明了检测与估计性能之间的权衡，同时算法展现了良好的多普勒鲁棒性。

Conclusion: 所提算法有效平衡了检测与测量精度，为有色干扰环境下的雷达波形设计提供了实用解决方案。

Abstract: This paper deals with the design of slow-time coded waveforms which jointly
optimize the detection probability and the measurements accuracy for track
maintenance in the presence of colored Gaussian interference. The output
signal-to-interference-plus-noise ratio (SINR) and Cram\'er Rao bounds (CRBs)
on time delay and Doppler shift are used as figures of merit to accomplish
reliable detection as well as accurate measurements. The transmitted code is
subject to radar power budget requirements and a similarity constraint. To
tackle the resulting non-convex multi-objective optimization problem, a
polynomial-time algorithm that integrates scalarization and tensor-based
relaxation methods is developed. The corresponding relaxed multi-linear
problems are solved by means of the maximum block improvement (MBI) framework,
where the optimal solution at each iteration is obtained in closed form.
Numeral results demonstrate the trade-off between the detection and the
estimation performance, along with the acceptable Doppler robustness achieved
by the proposed algorithm.

</details>


### [13] [A Purely Data-Driven Adaptive Impedance Matching Method Robust to Parasitic Effects](https://arxiv.org/abs/2504.14951)
*Wendong Cheng,Li Chen,Weidong Wang*

Main category: eess.SP

TL;DR: 论文提出了一种纯数据驱动的自适应阻抗匹配方法，通过深度学习模型和优化算法解决实际可调匹配网络中的性能问题，显著降低在线计算开销。


<details>
  <summary>Details</summary>
Motivation: 为解决实际可调匹配网络中由寄生效应导致的匹配性能下降问题，避免传统试错式物理调整的低效。

Method: 1. 提出RECBM-Net深度学习模型，映射TMN状态到S参数；2. 基于代理模型将匹配过程建模为优化问题；3. 使用SAPSO和AD-Adam两种数值方法搜索匹配解；4. 训练IMS-Net直接预测最优解以减少在线开销。

Result: RECBM-Net建模精度极高；AD-Adam相比SAPSO显著降低计算开销但略牺牲精度；IMS-Net在线开销最低且保持优秀匹配精度。

Conclusion: 纯数据驱动方法有效解决了阻抗匹配问题，IMS-Net在低开销与高精度间取得了最佳平衡。

Abstract: Adaptive impedance matching between antennas and radio frequency front-end
(RFFE) power modules is essential for mobile communication systems. To address
the matching performance degradation caused by parasitic effects in practical
tunable matching networks (TMN), this paper proposes a purely data-driven
adaptive impedance matching method that avoids trial-and-error physical
adjustment. First, we propose the residual enhanced circuit behavior modeling
network (RECBM-Net), a deep learning model that maps TMN operating states to
their scattering parameters (S-parameters). Then, we formulate the matching
process based on the trained surrogate model as a mathematical optimization
problem. We employ two classic numerical methods with different online
computational overhead, namely simulated annealing particle swarm optimization
(SAPSO) and adaptive moment estimation with automatic differentiation
(AD-Adam), to search for the matching solution. To further reduce the online
inference overhead caused by repeated forward propagation through RECBM-Net, we
train an inverse mapping solver network (IMS-Net) to directly predict the
optimal solution. Simulation results show that RECBM-Net achieves exceptionally
high modeling accuracy. While AD-Adam significantly reduces computational
overhead compared to SAPSO, it sacrifices slight accuracy. IMS-Net offers the
lowest online overhead while maintaining excellent matching accuracy.

</details>


### [14] [Blinding the Wiretapper: RIS-Enabled User Occultation in the ISAC Era](https://arxiv.org/abs/2504.15033)
*Getuar Rexhepi,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文提出了一种新型的ISAC方案，利用可重构智能表面（RIS）隐藏用户设备（UE）的位置以防窃听，同时保持与合法基站的通信和感知性能。通过优化RIS相位，方案有效干扰窃听者的定位能力。


<details>
  <summary>Details</summary>
Motivation: 随着ISAC技术的发展，恶意代理可能伪造用户的情境信息（如位置、方向等）。为了保护用户隐私，本文旨在通过RIS技术隐藏UE位置，同时不影响合法通信和感知性能。

Method: 首先，提出一个RIS相位偏移优化问题，联合最大化UE的通信性能和最小化窃听者对合法信道的感知能力；然后，设计一种流形优化算法求解这一非凸优化问题。

Result: 数值结果显示，该方案在保持合法ISAC性能的同时，显著削弱了窃听者的定位能力。

Conclusion: 提出的RIS辅助ISAC方案能有效保护用户位置隐私，且不影响正常通信和感知功能，为非授权感知威胁提供了可行解决方案。

Abstract: An undesirable consequence of the foreseeable proliferation of sophisticated
integrated sensing and communications (ISAC) technologies is the enabling of
spoofing, by malicious agents, of situational information (such as proximity,
direction or location) of legitimate users of wireless systems. In order to
mitigate this threat, we present a novel ISAC scheme that, aided by a
reconfigurable intelligent surface (RIS), enables the occultation of the
positions of user equipment (UE) from wiretappers, while maintaining both
sensing and desired communication performance between the UEs and a legitimate
base station (BS). To that end, we first formulate an RIS phase-shift
optimization problem that jointly maximizes the sum-rate performance of the UEs
(communication objective), while minimizing the projection of the wiretapper's
effective channel onto the legitimate channel (hiding objective), thereby
disrupting the attempts by a wiretapper of localizing the UEs. Then, in order
to efficiently solve the resulting non-convex joint optimization problem, a
novel manifold optimization algorithm is derived, whose effectiveness is
validated by numerical results, which demonstrate that the proposed approach
preserves legitimate ISAC performance while significantly degrading the
wiretapper's sensing capability.

</details>


### [15] [The PHD/CPHD filter for Multiple Extended Target Tracking with Trajectory Set Theory and Explicit Shape Estimation](https://arxiv.org/abs/2504.15040)
*Yuanhao Cheng,Yunhe Cao,Tat-Soon Yeo,Fu Jie,Wei Zhang*

Main category: eess.SP

TL;DR: 论文提出两种基于PHD和CPHD滤波器的多目标跟踪方法，结合轨迹集理论和解耦形状估计模型，实现了稳定的轨迹生成和精确的形状估计，实验证明优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 针对多目标跟踪中未解析群目标的椭圆形状估计问题，提出结合轨迹集理论的改进方法，以提升轨迹完整性和形状估计精度。

Method: 基于PHD和CPHD滤波器，结合轨迹集理论（TST）和解耦形状估计模型，推导出TPHD-E和TCPHD-E滤波器，并提供高斯混合实现（GM-TPHD-E/GM-TCPHD-E）。

Result: 仿真和真实数据实验表明，所提算法在目标形状估计（如椭圆方向和轴长）及轨迹生成的完整性和准确性上优于现有方法。

Conclusion: TPHD-E和TCPHD-E滤波器通过整合轨迹集与形状解耦模型，显著提升了多目标跟踪的性能，为实际应用提供了有效工具。

Abstract: In this paper, we propose two methods for tracking multiple extended targets
or unresolved group targets with elliptical extent shape. These two methods are
deduced from the famous Probability Hypothesis Density (PHD) filter and the
Cardinality-PHD (CPHD) filter, respectively. In these two methods, Trajectory
Set Theory (TST) is combined to establish the target trajectory estimates.
Moreover, by employing a decoupled shape estimation model, the proposed methods
can explicitly provide the shape estimation of the target, such as the
orientation of the ellipse extension and the length of its two axes. We derived
the closed Bayesian recursive of these two methods with stable trajectory
generation and accurate extent estimation, resulting in the TPHD-E filter and
the TCPHD-E filter. In addition, Gaussian mixture implementations of our
methods are provided, which are further referred to as the GM-TPHD-E filter and
the GM-TCPHD-E filters. We illustrate the ability of these methods through
simulations and experiments with real data. These experiments demonstrate that
the two proposed algorithms have advantages over existing algorithms in target
shape estimation, as well as in the completeness and accuracy of target
trajectory generation.

</details>


### [16] [Bayesian Sensing for Time-Varying Channels in ISAC Systems](https://arxiv.org/abs/2504.15042)
*Xueyang Wang,Kai Wu,J. Andrew Zhang,Shiqi Gong,Chengwen Xing*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于稀疏贝叶斯框架的新型感知方案，通过变分贝叶斯推断方法解决时变信道中的延迟和多普勒估计问题，并通过仿真验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 未来移动网络需要在高速通信场景中支持集成感知与通信，但时变信道引入的较大多普勒频移会导致严重的载波间干扰，因此需要一种高效的感知方法。

Method: 论文提出了一种基于稀疏贝叶斯框架的3D多测量稀疏信号恢复问题，采用两层变分贝叶斯推断方法分别估计多普勒和延迟，并进一步简化为一阶段MUSIC和二阶段VBI的混合方法。

Result: 仿真结果表明，所提方法在均方误差上优于传统方法，且对目标数量和速度具有鲁棒性。

Conclusion: 该方案在集成感知与通信中展现出广泛适用性和优于现有技术的优势。

Abstract: Future mobile networks are projected to support integrated sensing and
communications in high-speed communication scenarios. Nevertheless, large
Doppler shifts induced by time-varying channels may cause severe inter-carrier
interference (ICI). Frequency domain shows the potential of reducing ISAC
complexity as compared with other domains. However, parameter mismatching issue
still exists for such sensing. In this paper, we develop a novel sensing scheme
based on sparse Bayesian framework, where the delay and Doppler estimation
problem in time-varying channels is formulated as a 3D multiple
measurement-sparse signal recovery (MM-SSR) problem. We then propose a novel
two-layer variational Bayesian inference (VBI) method to decompose the 3D
MM-SSR problem into two layers and estimate the Doppler in the first layer and
the delay in the second layer alternatively. Subsequently, as is benefited from
newly unveiled signal construction, a simplified two-stage multiple signal
classification (MUSIC)-based VBI method is proposed, where the delay and the
Doppler are estimated by MUSIC and VBI, respectively. Additionally, the
Cram\'er-Rao bound (CRB) of the considered sensing parameters is derived to
characterize the lower bound for the proposed estimators. Corroborated by
extensive simulation results, our proposed method can achieve improved mean
square error (MSE) than its conventional counterparts and is robust against the
target number and target speed, thereby validating its wide applicability and
advantages over prior arts.

</details>


### [17] [Time-Series Analysis on Edge-AI Hardware for Healthcare Monitoring](https://arxiv.org/abs/2504.15178)
*Jinhai Hu*

Main category: eess.SP

TL;DR: 该论文提出了一种基于动态偏置长短时记忆网络（DB-LSTM）的实时生物医学信号分析方法，解决了传统方法的高延迟和高功耗问题，在ECG预测和分类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的生物医学信号分析方法需要长时间记录数据后进行离线分析，功耗高且反应延迟，不适合实时健康监测。

Method: 提出了一种基于DB-LSTM神经网络的时域ECG分析模型，支持同步预测和分类，并对硬件进行了优化（如权重量化至INT4/INT3）。

Result: 模型在多个ECG数据集上表现出高准确性（预测误差低于1e-3，分类准确率超97%）和低资源占用（量化后精度损失仅2%-6%）。

Conclusion: 该模型为实时健康监测提供了高效解决方案，未来将部署于FPGA/CMOS硬件平台以实现个性化医疗应用。

Abstract: This project addresses the need for efficient, real-time analysis of
biomedical signals such as electrocardiograms (ECG) and electroencephalograms
(EEG) for continuous health monitoring. Traditional methods rely on
long-duration data recording followed by offline analysis, which is
power-intensive and delays responses to critical symptoms such as arrhythmia.
To overcome these limitations, a time-domain ECG analysis model based on a
novel dynamically-biased Long Short-Term Memory (DB-LSTM) neural network is
proposed. This model supports simultaneous ECG forecasting and classification
with high performance-achieving over 98% accuracy and a normalized mean square
error below 1e-3 for forecasting, and over 97% accuracy with faster convergence
and fewer training parameters for classification. To enable edge deployment,
the model is hardware-optimized by quantizing weights to INT4 or INT3 formats,
resulting in only a 2% and 6% drop in classification accuracy during training
and inference, respectively, while maintaining full accuracy for forecasting.
Extensive simulations using multiple ECG datasets confirm the model's
robustness. Future work includes implementing the algorithm on FPGA and CMOS
circuits for practical cardiac monitoring, as well as developing a digital
hardware platform that supports flexible neural network configurations and
on-chip online training for personalized healthcare applications.

</details>


### [18] [Joint Knowledge and Power Management for Secure Semantic Communication Networks](https://arxiv.org/abs/2504.15260)
*Xuesong Liu,Yansong Liu,Haoyu Tang,Fangzhou Zhao,Le Xia,Yao Sun*

Main category: eess.SP

TL;DR: 论文提出了一种在安全语义通信网络中联合优化功率分配、知识库缓存和设备到设备用户配对的解决方案，通过引入语义保密吞吐量（SST）指标，显著提升了信息安全性能和延迟表现。


<details>
  <summary>Details</summary>
Motivation: 语义通信虽然在资源节省和信息交换方面具有优势，但背景知识的共享可能导致信息安全问题，尤其是潜在窃听者可能利用相同的知识解密私密信息，因此需解决功率分配、知识库缓存和用户配对的核心问题。

Method: 论文首先定义了语义保密吞吐量（SST）作为信息安全指标，并基于此构建优化问题，结合拉格朗日对偶方法和两阶段方法提出资源管理方案。

Result: 仿真结果显示，所提方案几乎使SST性能翻倍，并将排队延迟性能降至基准方案的一半以下。

Conclusion: 该研究为安全语义通信网络的资源管理提供了有效解决方案，显著提升了信息安全性和通信效率。

Abstract: Recently, semantic communication (SemCom) has shown its great superiorities
in resource savings and information exchanges. However, while its unique
background knowledge guarantees accurate semantic reasoning and recovery,
semantic information security-related concerns are introduced at the same time.
Since the potential eavesdroppers may have the same background knowledge to
accurately decrypt the private semantic information transmitted between legal
SemCom users, this makes the knowledge management in SemCom networks rather
challenging in joint consideration with the power control. To this end, this
paper focuses on jointly addressing three core issues of power allocation,
knowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in
secure SemCom networks. We first develop a novel performance metric, namely
semantic secrecy throughput (SST), to quantify the information security level
that can be achieved at each pair of D2D SemCom users. Next, an SST
maximization problem is formulated subject to secure SemCom-related delay and
reliability constraints. Afterward, we propose a security-aware resource
management solution using the Lagrange primal-dual method and a two-stage
method. Simulation results demonstrate our proposed solution nearly doubles the
SST performance and realizes less than half of the queuing delay performance
compared to different benchmarks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [19] [Wireless Silent Speech Interface Using Multi-Channel Textile EMG Sensors Integrated into Headphones](https://arxiv.org/abs/2504.13921)
*Chenyu Tang,Josée Mallah,Dominika Kazieczko,Wentian Yi,Tharun Reddy Kandukuri,Edoardo Occhipinti,Bhaskar Mishra,Sunita Mehta,Luigi G. Occhipinti*

Main category: cs.HC

TL;DR: 该论文提出了一种新型无线无声语音接口（SSI），将多通道纺织品EMG电极集成到耳机耳罩中，实现实时、免提通信，准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 传统的贴片式EMG系统需要大面积电极，不舒服且不隐蔽。本文旨在开发一种舒适、隐蔽且可穿戴的无声语音解码系统。

Method: 使用四个石墨烯/PEDOT:PSS涂层纺织品电极捕捉神经肌肉活动信号，通过ESP32-S3无线模块处理信号，并采用1D SE-ResNet架构动态调整通道注意力权重以应对阻抗变化。

Result: 系统在10个常用无声控制词上达到96%的准确率，优于传统单通道和非自适应基线。XAI注意力分析和t-SNE特征可视化验证了模型的有效性。

Conclusion: 该研究推进了可穿戴EMG-SSI的发展，为无声通信、辅助技术和人机交互提供了一个可扩展、低功耗且用户友好的平台。

Abstract: This paper presents a novel wireless silent speech interface (SSI)
integrating multi-channel textile-based EMG electrodes into headphone earmuff
for real-time, hands-free communication. Unlike conventional patch-based EMG
systems, which require large-area electrodes on the face or neck, our approach
ensures comfort, discretion, and wearability while maintaining robust silent
speech decoding. The system utilizes four graphene/PEDOT:PSS-coated textile
electrodes to capture speech-related neuromuscular activity, with signals
processed via a compact ESP32-S3-based wireless readout module. To address the
challenge of variable skin-electrode coupling, we propose a 1D SE-ResNet
architecture incorporating squeeze-and-excitation (SE) blocks to dynamically
adjust per-channel attention weights, enhancing robustness against
motion-induced impedance variations. The proposed system achieves 96% accuracy
on 10 commonly used voice-free control words, outperforming conventional
single-channel and non-adaptive baselines. Experimental validation, including
XAI-based attention analysis and t-SNE feature visualization, confirms the
adaptive channel selection capability and effective feature extraction of the
model. This work advances wearable EMG-based SSIs, demonstrating a scalable,
low-power, and user-friendly platform for silent communication, assistive
technologies, and human-computer interaction.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TL;DR: 论文提出SSENSE框架，通过对比学习将单主体sEEG信号映射到CLIP模型的句子嵌入空间，实现从脑活动直接检索句子，证明了通用语言表征可作为神经解码的有效先验。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型在将侵入式脑记录与自然语言对齐中的潜力，解决神经科学和人工智能交叉领域的复杂挑战。

Method: 采用对比学习框架SSENSE，将sEEG的频谱表示通过InfoNCE损失训练神经编码器，不微调文本编码器。

Result: 在自然电影观看数据集中，SSENSE展示了从脑活动直接检索句子的潜力，尽管数据有限。

Conclusion: 通用语言表征可作为神经解码的有效先验，SSENSE框架为神经活动与语言对齐提供了新思路。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [21] [Joint Channel Estimation and Signal Detection for MIMO-OFDM: A Novel Data-Aided Approach with Reduced Computational Overhead](https://arxiv.org/abs/2504.14463)
*Xinjie Li,Jing Zhang,Xingyu Zhou,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: 该论文提出了一种新型数据辅助方法，通过优化LMMSE算法和引入低复杂度替代方案，有效解决了MIMO-OFDM系统中因动态无线环境导致的性能下降和高计算开销问题，在不同配置下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态无线环境中因信道时变特性导致性能下降或计算开销过高，因此需要一种同时兼顾精度和复杂度的解决方案。

Method: 提出了一种基于LMMSE的迭代联合信道估计与信号检测算法，并进一步设计其低复杂度替代方案。

Result: 实验证明，所提算法在多种配置下均优于现有方法，并在精度与复杂度之间取得了良好平衡。

Conclusion: 该算法为5G及以上网络的快衰落场景提供了一种高效且性能优越的解决方案。

Abstract: The acquisition of channel state information (CSI) is essential in MIMO-OFDM
communication systems. Data-aided enhanced receivers, by incorporating domain
knowledge, effectively mitigate performance degradation caused by imperfect
CSI, particularly in dynamic wireless environments. However, existing
methodologies face notable challenges: they either refine channel estimates
within MIMO subsystems separately, which proves ineffective due to deviations
from assumptions regarding the time-varying nature of channels, or fully
exploit the time-frequency characteristics but incur significantly high
computational overhead due to dimensional concatenation. To address these
issues, this study introduces a novel data-aided method aimed at reducing
complexity, particularly suited for fast-fading scenarios in fifth-generation
(5G) and beyond networks. We derive a general form of a data-aided linear
minimum mean-square error (LMMSE)-based algorithm, optimized for iterative
joint channel estimation and signal detection. Additionally, we propose a
computationally efficient alternative to this algorithm, which achieves
comparable performance with significantly reduced complexity. Empirical
evaluations reveal that our proposed algorithms outperform several
state-of-the-art approaches across various MIMO-OFDM configurations, pilot
sequence lengths, and in the presence of time variability. Comparative analysis
with basis expansion model-based iterative receivers highlights the superiority
of our algorithms in achieving an effective trade-off between accuracy and
computational complexity.

</details>


### [22] [Wireless Large AI Model: Shaping the AI-Native Future of 6G and Beyond](https://arxiv.org/abs/2504.14653)
*Fenghao Zhu,Xinquan Wang,Xinyi Li,Maojun Zhang,Yixuan Chen,Chongwen Huang,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Richeng Jin,Yongming Huang,Wei Feng,Tingting Yang,Baoming Bai,Feifei Gao,Kun Yang,Yuanwen Liu,Sami Muhaidat,Chau Yuen,Kaibin Huang,Kai-Kit Wong,Dusit Niyato,Mérouane Debbah*

Main category: cs.IT

TL;DR: 本文综述了无线大AI模型（WLAM）在6G及以上通信系统中的潜力，涵盖其基本原理、应用、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨WLAM如何通过其卓越的数据处理、推理和决策能力，推动6G及以上通信系统的智能化和高效化。

Method: 通过系统分析WLAM的背景、与无线网络的协同效应、基础特性及其在优化通信系统中的应用，结合新兴技术的融合潜力进行探讨。

Result: 明确了WLAM在无线通信中的重要作用及其与新兴技术结合的潜力，同时识别了实际部署中的关键挑战。

Conclusion: WLAM有望在6G通信系统中发挥革命性作用，但其广泛应用仍需解决技术挑战并进一步探索研究方向。

Abstract: The emergence of sixth-generation and beyond communication systems is
expected to fundamentally transform digital experiences through introducing
unparalleled levels of intelligence, efficiency, and connectivity. A promising
technology poised to enable this revolutionary vision is the wireless large AI
model (WLAM), characterized by its exceptional capabilities in data processing,
inference, and decision-making. In light of these remarkable capabilities, this
paper provides a comprehensive survey of WLAM, elucidating its fundamental
principles, diverse applications, critical challenges, and future research
opportunities. We begin by introducing the background of WLAM and analyzing the
key synergies with wireless networks, emphasizing the mutual benefits.
Subsequently, we explore the foundational characteristics of WLAM, delving into
their unique relevance in wireless environments. Then, the role of WLAM in
optimizing wireless communication systems across various use cases and the
reciprocal benefits are systematically investigated. Furthermore, we discuss
the integration of WLAM with emerging technologies, highlighting their
potential to enable transformative capabilities and breakthroughs in wireless
communication. Finally, we thoroughly examine the high-level challenges
hindering the practical implementation of WLAM and discuss pivotal future
research directions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Generative Semantic Communications: Principles and Practices](https://arxiv.org/abs/2504.14947)
*Xiaojun Yuan,Haoming Ma,Yinuo Huang,Zhoufan Hua,Yong Zuo,Zhi Ding*

Main category: cs.AI

TL;DR: 提出了一种基于生成模型和基础模型的生成语义通信（GSC）范式，以应对AGI服务对语义通信的新挑战，并通过案例验证其优势。


<details>
  <summary>Details</summary>
Motivation: 随着人工通用智能（AGI）的发展，其对语义通信的需求日益增长，现有语义通信技术面临新挑战，需要更高效、灵活的通信范式。

Method: 引入GSC范式，利用基础模型和生成模型等先进AI技术，构建通用框架，并通过两个案例研究验证其性能。

Result: GSC在AGI驱动的应用中展现出显著优势，能够高效提取和传输语义信息，降低通信成本。

Conclusion: GSC为语义通信领域提供了创新方向，但仍存在开放性挑战，需进一步研究以实现实际应用。

Abstract: Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [24] [Frequency Comb-based Wavelength Division Multiplexing and Detection without Wavelength Demultiplexers](https://arxiv.org/abs/2504.15012)
*Di Che,Zhongdi Peng,Mikael Mazur,Nicolas Fontaine*

Main category: physics.optics

TL;DR: 提出了一种在4波长200-GHz网格WDM系统中使用无解复用器频率梳的WDM概念，避免了功耗高的波长控制。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统WDM系统中由于解复用器波长控制带来的高功耗问题。

Method: 在发射端和接收端均采用无解复用器的频率梳，支持灵活的符号速率。

Result: 成功实现了4波长200-GHz网格的WDM系统，避免了传统解复用器的功耗问题。

Conclusion: 无解复用器频率梳技术为WDM系统提供了一种低功耗的解决方案。

Abstract: We demonstrate a wavelength division multiplexing (WDM) concept using
demultiplexer-free frequency combs at both transmitter and receiver in a
4-wavelength 200-GHz-grid WDM system with flexible symbol rates, aiming to
avoid the power-hungry wavelength control on demultiplexers.

</details>
