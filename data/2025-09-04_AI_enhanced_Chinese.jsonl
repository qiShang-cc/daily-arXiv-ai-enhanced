{"id": "2509.02568", "pdf": "https://arxiv.org/pdf/2509.02568", "abs": "https://arxiv.org/abs/2509.02568", "authors": ["Mohammad Mehedi Hasan", "Pedro G. Lind", "Hernando Ombao", "Anis Yazidi", "Rabindra Khadka"], "title": "EEG-MSAF: An Interpretable Microstate Framework uncovers Default-Mode Decoherence in Early Neurodegeneration", "categories": ["eess.SP", "cs.LG"], "comment": "Dementia, EEG, Microstates, Explainable, SHAP", "summary": "Dementia (DEM) is a growing global health challenge, underscoring the need\nfor early and accurate diagnosis. Electroencephalography (EEG) provides a\nnon-invasive window into brain activity, but conventional methods struggle to\ncapture its transient complexity. We present the \\textbf{EEG Microstate\nAnalysis Framework (EEG-MSAF)}, an end-to-end pipeline that leverages EEG\nmicrostates discrete, quasi-stable topographies to identify DEM-related\nbiomarkers and distinguish DEM, mild cognitive impairment (MCI), and normal\ncognition (NC). EEG-MSAF comprises three stages: (1) automated microstate\nfeature extraction, (2) classification with machine learning (ML), and (3)\nfeature ranking using Shapley Additive Explanations (SHAP) to highlight key\nbiomarkers. We evaluate on two EEG datasets: the public Chung-Ang University\nEEG (CAUEEG) dataset and a clinical cohort from Thessaloniki Hospital. Our\nframework demonstrates strong performance and generalizability. On CAUEEG,\nEEG-MSAF-SVM achieves \\textbf{89\\% $\\pm$ 0.01 accuracy}, surpassing the deep\nlearning baseline CEEDNET by \\textbf{19.3\\%}. On the Thessaloniki dataset, it\nreaches \\textbf{95\\% $\\pm$ 0.01 accuracy}, comparable to EEGConvNeXt. SHAP\nanalysis identifies mean correlation and occurrence as the most informative\nmetrics: disruption of microstate C (salience/attention network) dominates DEM\nprediction, while microstate F, a novel default-mode pattern, emerges as a key\nearly biomarker for both MCI and DEM. By combining accuracy, generalizability,\nand interpretability, EEG-MSAF advances EEG-based dementia diagnosis and sheds\nlight on brain dynamics across the cognitive spectrum.", "AI": {"tldr": "EEG-MSAF\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684EEG\u5fae\u72b6\u6001\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u8bca\u65ad\u75f4\u5446\u75c7\uff0c\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51c6\u786e\u7387\u8fbe89%\u81f395%\uff0c\u5e76\u8bc6\u522b\u51fa\u5173\u952e\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u75f4\u5446\u75c7\u7684\u65e9\u671f\u8bca\u65ad\u662f\u91cd\u8981\u7684\u5168\u7403\u5065\u5eb7\u6311\u6218\uff0c\u4f20\u7edfEEG\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5176\u77ac\u6001\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "EEG-MSAF\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u81ea\u52a8\u5fae\u72b6\u6001\u7279\u5f81\u63d0\u53d6\u3001\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u548cSHAP\u7279\u5f81\u6392\u5e8f\uff0c\u7528\u4e8e\u533a\u5206\u75f4\u5446\u75c7\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u548c\u6b63\u5e38\u8ba4\u77e5\u3002", "result": "\u5728CAUEEG\u548cThessaloniki\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523089%\u548c95%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u51fa\u5fae\u72b6\u6001C\u548cF\u4f5c\u4e3a\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "EEG-MSAF\u901a\u8fc7\u7ed3\u5408\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u52a8\u4e86EEG\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e3a\u8ba4\u77e5\u8c31\u7cfb\u7684\u8111\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.02724", "pdf": "https://arxiv.org/pdf/2509.02724", "abs": "https://arxiv.org/abs/2509.02724", "authors": ["Xiang-Gen Xia"], "title": "Recall Gabor Communication Theory and Joint Time-Frequency Analysis", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "In this article, we first briefly recall Gabor's communication theory and\nthen Gabor transform and expansion, and also its connection with joint time\nfrequency analysis.", "AI": {"tldr": "\u7b80\u8981\u56de\u987eGabor\u7684\u901a\u4fe1\u7406\u8bba\u53ca\u5176\u53d8\u6362\u4e0e\u5c55\u5f00\uff0c\u4ee5\u53ca\u4e0e\u65f6\u9891\u5206\u6790\u7684\u5173\u8054\u3002", "motivation": "\u63a2\u8ba8Gabor\u901a\u4fe1\u7406\u8bba\u53ca\u5176\u6570\u5b66\u5de5\u5177\u5728\u65f6\u9891\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u56de\u987e\u548c\u5206\u6790Gabor\u53d8\u6362\u4e0e\u65f6\u9891\u5206\u6790\u7684\u8054\u7cfb\u3002", "result": "\u660e\u786e\u4e86Gabor\u7406\u8bba\u5728\u65f6\u9891\u5206\u6790\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "Gabor\u7406\u8bba\u4e3a\u65f6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.02797", "pdf": "https://arxiv.org/pdf/2509.02797", "abs": "https://arxiv.org/abs/2509.02797", "authors": ["Sagnik Bhattacharya", "Abhiram Rao Gorle", "John M. Cioffi"], "title": "minPIC: Towards Optimal Power Allocation in Multi-User Interference Channels", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "To appear in IEEE GLOBECOM 2025", "summary": "6G envisions massive cell-free networks with spatially nested multiple access\n(MAC) and broadcast (BC) channels without centralized coordination. This makes\noptimal resource allocation across power, subcarriers, and decoding orders\ncrucial for interference channels (ICs), where neither transmitters nor\nreceivers can cooperate. Current orthogonal multiple access (OMA) methods, as\nwell as non-orthogonal (NOMA) and rate-splitting (RSMA) schemes, rely on fixed\nheuristics for interference management, leading to suboptimal rates, power\ninefficiency, and scalability issues. This paper proposes a novel minPIC\nframework for optimal power, subcarrier, and decoding order allocation in\ngeneral multi-user ICs. Unlike existing methods, minPIC eliminates heuristic\nSIC order assumptions. Despite the convexity of the IC capacity region, fixing\nan SIC order induces non-convexity in resource allocation, traditionally\nrequiring heuristic approximations. We instead introduce a dual-variable-guided\nsorting criterion to identify globally optimal SIC orders, followed by convex\noptimization with auxiliary log-det constraints, efficiently solved via binary\nsearch. We also demonstrate that minPIC could potentially meet the stringent\nhigh-rate, low-power targets of immersive XR and other 6G applications. To the\nbest of our knowledge, minPIC is the first algorithmic realisation of the\nPareto boundary of the SIC-achievable rate region for Gaussian ICs, opening the\ndoor to scalable interference management in cell-free networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aminPIC\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u7528\u6237\u5e72\u6270\u4fe1\u9053\u4e2d\u7684\u529f\u7387\u3001\u5b50\u8f7d\u6ce2\u548c\u89e3\u7801\u987a\u5e8f\u5206\u914d\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u542f\u53d1\u5f0f\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u4f18\u89e3\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u9700\u89e3\u51b3\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5f53\u524d\u65b9\u6cd5\u56e0\u56fa\u5b9a\u542f\u53d1\u5f0f\u89c4\u5219\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u4e9f\u9700\u65b0\u65b9\u6848\u3002", "method": "minPIC\u91c7\u7528\u53cc\u53d8\u91cf\u5f15\u5bfc\u7684\u6392\u5e8f\u6807\u51c6\u548c\u51f8\u4f18\u5316\u6280\u672f\uff0c\u7ed3\u5408\u8f85\u52a9\u5bf9\u6570\u884c\u5217\u5f0f\u7ea6\u675f\uff0c\u9ad8\u6548\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002", "result": "minPIC\u9996\u6b21\u5b9e\u73b0\u4e86\u9ad8\u65af\u5e72\u6270\u4fe1\u9053\u4e2dSIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u6709\u671b\u6ee1\u8db36G\u5e94\u7528\u7684\u9ad8\u901f\u7387\u3001\u4f4e\u529f\u8017\u9700\u6c42\u3002", "conclusion": "minPIC\u4e3a\u65e0\u5c0f\u533a\u7f51\u7edc\u7684\u5e72\u6270\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.02819", "pdf": "https://arxiv.org/pdf/2509.02819", "abs": "https://arxiv.org/abs/2509.02819", "authors": ["Sameer Mathad", "Taejoon Kim", "David J. Love"], "title": "Protecting Legacy Wireless Systems Against Interference: Precoding and Codebook Approaches Using Massive MIMO and Region Constraints", "categories": ["eess.SP"], "comment": null, "summary": "The ever-increasing demand for high-speed wireless communication has\ngenerated significant interest in utilizing frequency bands that are adjacent\nto those occupied by legacy wireless systems. Since the legacy wireless systems\nwere designed based on often decades-old assumptions about wireless\ninterference, utilizing these new bands will result in interference with the\nexisting legacy users. Many of these legacy wireless devices are used by\ncritical infrastructure networks upon which society depends. There is an urgent\nneed to develop schemes that can protect legacy users from such interference.\nFor many applications, legacy users are located within\ngeographically-constrained regions. Several studies have proposed mitigating\ninterference through the implementation of exclusion zones near these\ngeographically-constrained regions. In contrast to solutions based on\ngeographic exclusion zones, this paper presents a communication theory-based\nsolution. By leveraging knowledge of these geographically-constrained regions,\nwe aim to reduce the interference impact on legacy users. We achieve this by\nincorporating received power constraints, termed as region constraints, in our\nmassive multiple-input multiple-output (MIMO) system design. We perform a\ncapacity analysis of single-user massive MIMO and a sum-rate analysis of the\nmulti-user massive MIMO system with transmit power and region constraints. We\npresent a precoding design method that allows for the utilization of new\nfrequency bands while protecting legacy users.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u4fe1\u7406\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728MIMO\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u52a0\u5165\u63a5\u6536\u529f\u7387\u7ea6\u675f\uff08\u533a\u57df\u7ea6\u675f\uff09\uff0c\u4ee5\u51cf\u5c11\u65b0\u9891\u6bb5\u5bf9\u4f20\u7edf\u65e0\u7ebf\u7cfb\u7edf\u7684\u5e72\u6270\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u7cfb\u7edf\u7684\u5bb9\u91cf\u548c\u603b\u548c\u901f\u7387\u3002", "motivation": "\u968f\u7740\u9ad8\u901f\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u7684\u589e\u957f\uff0c\u65b0\u9891\u6bb5\u7684\u4f7f\u7528\u53ef\u80fd\u5e72\u6270\u4f20\u7edf\u65e0\u7ebf\u7cfb\u7edf\uff0c\u800c\u8fd9\u4e9b\u4f20\u7edf\u7cfb\u7edf\u591a\u662f\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\u7684\u57fa\u7840\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u7684\u65b9\u6848\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u901a\u4fe1\u7406\u8bba\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u5730\u7406\u7ea6\u675f\u533a\u57df\u7684\u77e5\u8bc6\uff0c\u5728MIMO\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u52a0\u5165\u533a\u57df\u7ea6\u675f\uff08\u63a5\u6536\u529f\u7387\u9650\u5236\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u5bb9\u91cf\u5206\u6790\u548c\u603b\u548c\u901f\u7387\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7f16\u7801\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5229\u7528\u65b0\u9891\u6bb5\u7684\u540c\u65f6\u6709\u6548\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\uff0c\u51cf\u5c11\u5e72\u6270\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u4f20\u7edf\u5730\u7406\u6392\u9664\u533a\u57df\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u65b0\u9891\u6bb5\u7684\u5229\u7528\u4e0e\u4f20\u7edf\u7528\u6237\u4fdd\u62a4\u7684\u53cc\u8d62\u3002"}}
{"id": "2509.02727", "pdf": "https://arxiv.org/pdf/2509.02727", "abs": "https://arxiv.org/abs/2509.02727", "authors": ["Guillaume Gagn\u00e9-Labelle", "Vassil Atanassov", "Ioannis Havoutis"], "title": "Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour", "categories": ["cs.RO"], "comment": "Supplementary material can be found here:\n  https://drive.google.com/drive/folders/18h25azbCFfPF4fhSsRfxKrnZo3dPKs_j?usp=sharing", "summary": "Climbing, crouching, bridging gaps, and walking up stairs are just a few of\nthe advantages that quadruped robots have over wheeled robots, making them more\nsuitable for navigating rough and unstructured terrain. However, executing such\nmanoeuvres requires precise temporal coordination and complex agent-environment\ninteractions. Moreover, legged locomotion is inherently more prone to slippage\nand tripping, and the classical approach of modeling such cases to design a\nrobust controller thus quickly becomes impractical. In contrast, reinforcement\nlearning offers a compelling solution by enabling optimal control through trial\nand error. We present a generalist reinforcement learning algorithm for\nquadrupedal agents in dynamic motion scenarios. The learned policy rivals\nstate-of-the-art specialist policies trained using a mixture of experts\napproach, while using only 25% as many agents during training. Our experiments\nalso highlight the key components of the generalist locomotion policy and the\nprimary factors contributing to its success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e2d\uff0c\u5176\u6027\u80fd\u5ab2\u7f8e\u4e13\u5bb6\u7b56\u7565\uff0c\u4e14\u8bad\u7ec3\u6240\u9700\u4ee3\u7406\u6570\u91cf\u4ec5\u4e3a\u540e\u8005\u768425%\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u6bd4\u8f6e\u5f0f\u673a\u5668\u4eba\u66f4\u9002\u5408\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5bfc\u822a\uff0c\u4f46\u5176\u8fd0\u52a8\u63a7\u5236\u9700\u8981\u7cbe\u786e\u7684\u65f6\u95f4\u534f\u8c03\u548c\u590d\u6742\u7684\u4ea4\u4e92\uff0c\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u4e0d\u5b9e\u7528\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bd5\u9519\u5b9e\u73b0\u6700\u4f18\u63a7\u5236\u3002", "result": "\u5b66\u4e60\u7684\u7b56\u7565\u4e0e\u4e13\u5bb6\u7b56\u7565\u6027\u80fd\u76f8\u5f53\uff0c\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u662f\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.03038", "pdf": "https://arxiv.org/pdf/2509.03038", "abs": "https://arxiv.org/abs/2509.03038", "authors": ["Ruihong Jiang", "Ruichen Zhang", "Yanqing Xu", "Huimin Hu", "Yang Lu", "Dusit Niyato"], "title": "Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS Blockage", "categories": ["eess.SP"], "comment": "5 pages, 4 figures", "summary": "This paper considers a power-splitting (PS)-based simultaneous wireless\ninformation and power transfer (SWIPT) system employing a reconfigurable\npinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We\nformulate a joint optimization of the PA position and the PS ratio to maximize\nthe average signal-to-noise ratio (SNR) at a user, subject to its average\nenergy harvesting (EH) and PA placement limits. We derive a closed-form optimal\nsolution. Results demonstrate that the EH requirement has a deterministic\nimpact on the optimal PA position as well as its feasible region, requiring\ndeployment of the PA as close to the user as possible to maximize average\nchannel gain. This spatial adaptation, combined with dynamic PS, enables robust\nSWIPT performance in the presence of probabilistic LoS blockage, revealing that\nmechanical reconfigurability primarily enhances sustainability by ensuring\nenergy feasibility in dynamic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u7387\u5206\u5272\u7684\u53ef\u91cd\u6784\u5939\u6301\u5929\u7ebf\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u6700\u5927\u5316\u7528\u6237\u5e73\u5747\u4fe1\u566a\u6bd4\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\u548c\u5929\u7ebf\u653e\u7f6e\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5728\u6982\u7387\u89c6\u7ebf\u963b\u585e\u73af\u5883\u4e0b\uff0c\u4f18\u5316\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u80fd\u91cf\u6536\u96c6\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u529f\u7387\u5206\u5272\u6bd4\u4f8b\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u6700\u4f18\u89e3\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u80fd\u91cf\u6536\u96c6\u9700\u6c42\u5bf9\u5929\u7ebf\u6700\u4f18\u4f4d\u7f6e\u53ca\u5176\u53ef\u884c\u533a\u57df\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u9700\u5c06\u5929\u7ebf\u5c3d\u91cf\u9760\u8fd1\u7528\u6237\u4ee5\u6700\u5927\u5316\u4fe1\u9053\u589e\u76ca\u3002", "conclusion": "\u7a7a\u95f4\u9002\u5e94\u6027\u4e0e\u52a8\u6001\u529f\u7387\u5206\u5272\u7ed3\u5408\uff0c\u80fd\u5728\u6982\u7387\u89c6\u7ebf\u963b\u585e\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\u6027\u80fd\u3002"}}
{"id": "2509.02749", "pdf": "https://arxiv.org/pdf/2509.02749", "abs": "https://arxiv.org/abs/2509.02749", "authors": ["Giorgia Buracchio", "Ariele Callegari", "Massimo Donini", "Cristina Gena", "Antonio Lieto", "Alberto Lillo", "Claudio Mattutino", "Alessandro Mazzei", "Linda Pigureddu", "Manuel Striani", "Fabiana Vernero"], "title": "The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI", "categories": ["cs.RO"], "comment": "autohor copy of the paper accepted at ROMAN2025", "summary": "The paper presents an experiment on the effects of adaptive emotional\nalignment between agents, considered a prerequisite for empathic communication,\nin Human-Robot Interaction (HRI). Using the NAO robot, we investigate the\nimpact of an emotionally aligned, empathic, dialogue on these aspects: (i) the\nrobot's persuasive effectiveness, (ii) the user's communication style, and\n(iii) the attribution of mental states and empathy to the robot. In an\nexperiment with 42 participants, two conditions were compared: one with neutral\ncommunication and another where the robot provided responses adapted to the\nemotions expressed by the users. The results show that emotional alignment does\nnot influence users' communication styles or have a persuasive effect. However,\nit significantly influences attribution of mental states to the robot and its\nperceived empathy", "AI": {"tldr": "\u7814\u7a76\u4e86\u60c5\u611f\u5bf9\u9f50\u5728HRI\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5bf9\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u8bf4\u670d\u529b\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88ab\u8d4b\u4e88\u7684\u5fc3\u7406\u72b6\u6001\u548c\u5171\u60c5\u611f\u77e5\u3002", "motivation": "\u63a2\u7a76\u60c5\u611f\u5bf9\u9f50\u662f\u5426\u4f5c\u4e3a\u5171\u60c5\u6c9f\u901a\u7684\u524d\u63d0\uff0c\u5728HRI\u4e2d\u5f71\u54cd\u673a\u5668\u4eba\u7684\u8bf4\u670d\u529b\u3001\u7528\u6237\u6c9f\u901a\u98ce\u683c\u53ca\u673a\u5668\u4eba\u88ab\u611f\u77e5\u7684\u5fc3\u7406\u72b6\u6001\u548c\u5171\u60c5\u80fd\u529b\u3002", "method": "\u4f7f\u7528NAO\u673a\u5668\u4eba\uff0c\u572842\u540d\u53c2\u4e0e\u8005\u4e2d\u6bd4\u8f83\u4e2d\u6027\u6c9f\u901a\u4e0e\u60c5\u611f\u5bf9\u9f50\u6c9f\u901a\u4e24\u79cd\u6761\u4ef6\uff0c\u5206\u6790\u5176\u5bf9\u8bf4\u670d\u529b\u3001\u6c9f\u901a\u98ce\u683c\u53ca\u5fc3\u7406\u72b6\u6001\u5f52\u5c5e\u7684\u5f71\u54cd\u3002", "result": "\u60c5\u611f\u5bf9\u9f50\u5bf9\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u8bf4\u670d\u529b\u65e0\u5f71\u54cd\uff0c\u4f46\u663e\u8457\u589e\u52a0\u4e86\u673a\u5668\u4eba\u88ab\u8d4b\u4e88\u7684\u5fc3\u7406\u72b6\u6001\u548c\u5171\u60c5\u611f\u77e5\u3002", "conclusion": "\u60c5\u611f\u5bf9\u9f50\u5728HRI\u4e2d\u867d\u672a\u80fd\u63d0\u5347\u8bf4\u670d\u529b\u6216\u6539\u53d8\u6c9f\u901a\u98ce\u683c\uff0c\u4f46\u80fd\u6709\u6548\u589e\u5f3a\u5bf9\u673a\u5668\u4eba\u5fc3\u7406\u72b6\u6001\u548c\u5171\u60c5\u7684\u611f\u77e5\u3002"}}
{"id": "2509.03066", "pdf": "https://arxiv.org/pdf/2509.03066", "abs": "https://arxiv.org/abs/2509.03066", "authors": ["Huaicheng Zhang", "Ruoxin Wang", "Chenlian Zhou", "Jiguang Shi", "Yue Ge", "Zhoutong Li", "Sheng Chang", "Hao Wang", "Jin He", "Qijun Huang"], "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis.", "AI": {"tldr": "S2M2ECG\u662f\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u7ea7\u878d\u5408\u673a\u5236\u548c\u591a\u5206\u652f\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u5206\u6790\u4e0e\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548cECG\u7279\u5f02\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u5bfc\u8054ECG\u4fe1\u53f7\u5728\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u4e2d\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548c\u591a\u6e90\u7279\u5f81\u878d\u5408\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u7ea7\u878d\u5408\u673a\u5236\uff1a\u65f6\u7a7a\u53cc\u5411SSM\u3001\u5bfc\u8054\u5185\u65f6\u95f4\u4fe1\u606f\u878d\u5408\u548c\u5bfc\u8054\u95f4\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u591a\u5206\u652f\u8bbe\u8ba1\u548c\u5bfc\u8054\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728\u4e09\u7c7b\u573a\u666f\uff08\u8282\u5f8b\u3001\u5f62\u6001\u3001\u4e34\u5e8a\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u6781\u5c11\uff0c\u9002\u5408\u9ad8\u6548\u63a8\u7406\u4e0e\u90e8\u7f72\u3002", "conclusion": "S2M2ECG\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02760", "pdf": "https://arxiv.org/pdf/2509.02760", "abs": "https://arxiv.org/abs/2509.02760", "authors": ["Maximilian Neidhardt", "Ludwig Bosse", "Vidas Raudonis", "Kristina Allgoewer", "Axel Heinemann", "Benjamin Ondruschka", "Alexander Schlaefer"], "title": "A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality", "categories": ["cs.RO"], "comment": null, "summary": "Studying tissue samples obtained during autopsies is the gold standard when\ndiagnosing the cause of death and for understanding disease pathophysiology.\nRecently, the interest in post mortem minimally invasive biopsies has grown\nwhich is a less destructive approach in comparison to an open autopsy and\nreduces the risk of infection. While manual biopsies under ultrasound guidance\nare more widely performed, robotic post mortem biopsies have been recently\nproposed. This approach can further reduce the risk of infection for\nphysicians. However, planning of the procedure and control of the robot need to\nbe efficient and usable. We explore a virtual reality setup with a digital twin\nto realize fully remote planning and control of robotic post mortem biopsies.\nThe setup is evaluated with forensic pathologists in a usability study for\nthree interaction methods. Furthermore, we evaluate clinical feasibility and\nevaluate the system with three human cadavers. Overall, 132 needle insertions\nwere performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue\nsamples were successfully biopsied and histopathologically verified. Users\nreported a very intuitive needle placement approach, indicating that the system\nis a promising, precise, and low-risk alternative to conventional approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u8fdc\u7a0b\u89c4\u5212\u548c\u63a7\u5236\u673a\u5668\u4eba\u8fdb\u884c\u5c38\u4f53\u5fae\u521b\u6d3b\u68c0\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e34\u5e8a\u53ef\u884c\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u5c38\u68c0\u65b9\u5f0f\u5b58\u5728\u7834\u574f\u6027\u548c\u611f\u67d3\u98ce\u9669\uff0c\u800c\u673a\u5668\u4eba\u6d3b\u68c0\u6280\u672f\u53ef\u4ee5\u964d\u4f4e\u98ce\u9669\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u7814\u7a76\u91c7\u7528\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u4ea4\u4e92\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u4e34\u5e8a\u53ef\u884c\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "result": "\u7cfb\u7edf\u5b8c\u6210\u4e86132\u6b21\u9488\u5934\u63d2\u5165\uff0c\u5e73\u5747\u504f\u8f74\u8bef\u5dee\u4e3a5.30\u00b13.25\u6beb\u7c73\uff0c\u6d3b\u68c0\u6837\u672c\u7ecf\u7ec4\u7ec7\u75c5\u7406\u5b66\u9a8c\u8bc1\u6210\u529f\u3002\u7528\u6237\u53cd\u9988\u7cfb\u7edf\u76f4\u89c2\u3002", "conclusion": "\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u662f\u4e00\u79cd\u7cbe\u786e\u4e14\u4f4e\u98ce\u9669\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5c38\u4f53\u5fae\u521b\u6d3b\u68c0\uff0c\u7528\u6237\u4f53\u9a8c\u826f\u597d\u3002"}}
{"id": "2509.03070", "pdf": "https://arxiv.org/pdf/2509.03070", "abs": "https://arxiv.org/abs/2509.03070", "authors": ["Po-Heng Chou", "Wei-Lung Mao", "Ru-Ping Lin"], "title": "YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform", "categories": ["eess.SP"], "comment": "5 pages, 2 figures, 2 tables", "summary": "This letter proposes a YOLO-based framework for spatial bearing fault\ndiagnosis using time-frequency spectrograms derived from continuous wavelet\ntransform (CWT). One-dimensional vibration signals are first transformed into\ntime-frequency spectrograms using Morlet wavelets to capture transient fault\nsignatures. These spectrograms are then processed by YOLOv9, v10, and v11\nmodels to classify fault types. Evaluated on three benchmark datasets,\nincluding Case Western Reserve University (CWRU), Paderborn University (PU),\nand Intelligent Maintenance System (IMS), the proposed CWT--YOLO pipeline\nachieves significantly higher accuracy and generalizability than the baseline\nMCNN--LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%\n(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism\nenables direct visualization of fault locations in spectrograms, offering a\npractical solution for condition monitoring in rotating machinery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLO\u7684\u6846\u67b6\uff0c\u5229\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\uff08CWT\uff09\u751f\u6210\u7684\u65f6\u9891\u8c31\u56fe\u8fdb\u884c\u7a7a\u95f4\u8f74\u627f\u6545\u969c\u8bca\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u7684\u65f6\u9891\u5206\u6790\u548c\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u6539\u8fdb\u65cb\u8f6c\u673a\u68b0\u7684\u72b6\u6001\u76d1\u6d4b\u548c\u6545\u969c\u8bca\u65ad\u3002", "method": "\u5c06\u4e00\u7ef4\u632f\u52a8\u4fe1\u53f7\u901a\u8fc7Morlet\u5c0f\u6ce2\u8f6c\u6362\u4e3a\u65f6\u9891\u8c31\u56fe\uff0c\u5e76\u4f7f\u7528YOLOv9\u81f3v11\u6a21\u578b\u5206\u7c7b\u6545\u969c\u7c7b\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cYOLOv11\u7684mAP\u5206\u522b\u8fbe\u523099.4%\uff08CWRU\uff09\u300197.8%\uff08PU\uff09\u548c99.5%\uff08IMS\uff09\u3002", "conclusion": "CWT-YOLO\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u6545\u969c\uff0c\u8fd8\u80fd\u53ef\u89c6\u5316\u6545\u969c\u4f4d\u7f6e\uff0c\u4e3a\u65cb\u8f6c\u673a\u68b0\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02808", "pdf": "https://arxiv.org/pdf/2509.02808", "abs": "https://arxiv.org/abs/2509.02808", "authors": ["Isaac Ronald Ward", "Mark Paral", "Kristopher Riordan", "Mykel J. Kochenderfer"], "title": "Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted and awarded best paper at the 11th International Conference\n  on Control, Decision and Information Technologies (CoDIT 2025 -\n  https://codit2025.org/)", "summary": "Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668\u4e0e\u5b89\u5168\u63a7\u5236\u5668\u76f8\u7ed3\u5408\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528\u73af\u5883\u6570\u636e\u7684\u6b63\u6001\u6d41\u6a21\u578b\u6765\u68c0\u6d4b\u5206\u5e03\u5916\u60c5\u51b5\uff0c\u5b9e\u73b0\u5728\u590d\u6742\u5730\u4e0b\u73af\u5883\u4e2d\u5feb\u901f\u4e14\u5b89\u5168\u7684\u5bfc\u822a\u3002", "motivation": "\u5730\u4e0b\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u81ea\u4e3b\u63a7\u5236\u7684\u5e94\u7528\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668\u5728\u5206\u5e03\u5916\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6b63\u6001\u6d41\u6a21\u578b\u4f5c\u4e3a\u8fd0\u884c\u65f6\u76d1\u6d4b\u5668\uff0c\u7ed3\u5408\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u6839\u636e\u73af\u5883\u5206\u5e03\u5916\u60c5\u51b5\u52a8\u6001\u5207\u6362\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df3D\u6d1e\u7a74\u73af\u5883\u4e2d\u65e2\u80fd\u5feb\u901f\u5b8c\u6210\u4efb\u52a1\uff0c\u53c8\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "conclusion": "\u7ed3\u5408\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u5728\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.03077", "pdf": "https://arxiv.org/pdf/2509.03077", "abs": "https://arxiv.org/abs/2509.03077", "authors": ["Ogechukwu Kanu", "Ashkan Eshaghbeigi", "Hatem Abou-Zeid"], "title": "Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?", "categories": ["eess.SP"], "comment": "7 pages, 7 figures, IEEE international conference on communication\n  2025", "summary": "Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.\nHowever, a key challenge in developing AI-powered solutions is the extensive\ndata collection and labeling efforts required to train supervised deep learning\nmodels. To overcome this, self-supervised learning (SSL) approaches have\nrecently demonstrated remarkable success across various domains by leveraging\nlarge volumes of unlabeled data to achieve near-supervised performance. In this\npaper, we propose an effective SSL scheme for radio signal representation\nlearning using momentum contrast. By applying contrastive learning, our method\nextracts robust, transferable representations from a large real-world dataset.\nWe assess the generalizability of these learned representations across two\nwireless communications tasks: angle of arrival (AoA) estimation and automatic\nmodulation classification (AMC). Our results show that carefully designed\naugmentations and diverse data enable contrastive learning to produce\nhigh-quality, invariant latent representations. These representations are\neffective even with frozen encoder weights, and fine-tuning further enhances\nperformance, surpassing supervised baselines. To the best of our knowledge,\nthis is the first work to propose and demonstrate the effectiveness of\nself-supervised learning for radio signals across multiple tasks. Our findings\nhighlight the potential of self-supervised learning to transform AI for\nwireless communications by reducing dependence on labeled data and improving\nmodel generalization - paving the way for scalable foundational 6G AI models\nand solutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u65e0\u7ebf\u7535\u4fe1\u53f7\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ece\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u63d0\u53d6\u7a33\u5065\u4e14\u53ef\u8fc1\u79fb\u7684\u8868\u793a\uff0c\u5e76\u5728\u591a\u4e2a\u65e0\u7ebf\u901a\u4fe1\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1AI\u9a71\u52a8\u76846G\u89e3\u51b3\u65b9\u6848\u9700\u8981\u5927\u91cf\u6570\u636e\u6807\u6ce8\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u53ef\u901a\u8fc7\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u52a8\u91cf\u5bf9\u6bd4\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u548c\u591a\u6837\u5316\u6570\u636e\uff0c\u63d0\u53d6\u7a33\u5065\u7684\u4fe1\u53f7\u8868\u793a\u3002", "result": "\u5b66\u4e60\u5230\u7684\u8868\u793a\u5728AoA\u4f30\u8ba1\u548cAMC\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u4f18\u4e8e\u6709\u76d1\u7763\u57fa\u7ebf\uff0c\u4e14\u901a\u8fc7\u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u671b\u6539\u53d8\u65e0\u7ebf\u901a\u4fe1\u9886\u57dfAI\u5e94\u7528\u7684\u73b0\u72b6\uff0c\u4e3a6G AI\u6a21\u578b\u7684\u89c4\u6a21\u5316\u5e94\u7528\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2509.02815", "pdf": "https://arxiv.org/pdf/2509.02815", "abs": "https://arxiv.org/abs/2509.02815", "authors": ["Nico Bohlinger", "Jan Peters"], "title": "Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a single, general locomotion policy trained on a diverse\ncollection of 50 legged robots. By combining an improved embodiment-aware\narchitecture (URMAv2) with a performance-based curriculum for extreme\nEmbodiment Randomization, our policy learns to control millions of\nmorphological variations. Our policy achieves zero-shot transfer to unseen\nreal-world humanoid and quadruped robots.", "AI": {"tldr": "\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u9002\u7528\u4e8e50\u79cd\u4e0d\u540c\u7684\u817f\u90e8\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u67b6\u6784\u548c\u6027\u80fd\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u63a7\u5236\u591a\u79cd\u5f62\u6001\u7684\u817f\u90e8\u673a\u5668\u4eba\uff0c\u514b\u670d\u73b0\u5b9e\u4e16\u754c\u4e2d\u673a\u5668\u4eba\u5f62\u6001\u591a\u6837\u5316\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u7684URMAv2\u67b6\u6784\u548c\u6027\u80fd\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bad\u7ec3\u7b56\u7565\u4ee5\u9002\u5e94\u6570\u767e\u4e07\u79cd\u5f62\u6001\u53d8\u5316\u3002", "result": "\u7b56\u7565\u80fd\u591f\u5728\u672a\u7ecf\u8bad\u7ec3\u7684\u771f\u4eba\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5f62\u6001\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2509.03111", "pdf": "https://arxiv.org/pdf/2509.03111", "abs": "https://arxiv.org/abs/2509.03111", "authors": ["Hao Yang", "Guang Ouyang"], "title": "Handwriting Imagery EEG Classification based on Convolutional Neural Networks", "categories": ["eess.SP"], "comment": null, "summary": "Handwriting imagery has emerged as a promising paradigm for brain-computer\ninterfaces (BCIs) aimed at translating brain activity into text output.\nCompared with invasively recorded electroencephalography (EEG), non-invasive\nrecording offers a more practical and feasible approach to capturing brain\nsignals for BCI. This study explores the limit of decoding non-invasive EEG\nassociated with handwriting imagery into English letters using deep neural\nnetworks. To this end, five participants were instructed to imagine writing the\n26 English letters with their EEG being recorded from the scalp. A measurement\nof EEG similarity across letters was conducted to investigate letter-specific\npatterns in the dataset. Subsequently, four convolutional neural network (CNN)\nmodels were trained for EEG classification. Descriptively, the EEG data clearly\nexhibited letter-specific patterns serving as a proof-of-concept for\nEEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN\nclassifiers trained on each participant reached the highest limit of around\n20%. This study marks the first attempt to decode non-invasive EEG associated\nwith handwriting imagery. Although the achieved accuracy is not sufficient for\na usable brain-to-text BCI, the model's performance is noteworthy in revealing\nthe potential for translating non-invasively recorded brain signals into text\noutputs and establishing a baseline for future research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u4e0e\u975e\u4fb5\u5165\u6027\u8111\u7535\u56fe\uff08EEG\uff09\u76f8\u5173\u7684\u4e66\u5199\u60f3\u8c61\u7684\u82f1\u6587\u5b57\u6bcd\u7684\u6781\u9650\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86EEG\u5230\u6587\u672c\u7ffb\u8bd1\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u975e\u4fb5\u5165\u6027EEG\u5728\u4e66\u5199\u60f3\u8c61\u4efb\u52a1\u4e2d\u89e3\u7801\u82f1\u6587\u5b57\u6bcd\u7684\u6f5c\u529b\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bb0\u5f55\u4e94\u540d\u53c2\u4e0e\u8005\u60f3\u8c61\u4e66\u519926\u4e2a\u5b57\u6bcd\u65f6\u7684\u5934\u76aeEEG\uff0c\u5206\u6790\u5b57\u6bcd\u7279\u5f02\u6027\u6a21\u5f0f\uff0c\u5e76\u7528\u56db\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6570\u636e\u663e\u793a\u5b57\u6bcd\u7279\u5f02\u6027\u6a21\u5f0f\u7684EEG\u4fe1\u53f7\uff0cCNN\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u6700\u9ad8\u8fbe20%\uff08\u673a\u4f1a\u6c34\u5e73\u4e3a3.85%\uff09\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u867d\u7136\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u7814\u7a76\u8868\u660e\u975e\u4fb5\u5165\u6027EEG\u5728\u6587\u672c\u7ffb\u8bd1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.02870", "pdf": "https://arxiv.org/pdf/2509.02870", "abs": "https://arxiv.org/abs/2509.02870", "authors": ["Harsh Muriki", "Hong Ray Teo", "Ved Sengupta", "Ai-Ping Hu"], "title": "Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms", "categories": ["cs.RO"], "comment": "7 pages, 7 figures", "summary": "The small scale of urban farms and the commercial availability of low-cost\nrobots (such as the FarmBot) that automate simple tending tasks enable an\naccessible platform for plant phenotyping. We have used a FarmBot with a custom\ncamera end-effector to estimate strawberry plant flower pose (for robotic\npollination) from acquired 3D point cloud models. We describe a novel algorithm\nthat translates individual occupancy grids along orthogonal axes of a point\ncloud to obtain 2D images corresponding to the six viewpoints. For each image,\n2D object detection models for flowers are used to identify 2D bounding boxes\nwhich can be converted into the 3D space to extract flower point clouds. Pose\nestimation is performed by fitting three shapes (superellipsoids, paraboloids\nand planes) to the flower point clouds and compared with manually labeled\nground truth. Our method successfully finds approximately 80% of flowers\nscanned using our customized FarmBot platform and has a mean flower pose error\nof 7.7 degrees, which is sufficient for robotic pollination and rivals previous\nresults. All code will be made available at\nhttps://github.com/harshmuriki/flowerPose.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u6210\u672c\u519c\u573a\u673a\u5668\u4ebaFarmBot\u548c\u81ea\u5b9a\u4e49\u6444\u50cf\u5934\u7684\u65b0\u578b\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u8349\u8393\u82b1\u6735\u7684\u59ff\u6001\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u6388\u7c89\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u68c0\u6d4b\u7387\u548c\u8f83\u4f4e\u7684\u59ff\u6001\u8bef\u5dee\u3002", "motivation": "\u5229\u7528\u5c0f\u578b\u57ce\u5e02\u519c\u573a\u548c\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5b9e\u73b0\u690d\u7269\u8868\u578b\u5206\u6790\uff0c\u7279\u522b\u662f\u8349\u8393\u82b1\u6735\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u6388\u7c89\u3002", "method": "\u4f7f\u7528FarmBot\u91c7\u96c63D\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u65b0\u7b97\u6cd5\u5c06\u5176\u8f6c\u6362\u4e3a\u591a\u89c6\u89d22D\u56fe\u50cf\uff0c\u5229\u75282D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8bc6\u522b\u82b1\u6735\u5e76\u8f6c\u6362\u4e3a3D\u70b9\u4e91\uff0c\u6700\u540e\u901a\u8fc7\u62df\u5408\u5f62\u72b6\u4f30\u8ba1\u59ff\u6001\u3002", "result": "\u6210\u529f\u68c0\u6d4b\u7ea680%\u7684\u82b1\u6735\uff0c\u5e73\u5747\u59ff\u6001\u8bef\u5dee\u4e3a7.7\u5ea6\uff0c\u4f18\u4e8e\u5148\u524d\u7814\u7a76\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u6388\u7c89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8349\u8393\u82b1\u6735\u59ff\u6001\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4e3a\u673a\u5668\u4eba\u6388\u7c89\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.03193", "pdf": "https://arxiv.org/pdf/2509.03193", "abs": "https://arxiv.org/abs/2509.03193", "authors": ["Maximilian Neidhardt", "Sarah Latus", "Tim Eixmann", "Gereon H\u00fcttmann", "Alexander Schlaefer"], "title": "Deep Learning for High Speed Optical Coherence Elastography with a Fiber Scanning Endoscope", "categories": ["eess.SP"], "comment": null, "summary": "Tissue stiffness is related to soft tissue pathologies and can be assessed\nthrough palpation or via clinical imaging systems, e.g., ultrasound or magnetic\nresonance imaging. Typically, the image based approaches are not suitable\nduring interventions, particularly for minimally invasive surgery. To this end,\nwe present a miniaturized fiber scanning endoscope for fast and localized\nelastography. Moreover, we propose a deep learning based signal processing\npipeline to account for the intricate data and the need for real-time\nestimates. Our elasticity estimation approach is based on imaging complex and\ndiffuse wave fields that encompass multiple wave frequencies and propagate in\nvarious directions. We optimize the probe design to enable different scan\npatterns. To maximize temporal sampling while maintaining three-dimensional\ninformation we define a scan pattern in a conical shape with a temporal\nfrequency of 5.05 kHz. To efficiently process the image sequences of complex\nwave fields we consider a spatio-temporal deep learning network. We train the\nnetwork in an end-to-end fashion on measurements from phantoms representing\nmultiple elasticities. The network is used to obtain localized and robust\nelasticity estimates, allowing to create elasticity maps in real-time. For 2D\nscanning, our approach results in a mean absolute error of 6.31+-5.76 kPa\ncompared to 11.33+-12.78 kPa for conventional phase tracking. For scanning\nwithout estimating the wave direction, the novel 3D method reduces the error to\n4.48+-3.63 kPa compared to 19.75+-21.82 kPa for the conventional 2D method.\nFinally, we demonstrate feasibility of elasticity estimates in ex-vivo porcine\ntissue.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u578b\u5316\u5149\u7ea4\u626b\u63cf\u5185\u7aa5\u955c\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u5c40\u90e8\u5f39\u6027\u6210\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f39\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u50cf\u65b9\u6cd5\u5728\u5fae\u521b\u624b\u672f\u4e2d\u4e0d\u9002\u7528\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u65f6\u7684\u5c40\u90e8\u5f39\u6027\u6210\u50cf\u6280\u672f\u3002", "method": "\u91c7\u7528\u5fae\u578b\u5149\u7ea4\u626b\u63cf\u5185\u7aa5\u955c\u548c\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u4f18\u5316\u626b\u63cf\u6a21\u5f0f\u5e76\u5904\u7406\u590d\u6742\u6ce2\u573a\u6570\u636e\u3002", "result": "\u65b0\u65b9\u6cd5\u57282D\u548c3D\u626b\u63cf\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\uff0c\u5e76\u5728\u79bb\u4f53\u732a\u7ec4\u7ec7\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u5b9e\u65f6\u5f39\u6027\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5e72\u9884\u3002"}}
{"id": "2509.02876", "pdf": "https://arxiv.org/pdf/2509.02876", "abs": "https://arxiv.org/abs/2509.02876", "authors": ["Hongrui Yu", "Vineet R. Kamat", "Carol C. Menassa"], "title": "Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model", "categories": ["cs.RO"], "comment": "Under review for ASCE OPEN: Multidisciplinary Journal of Civil\n  Engineering", "summary": "The quasi-repetitive nature of construction work and the resulting lack of\ngeneralizability in programming construction robots presents persistent\nchallenges to the broad adoption of robots in the construction industry. Robots\ncannot achieve generalist capabilities as skills learnt from one domain cannot\nreadily transfer to another work domain or be directly used to perform a\ndifferent set of tasks. Human workers have to arduously reprogram their\nscene-understanding, path-planning, and manipulation components to enable the\nrobots to perform alternate work tasks. The methods presented in this paper\nresolve a significant proportion of such reprogramming workload by proposing a\ngeneralizable learning architecture that directly teaches robots versatile\ntask-performance skills through crowdsourced online natural language\ninstructions. A Large Language Model (LLM), a standardized and modularized\nhierarchical modeling approach, and Building Information Modeling-Robot sematic\ndata pipeline are developed to address the multi-task skill transfer problem.\nThe proposed skill standardization scheme and LLM-based hierarchical skill\nlearning framework were tested with a long-horizon drywall installation\nexperiment using a full-scale industrial robotic manipulator. The resulting\nrobot task learning scheme achieves multi-task reprogramming with minimal\neffort and high quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u4f17\u5305\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u76f4\u63a5\u6559\u6388\u673a\u5668\u4eba\u591a\u4efb\u52a1\u6280\u80fd\uff0c\u51cf\u5c11\u4e86\u91cd\u65b0\u7f16\u7a0b\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u673a\u5668\u4eba\u6280\u80fd\u96be\u4ee5\u901a\u7528\u5316\u7684\u6311\u6218\uff0c\u51cf\u5c11\u4eba\u5de5\u91cd\u65b0\u7f16\u7a0b\u7684\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u6807\u51c6\u5316\u6a21\u5757\u5316\u5206\u5c42\u5efa\u6a21\u65b9\u6cd5\u53caBIM-\u673a\u5668\u4eba\u8bed\u4e49\u6570\u636e\u7ba1\u9053\u3002", "result": "\u5728\u5e72\u5899\u5b89\u88c5\u5b9e\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u91cd\u65b0\u7f16\u7a0b\u7684\u9ad8\u6548\u4e0e\u9ad8\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u591a\u4efb\u52a1\u6280\u80fd\u8f6c\u79fb\u7684\u91cd\u65b0\u7f16\u7a0b\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2509.03273", "pdf": "https://arxiv.org/pdf/2509.03273", "abs": "https://arxiv.org/abs/2509.03273", "authors": ["Zeyuan Zhang", "Yue Xiu", "Zheng Dong", "Jiacheng Yin", "Maurice J. Khabbaz", "Chadi Assi", "Ning Wei"], "title": "Crosstalk-Resilient Beamforming for Movable Antenna Enabled Integrated Sensing and Communication", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates a movable antenna (MA) enabled integrated sensing and\ncommunication (ISAC) system under the influence of antenna crosstalk. First, it\ngeneralizes the antenna crosstalk model from the conventional fixed-position\nantenna (FPA) system to the MA scenario. Then, a Cramer-Rao bound (CRB)\nminimization problem driven by joint beamforming and antenna position design is\npresented. Specifically, to address this highly non-convex flexible beamforming\nproblem, we deploy a deep reinforcement learning (DRL) approach to train a\nflexible beamforming agent. To ensure stability during training, a Twin Delayed\nDeep Deterministic Policy Gradient (TD3) algorithm is adopted to balance\nexploration with reward maximization for efficient and reliable learning.\nNumerical results demonstrate that the proposed crosstalk-resilient (CR)\nalgorithm enhances the overall ISAC performance compared to other benchmark\nschemes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u652f\u6301\u7684\u7efc\u5408\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u5728\u4e32\u6270\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8054\u5408\u6ce2\u675f\u6210\u5f62\u4e0e\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u7cfb\u7edf\u5728\u4e32\u6270\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u7684TD3\u7b97\u6cd5\uff0c\u8054\u5408\u8bbe\u8ba1\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u4f4d\u7f6e\uff0c\u4ee5\u6700\u5c0f\u5316CRB\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6297\u4e32\u6270\uff08CR\uff09\u7b97\u6cd5\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86ISAC\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86DRL\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.02972", "pdf": "https://arxiv.org/pdf/2509.02972", "abs": "https://arxiv.org/abs/2509.02972", "authors": ["Haolan Zhang", "Thanh Nguyen Canh", "Chenghao Li", "Ruidong Yang", "Yonghoon Ji", "Nak Young Chong"], "title": "IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments", "categories": ["cs.RO"], "comment": "submitted to International Conference on Robotic Computing and\n  Communication(IEEE IRC)", "summary": "Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in\nautonomous systems. Traditional SLAM methods, based on static environment\nassumptions, struggle to handle complex dynamic environments. Recent dynamic\nSLAM systems employ geometric constraints and deep learning to remove dynamic\nfeatures, yet this creates a new challenge: insufficient remaining point\nfeatures for subsequent SLAM processes. Existing solutions address this by\ncontinuously introducing additional line and plane features to supplement point\nfeatures, achieving robust tracking and pose estimation. However, current\nmethods continuously introduce additional features regardless of necessity,\ncausing two problems: unnecessary computational overhead and potential\nperformance degradation from accumulated low-quality additional features and\nnoise. To address these issues, this paper proposes a feature-aware mechanism\nthat evaluates whether current features are adequate to determine if line\nfeature support should be activated. This decision mechanism enables the system\nto introduce line features only when necessary, significantly reducing\ncomputational complexity of additional features while minimizing the\nintroduction of low-quality features and noise. In subsequent processing, the\nintroduced line features assist in obtaining better initial camera poses\nthrough tracking, local mapping, and loop closure, but are excluded from global\noptimization to avoid potential negative impacts from low-quality additional\nfeatures in long-term process. Extensive experiments on TUM datasets\ndemonstrate substantial improvements in both ATE and RPE metrics compared to\nORB-SLAM3 baseline and superior performance over other dynamic SLAM and\nmulti-feature methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u611f\u77e5\u673a\u5236\uff0c\u4ec5\u5728\u9700\u8981\u65f6\u5f15\u5165\u7ebf\u7279\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u4f4e\u8d28\u91cf\u7279\u5f81\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u52a8\u6001SLAM\u7cfb\u7edf\u867d\u7136\u901a\u8fc7\u6dfb\u52a0\u7ebf\u3001\u9762\u7279\u5f81\u5f25\u8865\u70b9\u7279\u5f81\u7684\u4e0d\u8db3\uff0c\u4f46\u4f1a\u5e26\u6765\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u7279\u5f81\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u662f\u5426\u9700\u8981\u5f15\u5165\u7ebf\u7279\u5f81\uff0c\u5e76\u5728\u540e\u7eed\u5904\u7406\u4e2d\u907f\u514d\u4f4e\u8d28\u91cf\u7279\u5f81\u5bf9\u5168\u5c40\u4f18\u5316\u7684\u5e72\u6270\u3002", "result": "\u5728TUM\u6570\u636e\u96c6\u4e0a\uff0cATE\u548cRPE\u6307\u6807\u4f18\u4e8eORB-SLAM3\u57fa\u51c6\u548c\u5176\u4ed6\u52a8\u6001SLAM\u65b9\u6cd5\u3002", "conclusion": "\u7279\u5f81\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001SLAM\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.03311", "pdf": "https://arxiv.org/pdf/2509.03311", "abs": "https://arxiv.org/abs/2509.03311", "authors": ["Penggao Yan", "Li-Ta Hsu"], "title": "Credible Uncertainty Quantification under Noise and System Model Mismatch", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "This manuscript has been submitted to IEEE Signal Processing Letters", "summary": "State estimators often provide self-assessed uncertainty metrics, such as\ncovariance matrices, whose reliability is critical for downstream tasks.\nHowever, these self-assessments can be misleading due to underlying modeling\nviolations like noise or system model mismatch. This letter addresses the\nproblem of estimator credibility by introducing a unified, multi-metric\nevaluation framework. We construct a compact credibility portfolio that\nsynergistically combines traditional metrics like the Normalized Estimation\nError Squared (NEES) and the Noncredibility Index (NCI) with proper scoring\nrules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our\nkey contributions are a novel energy distance-based location test to robustly\ndetect system model misspecification and a method that leverages the asymmetric\nsensitivities of NLL and ES to distinguish optimism covariance scaling from\nsystem bias. Monte Carlo simulations across six distinct credibility scenarios\ndemonstrate that our proposed method achieves high classification accuracy\n(80-100%), drastically outperforming single-metric baselines which consistently\nfail to provide a complete and correct diagnosis. This framework provides a\npractical tool for turning patterns of credibility indicators into actionable\ndiagnoses of model deficiencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u6307\u6807\uff08\u5982NEES\u3001NCI\u3001NLL\u3001ES\uff09\u6765\u8bc4\u4f30\u72b6\u6001\u4f30\u8ba1\u5668\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5\u68c0\u6d4b\u6a21\u578b\u9519\u8bef\u6307\u5b9a\u548c\u533a\u5206\u534f\u65b9\u5dee\u7f29\u653e\u4e0e\u7cfb\u7edf\u504f\u5dee\u3002", "motivation": "\u72b6\u6001\u4f30\u8ba1\u5668\u7684\u81ea\u6211\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\u6307\u6807\uff08\u5982\u534f\u65b9\u5dee\u77e9\u9635\uff09\u7684\u53ef\u9760\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u53ef\u80fd\u56e0\u566a\u58f0\u6216\u6a21\u578b\u4e0d\u5339\u914d\u800c\u8bef\u5bfc\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7d27\u51d1\u7684\u53ef\u4fe1\u5ea6\u7ec4\u5408\uff0c\u7ed3\u5408\u4f20\u7edf\u6307\u6807\u548c\u8bc4\u5206\u89c4\u5219\uff08\u5982NLL\u3001ES\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u8ddd\u79bb\u7684\u4f4d\u7f6e\u6d4b\u8bd5\u548c\u65b0\u65b9\u6cd5\u533a\u5206\u95ee\u9898\u7c7b\u578b\u3002", "result": "\u8499\u7279\u5361\u6d1b\u6a21\u62df\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u4e0d\u540c\u573a\u666f\u4e0b\u5206\u7c7b\u51c6\u786e\u7387\u8fbe80-100%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6307\u6807\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u53ef\u4fe1\u5ea6\u6307\u6807\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6a21\u578b\u7f3a\u9677\u8bca\u65ad\u5de5\u5177\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2509.02983", "pdf": "https://arxiv.org/pdf/2509.02983", "abs": "https://arxiv.org/abs/2509.02983", "authors": ["Jinghe Yang", "Minh-Quan Le", "Mingming Gong", "Ye Pu"], "title": "DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous underwater navigation remains a challenging problem due to limited\nsensing capabilities and the difficulty of constructing accurate maps in\nunderwater environments. In this paper, we propose a Diffusion-based Underwater\nVisual Navigation policy via knowledge-transferred depth features, named DUViN,\nwhich enables vision-based end-to-end 4-DoF motion control for underwater\nvehicles in unknown environments. DUViN guides the vehicle to avoid obstacles\nand maintain a safe and perception awareness altitude relative to the terrain\nwithout relying on pre-built maps. To address the difficulty of collecting\nlarge-scale underwater navigation datasets, we propose a method that ensures\nrobust generalization under domain shifts from in-air to underwater\nenvironments by leveraging depth features and introducing a novel model\ntransfer strategy. Specifically, our training framework consists of two phases:\nwe first train the diffusion-based visual navigation policy on in-air datasets\nusing a pre-trained depth feature extractor. Secondly, we retrain the extractor\non an underwater depth estimation task and integrate the adapted extractor into\nthe trained navigation policy from the first step. Experiments in both\nsimulated and real-world underwater environments demonstrate the effectiveness\nand generalization of our approach. The experimental videos are available at\nhttps://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565DUViN\uff0c\u5229\u7528\u77e5\u8bc6\u8fc1\u79fb\u7684\u6df1\u5ea6\u7279\u5f81\u5b9e\u73b0\u6c34\u4e0b\u65e0\u4eba\u8f66\u8f86\u7684\u7aef\u5230\u7aef\u8fd0\u52a8\u63a7\u5236\uff0c\u65e0\u9700\u9884\u5efa\u5730\u56fe\u3002", "motivation": "\u6c34\u4e0b\u5bfc\u822a\u56e0\u611f\u77e5\u80fd\u529b\u53d7\u9650\u548c\u73af\u5883\u5730\u56fe\u6784\u5efa\u56f0\u96be\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u6c34\u4e0b\u6570\u636e\u96c6\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5728\u9646\u5730\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6269\u6563\u5bfc\u822a\u7b56\u7565\uff0c\u518d\u5728\u6c34\u4e0b\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e0a\u5fae\u8c03\u7279\u5f81\u63d0\u53d6\u5668\u5e76\u6574\u5408\u5230\u5bfc\u822a\u7b56\u7565\u4e2d\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u6c34\u4e0b\u73af\u5883\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DUViN\u662f\u4e00\u79cd\u65e0\u9700\u9884\u5efa\u5730\u56fe\u7684\u9c81\u68d2\u6c34\u4e0b\u89c6\u89c9\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03333", "pdf": "https://arxiv.org/pdf/2509.03333", "abs": "https://arxiv.org/abs/2509.03333", "authors": ["Tianfu Qi", "Jun Wang"], "title": "Baseband Model, Cutoff Rate Bounds and Constellation Shaping for Mixed Gaussian-Impulsive Noise", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "Mixed noise, composed of white Gaussian noise (WGN) and impulsive noise (IN),\nappears in numerous communication scenarios and can severely degrade system\nperformance. In this paper, we address this issue by optimizing the transmitted\nconstellation under mixed noise based on a theoretical analysis of the cutoff\nrate (CR). First, starting from the passband model of the mixed noise, we\nderive its corresponding baseband representation. Due to the complexity of the\nCR, an exact analytic expression is generally intractable. Therefore, the\nbaseband noise model is employed to obtain closed-form lower and upper bounds\nof the CR. A piecewise linear approximation is applied to derive efficient\nbounds by exploiting the algebraic properties of the integral terms. These\nbounds are then used as criteria to optimize the transmitted constellation\npoints in both geometric and probabilistic distributions. The projected\ngradient method is employed to solve the optimization problem, and the\nconvergence and properties of the solutions are analyzed. Numerical results\ndemonstrate that the proposed CR bounds are tight and exhibit the expected\nasymptotic behavior. Furthermore, the optimized constellation scheme achieves a\nsignificant rate improvement compared to baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u661f\u5ea7\u70b9\uff0c\u9488\u5bf9\u6df7\u5408\u566a\u58f0\uff08\u9ad8\u65af\u767d\u566a\u58f0\u548c\u8109\u51b2\u566a\u58f0\uff09\u95ee\u9898\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u622a\u6b62\u7387\u7684\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u51fa\u7d27\u51d1\u7684\u4e0a\u4e0b\u754c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u63d0\u5347\u3002", "motivation": "\u6df7\u5408\u566a\u58f0\uff08\u9ad8\u65af\u767d\u566a\u58f0\u548c\u8109\u51b2\u566a\u58f0\uff09\u5728\u901a\u4fe1\u573a\u666f\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u661f\u5ea7\u70b9\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9996\u5148\u4ece\u6df7\u5408\u566a\u58f0\u7684\u901a\u5e26\u6a21\u578b\u63a8\u5bfc\u51fa\u57fa\u5e26\u8868\u793a\uff0c\u5229\u7528\u622a\u6b62\u7387\u7684\u590d\u6742\u5206\u6790\uff0c\u5f97\u5230\u5176\u5c01\u95ed\u5f62\u5f0f\u7684\u4e0a\u4e0b\u754c\u3002\u91c7\u7528\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5229\u7528\u79ef\u5206\u9879\u7684\u4ee3\u6570\u6027\u8d28\uff0c\u9ad8\u6548\u63a8\u5bfc\u754c\u9650\u3002\u968f\u540e\uff0c\u5c06\u8fd9\u4e9b\u754c\u9650\u4f5c\u4e3a\u4f18\u5316\u661f\u5ea7\u70b9\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03\u7684\u6807\u51c6\uff0c\u4f7f\u7528\u6295\u5f71\u68af\u5ea6\u6cd5\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5bf9\u89e3\u7684\u6536\u655b\u6027\u548c\u6027\u8d28\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u622a\u6b62\u7387\u754c\u9650\u7d27\u51d1\u4e14\u5177\u5907\u9884\u671f\u7684\u6e10\u8fd1\u884c\u4e3a\u3002\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u7684\u661f\u5ea7\u65b9\u6848\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u63d0\u5347\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df7\u5408\u566a\u58f0\u73af\u5883\u4e0b\u7684\u4f20\u8f93\u661f\u5ea7\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.02986", "pdf": "https://arxiv.org/pdf/2509.02986", "abs": "https://arxiv.org/abs/2509.02986", "authors": ["Rankun Li", "Hao Wang", "Qi Li", "Zhuo Han", "Yifei Chu", "Linqi Ye", "Wende Xie", "Wenlong Liao"], "title": "CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "In recent years, wheeled bipedal robots have gained increasing attention due\nto their advantages in mobility, such as high-speed locomotion on flat terrain.\nHowever, their performance on complex environments (e.g., staircases) remains\ninferior to that of traditional legged robots. To overcome this limitation, we\npropose a general contact-triggered blind climbing (CTBC) framework for wheeled\nbipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a\nleg-lifting motion to overcome the obstacle. By leveraging a strongly-guided\nfeedforward trajectory, our method enables the robot to rapidly acquire agile\nleg-lifting skills, significantly enhancing its capability to traverse\nunstructured terrains. The approach has been experimentally validated and\nsuccessfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.\nReal-world tests demonstrate that Tron1 can reliably climb obstacles well\nbeyond its wheel radius using only proprioceptive feedback.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u63a5\u89e6\u89e6\u53d1\u76f2\u722c\uff08CTBC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u8f66\u8f6e-\u969c\u788d\u7269\u63a5\u89e6\u89e6\u53d1\u62ac\u817f\u52a8\u4f5c\uff0c\u7ed3\u5408\u5f3a\u5f15\u5bfc\u524d\u9988\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u697c\u68af\uff09\u4e2d\u7684\u79fb\u52a8\u80fd\u529b\uff0c\u5f25\u8865\u5176\u5728\u975e\u5e73\u5766\u5730\u5f62\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u63a5\u89e6\u89e6\u53d1\u76f2\u722c\uff08CTBC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8f66\u8f6e-\u969c\u788d\u7269\u63a5\u89e6\u68c0\u6d4b\u89e6\u53d1\u62ac\u817f\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408\u5f3a\u5f15\u5bfc\u524d\u9988\u8f68\u8ff9\u6765\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\u654f\u6377\u62ac\u817f\u6280\u80fd\u3002", "result": "\u5728LimX Dynamics\u7684\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4ebaTron1\u4e0a\u6210\u529f\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4ec5\u4f9d\u9760\u672c\u4f53\u611f\u77e5\u53cd\u9988\u5373\u53ef\u53ef\u9760\u6500\u722c\u8d85\u8fc7\u5176\u8f66\u8f6e\u534a\u5f84\u7684\u969c\u788d\u7269\u3002", "conclusion": "CTBC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u7684\u79fb\u52a8\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.03488", "pdf": "https://arxiv.org/pdf/2509.03488", "abs": "https://arxiv.org/abs/2509.03488", "authors": ["Miguel Rivas-Costa", "Carlos Mosquera"], "title": "Efficient DoA Estimation with Hybrid Linear and Rectangular Arrays Using Compact DFT Codebook", "categories": ["eess.SP"], "comment": null, "summary": "Hybrid Analog and Digital (HAD) architectures provide a cost-effective\nalternative for large-scale antenna arrays, but accurate Direction-of-Arrival\n(DoA) estimation remains challenging due to limited digital dimensionality and\nconstrained beamforming design. In this work, we propose a HAD architecture\nthat employs Butler matrices to synthesize DFT beams over a uniform linear\narray. By exploiting the Cauchy-like displacement structure of the beamformed\nsignal, we introduce a second-order statistics estimation algorithm that\nachieves near-optimal accuracy, approaching the Cram\\'er-Rao Lower Bound (CRLB)\nand outperforming state-of-the-art methods in simulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Butler\u77e9\u9635\u7684\u6df7\u5408\u6a21\u62df\u6570\u5b57\uff08HAD\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u65b9\u5411\u5230\u8fbe\uff08DoA\uff09\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u65b0\u7b97\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1CRLB\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u4e2dHAD\u67b6\u6784\u56e0\u6570\u5b57\u7ef4\u5ea6\u6709\u9650\u548c\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\u7ea6\u675f\u5bfc\u81f4\u7684DoA\u4f30\u8ba1\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528Butler\u77e9\u9635\u5728\u5747\u5300\u7ebf\u6027\u9635\u5217\u4e0a\u5408\u6210DFT\u6ce2\u675f\uff0c\u5e76\u5229\u7528\u6ce2\u675f\u5f62\u6210\u4fe1\u53f7\u7684Cauchy-like\u4f4d\u79fb\u7ed3\u6784\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e8c\u9636\u7edf\u8ba1\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728DoA\u4f30\u8ba1\u4e0a\u8fbe\u5230\u4e86\u63a5\u8fd1CRLB\u7684\u63a5\u8fd1\u6700\u4f18\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684HAD\u67b6\u6784\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86DoA\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.03012", "pdf": "https://arxiv.org/pdf/2509.03012", "abs": "https://arxiv.org/abs/2509.03012", "authors": ["Uddeshya Upadhyay"], "title": "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) are increasingly being used in autonomous\nsystems. However, DNNs do not generalize well to domain shift. Adapting to a\ncontinuously evolving environment is a safety-critical challenge inevitably\nfaced by all autonomous systems deployed to the real world. Recent work on\ntest-time training proposes methods that adapt to a new test distribution on\nthe fly by optimizing the DNN model for each test input using self-supervision.\nHowever, these techniques result in a sharp increase in inference time as\nmultiple forward and backward passes are required for a single test sample (for\ntest-time training) before finally making the prediction based on the\nfine-tuned features. This is undesirable for real-world robotics applications\nwhere these models may be deployed to resource constraint hardware with strong\nlatency requirements. In this work, we propose a new framework (called UT$^3$)\nthat leverages test-time training for improved performance in the presence of\ncontinuous domain shift while also decreasing the inference time, making it\nsuitable for real-world applications. Our method proposes an uncertainty-aware\nself-supervision task for efficient test-time training that leverages the\nquantified uncertainty to selectively apply the training leading to sharp\nimprovements in the inference time while performing comparably to standard\ntest-time training protocol. Our proposed protocol offers a continuous setting\nto identify the selected keyframes, allowing the end-user to control how often\nto apply test-time training. We demonstrate the efficacy of our method on a\ndense regression task - monocular depth estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aUT\u00b3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u5728\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u7684\u540c\u65f6\u5e94\u5bf9\u6301\u7eed\u57df\u504f\u79fb\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728\u9762\u5bf9\u57df\u504f\u79fb\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u65b9\u6cd5\u867d\u80fd\u9002\u5e94\u65b0\u6d4b\u8bd5\u5206\u5e03\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u5927\u5e45\u589e\u52a0\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u3002", "method": "UT\u00b3\u6846\u67b6\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u76d1\u7763\u4efb\u52a1\uff0c\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6027\u5e94\u7528\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002", "result": "\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0cUT\u00b3\u5728\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u7684\u540c\u65f6\u8868\u73b0\u4e0e\u6807\u51c6\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "UT\u00b3\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02571", "pdf": "https://arxiv.org/pdf/2509.02571", "abs": "https://arxiv.org/abs/2509.02571", "authors": ["Diego Di Carlo", "Koyama Shoichi", "Nugraha Aditya Arie", "Fontaine Mathieu", "Bando Yoshiaki", "Yoshii Kazuyoshi"], "title": "Gaussian Process Regression of Steering Vectors With Physics-Aware Deep Composite Kernels for Augmented Listening", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": null, "summary": "This paper investigates continuous representations of steering vectors over\nfrequency and position of microphone and source for augmented listening (e.g.,\nspatial filtering and binaural rendering) with precise control of the sound\nfield perceived by the user. Steering vectors have typically been used for\nrepresenting the spatial characteristics of the sound field as a function of\nthe listening position. The basic algebraic representation of steering vectors\nassuming an idealized environment cannot deal with the scattering effect of the\nsound field. One may thus collect a discrete set of real steering vectors\nmeasured in dedicated facilities and super-resolve (i.e., upsample) them.\nRecently, physics-aware deep learning methods have been effectively used for\nthis purpose. Such deterministic super-resolution, however, suffers from the\noverfitting problem due to the non-uniform uncertainty over the measurement\nspace. To solve this problem, we integrate an expressive representation based\non the neural field (NF) into the principled probabilistic framework based on\nthe Gaussian process (GP). Specifically, we propose a physics-aware composite\nkernel that model the directional incoming waves and the subsequent scattering\neffect. Our comprehensive comparative experiment showed the effectiveness of\nthe proposed method under data insufficiency conditions. In downstream tasks\nsuch as speech enhancement and binaural rendering using the simulated data of\nthe SPEAR challenge, the oracle performances were attained with less than ten\ntimes fewer measurements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\uff08NF\uff09\u548c\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u7269\u7406\u611f\u77e5\u590d\u5408\u6838\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u786e\u5b9a\u6027\u8d85\u5206\u8fa8\u7387\u5728\u58f0\u573a\u8868\u793a\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5bfc\u5411\u5411\u91cf\u8868\u793a\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u58f0\u573a\u7684\u6563\u5c04\u6548\u5e94\uff0c\u4e14\u786e\u5b9a\u6027\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5bb9\u6613\u56e0\u6d4b\u91cf\u7a7a\u95f4\u7684\u4e0d\u5747\u5300\u6027\u5bfc\u81f4\u8fc7\u62df\u5408\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u573a\uff08NF\uff09\u548c\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u7684\u590d\u5408\u6838\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u786e\u8868\u793a\u58f0\u573a\u7684\u5b9a\u5411\u5165\u5c04\u6ce2\u548c\u6563\u5c04\u6548\u5e94\u3002", "result": "\u5728\u6570\u636e\u4e0d\u8db3\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u5728\u8bed\u97f3\u589e\u5f3a\u548c\u53cc\u8033\u6e32\u67d3\u7b49\u4efb\u52a1\u4e2d\uff0c\u4ec5\u9700\u4e0d\u5230\u5341\u5206\u4e4b\u4e00\u7684\u6d4b\u91cf\u6570\u636e\u5373\u53ef\u8fbe\u5230\u57fa\u51c6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u58f0\u573a\u7684\u8fde\u7eed\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.03119", "pdf": "https://arxiv.org/pdf/2509.03119", "abs": "https://arxiv.org/abs/2509.03119", "authors": ["Yash Vyas", "Matteo Bottin"], "title": "Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "A force balanced manipulator design based on the closed chain planar five bar\nlinkage is developed and experimentally validated. We present 2 variants as a\nmodular design: Forbal-2, a planar 2-DOF manipulator, and its extension to\n5-DOF spatial motion called Forbal-5. The design considerations in terms of\ngeometric, kinematic, and dynamic design that fulfill the force balance\nconditions while maximizing workspace are discussed. Then, the inverse\nkinematics of both variants are derived from geometric principles.\n  We validate the improvements from force balancing the manipulator through\ncomparative experiments with counter mass balanced and unbalanced\nconfigurations. The results show how the balanced configuration yields a\nreduction in the average reaction moments of up to 66\\%, a reduction of average\njoint torques of up to 79\\%, as well as a noticeable reduction in position\nerror for Forbal-2. For Forbal-5, which has a higher end effector payload mass,\nthe joint torques are reduced up to 84\\% for the balanced configuration.\nExperimental results validate that the balanced manipulator design is suitable\nfor applications where the reduction of joint torques and reaction\nforces/moments helps achieve millimeter level precision.", "AI": {"tldr": "\u8bba\u6587\u57fa\u4e8e\u5c01\u95ed\u94fe\u5e73\u9762\u4e94\u6746\u673a\u6784\u5f00\u53d1\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u529b\u5e73\u8861\u673a\u68b0\u81c2\u8bbe\u8ba1\uff0c\u5305\u62ec2-DOF\u7684Forbal-2\u548c5-DOF\u7684Forbal-5\uff0c\u5c55\u793a\u4e86\u529b\u5e73\u8861\u914d\u7f6e\u5bf9\u964d\u4f4e\u5173\u8282\u529b\u77e9\u548c\u53cd\u4f5c\u7528\u529b\u77e9\u7684\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u68b0\u81c2\u7684\u7cbe\u5ea6\u5e76\u51cf\u5c11\u5173\u8282\u529b\u77e9\u548c\u53cd\u4f5c\u7528\u529b\u77e9\uff0c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u529b\u5e73\u8861\u7684\u673a\u68b0\u81c2\u8bbe\u8ba1\uff0c\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u5c01\u95ed\u94fe\u5e73\u9762\u4e94\u6746\u673a\u6784\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6a21\u5757\u5316\u53d8\u4f53\uff08Forbal-2\u548cForbal-5\uff09\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u529b\u5e73\u8861\u6761\u4ef6\u3002\u63a8\u5bfc\u4e86\u4e24\u79cd\u53d8\u4f53\u7684\u9006\u8fd0\u52a8\u5b66\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u529b\u5e73\u8861\u914d\u7f6e\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u529b\u5e73\u8861\u914d\u7f6e\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u53cd\u4f5c\u7528\u529b\u77e9\uff0866%\uff09\u3001\u5e73\u5747\u5173\u8282\u529b\u77e9\uff0879%-84%\uff09\uff0c\u5e76\u63d0\u9ad8\u4e86Forbal-2\u7684\u4f4d\u7f6e\u7cbe\u5ea6\u3002Forbal-5\u7684\u5173\u8282\u529b\u77e9\u51cf\u5c11\u4e8684%\uff0c\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u529b\u5e73\u8861\u673a\u68b0\u81c2\u8bbe\u8ba1\u9002\u7528\u4e8e\u9700\u8981\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5e94\u7528\u573a\u666f\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u5173\u8282\u529b\u77e9\u548c\u53cd\u4f5c\u7528\u529b\u77e9\u7684\u540c\u65f6\u63d0\u5347\u673a\u68b0\u81c2\u6027\u80fd\u3002"}}
{"id": "2509.02622", "pdf": "https://arxiv.org/pdf/2509.02622", "abs": "https://arxiv.org/abs/2509.02622", "authors": ["Berger Cl\u00e9mentine", "Stamadiatis Paraskevas", "Badeau Roland", "Essid Slim"], "title": "IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": null, "summary": "We are interested in audio systems capable of performing a differentiated\nprocessing of stationary backgrounds and isolated acoustic events within an\nacoustic scene, whether for applying specific processing methods to each part\nor for focusing solely on one while ignoring the other. Such systems have\napplications in real-world scenarios, including robust adaptive audio rendering\nsystems (e.g., EQ or compression), plosive attenuation in voice mixing, noise\nsuppression or reduction, robust acoustic event classification or even\nbioacoustics. To this end, we introduce IS${}^3$, a neural network designed for\nImpulsive--Stationary Sound Separation, that isolates impulsive acoustic events\nfrom the stationary background using a deep filtering approach, that can act as\na pre-processing stage for the above-mentioned tasks. To ensure optimal\ntraining, we propose a sophisticated data generation pipeline that curates and\nadapts existing datasets for this task. We demonstrate that a learning-based\napproach, build on a relatively lightweight neural architecture and trained\nwith well-designed and varied data, is successful in this previously\nunaddressed task, outperforming the Harmonic--Percussive Sound Separation\nmasking method, adapted from music signal processing research, and wavelet\nfiltering on objective separation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIS\u00b3\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5206\u79bb\u97f3\u9891\u4e2d\u7684\u77ac\u6001\u4e8b\u4ef6\u548c\u7a33\u6001\u80cc\u666f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u97f3\u9891\u7cfb\u7edf\u4e2d\u5bf9\u7a33\u6001\u80cc\u666f\u548c\u77ac\u6001\u4e8b\u4ef6\u7684\u5dee\u5f02\u5316\u5904\u7406\u9700\u6c42\uff0c\u5e94\u7528\u4e8eEQ\u3001\u566a\u58f0\u6291\u5236\u3001\u58f0\u5b66\u4e8b\u4ef6\u5206\u7c7b\u7b49\u5b9e\u9645\u573a\u666f\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784IS\u00b3\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u8fdb\u884c\u77ac\u6001\u4e0e\u7a33\u6001\u58f0\u97f3\u7684\u5206\u79bb\u3002", "result": "IS\u00b3\u5728\u5206\u79bb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u4e8e\u97f3\u4e50\u4fe1\u53f7\u5904\u7406\u7684HPSS\u65b9\u6cd5\u548c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6ee4\u6ce2\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u77ac\u6001-\u7a33\u6001\u58f0\u97f3\u5206\u79bb\u7684\u672a\u89e3\u95ee\u9898\u3002"}}
{"id": "2509.03211", "pdf": "https://arxiv.org/pdf/2509.03211", "abs": "https://arxiv.org/abs/2509.03211", "authors": ["Beibei Zhou", "Zhiyuan Zhang", "Zhenbo Song", "Jianhui Guo", "Hui Kong"], "title": "Efficient Active Training for Deep LiDAR Odometry", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63d0\u53d6\u591a\u6837\u5316\u73af\u5883\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u8bad\u7ec3\u8d1f\u8377\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u970052%\u7684\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4ee5\u9002\u5e94\u4e0d\u540c\u73af\u5883\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u8bad\u7ec3\u6846\u67b6\uff0c\u4f18\u5316\u6570\u636e\u9009\u62e9\u8fc7\u7a0b\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u4e24\u4e2a\u7b56\u7565\uff1a\u521d\u59cb\u8bad\u7ec3\u96c6\u9009\u62e9\uff08ITSS\uff09\u548c\u4e3b\u52a8\u589e\u91cf\u9009\u62e9\uff08AIS\uff09\u3002ITSS\u901a\u8fc7\u5206\u6790\u8f68\u8ff9\u9009\u62e9\u591a\u6837\u5316\u5e8f\u5217\uff0cAIS\u5219\u9488\u5bf9\u590d\u6742\u573a\u666f\uff08\u5982\u96ea\u5929\uff09\u52a8\u6001\u9009\u62e9\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u970052%\u7684\u6570\u636e\u91cf\u5373\u53ef\u8fbe\u5230\u4e0e\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u80fd\u66f4\u7cbe\u51c6\u5e94\u5bf9\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u3002"}}
{"id": "2509.02967", "pdf": "https://arxiv.org/pdf/2509.02967", "abs": "https://arxiv.org/abs/2509.02967", "authors": ["Chen Zeng", "Tiehang Xu", "Qiao Wang"], "title": "AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Conventional neural networks frequently face challenges in spectral analysis\nof signals. To address this challenge, Fourier neural networks (FNNs) and\nsimilar approaches integrate components of Fourier series into the structure of\nneural networks. Nonetheless, a significant hurdle is often overlooked: the\nsuperposition of periodic signals does not necessarily result in a periodic\nsignal. For example, when forecasting almost periodic functions composed of\nsignals with incommensurate frequencies, traditional models such as\nAutoregressive Integrated Moving Average (ARIMA) frequently outperform most\nneural networks including large language models (LLMs). To tackle this goal, we\npropose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the\nbenefits of both methods. Using the Universal Myopic Mapping Theorem, we apply\na Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include\nmemory through a pre-trained AR component, which can be explained to retain the\nmost useful information while eliminating redundancy. Experimental data\nindicates that AR-KAN delivers superior results on $72\\%$ of real-world\ndatasets.", "AI": {"tldr": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u4fe1\u53f7\u9891\u8c31\u5206\u6790\u4e2d\u5e38\u9047\u6311\u6218\uff0cFourier\u795e\u7ecf\u7f51\u7edc\uff08FNNs\uff09\u5c1d\u8bd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u5ffd\u7565\u4e86\u975e\u5468\u671f\u6027\u4fe1\u53f7\u7684\u53e0\u52a0\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6a21\u578bAR-KAN\uff0c\u7ed3\u5408\u4e86KAN\u548cAR\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u572872%\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u6790\u5468\u671f\u6027\u6216\u4e0d\u5b8c\u5168\u5468\u671f\u6027\u4fe1\u53f7\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5f53\u4fe1\u53f7\u9891\u7387\u4e0d\u53ef\u516c\u5ea6\u65f6\u3002\u73b0\u6709\u7684FNNs\u7b49\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u975e\u5468\u671f\u6027\u4fe1\u53f7\u7684\u53e0\u52a0\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6a21\u578b\u6765\u5e94\u5bf9\u6b64\u7c7b\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAR-KAN\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u4e86Kolmogorov-Arnold Network\uff08KAN\uff09\u7684\u9759\u6001\u975e\u7ebf\u6027\u90e8\u5206\u548c\u9884\u8bad\u7ec3\u7684Autoregressive\uff08AR\uff09\u7ec4\u4ef6\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5229\u7528Universal Myopic Mapping Theorem\u6765\u4fdd\u7559\u6709\u7528\u4fe1\u606f\u5e76\u6d88\u9664\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAR-KAN\u572872%\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u548c\u5176\u4ed6\u6a21\u578b\uff08\u5982ARIMA\uff09\u3002", "conclusion": "AR-KAN\u901a\u8fc7\u7ed3\u5408KAN\u548cAR\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u4fe1\u53f7\u9891\u8c31\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u5468\u671f\u6027\u4fe1\u53f7\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.03222", "pdf": "https://arxiv.org/pdf/2509.03222", "abs": "https://arxiv.org/abs/2509.03222", "authors": ["Sophia Bianchi Moyen", "Rickmer Krohn", "Sophie Lueth", "Kay Pompetzki", "Jan Peters", "Vignesh Prasad", "Georgia Chalvatzaki"], "title": "The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation", "categories": ["cs.RO", "cs.HC", "cs.LG"], "comment": "8 pages, 8 figures, Accepted at the IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Intuitive Teleoperation interfaces are essential for mobile manipulation\nrobots to ensure high quality data collection while reducing operator workload.\nA strong sense of embodiment combined with minimal physical and cognitive\ndemands not only enhances the user experience during large-scale data\ncollection, but also helps maintain data quality over extended periods. This\nbecomes especially crucial for challenging long-horizon mobile manipulation\ntasks that require whole-body coordination. We compare two distinct robot\ncontrol paradigms: a coupled embodiment integrating arm manipulation and base\nnavigation functions, and a decoupled embodiment treating these systems as\nseparate control entities. Additionally, we evaluate two visual feedback\nmechanisms: immersive virtual reality and conventional screen-based\nvisualization of the robot's field of view. These configurations were\nsystematically assessed across a complex, multi-stage task sequence requiring\nintegrated planning and execution. Our results show that the use of VR as a\nfeedback modality increases task completion time, cognitive workload, and\nperceived effort of the teleoperator. Coupling manipulation and navigation\nleads to a comparable workload on the user as decoupling the embodiments, while\npreliminary experiments suggest that data acquired by coupled teleoperation\nleads to better imitation learning performance. Our holistic view on intuitive\nteleoperation interfaces provides valuable insight into collecting\nhigh-quality, high-dimensional mobile manipulation data at scale with the human\noperator in mind. Project\nwebsite:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u673a\u5668\u4eba\u63a7\u5236\u8303\u5f0f\uff08\u8026\u5408\u4e0e\u89e3\u8026\uff09\u53ca\u4e24\u79cd\u89c6\u89c9\u53cd\u9988\u673a\u5236\uff08VR\u4e0e\u5c4f\u5e55\uff09\uff0c\u53d1\u73b0VR\u589e\u52a0\u4efb\u52a1\u65f6\u95f4\u548c\u8ba4\u77e5\u8d1f\u8377\uff0c\u8026\u5408\u63a7\u5236\u867d\u4e0e\u89e3\u8026\u8d1f\u8377\u76f8\u5f53\uff0c\u4f46\u53ef\u80fd\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4f18\u5316\u79fb\u52a8\u64cd\u63a7\u673a\u5668\u4eba\u7684\u9065\u64cd\u4f5c\u754c\u9762\uff0c\u4ee5\u964d\u4f4e\u64cd\u4f5c\u5458\u8d1f\u62c5\u5e76\u63d0\u9ad8\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u8026\u5408\u4e0e\u89e3\u8026\u7684\u63a7\u5236\u8303\u5f0f\uff0c\u4ee5\u53caVR\u4e0e\u5c4f\u5e55\u53cd\u9988\u673a\u5236\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u7528\u6237\u4f53\u9a8c\u3002", "result": "VR\u53cd\u9988\u589e\u52a0\u4efb\u52a1\u65f6\u95f4\u4e0e\u8ba4\u77e5\u8d1f\u8377\uff1b\u8026\u5408\u63a7\u5236\u5728\u7528\u6237\u8d1f\u62c5\u4e0a\u7b49\u540c\u4e8e\u89e3\u8026\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u53ef\u80fd\u66f4\u4f18\uff0c\u9002\u5408\u6a21\u4eff\u5b66\u4e60\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u79fb\u52a8\u64cd\u63a7\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\uff0c\u5f3a\u8c03\u8026\u5408\u63a7\u5236\u4e0e\u7528\u6237\u53cb\u597d\u53cd\u9988\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.03231", "pdf": "https://arxiv.org/pdf/2509.03231", "abs": "https://arxiv.org/abs/2509.03231", "authors": ["Stephan Vonschallen", "Larissa Julia Corina Finsler", "Theresa Schmiedel", "Friederike Eyssel"], "title": "Exploring persuasive Interactions with generative social robots: An experimental framework", "categories": ["cs.RO"], "comment": "A shortened version of this paper was accepted as poster for the\n  Thirteenth International Conference on Human-Agent Interaction (HAI2025)", "summary": "Integrating generative AI such as large language models into social robots\nhas improved their ability to engage in natural, human-like communication. This\nstudy presents a method to examine their persuasive capabilities. We designed\nan experimental framework focused on decision making and tested it in a pilot\nthat varied robot appearance and self-knowledge. Using qualitative analysis, we\nevaluated interaction quality, persuasion effectiveness, and the robot's\ncommunicative strategies. Participants generally experienced the interaction\npositively, describing the robot as competent, friendly, and supportive, while\nnoting practical limits such as delayed responses and occasional\nspeech-recognition errors. Persuasiveness was highly context dependent and\nshaped by robot behavior: participants responded well to polite, reasoned\nsuggestions and expressive gestures, but emphasized the need for more\npersonalized, context-aware arguments and clearer social roles. These findings\nsuggest that generative social robots can influence user decisions, but their\neffectiveness depends on communicative nuance and contextual relevance. We\npropose refinements to the framework to further study persuasive dynamics\nbetween robots and human users.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6574\u5408\u751f\u6210\u5f0fAI\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5728\u8bf4\u670d\u4eba\u7c7b\u7528\u6237\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u6548\u679c\u53d7\u6c9f\u901a\u7ec6\u8282\u548c\u60c5\u5883\u76f8\u5173\u6027\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u751f\u6210\u5f0fAI\u589e\u5f3a\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5982\u4f55\u5728\u81ea\u7136\u4ea4\u6d41\u4e2d\u63d0\u5347\u8bf4\u670d\u529b\uff0c\u4ee5\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u4e00\u4e2a\u5b9e\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u53d8\u673a\u5668\u4eba\u5916\u89c2\u548c\u81ea\u6211\u8ba4\u77e5\u8bbe\u5b9a\uff0c\u8fdb\u884c\u8bd5\u70b9\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u5b9a\u6027\u5206\u6790\u8bc4\u4f30\u4ea4\u4e92\u8d28\u91cf\u548c\u8bf4\u670d\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u5bf9\u4ea4\u4e92\u4f53\u9a8c\u8bc4\u4ef7\u79ef\u6781\uff0c\u4f46\u8bf4\u670d\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u60c5\u5883\u548c\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u9700\u66f4\u4e2a\u6027\u5316\u548c\u60c5\u5883\u611f\u77e5\u7684\u6c9f\u901a\u7b56\u7565\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u751f\u6210\u5f0f\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u5f71\u54cd\u7528\u6237\u51b3\u7b56\uff0c\u4f46\u9700\u4f18\u5316\u6c9f\u901a\u7ec6\u8282\u548c\u60c5\u5883\u9002\u5e94\u80fd\u529b\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u8bf4\u670d\u52a8\u6001\u3002"}}
{"id": "2509.03238", "pdf": "https://arxiv.org/pdf/2509.03238", "abs": "https://arxiv.org/abs/2509.03238", "authors": ["Martin Goubej", "Lauria Clarke", "Martin Hraba\u010dka", "David Tolar"], "title": "Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "10 pages, 10 figures", "summary": "This paper presents a comprehensive refurbishment of the interactive robotic\nart installation Standards and Double Standards by Rafael Lozano-Hemmer. The\ninstallation features an array of belts suspended from the ceiling, each\nactuated by stepper motors and dynamically oriented by a vision-based tracking\nsystem that follows the movements of exhibition visitors. The original system\nwas limited by oscillatory dynamics, resulting in torsional and pendulum-like\nvibrations that constrained rotational speed and reduced interactive\nresponsiveness. To address these challenges, the refurbishment involved\nsignificant upgrades to both hardware and motion control algorithms. A detailed\nmathematical model of the flying belt system was developed to accurately\ncapture its dynamic behavior, providing a foundation for advanced control\ndesign. An input shaping method, formulated as a convex optimization problem,\nwas implemented to effectively suppress vibrations, enabling smoother and\nfaster belt movements. Experimental results demonstrate substantial\nimprovements in system performance and audience interaction. This work\nexemplifies the integration of robotics, control engineering, and interactive\nart, offering new solutions to technical challenges in real-time motion control\nand vibration damping for large-scale kinetic installations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5bf9\u4e92\u52a8\u673a\u5668\u4eba\u827a\u672f\u88c5\u7f6e\u7684\u7cfb\u7edf\u6027\u5347\u7ea7\uff0c\u901a\u8fc7\u6539\u8fdb\u786c\u4ef6\u548c\u8fd0\u52a8\u63a7\u5236\u7b97\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u88c5\u7f6e\u7684\u6027\u80fd\u548c\u4e92\u52a8\u6027\u3002", "motivation": "\u539f\u88c5\u7f6e\u7531\u4e8e\u632f\u8361\u52a8\u529b\u5b66\u95ee\u9898\uff08\u5982\u626d\u8f6c\u548c\u6446\u52a8\u632f\u52a8\uff09\uff0c\u9650\u5236\u4e86\u65cb\u8f6c\u901f\u5ea6\u548c\u4e92\u52a8\u54cd\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5347\u7ea7\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u8be6\u7ec6\u7684\u6570\u5b66\u6a21\u578b\u4ee5\u6355\u6349\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u91c7\u7528\u51f8\u4f18\u5316\u7684\u8f93\u5165\u6210\u5f62\u65b9\u6cd5\u6291\u5236\u632f\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u548c\u66f4\u5feb\u7684\u76ae\u5e26\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u6027\u80fd\u548c\u89c2\u4f17\u4e92\u52a8\u6027\u5747\u5f97\u5230\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u4eba\u6280\u672f\u3001\u63a7\u5236\u5de5\u7a0b\u4e0e\u4e92\u52a8\u827a\u672f\u7684\u878d\u5408\uff0c\u4e3a\u5927\u578b\u52a8\u6001\u88c5\u7f6e\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u65f6\u8fd0\u52a8\u63a7\u5236\u548c\u632f\u52a8\u6291\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03261", "pdf": "https://arxiv.org/pdf/2509.03261", "abs": "https://arxiv.org/abs/2509.03261", "authors": ["Elias Fontanari", "Gianni Lunardi", "Matteo Saveriano", "Andrea Del Prete"], "title": "Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Ensuring constraint satisfaction is a key requirement for safety-critical\nsystems, which include most robotic platforms. For example, constraints can be\nused for modeling joint position/velocity/torque limits and collision\navoidance. Constrained systems are often controlled using Model Predictive\nControl, because of its ability to naturally handle constraints, relying on\nnumerical optimization. However, ensuring constraint satisfaction is\nchallenging for nonlinear systems/constraints. A well-known tool to make\ncontrollers safe is the so-called control-invariant set (a.k.a. safe set). In\nour previous work, we have shown that safety can be improved by letting the\nsafe-set constraint recede along the MPC horizon. In this paper, we push that\nidea further by exploiting parallel computation to improve safety. We solve\nseveral MPC problems at the same time, where each problem instantiates the\nsafe-set constraint at a different time step along the horizon. Finally, the\ncontroller can select the best solution according to some user-defined\ncriteria. We validated this idea through extensive simulations with a 3-joint\nrobotic arm, showing that significant improvements can be achieved in terms of\nsafety and performance, even using as little as 4 computational cores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u4f18\u5316\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u4e2d\u5b89\u5168\u6027\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u7cfb\u7edf\u7ea6\u675f\u6ee1\u8db3\u7684\u6311\u6218\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\uff09\u4e2d\uff0c\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684MPC\u867d\u80fd\u5904\u7406\u7ea6\u675f\uff0c\u4f46\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u7ea6\u675f\u6ee1\u8db3\u4ecd\u5177\u6311\u6218\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u540c\u65f6\u6c42\u89e3\u591a\u4e2aMPC\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5728\u9884\u6d4b\u65f6\u57df\u7684\u4e0d\u540c\u65f6\u95f4\u6b65\u5b9e\u4f8b\u5316\u5b89\u5168\u96c6\u7ea6\u675f\uff0c\u6700\u7ec8\u6839\u636e\u7528\u6237\u5b9a\u4e49\u7684\u51c6\u5219\u9009\u62e9\u6700\u4f18\u89e3\u3002", "result": "\u901a\u8fc73\u5173\u8282\u673a\u5668\u4eba\u81c2\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u4ec5\u97004\u4e2a\u8ba1\u7b97\u6838\u5fc3\u5373\u53ef\u5b9e\u73b0\u3002", "conclusion": "\u5e76\u884c\u8ba1\u7b97\u662f\u63d0\u5347MPC\u5728\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u7ea6\u675f\u65f6\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2509.03436", "pdf": "https://arxiv.org/pdf/2509.03436", "abs": "https://arxiv.org/abs/2509.03436", "authors": ["Md Mhamud Hussen Sifat", "Md Maruf", "Md Rokunuzzaman"], "title": "Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "I.2.9; C.3; J.3"], "comment": "11 pages, 10 figures, 4 tables, 1 algorithm. Corresponding author: Md\n  Maruf (maruf.mte.17@gmail.com)", "summary": "The utilization of robotic technology has gained traction in healthcare\nfacilities due to progress in the field that enables time and cost savings,\nminimizes waste, and improves patient care. Digital healthcare technologies\nthat leverage automation, such as robotics and artificial intelligence, have\nthe potential to enhance the sustainability and profitability of healthcare\nsystems in the long run. However, the recent COVID-19 pandemic has amplified\nthe need for cyber-physical robots to automate check-ups and medication\nadministration. A robot nurse is controlled by the Internet of Things (IoT) and\ncan serve as an automated medical assistant while also allowing supervisory\ncontrol based on custom commands. This system helps reduce infection risk and\nimproves outcomes in pandemic settings. This research presents a test case with\na nurse robot that can assess a patient's health status and take action\naccordingly. We also evaluate the system's performance in medication\nadministration, health-status monitoring, and life-cycle considerations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u6280\u672f\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728COVID-19\u75ab\u60c5\u671f\u95f4\uff0c\u5229\u7528\u7269\u8054\u7f51\u63a7\u5236\u7684\u62a4\u58eb\u673a\u5668\u4eba\u53ef\u4ee5\u51cf\u5c11\u611f\u67d3\u98ce\u9669\u5e76\u6539\u5584\u533b\u7597\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u6280\u672f\u548c\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\u53ef\u4ee5\u8282\u7701\u65f6\u95f4\u548c\u6210\u672c\uff0c\u51cf\u5c11\u6d6a\u8d39\uff0c\u5e76\u63d0\u5347\u60a3\u8005\u62a4\u7406\u8d28\u91cf\u3002COVID-19\u75ab\u60c5\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86\u81ea\u52a8\u5316\u533b\u7597\u52a9\u624b\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u7269\u8054\u7f51\u63a7\u5236\u7684\u62a4\u58eb\u673a\u5668\u4eba\uff0c\u80fd\u591f\u8bc4\u4f30\u60a3\u8005\u5065\u5eb7\u72b6\u51b5\u5e76\u91c7\u53d6\u76f8\u5e94\u884c\u52a8\uff0c\u6d4b\u8bd5\u4e86\u5176\u5728\u836f\u7269\u7ba1\u7406\u3001\u5065\u5eb7\u76d1\u6d4b\u548c\u751f\u547d\u5468\u671f\u7ba1\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u62a4\u58eb\u673a\u5668\u4eba\u5728\u836f\u7269\u7ba1\u7406\u3001\u5065\u5eb7\u76d1\u6d4b\u548c\u751f\u547d\u5468\u671f\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u611f\u67d3\u98ce\u9669\u5e76\u63d0\u5347\u4e86\u533b\u7597\u6548\u7387\u3002", "conclusion": "\u673a\u5668\u4eba\u6280\u672f\u5728\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u75ab\u60c5\u7b49\u7279\u6b8a\u65f6\u671f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u533b\u7597\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u548c\u76c8\u5229\u80fd\u529b\u3002"}}
{"id": "2509.03500", "pdf": "https://arxiv.org/pdf/2509.03500", "abs": "https://arxiv.org/abs/2509.03500", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien"], "title": "Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena", "categories": ["cs.RO", "cs.AI"], "comment": "Appears in Proceedings of 18th Symposium on Advanced Space\n  Technologies in Robotics and Automation", "summary": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u52a8\u6001\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u81ea\u4e3b\u8f68\u8ff9\u89c4\u5212\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u5206\u8fa8\u7387\u4f20\u611f\u5668\u7684\u89c2\u6d4b\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u706b\u5c71\u533a\u7fbd\u6d41\u89c2\u6d4b\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u548c\u5148\u8fdb\u89c6\u89c9\u6280\u672f\uff0c\u6355\u6349\u7f55\u89c1\u3001\u77ac\u6001\u7684\u79d1\u5b66\u73b0\u8c61\uff0c\u5982\u706b\u5c71\u7fbd\u6d41\u3002", "method": "\u91c7\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\uff0c\u7ed3\u5408\u591a\u79cd\u8f68\u8ff9\u89c4\u5212\u7b97\u6cd5\u8ffd\u8e2a\u7fbd\u6d41\u7279\u5f81\u3002", "result": "\u4eff\u771f\u663e\u793a\u9ad8\u5206\u8fa8\u7387\u4eea\u5668\u7684\u6548\u7528\u63d0\u5347\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8fd0\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u79d1\u5b66\u73b0\u8c61\u7684\u89c2\u6d4b\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.03515", "pdf": "https://arxiv.org/pdf/2509.03515", "abs": "https://arxiv.org/abs/2509.03515", "authors": ["Yanlin Zhang", "Sungyong Chung", "Nachuan Li", "Dana Monzer", "Hani S. Mahmassani", "Samer H. Hamdar", "Alireza Talebpour"], "title": "Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "stat.AP"], "comment": null, "summary": "The Waymo Open Motion Dataset (WOMD) has become a popular resource for\ndata-driven modeling of autonomous vehicles (AVs) behavior. However, its\nvalidity for behavioral analysis remains uncertain due to proprietary\npost-processing, the absence of error quantification, and the segmentation of\ntrajectories into 20-second clips. This study examines whether WOMD accurately\ncaptures the dynamics and interactions observed in real-world AV operations.\nLeveraging an independently collected naturalistic dataset from Level 4 AV\noperations in Phoenix, Arizona (PHX), we perform comparative analyses across\nthree representative urban driving scenarios: discharging at signalized\nintersections, car-following, and lane-changing behaviors. For the discharging\nanalysis, headways are manually extracted from aerial video to ensure\nnegligible measurement error. For the car-following and lane-changing cases, we\napply the Simulation-Extrapolation (SIMEX) method to account for empirically\nestimated error in the PHX data and use Dynamic Time Warping (DTW) distances to\nquantify behavioral differences. Results across all scenarios consistently show\nthat behavior in PHX falls outside the behavioral envelope of WOMD. Notably,\nWOMD underrepresents short headways and abrupt decelerations. These findings\nsuggest that behavioral models calibrated solely on WOMD may systematically\nunderestimate the variability, risk, and complexity of naturalistic driving.\nCaution is therefore warranted when using WOMD for behavior modeling without\nproper validation against independently collected data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6(WOMD)\u5728\u771f\u5b9eAV\u64cd\u4f5c\u4e2d\u7684\u884c\u4e3a\u548c\u52a8\u6001\u6355\u6349\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u884c\u4e3a\u6a21\u578b\u4f4e\u4f30\u9a7e\u9a76\u7684\u53d8\u5f02\u6027\u3001\u98ce\u9669\u548c\u590d\u6742\u6027\u3002", "motivation": "\u8bc4\u4f30WOMD\u6570\u636e\u96c6\u5728\u771f\u5b9eAV\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4WOMD\u4e0e\u72ec\u7acb\u91c7\u96c6\u7684\u51e4\u51f0\u57ceAV\u6570\u636e\u96c6(PHX)\uff0c\u5206\u6790\u4e09\u79cd\u5178\u578b\u9a7e\u9a76\u573a\u666f\uff08\u4fe1\u53f7\u706f\u8def\u53e3\u3001\u8ddf\u8f66\u3001\u53d8\u9053\uff09\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cPHX\u7684\u884c\u4e3a\u660e\u663e\u8d85\u51faWOMD\u7684\u884c\u4e3a\u8303\u56f4\uff0cWOMD\u4f4e\u4f30\u4e86\u77ed\u8f66\u8ddd\u548c\u6025\u51cf\u901f\u884c\u4e3a\u3002", "conclusion": "\u4ec5\u4f9d\u8d56WOMD\u6821\u51c6\u884c\u4e3a\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5efa\u8bae\u4f7f\u7528\u524d\u9700\u72ec\u7acb\u6570\u636e\u9a8c\u8bc1\u3002"}}
{"id": "2508.19805", "pdf": "https://arxiv.org/pdf/2508.19805", "abs": "https://arxiv.org/abs/2508.19805", "authors": ["Shota Naito", "Tsukasa Ninomiya", "Koichi Wada"], "title": "Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers", "categories": ["cs.DC", "cs.RO"], "comment": null, "summary": "Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.02624", "pdf": "https://arxiv.org/pdf/2509.02624", "abs": "https://arxiv.org/abs/2509.02624", "authors": ["Minja Axelsson", "Jiaee Cheong", "Rune Nyrup", "Hatice Gunes"], "title": "Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.RO", "I.2.9; K.4.2; K.4.1"], "comment": "Accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society.\n  23 pages, 1 figure", "summary": "Recent studies indicate that robotic coaches can play a crucial role in\npromoting wellbeing. However, the real-world deployment of wellbeing robots\nraises numerous ethical and socio-technical questions and concerns. To explore\nthese questions, we undertake a community-centered investigation to examine\nthree different communities' perspectives on using robotic wellbeing coaches in\nreal-world environments. We frame our work as an anticipatory ethical\ninvestigation, which we undertake to better inform the development of robotic\ntechnologies with communities' opinions, with the ultimate goal of aligning\nrobot development with public interest. We conducted workshops with three\ncommunities who are under-represented in robotics development: 1) members of\nthe public at a science festival, 2) women computer scientists at a conference,\nand 3) humanities researchers interested in history and philosophy of science.\nIn the workshops, we collected qualitative data using the Social Robot\nCo-Design Canvas on Ethics. We analysed the collected qualitative data with\nThematic Analysis, informed by notes taken during workshops. Through our\nanalysis, we identify four themes regarding key ethical and socio-technical\nquestions about the real-world use of wellbeing robots. We group participants'\ninsights and discussions around these broad thematic questions, discuss them in\nlight of state-of-the-art literature, and highlight areas for future\ninvestigation. Finally, we provide the four questions as a broad framework that\nroboticists can and should use during robotic development and deployment, in\norder to reflect on the ethics and socio-technical dimensions of their robotic\napplications, and to engage in dialogue with communities of robot users. The\nfour questions are: 1) Is the robot safe and how can we know that?, 2) Who is\nthe robot built for and with?, 3) Who owns the robot and the data?, and 4) Why\na robot?.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u793e\u533a\u4e2d\u5fc3\u7684\u8c03\u67e5\uff0c\u63a2\u8ba8\u4e86\u4e09\u4e2a\u4e0d\u540c\u793e\u533a\u5bf9\u4f7f\u7528\u673a\u5668\u4eba\u5065\u5eb7\u6559\u7ec3\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u5173\u952e\u4f26\u7406\u548c\u793e\u4f1a\u6280\u672f\u95ee\u9898\uff0c\u4f9b\u673a\u5668\u4eba\u5f00\u53d1\u8005\u5728\u5f00\u53d1\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u53c2\u8003\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u5065\u5eb7\u6559\u7ec3\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e94\u7528\u7684\u4f26\u7406\u548c\u793e\u4f1a\u6280\u672f\u95ee\u9898\uff0c\u4ee5\u66f4\u597d\u5730\u5c06\u673a\u5668\u4eba\u6280\u672f\u4e0e\u516c\u5171\u5229\u76ca\u76f8\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u5de5\u4f5c\u574a\u6536\u96c6\u4e09\u4e2a\u793e\u533a\uff08\u516c\u4f17\u3001\u5973\u6027\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u3001\u4eba\u6587\u7814\u7a76\u8005\uff09\u7684\u5b9a\u6027\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u793e\u4f1a\u673a\u5668\u4eba\u534f\u540c\u8bbe\u8ba1\u4f26\u7406\u753b\u5e03\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bc6\u522b\u4e86\u56db\u4e2a\u5173\u952e\u4f26\u7406\u548c\u793e\u4f1a\u6280\u672f\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u4f9b\u673a\u5668\u4eba\u5f00\u53d1\u8005\u53c2\u8003\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u793e\u533a\u53c2\u4e0e\u548c\u4f26\u7406\u53cd\u601d\u5728\u673a\u5668\u4eba\u6280\u672f\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u3002"}}
{"id": "2509.02659", "pdf": "https://arxiv.org/pdf/2509.02659", "abs": "https://arxiv.org/abs/2509.02659", "authors": ["Zilong Guo", "Yi Luo", "Long Sha", "Dongxu Wang", "Panqu Wang", "Chenyang Xu", "Yi Yang"], "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model", "categories": ["cs.CV", "cs.RO"], "comment": "2nd place in CVPR 2024 End-to-End Driving at Scale Challenge", "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.", "AI": {"tldr": "\u7ed3\u5408\u7aef\u5230\u7aef\u67b6\u6784\u8bbe\u8ba1\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5355\u6444\u50cf\u5934\u9a71\u52a8\u7684\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7aef\u5230\u7aef\u9a7e\u9a76\u4efb\u52a1\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u5f3a\u5927\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u67b6\u6784\u8bbe\u8ba1\uff0c\u7ed3\u5408\u77e5\u8bc6\u4e30\u5bcc\u7684VLM\uff0c\u4ec5\u4f7f\u7528\u5355\u6444\u50cf\u5934\u8fdb\u884c\u9a7e\u9a76\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5355\u6444\u50cf\u5934\u89e3\u51b3\u65b9\u6848\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408VLM\u7684\u7aef\u5230\u7aef\u8bbe\u8ba1\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5355\u6444\u50cf\u5934\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.02922", "pdf": "https://arxiv.org/pdf/2509.02922", "abs": "https://arxiv.org/abs/2509.02922", "authors": ["Shahbaz P Qadri Syed", "He Bai"], "title": "Approximate constrained stochastic optimal control via parameterized input inference", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "Approximate methods to solve stochastic optimal control (SOC) problems have\nreceived significant interest from researchers in the past decade.\nProbabilistic inference approaches to SOC have been developed to solve\nnonlinear quadratic Gaussian problems. In this work, we propose an\nExpectation-Maximization (EM) based inference procedure to generate\nstate-feedback controls for constrained SOC problems. We consider the\ninequality constraints for the state and controls and also the structural\nconstraints for the controls. We employ barrier functions to address state and\ncontrol constraints. We show that the expectation step leads to smoothing of\nthe state-control pair while the the maximization step on the non-zero subsets\nof the control parameters allows inference of structured stochastic optimal\ncontrollers. We demonstrate the effectiveness of the algorithm on unicycle\nobstacle avoidance, four-unicycle formation control, and quadcopter navigation\nin windy environment examples. In these examples, we perform an empirical study\non the parametric effect of barrier functions on the state constraint\nsatisfaction. We also present a comparative study of smoothing algorithms on\nthe performance of the proposed approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eExpectation-Maximization\uff08EM\uff09\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e26\u6709\u72b6\u6001\u548c\u63a7\u5236\u7ea6\u675f\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5c4f\u969c\u51fd\u6570\u5904\u7406\u7ea6\u675f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u4e2a\u793a\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u673a\u6700\u4f18\u63a7\u5236\uff08SOC\uff09\u95ee\u9898\u7684\u8fd1\u4f3c\u65b9\u6cd5\u5728\u8fc7\u53bb\u5341\u5e74\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u975e\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u95ee\u9898\u4ecd\u9700\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u5f0f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u7ea6\u675f\u6761\u4ef6\u7684SOC\u65b9\u6cd5\u3002", "method": "\u91c7\u7528EM\u7b97\u6cd5\uff0c\u7ed3\u5408\u5c4f\u969c\u51fd\u6570\u5904\u7406\u72b6\u6001\u548c\u63a7\u5236\u7684\u7ea6\u675f\uff0c\u901a\u8fc7\u671f\u671b\u6b65\u9aa4\u5e73\u6ed1\u72b6\u6001-\u63a7\u5236\u5bf9\uff0c\u6700\u5927\u5316\u6b65\u9aa4\u63a8\u65ad\u7ed3\u6784\u5316\u968f\u673a\u6700\u4f18\u63a7\u5236\u5668\u3002", "result": "\u5728\u72ec\u8f6e\u8f66\u907f\u969c\u3001\u56db\u72ec\u8f6e\u8f66\u7f16\u961f\u63a7\u5236\u548c\u56db\u65cb\u7ffc\u5bfc\u822a\u7b49\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u5c4f\u969c\u51fd\u6570\u5bf9\u72b6\u6001\u7ea6\u675f\u6ee1\u8db3\u7684\u53c2\u6570\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684EM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5e26\u7ea6\u675f\u7684SOC\u95ee\u9898\uff0c\u5c4f\u969c\u51fd\u6570\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u7ea6\u675f\u7684\u6ee1\u8db3\u6548\u679c\u3002"}}
{"id": "2509.02930", "pdf": "https://arxiv.org/pdf/2509.02930", "abs": "https://arxiv.org/abs/2509.02930", "authors": ["Erik M. Lintunen"], "title": "VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "17 pages including appendices", "summary": "In self-supervised reinforcement learning (RL), one of the key challenges is\nlearning a diverse set of skills to prepare agents for unknown future tasks.\nDespite impressive advances, scalability and evaluation remain prevalent\nissues. Regarding scalability, the search for meaningful skills can be obscured\nby high-dimensional feature spaces, where relevant features may vary across\ndownstream task domains. For evaluating skill diversity, defining what\nconstitutes \"diversity\" typically requires a hard commitment to a specific\nnotion of what it means for skills to be diverse, potentially leading to\ninconsistencies in how skill diversity is understood, making results across\ndifferent approaches hard to compare, and leaving many forms of diversity\nunexplored. To address these issues, we adopt a measure of sample diversity\nthat translates ideas from ecology to machine learning -- the Vendi Score --\nallowing the user to specify and evaluate any desired form of diversity. We\ndemonstrate how this metric facilitates skill evaluation and introduce VendiRL,\na unified framework for learning diversely diverse sets of skills. Given\ndistinct similarity functions, VendiRL motivates distinct forms of diversity,\nwhich could support skill-diversity pretraining in new and richly interactive\nenvironments where optimising for various forms of diversity may be desirable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6001\u5b66Vendi Score\u7684\u591a\u6837\u6027\u8bc4\u4f30\u65b9\u6cd5VendiRL\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u76d1\u7763RL\u4e2d\u6280\u80fd\u591a\u6837\u6027\u7684\u5b9a\u4e49\u548c\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u76d1\u7763RL\u4e2d\u6280\u80fd\u591a\u6837\u6027\u7684\u5b9a\u4e49\u548c\u8bc4\u4f30\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u6bd4\u8f83\u548c\u591a\u6837\u6027\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Vendi Score\u4f5c\u4e3a\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u63d0\u51fa\u4e86VendiRL\u6846\u67b6\uff0c\u652f\u6301\u901a\u8fc7\u4e0d\u540c\u76f8\u4f3c\u6027\u51fd\u6570\u5b9a\u4e49\u548c\u4f18\u5316\u591a\u6837\u6280\u80fd\u3002", "result": "VendiRL\u80fd\u591f\u7075\u6d3b\u5b9a\u4e49\u548c\u8bc4\u4f30\u591a\u6837\u6027\uff0c\u5e76\u5728\u591a\u79cd\u4ea4\u4e92\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "VendiRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6280\u80fd\u591a\u6837\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2509.03030", "pdf": "https://arxiv.org/pdf/2509.03030", "abs": "https://arxiv.org/abs/2509.03030", "authors": ["Zida Wu", "Mathieu Lauriere", "Matthieu Geist", "Olivier Pietquin", "Ankur Mehta"], "title": "Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning", "categories": ["cs.LG", "cs.MA", "cs.RO", "cs.SY", "eess.SY"], "comment": "2025 IEEE 64rd Conference on Decision and Control (CDC)", "summary": "Mean Field Games (MFGs) offer a powerful framework for studying large-scale\nmulti-agent systems. Yet, learning Nash equilibria in MFGs remains a\nchallenging problem, particularly when the initial distribution is unknown or\nwhen the population is subject to common noise. In this paper, we introduce an\nefficient deep reinforcement learning (DRL) algorithm designed to achieve\npopulation-dependent Nash equilibria without relying on averaging or historical\nsampling, inspired by Munchausen RL and Online Mirror Descent. The resulting\npolicy is adaptable to various initial distributions and sources of common\nnoise. Through numerical experiments on seven canonical examples, we\ndemonstrate that our algorithm exhibits superior convergence properties\ncompared to state-of-the-art algorithms, particularly a DRL version of\nFictitious Play for population-dependent policies. The performance in the\npresence of common noise underscores the robustness and adaptability of our\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u4f9d\u8d56\u5386\u53f2\u91c7\u6837\u6216\u5e73\u5747\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4eba\u7fa4\u4f9d\u8d56\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u7814\u7a76MFGs\u4e2d\u5b66\u4e60\u7eb3\u4ec0\u5747\u8861\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u521d\u59cb\u5206\u5e03\u672a\u77e5\u6216\u5b58\u5728\u5171\u540c\u566a\u58f0\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408Munchausen RL\u548c\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578bDRL\u7b97\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5386\u53f2\u91c7\u6837\u3002", "result": "\u7b97\u6cd5\u5728\u4e03\u9879\u6807\u51c6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6536\u655b\u6027\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u5b58\u5728\u5171\u540c\u566a\u58f0\u65f6\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\uff0c\u4e3aMFGs\u4e2d\u7684\u7eb3\u4ec0\u5747\u8861\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.03140", "pdf": "https://arxiv.org/pdf/2509.03140", "abs": "https://arxiv.org/abs/2509.03140", "authors": ["Nadezhda Dobreva", "Emmanuel Blazquez", "Jai Grover", "Dario Izzo", "Yuzhen Qin", "Dominik Dold"], "title": "Decentralised self-organisation of pivoting cube ensembles using geometric deep learning", "categories": ["cs.NE", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a decentralized model for autonomous reconfiguration of\nhomogeneous pivoting cube modular robots in two dimensions. Each cube in the\nensemble is controlled by a neural network that only gains information from\nother cubes in its local neighborhood, trained using reinforcement learning.\nFurthermore, using geometric deep learning, we include the grid symmetries of\nthe cube ensemble in the neural network architecture. We find that even the\nmost localized versions succeed in reconfiguring to the target shape, although\nreconfiguration happens faster the more information about the whole ensemble is\navailable to individual cubes. Near-optimal reconfiguration is achieved with\nonly nearest neighbor interactions by using multiple information passing\nbetween cubes, allowing them to accumulate more global information about the\nensemble. Compared to standard neural network architectures, using geometric\ndeep learning approaches provided only minor benefits. Overall, we successfully\ndemonstrate mostly local control of a modular self-assembling system, which is\ntransferable to other space-relevant systems with different action spaces, such\nas sliding cube modular robots and CubeSat swarms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u4e8c\u7ef4\u540c\u8d28\u65cb\u8f6c\u7acb\u65b9\u4f53\u6a21\u5757\u673a\u5668\u4eba\u81ea\u4e3b\u91cd\u6784\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\uff0c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u4ec5\u5e26\u6765\u5fae\u5c0f\u4f18\u52bf\uff0c\u5c40\u90e8\u4e92\u52a8\u53ef\u5b9e\u73b0\u8fd1\u6700\u4f18\u91cd\u6784\u3002", "motivation": "\u7814\u7a76\u6a21\u5757\u81ea\u7ec4\u88c5\u7cfb\u7edf\u7684\u5c40\u90e8\u63a7\u5236\u65b9\u6cd5\uff0c\u63a2\u7d22\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc\u7684\u7acb\u65b9\u4f53\u6a21\u5757\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u8003\u8651\u7f51\u683c\u5bf9\u79f0\u6027\u3002", "result": "\u5373\u4f7f\u662f\u5c40\u90e8\u7248\u672c\u4e5f\u80fd\u6210\u529f\u91cd\u6784\u76ee\u6807\u5f62\u72b6\uff0c\u8fd1\u6700\u4f18\u91cd\u6784\u901a\u8fc7\u90bb\u57df\u591a\u6b21\u4fe1\u606f\u4f20\u9012\u5b9e\u73b0\u3002", "conclusion": "\u5c40\u90e8\u63a7\u5236\u65b9\u6cd5\u6210\u529f\u4e14\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u7a7a\u95f4\u7cfb\u7edf\uff0c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2509.03270", "pdf": "https://arxiv.org/pdf/2509.03270", "abs": "https://arxiv.org/abs/2509.03270", "authors": ["Martin Skoglund", "Fredrik Warg", "Aria Mirzai", "Anders Thorsen", "Karl Lundgren", "Peter Folkesson", "Bastian Havers-zulka"], "title": "AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation", "categories": ["cs.SE", "cs.RO"], "comment": "12 pages, 9 figures, EVS38,\n  https://evs38-program.org/en/evs-38-proceedings/all", "summary": "Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)\nintroduces unique challenges for safety assurance, particularly within the\nframework of ISO 26262, which governs functional safety in the automotive\ndomain. Traditional assessment methodologies are not geared toward evaluating\nAI-based functions and require evolving standards and practices. This paper\nexplores how an independent assessment of an AI component in an EV can be\nachieved when combining ISO 26262 with the recently released ISO/PAS 8800,\nwhose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)\nbattery estimation exemplifies the process. Key features relevant to the\nindependent assessment of this extended evaluation approach are identified. As\npart of the evaluation, robustness testing of the AI component is conducted\nusing fault injection experiments, wherein perturbed sensor inputs are\nsystematically introduced to assess the component's resilience to input\nvariance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408ISO 26262\u548cISO/PAS 8800\u6807\u51c6\u5bf9\u7535\u52a8\u6c7d\u8f66\u4e2d\u7684AI\u529f\u80fd\u8fdb\u884c\u72ec\u7acb\u8bc4\u4f30\uff0c\u5e76\u4ee5AI\u9a71\u52a8\u7684\u7535\u6c60\u72b6\u6001\u4f30\u8ba1\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u57fa\u4e8eAI\u7684\u529f\u80fd\uff0c\u9700\u8981\u65b0\u7684\u6807\u51c6\u548c\u65b9\u6cd5\u6765\u786e\u4fddAI\u5728\u7535\u52a8\u6c7d\u8f66\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u7ed3\u5408ISO 26262\u548cISO/PAS 8800\u6807\u51c6\uff0c\u5bf9AI\u7ec4\u4ef6\u8fdb\u884c\u72ec\u7acb\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\u6d4b\u8bd5\u5176\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86AI\u7ec4\u4ef6\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u73b0\u6709\u6807\u51c6\u53ef\u4ee5\u4e3aAI\u5728\u7535\u52a8\u6c7d\u8f66\u4e2d\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684AI\u6280\u672f\u3002"}}
{"id": "2509.03381", "pdf": "https://arxiv.org/pdf/2509.03381", "abs": "https://arxiv.org/abs/2509.03381", "authors": ["Sanghoon Lee", "Junha Kang", "Kyung-Joon Park"], "title": "Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle Tutorial to Static Verification", "categories": ["cs.NI", "cs.RO"], "comment": "14 pages, 4 figures", "summary": "Robot Operating System 2 (ROS 2) relies on the Data Distribution Service\n(DDS), which offers more than 20 Quality of Service (QoS) policies governing\navailability, reliability, and resource usage. Yet ROS 2 users lack clear\nguidance on safe policy combinations and validation processes prior to\ndeployment, which often leads to trial-and-error tuning and unexpected runtime\nfailures. To address these challenges, we analyze DDS Publisher-Subscriber\ncommunication over a life cycle divided into Discovery, Data Exchange, and\nDisassociation, and provide a user oriented tutorial explaining how 16 QoS\npolicies operate in each phase. Building on this analysis, we derive a QoS\ndependency chain that formalizes inter-policy relationships and classifies 41\ndependency violation rules, capturing constraints that commonly cause\ncommunication failures in practice. Finally, we introduce QoS Guard, a ROS 2\npackage that statically validates DDS XML profiles offline, flags conflicts,\nand enables safe, predeployment tuning without establishing a live ROS 2\nsession. Together, these contributions give ROS 2 users both conceptual insight\nand a concrete tool that enables early detection of misconfigurations,\nimproving the reliability and resource efficiency of ROS 2 based robotic\nsystems.", "AI": {"tldr": "ROS 2\u7528\u6237\u7f3a\u4e4f\u5173\u4e8eDDS QoS\u653f\u7b56\u7684\u660e\u786e\u6307\u5bfc\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u95ee\u9898\u3002\u7814\u7a76\u5206\u6790\u4e86DDS\u901a\u4fe1\u751f\u547d\u5468\u671f\uff0c\u63d0\u51faQoS\u4f9d\u8d56\u94fe\uff0c\u5e76\u5f00\u53d1\u4e86QoS Guard\u5de5\u5177\uff0c\u7528\u4e8e\u79bb\u7ebf\u9a8c\u8bc1\u914d\u7f6e\u6587\u4ef6\u3002", "motivation": "ROS 2\u4f9d\u8d56DDS\u7684QoS\u653f\u7b56\uff0c\u4f46\u7f3a\u4e4f\u5b89\u5168\u7ec4\u5408\u7684\u6307\u5bfc\u548c\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u5931\u8d25\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u5206\u6790DDS\u901a\u4fe1\u751f\u547d\u5468\u671f\uff08\u53d1\u73b0\u3001\u6570\u636e\u4ea4\u6362\u3001\u89e3\u9664\u5173\u8054\uff09\uff0c\u63d0\u51faQoS\u4f9d\u8d56\u94fe\uff0c\u5e76\u5f00\u53d1\u9759\u6001\u9a8c\u8bc1\u5de5\u5177QoS Guard\u3002", "result": "41\u6761\u4f9d\u8d56\u8fdd\u89c4\u89c4\u5219\u88ab\u5206\u7c7b\uff0cQoS Guard\u5de5\u5177\u80fd\u79bb\u7ebf\u68c0\u6d4b\u914d\u7f6e\u51b2\u7a81\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86ROS 2\u7528\u6237\u6982\u5ff5\u6027\u6307\u5bfc\u548c\u5b89\u5168\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2509.03383", "pdf": "https://arxiv.org/pdf/2509.03383", "abs": "https://arxiv.org/abs/2509.03383", "authors": ["Yiyang Huang", "Zixuan Wang", "Zishen Wan", "Yapeng Tian", "Haobo Xu", "Yinhe Han", "Yiming Gan"], "title": "ANNIE: Be Careful of Your Robots", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "The integration of vision-language-action (VLA) models into embodied AI (EAI)\nrobots is rapidly advancing their ability to perform complex, long-horizon\ntasks in humancentric environments. However, EAI systems introduce critical\nsecurity risks: a compromised VLA model can directly translate adversarial\nperturbations on sensory input into unsafe physical actions. Traditional safety\ndefinitions and methodologies from the machine learning community are no longer\nsufficient. EAI systems raise new questions, such as what constitutes safety,\nhow to measure it, and how to design effective attack and defense mechanisms in\nphysically grounded, interactive settings. In this work, we present the first\nsystematic study of adversarial safety attacks on embodied AI systems, grounded\nin ISO standards for human-robot interactions. We (1) formalize a principled\ntaxonomy of safety violations (critical, dangerous, risky) based on physical\nconstraints such as separation distance, velocity, and collision boundaries;\n(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with\n2,400 video-action sequences for evaluating embodied safety; and (3)\nANNIE-Attack, a task-aware adversarial framework with an attack leader model\nthat decomposes long-horizon goals into frame-level perturbations. Our\nevaluation across representative EAI models shows attack success rates\nexceeding 50% across all safety categories. We further demonstrate sparse and\nadaptive attack strategies and validate the real-world impact through physical\nrobot experiments. These results expose a previously underexplored but highly\nconsequential attack surface in embodied AI systems, highlighting the urgent\nneed for security-driven defenses in the physical AI era. Code is available at\nhttps://github.com/RLCLab/Annie.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u5b89\u5168\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u5b89\u5168\u8fdd\u89c4\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u8bc4\u4f30\u5de5\u5177ANNIEBench\u548c\u653b\u51fb\u6846\u67b6ANNIE-Attack\uff0c\u63ed\u793a\u4e86\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u7684\u65b0\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\uff0c\u5c06\u5bf9\u6297\u6027\u6270\u52a8\u8f6c\u5316\u4e3a\u4e0d\u5b89\u5168\u7684\u7269\u7406\u884c\u4e3a\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u5b89\u5168\u65b9\u6cd5\u4e0d\u9002\u7528\uff0c\u4e9f\u9700\u65b0\u7684\u5b89\u5168\u7814\u7a76\u3002", "method": "\u8bba\u6587\u901a\u8fc7ISO\u4eba\u673a\u4ea4\u4e92\u6807\u51c6\uff0c\u5206\u7c7b\u5b89\u5168\u8fdd\u89c4\u7b49\u7ea7\uff08\u5173\u952e\u3001\u5371\u9669\u3001\u98ce\u9669\uff09\uff0c\u8bbe\u8ba1ANNIEBench\u57fa\u51c6\uff082,400\u4e2a\u89c6\u9891\u52a8\u4f5c\u5e8f\u5217\uff09\u548cANNIE-Attack\u653b\u51fb\u6846\u67b6\uff0c\u8bc4\u4f30\u5e76\u653b\u51fb\u4ee3\u8868\u6027EAI\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc750%\uff0c\u7a00\u758f\u548c\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u6709\u6548\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u73b0\u5b9e\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u66b4\u9732\u4e86\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u4e2d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4f46\u5173\u952e\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u7269\u7406AI\u65f6\u4ee3\u5b89\u5168\u9632\u5fa1\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2509.03430", "pdf": "https://arxiv.org/pdf/2509.03430", "abs": "https://arxiv.org/abs/2509.03430", "authors": ["Vimal Mollyn", "Nathan DeVrio", "Chris Harrison"], "title": "EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting", "categories": ["cs.HC", "cs.CV", "cs.GR", "cs.RO"], "comment": "Accepted to UIST 2025", "summary": "The ability to detect touch events on uninstrumented, everyday surfaces has\nbeen a long-standing goal for mixed reality systems. Prior work has shown that\nvirtual interfaces bound to physical surfaces offer performance and ergonomic\nbenefits over tapping at interfaces floating in the air. A wide variety of\napproaches have been previously developed, to which we contribute a new\nheadset-integrated technique called \\systemname. We use a combination of a\ncomputer-triggered camera and one or more infrared emitters to create\nstructured shadows, from which we can accurately estimate hover distance (mean\nerror of 6.9~mm) and touch contact (98.0\\% accuracy). We discuss how our\ntechnique works across a range of conditions, including surface material,\ninteraction orientation, and environmental lighting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5934\u6234\u8bbe\u5907\u96c6\u6210\u6280\u672f\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89e6\u53d1\u7684\u6444\u50cf\u5934\u548c\u7ea2\u5916\u53d1\u5c04\u5668\u521b\u5efa\u7ed3\u6784\u9634\u5f71\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65e5\u5e38\u7269\u4f53\u8868\u9762\u7684\u9ad8\u7cbe\u5ea6\u89e6\u6478\u4e8b\u4ef6\u68c0\u6d4b\u3002", "motivation": "\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\u957f\u671f\u4ee5\u6765\u81f4\u529b\u4e8e\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u8bbe\u5907\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u65e5\u5e38\u7269\u4f53\u8868\u9762\u7684\u89e6\u6478\u4e8b\u4ef6\uff0c\u865a\u62df\u754c\u9762\u7ed1\u5b9a\u7269\u7406\u8868\u9762\u7684\u6027\u80fd\u548c\u4eba\u673a\u5de5\u7a0b\u5b66\u4f18\u52bf\u662f\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u5934\u6234\u8bbe\u5907\u96c6\u6210\u7684\u6280\u672f\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89e6\u53d1\u7684\u6444\u50cf\u5934\u548c\u7ea2\u5916\u53d1\u5c04\u5668\u751f\u6210\u7ed3\u6784\u9634\u5f71\uff0c\u7528\u4e8e\u4f30\u8ba1\u60ac\u505c\u8ddd\u79bb\u548c\u89e6\u6478\u63a5\u89e6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u60ac\u505c\u8ddd\u79bb\u4f30\u8ba1\u4e0a\u7684\u5e73\u5747\u8bef\u5dee\u4e3a6.9\u6beb\u7c73\uff0c\u89e6\u6478\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523098%\uff0c\u4e14\u5728\u4e0d\u540c\u8868\u9762\u6750\u6599\u3001\u4ea4\u4e92\u65b9\u5411\u548c\u73af\u5883\u5149\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u8be5\u6280\u672f\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u5747\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u4e3a\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u89e6\u6478\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03451", "pdf": "https://arxiv.org/pdf/2509.03451", "abs": "https://arxiv.org/abs/2509.03451", "authors": ["Nathan DeVrio", "Vimal Mollyn", "Chris Harrison"], "title": "SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data", "categories": ["cs.HC", "cs.CV", "cs.GR", "cs.RO"], "comment": "The first two listed authors contributed equally. Published at UIST\n  2023", "summary": "The ability to track a user's arm pose could be valuable in a wide range of\napplications, including fitness, rehabilitation, augmented reality input, life\nlogging, and context-aware assistants. Unfortunately, this capability is not\nreadily available to consumers. Systems either require cameras, which carry\nprivacy issues, or utilize multiple worn IMUs or markers. In this work, we\ndescribe how an off-the-shelf smartphone and smartwatch can work together to\naccurately estimate arm pose. Moving beyond prior work, we take advantage of\nmore recent ultra-wideband (UWB) functionality on these devices to capture\nabsolute distance between the two devices. This measurement is the perfect\ncomplement to inertial data, which is relative and suffers from drift. We\nquantify the performance of our software-only approach using off-the-shelf\ndevices, showing it can estimate the wrist and elbow joints with a \\hl{median\npositional error of 11.0~cm}, without the user having to provide training data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u901a\u8fc7\u8d85\u5bbd\u5e26\uff08UWB\uff09\u548c\u60ef\u6027\u6570\u636e\u7ed3\u5408\u6765\u4f30\u8ba1\u624b\u81c2\u59ff\u52bf\u7684\u8f6f\u4ef6\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u9690\u79c1\u95ee\u9898\u548c\u591a\u8bbe\u5907\u7a7f\u6234\u7684\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u81c2\u59ff\u52bf\u8ffd\u8e2a\u7cfb\u7edf\u4f9d\u8d56\u6444\u50cf\u5934\uff08\u9690\u79c1\u95ee\u9898\uff09\u6216\u591aIMU\u8bbe\u5907\uff08\u7a7f\u6234\u590d\u6742\uff09\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u666e\u901a\u6d88\u8d39\u8bbe\u5907\u5b9e\u73b0\u9ad8\u6548\u8ffd\u8e2a\u3002", "method": "\u7ed3\u5408\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u7684\u8d85\u5bbd\u5e26\uff08UWB\uff09\u6d4b\u8ddd\u529f\u80fd\u548c\u60ef\u6027\u6570\u636e\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u4f30\u8ba1\u624b\u8155\u548c\u8098\u90e8\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684\u624b\u8155\u548c\u8098\u90e8\u4f4d\u7f6e\u4f30\u8ba1\u4e2d\u4f4d\u8bef\u5dee\u4e3a11.0\u5398\u7c73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d88\u8d39\u7ea7\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u53cb\u597d\u7684\u624b\u81c2\u59ff\u52bf\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03462", "pdf": "https://arxiv.org/pdf/2509.03462", "abs": "https://arxiv.org/abs/2509.03462", "authors": ["Zhuo Cao", "Yunxiao Shi", "Min Xu"], "title": "sam-llm: interpretable lane change trajectoryprediction via parametric finetuning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "5 pages", "summary": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.", "AI": {"tldr": "SAM-LLM\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u5b66\u8f66\u9053\u53d8\u6362\u6a21\u578b\u7684\u7269\u7406\u7cbe\u786e\u6027\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u65b9\u6cd5\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8f66\u9053\u53d8\u6362\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f66\u9053\u53d8\u6362\u8f68\u8ff9\u9884\u6d4b\u7684\u7269\u7406\u7cbe\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u8f93\u51fa\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5fae\u8c03LLM\uff0c\u4f7f\u5176\u8f93\u51fa\u8f68\u8ff9\u6a21\u578b\u7684\u6838\u5fc3\u7269\u7406\u53c2\u6570\u800c\u975e\u539f\u59cb\u5750\u6807\uff0c\u91c7\u7528\u589e\u5f3a\u7684\u6b63\u5f26\u52a0\u901f\u5ea6\u6a21\u578b\uff08SAM\uff09\u751f\u6210\u53c2\u6570\u3002", "result": "SAM-LLM\u5c06\u8f93\u51fa\u5927\u5c0f\u51cf\u5c1180%\uff0c\u610f\u56fe\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523098.73%\uff0c\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edfLLM\u76f8\u540c\u7684\u6027\u80fd\uff0c\u4f46\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "SAM-LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u7269\u7406\u7cbe\u786e\u7684\u8f66\u9053\u53d8\u6362\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
