{"id": "2508.18712", "pdf": "https://arxiv.org/pdf/2508.18712", "abs": "https://arxiv.org/abs/2508.18712", "authors": ["Samin Yaser", "Mahad Ali", "Laura J. Brattain", "Yang Jiang", "VP Nguyen", "Jing Xiang"], "title": "A Synoptic Review of High-Frequency Oscillations as a Biomarker in Neurodegenerative Disease", "categories": ["eess.SP"], "comment": null, "summary": "High Frequency Oscillations (HFOs), rapid bursts of brain activity above 80\nHz, have emerged as a highly specific biomarker for epileptogenic tissue.\nRecent evidence suggests that HFOs are also present in Alzheimer's Disease\n(AD), reflecting underlying network hyperexcitability and offering a promising,\nnoninvasive tool for early diagnosis and disease tracking. This synoptic review\nprovides a comprehensive analysis of publicly available electroencephalography\n(EEG) datasets relevant to HFO research in neurodegenerative disorders. We\nconducted a bibliometric analysis of 1,222 articles, revealing a significant\nand growing research interest in HFOs, particularly within the last ten years.\nWe then systematically profile and compare key public datasets, evaluating\ntheir participant cohorts, data acquisition parameters, and accessibility, with\na specific focus on their technical suitability for HFO analysis. Our\ncomparative synthesis highlights critical methodological heterogeneity across\ndatasets, particularly in sampling frequency and recording paradigms, which\nposes challenges for cross-study validation, but also offers opportunities for\nrobustness testing. By consolidating disparate information, clarifying\nnomenclature, and providing a detailed methodological framework, this review\nserves as a guide for researchers aiming to leverage public data to advance the\nrole of HFOs as a cross-disease biomarker for AD and related conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u9ad8\u9891\u632f\u8361\uff08HFOs\uff09\u4f5c\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u751f\u7269\u6807\u5fd7\u7269\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5206\u6790\u4e86\u516c\u5f00EEG\u6570\u636e\u96c6\uff0c\u5e76\u6307\u51fa\u4e86\u65b9\u6cd5\u5b66\u5dee\u5f02\u5bf9\u7814\u7a76\u7684\u5f71\u54cd\u3002", "motivation": "\u9ad8\u9891\u632f\u8361\uff08HFOs\uff09\u88ab\u8ba4\u4e3a\u5177\u6709\u6f5c\u529b\u6210\u4e3a\u975e\u4fb5\u5165\u6027\u7684\u65e9\u671f\u8bca\u65ad\u5de5\u5177\uff0c\u5c24\u5176\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b49\u9886\u57df\u3002\u7136\u800c\uff0c\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u5b66\u5f02\u8d28\u6027\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5bf91,222\u7bc7\u6587\u732e\u7684\u6587\u732e\u8ba1\u91cf\u5206\u6790\uff0c\u4ee5\u53ca\u5bf9\u516c\u5f00EEG\u6570\u636e\u96c6\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u6280\u672f\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0HFOs\u7814\u7a76\u5174\u8da3\u663e\u8457\u589e\u957f\uff0c\u4f46\u6570\u636e\u96c6\u5728\u91c7\u6837\u9891\u7387\u548c\u8bb0\u5f55\u8303\u5f0f\u4e0a\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u5f71\u54cd\u8de8\u7814\u7a76\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5229\u7528\u516c\u5f00\u6570\u636e\u63a8\u52a8HFOs\u4f5c\u4e3a\u8de8\u75be\u75c5\u751f\u7269\u6807\u5fd7\u7269\u7684\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u65b9\u6cd5\u5b66\u6807\u51c6\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.18735", "pdf": "https://arxiv.org/pdf/2508.18735", "abs": "https://arxiv.org/abs/2508.18735", "authors": ["Afan Ali", "Irfanullah Khan"], "title": "SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus", "categories": ["eess.SP", "cs.AI"], "comment": "6 pages, 7 figures", "summary": "Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as\nbase stations are extremely susceptible to security attacks due to their\ndistributed and dynamic nature, which makes them vulnerable to rogue nodes. In\nthis paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware\nConsensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The\nproposed framework integrates a permissioned Hyperledger Fabric blockchain with\nFederated Learning (FL) to support privacy-preserving trust evaluation. Trust\nratings are updated continuously through weighted aggregation of past trust,\npresent behavior, and energy contribution, thus making the system adaptive to\nchanging network conditions. An energy-aware consensus mechanism prioritizes\nUAVs with greater available energy for block validation, ensuring efficient use\nof resources under resource-constrained environments. FL aggregation with\ntrust-weighting further increases the resilience of the global trust model.\nSimulation results verify the designed framework achieves 94\\% trust score\nprediction accuracy and 96\\% rogue UAV detection rate while outperforming\ncentralized and static baselines of trust-based solutions on privacy, energy\nefficiency, and reliability. It complies with 6G requirements in terms of\ndistributed intelligence and sustainability and is an energy-efficient and\nscalable solution to secure NTNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u533a\u5757\u94fe\u548c\u8054\u90a6\u5b66\u4e60\u7684\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u8c03\u6574\u673a\u5236DTSAM-EAC\uff0c\u4ee5\u63d0\u5347\u65e0\u4eba\u673a\u975e\u5730\u9762\u7f51\u7edc\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u975e\u5730\u9762\u7f51\u7edc\u56e0\u5176\u5206\u5e03\u548c\u52a8\u6001\u7279\u6027\u6613\u53d7\u653b\u51fb\uff0c\u9700\u8981\u5b89\u5168\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u80fd\u6e90\u9ad8\u6548\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "\u6574\u5408Hyperledger Fabric\u533a\u5757\u94fe\u4e0e\u8054\u90a6\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u548c\u80fd\u6e90\u611f\u77e5\u5171\u8bc6\u673a\u5236\u589e\u5f3a\u5b89\u5168\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u6846\u67b6\u5728\u4fe1\u4efb\u8bc4\u5206\u9884\u6d4b\u51c6\u786e\u7387\u548c\u6076\u610f\u65e0\u4eba\u673a\u68c0\u6d4b\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "DTSAM-EAC\u6ee1\u8db36G\u5bf9\u5206\u5e03\u5f0f\u667a\u80fd\u548c\u53ef\u6301\u7eed\u6027\u7684\u9700\u6c42\uff0c\u662f\u80fd\u6e90\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18810", "pdf": "https://arxiv.org/pdf/2508.18810", "abs": "https://arxiv.org/abs/2508.18810", "authors": ["Yonghwi Kim", "Sang-Hyun Park", "Siyun Yang", "Kai-Kit Wong", "Linglong Dai", "Chan-Byoung Chae"], "title": "Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and System Insights", "categories": ["eess.SP"], "comment": "7 pages, 6 figures", "summary": "The shift toward sixth-generation (6G) wireless networks places integrated\nsensing and communications (ISAC) at the core of future applications such as\nautonomous driving, extended reality, and smart manufacturing. However, the\ncombination of large antenna arrays and ultra-wide bandwidths brings near-field\npropagation effects and beam squint to the forefront, fundamentally challenging\ntraditional far-field designs. True time delay units (TTDs) offer a potential\nsolution, but their cost and hardware complexity limit scalability. In this\narticle, we present practical beamforming strategies for near-field\nultra-wideband ISAC systems. We explore codebook designs across analog and\ndigital domains that mitigate beam squint, ensure reliable user coverage, and\nenhance sensing accuracy. We further validate these approaches through\nlarge-scale system-level simulations, including 3D map-based evaluations that\nreflect real-world urban environments. Our results demonstrate how carefully\ndesigned beamforming can balance communication throughput with sensing\nperformance, achieving reliable coverage and efficient resource use even under\nsevere near-field conditions. We conclude by highlighting open challenges in\nhardware, algorithms, and system integration, pointing toward research\ndirections that will shape the deployment of 6G-ready ISAC networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e866G\u7f51\u7edc\u4e2d\u96c6\u6210\u4ea4\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u8fd1\u573a\u8d85\u5bbd\u5e26\u6ce2\u675f\u6210\u5f62\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8fdc\u573a\u8bbe\u8ba1\u5728\u8fd1\u573a\u4f20\u64ad\u548c\u6ce2\u675f\u659c\u89c6\u4e2d\u7684\u5c40\u9650\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u77406G\u65e0\u7ebf\u7f51\u7edc\u7684\u53d1\u5c55\uff0cISAC\u6210\u4e3a\u672a\u6765\u5e94\u7528\u7684\u6838\u5fc3\uff0c\u4f46\u8fd1\u573a\u4f20\u64ad\u6548\u5e94\u548c\u6ce2\u675f\u659c\u89c6\u5bf9\u4f20\u7edf\u8bbe\u8ba1\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u9488\u5bf9\u8fd1\u573a\u8d85\u5bbd\u5e26ISAC\u7cfb\u7edf\u7684\u6ce2\u675f\u6210\u5f62\u7b56\u7565\uff0c\u63a2\u7d22\u4e86\u6a21\u62df\u548c\u6570\u5b57\u9886\u57df\u7684\u7801\u672c\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u7cfb\u7edf\u7ea7\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce2\u675f\u6210\u5f62\u53ef\u4ee5\u5728\u8fd1\u573a\u6761\u4ef6\u4e0b\u5e73\u8861\u901a\u4fe1\u541e\u5410\u91cf\u548c\u4f20\u611f\u6027\u80fd\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u8986\u76d6\u548c\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86\u786c\u4ef6\u3001\u7b97\u6cd5\u548c\u7cfb\u7edf\u96c6\u6210\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e866G-ready ISAC\u7f51\u7edc\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.18854", "pdf": "https://arxiv.org/pdf/2508.18854", "abs": "https://arxiv.org/abs/2508.18854", "authors": ["Ruifeng Dong", "Ming Wang", "Ning Liu", "Tong Guo", "Jiayi Kang", "Xiaojing Shen", "Yao Mao"], "title": "DIFNet: Decentralized Information Filtering Fusion Neural Network with Unknown Correlation in Sensor Measurement Noises", "categories": ["eess.SP"], "comment": null, "summary": "In recent years, decentralized sensor networks have garnered significant\nattention in the field of state estimation owing to enhanced robustness,\nscalability, and fault tolerance. Optimal fusion performance can be achieved\nunder fully connected communication and known noise correlation structures. To\nmitigate communication overhead, the global state estimation problem is\ndecomposed into local subproblems through structured observation model. This\nensures that even when the communication network is not fully connected, each\nsensor can achieve locally optimal estimates of its observable state\ncomponents. To address the degradation of fusion accuracy induced by unknown\ncorrelations in measurement noise, this paper proposes a data-driven method,\ntermed Decentralized Information Filter Neural Network (DIFNet), to learn\nunknown noise correlations in data for discrete-time nonlinear state space\nmodels with cross-correlated measurement noises. Numerical simulations\ndemonstrate that DIFNet achieves superior fusion performance compared to\nconventional filtering methods and exhibits robust characteristics in more\ncomplex scenarios, such as the presence of time-varying noise. The source code\nused in our numerical experiment can be found online at\nhttps://wisdom-estimation.github.io/DIFNet_Demonstrate/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIFNet\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u6563\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u672a\u77e5\u6d4b\u91cf\u566a\u58f0\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u878d\u5408\u6027\u80fd\u3002", "motivation": "\u5728\u5206\u6563\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\uff0c\u6d4b\u91cf\u566a\u58f0\u7684\u672a\u77e5\u76f8\u5173\u6027\u4f1a\u5f71\u54cd\u878d\u5408\u7cbe\u5ea6\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b66\u4e60\u548c\u9002\u5e94\u8fd9\u4e9b\u672a\u77e5\u76f8\u5173\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u89c2\u6d4b\u6a21\u578b\u5c06\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u5206\u89e3\u4e3a\u5c40\u90e8\u5b50\u95ee\u9898\uff0c\u5e76\u4f7f\u7528DIFNet\u5b66\u4e60\u672a\u77e5\u566a\u58f0\u76f8\u5173\u6027\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0cDIFNet\u5728\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "DIFNet\u4e3a\u5206\u6563\u4f20\u611f\u5668\u7f51\u7edc\u7684\u566a\u58f0\u76f8\u5173\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18397", "pdf": "https://arxiv.org/pdf/2508.18397", "abs": "https://arxiv.org/abs/2508.18397", "authors": ["Antonio Guillen-Perez"], "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83\u516d\u79cd\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u4e2d\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8106\u5f31\u4e14\u4e0d\u5b89\u5168\uff0c\u9700\u901a\u8fc7\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u4f18\u5316\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u7c7b\u578b\uff08\u542f\u53d1\u5f0f\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u884c\u4e3a\u57fa\uff09\u7684\u516d\u79cd\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e03\u79cdCQL\u667a\u80fd\u4f53\uff0c\u5e76\u5728Waymax\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\u3002", "result": "\u6240\u6709\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u5c06\u78b0\u649e\u7387\u4ece16.0%\u964d\u81f35.5%\uff0c\u4e14\u65f6\u6b65\u7ea7\u4e0e\u573a\u666f\u7ea7\u6743\u91cd\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "\u667a\u80fd\u975e\u5747\u5300\u91c7\u6837\u662f\u6784\u5efa\u5b89\u5168\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u7684\u5173\u952e\uff0c\u8bba\u6587\u4e3a\u79bb\u7ebfRL\u6570\u636e\u7b5b\u9009\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002"}}
{"id": "2508.19000", "pdf": "https://arxiv.org/pdf/2508.19000", "abs": "https://arxiv.org/abs/2508.19000", "authors": ["Atso Iivanainen", "Robin Rajam\u00e4ki", "Visa Koivunen"], "title": "Beyond-Diagonal RIS: Adversarial Channels and Optimality of Low-Complexity Architectures", "categories": ["eess.SP"], "comment": "\\copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) have recently\ngained attention as an enhancement to conventional RISs. BD-RISs allow\noptimizing not only the phase, but also the amplitude responses of their\ndiscrete surface elements by introducing adjustable inter-element couplings.\nVarious BD-RIS architectures have been proposed to optimally trade off between\naverage performance and complexity of the architecture. However, little\nattention has been paid to worst-case performance. This paper characterizes\nnovel sets of adversarial channels for which certain low-complexity BD-RIS\narchitectures have suboptimal performance in terms of received signal power at\nan intended communications user. Specifically, we consider two recent BD-RIS\nmodels: the so-called group-connected and tree-connected architecture. The\nderived adversarial channel sets reveal new surprising connections between the\ntwo architectures. We validate our analytical results numerically,\ndemonstrating that adversarial channels can cause a significant performance\nloss. Our results pave the way towards efficient BD-RIS designs that are robust\nto adversarial propagation conditions and malicious attacks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u2018\u5bf9\u89d2\u7ebf\u5916\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u2019(BD-RIS)\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u67d0\u4e9b\u4f4e\u590d\u6742\u5ea6\u67b6\u6784\u5728\u5bf9\u6297\u6027\u4fe1\u9053\u4e0b\u7684\u6027\u80fd\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684RIS\u4e3b\u8981\u4f18\u5316\u76f8\u4f4d\u54cd\u5e94\uff0c\u800cBD-RIS\u8fdb\u4e00\u6b65\u4f18\u5316\u5e45\u5ea6\u54cd\u5e94\u5e76\u901a\u8fc7\u53ef\u8c03\u8026\u5408\u63d0\u5347\u6027\u80fd\u3002\u7136\u800c\uff0c\u5176\u6700\u574f\u60c5\u51b5\u6027\u80fd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6BD-RIS\u67b6\u6784\uff08\u7ec4\u8fde\u63a5\u548c\u6811\u8fde\u63a5\uff09\u5728\u5bf9\u6297\u6027\u4fe1\u9053\u4e0b\u7684\u6027\u80fd\uff0c\u63a8\u5bfc\u5176\u5bf9\u6297\u6027\u4fe1\u9053\u96c6\u3002", "result": "\u5bf9\u6297\u6027\u4fe1\u9053\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u63ed\u793a\u4e24\u79cd\u67b6\u6784\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9c81\u68d2BD-RIS\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u6297\u6027\u4f20\u64ad\u6761\u4ef6\u548c\u6076\u610f\u653b\u51fb\u3002"}}
{"id": "2508.18399", "pdf": "https://arxiv.org/pdf/2508.18399", "abs": "https://arxiv.org/abs/2508.18399", "authors": ["Christian Friedrich", "Ralf Gulde", "Armin Lechler", "Alexander Verl"], "title": "Maintenance automation: methods for robotics manipulation planning and execution", "categories": ["cs.RO"], "comment": "11 pages, 12 figures", "summary": "Automating complex tasks using robotic systems requires skills for planning,\ncontrol and execution. This paper proposes a complete robotic system for\nmaintenance automation, which can automate disassembly and assembly operations\nunder environmental uncertainties (e.g. deviations between prior plan\ninformation). The cognition of the robotic system is based on a planning\napproach (using CAD and RGBD data) and includes a method to interpret a\nsymbolic plan and transform it to a set of executable robot instructions. The\ncomplete system is experimentally evaluated using real-world applications. This\nwork shows the first step to transfer these theoretical results into a\npractical robotic solution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7ef4\u62a4\u4efb\u52a1\uff0c\u5305\u62ec\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u62c6\u5378\u548c\u7ec4\u88c5\u64cd\u4f5c\u3002\u7cfb\u7edf\u57fa\u4e8e\u89c4\u5212\u65b9\u6cd5\uff08\u4f7f\u7528CAD\u548cRGBD\u6570\u636e\uff09\uff0c\u5e76\u5c06\u7b26\u53f7\u8ba1\u5212\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u6307\u4ee4\u3002\u901a\u8fc7\u771f\u5b9e\u5e94\u7528\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u7406\u8bba\u6210\u679c\u5411\u5b9e\u9645\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u7684\u521d\u6b65\u8f6c\u5316\u3002", "motivation": "\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u9700\u8981\u89e3\u51b3\u89c4\u5212\u3001\u63a7\u5236\u548c\u6267\u884c\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b8c\u6210\u7ef4\u62a4\u4efb\u52a1\uff0c\u63a8\u52a8\u7406\u8bba\u6210\u679c\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408CAD\u548cRGBD\u6570\u636e\u8fdb\u884c\u89c4\u5212\uff0c\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u5c06\u7b26\u53f7\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u6709\u6548\u5b8c\u6210\u81ea\u52a8\u5316\u7ef4\u62a4\u4efb\u52a1\uff0c\u5305\u62ec\u62c6\u5378\u548c\u7ec4\u88c5\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u7684\u521d\u6b65\u6210\u529f\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u52a8\u5316\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.19010", "pdf": "https://arxiv.org/pdf/2508.19010", "abs": "https://arxiv.org/abs/2508.19010", "authors": ["Poorya Mollahosseini", "Yasaman Ghasempour"], "title": "mmKey: Channel-Aware Beam Shaping for Reliable Key Generation in mmWave Wireless Networks", "categories": ["eess.SP", "cs.CR"], "comment": null, "summary": "Physical-layer key generation (PLKG) has emerged as a promising technique to\nsecure next-generation wireless networks by exploiting the inherent properties\nof the wireless channel. However, PLKG faces fundamental challenges in the\nmillimeter wave (mmWave) regime due to channel sparsity, higher phase noise,\nand higher path loss, which undermine both the randomness and reciprocity\nrequired for secure key generation. In this paper, we present mmKey, a novel\nPLKG framework that capitalizes on the availability of multiple antennas at\nmmWave wireless nodes to inject randomness into an otherwise quasi-static\nwireless channel. Different from prior works that sacrifice either the secrecy\nof the key generation or the robustness, mmKey balances these two requirements.\nIn particular, mmKey leverages a genetic algorithm to gradually evolve the\ninitial weight vector population toward configurations that suppress the LOS\ncomponent while taking into account the channel conditions, specifically, the\nsparsity and the signal-to-noise ratio (SNR). Extensive simulations show that\nmmKey improves the secrecy gap by an average of 39.4% over random beamforming\nand 34.0% over null beamforming, outperforming conventional schemes.", "AI": {"tldr": "mmKey\u662f\u4e00\u4e2a\u65b0\u7684\u7269\u7406\u5c42\u5bc6\u94a5\u751f\u6210\uff08PLKG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6beb\u7c73\u6ce2\uff08mmWave\uff09\u65e0\u7ebf\u7f51\u7edc\u4e2d\u56e0\u4fe1\u9053\u7a00\u758f\u6027\u3001\u76f8\u4f4d\u566a\u58f0\u548c\u8def\u5f84\u635f\u8017\u5bfc\u81f4\u7684\u5bc6\u94a5\u751f\u6210\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5929\u7ebf\u6ce8\u5165\u968f\u673a\u6027\uff0c\u5e76\u5229\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u3002", "motivation": "\u6beb\u7c73\u6ce2\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u4fe1\u9053\u7a00\u758f\u3001\u76f8\u4f4d\u566a\u58f0\u9ad8\u548c\u8def\u5f84\u635f\u8017\u5927\uff0c\u4f20\u7edfPLKG\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u5bc6\u94a5\u751f\u6210\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u9700\u6c42\u3002", "method": "mmKey\u5229\u7528\u591a\u5929\u7ebf\u5728\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u6ce8\u5165\u968f\u673a\u6027\uff0c\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u9010\u6b65\u4f18\u5316\u521d\u59cb\u6743\u91cd\u5411\u91cf\uff0c\u6291\u5236\u76f4\u89c6\u8def\u5f84\uff08LOS\uff09\u5e76\u8003\u8651\u4fe1\u9053\u7a00\u758f\u6027\u548c\u4fe1\u566a\u6bd4\uff08SNR\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793ammKey\u5728\u4fdd\u5bc6\u6027\u65b9\u9762\u6bd4\u968f\u673a\u6ce2\u675f\u5f62\u6210\u548c\u96f6\u6ce2\u675f\u5f62\u6210\u5206\u522b\u63d0\u9ad8\u4e8639.4%\u548c34.0%\u3002", "conclusion": "mmKey\u5728\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u5bc6\u94a5\u751f\u6210\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2508.18400", "pdf": "https://arxiv.org/pdf/2508.18400", "abs": "https://arxiv.org/abs/2508.18400", "authors": ["Christian Friedrich", "Akos Csiszar", "Armin Lechler", "Alexander Verl"], "title": "Efficient task and path planning for maintenance automation using a robot system", "categories": ["cs.RO"], "comment": "10 pages, 10 figures", "summary": "The research and development of intelligent automation solutions is a\nground-breaking point for the factory of the future. A promising and\nchallenging mission is the use of autonomous robot systems to automate tasks in\nthe field of maintenance. For this purpose, the robot system must be able to\nplan autonomously the different manipulation tasks and the corresponding paths.\nBasic requirements are the development of algorithms with a low computational\ncomplexity and the possibility to deal with environmental uncertainties. In\nthis work, an approach is presented, which is especially suited to solve the\nproblem of maintenance automation. For this purpose, offline data from CAD is\ncombined with online data from an RGBD vision system via a probabilistic\nfilter, to compensate uncertainties from offline data. For planning the\ndifferent tasks, a method is explained, which use a symbolic description,\nfounded on a novel sampling-based method to compute the disassembly space. For\npath planning we use global state-of-the art algorithms with a method that\nallows the adaption of the exploration stepsize in order to reduce the planning\ntime. Every method is experimentally validated and discussed.", "AI": {"tldr": "\u667a\u80fd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u7684\u7814\u53d1\u662f\u672a\u6765\u5de5\u5382\u7684\u5173\u952e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebfCAD\u6570\u636e\u548c\u5728\u7ebfRGBD\u89c6\u89c9\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ef4\u62a4\u4efb\u52a1\u81ea\u52a8\u5316\u3002", "motivation": "\u5229\u7528\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u7ef4\u62a4\u4efb\u52a1\u81ea\u52a8\u5316\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u76ee\u6807\uff0c\u9700\u8981\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7b97\u6cd5\u548c\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u79bb\u7ebfCAD\u6570\u636e\u548c\u5728\u7ebfRGBD\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u6982\u7387\u6ee4\u6ce2\u8865\u507f\u4e0d\u786e\u5b9a\u6027\uff1b\u5229\u7528\u7b26\u53f7\u63cf\u8ff0\u548c\u91c7\u6837\u65b9\u6cd5\u8ba1\u7b97\u62c6\u5378\u7a7a\u95f4\uff1b\u91c7\u7528\u5168\u5c40\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u641c\u7d22\u6b65\u957f\u4ee5\u51cf\u5c11\u89c4\u5212\u65f6\u95f4\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5747\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7ef4\u62a4\u4efb\u52a1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\u4e0e\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2508.19034", "pdf": "https://arxiv.org/pdf/2508.19034", "abs": "https://arxiv.org/abs/2508.19034", "authors": ["Poorya Mollahosseini", "Yasaman Ghasempour"], "title": "Fast Vortex Beam Alignment for OAM Mode Multiplexing in LOS MIMO Networks", "categories": ["eess.SP"], "comment": "13 pages, 12 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Orbital Angular Momentum (OAM)-based communication systems offer\nhigh-capacity multiplexing in line-of-sight (LOS) scenarios; yet, their\nperformance is sensitive to nodal misalignment, which disrupts modal\northogonality, hindering the data multiplexing gain. To tackle this challenge,\nwe present OrthoVortex, a novel framework that estimates the misalignment\nangles and applies the appropriate phase correction to restore orthogonality\nbetween modes. Unlike purely theoretical prior efforts that rely on impractical\nfully digital arrays or exhaustive beam scans, OrthoVortex introduces and\nleverages the cross-modal phase, as a unique signature for identifying the\nmisalignment angles. OrthoVortex is a few-shot alignment technique, making it\nfeasible for real-world implementations. Our key contributions include: (i) a\nrobust angle estimation and phase correction framework based on the physics of\nOAM propagation that estimates the misalignment and restores modal\northogonality, (ii) the first-ever experimental validation of OAM beam\nalignment with RF transceivers, and (iii) a comprehensive analysis of practical\nconstraints, including the impact of antenna count and bandwidth. Simulations\nand over-the-air measurements using low-cost, rapidly prototyped metasurfaces\noperating at 120 GHz demonstrate that OrthoVortex achieves fast and precise\nmisalignment estimation (mean absolute error of $0.69^{\\circ}$ for azimuth and\n$2.54^{\\circ}$ for elevation angle). Further, OrthoVortex can mitigate the\ninter-modal interference, yielding more than 12 dB increase in\nsignal-to-interference ratio and more than 4.5-fold improvement in link\ncapacity.", "AI": {"tldr": "OrthoVortex \u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u548c\u6821\u6b63\u57fa\u4e8e\u8f68\u9053\u89d2\u52a8\u91cf\uff08OAM\uff09\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8282\u70b9\u9519\u4f4d\uff0c\u4ee5\u6062\u590d\u6a21\u6001\u6b63\u4ea4\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6570\u636e\u590d\u7528\u589e\u76ca\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "OAM \u901a\u4fe1\u7cfb\u7edf\u5728\u89c6\u8ddd\uff08LOS\uff09\u573a\u666f\u4e2d\u5177\u6709\u9ad8\u5bb9\u91cf\u590d\u7528\u80fd\u529b\uff0c\u4f46\u5bf9\u8282\u70b9\u9519\u4f4d\u654f\u611f\uff0c\u9700\u8981\u6709\u6548\u7684\u9519\u4f4d\u4f30\u8ba1\u548c\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86 OrthoVortex \u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6a21\u6001\u76f8\u4f4d\u4f5c\u4e3a\u72ec\u7279\u7b7e\u540d\u6765\u8bc6\u522b\u9519\u4f4d\u89d2\u5ea6\uff0c\u5e76\u5e94\u7528\u76f8\u4f4d\u6821\u6b63\u6062\u590d\u6a21\u6001\u6b63\u4ea4\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u5c11\u6837\u672c\u5bf9\u9f50\u6280\u672f\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5728 120 GHz \u4e0b\u7684\u6a21\u62df\u548c\u5b9e\u6d4b\u4e2d\uff0cOrthoVortex \u5b9e\u73b0\u4e86\u5feb\u901f\u7cbe\u786e\u7684\u9519\u4f4d\u4f30\u8ba1\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a\u65b9\u4f4d\u89d2 0.69\u00b0 \u548c\u4ef0\u89d2 2.54\u00b0\uff09\uff0c\u663e\u8457\u6291\u5236\u4e86\u6a21\u6001\u95f4\u5e72\u6270\u3002", "conclusion": "OrthoVortex \u80fd\u591f\u9ad8\u6548\u6821\u6b63\u8282\u70b9\u9519\u4f4d\uff0c\u63d0\u5347\u4fe1\u53f7\u5e72\u6270\u6bd4\u548c\u94fe\u8def\u5bb9\u91cf\uff0c\u4e3a OAM \u901a\u4fe1\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18443", "pdf": "https://arxiv.org/pdf/2508.18443", "abs": "https://arxiv.org/abs/2508.18443", "authors": ["Ruohan Zhang", "Uksang Yoo", "Yichen Li", "Arpit Argawal", "Wenzhen Yuan"], "title": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing", "categories": ["cs.RO"], "comment": "16 pages, 12 figures, International Journal of Robotics Research\n  (accepted), 2025", "summary": "Soft pneumatic robot manipulators are popular in industrial and\nhuman-interactive applications due to their compliance and flexibility.\nHowever, deploying them in real-world scenarios requires advanced sensing for\ntactile feedback and proprioception. Our work presents a novel vision-based\napproach for sensorizing soft robots. We demonstrate our approach on\nPneuGelSight, a pioneering pneumatic manipulator featuring high-resolution\nproprioception and tactile sensing via an embedded camera. To optimize the\nsensor's performance, we introduce a comprehensive pipeline that accurately\nsimulates its optical and dynamic properties, facilitating a zero-shot\nknowledge transition from simulation to real-world applications. PneuGelSight\nand our sim-to-real pipeline provide a novel, easily implementable, and robust\nsensing methodology for soft robots, paving the way for the development of more\nadvanced soft robots with enhanced sensory capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89c6\u89c9\u9a71\u52a8\u7684\u8f6f\u673a\u5668\u4eba\u4f20\u611f\u65b9\u6cd5PneuGelSight\uff0c\u5e76\u7ed3\u5408\u4eff\u771f\u4f18\u5316\u5176\u6027\u80fd\uff0c\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u8f6f\u6c14\u52a8\u673a\u5668\u4eba\u56e0\u5176\u67d4\u6027\u548c\u7075\u6d3b\u6027\u5728\u5de5\u4e1a\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u9700\u8981\u9ad8\u7ea7\u7684\u89e6\u89c9\u53cd\u9988\u548c\u672c\u4f53\u611f\u77e5\u4f20\u611f\u6280\u672f\u3002", "method": "\u8bbe\u8ba1\u4e86PneuGelSight\u8f6f\u6c14\u52a8\u673a\u68b0\u624b\uff0c\u5229\u7528\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u672c\u4f53\u611f\u77e5\u548c\u89e6\u89c9\u4f20\u611f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u4eff\u771f\u7ba1\u9053\u4f18\u5316\u4f20\u611f\u5668\u6027\u80fd\u3002", "result": "PneuGelSight\u53ca\u5176\u4eff\u771f\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u5b9e\u73b0\u4e14\u9c81\u68d2\u7684\u8f6f\u673a\u5668\u4eba\u4f20\u611f\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8f6f\u673a\u5668\u4eba\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u9ad8\u7ea7\u611f\u5b98\u80fd\u529b\u7684\u8f6f\u673a\u5668\u4eba\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.19129", "pdf": "https://arxiv.org/pdf/2508.19129", "abs": "https://arxiv.org/abs/2508.19129", "authors": ["Tayfun Yilmaz", "Haci Ilhan", "Ibrahim Hokelek"], "title": "Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection Models: Error Rate Analysis and Negative Moment-Based Optimization with Saddle Point Approximation", "categories": ["eess.SP"], "comment": "This work has been submitted for consideration in an IEEE journal", "summary": "RIS-assisted communication has recently attracted significant attention for\nenhancing wireless performance in challenging environments, making accurate\nerror analysis under practical hardware constraints crucial for future\nmulti-antenna systems. This paper presents a theoretical framework for SER\nanalysis of RIS-assisted multiple antenna systems employing OSTBC under\npractical reflection models with amplitude-dependent and quantized phase\nresponses. By exploiting the Gramian structure of the cascaded channel f, we\nderive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS\nsizes. For large-scale RIS deployments, where closed-form analysis becomes\nintractable, we employ Saddle Point Approximation to approximate the eigenvalue\ndistribution. Using these results, we derive unified SER expressions using\nexact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase\nconfiguration, and both identical and non-identical amplitude responses.\nExtensive Monte Carlo simulations confirm the accuracy of the proposed SER\nexpressions, demonstrating very close agreement for all configurations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790RIS\u8f85\u52a9\u591a\u5929\u7ebf\u7cfb\u7edf\u5728\u5b9e\u7528\u53cd\u5c04\u6a21\u578b\u4e0b\u7684\u7b26\u53f7\u9519\u8bef\u7387\uff08SER\uff09\uff0c\u901a\u8fc7Gramian\u7ed3\u6784\u548cSaddle Point\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610fRIS\u89c4\u6a21\u548c\u76f8\u4f4d\u914d\u7f6e\u3002", "motivation": "\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u5728\u5b9e\u9645\u786c\u4ef6\u7ea6\u675f\u4e0bRIS\u8f85\u52a9\u7cfb\u7edf\u7684\u7cbe\u786e\u9519\u8bef\u5206\u6790\u3002", "method": "\u5229\u7528Gramian\u7ed3\u6784\u63a8\u5bfc\u5c0f\u89c4\u6a21RIS\u7684\u7cbe\u786eMGF\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7Saddle Point\u8fd1\u4f3c\u5904\u7406\u5927\u89c4\u6a21RIS\u7684\u5206\u5e03\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684SER\u8868\u8fbe\u5f0f\uff0c\u9002\u7528\u4e8e\u4e0d\u540cRIS\u89c4\u6a21\u548c\u914d\u7f6e\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u4e3aRIS\u8f85\u52a9\u591a\u5929\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684SER\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u591a\u5929\u7ebf\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2508.18460", "pdf": "https://arxiv.org/pdf/2508.18460", "abs": "https://arxiv.org/abs/2508.18460", "authors": ["Tianze Liu", "Md Abu Bakr Siddique", "Hongyu An"], "title": "Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable\nprowess across various cognitive tasks using extensive training data. However,\nthe reliance on large datasets and neural networks presents challenges such as\nhighpower consumption and limited adaptability, particularly in\nSWaP-constrained applications like planetary exploration. To address these\nissues, we propose enhancing the autonomous capabilities of intelligent robots\nby emulating the associative learning observed in animals. Associative learning\nenables animals to adapt to their environment by memorizing concurrent events.\nBy replicating this mechanism, neuromorphic robots can navigate dynamic\nenvironments autonomously, learning from interactions to optimize performance.\nThis paper explores the emulation of associative learning in rodents using\nneuromorphic robots within open-field maze environments, leveraging insights\nfrom spatial cells such as place and grid cells. By integrating these models,\nwe aim to enable online associative learning for spatial tasks in real-time\nscenarios, bridging the gap between biological spatial cognition and robotics\nfor advancements in autonomous systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u52a8\u7269\u7684\u8054\u60f3\u5b66\u4e60\u80fd\u529b\uff0c\u63d0\u5347\u667a\u80fd\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u9002\u5e94\u80fd\u529b\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5927\u6570\u636e\u548c\u9ad8\u529f\u8017\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edfAI\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u548c\u9ad8\u529f\u8017\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff08\u5982\u884c\u661f\u63a2\u7d22\uff09\u4e2d\u7684\u5e94\u7528\u3002\u6a21\u62df\u52a8\u7269\u7684\u8054\u60f3\u5b66\u4e60\u80fd\u529b\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u6a21\u62df\u556e\u9f7f\u7c7b\u52a8\u7269\u5728\u5f00\u653e\u8ff7\u5bab\u73af\u5883\u4e2d\u7684\u8054\u60f3\u5b66\u4e60\u673a\u5236\uff0c\u7ed3\u5408\u7a7a\u95f4\u7ec6\u80de\uff08\u5982\u4f4d\u7f6e\u7ec6\u80de\u548c\u7f51\u683c\u7ec6\u80de\uff09\u7684\u6a21\u578b\u3002", "result": "\u76ee\u6807\u662f\u901a\u8fc7\u5728\u7ebf\u8054\u60f3\u5b66\u4e60\u5b9e\u73b0\u5b9e\u65f6\u7a7a\u95f4\u4efb\u52a1\u5904\u7406\uff0c\u7f29\u5c0f\u751f\u7269\u7a7a\u95f4\u8ba4\u77e5\u4e0e\u673a\u5668\u4eba\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u6a21\u62df\u751f\u7269\u8054\u60f3\u5b66\u4e60\u673a\u5236\u4e3a\u795e\u7ecf\u5f62\u6001\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.19185", "pdf": "https://arxiv.org/pdf/2508.19185", "abs": "https://arxiv.org/abs/2508.19185", "authors": ["Nishant Mehrotra", "Sandesh Rao Mattu", "Robert Calderbank"], "title": "Instantaneous Polarimetry with Zak-OTFS", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "8 pages, 4 figures, submitted to IEEE Transactions on Radar Systems\n  (Correspondence)", "summary": "Polarimetry, which is the ability to measure the scattering response of the\nenvironment across orthogonal polarizations, is fundamental to enhancing\nwireless communication and radar system performance. In this paper, we utilize\nthe Zak-OTFS modulation to enable instantaneous polarimetry within a single\ntransmission frame. We transmit a Zak-OTFS carrier waveform and a spread\ncarrier waveform mutually unbiased to it simultaneously over orthogonal\npolarizations. The mutual unbiasedness of the two waveforms enables the\nreceiver to estimate the full polarimetric response of the scattering\nenvironment from a single received frame. Unlike existing methods for\ninstantaneous polarimetry with computational complexity quadratic in the\ntime-bandwidth product, the proposed method enables instantaneous polarimetry\nat complexity that is only sublinear in the time-bandwidth product. Via\nnumerical simulations, we show ideal polarimetric target detection and\nparameter estimation results with the proposed method, with improvements in\nperformance and computational complexity over comparable baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Zak-OTFS\u8c03\u5236\u5b9e\u73b0\u77ac\u65f6\u504f\u632f\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u504f\u632f\u6d4b\u91cf\u5bf9\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u548c\u96f7\u8fbe\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u540c\u65f6\u4f20\u8f93Zak-OTFS\u8f7d\u6ce2\u6ce2\u5f62\u548c\u4e0e\u5176\u76f8\u4e92\u65e0\u504f\u7684\u6269\u5c55\u8f7d\u6ce2\u6ce2\u5f62\uff0c\u63a5\u6536\u7aef\u53ef\u4ee5\u4ece\u5355\u4e2a\u5e27\u4e2d\u4f30\u8ba1\u6563\u5c04\u73af\u5883\u7684\u5168\u504f\u632f\u54cd\u5e94\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u632f\u76ee\u6807\u68c0\u6d4b\u548c\u53c2\u6570\u4f30\u8ba1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u4ec5\u4e3a\u65f6\u95f4\u5e26\u5bbd\u4e58\u79ef\u4e9a\u7ebf\u6027\u7684\u77ac\u65f6\u504f\u632f\u6d4b\u91cf\uff0c\u6027\u80fd\u548c\u6548\u7387\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.18606", "pdf": "https://arxiv.org/pdf/2508.18606", "abs": "https://arxiv.org/abs/2508.18606", "authors": ["Nicky Zimmerman", "Joel Loo", "Ayush Agrawal", "David Hsu"], "title": "SignLoc: Robust Localization using Navigation Signs and Public Maps", "categories": ["cs.RO"], "comment": "Under submission for Robotics and Automation Letters (RA-L)", "summary": "Navigation signs and maps, such as floor plans and street maps, are widely\navailable and serve as ubiquitous aids for way-finding in human environments.\nYet, they are rarely used by robot systems. This paper presents SignLoc, a\nglobal localization method that leverages navigation signs to localize the\nrobot on publicly available maps -- specifically floor plans and OpenStreetMap\n(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a\nnavigation graph from the input map. It then employs a probabilistic\nobservation model to match directional and locational cues from the detected\nsigns to the graph, enabling robust topo-semantic localization within a Monte\nCarlo framework. We evaluated SignLoc in diverse large-scale environments: part\nof a university campus, a shopping mall, and a hospital complex. Experimental\nresults show that SignLoc reliably localizes the robot after observing only one\nto two signs.", "AI": {"tldr": "SignLoc\u662f\u4e00\u79cd\u5229\u7528\u5bfc\u822a\u6807\u5fd7\u5728\u516c\u5f00\u5730\u56fe\uff08\u5982\u5e73\u9762\u56fe\u548cOpenStreetMap\uff09\u4e0a\u8fdb\u884c\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u7684\u4f20\u611f\u5668\u5730\u56fe\u3002", "motivation": "\u5bfc\u822a\u6807\u5fd7\u548c\u5730\u56fe\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u673a\u5668\u4eba\u7cfb\u7edf\u5f88\u5c11\u5229\u7528\u8fd9\u4e9b\u8d44\u6e90\u8fdb\u884c\u5b9a\u4f4d\u3002", "method": "SignLoc\u4ece\u8f93\u5165\u5730\u56fe\u4e2d\u63d0\u53d6\u5bfc\u822a\u56fe\uff0c\u4f7f\u7528\u6982\u7387\u89c2\u5bdf\u6a21\u578b\u5c06\u68c0\u6d4b\u5230\u7684\u6807\u5fd7\u4e0e\u56fe\u5339\u914d\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6846\u67b6\u7684\u8bed\u4e49\u5b9a\u4f4d\u3002", "result": "\u5728\u6821\u56ed\u3001\u8d2d\u7269\u4e2d\u5fc3\u548c\u533b\u9662\u7b49\u591a\u79cd\u73af\u5883\u4e2d\u6d4b\u8bd5\u8868\u660e\uff0cSignLoc\u4ec5\u9700\u89c2\u6d4b\u4e00\u5230\u4e24\u4e2a\u6807\u5fd7\u5373\u53ef\u53ef\u9760\u5b9a\u4f4d\u673a\u5668\u4eba\u3002", "conclusion": "SignLoc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5229\u7528\u516c\u5f00\u5730\u56fe\u8d44\u6e90\u5b9e\u73b0\u673a\u5668\u4eba\u5b9a\u4f4d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18274", "pdf": "https://arxiv.org/pdf/2508.18274", "abs": "https://arxiv.org/abs/2508.18274", "authors": ["Haorui Wang", "Bo Zhao"], "title": "Is Green Enough? A Remote Sensing Assessment of Environmental Impacts and Green Commitments at Beijing Daxing International Airport", "categories": ["physics.soc-ph", "eess.SP"], "comment": null, "summary": "Beijing Daxing International Airport has been promoted as a model of green\ninfrastructure under China's ecological modernization agenda. Featuring\nenergy-efficient design, renewable energy systems, and smart environmental\ncontrols, the airport embodies multiple green commitments. This study evaluates\nits environmental outcomes using multi-source remote sensing data -- including\nNDVI, NDBI, Land Surface Temperature (LST), VIIRS night-time lights, and PM2.5\n-- from 2014 to 2019. Through spatial and temporal comparisons, we assess\nlandscape-level changes during and after construction. Findings indicate\npartial gains from green initiatives but also reveal substantial vegetation\nloss, increased built-up surfaces, and intensified surface temperatures. The\nresults suggest a gap between sustainable design and ecological impact. We\npropose a remote-sensing-based framework for evaluating future infrastructure\nprojects, emphasizing the need for spatially explicit, independent monitoring\nto ensure environmental accountability.", "AI": {"tldr": "\u5317\u4eac\u5927\u5174\u56fd\u9645\u673a\u573a\u4f5c\u4e3a\u4e2d\u56fd\u751f\u6001\u73b0\u4ee3\u5316\u8bae\u7a0b\u4e0b\u7684\u7eff\u8272\u57fa\u7840\u8bbe\u65bd\u5178\u8303\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u591a\u6e90\u9065\u611f\u6570\u636e\u8bc4\u4f30\u5176\u73af\u5883\u5f71\u54cd\uff0c\u53d1\u73b0\u90e8\u5206\u7eff\u8272\u5021\u8bae\u6210\u6548\u663e\u8457\uff0c\u4f46\u4e5f\u5b58\u5728\u690d\u88ab\u51cf\u5c11\u3001\u5efa\u7b51\u8868\u9762\u589e\u52a0\u53ca\u5730\u8868\u6e29\u5ea6\u4e0a\u5347\u7b49\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30\u5317\u4eac\u5927\u5174\u56fd\u9645\u673a\u573a\u4f5c\u4e3a\u7eff\u8272\u57fa\u7840\u8bbe\u65bd\u7684\u73af\u5883\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5176\u751f\u6001\u73b0\u4ee3\u5316\u627f\u8bfa\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u4f7f\u7528\u5305\u62ecNDVI\u3001NDBI\u3001\u5730\u8868\u6e29\u5ea6\uff08LST\uff09\u3001VIIRS\u591c\u95f4\u706f\u5149\u548cPM2.5\u5728\u5185\u7684\u591a\u6e90\u9065\u611f\u6570\u636e\uff082014-2019\u5e74\uff09\uff0c\u8fdb\u884c\u7a7a\u95f4\u548c\u65f6\u95f4\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7eff\u8272\u5021\u8bae\u90e8\u5206\u6709\u6548\uff0c\u4f46\u4e5f\u5b58\u5728\u690d\u88ab\u635f\u5931\u3001\u5efa\u7b51\u8868\u9762\u589e\u52a0\u548c\u5730\u8868\u6e29\u5ea6\u4e0a\u5347\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\u91c7\u7528\u9065\u611f\u76d1\u6d4b\u6846\u67b6\uff0c\u5f3a\u5316\u7a7a\u95f4\u72ec\u7acb\u76d1\u6d4b\u4ee5\u786e\u4fdd\u73af\u4fdd\u8d23\u4efb\u3002"}}
{"id": "2508.18627", "pdf": "https://arxiv.org/pdf/2508.18627", "abs": "https://arxiv.org/abs/2508.18627", "authors": ["Ziyuan Jiao", "Yida Niu", "Zeyu Zhang", "Yangyang Wu", "Yao Su", "Yixin Zhu", "Hangxin Liu", "Song-Chun Zhu"], "title": "Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning", "categories": ["cs.RO"], "comment": "20 pages, 13 figures; accepted by Transactions on Robotics", "summary": "We present a Sequential Mobile Manipulation Planning (SMMP) framework that\ncan solve long-horizon multi-step mobile manipulation tasks with coordinated\nwhole-body motion, even when interacting with articulated objects. By\nabstracting environmental structures as kinematic models and integrating them\nwith the robot's kinematics, we construct an Augmented Configuration Apace\n(A-Space) that unifies the previously separate task constraints for navigation\nand manipulation, while accounting for the joint reachability of the robot\nbase, arm, and manipulated objects. This integration facilitates efficient\nplanning within a tri-level framework: a task planner generates symbolic action\nsequences to model the evolution of A-Space, an optimization-based motion\nplanner computes continuous trajectories within A-Space to achieve desired\nconfigurations for both the robot and scene elements, and an intermediate plan\nrefinement stage selects action goals that ensure long-horizon feasibility. Our\nsimulation studies first confirm that planning in A-Space achieves an 84.6\\%\nhigher task success rate compared to baseline methods. Validation on real\nrobotic systems demonstrates fluid mobile manipulation involving (i) seven\ntypes of rigid and articulated objects across 17 distinct contexts, and (ii)\nlong-horizon tasks of up to 14 sequential steps. Our results highlight the\nsignificance of modeling scene kinematics into planning entities, rather than\nencoding task-specific constraints, offering a scalable and generalizable\napproach to complex robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Sequential Mobile Manipulation Planning (SMMP)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u7ed3\u6784\u62bd\u8c61\u4e3a\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\uff0c\u6784\u5efa\u4e86\u7edf\u4e00\u7684Augmented Configuration Space (A-Space)\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u591a\u6b65\u9aa4\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u671f\u591a\u6b65\u9aa4\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u65f6\uff0c\u5e38\u56e0\u672a\u7edf\u4e00\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u7ea6\u675f\u800c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u5931\u8d25\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684A-Space\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u6b21\u89c4\u5212\u6846\u67b6\uff1a\u4efb\u52a1\u89c4\u5212\u5668\u751f\u6210\u7b26\u53f7\u52a8\u4f5c\u5e8f\u5217\uff0c\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u5668\u8ba1\u7b97\u8fde\u7eed\u8f68\u8ff9\uff0c\u4e2d\u95f4\u9636\u6bb5\u9009\u62e9\u80fd\u786e\u4fdd\u957f\u671f\u53ef\u884c\u6027\u7684\u76ee\u6807\u3002", "result": "\u4eff\u771f\u663e\u793aA-Space\u89c4\u5212\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad884.6%\uff1b\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u6210\u529f\u5e94\u5bf97\u7c7b\u7269\u4f53\u548c17\u79cd\u573a\u666f\uff0c\u5b8c\u6210\u6700\u591a14\u6b65\u7684\u957f\u671f\u4efb\u52a1\u3002", "conclusion": "\u5c06\u573a\u666f\u8fd0\u52a8\u5b66\u5efa\u6a21\u4e3a\u89c4\u5212\u5b9e\u4f53\uff0c\u800c\u975e\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\uff0c\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u6027\u5f3a\u7684\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u3002"}}
{"id": "2508.18483", "pdf": "https://arxiv.org/pdf/2508.18483", "abs": "https://arxiv.org/abs/2508.18483", "authors": ["Zhonggang Li", "Geert Leus", "Raj Thilak Rajan"], "title": "Fast Multiagent Formation Stabilization with Sparse Universally Rigid Frameworks", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "Affine formation control (AFC) is a distributed networked control system that\nhas recently received increasing attention in various applications. AFC is\ntypically achieved using a generalized consensus system where the stress\nmatrix, which encodes the graph structure, is used instead of a graph\nLaplacian. Universally rigid frameworks (URFs) guarantee the existence of the\nstress matrix and have thus become the guideline for such a network design. In\nthis work, we propose a convex optimization framework to design the stress\nmatrix for AFC without predefining a rigid graph. We aim to find a resulting\nnetwork with a reduced number of communication links, but still with a fast\nconvergence speed. We show through simulations that our proposed solutions can\nyield a more sparse graph, while admitting a faster convergence compared to the\nstate-of-the-art solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u5b9a\u4e49\u521a\u6027\u56fe\u7684\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1AFC\u7684\u5e94\u529b\u77e9\u9635\uff0c\u65e8\u5728\u51cf\u5c11\u901a\u4fe1\u94fe\u8def\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u6536\u655b\u901f\u5ea6\u3002", "motivation": "AFC\u4f5c\u4e3a\u4e00\u79cd\u5206\u5e03\u5f0f\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u521a\u6027\u56fe\u7684\u7f51\u7edc\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u901a\u4fe1\u94fe\u8def\u6570\u91cf\u5e76\u4fdd\u6301\u9ad8\u6548\u6027\u80fd\u3002", "method": "\u91c7\u7528\u51f8\u4f18\u5316\u6846\u67b6\u8bbe\u8ba1\u5e94\u529b\u77e9\u9635\uff0c\u907f\u514d\u4e86\u9884\u5148\u5b9a\u4e49\u521a\u6027\u56fe\u7684\u6b65\u9aa4\uff0c\u76f4\u63a5\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u7a00\u758f\u7684\u56fe\u7ed3\u6784\uff0c\u540c\u65f6\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u8be5\u51f8\u4f18\u5316\u6846\u67b6\u5728AFC\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18662", "pdf": "https://arxiv.org/pdf/2508.18662", "abs": "https://arxiv.org/abs/2508.18662", "authors": ["Stefan Ramdhan", "Winnie Trandinh", "Istvan David", "Vera Pantelic", "Mark Lawford"], "title": "Engineering Automotive Digital Twins on Standardized Architectures: A Case Study", "categories": ["cs.RO"], "comment": "7 pages, 6 figures. Submitted to EDTconf 2025", "summary": "Digital twin (DT) technology has become of interest in the automotive\nindustry. There is a growing need for smarter services that utilize the unique\ncapabilities of DTs, ranging from computer-aided remote control to cloud-based\nfleet coordination. Developing such services starts with the software\narchitecture. However, the scarcity of DT architectural guidelines poses a\nchallenge for engineering automotive DTs. Currently, the only DT architectural\nstandard is the one defined in ISO 23247. Though not developed for automotive\nsystems, it is one of the few feasible starting points for automotive DTs. In\nthis work, we investigate the suitability of the ISO 23247 reference\narchitecture for developing automotive DTs. Through the case study of\ndeveloping an Adaptive Cruise Control DT for a 1/10\\textsuperscript{th}-scale\nautonomous vehicle, we identify some strengths and limitations of the reference\narchitecture and begin distilling future directions for researchers,\npractitioners, and standard developers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86ISO 23247\u53c2\u8003\u67b6\u6784\u5728\u5f00\u53d1\u6c7d\u8f66\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u603b\u7ed3\u4e86\u5176\u4f18\u7f3a\u70b9\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6c7d\u8f66\u884c\u4e1a\u5bf9\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u67b6\u6784\u6307\u5357\uff0cISO 23247\u662f\u5c11\u6570\u53ef\u884c\u7684\u8d77\u70b9\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u9002\u7528\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a1/10\u6bd4\u4f8b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u6570\u5b57\u5b6a\u751f\u6848\u4f8b\uff0c\u5206\u6790ISO 23247\u67b6\u6784\u7684\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86ISO 23247\u67b6\u6784\u7684\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u4ece\u4e1a\u8005\u548c\u6807\u51c6\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "ISO 23247\u67b6\u6784\u5728\u6c7d\u8f66\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u6ee1\u8db3\u884c\u4e1a\u9700\u6c42\u3002"}}
{"id": "2508.18680", "pdf": "https://arxiv.org/pdf/2508.18680", "abs": "https://arxiv.org/abs/2508.18680", "authors": ["Yun-Feng Lo", "Yen-Chi Lee"], "title": "Joint Time-Position Statistics and Fisher Information in Drift-Diffusion Molecular Channels", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "4 pages, 2 figures", "summary": "This letter presents a closed-form characterization of the joint distribution\nof first arrival time (FAT) and first arrival position (FAP) in diffusion-based\nmolecular communication (MC) systems with drift. Prior studies have\ninvestigated FAT modeling via inverse Gaussian distributions [1] and applied\nFAT statistics for parameter estimation and synchronization tasks [2], [3],\nwhile more recent work has characterized FAP for spatial channel analysis [4].\nIn contrast, we derive an explicit joint probability density function (PDF)\nunder constant drift and isotropic diffusion in arbitrary spatial dimensions.\nOur result reveals a nontrivial coupling between arrival time and lateral\nposition, generalizing known inverse Gaussian models. We further compute the\nFisher information matrix (FIM) with respect to key channel parameters, showing\nthat the joint observation enables estimation of lateral drift and improves\nsensitivity to the diffusion coefficient -- capabilities not achievable with\ntime-only or position-only models. This joint framework enhances the modeling\nand inference capabilities for molecular communication channels where spatial\nrandomness itself carries non-negligible information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6f02\u79fb\u6761\u4ef6\u4e0b\u7684\u6269\u6563\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u4e2d\u9996\u6b21\u5230\u8fbe\u65f6\u95f4\uff08FAT\uff09\u548c\u9996\u6b21\u5230\u8fbe\u4f4d\u7f6e\uff08FAP\uff09\u8054\u5408\u5206\u5e03\u7684\u95ed\u5f0f\u89e3\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u4e0e\u4f4d\u7f6e\u7684\u8026\u5408\u5173\u7cfb\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4ec5\u5206\u522b\u7814\u7a76\u4e86FAT\u548cFAP\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800c\u672a\u63a2\u8ba8\u5b83\u4eec\u7684\u8054\u5408\u5206\u5e03\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5efa\u6a21\u548c\u53c2\u6570\u4f30\u8ba1\u80fd\u529b\u3002", "method": "\u5728\u6052\u5b9a\u6f02\u79fb\u548c\u5404\u5411\u540c\u6027\u6269\u6563\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u4e86\u4efb\u610f\u7a7a\u95f4\u7ef4\u5ea6\u4e2dFAT\u548cFAP\u7684\u8054\u5408\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08PDF\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e86\u65f6\u95f4\u4e0e\u6a2a\u5411\u4f4d\u7f6e\u7684\u8026\u5408\u5173\u7cfb\uff0c\u5e76\u6269\u5c55\u4e86\u5df2\u77e5\u7684\u53cd\u9ad8\u65af\u6a21\u578b\u3002\u901a\u8fc7\u8ba1\u7b97Fisher\u4fe1\u606f\u77e9\u9635\uff08FIM\uff09\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u89c2\u5bdf\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u6a2a\u5411\u6f02\u79fb\u548c\u6269\u6563\u7cfb\u6570\u7684\u4f30\u8ba1\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u7684\u8054\u5408\u5efa\u6a21\u6846\u67b6\u589e\u5f3a\u4e86\u5206\u5b50\u901a\u4fe1\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u63a8\u65ad\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u968f\u673a\u6027\u4fe1\u606f\u4e0d\u53ef\u5ffd\u7565\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.18691", "pdf": "https://arxiv.org/pdf/2508.18691", "abs": "https://arxiv.org/abs/2508.18691", "authors": ["Himanshu Gaurav Singh", "Pieter Abbeel", "Jitendra Malik", "Antonio Loquercio"], "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion", "categories": ["cs.RO"], "comment": "Blog Post: https://hgaurav2k.github.io/trackr/", "summary": "As the embodiment gap between a robot and a human narrows, new opportunities\narise to leverage datasets of humans interacting with their surroundings for\nrobot learning. We propose a novel technique for training sensorimotor policies\nwith reinforcement learning by imitating predictive models of human motions.\nOur key insight is that the motion of keypoints on human-inspired robot\nend-effectors closely mirrors the motion of corresponding human body keypoints.\nThis enables us to use a model trained to predict future motion on human data\n\\emph{zero-shot} on robot data. We train sensorimotor policies to track the\npredictions of such a model, conditioned on a history of past robot states,\nwhile optimizing a relatively sparse task reward. This approach entirely\nbypasses gradient-based kinematic retargeting and adversarial losses, which\nlimit existing methods from fully leveraging the scale and diversity of modern\nhuman-scene interaction datasets. Empirically, we find that our approach can\nwork across robots and tasks, outperforming existing baselines by a large\nmargin. In addition, we find that tracking a human motion model can substitute\nfor carefully designed dense rewards and curricula in manipulation tasks. Code,\ndata and qualitative results available at\nhttps://jirl-upenn.github.io/track_reward/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u4eba\u7c7b\u52a8\u4f5c\u9884\u6d4b\u6a21\u578b\u8bad\u7ec3\u673a\u5668\u4eba\u4f20\u611f\u5668\u8fd0\u52a8\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u68af\u5ea6\u91cd\u5b9a\u5411\u6216\u5bf9\u6297\u635f\u5931\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4ea4\u4e92\u80fd\u529b\u5dee\u8ddd\u7f29\u5c0f\uff0c\u5229\u7528\u4eba\u7c7b\u4e0e\u73af\u5883\u4ea4\u4e92\u6570\u636e\u96c6\u8bad\u7ec3\u673a\u5668\u4eba\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u76f4\u63a5\u8ddf\u8e2a\u9884\u6d4b\u7ed3\u679c\u5e76\u4f18\u5316\u7a00\u758f\u4efb\u52a1\u5956\u52b1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u673a\u5668\u4eba\u53ca\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u65e0\u9700\u8bbe\u8ba1\u590d\u6742\u7684\u5bc6\u96c6\u5956\u52b1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6280\u672f\u6709\u6548\u5229\u7528\u4e86\u4eba\u7c7b\u6570\u636e\u96c6\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.18734", "pdf": "https://arxiv.org/pdf/2508.18734", "abs": "https://arxiv.org/abs/2508.18734", "authors": ["DongHoon Lim", "YoungChae Kim", "Dong-Hyun Kim", "Da-Hee Yang", "Joon-Hyuk Chang"], "title": "Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "comment": "Accepted to IEEE ASRU 2025", "summary": "Robust audio-visual speech recognition (AVSR) in noisy environments remains\nchallenging, as existing systems struggle to estimate audio reliability and\ndynamically adjust modality reliance. We propose router-gated cross-modal\nfeature fusion, a novel AVSR framework that adaptively reweights audio and\nvisual features based on token-level acoustic corruption scores. Using an\naudio-visual feature fusion-based router, our method down-weights unreliable\naudio tokens and reinforces visual cues through gated cross-attention in each\ndecoder layer. This enables the model to pivot toward the visual modality when\naudio quality deteriorates. Experiments on LRS3 demonstrate that our approach\nachieves an 16.51-42.67% relative reduction in word error rate compared to\nAV-HuBERT. Ablation studies confirm that both the router and gating mechanism\ncontribute to improved robustness under real-world acoustic noise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u7531\u5668\u95e8\u63a7\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u7684\u65b0\u578bAVSR\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u7684\u6743\u91cd\u4ee5\u63d0\u9ad8\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u566a\u58f0\u73af\u5883\u4e2d\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u97f3\u9891\u53ef\u9760\u6027\u5e76\u52a8\u6001\u8c03\u6574\u6a21\u6001\u4f9d\u8d56\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u97f3\u9891-\u89c6\u89c9\u7279\u5f81\u878d\u5408\u7684\u8def\u7531\u5668\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u58f0\u5b66\u8150\u8d25\u5206\u6570\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff0c\u5e76\u5728\u89e3\u7801\u5668\u5c42\u4e2d\u4f7f\u7528\u95e8\u63a7\u8de8\u6ce8\u610f\u673a\u5236\u5f3a\u5316\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u5728LRS3\u6570\u636e\u4e0a\uff0c\u76f8\u6bd4AV-HuBERT\u6a21\u578b\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e\u4e8616.51-42.67%\u3002", "conclusion": "\u8def\u7531\u5668\u53ca\u5176\u95e8\u63a7\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.18694", "pdf": "https://arxiv.org/pdf/2508.18694", "abs": "https://arxiv.org/abs/2508.18694", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Mohammad Jony", "Shahab Ahmad", "Md. Mukhlesur Rahman", "Sangpil Kim", "M. Khalid Jawed"], "title": "AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Existing datasets for precision agriculture have primarily been collected in\nstatic or controlled environments such as indoor labs or greenhouses, often\nwith limited sensor diversity and restricted temporal span. These conditions\nfail to reflect the dynamic nature of real farmland, including illumination\nchanges, crop growth variation, and natural disturbances. As a result, models\ntrained on such data often lack robustness and generalization when applied to\nreal-world field scenarios. In this paper, we present AgriChrono, a novel\nrobotic data collection platform and multi-modal dataset designed to capture\nthe dynamic conditions of real-world agricultural environments. Our platform\nintegrates multiple sensors and enables remote, time-synchronized acquisition\nof RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable\nlong-term data collection across varying illumination and crop growth stages.\nWe benchmark a range of state-of-the-art 3D reconstruction models on the\nAgriChrono dataset, highlighting the difficulty of reconstruction in real-world\nfield environments and demonstrating its value as a research asset for\nadvancing model generalization under dynamic conditions. The code and dataset\nare publicly available at: https://github.com/StructuresComp/agri-chrono", "AI": {"tldr": "AgriChrono\u662f\u4e00\u4e2a\u65b0\u578b\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u5e73\u53f0\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6355\u6349\u771f\u5b9e\u519c\u4e1a\u73af\u5883\u7684\u52a8\u6001\u6761\u4ef6\uff0c\u5f25\u8865\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u9759\u6001\u6216\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u91c7\u96c6\uff0c\u7f3a\u4e4f\u4f20\u611f\u5668\u591a\u6837\u6027\u548c\u65f6\u95f4\u8de8\u5ea6\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u519c\u7530\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\uff08RGB\u3001\u6df1\u5ea6\u3001LiDAR\u548cIMU\uff09\u5e76\u5b9e\u73b0\u8fdc\u7a0b\u65f6\u95f4\u540c\u6b65\u91c7\u96c6\uff0cAgriChrono\u5e73\u53f0\u652f\u6301\u957f\u671f\u3001\u9ad8\u6548\u3001\u53ef\u91cd\u590d\u7684\u6570\u636e\u6536\u96c6\u3002", "result": "\u5728AgriChrono\u6570\u636e\u96c6\u4e0a\u5bf9\u591a\u79cd\u5148\u8fdb\u76843D\u91cd\u5efa\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6311\u6218\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u4ef7\u503c\u3002", "conclusion": "AgriChrono\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e0b\u7684\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.18755", "pdf": "https://arxiv.org/pdf/2508.18755", "abs": "https://arxiv.org/abs/2508.18755", "authors": ["Seungmin Lee", "Changmin Lee", "Si-Chan Noh", "Joonsoo Lee"], "title": "Performance Analysis of IEEE 802.11bn with Coordinated TDMA on Real-Time Applications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "Accepted by IEEE Global Communications Conference (GLOBECOM) 2025", "summary": "Wi-Fi plays a crucial role in connecting electronic devices and providing\ncommunication services in everyday life. Recently, there has been a growing\ndemand for services that require low-latency communication, such as real-time\napplications. The latest amendments to Wi-Fi, IEEE 802.11bn, are being\ndeveloped to address these demands with technologies such as the multiple\naccess point coordination (MAPC). In this paper, we demonstrate that\ncoordinated TDMA (Co-TDMA), one of the MAPC techniques, effectively reduces the\nlatency of transmitting time-sensitive traffic. In particular, we focus on\nworst-case latency and jitter, which are key metrics for evaluating the\nperformance of real-time applications. We first introduce a Co-TDMA scheduling\nstrategy. We then investigate how this scheduling strategy impacts latency\nunder varying levels of network congestion and traffic volume characteristics.\nFinally, we validate our findings through system-level simulations. Our\nsimulation results demonstrate that Co-TDMA effectively mitigates jitter and\nworst-case latency for LL traffic, with the latter exhibiting an improvement of\napproximately 24%.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Wi-Fi\u4e2d\u534f\u8c03TDMA\uff08Co-TDMA\uff09\u6280\u672f\u5982\u4f55\u901a\u8fc7\u8c03\u5ea6\u7b56\u7565\u964d\u4f4e\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u548c\u6296\u52a8\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7f51\u7edc\u8d1f\u8f7d\u4e0b\u3002\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4eff\u771f\u9a8c\u8bc1\uff0cCo-TDMA\u663e\u8457\u6539\u8fdb\u4e86\u6700\u574f\u60c5\u51b5\u5ef6\u8fdf\u548c\u6296\u52a8\uff0c\u5ef6\u8fdf\u6539\u5584\u7ea624%\u3002", "motivation": "\u7531\u4e8e\u5b9e\u65f6\u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7Wi-Fi\u6280\u672f\uff08\u5982IEEE 802.11bn\u4e2d\u7684\u591a\u63a5\u5165\u70b9\u534f\u8c03\u6280\u672f\uff09\u4f18\u5316\u65f6\u95f4\u654f\u611f\u6d41\u91cf\u7684\u4f20\u8f93\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8c03TDMA\uff08Co-TDMA\uff09\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u7b56\u7565\u5728\u7f51\u7edc\u62e5\u5835\u548c\u4e0d\u540c\u6d41\u91cf\u7279\u5f81\u4e0b\u5bf9\u5ef6\u8fdf\u548c\u6296\u52a8\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCo-TDMA\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u65f6\u6d41\u91cf\u4e2d\u7684\u6296\u52a8\u548c\u6700\u574f\u60c5\u51b5\u5ef6\u8fdf\uff0c\u7279\u522b\u662f\u540e\u8005\u6539\u5584\u4e86\u7ea624%\u3002", "conclusion": "Co-TDMA\u662f\u4e00\u79cd\u6709\u6548\u4f18\u5316Wi-Fi\u5b9e\u65f6\u901a\u4fe1\u6027\u80fd\u7684\u6280\u672f\uff0c\u5c24\u5176\u5728\u9ad8\u8d1f\u8f7d\u7f51\u7edc\u73af\u5883\u4e0b\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5ef6\u8fdf\u548c\u6296\u52a8\u3002"}}
{"id": "2508.18705", "pdf": "https://arxiv.org/pdf/2508.18705", "abs": "https://arxiv.org/abs/2508.18705", "authors": ["Santosh Thoduka", "Sebastian Houben", "Juergen Gall", "Paul G. Pl\u00f6ger"], "title": "Enhancing Video-Based Robot Failure Detection Using Task Knowledge", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ECMR 2025", "summary": "Robust robotic task execution hinges on the reliable detection of execution\nfailures in order to trigger safe operation modes, recovery strategies, or task\nreplanning. However, many failure detection methods struggle to provide\nmeaningful performance when applied to a variety of real-world scenarios. In\nthis paper, we propose a video-based failure detection approach that uses\nspatio-temporal knowledge in the form of the actions the robot performs and\ntask-relevant objects within the field of view. Both pieces of information are\navailable in most robotic scenarios and can thus be readily obtained. We\ndemonstrate the effectiveness of our approach on three datasets that we amend,\nin part, with additional annotations of the aforementioned task-relevant\nknowledge. In light of the results, we also propose a data augmentation method\nthat improves performance by applying variable frame rates to different parts\nof the video. We observe an improvement from 77.9 to 80.0 in F1 score on the\nARMBench dataset without additional computational expense and an additional\nincrease to 81.4 with test-time augmentation. The results emphasize the\nimportance of spatio-temporal information during failure detection and suggest\nfurther investigation of suitable heuristics in future implementations. Code\nand annotations are available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u673a\u5668\u4eba\u4efb\u52a1\u5931\u8d25\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u7a7a\u77e5\u8bc6\uff08\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u4efb\u52a1\u76f8\u5173\u7269\u4f53\uff09\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5931\u8d25\u68c0\u6d4b\u5bf9\u5b89\u5168\u64cd\u4f5c\u548c\u6062\u590d\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u4efb\u52a1\u76f8\u5173\u7269\u4f53\u7684\u65f6\u7a7a\u4fe1\u606f\u8fdb\u884c\u5931\u8d25\u68c0\u6d4b\uff0c\u5e76\u91c7\u7528\u53ef\u53d8\u5e27\u7387\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728ARMBench\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u4ece77.9\u63d0\u5347\u523080.0\uff0c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u540e\u8fdb\u4e00\u6b65\u589e\u81f381.4\u3002", "conclusion": "\u65f6\u7a7a\u4fe1\u606f\u5bf9\u5931\u8d25\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5408\u9002\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2508.18968", "pdf": "https://arxiv.org/pdf/2508.18968", "abs": "https://arxiv.org/abs/2508.18968", "authors": ["Hannah Och", "Andr\u00e9 Kaup"], "title": "Lossless 4:2:0 Screen Content Coding Using Luma-Guided Soft Context Formation", "categories": ["eess.IV", "cs.MM", "eess.SP"], "comment": "5 pages, 4 figures, 3 tables, accepted to EUSIPCO 2025", "summary": "The soft context formation coder is a pixel-wise state-of-the-art lossless\nscreen content coder using pattern matching and color palette coding in\ncombination with arithmetic coding. It achieves excellent compression\nperformance on screen content images in RGB 4:4:4 format with few distinct\ncolors. In contrast to many other lossless compression methods, it codes entire\ncolor pixels at once, i.e., all color components of one pixel are coded\ntogether. Consequently, it does not natively support image formats with\ndownsampled chroma, such as YCbCr 4:2:0, which is an often used chroma format\nin video compression. In this paper, we extend the soft context formation\ncoding capabilities to 4:2:0 image compression, by successively coding Y and\nCbCr planes based on an analysis of normalized mutual information between image\nplanes. Additionally, we propose an enhancement to the chroma prediction based\non the luminance plane. Furthermore, we propose to transmit side-information\nabout occurring luma-chroma combinations to improve chroma probability\ndistribution modelling. Averaged over a large screen content image dataset, our\nproposed method outperforms HEVC-SCC, with HEVC-SCC needing 5.66% more bitrate\ncompared to our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8f6f\u4e0a\u4e0b\u6587\u5f62\u6210\u7f16\u7801\u65b9\u6cd5\uff0c\u652f\u6301YCbCr 4:2:0\u683c\u5f0f\u56fe\u50cf\u538b\u7f29\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u5e73\u9762\u95f4\u7684\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\u548c\u589e\u5f3a\u8272\u5ea6\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8f6f\u4e0a\u4e0b\u6587\u5f62\u6210\u7f16\u7801\u5668\u867d\u7136\u5728\u67d0\u4e9b\u683c\u5f0f\uff08\u5982RGB 4:4:4\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e0d\u652f\u6301\u5e38\u89c1\u7684YCbCr 4:2:0\u683c\u5f0f\u3002\u4e3a\u4e86\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u5e76\u63d0\u9ad8\u538b\u7f29\u6548\u7387\uff0c\u9700\u8981\u5bf9\u539f\u6709\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u6269\u5c55\u4e86\u8f6f\u4e0a\u4e0b\u6587\u5f62\u6210\u7f16\u7801\u80fd\u529b\uff0c\u652f\u63014:2:0\u683c\u5f0f\uff0c\u901a\u8fc7\u5206\u6790\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\u4f9d\u6b21\u7f16\u7801Y\u548cCbCr\u5e73\u9762\uff0c\u589e\u5f3a\u8272\u5ea6\u9884\u6d4b\uff0c\u5e76\u4f20\u8f93\u4eae\u8272\u5ea6\u7ec4\u5408\u7684\u8f85\u52a9\u4fe1\u606f\u4ee5\u4f18\u5316\u6982\u7387\u5206\u5e03\u5efa\u6a21\u3002", "result": "\u5728\u5927\u91cf\u5c4f\u5e55\u5185\u5bb9\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6240\u63d0\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8eHEVC-SCC\uff0cHEVC-SCC\u7684\u6bd4\u7279\u7387\u6bd4\u6240\u63d0\u65b9\u6cd5\u9ad85.66%\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7684\u8f6f\u4e0a\u4e0b\u6587\u5f62\u6210\u7f16\u7801\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9YCbCr 4:2:0\u683c\u5f0f\u7684\u9ad8\u6548\u538b\u7f29\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6807\u51c6\u3002"}}
{"id": "2508.18802", "pdf": "https://arxiv.org/pdf/2508.18802", "abs": "https://arxiv.org/abs/2508.18802", "authors": ["Li Sun", "Jiefeng Wu", "Feng Chen", "Ruizhe Liu", "Yanchao Yang"], "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Effective policy learning for robotic manipulation requires scene\nrepresentations that selectively capture task-relevant environmental features.\nCurrent approaches typically employ task-agnostic representation extraction,\nfailing to emulate the dynamic perceptual adaptation observed in human\ncognition. We present HyperTASR, a hypernetwork-driven framework that modulates\nscene representations based on both task objectives and the execution phase.\nOur architecture dynamically generates representation transformation parameters\nconditioned on task specifications and progression state, enabling\nrepresentations to evolve contextually throughout task execution. This approach\nmaintains architectural compatibility with existing policy learning frameworks\nwhile fundamentally reconfiguring how visual features are processed. Unlike\nmethods that simply concatenate or fuse task embeddings with task-agnostic\nrepresentations, HyperTASR establishes computational separation between\ntask-contextual and state-dependent processing paths, enhancing learning\nefficiency and representational quality. Comprehensive evaluations in both\nsimulation and real-world environments demonstrate substantial performance\nimprovements across different representation paradigms. Through ablation\nstudies and attention visualization, we confirm that our approach selectively\nprioritizes task-relevant scene information, closely mirroring human adaptive\nperception during manipulation tasks. The project website is at\n\\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\\_projectpage}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHyperTASR\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u573a\u666f\u8868\u793a\u4ee5\u9002\u5e94\u4efb\u52a1\u76ee\u6807\u548c\u6267\u884c\u9636\u6bb5\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u4efb\u52a1\u65e0\u5173\u7684\u8868\u793a\u63d0\u53d6\uff0c\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4efb\u52a1\u611f\u77e5\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002", "method": "HyperTASR\u5229\u7528\u8d85\u7f51\u7edc\u6839\u636e\u4efb\u52a1\u76ee\u6807\u548c\u6267\u884c\u72b6\u6001\u52a8\u6001\u751f\u6210\u8868\u793a\u53d8\u6362\u53c2\u6570\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u7684\u517c\u5bb9\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHyperTASR\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u52a8\u6001\u9009\u62e9\u80fd\u529b\u3002", "conclusion": "HyperTASR\u6210\u529f\u6a21\u62df\u4e86\u4eba\u7c7b\u81ea\u9002\u5e94\u611f\u77e5\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u573a\u666f\u8868\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18817", "pdf": "https://arxiv.org/pdf/2508.18817", "abs": "https://arxiv.org/abs/2508.18817", "authors": ["Colin Merk", "Ismail Geles", "Jiaxu Xing", "Angel Romero", "Giorgia Ramponi", "Davide Scaramuzza"], "title": "Learning Real-World Acrobatic Flight from Human Preferences", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 7 figures", "summary": "Preference-based reinforcement learning (PbRL) enables agents to learn\ncontrol policies without requiring manually designed reward functions, making\nit well-suited for tasks where objectives are difficult to formalize or\ninherently subjective. Acrobatic flight poses a particularly challenging\nproblem due to its complex dynamics, rapid movements, and the importance of\nprecise execution. In this work, we explore the use of PbRL for agile drone\ncontrol, focusing on the execution of dynamic maneuvers such as powerloops.\nBuilding on Preference-based Proximal Policy Optimization (Preference PPO), we\npropose Reward Ensemble under Confidence (REC), an extension to the reward\nlearning objective that improves preference modeling and learning stability.\nOur method achieves 88.4% of the shaped reward performance, compared to 55.2%\nwith standard Preference PPO. We train policies in simulation and successfully\ntransfer them to real-world drones, demonstrating multiple acrobatic maneuvers\nwhere human preferences emphasize stylistic qualities of motion. Furthermore,\nwe demonstrate the applicability of our probabilistic reward model in a\nrepresentative MuJoCo environment for continuous control. Finally, we highlight\nthe limitations of manually designed rewards, observing only 60.7% agreement\nwith human preferences. These results underscore the effectiveness of PbRL in\ncapturing complex, human-centered objectives across both physical and simulated\ndomains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\uff08PbRL\uff09\u5728\u654f\u6377\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReward Ensemble under Confidence\uff08REC\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u504f\u597d\u5efa\u6a21\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u79cd\u7279\u6280\u52a8\u4f5c\uff0c\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76PbRL\u5728\u65e0\u4eba\u673a\u7279\u6280\u98de\u884c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u590d\u6742\u3001\u4e3b\u89c2\u6027\u5f3a\u7684\u4efb\u52a1\u76ee\u6807\u96be\u4ee5\u5f62\u5f0f\u5316\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8ePreference PPO\uff0c\u63d0\u51faREC\u65b9\u6cd5\u6539\u8fdb\u5956\u52b1\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u4e0a\u8bad\u7ec3\u7b56\u7565\u3002", "result": "REC\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u523088.4%\u7684\u6210\u578b\u5956\u52b1\u6548\u679c\uff0c\u4f18\u4e8e\u6807\u51c6Preference PPO\u768455.2%\uff0c\u5e76\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u79cd\u7279\u6280\u52a8\u4f5c\u3002", "conclusion": "PbRL\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u76ee\u6807\uff0c\u5728\u7269\u7406\u548c\u4eff\u771f\u9886\u57df\u5747\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.18820", "pdf": "https://arxiv.org/pdf/2508.18820", "abs": "https://arxiv.org/abs/2508.18820", "authors": ["Christian Henkel", "Marco Lampacrescia", "Michaela Klauck", "Matteo Morelli"], "title": "AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy", "categories": ["cs.RO", "cs.FL"], "comment": "Accepted at IROS2025", "summary": "Designing robotic systems to act autonomously in unforeseen environments is a\nchallenging task. This work presents a novel approach to use formal\nverification, specifically Statistical Model Checking (SMC), to verify system\nproperties of autonomous robots at design-time. We introduce an extension of\nthe SCXML format, designed to model system components including both Robot\nOperating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we\ncontribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the\nfull system model into JANI. The use of JANI, a standard format for\nquantitative model checking, enables verification of system properties with\noff-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both\nin terms of applicability to real-world autonomous robotic control systems, and\nin terms of verification runtime scaling. We provide a case study, where we\nsuccessfully identify problems in a ROS 2-based robotic manipulation use case\nthat is verifiable in less than one second using consumer hardware.\nAdditionally, we compare to the state of the art and demonstrate that our\nmethod is more comprehensive in system feature support, and that the\nverification runtime scales linearly with the size of the model, instead of\nexponentially.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u68c0\u67e5\uff08SMC\uff09\u5728\u8bbe\u8ba1\u65f6\u9a8c\u8bc1\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5c5e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55SCXML\u683c\u5f0f\u5e76\u5f00\u53d1AS2FM\u5de5\u5177\uff0c\u5c06\u7cfb\u7edf\u6a21\u578b\u8f6c\u6362\u4e3aJANI\u683c\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u884c\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u662f\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\uff0c\u9700\u8981\u9a8c\u8bc1\u7cfb\u7edf\u5c5e\u6027\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u6269\u5c55SCXML\u683c\u5f0f\u4ee5\u5efa\u6a21ROS 2\u548c\u884c\u4e3a\u6811\u7279\u5f81\uff0c\u5e76\u5f00\u53d1AS2FM\u5de5\u5177\u5c06\u7cfb\u7edf\u6a21\u578b\u8f6c\u6362\u4e3aJANI\u683c\u5f0f\uff0c\u5229\u7528SMC\u5de5\u5177\u9a8c\u8bc1\u7cfb\u7edf\u5c5e\u6027\u3002", "result": "\u5728ROS 2\u673a\u5668\u4eba\u64cd\u63a7\u6848\u4f8b\u4e2d\uff0c\u6210\u529f\u8bc6\u522b\u95ee\u9898\u4e14\u9a8c\u8bc1\u65f6\u95f4\u5c0f\u4e8e\u4e00\u79d2\uff0c\u4e14\u9a8c\u8bc1\u8fd0\u884c\u65f6\u95f4\u968f\u6a21\u578b\u89c4\u6a21\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cfb\u7edf\u7279\u6027\u652f\u6301\u548c\u9a8c\u8bc1\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u3002"}}
{"id": "2508.18937", "pdf": "https://arxiv.org/pdf/2508.18937", "abs": "https://arxiv.org/abs/2508.18937", "authors": ["Wang Jiayin", "Wei Yanran", "Jiang Lei", "Guo Xiaoyu", "Zheng Ayong", "Zhao Weidong", "Li Zhongkui"], "title": "VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "Autonomous control of the laparoscope in robot-assisted Minimally Invasive\nSurgery (MIS) has received considerable research interest due to its potential\nto improve surgical safety. Despite progress in pixel-level Image-Based Visual\nServoing (IBVS) control, the requirement of continuous visibility and the\nexistence of complex disturbances, such as parameterization error, measurement\nnoise, and uncertainties of payloads, could degrade the surgeon's visual\nexperience and compromise procedural safety. To address these limitations, this\npaper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and\nuncertainty-adaptive framework for autonomous laparoscope control that\nguarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian\nProcess Regression (GPR) is utilized to perform hybrid (deterministic +\nstochastic) quantification of operational uncertainties including residual\nmodel uncertainties, stochastic uncertainties, and external disturbances. Based\non uncertainty quantification, a novel safety aware trajectory optimization\nframework with probabilistic guarantees is proposed, where a\nuncertainty-adaptive safety Control Barrier Function (CBF) condition is given\nbased on uncertainty propagation, and chance constraints are simultaneously\nformulated based on probabilistic approximation. This uncertainty aware\nformulation enables adaptive control effort allocation, minimizing unnecessary\ncamera motion while maintaining robustness. The proposed method is validated\nthrough comparative simulations and experiments on a commercial surgical robot\nplatform (MicroPort MedBot Toumai) performing a sequential multi-target lymph\nnode dissection. Compared with baseline methods, the framework maintains\nnear-perfect target visibility (>99.9%), reduces tracking e", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u9884\u6d4b\u63a7\u5236\uff08VPC\uff09\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u8179\u8154\u955c\u63a7\u5236\uff0c\u786e\u4fdd\u89c6\u91ce\u5b89\u5168\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\uff08IBVS\uff09\u4e2d\u56e0\u590d\u6742\u5e72\u6270\uff08\u5982\u53c2\u6570\u5316\u8bef\u5dee\u3001\u6d4b\u91cf\u566a\u58f0\u7b49\uff09\u5bfc\u81f4\u7684\u5b89\u5168\u6027\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u5e26\u6982\u7387\u4fdd\u969c\u7684\u5b89\u5168\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u548c\u673a\u4f1a\u7ea6\u675f\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u76ee\u6807\u53ef\u89c1\u6027\uff08>99.9%\uff09\uff0c\u5e76\u51cf\u5c11\u4e86\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u4e3b\u8179\u8154\u955c\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u624b\u672f\u5b89\u5168\u6027\u3002"}}
{"id": "2508.18967", "pdf": "https://arxiv.org/pdf/2508.18967", "abs": "https://arxiv.org/abs/2508.18967", "authors": ["Hichem Cheriet", "Khellat Kihel Badra", "Chouraqui Samira"], "title": "Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for publication in JAMRIS Journal", "summary": "Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical\nfor various applications, including combat support, package delivery and Search\nand Rescue Operations. This paper introduces the Tangent Intersection Guidance\n(TIG) algorithm, an advanced approach for UAV path planning in both static and\ndynamic environments. The algorithm uses the elliptic tangent intersection\nmethod to generate feasible paths. It generates two sub-paths for each threat,\nselects the optimal route based on a heuristic rule, and iteratively refines\nthe path until the target is reached. Considering the UAV kinematic and dynamic\nconstraints, a modified smoothing technique based on quadratic B\\'ezier curves\nis adopted to generate a smooth and efficient route. Experimental results show\nthat the TIG algorithm can generate the shortest path in less time, starting\nfrom 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent\nGraph, and Static APPATT algorithms in static environments. Furthermore, in\ncompletely unknown and partially known environments, TIG demonstrates efficient\nreal-time path planning capabilities for collision avoidance, outperforming APF\nand Dynamic APPATT algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIG\u7b97\u6cd5\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\uff0c\u901a\u8fc7\u692d\u5706\u5207\u7ebf\u4ea4\u70b9\u751f\u6210\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u4e8c\u6b21\u8d1d\u585e\u5c14\u66f2\u7ebf\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u65e0\u4eba\u673a\u7684\u9ad8\u6548\u5b89\u5168\u5bfc\u822a\u5728\u4f5c\u6218\u652f\u63f4\u3001\u5305\u88f9\u6295\u9012\u548c\u641c\u6551\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u6700\u4f18\u8def\u5f84\u7684\u7b97\u6cd5\u3002", "method": "TIG\u7b97\u6cd5\u5229\u7528\u692d\u5706\u5207\u7ebf\u4ea4\u70b9\u65b9\u6cd5\u751f\u6210\u521d\u6b65\u8def\u5f84\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u89c4\u5219\u9009\u62e9\u6700\u4f18\u5b50\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u4e8c\u6b21\u8d1d\u585e\u5c14\u66f2\u7ebf\u4f18\u5316\u8def\u5f84\u5e73\u6ed1\u6027\u3002", "result": "TIG\u7b97\u6cd5\u5728\u9759\u6001\u73af\u5883\u4e2d\u751f\u6210\u6700\u77ed\u8def\u5f84\u65f6\u95f4\u4f4e\u81f30.01\u79d2\uff0c\u4e14\u8f6c\u5411\u89d2\u5ea6\u66f4\u5c11\uff1b\u5728\u672a\u77e5\u6216\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u5b9e\u65f6\u907f\u969c\u6027\u80fd\u4f18\u4e8eAPF\u548cDynamic APPATT\u7b97\u6cd5\u3002", "conclusion": "TIG\u7b97\u6cd5\u5728\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u65e0\u4eba\u673a\u5bfc\u822a\u573a\u666f\u3002"}}
{"id": "2508.19002", "pdf": "https://arxiv.org/pdf/2508.19002", "abs": "https://arxiv.org/abs/2508.19002", "authors": ["Shipeng Lyu", "Fangyuan Wang", "Weiwei Lin", "Luhao Zhu", "David Navarro-Alarcon", "Guodong Guo"], "title": "HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots", "categories": ["cs.RO"], "comment": "8 pages, 8 figures,4 tables", "summary": "Achieving both behavioral similarity and appropriateness in human-like motion\ngeneration for humanoid robot remains an open challenge, further compounded by\nthe lack of cross-embodiment adaptability. To address this problem, we propose\nHuBE, a bi-level closed-loop framework that integrates robot state, goal poses,\nand contextual situations to generate human-like behaviors, ensuring both\nbehavioral similarity and appropriateness, and eliminating structural\nmismatches between motion generation and execution. To support this framework,\nwe construct HPose, a context-enriched dataset featuring fine-grained\nsituational annotations. Furthermore, we introduce a bone scaling-based data\naugmentation strategy that ensures millimeter-level compatibility across\nheterogeneous humanoid robots. Comprehensive evaluations on multiple commercial\nplatforms demonstrate that HuBE significantly improves motion similarity,\nbehavioral appropriateness, and computational efficiency over state-of-the-art\nbaselines, establishing a solid foundation for transferable and human-like\nbehavior execution across diverse humanoid robots.", "AI": {"tldr": "HuBE\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u95ed\u73af\u8bbe\u8ba1\u751f\u6210\u62df\u4eba\u52a8\u4f5c\uff0c\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u884c\u4e3a\u76f8\u4f3c\u6027\u548c\u9002\u5f53\u6027\u6311\u6218\uff0c\u5e76\u63d0\u5347\u8de8\u673a\u5668\u4eba\u517c\u5bb9\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u4e2d\u884c\u4e3a\u76f8\u4f3c\u6027\u4e0e\u9002\u5f53\u6027\u4e0d\u8db3\u53ca\u8de8\u673a\u5668\u4eba\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHuBE\u53cc\u5c42\u95ed\u73af\u6846\u67b6\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u76ee\u6807\u4f4d\u59ff\u548c\u4e0a\u4e0b\u6587\uff0c\u5e76\u6784\u5efaHPose\u6570\u636e\u96c6\u548c\u9aa8\u9abc\u7f29\u653e\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728\u591a\u5e73\u53f0\u4e0a\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u76f8\u4f3c\u6027\u3001\u884c\u4e3a\u9002\u5f53\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HuBE\u4e3a\u8de8\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53ef\u8fc1\u79fb\u62df\u4eba\u884c\u4e3a\u6267\u884c\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.19074", "pdf": "https://arxiv.org/pdf/2508.19074", "abs": "https://arxiv.org/abs/2508.19074", "authors": ["ZhenDong Chen", "ZhanShang Nie", "ShiXing Wan", "JunYi Li", "YongTian Cheng", "Shuai Zhao"], "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees", "categories": ["cs.RO", "cs.AI", "cs.PL"], "comment": null, "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u7136\u8bed\u8a00\u4e0e\u673a\u5668\u4eba\u8bed\u8a00\u7ffb\u8bd1\u6846\u67b6\uff08NRTrans\uff09\uff0c\u901a\u8fc7\u9a8c\u8bc1\u548c\u53cd\u9988\u673a\u5236\u63d0\u9ad8LLM\u751f\u6210\u673a\u5668\u4eba\u63a7\u5236\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7LLM\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u867d\u7136\u5229\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u76f4\u63a5\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u4f46\u7531\u4e8eLLM\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u7684\u9ad8\u590d\u6742\u6027\uff0c\u751f\u6210\u7684\u4ee3\u7801\u5e38\u5b58\u5728\u5927\u91cf\u9519\u8bef\uff0c\u5c24\u5176\u662f\u8f7b\u91cf\u7ea7LLM\u6548\u679c\u66f4\u5dee\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u6280\u80fd\u8bed\u8a00\uff08RSL\uff09\u6765\u62bd\u8c61\u63a7\u5236\u7a0b\u5e8f\u7684\u590d\u6742\u7ec6\u8282\uff0c\u5e76\u6784\u5efa\u4e86RSL\u7f16\u8bd1\u5668\u548c\u8c03\u8bd5\u5668\uff0c\u7528\u4e8e\u9a8c\u8bc1LLM\u751f\u6210\u7684\u7a0b\u5e8f\u5e76\u63d0\u4f9b\u9519\u8bef\u53cd\u9988\uff0c\u901a\u8fc7\u53cd\u9988\u5fae\u8c03\u4f18\u5316LLM\u7684\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNRTrans\u5728\u591a\u79cdLLM\u548c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u8f7b\u91cf\u7ea7LLM\u7684\u6210\u529f\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "NRTrans\u6846\u67b6\u901a\u8fc7\u9a8c\u8bc1\u548c\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u673a\u5668\u4eba\u63a7\u5236\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u8f7b\u91cf\u7ea7LLM\u4e0a\u7684\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.19114", "pdf": "https://arxiv.org/pdf/2508.19114", "abs": "https://arxiv.org/abs/2508.19114", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Alexander Derrico", "Philip Dames"], "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning", "categories": ["cs.RO", "cs.MA"], "comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)", "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.", "AI": {"tldr": "DELIVER\u662f\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7a7a\u95f4\u5206\u89e3\u3001\u4e2d\u7ee7\u89c4\u5212\u548c\u8fd0\u52a8\u6267\u884c\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65e0\u78b0\u649e\u534f\u8c03\u3002", "motivation": "\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u534f\u4f5c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u51cf\u5c11\u5355\u673a\u7cfb\u7edf\u7684\u5de5\u4f5c\u91cf\u3002", "method": "\u4f7f\u7528LLaMA3\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u57fa\u4e8eVoronoi\u56fe\u5212\u5206\u7a7a\u95f4\uff0c\u8ba1\u7b97\u6700\u4f18\u4e2d\u7ee7\u70b9\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u72b6\u6001\u673a\u63a7\u5236\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u786c\u4ef6\u6d4b\u8bd5\u4e2d\uff0cDELIVER\u4fdd\u6301\u4e86\u4efb\u52a1\u6210\u672c\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u51cf\u5c11\u4e8655%\u7684\u5355\u673a\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DELIVER\u7684\u6a21\u5757\u5316\u67b6\u6784\u5728\u8bed\u8a00\u5f15\u5bfc\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u63a8\u52a8\u4e86\u7269\u7406\u7cfb\u7edf\u7684\u96c6\u6210\u524d\u6cbf\u3002"}}
{"id": "2508.19131", "pdf": "https://arxiv.org/pdf/2508.19131", "abs": "https://arxiv.org/abs/2508.19131", "authors": ["Shreya Gummadi", "Mateus V. Gasparino", "Gianluca Capezzuto", "Marcelo Becker", "Girish Chowdhary"], "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.", "AI": {"tldr": "ZeST \u662f\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u5b9e\u65f6\u751f\u6210\u5730\u5f62\u53ef\u7a7f\u8d8a\u6027\u5730\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u673a\u5668\u4eba\u66b4\u9732\u4e8e\u5371\u9669\u73af\u5883\u4e2d\u7684\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5c06\u673a\u5668\u4eba\u7f6e\u4e8e\u5371\u9669\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5b58\u5728\u8bbe\u5907\u548c\u5b89\u5168\u9690\u60a3\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u96f6\u6837\u672c\u751f\u6210\u53ef\u7a7f\u8d8a\u6027\u5730\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cZeST \u5728\u5ba4\u5185\u548c\u5ba4\u5916\u73af\u5883\u4e2d\u5747\u80fd\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u5bfc\u822a\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ZeST \u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u52a0\u901f\u9ad8\u7ea7\u5bfc\u822a\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.19150", "pdf": "https://arxiv.org/pdf/2508.19150", "abs": "https://arxiv.org/abs/2508.19150", "authors": ["Juan Carlos Sabor\u00edo", "Marc Vinci", "Oscar Lima", "Sebastian Stock", "Lennart Niecksch", "Martin G\u00fcnther", "Alexander Sung", "Joachim Hertzberg", "Martin Atzm\u00fcller"], "title": "Uncertainty-Resilient Active Intention Recognition for Robotic Assistants", "categories": ["cs.RO", "cs.AI"], "comment": "(To appear) In Proceedings of ECMR 2025", "summary": "Purposeful behavior in robotic assistants requires the integration of\nmultiple components and technological advances. Often, the problem is reduced\nto recognizing explicit prompts, which limits autonomy, or is oversimplified\nthrough assumptions such as near-perfect information. We argue that a critical\ngap remains unaddressed -- specifically, the challenge of reasoning about the\nuncertain outcomes and perception errors inherent to human intention\nrecognition. In response, we present a framework designed to be resilient to\nuncertainty and sensor noise, integrating real-time sensor data with a\ncombination of planners. Centered around an intention-recognition POMDP, our\napproach addresses cooperative planning and acting under uncertainty. Our\nintegrated framework has been successfully tested on a physical robot with\npromising results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u5e94\u5bf9\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\u548c\u4f20\u611f\u5668\u566a\u58f0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u610f\u56fe\u8bc6\u522b\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u52a9\u624b\u7684\u884c\u4e3a\u5f80\u5f80\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\u6216\u5047\u8bbe\u5b8c\u7f8e\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u610f\u56fe\u8bc6\u522b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u611f\u77e5\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u610f\u56fe\u8bc6\u522b\u7684POMDP\uff08\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff09\uff0c\u7ed3\u5408\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u548c\u591a\u79cd\u89c4\u5212\u5668\uff0c\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u6846\u67b6\u3002", "result": "\u6846\u67b6\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\u6210\u529f\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u610f\u56fe\u8bc6\u522b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.19153", "pdf": "https://arxiv.org/pdf/2508.19153", "abs": "https://arxiv.org/abs/2508.19153", "authors": ["Allen Wang", "Gavin Tao"], "title": "QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning", "categories": ["cs.RO"], "comment": "14pages, 9 figures, Journal paper", "summary": "We address vision-guided quadruped motion control with reinforcement learning\n(RL) and highlight the necessity of combining proprioception with vision for\nrobust control. We propose QuadKAN, a spline-parameterized cross-modal policy\ninstantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates\na spline encoder for proprioception and a spline fusion head for\nproprioception-vision inputs. This structured function class aligns the\nstate-to-action mapping with the piecewise-smooth nature of gait, improving\nsample efficiency, reducing action jitter and energy consumption, and providing\ninterpretable posture-action sensitivities. We adopt Multi-Modal Delay\nRandomization (MMDR) and perform end-to-end training with Proximal Policy\nOptimization (PPO). Evaluations across diverse terrains, including both even\nand uneven surfaces and scenarios with static or dynamic obstacles, demonstrate\nthat QuadKAN achieves consistently higher returns, greater distances, and fewer\ncollisions than state-of-the-art (SOTA) baselines. These results show that\nspline-parameterized policies offer a simple, effective, and interpretable\nalternative for robust vision-guided locomotion. A repository will be made\navailable upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9\u5f15\u5bfc\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5QuadKAN\uff0c\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\uff0c\u4f7f\u7528\u6837\u6761\u53c2\u6570\u5316\u7b56\u7565\u63d0\u5347\u7a33\u5065\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u5f15\u5bfc\u7684\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u5bf9\u4e8e\u9c81\u68d2\u63a7\u5236\u7684\u5fc5\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86QuadKAN\u6846\u67b6\uff0c\u57fa\u4e8e\u6837\u6761\u53c2\u6570\u5316\u7684\u8de8\u6a21\u6001\u7b56\u7565\uff0c\u5229\u7528Kolmogorov-Arnold Networks (KANs)\u5b9e\u73b0\uff0c\u5305\u62ec\u6837\u6761\u7f16\u7801\u5668\u548c\u878d\u5408\u5934\uff0c\u91c7\u7528Multi-Modal Delay Randomization (MMDR)\u548cPPO\u7b97\u6cd5\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u6837\u5730\u5f62\u548c\u969c\u788d\u7269\u573a\u666f\u4e0b\uff0cQuadKAN\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u56de\u62a5\u3001\u66f4\u8fdc\u7684\u79fb\u52a8\u8ddd\u79bb\u548c\u66f4\u5c11\u7684\u78b0\u649e\u3002", "conclusion": "\u6837\u6761\u53c2\u6570\u5316\u7b56\u7565\u4e3a\u89c6\u89c9\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19164", "pdf": "https://arxiv.org/pdf/2508.19164", "abs": "https://arxiv.org/abs/2508.19164", "authors": ["Morokot Sakal", "George Nehma", "Camilo Riano-Rios", "Madhur Tiwari"], "title": "Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "15 pages, 10 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite\nattitude control system with reaction wheel health estimation capabilities.\nPrevious simulations and Software-in-the-Loop testing have prompted further\nexperiments to explore the validity of the controller with real momentum\nexchange devices in the loop. This work is a step toward a comprehensive\ntesting framework for validation of spacecraft attitude control algorithms. The\nproposed HIL testbed includes brushless DC motors and drivers that communicate\nusing a CAN bus, an embedded computer that executes control and adaptation\nlaws, and a satellite simulator that produces simulated sensor data, estimated\nattitude states, and responds to actions of the external actuators. We propose\nmethods to artificially induce failures on the reaction wheels, and present\nrelated issues and lessons learned.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u53cd\u4f5c\u7528\u8f6e\u5065\u5eb7\u4f30\u8ba1\u80fd\u529b\u7684\u81ea\u9002\u5e94\u536b\u661f\u59ff\u6001\u63a7\u5236\u7cfb\u7edf\u7684\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u9a8c\u8bc1\u63a7\u5236\u5668\u5728\u5b9e\u9645\u52a8\u91cf\u4ea4\u6362\u8bbe\u5907\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u901a\u8fc7HIL\u6d4b\u8bd5\u9a8c\u8bc1\u536b\u661f\u59ff\u6001\u63a7\u5236\u7b97\u6cd5\u5728\u5b9e\u9645\u786c\u4ef6\u4e2d\u7684\u8868\u73b0\uff0c\u5f25\u8865\u4eff\u771f\u548c\u8f6f\u4ef6\u5728\u73af\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "method": "\u6d4b\u8bd5\u5e73\u53f0\u5305\u62ec\u65e0\u5237\u76f4\u6d41\u7535\u673a\u548c\u9a71\u52a8\u5668\uff08\u901a\u8fc7CAN\u603b\u7ebf\u901a\u4fe1\uff09\u3001\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\uff08\u6267\u884c\u63a7\u5236\u548c\u81ea\u9002\u5e94\u5f8b\uff09\u4ee5\u53ca\u536b\u661f\u6a21\u62df\u5668\uff08\u751f\u6210\u4f20\u611f\u5668\u6570\u636e\u548c\u54cd\u5e94\u5916\u90e8\u6267\u884c\u5668\u52a8\u4f5c\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4eba\u4e3a\u8bf1\u5bfc\u53cd\u4f5c\u7528\u8f6e\u6545\u969c\u7684\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u95ee\u9898\u548c\u7ecf\u9a8c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u822a\u5929\u5668\u59ff\u6001\u63a7\u5236\u7b97\u6cd5\u7684\u5168\u9762\u9a8c\u8bc1\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.19168", "pdf": "https://arxiv.org/pdf/2508.19168", "abs": "https://arxiv.org/abs/2508.19168", "authors": ["Liding Zhang", "Kejia Chen", "Kuanqi Cai", "Yu Zhang", "Yixuan Dang", "Yansong Wu", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic", "categories": ["cs.RO"], "comment": "7 pages, 5 figures, 2025 IEEE International Conference on Robotics\n  and Automation (ICRA)", "summary": "Optimal path planning requires finding a series of feasible states from the\nstarting point to the goal to optimize objectives. Popular path planning\nalgorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to\nguide the search. Effective heuristics are accurate and computationally\nefficient, but achieving both can be challenging due to their conflicting\nnature. This paper proposes Direction Informed Trees (DIT*), a sampling-based\nplanner that focuses on optimizing the search direction for each edge,\nresulting in goal bias during exploration. We define edges as generalized\nvectors and integrate similarity indexes to establish a directional filter that\nselects the nearest neighbors and estimates direction costs. The estimated\ndirection cost heuristics are utilized in edge evaluation. This strategy allows\nthe exploration to share directional information efficiently. DIT* convergence\nfaster than existing single-query, sampling-based planners on tested problems\nin R^4 to R^16 and has been demonstrated in real-world environments with\nvarious planning tasks. A video showcasing our experimental results is\navailable at: https://youtu.be/2SX6QT2NOek", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIT*\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u641c\u7d22\u65b9\u5411\u548c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff08\u5982EIT*\uff09\u5728\u542f\u53d1\u5f0f\u641c\u7d22\u4e2d\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DIT*\u901a\u8fc7\u5c06\u8fb9\u5b9a\u4e49\u4e3a\u5e7f\u4e49\u5411\u91cf\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u7d22\u5f15\uff0c\u5efa\u7acb\u65b9\u5411\u8fc7\u6ee4\u5668\uff0c\u4ee5\u9009\u62e9\u8fd1\u90bb\u548c\u4f30\u8ba1\u65b9\u5411\u6210\u672c\uff0c\u4ece\u800c\u4f18\u5316\u63a2\u7d22\u65b9\u5411\u3002", "result": "DIT*\u5728R^4\u5230R^16\u7684\u6d4b\u8bd5\u95ee\u9898\u4e2d\u6536\u655b\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u5355\u67e5\u8be2\u91c7\u6837\u89c4\u5212\u5668\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DIT*\u901a\u8fc7\u65b9\u5411\u4fe1\u606f\u7684\u6709\u6548\u5171\u4eab\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u89c4\u5212\u4efb\u52a1\u3002"}}
{"id": "2508.19172", "pdf": "https://arxiv.org/pdf/2508.19172", "abs": "https://arxiv.org/abs/2508.19172", "authors": ["Luca Grillotti", "Lisa Coiffard", "Oscar Pang", "Maxence Faldor", "Antoine Cully"], "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted at CoRL 2025", "summary": "Autonomous skill discovery aims to enable robots to acquire diverse behaviors\nwithout explicit supervision. Learning such behaviors directly on physical\nhardware remains challenging due to safety and data efficiency constraints.\nExisting methods, including Quality-Diversity Actor-Critic (QDAC), require\nmanually defined skill spaces and carefully tuned heuristics, limiting\nreal-world applicability. We propose Unsupervised Real-world Skill Acquisition\n(URSA), an extension of QDAC that enables robots to autonomously discover and\nmaster diverse, high-performing skills directly in the real world. We\ndemonstrate that URSA successfully discovers diverse locomotion skills on a\nUnitree A1 quadruped in both simulation and the real world. Our approach\nsupports both heuristic-driven skill discovery and fully unsupervised settings.\nWe also show that the learned skill repertoire can be reused for downstream\ntasks such as real-world damage adaptation, where URSA outperforms all\nbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.\nOur results establish a new framework for real-world robot learning that\nenables continuous skill discovery with limited human intervention,\nrepresenting a significant step toward more autonomous and adaptable robotic\nsystems. Demonstration videos are available at\nhttp://adaptive-intelligent-robotics.github.io/URSA .", "AI": {"tldr": "URSA\u901a\u8fc7\u81ea\u4e3b\u53d1\u73b0\u548c\u638c\u63e1\u591a\u6837\u5316\u7684\u6280\u80fd\uff0c\u76f4\u63a5\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u6280\u80fd\u53d1\u73b0\u4e2d\u624b\u52a8\u5b9a\u4e49\u6280\u80fd\u7a7a\u95f4\u548c\u8c03\u6574\u542f\u53d1\u5f0f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8eQDAC\u6269\u5c55\u7684URSA\u6846\u67b6\uff0c\u652f\u6301\u542f\u53d1\u5f0f\u9a71\u52a8\u548c\u5b8c\u5168\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u7684\u6280\u80fd\u53d1\u73b0\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5b9e\u73b0\u591a\u6837\u5316\u8fd0\u52a8\u6280\u80fd\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "URSA\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u65b0\u6846\u67b6\uff0c\u51cf\u5c11\u4eba\u4e3a\u5e72\u9884\uff0c\u8fc8\u5411\u66f4\u81ea\u4e3b\u9002\u5e94\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.19186", "pdf": "https://arxiv.org/pdf/2508.19186", "abs": "https://arxiv.org/abs/2508.19186", "authors": ["Christopher Chandler", "Bernd Porr", "Giulia Lafratta", "Alice Miller"], "title": "Real-Time Model Checking for Closed-Loop Robot Reactive Planning", "categories": ["cs.RO", "cs.AI", "cs.FL", "I.2.9; I.2; D.2.4"], "comment": "30 pages excluding references, 18 figures, submitted to Formal\n  Aspects of Computing", "summary": "We present a new application of model checking which achieves real-time\nmulti-step planning and obstacle avoidance on a real autonomous robot. We have\ndeveloped a small, purpose-built model checking algorithm which generates plans\nin situ based on \"core\" knowledge and attention as found in biological agents.\nThis is achieved in real-time using no pre-computed data on a low-powered\ndevice. Our approach is based on chaining temporary control systems which are\nspawned to counteract disturbances in the local environment that disrupt an\nautonomous agent from its preferred action (or resting state). A novel\ndiscretization of 2D LiDAR data sensitive to bounded variations in the local\nenvironment is used. Multi-step planning using model checking by forward\ndepth-first search is applied to cul-de-sac and playground scenarios. Both\nempirical results and informal proofs of two fundamental properties of our\napproach demonstrate that model checking can be used to create efficient\nmulti-step plans for local obstacle avoidance, improving on the performance of\na reactive agent which can only plan one step. Our approach is an instructional\ncase study for the development of safe, reliable and explainable planning in\nthe context of autonomous vehicles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u68c0\u67e5\u7684\u5b9e\u65f6\u591a\u6b65\u89c4\u5212\u4e0e\u907f\u969c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u8bbe\u5907\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u6b65\u89c4\u5212\u7684\u53cd\u5e94\u5f0f\u4ee3\u7406\u3002", "motivation": "\u65e8\u5728\u5229\u7528\u751f\u7269\u667a\u80fd\u4e2d\u7684\u6838\u5fc3\u77e5\u8bc6\u4e0e\u6ce8\u610f\u529b\uff0c\u53d1\u5c55\u4e00\u79cd\u65e0\u9700\u9884\u8ba1\u7b97\u6570\u636e\u7684\u5b9e\u65f6\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5c40\u90e8\u907f\u969c\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u68c0\u67e5\u7b97\u6cd5\uff0c\u901a\u8fc7\u94fe\u5f0f\u4e34\u65f6\u63a7\u5236\u7cfb\u7edf\u5e94\u5bf9\u73af\u5883\u6270\u52a8\uff0c\u5e76\u7ed3\u54082D LiDAR\u6570\u636e\u7684\u79bb\u6563\u5316\u4e0e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u5b9e\u73b0\u591a\u6b65\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u5c40\u90e8\u907f\u969c\u591a\u6b65\u8ba1\u5212\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u6b65\u89c4\u5212\u7684\u53cd\u5e94\u5f0f\u4ee3\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5b89\u5168\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u89c4\u5212\u63d0\u4f9b\u4e86\u793a\u4f8b\u548c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.19191", "pdf": "https://arxiv.org/pdf/2508.19191", "abs": "https://arxiv.org/abs/2508.19191", "authors": ["Yue Wang", "Wenjie Deng", "Haotian Xue", "Di Cui", "Yiqi Chen", "Mingchuan Zhou", "Haochao Ying", "Jian Wu"], "title": "AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot", "categories": ["cs.RO"], "comment": null, "summary": "Intraocular foreign body removal demands millimeter-level precision in\nconfined intraocular spaces, yet existing robotic systems predominantly rely on\nmanual teleoperation with steep learning curves. To address the challenges of\nautonomous manipulation (particularly kinematic uncertainties from variable\nmotion scaling and variation of the Remote Center of Motion (RCM) point), we\npropose AutoRing, an imitation learning framework for autonomous intraocular\nforeign body ring manipulation. Our approach integrates dynamic RCM calibration\nto resolve coordinate-system inconsistencies caused by intraocular instrument\nvariation and introduces the RCM-ACT architecture, which combines\naction-chunking transformers with real-time kinematic realignment. Trained\nsolely on stereo visual data and instrument kinematics from expert\ndemonstrations in a biomimetic eye model, AutoRing successfully completes ring\ngrasping and positioning tasks without explicit depth sensing. Experimental\nvalidation demonstrates end-to-end autonomy under uncalibrated microscopy\nconditions. The results provide a viable framework for developing intelligent\neye-surgical systems capable of complex intraocular procedures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u81ea\u4e3b\u773c\u5916\u79d1\u624b\u672f\u7cfb\u7edfAutoRing\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u624b\u52a8\u64cd\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u65e0\u6821\u51c6\u6761\u4ef6\u4e0b\u7684\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u773c\u5916\u79d1\u624b\u672f\u673a\u5668\u4eba\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u4e14\u5b58\u5728\u8fd0\u52a8\u7f29\u653e\u548c\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u70b9\uff08RCM\uff09\u53d8\u5316\u7684\u8fd0\u52a8\u5b66\u4e0d\u786e\u5b9a\u6027\uff0c\u4e9f\u9700\u81ea\u4e3b\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u6846\u67b6AutoRing\uff0c\u7ed3\u5408\u52a8\u6001RCM\u6821\u51c6\u548cRCM-ACT\u67b6\u6784\uff08\u52a8\u4f5c\u5206\u5757\u53d8\u6362\u5668\u4e0e\u5b9e\u65f6\u8fd0\u52a8\u5b66\u5bf9\u9f50\uff09\uff0c\u4ec5\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u6570\u636e\u548c\u4e13\u5bb6\u793a\u8303\u8bad\u7ec3\u5b9e\u73b0\u4efb\u52a1\u3002", "result": "AutoRing\u5728\u751f\u7269\u4eff\u751f\u773c\u6a21\u578b\u4e2d\u6210\u529f\u5b8c\u6210\u73af\u72b6\u7269\u6293\u53d6\u4e0e\u5b9a\u4f4d\u4efb\u52a1\uff0c\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u611f\u77e5\uff0c\u4e14\u5728\u672a\u6821\u51c6\u663e\u5fae\u955c\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u7aef\u5230\u7aef\u81ea\u4e3b\u6027\u3002", "conclusion": "AutoRing\u4e3a\u590d\u6742\u773c\u5916\u79d1\u624b\u672f\u7684\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u81ea\u4e3b\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19199", "pdf": "https://arxiv.org/pdf/2508.19199", "abs": "https://arxiv.org/abs/2508.19199", "authors": ["Alex LaGrassa", "Zixuan Huang", "Dmitry Berenson", "Oliver Kroemer"], "title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 7 figures", "summary": "Efficient planning in high-dimensional spaces, such as those involving\ndeformable objects, requires computationally tractable yet sufficiently\nexpressive dynamics models. This paper introduces a method that automatically\ngenerates task-specific, spatially adaptive dynamics models by learning which\nregions of the object require high-resolution modeling to achieve good task\nperformance for a given planning query. Task performance depends on the complex\ninterplay between the dynamics model, world dynamics, control, and task\nrequirements. Our proposed diffusion-based model generator predicts per-region\nmodel resolutions based on start and goal pointclouds that define the planning\nquery. To efficiently collect the data for learning this mapping, a two-stage\nprocess optimizes resolution using predictive dynamics as a prior before\ndirectly optimizing using closed-loop performance. On a tree-manipulation task,\nour method doubles planning speed with only a small decrease in task\nperformance over using a full-resolution model. This approach informs a path\ntowards using previous planning and control data to generate computationally\nefficient yet sufficiently expressive dynamics models for new tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u3001\u7a7a\u95f4\u81ea\u9002\u5e94\u52a8\u529b\u5b66\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0d\u540c\u533a\u57df\u7684\u5206\u8fa8\u7387\u9700\u6c42\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u9ad8\u7ef4\u5ea6\u7a7a\u95f4\uff08\u5982\u6d89\u53ca\u53ef\u53d8\u5f62\u7269\u4f53\uff09\u7684\u9ad8\u6548\u89c4\u5212\u9700\u8981\u8ba1\u7b97\u4e0a\u53ef\u5904\u7406\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u751f\u6210\u5668\uff0c\u6839\u636e\u89c4\u5212\u67e5\u8be2\u7684\u8d77\u70b9\u548c\u76ee\u6807\u70b9\u4e91\u9884\u6d4b\u5404\u533a\u57df\u7684\u5206\u8fa8\u7387\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4f18\u5316\u5206\u8fa8\u7387\u3002", "result": "\u5728\u6811\u6728\u64cd\u7eb5\u4efb\u52a1\u4e2d\uff0c\u89c4\u5212\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e00\u500d\uff0c\u4efb\u52a1\u6027\u80fd\u4ec5\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5229\u7528\u5386\u53f2\u89c4\u5212\u6570\u636e\u751f\u6210\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u52a8\u529b\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2508.19236", "pdf": "https://arxiv.org/pdf/2508.19236", "abs": "https://arxiv.org/abs/2508.19236", "authors": ["Hao Shi", "Bin Xie", "Yingfei Liu", "Lin Sun", "Fengrong Liu", "Tiancai Wang", "Erjin Zhou", "Haoqiang Fan", "Xiangyu Zhang", "Gao Huang"], "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "The project is available at https://shihao1895.github.io/MemoryVLA", "summary": "Temporal context is essential for robotic manipulation because such tasks are\ninherently non-Markovian, yet mainstream VLA models typically overlook it and\nstruggle with long-horizon, temporally dependent tasks. Cognitive science\nsuggests that humans rely on working memory to buffer short-lived\nrepresentations for immediate control, while the hippocampal system preserves\nverbatim episodic details and semantic gist of past experience for long-term\nmemory. Inspired by these mechanisms, we propose MemoryVLA, a\nCognition-Memory-Action framework for long-horizon robotic manipulation. A\npretrained VLM encodes the observation into perceptual and cognitive tokens\nthat form working memory, while a Perceptual-Cognitive Memory Bank stores\nlow-level details and high-level semantics consolidated from it. Working memory\nretrieves decision-relevant entries from the bank, adaptively fuses them with\ncurrent tokens, and updates the bank by merging redundancies. Using these\ntokens, a memory-conditioned diffusion action expert yields temporally aware\naction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks\nacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it\nachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming\nstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on\nBridge. On 12 real-world tasks spanning general skills and long-horizon\ntemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon\ntasks showing a +26 improvement over state-of-the-art baseline. Project Page:\nhttps://shihao1895.github.io/MemoryVLA", "AI": {"tldr": "MemoryVLA\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\u542f\u53d1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u4f5c\u8bb0\u5fc6\u548c\u611f\u77e5-\u8ba4\u77e5\u8bb0\u5fc6\u5e93\u5904\u7406\u957f\u671f\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5f80\u5f80\u5ffd\u89c6\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u5904\u7406\u957f\u671f\u4f9d\u8d56\u4efb\u52a1\u3002\u501f\u9274\u4eba\u7c7b\u8bb0\u5fc6\u673a\u5236\uff08\u5de5\u4f5c\u8bb0\u5fc6\u548c\u6d77\u9a6c\u4f53\u7cfb\u7edf\uff09\uff0c\u63d0\u51faMemoryVLA\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u91c7\u7528Cognition-Memory-Action\u6846\u67b6\uff0c\u5de5\u4f5c\u8bb0\u5fc6\u7f13\u5b58\u77ed\u671f\u4fe1\u606f\uff0c\u8bb0\u5fc6\u5e93\u5b58\u50a8\u7ec6\u8282\u548c\u8bed\u4e49\uff0c\u81ea\u9002\u5e94\u878d\u5408\u5e76\u751f\u6210\u65f6\u95f4\u611f\u77e5\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8fbe\u523071.9%\u300172.7%\u548c96.5%\u7684\u6210\u529f\u7387\uff0c\u771f\u5b9e\u4efb\u52a1\u4e2d84%\u6210\u529f\u7387\uff0c\u957f\u671f\u4efb\u52a1\u63d0\u534726%\u3002", "conclusion": "MemoryVLA\u901a\u8fc7\u8bb0\u5fc6\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u957f\u671f\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18293", "pdf": "https://arxiv.org/pdf/2508.18293", "abs": "https://arxiv.org/abs/2508.18293", "authors": ["M. Salman Shaukat", "Yannik K\u00e4ckenmeister", "Sebastian Bader", "Thomas Kirste"], "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering\n  (IEEE-JOE)", "summary": "Underwater 3D object detection remains one of the most challenging frontiers\nin computer vision, where traditional approaches struggle with the harsh\nacoustic environment and scarcity of training data. While deep learning has\nrevolutionized terrestrial 3D detection, its application underwater faces a\ncritical bottleneck: obtaining sufficient annotated sonar data is prohibitively\nexpensive and logistically complex, often requiring specialized vessels, expert\nsurveyors, and favorable weather conditions. This work addresses a fundamental\nquestion: Can we achieve reliable underwater 3D object detection without\nreal-world training data? We tackle this challenge by developing and comparing\ntwo paradigms for training-free detection of artificial structures in multibeam\necho-sounder point clouds. Our dual approach combines a physics-based sonar\nsimulation pipeline that generates synthetic training data for state-of-the-art\nneural networks, with a robust model-based template matching system that\nleverages geometric priors of target objects. Evaluation on real bathymetry\nsurveys from the Baltic Sea reveals surprising insights: while neural networks\ntrained on synthetic data achieve 98% mean Average Precision (mAP) on simulated\nscenes, they drop to 40% mAP on real sonar data due to domain shift.\nConversely, our template matching approach maintains 83% mAP on real data\nwithout requiring any training, demonstrating remarkable robustness to acoustic\nnoise and environmental variations. Our findings challenge conventional wisdom\nabout data-hungry deep learning in underwater domains and establish the first\nlarge-scale benchmark for training-free underwater 3D detection. This work\nopens new possibilities for autonomous underwater vehicle navigation, marine\narchaeology, and offshore infrastructure monitoring in data-scarce environments\nwhere traditional machine learning approaches fail.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u65e0\u9700\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u6c34\u4e0b3D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u51e0\u4f55\u5148\u9a8c\u7684\u6a21\u677f\u5339\u914d\u7cfb\u7edf\u3002", "motivation": "\u6c34\u4e0b3D\u7269\u4f53\u68c0\u6d4b\u56e0\u590d\u6742\u7684\u58f0\u5b66\u73af\u5883\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u68c0\u6d4b\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u7269\u7406\u7684\u58f0\u7eb3\u6a21\u62df\u751f\u6210\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff1b2) \u5229\u7528\u76ee\u6807\u51e0\u4f55\u5148\u9a8c\u7684\u6a21\u578b\u6a21\u677f\u5339\u914d\u7cfb\u7edf\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62df\u6570\u636e\u4e0a\u8fbe\u523098%\u7684mAP\uff0c\u4f46\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u964d\u81f340%\uff1b\u6a21\u677f\u5339\u914d\u65e0\u9700\u8bad\u7ec3\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4fdd\u630183%\u7684mAP\uff0c\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5728\u6570\u636e\u7a00\u7f3a\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\uff0c\u6a21\u578b\u6a21\u677f\u5339\u914d\u4f18\u4e8e\u4f9d\u8d56\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4e3a\u6c34\u4e0b3D\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18729", "pdf": "https://arxiv.org/pdf/2508.18729", "abs": "https://arxiv.org/abs/2508.18729", "authors": ["Melanie Wille", "Tobias Fischer", "Scarlett Raine"], "title": "Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "10 pages", "summary": "Underwater object detection is critical for monitoring marine ecosystems but\nposes unique challenges, including degraded image quality, imbalanced class\ndistribution, and distinct visual characteristics. Not every species is\ndetected equally well, yet underlying causes remain unclear. We address two key\nresearch questions: 1) What factors beyond data quantity drive class-specific\nperformance disparities? 2) How can we systematically improve detection of\nunder-performing marine species? We manipulate the DUO dataset to separate the\nobject detection task into localization and classification and investigate the\nunder-performance of the scallop class. Localization analysis using YOLO11 and\nTIDE finds that foreground-background discrimination is the most problematic\nstage regardless of data quantity. Classification experiments reveal persistent\nprecision gaps even with balanced data, indicating intrinsic feature-based\nchallenges beyond data scarcity and inter-class dependencies. We recommend\nimbalanced distributions when prioritizing precision, and balanced\ndistributions when prioritizing recall. Improving under-performing classes\nshould focus on algorithmic advances, especially within localization modules.\nWe publicly release our code and datasets.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6c34\u4e0b\u7269\u4f53\u68c0\u6d4b\u4e2d\u6027\u80fd\u5dee\u5f02\u7684\u6210\u56e0\uff0c\u91cd\u70b9\u5173\u6ce8\u6247\u8d1d\u7c7b\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u7269\u4f53\u68c0\u6d4b\u4e2d\u67d0\u4e9b\u7269\u79cd\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u7684\u6839\u672c\u539f\u56e0\u53ca\u5982\u4f55\u7cfb\u7edf\u6027\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5206\u5272\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528YOLO11\u548cTIDE\u5206\u6790DUO\u6570\u636e\u96c6\uff0c\u7814\u7a76\u6247\u8d1d\u7c7b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5b9a\u4f4d\u9636\u6bb5\u7684\u524d\u666f-\u80cc\u666f\u533a\u5206\u95ee\u9898\u6700\u7a81\u51fa\uff0c\u5206\u7c7b\u5b9e\u9a8c\u663e\u793a\u7279\u5f81\u56fa\u6709\u6311\u6218\u3002\u5efa\u8bae\u9488\u5bf9\u4e0d\u540c\u76ee\u6807\u9009\u62e9\u6570\u636e\u5206\u5e03\u3002", "conclusion": "\u6539\u8fdb\u4f4e\u6548\u7c7b\u522b\u7684\u68c0\u6d4b\u9700\u805a\u7126\u7b97\u6cd5\u4f18\u5316\uff0c\u5c24\u5176\u662f\u5b9a\u4f4d\u6a21\u5757\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u3002"}}
{"id": "2508.18788", "pdf": "https://arxiv.org/pdf/2508.18788", "abs": "https://arxiv.org/abs/2508.18788", "authors": ["Christian L\u00f6wens", "Thorben Funke", "Jingchao Xie", "Alexandru Paul Condurache"], "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted at ICCV 2025", "summary": "Online mapping models show remarkable results in predicting vectorized maps\nfrom multi-view camera images only. However, all existing approaches still rely\non ground-truth high-definition maps during training, which are expensive to\nobtain and often not geographically diverse enough for reliable generalization.\nIn this work, we propose PseudoMapTrainer, a novel approach to online mapping\nthat uses pseudo-labels generated from unlabeled sensor data. We derive those\npseudo-labels by reconstructing the road surface from multi-camera imagery\nusing Gaussian splatting and semantics of a pre-trained 2D segmentation\nnetwork. In addition, we introduce a mask-aware assignment algorithm and loss\nfunction to handle partially masked pseudo-labels, allowing for the first time\nthe training of online mapping models without any ground-truth maps.\nFurthermore, our pseudo-labels can be effectively used to pre-train an online\nmodel in a semi-supervised manner to leverage large-scale unlabeled\ncrowdsourced data. The code is available at\ngithub.com/boschresearch/PseudoMapTrainer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9ad8\u7cbe\u5730\u56fe\u8bad\u7ec3\u5728\u7ebf\u5730\u56fe\u6a21\u578b\u7684\u65b0\u65b9\u6cd5PseudoMapTrainer\uff0c\u5229\u7528\u4f2a\u6807\u7b7e\u548c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u5730\u7406\u591a\u6837\u6027\u4e0d\u8db3\u7684\u9ad8\u7cbe\u5730\u56fe\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u9884\u8bad\u7ec3\u5206\u5272\u7f51\u7edc\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4f7f\u7528\u63a9\u7801\u611f\u77e5\u7684\u5206\u914d\u7b97\u6cd5\u548c\u635f\u5931\u51fd\u6570\u5904\u7406\u90e8\u5206\u63a9\u7801\u4f2a\u6807\u7b7e\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u771f\u5b9e\u5730\u56fe\u7684\u5728\u7ebf\u5730\u56fe\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u80fd\u5229\u7528\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u534a\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "conclusion": "PseudoMapTrainer\u4e3a\u5728\u7ebf\u5730\u56fe\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18898", "pdf": "https://arxiv.org/pdf/2508.18898", "abs": "https://arxiv.org/abs/2508.18898", "authors": ["Mona Mirzaie", "Bodo Rosenhahn"], "title": "Interpretable Decision-Making for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Accepted to the ICCV 2025 2nd Workshop on the Challenge Of\n  Out-of-Label Hazards in Autonomous Driving (2COOOL)", "summary": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u547d\u4ee4\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u548c\u5c40\u90e8\u5316\u7684\u7279\u5f81\u56fe\u4f18\u5316\u51b3\u7b56\uff0c\u4e14\u5728CARLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u53ef\u4fe1\u8d56\u7684AI\uff0c\u4f46\u7aef\u5230\u7aef\u6a21\u578b\u7684\u975e\u7ebf\u6027\u51b3\u7b56\u8fb9\u754c\u5bfc\u81f4\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u4ee5\u4f18\u5316\u63a7\u5236\u547d\u4ee4\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u751f\u6210\u7a00\u758f\u548c\u5c40\u90e8\u5316\u7684\u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u7279\u5f81\u6fc0\u6d3b\u89e3\u91ca\u9884\u6d4b\u7684\u63a7\u5236\u547d\u4ee4\u3002", "result": "\u5728CARLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u76ee\u975e\u96c6\u6210\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u9876\u5c16\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u8fdd\u89c4\u7387\u5e76\u63d0\u5347\u4e86\u8def\u7ebf\u5b8c\u6210\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u4f18\u5316\u4e86\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u4e3a\u66f4\u5b89\u5168\u7684\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2508.19094", "pdf": "https://arxiv.org/pdf/2508.19094", "abs": "https://arxiv.org/abs/2508.19094", "authors": ["Vincenzo Polizzi", "Stephen Yang", "Quentin Clark", "Jonathan Kelly", "Igor Gilitschenski", "David B. Lindell"], "title": "VibES: Induced Vibration for Persistent Event-Based Sensing", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Event cameras are a bio-inspired class of sensors that asynchronously measure\nper-pixel intensity changes. Under fixed illumination conditions in static or\nlow-motion scenes, rigidly mounted event cameras are unable to generate any\nevents, becoming unsuitable for most computer vision tasks. To address this\nlimitation, recent work has investigated motion-induced event stimulation that\noften requires complex hardware or additional optical components. In contrast,\nwe introduce a lightweight approach to sustain persistent event generation by\nemploying a simple rotating unbalanced mass to induce periodic vibrational\nmotion. This is combined with a motion-compensation pipeline that removes the\ninjected motion and yields clean, motion-corrected events for downstream\nperception tasks. We demonstrate our approach with a hardware prototype and\nevaluate it on real-world captured datasets. Our method reliably recovers\nmotion parameters and improves both image reconstruction and edge detection\nover event-based sensing without motion induction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u65cb\u8f6c\u4e0d\u5e73\u8861\u8d28\u91cf\u5757\u8bf1\u5bfc\u5468\u671f\u6027\u632f\u52a8\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u5728\u9759\u6001\u6216\u4f4e\u8fd0\u52a8\u573a\u666f\u4e0b\u65e0\u6cd5\u751f\u6210\u4e8b\u4ef6\u7684\u95ee\u9898\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u9759\u6001\u6216\u4f4e\u8fd0\u52a8\u573a\u666f\u4e2d\u65e0\u6cd5\u751f\u6210\u4e8b\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u65cb\u8f6c\u4e0d\u5e73\u8861\u8d28\u91cf\u5757\u8bf1\u5bfc\u632f\u52a8\uff0c\u5e76\u7ed3\u5408\u8fd0\u52a8\u8865\u507f\u7ba1\u9053\u53bb\u9664\u6ce8\u5165\u7684\u8fd0\u52a8\u3002", "result": "\u65b9\u6cd5\u53ef\u9760\u5730\u6062\u590d\u4e86\u8fd0\u52a8\u53c2\u6570\uff0c\u5e76\u6539\u5584\u4e86\u56fe\u50cf\u91cd\u5efa\u548c\u8fb9\u7f18\u68c0\u6d4b\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u9759\u6001\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5176\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.19159", "pdf": "https://arxiv.org/pdf/2508.19159", "abs": "https://arxiv.org/abs/2508.19159", "authors": ["Ersin Das", "Rahal Nanayakkara", "Xiao Tan", "Ryan M. Bena", "Joel W. Burdick", "Paulo Tabuada", "Aaron D. Ames"], "title": "Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Measurements and state estimates are often imperfect in control practice,\nposing challenges for safety-critical applications, where safety guarantees\nrely on accurate state information. In the presence of estimation errors,\nseveral prior robust control barrier function (R-CBF) formulations have imposed\nstrict conditions on the input. These methods can be overly conservative and\ncan introduce issues such as infeasibility, high control effort, etc. This work\nproposes a systematic method to improve R-CBFs, and demonstrates its advantages\non a tracked vehicle that navigates among multiple obstacles. A primary\ncontribution is a new optimization-based online parameter adaptation scheme\nthat reduces the conservativeness of existing R-CBFs. In order to reduce the\ncomplexity of the parameter optimization, we merge several safety constraints\ninto one unified numerical CBF via Poisson's equation. We further address the\ndual relative degree issue that typically causes difficulty in vehicle\ntracking. Experimental trials demonstrate the overall performance improvement\nof our approach over existing formulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u9c81\u68d2\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08R-CBF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5728\u7ebf\u53c2\u6570\u81ea\u9002\u5e94\u65b9\u6848\u51cf\u5c11\u4fdd\u5b88\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u53cc\u76f8\u5bf9\u9636\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u9c81\u68d2\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u5b58\u5728\u4f30\u8ba1\u8bef\u5dee\u65f6\u5bf9\u8f93\u5165\u65bd\u52a0\u4e25\u683c\u6761\u4ef6\uff0c\u5bfc\u81f4\u4fdd\u5b88\u6027\u3001\u4e0d\u53ef\u884c\u6027\u548c\u9ad8\u63a7\u5236\u6210\u672c\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u51cf\u5c11\u8fd9\u4e9b\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u5728\u7ebf\u53c2\u6570\u81ea\u9002\u5e94\u65b9\u6848\uff0c\u7ed3\u5408\u6cca\u677e\u65b9\u7a0b\u5c06\u591a\u4e2a\u5b89\u5168\u7ea6\u675f\u5408\u5e76\u4e3a\u7edf\u4e00\u7684\u6570\u503cCBF\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u8f66\u8f86\u8ddf\u8e2a\u4e2d\u7684\u53cc\u76f8\u5bf9\u9636\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u73b0\u6709R-CBF\u7684\u4fdd\u5b88\u6027\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u53c2\u6570\u548c\u7edf\u4e00\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709R-CBF\u7684\u4fdd\u5b88\u6027\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
