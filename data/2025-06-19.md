<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder](https://arxiv.org/abs/2506.14906)
*Robert Czupryniak,Abhishek Chakraborty,Andrew N. Jordan,John C. Howell*

Main category: eess.SP

TL;DR: 利用机器学习方法实现雷达遥感中的超分辨率测距，通过去噪自动编码器优化亚波长散射体间距的估计。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过机器学习提升雷达检测中的距离分辨率，尤其在亚波长范围内。

Method: 使用去噪自动编码器，并在带限约束下训练模型，分析瓶颈层与真实间距的相关性。

Result: Bessel信号表现最佳，其次是三角波，sinc信号最差，表明信号设计对机器学习分辨率至关重要。

Conclusion: 信号设计对机器学习方法的成功有显著影响，Bessel信号是实现高分辨率的优选。

Abstract: We apply machine learning methods to demonstrate range superresolution in
remote sensing radar detection. Specifically, we implement a denoising
autoencoder to estimate the distance between two equal intensity scatterers in
the subwavelength regime. The machine learning models are trained on waveforms
subject to a bandlimit constraint such that ranges much smaller than the
inverse bandlimit are optimized in their precision. The autoencoder achieves
effective dimensionality reduction, with the bottleneck layer exhibiting a
strong and consistent correlation with the true scatterer separation. We
confirm reproducibility across different training sessions and network
initializations by analyzing the scaled encoder outputs and their robustness to
noise. We investigate the behavior of the bottleneck layer for the following
types of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,
and a theoretically near-optimal pulse created from a spherical Bessel function
basis. The Bessel signal performs best, followed by the triangle wave, with the
sinc signal performing worst, highlighting the crucial role of signal design in
the success of machine-learning-based range resolution.

</details>


### [2] [Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization](https://arxiv.org/abs/2506.14985)
*Kuranage Roche Rayan Ranasinghe,Hyeon Seok Rou,Iván Alexander Morales Sandoval,Giuseppe Thadeu Freitas de Abreu,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文提出了一种基于可重构智能表面（RIS）和堆叠智能超表面（SIM）的多输入多输出（MIMO）系统中双分散（DD）信道模型的新方法，MPDD模型，并探讨了其在优化波形中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统DD信道模型在高移动性场景和ISAC中表现优异，但扩展到MIMO系统，尤其是在RIS和SIM增强的环境中，仍是一个未解决的难题。

Method: 提出了一种新的MPDD信道模型，整合了任意数量的RIS，并在收发端引入了SIM。模型应用于OFDM、OTFS和AFDM等波形优化。

Result: 通过一个示例应用展示了模型的灵活性，证明了其在SIM辅助无线系统中提升波形性能的潜力。

Conclusion: MPDD模型为解决MIMO系统中的DD信道问题提供了新思路，尤其在RIS和SIM增强的环境中，具有重要的应用前景。

Abstract: The doubly-dispersive (DD) channel structure has played a pivotal role in
wireless communications, particularly in high-mobility scenarios and integrated
sensing and communications (ISAC), due to its ability to capture the key fading
effects experienced by a transmitted signal as it propagates through a dynamic
medium. However, extending the DD framework to multiple-input multiple-output
(MIMO) systems, especially in environments artificially enhanced by
reconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces
(SIM), remains a challenging open problem. In this chapter, a novel
metasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary
number of RISs, while also incorporating SIM at both the transmitter and
receiver is introduced. Next, the application of this model to some key
waveforms optimized for DD environments -- namely orthogonal frequency division
multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine
frequency division multiplexing (AFDM) -- is discussed. Finally, the
programmability of the proposed model is highlighted through an illustrative
application, demonstrating its potential for enhancing waveform performance in
SIM-assisted wireless systems.

</details>


### [3] [Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks](https://arxiv.org/abs/2506.14992)
*Zhihao Tao,Athina P. Petropulu*

Main category: eess.SP

TL;DR: 该论文提出了一种基于生成式AI的时间调制智能反射面（TM-IRS）设计方法，用于多用户OFDM系统的定向调制，以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 现有TM-IRS设计方法仅适用于单用户场景，无法满足多用户需求。因此，需要一种更高效的设计方法。

Method: 通过生成式AI方法，将TM-IRS参数选择建模为MDP问题，并利用GFlowNets学习策略，以高概率采样最优参数集。

Result: 实验表明，该方法显著提升了多用户TM-IRS-OFDM系统的安全性，且训练效率极高。

Conclusion: 该方法为多用户TM-IRS设计提供了高效解决方案，且具有强鲁棒性和可扩展性。

Abstract: We propose a novel directional modulation (DM) design for OFDM transmitters
aided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS
is configured to preserve the integrity of transmitted signals toward multiple
legitimate users while scrambling the signal in all other directions. Existing
TM-IRS design methods typically target a single user direction and follow
predefined rule-based procedures, making them unsuitable for multi-user
scenarios. Here, we propose a generative AI-based approach to design good sets
of TM-IRS parameters out of a set of all possible quantized ranges of
parameters. The design objective is to maximize the sum rate across the
authorized directions. We model the TM-IRS parameter selection as a
deterministic Markov decision process (MDP), where each terminal state
corresponds to a specific configuration of TM-IRS parameters. GFlowNets are
employed to learn a stochastic policy that samples TM-IRS parameter sets with
probability proportional to their associated sum rate reward. Experimental
results demonstrate that the proposed method effectively enhances the security
of the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size
of the TM-IRS configuration space, the GFlowNet is able to converge after
training on fewer than 0.000001% of all possible configurations, demonstrating
remarkable efficiency compared to exhaustive combinatorial search.
Implementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to
facilitate reproducibility.

</details>


### [4] [Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks](https://arxiv.org/abs/2506.15125)
*Linlin Wang,Wei Wang,Dezhao Wang,Shanwen Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于混合深度学习网络（HDLNet）的信号去噪算法，用于光纤分布式声学传感（DAS）系统在智能交通系统（ITS）中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着光纤DAS系统的应用需求增长，需要有效的信号处理方法以推动其在ITS中的普及。

Method: 采用自监督的混合深度学习网络（HDLNet），结合去噪自动编码器（DAE）和长短期记忆网络（LSTM）进行并行处理，并提出逐行匹配算法用于车辆检测与跟踪。

Result: 在自建的高速公路隧道数据集上的实验表明，该混合网络比空间域DAE具有更优的去噪性能。

Conclusion: HDLNet为光纤DAS信号处理提供了一种高效的去噪和特征提取解决方案，适用于ITS场景。

Abstract: With the applicability of optical fiber-based distributed acoustic sensing
(DAS) systems, effective signal processing and analysis approaches are needed
to promote its popularization in the field of intelligent transportation
systems (ITS). This paper presents a signal denoising algorithm using a hybrid
deep-learning network (HDLNet). Without annotated data and time-consuming
labeling, this self-supervised network runs in parallel, combining an
autoencoder for denoising (DAE) and a long short-term memory (LSTM) for
sequential processing. Additionally, a line-by-line matching algorithm for
vehicle detection and tracking is introduced, thus realizing the complete
processing of fiber signal denoising and feature extraction. Experiments were
carried out on a self-established real highway tunnel dataset, showing that our
proposed hybrid network yields more satisfactory denoising performance than
Spatial-domain DAE.

</details>


### [5] [Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead](https://arxiv.org/abs/2506.15136)
*Kehui Li,Binggui Zhou,Jiajia Guo,Feifei Gao,Guanghua Yang,Shaodan Ma*

Main category: eess.SP

TL;DR: 提出了一种基于视觉和位置信息的OOB模态协同（OMS）方案，用于多基站系统中减少信令开销并提升协调效率。


<details>
  <summary>Details</summary>
Motivation: 多用户毫米波通信中，多基站协调和光束跟踪导致高信令开销，现有OOB模态方法在单基站系统中有效，但在多基站系统中协调开销大。

Method: 通过视觉和位置信息协同，实现用户识别与跟踪，并设计BEM-GBPN网络预测光束增益和最优光束，支持中央单元控制基站切换和光束调整。

Result: 仿真结果表明，方案在零导频开销下实现91%的理想传输速率，显著提升了多基站协调效率。

Conclusion: OMS方案在多基站系统中有效减少了信令开销，提升了协调效率和用户传输性能。

Abstract: Multi-user millimeter-wave communication relies on narrow beams and dense
cell deployments to ensure reliable connectivity. However, tracking optimal
beams for multiple mobile users across multiple base stations (BSs) results in
significant signaling overhead. Recent works have explored the capability of
out-of-band (OOB) modalities in obtaining spatial characteristics of wireless
channels and reducing pilot overhead in single-BS single-user/multi-user
systems. However, applying OOB modalities for multi-BS selection towards dense
cell deployments leads to high coordination overhead, i.e, excessive computing
overhead and high latency in data exchange. How to leverage OOB modalities to
eliminate pilot overhead and achieve efficient multi-BS coordination in
multi-BS systems remains largely unexplored. In this paper, we propose a novel
OOB modality synergy (OMS) based mobility management scheme to realize
multi-user beam prediction and proactive BS selection by synergizing two OOB
modalities, i.e., vision and location. Specifically, mobile users are initially
identified via spatial alignment of visual sensing and location feedback, and
then tracked according to the temporal correlation in image sequence.
Subsequently, a binary encoding map based gain and beam prediction network
(BEM-GBPN) is designed to predict beamforming gains and optimal beams for
mobile users at each BS, such that a central unit can control the BSs to
perform user handoff and beam switching. Simulation results indicate that the
proposed OMS-based mobility management scheme enhances beam prediction and BS
selection accuracy and enables users to achieve 91% transmission rates of the
optimal with zero pilot overhead and significantly improve multi-BS
coordination efficiency compared to existing methods.

</details>


### [6] [Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation](https://arxiv.org/abs/2506.15148)
*Yuxuan Xia,Ángel F. García-Fernández,Johan Karlsson,Yu Ge,Lennart Svensson,Ting Yuan*

Main category: eess.SP

TL;DR: 提出了一种广义的轨迹最优子模式分配（GOSPA）指标，用于评估提供轨迹估计的多目标跟踪算法，该指标考虑了轨迹级别的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时考虑目标状态的存在和状态估计的不确定性，因此需要一种新的指标来更全面地评价多目标跟踪算法的性能。

Method: 基于概率GOSPA指标，将轨迹GOSPA（TGOSPA）推广到多维分配问题，并通过线性规划松弛实现多项式时间内可计算。

Result: 新指标保留了TGOSPA的可解释性，并通过仿真研究验证了其有效性。

Conclusion: 提出的指标能够直观地分解为定位误差、存在概率不匹配误差、漏检与误检误差以及轨迹切换误差，为多目标跟踪算法的评价提供了更全面的工具。

Abstract: This paper presents a generalization of the trajectory general optimal
sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking
algorithms that provide trajectory estimates with track-level uncertainties.
This metric builds on the recently introduced probabilistic GOSPA metric to
account for both the existence and state estimation uncertainties of individual
object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a
multidimensional assignment problem, and its linear programming
relaxation--also a valid metric--is computable in polynomial time.
Additionally, this metric retains the interpretability of TGOSPA, and we show
that its decomposition yields intuitive costs terms associated to expected
localization error and existence probability mismatch error for properly
detected objects, expected missed and false detection error, and track switch
error. The effectiveness of the proposed metric is demonstrated through a
simulation study.

</details>


### [7] [Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data](https://arxiv.org/abs/2506.15235)
*Taewon Kang,Seunghyeon Park,Pyo-Woong Son,Jiwon Seo*

Main category: eess.SP

TL;DR: 论文提出了一种名为WLR-AGRNN的模型，用于提高eLoran/GPS时间差（TD）的估计精度，通过结合气象因素和地形高程数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决eLoran系统信号传播延迟（ASF）带来的误差问题，提高其时间同步精度，使其能与GPS媲美。

Method: 提出WLR-AGRNN模型，结合加权线性回归（WLR）捕捉气象因素的线性关系，并通过各向异性广义回归神经网络（AGRNN）建模非线性关系。

Result: 基于四个月数据的实验表明，WLR-AGRNN模型优于其他方法，显著提高了eLoran/GPS TD的估计精度。

Conclusion: WLR-AGRNN模型通过有效结合气象和地形数据，为eLoran系统的时间同步精度提升提供了可行方案。

Abstract: The vulnerabilities of global navigation satellite systems (GNSS) to signal
interference have increased the demand for complementary positioning,
navigation, and timing (PNT) systems. To address this, South Korea has decided
to deploy an enhanced long-range navigation (eLoran) system as a complementary
PNT solution. Similar to GNSS, eLoran provides highly accurate timing
information, which is essential for applications such as telecommunications,
financial systems, and power distribution. However, the primary sources of
error for GNSS and eLoran differ. For eLoran, the main source of error is
signal propagation delay over land, known as the additional secondary factor
(ASF). This delay, influenced by ground conductivity and weather conditions
along the signal path, is challenging to predict and mitigate. In this paper,
we measure the time difference (TD) between GPS and eLoran using a time
interval counter and analyze the correlations between eLoran/GPS TD and eleven
meteorological factors. Accurate estimation of eLoran/GPS TD could enable
eLoran to achieve timing accuracy comparable to that of GPS. We propose two
estimation models for eLoran/GPS TD and compare their performance with existing
TD estimation methods. The proposed WLR-AGRNN model captures the linear
relationships between meteorological factors and eLoran/GPS TD using weighted
linear regression (WLR) and models nonlinear relationships between outputs from
expert networks through an anisotropic general regression neural network
(AGRNN). The model incorporates terrain elevation to appropriately weight
meteorological data, as elevation influences signal propagation delay.
Experimental results based on four months of data demonstrate that the
WLR-AGRNN model outperforms other models, highlighting its effectiveness in
improving eLoran/GPS TD estimation accuracy.

</details>


### [8] [Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access](https://arxiv.org/abs/2506.15273)
*Anup Mishra,Čedomir Stefanović,Xiuqiang Xu,Petar Popovski,Israel Leyva-Mayorga*

Main category: eess.SP

TL;DR: 研究提出了一种基于强化学习的资源分配方法，优化物联网设备的延迟性能，同时兼顾宽带用户的吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 未来的无线网络需要高效共享资源，因此研究了物联网设备与宽带用户在资源分配中的共存问题。

Method: 采用了基于双Q学习的强化学习方法，优化物联网设备的重复传输策略，并比较了正交RAN切片和共享访问两种资源分配方式。

Result: 结果显示，强化学习方法显著提升了物联网用户的延迟性能，并且在低物联网流量时共享访问更节能，高流量时切片更优。

Conclusion: 提出的方法在资源分配中平衡了物联网设备的延迟需求和宽带用户的性能，验证了其在不同流量场景下的适应性。

Abstract: Flexible and efficient wireless resource sharing across heterogeneous
services is a key objective for future wireless networks. In this context, we
investigate the performance of a system where latency-constrained
internet-of-things (IoT) devices coexist with a broadband user. The base
station adopts a grant-free access framework to manage resource allocation,
either through orthogonal radio access network (RAN) slicing or by allowing
shared access between services. For the IoT users, we propose a reinforcement
learning (RL) approach based on double Q-Learning (QL) to optimise their
repetition-based transmission strategy, allowing them to adapt to varying
levels of interference and meet a predefined latency target. We evaluate the
system's performance in terms of the cumulative distribution function of IoT
users' latency, as well as the broadband user's throughput and energy
efficiency (EE). Our results show that the proposed RL-based access policies
significantly enhance the latency performance of IoT users in both RAN Slicing
and RAN Sharing scenarios, while preserving desirable broadband throughput and
EE. Furthermore, the proposed policies enable RAN Sharing to be
energy-efficient at low IoT traffic levels, and RAN Slicing to be favourable
under high IoT traffic.

</details>


### [9] [Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry](https://arxiv.org/abs/2506.15338)
*Islam M. Tanash,Ayush Kumar Dwivedi,Taneli Riihonen*

Main category: eess.SP

TL;DR: 该论文研究了一种由可重构智能表面（RIS）支持的高空平台（HAP）网络，通过统计信道建模和仿真验证，展示了显著的性能提升和参数影响。


<details>
  <summary>Details</summary>
Motivation: 探索在城区环境中利用RIS和HAP网络提升连接性和数据卸载效率。

Method: 采用泊松点过程建模HAP和RIS的不规则布局，布尔矩形模型模拟建筑物遮挡，并提出基于广义Beta prime分布的统计信道分析方法。

Result: 仿真验证了覆盖概率和遍历容量的理论分析，结果表明系统参数（如遮挡效应）对性能有显著影响。

Conclusion: 该系统可有效提升城区环境的连接性，并优化数据卸载性能。

Abstract: This paper studies a high-altitude platform (HAP) network supported by
reconfigurable intelligent surfaces (RISs). The practical irregular placement
of HAPs and RISs is modeled using homogeneous Poisson point processes, while
buildings that cause blockages in urban areas are modeled as a Boolean scheme
of rectangles. We introduce a novel approach to characterize the statistical
channel based on generalized Beta prime distribution. Analytical expressions
for coverage probability and ergodic capacity in an interference-limited system
are derived and validated through Monte Carlo simulations. The findings show
notable performance improvements and reveal the impact of various system
parameters, including blockages effect which contribute in mitigating
interference from the other visible HAPs. This proposed system could enhance
connectivity and enable effective data offloading in urban environments.

</details>


### [10] [Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array](https://arxiv.org/abs/2506.15463)
*Shweta Pal,Arun Kumar,Monika Agrawal*

Main category: eess.SP

TL;DR: 本文研究了信号量化对一维一阶差分麦克风阵列（DMA）性能的影响，发现量化主要影响零深度（ND），而波束形状、方向性因子（DF）和前后比（FBR）保持不变。


<details>
  <summary>Details</summary>
Motivation: 量化在信号处理中至关重要，但此前研究未探讨其对一维一阶DMA性能的影响，本文填补了这一空白。

Method: 通过理论分析，推导了一维一阶DMA的量化波束形成输出表达式，并研究了量化对波束形状、DF、FBR和ND的影响。

Result: 量化主要影响ND，使其随量化比特数增加而提高；DF和FBR不受量化影响；波束形状结构不变；ND频率无关性但随零位接近主方向而降低。

Conclusion: 量化对一维一阶DMA性能的影响主要体现在ND上，优化量化比特数可增强干扰抑制能力。

Abstract: In practical systems, recorded analog signals must be digitized for
processing, introducing quantization as a critical aspect of data acquisition.
While prior studies have examined quantization effects in various signal
processing contexts, its impact on differential microphone arrays (DMAs),
particularly in one-dimensional (1D) first-order configurations, remains
unexplored. This paper investigates the influence of signal quantization on
performance of first-order 1D DMAs across various beampatterns. An analytical
expression for quantized beamformed output for a first-order 1D DMA has been
formulated. The effect of signal quantization has been studied on array
performance measures such as the Beampattern, Directivity Factor (DF),
Front-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that
beampattern shape remains structurally invariant across quantization bit
depths, with quantization primarily affecting ND. DF and FBR remain constant
with the varying number of quantization bits. Additionally, ND is shown to be
frequency-independent; however, it increases with increasing quantization bit
depths, enhancing interference suppression. The study also examines the effect
of steering nulls across the azimuthal range, showing that ND degrades as the
null moves closer to the source look direction, indicating reduced interference
suppression.

</details>


### [11] [Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage](https://arxiv.org/abs/2506.15470)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: 论文研究了大型天线阵列在高频段的近场光束聚焦性能，分析了不同几何形状的均匀矩形阵列（URA）对光束深度的影响，并提出了有效光束聚焦瑞利距离（EBRD）的概念。研究发现，宽或高的URA在EBRD和空间复用能力上优于方形URA。


<details>
  <summary>Details</summary>
Motivation: 未来无线通信系统可能会在辐射近场中运行，而近场光束聚焦可以在有限深度空间内实现空间复用。需要研究不同几何形状的天线阵列对近场光束深度和空间复用增益的影响。

Method: 论文推导了广义均匀矩形阵列（URA）的光束深度，并通过定义EBRD来分析近场边界的光束聚焦和空间复用增益。仿真比较了不同几何形状的URA性能。

Result: 研究发现，方形URA的光束深度最窄，但EBRD最小；宽或高的URA虽然光束深度较宽，但EBRD更大，空间复用能力更强，实现了比方形URA高3.5倍的总速率。

Conclusion: 论文表明，宽或高的URA更适合近场通信，因其更大的EBRD和优异的空间复用性能，能够显著提升多用户总速率。

Abstract: With the deployment of large antenna arrays at high frequency bands, future
wireless communication systems are likely to operate in the radiative
near-field. Unlike far-field beam steering, near-field beams can be focused
within a spatial region of finite depth, enabling spatial multiplexing in both
the angular and range dimensions. This paper derives the beamdepth for a
generalized uniform rectangular array (URA) and investigates how array geometry
influences the near-field beamdepth and the limits where near-field
beamfocusing is achievable. To characterize the near-field boundary in terms of
beamfocusing and spatial multiplexing gains, we define the effective
beamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis
reveals that while a square URA achieves the narrowest beamdepth, the EBRD is
maximized for a wide or tall URA. However, despite its narrow beamdepth, a
square URA may experience a reduction in multiuser sum rate due to its severely
constrained EBRD. Simulation results confirm that a wide or tall URA achieves a
sum rate of 3.5 X more than that of a square URA, benefiting from the extended
EBRD and improved spatial multiplexing capabilities.

</details>


### [12] [Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward](https://arxiv.org/abs/2506.15670)
*Özlem Tugfe Demir,Mustafa Ozger,Ferdi Kara,Woong-Hee Lee,Emil Björnson*

Main category: eess.SP

TL;DR: 论文探讨了在7-24 GHz频段结合SWIPT与gMIMO技术，利用近场传播实现高效节能的高容量通信系统，支持6G网络需求。


<details>
  <summary>Details</summary>
Motivation: 研究旨在满足6G无线网络对高容量和能源效率的需求，推动智能工厂与物联网等应用。

Method: 通过球形波传播、波束聚焦和大规模空间复用，结合先进的信道估计、预编码策略和动态阵列配置。

Result: 案例研究表明，在密集动态环境中能够优化能量收集和数据吞吐量。

Conclusion: 该技术有助于推进能源自主的物联网和智能工厂等应用，支持下一代无线技术的发展。

Abstract: This paper explores the integration of simultaneous wireless information and
power transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)
technology operating in the upper mid-band frequency range (7-24 GHz). The
near-field propagation achieved by gMIMO introduces unique opportunities for
energy-efficient, high-capacity communication systems that cater to the demands
of 6G wireless networks. Exploiting spherical wave propagation, near-field
SWIPT with gMIMO enables precise energy and data delivery, enhancing spectral
efficiency through beamfocusing and massive spatial multiplexing. This paper
discusses theoretical principles, design challenges, and enabling solutions,
including advanced channel estimation techniques, precoding strategies, and
dynamic array configurations such as sparse and modular arrays. Through
analytical insights and a case study, this paper demonstrates the feasibility
of achieving optimized energy harvesting and data throughput in dense and
dynamic environments. These findings contribute to advancing energy-autonomous
Internet-of-Everything (IoE) deployments, smart factory networks, and other
energy-autonomous applications aligned with the goals of next-generation
wireless technologies.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [13] [Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels](https://arxiv.org/abs/2506.15105)
*David Nozadze,Zurab Kiguradze,Amendra Koul,Mike Sapozhnikov*

Main category: eess.SY

TL;DR: 论文提出了两种新指标（SILD和FOM_SILD）来量化P/N偏斜对超高速互连性能的影响，并通过实验和模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载和数据中心需求的增加，传统方法难以准确评估P/N偏斜对超高速互连性能的影响。

Method: 引入了两种新指标（SILD和FOM_SILD），通过测量S参数和模拟224G PAM4 SerDes来分析偏斜效应。

Result: 实验证实FOM_SILD具有互易性，模拟结果与误码率（BER）趋势高度相关。

Conclusion: 该方法为下一代超高速互连中的偏斜分析提供了可靠框架。

Abstract: The rise of AI workloads and growing data center demands have driven the need
for ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI)
shrink, even a few picoseconds of P/N skew can degrade serializer-deserializer
(SerDes) performance. Traditional methods for quantifying skew fall short in
capturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion
Loss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD),
analytically developed to assess P/N skew effects. Measured S-parameters
confirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong
correlation with bit error rate (BER) trends. This approach offers a robust
framework for analyzing skew in next-generation ultra-high-speed interconnects.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets](https://arxiv.org/abs/2506.15075)
*Samhita Kuili,Mohammadreza Amini,Burak Kantarci*

Main category: cs.CR

TL;DR: 该论文提出了一种基于卷积自编码器（CAE）的5G-NR无线蜂窝网络中的干扰攻击检测方法，并通过生成对抗网络（CWGAN-GP）解决数据不平衡问题，最终在检测性能上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 5G-NR网络中干扰攻击严重影响了信号接收质量，因此需要开发一种高效的检测方法。

Method: 使用卷积自编码器（CAE）检测干扰，并结合Conv1D条件Wasserstein生成对抗网络（CWGAN-GP）处理数据不平衡问题。

Result: CAE在精度（97.33%）、召回率（91.33%）、F1分数（94.08%）和准确率（94.35%）上优于其他基准模型。

Conclusion: CAE在复杂的数据异质性下表现出色，能够有效检测干扰信号。

Abstract: In the ever-expanding domain of 5G-NR wireless cellular networks,
over-the-air jamming attacks are prevalent as security attacks, compromising
the quality of the received signal. We simulate a jamming environment by
incorporating additive white Gaussian noise (AWGN) into the real-world In-phase
and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is
exploited to implement a jamming detection over various characteristics such as
heterogenous I/Q datasets; extracting relevant information on Synchronization
Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance.
Given the characteristics of datasets, balanced datasets are acquired by
employing a Conv1D conditional Wasserstein Generative Adversarial
Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB
observations. Additionally, we compare the performance and detection ability of
the proposed CAE model on augmented datasets with benchmark models:
Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder
(CSAE). Despite the complexity of data heterogeneity involved across all
datasets, CAE depicts the robustness in detection performance of jammed signal
by achieving average values of 97.33% precision, 91.33% recall, 94.08%
F1-score, and 94.35% accuracy over CDAE and CSAE.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [15] [An Integrated Sensing and Communication System for Time-Sensitive Targets with Random Arrivals](https://arxiv.org/abs/2506.15045)
*Homa Nikbakht,Yonina C. Eldar,H. Vincent Poor*

Main category: cs.IT

TL;DR: 本文提出了一种双基地MIMO ISAC系统，用于检测URLLC消息并优先传输，结合DPC技术优化性能，显著优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 满足6G网络中智能城市和自动驾驶等应用对URLLC的需求，同时实现高效联合感知与通信。

Method: 使用双基地MIMO ISAC系统，结合DPC技术处理干扰，并通过两种方式解码eMBB消息。

Result: 提出的DPC方案在FBL环境下，实现了更高的eMBB传输速率，同时满足URLLC和感知约束。

Conclusion: 基于DPC的ISAC系统在性能和效率上优于传统方案，适合未来6G网络的高要求应用。

Abstract: In 6G networks, integrated sensing and communication (ISAC) is envisioned as
a key technology that enables wireless systems to perform joint sensing and
communication using shared hardware, antennas and spectrum. ISAC designs
facilitate emerging applications such as smart cities and autonomous driving.
Such applications also demand ultra-reliable and low-latency communication
(URLLC). Thus, an ISAC-enabled URLLC system can prioritize time-sensitive
targets and ensure information delivery under strict latency and reliability
constraints. We propose a bi-static MIMO ISAC system to detect the arrival of
URLLC messages and prioritize their delivery. In this system, a base station
(BS) communicates with a user equipment (UE) and a sensing receiver (SR) is
deployed to collect echo signals reflected from a target of interest. The BS
regularly transmits messages of enhanced mobile broadband (eMBB) services to
the UE. During each eMBB transmission, if the SR senses the presence of a
target of interest, it immediately triggers the transmission of an additional
URLLC message. To reinforce URLLC transmissions, we propose a dirty-paper
coding (DPC)-based technique that mitigates the interference of both eMBB and
sensing signals. To decode the eMBB message, we consider two approaches for
handling the URLLC interference: treating interference as noise and successive
interference cancellation. For this system, we formulate the
rate-reliability-detection trade-off in the finite blocklength (FBL) regime by
evaluating the communication rate of the eMBB transmissions, the reliability of
the URLLC transmissions and the probability of the target detection. Our
numerical analysis show that our proposed DPC-based ISAC scheme significantly
outperforms power-sharing and traditional time-sharing schemes. In particular,
it achieves higher eMBB transmission rate while satisfying both URLLC and
sensing constraints.

</details>


### [16] [MIMO Systems Aided by Microwave Linear Analog Computers: Capacity-Achieving Architectures with Reduced Circuit Complexity](https://arxiv.org/abs/2506.15052)
*Matteo Nerini,Bruno Clerckx*

Main category: cs.IT

TL;DR: 为了解决巨型MIMO的硬件和计算成本，提出了一种基于图论模型的低复杂度MiLAC架构，称为stem-connected MiLACs，其电路复杂度线性增长，同时保持容量性能。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络的需求促使天线阵列从大规模MIMO扩展到巨型MIMO，但传统的全连接MiLAC电路复杂度高，限制了实用性。

Method: 提出一种图论模型，设计了stem-connected MiLACs，并通过闭式解优化其性能。

Result: 理论和数值模拟表明，stem-connected MiLACs在电路复杂度线性增长的同时，仍保持容量性能。

Conclusion: stem-connected MiLACs为高性能、可扩展的巨型MIMO提供了可行方案。

Abstract: To meet the demands of future wireless networks, antenna arrays must scale
from massive multiple-input multiple-output (MIMO) to gigantic MIMO, involving
even larger numbers of antennas. To address the hardware and computational cost
of gigantic MIMO, several strategies are available that shift processing from
the digital to the analog domain. Among them, microwave linear analog computers
(MiLACs) offer a compelling solution by enabling fully analog beamforming
through reconfigurable microwave networks. Prior work has focused on
fully-connected MiLACs, whose ports are all interconnected to each other via
tunable impedance components. Although such MiLACs are capacity-achieving,
their circuit complexity, given by the number of required impedance components,
scales quadratically with the number of antennas, limiting their practicality.
To solve this issue, in this paper, we propose a graph theoretical model of
MiLAC facilitating the systematic design of lower-complexity MiLAC
architectures. Leveraging this model, we propose stem-connected MiLACs as a
family of MiLAC architectures maintaining capacity-achieving performance while
drastically reducing the circuit complexity. Besides, we optimize
stem-connected MiLACs with a closed-form capacity-achieving solution. Our
theoretical analysis, confirmed by numerical simulations, shows that
stem-connected MiLACs are capacity-achieving, but with circuit complexity that
scales linearly with the number of antennas, enabling high-performance,
scalable, gigantic MIMO.

</details>


### [17] [In-Context Learning for Gradient-Free Receiver Adaptation: Principles, Applications, and Theory](https://arxiv.org/abs/2506.15176)
*Matteo Zecchin,Tomer Raviv,Dileep Kalathil,Krishna Narayanan,Nir Shlezinger,Osvaldo Simeone*

Main category: cs.IT

TL;DR: 该论文提出了一种基于上下文学习（ICL）的无梯度适应技术，用于无线接收器的实时动态适应，避免了在线再训练的需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的无线接收器设计在复杂环境下面临挑战，而现有的深度学习适应方法灵活性不足或需要显式优化。本文旨在通过ICL技术解决这些问题。

Method: 论文研究了基于Transformer和结构化状态空间模型（SSMs）的ICL架构，探索了序列模型如何从上下文信息中学习适应的理论机制，并将其应用于无蜂窝大规模MIMO网络。

Result: 研究发现，ICL是一种有原则且高效的方法，能够通过导频信号和辅助上下文信息实现接收器的实时适应，无需在线再训练。

Conclusion: ICL为无线接收器的动态适应提供了新的解决方案，结合了算法和硬件的优势，具有广阔的应用前景。

Abstract: In recent years, deep learning has facilitated the creation of wireless
receivers capable of functioning effectively in conditions that challenge
traditional model-based designs. Leveraging programmable hardware
architectures, deep learning-based receivers offer the potential to dynamically
adapt to varying channel environments. However, current adaptation strategies,
including joint training, hypernetwork-based methods, and meta-learning, either
demonstrate limited flexibility or necessitate explicit optimization through
gradient descent. This paper presents gradient-free adaptation techniques
rooted in the emerging paradigm of in-context learning (ICL). We review
architectural frameworks for ICL based on Transformer models and structured
state-space models (SSMs), alongside theoretical insights into how sequence
models effectively learn adaptation from contextual information. Further, we
explore the application of ICL to cell-free massive MIMO networks, providing
both theoretical analyses and empirical evidence. Our findings indicate that
ICL represents a principled and efficient approach to real-time receiver
adaptation using pilot signals and auxiliary contextual information-without
requiring online retraining.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [18] [Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations](https://arxiv.org/abs/2506.14795)
*Batuhan Hangun,Emine Akpinar,Oguz Altun,Onder Eyecioglu*

Main category: quant-ph

TL;DR: 量子机器学习（QML）结合量子计算与机器学习，利用量子力学原理提升经典方法。本研究评估量子神经网络（QNNs）的效果，证明其在预测任务中优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 探索量子机器学习在实际应用中的潜力，评估QNNs的性能，解决当前量子设备的局限性问题。

Method: 构建12种QNN配置，结合两种量子特征映射和六种纠缠策略，在风能数据集上进行实验。

Result: QNNs在使用Z特征映射时达到93%的预测准确率，优于经典方法。

Conclusion: 研究表明QNNs在预测任务中具有优势，为QML的实际应用提供了支持。

Abstract: Quantum Machine Learning (QML) is an emerging field at the intersection of
quantum computing and machine learning, aiming to enhance classical machine
learning methods by leveraging quantum mechanics principles such as
entanglement and superposition. However, skepticism persists regarding the
practical advantages of QML, mainly due to the current limitations of noisy
intermediate-scale quantum (NISQ) devices. This study addresses these concerns
by extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired
counterparts of Artificial Neural Networks (ANNs), demonstrating their
effectiveness compared to classical methods. We systematically construct and
evaluate twelve distinct QNN configurations, utilizing two unique quantum
feature maps combined with six different entanglement strategies for ansatz
design. Experiments conducted on a wind energy dataset reveal that QNNs
employing the Z feature map achieve up to 93% prediction accuracy when
forecasting wind power output using only four input parameters. Our findings
show that QNNs outperform classical methods in predictive tasks, underscoring
the potential of QML in real-world applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [19] [Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models](https://arxiv.org/abs/2506.15530)
*Teysir Baoueb,Xiaoyu Bie,Xi Wang,Gaël Richard*

Main category: cs.SD

TL;DR: 提出了一种基于现有文本到音乐扩散模型的乐器编辑方法，通过选择合适的时间步长，实现在保留音频内容的同时更改乐器音色。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用现有文本到音乐扩散模型在不额外训练的情况下，实现对音频中乐器的精确编辑，解决生成过程中控制难的问题。

Method: 利用预训练模型的时间步长特性，通过乐器分类器选择中间时间步长，平衡内容保留与音色修改。

Result: 方法有效实现了乐器编辑，同时保持生成速度且无需额外训练。

Conclusion: 该方法为文本到音乐生成的控制提供了新思路，展示了预训练模型在编辑任务中的潜力。

Abstract: Breakthroughs in text-to-music generation models are transforming the
creative landscape, equipping musicians with innovative tools for composition
and experimentation like never before. However, controlling the generation
process to achieve a specific desired outcome remains a significant challenge.
Even a minor change in the text prompt, combined with the same random seed, can
drastically alter the generated piece. In this paper, we explore the
application of existing text-to-music diffusion models for instrument editing.
Specifically, for an existing audio track, we aim to leverage a pretrained
text-to-music diffusion model to edit the instrument while preserving the
underlying content. Based on the insight that the model first focuses on the
overall structure or content of the audio, then adds instrument information,
and finally refines the quality, we show that selecting a well-chosen
intermediate timestep, identified through an instrument classifier, yields a
balance between preserving the original piece's content and achieving the
desired timbre. Our method does not require additional training of the
text-to-music diffusion model, nor does it compromise the generation process's
speed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels](https://arxiv.org/abs/2506.15225)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.AI

TL;DR: 该论文提出了一种基于无人机与船舶合作的MEC框架，用于解决海事物联网中的计算卸载和资源分配问题，通过Lyapunov优化和马尔可夫游戏模型应对任务不确定性。


<details>
  <summary>Details</summary>
Motivation: 海事物联网计算需求快速增长，但不确定的任务带来计算卸载和资源分配的挑战，亟需高效解决方案。

Method: 提出合作MEC框架，利用Lyapunov优化处理任务不确定性，并将其转化为马尔可夫游戏模型，设计异构智能体的软演员-评论家算法求解。

Result: 仿真实验验证了所提方法在计算卸载和资源分配中的有效性。

Conclusion: 该方法为海事物联网中的计算需求提供了高效且适应性强的解决方案。

Abstract: The computation demands from the maritime Internet of Things (MIoT) increase
rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels
based multi-access edge computing (MEC) can fulfill these MIoT requirements.
However, the uncertain maritime tasks present significant challenges of
inefficient computation offloading and resource allocation. In this paper, we
focus on the maritime computation offloading and resource allocation through
the cooperation of UAVs and vessels, with consideration of uncertain tasks.
Specifically, we propose a cooperative MEC framework for computation offloading
and resource allocation, including MIoT devices, UAVs and vessels. Then, we
formulate the optimization problem to minimize the total execution time. As for
the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the
unpredictable task arrivals and varying computational resource availability. By
converting the long-term constraints into short-term constraints, we obtain a
set of small-scale optimization problems. Further, considering the
heterogeneity of actions and resources of UAVs and vessels, we reformulate the
small-scale optimization problem into a Markov game (MG). Moreover, a
heterogeneous-agent soft actor-critic is proposed to sequentially update
various neural networks and effectively solve the MG problem. Finally,
simulations are conducted to verify the effectiveness in addressing
computational offloading and resource allocation.

</details>
