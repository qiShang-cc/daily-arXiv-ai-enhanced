{"id": "2510.12910", "pdf": "https://arxiv.org/pdf/2510.12910", "abs": "https://arxiv.org/abs/2510.12910", "authors": ["Neda Abdollahpour", "N. Sertac Artan", "Ian Daly", "Mohammadreza Yazdchi", "Zahra Baharlouei"], "title": "Effective Connectivity-Based Unsupervised Channel Selection Method for EEG", "categories": ["eess.SP"], "comment": "(This The paper has been accepted for publication in the Journal of\n  Medical Signals & Sensors and will appear soon", "summary": "Analyzing neural data such as Electroencephalography (EEG) data often\ninvolves dealing with high-dimensional datasets, where not all channels provide\nequally meaningful informa- tion. Selecting the most relevant channels is\ncrucial for improving computational efficiency and ensuring robust insights\ninto neural dynamics. This study introduces the Importance of Channels based on\nEffective Connectivity (ICEC) criterion for quantifying effective connectivity\n(EC) in each channel. Effective connectivity refers to the causal influence one\nneural region exerts over another, providing insights into the directional flow\nof information. Using this criterion, we propose an unsupervised channel\nselection method that accounts for the intensity of interactions among\nchannels. To evaluate the proposed channel selection method, we applied it to\nthree well-known EEG datasets across four categories. The assessment involved\ncalculating the ICEC criterion using five effective connectivity metrics:\npartial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC\n(RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on\nthe effect of channel selection, we employed the Common Spatial Pattern (CSP)\nalgorithm for feature extraction and a Support Vector Machine (SVM) for\nclassification across all participants. Results were compared with other\nCSP-based methods. The evaluation included comparing participant- specific\naccuracies with and without the proposed method across five effective\nconnectivity metrics. The results showed consistent performance improvements\nand a significant reduction in the number of selected electrodes for all\nparticipants. Compared to state-of-the-art methods, our approach achieved the\nhighest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59\nchannels), and 87.56% (48 out of 118 channels) across three datasets."}
{"id": "2510.12912", "pdf": "https://arxiv.org/pdf/2510.12912", "abs": "https://arxiv.org/abs/2510.12912", "authors": ["Abdelali Arous", "Hamza Haif", "Huseyin Arslan"], "title": "Enabling Full Duplex ISAC Leveraging Waveform Domain Separability", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) in monostatic in-band full-duplex\n(IBFD) systems encounters significant challenges due to self-interference (SI)\nat the radar receiver during concurrent communication and radar operations.\nThis paper proposes a novel waveform-domain self-interference cancellation\n(SIC) technique that leverages the unique properties of orthogonal frequency\ndivision multiplexing (OFDM) and affine frequency division multiplexing (AFDM)\nsignals. The proposed approach designs the integrated dual-functionality frame\nto utilize OFDM for communication and AFDM for radar sensing, both generated\nusing the same modulator block. Then, we establish the conditions under which a\nwide sense stationary (WSS) process in the time domain appears as WSS in the\naffine domain and demonstrate that the interfering OFDM signal behaves as an\nadditive white Gaussian noise (AWGN) in this domain. Exploiting this property,\nthe received signal is projected into the affine domain, where the SI appears\nas AWGN, enabling its subtraction with minimal residual interference. To\nfurther mitigate the residual SI, an iterative low-complexity windowing scheme\nis applied, selectively locking onto the radar signal to reduce the processed\nsignal space. A subsequent time-domain spreading step is applied after\nconverting the SIC-processed signal into the post-coded time domain, wherein\nthe SI diminishes separately across the delay and Doppler axes. The proposed\nmethod demonstrates superior performance in terms of detection probability,\ntarget range and velocity root mean square error (RMSE), while maintaining high\nspectral efficiency and minimal computational complexity."}
{"id": "2510.12930", "pdf": "https://arxiv.org/pdf/2510.12930", "abs": "https://arxiv.org/abs/2510.12930", "authors": ["Cory Hilton", "Mohammad Rashid", "Faiz Sherman", "Steven Bush", "Jeffrey A. Nanzer"], "title": "Passive Microwave Tag Classification Using RF Fingerprinting and Machine Learning", "categories": ["eess.SP"], "comment": "7 pages,7 figures", "summary": "We present an approach to identifying wireless microwave tags using radio\nfrequency (RF) fingerprinting and machine learning. The tags are designed for\nlow cost and simplicity, consisting of only two antennas and a single nonlinear\nelement (a diode). An interrogating transceiver transmits a signal consisting\nof a set of individual frequency tones that is captured by the tag. The signal\nresponse of the diode is nonlinear, and can be represented by an infinite power\nseries, the coefficients of which are similar but not identical for different\nphysical diodes due to small manufacturing perturbations. The small differences\nin the signal responses manifest in the spectral signal response of the tag,\nwhich is retransmitted back to the interrogating transceiver. Input into\nmachine learning algorithms, the slight differences in the spectral responses\nof the diodes can be used to uniquely identify devices. To demonstrate the\nconcept, we designed 2.0 GHz tags consisting of patch antennas and a single\ndiode, along with a bi-static radar system operating at the 2.0 GHz 802.11\nWi-Fi band transmitting multi-tone continuous wave signals representing common\n802.11 training fields. The received signals were processed using a set of\nalgorithms for comparison purposes. A real-time classification accuracy of 95%\nbetween two tags was achieved."}
{"id": "2510.12941", "pdf": "https://arxiv.org/pdf/2510.12941", "abs": "https://arxiv.org/abs/2510.12941", "authors": ["SaiKrishna Saketh Yellapragada", "Atchutaram K. Kocharlakota", "MÃ¡rio Costa", "Esa Ollila", "Sergiy A. Vorobyov"], "title": "Computationally Efficient Neural Receivers via Axial Self-Attention", "categories": ["eess.SP"], "comment": "Submitted for IEEE International Conference on Communications", "summary": "Deep learning-based neural receivers are redefining physical-layer signal\nprocessing for next-generation wireless systems. We propose an axial\nself-attention transformer neural receiver designed for applicability to 6G and\nbeyond wireless systems, validated through 5G-compliant experimental\nconfigurations, that achieves state-of-the-art block error rate (BLER)\nperformance with significantly improved computational efficiency. By\nfactorizing attention operations along temporal and spectral axes, the proposed\narchitecture reduces the quadratic complexity of conventional multi-head\nself-attention from $O((TF)^2)$ to $O(T^2F+TF^2)$, yielding substantially fewer\ntotal floating-point operations and attention matrix multiplications per\ntransformer block compared to global self-attention. Relative to convolutional\nneural receiver baselines, the axial neural receiver achieves significantly\nlower computational cost with a fraction of the parameters. Experimental\nvalidation under 3GPP Clustered Delay Line (CDL) channels demonstrates\nconsistent performance gains across varying mobility scenarios. Under\nnon-line-of-sight CDL-C conditions, the axial neural receiver consistently\noutperforms all evaluated receiver architectures, including global\nself-attention, convolutional neural receivers, and traditional LS-LMMSE at\n10\\% BLER with reduced computational complexity per inference. At stringent\nreliability targets of 1\\% BLER, the axial receiver maintains robust symbol\ndetection at high user speeds, whereas the traditional LS-LMMSE receiver fails\nto converge, underscoring its suitability for ultra-reliable low-latency\n(URLLC) communication in dynamic 6G environments and beyond. These results\nestablish the axial neural receiver as a structured, scalable, and efficient\nframework for AI-Native 6G RAN systems, enabling deployment in\nresource-constrained edge environments."}
{"id": "2510.12866", "pdf": "https://arxiv.org/pdf/2510.12866", "abs": "https://arxiv.org/abs/2510.12866", "authors": ["Dantong Niu", "Yuvan Sharma", "Baifeng Shi", "Rachel Ding", "Matteo Gioia", "Haoru Xue", "Henry Tsai", "Konstantinos Kallidromitis", "Anirudh Pai", "Shankar Shastry", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "Learning to Grasp Anything by Playing with Random Toys", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ ."}
{"id": "2510.12968", "pdf": "https://arxiv.org/pdf/2510.12968", "abs": "https://arxiv.org/abs/2510.12968", "authors": ["Najme Ebrahimi", "Arun Paidmarri", "Alexandra Gallyas-Sanhueza", "Yuan Ma", "Haoling Li", "Basem Abdelaziz Abdelmagid", "Tzu-Yuan Huang", "Hua Wang"], "title": "Towards Spectrally Efficient and Physically Reconfigurable Architectures for Multibeam-Waveform Co-Design in Joint Communication and Sensing", "categories": ["eess.SP"], "comment": null, "summary": "Joint Communication and Sensing (JCAS) platforms are emerging as a foundation\nof next-generation mmWave (MMW) and sub-THz systems, enabling both\nhigh-throughput data transfer and angular localization within a shared signal\npath. This paper investigates multibeam architectures for JCAS that\nsimultaneously optimize waveform shaping and beamforming across the time,\nfrequency, code, and direct analog/ radio frequency (RF) domains. The paper\ncompares Orthogonal Frequency-Division Multiplexing (OFDM), Frequency Modulated\nArrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and\nCode-Division Multiple Access (CDMA)-based systems with respect to spectral\nefficiency, beam orthogonality, latency, and Angle-of-Arrival (AoA) estimation\naccuracy. The results highlight architecture-specific tradeoffs among beam\nagility, efficiency, accuracy and resolution, and complexity. It also provides\na framework for selecting JCAS front ends optimized for power, latency,\ninter-beam and multi-user interference, and rapid system reconfiguration"}
{"id": "2510.12919", "pdf": "https://arxiv.org/pdf/2510.12919", "abs": "https://arxiv.org/abs/2510.12919", "authors": ["Mouhyemen Khan", "Tatsuya Ibuki", "Abhijit Chatterjee"], "title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 7 figures, under review", "summary": "Level set methods underpin modern safety techniques such as control barrier\nfunctions (CBFs), while also serving as implicit surface representations for\ngeometric shapes via distance fields. Inspired by these two paradigms, we\npropose a unified framework where the implicit surface itself acts as a CBF. We\nleverage Gaussian process (GP) implicit surface (GPIS) to represent the safety\nboundaries, using safety samples which are derived from sensor measurements to\ncondition the GP. The GP posterior mean defines the implicit safety surface\n(safety belief), while the posterior variance provides a robust safety margin.\nAlthough GPs have favorable properties such as uncertainty estimation and\nanalytical tractability, they scale cubically with data. To alleviate this\nissue, we develop a sparse solution called sparse Gaussian CBFs. To the best of\nour knowledge, GPIS have not been explicitly used to synthesize CBFs. We\nvalidate the approach on collision avoidance tasks in two settings: a simulated\n7-DOF manipulator operating around the Stanford bunny, and a quadrotor\nnavigating in 3D around a physical chair. In both cases, Gaussian CBFs (with\nand without sparsity) enable safe interaction and collision-free execution of\ntrajectories that would otherwise intersect the objects."}
{"id": "2510.13101", "pdf": "https://arxiv.org/pdf/2510.13101", "abs": "https://arxiv.org/abs/2510.13101", "authors": ["Kawon Han", "Kaitao Meng", "Alexandra Chatzicharistou", "Christos Masouros"], "title": "Constellation Design in OFDM-ISAC over Data Payloads: From MSE Analysis to Experimentation", "categories": ["eess.SP"], "comment": "6 pages", "summary": "Orthogonal frequency division multiplexing (OFDM) is one of the most widely\nadopted waveforms for integrated sensing and communication (ISAC) systems,\nowing to its high spectral efficiency and compatibility with modern\ncommunication standards. This paper investigates the sensing performance of\nOFDM-based ISAC for multi-target delay (range) estimation under specific radar\nreceiver processing schemes. An estimation-theoretic framework is developed to\ncharacterize sensing performance with random communication payloads. We\nestablish the fundamental limit of delay estimation accuracy by deriving the\nclosed-form expression of the mean-square error (MSE) achieved using matched\nfiltering (MF) and reciprocal filtering (RF) receivers. The results show that,\nin multi-target scenarios, the impact of signal constellations on the delay\nestimation MSE differs across receivers: MF performance depends on the\nfourth-order moment of the zero-mean, unit-power constellation in the presence\nof multiple targets, whereas RF performance depends on its inverse second-order\nmoment, irrespective of the number of targets. Building on this analysis, we\npresent a ISAC constellation design under specific receiver architecture that\nbrings a receiver-dependent flexible trade-off between sensing and\ncommunication in OFDM-ISAC systems. The theoretical findings are validated\nthrough simulations and proof-of-concept experiments, and also the sensing and\ncommunication performance trade-off is experimentally shown with the proposed\nconstellation design."}
{"id": "2510.12924", "pdf": "https://arxiv.org/pdf/2510.12924", "abs": "https://arxiv.org/abs/2510.12924", "authors": ["Pavel PochobradskÃ½", "OndÅej ProchÃ¡zka", "Robert PÄniÄka", "VojtÄch VonÃ¡sek", "Martin Saska"], "title": "Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this letter, we introduce Geometric Model Predictive Path Integral\n(GMPPI), a sampling-based controller capable of tracking agile trajectories\nwhile avoiding obstacles. In each iteration, GMPPI generates a large number of\ncandidate rollout trajectories and then averages them to create a nominal\ncontrol to be followed by the Unmanned Aerial Vehicle (UAV). We propose using\ngeometric SE(3) control to generate part of the rollout trajectories,\nsignificantly increasing precision in agile flight. Furthermore, we introduce\nvarying rollout simulation time step length and dynamic cost and noise\nparameters, vastly improving tracking performance of smooth and low-speed\ntrajectories over an existing Model Predictive Path Integral (MPPI)\nimplementation. Finally, we propose an integration of GMPPI with a stereo depth\ncamera, enabling online obstacle avoidance at high speeds, a crucial step\ntowards autonomous UAV flights in complex environments. The proposed controller\ncan track simulated agile reference trajectories with position error similar to\nthe geometric SE(3) controller. However, the same configuration of the proposed\ncontroller can avoid obstacles in a simulated forest environment at speeds of\nup to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware\nplanner. In real-world experiments, GMPPI retains the capability to track agile\ntrajectories and avoids obstacles at speeds of up to 10m/s."}
{"id": "2510.13399", "pdf": "https://arxiv.org/pdf/2510.13399", "abs": "https://arxiv.org/abs/2510.13399", "authors": ["Shivani Ranjan", "Anant Jain", "Robin Badal", "Amit Kumar", "Harshal Shende", "Deepak Joshi", "Pramod Yadav", "Lalan Kumar"], "title": "Working Memory Functional Connectivity Analysis for Dementia Classification using EEG", "categories": ["eess.SP"], "comment": null, "summary": "Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive\nneurodegenerative disorder marked by cognitive decline. Early detection,\nespecially at the Mild Cognitive Impairment (MCI) stage, is essential for\ntimely intervention. Working Memory (WM) impairment is a key early indicator of\nneurodegeneration, affecting higher cognitive processes. Electroencephalography\n(EEG), with its high temporal resolution, offers a cost-effective method to\nassess brain dynamics. This study investigates WM-related EEG functional\nconnectivity (FC) to identify brain network alterations across dementia stages.\nMethods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8\nhealthy controls) during WM tasks, including encoding, recall, and retrieval\nstages. Data preprocessing involved noise reduction and feature extraction\nusing Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified\nusing Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network\nmetrics such as Degree and Eigenvector Centrality were analyzed using Support\nVector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based\nconnectivity metrics outperformed the traditional PLI approach in\ndifferentiating dementia stages, attaining a peak classification accuracy of\n97.53% during the retrieval phase with the Random Forest model. A connectivity\nthreshold of 0.5 was optimal for network discrimination. SHD and HHD features\nalso demonstrated strong discriminative potential. AD subjects exhibited higher\nsynchronization patterns during WM tasks than healthy controls. Conclusions:\nThe integration of WM tasks with EEG-based FC analysis provides a robust\nframework for dementia classification. The proposed CPTE-based approach offers\na robust, scalable, non-invasive, and effective diagnostic tool for early\ndetection and monitoring of neurodegenerative diseases."}
{"id": "2510.12962", "pdf": "https://arxiv.org/pdf/2510.12962", "abs": "https://arxiv.org/abs/2510.12962", "authors": ["Michal MinaÅÃ­k", "VojtÄch VonÃ¡sek", "Robert PÄniÄka"], "title": "Enhancing Sampling-based Planning with a Library of Paths", "categories": ["cs.RO"], "comment": null, "summary": "Path planning for 3D solid objects is a challenging problem, requiring a\nsearch in a six-dimensional configuration space, which is, nevertheless,\nessential in many robotic applications such as bin-picking and assembly. The\ncommonly used sampling-based planners, such as Rapidly-exploring Random Trees,\nstruggle with narrow passages where the sampling probability is low, increasing\nthe time needed to find a solution. In scenarios like robotic bin-picking,\nvarious objects must be transported through the same environment. However,\ntraditional planners start from scratch each time, losing valuable information\ngained during the planning process. We address this by using a library of past\nsolutions, allowing the reuse of previous experiences even when planning for a\nnew, previously unseen object. Paths for a set of objects are stored, and when\nplanning for a new object, we find the most similar one in the library and use\nits paths as approximate solutions, adjusting for possible mutual\ntransformations. The configuration space is then sampled along the approximate\npaths. Our method is tested in various narrow passage scenarios and compared\nwith state-of-the-art methods from the OMPL library. Results show significant\nspeed improvements (up to 85% decrease in the required time) of our method,\noften finding a solution in cases where the other planners fail. Our\nimplementation of the proposed method is released as an open-source package."}
{"id": "2510.13442", "pdf": "https://arxiv.org/pdf/2510.13442", "abs": "https://arxiv.org/abs/2510.13442", "authors": ["Lorenz Mohr", "Marc Miranda", "Sebastian Semper", "Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Christian Schneider", "Reiner S. ThomÃ¤"], "title": "Oscillator Drift Compensation by Line-of-Sight Tracking for Distributed Multisensor ISAC", "categories": ["eess.SP"], "comment": "6 pages, 4 figures", "summary": "We observed synchronization mismatches in the form of non-smooth phase\nprogressions and drifts within mobile multisensor channel sounding\nmeasurements. However, performing Doppler estimation in a distributed\nmultisensor integrated sensing and communications (ISAC) system requires\ncoherence among the nodes, which implies a continuously differentiable phase\nprogression of the received signals. To correct the sounding data in\npost-processing, we extend traditional geometry-based drift compensation\nalgorithms by utilizing Kalman filtering for line-of-sight (LoS) tracking,\nwhich improves the robustness of the LoS estimate in multipath scenarios. This\napproach smooths the phase progression and enables the correction of\ntime-varying drifts while preserving relative sensor motion. Furthermore, we\npropose using the relative residual power after high-resolution parameter\nestimation (HRPE) as a metric for ground-truth-independent comparison of\npost-processing synchronization methods for recorded channel sounding data.\nResults show that the proposed approach outperforms traditional LoS estimation\nheuristics, reducing the relative residual power by more than 5 dB and the\ndelay-Doppler estimate root mean square errors (RMSEs) by approximately 60 %."}
{"id": "2510.12970", "pdf": "https://arxiv.org/pdf/2510.12970", "abs": "https://arxiv.org/abs/2510.12970", "authors": ["Baxi Chong", "Tianyu Wang", "Kelimar Diaz", "Christopher J. Pierce", "Eva Erickson", "Julian Whitman", "Yuelin Deng", "Esteban Flores", "Ruijie Fu", "Juntao He", "Jianfeng Lin", "Hang Lu", "Guillaume Sartoretti", "Howie Choset", "Daniel I. Goldman"], "title": "The Omega Turn: A General Turning Template for Elongate Robots", "categories": ["cs.RO"], "comment": null, "summary": "Elongate limbless robots have the potential to locomote through tightly\npacked spaces for applications such as search-and-rescue and industrial\ninspections. The capability to effectively and robustly maneuver elongate\nlimbless robots is crucial to realize such potential. However, there has been\nlimited research on turning strategies for such systems. To achieve effective\nand robust turning performance in cluttered spaces, we take inspiration from a\nmicroscopic nematode, C. elegans, which exhibits remarkable maneuverability in\nrheologically complex environments partially because of its ability to perform\nomega turns. Despite recent efforts to analyze omega turn kinematics, it\nremains unknown if there exists a wave equation sufficient to prescribe an\nomega turn, let alone its reconstruction on robot platforms. Here, using a\ncomparative theory-biology approach, we prescribe the omega turn as a\nsuperposition of two traveling waves. With wave equations as a guideline, we\ndesign a controller for limbless robots enabling robust and effective turning\nbehaviors in lab and cluttered field environments. Finally, we show that such\nomega turn controllers can also generalize to elongate multi-legged robots,\ndemonstrating an alternative effective body-driven turning strategy for\nelongate robots, with and without limbs."}
{"id": "2510.13495", "pdf": "https://arxiv.org/pdf/2510.13495", "abs": "https://arxiv.org/abs/2510.13495", "authors": ["Dexin Kong", "Diana Pamela Moya Osorio", "Erik G. Larsson"], "title": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning", "categories": ["eess.SP"], "comment": null, "summary": "Recent advancements in polymer microwave fiber (PMF) technology have created\nsignificant opportunities for robust, low-cost, and high-speed sub-terahertz\n(THz) radio-over- fiber communications. Recognizing these potential benefits,\nthis paper explores a novel radio-over-fiber (RoF) structure that interconnects\nmultiple radio units (RUs) in cascade via fiber, envi- sioning its application\nin indoor scenarios. This structure creates a number of research opportunities\nwhen considering cascaded distortion effects introduced by non-linear power\namplifiers (PAs) at the RUs and the propagation channel over the fiber. We\npropose maximum-likelihood and non-linear least-squares algorithms to estimate\nthe propagation distance along the RoF and the time-of-arrival between the RoF\nand the user equipment. For the case of linear PAs, we derive the Cram\\'er-Rao\nlower bound to benchmark the performance of the estimators. Finally, we\ninvestigate the use of the system for uplink positioning. Our simulation\nresults demonstrate that the proposed estimators perform satisfactorily even\nwith the cascaded effects of non- linear PAs, and that the deployment of this\nRoF structure can enable new cost-effective opportunities for high-resolution\npositioning in indoor scenarios. In the numerical evaluation, we also use\nmeasured PMF characteristics for high-density polyethylene fibers."}
{"id": "2510.12971", "pdf": "https://arxiv.org/pdf/2510.12971", "abs": "https://arxiv.org/abs/2510.12971", "authors": ["Anran Zhang", "Hanzhi Chen", "Yannick Burkhardt", "Yao Zhong", "Johannes Betz", "Helen Oleynikova", "Stefan Leutenegger"], "title": "Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "We present Actron3D, a framework that enables robots to acquire transferable\n6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only\nhuman videos. At its core lies the Neural Affordance Function, a compact\nobject-centric representation that distills actionable cues from diverse\nuncalibrated videos-geometry, visual appearance, and affordance-into a\nlightweight neural network, forming a memory bank of manipulation skills.\nDuring deployment, we adopt a pipeline that retrieves relevant affordance\nfunctions and transfers precise 6-DoF manipulation policies via coarse-to-fine\noptimization, enabled by continuous queries to the multimodal features encoded\nin the neural functions. Experiments in both simulation and the real world\ndemonstrate that Actron3D significantly outperforms prior methods, achieving a\n14.9 percentage point improvement in average success rate across 13 tasks while\nrequiring only 2-3 demonstration videos per task."}
{"id": "2510.13498", "pdf": "https://arxiv.org/pdf/2510.13498", "abs": "https://arxiv.org/abs/2510.13498", "authors": ["Mingyu Zhao", "Qingna Li", "Hou-Duo Qi"], "title": "A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements", "categories": ["eess.SP", "math.OC"], "comment": "12 pages, 9 figures", "summary": "For the problem of source localization, three elements usually play a very\nimportant role in accurate localization. They are the range measurements, the\nangle measurements and the least absolute deviation criterion, which is\nregarded as a robust metric for denoising the measurements. Building the three\nelements into a computationally tractable model is challenging. In this paper,\nwe introduce a robust Euclidean Distance Matrix (EDM) optimization model that\nsimultaneously incorporates the three elements. For the first time, we show\nthat for the case of 3D single-source localization (3DSSL), the angle\nmeasurements can be represented as a simple box constraint of distances. It is\nachieved by reducing each of the 3D angle measurements to a two-dimensional\nnonlinear optimization problem, whose global minimum and maximum solutions can\nbe characterized and utilized to get the lower and upper bounds of the\ndistances from the unknown source to the sensors. We further develop an\nefficient algorithm. The high quality of the localization by the new EDM model\nis assessed through extensive numerical experiments in comparison with leading\nsolvers for 3DSSL."}
{"id": "2510.12992", "pdf": "https://arxiv.org/pdf/2510.12992", "abs": "https://arxiv.org/abs/2510.12992", "authors": ["Neel P. Bhatt", "Po-han Li", "Kushagra Gupta", "Rohan Siva", "Daniel Milan", "Alexander T. Hogue", "Sandeep P. Chinchali", "David Fridovich-Keil", "Zhangyang Wang", "Ufuk Topcu"], "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.MA"], "comment": null, "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/"}
{"id": "2510.12832", "pdf": "https://arxiv.org/pdf/2510.12832", "abs": "https://arxiv.org/abs/2510.12832", "authors": ["Alistair Brash", "Junyi Lu", "Bruce Stephen", "Blair Brown", "Robert Atkinson", "Craig Michie", "Fraser MacIntyre", "Christos Tachtatzis"], "title": "Coherent Load Profile Synthesis with Conditional Diffusion for LV Distribution Network Scenario Generation", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "eess.SP"], "comment": null, "summary": "Limited visibility of power distribution network power flows at the low\nvoltage level presents challenges to both distribution network operators from a\nplanning perspective and distribution system operators from a congestion\nmanagement perspective. Forestalling these challenges through scenario analysis\nis confounded by the lack of realistic and coherent load data across\nrepresentative distribution feeders. Load profiling approaches often rely on\nsummarising demand through typical profiles, which oversimplifies the\ncomplexity of substation-level operations and limits their applicability in\nspecific power system studies. Sampling methods, and more recently generative\nmodels, have attempted to address this through synthesising representative\nloads from historical exemplars; however, while these approaches can\napproximate load shapes to a convincing degree of fidelity, the co-behaviour\nbetween substations, which ultimately impacts higher voltage level network\noperation, is often overlooked. This limitation will become even more\npronounced with the increasing integration of low-carbon technologies, as\nestimates of base loads fail to capture load diversity. To address this gap, a\nConditional Diffusion model for synthesising daily active and reactive power\nprofiles at the low voltage distribution substation level is proposed. The\nevaluation of fidelity is demonstrated through conventional metrics capturing\ntemporal and statistical realism, as well as power flow modelling. The results\nshow synthesised load profiles are plausible both independently and as a cohort\nin a wider power systems context. The Conditional Diffusion model is\nbenchmarked against both naive and state-of-the-art models to demonstrate its\neffectiveness in producing realistic scenarios on which to base sub-regional\npower distribution network planning and operations."}
{"id": "2510.13005", "pdf": "https://arxiv.org/pdf/2510.13005", "abs": "https://arxiv.org/abs/2510.13005", "authors": ["Robert Muldrow", "Channing Ludden", "Christopher Petersen"], "title": "Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations", "categories": ["cs.RO"], "comment": "12 pages, 4 figures, AAS/AIAA Space Flight Mechanics", "summary": "In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging\noperations that provides several benefits to improve the longevity, capacity,\nmo- bility, and expandability of existing and future space assets. Serial\nrobotic ma- nipulators are particularly vital in accomplishing ISAM operations,\nhowever, the complex perturbation forces and motions associated with movement\nof a robotic arm on a free-flying satellite presents a complex controls problem\nrequiring addi- tional study. While many dynamical models are developed,\nexperimentally test- ing and validating these models is challenging given that\nthe models operate in space, where satellites have six-degrees-of-freedom\n(6-DOF). This paper attempts to resolve those challenges by presenting the\ndesign and development of a new hardware-in-the-loop (HIL) experimental testbed\nutilized to emulate ISAM. This emulation will be accomplished by means of a\n6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is\nmounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic\narm to move freely in one linear direction. This experimental ISAM emulation\nsystem will explore and validate models for space motion, serial robot\nmanipulation, and contact mechanics."}
{"id": "2510.12983", "pdf": "https://arxiv.org/pdf/2510.12983", "abs": "https://arxiv.org/abs/2510.12983", "authors": ["Lorenzo Marinucci", "Gabriele D'Acunto", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "Simplicial Gaussian Models: Representation and Inference", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "comment": null, "summary": "Probabilistic graphical models (PGMs) are powerful tools for representing\nstatistical dependencies through graphs in high-dimensional systems. However,\nthey are limited to pairwise interactions. In this work, we propose the\nsimplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial\ncomplexes. SGM jointly models random variables supported on vertices, edges,\nand triangles, within a single parametrized Gaussian distribution. Our model\nbuilds upon discrete Hodge theory and incorporates uncertainty at every\ntopological level through independent random components. Motivated by\napplications, we focus on the marginal edge-level distribution while treating\nnode- and triangle-level variables as latent. We then develop a\nmaximum-likelihood inference algorithm to recover the parameters of the full\nSGM and the induced conditional dependence structure. Numerical experiments on\nsynthetic simplicial complexes with varying size and sparsity confirm the\neffectiveness of our algorithm."}
{"id": "2510.13048", "pdf": "https://arxiv.org/pdf/2510.13048", "abs": "https://arxiv.org/abs/2510.13048", "authors": ["Minghao Guo", "Victor Zordan", "Sheldon Andrews", "Wojciech Matusik", "Maneesh Agrawala", "Hsueh-Ti Derek Liu"], "title": "Kinematic Kitbashing for Modeling Functional Articulated Objects", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "We introduce Kinematic Kitbashing, an automatic framework that synthesizes\nfunctionality-aware articulated objects by reusing parts from existing models.\nGiven a kinematic graph with a small collection of articulated parts, our\noptimizer jointly solves for the spatial placement of every part so that (i)\nattachments remain geometrically sound over the entire range of motion and (ii)\nthe assembled object satisfies user-specified functional goals such as\ncollision-free actuation, reachability, or trajectory following. At its core is\na kinematics-aware attachment energy that aligns vector distance function\nfeatures sampled across multiple articulation snapshots. We embed this\nattachment term within an annealed Riemannian Langevin dynamics sampler that\ntreats functionality objectives as additional energies, enabling robust global\nexploration while accommodating non-differentiable functionality objectives and\nconstraints. Our framework produces a wide spectrum of assembled articulated\nshapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,\ngear-driven paddlers, and reconfigurable furniture, and delivers strong\nquantitative improvements over state-of-the-art baselines across geometric,\nkinematic, and functional metrics. By tightly coupling articulation-aware\ngeometry matching with functionality-driven optimization, Kinematic Kitbashing\nbridges part-based shape modeling and functional assembly design, empowering\nrapid creation of interactive articulated assets."}
{"id": "2510.13052", "pdf": "https://arxiv.org/pdf/2510.13052", "abs": "https://arxiv.org/abs/2510.13052", "authors": ["Muhammad Faraz Ul Abrar", "NicolÃ² Michelusi", "Erik G. Larsson"], "title": "Time-Varying Optimization for Streaming Data Via Temporal Weighting", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SP", "eess.SY", "math.OC"], "comment": "Accepted at IEEE Asilomar, 2025", "summary": "Classical optimization theory deals with fixed, time-invariant objective\nfunctions. However, time-varying optimization has emerged as an important\nsubject for decision-making in dynamic environments. In this work, we study the\nproblem of learning from streaming data through a time-varying optimization\nlens. Unlike prior works that focus on generic formulations, we introduce a\nstructured, \\emph{weight-based} formulation that explicitly captures the\nstreaming-data origin of the time-varying objective, where at each time step,\nan agent aims to minimize a weighted average loss over all the past data\nsamples. We focus on two specific weighting strategies: (1) uniform weights,\nwhich treat all samples equally, and (2) discounted weights, which\ngeometrically decay the influence of older data. For both schemes, we derive\ntight bounds on the ``tracking error'' (TE), defined as the deviation between\nthe model parameter and the time-varying optimum at a given time step, under\ngradient descent (GD) updates. We show that under uniform weighting, the TE\nvanishes asymptotically with a $\\mathcal{O}(1/t)$ decay rate, whereas\ndiscounted weighting incurs a nonzero error floor controlled by the discount\nfactor and the number of gradient updates performed at each time step. Our\ntheoretical findings are validated through numerical simulations."}
{"id": "2510.13054", "pdf": "https://arxiv.org/pdf/2510.13054", "abs": "https://arxiv.org/abs/2510.13054", "authors": ["Ankit Goyal", "Hugo Hadfield", "Xuning Yang", "Valts Blukis", "Fabio Ramos"], "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding $\\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like $\\pi_0.5$-KI, $\\pi_0$, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/."}
{"id": "2510.13077", "pdf": "https://arxiv.org/pdf/2510.13077", "abs": "https://arxiv.org/abs/2510.13077", "authors": ["Yubo Zhang", "Xiao-Yang Liu", "Xiaodong Wang"], "title": "Transformer-based Scalable Beamforming Optimization via Deep Residual Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "7 pages, 5 figures", "summary": "We develop an unsupervised deep learning framework for downlink beamforming\nin large-scale MU-MISO channels. The model is trained offline, allowing\nreal-time inference through lightweight feedforward computations in dynamic\ncommunication environments. Following the learning-to-optimize (L2O) paradigm,\na multi-layer Transformer iteratively refines both channel and beamformer\nfeatures via residual connections. To enhance training, three strategies are\nintroduced: (i) curriculum learning (CL) to improve early-stage convergence and\navoid local optima, (ii) semi-amortized learning to refine each Transformer\nblock with a few gradient ascent steps, and (iii) sliding-window training to\nstabilize optimization by training only a subset of Transformer blocks at a\ntime. Extensive simulations show that the proposed scheme outperforms existing\nbaselines at low-to-medium SNRs and closely approaches WMMSE performance at\nhigh SNRs, while achieving substantially faster inference than iterative and\nonline learning approaches."}
{"id": "2510.13149", "pdf": "https://arxiv.org/pdf/2510.13149", "abs": "https://arxiv.org/abs/2510.13149", "authors": ["Yangtao Chen", "Zixuan Chen", "Nga Teng Chan", "Junting Chen", "Junhui Yin", "Jieqi Shi", "Yang Gao", "Yong-Lu Li", "Jing Huo"], "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation", "categories": ["cs.RO"], "comment": "Under review. These first two authors contributed equally to this\n  work", "summary": "Enabling robots to flexibly schedule and compose learned skills for novel\nlong-horizon manipulation under diverse perturbations remains a core challenge.\nEarly explorations with end-to-end VLA models show limited success, as these\nmodels struggle to generalize beyond the training distribution. Hierarchical\napproaches, where high-level planners generate subgoals for low-level policies,\nbring certain improvements but still suffer under complex perturbations,\nrevealing limited capability in skill composition. However, existing benchmarks\nprimarily emphasize task completion in long-horizon settings, offering little\ninsight into compositional generalization, robustness, and the interplay\nbetween planning and execution. To systematically investigate these gaps, we\npropose RoboHiMan, a hierarchical evaluation paradigm for compositional\ngeneralization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,\na benchmark of atomic and compositional tasks under diverse perturbations,\nsupported by a multi-level training dataset for analyzing progressive data\nscaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)\nthat probe the necessity of skill composition and reveal bottlenecks in\nhierarchical architectures. Experiments highlight clear capability gaps across\nrepresentative models and architectures, pointing to directions for advancing\nmodels better suited to real-world long-horizon manipulation tasks. Videos and\nopen-source code can be found on our project website:\nhttps://chenyt31.github.io/robo-himan.github.io/."}
{"id": "2510.13209", "pdf": "https://arxiv.org/pdf/2510.13209", "abs": "https://arxiv.org/abs/2510.13209", "authors": ["Lipeng Zhu", "Haobin Mao", "Ge Yan", "Wenyan Ma", "Zhenyu Xiao", "Rui Zhang"], "title": "Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "The growing demands of 6G mobile communication networks necessitate advanced\nantenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)\nenable dynamic control over antenna's position, orientation, radiation,\npolarization, and frequency response, introducing rich electromagnetic-domain\ndegrees of freedom for the design and performance enhancement of wireless\nsystems. This article overviews their application scenarios, hardware\narchitectures, and design methods. Field test and simulation results highlight\ntheir performance benefits over conventional fixed/non-reconfigurable antennas."}
{"id": "2510.13284", "pdf": "https://arxiv.org/pdf/2510.13284", "abs": "https://arxiv.org/abs/2510.13284", "authors": ["Haoyang Wu", "Siheng Wu", "William X. Liu", "Fangui Zeng"], "title": "ALOHA2 Robot Kitchen Application Scenario Reproduction Report", "categories": ["cs.RO"], "comment": null, "summary": "ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,\nfeaturing higher performance and robustness compared to the original design,\nwhile also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers\nand two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control\nthe follower mechanical arms by operating the leader mechanical arms through\nback-driving. The device also includes cameras that generate images from\nmultiple viewpoints, allowing for RGB data collection during teleoperation. The\nrobot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame\nthat provides additional mounting points for cameras and gravity compensation\nsystems."}
{"id": "2510.13408", "pdf": "https://arxiv.org/pdf/2510.13408", "abs": "https://arxiv.org/abs/2510.13408", "authors": ["Jingkai Ying", "Zhiyuan Qi", "Yulong Feng", "Zhijin Qin", "Zhu Han", "Rahim Tafazolli", "Yonina C. Eldar"], "title": "Semantic Communication Enabled Holographic Video Processing and Transmission", "categories": ["eess.IV", "cs.AI", "cs.IT", "cs.MM", "eess.SP", "math.IT"], "comment": "7 pages, 6 figures, Submit for review", "summary": "Holographic video communication is considered a paradigm shift in visual\ncommunications, becoming increasingly popular for its ability to offer\nimmersive experiences. This article provides an overview of holographic video\ncommunication and outlines the requirements of a holographic video\ncommunication system. Particularly, following a brief review of semantic com-\nmunication, an architecture for a semantic-enabled holographic video\ncommunication system is presented. Key technologies, including semantic\nsampling, joint semantic-channel coding, and semantic-aware transmission, are\ndesigned based on the proposed architecture. Two related use cases are\npresented to demonstrate the performance gain of the proposed methods. Finally,\npotential research topics are discussed to pave the way for the realization of\nsemantic-enabled holographic video communications."}
{"id": "2510.13287", "pdf": "https://arxiv.org/pdf/2510.13287", "abs": "https://arxiv.org/abs/2510.13287", "authors": ["Nishant Chandna", "Akshat Kaushal"], "title": "DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping", "categories": ["cs.RO"], "comment": "Accepted at IROS Active Perception Workshop", "summary": "LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for\nenabling precise navigation and environmental reconstruction across various\napplications. Although current point-to-plane ICP algorithms perform effec-\ntively in structured, feature-rich environments, they struggle in scenarios\nwith sparse features, repetitive geometric structures, and high-frequency\nmotion. This leads to degeneracy in 6- DOF pose estimation. Most\nstate-of-the-art algorithms address these challenges by incorporating\nadditional sensing modalities, but LiDAR-only solutions continue to face\nlimitations under such conditions. To address these issues, we propose a novel\nDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.\nOur system improves mapping accuracy through point cloud classification based\non surface normals and neighborhood analysis. Points are classified into\nground, walls, roof, edges, and non-planar points, enabling accurate\ncorrespondences. A Degeneracy-based weighted least squares-based ICP algorithm\nis then applied for accurate odom- etry estimation. Additionally, a Scan\nContext based back-end is implemented to support robust loop closures.\nDAMM-LOAM demonstrates significant improvements in odometry accuracy,\nespecially in indoor environments such as long corridors"}
{"id": "2510.13324", "pdf": "https://arxiv.org/pdf/2510.13324", "abs": "https://arxiv.org/abs/2510.13324", "authors": ["Erik Helmut", "Niklas Funk", "Tim Schneider", "Cristiana de Farias", "Jan Peters"], "title": "Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Contact-rich manipulation depends on applying the correct grasp forces\nthroughout the manipulation task, especially when handling fragile or\ndeformable objects. Most existing imitation learning approaches often treat\nvisuotactile feedback only as an additional observation, leaving applied forces\nas an uncontrolled consequence of gripper commands. In this work, we present\nForce-Aware Robotic Manipulation (FARM), an imitation learning framework that\nintegrates high-dimensional tactile data to infer tactile-conditioned force\nsignals, which in turn define a matching force-based action space. We collect\nhuman demonstrations using a modified version of the handheld Universal\nManipulation Interface (UMI) gripper that integrates a GelSight Mini visual\ntactile sensor. For deploying the learned policies, we developed an actuated\nvariant of the UMI gripper with geometry matching our handheld version. During\npolicy rollouts, the proposed FARM diffusion policy jointly predicts robot\npose, grip width, and grip force. FARM outperforms several baselines across\nthree tasks with distinct force requirements -- high-force, low-force, and\ndynamic force adaptation -- demonstrating the advantages of its two key\ncomponents: leveraging force-grounded, high-dimensional tactile observations\nand a force-based control space. The codebase and design files are open-sourced\nand available at https://tactile-farm.github.io ."}
{"id": "2510.13356", "pdf": "https://arxiv.org/pdf/2510.13356", "abs": "https://arxiv.org/abs/2510.13356", "authors": ["Jie Gu", "Tin Lun Lam", "Chunxu Tian", "Zhihao Xia", "Yongheng Xing", "Dan Zhang"], "title": "MODUR: A Modular Dual-reconfigurable Robot", "categories": ["cs.RO"], "comment": null, "summary": "Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots\ncapable of forming higher-level robotic systems by altering the topological\nrelationships between modules, offering enhanced adaptability and robustness in\nvarious environments. This paper presents a novel MSRR called MODUR, featuring\ndual-level reconfiguration capabilities designed to integrate reconfigurable\nmechanisms into MSRR. Specifically, MODUR can perform high-level\nself-reconfiguration among modules to create different configurations, while\neach module is also able to change its shape to execute basic motions. The\ndesign of MODUR primarily includes a compact connector and scissor linkage\ngroups that provide actuation, forming a parallel mechanism capable of\nachieving both connector motion decoupling and adjacent position migration\ncapabilities. Furthermore, the workspace, considering the interdependent\nconnectors, is comprehensively analyzed, laying a theoretical foundation for\nthe design of the module's basic motion. Finally, the motion of MODUR is\nvalidated through a series of experiments."}
{"id": "2510.13358", "pdf": "https://arxiv.org/pdf/2510.13358", "abs": "https://arxiv.org/abs/2510.13358", "authors": ["Shingo Ayabe", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control", "categories": ["cs.RO", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability."}
{"id": "2510.13443", "pdf": "https://arxiv.org/pdf/2510.13443", "abs": "https://arxiv.org/abs/2510.13443", "authors": ["Mojtaba Mollahossein", "Gholamreza Vossoughi", "Mohammad Hossein Rohban"], "title": "Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets", "categories": ["cs.RO"], "comment": null, "summary": "Electromyography (EMG) signals are widely used for predicting body joint\nangles through machine learning (ML) and deep learning (DL) methods. However,\nthese approaches often face challenges such as limited real-time applicability,\nnon-representative test conditions, and the need for large datasets to achieve\noptimal performance. This paper presents a transfer-learning framework for knee\njoint angle prediction that requires only a few gait cycles from new subjects.\nThree datasets - Georgia Tech, the University of California Irvine (UCI), and\nthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels\nrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTM\nmodel was developed and pre-trained on the Georgia Tech dataset, then\ntransferred to the UCI and SMLE datasets. The proposed model achieved\nNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for\none-step and 50-step predictions on abnormal subjects using EMG inputs alone.\nIncorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5\npercent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal\nsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and\ninteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE\nfor one- and 50-step predictions, respectively. These results demonstrate\nrobust performance and strong generalization for both short- and long-term\nrehabilitation scenarios."}
{"id": "2510.13488", "pdf": "https://arxiv.org/pdf/2510.13488", "abs": "https://arxiv.org/abs/2510.13488", "authors": ["Maximilian Stasica", "Arne Bick", "Nico Bohlinger", "Omid Mohseni", "Max Johannes Alois Fritzsche", "Clemens HÃ¼bler", "Jan Peters", "AndrÃ© Seyfarth"], "title": "Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations", "categories": ["cs.RO"], "comment": null, "summary": "Legged robots, particularly quadrupeds, excel at navigating rough terrains,\nyet their performance under vertical ground perturbations, such as those from\noscillating surfaces, remains underexplored. This study introduces a novel\napproach to enhance quadruped locomotion robustness by training the Unitree Go2\nrobot on an oscillating bridge - a 13.24-meter steel-and-concrete structure\nwith a 2.0 Hz eigenfrequency designed to perturb locomotion. Using\nReinforcement Learning (RL) with the Proximal Policy Optimization (PPO)\nalgorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,\ncombining five gaits (trot, pace, bound, free, default) with three training\nconditions: rigid bridge and two oscillating bridge setups with differing\nheight regulation strategies (relative to bridge surface or ground). Domain\nrandomization ensured zero-shot transfer to the real-world bridge. Our results\ndemonstrate that policies trained on the oscillating bridge exhibit superior\nstability and adaptability compared to those trained on rigid surfaces. Our\nframework enables robust gait patterns even without prior bridge exposure.\nThese findings highlight the potential of simulation-based RL to improve\nquadruped locomotion during dynamic ground perturbations, offering insights for\ndesigning robots capable of traversing vibrating environments."}
{"id": "2510.13535", "pdf": "https://arxiv.org/pdf/2510.13535", "abs": "https://arxiv.org/abs/2510.13535", "authors": ["Wentao Guo", "Yizhou Wang", "Wenzeng Zhang"], "title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints", "categories": ["cs.RO"], "comment": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025, Hangzhou. This version includes updated contact\n  information", "summary": "This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability."}
{"id": "2510.13553", "pdf": "https://arxiv.org/pdf/2510.13553", "abs": "https://arxiv.org/abs/2510.13553", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping", "categories": ["cs.RO"], "comment": "Accepted by IEEE International Conference on Robotics and Biomimetics\n  (IROS) 2025, Hangzhou, China. This version includes updated contact\n  information", "summary": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments."}
{"id": "2510.13594", "pdf": "https://arxiv.org/pdf/2510.13594", "abs": "https://arxiv.org/abs/2510.13594", "authors": ["Austin Barret", "Meng Cheng Lau"], "title": "Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots", "categories": ["cs.RO"], "comment": "9 Figure. Presented at FIRA Summit 2025, Daegu, S. Korea", "summary": "The operation of humanoid robotics is an essential field of research with\nmany practical and competitive applications. Many of these systems, however, do\nnot invest heavily in developing a non-expert-centered graphical user interface\n(GUI) for operation. The focus of this research is to develop a scalable GUI\nthat is tailored to be simple and intuitive so non-expert operators can control\nthe robot through a FIRA-regulated obstacle course. Using common practices from\nuser interface development (UI) and understanding concepts described in\nhuman-robot interaction (HRI) and other related concepts, we will develop a new\ninterface with the goal of a non-expert teleoperation system."}
{"id": "2510.13595", "pdf": "https://arxiv.org/pdf/2510.13595", "abs": "https://arxiv.org/abs/2510.13595", "authors": ["Ethan K. Gordon", "Bruke Baraki", "Hien Bui", "Michael Posa"], "title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile"}
{"id": "2510.13599", "pdf": "https://arxiv.org/pdf/2510.13599", "abs": "https://arxiv.org/abs/2510.13599", "authors": ["Jiahao Wang", "Nived Chebrolu", "Yifu Tao", "Lintong Zhang", "Ayoung Kim", "Maurice Fallon"], "title": "PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "Building an online 3D LiDAR mapping system that produces a detailed surface\nreconstruction while remaining computationally efficient is a challenging task.\nIn this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR\nreconstruction system that adaptively adjusts mesh resolution to achieve\ncompact, detailed reconstructions in real-time. It introduces a new\nrepresentation, planar-mesh, which combines plane modeling and meshing to\ncapture both large surfaces and detailed geometry. The planar-mesh can be\nincrementally updated considering both local surface curvature and free-space\ninformation from sensor measurements. We employ a multi-threaded architecture\nwith a Bounding Volume Hierarchy (BVH) for efficient data storage and fast\nsearch operations, enabling real-time performance. Experimental results show\nthat our method achieves reconstruction accuracy on par with, or exceeding,\nstate-of-the-art techniques-including truncated signed distance functions,\noccupancy mapping, and voxel-based meshing-while producing smaller output file\nsizes (10 times smaller than raw input and more than 5 times smaller than\nmesh-based methods) and maintaining real-time performance (around 2 Hz for a\n64-beam sensor)."}
{"id": "2510.13616", "pdf": "https://arxiv.org/pdf/2510.13616", "abs": "https://arxiv.org/abs/2510.13616", "authors": ["Preston Fairchild", "Claudia Chen", "Xiaobo Tan"], "title": "Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "For supplementary videos, see\n  https://drive.google.com/drive/folders/1jol-_z6gaUfjpL1Qi7EG420usTbVSodv?usp=sharing", "summary": "Properly handling delicate produce with robotic manipulators is a major part\nof the future role of automation in agricultural harvesting and processing.\nGrasping with the correct amount of force is crucial in not only ensuring\nproper grip on the object, but also to avoid damaging or bruising the product.\nIn this work, a flexible pressure sensor that is both low cost and easy to\nfabricate is integrated with robotic grippers for working with produce of\nvarying shapes, sizes, and stiffnesses. The sensor is successfully integrated\nwith both a rigid robotic gripper, as well as a pneumatically actuated soft\nfinger. Furthermore, an algorithm is proposed for accelerated estimation of the\nsteady-state value of the sensor output based on the transient response data,\nto enable real-time applications. The sensor is shown to be effective in\nincorporating feedback to correctly grasp objects of unknown sizes and\nstiffnesses. At the same time, the sensor provides estimates for these values\nwhich can be utilized for identification of qualities such as ripeness levels\nand bruising. It is also shown to be able to provide force feedback for objects\nof variable stiffnesses. This enables future use not only for produce\nidentification, but also for tasks such as quality control and selective\ndistribution based on ripeness levels."}
{"id": "2510.13619", "pdf": "https://arxiv.org/pdf/2510.13619", "abs": "https://arxiv.org/abs/2510.13619", "authors": ["Daniel Choate", "Jason Rife"], "title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization", "categories": ["cs.RO"], "comment": "This is the preprint version of the paper published in: Proceedings\n  of the 37th International Technical Meeting of the Satellite Division of The\n  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is\n  available at https://doi.org/10.33012/2024.19864", "summary": "In this paper we introduce a visualization methodology to aid a human analyst\nin classifying adversity modes that impact lidar scan matching. Our methodology\nis intended for offline rather than real-time analysis. The method generates a\nvector-field plot that characterizes local discrepancies between a pair of\nregistered point clouds. The vector field plot reveals patterns that would be\ndifficult for the analyst to extract from raw point-cloud data. After\nintroducing our methodology, we apply the process to two proof-of-concept\nexamples: one a simulation study and the other a field experiment. For both\ndata sets, a human analyst was able to reason about a series of adversity\nmechanisms and iteratively remove those mechanisms from the raw data, to help\nfocus attention on progressively smaller discrepancies."}
{"id": "2510.13625", "pdf": "https://arxiv.org/pdf/2510.13625", "abs": "https://arxiv.org/abs/2510.13625", "authors": ["Nicolas Pottier", "Meng Cheng Lau"], "title": "A Modular Object Detection System for Humanoid Robots Using YOLO", "categories": ["cs.RO"], "comment": "7 Figures, 5 tables. This article was presented at FIRA Summit 2025.\n  It will be updated for journal submission", "summary": "Within the field of robotics, computer vision remains a significant barrier\nto progress, with many tasks hindered by inefficient vision systems. This\nresearch proposes a generalized vision module leveraging YOLOv9, a\nstate-of-the-art framework optimized for computationally constrained\nenvironments like robots. The model is trained on a dataset tailored to the\nFIRA robotics Hurocup. A new vision module is implemented in ROS1 using a\nvirtual environment to enable YOLO compatibility. Performance is evaluated\nusing metrics such as frames per second (FPS) and Mean Average Precision (mAP).\nPerformance is then compared to the existing geometric framework in static and\ndynamic contexts. The YOLO model achieved comparable precision at a higher\ncomputational cost then the geometric model, while providing improved\nrobustness."}
{"id": "2510.13626", "pdf": "https://arxiv.org/pdf/2510.13626", "abs": "https://arxiv.org/abs/2510.13626", "authors": ["Senyu Fei", "Siyin Wang", "Junhao Shi", "Zihao Dai", "Jikun Cai", "Pengfang Qian", "Li Ji", "Xinzhe He", "Shiduo Zhang", "Zhaoye Fei", "Jinlan Fu", "Jingjing Gong", "Xipeng Qiu"], "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": null, "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation."}
{"id": "2510.13644", "pdf": "https://arxiv.org/pdf/2510.13644", "abs": "https://arxiv.org/abs/2510.13644", "authors": ["Michael Bosello", "Flavio Pinzarrone", "Sara Kiade", "Davide Aguiari", "Yvo Keuter", "Aaesha AlShehhi", "Gyordan Caminati", "Kei Long Wong", "Ka Seng Chou", "Junaid Halepota", "Fares Alneyadi", "Jacopo Panerati", "Giovanni Pau"], "title": "On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas", "categories": ["cs.RO"], "comment": null, "summary": "Drone technology is proliferating in many industries, including agriculture,\nlogistics, defense, infrastructure, and environmental monitoring. Vision-based\nautonomy is one of its key enablers, particularly for real-world applications.\nThis is essential for operating in novel, unstructured environments where\ntraditional navigation methods may be unavailable. Autonomous drone racing has\nbecome the de facto benchmark for such systems. State-of-the-art research has\nshown that autonomous systems can surpass human-level performance in racing\narenas. However, direct applicability to commercial and field operations is\nstill limited as current systems are often trained and evaluated in highly\ncontrolled environments. In our contribution, the system's capabilities are\nanalyzed within a controlled environment -- where external tracking is\navailable for ground-truth comparison -- but also demonstrated in a\nchallenging, uninstrumented environment -- where ground-truth measurements were\nnever available. We show that our approach can match the performance of\nprofessional human pilots in both scenarios. We also publicly release the data\nfrom the flights carried out by our approach and a world-class human pilot."}
{"id": "2510.13686", "pdf": "https://arxiv.org/pdf/2510.13686", "abs": "https://arxiv.org/abs/2510.13686", "authors": ["Miana Smith", "Paul Arthur Richard", "Alexander Htet Kyaw", "Neil Gershenfeld"], "title": "Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures", "categories": ["cs.RO"], "comment": "In ACM Symposium on Computational Fabrication (SCF '25), November\n  20-21, 2025, Cambridge, MA, USA. ACM, New York, NY, USA, 15 pages", "summary": "Although digital fabrication processes at the desktop scale have become\nproficient and prolific, systems aimed at producing larger-scale structures are\nstill typically complex, expensive, and unreliable. In this work, we present an\napproach for the fabrication of scalable macroscale structures using simple\nrobots and interlocking lattice building blocks. A target structure is first\nvoxelized so that it can be populated with an architected lattice. These voxels\nare then grouped into larger interconnected blocks, which are produced using\nstandard digital fabrication processes, leveraging their capability to produce\nhighly complex geometries at a small scale. These blocks, on the size scale of\ntens of centimeters, are then fed to mobile relative robots that are able to\ntraverse over the structure and place new blocks to form structures on the\nmeter scale. To facilitate the assembly of large structures, we introduce a\nlive digital twin simulation tool for controlling and coordinating assembly\nrobots that enables both global planning for a target structure and live user\ndesign, interaction, or intervention. To improve assembly throughput, we\nintroduce a new modular assembly robot, designed for hierarchical voxel\nhandling. We validate this system by demonstrating the voxelization,\nhierarchical blocking, path planning, and robotic fabrication of a set of\nmeter-scale objects."}
{"id": "2510.13778", "pdf": "https://arxiv.org/pdf/2510.13778", "abs": "https://arxiv.org/abs/2510.13778", "authors": ["Xinyi Chen", "Yilun Chen", "Yanwei Fu", "Ning Gao", "Jiaya Jia", "Weiyang Jin", "Hao Li", "Yao Mu", "Jiangmiao Pang", "Yu Qiao", "Yang Tian", "Bin Wang", "Bolun Wang", "Fangjing Wang", "Hanqing Wang", "Tai Wang", "Ziqin Wang", "Xueyuan Wei", "Chao Wu", "Shuai Yang", "Jinhui Ye", "Junqiu Yu", "Jia Zeng", "Jingjing Zhang", "Jinyu Zhang", "Shi Zhang", "Feng Zheng", "Bowen Zhou", "Yangkun Zhu"], "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Technical report", "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1."}
{"id": "2510.12845", "pdf": "https://arxiv.org/pdf/2510.12845", "abs": "https://arxiv.org/abs/2510.12845", "authors": ["Jesse Atuhurra", "Iqra Ali", "Tomoya Iwakura", "Hidetaka Kamigaito", "Tatsuya Hiraoka"], "title": "VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Vision Language Models (VLMs) are pivotal for advancing perception in\nintelligent agents. Yet, evaluation of VLMs remains limited to predominantly\nEnglish-centric benchmarks in which the image-text pairs comprise short texts.\nTo evaluate VLM fine-grained abilities, in four languages under long-text\nsettings, we introduce a novel multilingual benchmark VLURes featuring eight\nvision-and-language tasks, and a pioneering unrelatedness task, to probe the\nfine-grained Visual and Linguistic Understanding capabilities of VLMs across\nEnglish, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,\ncurated from web resources in the target language, encompass ten diverse image\ncategories and rich textual context, introducing valuable vision-language\nresources for Swahili and Urdu. By prompting VLMs to generate responses and\nrationales, evaluated automatically and by native speakers, we uncover\nperformance disparities across languages and tasks critical to intelligent\nagents, such as object recognition, scene understanding, and relationship\nunderstanding. We conducted evaluations of ten VLMs with VLURes. The best\nperforming model, GPT-4o, achieves an overall accuracy of 90.8% and lags human\nperformance by 6.7%, though the gap is larger for open-source models. The gap\nhighlights VLURes' critical role in developing intelligent agents to tackle\nmulti-modal visual reasoning."}
{"id": "2510.12901", "pdf": "https://arxiv.org/pdf/2510.12901", "abs": "https://arxiv.org/abs/2510.12901", "authors": ["Haithem Turki", "Qi Wu", "Xin Kang", "Janick Martinez Esturo", "Shengyu Huang", "Ruilong Li", "Zan Gojcic", "Riccardo de Lutio"], "title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "comment": "Project page: https://research.nvidia.com/labs/sil/projects/simuli", "summary": "Rigorous testing of autonomous robots, such as self-driving vehicles, is\nessential to ensure their safety in real-world deployments. This requires\nbuilding high-fidelity simulators to test scenarios beyond those that can be\nsafely or exhaustively collected in the real-world. Existing neural rendering\nmethods based on NeRF and 3DGS hold promise but suffer from low rendering\nspeeds or can only render pinhole camera models, hindering their suitability to\napplications that commonly require high-distortion lenses and LiDAR data.\nMulti-sensor simulation poses additional challenges as existing methods handle\ncross-sensor inconsistencies by favoring the quality of one modality at the\nexpense of others. To overcome these limitations, we propose SimULi, the first\nmethod capable of rendering arbitrary camera models and LiDAR data in\nreal-time. Our method extends 3DGUT, which natively supports complex camera\nmodels, with LiDAR support, via an automated tiling strategy for arbitrary\nspinning LiDAR models and ray-based culling. To address cross-sensor\ninconsistencies, we design a factorized 3D Gaussian representation and\nanchoring strategy that reduces mean camera and depth error by up to 40%\ncompared to existing methods. SimULi renders 10-20x faster than ray tracing\napproaches and 1.5-10x faster than prior rasterization-based work (and handles\na wider range of camera models). When evaluated on two widely benchmarked\nautonomous driving datasets, SimULi matches or exceeds the fidelity of existing\nstate-of-the-art methods across numerous camera and LiDAR metrics."}
{"id": "2510.13004", "pdf": "https://arxiv.org/pdf/2510.13004", "abs": "https://arxiv.org/abs/2510.13004", "authors": ["Robert Muldrow", "Channing Ludden", "Christopher Petersen"], "title": "Comparison of Forced and Unforced Rendezvous, Proximity Operations, and Docking Under Model Mismatch", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "12 pages, 4 figures, AAS/AIAA Space Flight Mechanics", "summary": "This paper compares the required fuel usage for forced and unforced motion of\na chaser satellite engaged in Rendezvous, Proximity Operations, and Docking\n(RPOD) maneuvers. Improved RPOD models are vital, particularly as the space\nindustry expands and demands for improved fuel efficiency, cost effectiveness,\nand mission life span increase. This paper specifically examines the Clohessy-\nWiltshire (CW) Equations and the extent of model mismatch by comparing pre-\ndicted trajectories from this model with a more computationally complex, higher\nfidelity RPOD model. This paper assesses several test cases of similar mission\nparameters, in each case comparing natural motion circumnavigation (NMC) with\ncomparable forced motion circumnavigation. The Guidance, Navigation, and Con-\ntrol (GNC) impulse maneuvers required to maintain the supposedly zero fuel CW\ntrajectories is representative of the extent of CW model mismatch. This paper\ndemonstrates that unforced motions are not inherently more fuel efficient than\nforced motions, thus permitting extended orbital operations given the higher\nfuel efficiency."}
{"id": "2510.13108", "pdf": "https://arxiv.org/pdf/2510.13108", "abs": "https://arxiv.org/abs/2510.13108", "authors": ["Jingyu Song", "Zhenxin Li", "Shiyi Lan", "Xinglong Sun", "Nadine Chang", "Maying Shen", "Joshua Chen", "Katherine A. Skinner", "Jose M. Alvarez"], "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "9 pages, 3 figures", "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems."}
{"id": "2510.13114", "pdf": "https://arxiv.org/pdf/2510.13114", "abs": "https://arxiv.org/abs/2510.13114", "authors": ["Zhuoyuan Wang", "Tongyao Jia", "Pharuj Rajborirug", "Neeraj Ramesh", "Hiroyuki Okuda", "Tatsuya Suzuki", "Soummya Kar", "Yorie Nakahira"], "title": "Safe Driving in Occluded Environments", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Ensuring safe autonomous driving in the presence of occlusions poses a\nsignificant challenge in its policy design. While existing model-driven control\ntechniques based on set invariance can handle visible risks, occlusions create\nlatent risks in which safety-critical states are not observable. Data-driven\ntechniques also struggle to handle latent risks because direct mappings from\nrisk-critical objects in sensor inputs to safe actions cannot be learned\nwithout visible risk-critical objects. Motivated by these challenges, in this\npaper, we propose a probabilistic safety certificate for latent risk. Our key\ntechnical enabler is the application of probabilistic invariance: It relaxes\nthe strict observability requirements imposed by set-invariance methods that\ndemand the knowledge of risk-critical states. The proposed techniques provide\nlinear action constraints that confine the latent risk probability within\ntolerance. Such constraints can be integrated into model predictive controllers\nor embedded in data-driven policies to mitigate latent risks. The proposed\nmethod is tested using the CARLA simulator and compared with a few existing\ntechniques. The theoretical and empirical analysis jointly demonstrate that the\nproposed methods assure long-term safety in real-time control in occluded\nenvironments without being overly conservative and with transparency to exposed\nrisks."}
{"id": "2510.13367", "pdf": "https://arxiv.org/pdf/2510.13367", "abs": "https://arxiv.org/abs/2510.13367", "authors": ["Nikita Kachaev", "Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovelev", "Aleksandr I. Panov"], "title": "A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Despite their effectiveness and popularity in offline or model-based\nreinforcement learning (RL), transformers remain underexplored in online\nmodel-free RL due to their sensitivity to training setups and model design\ndecisions such as how to structure the policy and value networks, share\ncomponents, or handle temporal information. In this paper, we show that\ntransformers can be strong baselines for continuous control in online\nmodel-free RL. We investigate key design questions: how to condition inputs,\nshare components between actor and critic, and slice sequential data for\ntraining. Our experiments reveal stable architectural and training strategies\nenabling competitive performance across fully and partially observable tasks,\nand in both vector- and image-based settings. These findings offer practical\nguidance for applying transformers in online RL."}
{"id": "2510.13461", "pdf": "https://arxiv.org/pdf/2510.13461", "abs": "https://arxiv.org/abs/2510.13461", "authors": ["Yangye Jiang", "Jiachen Wang", "Daofei Li"], "title": "Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Accurate prediction of vehicle collision dynamics is crucial for advanced\nsafety systems and post-impact control applications, yet existing methods face\ninherent trade-offs among computational efficiency, prediction accuracy, and\ndata requirements. This paper proposes a dual Physics-Informed Neural Network\nframework addressing these challenges through two complementary networks. The\nfirst network integrates Gaussian Mixture Models with PINN architecture to\nlearn impact force distributions from finite element analysis data while\nenforcing momentum conservation and energy consistency constraints. The second\nnetwork employs an adaptive PINN with dynamic constraint weighting to predict\npost-collision vehicle dynamics, featuring an adaptive physics guard layer that\nprevents unrealistic predictions whil e preserving data-driven learning\ncapabilities. The framework incorporates uncertainty quantification through\ntime-varying parameters and enables rapid adaptation via fine-tuning\nstrategies. Validation demonstrates significant improvements: the impact force\nmodel achieves relative errors below 15.0% for force prediction on finite\nelement analysis (FEA) datasets, while the vehicle dynamics model reduces\naverage trajectory prediction error by 63.6% compared to traditional\nfour-degree-of-freedom models in scaled vehicle experiments. The integrated\nsystem maintains millisecond-level computational efficiency suitable for\nreal-time applications while providing probabilistic confidence bounds\nessential for safety-critical control. Comprehensive validation through FEA\nsimulation, dynamic modeling, and scaled vehicle experiments confirms the\nframework's effectiveness for Precision Immobilization Technique scenarios and\ngeneral collision dynamics prediction."}
{"id": "2510.13464", "pdf": "https://arxiv.org/pdf/2510.13464", "abs": "https://arxiv.org/abs/2510.13464", "authors": ["Emily Miller", "Michael Milford", "Muhammad Burhan Hafez", "SD Ramchurn", "Shoaib Ehsan"], "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance."}
{"id": "2510.13546", "pdf": "https://arxiv.org/pdf/2510.13546", "abs": "https://arxiv.org/abs/2510.13546", "authors": ["Ruiqi Ye", "Mikel LujÃ¡n"], "title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU", "categories": ["cs.CV", "cs.ET", "cs.PF", "cs.RO", "C.3; C.4; I.4.6"], "comment": "12 pages, 7 figures", "summary": "Feature detection is a common yet time-consuming module in Simultaneous\nLocalization and Mapping (SLAM) implementations, which are increasingly\ndeployed on power-constrained platforms, such as drones. Graphics Processing\nUnits (GPUs) have been a popular accelerator for computer vision in general,\nand feature detection and SLAM in particular.\n  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable\nGate Array (FPGA) are also widely available. This paper presents the first\nstudy of hardware-accelerated feature detectors considering a Visual SLAM\n(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated\nFAST, Harris, and SuperPoint implementations against the FPGA-accelerated\ncounterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).\n  The evaluation shows that when using a non-learning-based feature detector\nsuch as FAST and Harris, their GPU implementations, and the GPU-accelerated\nV-SLAM can achieve better run-time performance and energy efficiency than the\nFAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.\nHowever, when considering a learning-based detector such as SuperPoint, its\nFPGA implementation can achieve better run-time performance and energy\nefficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than\nthe GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable\nrun-time performance compared to the GPU-accelerated V-SLAM, with better FPS in\n2 out of 5 dataset sequences. When considering the accuracy, the results show\nthat the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated\nV-SLAM in general. Last but not least, the use of hardware acceleration for\nfeature detection could further improve the performance of the V-SLAM pipeline\nby having the global bundle adjustment module invoked less frequently without\nsacrificing accuracy."}
{"id": "2510.13704", "pdf": "https://arxiv.org/pdf/2510.13704", "abs": "https://arxiv.org/abs/2510.13704", "authors": ["Johan Obando-Ceron", "Walter Mayor", "Samuel Lavoie", "Scott Fujimoto", "Aaron Courville", "Pablo Samuel Castro"], "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed."}
{"id": "2510.13794", "pdf": "https://arxiv.org/pdf/2510.13794", "abs": "https://arxiv.org/abs/2510.13794", "authors": ["Xue Bin Peng"], "title": "MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control", "categories": ["cs.GR", "cs.LG", "cs.RO"], "comment": null, "summary": "MimicKit is an open-source framework for training motion controllers using\nmotion imitation and reinforcement learning. The codebase provides\nimplementations of commonly-used motion-imitation techniques and RL algorithms.\nThis framework is intended to support research and applications in computer\ngraphics and robotics by providing a unified training framework, along with\nstandardized environment, agent, and data structures. The codebase is designed\nto be modular and easily configurable, enabling convenient modification and\nextension to new characters and tasks. The open-source codebase is available\nat: https://github.com/xbpeng/MimicKit."}
