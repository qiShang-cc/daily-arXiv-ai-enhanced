{"id": "2510.03516", "pdf": "https://arxiv.org/pdf/2510.03516", "abs": "https://arxiv.org/abs/2510.03516", "authors": ["Boyang Chen", "Mohd Tasleem Khan", "George Goussetis", "Mathini Sellathurai", "Yuan Ding", "Jo\u00e3o F. C. Mota"], "title": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques", "categories": ["eess.SP", "I.2.7"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy.", "AI": {"tldr": "COMET\u662f\u7528\u4e8eCNN\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u79fb\u4e8c\u8fdb\u5236\u7f16\u7801\uff08OBC\uff09\u6280\u672f\u5b9e\u73b0\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7684\u534f\u540c\u4f18\u5316\uff0c\u51cf\u5c11FPGA\u8d44\u6e90\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "CNN\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u786c\u4ef6\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\u3002", "method": "\u63d0\u51faCOMET\u6846\u67b6\uff0c\u91c7\u7528OBC\u6280\u672f\u5206\u522b\u8868\u793a\u8f93\u5165\u548c\u6743\u91cd\uff0c\u5f15\u5165\u56db\u79cdLUT\u6280\u672f\uff0c\u5e76\u5f00\u53d1OBC-based\u77e9\u9635\u4e58\u6cd5\u6838\u5fc3\u3002", "result": "FPGA\u5b9e\u9a8c\u663e\u793a\u8d44\u6e90\u5229\u7528\u663e\u8457\u51cf\u5c11\uff0c\u5bf9\u51c6\u786e\u6027\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "COMET\u6846\u67b6\u901a\u8fc7OBC\u548cLUT\u6280\u672f\u6709\u6548\u4f18\u5316\u4e86CNN\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002"}}
{"id": "2510.03594", "pdf": "https://arxiv.org/pdf/2510.03594", "abs": "https://arxiv.org/abs/2510.03594", "authors": ["Tuo Wu", "Kwai-Man Luk", "Jie Tang", "Kai-Kit Wong", "Jianchao Zheng", "Baiyang Liu", "David Morales-Jimenez", "Maged Elkashlan", "Kin-Fai Tong", "Chan-Byoung Chae", "Fumiyuki Adachi", "George K. Karagiannidis"], "title": "Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems", "categories": ["eess.SP"], "comment": "13 pages", "summary": "Fluid antenna systems (FAS) are emerging as a transformative enabler for\nsixth-generation (6G) wireless communications, providing unprecedented spatial\ndiversity through dynamic reconfiguration of antenna ports. However, the\ninherent spatial correlation among ports poses significant challenges for\naccurate analysis. Conventional models such as Jakes are analytically\nintractable, while oversimplified constant-correlation models fail to capture\nthe true behavior. In this work, we address these challenges by applying the\nvariable block-correlation model (VBCM) -- originally proposed by\nRam\\'{i}rez-Espinosa \\textit{et al.} in 2024 -- to FAS security analysis, and\nby developing comprehensive optimization methods to enhance analytical\naccuracy. We derive new closed-form expressions for average secrecy capacity\n(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM\nframework achieves simulation-aligned accuracy, with relative errors\nconsistently below $5\\%$ (compared to $10$--$15\\%$ for constant-correlation\nmodels). To maximize ASC, we further design two algorithms: a grid search (GS)\nmethod and a gradient descent (GD) method. Numerical results reveal that the\nVBCM-based approach not only provides reliable insights into FAS security\nperformance, but also yields substantial gains -- ASC improvements exceeding\n$120\\%$ in high-threat scenarios and $18$--$19\\%$ performance enhancements for\ncompact antenna configurations. These findings underscore the practical value\nof integrating VBCM into FAS security analysis and optimization, establishing\nit as a powerful tool for advancing 6G communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5757\u76f8\u5173\u6a21\u578b\uff08VBCM\uff09\u7684\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u5b89\u5168\u5206\u6790\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u6790\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u63d0\u5347\u4e86\u4fdd\u5bc6\u6027\u80fd\u3002", "motivation": "\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u57286G\u901a\u4fe1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5929\u7ebf\u7aef\u53e3\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u5bfc\u81f4\u4f20\u7edf\u6a21\u578b\uff08\u5982Jakes\u6a21\u578b\uff09\u5206\u6790\u56f0\u96be\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u3002", "method": "\u5e94\u7528VBCM\u6a21\u578b\u5206\u6790FAS\u5b89\u5168\u6027\uff0c\u5e76\u8bbe\u8ba1\u7f51\u683c\u641c\u7d22\uff08GS\uff09\u548c\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u7b97\u6cd5\u4f18\u5316\u5e73\u5747\u4fdd\u5bc6\u5bb9\u91cf\uff08ASC\uff09\u3002", "result": "VBCM\u6846\u67b6\u7684\u5206\u6790\u8bef\u5dee\u4f4e\u4e8e5%\uff0c\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u51c6\u786e\uff1bASC\u5728\u9ad8\u5a01\u80c1\u573a\u666f\u4e0b\u63d0\u5347\u8d85\u8fc7120%\uff0c\u7d27\u51d1\u5929\u7ebf\u914d\u7f6e\u4e0b\u63d0\u534718-19%\u3002", "conclusion": "VBCM\u662fFAS\u5b89\u5168\u5206\u6790\u548c\u4f18\u5316\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5bf96G\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2510.03626", "pdf": "https://arxiv.org/pdf/2510.03626", "abs": "https://arxiv.org/abs/2510.03626", "authors": ["Jun Tong"], "title": "On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models", "categories": ["eess.SP"], "comment": "This paper was presented at 2025 IEEE International Conference on\n  Communications Workshops (ICC Workshops)", "summary": "Significant studies on communications over doubly selective channels have\nutilized on-grid DD channel models, which are previously investigated in\nBello's seminar paper in 1963. The DD grid is typically specified by the\nbandwidth and time duration of the transmission frames. However, the physical\nchannels are determined by the propagation environments and they are typically\noff-grid. Hence, there is often a gap between an actual physical channel and\nthe on-grid model. This paper revisits the on-grid modeling of practical\nphysical channels. We study the associated on-grid DD-domain representations\nfor continuous-time, doubly selective channels with off-grid delay and Doppler\nshifts, accounting for practical time/frequency-domain windowing at the\ntransceivers. The universal models obtained are applicable under the mild\nassumption that the windows have finite supports, and they extend Bello's\nclassical results to account for more general windows. We also discuss the\nfeatures and implications of the equivalent on-grid models.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u63a2\u8ba8\u4e86\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u5728\u79bb\u6563\u7f51\u683c\uff08on-grid\uff09\u6a21\u578b\u4e0b\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u4e2d\u79bb\u7f51\u683c\uff08off-grid\uff09\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u7684\u7b49\u6548\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u901a\u7528\u7a97\u53e3\u51fd\u6570\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u7814\u7a76\u4e3b\u8981\u57fa\u4e8eBello\u57281963\u5e74\u63d0\u51fa\u7684\u79bb\u6563\u7f51\u683c\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u901a\u5e38\u4e0d\u7b26\u5408\u7f51\u683c\u5047\u8bbe\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0e\u5b9e\u9645\u4fe1\u9053\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6\u79bb\u6563\u7f51\u683c\u6a21\u578b\u5728\u63cf\u8ff0\u5b9e\u9645\u4fe1\u9053\u65f6\u7684\u9002\u7528\u6027\u3002", "method": "\u7814\u7a76\u4e86\u8fde\u7eed\u65f6\u95f4\u3001\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u5728\u79bb\u6563\u7f51\u683c\u57df\u4e2d\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u6536\u53d1\u7aef\u5b9e\u9645\u4f7f\u7528\u7684\u65f6\u9891\u57df\u7a97\u53e3\u51fd\u6570\uff0c\u5047\u8bbe\u7a97\u53e3\u51fd\u6570\u5177\u6709\u6709\u9650\u652f\u6491\u3002", "result": "\u63d0\u51fa\u7684\u901a\u7528\u6a21\u578b\u9002\u7528\u4e8e\u7a97\u53e3\u51fd\u6570\u5177\u6709\u6709\u9650\u652f\u6491\u7684\u60c5\u51b5\uff0c\u6269\u5c55\u4e86Bello\u7684\u7ecf\u5178\u7ed3\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u7b49\u6548\u79bb\u6563\u7f51\u683c\u6a21\u578b\u7684\u7279\u5f81\u548c\u610f\u4e49\u3002", "conclusion": "\u8bba\u6587\u6269\u5c55\u4e86\u7ecf\u5178\u7684\u79bb\u6563\u7f51\u683c\u4fe1\u9053\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u5b9e\u9645\u7269\u7406\u4fe1\u9053\uff0c\u7279\u522b\u662f\u5728\u6536\u53d1\u7aef\u4f7f\u7528\u65f6\u9891\u57df\u7a97\u53e3\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.03342", "pdf": "https://arxiv.org/pdf/2510.03342", "abs": "https://arxiv.org/abs/2510.03342", "authors": ["Abbas Abdolmaleki", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Ashwin Balakrishna", "Nathan Batchelor", "Alex Bewley", "Jeff Bingham", "Michael Bloesch", "Konstantinos Bousmalis", "Philemon Brakel", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Christine Chan", "Oscar Chang", "London Chappellet-Volpini", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "Adrian Collister", "David B. D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Meet Kirankumar Dave", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Carl Doersch", "Adil Dostmohamed", "Yilun Du", "Debidatta Dwibedi", "Sathish Thoppay Egambaram", "Michael Elabd", "Tom Erez", "Xiaolin Fang", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Ruiqi Gao", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Oliver Groth", "Agrim Gupta", "Roland Hafner", "Steven Hansen", "Leonard Hasenclever", "Sam Haves", "Nicolas Heess", "Brandon Hernaez", "Alex Hofer", "Jasmine Hsu", "Lu Huang", "Sandy H. Huang", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Sally Jesmonth", "Abhishek Jindal", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Matija Kecman", "J. Chase Kew", "Donnie Kim", "Frank Kim", "Junkyung Kim", "Thomas Kipf", "Sean Kirmani", "Ksenia Konyushkova", "Li Yang Ku", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Tuan Anh Le", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Guy Lever", "Jacky Liang", "Li-Heng Lin", "Fangchen Liu", "Shangbang Long", "Caden Lu", "Sharath Maddineni", "Anirudha Majumdar", "Kevis-Kokitsi Maninis", "Andrew Marmon", "Sergio Martinez", "Assaf Hurwitz Michaely", "Niko Milonopoulos", "Joss Moore", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Joy Ortiz", "Kenneth Oslund", "Carolina Parada", "Emilio Parisotto", "Amaris Paryag", "Acorn Pooley", "Thomas Power", "Alessio Quaglino", "Haroon Qureshi", "Rajkumar Vasudeva Raju", "Helen Ran", "Dushyant Rao", "Kanishka Rao", "Isaac Reid", "David Rendleman", "Krista Reymann", "Miguel Rivas", "Francesco Romano", "Yulia Rubanova", "Peter Pastor Sampedro", "Pannag R Sanketi", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Mohit Shridhar", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Rachel Sterneck", "Ian Storz", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Saran Tunyasuvunakool", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Maria Bauza Villalonga", "Oriol Vinyals", "Ren\u00e9 Wagner", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Chengda Wu", "Markus Wulfmeier", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Jimmy Yan", "Sherry Yang", "Skye Yang", "Yuxiang Yang", "Hiu Hong Yu", "Wenhao Yu", "Wentao Yuan", "Yuan Yuan", "Jingwei Zhang", "Tingnan Zhang", "Zhiyuan Zhang", "Allan Zhou", "Guangyao Zhou", "Yuxiang Zhou"], "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer", "categories": ["cs.RO"], "comment": null, "summary": "General-purpose robots need a deep understanding of the physical world,\nadvanced reasoning, and general and dexterous control. This report introduces\nthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,\na multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER\n1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together\nthree major innovations. First, Gemini Robotics 1.5 features a novel\narchitecture and a Motion Transfer (MT) mechanism, which enables it to learn\nfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.\nSecond, Gemini Robotics 1.5 interleaves actions with a multi-level internal\nreasoning process in natural language. This enables the robot to \"think before\nacting\" and notably improves its ability to decompose and execute complex,\nmulti-step tasks, and also makes the robot's behavior more interpretable to the\nuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for\nembodied reasoning, i.e., for reasoning capabilities that are critical for\nrobots, such as visual and spatial understanding, task planning, and progress\nestimation. Together, this family of models takes us a step towards an era of\nphysical agents-enabling robots to perceive, think and then act so they can\nsolve complex multi-step tasks.", "AI": {"tldr": "Gemini Robotics 1.5\u548cGemini Robotics-ER 1.5\u662f\u591a\u6a21\u6001\u673a\u5668\u4eba\u6a21\u578b\u5bb6\u65cf\u7684\u6700\u65b0\u6210\u5458\uff0c\u901a\u8fc7\u65b0\u67b6\u6784\u3001\u8fd0\u52a8\u8f6c\u79fb\u673a\u5236\u548c\u591a\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u63d0\u5347\u673a\u5668\u4eba\u7684\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u9700\u8981\u6df1\u5ea6\u7406\u89e3\u7269\u7406\u4e16\u754c\u3001\u9ad8\u7ea7\u63a8\u7406\u548c\u7075\u6d3b\u63a7\u5236\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7Gemini Robotics\u6a21\u578b\u5bb6\u65cf\u7684\u65b0\u4e00\u4ee3\u6280\u672f\u63a8\u52a8\u8fd9\u4e00\u76ee\u6807\u3002", "method": "Gemini Robotics 1.5\u91c7\u7528\u65b0\u67b6\u6784\u548c\u8fd0\u52a8\u8f6c\u79fb\u673a\u5236\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u5b66\u4e60\uff1b\u540c\u65f6\u7ed3\u5408\u591a\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\u3002Gemini Robotics-ER 1.5\u5219\u63d0\u5347\u4e86\u89c6\u89c9\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u4efb\u52a1\u89c4\u5212\u7b49\u63a8\u7406\u80fd\u529b\u3002", "result": "Gemini Robotics 1.5\u901a\u8fc7\u63a8\u7406\u524d\u7f6e\u6539\u5584\u4e86\u590d\u6742\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u884c\u4e3a\u66f4\u53ef\u89e3\u91ca\uff1bGemini Robotics-ER 1.5\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u8fd9\u4e00\u7cfb\u5217\u6a21\u578b\u63a8\u52a8\u4e86\u7269\u7406\u667a\u80fd\u4f53\u7684\u53d1\u5c55\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u3001\u601d\u8003\u5e76\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u4efb\u52a1\u3002"}}
{"id": "2510.03628", "pdf": "https://arxiv.org/pdf/2510.03628", "abs": "https://arxiv.org/abs/2510.03628", "authors": ["Haochen Li"], "title": "Pinching Antenna Systems (PASS) for Cell-Free Communications", "categories": ["eess.SP"], "comment": "5 pages, 5 figures", "summary": "A pinching antenna system (PASS) assisted cell-free communication system is\nproposed. A sum rate maximization problem under the BS power budget constraint\nand PA deployment constraint is formulated. To tackle the proposed non-convex\noptimization problem, an alternating optimization (AO) algorithm is developed.\nIn particular, the digital beamforming sub-problem is solved using the weighted\nminimum mean square error (WMMSE) method, whereas the pinching beamforming\nsub-problem is handled via a penalty based approach combined with element-wise\noptimization. Simulation results demonstrate that: 1) the PASS assisted\ncell-free systems achieve superior performance over benchmark schemes; 2)\nincreasing the number of PAs per waveguides can improve the advantage of PASS\nassisted cell-free systems; and 3) the cell-free architecture mitigates the\naverage user rate degradation as the number of users increases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePinching\u5929\u7ebf\u7cfb\u7edf\uff08PASS\uff09\u7684\u65e0\u8702\u7a9d\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\u89e3\u51b3\u4e86\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660ePASS\u7cfb\u7edf\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7PASS\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u901a\u4fe1\u7cfb\u7edf\u63d0\u5347\u6027\u80fd\u548c\u7528\u6237\u901f\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u7cfb\u7edf\u5728\u591a\u7528\u6237\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408WMMSE\u65b9\u6cd5\u548c\u57fa\u4e8e\u60e9\u7f5a\u7684\u5143\u7d20\u4f18\u5316\u6280\u672f\uff0c\u89e3\u51b3\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u548cPinching\u6ce2\u675f\u6210\u5f62\u7684\u5b50\u95ee\u9898\u3002", "result": "PASS\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u589e\u52a0\u6bcf\u4e2a\u6ce2\u5bfc\u7684PA\u6570\u91cf\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u4f18\u52bf\uff0c\u4e14\u65e0\u8702\u7a9d\u67b6\u6784\u80fd\u7f13\u89e3\u591a\u7528\u6237\u60c5\u51b5\u4e0b\u7684\u901f\u7387\u4e0b\u964d\u3002", "conclusion": "PASS\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u7cfb\u7edf\u5728\u6027\u80fd\u548c\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2510.03457", "pdf": "https://arxiv.org/pdf/2510.03457", "abs": "https://arxiv.org/abs/2510.03457", "authors": ["Jianfeng Lin", "Tianyu Wang", "Baxi Chong", "Matthew Fernandez", "Zhaochen Xu", "Daniel I. Goldman"], "title": "Optimal swimming with body compliance in an overdamped medium", "categories": ["cs.RO", "physics.app-ph"], "comment": null, "summary": "Elongate animals and robots use undulatory body waves to locomote through\ndiverse environments. Geometric mechanics provides a framework to model and\noptimize such systems in highly damped environments, connecting a prescribed\nshape change pattern (gait) with locomotion displacement. However, existing\napproaches assume precise execution of prescribed gaits, whereas in practice\nenvironmental interactions with compliant bodies of animals or robots\nfrequently perturb the realized trajectories. In this work, we extend geometric\nmechanics to predict locomotor performance and search for optimal swimming\nstrategy of compliant undulators. We introduce a compliant extension of\nPurcell's three-link swimmer by incorporating series-connected springs at the\njoints. Body dynamics are derived with resistive force theory. Geometric\nmechanics is incorporated into movement prediction and into an optimization\nframework that identifies strategies for controlling compliant swimmers to\nachieve maximal displacement. We validate our framework on a physical\ncable-driven three-link limbless robot, and demonstrate accurate prediction and\noptimization of locomotor performance under varied programmed, state-dependent\ncompliance in a granular medium. Our results establish a systematic\nphysics-based approach for modeling and controlling compliant swimming\nlocomotion, highlighting compliance as a design feature that can be exploited\nfor robust movement in homogeneous and heterogeneous environments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u51e0\u4f55\u529b\u5b66\u6a21\u578b\u4f18\u5316\u5177\u6709\u67d4\u6027\u7684\u6ce2\u52a8\u6e38\u6cf3\u8005\u5728\u963b\u5c3c\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\uff0c\u901a\u8fc7\u5f15\u5165\u5f39\u7c27\u5173\u8282\u7684\u4e09\u8fde\u6746\u6e38\u6cf3\u5668\uff0c\u7ed3\u5408\u963b\u529b\u7406\u8bba\u9884\u6d4b\u8fd0\u52a8\u6027\u80fd\u5e76\u4f18\u5316\u6e38\u6cf3\u7b56\u7565\uff0c\u6700\u7ec8\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u51e0\u4f55\u529b\u5b66\u6a21\u578b\u5047\u8bbe\u7cbe\u786e\u6267\u884c\u9884\u8bbe\u6b65\u6001\uff0c\u800c\u5b9e\u9645\u4e2d\u73af\u5883\u4e0e\u67d4\u6027\u4f53\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u5e72\u6270\u8fd0\u52a8\u8f68\u8ff9\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u8be5\u7406\u8bba\u4ee5\u9884\u6d4b\u548c\u4f18\u5316\u67d4\u6027\u6ce2\u52a8\u6e38\u6cf3\u8005\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u67d4\u6027\u7684\u4e09\u8fde\u6746\u6e38\u6cf3\u5668\u6a21\u578b\uff0c\u7ed3\u5408\u5f39\u7c27\u5173\u8282\u548c\u963b\u529b\u7406\u8bba\u63a8\u5bfc\u8eab\u4f53\u52a8\u529b\u5b66\uff0c\u5e76\u5c06\u51e0\u4f55\u529b\u5b66\u7eb3\u5165\u8fd0\u52a8\u9884\u6d4b\u548c\u4f18\u5316\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u67d4\u6027\u6e38\u6cf3\u8005\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u8fd0\u52a8\u6027\u80fd\u7684\u9884\u6d4b\u548c\u4f18\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u67d4\u6027\u53ef\u4f5c\u4e3a\u7a33\u5065\u8fd0\u52a8\u7684\u8bbe\u8ba1\u7279\u5f81\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u63a7\u5236\u67d4\u6027\u6ce2\u52a8\u6e38\u6cf3\u8005\u7684\u8fd0\u52a8\uff0c\u5c55\u793a\u4e86\u67d4\u6027\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u8fd0\u52a8\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.03749", "pdf": "https://arxiv.org/pdf/2510.03749", "abs": "https://arxiv.org/abs/2510.03749", "authors": ["Fanghao Xia", "Zesong Fei", "Xinyi Wang", "Nanchi Su", "Zhaolin Wang", "Yuanwei Liu", "Jie Xu"], "title": "Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?", "categories": ["eess.SP"], "comment": "13 pages, 12 figures", "summary": "In this paper, sensing-assisted secure communication in a multi-user\nmulti-eavesdropper integrated sensing and communication (ISAC) system is\ninvestigated. Confidential communication signals and dedicated sensing signals\nare jointly transmitted by a base station (BS) to simultaneously serve users\nand sense aerial eavesdroppers (AEs). A sum rate maximization problem is\nformulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and\nsensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A\nfractional-programming-based alternating optimization algorithm is developed to\nsolve this problem for fully digital arrays, where successive convex\napproximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle\nnon-convex constraints. Furthermore, the minimum number of dedicated sensing\nbeams is analyzed via a worst-case rank bound, upon which the proposed\nbeamforming design is further extended to the hybrid analog-digital (HAD) array\narchitecture, where the unit-modulus constraint is addressed by manifold\noptimization. Simulation results demonstrate that only a small number of\nsensing beams are sufficient for both sensing and jamming AEs, and the proposed\ndesigns consistently outperform strong baselines while also revealing the\ncommunication-sensing trade-off.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u7528\u6237\u591a\u7a83\u542c\u8005\u7684ISAC\u7cfb\u7edf\u4e2d\u611f\u5e94\u8f85\u52a9\u7684\u5b89\u5168\u901a\u4fe1\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u6570\u89c4\u5212\u548c\u4ea4\u66ff\u4f18\u5316\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5168\u6570\u5b57\u548c\u6df7\u5408\u6a21\u62df\u6570\u5b57\u9635\u5217\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u591a\u7528\u6237\u591a\u7a83\u542c\u8005\u7684ISAC\u7cfb\u7edf\u4e2d\u5982\u4f55\u5229\u7528\u4e13\u7528\u611f\u5e94\u4fe1\u53f7\u63d0\u5347\u901a\u4fe1\u5b89\u5168\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u5e94\u548c\u901a\u4fe1\u7684\u53cc\u91cd\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u89c4\u5212\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408SCA\u548cSDR\u5904\u7406\u975e\u51f8\u7ea6\u675f\uff0c\u5e76\u6269\u5c55\u5230HAD\u9635\u5217\u7ed3\u6784\uff0c\u5229\u7528\u6d41\u5f62\u4f18\u5316\u89e3\u51b3\u5355\u4f4d\u6a21\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u5c11\u91cf\u611f\u5e94\u6ce2\u675f\u5373\u53ef\u6ee1\u8db3\u611f\u5e94\u548c\u5e72\u6270\u9700\u6c42\uff0c\u6240\u63d0\u8bbe\u8ba1\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u611f\u5e94\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aISAC\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u9635\u5217\u67b6\u6784\u8bbe\u8ba1\u4e0a\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.03460", "pdf": "https://arxiv.org/pdf/2510.03460", "abs": "https://arxiv.org/abs/2510.03460", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching", "categories": ["cs.RO"], "comment": null, "summary": "Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)\nsystems, as robots need to respond to dynamic environments in real time by\ncontinuously observing their surroundings and replanning their motions to\nensure both safe interactions and efficient task execution. Current\nsampling-based motion planners face challenges in scaling to high-dimensional\nconfiguration spaces and often require post-processing to interpolate and\nsmooth the generated paths, resulting in time inefficiency in complex\nenvironments. Optimization-based planners, on the other hand, can incorporate\nmultiple constraints and generate smooth trajectories directly, making them\npotentially more time-efficient. However, optimization-based planners are\nsensitive to initialization and may get stuck in local minima. In this work, we\npresent a novel learning-based method that utilizes a Flow Matching model\nconditioned on a single-view point cloud to learn near-optimal solutions for\noptimization initialization. Our method does not require prior knowledge of the\nenvironment, such as obstacle locations and geometries, and can generate\nfeasible trajectories directly from single-view depth camera input. Simulation\nstudies on a UR5e robotic manipulator in cluttered workspaces demonstrate that\nthe proposed generative initializer achieves a high success rate on its own,\nsignificantly improves the success rate of trajectory optimization compared\nwith traditional and learning-based benchmark initializers, requires fewer\noptimization iterations, and exhibits strong generalization to unseen\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7Flow Matching\u6a21\u578b\u76f4\u63a5\u4ece\u5355\u89c6\u56fe\u70b9\u4e91\u751f\u6210\u4f18\u5316\u7684\u521d\u59cb\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u5728HRC\u7cfb\u7edf\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u5b9e\u65f6\u54cd\u5e94\u52a8\u6001\u73af\u5883\uff0c\u800c\u73b0\u6709\u91c7\u6837\u548c\u4f18\u5316\u65b9\u6cd5\u5728\u6548\u7387\u548c\u521d\u59cb\u5316\u654f\u611f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Flow Matching\u6a21\u578b\uff0c\u57fa\u4e8e\u5355\u89c6\u56fe\u70b9\u4e91\u5b66\u4e60\u751f\u6210\u4f18\u5316\u7684\u521d\u59cb\u8f68\u8ff9\uff0c\u65e0\u9700\u5148\u9a8c\u73af\u5883\u77e5\u8bc6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u4f18\u5316\u7684\u6210\u529f\u7387\uff0c\u51cf\u5c11\u4e86\u8fed\u4ee3\u6b21\u6570\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u65b0\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u3001\u9ad8\u6210\u529f\u7387\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002"}}
{"id": "2510.03780", "pdf": "https://arxiv.org/pdf/2510.03780", "abs": "https://arxiv.org/abs/2510.03780", "authors": ["Yiqiao Chen"], "title": "A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification", "categories": ["eess.SP", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Cardiovascular disease (CVD) is a major pediatric health burden, and early\nscreening is of critical importance. Electrocardiography (ECG), as a\nnoninvasive and accessible tool, is well suited for this purpose. This paper\npresents the first benchmark study of deep learning for multi-label pediatric\nCVD classification on the recently released ZZU-pECG dataset, comprising 3716\nrecordings with 19 CVD categories. We systematically evaluate four\nrepresentative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under\nboth 9-lead and 12-lead configurations. All models achieved strong results,\nwith Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.\nResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and\nTransformer also showed competitive performance. Per-class analysis indicated\nchallenges for rare conditions such as hypertrophic cardiomyopathy in the\n9-lead subset, reflecting the effect of limited positive samples. This\nbenchmark establishes reusable baselines and highlights complementary strengths\nacross paradigms. It further points to the need for larger-scale, multi-center\nvalidation, age-stratified analysis, and broader disease coverage to support\nreal-world pediatric ECG applications.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728ZZU-pECG\u6570\u636e\u96c6\u4e0a\u5bf9\u591a\u6807\u7b7e\u513f\u7ae5\u5fc3\u8840\u7ba1\u75be\u75c5\u5206\u7c7b\u8fdb\u884c\u4e86\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u7f55\u89c1\u75c5\u5206\u7c7b\u7684\u6311\u6218\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u513f\u79d1\u5065\u5eb7\u7684\u91cd\u8981\u8d1f\u62c5\uff0c\u65e9\u671f\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\uff0c\u5fc3\u7535\u56fe\u662f\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u5de5\u5177\u3002", "method": "\u5728ZZU-pECG\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08ResNet-1D\u3001BiLSTM\u3001Transformer\u548cMamba 2\uff09\uff0c\u5305\u62ec9\u5bfc\u8054\u548c12\u5bfc\u8054\u914d\u7f6e\u3002", "result": "\u6240\u6709\u6a21\u578b\u5747\u8868\u73b0\u4f18\u5f02\uff0cHamming Loss\u4f4e\u81f30.0069\uff0cF1\u5206\u6570\u5927\u591a\u8d85\u8fc785%\u3002ResNet-1D\u572812\u5bfc\u8054\u5b50\u96c6\u4e0a\u7684macro-F1\u8fbe\u523094.67%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u513f\u79d1\u5fc3\u7535\u56fe\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u57fa\u51c6\u6a21\u578b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u9700\u8981\u66f4\u5927\u89c4\u6a21\u3001\u591a\u4e2d\u5fc3\u9a8c\u8bc1\u548c\u66f4\u5e7f\u6cdb\u75be\u75c5\u8986\u76d6\u7684\u65b9\u5411\u3002"}}
{"id": "2510.03471", "pdf": "https://arxiv.org/pdf/2510.03471", "abs": "https://arxiv.org/abs/2510.03471", "authors": ["Dingqi Zhang", "Ran Tao", "Sheng Cheng", "Naira Hovakimyan", "Mark W. Mueller"], "title": "A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control", "categories": ["cs.RO"], "comment": null, "summary": "Robust adaptive control methods are essential for maintaining quadcopter\nperformance under external disturbances and model uncertainties. However,\nfragmented evaluations across tasks, simulators, and implementations hinder\nsystematic comparison of these methods. This paper introduces an\neasy-to-deploy, modular simulation testbed for quadcopter control, built on\nRotorPy, that enables evaluation under a wide range of disturbances such as\nwind, payload shifts, rotor faults, and control latency. The framework includes\na library of representative adaptive and non-adaptive controllers and provides\ntask-relevant metrics to assess tracking accuracy and robustness. The unified\nmodular environment enables reproducible evaluation across control methods and\neliminates redundant reimplementation of components such as disturbance models,\ntrajectory generators, and analysis tools. We illustrate the testbed's\nversatility through examples spanning multiple disturbance scenarios and\ntrajectory types, including automated stress testing, to demonstrate its\nutility for systematic analysis. Code is available at\nhttps://github.com/Dz298/AdaptiveQuadBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u56db\u65cb\u7ffc\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0RotorPy\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u79cd\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u8bc4\u4f30\u5206\u6563\u7684\u95ee\u9898\u3002", "motivation": "\u56db\u65cb\u7ffc\u5728\u5916\u754c\u5e72\u6270\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6027\u80fd\u9700\u8981\u9c81\u68d2\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u5e73\u53f0\u6765\u7cfb\u7edf\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eRotorPy\u5f00\u53d1\u4e86\u4e00\u4e2a\u6613\u4e8e\u90e8\u7f72\u7684\u6a21\u5757\u5316\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u5e72\u6270\u6a21\u578b\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4efb\u52a1\u76f8\u5173\u6307\u6807\u3002", "result": "\u5e73\u53f0\u652f\u6301\u591a\u79cd\u5e72\u6270\u573a\u666f\u548c\u8f68\u8ff9\u7c7b\u578b\u7684\u8bc4\u4f30\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u538b\u529b\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u7cfb\u7edf\u5206\u6790\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6d4b\u8bd5\u5e73\u53f0\u4e3a\u56db\u65cb\u7ffc\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u3001\u6a21\u5757\u5316\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u4fc3\u8fdb\u4e86\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\u7684\u7814\u7a76\u3002"}}
{"id": "2510.03787", "pdf": "https://arxiv.org/pdf/2510.03787", "abs": "https://arxiv.org/abs/2510.03787", "authors": ["Jacopo Pegoraro", "Gianmaria Ventura", "Dario Tagliaferri", "Marco Mezzavilla", "Andrea Bedin", "Michele Rossi", "Joerg Widmer"], "title": "Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms", "categories": ["eess.SP"], "comment": "19 pages, 14 figures", "summary": "Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G\nwireless networks. The bandwidth availability and diversity of FR3 offer\nunprecedented opportunities for coherent multiband Integrated Sensing and\nCommunications (ISAC), which aggregates the carrier phase information from\nmultiple frequency bands to increase the sensing resolution to the cm-level.\nHowever, the frequency anisotropy of sensing targets over GHz-wide bands and\nthe non-contiguity of the 6G spectrum, pose critical challenges to the\napplication of existing multiband ISAC techniques. We present the first study\non coherent multiband sensing in FR3. We experimentally characterize the\nfrequency anisotropy of targets and propose new phase coherence metrics for\nmultiband processing. Then, we analyze the impact of non-contiguous FR3 bands\nconsidered by 3GPP, and design a new algorithm to mitigate the resulting\nsensing artifacts, outperforming existing techniques. Our results represent a\nfirst step toward fully developing multiband ISAC for FR3.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e866G\u7f51\u7edc\u4e2d7-24 GHz\u9891\u6bb5\uff08FR3\uff09\u7684\u591a\u9891\u6bb5\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u9488\u5bf9FR3\u7684\u76f8\u5e72\u591a\u9891\u6bb5\u611f\u77e5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9891\u8c31\u975e\u8fde\u7eed\u6027\u548c\u9891\u7387\u5404\u5411\u5f02\u6027\u5bfc\u81f4\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5\u4f18\u5316\u611f\u77e5\u6027\u80fd\u3002", "motivation": "FR3\u9891\u6bb5\u4e3a6G\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u5e26\u5bbd\u591a\u6837\u6027\u548c\u611f\u77e5\u673a\u4f1a\uff0c\u4f46\u5176\u9891\u7387\u5404\u5411\u5f02\u6027\u548c\u975e\u8fde\u7eed\u9891\u8c31\u7279\u6027\u5bf9\u73b0\u6709\u591a\u9891\u6bb5ISAC\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8868\u5f81\u76ee\u6807\u7684\u9891\u7387\u5404\u5411\u5f02\u6027\uff0c\u63d0\u51fa\u65b0\u7684\u76f8\u4f4d\u76f8\u5e72\u6027\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u4ee5\u6d88\u9664\u975e\u8fde\u7eed\u9891\u6bb5\u5e26\u6765\u7684\u611f\u77e5\u4f2a\u5f71\u3002", "result": "\u65b0\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3aFR3\u591a\u9891\u6bb5ISAC\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3FR3\u9891\u6bb5ISAC\u7684\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e866G\u591a\u9891\u6bb5\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.03472", "pdf": "https://arxiv.org/pdf/2510.03472", "abs": "https://arxiv.org/abs/2510.03472", "authors": ["Yulun Zhang", "Alexandre O. G. Barbosa", "Federico Pecora", "Jiaoyang Li"], "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS) 2025", "summary": "We study optimizing a destination-to-chutes task mapping to improve\nthroughput in Robotic Sorting Systems (RSS), where a team of robots sort\npackages on a sortation floor by transporting them from induct workstations to\neject chutes based on their shipping destinations (e.g. Los Angeles or\nPittsburgh). The destination-to-chutes task mapping is used to determine which\nchutes a robot can drop its package. Finding a high-quality task mapping is\nchallenging because of the complexity of a real-world RSS. First, optimizing\ntask mapping is interdependent with robot target assignment and path planning.\nSecond, chutes will be CLOSED for a period of time once they receive sufficient\npackages to allow for downstream processing. Third, task mapping quality\ndirectly impacts the downstream processing, as scattered chutes for the same\ndestination increase package handling time. In this paper, we first formally\ndefine task mappings and the problem of Task Mapping Optimization (TMO). We\nthen present a simulator of RSS to evaluate task mappings. We then present a\nsimple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear\nProgramming, demonstrating the advantage of our optimized task mappings over\nthe greedily generated ones in various RSS setups with different map sizes,\nnumbers of chutes, and destinations. Finally, we use Quality Diversity\nalgorithms to analyze the throughput of a diverse set of task mappings. Our\ncode is available online at https://github.com/lunjohnzhang/tmo_public.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4f18\u5316\u76ee\u7684\u5730\u5230\u6ed1\u69fd\u7684\u4efb\u52a1\u6620\u5c04\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\uff08RSS\uff09\u7684\u541e\u5410\u91cf\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u6620\u5c04\u4e0e\u673a\u5668\u4eba\u76ee\u6807\u5206\u914d\u548c\u8def\u5f84\u89c4\u5212\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u5316\u8d28\u91cf\u7b97\u6cd5\u5206\u6790\u4e86\u4efb\u52a1\u6620\u5c04\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u4e2d\u76ee\u7684\u5730\u5230\u6ed1\u69fd\u4efb\u52a1\u6620\u5c04\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u5e94\u5bf9\u6ed1\u69fd\u5173\u95ed\u548c\u4e0b\u6e38\u5904\u7406\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6b63\u5f0f\u5b9a\u4e49\u4efb\u52a1\u6620\u5c04\u548c\u4efb\u52a1\u6620\u5c04\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1RSS\u6a21\u62df\u5668\u8fdb\u884c\u8bc4\u4ef7\uff0c\u4ee5\u53ca\u63d0\u51fa\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684\u4efb\u52a1\u6620\u5c04\u5728\u5404\u79cdRSS\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u8d2a\u5a6a\u751f\u6210\u7684\u4efb\u52a1\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u5316\u8d28\u91cf\u7b97\u6cd5\u5206\u6790\u4e86\u591a\u6837\u5316\u7684\u4efb\u52a1\u6620\u5c04\u541e\u5410\u91cf\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u6620\u5c04\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u7684\u541e\u5410\u91cf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03818", "pdf": "https://arxiv.org/pdf/2510.03818", "abs": "https://arxiv.org/abs/2510.03818", "authors": ["Lulu Song", "Di Zhang", "Tingting Zhang"], "title": "Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "Source polar coding is a potential solution for short blocklength-based\nlow-latency key generation with limited sources, which is a critical aspect of\nsix generation (6G) Internet of things. However, existing source coding schemes\nstill suffer from significant degradation in key generation rate and\nreconciliation reliability in short blocklength regime. To address this issue,\nwe introduce a multilevel source polarization-adjusted convolutional (PAC)\ncoding framework. Furthermore, we propose a novel code construction algorithm\nthat jointly leverages polarization effects and the maximum likelihood (ML)\ndecoding error coefficient. Simulations demonstrate that the multilevel source\nPAC scheme with the proposed code construction achieves superior key generation\nrate under key disagreement constraints compared to conventional and multilevel\nsource polar coding methods even in short blocklength regimes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u6e90\u6781\u5316\u81ea\u9002\u5e94\u5377\u79ef\uff08PAC\uff09\u7f16\u7801\u6846\u67b6\u53ca\u65b0\u7684\u7801\u6784\u9020\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u77ed\u7801\u957f\u4e0b\u5bc6\u94a5\u751f\u6210\u7387\u548c\u534f\u8c03\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002", "motivation": "6G\u7269\u8054\u7f51\u4e2d\u77ed\u7801\u957f\u4f4e\u5ef6\u8fdf\u5bc6\u94a5\u751f\u6210\u7684\u9700\u6c42\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u6e90\u7f16\u7801\u65b9\u6848\u5728\u77ed\u7801\u957f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u6e90PAC\u7f16\u7801\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u6781\u5316\u6548\u5e94\u548c\u6700\u5927\u4f3c\u7136\u89e3\u7801\u9519\u8bef\u7cfb\u6570\u8bbe\u8ba1\u65b0\u578b\u7801\u6784\u9020\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u77ed\u7801\u957f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u548c\u591a\u7ea7\u6e90\u6781\u5316\u7f16\u7801\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u5bc6\u94a5\u751f\u6210\u7387\u3002", "conclusion": "\u591a\u7ea7\u6e90PAC\u7f16\u7801\u6846\u67b6\u53ca\u8054\u5408\u4f18\u5316\u7684\u7801\u6784\u9020\u662f\u77ed\u7801\u957f\u5bc6\u94a5\u751f\u6210\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.03481", "pdf": "https://arxiv.org/pdf/2510.03481", "abs": "https://arxiv.org/abs/2510.03481", "authors": ["Khang Vo Huynh", "David Parker", "Lu Feng"], "title": "Robust Permissive Controller Synthesis for Interval MDPs", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We address the problem of robust permissive controller synthesis for robots\noperating under uncertain dynamics, modeled as Interval Markov Decision\nProcesses (IMDPs). IMDPs generalize standard MDPs by allowing transition\nprobabilities to vary within intervals, capturing epistemic uncertainty from\nsensing noise, actuation imprecision, and coarse system abstractions-common in\nrobotics. Traditional controller synthesis typically yields a single\ndeterministic strategy, limiting adaptability. In contrast, permissive\ncontrollers (multi-strategies) allow multiple actions per state, enabling\nruntime flexibility and resilience. However, prior work on permissive\ncontroller synthesis generally assumes exact transition probabilities, which is\nunrealistic in many robotic applications. We present the first framework for\nrobust permissive controller synthesis on IMDPs, guaranteeing that all\nstrategies compliant with the synthesized multi-strategy satisfy reachability\nor reward-based specifications under all admissible transitions. We formulate\nthe problem as mixed-integer linear programs (MILPs) and propose two encodings:\na baseline vertex-enumeration method and a scalable duality-based method that\navoids explicit enumeration. Experiments on four benchmark domains show that\nboth methods synthesize robust, maximally permissive controllers and scale to\nlarge IMDPs with up to hundreds of thousands of states.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIMDP\u7684\u9c81\u68d2\u5bb9\u5fcd\u63a7\u5236\u5668\u5408\u6210\u6846\u67b6\uff0c\u9996\u6b21\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u52a8\u6001\u4e0b\u7684\u591a\u7b56\u7565\u5408\u6210\u95ee\u9898\uff0c\u5e76\u901a\u8fc7MILP\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u4e0d\u786e\u5b9a\u52a8\u6001\uff08IMDP\u5efa\u6a21\uff09\uff0c\u4f20\u7edf\u786e\u5b9a\u6027\u7b56\u7565\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u73b0\u6709\u5bb9\u5fcd\u63a7\u5236\u5668\u65b9\u6cd5\u672a\u8003\u8651\u6982\u7387\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\uff0c\u63d0\u51fa\u4e24\u79cd\u7f16\u7801\u65b9\u6cd5\uff1a\u57fa\u7ebf\u9876\u70b9\u679a\u4e3e\u6cd5\u548c\u53ef\u6269\u5c55\u7684\u5bf9\u5076\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u5408\u6210\u9c81\u68d2\u4e14\u6700\u5927\u5316\u5bb9\u5fcd\u7684\u63a7\u5236\u5668\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u6570\u5341\u4e07\u72b6\u6001\u7684\u5927\u578bIMDP\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5728IMDP\u4e0a\u5b9e\u73b0\u9c81\u68d2\u5bb9\u5fcd\u63a7\u5236\u5668\u5408\u6210\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u9e3f\u6c9f\uff0c\u5177\u5907\u9ad8\u6548\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.03848", "pdf": "https://arxiv.org/pdf/2510.03848", "abs": "https://arxiv.org/abs/2510.03848", "authors": ["Jianyu Wang", "Zhichao Li", "Wenchi Cheng", "Wei Zhang", "Hailin Zhang"], "title": "Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums", "categories": ["eess.SP"], "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. However, the propagation mediums in practical MI based underground\nemergency communications are usually diverse and composed randomly due to the\nimpact of disasters, which poses a challenge for MI communication in practical\napplications. In this paper, we formulate a statistical fading channel model,\nwhich reflects the random composition of diverse mediums and is shown to follow\na lognormal distribution. To mitigate the impact of diverse medium fading,\nMulti-frequency Resonating Compensation (MuReC) based coils are used to achieve\nmultiband transmission. Then, we analyze the performance of MuReC based\nmulti-band MI communication with diverse medium fading and derive the\nexpressions of signal-to-noise ratio (SNR) probability density functions,\nergodic capacities, average bit error rates (BERs), and outage probabilities\nfor both multiplexing and diversity cases. Numerical results show that MuReC\nbased multiband transmission schemes can effectively reduce the impact of\ndiverse medium fading and enhance the performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9891\u8c10\u632f\u8865\u507f\u7684\u78c1\u611f\u5e94\u901a\u4fe1\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5730\u4e0b\u707e\u5bb3\u5e94\u6025\u901a\u4fe1\u4e2d\u591a\u6837\u4ecb\u8d28\u5bfc\u81f4\u7684\u4fe1\u53f7\u8870\u51cf\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u707e\u5bb3\u540e\u5730\u4e0b\u591a\u6837\u4ecb\u8d28\u968f\u673a\u7ec4\u6210\u5bfc\u81f4\u7684\u78c1\u611f\u5e94\u901a\u4fe1\u4fe1\u9053\u8870\u843d\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u7edf\u8ba1\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u591a\u9891\u8c10\u632f\u8865\u507f\u7ebf\u5708\u5b9e\u73b0\u591a\u9891\u5e26\u4f20\u8f93\u3002", "result": "\u591a\u9891\u5e26\u4f20\u8f93\u80fd\u6709\u6548\u51cf\u5c11\u4ecb\u8d28\u8870\u843d\u5f71\u54cd\uff0c\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u591a\u9891\u8c10\u632f\u8865\u507f\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u707e\u5bb3\u73af\u5883\u4e0b\u78c1\u611f\u5e94\u901a\u4fe1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.03496", "pdf": "https://arxiv.org/pdf/2510.03496", "abs": "https://arxiv.org/abs/2510.03496", "authors": ["Vadivelan Murugesan", "Rajasundaram Mathiazhagan", "Sanjana Joshi", "Aliasghar Arab"], "title": "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*", "categories": ["cs.RO"], "comment": null, "summary": "Human-robot collaboration requires precise prediction of human motion over\nextended horizons to enable proactive collision avoidance. Unlike existing\nplanners that rely solely on kinodynamic models, we present a prediction-driven\nsafe planning framework that leverages granular, joint-by-joint human motion\nforecasting validated in a physics-based digital twin. A capsule-based\nartificial potential field (APF) converts these granular predictions into\ncollision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when\nthresholds are exceeded. The depth camera is used to extract 3D skeletal poses\nand a convolutional neural network-bidirectional long short-term memory\n(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A\ndigital twin model integrates real-time human posture prediction placed in\nfront of a simulated robot to evaluate motions and physical contacts. The\nproposed method enables validation of planned trajectories ahead of time and\nbridging potential latency gaps in updating planned trajectories in real-time.\nIn 50 trials, our method achieved 100% proactive avoidance with > 250 mm\nclearance and sub-2 s replanning, demonstrating superior precision and\nreliability compared to existing kinematic-only planners through the\nintegration of predictive human modeling with digital twin validation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u7684\u5b89\u5168\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u5b9e\u73b0\u4eba\u7c7b\u8fd0\u52a8\u7684\u7cbe\u786e\u9884\u6d4b\u548c\u4e3b\u52a8\u907f\u78b0\u3002", "motivation": "\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u9700\u8981\u7cbe\u786e\u9884\u6d4b\u4eba\u7c7b\u8fd0\u52a8\u4ee5\u907f\u514d\u78b0\u649e\uff0c\u73b0\u6709\u89c4\u5212\u5668\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e3b\u52a8\u907f\u78b0\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408CNN-BiLSTM\u6a21\u578b\u9884\u6d4b\u5173\u8282\u8f68\u8ff9\uff0c\u5229\u7528\u80f6\u56ca\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u8ba1\u7b97\u78b0\u649e\u98ce\u9669\uff0c\u89e6\u53d1\u81ea\u9002\u5e94RRT*\u89c4\u5212\u5668\uff0c\u5e76\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u5b9e\u65f6\u9a8c\u8bc1\u3002", "result": "\u572850\u6b21\u8bd5\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u4e3b\u52a8\u907f\u78b0\uff0c\u78b0\u649e\u8ddd\u79bb\u5927\u4e8e250\u6beb\u7c73\uff0c\u4e14\u91cd\u65b0\u89c4\u5212\u65f6\u95f4\u4f4e\u4e8e2\u79d2\u3002", "conclusion": "\u7ed3\u5408\u9884\u6d4b\u6a21\u578b\u548c\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u7684\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u4e0a\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5b66\u7684\u89c4\u5212\u5668\u3002"}}
{"id": "2510.03850", "pdf": "https://arxiv.org/pdf/2510.03850", "abs": "https://arxiv.org/abs/2510.03850", "authors": ["Fernando Dar\u00edo Almeida Garc\u00eda", "Francisco Raimundo Albuquerque Parente", "Michel Daoud Yacoub", "Jose C\u00e2ndido Silveira Santos Filho"], "title": "On the Exact Sum PDF and CDF of \u03b1-\u03bc Variates", "categories": ["eess.SP"], "comment": null, "summary": "The sum of random variables (RVs) appears extensively in wireless\ncommunications, at large, both conventional and advanced, and has been subject\nof longstanding research. The statistical characterization of the referred sum\nis crucial to determine the performance of such communications systems.\nAlthough efforts have been undertaken to unveil these sum statistics, e.g.,\nprobability density function (PDF) and cumulative distribution function (CDF),\nno general efficient nor manageable solutions capable of evaluating the exact\nsum PDF and CDF are available to date. The only formulations are given in terms\nof either the multi-fold Brennan's integral or the multivariate Fox H-function.\nUnfortunately, these methods are only feasible up to a certain number of RVs,\nmeaning that when the number of RVs in the sum increases, the computation of\nthe sum PDF and CDF is subject to stability problems, convergence issues, or\ninaccurate results. In this paper, we derive new, simple, exact formulations\nfor the PDF and CDF of the sum of L independent and identically distributed\n{\\alpha}-{\\mu} RVs. Unlike the available solutions, the computational\ncomplexity of our analytical expressions is independent of the number of\nsummands. Capitalizing on our unprecedented findings, we analyze, in exact and\nasymptotic manners, the performance of L-branch pre-detection equal-gain\ncombining and maximal-ratio combining receivers over {\\alpha}-{\\mu} fading\nenvironments. The coding and diversity gains of the system for both receivers\nare analyzed and quantified. Moreover, numerical simulations show that the\ncomputation time reduces drastically when using our expressions, which are\narguably the most efficient and manageable formulations derived so far.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u7b80\u5355\u4e14\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u72ec\u7acb\u540c\u5206\u5e03\u03b1-\u03bc\u968f\u673a\u53d8\u91cf\u548c\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08PDF\uff09\u548c\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\uff0c\u968f\u673a\u53d8\u91cf\u548c\u7684\u7edf\u8ba1\u7279\u6027\u5bf9\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u968f\u673a\u53d8\u91cf\u65f6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63a8\u5bfc\u4e86\u72ec\u7acb\u540c\u5206\u5e03\u03b1-\u03bc\u968f\u673a\u53d8\u91cf\u548c\u7684PDF\u548cCDF\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u6c42\u548c\u53d8\u91cf\u7684\u6570\u91cf\u65e0\u5173\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u5728L\u5206\u652f\u9884\u68c0\u6d4b\u7b49\u589e\u76ca\u5408\u5e76\u548c\u6700\u5927\u6bd4\u7387\u5408\u5e76\u63a5\u6536\u5668\u4e2d\u5f97\u5230\u4e86\u5e94\u7528\u548c\u5206\u6790\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u7ba1\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u7cbe\u786e\u548c\u6e10\u8fdb\u5206\u6790\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03504", "pdf": "https://arxiv.org/pdf/2510.03504", "abs": "https://arxiv.org/abs/2510.03504", "authors": ["Yutong Wang", "Yichun Qu", "Tengxiang Wang", "Lishuo Pan", "Nora Ayanian"], "title": "Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning", "categories": ["cs.RO"], "comment": null, "summary": "Maintaining connectivity is crucial in many multi-robot applications, yet\nfragile to obstacles and visual occlusions. We present a real-time distributed\nframework for multi-robot navigation certified by high-order control barrier\nfunctions (HOCBFs) that controls inter-robot proximity to maintain connectivity\nwhile avoiding collisions. We incorporate control Lyapunov functions to enable\nconnectivity recovery from initial disconnected configurations and temporary\nlosses, providing robust connectivity during navigation in obstacle-rich\nenvironments. Our trajectory generation framework concurrently produces\nplanning and control through a Bezier-parameterized trajectory, which naturally\nprovides smooth curves with arbitrary degree of derivatives. The main\ncontribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory\ngeneration and control method for connectivity maintenance and recovery of\nmulti-robot systems. We validate the framework through extensive simulations\nand a physical experiment with 4 Crazyflie nano-quadrotors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u9636\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08HOCBFs\uff09\u4fdd\u8bc1\u673a\u5668\u4eba\u95f4\u7684\u8fde\u63a5\u6027\uff0c\u540c\u65f6\u907f\u514d\u78b0\u649e\uff0c\u5e76\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u8fde\u63a5\u6062\u590d\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4fdd\u6301\u8fde\u63a5\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6613\u53d7\u969c\u788d\u7269\u548c\u89c6\u89c9\u906e\u6321\u5f71\u54cd\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63a7\u5236\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\uff08CLFs\uff09\u548cHOCBFs\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u585e\u5c14\u53c2\u6570\u5316\u8f68\u8ff9\u751f\u6210\u5e73\u6ed1\u66f2\u7ebf\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684MPC-CLF-CBF\u65b9\u6cd5\u3002", "result": "\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u8fde\u63a5\u6027\u7ef4\u62a4\u4e0e\u6062\u590d\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c4\u4e2aCrazyflie\u7eb3\u7c73\u56db\u65cb\u7ffc\u7684\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684MPC-CLF-CBF\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2510.03852", "pdf": "https://arxiv.org/pdf/2510.03852", "abs": "https://arxiv.org/abs/2510.03852", "authors": ["Jianyu Wang", "Tianrui Hou", "Wenchi Cheng", "Hailin Zhang"], "title": "Robust Beamforming for Magnetic Induction Based Underground Emergency Communications", "categories": ["eess.SP"], "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. Based on channel state information (CSI), magnetic beamforming can\nsignificantly improve the performance of MI communication. However, in\npost-disaster underground communication, channel estimation may suffer from\nerrors due to factors such as complex environmental interferences. Taking\nchannel estimation error into account, we formulate a beamforming optimization\nproblem for multi-user MI underground emergency communications, which aims to\nminimize the power consumption under the constraints of sum rate and signal to\ninterference plus noise ratio (SINR) of each user. Based on the worst-case\noptimization criterion and the S-procedure, the non-convex optimization problem\nis transformed into convex and solved. Numerical results show that the proposed\nrobust beamforming scheme can effectively enhance communication reliability and\neffective throughput in the presence of channel estimation errors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8003\u8651\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u7684\u78c1\u611f\u5e94\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u529f\u8017\u7684\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u7684\u603b\u901f\u7387\u548c\u4fe1\u566a\u6bd4\u8981\u6c42\u3002", "motivation": "\u7531\u4e8e\u5730\u4e0b\u73af\u5883\u590d\u6742\uff0c\u4fe1\u9053\u4f30\u8ba1\u53ef\u80fd\u53d7\u5e72\u6270\u800c\u4ea7\u751f\u8bef\u5dee\uff0c\u5f71\u54cd\u78c1\u611f\u5e94\u901a\u4fe1\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6700\u574f\u60c5\u51b5\u4e0b\u4f18\u5316\u51c6\u5219\u548cS\u8fc7\u7a0b\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u95ee\u9898\u5e76\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6848\u80fd\u6709\u6548\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u5b58\u5728\u65f6\u7684\u901a\u4fe1\u53ef\u9760\u6027\u548c\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03529", "pdf": "https://arxiv.org/pdf/2510.03529", "abs": "https://arxiv.org/abs/2510.03529", "authors": ["Zekai Liang", "Xiao Liang", "Soofiyan Atar", "Sreyan Das", "Zoe Chiu", "Peihan Zhang", "Florian Richter", "Shanglei Liu", "Michael C. Yip"], "title": "LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy", "categories": ["cs.RO"], "comment": null, "summary": "Robotic laparoscopic surgery has gained increasing attention in recent years\nfor its potential to deliver more efficient and precise minimally invasive\nprocedures. However, adoption of surgical robotic platforms remains largely\nconfined to high-resource medical centers, exacerbating healthcare disparities\nin rural and low-resource regions. To close this gap, a range of solutions has\nbeen explored, from remote mentorship to fully remote telesurgery. Yet, the\npractical deployment of surgical robotic systems to underserved communities\nremains an unsolved challenge. Humanoid systems offer a promising path toward\ndeployability, as they can directly operate in environments designed for humans\nwithout extensive infrastructure modifications -- including operating rooms. In\nthis work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic\nteleoperation framework. The system leverages an inverse-mapping strategy for\nmanual-wristed laparoscopic instruments that abides to remote center-of-motion\nconstraints, enabling precise hand-to-tool control of off-the-shelf surgical\nlaparoscopic tools without additional setup requirements. A control console\nequipped with a stereo vision system provides real-time visual feedback.\nFinally, a comprehensive user study across platforms demonstrates the\neffectiveness of the proposed framework and provides initial evidence for the\nfeasibility of deploying humanoid robots in laparoscopic procedures.", "AI": {"tldr": "LapSurgie\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8179\u8154\u955c\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u57fa\u7840\u8bbe\u65bd\u6539\u9020\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u5728\u9ad8\u8d44\u6e90\u533b\u7597\u4e2d\u5fc3\u4ee5\u5916\u7684\u5730\u533a\u90e8\u7f72\u56f0\u96be\uff0c\u52a0\u5267\u4e86\u533b\u7597\u8d44\u6e90\u4e0d\u5747\u3002\u4eba\u5f62\u673a\u5668\u4eba\u56e0\u5176\u53ef\u76f4\u63a5\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u64cd\u4f5c\u800c\u88ab\u89c6\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "LapSurgie\u91c7\u7528\u9006\u6620\u5c04\u7b56\u7565\u63a7\u5236\u624b\u52a8\u8179\u8154\u955c\u5668\u68b0\uff0c\u9075\u5b88\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u7ea6\u675f\uff0c\u901a\u8fc7\u914d\u5907\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u7684\u63a7\u5236\u53f0\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u5168\u9762\u7684\u7528\u6237\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8179\u8154\u955c\u624b\u672f\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "LapSurgie\u4e3a\u624b\u672f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.03901", "pdf": "https://arxiv.org/pdf/2510.03901", "abs": "https://arxiv.org/abs/2510.03901", "authors": ["Vincent Savaux", "Steve Sawadogo", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications", "categories": ["eess.SP"], "comment": "9 pages, 5 figures, conference", "summary": "This paper investigates the robustness of affine frequency division\nmultiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation\nschemes against non-white Gaussian noise, which can model various sources of\nadditive disturbances to the received signal. The proposed approach\ndemonstrates that the performance of these waveforms depends on the ability of\nthe demodulation matrix to whiten the noise-a property that is, in turn,\nrelated to the sparsity of the matrix. AFDM is shown to outperform OTFS and\northogonal frequency division multiplexing (OFDM), as its demodulation matrix\nis generally less sparse than those of the other waveforms. Based on this\nanalysis, several application examples and use cases are presented, such as the\nuse of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.\nFinally, simulation results confirm that AFDM achieves better performance than\nOTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in\nmost application scenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AFDM\u548cOTFS\u8c03\u5236\u65b9\u6848\u5728\u975e\u767d\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0AFDM\u56e0\u89e3\u8c03\u77e9\u9635\u7a00\u758f\u6027\u8f83\u4f4e\u800c\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8AFDM\u548cOTFS\u8c03\u5236\u65b9\u6848\u5728\u975e\u767d\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u4f18\u5316\u901a\u4fe1\u7cfb\u7edf\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u89e3\u8c03\u77e9\u9635\u7684\u767d\u5316\u548c\u7a00\u758f\u6027\u7279\u6027\uff0c\u6bd4\u8f83AFDM\u3001OTFS\u548cOFDM\u7684\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cAFDM\u5728\u975e\u767d\u566a\u58f0\u4e0b\u6027\u80fd\u6700\u4f73\uff0c\u589e\u76ca\u8d85\u8fc71 dB\u3002", "conclusion": "AFDM\u56e0\u5176\u89e3\u8c03\u77e9\u9635\u7279\u6027\u5728\u975e\u767d\u566a\u58f0\u73af\u5883\u4e2d\u4f18\u4e8eOTFS\u548cOFDM\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.03532", "pdf": "https://arxiv.org/pdf/2510.03532", "abs": "https://arxiv.org/abs/2510.03532", "authors": ["Zekai Liang", "Kazuya Miyata", "Xiao Liang", "Florian Richter", "Michael C. Yip"], "title": "Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate camera-to-robot calibration is essential for any vision-based\nrobotic control system and especially critical in minimally invasive surgical\nrobots, where instruments conduct precise micro-manipulations. However, MIS\nrobots have long kinematic chains and partial visibility of their degrees of\nfreedom in the camera, which introduces challenges for conventional\ncamera-to-robot calibration methods that assume stiff robots with good\nvisibility. Previous works have investigated both keypoint-based and\nrendering-based approaches to address this challenge in real-world conditions;\nhowever, they often struggle with consistent feature detection or have long\ninference times, neither of which are ideal for online robot control. In this\nwork, we propose a novel framework that unifies the detection of geometric\nprimitives (keypoints and shaft edges) through a shared encoding, enabling\nefficient pose estimation via projection geometry. This architecture detects\nboth keypoints and edges in a single inference and is trained on large-scale\nsynthetic data with projective labeling. This method is evaluated across both\nfeature detection and pose estimation, with qualitative and quantitative\nresults demonstrating fast performance and state-of-the-art accuracy in\nchallenging surgical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u7edf\u4e00\u51e0\u4f55\u57fa\u5143\uff08\u5173\u952e\u70b9\u548c\u8f74\u8fb9\u7f18\uff09\u7684\u68c0\u6d4b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5fae\u521b\u624b\u672f\u673a\u5668\u4eba\u4e2d\u4f20\u7edf\u76f8\u673a-\u673a\u5668\u4eba\u6807\u5b9a\u65b9\u6cd5\u7531\u4e8e\u957f\u8fd0\u52a8\u94fe\u548c\u90e8\u5206\u81ea\u7531\u5ea6\u53ef\u89c1\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5355\u4e00\u63a8\u7406\u540c\u65f6\u68c0\u6d4b\u5173\u952e\u70b9\u548c\u8fb9\u7f18\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u4e0e\u6295\u5f71\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u7279\u5f81\u68c0\u6d4b\u548c\u4f4d\u59ff\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u5feb\u4e14\u5728\u624b\u672f\u73af\u5883\u4e2d\u8fbe\u5230\u6700\u65b0\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a-\u673a\u5668\u4eba\u6807\u5b9a\u3002"}}
{"id": "2510.04037", "pdf": "https://arxiv.org/pdf/2510.04037", "abs": "https://arxiv.org/abs/2510.04037", "authors": ["Mohammad Salman", "Hadi Zayyani", "Hasan Abu Hilal", "Mostafa Rashdan"], "title": "Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate", "categories": ["eess.SP"], "comment": null, "summary": "This letter presents a novel method for estimating the position, velocity,\nand acceleration of a moving target using range-based measurements. Although\nmost existing studies focus on position and velocity estimation, the framework\nof this letter is extended to include acceleration. To achieve this, we propose\nusing the derivative of the range rate, in addition to the range and range rate\nmeasurements. The proposed method estimates the position at first using\nTime-of-Arrival (TOA)-based techniques; then, develops a reformulated least\nsquares (LS) and weighted least squares (WLS) approaches for velocity\nestimation; and finally, employs the derivative of the range rate to estimate\nthe acceleration using previous position and velocity estimates. On the other\nhand, closed-form LS and WLS solutions are derived for both velocity and\nacceleration. The simulation results show that the proposed approach provides\nimproved performance in estimating moving target kinematics compared to\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8ddd\u79bb\u6d4b\u91cf\u7684\u8fd0\u52a8\u76ee\u6807\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8ddd\u79bb\u7387\u5bfc\u6570\u6765\u63d0\u9ad8\u4f30\u8ba1\u6027\u80fd\uff0c\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u800c\u5ffd\u7565\u4e86\u52a0\u901f\u5ea6\u7684\u91cd\u8981\u6027\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u6846\u67b6\uff0c\u5c06\u52a0\u901f\u5ea6\u7eb3\u5165\u4f30\u8ba1\u8303\u56f4\u3002", "method": "\u9996\u5148\u5229\u7528\u5230\u8fbe\u65f6\u95f4\uff08TOA\uff09\u6280\u672f\u4f30\u8ba1\u76ee\u6807\u4f4d\u7f6e\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u91cd\u65b0\u6784\u9020\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08LS\uff09\u548c\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08WLS\uff09\u4f30\u8ba1\u901f\u5ea6\uff1b\u6700\u540e\uff0c\u5229\u7528\u8ddd\u79bb\u7387\u7684\u5bfc\u6570\u5e76\u7ed3\u5408\u5df2\u6709\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u6765\u4f30\u8ba1\u52a0\u901f\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fd0\u52a8\u76ee\u6807\u8fd0\u52a8\u5b66\u53c2\u6570\u4f30\u8ba1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8ddd\u79bb\u7387\u5bfc\u6570\uff0c\u6210\u529f\u6269\u5c55\u4e86\u8fd0\u52a8\u76ee\u6807\u7684\u8fd0\u52a8\u5b66\u53c2\u6570\u4f30\u8ba1\u8303\u56f4\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.03547", "pdf": "https://arxiv.org/pdf/2510.03547", "abs": "https://arxiv.org/abs/2510.03547", "authors": ["Carina Veil", "Moritz Flaschel", "Ellen Kuhl"], "title": "Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots", "categories": ["cs.RO"], "comment": null, "summary": "Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary\nflexibility to bend, twist, and elongate in ways that rigid robots cannot.\nHowever, their motion planning remains a challenge, especially in cluttered\nenvironments with obstacles, due to their highly nonlinear and\ninfinite-dimensional kinematics. Here, we present a graph-based path planning\ntool for an elephant-trunk-inspired soft robotic arm designed with three\nartificial muscle fibers that allow for multimodal continuous deformation\nthrough contraction. Using a biomechanical model inspired by morphoelasticity\nand active filament theory, we precompute a shape library and construct a\n$k$-nearest neighbor graph in \\emph{shape space}, ensuring that each node\ncorresponds to a mechanically accurate and physically valid robot shape. For\nthe graph, we use signed distance functions to prune nodes and edges colliding\nwith obstacles, and define multi-objective edge costs based on geometric\ndistance and actuation effort, enabling energy-efficient planning with\ncollision avoidance. We demonstrate that our algorithm reliably avoids\nobstacles and generates feasible paths within milliseconds from precomputed\ngraphs using Dijkstra's algorithm. We show that including energy costs can\ndrastically reduce the actuation effort compared to geometry-only planning, at\nthe expense of longer tip trajectories. Our results highlight the potential of\nshape-space graph search for fast and reliable path planning in the field of\nsoft robotics, paving the way for real-time applications in surgical,\nindustrial, and assistive settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8def\u5f84\u89c4\u5212\u5de5\u5177\uff0c\u7528\u4e8e\u8c61\u9f3b\u542f\u53d1\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u81c2\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u5f62\u72b6\u5e93\u548c\u6784\u5efa\u5f62\u72b6\u7a7a\u95f4\u4e2d\u7684\u8fd1\u90bb\u56fe\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u78b0\u649e\u907f\u514d\u548c\u80fd\u91cf\u4f18\u5316\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u56e0\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u65e0\u9650\u7ef4\u8fd0\u52a8\u5b66\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u751f\u7269\u529b\u5b66\u6a21\u578b\u9884\u8ba1\u7b97\u5f62\u72b6\u5e93\uff0c\u6784\u5efa\u5f62\u72b6\u7a7a\u95f4\u7684\u8fd1\u90bb\u56fe\uff0c\u901a\u8fc7\u8ddd\u79bb\u51fd\u6570\u4fee\u526a\u78b0\u649e\u8282\u70b9\u548c\u8fb9\uff0c\u5e76\u57fa\u4e8e\u51e0\u4f55\u8ddd\u79bb\u548c\u9a71\u52a8\u52aa\u529b\u5b9a\u4e49\u591a\u76ee\u6807\u8fb9\u6210\u672c\u3002", "result": "\u7b97\u6cd5\u80fd\u5728\u6beb\u79d2\u5185\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u80fd\u91cf\u6210\u672c\u4f18\u5316\u663e\u8457\u51cf\u5c11\u9a71\u52a8\u52aa\u529b\uff0c\u5c3d\u7ba1\u5c16\u7aef\u8f68\u8ff9\u7565\u957f\u3002", "conclusion": "\u5f62\u72b6\u7a7a\u95f4\u56fe\u641c\u7d22\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u5feb\u901f\u53ef\u9760\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u624b\u672f\u3001\u5de5\u4e1a\u548c\u8f85\u52a9\u9886\u57df\u3002"}}
{"id": "2510.04160", "pdf": "https://arxiv.org/pdf/2510.04160", "abs": "https://arxiv.org/abs/2510.04160", "authors": ["Mohammad Kazzazi", "Mohammad Morsali", "Rouhollah Amiri"], "title": "CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization", "categories": ["eess.SP"], "comment": "Mohammad Kazzazi and Mohammad Morsali contributed equally to this\n  work", "summary": "This paper presents CLEAR -- a closed-form localization estimator with a\nreduced sensor network. The proposed method is a computationally efficient,\ntwo-stage estimator that fuses time-difference-of-arrival (TDOA) and\nfrequency-difference-of-arrival (FDOA) measurements with a minimal number of\nsensors. CLEAR localizes a moving source in N-dimensional space using only N+1\nsensors, achieving the theoretical minimum sensor count. The first stage\nintroduces auxiliary range and range-rate parameters to construct a set of\npseudo-linear equations, solved via weighted least squares. An algebraic\nelimination using Sylvester's resultant then reduces the problem to a quartic\nequation, yielding closed-form estimates for the nuisance variables. A second,\nlightweight linear refinement stage is applied to mitigate residual bias. Under\nmild Gaussian noise assumptions, the estimator's position and velocity\nestimates are statistically efficient, closely approaching the Cramer-Rao lower\nbound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios\ndemonstrate CRLB-level accuracy and consistent performance gains over\nrepresentative two-stage and iterative baselines, confirming the method's high\nsuitability for power-constrained, distributed Internet of Things (IoT)\napplications such as UAV tracking and smart transportation.", "AI": {"tldr": "CLEAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u9636\u6bb5\u5b9a\u4f4d\u4f30\u8ba1\u5668\uff0c\u878d\u5408TDOA\u548cFDOA\u6d4b\u91cf\uff0c\u4f7f\u7528\u6700\u5c11\u4f20\u611f\u5668\uff08N+1\u4e2a\uff09\u5b9a\u4f4d\u79fb\u52a8\u76ee\u6807\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0fIoT\u5e94\u7528\u4e2d\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u5982\u65e0\u4eba\u673a\u8ffd\u8e2a\u548c\u667a\u80fd\u4ea4\u901a\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u5f15\u5165\u8f85\u52a9\u53c2\u6570\u6784\u5efa\u4f2a\u7ebf\u6027\u65b9\u7a0b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4ee3\u6570\u6d88\u9664\u548c\u7ebf\u6027\u4f18\u5316\u51cf\u5c11\u8bef\u5dee\u3002", "result": "\u4eff\u771f\u663e\u793aCLEAR\u63a5\u8fd1CRLB\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLEAR\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684IoT\u5e94\u7528\uff0c\u63d0\u4f9b\u9ad8\u6548\u51c6\u786e\u7684\u5b9a\u4f4d\u3002"}}
{"id": "2510.03599", "pdf": "https://arxiv.org/pdf/2510.03599", "abs": "https://arxiv.org/abs/2510.03599", "authors": ["Shafeef Omar", "Majid Khadiv"], "title": "Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning", "categories": ["cs.RO"], "comment": null, "summary": "We present a unified framework for multi-task locomotion and manipulation\npolicy learning grounded in a contact-explicit representation. Instead of\ndesigning different policies for different tasks, our approach unifies the\ndefinition of a task through a sequence of contact goals-desired contact\npositions, timings, and active end-effectors. This enables leveraging the\nshared structure across diverse contact-rich tasks, leading to a single policy\nthat can perform a wide range of tasks. In particular, we train a\ngoal-conditioned reinforcement learning (RL) policy to realise given contact\nplans. We validate our framework on multiple robotic embodiments and tasks: a\nquadruped performing multiple gaits, a humanoid performing multiple biped and\nquadrupedal gaits, and a humanoid executing different bimanual object\nmanipulation tasks. Each of these scenarios is controlled by a single policy\ntrained to execute different tasks grounded in contacts, demonstrating\nversatile and robust behaviours across morphologically distinct systems. Our\nresults show that explicit contact reasoning significantly improves\ngeneralisation to unseen scenarios, positioning contact-explicit policy\nlearning as a promising foundation for scalable loco-manipulation.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63a5\u89e6\u663e\u5f0f\u8868\u793a\u5b66\u4e60\u591a\u4efb\u52a1\u7684\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5355\u7b56\u7565\u63a7\u5236\u4e0d\u540c\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u540c\u7b56\u7565\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63a5\u89e6\u76ee\u6807\u7684\u7edf\u4e00\u5b9a\u4e49\uff0c\u5171\u4eab\u8de8\u4efb\u52a1\u7684\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bad\u7ec3\u5355\u7b56\u7565\u4ee5\u5b9e\u73b0\u7ed9\u5b9a\u7684\u63a5\u89e6\u8ba1\u5212\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\u548c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u63a5\u89e6\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u8054\u5408\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04240", "pdf": "https://arxiv.org/pdf/2510.04240", "abs": "https://arxiv.org/abs/2510.04240", "authors": ["Dario Tagliaferri", "Silvia Mura", "Musa Furkan Keskin", "Sauradeep Dey", "Henk Wymeersch"], "title": "Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks", "categories": ["eess.SP"], "comment": "13 pages", "summary": "This paper addresses the challenge of integrating multistatic coherent\nimaging functionalities in the downlink (DL) of a phase-coherent distributed\nmultiple input multiple output (D-MIMO) communication network. During DL, the\nD-MIMO access points (APs) jointly precode the transmitted signals to maximize\nthe spectral efficiency (SE) at the users (UEs) locations. However, imaging\nrequires that \\textit{(i)} a fraction of the APs work as receivers for sensing\nand \\textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals\nto illuminate the area to be imaged and allow multistatic operation. In these\nsettings, our contribution is twofold. We propose a novel distributed\nintegrated sensing and communication (D-ISAC) system that superposes a\npurposely designed AP-specific signal for imaging to the legacy UE-specific\ncommunication one, with a tunable trade-off factor. We detail both the imaging\nwaveform design according to the \\textit{extended orthogonality condition} and\nthe space-frequency precoder design. Then, we propose an optimized selection\nstrategy for the receiving APs, in order to maximize imaging performance under\nhalf-duplex constraints. Extensive numerical results prove the feasibility and\nbenefits of our proposal, materializing the potential of joint multistatic\nimaging and communications in practical D-MIMO deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5e03\u5f0f\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff08D-ISAC\uff09\uff0c\u7528\u4e8e\u5728\u591a\u8f93\u5165\u591a\u8f93\u51fa\u901a\u4fe1\u7f51\u7edc\u4e2d\u5b9e\u73b0\u591a\u9759\u6001\u76f8\u5e72\u6210\u50cf\u529f\u80fd\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u591a\u8f93\u5165\u591a\u8f93\u51fa\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u4e0b\u884c\u94fe\u8def\u901a\u4fe1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u591a\u9759\u6001\u76f8\u5e72\u6210\u50cf\u529f\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u9488\u5bf9\u6210\u50cf\u7684\u7279\u5b9a\u4fe1\u53f7\u4e0e\u901a\u4fe1\u4fe1\u53f7\u7684\u53e0\u52a0\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u63a5\u6536AP\u7684\u4f18\u5316\u9009\u62e9\u7b56\u7565\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u6210\u50cf\u4e0e\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8054\u5408\u591a\u9759\u6001\u6210\u50cf\u4e0e\u901a\u4fe1\u5728D-MIMO\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2510.03640", "pdf": "https://arxiv.org/pdf/2510.03640", "abs": "https://arxiv.org/abs/2510.03640", "authors": ["Mostafa Emam", "Matthias Gerdts"], "title": "Safety-Oriented Dynamic Path Planning for Automated Vehicles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Published in 2025 IEEE 101st Vehicular Technology Conference\n  (VTC2025-Spring), Oslo, Norway, June 17-20, 2025. Received Best Conference\n  Paper Award", "summary": "Ensuring safety in autonomous vehicles necessitates advanced path planning\nand obstacle avoidance capabilities, particularly in dynamic environments. This\npaper introduces a bi-level control framework that efficiently augments road\nboundaries by incorporating time-dependent grid projections of obstacle\nmovements, thus enabling precise and adaptive path planning. The main control\nloop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path\noptimization, wherein homotopy-based constraint relaxation is employed to\nimprove the solvability of the optimal control problem (OCP). Furthermore, an\nindependent backup loop runs concurrently to provide safe fallback trajectories\nwhen an optimal trajectory cannot be computed by the main loop within a\ncritical time frame, thus enhancing safety and real-time performance. Our\nevaluation showcases the benefits of the proposed methods in various driving\nscenarios, highlighting the real-time applicability and robustness of our\napproach. Overall, the framework represents a significant step towards safer\nand more reliable autonomous driving in complex and dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u76f8\u5173\u7684\u969c\u788d\u7269\u8fd0\u52a8\u7f51\u683c\u6295\u5f71\uff0c\u5b9e\u73b0\u7cbe\u786e\u548c\u81ea\u9002\u5e94\u7684\u8def\u5f84\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\uff0c\u9700\u8981\u63d0\u5347\u8def\u5f84\u89c4\u5212\u548c\u907f\u969c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u4e3b\u5faa\u73af\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8fdb\u884c\u5b9e\u65f6\u8def\u5f84\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u540c\u4f26\u7684\u7ea6\u675f\u677e\u5f1b\u6280\u672f\u4f18\u5316\u95ee\u9898\u6c42\u89e3\uff1b\u540c\u65f6\u8fd0\u884c\u72ec\u7acb\u5907\u4efd\u5faa\u73af\u4ee5\u63d0\u4f9b\u5b89\u5168\u7684\u5907\u7528\u8f68\u8ff9\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5b9e\u65f6\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u548c\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04258", "pdf": "https://arxiv.org/pdf/2510.04258", "abs": "https://arxiv.org/abs/2510.04258", "authors": ["Ziang Zhao", "Weixi Liang", "Kai Hu", "Qun Zhang", "Xiongbin Yu", "Qiang Li"], "title": "Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments", "categories": ["eess.SP"], "comment": null, "summary": "Accurate channel modeling is essential for realizing the potential of\nterahertz (THz) communications in 6G indoor networks, where existing models\nstruggle with severe frequency selectivity and multipath effects. We propose a\nphysically grounded Rician fading channel model that jointly incorporates\ndeterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)\ncomponents, enhanced by frequency-dependent attenuation characterized by\noptimized exponents alpha and beta. Unlike conventional approaches, our model\nintegrates a two-ray reflection framework to capture standing wave phenomena\nand employs wideband spectral averaging to mitigate frequency selectivity over\nbandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning\n0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)\nas low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and\nreducing RMSE by 73.3% as bandwidth increases. These findings underscore the\nimportance of bandwidth in suppressing oscillatory artifacts and improving\nmodeling accuracy. Our approach provides a robust foundation for THz system\ndesign, supporting reliable indoor wireless personal area networks (WPANs),\ndevice-to-device (D2D) communications, and precise localization in future 6G\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684Rician\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b36G\u5ba4\u5185\u7f51\u7edc\u4e2dTHz\u901a\u4fe1\u7684\u9891\u9053\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LOS\u548cNLOS\u7ec4\u4ef6\u4ee5\u53ca\u9891\u7387\u76f8\u5173\u8870\u51cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5efa\u6a21\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u9891\u9053\u6a21\u578b\u5728\u5904\u7406THz\u901a\u4fe1\u4e2d\u7684\u9891\u7387\u9009\u62e9\u6027\u548c\u591a\u5f84\u6548\u5e94\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u6a21\u578b\u4ee5\u652f\u63016G\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdRician\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u6574\u5408\u4e86\u786e\u5b9a\u6027LOS\u548c\u968f\u673aNLOS\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u4e24\u5c04\u7ebf\u53cd\u5c04\u6846\u67b6\u548c\u5bbd\u5e26\u9891\u8c31\u5e73\u5747\u6765\u4f18\u5316\u5efa\u6a21\u3002", "result": "\u6a21\u578b\u5728208 GHz\u8f7d\u6ce2\u4e0a\u7684\u5b9e\u6d4bRMSE\u4f4e\u81f32.54 dB\uff0c\u6bd4FSPL\u63d0\u534714.2%\uff0c\u5e26\u5bbd\u589e\u52a0\u65f6RMSE\u964d\u4f4e73.3%\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aTHz\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u652f\u6301\u672a\u67656G\u4e2d\u7684\u53ef\u9760\u901a\u4fe1\u548c\u7cbe\u786e\u5b9a\u4f4d\u3002"}}
{"id": "2510.03644", "pdf": "https://arxiv.org/pdf/2510.03644", "abs": "https://arxiv.org/abs/2510.03644", "authors": ["Mohammadjavad Javadi", "Robin Chhabra"], "title": "Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing", "categories": ["cs.RO"], "comment": null, "summary": "Cosserat rod theory is the popular approach to modeling ferromagnetic soft\nrobots as 1-Dimensional (1D) slender structures in most applications, such as\nbiomedical. However, recent soft robots designed for locomotion and\nmanipulation often exhibit a large width-to-length ratio that categorizes them\nas 2D shells. For analysis and shape-morphing control purposes, we develop an\nefficient coordinate-free static model of hard-magnetic shells found in soft\nmagnetic grippers and walking soft robots. The approach is based on a novel\nformulation of Cosserat shell theory on the Special Euclidean group\n($\\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points\nwith six degrees of freedom (position & rotation) suitable for capturing the\nbehavior of a uniformly distributed array of spheroidal hard magnetic particles\nembedded in the rheological elastomer. The shell's configuration manifold is\nthe space of all smooth embeddings $\\mathbb{R}^2\\rightarrow\\mathbf{SE}(3)$.\nAccording to a novel definition of local deformation gradient based on the Lie\ngroup structure of $\\mathbf{SE}(3)$, we derive the strong and weak forms of\nequilibrium equations, following the principle of virtual work. We extract the\nlinearized version of the weak form for numerical implementations. The\nresulting finite element approach can avoid well-known challenges such as\nsingularity and locking phenomenon in modeling shell structures. The proposed\nmodel is analytically and experimentally validated through a series of test\ncases that demonstrate its superior efficacy, particularly when the shell\nundergoes severe rotations and displacements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u58f3\u7406\u8bba\u7684\u5750\u6807\u65e0\u5173\u9759\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u548c\u63a7\u5236\u5177\u6709\u5927\u5bbd\u957f\u6bd4\u7684\u78c1\u6027\u8f6f\u673a\u5668\u4eba\u7684\u5f62\u53d8\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684Cosserat\u6746\u7406\u8bba\u9002\u7528\u4e8e1D\u7ec6\u957f\u7ed3\u6784\uff0c\u4f46\u8fd1\u5e74\u6765\u8f6f\u673a\u5668\u4eba\u8bbe\u8ba1\u591a\u4e3a2D\u58f3\u7ed3\u6784\uff0c\u9700\u5f00\u53d1\u65b0\u6a21\u578b\u4ee5\u9002\u5e94\u5176\u529b\u5b66\u884c\u4e3a\u3002", "method": "\u5229\u7528\u7279\u6b8a\u7684\u6b27\u51e0\u91cc\u5f97\u7fa4\uff08SE(3)\uff09\u548cLie\u7fa4\u7ed3\u6784\uff0c\u63a8\u5bfc\u51fa\u5f3a\u5f62\u5f0f\u548c\u5f31\u5f62\u5f0f\u7684\u5e73\u8861\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u65b9\u6cd5\u5b9e\u73b0\u6570\u503c\u6a21\u62df\u3002", "result": "\u6a21\u578b\u5728\u6a21\u62df\u58f3\u7ed3\u6784\u65f6\u907f\u514d\u4e86\u5947\u5f02\u6027\u548c\u9501\u5b9a\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e25\u91cd\u65cb\u8f6c\u548c\u4f4d\u79fb\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u6a21\u578b\u4e3a\u78c1\u6027\u8f6f\u58f3\u7ed3\u6784\u7684\u5206\u6790\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u5c24\u5176\u5728\u5f62\u53d8\u5267\u70c8\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.04359", "pdf": "https://arxiv.org/pdf/2510.04359", "abs": "https://arxiv.org/abs/2510.04359", "authors": ["Minsu Kim", "Walid Saad", "Dour Calin"], "title": "Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data", "categories": ["eess.SP"], "comment": "Submitted to IEEE TWC", "summary": "In 6G wireless networks, multi-modal ML models can be leveraged to enable\nsituation-aware network decisions in dynamic environments. However, trained ML\nmodels often fail to generalize under domain shifts when training and test data\ndistributions are different because they often focus on modality-specific\nspurious features. In practical wireless systems, domain shifts occur\nfrequently due to dynamic channel statistics, moving obstacles, or hardware\nconfiguration. Thus, there is a need for learning frameworks that can achieve\nrobust generalization under scarce multi-modal data in wireless networks. In\nthis paper, a novel and data-efficient two-phase learning framework is proposed\nto improve generalization performance in unseen and unfamiliar wireless\nenvironments with minimal amount of multi-modal data. In the first stage, a\nphysics-based loss function is employed to enable each BS to learn the physics\nunderlying its wireless environment captured by multi-modal data. The\ndata-efficiency of the physics-based loss function is analytically\ninvestigated. In the second stage, collaborative domain adaptation is proposed\nto leverage the wireless environment knowledge of multiple BSs to guide\nunder-performing BSs under domain shift. Specifically, domain-similarity-aware\nmodel aggregation is proposed to utilize the knowledge of BSs that experienced\nsimilar domains. To validate the proposed framework, a new dataset generation\nframework is developed by integrating CARLA and MATLAB-based mmWave channel\nmodeling to predict mmWave RSS. Simulation results show that the proposed\nphysics-based training requires only 13% of data samples to achieve the same\nperformance as a state-of-the-art baseline that does not use physics-based\ntraining. Moreover, the proposed collaborative domain adaptation needs only 25%\nof data samples and 20% of FLOPs to achieve the convergence compared to\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e6G\u65e0\u7ebf\u7f51\u7edc\u4e2d\u591a\u6a21\u6001\u6570\u636e\u7684\u9c81\u68d2\u6cdb\u5316\uff0c\u901a\u8fc7\u7269\u7406\u635f\u5931\u51fd\u6570\u548c\u534f\u4f5c\u57df\u9002\u5e94\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u5dee\u5f02\uff0c\u4f20\u7edf\u591a\u6a21\u6001ML\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u7269\u7406\u635f\u5931\u51fd\u6570\u5b66\u4e60\u65e0\u7ebf\u73af\u5883\u7269\u7406\u7279\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u534f\u4f5c\u57df\u9002\u5e94\u548c\u57df\u76f8\u4f3c\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u4f18\u5316\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u7269\u7406\u8bad\u7ec3\u4ec5\u970013%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u57fa\u51c6\u6027\u80fd\uff1b\u534f\u4f5c\u57df\u9002\u5e94\u4ec5\u970025%\u6570\u636e\u548c20%\u8ba1\u7b97\u91cf\u5373\u53ef\u6536\u655b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e866G\u65e0\u7ebf\u7f51\u7edc\u4e2d\u591a\u6a21\u6001\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.03660", "pdf": "https://arxiv.org/pdf/2510.03660", "abs": "https://arxiv.org/abs/2510.03660", "authors": ["Mohammadjavad Javadi", "Charlie Wadds", "Robin Chhabra"], "title": "An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Untethered soft robots are essential for advancing the real-world deployment\nof soft robotic systems in diverse and multitasking environments. Inspired by\nsoft-bodied inchworm, we present a fully untethered soft robot with a curved,\nflexible structure actuated by magnetic forces. The robot has a total mass of\n102.63 g and demonstrates multimodal locomotion, achieving a maximum walking\nspeed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight\nonboard control circuit enables wireless command transmission, while an\nintegrated camera provides environmental perception. Through structural\noptimization and system-level integration, the robot successfully performs\nwalking, steering, swimming, and payload transport without reliance on external\ninfrastructure. The robot's dynamic performance and locomotion capabilities are\nsystematically validated through experimental characterization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u675f\u7f1a\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u53d7\u5c3a\u8816\u542f\u53d1\uff0c\u901a\u8fc7\u78c1\u529b\u9a71\u52a8\u5b9e\u73b0\u591a\u6a21\u6001\u8fd0\u52a8\uff0c\u5305\u62ec\u884c\u8d70\u3001\u8f6c\u5411\u3001\u6e38\u6cf3\u548c\u8d1f\u8f7d\u8fd0\u8f93\u3002", "motivation": "\u63a8\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u89e3\u51b3\u4f20\u7edf\u8f6f\u4f53\u673a\u5668\u4eba\u4f9d\u8d56\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u78c1\u529b\u9a71\u52a8\u7684\u5f2f\u66f2\u67d4\u6027\u7ed3\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u7cfb\u7edf\u96c6\u6210\u5b9e\u73b0\u65e0\u7ebf\u63a7\u5236\u548c\u73af\u5883\u611f\u77e5\u3002", "result": "\u673a\u5668\u4eba\u91cd102.63\u514b\uff0c\u884c\u8d70\u901f\u5ea63.74\u5398\u7c73/\u79d2\uff0c\u6e38\u6cf3\u901f\u5ea60.82\u5398\u7c73/\u79d2\uff0c\u5e76\u80fd\u5b8c\u6210\u591a\u9879\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u826f\u597d\u7684\u52a8\u6001\u6027\u80fd\u548c\u8fd0\u52a8\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.04402", "pdf": "https://arxiv.org/pdf/2510.04402", "abs": "https://arxiv.org/abs/2510.04402", "authors": ["Binyu Lu", "Matthias Frey", "Stark Draper", "Jingge Zhu"], "title": "Low-Rank-Based Approximate Computation with Memristors", "categories": ["eess.SP"], "comment": "5 pages, 2 figures, submitted to an IEEE conference for possible\n  publication", "summary": "Memristor crossbars enable vector-matrix multiplication (VMM), and are\npromising for low-power applications. However, it can be difficult to write the\nmemristor conductance values exactly. To improve the accuracy of VMM, we\npropose a scheme based on low-rank matrix approximation. Specifically, singular\nvalue decomposition (SVD) is first applied to obtain a low-rank approximation\nof the target matrix, which is then factored into a pair of smaller matrices.\nSubsequently, a two-step serial VMM is executed, where the stochastic write\nerrors are mitigated through step-wise averaging. To evaluate the performance\nof the proposed scheme, we derive a general expression for the resulting\ncomputation error and provide an asymptotic analysis under a prescribed\nsingular-value profile, which reveals how the error scales with matrix size and\nrank. Both analytical and numerical results confirm the superiority of the\nproposed scheme compared with the benchmark scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c\u7684\u65b9\u6848\uff0c\u901a\u8fc7SVD\u5206\u89e3\u548c\u76ee\u6807\u77e9\u9635\u7684\u5206\u6b65\u5904\u7406\uff0c\u4ee5\u51cf\u5c11\u5fc6\u963b\u5668\u4ea4\u53c9\u68d2\u4e2d\u7684\u968f\u673a\u5199\u5165\u8bef\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u7684\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u5fc6\u963b\u5668\u5728\u5199\u5165\u7535\u5bfc\u503c\u65f6\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c0f\u8bef\u5dee\u3002", "method": "\u91c7\u7528SVD\u5206\u89e3\u5c06\u76ee\u6807\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u8f83\u5c0f\u7684\u4f4e\u79e9\u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u5206\u6b65\u5e73\u5747\u51cf\u8f7b\u968f\u673a\u5199\u5165\u8bef\u5dee\u3002", "result": "\u63a8\u5bfc\u4e86\u8ba1\u7b97\u8bef\u5dee\u7684\u901a\u7528\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u6e10\u8fd1\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6848\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u5c24\u5176\u5728\u77e9\u9635\u5927\u5c0f\u548c\u79e9\u7684\u5f71\u54cd\u4e0b\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u8bef\u5dee\u7f29\u653e\u7279\u6027\u3002"}}
{"id": "2510.03677", "pdf": "https://arxiv.org/pdf/2510.03677", "abs": "https://arxiv.org/abs/2510.03677", "authors": ["Salim Rezvani", "Ammar Jaleel Mahmood", "Robin Chhabra"], "title": "Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments", "categories": ["cs.RO"], "comment": null, "summary": "Robots with internal visual self-models promise unprecedented adaptability,\nyet existing autonomous modeling pipelines remain fragile under realistic\nsensing conditions such as noisy imagery and cluttered backgrounds. This paper\npresents the first systematic study quantifying how visual\ndegradations--including blur, salt-and-pepper noise, and Gaussian noise--affect\nrobotic self-modeling. Through both simulation and physical experiments, we\ndemonstrate their impact on morphology prediction, trajectory planning, and\ndamage recovery in state-of-the-art pipelines. To overcome these challenges, we\nintroduce a task-aware denoising framework that couples classical restoration\nwith morphology-preserving constraints, ensuring retention of structural cues\ncritical for self-modeling. In addition, we integrate semantic segmentation to\nrobustly isolate robots from cluttered and colorful scenes. Extensive\nexperiments show that our approach restores near-baseline performance across\nsimulated and physical platforms, while existing pipelines degrade\nsignificantly. These contributions advance the robustness of visual\nself-modeling and establish practical foundations for deploying self-aware\nrobots in unpredictable real-world environments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u9000\u5316\u5bf9\u673a\u5668\u4eba\u81ea\u6211\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u548c\u8bed\u4e49\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6211\u5efa\u6a21\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u73b0\u6709\u81ea\u4e3b\u5efa\u6a21\u7ba1\u9053\u5728\u73b0\u5b9e\u611f\u77e5\u6761\u4ef6\u4e0b\uff08\u5982\u566a\u58f0\u56fe\u50cf\u548c\u6742\u4e71\u80cc\u666f\uff09\u8868\u73b0\u8106\u5f31\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u81ea\u6211\u5efa\u6a21\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6a21\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u91cf\u5316\u89c6\u89c9\u9000\u5316\u5f71\u54cd\uff0c\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\uff08\u7ed3\u5408\u7ecf\u5178\u6062\u590d\u6280\u672f\u548c\u5f62\u6001\u4fdd\u7559\u7ea6\u675f\uff09\uff0c\u5e76\u96c6\u6210\u8bed\u4e49\u5206\u5272\u4ee5\u9694\u79bb\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u7269\u7406\u5e73\u53f0\u4e0a\u6062\u590d\u4e86\u63a5\u8fd1\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u7ba1\u9053\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u89c6\u89c9\u81ea\u6211\u5efa\u6a21\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u5b9e\u9645\u73af\u5883\u4e2d\u90e8\u7f72\u81ea\u611f\u77e5\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04409", "pdf": "https://arxiv.org/pdf/2510.04409", "abs": "https://arxiv.org/abs/2510.04409", "authors": ["Samyadip Sarkar", "Arunashish Datta", "David Yang", "Mayukh Nath", "Shovan Maity", "Shreyas Sen"], "title": "Effect of nearby Metals on Electro-Quasistatic Human Body Communication", "categories": ["eess.SP"], "comment": "18 pages, 25 Figures, 2 Tables, 5 Appendix", "summary": "In recent decades Human Body Communication has emerged as a promising\nalternative to traditional radio wave communication, utilizing the body's\nconductive properties for low-power connectivity among wearables. This method\nharnesses the human body as an energy-efficient channel for data transmission\nwithin the electro-quasistatic frequency range, enabling advancements in\nhuman-machine interaction. While prior work has noted the role of parasitic\nreturn paths in such capacitively coupled systems, the influence of surrounding\nmetallic objects on these paths, which are critical for EQS wireless signaling,\nhas not been fully explored. This paper fills that gap with a structured study\nof how various conducting objects, from non-grounded (floating) metals and\ngrounded metals to enclosed metallic environments such as elevators and cars,\naffect the body-communication channel. We present a theoretical framework\nsupported by finite element method simulations and experiments with wearable\ndevices. Results show that metallic objects within 20 cm of devices can reduce\ntransmission loss by about 10 dB. When a device ground connects to a grounded\nmetallic object, channel gain can increase by at least 20 dB. Contact area\nduring touch-based interactions with grounded metals produces contact-impedance\ndependent high-pass channel characteristics. Proximity to metallic objects\nintroduces variability within a critical distance, with grounded metals\nproducing a larger overall effect than floating metals. These findings improve\nunderstanding of body-centric communication links and inform design for\nhealthcare, consumer electronics, defense, and industrial applications.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5468\u56f4\u91d1\u5c5e\u7269\u4f53\u5bf9\u4eba\u4f53\u901a\u4fe1\u4fe1\u9053\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u975e\u63a5\u5730\u91d1\u5c5e\u3001\u63a5\u5730\u91d1\u5c5e\u53ca\u5c01\u95ed\u91d1\u5c5e\u73af\u5883\uff0c\u53d1\u73b0\u4e86\u91d1\u5c5e\u7269\u4f53\u53ef\u663e\u8457\u51cf\u5c11\u4f20\u8f93\u635f\u8017\u548c\u63d0\u9ad8\u4fe1\u9053\u589e\u76ca\u3002", "motivation": "\u63a2\u7d22\u91d1\u5c5e\u7269\u4f53\u5bf9\u4eba\u4f53\u901a\u4fe1\u4fe1\u9053\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e4b\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u3001\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u53ca\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u9a8c\u3002", "result": "\u91d1\u5c5e\u7269\u4f53\u53ef\u51cf\u5c1110 dB\u4f20\u8f93\u635f\u5931\uff0c\u63a5\u5730\u91d1\u5c5e\u8fde\u63a5\u53ef\u63d0\u9ad820 dB\u4fe1\u9053\u589e\u76ca\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4f18\u5316\u4eba\u4f53\u901a\u4fe1\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u533b\u7597\u3001\u6d88\u8d39\u7535\u5b50\u7b49\u9886\u57df\u3002"}}
{"id": "2510.03706", "pdf": "https://arxiv.org/pdf/2510.03706", "abs": "https://arxiv.org/abs/2510.03706", "authors": ["Eadom Dessalene", "Pavan Mantripragada", "Michael Maynord", "Yiannis Aloimonos"], "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Video link:\n  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing", "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption.", "AI": {"tldr": "EmbodiSwap\u662f\u4e00\u79cd\u5c06\u673a\u5668\u4eba\u771f\u5b9e\u5408\u6210\u56fe\u50cf\u53e0\u52a0\u5230\u4eba\u7c7b\u89c6\u9891\u4e0a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u95ed\u73af\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u8bad\u7ec3\u3002\u901a\u8fc7\u91c7\u7528V-JEPA\u89c6\u89c9\u9aa8\u5e72\uff0c\u5176\u5728\u771f\u5b9e\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u9ad8\u8fbe82%\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u89c6\u9891\u4e0e\u76ee\u6807\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528EmbodiSwap\u751f\u6210\u5408\u6210\u673a\u5668\u4eba\u89c6\u9891\u6570\u636e\uff0c\u91c7\u7528V-JEPA\u4f5c\u4e3a\u89c6\u89c9\u9aa8\u5e72\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8fdb\u884c\u95ed\u73af\u7b56\u7565\u5b66\u4e60\u3002", "result": "V-JEPA\u6a21\u578b\u5728\u771f\u5b9e\u6d4b\u8bd5\u4e2d\u8fbe\u523082%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548cEmbodiSwap\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "EmbodiSwap\u7ed3\u5408V-JEPA\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u4fc3\u8fdb\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2510.04413", "pdf": "https://arxiv.org/pdf/2510.04413", "abs": "https://arxiv.org/abs/2510.04413", "authors": ["Muhammad Umar Farooq Qaisar", "Weijie Yuan", "Onur G\u00fcnl\u00fc", "Taneli Riihonen", "Yuanhao Cui", "Lin Zhang", "Nuria Gonzalez-Prelcic", "Marco Di Renzo", "Zhu Han"], "title": "The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems", "categories": ["eess.SP", "cs.NI"], "comment": "28 pages, 6 figures, and 5 tables", "summary": "The commencement of the sixth-generation (6G) wireless networks represents a\nfundamental shift in the integration of communication and sensing technologies\nto support next-generation applications. Integrated sensing and communication\n(ISAC) is a key concept in this evolution, enabling end-to-end support for both\ncommunication and sensing within a unified framework. It enhances spectrum\nefficiency, reduces latency, and supports diverse use cases, including smart\ncities, autonomous systems, and perceptive environments. This tutorial provides\na comprehensive overview of ISAC's role in 6G networks, beginning with its\nevolution since 5G and the technical drivers behind its adoption. Core\nprinciples and system variations of ISAC are introduced, followed by an\nin-depth discussion of the enabling technologies that facilitate its practical\ndeployment. The paper further analyzes current research directions to highlight\nkey challenges, open issues, and emerging trends. Design insights and\nrecommendations are also presented to support future development and\nimplementation. This work ultimately try to address three central questions:\nWhy is ISAC essential for 6G? What innovations does it bring? How will it shape\nthe future of wireless communication?", "AI": {"tldr": "\u672c\u6587\u6458\u8981\u4ecb\u7ecd\u4e86\u7b2c\u516d\u4ee3\uff086G\uff09\u65e0\u7ebf\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u6280\u672f\u9a71\u52a8\u3001\u6838\u5fc3\u539f\u7406\u3001\u7cfb\u7edf\u53d8\u5316\u3001\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u652f\u6301\u6280\u672f\uff0c\u4ee5\u53ca\u672a\u6765\u53d1\u5c55\u7684\u6311\u6218\u548c\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a766G\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u5fc5\u8981\u6027\u53ca\u5176\u5728\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\u548c\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0ISAC\u7684\u6f14\u53d8\u3001\u6280\u672f\u9a71\u52a8\u3001\u6838\u5fc3\u539f\u7406\u3001\u7cfb\u7edf\u53d8\u5316\u548c\u5b9e\u8df5\u6280\u672f\uff0c\u5206\u6790\u5f53\u524d\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86ISAC\u57286G\u4e2d\u7684\u5173\u952e\u6311\u6218\u3001\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "ISAC\u662f6G\u7f51\u7edc\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u521b\u65b0\u548c\u672a\u6765\u53d1\u5c55\u5c06\u5bf9\u65e0\u7ebf\u901a\u4fe1\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2510.03768", "pdf": "https://arxiv.org/pdf/2510.03768", "abs": "https://arxiv.org/abs/2510.03768", "authors": ["Aydin Ahmadi", "Baris Akgun"], "title": "Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics", "categories": ["cs.RO"], "comment": null, "summary": "Data-driven planar pushing methods have recently gained attention as they\nreduce manual engineering effort and improve generalization compared to\nanalytical approaches. However, most prior work targets narrow capabilities\n(e.g., side switching, precision, or single-task training), limiting broader\napplicability. We present a model-based framework for non-prehensile tabletop\npushing that uses a single learned model to address multiple tasks without\nretraining. Our approach employs a recurrent GRU-based architecture with\nadditional non-linear layers to capture object-environment dynamics while\nensuring stability. A tailored state-action representation enables the model to\ngeneralize across uncertain dynamics, variable push lengths, and diverse tasks.\nFor control, we integrate the learned dynamics with a sampling-based Model\nPredictive Path Integral (MPPI) controller, which generates adaptive,\ntask-oriented actions. This framework supports side switching, variable-length\npushes, and objectives such as precise positioning, trajectory following, and\nobstacle avoidance. Training is performed in simulation with domain\nrandomization to support sim-to-real transfer. We first evaluate the\narchitecture through ablation studies, showing improved prediction accuracy and\nstable rollouts. We then validate the full system in simulation and real-world\nexperiments using a Franka Panda robot with markerless tracking. Results\ndemonstrate high success rates in precise positioning under strict thresholds\nand strong performance in trajectory tracking and obstacle avoidance. Moreover,\nmultiple tasks are solved simply by changing the controller's objective\nfunction, without retraining. While our current focus is on a single object\ntype, we extend the framework by training on wider push lengths and designing a\nbalanced controller that reduces the number of steps for longer-horizon goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u975e\u6293\u53d6\u684c\u9762\u63a8\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u591a\u4efb\u52a1\uff0c\u7ed3\u5408\u91c7\u6837\u63a7\u5236\u63d0\u5347\u901a\u7528\u6027\u3002", "motivation": "\u51cf\u5c11\u624b\u5de5\u5de5\u7a0b\u8bbe\u8ba1\u5e76\u63d0\u5347\u901a\u7528\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528GRU\u5faa\u73af\u67b6\u6784\u548c\u975e\u7ebf\u6027\u5c42\u5efa\u6a21\u7269\u4f53-\u73af\u5883\u52a8\u6001\uff0c\u7ed3\u5408MPPI\u63a7\u5236\u5668\u751f\u6210\u81ea\u9002\u5e94\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u786e\u5b9a\u4f4d\u3001\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u7684\u6210\u529f\u7387\uff0c\u591a\u4efb\u52a1\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u6846\u67b6\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u7269\u4f53\u7c7b\u578b\u548c\u4efb\u52a1\u3002"}}
{"id": "2510.04492", "pdf": "https://arxiv.org/pdf/2510.04492", "abs": "https://arxiv.org/abs/2510.04492", "authors": ["Zhou Zhang", "Yizhu Wang", "Saman Atapattu", "Sumei Sun"], "title": "Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks", "categories": ["eess.SP"], "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u536b\u661f-\u5730\u9762\u6df7\u5408\u7f51\u7edc\u4e2d\u901a\u8fc7\u534f\u540c\u7f13\u5b58\u4f18\u5316\u541e\u5410\u91cf\u7684\u7b56\u7565\uff0c\u5229\u7528\u6700\u4f18\u505c\u6b62\u7406\u8bba\u52a8\u6001\u51b3\u7b56\u94fe\u8def\u63a2\u6d4b\u548c\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e26\u5bbd\u6709\u9650\u7684\u536b\u661f\u7cfb\u7edf\u4e2d\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4f18\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u540c\u7f13\u5b58\u5c06\u9ad8\u9891\u8bbf\u95ee\u5185\u5bb9\u5b58\u50a8\u5728\u79bb\u7528\u6237\u66f4\u8fd1\u7684\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u8054\u5408\u63a2\u6d4b\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5229\u7528LEO\u536b\u661f\u63a2\u6d4b\u94fe\u8def\u548c\u7f13\u5b58\u72b6\u6001\uff0c\u52a8\u6001\u8c03\u5ea6\u7528\u6237\u5185\u5bb9\u5206\u53d1\uff0c\u57fa\u4e8e\u6700\u4f18\u505c\u6b62\u7406\u8bba\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7b56\u7565\u901a\u8fc7\u534f\u540c\u7f13\u5b58\u548c\u52a8\u6001\u94fe\u8def\u63a2\u6d4b\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u536b\u661f-\u5730\u9762\u6df7\u5408\u7f51\u7edc\u7684\u541e\u5410\u91cf\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2510.03776", "pdf": "https://arxiv.org/pdf/2510.03776", "abs": "https://arxiv.org/abs/2510.03776", "authors": ["Tiago Rodrigues de Almeida", "Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Johannes A. Stork", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets", "categories": ["cs.RO", "cs.LG"], "comment": "This paper has been accepted to the IEEE Robotics and Automation\n  Letters journal and presented at the 40th Anniversary of the IEEE\n  International Conference on Robotics and Automation, which was held in\n  Rotterdam, Netherlands on 23-26 September, 2024", "summary": "Robots and other intelligent systems navigating in complex dynamic\nenvironments should predict future actions and intentions of surrounding agents\nto reach their goals efficiently and avoid collisions. The dynamics of those\nagents strongly depends on their tasks, roles, or observable labels.\nClass-conditioned motion prediction is thus an appealing way to reduce forecast\nuncertainty and get more accurate predictions for heterogeneous agents.\nHowever, this is hardly explored in the prior art, especially for mobile robots\nand in limited data applications. In this paper, we analyse different\nclass-conditioned trajectory prediction methods on two datasets. We propose a\nset of conditional pattern-based and efficient deep learning-based baselines,\nand evaluate their performance on robotics and outdoors datasets (TH\\\"OR-MAGNI\nand Stanford Drone Dataset). Our experiments show that all methods improve\naccuracy in most of the settings when considering class labels. More\nimportantly, we observe that there are significant differences when learning\nfrom imbalanced datasets, or in new environments where sufficient data is not\navailable. In particular, we find that deep learning methods perform better on\nbalanced datasets, but in applications with limited data, e.g., cold start of a\nrobot in a new environment, or imbalanced classes, pattern-based methods may be\npreferable.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u7c7b\u522b\u6761\u4ef6\u5316\u7684\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u548c\u5176\u4ed6\u667a\u80fd\u7cfb\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u9884\u6d4b\u5468\u56f4\u667a\u80fd\u4f53\u7684\u672a\u6765\u52a8\u4f5c\u548c\u610f\u56fe\u5bf9\u673a\u5668\u4eba\u9ad8\u6548\u5bfc\u822a\u548c\u907f\u514d\u78b0\u649e\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7c7b\u522b\u6761\u4ef6\u5316\u9884\u6d4b\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u51e0\u79cd\u7c7b\u522b\u6761\u4ef6\u5316\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u6761\u4ef6\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u5728TH\u00d6R-MAGNI\u548cStanford Drone\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8003\u8651\u7c7b\u522b\u6807\u7b7e\u65f6\uff0c\u6240\u6709\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5747\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u6216\u4e0d\u5e73\u8861\u65f6\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u5728\u6570\u636e\u6709\u9650\u6216\u4e0d\u5e73\u8861\u7684\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u53ef\u80fd\u6bd4\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u66f4\u9002\u7528\uff0c\u8fd9\u4e3a\u673a\u5668\u4eba\u5728\u65b0\u73af\u5883\u4e2d\u7684\u51b7\u542f\u52a8\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.04530", "pdf": "https://arxiv.org/pdf/2510.04530", "abs": "https://arxiv.org/abs/2510.04530", "authors": ["Gayathri Shekar", "Saman Atapattu", "Prathapasinghe Dharmawansa", "Kandeepan Sithamparanathan"], "title": "Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding", "categories": ["eess.SP"], "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Holographic MIMO (HMIMO) has emerged as a promising solution for future\nwireless systems by enabling ultra-dense, spatially continuous antenna\ndeployments. While prior studies have primarily focused on electromagnetic (EM)\nmodeling or simulation-based performance analysis, a rigorous\ncommunication-theoretic framework remains largely unexplored. This paper\npresents the first analytical performance study of a multi-user HMIMO downlink\nsystem with matched filter (MF) precoding - a low-complexity baseline scheme.\nBy incorporating multipath propagation, mutual coupling, and element\nexcitation, we derive a novel closed-form expression for the MF\nsignal-to-interference-plus-noise ratio (SINR) using an equivalent random\nvariable model. Leveraging bivariate gamma distributions, we then develop\ntractable throughput approximations under full, partial, and no channel state\ninformation (CSI) scenarios. Additionally, we formulate a max-min beamforming\nproblem to benchmark optimal user fairness performance. Numerical results\nvalidate the accuracy of the proposed framework and reveal that MF precoding\nachieves competitive performance with strong robustness to low SINR and CSI\nuncertainty.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u591a\u7528\u6237HMIMO\u4e0b\u884c\u94fe\u8def\u7cfb\u7edf\u8fdb\u884c\u4e86\u5206\u6790\u6027\u80fd\u7814\u7a76\uff0c\u91c7\u7528\u4e86\u4f4e\u590d\u6742\u5ea6\u7684MF\u9884\u7f16\u7801\uff0c\u5e76\u63a8\u5bfc\u4e86\u65b0\u578b\u95ed\u5f0f\u8868\u8fbe\u5f0f\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u7814\u7a76HMIMO\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u7406\u8bba\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5728\u7535\u78c1\u5efa\u6a21\u548c\u4eff\u771f\u5206\u6790\u4e4b\u5916\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u591a\u5f84\u4f20\u64ad\u3001\u4e92\u8026\u548c\u5143\u4ef6\u6fc0\u52b1\uff0c\u5229\u7528\u7b49\u6548\u968f\u673a\u53d8\u91cf\u6a21\u578b\u63a8\u5bfcMF\u7684SINR\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u53cc\u53d8\u91cf\u4f3d\u9a6c\u5206\u5e03\u7684\u541e\u5410\u91cf\u8fd1\u4f3c\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cMF\u9884\u7f16\u7801\u5728\u4f4eSINR\u548cCSI\u4e0d\u786e\u5b9a\u6027\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u4e14\u6027\u80fd\u4f18\u5f02\u3002", "conclusion": "MF\u9884\u7f16\u7801\u5728HMIMO\u7cfb\u7edf\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.03875", "pdf": "https://arxiv.org/pdf/2510.03875", "abs": "https://arxiv.org/abs/2510.03875", "authors": ["Niranjan Kumar Ilampooranan", "Constantinos Chamzas"], "title": "COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments", "categories": ["cs.RO"], "comment": null, "summary": "Having the ability to answer motion-planning queries within a fixed time\nbudget is critical for the widespread deployment of robotic systems.\nSemi-static environments, where most obstacles remain static but a limited set\ncan vary across queries, exhibit structured variability that can be\nsystematically exploited to provide stronger guarantees than in general\nmotion-planning problems. However, prior approaches in this setting either lack\nformal guarantees or rely on restrictive discretizations of obstacle\nconfigurations, limiting their applicability in realistic domains. This paper\nintroduces COVER, a novel framework that incrementally constructs a\ncoverage-verified roadmap in semi-static environments. By partitioning the\nobstacle configuration space and solving for feasible paths within each\npartition, COVER systematically verifies feasibility of the roadmap in each\npartition and guarantees fixed-time motion planning queries within the verified\nregions. We validate COVER with a 7-DOF simulated Panda robot performing table\nand shelf tasks, demonstrating that COVER achieves broader coverage with higher\nquery success rates than prior works.", "AI": {"tldr": "COVER\u662f\u4e00\u79cd\u5728\u534a\u9759\u6001\u73af\u5883\u4e2d\u589e\u91cf\u6784\u5efa\u8986\u76d6\u7387\u9a8c\u8bc1\u8def\u7ebf\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u533a\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u786e\u4fdd\u56fa\u5b9a\u65f6\u95f4\u5185\u5b8c\u6210\u8fd0\u52a8\u89c4\u5212\u67e5\u8be2\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u534a\u9759\u6001\u73af\u5883\u4e2d\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\u6216\u4f9d\u8d56\u9650\u5236\u6027\u79bb\u6563\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "COVER\u901a\u8fc7\u5206\u533a\u969c\u788d\u7269\u914d\u7f6e\u7a7a\u95f4\u5e76\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u6c42\u89e3\u53ef\u884c\u8def\u5f84\uff0c\u7cfb\u7edf\u6027\u9a8c\u8bc1\u8def\u7ebf\u56fe\u7684\u53ef\u884c\u6027\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u6a21\u62dfPanda\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cCOVER\u6bd4\u73b0\u6709\u65b9\u6cd5\u8986\u76d6\u66f4\u5e7f\u4e14\u67e5\u8be2\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "COVER\u4e3a\u534a\u9759\u6001\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fa\u5b9a\u65f6\u95f4\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04600", "pdf": "https://arxiv.org/pdf/2510.04600", "abs": "https://arxiv.org/abs/2510.04600", "authors": ["Meidong Xia", "Zhenyao He", "Wei Xu", "Yongming Huang", "Derrick Wing Kwan Ng", "Naofal Al-Dhahir"], "title": "Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Networked integrated sensing and communication (ISAC) has gained significant\nattention as a promising technology for enabling next-generation wireless\nsystems. To further enhance networked ISAC, delegating the reception of sensing\nsignals to dedicated target monitoring terminals (TMTs) instead of base\nstations (BSs) offers significant advantages in terms of sensing capability and\ndeployment flexibility. Despite its potential, the coordinated beamforming\ndesign for networked integrated communication and time-of-arrival (ToA)-based\nmulti-TMT localization remains largely unexplored. In this paper, we present a\ncomprehensive study to fill this gap. Specifically, we first establish signal\nmodels for both communication and localization, and, for the first time, derive\na closed-form Cram\\'er-Rao lower bound (CRLB) to characterize the localization\nperformance. Subsequently, we exploit this CRLB to formulate two optimization\nproblems, focusing on sensing-centric and communication-centric criteria,\nrespectively. For the sensing-centric problem, we develop a globally optimal\nalgorithm based on semidefinite relaxation (SDR) when each BS is equipped with\nmore antennas than the total number of communication users. While for the\ncommunication-centric problem, we design a globally optimal algorithm for the\nsingle-BS case using bisection search. For the general case of both problems,\nwe propose a unified successive convex approximation (SCA)-based algorithm,\nwhich is suboptimal yet efficient, and further extend it from single-target\nscenarios to more practical multi-target scenarios. Finally, simulation results\ndemonstrate the effectiveness of our proposed algorithms, reveal the intrinsic\nperformance trade-offs between communication and localization, and further show\nthat deploying more TMTs is always preferable to deploying more BSs in\nnetworked ISAC systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7f51\u7edc\u5316\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u57fa\u4e8e\u5230\u8fbe\u65f6\u95f4\uff08ToA\uff09\u7684\u591a\u76ee\u6807\u76d1\u6d4b\u7ec8\u7aef\uff08TMT\uff09\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u5b9a\u4f4d\u7684\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1\u7f51\u7edc\u5316ISAC\u6280\u672f\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u591aTMT\u5b9a\u4f4d\u4e0e\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u65b9\u9762\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u8bba\u6587\u9996\u5148\u5efa\u7acb\u4e86\u901a\u4fe1\u4e0e\u5b9a\u4f4d\u7684\u4fe1\u53f7\u6a21\u578b\uff0c\u63a8\u5bfc\u4e86\u5b9a\u4f4d\u6027\u80fd\u7684\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c\uff08CRLB\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u611f\u77e5\u4e2d\u5fc3\u4e0e\u901a\u4fe1\u4e2d\u5fc3\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5305\u62ecSDR\u548cSCA\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\uff0c\u5e76\u63ed\u793a\u4e86\u90e8\u7f72\u66f4\u591aTMT\u6bd4\u90e8\u7f72\u66f4\u591a\u57fa\u7ad9\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u7f51\u7edc\u5316ISAC\u5728\u591aTMT\u5b9a\u4f4d\u4e0e\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03885", "pdf": "https://arxiv.org/pdf/2510.03885", "abs": "https://arxiv.org/abs/2510.03885", "authors": ["Sunghwan Kim", "Woojeh Chung", "Zhirui Dai", "Dwait Bhatt", "Arth Shukla", "Hao Su", "Yulun Tian", "Nikolay Atanasov"], "title": "Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning", "categories": ["cs.RO"], "comment": "Project website can be found at\n  https://existentialrobotics.org/sbp_page/", "summary": "In this paper, we demonstrate that mobile manipulation policies utilizing a\n3D latent map achieve stronger spatial and temporal reasoning than policies\nrelying solely on images. We introduce Seeing the Bigger Picture (SBP), an\nend-to-end policy learning approach that operates directly on a 3D map of\nlatent features. In SBP, the map extends perception beyond the robot's current\nfield of view and aggregates observations over long horizons. Our mapping\napproach incrementally fuses multiview observations into a grid of\nscene-specific latent features. A pre-trained, scene-agnostic decoder\nreconstructs target embeddings from these features and enables online\noptimization of the map features during task execution. A policy, trainable\nwith behavior cloning or reinforcement learning, treats the latent map as a\nstate variable and uses global context from the map obtained via a 3D feature\naggregator. We evaluate SBP on scene-level mobile manipulation and sequential\ntabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons\nglobally over the scene, (ii) leverages the map as long-horizon memory, and\n(iii) outperforms image-based policies in both in-distribution and novel\nscenes, e.g., improving the success rate by 25% for the sequential manipulation\ntask.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6f5c\u5728\u5730\u56fe\u7684\u79fb\u52a8\u64cd\u4f5c\u7b56\u7565SBP\uff0c\u5176\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4e0a\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5f53\u524d\u89c6\u91ce\u7684\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u5168\u5c40\u611f\u77e5\u548c\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "SBP\u901a\u8fc7\u7aef\u5230\u7aef\u7b56\u7565\u5b66\u4e60\uff0c\u5229\u75283D\u6f5c\u5728\u5730\u56fe\u805a\u5408\u591a\u89c6\u89d2\u89c2\u5bdf\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89e3\u7801\u5668\u548c3D\u7279\u5f81\u805a\u5408\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSBP\u5728\u5168\u5c40\u63a8\u7406\u548c\u957f\u671f\u8bb0\u5fc6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u9ad825%\u3002", "conclusion": "SBP\u8bc1\u660e\u4e863D\u6f5c\u5728\u5730\u56fe\u5728\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.04734", "pdf": "https://arxiv.org/pdf/2510.04734", "abs": "https://arxiv.org/abs/2510.04734", "authors": ["Juan Vidal Alegr\u00eda"], "title": "Dimensionally-Efficient Transmission and Storage of Unitary Matrices", "categories": ["eess.SP"], "comment": "13 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Unitary matrices are the basis of a large number of signal processing\napplications. In many of these applications, finding ways to efficiently store,\nand even transmit these matrices, can significantly reduce memory and\nthroughput requirements. In this work, we study the problem of efficient\ntransmission and storage of unitary matrices. Specifically, we explicitly\nderive a dimensionally-efficient parametrization (DEP) for unitary matrices\nthat allows identifying them with sequences of real numbers, where the\ndimension coincides with the dimension of the unitary group where they lie. We\nalso characterize its inverse map that allows retrieving the original unitary\nmatrices from their DEP. The proposed approach effectively allows halving the\ndimension with respect to naively considering all the entries of each unitary\nmatrix, thus reducing the resources required to store and transmit these\nmatrices. Furthermore, we show that the sequence of real numbers associated to\nthe proposed DEP is bounded, and we delimit the interval where these numbers\nare contained, facilitating the implementation of quantization approaches with\nlimited distortion. On the other hand, we outline ways to further reduce the\ndimension of the DEP when considering more restrictive constraints for matrices\nthat show up in certain applications. The numerical results showcase the\npotential of the proposed approach in general settings, as well as in three\nspecific applications of current interest for wireless communications research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ef4\u5ea6\u9ad8\u6548\u53c2\u6570\u5316\uff08DEP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5b58\u50a8\u548c\u4f20\u8f93\u9149\u77e9\u9635\uff0c\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u9149\u77e9\u9635\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5b58\u50a8\u548c\u4f20\u8f93\u9700\u8981\u9ad8\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u5904\u7406\u6b64\u7c7b\u77e9\u9635\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u63a8\u5bfcDEP\u65b9\u6cd5\uff0c\u5c06\u9149\u77e9\u9635\u6620\u5c04\u4e3a\u5b9e\u6570\u5e8f\u5217\uff0c\u7ef4\u5ea6\u4e0e\u9149\u7fa4\u4e00\u81f4\uff0c\u5e76\u5b9a\u4e49\u9006\u6620\u5c04\u4ee5\u6062\u590d\u539f\u77e9\u9635\u3002", "result": "DEP\u65b9\u6cd5\u8d44\u6e90\u9700\u6c42\u51cf\u534a\uff0c\u5b9e\u6570\u5e8f\u5217\u6709\u754c\u4e14\u6613\u4e8e\u91cf\u5316\uff0c\u6570\u503c\u7ed3\u679c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "DEP\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u9149\u77e9\u9635\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.03895", "pdf": "https://arxiv.org/pdf/2510.03895", "abs": "https://arxiv.org/abs/2510.03895", "authors": ["Zheng Huang", "Mingyu Liu", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Xiaoman Li", "Yiduo Jia", "Hao Zhong", "Hao Chen", "Chunhua Shen"], "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied\nintelligence, yet they confront critical barriers to real-world deployment,\nmost notably catastrophic forgetting. This issue stems from their overreliance\non continuous action sequences or action chunks, which inadvertently create\nisolated data silos that disrupt knowledge retention across tasks. To tackle\nthese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)\nframework: a novel approach that narrows its focus to sparse trajectories,\nthereby avoiding the catastrophic forgetting associated with dense trajectory\nfine-tuning. A key innovation of NoTVLA lies in its trajectory planning\nstrategy: instead of centering on the target object's trajectory, it leverages\ntemporal compression and spatial reasoning pruning specifically for the robot\nend effector's trajectory. Furthermore, training is conducted using these\nsparse trajectories rather than dense action trajectories, an optimization that\ndelivers remarkable practical advantages with better performance in zero-shot.\nIn multi-task evaluation scenarios, NoTVLA achieves superior performance and\ngeneralization compared to pi0 while operating under two critical constraints:\nit uses over an order of magnitude less computing power than pi0 and requires\nno wrist-mounted camera. This design ensures that NoTVLA's operational accuracy\nclosely approximates that of single-task expert models. Crucially, it also\npreserves the model's inherent language capabilities, enabling zero-shot\ngeneralization in specific scenarios, supporting unified model deployment\nacross multiple robot platforms, and fostering a degree of generalization even\nwhen perceiving tasks from novel perspectives.", "AI": {"tldr": "NoTVLA\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u8f68\u8ff9\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u4e0e\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u56e0\u4f9d\u8d56\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u9645\u90e8\u7f72\u6548\u679c\u3002", "method": "\u91c7\u7528\u7a00\u758f\u8f68\u8ff9\u8bad\u7ec3\uff0c\u7ed3\u5408\u65f6\u95f4\u538b\u7f29\u4e0e\u7a7a\u95f4\u63a8\u7406\u526a\u679d\uff0c\u4f18\u5316\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u57fa\u7ebfpi0\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u4e14\u65e0\u9700\u989d\u5916\u6444\u50cf\u5934\uff0c\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "NoTVLA\u5728\u6027\u80fd\u3001\u6cdb\u5316\u4e0e\u8d44\u6e90\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u5408\u591a\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2510.04744", "pdf": "https://arxiv.org/pdf/2510.04744", "abs": "https://arxiv.org/abs/2510.04744", "authors": ["Wali Ullah Khan", "Chandan Kumar Sheemar", "Eva Lagunas", "Xingwang Li", "Symeon Chatzinotas", "Petar Popovski", "Zhu Han"], "title": "Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS", "categories": ["eess.SP"], "comment": "13, 10", "summary": "In this work, we study a multi-user NTN in which a satellite serves as the\nprimary network and a high-altitude platform station (HAPS) operates as the\nsecondary network, acting as a cognitive radio. To reduce the cost, complexity,\nand power consumption of conventional antenna arrays, we equip the HAPS with a\ntransmissive BD-RIS antenna front end. We then formulate a joint optimization\nproblem for the BD-RIS phase response and the HAPS transmit power allocation\nunder strict per-user interference temperature constraints. To tackle the\nresulting highly nonconvex problem, we propose an alternating-optimization\nframework: the power-allocation subproblem admits a closed-form,\nwater-filling-type solution derived from the Karush-Kuhn-Tucker (KKT)\nconditions, while the BD-RIS configuration is refined via Riemannian manifold\noptimization. Simulation results show significant gains in data rate and\ninterference suppression over diagonal RIS-assisted benchmarks, establishing\nBD-RIS as a promising enabler for future multilayer NTNs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u7528\u6237NTN\u4e2d\u536b\u661f\u4f5c\u4e3a\u4e3b\u7f51\u7edc\u3001HAPS\u4f5c\u4e3a\u6b21\u7f51\u7edc\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535\u7cfb\u7edf\uff0c\u91c7\u7528BD-RIS\u5929\u7ebf\u524d\u7aef\u4f18\u5316\u76f8\u4f4d\u54cd\u5e94\u548c\u529f\u7387\u5206\u914d\uff0c\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u4f20\u8f93\u7387\u548c\u5e72\u6270\u6291\u5236\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u5929\u7ebf\u9635\u5217\u7684\u6210\u672c\u3001\u590d\u6742\u6027\u548c\u529f\u8017\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u591a\u5c42NTN\u4e2d\u7684\u5e72\u6270\u63a7\u5236\u3002", "method": "\u91c7\u7528BD-RIS\u5929\u7ebf\u524d\u7aef\uff0c\u7ed3\u5408\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff08\u529f\u7387\u5206\u914d\u57fa\u4e8eKKT\u6761\u4ef6\u95ed\u5f0f\u89e3\uff0cBD-RIS\u914d\u7f6e\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5bf9\u89d2RIS\u8f85\u52a9\u57fa\u51c6\uff0c\u6570\u636e\u4f20\u8f93\u7387\u548c\u5e72\u6270\u6291\u5236\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "BD-RIS\u662f\u672a\u6765\u591a\u5c42NTN\u7684\u6709\u529b\u652f\u6301\u6280\u672f\u3002"}}
{"id": "2510.03910", "pdf": "https://arxiv.org/pdf/2510.03910", "abs": "https://arxiv.org/abs/2510.03910", "authors": ["Akhil Padmanabha", "Jessie Yuan", "Tanisha Mehta", "Rajat Kumar Jenamani", "Eric Hu", "Victoria de Le\u00f3n", "Anthony Wertz", "Janavi Gupta", "Ben Dodson", "Yunting Yan", "Carmel Majidi", "Tapomayukh Bhattacharjee", "Zackory Erickson"], "title": "WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding", "categories": ["cs.RO"], "comment": null, "summary": "Millions of people around the world need assistance with feeding. Robotic\nfeeding systems offer the potential to enhance autonomy and quality of life for\nindividuals with impairments and reduce caregiver workload. However, their\nwidespread adoption has been limited by technical challenges such as estimating\nbite timing, the appropriate moment for the robot to transfer food to a user's\nmouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with\nLEarned bite timing, a system that accurately predicts bite timing by\nleveraging wearable sensor data to be highly reactive to natural user cues such\nas head movements, chewing, and talking. We train a supervised regression model\non bite timing data from 14 participants and incorporate a user-adjustable\nassertiveness threshold to convert predictions into proceed or stop commands.\nIn a study with 15 participants without motor impairments with the Obi feeding\nrobot, WAFFLE performs statistically on par with or better than baseline\nmethods across measures of feeling of control, robot understanding, and\nworkload, and is preferred by the majority of participants for both individual\nand social dining. We further demonstrate WAFFLE's generalizability in a study\nwith 2 participants with motor impairments in their home environments using a\nKinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling\nnatural, reactive bite timing that generalizes across users, robot hardware,\nrobot positioning, feeding trajectories, foods, and both individual and social\ndining contexts.", "AI": {"tldr": "WAFFLE\u662f\u4e00\u79cd\u57fa\u4e8e\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u5582\u98df\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u884c\u4e3a\uff08\u5982\u5934\u90e8\u52a8\u4f5c\u3001\u5480\u56bc\u548c\u8bf4\u8bdd\uff09\u9884\u6d4b\u54ac\u5408\u65f6\u673a\uff0c\u63d0\u5347\u5582\u98df\u673a\u5668\u4eba\u7684\u53cd\u5e94\u901f\u5ea6\u548c\u4f7f\u7528\u4f53\u9a8c\u3002", "motivation": "\u4e3a\u884c\u52a8\u4e0d\u4fbf\u8005\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u5582\u98df\u8f85\u52a9\u6280\u672f\uff0c\u51cf\u5c11\u62a4\u7406\u8d1f\u62c5\uff0c\u63d0\u5347\u7528\u6237\u81ea\u4e3b\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u5229\u7528\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u8bad\u7ec3\u76d1\u7763\u56de\u5f52\u6a21\u578b\uff0c\u7ed3\u5408\u7528\u6237\u53ef\u8c03\u7684\u9608\u503c\u751f\u6210\u63a7\u5236\u6307\u4ee4\u3002", "result": "\u572815\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0cWAFFLE\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u88ab\u591a\u6570\u7528\u6237\u504f\u597d\uff1b\u57282\u540d\u8fd0\u52a8\u969c\u788d\u8005\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u666e\u9002\u6027\u3002", "conclusion": "WAFFLE\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u884c\u4e3a\u5b9e\u73b0\u81ea\u7136\u7684\u54ac\u5408\u65f6\u673a\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7528\u6237\u3001\u673a\u5668\u4eba\u548c\u573a\u666f\u3002"}}
{"id": "2510.04745", "pdf": "https://arxiv.org/pdf/2510.04745", "abs": "https://arxiv.org/abs/2510.04745", "authors": ["Lucas Semp\u00e9r\u00e9", "Yue Bi", "Yue Wu", "Pengwenlong Gu", "Selma Boumerdassi"], "title": "Interference Alignment for Multi-cluster Over-the-Air Computation", "categories": ["eess.SP"], "comment": null, "summary": "One of the main challenges facing Internet of Things (IoT) networks is\nmanaging interference caused by the large number of devices communicating\nsimultaneously, particularly in multi-cluster networks where multiple devices\nsimultaneously transmit to their respective receiver. Over-the-Air Computation\n(AirComp) has emerged as a promising solution for efficient real-time data\naggregation, yet its performance suffers in dense, interference-limited\nenvironments. To address this, we propose a novel Interference Alignment (IA)\nscheme tailored for up-link AirComp systems. Unlike previous approaches, the\nproposed method scales to an arbitrary number $\\sf K$ of clusters and enables\neach cluster to exploit half of the available channels, instead of only\n$\\tfrac{1}{\\sf K}$ as in time-sharing. In addition, we develop schemes tailored\nto scenarios where users are shared between adjacent clusters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5e72\u6270\u5bf9\u9f50\u65b9\u6848\uff0c\u7528\u4e8e\u4e0a\u884c\u94fe\u8defAirComp\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u591a\u96c6\u7fa4IoT\u7f51\u7edc\u4e2d\u5e72\u6270\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u4fe1\u9053\u5229\u7528\u7387\u3002", "motivation": "\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u591a\u8bbe\u5907\u540c\u65f6\u901a\u4fe1\u5bfc\u81f4\u7684\u5e72\u6270\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u96c6\u7fa4\u7f51\u7edc\u4e2d\uff0c\u4f20\u7edf\u7684AirComp\u5728\u5bc6\u96c6\u5e72\u6270\u73af\u5883\u4e0b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5e72\u6270\u5bf9\u9f50\uff08IA\uff09\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0a\u884c\u94fe\u8defAirComp\u7cfb\u7edf\uff0c\u80fd\u591f\u6269\u5c55\u5230\u4efb\u610f\u6570\u91cf\u7684\u96c6\u7fa4\uff0c\u5e76\u5c06\u4fe1\u9053\u5229\u7528\u7387\u63d0\u9ad8\u5230\u53ef\u7528\u4fe1\u9053\u7684\u4e00\u534a\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u96c6\u7fa4\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5171\u4eab\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u76f8\u90bb\u96c6\u7fa4\u95f4\u5171\u4eab\u7528\u6237\u7684\u573a\u666f\u4e2d\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3IoT\u7f51\u7edc\u4e2d\u7684\u5e72\u6270\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86AirComp\u5728\u5bc6\u96c6\u5e72\u6270\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03919", "pdf": "https://arxiv.org/pdf/2510.03919", "abs": "https://arxiv.org/abs/2510.03919", "authors": ["Matthew Lisondra", "Junseo Kim", "Glenn Takashi Shimoda", "Kourosh Zareinia", "Sajad Saeedi"], "title": "TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry", "categories": ["cs.RO"], "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Vision algorithms can be executed directly on the image sensor when\nimplemented on the next-generation sensors known as focal-plane\nsensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs\ngreatly improve latency, reducing the problems associated with the bottleneck\nof data transfer from a vision sensor to a processor. FPSPs accelerate\nvision-based algorithms such as visual-inertial odometry (VIO). However, VIO\nframeworks suffer from spatial drift due to the vision-based pose estimation,\nwhilst temporal drift arises from the inertial measurements. FPSPs circumvent\nthe spatial drift by operating at a high frame rate to match the high-frequency\noutput of the inertial measurements. In this paper, we present TCB-VIO, a\ntightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman\nFilter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU\nmeasurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:\nROVIO, VINS-Mono, and ORB-SLAM3.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPSP\u7684TCB-VIO\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86VIO\u6846\u67b6\u4e2d\u7684\u65f6\u7a7a\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u9891\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u6846\u67b6\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5229\u7528FPSP\u7684\u9ad8\u5e27\u7387\u4f18\u52bf\uff0c\u672c\u6587\u63d0\u51fa\u4e86TCB-VIO\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u72b6\u6001\u7ea6\u675f\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08MSCKF\uff09\uff0c\u5728\u9ad8\u5e27\u7387\uff08250 FPS\uff09\u548cIMU\u9ad8\u9891\u6d4b\u91cf\uff08400 Hz\uff09\u4e0b\u5b9e\u73b0\u7d27\u5bc6\u8026\u5408\u76846\u81ea\u7531\u5ea6VIO\u3002", "result": "TCB-VIO\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eROVIO\u3001VINS-Mono\u548cORB-SLAM3\u7b49\u6700\u5148\u8fdb\u7684VIO\u65b9\u6cd5\u3002", "conclusion": "TCB-VIO\u901a\u8fc7FPSP\u7684\u9ad8\u5e27\u7387\u7279\u6027\u6709\u6548\u51cf\u5c11\u4e86\u65f6\u7a7a\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04913", "pdf": "https://arxiv.org/pdf/2510.04913", "abs": "https://arxiv.org/abs/2510.04913", "authors": ["Andreas Bathelt", "Benjamin Deutschmann", "Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Peter Vouras"], "title": "The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II", "categories": ["eess.SP"], "comment": "Submitted to the IEEE for possible publication", "summary": "In every imaging or sensing application, the physical hardware creates\nconstraints that must be overcome or they limit system performance. Techniques\nthat leverage additional degrees of freedom can effectively extend performance\nbeyond the inherent physical capabilities of the hardware. An example includes\nsynchronizing distributed sensors so as to synthesize a larger aperture for\nremote sensing applications. An additional example is integrating the\ncommunication and sensing functions in a wireless system through the clever\ndesign of waveforms and optimized resource management. As these technologies\nmature beyond the conceptual and prototype phase they will ultimately\ntransition to the commercial market. Here, standards play a critical role in\nensuring success. Standards ensure interoperability between systems\nmanufactured by different vendors and define industry best practices for\nvendors and customers alike. The Signal Processing Society of the Institute for\nElectrical and Electronics Engineers (IEEE) plays a leading role in developing\nhigh-quality standards for computational sensing technologies through the\nworking groups of the Synthetic Aperture Standards Committee (SASC). In this\ncolumn we highlight the standards activities of the P3383 Performance Metrics\nfor Integrated Sensing and Communication (ISAC) Systems Working Group and the\nP3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed\nSensors Working Group.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u81ea\u7531\u5ea6\u6765\u7a81\u7834\u786c\u4ef6\u9650\u5236\uff0c\u63d0\u5347\u6210\u50cf\u548c\u611f\u77e5\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03IEEE\u4fe1\u53f7\u5904\u7406\u534f\u4f1a\u5728\u76f8\u5173\u6807\u51c6\u5236\u5b9a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u89e3\u51b3\u786c\u4ef6\u9650\u5236\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63a8\u52a8\u96c6\u6210\u611f\u77e5\u548c\u901a\u4fe1\u6280\u672f\u7684\u5546\u4e1a\u5316\u3002", "method": "\u5229\u7528\u989d\u5916\u7684\u81ea\u7531\u5ea6\uff08\u5982\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u540c\u6b65\u3001\u6ce2\u5f62\u8bbe\u8ba1\u548c\u8d44\u6e90\u7ba1\u7406\uff09\u6269\u5c55\u786c\u4ef6\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u5e76\u63a8\u8fdb\u4e86\u4e24\u4e2a\u5de5\u4f5c\u7ec4\u7684\u6807\u51c6\u5236\u5b9a\uff08P3383\u548cP3343\uff09\uff0c\u786e\u4fdd\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u548c\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "IEEE\u4fe1\u53f7\u5904\u7406\u534f\u4f1a\u5728\u6807\u51c6\u5236\u5b9a\u4e2d\u53d1\u6325\u6838\u5fc3\u4f5c\u7528\uff0c\u63a8\u52a8\u6280\u672f\u4ece\u6982\u5ff5\u5230\u5e02\u573a\u7684\u6210\u529f\u8fc7\u6e21\u3002"}}
{"id": "2510.03948", "pdf": "https://arxiv.org/pdf/2510.03948", "abs": "https://arxiv.org/abs/2510.03948", "authors": ["Otobong Jerome", "Geesara Prathap Kulathunga", "Devitt Dmitry", "Eugene Murawjow", "Alexandr Klimchik"], "title": "A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Off-road environments present unique challenges for autonomous navigation due\nto their complex and unstructured nature. Traditional global path-planning\nmethods, which typically aim to minimize path length and travel time, perform\npoorly on large-scale maps and fail to account for critical factors such as\nreal-time performance, kinematic feasibility, and memory efficiency. This paper\nintroduces a novel global path-planning method specifically designed for\noff-road environments, addressing these essential factors. The method begins by\nconstructing an intermediate map within the pixel coordinate system,\nincorporating geographical features like off-road trails, waterways, restricted\nand passable areas, and trees. The planning problem is then divided into three\nsub-problems: graph-based path planning, kinematic feasibility checking, and\npath smoothing. This approach effectively meets real-time performance\nrequirements while ensuring kinematic feasibility and efficient memory use. The\nmethod was tested in various off-road environments with large-scale maps up to\nseveral square kilometers in size, successfully identifying feasible paths in\nan average of 1.5 seconds and utilizing approximately 1.5GB of memory under\nextreme conditions. The proposed framework is versatile and applicable to a\nwide range of off-road autonomous navigation tasks, including search and rescue\nmissions and agricultural operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u590d\u6742\u8d8a\u91ce\u73af\u5883\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u8d8a\u91ce\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u975e\u7ed3\u6784\u5316\u7279\u6027\u4f7f\u5f97\u4f20\u7edf\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u517c\u987e\u5b9e\u65f6\u6027\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u50cf\u7d20\u5750\u6807\u7cfb\u4e0b\u7684\u4e2d\u95f4\u5730\u56fe\uff0c\u6574\u5408\u5730\u7406\u7279\u5f81\uff0c\u5e76\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u57fa\u4e8e\u56fe\u7684\u8def\u5f84\u89c4\u5212\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u68c0\u67e5\u548c\u8def\u5f84\u5e73\u6ed1\u4e09\u4e2a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5730\u56fe\uff08\u6570\u5e73\u65b9\u516c\u91cc\uff09\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u57471.5\u79d2\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u6781\u7aef\u6761\u4ef6\u4e0b\u4ec5\u5360\u7528\u7ea61.5GB\u5185\u5b58\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u591a\u79cd\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\uff0c\u5982\u641c\u6551\u548c\u519c\u4e1a\u64cd\u4f5c\u3002"}}
{"id": "2510.04924", "pdf": "https://arxiv.org/pdf/2510.04924", "abs": "https://arxiv.org/abs/2510.04924", "authors": ["Ardavan Rahimian"], "title": "Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "We study how far a diffusion process on a graph can drift from a designed\nstarting pattern when that pattern is produced using Laplacian regularisation.\nUnder standard stability conditions for undirected, entrywise nonnegative\ngraphs, we give a closed-form, instance-specific upper bound on the\nsteady-state spread, measured as the relative change between the final and\ninitial profiles. The bound separates two effects: (i) an irreducible term\ndetermined by the graph's maximum node degree, and (ii) a design-controlled\nterm that shrinks as the regularisation strength increases (following an\ninverse square-root law). This leads to a simple design rule: given any target\nlimit on spread, one can choose a sufficient regularisation strength in closed\nform. Although one motivating application is array beamforming, where the\ninitial pattern is the squared magnitude of the beamformer weights, the result\napplies to any scenario that first enforces Laplacian smoothness and then\nevolves by linear diffusion on a graph. Overall, the guarantee is\nnon-asymptotic, easy to compute, and certifies how much steady-state deviation\ncan occur.", "AI": {"tldr": "\u7814\u7a76\u4e86\u56fe\u6269\u6563\u8fc7\u7a0b\u5728\u8bbe\u8ba1\u8d77\u59cb\u6a21\u5f0f\u4e0b\u80fd\u504f\u79bb\u591a\u8fdc\uff0c\u7ed9\u51fa\u4e86\u7a33\u6001\u6269\u6563\u7684\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8bbe\u8ba1\u89c4\u5219\u3002", "motivation": "\u8bd5\u56fe\u91cf\u5316\u56fe\u6269\u6563\u8fc7\u7a0b\u4e2d\u8bbe\u8ba1\u8d77\u59cb\u6a21\u5f0f\u7684\u7a33\u6001\u504f\u79bb\u7a0b\u5ea6\uff0c\u4e3a\u5e94\u7528\uff08\u5982\u9635\u5217\u6ce2\u675f\u6210\u5f62\uff09\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5728\u975e\u8d1f\u65e0\u5411\u56fe\u7684\u7a33\u5b9a\u6027\u6761\u4ef6\u4e0b\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u63a8\u5bfc\u51fa\u7a33\u6001\u6269\u6563\u7684\u95ed\u5f0f\u4e0a\u754c\uff0c\u5206\u79bb\u4e86\u8282\u70b9\u6700\u5927\u5ea6\u548c\u8bbe\u8ba1\u63a7\u5236\u9879\u7684\u5f71\u54cd\u3002", "result": "\u7ed9\u51fa\u4e86\u7a33\u6001\u6269\u6563\u7684\u76f8\u5bf9\u53d8\u5316\u4e0a\u754c\uff0c\u8868\u660e\u6b63\u5219\u5316\u5f3a\u5ea6\u589e\u52a0\u65f6\u8bbe\u8ba1\u63a7\u5236\u9879\u6309\u53cd\u5e73\u65b9\u6839\u89c4\u5f8b\u51cf\u5c0f\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u89c4\u5219\u3002", "conclusion": "\u7814\u7a76\u4e3a\u975e\u6e10\u8fd1\u3001\u6613\u8ba1\u7b97\u7684\u7a33\u6001\u504f\u79bb\u63d0\u4f9b\u4e86\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u62c9\u666e\u62c9\u65af\u5e73\u6ed1\u548c\u7ebf\u6027\u6269\u6563\u7684\u573a\u666f\u3002"}}
{"id": "2510.04041", "pdf": "https://arxiv.org/pdf/2510.04041", "abs": "https://arxiv.org/abs/2510.04041", "authors": ["Ayudh Saxena", "Harsh Shah", "Sandeep Routray", "Rishi Rajesh Shah", "Esha Pahwa"], "title": "SITCOM: Scaling Inference-Time COMpute for VLAs", "categories": ["cs.RO"], "comment": "Accepted at the NeurIPS 2025 Workshop on Space in Vision, Language,\n  and Embodied AI (SpaVLE). *Equal contribution", "summary": "Learning robust robotic control policies remains a major challenge due to the\nhigh cost of collecting labeled data, limited generalization to unseen\nenvironments, and difficulties in planning over long horizons. While\nVision-Language-Action (VLA) models offer a promising solution by grounding\nnatural language instructions into single-step control commands, they often\nlack mechanisms for lookahead and struggle with compounding errors in dynamic\ntasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs\n(SITCOM), a framework that augments any pretrained VLA with model-based\nrollouts and reward-based trajectory selection, inspired by Model Predictive\nControl algorithm. SITCOM leverages a learned dynamics model to simulate\nmulti-step action rollouts to select the best candidate plan for real-world\nexecution, transforming one-shot VLAs into robust long-horizon planners. We\ndevelop an efficient transformer-based dynamics model trained on large-scale\nBridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim\ngap, and score candidate rollouts using rewards from simulator. Through\ncomprehensive evaluation across multiple tasks and settings in the SIMPLER\nenvironment, we demonstrate that SITCOM when combined with a good reward\nfunction can significantly improve task completion rate from 48% to 72% using\ntrained dynamics model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SITCOM\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u4e2d\u5f15\u5165\u57fa\u4e8e\u6a21\u578b\u7684\u63a8\u6f14\u548c\u5956\u52b1\u8f68\u8ff9\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u4e2d\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u957f\u65f6\u7a0b\u89c4\u5212\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b97\u6cd5\uff0c\u5229\u7528\u5b66\u4e60\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u52a8\u4f5c\u63a8\u6f14\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u9009\u62e9\u6700\u4f73\u8ba1\u5212\u3002", "result": "\u5728SIMPLER\u73af\u5883\u4e2d\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4ece48%\u63d0\u5347\u523072%\u3002", "conclusion": "SITCOM\u6846\u67b6\u7ed3\u5408\u826f\u597d\u7684\u5956\u52b1\u51fd\u6570\uff0c\u53ef\u6709\u6548\u63d0\u5347VLA\u6a21\u578b\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2510.05000", "pdf": "https://arxiv.org/pdf/2510.05000", "abs": "https://arxiv.org/abs/2510.05000", "authors": ["Xiang-Gen Xia"], "title": "My First Five Years of Faculty Career at the University of Delaware", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "In this short article, I would like to briefly summarize my research in the\nfirst 5 years in my university academia life in USA. I think that my research\nresults obtained in these 5 years are the best in my career, at least which I\nlike the most by myself. I wish that my experience in my junior academia career\ncould be of some help to young researchers.", "AI": {"tldr": "\u4f5c\u8005\u603b\u7ed3\u4e86\u81ea\u5df1\u5728\u7f8e\u56fd\u5927\u5b66\u5b66\u672f\u751f\u6daf\u524d\u4e94\u5e74\u7684\u7814\u7a76\u6210\u679c\uff0c\u8ba4\u4e3a\u8fd9\u662f\u4ed6\u804c\u4e1a\u751f\u6daf\u4e2d\u6700\u559c\u6b22\u7684\u6210\u679c\uff0c\u5e0c\u671b\u80fd\u5bf9\u5e74\u8f7b\u7814\u7a76\u8005\u6709\u6240\u5e2e\u52a9\u3002", "motivation": "\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u4eab\u81ea\u5df1\u5728\u5b66\u672f\u751f\u6daf\u65e9\u671f\u7684\u7ecf\u9a8c\u548c\u6210\u679c\uff0c\u6fc0\u52b1\u548c\u5e2e\u52a9\u5e74\u8f7b\u7684\u79d1\u7814\u4eba\u5458\u3002", "method": "\u4f5c\u8005\u672a\u5728\u6458\u8981\u4e2d\u5177\u4f53\u63cf\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u4ec5\u63d0\u53ca\u4e86\u603b\u7ed3\u524d\u4e94\u5e74\u7684\u7814\u7a76\u6210\u679c\u3002", "result": "\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u4ed6\u804c\u4e1a\u751f\u6daf\u4e2d\u6700\u597d\u7684\u6210\u679c\uff0c\u4e5f\u662f\u4ed6\u81ea\u5df1\u6700\u559c\u6b22\u7684\u90e8\u5206\u3002", "conclusion": "\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u4eab\u8fd9\u4e9b\u7ecf\u9a8c\u548c\u6210\u679c\uff0c\u5bf9\u5e74\u8f7b\u7684\u7814\u7a76\u8005\u6709\u6240\u542f\u53d1\u548c\u5e2e\u52a9\u3002"}}
{"id": "2510.04074", "pdf": "https://arxiv.org/pdf/2510.04074", "abs": "https://arxiv.org/abs/2510.04074", "authors": ["Chung-Pang Wang", "Changwei Chen", "Xiao Liang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous surgical systems must adapt to highly dynamic environments where\ntissue properties and visual cues evolve rapidly. Central to such adaptability\nis feedback: the ability to sense, interpret, and respond to changes during\nexecution. While feedback mechanisms have been explored in surgical robotics,\nranging from tool and tissue tracking to error detection, existing methods\nremain limited in handling the topological and perceptual challenges of tissue\ndissection. In this work, we propose a feedback-enabled framework for\nautonomous tissue dissection that explicitly reasons about topological changes\nfrom endoscopic images after each dissection action. This structured feedback\nguides subsequent actions, enabling the system to localize dissection progress\nand adapt policies online. To improve the reliability of such feedback, we\nintroduce visibility metrics that quantify tissue exposure and formulate\noptimal controller designs that actively manipulate tissue to maximize\nvisibility. Finally, we integrate these feedback mechanisms with both\nplanning-based and learning-based dissection methods, and demonstrate\nexperimentally that they significantly enhance autonomy, reduce errors, and\nimprove robustness in complex surgical scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u7684\u81ea\u4e3b\u7ec4\u7ec7\u89e3\u5256\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u62d3\u6251\u53d8\u5316\uff0c\u4f18\u5316\u624b\u672f\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u673a\u5668\u4eba\u53cd\u9988\u673a\u5236\u5728\u7ec4\u7ec7\u89e3\u5256\u7684\u62d3\u6251\u548c\u611f\u77e5\u6311\u6218\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u53cd\u9988\u6846\u67b6\uff0c\u7ed3\u5408\u80fd\u89c1\u5ea6\u6307\u6807\u548c\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u6574\u5408\u89c4\u5212\u548c\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u6027\u3001\u51cf\u5c11\u4e86\u9519\u8bef\uff0c\u5e76\u589e\u5f3a\u4e86\u590d\u6742\u624b\u672f\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u53cd\u9988\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u7ec7\u89e3\u5256\u7684\u52a8\u6001\u548c\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.03431", "pdf": "https://arxiv.org/pdf/2510.03431", "abs": "https://arxiv.org/abs/2510.03431", "authors": ["Refik Mert Cam", "Seonyeong Park", "Umberto Villa", "Mark A. Anastasio"], "title": "Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography", "categories": ["physics.med-ph", "cs.AI", "eess.SP"], "comment": "Preprint submitted to Elsevier Photoacoustics", "summary": "Quantitative photoacoustic computed tomography (qPACT) is a promising imaging\nmodality for estimating physiological parameters such as blood oxygen\nsaturation. However, developing robust qPACT reconstruction methods remains\nchallenging due to computational demands, modeling difficulties, and\nexperimental uncertainties. Learning-based methods have been proposed to\naddress these issues but remain largely unvalidated. Virtual imaging (VI)\nstudies are essential for validating such methods early in development, before\nproceeding to less-controlled phantom or in vivo studies. Effective VI studies\nmust employ ensembles of stochastically generated numerical phantoms that\naccurately reflect relevant anatomy and physiology. Yet, most prior VI studies\nfor qPACT relied on overly simplified phantoms. In this work, a realistic VI\ntestbed is employed for the first time to assess a representative 3D\nlearning-based qPACT reconstruction method for breast imaging. The method is\nevaluated across subject variability and physical factors such as measurement\nnoise and acoustic aberrations, offering insights into its strengths and\nlimitations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4f7f\u7528\u865a\u62df\u6210\u50cf\uff08VI\uff09\u6d4b\u8bd5\u5e73\u53f0\u8bc4\u4f30\u4e09\u7ef4\u5b66\u4e60\u578bqPACT\u91cd\u5efa\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u89e3\u51b3\u8ba1\u7b97\u56f0\u96be\u548c\u5b9e\u9a8c\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "qPACT\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6210\u50cf\u6280\u672f\uff0c\u4f46\u5176\u91cd\u5efa\u65b9\u6cd5\u5728\u8ba1\u7b97\u9700\u6c42\u548c\u5b9e\u9a8c\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u9a8c\u8bc1\u624b\u6bb5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u771f\u5b9e\u7684\u865a\u62df\u6210\u50cf\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bc4\u4f30\u4e86\u4e00\u79cd\u4e09\u7ef4\u5b66\u4e60\u578bqPACT\u91cd\u5efa\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u53d7\u8bd5\u8005\u53d8\u5f02\u6027\u548c\u7269\u7406\u56e0\u7d20\uff08\u5982\u6d4b\u91cf\u566a\u58f0\uff09\u3002", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u5728\u4e73\u817a\u6210\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u771f\u5b9e\u7684VI\u6d4b\u8bd5\u5e73\u53f0\u53ef\u4ee5\u6709\u6548\u9a8c\u8bc1\u5b66\u4e60\u578bqPACT\u65b9\u6cd5\uff0c\u4e3a\u5176\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04076", "pdf": "https://arxiv.org/pdf/2510.04076", "abs": "https://arxiv.org/abs/2510.04076", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "One of the main challenges in modern control applications, particularly in\nrobot and vehicle motion control, is achieving accurate, fast, and safe\nmovement. To address this, optimal control policies have been developed to\nenforce safety while ensuring high performance. Since basic first-principles\nmodels of real systems are often available, model-based controllers are widely\nused. Model predictive control (MPC) is a leading approach that optimizes\nperformance while explicitly handling safety constraints. However, obtaining\naccurate models for complex systems is difficult, which motivates data-driven\nalternatives. ML-based MPC leverages learned models to reduce reliance on\nhand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal\npolicies directly from interaction data. Data-enabled predictive control\n(DeePC) goes further by bypassing modeling altogether, directly learning safe\npolicies from raw input-output data. Recently, large language model (LLM)\nagents have also emerged, translating natural language instructions into\nstructured formulations of optimal control problems. Despite these advances,\ndata-driven policies face significant limitations. They often suffer from slow\nresponse times, high computational demands, and large memory needs, making them\nless practical for real-world systems with fast dynamics, limited onboard\ncomputing, or strict memory constraints. To address this, various technique,\nsuch as reduced-order modeling, function-approximated policy learning, and\nconvex relaxations, have been proposed to reduce computational complexity. In\nthis paper, we present eight such approaches and demonstrate their\neffectiveness across real-world applications, including robotic arms, soft\nrobots, and vehicle motion control.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u4ee3\u63a7\u5236\u5e94\u7528\u4e2d\u5b9e\u73b0\u7cbe\u51c6\u3001\u5feb\u901f\u4e14\u5b89\u5168\u8fd0\u52a8\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u5176\u6539\u8fdb\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u7cfb\u7edf\u4e2d\u9ad8\u6027\u80fd\u548c\u5b89\u5168\u63a7\u5236\u7684\u5efa\u6a21\u96be\u9898\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u9650\u5236\u3002", "method": "\u4ecb\u7ecd\u4e86\u516b\u79cd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6280\u672f\uff0c\u5305\u62ec\u964d\u9636\u5efa\u6a21\u3001\u51fd\u6570\u903c\u8fd1\u7b56\u7565\u5b66\u4e60\u548c\u51f8\u677e\u5f1b\u7b49\u3002", "result": "\u5728\u673a\u5668\u4eba\u81c2\u3001\u8f6f\u673a\u5668\u4eba\u548c\u8f66\u8f86\u8fd0\u52a8\u63a7\u5236\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5728\u5feb\u901f\u52a8\u6001\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.03601", "pdf": "https://arxiv.org/pdf/2510.03601", "abs": "https://arxiv.org/abs/2510.03601", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Kai-Chun Liu", "Yu Tsao"], "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SP", "I.2.6; C.2.4"], "comment": "15 pages, 7 figures, and published in IEEE Sensors Journal", "summary": "The rising aging population has increased the importance of fall detection\n(FD) systems as an assistive technology, where deep learning techniques are\nwidely applied to enhance accuracy. FD systems typically use edge devices (EDs)\nworn by individuals to collect real-time data, which are transmitted to a cloud\ncenter (CC) or processed locally. However, this architecture faces challenges\nsuch as a limited ED model size and data transmission latency to the CC. Mobile\nedge computing (MEC), which allows computations at MEC servers deployed between\nEDs and CC, has been explored to address these challenges. We propose a\nmultilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC\nsplits the architecture into stations, each with a neural network model. If\nfront-end equipment cannot detect falls reliably, data are transmitted to a\nstation with more robust back-end computing. The knowledge distillation (KD)\napproach was employed to improve front-end detection accuracy by allowing\nhigh-power back-end stations to provide additional learning experiences,\nenhancing precision while reducing latency and processing loads. Simulation\nresults demonstrate that the KD approach improved accuracy by 11.65% on the\nSisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also\nreduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on\nthe SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD\nsystem exhibits improved accuracy and reduced latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MLMEC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u8dcc\u5012\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u8001\u9f84\u5316\u4eba\u53e3\u7684\u589e\u52a0\u4f7f\u5f97\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\uff08FD\uff09\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5728\u8fb9\u7f18\u8bbe\u5907\uff08ED\uff09\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u548c\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u591a\u5c42MEC\u6846\u67b6\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\u5230\u4e0d\u540c\u5c42\u7ea7\u7684\u7ad9\u70b9\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u540e\u7aef\u8ba1\u7b97\u80fd\u529b\u5f3a\u7684\u7ad9\u70b9\u4e3a\u524d\u7aef\u63d0\u4f9b\u989d\u5916\u7684\u5b66\u4e60\u7ecf\u9a8c\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728SisFall\u548cFallAllD\u6570\u636e\u96c6\u4e0a\uff0cKD\u6280\u672f\u5206\u522b\u63d0\u5347\u4e8611.65%\u548c2.78%\u7684\u51c6\u786e\u6027\uff1b\u540c\u65f6\uff0cMLMEC\u4e0eKD\u7ed3\u5408\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u3002", "conclusion": "MLMEC\u6846\u67b6\u7ed3\u5408KD\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u901f\u5ea6\uff0c\u4e3a\u89e3\u51b3\u73b0\u6709\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2510.04161", "pdf": "https://arxiv.org/pdf/2510.04161", "abs": "https://arxiv.org/abs/2510.04161", "authors": ["Longrui Yang", "Yiyu Wang", "Jingfan Tang", "Yunpeng Lv", "Shizhe Zhao", "Chao Cao", "Zhongqiang Ren"], "title": "HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments", "categories": ["cs.RO"], "comment": "5 Figures", "summary": "This paper considers the path planning problem for autonomous exploration of\nan unknown environment using multiple heterogeneous robots such as drones,\nwheeled, and legged robots, which have different capabilities to traverse\ncomplex terrains. A key challenge there is to intelligently allocate the robots\nto the unknown areas to be explored and determine the visiting order of those\nspaces subject to traversablity constraints, which leads to a large scale\nconstrained optimization problem that needs to be quickly and iteratively\nsolved every time when new space are explored. To address the challenge, we\npropose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging\na recent hierarchical method that decompose the exploration into global\nplanning and local planning. The major contribution in HEHA is its global\nplanning, where we propose a new routing algorithm PEAF (Partial Anytime Focal\nsearch) that can quickly find bounded sub-optimal solutions to minimize the\nmaximum path length among the agents subject to traversability constraints.\nAdditionally, the local planner in HEHA also considers heterogeneity to avoid\nrepeated and duplicated exploration among the robots. The experimental results\nshow that, our HEHA can reduce up to 30% of the exploration time than the\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEHA\u7684\u5206\u5c42\u63a2\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u89c4\u5212\u7684\u534f\u540c\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a2\u7d22\u65f6\u95f4\u3002", "motivation": "\u5f02\u6784\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u63a2\u7d22\u65f6\uff0c\u5982\u4f55\u667a\u80fd\u5206\u914d\u8d44\u6e90\u5e76\u4f18\u5316\u8bbf\u95ee\u987a\u5e8f\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u63a2\u7d22\u65b9\u6cd5HEHA\uff0c\u5305\u62ec\u5168\u5c40\u89c4\u5212\u4e2d\u7684PEAF\u7b97\u6cd5\u548c\u8003\u8651\u5f02\u6784\u6027\u7684\u5c40\u90e8\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHEHA\u80fd\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u9ad8\u8fbe30%\u7684\u63a2\u7d22\u65f6\u95f4\u3002", "conclusion": "HEHA\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548c\u5f02\u6784\u6027\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u63a2\u7d22\u7684\u6548\u7387\u3002"}}
{"id": "2510.03728", "pdf": "https://arxiv.org/pdf/2510.03728", "abs": "https://arxiv.org/abs/2510.03728", "authors": ["Kuang Yuan", "Yang Gao", "Xilin Li", "Xinhao Mei", "Syavosh Zadissa", "Tarun Pruthi", "Saeed Bagheri Sereshki"], "title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "comment": null, "summary": "Acoustic scene classification (ASC) models on edge devices typically operate\nunder fixed class assumptions, lacking the transferability needed for\nreal-world applications that require adaptation to new or refined acoustic\ncategories. We propose ContrastASC, which learns generalizable acoustic scene\nrepresentations by structuring the embedding space to preserve semantic\nrelationships between scenes, enabling adaptation to unseen categories without\nretraining. Our approach combines supervised contrastive fine-tuning of\npre-trained models with contrastive representation distillation to transfer\nthis structured knowledge to compact student models. Our evaluation shows that\nContrastASC demonstrates improved few-shot adaptation to unseen categories\nwhile maintaining strong closed-set performance.", "AI": {"tldr": "ContrastASC\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u58f0\u5b66\u573a\u666f\u5206\u7c7b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u58f0\u5b66\u573a\u666f\u5206\u7c7b\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9002\u5e94\u73b0\u5b9e\u5e94\u7528\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u573a\u666f\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5fae\u8c03\u548c\u5bf9\u6bd4\u8868\u5f81\u84b8\u998f\uff0c\u5b66\u4e60\u901a\u7528\u58f0\u5b66\u573a\u666f\u8868\u5f81\uff0c\u5e76\u8fc1\u79fb\u5230\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "ContrastASC\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5c11\u6837\u672c\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5c01\u95ed\u96c6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u53d8\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.04168", "pdf": "https://arxiv.org/pdf/2510.04168", "abs": "https://arxiv.org/abs/2510.04168", "authors": ["Amirmasoud Molaei", "Reza Ghabcheloo"], "title": "Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Rock capturing with standard excavator buckets is a challenging task\ntypically requiring the expertise of skilled operators. Unlike soil digging, it\ninvolves manipulating large, irregular rocks in unstructured environments where\ncomplex contact interactions with granular material make model-based control\nimpractical. Existing autonomous excavation methods focus mainly on continuous\nmedia or rely on specialized grippers, limiting their applicability to\nreal-world construction sites. This paper introduces a fully data-driven\ncontrol framework for rock capturing that eliminates the need for explicit\nmodeling of rock or soil properties. A model-free reinforcement learning agent\nis trained in the AGX Dynamics simulator using the Proximal Policy Optimization\n(PPO) algorithm and a guiding reward formulation. The learned policy outputs\njoint velocity commands directly to the boom, arm, and bucket of a CAT365\nexcavator model. Robustness is enhanced through extensive domain randomization\nof rock geometry, density, and mass, as well as the initial configurations of\nthe bucket, rock, and goal position. To the best of our knowledge, this is the\nfirst study to develop and evaluate an RL-based controller for the rock\ncapturing task. Experimental results show that the policy generalizes well to\nunseen rocks and varying soil conditions, achieving high success rates\ncomparable to those of human participants while maintaining machine stability.\nThese findings demonstrate the feasibility of learning-based excavation\nstrategies for discrete object manipulation without requiring specialized\nhardware or detailed material models.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u6316\u6398\u673a\u6293\u53d6\u5ca9\u77f3\u7684\u8bba\u6587\uff0c\u65e0\u9700\u5efa\u6a21\u5ca9\u77f3\u6216\u571f\u58e4\u7279\u6027\uff0c\u5c55\u793a\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u5ca9\u77f3\u6293\u53d6\u4f9d\u8d56\u719f\u7ec3\u64cd\u4f5c\u5458\uff0c\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5c40\u9650\u4e8e\u8fde\u7eed\u4ecb\u8d28\u6216\u4e13\u7528\u5939\u5177\uff0c\u96be\u4ee5\u9002\u5e94\u5b9e\u9645\u5efa\u7b51\u5de5\u5730\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08PPO\u7b97\u6cd5\uff09\uff0c\u5728AGX Dynamics\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u57df\u968f\u673a\u5316\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u76f4\u63a5\u8f93\u51fa\u6316\u6398\u673a\u5173\u8282\u901f\u5ea6\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u5ca9\u77f3\u548c\u571f\u58e4\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u6210\u529f\u7387\u63a5\u8fd1\u4eba\u7c7b\u64cd\u4f5c\u5458\uff0c\u540c\u65f6\u4fdd\u6301\u673a\u5668\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u4e13\u7528\u786c\u4ef6\u6216\u8be6\u7ec6\u6750\u6599\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5904\u7406\u79bb\u6563\u7269\u4f53\u64cd\u4f5c\u95ee\u9898\u3002"}}
{"id": "2510.03769", "pdf": "https://arxiv.org/pdf/2510.03769", "abs": "https://arxiv.org/abs/2510.03769", "authors": ["Shimaa Elbana", "Ahmad Kamal", "Shahd Ahmed Ali", "Ahmad Al-Kabbany"], "title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "The increasing size and complexity of medical imaging datasets, particularly\nin 3D formats, present significant barriers to collaborative research and\ntransferability. This study investigates whether the ZFP compression technique\ncan mitigate these challenges without compromising the performance of automated\ncerebrovascular segmentation, a critical first step in intracranial aneurysm\ndetection. We apply ZFP in both its error tolerance and fixed-rate modes to a\nlarge scale, and one of the most recent, datasets in the literature, 3D medical\ndataset containing ground-truth vascular segmentations. The segmentation\nquality on the compressed volumes is rigorously compared to the uncompressed\nbaseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can\nachieve substantial data reduction--up to a 22.89:1 ratio in error tolerance\nmode--while maintaining a high degree of fidelity, with the mean Dice\ncoefficient remaining high at 0.87656. These results demonstrate that ZFP is a\nviable and powerful tool for enabling more efficient and accessible research on\nlarge-scale medical datasets, fostering broader collaboration across the\ncommunity.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cZFP\u538b\u7f29\u6280\u672f\u80fd\u5728\u663e\u8457\u51cf\u5c0f3D\u533b\u5b66\u5f71\u50cf\u6570\u636e\u5927\u5c0f\u7684\u540c\u65f6\u4fdd\u6301\u8111\u8840\u7ba1\u5206\u5272\u7684\u9ad8\u7cbe\u5ea6\uff0c\u63a8\u52a8\u5927\u89c4\u6a21\u6570\u636e\u7684\u9ad8\u6548\u5408\u4f5c\u7814\u7a76\u3002", "motivation": "\u89e3\u51b33D\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u589e\u5927\u548c\u590d\u6742\u6027\u5e26\u6765\u7684\u534f\u4f5c\u4e0e\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u5728\u8bef\u5dee\u5bb9\u5fcd\u548c\u56fa\u5b9a\u901f\u7387\u6a21\u5f0f\u4e0b\u5e94\u7528ZFP\u538b\u7f29\u6280\u672f\uff0c\u5e76\u4e0e\u672a\u538b\u7f29\u57fa\u7ebf\u5bf9\u6bd4\u5206\u5272\u8d28\u91cf\u3002", "result": "ZFP\u53ef\u5b9e\u73b0\u9ad8\u8fbe22.89:1\u7684\u6570\u636e\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u771f\u5ea6\u9ad8\uff08\u5e73\u5747Dice\u7cfb\u65700.87656\uff09\u3002", "conclusion": "ZFP\u662f\u4fc3\u8fdb\u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u9ad8\u6548\u534f\u4f5c\u7814\u7a76\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.04171", "pdf": "https://arxiv.org/pdf/2510.04171", "abs": "https://arxiv.org/abs/2510.04171", "authors": ["Lakshadeep Naik", "Adam Fischer", "Daniel Duberg", "Danica Kragic"], "title": "VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs", "categories": ["cs.RO"], "comment": null, "summary": "In Mobile Manipulation, selecting an optimal mobile base pose is essential\nfor successful object grasping. Previous works have addressed this problem\neither through classical planning methods or by learning state-based policies.\nThey assume access to reliable state information, such as the precise object\nposes and environment models. In this work, we study base pose planning\ndirectly from top-down orthographic projections of the scene, which provide a\nglobal overview of the scene while preserving spatial structure. We propose\nVBM-NET, a learning-based method for base pose selection using such top-down\northographic projections. We use equivariant TransporterNet to exploit spatial\nsymmetries and efficiently learn candidate base poses for grasping. Further, we\nuse graph neural networks to represent a varying number of candidate base poses\nand use Reinforcement Learning to determine the optimal base pose among them.\nWe show that VBM-NET can produce comparable solutions to the classical methods\nin significantly less computation time. Furthermore, we validate sim-to-real\ntransfer by successfully deploying a policy trained in simulation to real-world\nmobile manipulation.", "AI": {"tldr": "VBM-NET \u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u79fb\u52a8\u57fa\u5ea7\u59ff\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u5229\u7528\u4fef\u89c6\u6295\u5f71\u56fe\u548c\u5f3a\u5316\u5b66\u4e60\u9ad8\u6548\u9009\u62e9\u6700\u4f73\u59ff\u6001\uff0c\u8ba1\u7b97\u65f6\u95f4\u77ed\u4e14\u80fd\u5b9e\u73b0\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u64cd\u4f5c\u4e2d\u57fa\u5ea7\u59ff\u6001\u9009\u62e9\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u7cbe\u786e\u72b6\u6001\u4fe1\u606f\uff0c\u76f4\u63a5\u5229\u7528\u573a\u666f\u7684\u5168\u5c40\u4fef\u89c6\u6295\u5f71\u3002", "method": "\u4f7f\u7528\u7b49\u53d8TransporterNet\u5904\u7406\u7a7a\u95f4\u5bf9\u79f0\u6027\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u5019\u9009\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u59ff\u6001\u3002", "result": "VBM-NET\u5728\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u5ab2\u7f8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "VBM-NET\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u57fa\u5ea7\u59ff\u6001\u9009\u62e9\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2510.03815", "pdf": "https://arxiv.org/pdf/2510.03815", "abs": "https://arxiv.org/abs/2510.03815", "authors": ["Yue wu"], "title": "A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP"], "comment": "1tables,6 figs,11pages", "summary": "There are limitations of traditional methods and deep learning methods in\nterms of interpretability, generalization, and quantification of uncertainty in\nindustrial fault diagnosis, and there are core problems of insufficient\ncredibility in industrial fault diagnosis. The architecture performs\npreliminary analysis through a Bayesian network-based diagnostic engine and\nfeatures an LLM-driven cognitive quorum module with multimodal input\ncapabilities. The module conducts expert-level arbitration of initial diagnoses\nby analyzing structured features and diagnostic charts, prioritizing final\ndecisions after conflicts are identified. To ensure the reliability of the\nsystem output, the architecture integrates a confidence calibration module\nbased on temperature calibration and a risk assessment module, which\nobjectively quantifies the reliability of the system using metrics such as\nexpected calibration error (ECE). Experimental results on a dataset containing\nmultiple fault types showed that the proposed framework improved diagnostic\naccuracy by more than 28 percentage points compared to the baseline model,\nwhile the calibrated ECE was reduced by more than 75%. Case studies have\nconfirmed that HCAA effectively corrects misjudgments caused by complex feature\npatterns or knowledge gaps in traditional models, providing novel and practical\nengineering solutions for building high-trust, explainable AI diagnostic\nsystems for industrial applications.", "AI": {"tldr": "\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u3002\u65b0\u67b6\u6784\u7ed3\u5408\u8d1d\u53f6\u65af\u7f51\u7edc\u8bca\u65ad\u5f15\u64ce\u548cLLM\u9a71\u52a8\u7684\u8ba4\u77e5\u4ef2\u88c1\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u51b2\u7a81\u89e3\u51b3\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5f15\u5165\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u98ce\u9669\u8bc4\u4f30\u6a21\u5757\u91cf\u5316\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u534728%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e75%\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u4e2d\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u4fe1\u5ea6\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u8bca\u65ad\u5f15\u64ce\u548cLLM\u9a71\u52a8\u7684\u8ba4\u77e5\u4ef2\u88c1\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\u548c\u51b2\u7a81\u89e3\u51b3\u673a\u5236\uff0c\u5f15\u5165\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u98ce\u9669\u8bc4\u4f30\u6a21\u5757\u3002", "result": "\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u534728%\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e75%\uff0c\u6709\u6548\u7ea0\u6b63\u4f20\u7edf\u6a21\u578b\u8bef\u5224\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u9ad8\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684\u5de5\u4e1aAI\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u7a0b\u65b9\u6848\u3002"}}
{"id": "2510.04178", "pdf": "https://arxiv.org/pdf/2510.04178", "abs": "https://arxiv.org/abs/2510.04178", "authors": ["L\u00e9a Pistorius", "Namrata U. Nayar", "Phillip Tran", "Sammy Elmariah", "Pierre E. Dupont"], "title": "Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve", "categories": ["cs.RO"], "comment": "7 pages, 9 figures", "summary": "Transcatheter valve repair presents significant challenges due to the\nmechanical limitations and steep learning curve associated with manual catheter\nsystems. This paper investigates the use of robotics to facilitate\ntranscatheter procedures in the context of mitral valve edge-to-edge repair.\nThe complex handle-based control of a clinical repair device is replaced by\nintuitive robotic joint-based control via a game controller. Manual versus\nrobotic performance is analyzed by decomposing the overall device delivery task\ninto motion-specific steps and comparing capabilities on a step-by-step basis\nin a phantom model of the heart and vasculature. Metrics include procedure\nduration and clip placement accuracy. Results demonstrate that the robotic\nsystem can reduce procedural time and motion errors while also improving\naccuracy of clip placement. These findings suggest that robotic assistance can\naddress key limitations of manual systems, offering a more reliable and\nuser-friendly platform for complex transcatheter procedures.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u5728\u4e8c\u5c16\u74e3\u8fb9\u5bf9\u8fb9\u4fee\u590d\u624b\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6e38\u620f\u63a7\u5236\u5668\u5b9e\u73b0\u76f4\u89c2\u63a7\u5236\uff0c\u76f8\u6bd4\u624b\u52a8\u64cd\u4f5c\u7f29\u77ed\u4e86\u624b\u672f\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u5bfc\u7ba1\u7cfb\u7edf\u5728\u74e3\u819c\u4fee\u590d\u4e2d\u5b58\u5728\u673a\u68b0\u9650\u5236\u548c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u7684\u95ee\u9898\uff0c\u673a\u5668\u4eba\u8f85\u52a9\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u9650\u5236\u3002", "method": "\u7528\u673a\u5668\u4eba\u5173\u8282\u63a7\u5236\u66ff\u4ee3\u624b\u52a8\u63a7\u5236\uff0c\u5206\u89e3\u4efb\u52a1\u6b65\u9aa4\uff0c\u901a\u8fc7\u5e7b\u5f71\u6a21\u578b\u6bd4\u8f83\u624b\u52a8\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8868\u73b0\u3002", "result": "\u673a\u5668\u4eba\u7cfb\u7edf\u51cf\u5c11\u4e86\u624b\u672f\u65f6\u95f4\u548c\u8fd0\u52a8\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u74e3\u819c\u5939\u653e\u7f6e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u673a\u5668\u4eba\u8f85\u52a9\u4e3a\u590d\u6742\u5bfc\u7ba1\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.03831", "pdf": "https://arxiv.org/pdf/2510.03831", "abs": "https://arxiv.org/abs/2510.03831", "authors": ["Pedro Ivo da Cruz", "Dimitri Silva", "Tito Spadini", "Ricardo Suyama", "Murilo Bellezoni Loiola"], "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO", "categories": ["cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": "This version of the article has been accepted for publication, after\n  peer review and is subject to Springer Nature's AM terms of use, but is not\n  the Version of Record and does not reflect post-acceptance improvements, or\n  any corrections. The Version of Record is available online at:\n  https://doi.org/10.1007/s11235-024-01163-0", "summary": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u51b3\u7b56\u6811\uff08DT\uff09\u7b97\u6cd5\u68c0\u6d4b\u5927\u5bb9\u91cf\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u4e2d\u7684\u5bfc\u9891\u6c61\u67d3\u653b\u51fb\uff08PCA\uff09\uff0cDT\u5728\u566a\u58f0\u548c\u4f4e\u529f\u7387\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u4f3c\u7136\u6bd4\u6d4b\u8bd5\uff08LRT\uff09\uff0c\u4e14\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u5bfc\u9891\u6c61\u67d3\u653b\u51fb\uff08PCA\uff09\u5a01\u80c1\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982LRT\uff09\u4f9d\u8d56\u5148\u9a8c\u5047\u8bbe\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22DT\u7b97\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u751f\u6210DT\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u9009\u62e9\u6700\u4f73\u6df1\u5ea6\u7684DT\uff0c\u5e76\u5728\u591a\u573a\u666f\u4e2d\u6a21\u62df\u6bd4\u8f83DT\u4e0eLRT\u7684\u6027\u80fd\u3002", "result": "\u5355\u5c42\u6df1\u5ea6\u7684DT\u5373\u53ef\u8d85\u8d8aLRT\uff0c\u5c24\u5176\u5728\u566a\u58f0\u548c\u4f4e\u529f\u7387\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u65e0\u9700\u566a\u58f0\u6216\u4fe1\u53f7\u529f\u7387\u7684\u5148\u9a8c\u4fe1\u606f\u3002", "conclusion": "DT\u7b97\u6cd5\u5728PCA\u68c0\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u65e0\u9700\u590d\u6742\u5047\u8bbe\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u3002"}}
{"id": "2510.04190", "pdf": "https://arxiv.org/pdf/2510.04190", "abs": "https://arxiv.org/abs/2510.04190", "authors": ["Jian-jie Zheng", "Chih-kai Yang", "Po-han Chen", "Lyn Chao-ling Chen"], "title": "Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification", "categories": ["cs.RO"], "comment": null, "summary": "In the study, the social robot act as a patrol to recognize and notify\nillegal parking in real-time. Dual-model pipeline method and large multimodal\nmodel were compared, and the GPT-4o multimodal model was adopted in license\nplate recognition without preprocessing. For moving smoothly on a flat ground,\nthe robot navigated in a simulated parking lot in the experiments. The robot\nchanges angle view of the camera automatically to capture the images around\nwith the format of license plate number. From the captured images of the robot,\nthe numbers on the plate are recognized through the GPT-4o model, and\nidentifies legality of the numbers. When an illegal parking is detected, the\nrobot sends Line messages to the system manager immediately. The contribution\nof the work is that a novel multimodal deep learning method has validated with\nhigh accuracy in license plate recognition, and a social assistive robot is\nalso provided for solving problems in a real scenario, and can be applied in an\nindoor parking lot.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u793e\u4ea4\u673a\u5668\u4eba\u5de1\u903b\u5e76\u5b9e\u65f6\u8bc6\u522b\u975e\u6cd5\u505c\u8f66\u7684\u65b9\u6cd5\uff0c\u91c7\u7528GPT-4o\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u65e0\u9700\u9884\u5904\u7406\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u901a\u77e5\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u505c\u8f66\u573a\u4e2d\u975e\u6cd5\u505c\u8f66\u7684\u5b9e\u65f6\u8bc6\u522b\u4e0e\u901a\u77e5\u95ee\u9898\uff0c\u63d0\u5347\u505c\u8f66\u7ba1\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u53cc\u6a21\u578b\u6d41\u6c34\u7ebf\u65b9\u6cd5\u548cGPT-4o\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u673a\u5668\u4eba\u901a\u8fc7\u81ea\u52a8\u8c03\u6574\u6444\u50cf\u5934\u89d2\u5ea6\u6355\u83b7\u8f66\u724c\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7GPT-4o\u8bc6\u522b\u8f66\u724c\u53f7\u7801\u53ca\u5176\u5408\u6cd5\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u975e\u6cd5\u505c\u8f66\u5e76\u901a\u8fc7Line\u6d88\u606f\u901a\u77e5\u7cfb\u7edf\u7ba1\u7406\u5458\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u505c\u8f66\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u505c\u8f66\u573a\u573a\u666f\u3002"}}
{"id": "2510.03860", "pdf": "https://arxiv.org/pdf/2510.03860", "abs": "https://arxiv.org/abs/2510.03860", "authors": ["Faeze Moradi Kalarde", "Ben Liang", "Min Dong", "Yahia A. Eldemerdash Ahmed", "Ho Ting Cheng"], "title": "Privacy Enhancement in Over-the-Air Federated Learning via Adaptive Receive Scaling", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "12 pages, 2 figures", "summary": "In Federated Learning (FL) with over-the-air aggregation, the quality of the\nsignal received at the server critically depends on the receive scaling\nfactors. While a larger scaling factor can reduce the effective noise power and\nimprove training performance, it also compromises the privacy of devices by\nreducing uncertainty. In this work, we aim to adaptively design the receive\nscaling factors across training rounds to balance the trade-off between\ntraining convergence and privacy in an FL system under dynamic channel\nconditions. We formulate a stochastic optimization problem that minimizes the\noverall R\\'enyi differential privacy (RDP) leakage over the entire training\nprocess, subject to a long-term constraint that ensures convergence of the\nglobal loss function. Our problem depends on unknown future information, and we\nobserve that standard Lyapunov optimization is not applicable. Thus, we develop\na new online algorithm, termed AdaScale, based on a sequence of novel per-round\nproblems that can be solved efficiently. We further derive upper bounds on the\ndynamic regret and constraint violation of AdaSacle, establishing that it\nachieves diminishing dynamic regret in terms of time-averaged RDP leakage while\nensuring convergence of FL training to a stationary point. Numerical\nexperiments on canonical classification tasks show that our approach\neffectively reduces RDP and DP leakages compared with state-of-the-art\nbenchmarks without compromising learning performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaScale\u7684\u81ea\u9002\u5e94\u63a5\u6536\u7f29\u653e\u56e0\u5b50\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5e73\u8861\u8bad\u7ec3\u6536\u655b\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u7684\u65e0\u7ebf\u805a\u5408\u4e2d\uff0c\u63a5\u6536\u7f29\u653e\u56e0\u5b50\u7684\u9009\u62e9\u76f4\u63a5\u5f71\u54cd\u4fe1\u53f7\u8d28\u91cf\u548c\u9690\u79c1\u4fdd\u62a4\u3002\u8fc7\u5927\u7684\u7f29\u653e\u56e0\u5b50\u867d\u80fd\u51cf\u5c11\u566a\u58f0\uff0c\u4f46\u4f1a\u964d\u4f4e\u9690\u79c1\u4fdd\u62a4\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u8c03\u6574\u4ee5\u517c\u987e\u4e24\u8005\u3002", "method": "\u901a\u8fc7\u968f\u673a\u4f18\u5316\u5efa\u6a21\uff0c\u8bbe\u8ba1\u65b0\u7b97\u6cd5AdaScale\uff0c\u52a8\u6001\u8c03\u6574\u7f29\u653e\u56e0\u5b50\uff0c\u786e\u4fdd\u957f\u671f\u6536\u655b\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316R\u00e9nyi\u5dee\u5206\u9690\u79c1\u6cc4\u6f0f\u3002", "result": "AdaScale\u7b97\u6cd5\u5728\u52a8\u6001\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u65b9\u9762\u6709\u7406\u8bba\u4e0a\u754c\uff0c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u6709\u6548\u964d\u4f4e\u4e86RDP\u548cDP\u6cc4\u6f0f\uff0c\u4e0d\u5f71\u54cd\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "AdaScale\u5728\u52a8\u6001\u4fe1\u9053\u6761\u4ef6\u4e0b\u6709\u6548\u5e73\u8861\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u8bad\u7ec3\u6536\u655b\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002"}}
{"id": "2510.04234", "pdf": "https://arxiv.org/pdf/2510.04234", "abs": "https://arxiv.org/abs/2510.04234", "authors": ["Runhan Huang", "Haldun Balim", "Heng Yang", "Yilun Du"], "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "Legged locomotion demands controllers that are both robust and adaptable,\nwhile remaining compatible with task and safety considerations. However,\nmodel-free reinforcement learning (RL) methods often yield a fixed policy that\ncan be difficult to adapt to new behaviors at test time. In contrast, Model\nPredictive Control (MPC) provides a natural approach to flexible behavior\nsynthesis by incorporating different objectives and constraints directly into\nits optimization process. However, classical MPC relies on accurate dynamics\nmodels, which are often difficult to obtain in complex environments and\ntypically require simplifying assumptions. We present Diffusion-MPC, which\nleverages a learned generative diffusion model as an approximate dynamics prior\nfor planning, enabling flexible test-time adaptation through reward and\nconstraint based optimization. Diffusion-MPC jointly predicts future states and\nactions; at each reverse step, we incorporate reward planning and impose\nconstraint projection, yielding trajectories that satisfy task objectives while\nremaining within physical limits. To obtain a planning model that adapts beyond\nimitation pretraining, we introduce an interactive training algorithm for\ndiffusion based planner: we execute our reward-and-constraint planner in\nenvironment, then filter and reweight the collected trajectories by their\nrealized returns before updating the denoiser. Our design enables strong\ntest-time adaptability, allowing the planner to adjust to new reward\nspecifications without retraining. We validate Diffusion-MPC on real world,\ndemonstrating strong locomotion and flexible adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u65b0\u65b9\u6cd5Diffusion-MPC\uff0c\u7528\u4e8e\u89e3\u51b3\u817f\u90e8\u8fd0\u52a8\u63a7\u5236\u4e2d\u56fa\u5b9a\u7b56\u7565\u96be\u4ee5\u9002\u5e94\u65b0\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7684\u7b56\u7565\u56fa\u5b9a\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u884c\u4e3a\uff0c\u800c\u7ecf\u5178MPC\u4f9d\u8d56\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002\u8bba\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u66f4\u7075\u6d3b\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "Diffusion-MPC\u5229\u7528\u5b66\u4e60\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u52a8\u529b\u5b66\u5148\u9a8c\uff0c\u901a\u8fc7\u5956\u52b1\u548c\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u7075\u6d3b\u89c4\u5212\u3002\u8bbe\u8ba1\u4e86\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u7b97\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "result": "Diffusion-MPC\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u7075\u6d3b\u7684\u9002\u5e94\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u7684\u5956\u52b1\u89c4\u8303\u3002", "conclusion": "Diffusion-MPC\u6210\u529f\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548cMPC\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u9002\u5e94\u6027\u7684\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04339", "pdf": "https://arxiv.org/pdf/2510.04339", "abs": "https://arxiv.org/abs/2510.04339", "authors": ["Christian Limberg", "Fares Schulz", "Zhe Zhang", "Stefan Weinzierl"], "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on\n  Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com", "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u795e\u7ecf\u4e50\u5668\u58f0\u97f3\u5408\u6210\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u97f3\u9ad8\u51c6\u786e\u7684\u97f3\u4e50\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u76f4\u89c2\u76842D\u6f5c\u5728\u7a7a\u95f4\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u4e50\u6837\u672c\uff0c\u4f46\u5176\u9ad8\u7ef4\u6f5c\u5728\u8868\u793a\u96be\u4ee5\u5bfc\u822a\u4e14\u7528\u6237\u4f53\u9a8c\u4e0d\u76f4\u89c2\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u9996\u5148\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u97f3\u9ad8\u4e0e\u97f3\u8272\u5206\u79bb\u76842D\u8868\u793a\uff0c\u518d\u5c06\u5176\u4f5c\u4e3aTransformer\u751f\u6210\u6a21\u578b\u7684\u8f93\u5165\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u5206\u79bb\u7684\u97f3\u8272\u7a7a\u95f4\uff0c\u751f\u6210\u8868\u8fbe\u529b\u5f3a\u4e14\u97f3\u9ad8\u53ef\u63a7\u7684\u97f3\u9891\uff0c\u540c\u65f6\u6355\u83b7\u97f3\u8272\u7ec6\u5fae\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u89c2\u76842D\u7a7a\u95f4\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u672a\u6765\u97f3\u4e50\u5236\u4f5c\u73af\u5883\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04246", "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "categories": ["cs.RO", "cs.AI"], "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times.", "AI": {"tldr": "ContextVLA\u662f\u4e00\u79cd\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u5e27\u89c2\u6d4b\u6765\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u7684\u7b56\u7565\u6a21\u578b\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u591a\u5e27\u89c2\u6d4b\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5728\u591a\u5e27\u89c2\u6d4b\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u4e0d\u4e00\u81f4\u3002Vision-Language-Action\u6a21\u578b\uff08VLA\uff09\u5728\u591a\u5e27\u89c2\u6d4b\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faContextVLA\uff0c\u5c06\u8fc7\u53bb\u7684\u89c2\u6d4b\u538b\u7f29\u4e3a\u4e00\u4e2a\u4e0a\u4e0b\u6587token\uff0c\u4f7f\u7b56\u7565\u80fd\u9ad8\u6548\u5229\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u751f\u6210\u52a8\u4f5c\u3002", "result": "ContextVLA\u76f8\u6bd4\u5355\u5e27VLA\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u51cf\u5c11\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "ContextVLA\u63d0\u4f9b\u4e86\u9ad8\u6548\u5229\u7528\u591a\u5e27\u89c2\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.04346", "pdf": "https://arxiv.org/pdf/2510.04346", "abs": "https://arxiv.org/abs/2510.04346", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins", "categories": ["cs.NI", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "comment": "Code: https://github.com/nahshonmokua/LoRaWAN-Indoor-PL-parametrics", "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u73af\u5883\u611f\u77e5\u7684\u5ba4\u5185LoRaWAN\u4f20\u64ad\u8def\u5f84\u635f\u8017\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u73af\u5883\u534f\u53d8\u91cf\u548c\u591a\u65b9\u6cd5\u5efa\u6a21\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6570\u8ddd\u79bb\u6a21\u578b\u548c\u6b63\u6001\u9634\u5f71\u5047\u8bbe\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u53d7\u5230\u7ed3\u6784\u548c\u65f6\u53d8\u56e0\u7d20\u7684\u6311\u6218\uff0c\u9700\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u73af\u5883\u534f\u53d8\u91cf\u548c\u591a\u65b9\u6cd5\u5efa\u6a21\uff08\u5982\u591a\u9879\u5f0f\u56de\u5f52\u3001\u8d1d\u53f6\u65af\u56de\u5f52\u7b49\uff09\uff0c\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u5206\u6790\u9634\u5f71\u8870\u843d\u3002", "result": "\u591a\u9879\u5f0f\u5747\u503c\u5c06\u4ea4\u53c9\u9a8c\u8bc1RMSE\u4ece8.07\u964d\u81f37.09 dB\uff0cR^2\u4ece0.81\u63d0\u5347\u81f30.86\uff1b\u572899%\u5305\u4ea4\u4ed8\u7387\u4e0b\uff0c\u6240\u9700\u8870\u51cf\u4f59\u91cf\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5ba4\u5185\u7269\u8054\u7f51\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7b26\u54086G\u76ee\u6807\u3002"}}
{"id": "2510.04278", "pdf": "https://arxiv.org/pdf/2510.04278", "abs": "https://arxiv.org/abs/2510.04278", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yuanyuan Zhang", "Jiahao Hu", "Yingming Chen", "Naigui Xiao", "Jiaqi Zhao"], "title": "Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit", "categories": ["cs.RO"], "comment": null, "summary": "Model predictive control (MPC) faces significant limitations when applied to\nsystems evolving on nonlinear manifolds, such as robotic attitude dynamics and\nconstrained motion planning, where traditional Euclidean formulations struggle\nwith singularities, over-parameterization, and poor convergence. To overcome\nthese challenges, this paper introduces FactorMPC, a factor-graph based MPC\ntoolkit that unifies system dynamics, constraints, and objectives into a\nmodular, user-friendly, and efficient optimization structure. Our approach\nnatively supports manifold-valued states with Gaussian uncertainties modeled in\ntangent spaces. By exploiting the sparsity and probabilistic structure of\nfactor graphs, the toolkit achieves real-time performance even for\nhigh-dimensional systems with complex constraints. The velocity-extended\non-manifold control barrier function (CBF)-based obstacle avoidance factors are\ndesigned for safety-critical applications. By bridging graphical models with\nsafety-critical MPC, our work offers a scalable and geometrically consistent\nframework for integrated planning and control. The simulations and experimental\nresults on the quadrotor demonstrate superior trajectory tracking and obstacle\navoidance performance compared to baseline methods. To foster research\nreproducibility, we have provided open-source implementation offering\nplug-and-play factors.", "AI": {"tldr": "FactorMPC\u662f\u4e00\u4e2a\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684MPC\u5de5\u5177\u5305\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u6d41\u5f62\u4e0a\u7cfb\u7edf\u7684MPC\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97MPC\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u4e0a\u5b58\u5728\u5947\u5f02\u6027\u3001\u8fc7\u53c2\u6570\u5316\u548c\u6536\u655b\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u51e0\u4f55\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFactorMPC\uff0c\u5229\u7528\u56e0\u5b50\u56fe\u7edf\u4e00\u7cfb\u7edf\u52a8\u6001\u3001\u7ea6\u675f\u548c\u76ee\u6807\uff0c\u652f\u6301\u6d41\u5f62\u72b6\u6001\u548c\u5207\u7ebf\u7a7a\u95f4\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u56db\u65cb\u7ffc\u4e0a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FactorMPC\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u51e0\u4f55\u4e00\u81f4\u7684\u5b89\u5168\u5173\u952eMPC\u6846\u67b6\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u7814\u7a76\u590d\u73b0\u3002"}}
{"id": "2510.04622", "pdf": "https://arxiv.org/pdf/2510.04622", "abs": "https://arxiv.org/abs/2510.04622", "authors": ["Youngjoon Lee", "Seongmin Cho", "Yehhyun Jo", "Jinu Gong", "Hyunjoo Jenny Lee", "Joonhyuk Kang"], "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI", "categories": ["cs.LG", "eess.SP"], "comment": "Under Review", "summary": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7ea7\u9884\u6d4b\u6a21\u578b\u7684\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728AI\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u548c\u8d44\u6e90\u9700\u6c42\u9650\u5236\u4e86\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217AI\u7684\u53d1\u5c55\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\u53ef\u586b\u8865\u6570\u636e\u9700\u6c42\u4e0e\u53ef\u8bbf\u95ee\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u9ad8\u7ea7\u9884\u6d4b\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u51c6\u786e\u590d\u5236EEG\u548cEMG\u7b49\u590d\u6742\u7535\u751f\u7406\u4fe1\u53f7\uff0c\u4fdd\u7559\u771f\u5b9e\u6570\u636e\u7684\u65f6\u7a7a\u548c\u9891\u8c31\u7279\u6027\u3002", "result": "\u5408\u6210\u6570\u636e\u4e0d\u4ec5\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u8fd8\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u751f\u7269\u533b\u5b66\u7279\u5f81\u548c\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u7684\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u652f\u6301\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u53cc\u91cd\u6311\u6218\u3002"}}
{"id": "2510.04353", "pdf": "https://arxiv.org/pdf/2510.04353", "abs": "https://arxiv.org/abs/2510.04353", "authors": ["Stephen McCrory", "Romeo Orsolino", "Dhruv Thanki", "Luigi Penco", "Robert Griffin"], "title": "Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation", "categories": ["cs.RO"], "comment": null, "summary": "Teleoperation is a powerful method to generate reference motions and enable\nhumanoid robots to perform a broad range of tasks. However, teleoperation\nbecomes challenging when using hand contacts and non-coplanar surfaces, often\nleading to motor torque saturation or loss of stability through slipping. We\npropose a centroidal stability-based retargeting method that dynamically\nadjusts contact points and posture during teleoperation to enhance stability in\nthese difficult scenarios. Central to our approach is an efficient analytical\ncalculation of the stability margin gradient. This gradient is used to identify\nscenarios for which stability is highly sensitive to teleoperation setpoints\nand inform the local adjustment of these setpoints. We validate the framework\nin simulation and hardware by teleoperating manipulation tasks on a humanoid,\ndemonstrating increased stability margins. We also demonstrate empirically that\nhigher stability margins correlate with improved impulse resilience and joint\ntorque margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d28\u5fc3\u7a33\u5b9a\u6027\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u9065\u63a7\u64cd\u4f5c\u4e2d\u7684\u63a5\u89e6\u70b9\u548c\u59ff\u52bf\uff0c\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a5\u89e6\u70b9\u548c\u59ff\u52bf\u6765\u89e3\u51b3\u9065\u63a7\u64cd\u4f5c\u4e2d\u56e0\u624b\u90e8\u63a5\u89e6\u548c\u975e\u5171\u9762\u8868\u9762\u5bfc\u81f4\u7684\u529b\u77e9\u9971\u548c\u6216\u6ed1\u52a8\u5931\u7a33\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684\u5206\u6790\u8ba1\u7b97\u65b9\u6cd5\u8ba1\u7b97\u7a33\u5b9a\u6027\u8fb9\u754c\u68af\u5ea6\uff0c\u5e76\u6839\u636e\u68af\u5ea6\u8c03\u6574\u63a7\u5236\u70b9\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7a33\u5b9a\u6027\u8fb9\u754c\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u9ad8\u7a33\u5b9a\u6027\u8fb9\u754c\u53ef\u4ee5\u589e\u5f3a\u6297\u51b2\u51fb\u80fd\u529b\u548c\u5173\u8282\u529b\u77e9\u88d5\u5ea6\u3002"}}
{"id": "2510.04927", "pdf": "https://arxiv.org/pdf/2510.04927", "abs": "https://arxiv.org/abs/2510.04927", "authors": ["Usman Akram", "Yiyue Chen", "Haris Vikalo"], "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.", "AI": {"tldr": "FedSSL-AMC \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u95ee\u9898\u548c\u6570\u636e\u5206\u5e03\u4e0d\u5747\u7684\u6311\u6218\uff0c\u800c\u8054\u90a6\u5b66\u4e60\u5728\u6b64\u573a\u666f\u4e0b\u4ecd\u53d7\u5230\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa FedSSL-AMC\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u4e09\u5143\u7ec4\u635f\u5931\uff09\u5728\u5ba2\u6237\u7aef\u4e0a\u8bad\u7ec3\u56e0\u679c\u65f6\u95f4\u81a8\u80c0 CNN\uff0c\u518d\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3 SVM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u6784 SNR\u3001\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u548c\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6807\u7b7e\u5206\u533a\u4e0b\u5747\u4f18\u4e8e\u76d1\u7763\u5f0f\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "FedSSL-AMC \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u9690\u79c1\u548c\u5206\u5e03\u4e0d\u5747\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04354", "pdf": "https://arxiv.org/pdf/2510.04354", "abs": "https://arxiv.org/abs/2510.04354", "authors": ["Apurva Badithela", "David Snyder", "Lihan Zha", "Joseph Mikhail", "Matthew O'Kelly", "Anushri Dixit", "Anirudha Majumdar"], "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance.", "AI": {"tldr": "SureSim\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5c0f\u89c4\u6a21\u5b9e\u9645\u6d4b\u8bd5\u548c\u5927\u89c4\u6a21\u4eff\u771f\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u8282\u7ea6\u4e8620-25%\u7684\u786c\u4ef6\u8bc4\u4f30\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5c11\u91cf\u786c\u4ef6\u8bd5\u9a8c\uff0c\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u96be\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "method": "SureSim\u5c06\u771f\u5b9e\u4e0e\u4eff\u771f\u8bc4\u4f30\u7ed3\u5408\uff0c\u901a\u8fc7\u5c11\u91cf\u914d\u5bf9\u6570\u636e\u6821\u6b63\u4eff\u771f\u504f\u5dee\uff0c\u5e76\u5229\u7528\u975e\u6e10\u8fd1\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\u63d0\u4f9b\u6027\u80fd\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728\u7269\u7406\u5b66\u4eff\u771f\u4e2d\uff0cSureSim\u4e3a\u6269\u6563\u7b56\u7565\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u663e\u8457\u8282\u7701\u4e86\u786c\u4ef6\u8bc4\u4f30\u6210\u672c\u3002", "conclusion": "SureSim\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u786c\u4ef6\u8bd5\u9a8c\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.05068", "pdf": "https://arxiv.org/pdf/2510.05068", "abs": "https://arxiv.org/abs/2510.05068", "authors": ["Shreya Meel", "Sennur Ulukus"], "title": "Multi-Agent Distributed Optimization With Feasible Set Privacy", "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.NI", "eess.SP", "math.IT"], "comment": null, "summary": "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5728\u4fdd\u62a4\u5404\u81ea\u53ef\u884c\u96c6\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5206\u6563\u5f0f\u901a\u4fe1\u5171\u540c\u5b66\u4e60\u6700\u4f18\u89e3\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7f51\u7edc\u8bbe\u7f6e\u4e0b\u7684\u901a\u4fe1\u9ad8\u6548\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u5206\u6563\u5f0f\u4f18\u5316\u4e2d\u5982\u4f55\u5171\u540c\u5b66\u4e60\u6700\u4f18\u89e3\u96c6\uff0c\u540c\u65f6\u4fdd\u62a4\u5404\u81ea\u53ef\u884c\u96c6\u7684\u9690\u79c1\u4e0d\u88ab\u6cc4\u9732\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u4fe1\u65b9\u6848\uff0c\u9886\u5bfc\u667a\u80fd\u4f53\u901a\u8fc7\u67e5\u8be2\u975e\u9886\u5bfc\u667a\u80fd\u4f53\u6765\u68c0\u7d22\u89e3\u96c6\uff0c\u786e\u4fdd\u4fe1\u606f\u6cc4\u9732\u6700\u5c0f\u3002\u7814\u7a76\u4e86\u73af\u5f62\u548c\u661f\u5f62\u4e24\u79cd\u901a\u4fe1\u7f51\u7edc\u8bbe\u7f6e\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u89e3\u96c6\u800c\u975e\u8054\u5408\u53ef\u884c\u96c6\uff0c\u53ef\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u901a\u4fe1\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6848\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u901a\u4fe1\u9ad8\u6548\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u968f\u673a\u6620\u5c04\u7684\u76ee\u6807\u51fd\u6570\uff0c\u901a\u4fe1\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u7684\u79c1\u6709\u96c6\u5408\u4ea4\u96c6\u534f\u8bae\u3002"}}
{"id": "2510.04436", "pdf": "https://arxiv.org/pdf/2510.04436", "abs": "https://arxiv.org/abs/2510.04436", "authors": ["Jushan Chen", "Santiago Paternain"], "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u8f68\u8ff9\u4f18\u5316\u4e2d\u56e0\u591a\u6a21\u6001\u6982\u7387\u5efa\u6a21\u80fd\u529b\u53d7\u6b22\u8fce\uff0c\u4f46\u975e\u7ebf\u6027\u7b49\u5f0f\u7ea6\u675f\uff08\u52a8\u6001\u53ef\u884c\u6027\uff09\u4ecd\u662f\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u6269\u6563\u7684\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u65e0\u6295\u5f71\u673a\u5236\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u4e3a\u96f6\u4e14\u6210\u529f\u7387\u63d0\u9ad84\u500d\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f68\u8ff9\u4f18\u5316\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u53ef\u884c\u6027\u7ea6\u675f\uff0c\u5bfc\u81f4\u6b21\u4f18\u89e3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u6269\u6563\u751f\u6210\u72b6\u6001\u5e8f\u5217\uff0c\u5e76\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u68af\u5ea6\u65e0\u6295\u5f71\u673a\u5236\u4ee5\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u4e3a\u96f6\uff0c\u5728\u56db\u65cb\u7ffc\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u9ad8\u7ea64\u500d\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u52a8\u6001\u53ef\u884c\u6027\u7684\u7ea6\u675f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u4f18\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04509", "pdf": "https://arxiv.org/pdf/2510.04509", "abs": "https://arxiv.org/abs/2510.04509", "authors": ["Huanqing Wang", "Kaixiang Zhang", "Kyungjoon Lee", "Yu Mei", "Vaibhav Srivastava", "Jun Sheng", "Ziyou Song", "Zhaojian Li"], "title": "Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Data-driven control methods such as data-enabled predictive control (DeePC)\nhave shown strong potential in efficient control of soft robots without\nexplicit parametric models. However, in object manipulation tasks, unknown\nexternal payloads and disturbances can significantly alter the system dynamics\nand behavior, leading to offset error and degraded control performance. In this\npaper, we present a novel velocity-form DeePC framework that achieves robust\nand optimal control of soft robots under unknown payloads. The proposed\nframework leverages input-output data in an incremental representation to\nmitigate performance degradation induced by unknown payloads, eliminating the\nneed for weighted datasets or disturbance estimators. We validate the method\nexperimentally on a planar soft robot and demonstrate its superior performance\ncompared to standard DeePC in scenarios involving unknown payloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u901f\u5ea6\u5f62\u5f0fDeePC\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9c81\u68d2\u548c\u6700\u4f18\u63a7\u5236\u3002", "motivation": "\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u4e2d\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u5982DeePC\uff09\u867d\u9ad8\u6548\u4f46\u53d7\u672a\u77e5\u5916\u90e8\u8d1f\u8f7d\u548c\u5e72\u6270\u5f71\u54cd\uff0c\u5bfc\u81f4\u504f\u79fb\u8bef\u5dee\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u589e\u91cf\u8868\u793a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e\uff0c\u907f\u514d\u52a0\u6743\u6570\u636e\u96c6\u6216\u6270\u52a8\u4f30\u8ba1\u5668\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u77e5\u8d1f\u8f7d\u4e0b\u4f18\u4e8e\u6807\u51c6DeePC\u3002", "conclusion": "\u65b0\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u672a\u77e5\u8d1f\u8f7d\u548c\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.04585", "pdf": "https://arxiv.org/pdf/2510.04585", "abs": "https://arxiv.org/abs/2510.04585", "authors": ["Jianshu Zhou", "Jing Shu", "Tianle Pan", "Puchen Zhu", "Jiajun An", "Huayu Zhang", "Junda Huang", "Upinder Kaur", "Xin Ma", "Masayoshi Tomizuka"], "title": "Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation", "categories": ["cs.RO"], "comment": "19 pages, 10 figures, journal", "summary": "Grasping objects across vastly different sizes and physical states-including\nboth solids and liquids-with a single robotic gripper remains a fundamental\nchallenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a\nsoft end-effector that synergistically integrates distributed surface suction\nwith internal granular jamming, enabling cross-scale and cross-state\nmanipulation without requiring airtight sealing at the contact interface with\ntarget objects. The EG Gripper can handle objects with surface areas ranging\nfrom sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized\npaper and woven bag), enabling manipulation of objects nearly 3,500X smaller\nand 88X larger than its own contact area (approximated at 707 mm2 for a 30\nmm-diameter base). We further introduce a tactile sensing framework that\ncombines liquid detection and pressure-based suction feedback, enabling\nreal-time differentiation between solid and liquid targets. Guided by the\nactile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper\nautonomously selects grasping modes based on distributed pressure and voltage\nsignals. Experiments across diverse tasks-including underwater grasping,\nfragile object handling, and liquid capture-demonstrate robust and repeatable\nperformance. To our knowledge, this is the first soft gripper to reliably grasp\nboth solid and liquid objects across scales using a unified compliant\narchitecture.", "AI": {"tldr": "EG Gripper\u7ed3\u5408\u5206\u5e03\u5f0f\u8868\u9762\u5438\u529b\u548c\u5185\u90e8\u9897\u7c92\u5835\u585e\u6280\u672f\uff0c\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u548c\u8de8\u72b6\u6001\u7684\u7269\u4f53\u6293\u53d6\uff0c\u65e0\u9700\u6c14\u5bc6\u5bc6\u5c01\u3002", "motivation": "\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u4e2d\u8de8\u5c3a\u5bf8\u548c\u7269\u7406\u72b6\u6001\uff08\u56fa\u4f53\u548c\u6db2\u4f53\uff09\u6293\u53d6\u7269\u4f53\u7684\u6311\u6218\u3002", "method": "\u96c6\u6210\u5206\u5e03\u5f0f\u8868\u9762\u5438\u529b\u548c\u5185\u90e8\u9897\u7c92\u5835\u585e\u6280\u672f\uff0c\u7ed3\u5408\u89e6\u89c9\u611f\u5e94\u6846\u67b6\u548cTIGMS\u7b97\u6cd5\u3002", "result": "EG Gripper\u53ef\u6293\u53d6\u4ece0.2 mm\u00b2\u523062,000 mm\u00b2\u7684\u7269\u4f53\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u80fd\u53ef\u9760\u6293\u53d6\u8de8\u5c3a\u5ea6\u56fa\u4f53\u548c\u6db2\u4f53\u7684\u8f6f\u5939\u722a\u3002"}}
{"id": "2510.04592", "pdf": "https://arxiv.org/pdf/2510.04592", "abs": "https://arxiv.org/abs/2510.04592", "authors": ["Yilin Mei", "Peng Qiu", "Wei Zhang", "WenChao Zhang", "Wenjie Song"], "title": "MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in robotics have been largely driven by imitation learning,\nwhich depends critically on large-scale, high-quality demonstration data.\nHowever, collecting such data remains a significant challenge-particularly for\nmobile manipulators, which must coordinate base locomotion and arm manipulation\nin high-dimensional, dynamic, and partially observable environments.\nConsequently, most existing research remains focused on simpler tabletop\nscenarios, leaving mobile manipulation relatively underexplored. To bridge this\ngap, we present \\textit{MobRT}, a digital twin-based framework designed to\nsimulate two primary categories of complex, whole-body tasks: interaction with\narticulated objects (e.g., opening doors and drawers) and mobile-base\npick-and-place operations. \\textit{MobRT} autonomously generates diverse and\nrealistic demonstrations through the integration of virtual kinematic control\nand whole-body motion planning, enabling coherent and physically consistent\nexecution. We evaluate the quality of \\textit{MobRT}-generated data across\nmultiple baseline algorithms, establishing a comprehensive benchmark and\ndemonstrating a strong correlation between task success and the number of\ngenerated trajectories. Experiments integrating both simulated and real-world\ndemonstrations confirm that our approach markedly improves policy\ngeneralization and performance, achieving robust results in both simulated and\nreal-world environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6570\u5b57\u5b6a\u751f\u6846\u67b6MobRT\uff0c\u7528\u4e8e\u6a21\u62df\u590d\u6742\u5168\u8eab\u4efb\u52a1\uff08\u5982\u6253\u5f00\u95e8/\u62bd\u5c49\u548c\u79fb\u52a8\u57fa\u5ea7\u62fe\u53d6\u653e\u7f6e\uff09\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\uff0c\u4f46\u79fb\u52a8\u673a\u68b0\u81c2\u6570\u636e\u6536\u96c6\u56f0\u96be\uff0c\u73b0\u6709\u7814\u7a76\u591a\u9650\u4e8e\u7b80\u5355\u684c\u9762\u573a\u666f\u3002", "method": "\u63d0\u51faMobRT\u6846\u67b6\uff0c\u7ed3\u5408\u865a\u62df\u8fd0\u52a8\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\uff0c\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u793a\u8303\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MobRT\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u663e\u793a\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u751f\u6210\u8f68\u8ff9\u6570\u91cf\u5f3a\u76f8\u5173\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "MobRT\u586b\u8865\u4e86\u79fb\u52a8\u673a\u68b0\u81c2\u6570\u636e\u6536\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.04612", "pdf": "https://arxiv.org/pdf/2510.04612", "abs": "https://arxiv.org/abs/2510.04612", "authors": ["Simon Boche", "Jaehyung Jung", "Sebasti\u00e1n Barbas Laina", "Stefan Leutenegger"], "title": "OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS", "categories": ["cs.RO"], "comment": "IEEE Transactions on Robotics (T-RO) - Special Issue: Visual SLAM", "summary": "To empower mobile robots with usable maps as well as highest state estimation\naccuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor\nSimultaneous Localization and Mapping (SLAM) system building dense volumetric\noccupancy maps, while scalable to large environments and operating in realtime.\nOur unified SLAM framework seamlessly integrates different sensor modalities:\nvisual, inertial, measured or learned depth, LiDAR and Global Navigation\nSatellite System (GNSS) measurements. Unlike most state-of-the-art SLAM\nsystems, we advocate using dense volumetric map representations when leveraging\ndepth or range-sensing capabilities. We employ an efficient submapping strategy\nthat allows our system to scale to large environments, showcased in sequences\nof up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by\ntightly-coupling the estimator and submaps through map alignment factors. Our\nsystem provides globally consistent maps, directly usable for autonomous\nnavigation. To further improve the accuracy of OKVIS2-X, we also incorporate\nthe option of performing online calibration of camera extrinsics. Our system\nachieves the highest trajectory accuracy in EuRoC against state-of-the-art\nalternatives, outperforms all competitors in the Hilti22 VI-only benchmark,\nwhile also proving competitive in the LiDAR version, and showcases state of the\nart accuracy in the diverse and large-scale sequences from the VBR dataset.", "AI": {"tldr": "OKVIS2-X\u662f\u4e00\u4e2a\u591a\u4f20\u611f\u5668SLAM\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u9c81\u68d2\u6027\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u53ef\u7528\u7684\u5bc6\u96c6\u4f53\u79ef\u5360\u636e\u5730\u56fe\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\u5e76\u4f7f\u7528\u5bc6\u96c6\u4f53\u79ef\u5730\u56fe\u8868\u793a\uff0c\u63d0\u5347SLAM\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684\u5b50\u5730\u56fe\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5730\u56fe\u5bf9\u9f50\u56e0\u5b50\u7d27\u5bc6\u8026\u5408\u4f30\u8ba1\u5668\u548c\u5b50\u5730\u56fe\uff0c\u540c\u65f6\u652f\u6301\u5728\u7ebf\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u3002", "result": "\u5728EuRoC\u3001Hilti22\u548cVBR\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\u548c\u7ade\u4e89\u529b\u3002", "conclusion": "OKVIS2-X\u5728\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.04692", "pdf": "https://arxiv.org/pdf/2510.04692", "abs": "https://arxiv.org/abs/2510.04692", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u751f\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6a21\u62df\u96cc\u6027\u6ce2\u6591\u9e28\u7684\u5f62\u6001\u548c\u5916\u89c2\uff0c\u7528\u4e8e\u91ce\u5916\u52a8\u7269\u884c\u4e3a\u7814\u7a76\u548c\u4fdd\u62a4\u5de5\u4f5c\uff0c\u7ed3\u5408\u4e86\u6570\u5b57\u5316\u5236\u9020\u3001\u89c6\u89c9\u667a\u80fd\u548c\u751f\u6001\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u91ce\u751f\u9e1f\u7c7b\u884c\u4e3a\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u9ad8\u5ea6\u4eff\u771f\u7684\u5f62\u6001\u3001\u8010\u7528\u7684\u6237\u5916\u64cd\u4f5c\u548c\u667a\u80fd\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u9002\u5e94\u4e0d\u53ef\u63a7\u73af\u5883\u3002", "method": "\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u51493D\u626b\u63cf\u3001\u53c2\u6570\u5316CAD\u5efa\u6a21\u3001\u5173\u8282\u5f0f3D\u6253\u5370\u548cUV\u7eb9\u7406\u8d34\u56fe\u6280\u672f\uff0c\u7ed3\u5408\u516d\u8f6e\u5e95\u76d8\u548c\u5d4c\u5165\u5f0fNVIDIA Jetson\u6a21\u5757\uff0c\u5b9e\u73b0\u5b9e\u65f6\u611f\u77e5\u548c\u81ea\u4e3b\u63a7\u5236\u3002", "result": "\u5728\u6c99\u6f20\u9e1f\u820d\u7684\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u673a\u5668\u4eba\u4ee515\u81f322 FPS\u7684\u5e27\u7387\u7a33\u5b9a\u8fd0\u884c\uff0c\u5ef6\u8fdf\u4f4e\u4e8e100\u6beb\u79d2\uff0c\u5e76\u6210\u529f\u5f15\u53d1\u6ce2\u6591\u9e28\u7684\u81ea\u7136\u4e92\u52a8\u53cd\u5e94\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u4e3a\u4eff\u751f\u91ce\u5916\u673a\u5668\u4eba\u3001\u52a8\u7269\u673a\u5668\u4eba\u4e92\u52a8\u7814\u7a76\u548c\u4fdd\u62a4\u5de5\u4f5c\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u84dd\u56fe\u3002"}}
{"id": "2510.04696", "pdf": "https://arxiv.org/pdf/2510.04696", "abs": "https://arxiv.org/abs/2510.04696", "authors": ["Alexander L. Mitchell", "Joe Watson", "Ingmar Posner"], "title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "categories": ["cs.RO"], "comment": "8 pages, 6 figures, 1 table", "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6563\u5f0f\u68af\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u53cc\u81c2\u88c5\u914d\u4e2d\u7684\u89c4\u5212\u548c\u5feb\u901f\u91cd\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u9ad8\u5e8f\u5217\u5316\u548c\u63a5\u89e6\u64cd\u4f5c\u7b49\u6311\u6218\u3002", "motivation": "\u53cc\u81c2\u88c5\u914d\u4e2d\u5b58\u5728\u9ad8\u96be\u5ea6\u5e8f\u5217\u5316\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u63a5\u89e6\u64cd\u4f5c\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9002\u5e94\u5e72\u6270\u65f6\u6536\u655b\u6162\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0f\u68af\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\u751f\u6210\u5b50\u76ee\u6807\uff0c\u4ec5\u9700\u77ed\u89c6\u4f18\u5316\u800c\u975e\u957f\u65f6\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7269\u7406\u53cc\u81c2\u88c5\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u91cd\u8bd5\u3001\u534f\u8c03\u52a8\u4f5c\u548c\u81ea\u4e3b\u4ea4\u63a5\u3002", "conclusion": "\u68af\u5ea6\u6846\u67b6\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u7d27\u516c\u5dee\u88c5\u914d\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86\u9002\u5e94\u6027\u5f3a\u7684\u7279\u70b9\u3002"}}
{"id": "2510.04724", "pdf": "https://arxiv.org/pdf/2510.04724", "abs": "https://arxiv.org/abs/2510.04724", "authors": ["Etor Arza", "Welf Rehberg", "Philipp Weiss", "Mihir Kulkarni", "Kostas Alexis"], "title": "Performance-guided Task-specific Optimization for Multirotor Design", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces a methodology for task-specific design optimization of\nmultirotor Micro Aerial Vehicles. By leveraging reinforcement learning,\nBayesian optimization, and covariance matrix adaptation evolution strategy, we\noptimize aerial robot designs guided exclusively by their closed-loop\nperformance in a considered task. Our approach systematically explores the\ndesign space of motor pose configurations while ensuring manufacturability\nconstraints and minimal aerodynamic interference. Results demonstrate that\noptimized designs achieve superior performance compared to conventional\nmultirotor configurations in agile waypoint navigation tasks, including against\nfully actuated designs from the literature. We build and test one of the\noptimized designs in the real world to validate the sim2real transferability of\nour approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u4f18\u5316\u6280\u672f\u7684\u591a\u65cb\u7ffc\u5fae\u578b\u98de\u884c\u5668\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u654f\u6377\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u95ed\u73af\u6027\u80fd\u4f18\u5316\u98de\u884c\u5668\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4f20\u7edf\u591a\u65cb\u7ffc\u914d\u7f6e\u5728\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff0c\u4f18\u5316\u7535\u673a\u59ff\u6001\u914d\u7f6e\uff0c\u5e76\u8003\u8651\u5236\u9020\u7ea6\u675f\u548c\u6c14\u52a8\u5e72\u6270\u3002", "result": "\u4f18\u5316\u8bbe\u8ba1\u5728\u654f\u6377\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u591a\u65cb\u7ffc\u914d\u7f6e\uff0c\u5e76\u5728\u73b0\u5b9e\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u62df\u5230\u5b9e\u9645\u7684\u8fc1\u79fb\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4efb\u52a1\u7279\u5b9a\u98de\u884c\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.04774", "pdf": "https://arxiv.org/pdf/2510.04774", "abs": "https://arxiv.org/abs/2510.04774", "authors": ["Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf\uff08SoNS\uff09\uff0c\u4f7f\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u8f7b\u677e\u8bbe\u8ba1\u884c\u4e3a\u5e76\u5168\u5c40\u4f30\u8ba1\u7fa4\u4f53\u914d\u7f6e\u53ca\u5176\u73af\u5883\uff0c\u652f\u6301\u5728\u7ebf\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0cSoNS\u589e\u5f3a\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u5361\u4f4f\u65f6\u53ef\u4ee5\u81ea\u52a8\u8bf7\u6c42\u5e76\u8fd0\u884c\u5916\u90e8LLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u6210\u529f\u7387\u8fbe\u523085%\u3002", "motivation": "\u4e3a\u4e86\u7b80\u5316\u673a\u5668\u4eba\u7fa4\u4f53\u884c\u4e3a\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u7fa4\u4f53\u914d\u7f6e\u548c\u73af\u5883\u7684\u5168\u5c40\u4f30\u8ba1\uff0c\u540c\u65f6\u652f\u6301\u5728\u7ebf\u81ea\u52a8\u4ee3\u7801\u751f\u6210\uff0c\u63d0\u9ad8\u7fa4\u4f53\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "method": "\u91c7\u7528\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf\uff08SoNS\uff09\uff0c\u901a\u8fc7\u5916\u90e8LLM\u751f\u6210\u4ee3\u7801\uff0c\u5e76\u5728\u7ebf\u81ea\u52a8\u6267\u884c\u3002\u5b9e\u9a8c\u5305\u62ec6\u53f0\u771f\u5b9e\u673a\u5668\u4eba\u548c\u8d85\u8fc730\u53f0\u6a21\u62df\u673a\u5668\u4eba\u7684\u6d4b\u8bd5\u3002", "result": "\u5728SoNS\u652f\u6301\u4e0b\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u5361\u4f4f\u65f6\u80fd\u591f\u81ea\u52a8\u8bf7\u6c42\u5e76\u8fd0\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u4efb\u52a1\u5b8c\u6210\u6210\u529f\u7387\u8fbe\u523085%\u3002", "conclusion": "SoNS\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u81ea\u4e3b\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u5c55\u793a\u4e86\u5728\u7ebf\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04839", "pdf": "https://arxiv.org/pdf/2510.04839", "abs": "https://arxiv.org/abs/2510.04839", "authors": ["Shuo Sha", "Anupam Bhakta", "Zhenyuan Jiang", "Kevin Qiu", "Ishaan Mahajan", "Gabriel Bravo", "Brian Plancher"], "title": "TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation", "categories": ["cs.RO"], "comment": null, "summary": "Accurate online inertial parameter estimation is essential for adaptive\nrobotic control, enabling real-time adjustment to payload changes,\nenvironmental interactions, and system wear. Traditional methods such as\nRecursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to\ntrack abrupt parameter shifts or incur high computational costs, limiting their\neffectiveness in dynamic environments and for computationally constrained\nrobotic systems. As such, we introduce TAG-K, a lightweight extension of the\nKaczmarz method that combines greedy randomized row selection for rapid\nconvergence with tail averaging for robustness under noise and inconsistency.\nThis design enables fast, stable parameter adaptation while retaining the low\nper-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K\nin synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other\nKaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class\nCPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More\nimportantly, these speedups are paired with improved resilience to measurement\nnoise and a 25% reduction in estimation error, leading to nearly 2x better\nend-to-end tracking performance.", "AI": {"tldr": "TAG-K\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Kaczmarz\u65b9\u6cd5\u6269\u5c55\uff0c\u7ed3\u5408\u8d2a\u5a6a\u968f\u673a\u884c\u9009\u62e9\u548c\u5c3e\u90e8\u5e73\u5747\uff0c\u7528\u4e8e\u5728\u7ebf\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\u5bf9\u4e8e\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982RLS\u548cKF\uff09\u96be\u4ee5\u5e94\u5bf9\u53c2\u6570\u7a81\u53d8\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTAG-K\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d2a\u5a6a\u968f\u673a\u884c\u9009\u62e9\u548c\u5c3e\u90e8\u5e73\u5747\uff0c\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "result": "TAG-K\u5728\u5408\u6210\u57fa\u51c6\u548c\u56db\u65cb\u7ffc\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53471.5x-20.7x\uff0c\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e25%\uff0c\u8ddf\u8e2a\u6027\u80fd\u63d0\u9ad8\u8fd12\u500d\u3002", "conclusion": "TAG-K\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2510.04883", "pdf": "https://arxiv.org/pdf/2510.04883", "abs": "https://arxiv.org/abs/2510.04883", "authors": ["Nathan Shankar", "Pawel Ladosz", "Hujun Yin"], "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "8 pages, 8 figures", "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u53d7\u4e3b\u52a8\u53d1\u5c04\u5668\u6a21\u5f0f\u5e72\u6270\u7684\u7ea2\u5916(IR)\u6d41\u4e2d\u91cd\u5efa\u6e05\u6670\u56fe\u50cf\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u9ed1\u6697\u73af\u5883\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u7ea2\u5916\u6d41\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u6bd4RGB\u66f4\u6297\u566a\uff0c\u4f46\u53d7\u4e3b\u52a8\u53d1\u5c04\u5668\u6a21\u5f0f\u5e72\u6270\uff0c\u5f71\u54cd\u9ad8\u7ea7\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u5b9a\u4f4d\uff09\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\u91cd\u5efa\u5e72\u51c0\u7684\u7ea2\u5916\u56fe\u50cf\uff0c\u51cf\u5c11\u53d1\u5c04\u5668\u6a21\u5f0f\u7684\u5e72\u6270\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u589e\u5f3a\u6280\u672f\uff0c\u652f\u6301\u673a\u5668\u4eba\u5728\u4ece\u5149\u7167\u5145\u8db3\u5230\u6781\u4f4e\u5149\u573a\u666f\u4e0b\u7684\u53ef\u9760\u8fd0\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u56fe\u50cf\u91cd\u5efa\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u89c6\u89c9\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2510.04898", "pdf": "https://arxiv.org/pdf/2510.04898", "abs": "https://arxiv.org/abs/2510.04898", "authors": ["Zheng Xiong", "Kang Li", "Zilin Wang", "Matthew Jackson", "Jakob Foerster", "Shimon Whiteson"], "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA", "AI": {"tldr": "HyperVLA\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684Vision-Language-Action\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Vision-Language-Action\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u4ec5\u6fc0\u6d3b\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u7559\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u79cd\u7b97\u6cd5\u8bbe\u8ba1\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u6027\u80fd\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e8690\u500d\u7684\u53c2\u6570\u6fc0\u6d3b\u548c120\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "HyperVLA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u673a\u5668\u4eba\u653f\u7b56\u5b66\u4e60\u3002"}}
{"id": "2510.04991", "pdf": "https://arxiv.org/pdf/2510.04991", "abs": "https://arxiv.org/abs/2510.04991", "authors": ["D. Schwartz", "K. Kondo", "J. P. How"], "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language Models", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9ad8\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8def\u5f84\u7f29\u77ed\u7ea610%\u3002", "motivation": "\u4f20\u7edf\u63a2\u7d22\u65b9\u6cd5\u56e0\u5168\u5c40\u63a8\u7406\u80fd\u529b\u6709\u9650\u4e14\u4f9d\u8d56\u5c40\u90e8\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5e38\u5bfc\u81f4\u8def\u5f84\u4f4e\u6548\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7VLM\u7684\u76f4\u63a5\u63a8\u7406\u80fd\u529b\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c063D\u5360\u636e\u7f51\u683c\u8f6c\u5316\u4e3a\u90e8\u52062D\u5730\u56fe\uff0c\u751f\u6210\u5019\u9009\u5b50\u76ee\u6807\uff0c\u5e76\u7531VLM\u8bc4\u4f30\u6392\u5e8f\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u81f3\u5148\u8fdb\u8f68\u8ff9\u89c4\u5212\u5668DYNUS\u4e2d\u3002", "result": "VLM\u80fd\u4ece\u672a\u5b8c\u6574\u5730\u56fe\u63a8\u65ad\u7ed3\u6784\u6a21\u5f0f\uff08\u5982\u623f\u95f4\u3001\u8d70\u5eca\uff09\uff0c\u5e73\u8861\u76ee\u6807\u8fdb\u5c55\u4e0e\u672a\u77e5\u7a7a\u95f4\u98ce\u9669\uff0c\u8def\u5f84\u5e73\u5747\u7f29\u77ed\u7ea610%\u3002", "conclusion": "\u7ed3\u5408VLM\u7684\u89c4\u5212\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u8d2a\u5a6a\u7b56\u7565\u7684\u5e38\u89c1\u5931\u8d25\u3002"}}
{"id": "2510.05001", "pdf": "https://arxiv.org/pdf/2510.05001", "abs": "https://arxiv.org/abs/2510.05001", "authors": ["Aditya Sripada", "Abhishek Warrier"], "title": "Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot", "categories": ["cs.RO"], "comment": "6 pages, 10 figures. Presented at IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Robotic locomotion research typically draws from biologically inspired leg\ndesigns, yet many human-engineered settings can benefit from\nnon-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from\nInterstellar into a 0.25 m, 0.99 kg research platform with seven actuated\ndegrees of freedom. The film shows two primary gaits: a bipedal-like walk and a\nhigh-speed rolling mode. For TARS3D, we build reduced-order models for each,\nderive closed-form limit-cycle conditions, and validate the predictions on\nhardware. Experiments confirm that the robot respects its +/-150 degree hip\nlimits, alternates left-right contacts without interference, and maintains an\neight-step hybrid limit cycle in rolling mode. Because each telescopic leg\nprovides four contact corners, the rolling gait is modeled as an eight-spoke\ndouble rimless wheel. The robot's telescopic leg redundancy implies a far\nricher gait repertoire than the two limit cycles treated analytically. So, we\nused deep reinforcement learning (DRL) in simulation to search the unexplored\nspace. We observed that the learned policy can recover the analytic gaits under\nthe right priors and discover novel behaviors as well. Our findings show that\nTARS3D's fiction-inspired bio-transcending morphology can realize multiple\npreviously unexplored locomotion modes and that further learning-driven search\nis likely to reveal more. This combination of analytic synthesis and\nreinforcement learning opens a promising pathway for multimodal robotics.", "AI": {"tldr": "TARS3D\u662f\u4e00\u4e2a\u53d7\u7535\u5f71\u300a\u661f\u9645\u7a7f\u8d8a\u300b\u542f\u53d1\u7684\u975e\u751f\u7269\u5f62\u6001\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5206\u6790\u5efa\u6a21\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u591a\u79cd\u6b65\u6001\u3002", "motivation": "\u63a2\u7d22\u975e\u751f\u7269\u5f62\u6001\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u5de5\u7a0b\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "method": "\u7ed3\u5408\u4e86\u7b80\u5316\u6a21\u578b\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6765\u5f00\u53d1\u548c\u63a7\u5236\u673a\u5668\u4eba\u7684\u591a\u79cd\u6b65\u6001\u3002", "result": "\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u4e24\u79cd\u7406\u8bba\u6b65\u6001\u5e76\u901a\u8fc7DRL\u53d1\u73b0\u65b0\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u975e\u751f\u7269\u5f62\u6001\u7684\u591a\u6a21\u6001\u8fd0\u52a8\u6f5c\u529b\u3002", "conclusion": "TARS3D\u5c55\u793a\u4e86\u975e\u751f\u7269\u5f62\u6001\u673a\u5668\u4eba\u5728\u591a\u6a21\u6001\u8fd0\u52a8\u4e2d\u7684\u6f5c\u529b\uff0c\u7ed3\u5408\u5206\u6790\u4e0e\u5b66\u4e60\u7684\u65b9\u6cd5\u662f\u672a\u6765\u673a\u5668\u4eba\u7814\u7a76\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.05057", "pdf": "https://arxiv.org/pdf/2510.05057", "abs": "https://arxiv.org/abs/2510.05057", "authors": ["Mingyu Liu", "Jiuhe Shu", "Hui Chen", "Zeju Li", "Canyu Zhao", "Jiange Yang", "Shenyuan Gao", "Hao Chen", "Chunhua Shen"], "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStaMo\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u9ad8\u5ea6\u538b\u7f29\u7684\u53cc\u4ee4\u724c\u72b6\u6001\u8868\u793a\uff0c\u7ed3\u5408\u8f7b\u91cf\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3DiT\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u6f5c\u5728\u52a8\u4f5c\u7684\u81ea\u7136\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u72b6\u6001\u8868\u793a\u4e2d\u5197\u4f59\u6216\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5efa\u6a21\u4e0e\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3DiT\u89e3\u7801\u5668\uff0c\u5b66\u4e60\u4e24\u4ee4\u724c\u72b6\u6001\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u63d2\u503c\u751f\u6210\u6f5c\u5728\u52a8\u4f5c\u3002", "result": "\u5728LIBERO\u4e0a\u6027\u80fd\u63d0\u534714.3%\uff0c\u5b9e\u9645\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad830%\uff0c\u6f5c\u5728\u52a8\u4f5c\u589e\u5f3a\u7b56\u7565\u8bad\u7ec3\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd510.4%\u3002", "conclusion": "StaMo\u65b9\u6cd5\u901a\u8fc7\u9759\u6001\u56fe\u50cf\u5b66\u4e60\u901a\u7528\u8fd0\u52a8\u8868\u793a\uff0c\u51cf\u5c11\u4e86\u5bf9\u590d\u6742\u67b6\u6784\u548c\u89c6\u9891\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.05061", "pdf": "https://arxiv.org/pdf/2510.05061", "abs": "https://arxiv.org/abs/2510.05061", "authors": ["Anastasios Manganaris", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Automaton Constrained Q-Learning", "categories": ["cs.RO"], "comment": "9 pages, 4 figures, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.", "AI": {"tldr": "ACQL\u7b97\u6cd5\u7ed3\u5408\u76ee\u6807\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u3001\u8fde\u7eed\u73af\u5883\u4e2dRL\u4e0e\u65f6\u5e8f\u903b\u8f91\u7ed3\u5408\u7684\u6311\u6218\uff0c\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u65f6\u5e8f\u76ee\u6807\u4e0e\u52a8\u6001\u5b89\u5168\u7ea6\u675f\u7684\u6311\u6218\uff0c\u73b0\u6709RL\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51faACQL\u7b97\u6cd5\uff0c\u7ed3\u5408\u76ee\u6807\u6761\u4ef6\u503c\u5b66\u4e60\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u5f3a\u5316\uff0c\u652f\u6301LTL\u4efb\u52a1\u89c4\u8303\u3002", "result": "ACQL\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e6-DOF\u673a\u68b0\u81c2\u4efb\u52a1\u3002", "conclusion": "ACQL\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65f6\u5e8f\u89c4\u8303\u7684\u673a\u5668\u4eba\u5b66\u4e60\u3002"}}
{"id": "2510.05070", "pdf": "https://arxiv.org/pdf/2510.05070", "abs": "https://arxiv.org/abs/2510.05070", "authors": ["Siheng Zhao", "Yanjie Ze", "Yue Wang", "C. Karen Liu", "Pieter Abbeel", "Guanya Shi", "Rocky Duan"], "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 8 figures", "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .", "AI": {"tldr": "ResMimic\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u5b9e\u73b0\u7cbe\u786e\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u3002\u5b83\u5229\u7528GMT\u7b56\u7565\u751f\u6210\u57fa\u7840\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u7b56\u7565\u4f18\u5316\u4ee5\u63d0\u5347\u52a8\u4f5c\u7cbe\u5ea6\u548c\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eGMT\u7684\u4eba\u5f62\u673a\u5668\u4eba\u52a8\u4f5c\u8ddf\u8e2a\u7b56\u7565\u7f3a\u4e4f\u7cbe\u51c6\u6027\u548c\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u65e5\u5e38\u670d\u52a1\u548c\u4ed3\u5e93\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "ResMimic\u5206\u4e24\u9636\u6bb5\uff1aGMT\u7b56\u7565\u751f\u6210\u57fa\u7840\u52a8\u4f5c\uff0c\u6b8b\u5dee\u7b56\u7565\u4f18\u5316\u52a8\u4f5c\u5e76\u878d\u5165\u7269\u4f53\u4ea4\u4e92\uff1b\u8bbe\u8ba1\u4e86\u70b9\u4e91\u7269\u4f53\u8ffd\u8e2a\u5956\u52b1\u3001\u63a5\u89e6\u5956\u52b1\u548c\u8bfe\u7a0b\u865a\u62df\u7269\u4f53\u63a7\u5236\u5668\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9eUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cResMimic\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ResMimic\u4e3a\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u52a8\u4f5c\u7cbe\u5ea6\u548c\u4ea4\u4e92\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2510.03300", "pdf": "https://arxiv.org/pdf/2510.03300", "abs": "https://arxiv.org/abs/2510.03300", "authors": ["Shradha Bavalatti", "Yash Kangralkar", "Santosh Pattar", "Veena P Badiger"], "title": "Adaptive Cruise Control in Autonomous Vehicles: Challenges, Gaps, Comprehensive Review, and, Future Directions", "categories": ["cs.SY", "cs.RO"], "comment": null, "summary": "The development of Autonomous Vehicles (AVs) has redefined the way of\ntransportation by eliminating the need for human intervention in driving. This\nrevolution is fueled by rapid advancements in adaptive cruise control (ACC),\nwhich make AVs capable of interpreting their surroundings and responding\nintelligently. While AVs offer significant advantages, such as enhanced safety\nand improved traffic efficiency, they also face several challenges that need to\nbe addressed. Existing survey papers often lack a comprehensive analysis of\nthese challenges and their potential solutions. Our paper stands out by\nmeticulously identifying these gaps in current ACC research and offering\nimpactful future directions to guide researchers in designing next-generation\nACC systems. Our survey provides a detailed and systematic review, addressing\nthe limitations of previous studies and proposing innovative approaches to\nachieve sustainable and fault-resilient urban transportation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u5728\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u9886\u57df\u7684\u6311\u6218\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53d1\u5c55\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u5206\u6790\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u7f3a\u3002", "method": "\u901a\u8fc7\u5bf9\u73b0\u6709ACC\u7814\u7a76\u7684\u7cfb\u7edf\u56de\u987e\uff0c\u8bc6\u522b\u7814\u7a76\u4e2d\u7684\u4e0d\u8db3\u5e76\u63d0\u51fa\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u6301\u7eed\u4e14\u5177\u5907\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3ACC\u7cfb\u7edf\u7684\u8bbe\u8ba1\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8be6\u5c3d\u7684\u7efc\u8ff0\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u63a8\u52a8\u4e86ACC\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.03367", "pdf": "https://arxiv.org/pdf/2510.03367", "abs": "https://arxiv.org/abs/2510.03367", "authors": ["Zizhe Zhang", "Yicong Wang", "Zhiquan Zhang", "Tianyu Li", "Nadia Figueroa"], "title": "Viability-Preserving Passive Torque Control", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "8 pages, 7 figures, Project Website:\n  https://vpp-tc.github.io/webpage/", "summary": "Conventional passivity-based torque controllers for manipulators are\ntypically unconstrained, which can lead to safety violations under external\nperturbations. In this paper, we employ viability theory to pre-compute safe\nsets in the state-space of joint positions and velocities. These viable sets,\nconstructed via data-driven and analytical methods for self-collision\navoidance, external object collision avoidance and joint-position and\njoint-velocity limits, provide constraints on joint accelerations and thus\njoint torques via the robot dynamics. A quadratic programming-based control\nframework enforces these constraints on a passive controller tracking a\ndynamical system, ensuring the robot states remain within the safe set in an\ninfinite time horizon. We validate the proposed approach through simulations\nand hardware experiments on a 7-DoF Franka Emika manipulator. In comparison to\na baseline constrained passive controller, our method operates at higher\ncontrol-loop rates and yields smoother trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d3b\u4f53\u7406\u8bba\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u626d\u77e9\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u5b89\u5168\u96c6\u6765\u786e\u4fdd\u673a\u5668\u4eba\u5728\u5916\u529b\u6270\u52a8\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5e73\u6ed1\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u7ea6\u675f\u88ab\u52a8\u626d\u77e9\u63a7\u5236\u5668\u5728\u5916\u529b\u6270\u52a8\u4e0b\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5229\u7528\u6d3b\u4f53\u7406\u8bba\u9884\u8ba1\u7b97\u5b89\u5168\u96c6\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u72b6\u6001\u59cb\u7ec8\u5728\u5b89\u5168\u8303\u56f4\u5185\u3002", "method": "\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u89e3\u6790\u65b9\u6cd5\uff0c\u9884\u8ba1\u7b97\u5173\u8282\u4f4d\u7f6e\u548c\u901f\u5ea6\u7684\u5b89\u5168\u96c6\uff0c\u5e76\u901a\u8fc7\u4e8c\u6b21\u89c4\u5212\u63a7\u5236\u6846\u67b6\u7ea6\u675f\u88ab\u52a8\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u72b6\u6001\u65e0\u9650\u65f6\u95f4\u4fdd\u6301\u5728\u5b89\u5168\u96c6\u5185\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u63a7\u5236\u5668\u8fd0\u884c\u66f4\u5feb\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u63a7\u5236\u5668\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u3002"}}
{"id": "2510.03501", "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408YOLOv10\u548cMobileSAM\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u68c0\u6d4b\u4e0e\u5206\u5272\u63d0\u5347\u5b9e\u65f6\u6027\u80fd\uff0c\u7528\u4e8e\u81ea\u7136\u73af\u5883\u4e2d\u52a8\u7269\u68c0\u6d4b\u4e0e\u5206\u5272\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u9700\u8981\u975e\u4fb5\u5165\u6027\u76d1\u6d4b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u7269\u79cd\u9690\u853d\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528YOLOv10\u68c0\u6d4b\u548cMobileSAM\u5206\u5272\u7684\u5e76\u884c\u6846\u67b6\uff0c\u964d\u4f4e\u5ef6\u8fdf\u4ee5\u63d0\u5347\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728Houbara Bustard\u6570\u636e\u96c6\u4e0a\uff0cmAP50\u8fbe0.9627\uff0cmAP75\u4e3a0.7731\uff0cmAP95\u4e3a0.7178\uff0cMobileSAM mIoU\u4e3a0.7421\uff0cYOLOv10\u6bcf\u5e27\u8017\u65f643.7\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u9002\u7528\u4e8e\u5b9e\u65f6\u52a8\u7269\u76d1\u6d4b\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2510.03544", "pdf": "https://arxiv.org/pdf/2510.03544", "abs": "https://arxiv.org/abs/2510.03544", "authors": ["Yuji Takubo", "Daniele Gammelli", "Marco Pavone", "Simone D'Amico"], "title": "Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers", "categories": ["math.OC", "cs.AI", "cs.RO"], "comment": "14 pages, 7 figures", "summary": "Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed\ndocking, forming the foundation for a scalable space economy. Designing such\nmissions requires rapid exploration of the tradespace between control cost and\nflight time across multiple candidate targets. However, multi-objective\noptimization in this setting is challenging, as the underlying constraints are\noften highly nonconvex, and mission designers must balance accuracy (e.g.,\nsolving the full problem) with efficiency (e.g., convex relaxations), slowing\niteration and limiting design agility. To address these challenges, this paper\nproposes an AI-powered framework that enables agile mission design for a wide\nrange of Earth orbit rendezvous scenarios. Given the orbital information of the\ntarget spacecraft, boundary conditions, and a range of flight times, this work\nproposes a Transformer-based architecture that generates, in a single\nparallelized inference step, a set of near-Pareto optimal trajectories across\nvarying flight times, thereby enabling rapid mission trade studies. The model\nis further extended to accommodate variable flight times and perturbed orbital\ndynamics, supporting realistic multi-objective trade-offs. Validation on\nchance-constrained rendezvous problems with passive safety constraints\ndemonstrates that the model generalizes across both flight times and dynamics,\nconsistently providing high-quality initial guesses that converge to superior\nsolutions in fewer iterations. Moreover, the framework efficiently approximates\nthe Pareto front, achieving runtimes comparable to convex relaxation by\nexploiting parallelized inference. Together, these results position the\nproposed framework as a practical surrogate for nonconvex trajectory generation\nand mark an important step toward AI-driven trajectory design for accelerating\npreliminary mission planning in real-world rendezvous applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684Transformer\u67b6\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u591a\u76ee\u6807\u4f18\u5316\u7684\u592a\u7a7a\u4ea4\u4f1a\u4efb\u52a1\u8f68\u8ff9\uff0c\u652f\u6301\u9ad8\u6548\u7684\u4efb\u52a1\u8bbe\u8ba1\u3002", "motivation": "\u592a\u7a7a\u4ea4\u4f1a\u4efb\u52a1\u8bbe\u8ba1\u9700\u8981\u5728\u63a7\u5236\u6210\u672c\u548c\u98de\u884c\u65f6\u95f4\u4e4b\u95f4\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u4f46\u975e\u51f8\u7ea6\u675f\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u8fed\u4ee3\u6162\u3002", "method": "\u5229\u7528Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u751f\u6210\u8fd1\u4f3cPareto\u6700\u4f18\u8f68\u8ff9\uff0c\u9002\u5e94\u53ef\u53d8\u98de\u884c\u65f6\u95f4\u548c\u6270\u52a8\u8f68\u9053\u52a8\u529b\u5b66\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u6a21\u578b\u5728\u98de\u884c\u65f6\u95f4\u548c\u52a8\u529b\u5b66\u4e0a\u5747\u80fd\u6cdb\u5316\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u521d\u59cb\u89e3\uff0c\u663e\u8457\u51cf\u5c11\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u975e\u7ebf\u6027\u8f68\u8ff9\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u4efb\u52a1\u8bbe\u8ba1\u5728\u73b0\u5b9e\u4ea4\u4f1a\u5e94\u7528\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.03545", "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "categories": ["cs.CV", "cs.RO"], "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u67902D\u624b\u7ed8\u8349\u56fe\u751f\u62103D\u98de\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "motivation": "\u4e3a\u89e3\u51b3\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u5982\u4f55\u51c6\u786e\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u5e76\u751f\u6210\u5b89\u5168\u98de\u884c\u8def\u5f84\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86SketchPlan\u3002", "method": "SketchPlan\u5305\u62ecSketchAdapter\uff08\u5c06\u624b\u7ed8\u8349\u56fe\u6620\u5c04\u52302D\u8def\u5f84\uff09\u548cDiffPath\uff08\u4ece2D\u6295\u5f71\u548c\u6df1\u5ea6\u56fe\u50cf\u751f\u62103D\u8f68\u8ff9\uff09\u3002\u8bad\u7ec3\u6570\u636e\u5305\u62ec\u5408\u6210\u768432k\u8def\u5f84\u548c872\u4eba\u7c7b\u6807\u6ce8\u7684\u8349\u56fe\u3002", "result": "SketchPlan\u5728\u4f4e/\u4e2d\u5bc6\u5ea6\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff0c\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\u4e3a40%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd520-60%\u3002", "conclusion": "\u7ed3\u5408\u4eba\u7c7b\u6807\u6ce8\u4e0e\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03592", "pdf": "https://arxiv.org/pdf/2510.03592", "abs": "https://arxiv.org/abs/2510.03592", "authors": ["Kehinde O. Aina", "Sehoon Ha"], "title": "Deep Reinforcement Learning for Multi-Agent Coordination", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.RO"], "comment": "11 pages, 8 figures, 1 table, presented at SWARM 2022, to be\n  published in Journal of Artificial Life and Robotics", "summary": "We address the challenge of coordinating multiple robots in narrow and\nconfined environments, where congestion and interference often hinder\ncollective task performance. Drawing inspiration from insect colonies, which\nachieve robust coordination through stigmergy -- modifying and interpreting\nenvironmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement\nLearning (S-MADRL) framework that leverages virtual pheromones to model local\nand social interactions, enabling decentralized emergent coordination without\nexplicit communication. To overcome the convergence and scalability limitations\nof existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum\nlearning, which decomposes complex tasks into progressively harder\nsub-problems. Simulation results show that our framework achieves the most\neffective coordination of up to eight agents, where robots self-organize into\nasymmetric workload distributions that reduce congestion and modulate group\nperformance. This emergent behavior, analogous to strategies observed in\nnature, demonstrates a scalable solution for decentralized multi-agent\ncoordination in crowded environments with communication constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStigmergic\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08S-MADRL\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u4fe1\u606f\u7d20\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff0c\u89e3\u51b3\u4e86\u72ed\u7a84\u73af\u5883\u4e2d\u591a\u673a\u5668\u4eba\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u72ed\u7a84\u548c\u53d7\u9650\u73af\u5883\u4e2d\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u62e5\u5835\u548c\u5e72\u6270\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u96c6\u4f53\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u501f\u9274\u6606\u866b\u7fa4\u4f53\u7684Stigmergy\u673a\u5236\uff0c\u63d0\u51fa\u4e86S-MADRL\u6846\u67b6\uff0c\u5229\u7528\u865a\u62df\u4fe1\u606f\u7d20\u5efa\u6a21\u5c40\u90e8\u548c\u793e\u4f1a\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5206\u89e3\u590d\u6742\u4efb\u52a1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u534f\u8c03\u591a\u8fbe8\u4e2a\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e0d\u5bf9\u79f0\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u4ee5\u51cf\u5c11\u62e5\u5835\u3002", "conclusion": "S-MADRL\u6846\u67b6\u5c55\u73b0\u4e86\u5728\u62e5\u6324\u73af\u5883\u4e2d\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03823", "pdf": "https://arxiv.org/pdf/2510.03823", "abs": "https://arxiv.org/abs/2510.03823", "authors": ["Adam Haroon", "Tristan Schuler"], "title": "Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning", "categories": ["cs.LG", "cs.MA", "cs.RO"], "comment": null, "summary": "High Altitude Balloons (HABs) can leverage stratospheric wind layers for\nlimited horizontal control, enabling applications in reconnaissance,\nenvironmental monitoring, and communications networks. Existing multi-agent HAB\ncoordination approaches use deterministic methods like Voronoi partitioning and\nextremum seeking control for large global constellations, which perform poorly\nfor smaller teams and localized missions. While single-agent HAB control using\nreinforcement learning has been demonstrated on HABs, coordinated multi-agent\nreinforcement learning (MARL) has not yet been investigated. This work presents\nthe first systematic application of multi-agent reinforcement learning (MARL)\nto HAB coordination for distributed area coverage. We extend our previously\ndeveloped reinforcement learning simulation environment (RLHAB) to support\ncooperative multi-agent learning, enabling multiple agents to operate\nsimultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area\ncoverage coordination, leveraging Centralized Training with Decentralized\nExecution to address atmospheric vehicle coordination challenges. Our approach\nemploys specialized observation spaces providing individual state,\nenvironmental context, and teammate data, with hierarchical rewards\nprioritizing coverage while encouraging spatial distribution. We demonstrate\nthat QMIX achieves similar performance to the theoretically optimal geometric\ndeterministic method for distributed area coverage, validating the MARL\napproach and providing a foundation for more complex autonomous multi-HAB\nmissions where deterministic methods become intractable.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5e94\u7528\u4e8e\u9ad8\u7a7a\u6c14\u7403\uff08HABs\uff09\u7684\u5206\u5e03\u5f0f\u533a\u57df\u8986\u76d6\u534f\u8c03\u95ee\u9898\uff0c\u6269\u5c55\u4e86RLHAB\u4eff\u771f\u73af\u5883\u4ee5\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u9a8c\u8bc1\u4e86QMIX\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u786e\u5b9a\u6027\u65b9\u6cd5\uff08\u5982Voronoi\u5206\u533a\uff09\u5bf9\u5c0f\u89c4\u6a21\u56e2\u961f\u548c\u5c40\u90e8\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5df2\u8bc1\u660e\u53ef\u884c\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5c1a\u672a\u63a2\u8ba8\u3002", "method": "\u6269\u5c55RLHAB\u4eff\u771f\u73af\u5883\uff0c\u91c7\u7528QMIX\u7b97\u6cd5\uff08\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\uff09\uff0c\u8bbe\u8ba1\u4e13\u7528\u89c2\u6d4b\u7a7a\u95f4\u548c\u5206\u5c42\u5956\u52b1\u673a\u5236\u3002", "result": "QMIX\u5728\u5206\u5e03\u5f0f\u533a\u57df\u8986\u76d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u7406\u8bba\u6700\u4f18\u51e0\u4f55\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "MARL\u4e3a\u590d\u6742\u591aHAB\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u786e\u5b9a\u6027\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03827", "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "categories": ["cs.CV", "cs.RO"], "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "LIBERO-PRO\u6269\u5c55\u4e86LIBERO\u57fa\u51c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6270\u52a8\u8bc4\u4f30\u6a21\u578b\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u771f\u5b9e\u7406\u89e3\uff0c\u51c6\u786e\u6027\u4ece90%\u964d\u81f30%\u3002", "motivation": "LIBERO\u57fa\u51c6\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u504f\u9ad8\u4e14\u4e0d\u516c\u5e73\uff0c\u9700\u5f15\u5165\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5728LIBERO-PRO\u4e2d\uff0c\u901a\u8fc7\u5bf9\u7269\u4f53\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\u56db\u4e2a\u7ef4\u5ea6\u7684\u6270\u52a8\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6LIBERO\u8bc4\u4f30\u4e2d\u51c6\u786e\u7387\u8d8590%\uff0c\u4f46\u5728\u6270\u52a8\u4e0b\u5d29\u6e83\u81f30%\uff0c\u663e\u793a\u5176\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u7406\u89e3\u3002", "conclusion": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u793e\u533a\u5e94\u91c7\u7528\u66f4\u5065\u58ee\u7684\u8bc4\u6d4b\u6807\u51c6\uff0c\u5173\u6ce8\u6a21\u578b\u6cdb\u5316\u4e0e\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.03896", "pdf": "https://arxiv.org/pdf/2510.03896", "abs": "https://arxiv.org/abs/2510.03896", "authors": ["Mingyu Liu", "Zheng Huang", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Yating Wang", "Haoyi Zhu", "Hao Chen", "Chunhua Shen"], "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although Vision-Language Models (VLM) have demonstrated impressive planning\nand reasoning capabilities, translating these abilities into the physical world\nintroduces significant challenges. Conventional Vision-Language-Action (VLA)\nmodels, which integrate reasoning and action into a monolithic architecture,\ngeneralize poorly because they are constrained by scarce, narrow-domain data.\nWhile recent dual-system approaches attempt to decouple \"thinking\" from\n\"acting\", they are often constrained by semantic ambiguities within the action\nmodule. This ambiguity makes large-scale, cross-task training infeasible.\nConsequently, these systems typically necessitate fine-tuning on newly\ncollected data when deployed to novel environments, and the cooperation\nmechanism between the two systems remains ill-defined. To address these\nlimitations, we introduce, for the first time, a framework centered around a\ngeneralizable action expert. Our approach utilizes sparse 3D trajectories as an\nintermediate representation, effectively bridging the high-level planning\ncapabilities of the VLM with the low-level physical action module. During the\nplanning phase, the VLM is only required to generate coarse 3D waypoints. These\nwaypoints are then processed by our generalizable action expert, which refines\nthem into dense, executable action sequences by sampling real-time point cloud\nobservations of the environment. To promote training efficiency and robust\ngeneralization, we introduce a novel \"Action Pre-training, Pointcloud\nFine-tuning\" paradigm. Our method combines the broad generalization\ncapabilities of VLMs in visual understanding and planning with the\nfine-grained, action-level generalization of action expert.", "AI": {"tldr": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u901a\u8fc7\u5c063D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u7ed3\u5408VLM\u7684\u9ad8\u5c42\u89c4\u5212\u80fd\u529b\u548c\u52a8\u4f5c\u6a21\u5757\u7684\u4f4e\u5c42\u6267\u884c\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "motivation": "\u4f20\u7edfVLA\u6a21\u578b\u5728\u5904\u7406\u7269\u7406\u4e16\u754c\u4efb\u52a1\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u53cc\u7cfb\u7edf\u65b9\u6cd5\u53c8\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u8de8\u4efb\u52a1\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5229\u7528\u7a00\u758f3D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06VLM\u7684\u7c97\u7c92\u5ea6\u8def\u70b9\u901a\u8fc7\u52a8\u4f5c\u4e13\u5bb6\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u5e76\u63d0\u51fa\u201c\u52a8\u4f5c\u9884\u8bad\u7ec3\uff0c\u70b9\u4e91\u5fae\u8c03\u201d\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86VLM\u7684\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u548c\u52a8\u4f5c\u4e13\u5bb6\u7684\u7cbe\u7ec6\u52a8\u4f5c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u63d0\u5347VLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.03915", "pdf": "https://arxiv.org/pdf/2510.03915", "abs": "https://arxiv.org/abs/2510.03915", "authors": ["Sagar Bharadwaj", "Harrison Williams", "Luke Wang", "Michael Liang", "Tao Jin", "Srinivasan Seshan", "Anthony Rowe"], "title": "OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications", "categories": ["cs.CV", "cs.DC", "cs.RO"], "comment": null, "summary": "World-scale augmented reality (AR) applications need a ubiquitous 6DoF\nlocalization backend to anchor content to the real world consistently across\ndevices. Large organizations such as Google and Niantic are 3D scanning outdoor\npublic spaces in order to build their own Visual Positioning Systems (VPS).\nThese centralized VPS solutions fail to meet the needs of many future AR\napplications -- they do not cover private indoor spaces because of privacy\nconcerns, regulations, and the labor bottleneck of updating and maintaining 3D\nscans. In this paper, we present OpenFLAME, a federated VPS backend that allows\nindependent organizations to 3D scan and maintain a separate VPS service for\ntheir own spaces. This enables access control of indoor 3D scans, distributed\nmaintenance of the VPS backend, and encourages larger coverage. Sharding of VPS\nservices introduces several unique challenges -- coherency of localization\nresults across spaces, quality control of VPS services, selection of the right\nVPS service for a location, and many others. We introduce the concept of\nfederated image-based localization and provide reference solutions for managing\nand merging data across maps without sharing private data.", "AI": {"tldr": "OpenFLAME\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\uff08VPS\uff09\u540e\u7aef\uff0c\u89e3\u51b3\u96c6\u4e2d\u5f0f\u65b9\u6848\u5728\u9690\u79c1\u3001\u8986\u76d6\u8303\u56f4\u548c\u7ef4\u62a4\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u96c6\u4e2d\u5f0fVPS\u65e0\u6cd5\u8986\u76d6\u79c1\u4eba\u5ba4\u5185\u7a7a\u95f4\uff0c\u4e14\u5b58\u5728\u9690\u79c1\u548c\u7ef4\u62a4\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5316\u7684\u65b9\u6cd5\uff0c\u5141\u8bb8\u72ec\u7acb\u7ec4\u7ec7\u7ba1\u7406\u81ea\u5df1\u7684VPS\u670d\u52a1\u3002", "result": "\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u7684\u5b9a\u4f4d\u4e00\u81f4\u6027\u3001\u6570\u636e\u8d28\u91cf\u548c\u9009\u62e9\u4f18\u5316\u3002", "conclusion": "OpenFLAME\u4e3a\u672a\u6765AR\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u3001\u9690\u79c1\u53cb\u597d\u7684VPS\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04280", "pdf": "https://arxiv.org/pdf/2510.04280", "abs": "https://arxiv.org/abs/2510.04280", "authors": ["\u00c1lvaro Serra-Gomez", "Daniel Jarne Ornia", "Dhruva Tirumala", "Thomas Moerland"], "title": "A KL-regularization framework for learning to plan with adaptive priors", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Preprint", "summary": "Effective exploration remains a central challenge in model-based\nreinforcement learning (MBRL), particularly in high-dimensional continuous\ncontrol tasks where sample efficiency is crucial. A prominent line of recent\nwork leverages learned policies as proposal distributions for Model-Predictive\nPath Integral (MPPI) planning. Initial approaches update the sampling policy\nindependently of the planner distribution, typically maximizing a learned value\nfunction with deterministic policy gradient and entropy regularization.\nHowever, because the states encountered during training depend on the MPPI\nplanner, aligning the sampling policy with the planner improves the accuracy of\nvalue estimation and long-term performance. To this end, recent methods update\nthe sampling policy by minimizing KL divergence to the planner distribution or\nby introducing planner-guided regularization into the policy update. In this\nwork, we unify these MPPI-based reinforcement learning methods under a single\nframework by introducing Policy Optimization-Model Predictive Control (PO-MPC),\na family of KL-regularized MBRL methods that integrate the planner's action\ndistribution as a prior in policy optimization. By aligning the learned policy\nwith the planner's behavior, PO-MPC allows more flexibility in the policy\nupdates to trade off Return maximization and KL divergence minimization. We\nclarify how prior approaches emerge as special cases of this family, and we\nexplore previously unstudied variations. Our experiments show that these\nextended configurations yield significant performance improvements, advancing\nthe state of the art in MPPI-based RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPO-MPC\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408KL\u6b63\u5219\u5316\u7684MBRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c4\u5212\u5668\u7684\u884c\u4e3a\u5206\u5e03\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u6709\u6548\u63a2\u7d22\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86PO-MPC\u6846\u67b6\uff0c\u901a\u8fc7KL\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u5c06\u89c4\u5212\u5668\u7684\u884c\u52a8\u5206\u5e03\u7eb3\u5165\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u7edf\u4e00\u4e86\u73b0\u6709\u7684MPPI-based\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPO-MPC\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "conclusion": "PO-MPC\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u548c\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3MBRL\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04333", "pdf": "https://arxiv.org/pdf/2510.04333", "abs": "https://arxiv.org/abs/2510.04333", "authors": ["Lan Feng", "Yang Gao", "Eloi Zablocki", "Quanyi Li", "Wuyang Li", "Sichao Liu", "Matthieu Cord", "Alexandre Alahi"], "title": "RAP: 3D Rasterization Augmented End-to-End Planning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea73D\u5149\u6805\u5316\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u6062\u590d\u6570\u636e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u4e13\u5bb6\u6f14\u793a\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u7f3a\u4e4f\u6062\u590d\u6570\u636e\u5bb9\u6613\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u795e\u7ecf\u6e32\u67d3\u6216\u6e38\u620f\u5f15\u64ce\u6210\u672c\u9ad8\u4e14\u6162\u3002\u8bba\u6587\u8ba4\u4e3a\u9a7e\u9a76\u89c4\u5212\u7684\u5173\u952e\u662f\u51e0\u4f55\u548c\u52a8\u6001\uff0c\u800c\u975e\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u7531\u6b64\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86RAP\u65b9\u6cd5\uff0c\u5305\u62ec3D\u5149\u6805\u5316\u751f\u6210\u5408\u6210\u89c6\u56fe\u548cRaster-to-Real\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff0c\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u6570\u636e\u589e\u5f3a\u548c\u8de8\u9886\u57df\u8fc1\u79fb\u3002", "result": "RAP\u5728NAVSIM v1/v2\u3001Waymo Open Dataset\u7b49\u56db\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u95ed\u73af\u9c81\u68d2\u6027\u548c\u957f\u5c3e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u5149\u6805\u5316\u7ed3\u5408\u7279\u5f81\u5bf9\u9f50\u8db3\u4ee5\u53d6\u4ee3\u9ad8\u6210\u672c\u7684\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u4e3a\u7aef\u5230\u7aef\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04532", "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DriveMind\u6570\u636e\u96c6\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u7814\u7a76\u4e86VLM\u9a7e\u9a76\u4ee3\u7406\u7684\u63a8\u7406\u4e0e\u89c4\u5212\u95f4\u7684\u56e0\u679c\u8131\u8282\uff0c\u53d1\u73b0\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u800c\u975e\u63a8\u7406\u3002", "motivation": "\u63a2\u7a76VLM\u9a7e\u9a76\u4ee3\u7406\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u662f\u5426\u771f\u5b9e\u9a71\u52a8\u8f68\u8ff9\u89c4\u5212\uff0c\u9a8c\u8bc1\u56e0\u679c\u5047\u8bbe\u3002", "method": "\u6784\u5efaDriveMind\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4fe1\u606f\u6d88\u878d\u5b9e\u9a8c\u8bad\u7ec3VLM\u4ee3\u7406\uff0c\u8bc4\u4f30\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u5173\u8054\u6027\u3002", "result": "\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u4fe1\u606f\uff0c\u63a8\u7406\u5bf9\u89c4\u5212\u5f71\u54cd\u5fae\u5f31\uff0c\u63d0\u51fa\u63a8\u7406-\u89c4\u5212\u8131\u8282\u5047\u8bbe\u3002", "conclusion": "\u8bba\u6587\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8bc4\u4f30\u6a21\u578b\u7684\u56e0\u679c\u771f\u5b9e\u6027\u3002"}}
{"id": "2510.04666", "pdf": "https://arxiv.org/pdf/2510.04666", "abs": "https://arxiv.org/abs/2510.04666", "authors": ["Zhimin Hou", "Jiacheng Hou", "Xiao Chen", "Hamid Sadeghian", "Tianyu Ren", "Sami Haddadin"], "title": "Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from Therapist-informed Input", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Therapist-in-the-loop robotic rehabilitation has shown great promise in\nenhancing rehabilitation outcomes by integrating the strengths of therapists\nand robotic systems. However, its broader adoption remains limited due to\ninsufficient safe interaction and limited adaptation capability. This article\nproposes a novel telerobotics-mediated framework that enables therapists to\nintuitively and safely deliver assist-as-needed~(AAN) therapy based on two\nprimary contributions. First, our framework encodes the therapist-informed\ncorrective force into via-points in a latent space, allowing the therapist to\nprovide only minimal assistance while encouraging patient maintaining own\nmotion preferences. Second, a shape-adaptive ANN rehabilitation policy is\nlearned to partially and progressively deform the reference trajectory for\nmovement therapy based on encoded patient motion preferences and\ntherapist-informed via-points. The effectiveness of the proposed shape-adaptive\nAAN strategy was validated on a telerobotic rehabilitation system using two\nrepresentative tasks. The results demonstrate its practicality for remote AAN\ntherapy and its superiority over two state-of-the-art methods in reducing\ncorrective force and improving movement smoothness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8fdc\u7a0b\u673a\u5668\u4eba\u5eb7\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u7801\u6cbb\u7597\u5e08\u7684\u77eb\u6b63\u529b\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u70b9\uff0c\u5e76\u5b66\u4e60\u5f62\u72b6\u81ea\u9002\u5e94\u7684\u8f85\u52a9\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5eb7\u590d\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u5eb7\u590d\u4e2d\u5b89\u5168\u4e92\u52a8\u4e0d\u8db3\u548c\u9002\u5e94\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u5eb7\u590d\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u5f84\u70b9\u7f16\u7801\u548c\u5f62\u72b6\u81ea\u9002\u5e94\u8f85\u52a9\u7b56\u7565\u7684\u8fdc\u7a0b\u673a\u5668\u4eba\u5eb7\u590d\u6846\u67b6\u3002", "result": "\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u5176\u5728\u51cf\u5c11\u77eb\u6b63\u529b\u548c\u63d0\u9ad8\u8fd0\u52a8\u6d41\u7545\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fdc\u7a0b\u8f85\u52a9\u5eb7\u590d\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.04807", "pdf": "https://arxiv.org/pdf/2510.04807", "abs": "https://arxiv.org/abs/2510.04807", "authors": ["Alex Rose", "Naman Aggarwal", "Christopher Jewison", "Jonathan P. How"], "title": "Efficient Probabilistic Planning with Maximum-Coverage Distributionally Robust Backward Reachable Trees", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "This paper presents a new multi-query motion planning algorithm for linear\nGaussian systems with the goal of reaching a Euclidean ball with high\nprobability. We develop a new formulation for ball-shaped ambiguity sets of\nGaussian distributions and leverage it to develop a distributionally robust\nbelief roadmap construction algorithm. This algorithm synthe- sizes robust\ncontrollers which are certified to be safe for maximal size ball-shaped\nambiguity sets of Gaussian distributions. Our algorithm achieves better\ncoverage than the maximal coverage algorithm for planning over Gaussian\ndistributions [1], and we identify mild conditions under which our algorithm\nachieves strictly better coverage. For the special case of no process noise or\nstate constraints, we formally prove that our algorithm achieves maximal\ncoverage. In addition, we present a second multi-query motion planning\nalgorithm for linear Gaussian systems with the goal of reaching a region\nparameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with\nhigh probability. This algorithm plans over ellipsoidal sets of maximal size\nball-shaped ambiguity sets of Gaussian distributions, and provably achieves\nequal or better coverage than the best-known algorithm for planning over\nellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the\nefficacy of both methods in a wide range of conditions via extensive simulation\nexperiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u591a\u67e5\u8be2\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u5206\u522b\u9488\u5bf9\u7ebf\u6027\u9ad8\u65af\u7cfb\u7edf\u5230\u8fbe\u6b27\u51e0\u91cc\u5f97\u7403\u548c\u9ad8\u6982\u7387\u533a\u57df\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8986\u76d6\u8303\u56f4\u548c\u5b89\u5168\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u5f00\u53d1\u80fd\u591f\u5728\u9ad8\u6982\u7387\u4e0b\u5b89\u5168\u5230\u8fbe\u76ee\u6807\u533a\u57df\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9ad8\u65af\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u8986\u76d6\u8303\u56f4\u548c\u9c81\u68d2\u6027\u3002", "method": "1. \u5f00\u53d1\u4e86\u7403\u72b6\u9ad8\u65af\u5206\u5e03\u7684\u65b0\u5f62\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u5206\u5e03\u9c81\u68d2\u7684\u4fe1\u5ff5\u8def\u7ebf\u56fe\u7b97\u6cd5\u30022. \u63d0\u51fa\u4e86\u7b2c\u4e8c\u79cd\u7b97\u6cd5\uff0c\u9488\u5bf9\u7ebf\u6027\u9ad8\u65af\u7cfb\u7edf\u7684\u9ad8\u6982\u7387\u533a\u57df\u8fd0\u52a8\u89c4\u5212\uff0c\u5229\u7528\u692d\u7403\u72b6\u6700\u5927\u7403\u6a21\u7cca\u96c6\u3002", "result": "\u4e24\u79cd\u7b97\u6cd5\u5728\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7b2c\u4e00\u79cd\u5728\u65e0\u8fc7\u7a0b\u566a\u58f0\u6216\u65e0\u72b6\u6001\u7ea6\u675f\u7684\u7279\u6b8a\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u5927\u8986\u76d6\u8303\u56f4\uff0c\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5728\u692d\u7403\u6a21\u7cca\u96c6\u4e0b\u7684\u8986\u76d6\u8303\u56f4\u4f18\u4e8e\u5df2\u77e5\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9ad8\u65af\u7cfb\u7edf\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
