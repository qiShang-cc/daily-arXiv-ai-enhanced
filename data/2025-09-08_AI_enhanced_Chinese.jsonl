{"id": "2509.04576", "pdf": "https://arxiv.org/pdf/2509.04576", "abs": "https://arxiv.org/abs/2509.04576", "authors": ["Ce Zheng", "Tingting Yang"], "title": "Communication-Efficient Collaborative LLM Inference via Distributed Speculative Decoding", "categories": ["eess.SP"], "comment": "Accepted in the Seventeenth International Conference on Wireless\n  Communications and Signal Processing Oct. 23-25, 2025", "summary": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTop-K\u7a00\u758f\u5bf9\u6570\u4f20\u8f93\uff08TK-SLT\uff09\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u4ec5\u4f20\u8f93\u524dK\u4e2atoken\u7684\u6982\u7387\u548c\u7d22\u5f15\uff0c\u51cf\u5c11\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5728AI-RAN\u4e2d\uff0c\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u9700\u8981\u4f20\u8f93\u5b8c\u6574\u7684\u8bcd\u6c47\u6982\u7387\u5206\u5e03\uff0c\u5bfc\u81f4\u4e0a\u884c\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86TK-SLT\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4ec5\u4f20\u8f93\u524dK\u4e2atoken\u7684\u539f\u59cb\u6982\u7387\u548c\u5bf9\u5e94\u7684\u7d22\u5f15\uff0c\u66ff\u4ee3\u5b8c\u6574\u7684\u6982\u7387\u5206\u5e03\u4f20\u8f93\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\u6d88\u8017\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u6700\u5927\u5316\u63a8\u7406\u541e\u5410\u91cf\u7684\u6700\u4f18\u8349\u6848\u957f\u5ea6\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86TK-SLT\u7684\u53ef\u5b9e\u73b0\u52a0\u901f\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TK-SLT\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.04692", "pdf": "https://arxiv.org/pdf/2509.04692", "abs": "https://arxiv.org/abs/2509.04692", "authors": ["Michael Shifrin", "Joseph Tabrikian", "Igal Bilik"], "title": "Tangential Velocity Estimation Using Near-Field Automotive Radar Model", "categories": ["eess.SP"], "comment": null, "summary": "This work investigates the problem of tangential velocity estimation in\nautomotive radar systems, addressing the limitations of conventionally\nconsidered models. Conventional automotive radars are usually based on\nfar-field models and estimate the target's range, radial velocity, and\ndirection-of-arrival (DOA) but are not able to estimate the tangential\ncomponent of the target 2-D velocity, which is a critical parameter for\nreliable perception of dynamic environments. To address this challenge, we\nintroduce the near-field radar model, which considers various migration\nelements in range, radial velocity, and Doppler along time and space.\nConventionally, these migration effects result in smearing of the likelihood\nfunction for estimating the target parameters. However, if the model is\ncorrectly specified, these migration effects are informative for tangential\nvelocity estimation. We conduct an identifiability analysis for tangential\nvelocity estimation using the Cram\\'er-Rao bound and ambiguity function. The\ninsights from this study motivate the use of a separated array configuration\nand the development of a computationally efficient maximum likelihood based\nalgorithm designed to utilize target migrations for tangential velocity\nestimation, while maintaining practical computational complexity. In addition\nto tangential velocity estimation, the proposed algorithm mitigates likelihood\nsmearing in range, radial velocity, and Doppler. Simulations validate the\ntheoretical feasibility study, and evaluate the algorithms' performance in both\nsingle- and multi-target scenarios. The proposed approach improves the accuracy\nand reliability of automotive radars, enhancing situational awareness for\nadvanced driver assistance systems and autonomous vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u5207\u5411\u901f\u5ea6\u7684\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8fdc\u573a\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u6c7d\u8f66\u96f7\u8fbe\u65e0\u6cd5\u4f30\u8ba1\u76ee\u6807\u7684\u5207\u5411\u901f\u5ea6\uff0c\u800c\u8fd9\u4e00\u53c2\u6570\u5bf9\u52a8\u6001\u73af\u5883\u7684\u53ef\u9760\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\uff0c\u5206\u6790\u53ef\u8bc6\u522b\u6027\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u6700\u5927\u4f3c\u7136\u7b97\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u8fc1\u79fb\u6548\u5e94\u4f30\u8ba1\u5207\u5411\u901f\u5ea6\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u53ef\u884c\u6027\uff0c\u7b97\u6cd5\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6c7d\u8f66\u96f7\u8fbe\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u589e\u5f3a\u4e86\u5148\u8fdb\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.04768", "pdf": "https://arxiv.org/pdf/2509.04768", "abs": "https://arxiv.org/abs/2509.04768", "authors": ["Yilong Chen", "Zixiang Ren", "Jie Xu", "Rui Zhang"], "title": "Environment-Aware IRS Deployment via Channel Knowledge Map: Joint Sensing-Communications Coverage Optimization", "categories": ["eess.SP"], "comment": "13 pages, 11 figures", "summary": "This paper studies the intelligent reflecting surface (IRS) deployment\noptimization problem for IRS-enabled integrated sensing and communications\n(ISAC) systems, in which multiple IRSs are strategically deployed at candidate\nlocations to assist a base station (BS) to enhance the coverage of both sensing\nand communications. We present an environment-aware IRS deployment design via\nexploiting the channel knowledge map (CKM), which provides the channel state\ninformation (CSI) between each candidate IRS location and BS or targeted\nsensing/communication points. Based on the obtained CSI from CKM, we optimize\nthe deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'\nreflective beamforming during operation, with the objective of minimizing the\nsystem cost, while guaranteeing the minimum illumination power requirements at\nsensing areas and the minimum signal-to-noise ratio (SNR) requirements at\ncommunication areas. In particular, we consider two cases when the IRSs'\nreflective beamforming optimization can be implemented dynamically in real time\nand quasi-stationarily over the whole operation period, respectively. For both\ncases, the joint IRS deployment and transmit/reflective beamforming designs are\nformulated as mixed-integer non-convex optimization problems, which are solved\nvia the successive convex approximation (SCA)-based relax-and-bound method.\nSpecifically, we first relax the binary IRS deployment indicators into\ncontinuous variables, then find converged solutions via SCA, and finally round\nrelaxed indicators back to binary values. Numerical results demonstrate the\neffectiveness of our proposed algorithms in reducing the system cost while\nmeeting the sensing and communication requirements.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u667a\u80fd\u53cd\u5c04\u9762\uff08IRS\uff09\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u901a\u9053\u77e5\u8bc6\u56fe\uff08CKM\uff09\u4f18\u5316IRS\u90e8\u7f72\u3001\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cIRS\u53cd\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u6210\u672c\u5e76\u6ee1\u8db3\u4f20\u611f\u4e0e\u901a\u4fe1\u9700\u6c42\u3002", "motivation": "\u5728IRS\u652f\u6301\u7684ISAC\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316IRS\u90e8\u7f72\u6765\u589e\u5f3a\u57fa\u7ad9\u7684\u53cc\u91cd\u8986\u76d6\uff08\u4f20\u611f\u4e0e\u901a\u4fe1\uff09\u662f\u5f53\u524d\u7814\u7a76\u7684\u91cd\u70b9\u3002", "method": "\u57fa\u4e8eCKM\u83b7\u53d6\u901a\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u63d0\u51fa\u6df7\u5408\u6574\u6570\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eSCA\u7684\u677e\u5f1b-\u8fb9\u754c\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\uff0c\u540c\u65f6\u6ee1\u8db3\u4f20\u611f\u4e0e\u901a\u4fe1\u7684\u6700\u5c0f\u529f\u7387\u548c\u4fe1\u566a\u6bd4\u8981\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684IRS\u90e8\u7f72\u548c\u6ce2\u675f\u6210\u5f62\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u5b9e\u65f6\u548c\u51c6\u9759\u6001\u60c5\u51b5\u4e0b\u5747\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.04787", "pdf": "https://arxiv.org/pdf/2509.04787", "abs": "https://arxiv.org/abs/2509.04787", "authors": ["Zhidi Zhang", "Rui Meng", "Song Gao", "Haixiao Gao", "Xiaodong Xu"], "title": "SREC: Encrypted Semantic Super-Resolution Enhanced Communication", "categories": ["eess.SP"], "comment": "7 pages, 6 figures, conference", "summary": "Semantic communication (SemCom), as a typical paradigm of deep integration\nbetween artificial intelligence (AI) and communication technology,\nsignificantly improves communication efficiency and resource utilization\nefficiency. However, the security issues of SemCom are becoming increasingly\nprominent. Semantic features transmitted in plaintext over physical channels\nare easily intercepted by eavesdroppers. To address this issue, this paper\nproposes Encrypted Semantic Super-Resolution Enhanced Communication (SREC) to\nsecure SemCom. SREC uses the modulo-256 encryption method to encrypt semantic\nfeatures, and employs super-resolution reconstruction method to improve the\nreconstruction quality of images. The simulation results show that in the\nadditive Gaussian white noise (AWGN) channel, when different modulation methods\nare used, SREC can not only stably guarantee security, but also achieve better\ntransmission performance under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u5bc6\u7684\u8bed\u4e49\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u901a\u4fe1\uff08SREC\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u6a21256\u52a0\u5bc6\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6280\u672f\u63d0\u5347\u901a\u4fe1\u5b89\u5168\u6027\u548c\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u8bed\u4e49\u901a\u4fe1\uff08SemCom\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f8b\u5982\u901a\u8fc7\u7269\u7406\u4fe1\u9053\u660e\u4f20\u8f93\u7684\u8bed\u4e49\u7279\u5f81\u5bb9\u6613\u88ab\u7a83\u542c\u8005\u62e6\u622a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u901a\u4fe1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7684SREC\u65b9\u6cd5\u7ed3\u5408\u4e86\u6a21256\u52a0\u5bc6\u6280\u672f\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u52a0\u5bc6\u8bed\u4e49\u7279\u5f81\u548c\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u6765\u589e\u5f3a\u901a\u4fe1\u5b89\u5168\u6027\u3002", "result": "\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\uff08AWGN\uff09\u4fe1\u9053\u4e2d\uff0cSREC\u5728\u4e0d\u540c\u8c03\u5236\u65b9\u6cd5\u4e0b\u4e0d\u4ec5\u80fd\u7a33\u5b9a\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u8fd8\u80fd\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u4f20\u8f93\u6027\u80fd\u3002", "conclusion": "SREC\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b89\u5168\u8bed\u4e49\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.04535", "pdf": "https://arxiv.org/pdf/2509.04535", "abs": "https://arxiv.org/abs/2509.04535", "authors": ["Minjong Yoo", "Woo Kyung Kim", "Honguk Woo"], "title": "In-Context Policy Adaptation via Cross-Domain Skill Diffusion", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages", "summary": "In this work, we present an in-context policy adaptation (ICPAD) framework\ndesigned for long-horizon multi-task environments, exploring diffusion-based\nskill learning techniques in cross-domain settings. The framework enables rapid\nadaptation of skill-based reinforcement learning policies to diverse target\ndomains, especially under stringent constraints on no model updates and only\nlimited target domain data. Specifically, the framework employs a cross-domain\nskill diffusion scheme, where domain-agnostic prototype skills and a\ndomain-grounded skill adapter are learned jointly and effectively from an\noffline dataset through cross-domain consistent diffusion processes. The\nprototype skills act as primitives for common behavior representations of\nlong-horizon policies, serving as a lingua franca to bridge different domains.\nFurthermore, to enhance the in-context adaptation performance, we develop a\ndynamic domain prompting scheme that guides the diffusion-based skill adapter\ntoward better alignment with the target domain. Through experiments with\nrobotic manipulation in Metaworld and autonomous driving in CARLA, we show that\nour $\\oursol$ framework achieves superior policy adaptation performance under\nlimited target domain data conditions for various cross-domain configurations\nincluding differences in environment dynamics, agent embodiment, and task\nhorizon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6280\u80fd\u7684\u4e0a\u4e0b\u6587\u7b56\u7565\u9002\u5e94\u6846\u67b6\uff08ICPAD\uff09\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u957f\u65f6\u7a0b\u73af\u5883\uff0c\u652f\u6301\u5728\u6709\u9650\u76ee\u6807\u57df\u6570\u636e\u548c\u65e0\u6a21\u578b\u66f4\u65b0\u7684\u6761\u4ef6\u4e0b\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u7814\u7a76\u5728\u8de8\u57df\u8bbe\u7f6e\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u6269\u6563\u6280\u80fd\u5b66\u4e60\u6280\u672f\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5feb\u901f\u9002\u5e94\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u4e0d\u53ef\u66f4\u65b0\u548c\u76ee\u6807\u57df\u6570\u636e\u6709\u9650\u7684\u4e25\u683c\u7ea6\u675f\u4e0b\u3002", "method": "\u91c7\u7528\u8de8\u57df\u6280\u80fd\u6269\u6563\u65b9\u6848\uff0c\u5b66\u4e60\u57df\u65e0\u5173\u7684\u539f\u578b\u6280\u80fd\u548c\u57df\u57fa\u7840\u7684\u6280\u80fd\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u57df\u63d0\u793a\u65b9\u6848\u4f18\u5316\u9002\u914d\u6027\u80fd\u3002", "result": "\u5728Metaworld\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548cCARLA\u7684\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u4e2d\uff0cICPAD\u5728\u591a\u79cd\u8de8\u57df\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7b56\u7565\u9002\u5e94\u6027\u80fd\u3002", "conclusion": "ICPAD\u6846\u67b6\u4e3a\u957f\u65f6\u7a0b\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u7684\u7b56\u7565\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.04801", "pdf": "https://arxiv.org/pdf/2509.04801", "abs": "https://arxiv.org/abs/2509.04801", "authors": ["Dayu Fan", "Rui Meng", "Song Gao", "Xiaodong Xu"], "title": "KGRAG-SC: Knowledge Graph RAG-Assisted Semantic Communication", "categories": ["eess.SP"], "comment": "7 pages,4 figures,conference", "summary": "The state-of-the-art semantic communication (SC) schemes typically rely on\nend-to-end deep learning frameworks that lack interpretability and struggle\nwith robust semantic selection and reconstruction under noisy conditions. To\naddress this issue, this paper presents KGRAG-SC, a knowledge graph-assisted SC\nframework that leverages retrieval-augmented generation principles. KGRAG-SC\nemploys a multi-dimensional knowledge graph, enabling efficient semantic\nextraction through community-guided entity linking and GraphRAG-assisted\nprocessing. The transmitter constructs minimal connected subgraphs that capture\nessential semantic relationships and transmits only compact entity indices\nrather than full text or semantic triples. An importance-aware adaptive\ntransmission strategy provides unequal error protection based on structural\ncentrality metrics, prioritizing critical semantic elements under adverse\nchannel conditions. At the receiver, large language models perform\nknowledge-driven text reconstruction using the shared knowledge graph as\nstructured context, ensuring robust semantic recovery even with partial\ninformation loss. Experimental results demonstrate that KGRAG-SC achieves\nsuperior semantic fidelity in low Signal-to-Noise Ratio (SNR) conditions while\nsignificantly reducing transmission overhead compared to traditional\ncommunication methods, highlighting the effectiveness of integrating structured\nknowledge representation with generative language models for SC systems.", "AI": {"tldr": "KGRAG-SC\u662f\u4e00\u79cd\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u89e3\u51b3\u4e86\u73b0\u6709\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u63d0\u53d6\u548c\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u65b9\u6848\u4f9d\u8d56\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u591a\u7ef4\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u8bed\u4e49\u63d0\u53d6\uff0c\u91c7\u7528\u793e\u533a\u5f15\u5bfc\u7684\u5b9e\u4f53\u94fe\u63a5\u548cGraphRAG\u8f85\u52a9\u5904\u7406\uff0c\u4f20\u8f93\u6700\u5c0f\u8fde\u63a5\u5b50\u56fe\u548c\u7d27\u51d1\u5b9e\u4f53\u7d22\u5f15\uff0c\u5e76\u7ed3\u5408\u91cd\u8981\u6027\u611f\u77e5\u7684\u81ea\u9002\u5e94\u4f20\u8f93\u7b56\u7565\u3002", "result": "\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f20\u8f93\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u751f\u6210\u8bed\u8a00\u6a21\u578b\u53ef\u6709\u6548\u63d0\u5347\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.04628", "pdf": "https://arxiv.org/pdf/2509.04628", "abs": "https://arxiv.org/abs/2509.04628", "authors": ["Alejandro Posadas-Nava", "Andrea Scorsoglio", "Luca Ghilardi", "Roberto Furfaro", "Richard Linares"], "title": "Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 6 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We present an imitation learning approach for spacecraft guidance,\nnavigation, and control(GNC) that achieves high performance from limited data.\nUsing only 100 expert demonstrations, equivalent to 6,300 environment\ninteractions, our method, which implements Action Chunking with Transformers\n(ACT), learns a control policy that maps visual and state observations to\nthrust and torque commands. ACT generates smoother, more consistent\ntrajectories than a meta-reinforcement learning (meta-RL) baseline trained with\n40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking\nwith the International Space Station (ISS). We show that our approach achieves\ngreater accuracy, smoother control, and greater sample efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u822a\u5929\u5668\u5bfc\u822a\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u9700100\u6b21\u4e13\u5bb6\u6f14\u793a\uff08\u76f8\u5f53\u4e8e6300\u6b21\u73af\u5883\u4ea4\u4e92\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u822a\u5929\u5668\u5bfc\u822a\u63a7\u5236\u3002", "method": "\u91c7\u7528Action Chunking with Transformers (ACT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u72b6\u6001\u89c2\u6d4b\u6620\u5c04\u63a8\u529b\u4e0e\u626d\u77e9\u6307\u4ee4\u3002", "result": "ACT\u5728ISS\u5bf9\u63a5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5143\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u3001\u66f4\u51c6\u786e\u4e14\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "ACT\u65b9\u6cd5\u5728\u822a\u5929\u5668\u5bfc\u822a\u63a7\u5236\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u7684\u9ad8\u6027\u80fd\u63a7\u5236\u573a\u666f\u3002"}}
{"id": "2509.04803", "pdf": "https://arxiv.org/pdf/2509.04803", "abs": "https://arxiv.org/abs/2509.04803", "authors": ["Song Gao", "Rui Meng", "Xiaodong Xu", "Haixiao Gao", "Yiming Liu", "Chenyuan Feng", "Ping Zhang", "Tony Q. S. Quek", "Dusit Niyato"], "title": "SemSteDiff: Generative Diffusion Model-based Coverless Semantic Steganography Communication", "categories": ["eess.SP"], "comment": "13 pages, 11 figures", "summary": "Semantic communication (SemCom), as a novel paradigm for future communication\nsystems, has recently attracted much attention due to its superiority in\ncommunication efficiency. However, similar to traditional communication, it\nalso suffers from eavesdropping threats. Intelligent eavesdroppers could launch\nadvanced semantic analysis techniques to infer secret semantic information.\nTherefore, some researchers have designed Semantic Steganography Communication\n(SemSteCom) scheme to confuse semantic eavesdroppers. However, the\nstate-of-the-art SemSteCom schemes for image transmission rely on the\npre-selected cover image, which limits the universality. To address this issue,\nwe propose a Generative Diffusion Model-based Coverless Semantic Steganography\nCommunication (SemSteDiff) scheme to hide secret images into generated stego\nimages. The semantic related private and public keys enable legitimate receiver\nto decode secret images correctly while the eavesdropper without completely\ntrue key-pairs fail to obtain them. Simulation results demonstrate the\neffectiveness of the plug-and-play design in different Joint Source-Channel\nCoding (JSCC) frameworks. The comparison results under different eavesdroppers'\nthreats show that, when Signal-to-Noise Ratio (SNR) = 0 dB, the peak\nsignal-to-noise ratio (PSNR) of the legitimate receiver is 4.14 dB higher than\nthat of the eavesdropper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\u7684SemSteDiff\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u5bf9\u9884\u9009\u5c01\u9762\u56fe\u50cf\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u901a\u7528\u6027\u3002", "motivation": "\u8bed\u4e49\u901a\u4fe1\u867d\u9ad8\u6548\u4f46\u6613\u53d7\u7a83\u542c\u5a01\u80c1\uff0c\u73b0\u6709\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u65b9\u6848\u4f9d\u8d56\u9884\u9009\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faSemSteDiff\u65b9\u6848\uff0c\u901a\u8fc7\u751f\u6210\u6269\u6563\u6a21\u578b\u5c06\u79d8\u5bc6\u56fe\u50cf\u5d4c\u5165\u751f\u6210\u7684\u9690\u5199\u56fe\u50cf\u4e2d\uff0c\u5229\u7528\u8bed\u4e49\u76f8\u5173\u7684\u516c\u79c1\u94a5\u5bf9\u786e\u4fdd\u5408\u6cd5\u63a5\u6536\u8005\u6b63\u786e\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSNR\u4e3a0 dB\u65f6\uff0c\u5408\u6cd5\u63a5\u6536\u8005\u7684PSNR\u6bd4\u7a83\u542c\u8005\u9ad84.14 dB\u3002", "conclusion": "SemSteDiff\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u79cdJSCC\u6846\u67b6\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.04645", "pdf": "https://arxiv.org/pdf/2509.04645", "abs": "https://arxiv.org/abs/2509.04645", "authors": ["Kallol Saha", "Amber Li", "Angela Rodriguez-Izquierdo", "Lifan Yu", "Ben Eisner", "Maxim Likhachev", "David Held"], "title": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement", "categories": ["cs.RO"], "comment": "Conference on Robot Learning (CoRL) 2025\n  (https://planning-from-point-clouds.github.io/)", "summary": "Long-horizon planning for robot manipulation is a challenging problem that\nrequires reasoning about the effects of a sequence of actions on a physical 3D\nscene. While traditional task planning methods are shown to be effective for\nlong-horizon manipulation, they require discretizing the continuous state and\naction space into symbolic descriptions of objects, object relationships, and\nactions. Instead, we propose a hybrid learning-and-planning approach that\nleverages learned models as domain-specific priors to guide search in\nhigh-dimensional continuous action spaces. We introduce SPOT: Search over Point\ncloud Object Transformations, which plans by searching for a sequence of\ntransformations from an initial scene point cloud to a goal-satisfying point\ncloud. SPOT samples candidate actions from learned suggesters that operate on\npartially observed point clouds, eliminating the need to discretize actions or\nobject relationships. We evaluate SPOT on multi-object rearrangement tasks,\nreporting task planning success and task execution success in both simulation\nand real-world environments. Our experiments show that SPOT generates\nsuccessful plans and outperforms a policy-learning approach. We also perform\nablations that highlight the importance of search-based planning.", "AI": {"tldr": "SPOT\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u548c\u89c4\u5212\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u70b9\u4e91\u53d8\u6362\u5e8f\u5217\u6765\u89c4\u5212\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u8fde\u7eed\u7a7a\u95f4\u7684\u79bb\u6563\u5316\u9700\u6c42\u3002", "motivation": "\u7814\u7a76\u4e2d\u53d1\u73b0\u4f20\u7edf\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u5bf9\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u7b26\u53f7\u5316\u63cf\u8ff0\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7ef4\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7684\u5e94\u7528\u3002", "method": "SPOT\u5229\u7528\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u9886\u57df\u5148\u9a8c\uff0c\u6307\u5bfc\u5728\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u641c\u7d22\uff0c\u901a\u8fc7\u64cd\u4f5c\u90e8\u5206\u89c2\u6d4b\u7684\u70b9\u4e91\u751f\u6210\u5019\u9009\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u6d4b\u8bd5\uff0cSPOT\u751f\u6210\u7684\u8ba1\u5212\u6210\u529f\u7387\u9ad8\uff0c\u4e14\u4f18\u4e8e\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u641c\u7d22\u7684\u89c4\u5212\u65b9\u6cd5\u5bf9\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.04805", "pdf": "https://arxiv.org/pdf/2509.04805", "abs": "https://arxiv.org/abs/2509.04805", "authors": ["Keqin Zhang"], "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.", "AI": {"tldr": "\u6982\u8ff0\u4e86AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\u5728\u73b0\u4ee3\u65e0\u7ebf\u524d\u7aef\u94fe\u8def\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e24\u79cd\u9ad8\u538b\u7f29\u7387\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u65e0\u8702\u7a9d\u67b6\u6784\u7684\u538b\u7f29\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u524d\u7aef\u94fe\u8def\u9700\u8981\u5728\u9ad8\u5e26\u5bbd\u548c\u4f4e\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4f20\u8f93\u9ad8\u7ef4\u4fe1\u53f7\uff0c\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u9996\u5148\u7efc\u8ff0\u4e86AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\uff0c\u7136\u540e\u5206\u6790\u4e86\u4e24\u79cd\u4ee3\u8868\u6027\u7684\u9ad8\u538b\u7f29\u7387\u65b9\u6cd5\uff1a\u7aef\u5230\u7aef\u5b66\u4e60\u7684CSI\u53cd\u9988\u548cRB\u7c92\u5ea6\u9884\u7f16\u7801\u4f18\u5316\u7ed3\u5408\u538b\u7f29\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u65e0\u8702\u7a9d\u67b6\u6784\u7684\u538b\u7f29\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u538b\u7f29\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u4e14\u6027\u80fd\u635f\u5931\u53ef\u63a7\uff0c\u652f\u6301RB\u7ea7\u901f\u7387\u9002\u5e94\uff0c\u5e76\u9002\u5408\u4e0b\u4e00\u4ee3\u7f51\u7edc\u7684\u96c6\u4e2d\u5f0f\u534f\u4f5c\u4f20\u8f93\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\u4e3a\u65e0\u7ebf\u524d\u7aef\u94fe\u8def\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u7f29\u7387\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\u573a\u666f\u4e0b\u3002"}}
{"id": "2509.04658", "pdf": "https://arxiv.org/pdf/2509.04658", "abs": "https://arxiv.org/abs/2509.04658", "authors": ["Manish Kansana", "Sindhuja Penchala", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision", "categories": ["cs.RO"], "comment": "6 pages", "summary": "Multimodal surface material classification plays a critical role in advancing\ntactile perception for robotic manipulation and interaction. In this paper, we\npresent Surformer v2, an enhanced multi-modal classification architecture\ndesigned to integrate visual and tactile sensory streams through a\nlate(decision level) fusion mechanism. Building on our earlier Surformer v1\nframework [1], which employed handcrafted feature extraction followed by\nmid-level fusion architecture with multi-head cross-attention layers, Surformer\nv2 integrates the feature extraction process within the model itself and shifts\nto late fusion. The vision branch leverages a CNN-based classifier(Efficient\nV-Net), while the tactile branch employs an encoder-only transformer model,\nallowing each modality to extract modality-specific features optimized for\nclassification. Rather than merging feature maps, the model performs\ndecision-level fusion by combining the output logits using a learnable weighted\nsum, enabling adaptive emphasis on each modality depending on data context and\ntraining dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a\nmulti-modal benchmark comprising surface images and corresponding tactile\nsensor readings. Our results demonstrate that Surformer v2 performs well,\nmaintaining competitive inference speed, suitable for real-time robotic\napplications. These findings underscore the effectiveness of decision-level\nfusion and transformer-based tactile modeling for enhancing surface\nunderstanding in multi-modal robotic perception.", "AI": {"tldr": "Surformer v2\u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u591a\u6a21\u6001\u5206\u7c7b\u67b6\u6784\uff0c\u901a\u8fc7\u51b3\u7b56\u7ea7\u878d\u5408\u673a\u5236\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u611f\u5b98\u6d41\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4ea4\u4e92\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528CNN\u548cTransformer\u5206\u522b\u5904\u7406\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\uff0c\u901a\u8fc7\u51b3\u7b56\u7ea7\u878d\u5408\u7ed3\u5408\u8f93\u51falogits\u3002", "result": "\u5728Touch and Go\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u9002\u5408\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u3002", "conclusion": "\u51b3\u7b56\u7ea7\u878d\u5408\u548c\u57fa\u4e8eTransformer\u7684\u89e6\u89c9\u5efa\u6a21\u6709\u6548\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u673a\u5668\u4eba\u611f\u77e5\u7684\u8868\u9762\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.04860", "pdf": "https://arxiv.org/pdf/2509.04860", "abs": "https://arxiv.org/abs/2509.04860", "authors": ["Rui Guo", "Yi Zhang", "Yhonatan Kvich", "Tianyao Huang", "Maokun Li", "Yonina C. Eldar"], "title": "Plug-and-Play Latent Diffusion for Electromagnetic Inverse Scattering with Application to Brain Imaging", "categories": ["eess.SP"], "comment": null, "summary": "Electromagnetic (EM) imaging is an important tool for non-invasive sensing\nwith low-cost and portable devices. One emerging application is EM stroke\nimaging, which enables early diagnosis and continuous monitoring of brain\nstrokes. Quantitative imaging is achieved by solving an inverse scattering\nproblem (ISP) that reconstructs permittivity and conductivity maps from\nmeasurements. In general, the reconstruction accuracy is limited by its\ninherent nonlinearity and ill-posedness. Existing methods, including\nlearning-free and learning-based approaches, fail to either incorporate\ncomplicated prior distributions or provide theoretical guarantees, posing\ndifficulties in balancing interpretability, distortion error, and reliability.\nTo overcome these limitations, we propose a posterior sampling method based on\nlatent diffusion for quantitative EM brain imaging, adapted from a generative\nplug-and-play (PnP) posterior sampling framework. Our approach allows to\nflexibly integrate prior knowledge into physics-based inversion without\nrequiring paired measurement-label datasets. We first learn the prior\ndistribution of targets from an unlabeled dataset, and then incorporate the\nlearned prior into posterior sampling. In particular, we train a latent\ndiffusion model on permittivity and conductivity maps to capture their prior\ndistribution. Then, given measurements and the forward model describing EM wave\nphysics, we perform posterior sampling by alternating between two samplers that\nrespectively enforce the likelihood and prior distributions. Finally, reliable\nreconstruction is obtained through minimum mean squared error (MMSE) estimation\nbased on the samples. Experimental results on brain imaging demonstrate that\nour approach achieves state-of-the-art performance in reconstruction accuracy\nand structural similarity while maintaining high measurement fidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u91cf\u7535\u78c1\u8111\u6210\u50cf\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u548c\u7269\u7406\u6a21\u578b\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "motivation": "\u7535\u78c1\u6210\u50cf\u5728\u975e\u4fb5\u5165\u5f0f\u4f20\u611f\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u53ef\u89e3\u91ca\u6027\u3001\u5931\u771f\u8bef\u5dee\u548c\u53ef\u9760\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b66\u4e60\u76ee\u6807\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u4ea4\u66ff\u91c7\u6837\u5668\u7ed3\u5408\u4f3c\u7136\u548c\u5148\u9a8c\u5206\u5e03\uff0c\u6700\u7ec8\u901a\u8fc7\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u83b7\u5f97\u53ef\u9760\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6d4b\u91cf\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7535\u78c1\u8111\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u9760\u7684\u91cd\u5efa\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u5148\u9a8c\u77e5\u8bc6\u4e0e\u7269\u7406\u6a21\u578b\u3002"}}
{"id": "2509.04712", "pdf": "https://arxiv.org/pdf/2509.04712", "abs": "https://arxiv.org/abs/2509.04712", "authors": ["Zhihao Zhang", "Chengyang Peng", "Ekim Yurtsever", "Keith A. Redmill"], "title": "Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Automated vehicle control using reinforcement learning (RL) has attracted\nsignificant attention due to its potential to learn driving policies through\nenvironment interaction. However, RL agents often face training challenges in\nsample efficiency and effective exploration, making it difficult to discover an\noptimal driving strategy. To address these issues, we propose guiding the RL\ndriving agent with a demonstration policy that need not be a highly optimized\nor expert-level controller. Specifically, we integrate a rule-based lane change\ncontroller with the Soft Actor Critic (SAC) algorithm to enhance exploration\nand learning efficiency. Our approach demonstrates improved driving performance\nand can be extended to other driving scenarios that can similarly benefit from\ndemonstration-based guidance.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u56e0\u80fd\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u9a7e\u9a76\u7b56\u7565\u800c\u53d7\u5173\u6ce8\uff0c\u4f46\u9762\u4e34\u6837\u672c\u6548\u7387\u548c\u63a2\u7d22\u6709\u6548\u6027\u7684\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u8f66\u9053\u53d8\u6362\u63a7\u5236\u5668\u4e0eSAC\u7b97\u6cd5\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3RL\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u63a2\u7d22\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u4e13\u5bb6\u7ea7\u6f14\u793a\u7b56\u7565\u5f15\u5bfcRL\u667a\u80fd\u4f53\u3002", "method": "\u96c6\u6210\u57fa\u4e8e\u89c4\u5219\u7684\u8f66\u9053\u53d8\u6362\u63a7\u5236\u5668\u4e0eSoft Actor Critic\uff08SAC\uff09\u7b97\u6cd5\uff0c\u63d0\u5347\u63a2\u7d22\u548c\u5b66\u4e60\u6548\u7387\u3002", "result": "\u9a7e\u9a76\u6027\u80fd\u5f97\u5230\u6539\u5584\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9002\u7528\u6f14\u793a\u5f15\u5bfc\u7684\u9a7e\u9a76\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u975e\u4e13\u5bb6\u7ea7\u6f14\u793a\u7b56\u7565\u7ed3\u5408SAC\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86RL\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.04865", "pdf": "https://arxiv.org/pdf/2509.04865", "abs": "https://arxiv.org/abs/2509.04865", "authors": ["Yunpu Zhang", "Changsheng You", "Hing Cheung So", "Dusit Niyato"], "title": "Rotatable Antenna Aided Mixed Near-Field and Far-Field Communications in the Upper Mid-Band: Interference Analysis and Joint Optimization", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "13 pages, 12 figures", "summary": "In this paper, we propose to leverage rotatable antennas (RAs) for improving\nthe communication performance in mixed near-field and far-field communication\nsystems by exploiting a new spatial degree-of-freedom (DoF) offered by antenna\nrotation to mitigate complex near-field interference and mixed-field\ninterference. Specifically, we investigate a modular RA-enabled mixed-field\ndownlink communication system, where a base station (BS) consisting of multiple\nRA subarrays communicates with multiple near-field users in the presence of\nseveral legacy far-field users. We formulate an optimization problem to\nmaximize the sum-rate of the near-field users by jointly optimizing the power\nallocation and rotation angles of all subarrays at the BS. To gain useful\ninsights into the effect of RAs on mixed-field communications, we first analyze\na special case where all subarrays share the same rotation angle and obtain\nclosed-form expressions for the rotation-aware normalized near-field\ninterference and the rotation-aware normalized mixed-field interference using\nthe Fresnel integrals. We then analytically reveal that array rotation\neffectively suppresses both interference types, thereby significantly enhancing\nmixed-field communication performance. For the general case involving\nsubarray-wise rotation, we propose an efficient double-layer algorithm to\nobtain a high-quality solution, where the inner layer optimizes power\nallocation using the successive convex approximation (SCA) technique, while the\nouter layer determines the rotation angles of all subarrays via particle swarm\noptimization (PSO). Finally, numerical results highlight the significant\nperformance gains achieved by RAs over conventional fixed-antenna systems and\ndemonstrate the effectiveness of our developed joint design compared to\nbenchmark schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5229\u7528\u53ef\u65cb\u8f6c\u5929\u7ebf\uff08RAs\uff09\u63d0\u5347\u6df7\u5408\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5929\u7ebf\u65cb\u8f6c\u63d0\u4f9b\u7684\u81ea\u7531\u5ea6\u6291\u5236\u5e72\u6270\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u6df7\u5408\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5b58\u5728\u590d\u6742\u7684\u5e72\u6270\u95ee\u9898\uff0c\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u65cb\u8f6c\u5929\u7ebf\u63d0\u4f9b\u7684\u65b0\u81ea\u7531\u5ea6\u6765\u6539\u5584\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4e86\u57fa\u4e8e\u6a21\u5757\u5316RA\u7684\u6df7\u5408\u573a\u4e0b\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u65cb\u8f6c\u89d2\u5ea6\u3002\u5206\u6790\u4e86\u7279\u6b8a\u548c\u4e00\u822c\u60c5\u51b5\uff0c\u5e76\u63d0\u51fa\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\uff08SCA\u548cPSO\uff09\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5929\u7ebf\u65cb\u8f6c\u53ef\u6709\u6548\u6291\u5236\u5e72\u6270\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86RA\u76f8\u6bd4\u4f20\u7edf\u7cfb\u7edf\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u53ef\u65cb\u8f6c\u5929\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u573a\u901a\u4fe1\u6027\u80fd\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002"}}
{"id": "2509.04722", "pdf": "https://arxiv.org/pdf/2509.04722", "abs": "https://arxiv.org/abs/2509.04722", "authors": ["Adrian B. Ghansah", "Sergio A. Esteban", "Aaron D. Ames"], "title": "Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots", "categories": ["cs.RO"], "comment": "8 pages, 6 figures, accepted to IEEE-RAS International Conference on\n  Humanoid Robots 2025", "summary": "As humanoid robots enter real-world environments, ensuring robust locomotion\nacross diverse environments is crucial. This paper presents a computationally\nefficient hierarchical control framework for humanoid robot locomotion based on\nreduced-order models -- enabling versatile step planning and incorporating arm\nand torso dynamics to better stabilize the walking. At the high level, we use\nthe step-to-step dynamics of the ALIP model to simultaneously optimize over\nstep periods, step lengths, and ankle torques via nonlinear MPC. The ALIP\ntrajectories are used as references to a linear MPC framework that extends the\nstandard SRB-MPC to also include simplified arm and torso dynamics. We validate\nthe performance of our approach through simulation and hardware experiments on\nthe Unitree G1 humanoid robot. In the proposed framework the high-level step\nplanner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard\nmini-PC. Adaptive step timing increased the push recovery success rate by 36%,\nand the upper body control improved the yaw disturbance rejection. We also\ndemonstrate robust locomotion across diverse indoor and outdoor terrains,\nincluding grass, stone pavement, and uneven gym mats.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u964d\u9636\u6a21\u578b\u7684\u8ba1\u7b97\u9ad8\u6548\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u63a7\u5236\uff0c\u7ed3\u5408\u81c2\u548c\u8eaf\u5e72\u52a8\u529b\u5b66\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3a\u786e\u4fdd\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\uff0c\u9700\u8bbe\u8ba1\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u63a7\u5236\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\uff1a\u9ad8\u5c42\u4f7f\u7528ALIP\u6a21\u578b\u901a\u8fc7\u975e\u7ebf\u6027MPC\u4f18\u5316\u6b65\u5e45\u53c2\u6570\uff1b\u4e2d\u5c42\u57fa\u4e8e\u7ebf\u6027MPC\u7ed3\u5408\u7b80\u5316\u81c2\u548c\u8eaf\u5e72\u52a8\u529b\u5b66\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u81ea\u9002\u5e94\u6b65\u5e45\u65f6\u95f4\u63d0\u5347\u6297\u63a8\u6210\u529f\u738736%\uff0c\u4e0a\u8eab\u63a7\u5236\u6539\u5584\u504f\u822a\u6270\u52a8\u6291\u5236\uff0c\u4e14\u5728\u591a\u7c7b\u5730\u5f62\u4e2d\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.04873", "pdf": "https://arxiv.org/pdf/2509.04873", "abs": "https://arxiv.org/abs/2509.04873", "authors": ["Yue Geng", "Tee Hiang Cheng", "Kai Zhong", "Kah Chan Teh", "Qingqing Wu"], "title": "Movable IRS-Aided ISAC Systems: Joint Beamforming and Position Optimization", "categories": ["eess.SP"], "comment": "13 pages, 8 figures", "summary": "Driven by intelligent reflecting surface (IRS) and movable antenna (MA)\ntechnologies, movable IRS (MIRS) has been proposed to improve the adaptability\nand performance of conventional IRS, enabling flexible adjustment of the IRS\nreflecting element positions. This paper investigates MIRS-aided integrated\nsensing and communication (ISAC) systems. The objective is to minimize the\npower required for satisfying the quality-of-service (QoS) of sensing and\ncommunication by jointly optimizing the MIRS element positions, IRS reflection\ncoefficients, transmit beamforming, and receive filters. To balance the\nperformance-cost trade-off, we proposed two MIRS schemes: element-wise control\nand array-wise control, where the positions of individual reflecting elements\nand arrays consisting of multiple elements are controllable, respectively. To\naddress the joint beamforming and position optimization, a product Riemannian\nmanifold optimization (PRMO) method is proposed, where the variables are\nupdated over a constructed product Riemannian manifold space (PRMS) in parallel\nvia penalty-based transformation and Riemannian\nBroyden-Fletcher-Goldfarb-Shanno (RBFGS) algorithm. Simulation results\ndemonstrate that the proposed MIRS outperforms conventional IRS in power\nminimization with both element-wise control and array-wise control.\nSpecifically, with different system parameters, the minimum power is achieved\nby the MIRS with the element-wise control scheme, while suboptimal solution and\nhigher computational efficiency are achieved by the MIRS with array-wise\ncontrol scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u667a\u80fd\u53cd\u5c04\u8868\u9762\uff08MIRS\uff09\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53cd\u5c04\u5143\u4ef6\u4f4d\u7f6e\u548c\u53cd\u5c04\u7cfb\u6570\u7b49\uff0c\u5b9e\u73b0\u4e86\u529f\u7387\u6700\u5c0f\u5316\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u53cd\u5c04\u8868\u9762\uff08IRS\uff09\u6027\u80fd\u53d7\u9650\uff0cMIRS\u901a\u8fc7\u53ef\u79fb\u52a8\u53cd\u5c04\u5143\u4ef6\u63d0\u9ad8\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cdMIRS\u63a7\u5236\u65b9\u6848\uff08\u5143\u4ef6\u7ea7\u548c\u9635\u5217\u7ea7\uff09\u53caPRMO\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528RBFGS\u7b97\u6cd5\u5e76\u884c\u66f4\u65b0\u53d8\u91cf\u3002", "result": "\u4eff\u771f\u663e\u793aMIRS\u5728\u529f\u7387\u6700\u5c0f\u5316\u4e0a\u4f18\u4e8e\u4f20\u7edfIRS\uff0c\u5143\u4ef6\u7ea7\u63a7\u5236\u65b9\u6848\u6548\u679c\u6700\u4f73\uff0c\u9635\u5217\u7ea7\u65b9\u6848\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "MIRS\u663e\u8457\u63d0\u5347ISAC\u7cfb\u7edf\u6027\u80fd\uff0c\u5c24\u5176\u5728\u529f\u7387\u6548\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2509.04737", "pdf": "https://arxiv.org/pdf/2509.04737", "abs": "https://arxiv.org/abs/2509.04737", "authors": ["Ryoga Oishi", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics", "categories": ["cs.RO"], "comment": "16 pages, 5 figures, Accepted at CoRL2025", "summary": "In the field of robot learning, coordinating robot actions through language\ninstructions is becoming increasingly feasible. However, adapting actions to\nhuman instructions remains challenging, as such instructions are often\nqualitative and require exploring behaviors that satisfy varying conditions.\nThis paper proposes a motion generation model that adapts robot actions in\nresponse to modifier directives human instructions imposing behavioral\nconditions during task execution. The proposed method learns a mapping from\nmodifier directives to actions by segmenting demonstrations into short\nsequences, assigning weakly supervised labels corresponding to specific\nmodifier types. We evaluated our method in wiping and pick and place tasks.\nResults show that it can adjust motions online in response to modifier\ndirectives, unlike conventional batch-based methods that cannot adapt during\nexecution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u8c03\u6574\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6279\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u5728\u7ebf\u8c03\u6574\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u534f\u8c03\u673a\u5668\u4eba\u52a8\u4f5c\u8d8a\u6765\u8d8a\u53ef\u884c\uff0c\u4f46\u9002\u5e94\u4eba\u7c7b\u6307\u4ee4\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6307\u4ee4\u901a\u5e38\u662f\u5b9a\u6027\u7684\uff0c\u9700\u8981\u63a2\u7d22\u6ee1\u8db3\u4e0d\u540c\u6761\u4ef6\u7684\u884c\u4e3a\u3002", "method": "\u65b9\u6cd5\u901a\u8fc7\u5c06\u6f14\u793a\u5206\u5272\u6210\u77ed\u5e8f\u5217\u5e76\u5206\u914d\u5f31\u76d1\u7763\u6807\u7b7e\uff0c\u5b66\u4e60\u4ece\u4fee\u9970\u6307\u4ee4\u5230\u52a8\u4f5c\u7684\u6620\u5c04\u3002", "result": "\u5728\u64e6\u62ed\u548c\u6293\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u5728\u7ebf\u54cd\u5e94\u4fee\u9970\u6307\u4ee4\u8c03\u6574\u52a8\u4f5c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6279\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5728\u7ebf\u9002\u5e94\u4eba\u7c7b\u6307\u4ee4\uff0c\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.04930", "pdf": "https://arxiv.org/pdf/2509.04930", "abs": "https://arxiv.org/abs/2509.04930", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part I, Principles and algorithms", "categories": ["eess.SP"], "comment": null, "summary": "In this article, a Probability Mass Function (PMF) estimation method which\ntames the curse of dimensionality is proposed. This method, called Partial\nCoupled Tensor Factorization of 3D marginals or PCTF3D, has for principle to\npartially couple order-3 data projections -- seen as order-3 tensors -- to\nobtain a tensor decomposition of the probability mass tensor. The novelty of\nPCTF3D relies on partial coupling which consists in choosing a subset of 3D\nmarginals. The choice of marginals is then formulated with hypergraphs. After\npresenting possible coupling strategies, some numerical experiments and an\napplication of the method are proposed. This article is the first of a two-part\narticle. While this first article focuses on a new algorithmic framework for\nPMF estimation, the second studies uniqueness properties of the model\nintroduced in this article.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aPCTF3D\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u8026\u5408\u4e09\u7ef4\u8fb9\u9645\u6765\u4f30\u8ba1\u6982\u7387\u8d28\u91cf\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u96be\u4ee5\u5904\u7406\u6982\u7387\u8d28\u91cf\u51fd\u6570\u7684\u4f30\u8ba1\u95ee\u9898\uff0cPCTF3D\u65e8\u5728\u901a\u8fc7\u90e8\u5206\u8026\u5408\u548c\u8d85\u56fe\u9009\u62e9\u5408\u9002\u7684\u8fb9\u9645\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PCTF3D\u65b9\u6cd5\u901a\u8fc7\u90e8\u5206\u8026\u5408\u4e09\u7ef4\u6570\u636e\u6295\u5f71\uff08\u89c6\u4e3a\u4e09\u9636\u5f20\u91cf\uff09\uff0c\u5e76\u4f7f\u7528\u8d85\u56fe\u9009\u62e9\u8fb9\u9645\uff0c\u4ece\u800c\u5b9e\u73b0\u5f20\u91cf\u5206\u89e3\u3002", "result": "\u63d0\u4f9b\u4e86\u6570\u503c\u5b9e\u9a8c\u548c\u5e94\u7528\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PCTF3D\u4e3a\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u7b2c\u4e8c\u90e8\u5206\u5c06\u7814\u7a76\u5176\u552f\u4e00\u6027\u7279\u6027\u3002"}}
{"id": "2509.04836", "pdf": "https://arxiv.org/pdf/2509.04836", "abs": "https://arxiv.org/abs/2509.04836", "authors": ["Dongping Li", "Shaoting Peng", "John Pohovey", "Katherine Rose Driggs-Campbell"], "title": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments.", "AI": {"tldr": "COMMET\u7cfb\u7edf\u901a\u8fc7\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\u89e3\u51b3\u4e86\u65e5\u5e38\u4efb\u52a1\u4e2d\u673a\u5668\u4eba\u79fb\u52a8\u64cd\u4f5c\u7684\u4eba\u7c7b\u5f15\u53d1\u51b2\u7a81\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u68c0\u7d22\u548c\u7cbe\u7ec6\u6a21\u578b\u63a8\u65ad\uff0c\u5e76\u5229\u7528GPT-4\u603b\u7ed3\u7528\u6237\u504f\u597d\u3002\u521d\u6b65\u7814\u7a76\u8868\u660e\u5176\u68c0\u6d4b\u6a21\u5757\u4f18\u4e8eGPT\u6a21\u578b\u3002", "motivation": "\u52a8\u6001\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u4eba\u7c7b\u6d3b\u52a8\u4f1a\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u4ea7\u751f\u51b2\u7a81\uff0c\u4e14\u8fd9\u7c7b\u51b2\u7a81\u7684\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u7528\u6237\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u5e94\u6027\u5f3a\u4e14\u4e2a\u6027\u5316\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\uff08\u591a\u6a21\u6001\u68c0\u7d22\u548c\u7cbe\u7ec6\u6a21\u578b\u63a8\u65ad\uff09\uff0c\u5e76\u5229\u7528GPT-4\u603b\u7ed3\u7528\u6237\u504f\u597d\uff0c\u8bbe\u8ba1\u4e86\u7528\u6237\u53cb\u597d\u7684\u6570\u636e\u6536\u96c6\u754c\u9762\u3002", "result": "\u68c0\u6d4b\u6a21\u5757\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e0a\u4f18\u4e8eGPT\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u6548\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "COMMET\u4e3a\u89e3\u51b3\u4eba\u7c7b\u5f15\u53d1\u51b2\u7a81\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u652f\u6301\u672a\u6765\u5bb6\u7528\u673a\u5668\u4eba\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.04931", "pdf": "https://arxiv.org/pdf/2509.04931", "abs": "https://arxiv.org/abs/2509.04931", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part II, Uniqueness of the model", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, uniqueness properties of a coupled tensor model are studied.\nThis new coupled tensor model is used in a new method called Partial Coupled\nTensor Factorization of 3D marginals or PCTF3D. This method performs estimation\nof probability mass functions by coupling 3D marginals, seen as order-3\ntensors. The core novelty of PCTF3D's approach (detailed in the part I article)\nrelies on the partial coupling which consists on the choice of 3D marginals to\nbe coupled. Tensor methods are ubiquitous in many applications of statistical\nlearning, with their biggest advantage of having strong uniqueness properties.\nIn this paper, the uniqueness properties of PCTF3D's constrained coupled\nlow-rank model is assessed. While probabilistic constraints of the coupled\nmodel are handled properly, it is shown that uniqueness highly depends on the\ncoupling used in PCTF3D. After proposing a Jacobian algorithm providing maximum\nrecoverable rank, different coupling strategies presented in the Part I article\nare examined with respect to their uniqueness properties. Finally, an\nidentifiability bound is given for a so-called Cartesian coupling which permits\nenhancing sufficient bounds of the literature.", "AI": {"tldr": "\u7814\u7a76\u4e86PCTF3D\u65b9\u6cd5\u4e2d\u8026\u5408\u5f20\u91cf\u6a21\u578b\u7684\u552f\u4e00\u6027\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u4e8e\u8026\u5408\u7b56\u7565\uff0c\u5e76\u901a\u8fc7Jacobian\u7b97\u6cd5\u63d0\u51fa\u4e86\u6700\u5927\u53ef\u6062\u590d\u79e9\u3002", "motivation": "\u63a2\u7a76PCTF3D\u65b9\u6cd5\u4e2d\u5f20\u91cf\u8026\u5408\u6a21\u578b\u7684\u552f\u4e00\u6027\uff0c\u4ee5\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7b56\u7565\u9009\u62e9\u3002", "method": "\u63d0\u51faJacobian\u7b97\u6cd5\u5206\u6790\u6700\u5927\u53ef\u6062\u590d\u79e9\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u8026\u5408\u7b56\u7565\u7684\u552f\u4e00\u6027\u3002", "result": "\u552f\u4e00\u6027\u9ad8\u5ea6\u4f9d\u8d56\u8026\u5408\u7b56\u7565\uff0c\u63d0\u51faCartesian\u8026\u5408\u7684\u8bc6\u522b\u6027\u754c\u9650\u3002", "conclusion": "PCTF3D\u65b9\u6cd5\u7684\u552f\u4e00\u6027\u5206\u6790\u4e3a\u5f20\u91cf\u8026\u5408\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.04853", "pdf": "https://arxiv.org/pdf/2509.04853", "abs": "https://arxiv.org/abs/2509.04853", "authors": ["Chengkai Xu", "Jiaqi Liu", "Yicheng Guo", "Peng Hang", "Jian Sun"], "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing", "categories": ["cs.RO", "cs.AI"], "comment": "https://perfectxu88.github.io/KDP-AD/", "summary": "End-to-end autonomous driving remains constrained by the need to generate\nmulti-modal actions, maintain temporal stability, and generalize across diverse\nscenarios. Existing methods often collapse multi-modality, struggle with\nlong-horizon consistency, or lack modular adaptability. This paper presents\nKDP, a knowledge-driven diffusion policy that integrates generative diffusion\nmodeling with a sparse mixture-of-experts routing mechanism. The diffusion\ncomponent generates temporally coherent and multi-modal action sequences, while\nthe expert routing mechanism activates specialized and reusable experts\naccording to context, enabling modular knowledge composition. Extensive\nexperiments across representative driving scenarios demonstrate that KDP\nachieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u9a71\u52a8\u7684\u6269\u6563\u7b56\u7565\uff08KDP\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6269\u6563\u6a21\u578b\u548c\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u8def\u7531\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u573a\u666f\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5728\u7aef\u5230\u7aef\u5b9e\u73b0\u4e2d\u9762\u4e34\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u573a\u666f\u591a\u6837\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u65e0\u6cd5\u517c\u987e\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "KDP\u7ed3\u5408\u4e86\u751f\u6210\u6269\u6563\u6a21\u578b\u548c\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u8def\u7531\u673a\u5236\uff0c\u6269\u6563\u6a21\u578b\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5e8f\u5217\uff0c\u8def\u7531\u673a\u5236\u6fc0\u6d3b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4e13\u5bb6\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKDP\u5728\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u4f4e\u7684\u78b0\u649e\u98ce\u9669\u548c\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u6548\u679c\u3002", "conclusion": "KDP\u4e3a\u77e5\u8bc6\u9a71\u52a8\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u4e0e\u4e13\u5bb6\u8def\u7531\u673a\u5236\u7684\u6709\u6548\u7ed3\u5408\u3002"}}
{"id": "2509.04962", "pdf": "https://arxiv.org/pdf/2509.04962", "abs": "https://arxiv.org/abs/2509.04962", "authors": ["Antonio Spallone", "Marco Coraggio", "Francesco De Lellis", "Mario di Bernardo"], "title": "ROPE: A Novel Method for Real-Time Phase Estimation of Complex Biological Rhythms", "categories": ["eess.SP"], "comment": null, "summary": "Accurate phase estimation -- the process of assigning phase values between\n$0$ and $2\\pi$ to repetitive or periodic signals -- is a cornerstone in the\nanalysis of oscillatory signals across diverse fields, from neuroscience to\nrobotics, where it is fundamental, e.g., to understanding coordination in\nneural networks, cardiorespiratory coupling, and human-robot interaction.\nHowever, existing methods are often limited to offline processing and/or\nconstrained to one-dimensional signals. In this paper, we introduce ROPE,\nwhich, to the best of our knowledge, is the first phase-estimation algorithm\ncapable of (i) handling signals of arbitrary dimension and (ii) operating in\nreal-time, with minimal error. ROPE identifies repetitions within the signal to\nsegment it into (pseudo-)periods and assigns phase values by performing\nefficient, tractable searches over previous signal segments. We extensively\nvalidate the algorithm on a variety of signal types, including trajectories\nfrom chaotic dynamical systems, human motion-capture data, and\nelectrocardiographic recordings. Our results demonstrate that ROPE is robust\nagainst noise and signal drift, and achieves significantly superior performance\ncompared to state-of-the-art phase estimation methods. This advancement enables\nreal-time analysis of complex biological rhythms, opening new pathways, for\nexample, for early diagnosis of pathological rhythm disruptions and developing\nrhythm-based therapeutic interventions in neurological and cardiovascular\ndisorders.", "AI": {"tldr": "ROPE\u662f\u4e00\u79cd\u65b0\u578b\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\uff0c\u80fd\u5904\u7406\u4efb\u610f\u7ef4\u5ea6\u7684\u4fe1\u53f7\u5e76\u5b9e\u65f6\u8fd0\u884c\uff0c\u5728\u5404\u79cd\u4fe1\u53f7\u7c7b\u578b\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u751f\u7269\u8282\u5f8b\u5206\u6790\u7b49\u9886\u57df\u3002", "motivation": "\u76f8\u4f4d\u4f30\u8ba1\u5728\u795e\u7ecf\u79d1\u5b66\u3001\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u79bb\u7ebf\u5904\u7406\u548c\u4e00\u7ef4\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7b97\u6cd5\u3002", "method": "ROPE\u901a\u8fc7\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u91cd\u590d\u90e8\u5206\u5e76\u5c06\u5176\u5206\u6bb5\uff0c\u901a\u8fc7\u9ad8\u6548\u641c\u7d22\u5148\u524d\u4fe1\u53f7\u6bb5\u6765\u5206\u914d\u76f8\u4f4d\u503c\u3002", "result": "ROPE\u5728\u566a\u58f0\u548c\u4fe1\u53f7\u6f02\u79fb\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ROPE\u4e3a\u590d\u6742\u751f\u7269\u8282\u5f8b\u7684\u5b9e\u65f6\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u75c5\u7406\u8bca\u65ad\u548c\u8282\u5f8b\u6cbb\u7597\u3002"}}
{"id": "2509.04948", "pdf": "https://arxiv.org/pdf/2509.04948", "abs": "https://arxiv.org/abs/2509.04948", "authors": ["Emanuela Boros"], "title": "Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)", "categories": ["cs.RO", "cs.CV"], "comment": "Master's thesis", "summary": "Topological localization is a fundamental problem in mobile robotics, since\nrobots must be able to determine their position in order to accomplish tasks.\nVisual localization and place recognition are challenging due to perceptual\nambiguity, sensor noise, and illumination variations. This work addresses\ntopological localization in an office environment using only images acquired\nwith a perspective color camera mounted on a robot platform, without relying on\ntemporal continuity of image sequences. We evaluate state-of-the-art visual\ndescriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and\nBag-of-Visual-Words approaches inspired by text retrieval. Our contributions\ninclude a systematic, quantitative comparison of these features, distance\nmeasures, and classifiers. Performance was analyzed using standard evaluation\nmetrics and visualizations, extending previous experiments. Results demonstrate\nthe advantages of proper configurations of appearance descriptors, similarity\nmeasures, and classifiers. The quality of these configurations was further\nvalidated in the Robot Vision task of the ImageCLEF evaluation campaign, where\nthe system identified the most likely location of novel image sequences. Future\nwork will explore hierarchical models, ranking methods, and feature\ncombinations to build more robust localization systems, reducing training and\nruntime while avoiding the curse of dimensionality. Ultimately, this aims\ntoward integrated, real-time localization across varied illumination and longer\nroutes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u529e\u516c\u73af\u5883\u4e2d\u4f7f\u7528\u5355\u4e00\u5f69\u8272\u76f8\u673a\u8fdb\u884c\u62d3\u6251\u5b9a\u4f4d\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u89c6\u89c9\u63cf\u8ff0\u7b26\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u914d\u7f6e\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u56e0\u611f\u77e5\u6a21\u7cca\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u5149\u7167\u53d8\u5316\u5bfc\u81f4\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u4f4d\u7f6e\u8bc6\u522b\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u4e86\u989c\u8272\u76f4\u65b9\u56fe\u3001SIFT\u3001ASIFT\u3001RGB-SIFT\u548c\u8bcd\u888b\u6a21\u578b\u7b49\u591a\u79cd\u89c6\u89c9\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408\u8ddd\u79bb\u5ea6\u91cf\u548c\u5206\u7c7b\u5668\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u4e86\u63cf\u8ff0\u7b26\u914d\u7f6e\u7684\u4f18\u52bf\uff0c\u5e76\u5728ImageCLEF\u8bc4\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5206\u5c42\u6a21\u578b\u3001\u6392\u540d\u65b9\u6cd5\u548c\u7279\u5f81\u7ec4\u5408\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u3002"}}
{"id": "2509.04677", "pdf": "https://arxiv.org/pdf/2509.04677", "abs": "https://arxiv.org/abs/2509.04677", "authors": ["Mayur S Gowda", "John Shi", "Augusto Santos", "Jos\u00e9 M. F. Moura"], "title": "Inferring the Graph Structure of Images for Graph Neural Networks", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "Image datasets such as MNIST are a key benchmark for testing Graph Neural\nNetwork (GNN) architectures. The images are traditionally represented as a grid\ngraph with each node representing a pixel and edges connecting neighboring\npixels (vertically and horizontally). The graph signal is the values\n(intensities) of each pixel in the image. The graphs are commonly used as input\nto graph neural networks (e.g., Graph Convolutional Neural Networks (Graph\nCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the\nimages. In this work, we improve the accuracy of downstream graph neural\nnetwork tasks by finding alternative graphs to the grid graph and superpixel\nmethods to represent the dataset images, following the approach in [5, 6]. We\nfind row correlation, column correlation, and product graphs for each image in\nMNIST and Fashion-MNIST using correlations between the pixel values building on\nthe method in [5, 6]. Experiments show that using these different graph\nrepresentations and features as input into downstream GNN models improves the\naccuracy over using the traditional grid graph and superpixel methods in the\nliterature.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdbMNIST\u548cFashion-MNIST\u6570\u636e\u96c6\u7684\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u7f51\u683c\u56fe\u548c\u8d85\u50cf\u7d20\u65b9\u6cd5\u8868\u793a\u56fe\u50cf\u6570\u636e\u96c6\uff08\u5982MNIST\uff09\uff0c\u4f46\u5e0c\u671b\u627e\u5230\u66f4\u4f18\u7684\u56fe\u8868\u793a\u65b9\u6cd5\u4ee5\u63d0\u5347GNN\u7684\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u50cf\u7d20\u4e4b\u95f4\u7684\u884c\u76f8\u5173\u6027\u3001\u5217\u76f8\u5173\u6027\u548c\u4e58\u79ef\u56fe\uff0c\u751f\u6210\u65b0\u7684\u56fe\u8868\u793a\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u65b0\u7684\u56fe\u8868\u793a\u548c\u7279\u5f81\u4f5c\u4e3aGNN\u8f93\u5165\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u6539\u8fdb\u56fe\u8868\u793a\u65b9\u6cd5\u662f\u63d0\u5347GNN\u4efb\u52a1\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.04950", "pdf": "https://arxiv.org/pdf/2509.04950", "abs": "https://arxiv.org/abs/2509.04950", "authors": ["Byeong-Il Ham", "Hyun-Bin Kim", "Kyung-Soo Kim"], "title": "Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles", "categories": ["cs.RO"], "comment": "6 pages, 3 figures. Accepted at The 25th International Conference on\n  Control, Automation, and Systems (ICCAS 2025). This is arXiv v1\n  (pre-revision); the camera-ready has been submitted", "summary": "In this paper, we propose a 3D path planning method that integrates the A*\nalgorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged\nrobots have been extensively studied, enabling locomotion across a variety of\nterrains. Advances in mobility have enabled obstacles to be regarded not only\nas hindrances to be avoided, but also as navigational aids when beneficial. A\nmodified 3D A* algorithm generates an optimal path by leveraging obstacles\nduring the planning process. By incorporating a height-based penalty into the\ncost function, the algorithm enables the use of traversable obstacles to aid\nlocomotion while avoiding those that are impassable, resulting in more\nefficient and realistic path generation. The octree-based 3D grid map achieves\ncompression by merging high-resolution nodes into larger blocks, especially in\nobstacle-free or sparsely populated areas. This reduces the number of nodes\nexplored by the A* algorithm, thereby improving computational efficiency and\nmemory usage, and supporting real-time path planning in practical environments.\nBenchmark results demonstrate that the use of octree structure ensures an\noptimal path while significantly reducing memory usage and computation time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408A*\u7b97\u6cd5\u4e0e\u516b\u53c9\u6811\u7ed3\u6784\u76843D\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86UGV\u548c\u817f\u5f0f\u673a\u5668\u4eba\u7684\u8def\u5f84\u751f\u6210\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "motivation": "\u63d0\u5347UGV\u548c\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u5229\u7528\u969c\u788d\u7269\u4f5c\u4e3a\u5bfc\u822a\u8f85\u52a9\uff0c\u800c\u975e\u5b8c\u5168\u89c4\u907f\u3002", "method": "\u7ed3\u5408\u6539\u8fdb\u76843D A*\u7b97\u6cd5\u548c\u516b\u53c9\u6811\u7ed3\u6784\uff0c\u5f15\u5165\u9ad8\u5ea6\u60e9\u7f5a\u7684\u4ee3\u4ef7\u51fd\u6570\uff0c\u5229\u7528\u53ef\u7a7f\u8d8a\u969c\u788d\u7269\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u8282\u70b9\u5408\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u8def\u5f84\u6700\u4f18\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9ad8\u6548\u3001\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\uff0c\u5c24\u5176\u5728\u5730\u5f62\u590d\u6742\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.04699", "pdf": "https://arxiv.org/pdf/2509.04699", "abs": "https://arxiv.org/abs/2509.04699", "authors": ["Wenhui Cui", "Christopher Sandino", "Hadi Pouransari", "Ran Liu", "Juri Minxha", "Ellen L. Zippi", "Aman Verma", "Anna Sedlackova", "Behrooz Mahasseni", "Erdrin Azemi"], "title": "CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Hand gesture classification using high-quality structured data such as\nvideos, images, and hand skeletons is a well-explored problem in computer\nvision. Leveraging low-power, cost-effective biosignals, e.g. surface\nelectromyography (sEMG), allows for continuous gesture prediction on wearables.\nIn this paper, we demonstrate that learning representations from weak-modality\ndata that are aligned with those from structured, high-quality data can improve\nrepresentation quality and enables zero-shot classification. Specifically, we\npropose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and\npose representations, where we learn an EMG encoder that produces high-quality\nand pose-informative representations. We assess the gesture classification\nperformance of our model through linear probing and zero-shot setups. Our model\noutperforms emg2pose benchmark models by up to 21% on in-distribution gesture\nclassification and 72% on unseen (out-of-distribution) gesture classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPEP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50EMG\u548c\u59ff\u6001\u8868\u793a\uff0c\u63d0\u5347\u4e86\u5f31\u6a21\u6001\u6570\u636e\u7684\u8868\u5f81\u8d28\u91cf\uff0c\u5e76\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5229\u7528\u4f4e\u6210\u672c\u3001\u4f4e\u529f\u8017\u7684\u751f\u7269\u4fe1\u53f7\uff08\u5982sEMG\uff09\u5b9e\u73b0\u624b\u52bf\u5206\u7c7b\uff0c\u4f46\u5f31\u6a21\u6001\u6570\u636e\u7684\u8868\u5f81\u8d28\u91cf\u8f83\u4f4e\u3002\u901a\u8fc7\u4e0e\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u6570\u636e\u7684\u5bf9\u9f50\u5b66\u4e60\uff0c\u53ef\u4ee5\u63d0\u5347\u8868\u5f81\u8d28\u91cf\u5e76\u652f\u6301\u96f6\u6837\u672c\u5206\u7c7b\u3002", "method": "\u63d0\u51faCPEP\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50EMG\u548c\u59ff\u6001\u8868\u793a\uff0c\u8bad\u7ec3\u4e00\u4e2a\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u5305\u542b\u59ff\u6001\u4fe1\u606f\u7684EMG\u7f16\u7801\u5668\u3002", "result": "\u6a21\u578b\u5728\u7ebf\u6027\u63a2\u6d4b\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b21%\uff08\u540c\u5206\u5e03\u624b\u52bf\u5206\u7c7b\uff09\u548c72%\uff08\u5206\u5e03\u5916\u624b\u52bf\u5206\u7c7b\uff09\u3002", "conclusion": "CPEP\u6846\u67b6\u901a\u8fc7\u5bf9\u9f50\u5b66\u4e60\u63d0\u5347\u4e86EMG\u6570\u636e\u7684\u8868\u5f81\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u751f\u7269\u4fe1\u53f7\u7684\u624b\u52bf\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04970", "pdf": "https://arxiv.org/pdf/2509.04970", "abs": "https://arxiv.org/abs/2509.04970", "authors": ["Tien Pham", "Xinyun Chi", "Khang Nguyen", "Manfred Huber", "Angelo Cangelosi"], "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeGuV\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc7\u5728\u6df1\u5ea6\u8f93\u5165\u4e0a\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u7f51\u7edc\u6765\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7a33\u5b9aQ\u503c\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u89c6\u89c9\u8f93\u5165\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u6570\u636e\u589e\u5f3a\u867d\u80fd\u6539\u5584\u6cdb\u5316\u4f46\u4f1a\u727a\u7272\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "DeGuV\u6846\u67b6\u5229\u7528\u63a9\u7801\u7f51\u7edc\u4ece\u6df1\u5ea6\u8f93\u5165\u4e2d\u63d0\u53d6\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u7a33\u5b9a\u7684Q\u503c\u4f30\u8ba1\u6765\u63d0\u5347\u6cdb\u5316\u4e0e\u6837\u672c\u6548\u7387\u3002", "result": "\u5728RL-ViGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeGuV\u5728\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u6cdb\u5316\u548c\u6837\u672c\u6548\u7387\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "DeGuV\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5173\u952e\u89c6\u89c9\u7279\u5f81\u548c\u7a33\u5b9a\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u548c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.05079", "pdf": "https://arxiv.org/pdf/2509.05079", "abs": "https://arxiv.org/abs/2509.05079", "authors": ["Konstantinos Drossos", "Mikko Heikkinen", "Paschalis Tsiaflakis"], "title": "Lightweight DNN for Full-Band Speech Denoising on Mobile Devices: Exploiting Long and Short Temporal Patterns", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": "Accepted for publication in Proceedings of the 2025 IEEE 27th\n  International Workshop on Multimedia Signal Processing (MMSP)", "summary": "Speech denoising (SD) is an important task of many, if not all, modern signal\nprocessing chains used in devices and for everyday-life applications. While\nthere are many published and powerful deep neural network (DNN)-based methods\nfor SD, few are optimized for resource-constrained platforms such as mobile\ndevices. Additionally, most DNN-based methods for SD are not focusing on\nfull-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency\ncases. In this paper we present a causal, low latency, and lightweight\nDNN-based method for full-band SD, leveraging both short and long temporal\npatterns. The method is based on a modified UNet architecture employing\nlook-back frames, temporal spanning of convolutional kernels, and recurrent\nneural networks for exploiting short and long temporal patterns in the signal\nand estimated denoising mask. The DNN operates on a causal frame-by-frame basis\ntaking as an input the STFT magnitude, utilizes inverted bottlenecks inspired\nby MobileNet, employs causal instance normalization for channel-wise\nnormalization, and achieves a real-time factor below 0.02 when deployed on a\nmodern mobile phone. The proposed method is evaluated using established speech\ndenoising metrics and publicly available datasets, demonstrating its\neffectiveness in achieving an (SI-)SDR value that outperforms existing FB and\nlow latency SD methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u8f7b\u91cf\u7ea7\u3001\u4f4e\u5ef6\u8fdf\u4e14\u56e0\u679c\u7684DNN\u65b9\u6cd5\uff0c\u7528\u4e8e\u5168\u9891\u5e26\u8bed\u97f3\u964d\u566a\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u77ed\u671f\u548c\u957f\u671f\u65f6\u95f4\u6a21\u5f0f\uff0c\u5e76\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u57fa\u4e8eDNN\u7684\u8bed\u97f3\u964d\u566a\u65b9\u6cd5\u672a\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff08\u5982\u79fb\u52a8\u8bbe\u5907\uff09\u4f18\u5316\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5168\u9891\u5e26\u4fe1\u53f7\u548c\u4f4e\u5ef6\u8fdf\u573a\u666f\u7684\u5173\u6ce8\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684UNet\u67b6\u6784\uff0c\u5229\u7528\u56de\u6eaf\u5e27\u3001\u5377\u79ef\u6838\u7684\u65f6\u95f4\u8de8\u5ea6\u4ee5\u53ca\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7STFT\u5e45\u5ea6\u8f93\u5165\u548c\u5012\u7f6e\u74f6\u9888\u7ed3\u6784\uff08\u53d7MobileNet\u542f\u53d1\uff09\uff0c\u5b9e\u73b0\u56e0\u679c\u5e27\u5904\u7406\u3002", "result": "\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u4e8e0.02\u7684\u5b9e\u65f6\u56e0\u5b50\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u5168\u9891\u5e26\u548c\u4f4e\u5ef6\u8fdf\u65b9\u6cd5\u7684SI-SDR\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u5b9e\u73b0\u4e86\u5168\u9891\u5e26\u8bed\u97f3\u964d\u566a\uff0c\u4e14\u5177\u6709\u4f4e\u5ef6\u8fdf\u7279\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.04984", "pdf": "https://arxiv.org/pdf/2509.04984", "abs": "https://arxiv.org/abs/2509.04984", "authors": ["Koji Matsuno", "Chien Chern Cheah"], "title": "Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian", "categories": ["cs.RO"], "comment": null, "summary": "Deep learning, with its exceptional learning capabilities and flexibility,\nhas been widely applied in various applications. However, its black-box nature\nposes a significant challenge in real-time robotic applications, particularly\nin robot control, where trustworthiness and robustness are critical in ensuring\nsafety. In robot motion control, it is essential to analyze and ensure system\nstability, necessitating the establishment of methodologies that address this\nneed. This paper aims to develop a theoretical framework for end-to-end deep\nlearning control that can be integrated into existing robot control theories.\nThe proposed control algorithm leverages a modular learning approach to update\nthe weights of all layers in real time, ensuring system stability based on\nLyapunov-like analysis. Experimental results on industrial robots are presented\nto illustrate the performance of the proposed deep learning controller. The\nproposed method offers an effective solution to the black-box problem in deep\nlearning, demonstrating the possibility of deploying real-time deep learning\nstrategies for robot kinematic control in a stable manner. This achievement\nprovides a critical foundation for future advancements in deep learning based\nreal-time robotic applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5b66\u4e60\u548c\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u7684\u9ed1\u76d2\u95ee\u9898\uff0c\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7684\u9ed1\u76d2\u7279\u6027\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5e26\u6765\u4e86\u4fe1\u4efb\u548c\u9c81\u68d2\u6027\u6311\u6218\uff0c\u73b0\u6709\u7406\u8bba\u6846\u67b6\u5c1a\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u65f6\u66f4\u65b0\u5404\u5c42\u6743\u91cd\uff0c\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\uff0c\u4e3a\u5b9e\u65f6\u6df1\u5ea6\u5b66\u4e60\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u7a33\u5b9a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u9ed1\u76d2\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.04996", "pdf": "https://arxiv.org/pdf/2509.04996", "abs": "https://arxiv.org/abs/2509.04996", "authors": ["Moritz Reuss", "Hongyi Zhou", "Marcel R\u00fchle", "\u00d6mer Erdin\u00e7 Ya\u011fmurlu", "Fabian Otto", "Rudolf Lioutikov"], "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies", "categories": ["cs.RO"], "comment": "Published at CoRL 2025", "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFLOWER\u7684\u9ad8\u6548\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u4e2d\u95f4\u6a21\u6001\u878d\u5408\u548c\u52a8\u4f5c\u7279\u5b9a\u7684\u5168\u5c40\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316\uff08Global-AdaLN\uff09\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u53c2\u6570\u9700\u6c42\uff0c\u540c\u65f6\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684VLA\u7b56\u7565\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u53c2\u6570\u89c4\u6a21\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6280\u672f\u521b\u65b0\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e2d\u95f4\u6a21\u6001\u878d\u5408\u6280\u672f\uff0c\u901a\u8fc7\u88c1\u526aLLM\u5c42\u51cf\u5c1150%\u5bb9\u91cf\uff1b\u63d0\u51fa\u52a8\u4f5c\u7279\u5b9a\u7684Global-AdaLN\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u524a\u51cf20%\u53c2\u6570\u3002", "result": "FLOWER\u4ec5\u9700950M\u53c2\u6570\u548c200\u5c0f\u65f6H100 GPU\u8bad\u7ec3\u65f6\u95f4\uff0c\u5728190\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728CALVIN ABC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SoTA\uff084.53\uff09\u3002", "conclusion": "FLOWER\u5c55\u73b0\u4e86\u9ad8\u6548VLA\u7b56\u7565\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05031", "pdf": "https://arxiv.org/pdf/2509.05031", "abs": "https://arxiv.org/abs/2509.05031", "authors": ["Luca M\u00fcller", "Hassan Ali", "Philipp Allgeuer", "Luk\u00e1\u0161 Gajdo\u0161ech", "Stefan Wermter"], "title": "Pointing-Guided Target Estimation via Transformer-Based Attention", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.6"], "comment": "Accepted at the 34th International Conference on Artificial Neural\n  Networks (ICANN) 2025,12 pages,4 figures,1 table; work was co-funded by\n  Horizon Europe project TERAIS under Grant agreement number 101079338", "summary": "Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316\u67b6\u6784MM-ITF\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4ea4\u4e92\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c062D\u6307\u5411\u624b\u52bf\u6620\u5c04\u5230\u7269\u4f53\u4f4d\u7f6e\uff0c\u9884\u6d4b\u4eba\u7c7b\u610f\u56fe\uff0c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7684\u76f4\u89c9\u6027\u548c\u53ef\u8fbe\u6027\u3002", "motivation": "\u5728HRI\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u9884\u6d4b\u4eba\u7c7b\u610f\u56fe\u5e76\u505a\u51fa\u9002\u5f53\u54cd\u5e94\u3002\u6307\u5411\u624b\u52bf\u4f5c\u4e3a\u975e\u8bed\u8a00\u4ea4\u6d41\u7684\u57fa\u672c\u5f62\u5f0f\uff0c\u662f\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u3002", "method": "\u91c7\u7528Multi-Modality Inter-TransFormer (MM-ITF)\u67b6\u6784\uff0c\u5229\u7528\u4ea4\u4e92\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5904\u7406\u5355\u76eeRGB\u6570\u636e\uff0c\u9884\u6d4b\u6307\u5411\u624b\u52bf\u7684\u76ee\u6807\u7269\u4f53\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u901a\u8fc7\u5f15\u5165patch confusion matrix\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "MM-ITF\u4e3a\u76f4\u89c9\u6027\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.05042", "pdf": "https://arxiv.org/pdf/2509.05042", "abs": "https://arxiv.org/abs/2509.05042", "authors": ["Cristiano Caissutti", "Estelle Gerbier", "Ehsan Khorrambakht", "Paolo Marinelli", "Andrea Munafo'", "Andrea Caiti"], "title": "Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections", "categories": ["cs.RO"], "comment": null, "summary": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5171\u4eab\u81ea\u6cbb\u5728\u6d77\u6d0b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff1aLLMs\u96c6\u6210\u3001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u548c\u6a21\u5757\u5316\u4efb\u52a1\u7ba1\u7406\u5668\uff0c\u65e8\u5728\u51cf\u5c11\u64cd\u4f5c\u8ba4\u77e5\u8d1f\u62c5\u5e76\u589e\u5f3a\u900f\u660e\u5ea6\u3002", "motivation": "\u6d77\u6d0b\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u9ad8\u98ce\u9669\u6027\u9700\u8981\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u56e0\u6b64\u7814\u7a76\u5171\u4eab\u81ea\u6cbb\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u4efb\u52a1\u63cf\u8ff0\u3001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u4ee5\u53ca\u57fa\u4e8e\u884c\u4e3a\u6811\u7684\u6a21\u5757\u5316\u4efb\u52a1\u7ba1\u7406\u5668\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9645\u6e56\u9762\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3001\u63d0\u9ad8\u900f\u660e\u5ea6\u5e76\u589e\u5f3a\u884c\u4e3a\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5339\u914d\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b89\u5168\u5173\u952e\u7684\u6d77\u4e8b\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5171\u4eab\u81ea\u6cbb\u57fa\u7840\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2509.05201", "pdf": "https://arxiv.org/pdf/2509.05201", "abs": "https://arxiv.org/abs/2509.05201", "authors": ["Nariman Niknejad", "Gokul S. Sankar", "Bahare Kiumarsi", "Hamidreza Modares"], "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u4e2d\u7684\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u901a\u8fc7\u96c6\u5408\u72b6\u6001\u4f30\u8ba1\u548c\u7ebf\u6027\u89c4\u5212\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u63a7\u5236\u3002", "motivation": "\u4f20\u7edfMPC\u5047\u8bbe\u611f\u77e5\u8bef\u5dee\u4e3a\u96f6\u5747\u503c\u566a\u58f0\uff0c\u4f46\u5728\u5b9e\u9645\u4e2d\u566a\u58f0\u53ef\u80fd\u5177\u6709\u504f\u6001\u6216\u539a\u5c3e\u5206\u5e03\uff0c\u8fd9\u5bf9\u5b89\u5168\u53cd\u9988\u63a7\u5236\u6784\u6210\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7ea6\u675fzonotopes\u8fdb\u884c\u96c6\u5408\u72b6\u6001\u4f30\u8ba1\uff0c\u5c06\u9c81\u68d2MPC\u91cd\u6784\u4e3a\u7ebf\u6027\u89c4\u5212\uff0c\u91c7\u7528Minkowski-Lyapunov\u6210\u672c\u51fd\u6570\u786e\u4fdd\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u539a\u5c3e\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u4e14\u51c6\u786e\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u566a\u58f0\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2403.18767", "pdf": "https://arxiv.org/pdf/2403.18767", "abs": "https://arxiv.org/abs/2403.18767", "authors": ["Daniel Reem", "Yair Censor"], "title": "The best approximation pair problem relative to two subsets in a normed space", "categories": ["math.OC", "cs.GR", "cs.RO", "math.FA", "math.MG", "41A50, 41A52, 41A65, 90C25, 46N10, 90C26, 46B20, 68U05, 65D18", "G.1.6; G.1.2; I.3.5"], "comment": "Major revision, Introduction and abstract were rewritten, added\n  Theorem 4.8 and Remark 4(ii), minor changes here and there such as in MSC and\n  in the proof of Lemma 3.2 and in Theorem 5.1 (added one sufficient condition,\n  two were removed), Remark 5.2 was extended, added several figures and many\n  more references, added acknowledgements", "summary": "In the classical best approximation pair (BAP) problem, one is given two\nnonempty, closed, convex and disjoint subsets in a finite- or an\ninfinite-dimensional Hilbert space, and the goal is to find a pair of points,\neach from each subset, which realizes the distance between the subsets. We\ndiscuss the problem in more general normed spaces and with possibly non-convex\nsubsets, and focus our attention on the issues of uniqueness and existence of\nthe solution to the problem. As far as we know, these fundamental issues have\nnot received much attention. We present several sufficient geometric conditions\nfor the (at most) uniqueness of a BAP. These conditions are related to the\nstructure and the relative orientation of the boundaries of the subsets and to\nthe norm. We also present many sufficient conditions for the existence of a\nBAP. Our results significantly extend the horizon of a recent algorithm for\nsolving the BAP problem [Censor, Mansour, Reem, J. Approx. Theory (2024)]. The\npaper also shows, perhaps for the first time, how wide is the scope of the BAP\nproblem in terms of the scientific communities which are involved in it\n(frequently independently) and in terms of its applications.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5e7f\u4e49\u8d4b\u8303\u7a7a\u95f4\u4e2d\u6700\u4f73\u903c\u8fd1\u5bf9\uff08BAP\uff09\u95ee\u9898\u7684\u552f\u4e00\u6027\u548c\u5b58\u5728\u6027\uff0c\u63d0\u51fa\u4e86\u82e5\u5e72\u51e0\u4f55\u6761\u4ef6\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u975e\u51f8\u96c6\u5408\u3002", "motivation": "\u7814\u7a76BAP\u95ee\u9898\u5728\u66f4\u4e00\u822c\u7684\u7a7a\u95f4\u548c\u975e\u51f8\u96c6\u5408\u4e2d\u7684\u552f\u4e00\u6027\u548c\u5b58\u5728\u6027\uff0c\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u96c6\u5408\u8fb9\u754c\u7ed3\u6784\u548c\u76f8\u5bf9\u65b9\u5411\u4ee5\u53ca\u8303\u6570\u6027\u8d28\uff0c\u63d0\u51fa\u82e5\u5e72\u51e0\u4f55\u6761\u4ef6\u6765\u4fdd\u8bc1\u552f\u4e00\u6027\u548c\u5b58\u5728\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u79cd\u5145\u5206\u6761\u4ef6\uff0c\u663e\u8457\u6269\u5c55\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u5c55\u793a\u4e86BAP\u95ee\u9898\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "conclusion": "BAP\u95ee\u9898\u5728\u79d1\u5b66\u754c\u548c\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u6027\u9996\u6b21\u88ab\u63ed\u793a\uff0c\u63d0\u51fa\u7684\u6761\u4ef6\u4e3a\u5176\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.04451", "pdf": "https://arxiv.org/pdf/2509.04451", "abs": "https://arxiv.org/abs/2509.04451", "authors": ["Nicole Fronda", "Hariharan Narayanan", "Sadia Afrin Ananna", "Steven Weber", "Houssam Abbas"], "title": "PRREACH: Probabilistic Risk Assessment Using Reachability for UAV Control", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "We present a new approach for designing risk-bounded controllers for Uncrewed\nAerial Vehicles (UAVs). Existing frameworks for assessing risk of UAV\noperations rely on knowing the conditional probability of an incident occurring\ngiven different causes. Limited data for computing these probabilities makes\nreal-world implementation of these frameworks difficult. Furthermore, existing\nframeworks do not include control methods for risk mitigation. Our approach\nrelies on UAV dynamics, and employs reachability analysis for a probabilistic\nrisk assessment over all feasible UAV trajectories. We use this holistic risk\nassessment to formulate a control optimization problem that minimally changes a\nUAV's existing control law to be bounded by an accepted risk threshold. We call\nour approach PRReach. Public and readily available UAV dynamics models and open\nsource spatial data for mapping hazard outcomes enables practical\nimplementation of PRReach for both offline pre-flight and online in-flight risk\nassessment and mitigation. We evaluate PRReach through simulation experiments\non real-world data. Results show that PRReach controllers reduce risk by up to\n24% offline, and up to 53% online from classical controllers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u8fbe\u6027\u5206\u6790\u7684\u65e0\u4eba\u673a\u98ce\u9669\u6709\u754c\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5PRReach\uff0c\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u5f8b\u964d\u4f4e\u98ce\u9669\uff0c\u79bb\u7ebf\u53ef\u51cf\u5c1124%\u98ce\u9669\uff0c\u5728\u7ebf\u53ef\u51cf\u5c1153%\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u4f9d\u8d56\u5df2\u77e5\u6761\u4ef6\u6982\u7387\u4e14\u7f3a\u4e4f\u98ce\u9669\u7f13\u89e3\u63a7\u5236\u65b9\u6cd5\uff0c\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u5b9e\u65bd\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u65e0\u4eba\u673a\u52a8\u529b\u5b66\u548c\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u8bc4\u4f30\u6240\u6709\u53ef\u884c\u8f68\u8ff9\u7684\u98ce\u9669\uff0c\u4f18\u5316\u63a7\u5236\u5f8b\u4ee5\u98ce\u9669\u9608\u503c\u4e3a\u754c\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0cPRReach\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u573a\u666f\u4e2d\u5206\u522b\u964d\u4f4e\u98ce\u966924%\u548c53%\u3002", "conclusion": "PRReach\u5229\u7528\u5f00\u6e90\u6570\u636e\u548c\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5b9e\u9645\u53ef\u884c\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u65e0\u4eba\u673a\u64cd\u4f5c\u98ce\u9669\u3002"}}
{"id": "2509.04624", "pdf": "https://arxiv.org/pdf/2509.04624", "abs": "https://arxiv.org/abs/2509.04624", "authors": ["Ali Khanpour", "Tianyi Wang", "Afra Vahidi-Shams", "Wim Ectors", "Farzam Nakhaie", "Amirhossein Taheri", "Christian Claudel"], "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis", "categories": ["cs.CV", "cs.ET", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Traffic congestion and violations pose significant challenges for urban\nmobility and road safety. Traditional traffic monitoring systems, such as fixed\ncameras and sensor-based methods, are often constrained by limited coverage,\nlow adaptability, and poor scalability. To address these challenges, this paper\nintroduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance\nsystem capable of accurate vehicle detection, classification, tracking, and\nbehavioral analysis in real-world, unconstrained urban environments. The system\nleverages multi-scale and multi-angle template matching, Kalman filtering, and\nhomography-based calibration to process aerial video data collected from\naltitudes of approximately 200 meters. A case study in urban area demonstrates\nrobust performance, achieving a detection precision of 91.8%, an F1-score of\n90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.\nBeyond precise detection, the system classifies five vehicle types and\nautomatically detects critical traffic violations, including unsafe lane\nchanges, illegal double parking, and crosswalk obstructions, through the fusion\nof geofencing, motion filtering, and trajectory deviation analysis. The\nintegrated analytics module supports origin-destination tracking, vehicle count\nvisualization, inter-class correlation analysis, and heatmap-based congestion\nmodeling. Additionally, the system enables entry-exit trajectory profiling,\nvehicle density estimation across road segments, and movement direction\nlogging, supporting comprehensive multi-scale urban mobility analytics.\nExperimental results confirms the system's scalability, accuracy, and practical\nrelevance, highlighting its potential as an enforcement-aware,\ninfrastructure-independent traffic monitoring solution for next-generation\nsmart cities.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u548c\u591a\u89d2\u5ea6\u6a21\u677f\u5339\u914d\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8f66\u8f86\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u8ddf\u8e2a\u548c\u4ea4\u901a\u8fdd\u6cd5\u884c\u4e3a\u8bc6\u522b\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\u56e0\u8986\u76d6\u8303\u56f4\u6709\u9650\u3001\u9002\u5e94\u6027\u5dee\u548c\u53ef\u6269\u5c55\u6027\u4f4e\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4ea4\u901a\u62e5\u5835\u548c\u8fdd\u89c4\u884c\u4e3a\u3002\u65e0\u4eba\u673a\u76d1\u63a7\u7cfb\u7edf\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u7cfb\u7edf\u5229\u7528\u591a\u5c3a\u5ea6\u548c\u591a\u89d2\u5ea6\u6a21\u677f\u5339\u914d\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u57fa\u4e8e\u5355\u5e94\u6027\u7684\u6821\u51c6\u6280\u672f\u5904\u7406\u65e0\u4eba\u673a\u91c7\u96c6\u7684\u89c6\u9891\u6570\u636e\uff0c\u540c\u65f6\u7ed3\u5408\u5730\u7406\u56f4\u680f\u3001\u8fd0\u52a8\u8fc7\u6ee4\u548c\u8f68\u8ff9\u504f\u5dee\u5206\u6790\u8bc6\u522b\u4ea4\u901a\u8fdd\u6cd5\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u7684\u68c0\u6d4b\u7cbe\u5ea6\u8fbe91.8%\uff0cF1\u5206\u6570\u4e3a90.5%\uff0c\u8ddf\u8e2a\u6307\u6807MOTA/MOTP\u5206\u522b\u4e3a92.1%\u548c93.7%\uff0c\u5e76\u80fd\u8bc6\u522b\u4e94\u79cd\u8f66\u8f86\u7c7b\u578b\u548c\u591a\u79cd\u4ea4\u901a\u8fdd\u89c4\u884c\u4e3a\u3002", "conclusion": "\u7cfb\u7edf\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u53ef\u4f5c\u4e3a\u65b0\u4e00\u4ee3\u667a\u6167\u57ce\u5e02\u7684\u57fa\u7840\u8bbe\u65bd\u72ec\u7acb\u4ea4\u901a\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04711", "pdf": "https://arxiv.org/pdf/2509.04711", "abs": "https://arxiv.org/abs/2509.04711", "authors": ["Satoshi Tanaka", "Kok Seang Tan", "Isamu Yamashita"], "title": "Domain Adaptation for Different Sensor Configurations in 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent advances in autonomous driving have underscored the importance of\naccurate 3D object detection, with LiDAR playing a central role due to its\nrobustness under diverse visibility conditions. However, different vehicle\nplatforms often deploy distinct sensor configurations, causing performance\ndegradation when models trained on one configuration are applied to another\nbecause of shifts in the point cloud distribution. Prior work on multi-dataset\ntraining and domain adaptation for 3D object detection has largely addressed\nenvironmental domain gaps and density variation within a single LiDAR; in\ncontrast, the domain gap for different sensor configurations remains largely\nunexplored. In this work, we address domain adaptation across different sensor\nconfigurations in 3D object detection. We propose two techniques: Downstream\nFine-tuning (dataset-specific fine-tuning after multi-dataset training) and\nPartial Layer Fine-tuning (updating only a subset of layers to improve\ncross-configuration generalization). Using paired datasets collected in the\nsame geographic region with multiple sensor configurations, we show that joint\ntraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistently\noutperforms naive joint training for each configuration. Our findings provide a\npractical and scalable solution for adapting 3D object detection models to the\ndiverse vehicle platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u81ea\u9002\u5e94\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u95f4\u76843D\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e0b\u6e38\u5fae\u8c03\u548c\u90e8\u5206\u5c42\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347\u8de8\u914d\u7f6e\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e24\u79cd\u6280\u672f\u7684\u65b9\u6cd5\u4f18\u4e8e\u7b80\u5355\u7684\u8054\u5408\u8bad\u7ec3\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u9002\u5e94\u591a\u6837\u5316\u8f66\u8f86\u5e73\u53f0\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04731", "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u3001\u4ee3\u7406\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u5206\u5c42\u811a\u624b\u67b6\u548c\u8bed\u8a00\u9a71\u52a8\u7684\u65b9\u6cd5\u6784\u5efa\u663e\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u7684\u6548\u7387\u548c\u7b56\u7565\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u3001\u957f\u671f\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u4e16\u754c\u6a21\u578b\u53d1\u5c55\u7684\u74f6\u9888\u95ee\u9898\uff0c\u7279\u522b\u662f\u63a2\u7d22\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u811a\u624b\u67b6\u65b9\u6cd5\uff0c\u5206\u89e3\u590d\u6742\u76ee\u6807\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u751f\u6210\u5206\u5c42\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7b26\u53f7\u5316\u3001\u5206\u5c42\u65b9\u6cd5\u4e0e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u8d8b\u52bf\u663e\u8457\uff0c\u4e14\u8bed\u8a00\u9a71\u52a8\u7684\u4e16\u754c\u6a21\u578b\u80fd\u63d0\u4f9b\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\u548c\u9ad8\u6548\u8bad\u7ec3\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u8bed\u8a00\u53ef\u914d\u7f6e\u7684\u4efb\u52a1\u5c42\u73af\u5883\uff0c\u53ef\u4ee5\u8fde\u63a5\u4f4e\u7ea7\u53cd\u5e94\u884c\u4e3a\u548c\u9ad8\u7ea7\u7b56\u7565\u56e2\u961f\u534f\u4f5c\uff0c\u63a8\u52a8\u65b0\u4e00\u4ee3\u667a\u80fd\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.05116", "pdf": "https://arxiv.org/pdf/2509.05116", "abs": "https://arxiv.org/abs/2509.05116", "authors": ["Jialin Chen", "Jeremie Clos", "Dominic Price", "Praminda Caleb-Solly"], "title": "Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins", "categories": ["cs.ET", "cs.RO"], "comment": "7 pages, accepted at EMBC 2025, presented at the conference", "summary": "To advance the development of assistive and rehabilitation robots, it is\nessential to conduct experiments early in the design cycle. However, testing\nearly prototypes directly with users can pose safety risks. To address this, we\nexplore the use of condition-specific simulation suits worn by healthy\nparticipants in controlled environments as a means to study gait changes\nassociated with various impairments and support rapid prototyping. This paper\npresents a study analyzing the impact of a hemiplegia simulation suit on gait.\nWe collected biomechanical data using a Vicon motion capture system and Delsys\nTrigno EMG and IMU sensors under four walking conditions: with and without a\nrollator, and with and without the simulation suit. The gait data was\nintegrated into a digital twin model, enabling machine learning analyses to\ndetect the use of the simulation suit and rollator, identify turning behavior,\nand evaluate how the suit affects gait over time. Our findings show that the\nsimulation suit significantly alters movement and muscle activation patterns,\nprompting users to compensate with more abrupt motions. We also identify key\nfeatures and sensor modalities that are most informative for accurately\ncapturing gait dynamics and modeling human-rollator interaction within the\ndigital twin framework.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u6a21\u62df\u670d\u88c5\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5206\u6790\u6b65\u6001\u53d8\u5316\uff0c\u53d1\u73b0\u6a21\u62df\u670d\u88c5\u663e\u8457\u6539\u53d8\u8fd0\u52a8\u548c\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u5728\u8f85\u52a9\u548c\u5eb7\u590d\u673a\u5668\u4eba\u8bbe\u8ba1\u65e9\u671f\u6d4b\u8bd5\u4e2d\u907f\u514d\u5b89\u5168\u9690\u60a3\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5065\u5eb7\u53c2\u4e0e\u8005\u4f69\u6234\u6a21\u62df\u670d\u88c5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8fdb\u884c\u6b65\u6001\u5206\u6790\u3002", "method": "\u901a\u8fc7Vicon\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u548cEMG/IMU\u4f20\u611f\u5668\u6536\u96c6\u56db\u79cd\u6b65\u884c\u6761\u4ef6\u4e0b\u7684\u751f\u7269\u529b\u5b66\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u5206\u6790\u3002", "result": "\u6a21\u62df\u670d\u88c5\u663e\u8457\u6539\u53d8\u4e86\u4f7f\u7528\u8005\u7684\u6b65\u6001\u548c\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4f7f\u7528\u8005\u9700\u8981\u66f4\u7a81\u7136\u7684\u52a8\u4f5c\u6765\u8865\u507f\u3002\u540c\u65f6\u786e\u5b9a\u4e86\u5173\u952e\u7279\u5f81\u548c\u4f20\u611f\u5668\u6a21\u6001\u3002", "conclusion": "\u6a21\u62df\u670d\u88c5\u53ef\u7528\u4e8e\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u6b65\u6001\u7814\u7a76\uff0c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e3a\u4eba\u7c7b-\u52a9\u884c\u5668\u4ea4\u4e92\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.05198", "pdf": "https://arxiv.org/pdf/2509.05198", "abs": "https://arxiv.org/abs/2509.05198", "authors": ["Mohammad Saeid", "Amir Salarpour", "Pedram MohajerAnsari"], "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "This paper has been accepted for presentation at the 7th\n  International Conference on Pattern Recognition and Image Analysis (IPRIA\n  2025)", "summary": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e863D\u70b9\u4e91\u5206\u7c7b\u7684\u91cd\u8981\u6027\u53ca\u73b0\u6709\u6570\u636e\u96c6ModelNet40\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u6570\u636e\u96c6ModelNet-R\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edcPoint-SkipNet\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7531\u4e8eModelNet40\u6570\u636e\u96c6\u5b58\u5728\u6807\u6ce8\u4e0d\u4e00\u81f4\u30012D\u6570\u636e\u3001\u5c3a\u5bf8\u4e0d\u5339\u914d\u548c\u7c7b\u522b\u533a\u5206\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7f51\u7edc\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faModelNet-R\u6570\u636e\u96c6\u4ee5\u89e3\u51b3ModelNet40\u7684\u7f3a\u9677\uff0c\u5e76\u8bbe\u8ba1Point-SkipNet\u7f51\u7edc\uff0c\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u3001\u90bb\u57df\u5206\u7ec4\u548c\u8df3\u8fde\u5b9e\u73b0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728ModelNet-R\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cPoint-SkipNet\u5728\u4f4e\u53c2\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9\u4f18\u53163D\u70b9\u4e91\u5206\u7c7b\u6a21\u578b\u6548\u7387\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e14Point-SkipNet\u4e3a\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
