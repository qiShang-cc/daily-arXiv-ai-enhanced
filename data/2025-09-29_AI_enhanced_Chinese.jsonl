{"id": "2509.21370", "pdf": "https://arxiv.org/pdf/2509.21370", "abs": "https://arxiv.org/abs/2509.21370", "authors": ["Yashom Dighe", "Yash Turkar", "Karthik Dantu"], "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal", "categories": ["cs.RO", "cs.CV"], "comment": "First two authors contributed equally", "summary": "Culverts on canals such as the Erie Canal, built originally in 1825, require\nfrequent inspections to ensure safe operation. Human inspection of culverts is\nchallenging due to age, geometry, poor illumination, weather, and lack of easy\naccess. We introduce VISION, an end-to-end, language-in-the-loop autonomy\nsystem that couples a web-scale vision-language model (VLM) with constrained\nviewpoint planning for autonomous inspection of culverts. Brief prompts to the\nVLM solicit open-vocabulary ROI proposals with rationales and confidences,\nstereo depth is fused to recover scale, and a planner -- aware of culvert\nconstraints -- commands repositioning moves to capture targeted close-ups.\nDeployed on a quadruped in a culvert under the Erie Canal, VISION closes the\nsee, decide, move, re-image loop on-board and produces high-resolution images\nfor detailed reporting without domain-specific fine-tuning. In an external\nevaluation by New York Canal Corporation personnel, initial ROI proposals\nachieved 61.4\\% agreement with subject-matter experts, and final\npost-re-imaging assessments reached 80\\%, indicating that VISION converts\ntentative hypotheses into grounded, expert-aligned findings.", "AI": {"tldr": "VISION\u7cfb\u7edf\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3b\u8def\u5f84\u89c4\u5212\uff0c\u7528\u4e8e\u8fd0\u6cb3\u6db5\u6d1e\u7684\u81ea\u52a8\u68c0\u67e5\uff0c\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u63d0\u51fa\u611f\u5174\u8da3\u533a\u57df\uff0c\u89c4\u5212\u52a8\u4f5c\u6355\u83b7\u9ad8\u6e05\u56fe\u50cf\uff0c\u6700\u7ec8\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4e13\u5bb6\u4e00\u81f4\u7387\u8fbe80%\u3002", "motivation": "\u4eba\u5de5\u68c0\u67e5\u8fd0\u6cb3\u6db5\u6d1e\u5b58\u5728\u8bf8\u591a\u56f0\u96be\uff08\u5982\u7a7a\u95f4\u72ed\u5c0f\u3001\u5149\u7167\u4e0d\u8db3\uff09\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f00\u53d1\u4e86VISION\u7cfb\u7edf\uff0c\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3001\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u548c\u7ea6\u675f\u89c4\u5212\uff0c\u8bed\u8a00\u63d0\u793a\u751f\u6210\u611f\u5174\u8da3\u533a\u57df\uff0c\u89c4\u5212\u52a8\u4f5c\u6355\u83b7\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u521d\u59cb\u63d0\u6848\u4e0e\u4e13\u5bb6\u4e00\u81f4\u7387\u4e3a61.4%\uff0c\u6700\u7ec8\u63d0\u5347\u81f380%\uff0c\u7cfb\u7edf\u53ef\u751f\u6210\u9ad8\u6e05\u56fe\u50cf\u7528\u4e8e\u8be6\u7ec6\u62a5\u544a\u3002", "conclusion": "VISION\u5c55\u793a\u4e86\u8bed\u8a00\u5f15\u5bfc\u81ea\u52a8\u5316\u68c0\u67e5\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u9886\u57df\u5fae\u8c03\u5373\u53ef\u751f\u6210\u4e13\u5bb6\u8ba4\u53ef\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.21445", "pdf": "https://arxiv.org/pdf/2509.21445", "abs": "https://arxiv.org/abs/2509.21445", "authors": ["Archie Webster", "Lee Skull", "Seyed Amir Tafrishi"], "title": "Developing a Mono-Actuated Compliant GeoGami Robot", "categories": ["cs.RO"], "comment": "8 pages, 12 figures, under-review", "summary": "This paper presents the design of a new soft-rigid robotic platform,\n\"GeoGami\". We leverage origami surface capabilities to achieve shape\ncontraction and to support locomotion with underactuated forms. A key challenge\nis that origami surfaces have high degrees of freedom and typically require\nmany actuators; we address repeatability by integrating surface compliance. We\npropose a mono-actuated GeoGami mobile platform that combines origami surface\ncompliance with a geometric compliant skeleton, enabling the robot to transform\nand locomote using a single actuator. We demonstrate the robot, develop a\nstiffness model, and describe the central gearbox mechanism. We also analyze\nalternative cable-driven actuation methods for the skeleton to enable surface\ntransformation. Finally, we evaluate the GeoGami platform for capabilities,\nincluding shape transformation and rolling. This platform opens new\ncapabilities for robots that change shape to access different environments and\nthat use shape transformation for locomotion.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u8f6f\u521a\u7ed3\u5408\u7684\u673a\u5668\u4eba\u5e73\u53f0'GeoGami'\uff0c\u5229\u7528\u6298\u7eb8\u8868\u9762\u7684\u7279\u6027\u5b9e\u73b0\u5f62\u72b6\u6536\u7f29\u548c\u8fd0\u52a8\u3002\u901a\u8fc7\u96c6\u6210\u8868\u9762\u987a\u5e94\u6027\u89e3\u51b3\u4e86\u6298\u7eb8\u8868\u9762\u81ea\u7531\u5ea6\u591a\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u5355\u9a71\u52a8\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u6298\u7eb8\u8868\u9762\u81ea\u7531\u5ea6\u591a\u4e14\u9700\u8981\u591a\u4e2a\u9a71\u52a8\u5668\u7684\u6311\u6218\uff0c\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u5f62\u72b6\u53d8\u5316\u9002\u5e94\u4e0d\u540c\u73af\u5883\u5e76\u7528\u4e8e\u8fd0\u52a8\u7684\u673a\u5668\u4eba\u3002", "method": "\u7ed3\u5408\u6298\u7eb8\u8868\u9762\u987a\u5e94\u6027\u548c\u51e0\u4f55\u987a\u5e94\u9aa8\u67b6\uff0c\u8bbe\u8ba1\u5355\u9a71\u52a8\u79fb\u52a8\u5e73\u53f0\uff0c\u5f00\u53d1\u521a\u5ea6\u6a21\u578b\u548c\u9f7f\u8f6e\u7bb1\u673a\u5236\uff0c\u5e76\u5206\u6790\u7535\u7f06\u9a71\u52a8\u65b9\u6cd5\u3002", "result": "GeoGami\u5e73\u53f0\u5c55\u793a\u4e86\u5f62\u72b6\u53d8\u6362\u548c\u6eda\u52a8\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u529f\u80fd\u3002", "conclusion": "GeoGami\u5e73\u53f0\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u901a\u8fc7\u5f62\u72b6\u53d8\u5316\u9002\u5e94\u73af\u5883\u548c\u7528\u4e8e\u8fd0\u52a8\u7684\u65b0\u529f\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.21496", "pdf": "https://arxiv.org/pdf/2509.21496", "abs": "https://arxiv.org/abs/2509.21496", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yingming Chen", "Cheuk Chi Tsang"], "title": "Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation", "categories": ["cs.RO"], "comment": null, "summary": "The safe operation of quadrotors in near-wall urban or indoor environments\n(e.g., inspection and search-and-rescue missions) is challenged by unmodeled\naerodynamic effects arising from wall-proximity. It generates complex vortices\nthat induce destabilizing suction forces, potentially leading to hazardous\nvibrations or collisions. This paper presents a comprehensive solution\nfeaturing (1) a physics-based suction force model that explicitly characterizes\nthe dependency on both rotor speed and wall distance, and (2) a\nsuction-compensated model predictive control (SC-MPC) framework designed to\nensure accurate and stable trajectory tracking during wall-proximity\noperations. The proposed SC-MPC framework incorporates an enhanced dynamics\nmodel that accounts for suction force effects, formulated as a factor graph\noptimization problem integrating system dynamics constraints, trajectory\ntracking objectives, control input smoothness requirements, and actuator\nphysical limitations. The suction force model parameters are systematically\nidentified through extensive experimental measurements across varying\noperational conditions. Experimental validation demonstrates SC-MPC's superior\nperformance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0\ncm RMSE in Y-axis position control - representing 74% and 79% improvements over\ncascaded proportional-integral-derivative (PID) control, and 60% and 53%\nimprovements over standard MPC respectively. The corresponding mean absolute\nerror (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both\nbaselines. The evaluation platform employs a ducted quadrotor design that\nprovides collision protection while maintaining aerodynamic efficiency. To\nfacilitate reproducibility and community adoption, we have open-sourced our\ncomplete implementation, available at\nhttps://anonymous.4open.science/r/SC-MPC-6A61.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5728\u8fd1\u5899\u73af\u5883\u4e2d\u7a33\u5b9a\u98de\u884c\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u57fa\u4e8e\u7269\u7406\u7684\u5438\u529b\u6a21\u578b\u548c\u5438\u529b\u8865\u507f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u8fd1\u5899\u73af\u5883\u4e0b\u590d\u6742\u7684\u6c14\u52a8\u6548\u5e94\u4f1a\u5bf9\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u5438\u529b\uff0c\u5bfc\u81f4\u5371\u9669\u632f\u52a8\u6216\u78b0\u649e\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u5efa\u6a21\u548c\u63a7\u5236\u7b56\u7565\u63d0\u5347\u98de\u884c\u5b89\u5168\u6027\u3002", "method": "1. \u5efa\u7acb\u57fa\u4e8e\u7269\u7406\u7684\u5438\u529b\u6a21\u578b\uff0c\u660e\u786e\u8868\u5f81\u8f6c\u901f\u548c\u8ddd\u79bb\u5bf9\u5438\u529b\u7684\u5f71\u54cd\uff1b2. \u8bbe\u8ba1\u5438\u529b\u8865\u507f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08SC-MPC\uff09\uff0c\u4f18\u5316\u7cfb\u7edf\u52a8\u6001\u7ea6\u675f\u548c\u8f68\u8ff9\u8ddf\u8e2a\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cSC-MPC\u5728X\u8f74\u548cY\u8f74\u7684\u4f4d\u7f6e\u63a7\u5236\u8bef\u5dee\u5206\u522b\u6bd4PID\u63a7\u5236\u63d0\u5347\u4e8674%\u548c79%\uff0c\u6bd4\u6807\u51c6MPC\u63d0\u5347\u4e8660%\u548c53%\u3002", "conclusion": "SC-MPC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fd1\u5899\u73af\u5883\u4e0b\u7684\u98de\u884c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u5b9e\u73b0\u4ee5\u4fc3\u8fdb\u793e\u533a\u91c7\u7528\u3002"}}
{"id": "2509.21523", "pdf": "https://arxiv.org/pdf/2509.21523", "abs": "https://arxiv.org/abs/2509.21523", "authors": ["Xiaofan Yu", "Yuwei Wu", "Katherine Mao", "Ye Tian", "Vijay Kumar", "Tajana Rosing"], "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking", "categories": ["cs.RO"], "comment": null, "summary": "Multi-robot target tracking is a fundamental problem that requires\ncoordinated monitoring of dynamic entities in applications such as precision\nagriculture, environmental monitoring, disaster response, and security\nsurveillance. While Federated Learning (FL) has the potential to enhance\nlearning across multiple robots without centralized data aggregation, its use\nin multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely\nunderexplored. Key challenges include limited onboard computational resources,\nsignificant data heterogeneity in FL due to varying targets and the fields of\nview, and the need for tight coupling between trajectory prediction and\nmulti-robot planning. In this paper, we introduce DroneFL, the first federated\nlearning framework specifically designed for efficient multi-UAV target\ntracking. We design a lightweight local model to predict target trajectories\nfrom sensor inputs, using a frozen YOLO backbone and a shallow transformer for\nefficient onboard training. The updated models are periodically aggregated in\nthe cloud for global knowledge sharing. To alleviate the data heterogeneity\nthat hinders FL convergence, DroneFL introduces a position-invariant model\narchitecture with altitude-based adaptive instance normalization. Finally, we\nfuse predictions from multiple UAVs in the cloud and generate optimal\ntrajectories that balance target prediction accuracy and overall tracking\nperformance. Our results show that DroneFL reduces prediction error by 6%-83%\nand tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.\nIn terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has\non average just 1.56 KBps data rate to the cloud.", "AI": {"tldr": "DroneFL\u662f\u4e00\u4e2a\u4e13\u4e3a\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u8bbe\u8ba1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u3001\u4f4d\u7f6e\u4e0d\u53d8\u67b6\u6784\u548c\u4e91\u7aef\u591a\u65e0\u4eba\u673a\u9884\u6d4b\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u5728\u7cbe\u51c6\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u6570\u636e\u5f02\u8d28\u6027\u9ad8\u4ee5\u53ca\u8f68\u8ff9\u9884\u6d4b\u4e0e\u89c4\u5212\u7d27\u5bc6\u8026\u5408\u7b49\u6311\u6218\u3002", "method": "DroneFL\u91c7\u7528\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\uff08YOLO\u4e3b\u5e72\u7f51\u7edc+\u6d45\u5c42Transformer\uff09\u8fdb\u884c\u76ee\u6807\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u4f4d\u7f6e\u4e0d\u53d8\u67b6\u6784\u548c\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\uff0c\u6700\u540e\u5728\u4e91\u7aef\u878d\u5408\u9884\u6d4b\u5e76\u89c4\u5212\u6700\u4f18\u8f68\u8ff9\u3002", "result": "\u76f8\u6bd4\u975e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0cDroneFL\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e6%-83%\uff0c\u8ddf\u8e2a\u8ddd\u79bb\u51cf\u5c110.4%-4.6%\uff0c\u4e14\u5728\u6811\u8393\u6d3e5\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u4e91\u7aef\u6570\u636e\u7387\u4ec5\u4e3a1.56 KBps\u3002", "conclusion": "DroneFL\u4e3a\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.21543", "pdf": "https://arxiv.org/pdf/2509.21543", "abs": "https://arxiv.org/abs/2509.21543", "authors": ["Jinbang Huang", "Zhiyuan Li", "Zhanguang Zhang", "Xingyue Quan", "Jianye Hao", "Yingxue Zhang"], "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation", "categories": ["cs.RO"], "comment": "25 pages, 7 figures", "summary": "Large Language Models (LLMs) have recently shown strong potential in robotic\ntask planning, particularly through automatic planning domain generation that\nintegrates symbolic search. Prior approaches, however, have largely treated\nthese domains as search utilities, with limited attention to their potential as\nscalable sources of reasoning data. At the same time, progress in reasoning\nLLMs has been driven by chain-of-thought (CoT) supervision, whose application\nin robotics remains dependent on costly, human-curated datasets. We propose\nPlan2Evolve, an LLM self-evolving framework in which the base model generates\nplanning domains that serve as engines for producing symbolic problem-plan\npairs as reasoning traces. These pairs are then transformed into extended CoT\ntrajectories by the same model through natural-language explanations, thereby\nexplicitly aligning symbolic planning structures with natural language\nreasoning. The resulting data extend beyond the model's intrinsic planning\ncapacity, enabling model fine-tuning that yields a planning-enhanced LLM with\nimproved planning success, stronger cross-task generalization, and reduced\ninference costs.", "AI": {"tldr": "Plan2Evolve\u662f\u4e00\u4e2aLLM\u81ea\u6211\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u89c4\u5212\u9886\u57df\u548c\u95ee\u9898-\u8ba1\u5212\u5bf9\uff0c\u5e76\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4ee5\u589e\u5f3aLLM\u7684\u89c4\u5212\u80fd\u529b\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u89c4\u5212\u9886\u57df\u4f5c\u4e3a\u641c\u7d22\u5de5\u5177\uff0c\u5ffd\u89c6\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u63a8\u7406\u6570\u636e\u7684\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u673a\u5668\u4eba\u9886\u57df\u7684\u63a8\u7406LLM\u4f9d\u8d56\u4e8e\u6210\u672c\u9ad8\u6602\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "method": "\u63d0\u51faPlan2Evolve\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u89c4\u5212\u9886\u57df\u548c\u95ee\u9898-\u8ba1\u5212\u5bf9\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u6269\u5c55\u4e3a\u94fe\u5f0f\u601d\u7ef4\u8f68\u8ff9\uff0c\u4ece\u800c\u5bf9\u9f50\u7b26\u53f7\u89c4\u5212\u4e0e\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u6269\u5c55\u4e86\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u89c4\u5212\u6210\u529f\u7387\u3001\u8de8\u4efb\u52a1\u6cdb\u5316\u548c\u63a8\u7406\u6210\u672c\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Plan2Evolve\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u7684\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.21563", "pdf": "https://arxiv.org/pdf/2509.21563", "abs": "https://arxiv.org/abs/2509.21563", "authors": ["Zhixin Zhang", "Liang Zhao", "Pawel Ladosz"], "title": "PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines", "categories": ["cs.RO"], "comment": "16 pages", "summary": "Vision-based odometry has been widely adopted in autonomous driving owing to\nits low cost and lightweight setup; however, its performance often degrades in\ncomplex outdoor urban environments. To address these challenges, we propose\nPL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates\nan IMU, wheel encoder, and camera (supporting both monocular and stereo) for\nlong-term robust state estimation. The main contributions are: (i) a novel line\nfeature processing framework that exploits the geometric relationship between\n2D feature points and lines, enabling fast and robust line tracking and\ntriangulation while ensuring real-time performance; (ii) an SE(2)-constrained\nSE(3) wheel pre-integration method that leverages the planar motion\ncharacteristics of ground vehicles for accurate wheel updates; and (iii) an\nefficient motion consistency check (MCC) that filters out dynamic features by\njointly using IMU and wheel measurements. Extensive experiments on Monte Carlo\nsimulations and public autonomous driving datasets demonstrate that PL-VIWO2\noutperforms state-of-the-art methods in terms of accuracy, efficiency, and\nrobustness.", "AI": {"tldr": "PL-VIWO2\u662f\u4e00\u79cd\u7ed3\u5408IMU\u3001\u8f66\u8f6e\u7f16\u7801\u5668\u548c\u6444\u50cf\u5934\u7684\u89c6\u89c9-\u60ef\u6027-\u8f66\u8f6e\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u7ebf\u7279\u5f81\u5904\u7406\u548cSE(2)-\u7ea6\u675f\u7684SE(3)\u8f66\u8f6e\u9884\u79ef\u5206\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6574\u5408\u591a\u4f20\u611f\u5668\u7684\u65b9\u6848\u6765\u63d0\u5347\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faPL-VIWO2\u7cfb\u7edf\uff0c\u5229\u7528\u7ebf\u7279\u5f81\u6846\u67b6\u3001SE(2)-\u7ea6\u675f\u7684SE(3)\u8f66\u8f6e\u9884\u79ef\u5206\u65b9\u6cd5\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u68c0\u67e5\uff08MCC\uff09\u6765\u4f18\u5316\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePL-VIWO2\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PL-VIWO2\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u521b\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2509.21571", "pdf": "https://arxiv.org/pdf/2509.21571", "abs": "https://arxiv.org/abs/2509.21571", "authors": ["HaoZhe Xu", "Cheng Cheng", "HongRui Sang", "Zhipeng Wang", "Qiyong He", "Xiuxian Li", "Bin He"], "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots\nis essential for heterogeneous systems, yet most existing approaches target\nwheeled platforms whose limited mobility constrains exploration in complex\nterrains. Quadruped robots offer superior adaptability but undergo frequent\nposture variations, making it difficult to provide a stable landing surface for\nUAVs. To address these challenges, we propose an autonomous UAV-quadruped\ndocking framework for GPS-denied environments. On the quadruped side, a Hybrid\nInternal Model with Horizontal Alignment (HIM-HA), learned via deep\nreinforcement learning, actively stabilizes the torso to provide a level\nplatform. On the UAV side, a three-phase strategy is adopted, consisting of\nlong-range acquisition with a median-filtered YOLOv8 detector, close-range\ntracking with a constraint-aware controller that integrates a Nonsingular Fast\nTerminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function\n(BF) to guarantee finite-time error convergence under field-of-view (FOV)\nconstraints, and terminal descent guided by a Safety Period (SP) mechanism that\njointly verifies tracking accuracy and platform stability. The proposed\nframework is validated in both simulation and real-world scenarios,\nsuccessfully achieving docking on outdoor staircases higher than 17 cm and\nrough slopes steeper than 30 degrees. Supplementary materials and videos are\navailable at: https://uav-quadruped-docking.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u65e0\u4eba\u673a-\u56db\u8db3\u673a\u5668\u4eba\u5bf9\u63a5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86GPS\u7f3a\u5931\u73af\u5883\u4e0b\u7531\u4e8e\u5730\u5f62\u590d\u6742\u548c\u59ff\u52bf\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u7ea6\u675f\u611f\u77e5\u63a7\u5236\u5668\u5b9e\u73b0\u7a33\u5b9a\u5bf9\u63a5\u3002", "motivation": "\u5f02\u6784\u7cfb\u7edf\u4e2d\u7684\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u673a\u5668\u4eba\u81ea\u4e3b\u5bf9\u63a5\u662f\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u8f6e\u5f0f\u5e73\u53f0\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u5730\u5f62\uff1b\u56db\u8db3\u673a\u5668\u4eba\u7075\u6d3b\u6027\u9ad8\u4f46\u59ff\u52bf\u53d8\u5316\u9891\u7e41\uff0c\u9700\u8981\u7a33\u5b9a\u7740\u9646\u9762\u3002", "method": "\u56db\u8db3\u4fa7\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5185\u7f6e\u6a21\u578b\u7a33\u5b9a\u8eaf\u5e72\uff1b\u65e0\u4eba\u673a\u4fa7\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\uff1a\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u3001\u8fd1\u8ddd\u79bb\u8ddf\u8e2a\uff08\u7ed3\u5408NFTSMC\u548c\u5bf9\u6570\u969c\u788d\u51fd\u6570\uff09\u53ca\u7ec8\u7aef\u4e0b\u964d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u6210\u529f\u572817\u5398\u7c73\u9ad8\u53f0\u9636\u548c30\u5ea6\u9661\u5761\u4e0a\u5b9e\u73b0\u5bf9\u63a5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u5730\u5f62\u4e0b\u7684\u5f02\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u81ea\u4e3b\u5bf9\u63a5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.21872", "pdf": "https://arxiv.org/pdf/2509.21872", "abs": "https://arxiv.org/abs/2509.21872", "authors": ["Jan C Olivier", "Etienne Barnard"], "title": "Hidden Markov Model Decoding for LDPC Codes", "categories": ["eess.SP"], "comment": "11 pages, and 9 figures", "summary": "The paper proposes an iterative Hidden Markov Model (HMM) for decoding a Low\nDensity Parity Check (LDPC) code. It is demonstrated that a first-order HMM\nprovides a natural framework for the decoder. The HMM is time-homogeneous with\na fixed transition matrix and is based on a random walk through the encoded\nframe bits. Each hidden state contains a pair of two encoded bits, and parity\nchecks are naturally incorporated into the observation model. The paper shows\nthat by implementing a forward-backward smoothing estimator for the hidden\nstates, decoding is efficient and requires only a small number of iterations in\nmost cases. The results show that the LDPC decoding threshold is significantly\nimproved compared to belief propagation (BP) on a Tanner graph. Numerical\nresults are presented showing that LDPC codes under the proposed decoder yield\na frame error rate (FER) and decoding threshold comparable to that of a Polar\ncode where Successive Cancellation List (SCL) - Cyclic Redundancy Check (CRC)\ndecoding is deployed. This is shown to be achieved even if the frame length is\nshort (on the order of $512$ bits or less) and a regular LDPC code is used. 1", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89e3\u7801LDPC\u7801\u7684\u8fed\u4ee3\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\uff0c\u901a\u8fc7\u524d\u5411\u540e\u5411\u5e73\u6ed1\u4f30\u8ba1\u5668\u9ad8\u6548\u89e3\u7801\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528HMM\u6846\u67b6\u6539\u5584LDPC\u7801\u7684\u89e3\u7801\u6548\u7387\uff0c\u5c24\u5176\u9488\u5bf9\u77ed\u5e27\u957f\u5ea6\u548c\u89c4\u5219LDPC\u7801\u7684\u60c5\u51b5\u3002", "method": "\u91c7\u7528\u4e00\u9636\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u7ed3\u5408\u56fa\u5b9a\u8f6c\u79fb\u77e9\u9635\u548c\u968f\u673a\u6e38\u8d70\uff0c\u901a\u8fc7\u524d\u5411\u540e\u5411\u5e73\u6ed1\u4f30\u8ba1\u5668\u5b9e\u73b0\u9ad8\u6548\u89e3\u7801\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u4fe1\u5ff5\u4f20\u64ad\u65b9\u6cd5\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LDPC\u7801\u7684\u89e3\u7801\u9608\u503c\uff0c\u6027\u80fd\u63a5\u8fd1Polar\u7801\u7684SCL-CRC\u89e3\u7801\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLDPC\u7801\u89e3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65b0\u601d\u8def\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u77ed\u5e27\u957f\u5ea6\u573a\u666f\u3002"}}
{"id": "2509.21602", "pdf": "https://arxiv.org/pdf/2509.21602", "abs": "https://arxiv.org/abs/2509.21602", "authors": ["Yang Jiao", "Yiding Qiu", "Henrik I. Christensen"], "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors", "categories": ["cs.RO"], "comment": null, "summary": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates\nsemantic information for high-level scene understanding, faces challenges of\nunder-constrained optimization due to sparse observations. Prior work has\nintroduced additional constraints using commonsense knowledge, but obtaining\nsuch priors has traditionally been labor-intensive and lacks generalizability\nacross diverse object categories. We address this limitation by leveraging\nlarge language models (LLMs) to provide commonsense knowledge of object\ngeometric attributes, specifically size and orientation, as prior factors in a\ngraph-based SLAM framework. These priors are particularly beneficial during the\ninitial phase when object observations are limited. We implement a complete\npipeline integrating these priors, achieving robust data association on sparse\nobject-level features and enabling real-time object SLAM. Our system, evaluated\non the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over\nthe latest baseline. Additionally, we present real-world experiments in the\nsupplementary video, demonstrating its real-time performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u83b7\u53d6\u7269\u4f53\u51e0\u4f55\u5c5e\u6027\u5148\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5bf9\u8c61\u7ea7SLAM\u4e2d\u7a00\u758f\u89c2\u6d4b\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5bf9\u8c61\u7ea7SLAM\u7531\u4e8e\u7a00\u758f\u89c2\u6d4b\u9762\u4e34\u4f18\u5316\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5148\u9a8c\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u5229\u7528LLMs\u751f\u6210\u7269\u4f53\u51e0\u4f55\u5c5e\u6027\uff08\u5927\u5c0f\u548c\u65b9\u5411\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u57fa\u4e8e\u56fe\u7684SLAM\u6846\u67b6\u4e2d\u3002", "result": "\u5728TUM RGB-D\u548c3RScan\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u6620\u5c04\u7cbe\u5ea6\u63d0\u5347\u4e8636.8%\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7LLMs\u63d0\u4f9b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8bba\u6587\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u5bf9\u8c61\u7ea7SLAM\u7cfb\u7edf\u3002"}}
{"id": "2509.22181", "pdf": "https://arxiv.org/pdf/2509.22181", "abs": "https://arxiv.org/abs/2509.22181", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Pan Zhiwen", "Yuanwei Liu"], "title": "CRB minimization for PASS Assisted ISAC", "categories": ["eess.SP"], "comment": "6 pages, 5 figures, conference", "summary": "A multiple waveguide PASS assisted integrated sensing and communication\n(ISAC) system is proposed, where the base station (BS) is equipped with\ntransmitting pinching antennas (PAs) and receiving uniform linear array (ULA)\nantennas. The PASS-transmitting-ULA-receiving (PTUR) BS transmits the\ncommunication and sensing signals through the stretched PAs on waveguides and\ncollects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraints, and PA deployment constraints. An alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. Simulation results demonstrate that the proposed PASS assisted ISAC\nframework achieves superior performance over benchmark schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6ce2\u5bfcPASS\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u4ee5\u9ad8\u6548\u5229\u7528\u786c\u4ef6\u8d44\u6e90\u5e76\u6ee1\u8db3\u901a\u4fe1\u548c\u670d\u52a1\u8d28\u91cf\u8981\u6c42\u3002", "method": "\u91c7\u7528PASS\u53d1\u5c04\u5929\u7ebf\u548cULA\u63a5\u6536\u5929\u7ebf\u914d\u7f6e\uff0c\u901a\u8fc7\u6ce2\u5bfc\u4f20\u8f93\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u6700\u5c0f\u5316\u76ee\u6807\u611f\u77e5\u7684CRB\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "PASS\u8f85\u52a9\u7684ISAC\u6846\u67b6\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21664", "pdf": "https://arxiv.org/pdf/2509.21664", "abs": "https://arxiv.org/abs/2509.21664", "authors": ["Philippe Nadeau", "Miguel Rogel", "Ivan Bili\u0107", "Ivan Petrovi\u0107", "Jonathan Kelly"], "title": "Generating Stable Placements via Physics-guided Diffusion Models", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2026, Vienna, Austria, June 1-5, 2026", "summary": "Stably placing an object in a multi-object scene is a fundamental challenge\nin robotic manipulation, as placements must be penetration-free, establish\nprecise surface contact, and result in a force equilibrium. To assess\nstability, existing methods rely on running a simulation engine or resort to\nheuristic, appearance-based assessments. In contrast, our approach integrates\nstability directly into the sampling process of a diffusion model. To this end,\nwe query an offline sampling-based planner to gather multi-modal placement\nlabels and train a diffusion model to generate stable placements. The diffusion\nmodel is conditioned on scene and object point clouds, and serves as a\ngeometry-aware prior. We leverage the compositional nature of score-based\ngenerative models to combine this learned prior with a stability-aware loss,\nthereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning,\nand can be directly applied to off-the-shelf models. We evaluate our method on\nfour benchmark scenes where stability can be accurately computed. Our\nphysics-guided models achieve placements that are 56% more robust to forceful\nperturbations while reducing runtime by 47% compared to a state-of-the-art\ngeometric method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u7a33\u5b9a\u6027\u76f4\u63a5\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u591a\u76ee\u6807\u573a\u666f\u4e0b\u7684\u7269\u4f53\u7a33\u5b9a\u653e\u7f6e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eff\u771f\u5f15\u64ce\u6216\u542f\u53d1\u5f0f\u5916\u89c2\u8bc4\u4f30\uff0c\u96be\u4ee5\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8bc4\u4f30\u653e\u7f6e\u7a33\u5b9a\u6027\u3002", "method": "\u5229\u7528\u79bb\u7ebf\u91c7\u6837\u89c4\u5212\u5668\u6536\u96c6\u591a\u6a21\u6001\u653e\u7f6e\u6807\u7b7e\uff0c\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u653e\u7f6e\uff1b\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u5148\u9a8c\u548c\u7a33\u5b9a\u6027\u611f\u77e5\u635f\u5931\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u6700\u4f18\u51e0\u4f55\u65b9\u6cd5\uff0c\u7269\u7406\u5f15\u5bfc\u6a21\u578b\u4f7f\u653e\u7f6e\u6297\u6270\u52a8\u80fd\u529b\u63d0\u9ad856%\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1147%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u7f6e\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.22204", "pdf": "https://arxiv.org/pdf/2509.22204", "abs": "https://arxiv.org/abs/2509.22204", "authors": ["Mohammadhossein Karimi", "Yuanzhe Gong", "Tho Le-Ngoc"], "title": "A Deep Neural Network Codebook Approach for Near-Field Nulling Control Beam Focusing", "categories": ["eess.SP"], "comment": "6 pages, 5 figures, Submitted to IEEE International Conference on\n  Communications 2026", "summary": "This paper proposes a deep neural network (DNN) codebook approach for\nmulti-user interference (MUI) mitigation in extremely large multiple-input\nmultiple-output (XL-MIMO) systems operating in the near-field region. Unlike\nexisting DNN-based nulling control beamforming (NCBF) methods that face\nscalability and complexity challenges, the proposed framework partitions the\nFresnel region using correlation-based sampling and assigns a lightweight fully\nconnected DNN model to each subsection. Each model is trained on beamforming\nweights generated using the linearly constrained minimum variance (LCMV)\nmethod, enabling accurate prediction of nulling control beam-focusing weights\nthat simultaneously optimize the desired signal strength and suppress potential\ninterference for both collinear and non-collinear user configurations.\nSimulation results show that the trained models achieve average phase and\nmagnitude prediction errors of 0.085 radians and 0.52 dB, respectively, across\n75 sample subsections. Full-wave simulations in Ansys HFSS further demonstrate\nthat the proposed DNN codebook achieves interference suppression better than\n31.64 dB, with a performance gap within 2 dB of the LCMV method, thereby\nvalidating its effectiveness in mitigating MUI while reducing computational\ncomplexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDNN\u7684\u7801\u672c\u65b9\u6cd5\uff0c\u7528\u4e8eXL-MIMO\u7cfb\u7edf\u4e2d\u7684\u591a\u7528\u6237\u5e72\u6270\u6291\u5236\uff0c\u901a\u8fc7\u5206\u533a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DNN\u65b9\u6cd5\u5728XL-MIMO\u7cfb\u7edf\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u590d\u6742\u5ea6\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5e72\u6270\u6291\u5236\u65b9\u6848\u3002", "method": "\u5229\u7528\u76f8\u5173\u6027\u91c7\u6837\u5206\u5272\u83f2\u6d85\u5c14\u533a\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u533a\u57df\u8bad\u7ec3\u8f7b\u91cfDNN\u6a21\u578b\uff0c\u57fa\u4e8eLCMV\u751f\u6210\u7684\u6ce2\u675f\u8d4b\u5f62\u6743\u91cd\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u4f4e\uff0c\u5e72\u6270\u6291\u5236\u4f18\u4e8e31.64 dB\uff0c\u6027\u80fd\u63a5\u8fd1LCMV\u65b9\u6cd5\u4f46\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u6291\u5236\u4e86\u591a\u7528\u6237\u5e72\u6270\u3002"}}
{"id": "2509.21690", "pdf": "https://arxiv.org/pdf/2509.21690", "abs": "https://arxiv.org/abs/2509.21690", "authors": ["Muqun Hu", "Wenxi Chen", "Wenjing Li", "Falak Mandali", "Zijian He", "Renhong Zhang", "Praveen Krisna", "Katherine Christian", "Leo Benaharon", "Dizhi Ma", "Karthik Ramani", "Yan Gu"], "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u6620\u5c04\u7403\u7684\u4f4d\u7f6e\u89c2\u5bdf\u5230\u5168\u8eab\u5173\u8282\u6307\u4ee4\uff0c\u4ee5\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u4efb\u52a1\u4e2d\u7684\u5feb\u901f\u611f\u77e5\u548c\u52a8\u4f5c\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u9700\u8981\u5feb\u901f\u611f\u77e5\u3001\u4e3b\u52a8\u5168\u8eab\u8fd0\u52a8\u548c\u654f\u6377\u6b65\u6cd5\uff0c\u4f20\u7edf\u7edf\u4e00\u63a7\u5236\u5668\u96be\u4ee5\u5b9e\u73b0\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u6d4b\u4fe1\u53f7\u548c\u5bc6\u96c6\u7269\u7406\u5f15\u5bfc\u5956\u52b1\uff0c\u76f4\u63a5\u751f\u6210\u5168\u8eab\u5173\u8282\u6307\u4ee4\u3002\u8f7b\u91cf\u7ea7\u5b66\u4e60\u9884\u6d4b\u5668\u7528\u4e8e\u4f30\u8ba1\u672a\u6765\u7403\u7684\u72b6\u6001\uff0c\u8f85\u52a9\u51b3\u7b56\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u7b56\u7565\u5728\u4e0d\u540c\u53d1\u7403\u8303\u56f4\u5185\u8868\u73b0\u51fa\u8272\uff08\u547d\u4e2d\u7387\u226596%\uff0c\u6210\u529f\u7387\u226592%\uff09\u3002\u5b9e\u673a\u6d4b\u8bd5\u663e\u793a\u534f\u8c03\u7684\u6b65\u6cd5\u548c\u5feb\u901f\u56de\u7403\u80fd\u529b\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e3a\u5b9e\u73b0\u591a\u529f\u80fd\u3001\u7ade\u4e89\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.22327", "pdf": "https://arxiv.org/pdf/2509.22327", "abs": "https://arxiv.org/abs/2509.22327", "authors": ["Zheao Li", "Jiancheng An", "Chau Yuen"], "title": "Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM Communications", "categories": ["eess.SP"], "comment": null, "summary": "Leveraging the multilayer realization of programmable metasurfaces, stacked\nintelligent metasurfaces (SIM) enable fine-grained wave-domain control.\nHowever, their wideband deployment is impeded by two structural factors: (i) a\nsingle, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)\nmultiuser scheduling changes the subcarrier activation pattern frame by frame,\nrequiring rapid reconfiguration. To address both challenges, we develop a\nSIM-enhanced wideband multiuser transceiver built on orthogonal\nfrequency-division multiplexing with index modulation (OFDM-IM). The sparse\nactivation of OFDM-IM confines high-fidelity equalization to the active tones,\neffectively widening the usable bandwidth. To make the design\nreliability-aware, we directly target the worst-link bit-error rate (BER) and\nadopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a\nprincipled surrogate, turning the reliability optimization tractable. For\nframe-rate inference and interpretability, we propose an unfolded\nprojected-gradient-descent network (UPGD-Net) that double-unrolls across the\nSIM's layers and algorithmic iterations: each cell computes the analytic\ngradient from the cascaded precoder with a learnable per-iteration step size.\nSimulations on wideband multiuser downlinks show fast, monotone convergence, an\nevident layer-depth sweet spot, and consistent gains in worst-link BER and sum\nrate. By combining structural sparsity with a BER-driven, deep-unfolded\noptimization backbone, the proposed framework directly addresses the key\nwideband deficiencies of SIM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOFDM-IM\u7684SIM\u589e\u5f3a\u5bbd\u5e26\u591a\u7528\u6237\u6536\u53d1\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u548c\u6df1\u5ea6\u5c55\u5f00\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u5bbd\u5e26\u90e8\u7f72\u4e2d\u7684\u9759\u6001\u76f8\u4f4d\u5f20\u91cf\u548c\u5feb\u901f\u91cd\u914d\u7f6e\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u53ef\u7f16\u7a0b\u8d85\u8868\u9762\u7684\u591a\u5c42\u5b9e\u73b0\uff08SIM\uff09\u5728\u5bbd\u5e26\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u9759\u6001\u76f8\u4f4d\u5f20\u91cf\u9002\u5e94\u6027\u548c\u591a\u7528\u6237\u8c03\u5ea6\u5feb\u901f\u91cd\u914d\u7f6e\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eOFDM-IM\u7684\u6536\u53d1\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u9650\u5236\u9ad8\u4fdd\u771f\u5747\u8861\uff0c\u5e76\u91c7\u7528max-min SINR\u4f5c\u4e3a\u53ef\u9760\u6027\u4f18\u5316\u7684\u66ff\u4ee3\u6307\u6807\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5c55\u5f00\u7684UPGD-Net\u3002", "result": "\u4eff\u771f\u663e\u793a\u5feb\u901f\u5355\u8c03\u6536\u655b\u3001\u660e\u663e\u7684\u5c42\u6df1\u4f18\u5316\u70b9\u4ee5\u53ca\u6700\u5dee\u94fe\u8defBER\u548c\u603b\u901f\u7387\u7684\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u7a00\u758f\u6027\u548cBER\u9a71\u52a8\u7684\u6df1\u5ea6\u5c55\u5f00\u4f18\u5316\uff0c\u76f4\u63a5\u89e3\u51b3\u4e86SIM\u5728\u5bbd\u5e26\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u7f3a\u9677\u3002"}}
{"id": "2509.21723", "pdf": "https://arxiv.org/pdf/2509.21723", "abs": "https://arxiv.org/abs/2509.21723", "authors": ["Huayi Zhou", "Kui Jia"], "title": "VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation", "categories": ["cs.RO"], "comment": "under review", "summary": "Achieving generalizable bimanual manipulation requires systems that can learn\nefficiently from minimal human input while adapting to real-world uncertainties\nand diverse embodiments. Existing approaches face a dilemma: imitation policy\nlearning demands extensive demonstrations to cover task variations, while\nmodular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,\na framework that derives reusable skills from a single human example through\ntask-aware decomposition, preserving invariant primitives as anchors while\ndynamically adapting adjustable components via vision-language grounding. This\nadaptation mechanism resolves scene ambiguities caused by background changes,\nobject repositioning, or visual clutter without policy retraining, leveraging\nsemantic parsing and geometric feasibility constraints. Moreover, the system\ninherits human-like hybrid control capabilities, enabling mixed synchronous and\nasynchronous use of both arms. Extensive experiments validate VLBiMan across\ntool-use and multi-object tasks, demonstrating: (1) a drastic reduction in\ndemonstration requirements compared to imitation baselines, (2) compositional\ngeneralization through atomic skill splicing for long-horizon tasks, (3)\nrobustness to novel but semantically similar objects and external disturbances,\nand (4) strong cross-embodiment transfer, showing that skills learned from\nhuman demonstrations can be instantiated on different robotic platforms without\nretraining. By bridging human priors with vision-language anchored adaptation,\nour work takes a step toward practical and versatile dual-arm manipulation in\nunstructured settings.", "AI": {"tldr": "VLBiMan\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\uff0c\u4ece\u5355\u4e2a\u4eba\u7c7b\u793a\u4f8b\u4e2d\u5b66\u4e60\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u6f14\u793a\u9700\u6c42\uff0c\u652f\u6301\u957f\u65f6\u4efb\u52a1\u7ec4\u5408\u6cdb\u5316\uff0c\u5e76\u5b9e\u73b0\u8de8\u5e73\u53f0\u6280\u80fd\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53cc\u624b\u673a\u5668\u4eba\u64cd\u7eb5\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u9002\u5e94\u6027\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u5982\u9700\u8981\u5927\u91cf\u6f14\u793a\u6216\u7f3a\u4e4f\u52a8\u6001\u573a\u666f\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5206\u89e3\u63d0\u53d6\u4e0d\u53d8\u539f\u8bed\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u52a8\u6001\u8c03\u6574\u9002\u5e94\u7ec4\u4ef6\uff0c\u7ed3\u5408\u8bed\u4e49\u89e3\u6790\u548c\u51e0\u4f55\u53ef\u884c\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aVLBiMan\u5728\u51cf\u5c11\u6f14\u793a\u9700\u6c42\u3001\u7ec4\u5408\u6cdb\u5316\u3001\u9c81\u68d2\u6027\u548c\u8de8\u5e73\u53f0\u6280\u80fd\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VLBiMan\u7ed3\u5408\u4eba\u7c7b\u5148\u9a8c\u548c\u89c6\u89c9\u8bed\u8a00\u9002\u5e94\u673a\u5236\uff0c\u4e3a\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53cc\u624b\u673a\u5668\u4eba\u64cd\u7eb5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22396", "pdf": "https://arxiv.org/pdf/2509.22396", "abs": "https://arxiv.org/abs/2509.22396", "authors": ["Yuhao Chen", "Boxiang He", "Shilian Wang", "Jing Lei"], "title": "Specific multi-emitter identification via multi-label learning", "categories": ["eess.SP"], "comment": null, "summary": "Specific emitter identification leverages hardware-induced impairments to\nuniquely determine a specific transmitter. However, existing approaches fail to\naddress scenarios where signals from multiple emitters overlap. In this paper,\nwe propose a specific multi-emitter identification (SMEI) method via\nmulti-label learning to determine multiple transmitters. Specifically, the\nmulti-emitter fingerprint extractor is designed to mitigate the mutual\ninterference among overlapping signals. Then, the multi-emitter decision maker\nis proposed to assign the all emitter identification using the previous\nextracted fingerprint. Experimental results demonstrate that, compared with\nbaseline approach, the proposed SMEI scheme achieves comparable identification\naccuracy under various overlapping conditions, while operating at significantly\nlower complexity. The significance of this paper is to identify multiple\nemitters from overlapped signal with a low complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6807\u7b7e\u5b66\u4e60\u7684\u591a\u53d1\u5c04\u5668\u8bc6\u522b\u65b9\u6cd5\uff08SMEI\uff09\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6307\u7eb9\u63d0\u53d6\u5668\u548c\u51b3\u7b56\u5668\uff0c\u6709\u6548\u8bc6\u522b\u91cd\u53e0\u4fe1\u53f7\u4e2d\u7684\u591a\u4e2a\u53d1\u5c04\u5668\uff0c\u4e14\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u4e2a\u53d1\u5c04\u5668\u4fe1\u53f7\u91cd\u53e0\u7684\u60c5\u51b5\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8bc6\u522b\u591a\u4e2a\u53d1\u5c04\u5668\u7684\u4f4e\u590d\u6742\u5ea6\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u53d1\u5c04\u5668\u6307\u7eb9\u63d0\u53d6\u5668\u548c\u51b3\u7b56\u5668\uff0c\u524d\u8005\u51cf\u5c11\u4fe1\u53f7\u95f4\u5e72\u6270\uff0c\u540e\u8005\u57fa\u4e8e\u63d0\u53d6\u7684\u6307\u7eb9\u8bc6\u522b\u6240\u6709\u53d1\u5c04\u5668\u3002", "result": "\u5728\u4e0d\u540c\u91cd\u53e0\u6761\u4ef6\u4e0b\uff0cSMEI\u65b9\u6cd5\u8fbe\u5230\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "SMEI\u65b9\u6cd5\u80fd\u591f\u4ee5\u4f4e\u590d\u6742\u5ea6\u6709\u6548\u8bc6\u522b\u91cd\u53e0\u4fe1\u53f7\u4e2d\u7684\u591a\u4e2a\u53d1\u5c04\u5668\u3002"}}
{"id": "2509.21776", "pdf": "https://arxiv.org/pdf/2509.21776", "abs": "https://arxiv.org/abs/2509.21776", "authors": ["Hyeonseong Kim", "Roy El-Helou", "Seungbeen Lee", "Sungjoon Choi", "Matthew Pan"], "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions", "categories": ["cs.RO", "cs.HC"], "comment": "for more videos, see\n  https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/", "summary": "Playful deception, a common feature in human social interactions, remains\nunderexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice\nCream (TIC) vendor routine, we investigate how bounded, culturally familiar\nforms of deception influence user trust, enjoyment, and engagement during\nrobotic handovers. We design a robotic manipulator equipped with a custom\nend-effector and implement five TIC-inspired trick policies that deceptively\ndelay the handover of an ice cream-shaped object. Through a mixed-design user\nstudy with 91 participants, we evaluate the effects of playful deception and\ninteraction duration on user experience. Results reveal that TIC-inspired\ndeception significantly enhances enjoyment and engagement, though reduces\nperceived safety and trust, suggesting a structured trade-off across the\nmulti-dimensional aspects. Our findings demonstrate that playful deception can\nbe a valuable design strategy for interactive robots in entertainment and\nengagement-focused contexts, while underscoring the importance of deliberate\nconsideration of its complex trade-offs. You can find more information,\nincluding demonstration videos, on\nhttps://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u9500\u552e\u5546\u4e92\u52a8\u7684\u6e38\u620f\u6027\u6b3a\u9a97\u5bf9\u7528\u6237\u4fe1\u4efb\u3001\u4eab\u53d7\u548c\u53c2\u4e0e\u611f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fd9\u79cd\u6b3a\u9a97\u80fd\u63d0\u5347\u4e50\u8da3\u548c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u4f1a\u964d\u4f4e\u611f\u77e5\u5b89\u5168\u548c\u4fe1\u4efb\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u6e38\u620f\u6027\u6b3a\u9a97\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5176\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u591a\u7ef4\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u914d\u5907\u5b9a\u5236\u672b\u7aef\u6267\u884c\u5668\u7684\u673a\u5668\u4eba\u673a\u68b0\u81c2\uff0c\u5b9e\u73b0\u4e86\u4e94\u79cd\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u98ce\u683c\u7684\u6b3a\u9a97\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u4e8691\u540d\u53c2\u4e0e\u8005\u7684\u6df7\u5408\u8bbe\u8ba1\u7528\u6237\u7814\u7a76\u3002", "result": "\u6e38\u620f\u6027\u6b3a\u9a97\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u4eab\u53d7\u548c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u964d\u4f4e\u4e86\u611f\u77e5\u5b89\u5168\u548c\u4fe1\u4efb\u3002", "conclusion": "\u6e38\u620f\u6027\u6b3a\u9a97\u662f\u5a31\u4e50\u548c\u4e92\u52a8\u5bfc\u5411\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u6709\u6548\u7b56\u7565\uff0c\u4f46\u4e5f\u9700\u6743\u8861\u5176\u590d\u6742\u5f71\u54cd\u3002"}}
{"id": "2509.22423", "pdf": "https://arxiv.org/pdf/2509.22423", "abs": "https://arxiv.org/abs/2509.22423", "authors": ["Marcin Wachowiak", "Andr\u00e9 Bourdoux", "Sofie Pollin"], "title": "Approximation of the Range Ambiguity Function in Near-field Sensing Systems", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Radar Systems", "summary": "This paper investigates the range ambiguity function of near-field systems\nwhere bandwidth and near-field beamfocusing jointly determine the resolution.\nFirst, the general matched filter ambiguity function is derived and the\nnear-field array factors of different antenna array geometries are introduced.\nNext, the near-field ambiguity function is approximated as a product of the\nrange-dependent near-field array factor and the ambiguity function due to the\nutilized bandwidth and waveform. An approximation criterion based on the\naperture-bandwidth product is formulated, and its accuracy is examined.\nFinally, the improvements to the ambiguity function offered by the near-field\nbeamfocusing, as compared to the far-field case, are presented. The performance\ngains are evaluated in terms of resolution improvement offered by beamfocusing,\npeak-to-sidelobe and integrated-sidelobe level improvement. The gains offered\nby the near-field regime are shown to be range-dependent and substantial only\nin close proximity to the array.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8fd1\u573a\u7cfb\u7edf\u4e2d\u8303\u56f4\u548c\u5e26\u5bbd\u8054\u5408\u51b3\u5b9a\u7684\u6a21\u7cca\u51fd\u6570\uff0c\u63d0\u51fa\u4e86\u9635\u5217\u56e0\u5b50\u548c\u5e26\u5bbd\u6ce2\u5f62\u7684\u8fd1\u4f3c\u51c6\u5219\uff0c\u5e76\u5206\u6790\u4e86\u8fd1\u573a\u6ce2\u675f\u805a\u7126\u7684\u6027\u80fd\u589e\u76ca\u3002", "motivation": "\u63a2\u8ba8\u8fd1\u573a\u7cfb\u7edf\u4e2d\u8303\u56f4\u548c\u5e26\u5bbd\u5982\u4f55\u5171\u540c\u5f71\u54cd\u5206\u8fa8\u7387\u7684\u6a21\u7cca\u51fd\u6570\u3002", "method": "\u63a8\u5bfc\u5339\u914d\u6ee4\u6ce2\u6a21\u7cca\u51fd\u6570\uff0c\u5f15\u5165\u4e0d\u540c\u5929\u7ebf\u9635\u5217\u51e0\u4f55\u7684\u8fd1\u573a\u9635\u5217\u56e0\u5b50\uff0c\u8fd1\u4f3c\u4e3a\u8303\u56f4\u548c\u5e26\u5bbd\u6ce2\u5f62\u7684\u7ec4\u5408\uff0c\u63d0\u51fa\u5b54\u5f84-\u5e26\u5bbd\u4e58\u79ef\u51c6\u5219\u3002", "result": "\u8fd1\u573a\u6ce2\u675f\u805a\u7126\u5728\u5206\u8fa8\u7387\u3001\u5cf0\u503c\u65c1\u74e3\u6bd4\u548c\u79ef\u5206\u65c1\u74e3\u6c34\u5e73\u4e0a\u6709\u663e\u8457\u6539\u5584\uff0c\u4f46\u589e\u76ca\u968f\u8ddd\u79bb\u53d8\u5316\u4e14\u4ec5\u5728\u8fd1\u573a\u663e\u8457\u3002", "conclusion": "\u8fd1\u573a\u6ce2\u675f\u805a\u7126\u5728\u8fd1\u8ddd\u79bb\u5185\u80fd\u663e\u8457\u6539\u5584\u6a21\u7cca\u51fd\u6570\u6027\u80fd\uff0c\u4f46\u589e\u76ca\u968f\u8ddd\u79bb\u589e\u52a0\u800c\u51cf\u5f31\u3002"}}
{"id": "2509.21810", "pdf": "https://arxiv.org/pdf/2509.21810", "abs": "https://arxiv.org/abs/2509.21810", "authors": ["Ning Huang", "Zhentao Xie", "Qinchuan Li"], "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors", "categories": ["cs.RO"], "comment": null, "summary": "Despite growing interest in developing legged robots that emulate biological\nlocomotion for agile navigation of complex environments, acquiring a diverse\nrepertoire of skills remains a fundamental challenge in robotics. Existing\nmethods can learn motion behaviors from expert data, but they often fail to\nacquire multiple locomotion skills through a single policy and lack smooth\nskill transitions. We propose a multi-skill learning framework based on\nConditional Adversarial Motion Priors (CAMP), with the aim of enabling\nquadruped robots to efficiently acquire a diverse set of locomotion skills from\nexpert demonstrations. Precise skill reconstruction is achieved through a novel\nskill discriminator and skill-conditioned reward design. The overall framework\nsupports the active control and reuse of multiple skills, providing a practical\nsolution for learning generalizable policies in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08CAMP\uff09\u7684\u591a\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u9ad8\u6548\u5b66\u4e60\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6280\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u5b66\u4e60\u591a\u79cd\u8fd0\u52a8\u6280\u80fd\u4e14\u7f3a\u4e4f\u5e73\u6ed1\u8fc7\u6e21\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6280\u80fd\u5224\u522b\u5668\u548c\u6280\u80fd\u6761\u4ef6\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u7cbe\u786e\u6280\u80fd\u91cd\u5efa\u548c\u591a\u6280\u80fd\u4e3b\u52a8\u63a7\u5236\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u591a\u6837\u5316\u6280\u80fd\uff0c\u5e76\u652f\u6301\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b66\u4e60\u6cdb\u5316\u6027\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u591a\u6280\u80fd\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u3002"}}
{"id": "2509.21572", "pdf": "https://arxiv.org/pdf/2509.21572", "abs": "https://arxiv.org/abs/2509.21572", "authors": ["Jakob M\u00f6derl", "Erik Leitinger", "Bernard Henri Fleury"], "title": "General Pruning Criteria for Fast SBL", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "5 pages, 2 figures, submitted to IEEE Signal Processing Letters", "summary": "Sparse Bayesian learning (SBL) associates to each weight in the underlying\nlinear model a hyperparameter by assuming that each weight is Gaussian\ndistributed with zero mean and precision (inverse variance) equal to its\nassociated hyperparameter. The method estimates the hyperparameters by\nmarginalizing out the weights and performing (marginalized) maximum likelihood\n(ML) estimation. SBL returns many hyperparameter estimates to diverge to\ninfinity, effectively setting the estimates of the corresponding weights to\nzero (i.e., pruning the corresponding weights from the model) and thereby\nyielding a sparse estimate of the weight vector.\n  In this letter, we analyze the marginal likelihood as function of a single\nhyperparameter while keeping the others fixed, when the Gaussian assumptions on\nthe noise samples and the weight distribution that underlies the derivation of\nSBL are weakened. We derive sufficient conditions that lead, on the one hand,\nto finite hyperparameter estimates and, on the other, to infinite ones.\nFinally, we show that in the Gaussian case, the two conditions are\ncomplementary and coincide with the pruning condition of fast SBL (F-SBL),\nthereby providing additional insights into this algorithm.", "AI": {"tldr": "\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\uff08SBL\uff09\u901a\u8fc7\u5047\u8bbe\u6743\u91cd\u4e3a\u96f6\u5747\u503c\u4e14\u7cbe\u5ea6\u4e3a\u8d85\u53c2\u6570\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4f30\u8ba1\u8d85\u53c2\u6570\u5e76\u5b9e\u73b0\u6743\u91cd\u7a00\u758f\u3002\u672c\u6587\u5206\u6790\u4e86\u5728\u51cf\u5f31\u9ad8\u65af\u5047\u8bbe\u4e0b\u5355\u4e2a\u8d85\u53c2\u6570\u7684\u8fb9\u9645\u4f3c\u7136\uff0c\u63a8\u5bfc\u4e86\u8d85\u53c2\u6570\u4f30\u8ba1\u6709\u9650\u6216\u65e0\u9650\u7684\u6761\u4ef6\uff0c\u5e76\u5728\u9ad8\u65af\u60c5\u51b5\u4e0b\u4e0eF-SBL\u7684\u526a\u679d\u6761\u4ef6\u4e00\u81f4\u3002", "motivation": "\u7814\u7a76\u5f31\u5316\u9ad8\u65af\u5047\u8bbe\u5bf9\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\uff08SBL\uff09\u4e2d\u8d85\u53c2\u6570\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u8fdb\u4e00\u6b65\u7406\u89e3SBL\u7684\u7a00\u758f\u6027\u673a\u5236\u3002", "method": "\u5206\u6790\u5355\u8d85\u53c2\u6570\u7684\u8fb9\u9645\u4f3c\u7136\u51fd\u6570\uff0c\u56fa\u5b9a\u5176\u4ed6\u8d85\u53c2\u6570\uff0c\u63a8\u5bfc\u8d85\u53c2\u6570\u4f30\u8ba1\u6709\u9650\u6216\u65e0\u9650\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u5728\u9ad8\u65af\u60c5\u51b5\u4e0b\uff0c\u63a8\u5bfc\u7684\u6761\u4ef6\u4e0eF-SBL\u7684\u526a\u679d\u6761\u4ef6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f31\u5316\u9ad8\u65af\u5047\u8bbe\u540e\uff0cSBL\u7684\u7a00\u758f\u6027\u6761\u4ef6\u4f9d\u7136\u6210\u7acb\uff0c\u4e3a\u7406\u89e3\u5176\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.21873", "pdf": "https://arxiv.org/pdf/2509.21873", "abs": "https://arxiv.org/abs/2509.21873", "authors": ["Nishant Doshi"], "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors", "categories": ["cs.RO"], "comment": null, "summary": "Conventional maneuver prediction methods use some sort of classification\nmodel on temporal trajectory data to predict behavior of agents over a set time\nhorizon. Despite of having the best precision and recall, these models cannot\npredict a lane change accurately unless they incorporate information about the\nentire scene. Level-k game theory can leverage the human-like hierarchical\nreasoning to come up with the most rational decisions each agent can make in a\ngroup. This can be leveraged to model interactions between different vehicles\nin presence of each other and hence compute the most rational decisions each\nagent would make. The result of game theoretic evaluation can be used as a\n\"prior\" or combined with a traditional motion-based classification model to\nachieve more accurate predictions. The proposed approach assumes that the\nstates of the vehicles around the target lead vehicle are known. The module\nwill output the most rational maneuver prediction of the target vehicle based\non an online optimization solution. These predictions are instrumental in\ndecision making systems like Adaptive Cruise Control (ACC) or Traxen's\niQ-Cruise further improving the resulting fuel savings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u535a\u5f08\u8bba\u4e0e\u4f20\u7edf\u8f68\u8ff9\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u8f66\u8f86\u53d8\u9053\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u6a21\u578b\u5728\u7f3a\u4e4f\u5168\u5c40\u573a\u666f\u4fe1\u606f\u65f6\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u53d8\u9053\u884c\u4e3a\uff0c\u535a\u5f08\u8bba\u53ef\u6a21\u62df\u591a\u8f66\u95f4\u7684\u7406\u6027\u51b3\u7b56\u3002", "method": "\u5229\u7528Level-k\u535a\u5f08\u8bba\u5efa\u6a21\u8f66\u8f86\u4e92\u52a8\uff0c\u7ed3\u5408\u4f20\u7edf\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u7ebf\u4f18\u5316\u8f93\u51fa\u6700\u7406\u6027\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u76ee\u6807\u8f66\u8f86\u7684\u53d8\u9053\u884c\u4e3a\uff0c\u63d0\u5347\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7b49\u7cfb\u7edf\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u535a\u5f08\u8bba\u4e0e\u8fd0\u52a8\u5206\u7c7b\u7ed3\u5408\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.21594", "pdf": "https://arxiv.org/pdf/2509.21594", "abs": "https://arxiv.org/abs/2509.21594", "authors": ["Weitai Qian", "Rishad Raiyan Joarder", "Randall Fowler", "Begum Kasap", "Mahya Saffarpour", "Kourosh Vali", "Tailai Lihe", "Aijun Wang", "Diana Farmer", "Soheil Ghiasi"], "title": "Transabdominal Fetal Oximetry via Diffuse Optics: Principled Analysis and Demonstration in Pregnant Ovine Models", "categories": ["eess.IV", "eess.SP", "q-bio.QM"], "comment": "18 pages, 14 figures", "summary": "Diffuse optics has the potential to offer a substantial advancement in fetal\nhealth monitoring via enabling continuous measurement of fetal blood oxygen\nsaturation (fSpO$_2$). Aiming to enhance the sensing accuracy and to elucidate\nthe foundational limits of Transabdominal Fetal Oximetry (TFO) via diffuse\noptics, we introduce a theoretical derivation, and a comprehensive pipeline for\nfSpO$_2$ estimation from non-invasively sensed diffuse light intensity values,\nwhich are leveraged to analyze datasets obtained through both simulations and\nin-vivo experiments in gold standard large animal model of pregnancy. We\npropose the Exponential Pulsation Ratio (EPR) as a key feature, and develop\nmachine-learning models to fuse the information collected across multiple\ndetectors. Our proposed method demonstrates a Mean Absolute Error (MAE) of\n4.81% and 6.85% with a Pearson's r correlation of 0.81 (p<0.001) and 0.71\n(p<0.001) for estimation of fSpO$_2$ in simulated dataset and in-vivo dataset,\nrespectively. Across both datasets, our method outperforms the existing\napproaches, enhancing the accuracy of the fSpO$_2$ estimation and demonstrates\nits viability as a supplemental technology for intrapartum fetal monitoring.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5149\u5b66\u7684\u65e0\u521b\u80ce\u513f\u8840\u6c27\u9971\u548c\u5ea6\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u591a\u68c0\u6d4b\u5668\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u91cf\u7cbe\u5ea6\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6269\u6563\u5149\u5b66\u6280\u672f\u63d0\u5347\u80ce\u513f\u8840\u6c27\u9971\u548c\u5ea6\u76d1\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u6280\u672f\u6781\u9650\u3002", "method": "\u63d0\u51fa\u6307\u6570\u640f\u52a8\u6bd4\uff08EPR\uff09\u4f5c\u4e3a\u5173\u952e\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u878d\u5408\u591a\u68c0\u6d4b\u5668\u6570\u636e\u3002", "result": "\u5728\u4eff\u771f\u548c\u52a8\u7269\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a4.81%\u548c6.85%\uff0c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5206\u522b\u4e3a0.81\u548c0.71\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6709\u671b\u6210\u4e3a\u4ea7\u65f6\u80ce\u513f\u76d1\u6d4b\u7684\u8865\u5145\u6280\u672f\u3002"}}
{"id": "2509.21878", "pdf": "https://arxiv.org/pdf/2509.21878", "abs": "https://arxiv.org/abs/2509.21878", "authors": ["Moses Gladson Selvamuthu", "Tomoya Takahashi", "Riichiro Tadakuma", "Kazutoshi Tanaka"], "title": "WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulators capable of regulating both compliance and stiffness\noffer enhanced operational safety and versatility. Here, we introduce Worm\nGear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator\n(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving\nmotor from external forces using this gear, WAVE enables precise force\ntransmission to the joint, while absorbing positional discrepancies through\ncompliance. WAVE is protected from excessive loads by converting impact forces\ninto elastic energy stored in a spring. In addition, the actuator achieves\ncontinuous joint stiffness modulation by changing the spring's precompression\nlength. We demonstrate these capabilities, experimentally validate the proposed\nstiffness model, show that motor loads approach zero at rest--even under\nexternal loading--and present applications using a manipulator with WAVE. This\noutcome showcases the successful decoupling of external forces. The protective\nattributes of this actuator allow for extended operation in contact-intensive\ntasks, and for robust robotic applications in challenging environments.", "AI": {"tldr": "WAVE\u662f\u4e00\u79cd\u96c6\u6210\u8717\u8f6e\u7684\u975e\u53cd\u5411\u9a71\u52a8\u53ef\u53d8\u521a\u5ea6\u6267\u884c\u5668\uff0c\u901a\u8fc7\u5f39\u6027\u5f39\u7c27\u5438\u6536\u5916\u529b\u5e76\u8c03\u8282\u521a\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5916\u529b\u89e3\u8026\u548c\u5b89\u5168\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u673a\u68b0\u624b\u7684\u5b89\u5168\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8c03\u8282\u521a\u5ea6\u548c\u987a\u5e94\u6027\u7684\u6267\u884c\u5668\u3002", "method": "\u96c6\u6210\u975e\u53cd\u5411\u9a71\u52a8\u8717\u8f6e\uff0c\u901a\u8fc7\u5f39\u7c27\u5b58\u50a8\u5f39\u6027\u80fd\u91cf\uff0c\u5e76\u8c03\u8282\u5f39\u7c27\u9884\u538b\u7f29\u957f\u5ea6\u5b9e\u73b0\u521a\u5ea6\u8c03\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u521a\u5ea6\u6a21\u578b\uff0c\u7535\u673a\u8d1f\u8f7d\u5728\u9759\u6b62\u65f6\u63a5\u8fd1\u96f6\uff0c\u5c55\u793a\u4e86\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "WAVE\u6210\u529f\u89e3\u8026\u5916\u529b\uff0c\u9002\u7528\u4e8e\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u7a33\u5065\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2509.21601", "pdf": "https://arxiv.org/pdf/2509.21601", "abs": "https://arxiv.org/abs/2509.21601", "authors": ["Jason Anderson"], "title": "World's First Authenticated Satellite Pseudorange from Orbit", "categories": ["cs.CR", "eess.SP"], "comment": "Pending publication:\n  https://www.ion.org/gnss/abstracts.cfm?paperID=16052", "summary": "Cryptographic Ranging Authentication is here! We present initial results on\nthe Pulsar authenticated ranging service broadcast from space with Pulsar-0\nutilizing a recording taken at Xona headquarters in Burlingame, CA. No\nassumptions pertaining to the ownership or leakage of encryption keys are\nrequired. This work discusses the Pulsar watermark design and security\nanalysis. We derive the Pulsar watermark's probabilities of missed detection\nand false alarm, and we discuss the required receiver processing needed to\nutilize the Pulsar watermark. We present validation results of the Pulsar\nwatermark utilizing the transmissions from orbit. Lastly, we provide results\nthat demonstrate the spoofing detection efficacy with a spoofing scenario that\nincorporates the authentic transmissions from orbit. Because we make no\nassumption about the leakage of symmetric encryption keys, this work provides\nmathematical justification of the watermark's security, and our July 2025\ntransmissions from orbit, we claim the world's first authenticated satellite\npseudorange from orbit.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Pulsar\u8ba4\u8bc1\u6d4b\u8ddd\u670d\u52a1\u7684\u521d\u6b65\u7ed3\u679c\uff0c\u5305\u62ec\u6c34\u5370\u8bbe\u8ba1\u3001\u5b89\u5168\u6027\u5206\u6790\u53ca\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5176\u53cd\u6b3a\u9a97\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u4f9b\u65e0\u9700\u5047\u8bbe\u52a0\u5bc6\u5bc6\u94a5\u6cc4\u6f0f\u7684\u8ba4\u8bc1\u6d4b\u8ddd\u670d\u52a1\uff0c\u5b9e\u73b0\u4e16\u754c\u4e0a\u9996\u6b21\u8ba4\u8bc1\u536b\u661f\u4f2a\u8ddd\u6d4b\u8ddd\u3002", "method": "\u8bbe\u8ba1Pulsar\u6c34\u5370\uff0c\u5206\u6790\u5176\u5b89\u5168\u6027\u548c\u68c0\u6d4b\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u8f68\u9053\u4f20\u8f93\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPulsar\u6c34\u5370\u5177\u6709\u8f83\u9ad8\u7684\u6b3a\u9a97\u68c0\u6d4b\u6548\u80fd\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5bf9\u79f0\u5bc6\u94a5\u6cc4\u6f0f\u5047\u8bbe\u3002", "conclusion": "Pulsar\u8ba4\u8bc1\u6d4b\u8ddd\u670d\u52a1\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u536b\u661f\u4f2a\u8ddd\u8ba4\u8bc1\u3002"}}
{"id": "2509.21928", "pdf": "https://arxiv.org/pdf/2509.21928", "abs": "https://arxiv.org/abs/2509.21928", "authors": ["Jialiang Li", "Wenzheng Wu", "Gaojing Zhang", "Yifan Han", "Wenzhao Lian"], "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Successfully solving long-horizon manipulation tasks remains a fundamental\nchallenge. These tasks involve extended action sequences and complex object\ninteractions, presenting a critical gap between high-level symbolic planning\nand low-level continuous control. To bridge this gap, two essential\ncapabilities are required: robust long-horizon task planning and effective\ngoal-conditioned manipulation. Existing task planning methods, including\ntraditional and LLM-based approaches, often exhibit limited generalization or\nsparse semantic reasoning. Meanwhile, image-conditioned control methods\nstruggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a\nnovel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon\nManipulation Tasks. SAGE utilizes semantic scene graphs as a structural\nrepresentation for scene states. A structural scene graph enables bridging\ntask-level semantic reasoning and pixel-level visuo-motor control. This also\nfacilitates the controllable synthesis of accurate, novel sub-goal images. SAGE\nconsists of two key components: (1) a scene graph-based task planner that uses\nVLMs and LLMs to parse the environment and reason about physically-grounded\nscene state transition sequences, and (2) a decoupled structural image editing\npipeline that controllably converts each target sub-goal graph into a\ncorresponding image through image inpainting and composition. Extensive\nexperiments have demonstrated that SAGE achieves state-of-the-art performance\non distinct long-horizon tasks.", "AI": {"tldr": "SAGE\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u8bed\u4e49\u573a\u666f\u56fe\u4f5c\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff0c\u7ed3\u5408\u4efb\u52a1\u89c4\u5212\u4e0e\u56fe\u50cf\u7f16\u8f91\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u6d89\u53ca\u590d\u6742\u7684\u52a8\u4f5c\u5e8f\u5217\u548c\u5bf9\u8c61\u4ea4\u4e92\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u8bed\u4e49\u63a8\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "SAGE\u901a\u8fc7\u8bed\u4e49\u573a\u666f\u56fe\u8fde\u63a5\u4efb\u52a1\u7ea7\u63a8\u7406\u548c\u50cf\u7d20\u7ea7\u63a7\u5236\uff0c\u5305\u62ec\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u4efb\u52a1\u89c4\u5212\u5668\u548c\u7ed3\u6784\u56fe\u50cf\u7f16\u8f91\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "SAGE\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u56fe\u50cf\u7f16\u8f91\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u7814\u7a76\u3002"}}
{"id": "2509.21682", "pdf": "https://arxiv.org/pdf/2509.21682", "abs": "https://arxiv.org/abs/2509.21682", "authors": ["Janith B. Senanayaka", "Christopher A. Metzler"], "title": "Snapshot Synthetic Aperture Imaging with Boiling Speckle", "categories": ["physics.optics", "eess.SP"], "comment": "10 pages, 11 figures, this is a preprint and this work will be\n  submitted to IEEE Transactions on Computational Imaging", "summary": "Light-based synthetic aperture (SA) imaging methods, such as Fourier\nPtychography, have brought breakthrough high-resolution wide-field-of-view\nimaging capabilities to microscopy. While these technologies promise similar\nimprovements in long-range imaging applications, macroscale light-based SA\nimaging is significantly more challenging. In this work, we first demonstrate\nthat speckle noise is particularly problematic for light-based SA systems.\nSpecifically, we prove that it is fundamentally impossible to perform SA\nimaging of fully diffuse scenes if one captures sequential measurements that\nsuffer from per-measurement-independent speckle. We then develop a snapshot SA\nimaging method and aperture-phase-synchronization strategy that can overcome\nthis limitation and enable SA imaging. Remarkably, we further demonstrate, in\nsimulation, that speckle can be exploited to recover missing spatial frequency\ninformation in SA imaging systems with distributed, non-overlapping apertures.\nThat is, one can use speckle to improve the resolution of an SA imaging system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5149\u57fa\u5408\u6210\u5b54\u5f84(SA)\u6210\u50cf\u65b9\u6cd5\u5728\u8fdc\u8ddd\u79bb\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u95ee\u9898\uff0c\u7279\u522b\u662f\u6563\u6591\u566a\u58f0\u7684\u6311\u6218\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u7167SA\u6210\u50cf\u65b9\u6cd5\u548c\u5b54\u5f84\u76f8\u4f4d\u540c\u6b65\u7b56\u7565\uff0c\u514b\u670d\u4e86\u9650\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5229\u7528\u6563\u6591\u63d0\u9ad8SA\u6210\u50cf\u7cfb\u7edf\u5206\u8fa8\u7387\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u9488\u5bf9\u5149\u57faSA\u6210\u50cf\u5728\u5b8f\u89c2\u5c3a\u5ea6\u4e0a\u9762\u4e34\u7684\u6563\u6591\u566a\u58f0\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5e76\u514b\u670d\u8fd9\u4e00\u6280\u672f\u96be\u70b9\u3002", "method": "\u63d0\u51fa\u5feb\u7167SA\u6210\u50cf\u65b9\u6cd5\u548c\u5b54\u5f84\u76f8\u4f4d\u540c\u6b65\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u72ec\u7acb\u6563\u6591\u5bfc\u81f4\u7684\u6210\u50cf\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u8bc1\u660e\u4e86\u6563\u6591\u53ef\u7528\u4e8e\u6062\u590dSA\u6210\u50cf\u7cfb\u7edf\u4e2d\u7f3a\u5931\u7684\u7a7a\u95f4\u9891\u7387\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u8fa8\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6563\u6591\u566a\u58f0\u53ef\u4ee5\u8f6c\u5316\u4e3a\u4f18\u52bf\uff0c\u63d0\u5347SA\u6210\u50cf\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.21955", "pdf": "https://arxiv.org/pdf/2509.21955", "abs": "https://arxiv.org/abs/2509.21955", "authors": ["Divake Kumar", "Sina Tayebati", "Francesco Migliarba", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception", "categories": ["cs.RO", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Deep learning models in robotics often output point estimates with poorly\ncalibrated confidences, offering no native mechanism to quantify predictive\nreliability under novel, noisy, or out-of-distribution inputs. Conformal\nprediction (CP) addresses this gap by providing distribution-free coverage\nguarantees, yet its reliance on fixed nonconformity scores ignores context and\ncan yield intervals that are overly conservative or unsafe. We address this\nwith Learnable Conformal Prediction (LCP), which replaces fixed scores with a\nlightweight neural function that leverages geometric, semantic, and\ntask-specific features to produce context-aware uncertainty sets.\n  LCP maintains CP's theoretical guarantees while reducing prediction set sizes\nby 18% in classification, tightening detection intervals by 52%, and improving\npath planning safety from 72% to 91% success with minimal overhead. Across\nthree robotic tasks on seven benchmarks, LCP consistently outperforms Standard\nCP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it\nachieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object\ndetection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding\nboxes. In path planning through cluttered environments, it improves success to\n91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.\n  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)\nand supports online adaptation, making it well suited to resource-constrained\nautonomous systems. Hardware evaluation shows LCP adds less than 1% memory and\n15.9% inference overhead, yet sustains 39 FPS on detection tasks while being\n7.4 times more energy-efficient than ensembles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u5b66\u4e60\u7684\u5171\u5f62\u9884\u6d4b\uff08LCP\uff09\uff0c\u901a\u8fc7\u5229\u7528\u4e0a\u4e0b\u6587\u7279\u5f81\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u672a\u77e5\u6216\u566a\u58f0\u8f93\u5165\u4e0b\u9884\u6d4b\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u7684\u56fa\u5b9a\u8bc4\u5206\u65e0\u6cd5\u9002\u5e94\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u9884\u6d4b\u533a\u95f4\u8fc7\u4fdd\u5b88\u6216\u4e0d\u5b89\u5168\u3002", "method": "LCP\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3CP\u7684\u56fa\u5b9a\u8bc4\u5206\uff0c\u7ed3\u5408\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u4efb\u52a1\u7279\u5f81\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u96c6\u3002", "result": "LCP\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f18%\uff0c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6536\u7d27\u533a\u95f452%\uff0c\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\u4ece72%\u63d0\u5347\u81f391%\uff1b\u8ba1\u7b97\u5f00\u9500\u4f4e\uff084.8%\u8fd0\u884c\u65f6\u5f00\u9500\uff0c42KB\u5185\u5b58\uff09\u3002", "conclusion": "LCP\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002"}}
{"id": "2509.21964", "pdf": "https://arxiv.org/pdf/2509.21964", "abs": "https://arxiv.org/abs/2509.21964", "authors": ["Fiona Meier", "Giusy Spacone", "Sebastian Frey", "Luca Benini", "Andrea Cossettini"], "title": "A Parallel Ultra-Low Power Silent Speech Interface based on a Wearable, Fully-dry EMG Neckband", "categories": ["eess.SY", "cs.SD", "cs.SY", "eess.AS", "eess.SP"], "comment": "4 pages, 4 figures", "summary": "We present a wearable, fully-dry, and ultra-low power EMG system for silent\nspeech recognition, integrated into a textile neckband to enable comfortable,\nnon-intrusive use. The system features 14 fully-differential EMG channels and\nis based on the BioGAP-Ultra platform for ultra-low power (22 mW) biosignal\nacquisition and wireless transmission. We evaluate its performance on eight\nspeech commands under both vocalized and silent articulation, achieving average\nclassification accuracies of 87$\\pm$3% and 68$\\pm$3% respectively, with a\n5-fold CV approach. To mimic everyday-life conditions, we introduce\nsession-to-session variability by repositioning the neckband between sessions,\nachieving leave-one-session-out accuracies of 64$\\pm$18% and 54$\\pm$7% for the\nvocalized and silent experiments, respectively. These results highlight the\nrobustness of the proposed approach and the promise of energy-efficient\nsilent-speech decoding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7a7f\u6234\u3001\u5b8c\u5168\u5e72\u5f0f\u4e14\u8d85\u4f4e\u529f\u8017\u7684EMG\u7cfb\u7edf\uff0c\u7528\u4e8e\u65e0\u58f0\u8bed\u97f3\u8bc6\u522b\uff0c\u96c6\u6210\u5728\u7eba\u7ec7\u9888\u5e26\u4e2d\u4ee5\u5b9e\u73b0\u8212\u9002\u3001\u975e\u4fb5\u5165\u6027\u4f7f\u7528\u3002\u7cfb\u7edf\u5728\u516b\u79cd\u8bed\u97f3\u547d\u4ee4\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a87%\uff08\u6709\u58f0\uff09\u548c68%\uff08\u65e0\u58f0\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u8212\u9002\u4e14\u9ad8\u6548\u7684\u65e0\u58f0\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u4ee5\u6ee1\u8db3\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u65e0\u5e72\u6270\u4f7f\u7528\u9700\u6c42\u3002", "method": "\u57fa\u4e8eBioGAP-Ultra\u5e73\u53f0\uff0c\u5f00\u53d1\u4e8614\u901a\u9053\u5168\u5dee\u5206EMG\u7cfb\u7edf\uff0c\u901a\u8fc7\u7eba\u7ec7\u9888\u5e26\u5b9e\u73b0\u4fe1\u53f7\u91c7\u96c6\u548c\u65e0\u7ebf\u4f20\u8f93\u3002\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4f1a\u8bdd\u95f4\u53d8\u5f02\u6027\u6a21\u62df\u65e5\u5e38\u4f7f\u7528\u6761\u4ef6\u3002", "result": "\u5728\u6709\u58f0\u548c\u65e0\u58f0\u5b9e\u9a8c\u4e2d\u7684\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a87\u00b13%\u548c68\u00b13%\uff0c\u5728\u4f1a\u8bdd\u95f4\u53d8\u5f02\u6027\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u5206\u522b\u4e3a64\u00b118%\u548c54\u00b17%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u53ef\u7a7f\u6234\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u5411\u3002"}}
{"id": "2509.21961", "pdf": "https://arxiv.org/pdf/2509.21961", "abs": "https://arxiv.org/abs/2509.21961", "authors": ["Lingguang Wang", "\u00d6mer \u015eahin Ta\u015f", "Marlon Steiner", "Christoph Stiller"], "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning-based planners are sensitive to the long-tailed distribution of\ndriving data. Common maneuvers dominate datasets, while dangerous or rare\nscenarios are sparse. This imbalance can bias models toward the frequent cases\nand degrade performance on critical scenarios. To tackle this problem, we\ncompare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a\nflow-matching trajectory planner that learns a conditional rectified flow to\nmap noise directly to trajectory distributions with few flow-matching steps. We\nfurther introduce moderated, in-the-loop guidance that injects small\nperturbation between flow steps to systematically increase trajectory diversity\nwhile remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among\nlearning-based planners and approaches methods with rule-based refinements.\nAfter adding moderated guidance and light post-processing (FlowDrive*), it\nachieves overall state-of-the-art performance across nearly all benchmark\nsplits.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u5b66\u4e60\u578b\u89c4\u5212\u5668\u5728\u9a7e\u9a76\u6570\u636e\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51faFlowDrive\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u6a21\u5f0f\u91cd\u52a0\u6743\u548c\u6539\u8fdb\u7684\u6d41\u5339\u914d\u6280\u672f\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u9a7e\u9a76\u6570\u636e\u7684\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u5b66\u4e60\u578b\u89c4\u5212\u5668\u5bf9\u5e38\u89c1\u9a7e\u9a76\u884c\u4e3a\u8fc7\u5ea6\u504f\u91cd\uff0c\u800c\u5728\u5371\u9669\u6216\u7f55\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u89c4\u5212\u7684\u5168\u9762\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u8f68\u8ff9\u6a21\u5f0f\u91cd\u52a0\u6743\u5e73\u8861\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1FlowDrive\u65b9\u6cd5\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u76f4\u63a5\u751f\u6210\u8f68\u8ff9\u5206\u5e03\uff1b\u5f15\u5165\u9002\u5ea6\u5f15\u5bfc\u589e\u52a0\u8f68\u8ff9\u591a\u6837\u6027\u3002", "result": "FlowDrive\u5728nuPlan\u548cinterPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u5b66\u4e60\u578b\u89c4\u5212\u5668\uff0c\u63a5\u8fd1\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff1b\u7ecf\u8fc7\u6539\u8fdb\u540e\uff08FlowDrive*\uff09\u66f4\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "FlowDrive\u901a\u8fc7\u6570\u636e\u5e73\u8861\u548c\u6539\u8fdb\u7684\u6d41\u5339\u914d\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u5668\u7684\u6027\u80fd\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2509.21973", "pdf": "https://arxiv.org/pdf/2509.21973", "abs": "https://arxiv.org/abs/2509.21973", "authors": ["Dibyabha Deb", "Ujjwal Verma"], "title": "Multicollinearity-Aware Parameter-Free Strategy for Hyperspectral Band Selection: A Dependence Measures-Based Approach", "categories": ["eess.IV", "eess.SP"], "comment": "13 pages", "summary": "Hyperspectral bands offer rich spectral and spatial information; however,\ntheir high dimensionality poses challenges for efficient processing. Band\nselection (BS) methods aim to extract a smaller subset of bands to reduce\nspectral redundancy. Existing approaches, such as ranking-based,\nclustering-based, and iterative methods, often suffer from issues like\nsensitivity to initialization, parameter tuning, and high computational cost.\nThis work introduces a BS strategy integrating three dependence measures:\nAverage Band Correlation (ABC) and Mutual Information (MI), and Variance\nInflation Factor (VIF). ABC quantifies linear correlations between spectral\nbands, while MI measures uncertainty reduction relative to ground truth labels.\nTo address multicollinearity and reduce the search space, the approach first\napplies a VIF-based pre-selection of spectral bands. Subsequently, a clustering\nalgorithm is used to identify the optimal subset of bands based on the ABC and\nMI values. Unlike previous methods, this approach is completely parameter-free\nfor hyperspectral band selection, eliminating the need for optimal parameter\nestimation. The proposed method is evaluated on four standard benchmark\ndatasets: WHU-Hi-LongKou, Pavia University, Salinas, and Oil Spill datasets,\nand is compared to existing state-of-the-art approaches. There is significant\noverlap between the bands identified by our proposed method and those selected\nby other methods, indicating that our approach effectively captures the most\nrelevant spectral features. Further, support vector machine (SVM)\nclassification validates that VIF-driven pruning enhances classification by\nminimizing multicollinearity. Ablation studies confirm that combining ABC with\nMI yields robust, discriminative band subsets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e09\u79cd\u4f9d\u8d56\u5ea6\u91cf\uff08ABC\u3001MI\u548cVIF\uff09\u7684\u65e0\u53c2\u6570\u6ce2\u6bb5\u9009\u62e9\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u6570\u636e\u7ef4\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u5e26\u6765\u4e86\u5904\u7406\u6548\u7387\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u6ce2\u6bb5\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u521d\u59cb\u5316\u654f\u611f\u3001\u53c2\u6570\u8c03\u4f18\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7VIF\u9884\u9009\u6ce2\u6bb5\u4ee5\u51cf\u5c11\u591a\u91cd\u5171\u7ebf\u6027\uff0c\u5e76\u57fa\u4e8eABC\u548cMI\u503c\u7684\u805a\u7c7b\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u6ce2\u6bb5\u5b50\u96c6\uff0c\u5b8c\u5168\u65e0\u9700\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u83b7\u76f8\u5173\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u901a\u8fc7SVM\u5206\u7c7b\u9a8c\u8bc1\u4e86\u5176\u589e\u5f3a\u5206\u7c7b\u6027\u80fd\u7684\u6548\u679c\u3002", "conclusion": "\u7ed3\u5408ABC\u548cMI\u7684\u6ce2\u6bb5\u9009\u62e9\u65b9\u6cd5\u4e0d\u4ec5\u9c81\u68d2\u4e14\u5224\u522b\u6027\u5f3a\uff0c\u4e14\u65e0\u9700\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.21983", "pdf": "https://arxiv.org/pdf/2509.21983", "abs": "https://arxiv.org/abs/2509.21983", "authors": ["Sigmund Hennum H\u00f8eg", "Aksel Vaaler", "Chaoqi Liu", "Olav Egeland", "Yilun Du"], "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning", "categories": ["cs.RO", "cs.AI"], "comment": "10 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication. See https://sigmundhh.com/hybrid_diffusion/ for the\n  project website", "summary": "Constructing robots to accomplish long-horizon tasks is a long-standing\nchallenge within artificial intelligence. Approaches using generative methods,\nparticularly Diffusion Models, have gained attention due to their ability to\nmodel continuous robotic trajectories for planning and control. However, we\nshow that these models struggle with long-horizon tasks that involve complex\ndecision-making and, in general, are prone to confusing different modes of\nbehavior, leading to failure. To remedy this, we propose to augment continuous\ntrajectory generation by simultaneously generating a high-level symbolic plan.\nWe show that this requires a novel mix of discrete variable diffusion and\ncontinuous diffusion, which dramatically outperforms the baselines. In\naddition, we illustrate how this hybrid diffusion process enables flexible\ntrajectory synthesis, allowing us to condition synthesized actions on partial\nand complete symbolic conditions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u8ba1\u5212\u4e0e\u8fde\u7eed\u8f68\u8ff9\u751f\u6210\u6765\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u957f\u671f\u590d\u6742\u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u5728\u5904\u7406\u6d89\u53ca\u590d\u6742\u51b3\u7b56\u7684\u957f\u671f\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u6df7\u6dc6\u4e0d\u540c\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u53d8\u91cf\u6269\u6563\u548c\u8fde\u7eed\u6269\u6563\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u751f\u6210\u9ad8\u5c42\u7b26\u53f7\u8ba1\u5212\u6765\u589e\u5f3a\u8fde\u7eed\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u652f\u6301\u6839\u636e\u90e8\u5206\u6216\u5b8c\u6574\u7b26\u53f7\u6761\u4ef6\u7075\u6d3b\u5408\u6210\u8f68\u8ff9\u3002", "conclusion": "\u6df7\u5408\u6269\u6563\u8fc7\u7a0b\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u957f\u671f\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.22197", "pdf": "https://arxiv.org/pdf/2509.22197", "abs": "https://arxiv.org/abs/2509.22197", "authors": ["Duc Thien Nguyen", "Konstantinos Slavakis", "Eleftherios Kofidis", "Dimitris Pados"], "title": "Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard Overparametrization: The Dynamic Graph Flow Case", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "A regression-based framework for interpretable multi-way data imputation,\ntermed Kernel Regression via Tensor Trains with Hadamard overparametrization\n(KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting\nimputation as regression via reproducing kernel Hilbert spaces. Parameter\nefficiency is achieved through tensors of fixed tensor-train (TT) rank, which\nreside on low-dimensional Riemannian manifolds, and is further enhanced via\nHadamard overparametrization, which promotes sparsity within the TT parameter\nspace. Learning is accomplished by solving a smooth inverse problem posed on\nthe Riemannian manifold of fixed TT-rank tensors. As a representative\napplication, the estimation of dynamic graph flows is considered. In this\nsetting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based\n(topological) priors via its inverse problem formulation. Numerical tests on\nreal-world graph datasets demonstrate that KReTTaH consistently outperforms\nstate-of-the-art alternatives-including a nonparametric tensor- and a\nneural-network-based methods-for imputing missing, time-varying edge flows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKReTTaH\u7684\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8def\u6570\u636e\u63d2\u503c\uff0c\u901a\u8fc7\u5185\u6838\u56de\u5f52\u548c\u5f20\u91cf\u706b\u8f66\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5316\uff0c\u5e76\u5728\u52a8\u6001\u56fe\u6d41\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u8def\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u63d2\u503c\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u52a8\u6001\u56fe\u6d41\u7b49\u573a\u666f\u3002", "method": "KReTTaH\u7ed3\u5408\u4e86\u975e\u53c2\u6570\u5316\u7684\u6838\u56de\u5f52\u548c\u5f20\u91cf\u706b\u8f66\u7ed3\u6784\uff08\u56fa\u5b9aTT\u79e9\uff09\uff0c\u901a\u8fc7\u54c8\u8fbe\u739b\u8d85\u53c2\u6570\u5316\u63d0\u5347\u7a00\u758f\u6027\uff0c\u5e76\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u6c42\u89e3\u5e73\u6ed1\u9006\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKReTTaH\u5728\u7f3a\u5931\u65f6\u95f4\u8fb9\u6d41\u63d2\u503c\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u5305\u62ec\u975e\u53c2\u6570\u5f20\u91cf\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "conclusion": "KReTTaH\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u8def\u6570\u636e\u63d2\u503c\u6846\u67b6\uff0c\u5c24\u5176\u5728\u52a8\u6001\u56fe\u6d41\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.21986", "pdf": "https://arxiv.org/pdf/2509.21986", "abs": "https://arxiv.org/abs/2509.21986", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Developing Vision-Language-Action Model from Egocentric Videos", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Egocentric videos capture how humans manipulate objects and tools, providing\ndiverse motion cues for learning object manipulation. Unlike the costly,\nexpert-driven manual teleoperation commonly used in training\nVision-Language-Action models (VLAs), egocentric videos offer a scalable\nalternative. However, prior studies that leverage such videos for training\nrobot policies typically rely on auxiliary annotations, such as detailed\nhand-pose recordings. Consequently, it remains unclear whether VLAs can be\ntrained directly from raw egocentric videos. In this work, we address this\nchallenge by leveraging EgoScaler, a framework that extracts 6DoF object\nmanipulation trajectories from egocentric videos without requiring auxiliary\nrecordings. We apply EgoScaler to four large-scale egocentric video datasets\nand automatically refine noisy or incomplete trajectories, thereby constructing\na new large-scale dataset for VLA pre-training. Our experiments with a\nstate-of-the-art $\\pi_0$ architecture in both simulated and real-robot\nenvironments yield three key findings: (i) pre-training on our dataset improves\ntask success rates by over 20\\% compared to training from scratch, (ii) the\nperformance is competitive with that achieved using real-robot datasets, and\n(iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and\nscalable resource for advancing VLA research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528EgoScaler\u6846\u67b6\u4ece\u539f\u59cb\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u63d0\u53d66DoF\u7269\u4f53\u64cd\u4f5c\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4f5c\u4e3a\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u66ff\u4ee3\u4f20\u7edf\u6602\u8d35\u4e14\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u7684VLAs\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5229\u7528EgoScaler\u6846\u67b6\u4ece\u56db\u79cd\u5927\u89c4\u6a21\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u5e76\u81ea\u52a8\u4f18\u53166DoF\u64cd\u4f5c\u8f68\u8ff9\uff0c\u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6\u7528\u4e8eVLA\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a\uff0c\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u4e14\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u540e\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u662fVLA\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\u4e14\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.22267", "pdf": "https://arxiv.org/pdf/2509.22267", "abs": "https://arxiv.org/abs/2509.22267", "authors": ["Jo\u00e3o Paulo Vieira", "Victor Afonso Bauler", "Rodrigo Kobashikawa Rosa", "Danilo Silva"], "title": "Towards a more realistic evaluation of machine learning models for bearing fault diagnosis", "categories": ["cs.LG", "eess.SP"], "comment": "Submitted to Mechanical Systems and Signal Processing", "summary": "Reliable detection of bearing faults is essential for maintaining the safety\nand operational efficiency of rotating machinery. While recent advances in\nmachine learning (ML), particularly deep learning, have shown strong\nperformance in controlled settings, many studies fail to generalize to\nreal-world applications due to methodological flaws, most notably data leakage.\nThis paper investigates the issue of data leakage in vibration-based bearing\nfault diagnosis and its impact on model evaluation. We demonstrate that common\ndataset partitioning strategies, such as segment-wise and condition-wise\nsplits, introduce spurious correlations that inflate performance metrics. To\naddress this, we propose a rigorous, leakage-free evaluation methodology\ncentered on bearing-wise data partitioning, ensuring no overlap between the\nphysical components used for training and testing. Additionally, we reformulate\nthe classification task as a multi-label problem, enabling the detection of\nco-occurring fault types and the use of prevalence-independent metrics such as\nMacro AUROC. Beyond preventing leakage, we also examine the effect of dataset\ndiversity on generalization, showing that the number of unique training\nbearings is a decisive factor for achieving robust performance. We evaluate our\nmethodology on three widely adopted datasets: CWRU, Paderborn University (PU),\nand University of Ottawa (UORED-VAFCLS). This study highlights the importance\nof leakage-aware evaluation protocols and provides practical guidelines for\ndataset partitioning, model selection, and validation, fostering the\ndevelopment of more trustworthy ML systems for industrial fault diagnosis\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u8f74\u627f\u6545\u969c\u8bca\u65ad\u4e2d\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u53ca\u5176\u5bf9\u6a21\u578b\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6cc4\u6f0f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f3a\u8c03\u8f74\u627f\u5206\u533a\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u8f74\u627f\u6545\u969c\u8bca\u65ad\u4e2d\u56e0\u6570\u636e\u6cc4\u6f0f\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u7814\u7a76\u9700\u8981\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8f74\u627f\u5206\u533a\u7684\u65e0\u6cc4\u6f0f\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5c06\u5206\u7c7b\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u6807\u7b7e\u95ee\u9898\uff0c\u4f7f\u7528Macro AUROC\u7b49\u6307\u6807\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5e38\u89c1\u7684\u6570\u636e\u5206\u5272\u7b56\u7565\u4f1a\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\uff0c\u800c\u8f74\u627f\u5206\u533a\u548c\u6570\u636e\u96c6\u591a\u6837\u6027\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u65e0\u6cc4\u6f0f\u8bc4\u4f30\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u96c6\u5206\u533a\u548c\u6a21\u578b\u9009\u62e9\u6307\u5357\u3002"}}
{"id": "2509.22002", "pdf": "https://arxiv.org/pdf/2509.22002", "abs": "https://arxiv.org/abs/2509.22002", "authors": ["Yuping Gu", "Bangchao Huang", "Haoran Sun", "Ronghan Xu", "Jiayi Yin", "Wei Zhang", "Fang Wan", "Jia Pan", "Chaoyang Song"], "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion", "categories": ["cs.RO"], "comment": "23 pages, 11 figures, 2 tables. Accepted by Fundamental Research. For\n  Supplementary Videos, see https://bionicdl.ancorasir.com/?p=1668", "summary": "While it is expected to build robotic limbs with multiple degrees of freedom\n(DoF) inspired by nature, a single DoF design remains fundamental, providing\nbenefits that include, but are not limited to, simplicity, robustness,\ncost-effectiveness, and efficiency. Mechanisms, especially those with multiple\nlinks and revolute joints connected in closed loops, play an enabling factor in\nintroducing motion diversity for 1-DoF systems, which are usually constrained\nby self-collision during a full-cycle range of motion. This study presents a\nnovel computational approach to designing one-degree-of-freedom (1-DoF)\noverconstrained robotic limbs for a desired spatial trajectory, while achieving\nenergy-efficient, self-collision-free motion in full-cycle rotations. Firstly,\nwe present the geometric optimization problem of linkage-based robotic limbs in\na generalized formulation for self-collision-free design. Next, we formulate\nthe spatial trajectory generation problem with the overconstrained linkages by\noptimizing the similarity and dynamic-related metrics. We further optimize the\ngeometric shape of the overconstrained linkage to ensure smooth and\ncollision-free motion driven by a single actuator. We validated our proposed\nmethod through various experiments, including personalized automata and\nbio-inspired hexapod robots. The resulting hexapod robot, featuring\noverconstrained robotic limbs, demonstrated outstanding energy efficiency\nduring forward walking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5355\u81ea\u7531\u5ea6\uff081-DoF\uff09\u8fc7\u7ea6\u675f\u673a\u5668\u4eba\u80a2\u4f53\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u81ea\u78b0\u649e\u7684\u5168\u5468\u671f\u8fd0\u52a8\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u754c\u4e2d\u7684\u591a\u81ea\u7531\u5ea6\u80a2\u4f53\u8bbe\u8ba1\u66f4\u5177\u591a\u6837\u6027\uff0c\u4f46\u5355\u81ea\u7531\u5ea6\u8bbe\u8ba1\u56e0\u5176\u7b80\u5355\u6027\u3001\u9c81\u68d2\u6027\u548c\u6210\u672c\u6548\u76ca\u4ecd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b31-DoF\u7cfb\u7edf\u5728\u8fd0\u52a8\u591a\u6837\u6027\u548c\u81ea\u78b0\u649e\u95ee\u9898\u4e0a\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u95ee\u9898\u751f\u6210\u65e0\u81ea\u78b0\u649e\u7684\u8fde\u6746\u673a\u6784\uff0c\u5e76\u4f7f\u7528\u8fc7\u7ea6\u675f\u8fde\u6746\u4f18\u5316\u7a7a\u95f4\u8f68\u8ff9\u751f\u6210\uff0c\u540c\u65f6\u4f18\u5316\u51e0\u4f55\u5f62\u72b6\u4ee5\u786e\u4fdd\u5355\u9a71\u52a8\u4e0b\u7684\u5e73\u6ed1\u65e0\u78b0\u649e\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u81ea\u52a8\u673a\u548c\u4eff\u751f\u516d\u8db3\u673a\u5668\u4eba\u3002\u7ed3\u679c\u663e\u793a\uff0c\u91c7\u7528\u8fc7\u7ea6\u675f\u673a\u5668\u4eba\u80a2\u4f53\u7684\u516d\u8db3\u673a\u5668\u4eba\u5728\u524d\u8fdb\u884c\u8d70\u65f6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u65e0\u81ea\u78b0\u649e\u7684\u5355\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80a2\u4f53\u8bbe\u8ba1\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2509.22321", "pdf": "https://arxiv.org/pdf/2509.22321", "abs": "https://arxiv.org/abs/2509.22321", "authors": ["Bowen Wang", "Matteo Zecchin", "Osvaldo Simeone"], "title": "Distributed Associative Memory via Online Convex Optimization", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "An associative memory (AM) enables cue-response recall, and associative\nmemorization has recently been noted to underlie the operation of modern neural\narchitectures such as Transformers. This work addresses a distributed setting\nwhere agents maintain a local AM to recall their own associations as well as\nselective information from others. Specifically, we introduce a distributed\nonline gradient descent method that optimizes local AMs at different agents\nthrough communication over routing trees. Our theoretical analysis establishes\nsublinear regret guarantees, and experiments demonstrate that the proposed\nprotocol consistently outperforms existing online optimization baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5c40\u90e8\u5173\u8054\u8bb0\u5fc6\uff08AM\uff09\uff0c\u5e76\u901a\u8fc7\u6811\u5f62\u8def\u7531\u901a\u4fe1\u5b9e\u73b0\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformers\uff09\u7684\u8fd0\u4f5c\u57fa\u4e8e\u5173\u8054\u8bb0\u5fc6\uff08AM\uff09\uff0c\u672c\u6587\u7814\u7a76\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u591a\u667a\u80fd\u4f53\u901a\u8fc7\u672c\u5730AM\u8fdb\u884c\u81ea\u6211\u5173\u8054\u8bb0\u5fc6\u548c\u9009\u62e9\u6027\u4fe1\u606f\u5171\u4eab\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6811\u5f62\u8def\u7531\u901a\u4fe1\u4f18\u5316\u5404\u667a\u80fd\u4f53\u7684\u5c40\u90e8AM\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf\u4f18\u5316\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u591a\u667a\u80fd\u4f53\u7684\u5173\u8054\u8bb0\u5fc6\u4f18\u5316\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2509.22058", "pdf": "https://arxiv.org/pdf/2509.22058", "abs": "https://arxiv.org/abs/2509.22058", "authors": ["Qifeng Wang", "Weigang Li", "Lei Nie", "Xin Xu", "Wenping Liu", "Zhe Xu"], "title": "An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As a key technology for autonomous navigation and positioning in mobile\nrobots, light detection and ranging (LiDAR) odometry is widely used in\nautonomous driving applications. The Iterative Closest Point (ICP)-based\nmethods have become the core technique in LiDAR odometry due to their efficient\nand accurate point cloud registration capability. However, some existing\nICP-based methods do not consider the reliability of the initial pose, which\nmay cause the method to converge to a local optimum. Furthermore, the absence\nof an adaptive mechanism hinders the effective handling of complex dynamic\nenvironments, resulting in a significant degradation of registration accuracy.\nTo address these issues, this paper proposes an adaptive ICP-based LiDAR\nodometry method that relies on a reliable initial pose. First, distributed\ncoarse registration based on density filtering is employed to obtain the\ninitial pose estimation. The reliable initial pose is then selected by\ncomparing it with the motion prediction pose, reducing the initial error\nbetween the source and target point clouds. Subsequently, by combining the\ncurrent and historical errors, the adaptive threshold is dynamically adjusted\nto accommodate the real-time changes in the dynamic environment. Finally, based\non the reliable initial pose and the adaptive threshold, point-to-plane\nadaptive ICP registration is performed from the current frame to the local map,\nachieving high-precision alignment of the source and target point clouds.\nExtensive experiments on the public KITTI dataset demonstrate that the proposed\nmethod outperforms existing approaches and significantly enhances the accuracy\nof LiDAR odometry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94ICP\u7684LiDAR\u6d4b\u7a0b\u6cd5\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u521d\u59cb\u59ff\u6001\u548c\u52a8\u6001\u8c03\u6574\u7684\u81ea\u9002\u5e94\u9608\u503c\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u70b9\u4e91\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684ICP\u65b9\u6cd5\u5728\u521d\u59cb\u59ff\u6001\u53ef\u9760\u6027\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u81ea\u9002\u5e94\u673a\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u4e14\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\u3002", "method": "\u91c7\u7528\u5bc6\u5ea6\u8fc7\u6ee4\u7684\u5206\u5e03\u5f0f\u7c97\u914d\u51c6\u83b7\u53d6\u521d\u59cb\u59ff\u6001\uff0c\u7ed3\u5408\u8fd0\u52a8\u9884\u6d4b\u59ff\u6001\u9009\u62e9\u53ef\u9760\u521d\u59cb\u59ff\u6001\uff1b\u52a8\u6001\u8c03\u6574\u81ea\u9002\u5e94\u9608\u503c\uff1b\u57fa\u4e8e\u70b9\u5bf9\u5e73\u9762\u81ea\u9002\u5e94ICP\u8fdb\u884c\u914d\u51c6\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LiDAR\u6d4b\u7a0b\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53ef\u9760\u7684\u521d\u59cb\u59ff\u6001\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u914d\u51c6\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.22326", "pdf": "https://arxiv.org/pdf/2509.22326", "abs": "https://arxiv.org/abs/2509.22326", "authors": ["Israel Jesus Santos Filho", "Muhammad Mahboob Ur Rahman", "Taous-Meriem Laleg-Kirati", "Tareq Al-Naffouri"], "title": "Radio-PPG: photoplethysmogram digital twin synthesis using deep neural representation of 6G/WiFi ISAC signals", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "15 pages, 13 figures, 7 tables, under review with a journal", "summary": "Digital twins for 1D bio-signals enable real-time monitoring of physiological\nprocesses of a person, which enables early disease diagnosis and personalized\ntreatment. This work introduces a novel non-contact method for digital twin\n(DT) photoplethysmogram (PPG) signal synthesis under the umbrella of 6G/WiFi\nintegrated sensing and communication (ISAC) systems. We employ a\nsoftware-defined radio (SDR) operating at 5.23 GHz that illuminates the chest\nof a nearby person with a wideband 6G/WiFi signal and collects the reflected\nsignals. This allows us to acquire Radio-PPG dataset that consists of 300\nminutes worth of near synchronous 64-channel radio data, PPG data, along with\nthe labels (three body vitals) of 30 healthy subjects. With this, we test two\nartificial intelligence (AI) models for DT-PPG signal synthesis: i) discrete\ncosine transform followed by a multi-layer perceptron, ii) two U-NET models\n(Approximation network, Refinement network) in cascade, along with a custom\nloss function. Experimental results indicate that U-NET model achieves an\nimpressive relative mean absolute error of 0.194 with a small ISAC sensing\noverhead of 15.62%, for DT-PPG synthesis. Furthermore, we performed quality\nassessment of the synthetic DT-PPG by computing the accuracy of DT-PPG-based\nvitals estimation and feature extraction, which turned out to be at par with\nthat of reference PPG-based vitals estimation and feature extraction. This work\nhighlights the potential of generative AI and 6G/WiFi ISAC technologies and\nserves as a foundational step towards the development of non-contact screening\ntools for covid-19, cardiovascular diseases and well-being assessment of people\nwith special needs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e6G/WiFi\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u7684\u975e\u63a5\u89e6\u5f0f\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2\uff08PPG\uff09\u4fe1\u53f7\u5408\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684DT-PPG\u4fe1\u53f7\u5408\u6210\u3002", "motivation": "\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u65f6\u76d1\u6d4b\u751f\u7406\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0\u65e9\u671f\u75be\u75c5\u8bca\u65ad\u548c\u4e2a\u6027\u5316\u6cbb\u7597\uff0c\u540c\u65f6\u63a2\u7d226G/WiFi ISAC\u6280\u672f\u5728\u975e\u63a5\u89e6\u533b\u7597\u7b5b\u67e5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u75285.23 GHz\u7684\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff08SDR\uff09\u91c7\u96c664\u901a\u9053\u65e0\u7ebf\u7535\u6570\u636e\uff0c\u7ed3\u5408PPG\u6570\u636e\uff0c\u6d4b\u8bd5\u4e86\u79bb\u6563\u4f59\u5f26\u53d8\u6362+\u591a\u5c42\u611f\u77e5\u673a\u53ca\u7ea7\u8054U-NET\u6a21\u578b\uff08\u8fd1\u4f3c\u7f51\u7edc\u4e0e\u7ec6\u5316\u7f51\u7edc\uff09\u3002", "result": "U-NET\u6a21\u578b\u5b9e\u73b0\u4e86\u76f8\u5bf9\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.194\uff0cISAC\u611f\u77e5\u5f00\u9500\u4ec5\u4e3a15.62%\uff0c\u4e14\u5408\u6210\u7684DT-PPG\u4fe1\u53f7\u5728\u4f53\u5f81\u4f30\u8ba1\u548c\u7279\u5f81\u63d0\u53d6\u4e0a\u4e0e\u53c2\u8003PPG\u4fe1\u53f7\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u4e0e6G/WiFi ISAC\u6280\u672f\u5728\u975e\u63a5\u89e6\u5f0f\u533b\u7597\u7b5b\u67e5\uff08\u5982\u65b0\u51a0\u3001\u5fc3\u8840\u7ba1\u75be\u75c5\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7279\u6b8a\u9700\u6c42\u4eba\u7fa4\u7684\u5065\u5eb7\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.22065", "pdf": "https://arxiv.org/pdf/2509.22065", "abs": "https://arxiv.org/abs/2509.22065", "authors": ["Ethan Fulcher", "J. Diego Caporale", "Yifeng Zhang", "John Ruck", "Feifei Qian"], "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7+1 pages, 5 figures, ICRA Submission This work has been submitted to\n  the IEEE for possible publication", "summary": "In-situ robotic exploration is an important tool for advancing knowledge of\ngeological processes that describe the Earth and other Planetary bodies. To\ninform and enhance operations for these roving laboratories, it is imperative\nto understand the terramechanical properties of their environments, especially\nfor traversing on loose, deformable substrates. Recent research suggested that\nlegged robots with direct-drive and low-gear ratio actuators can sensitively\ndetect external forces, and therefore possess the potential to measure terrain\nproperties with their legs during locomotion, providing unprecedented sampling\nspeed and density while accessing terrains previously too risky to sample. This\npaper explores these ideas by investigating the impact of gait on\nproprioceptive terrain sensing accuracy, particularly comparing a\nsensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,\nTrot-Walk. Each gait's ability to measure the strength and texture of\ndeformable substrate is quantified as the robot locomotes over a laboratory\ntransect consisting of a rigid surface, loose sand, and loose sand with\nsynthetic surface crusts. Our results suggest that with both the\nsensing-oriented crawling gait and locomotion-oriented trot gait, the robot can\nmeasure a consistent difference in the strength (in terms of penetration\nresistance) between the low- and high-resistance substrates; however, the\nlocomotion-oriented trot gait contains larger magnitude and variance in\nmeasurements. Furthermore, the slower crawl gait can detect brittle ruptures of\nthe surface crusts with significantly higher accuracy than the faster trot\ngait. Our results offer new insights that inform legged robot \"sensing during\nlocomotion\" gait design and planning for scouting the terrain and producing\nscientific measurements on other worlds to advance our understanding of their\ngeology and formation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u677e\u6563\u53ef\u53d8\u5f62\u57fa\u5e95\u4e0a\u7684\u6b65\u6001\u5bf9\u5730\u5f62\u4f20\u611f\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4f20\u611f\u5bfc\u5411\u7684Crawl N' Sense\u6b65\u6001\u548c\u8fd0\u52a8\u5bfc\u5411\u7684Trot-Walk\u6b65\u6001\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e24\u79cd\u6b65\u6001\u5747\u53ef\u533a\u5206\u57fa\u5e95\u5f3a\u5ea6\u5dee\u5f02\uff0c\u4f46Trot-Walk\u7684\u6570\u636e\u6ce2\u52a8\u66f4\u5927\uff1bCrawl N' Sense\u80fd\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u8868\u9762\u8106\u6027\u65ad\u88c2\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u884c\u661f\u63a2\u6d4b\u4e2d\u7684\u5730\u5f62\u4f20\u611f\u80fd\u529b\uff0c\u7814\u7a76\u4e0d\u540c\u6b65\u6001\u5bf9\u5730\u5f62\u6027\u8d28\uff08\u5982\u5f3a\u5ea6\u548c\u7eb9\u7406\uff09\u6d4b\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\uff0c\u6bd4\u8f83Crawl N' Sense\uff08\u4f20\u611f\u5bfc\u5411\uff09\u548cTrot-Walk\uff08\u8fd0\u52a8\u5bfc\u5411\uff09\u4e24\u79cd\u6b65\u6001\u5728\u521a\u6027\u8868\u9762\u3001\u677e\u6563\u6c99\u5730\u548c\u5e26\u5408\u6210\u5916\u58f3\u7684\u6c99\u5730\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e24\u79cd\u6b65\u6001\u5747\u80fd\u533a\u5206\u57fa\u5e95\u5f3a\u5ea6\u5dee\u5f02\uff0c\u4f46Trot-Walk\u6d4b\u91cf\u6570\u636e\u7684\u65b9\u5dee\u66f4\u5927\uff1bCrawl N' Sense\u80fd\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u8106\u6027\u8868\u9762\u65ad\u88c2\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f20\u611f\u5bfc\u5411\u7684\u6162\u901f\u6b65\u6001\u66f4\u9002\u5408\u9ad8\u7cbe\u5ea6\u5730\u5f62\u6d4b\u91cf\uff0c\u4e3a\u672a\u6765\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u7684\u6b65\u6001\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.22497", "pdf": "https://arxiv.org/pdf/2509.22497", "abs": "https://arxiv.org/abs/2509.22497", "authors": ["Xuhui Zhang", "Wenchao Liu", "Chunjie Wang", "Jinke Ren", "Huijun Xing", "Shuqiang Wang", "Yanyan Shen"], "title": "UAV-Enabled Fluid Antenna Systems for Multi-Target Wireless Sensing over LAWCNs", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Fluid antenna system (FAS) is emerging as a key technology for enhancing\nspatial flexibility and sensing accuracy in future wireless systems. This paper\ninvestigates an unmanned aerial vehicle (UAV)-enabled FAS for multi-target\nwireless sensing in low-altitude wireless consumer networks (LAWCNs) for\nachieving the low-altitude economy (LAE) missions. We formulate an optimization\nproblem aimed at minimizing the average Cram\\'er-Rao bound (CRB) for multiple\ntarget estimations. To tackle this non-convex problem, an efficient alternating\noptimization (AO) algorithm is proposed, which jointly optimizes the UAV\ntrajectory, the antenna position of the transmit fluid antennas (FAs) and the\nreceive FAs, and the transmit beamforming at the UAV. Simulation results\ndemonstrate significant performance improvements in estimation accuracy and\nsensing reliability compared to conventional schemes, e.g., the fixed position\nantenna scheme. The proposed system achieves enhanced sensing performance\nthrough adaptive trajectory design and beamforming, alongside effective\ninterference suppression via the flexible FAS antenna repositioning,\nunderscoring its practical potential for precision sensing in the UAV-enabled\nLAWCNs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u5929\u7ebf\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u76ee\u6807\u65e0\u7ebf\u4f20\u611f\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u4e2d\uff0c\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u56e0\u5176\u7a7a\u95f4\u7075\u6d3b\u6027\u548c\u4f20\u611f\u7cbe\u5ea6\u4f18\u52bf\u6210\u4e3a\u5173\u952e\u6280\u672f\uff0c\u672c\u6587\u9488\u5bf9\u4f4e\u7a7a\u65e0\u7ebf\u6d88\u8d39\u7f51\u7edc\uff08LAWCNs\uff09\u4e2d\u7684\u591a\u76ee\u6807\u65e0\u7ebf\u4f20\u611f\u9700\u6c42\uff0c\u63d0\u51fa\u65e0\u4eba\u673a\u9a71\u52a8\u7684FAS\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u53d1\u5c04\u548c\u63a5\u6536\u6d41\u4f53\u5929\u7ebf\u7684\u4f4d\u7f6e\u4ee5\u53ca\u65e0\u4eba\u673a\u7684\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u6700\u5c0f\u5316\u591a\u76ee\u6807\u4f30\u8ba1\u7684Cram\u00e9r-Rao\u754c\uff08CRB\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u65b9\u6848\uff0c\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u4f20\u611f\u53ef\u9760\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u7075\u6d3b\u7684FAS\u5929\u7ebf\u91cd\u5b9a\u4f4d\u6709\u6548\u6291\u5236\u5e72\u6270\u3002", "conclusion": "\u65e0\u4eba\u673a\u9a71\u52a8\u7684FAS\u7cfb\u7edf\u901a\u8fc7\u81ea\u9002\u5e94\u8f68\u8ff9\u8bbe\u8ba1\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u5728\u4f4e\u7a7a\u65e0\u7ebf\u6d88\u8d39\u7f51\u7edc\u4e2d\u5c55\u73b0\u51fa\u7cbe\u51c6\u4f20\u611f\u7684\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2509.22093", "pdf": "https://arxiv.org/pdf/2509.22093", "abs": "https://arxiv.org/abs/2509.22093", "authors": ["Xiaohuan Pei", "Yuxing Chen", "Siyu Xu", "Yunke Wang", "Yuheng Shi", "Chang Xu"], "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic manipulation with Vision-Language-Action models requires efficient\ninference over long-horizon multi-modal context, where attention to dense\nvisual tokens dominates computational cost. Existing methods optimize inference\nspeed by reducing visual redundancy within VLA models, but they overlook the\nvarying redundancy across robotic manipulation stages. We observe that the\nvisual token redundancy is higher in coarse manipulation phase than in\nfine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose \\textbf{A}ction-aware\n\\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning\nframework that integrates text-driven token selection with action-aware\ntrajectory gating. Our method introduces a gating mechanism that conditions the\npruning signal on recent action trajectories, using past motion windows to\nadaptively adjust token retention ratios in accordance with dynamics, thereby\nbalancing computational efficiency and perceptual precision across different\nmanipulation stages. Extensive experiments on the LIBERO suites and diverse\nreal-world scenarios demonstrate that our method significantly reduces FLOPs\nand action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on\nOpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\%\nimprovements with OpenVLA) compared to baselines, thereby providing a simple\nplug-in path to efficient robot policies that advances the efficiency and\nperformance frontier of robotic manipulation. Our project website is:\n\\href{https://vla-adp.github.io/}{ADP.com}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u679d\u6280\u672f\u5728\u4e0d\u540c\u673a\u5668\u4eba\u64cd\u4f5c\u9636\u6bb5\u8c03\u6574\u89c6\u89c9\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728VLA\u6a21\u578b\u63a8\u7406\u4e2d\u5ffd\u7565\u4e86\u4e0d\u540c\u673a\u5668\u4eba\u64cd\u4f5c\u9636\u6bb5\u89c6\u89c9\u5197\u4f59\u7684\u5dee\u5f02\u6027\uff0c\u4f5c\u8005\u89c2\u5bdf\u5230\u7c97\u7c92\u5ea6\u64cd\u4f5c\u9636\u6bb5\u89c6\u89c9\u5197\u4f59\u66f4\u9ad8\uff0c\u4e14\u4e0e\u52a8\u4f5c\u52a8\u6001\u76f8\u5173\uff0c\u4ece\u800c\u63d0\u51fa\u52a8\u6001\u526a\u679d\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u52a8\u4f5c\u611f\u77e5\u7684\u52a8\u6001\u526a\u679d\uff08ADP\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u9a71\u52a8\u7684\u4ee4\u724c\u9009\u62e9\u548c\u52a8\u4f5c\u611f\u77e5\u7684\u8f68\u8ff9\u95e8\u63a7\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6839\u636e\u8fd1\u671f\u52a8\u4f5c\u8f68\u8ff9\u52a8\u6001\u8c03\u6574\u4ee4\u724c\u4fdd\u7559\u6bd4\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cADP\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u548c\u63a8\u7406\u5ef6\u8fdf\uff08\u59821.35\u500d\u52a0\u901f\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u6210\u529f\u7387\uff08\u5982OpenVLA\u63d0\u534725.8%\uff09\u3002", "conclusion": "ADP\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u5b9e\u73b0\u8def\u5f84\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u524d\u6cbf\u6027\u80fd\u3002"}}
{"id": "2509.22556", "pdf": "https://arxiv.org/pdf/2509.22556", "abs": "https://arxiv.org/abs/2509.22556", "authors": ["Chenyu Liu", "Yuqiu Deng", "Tianyu Liu", "Jinan Zhou", "Xinliang Zhou", "Ziyu Jia", "Yi Ding"], "title": "ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Electroencephalography (EEG), with its broad range of applications,\nnecessitates models that can generalize effectively across various tasks and\ndatasets. Large EEG Models (LEMs) address this by pretraining encoder-centric\narchitectures on large-scale unlabeled data to extract universal\nrepresentations. While effective, these models lack decoders of comparable\ncapacity, limiting the full utilization of the learned features. To address\nthis issue, we introduce ECHO, a novel decoder-centric LEM paradigm that\nreformulates EEG modeling as sequence-to-sequence learning. ECHO captures\nlayered relationships among signals, labels, and tasks within sequence space,\nwhile incorporating discrete support samples to construct contextual cues. This\ndesign equips ECHO with in-context learning, enabling dynamic adaptation to\nheterogeneous tasks without parameter updates. Extensive experiments across\nmultiple datasets demonstrate that, even with basic model components, ECHO\nconsistently outperforms state-of-the-art single-task LEMs in multi-task\nsettings, showing superior generalization and adaptability.", "AI": {"tldr": "ECHO\u662f\u4e00\u79cd\u65b0\u578b\u7684EEG\u89e3\u7801\u5668\u4e2d\u5fc3\u5927\u6a21\u578b\u8303\u5f0f\uff0c\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u63d0\u5347\u591a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfEEG\u5927\u6a21\u578b\u7f3a\u4e4f\u9ad8\u5bb9\u91cf\u89e3\u7801\u5668\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u7279\u5f81\u7684\u5145\u5206\u5229\u7528\u3002", "method": "ECHO\u5c06EEG\u5efa\u6a21\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\uff0c\u6355\u6349\u4fe1\u53f7\u3001\u6807\u7b7e\u548c\u4efb\u52a1\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u79bb\u6563\u652f\u6301\u6837\u672c\u6784\u5efa\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "result": "ECHO\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u73b0\u6709\u5355\u4efb\u52a1\u5927\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u548c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "ECHO\u901a\u8fc7\u89e3\u7801\u5668\u4e2d\u5fc3\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u6a21\u578b\u7684\u591a\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2509.22120", "pdf": "https://arxiv.org/pdf/2509.22120", "abs": "https://arxiv.org/abs/2509.22120", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot", "categories": ["cs.RO"], "comment": "12 pages, 11 figures, 2 tables, under review at the journal of\n  \"Transactions of the Canadian Society for Mechanical Engineering\"", "summary": "The use of exoskeleton robots is increasing due to the rising number of\nmusculoskeletal injuries. However, their effectiveness depends heavily on the\ndesign of control systems. Designing robust controllers is challenging because\nof uncertainties in human-robot systems. Among various control strategies,\nModel Predictive Control (MPC) is a powerful approach due to its ability to\nhandle constraints and optimize performance. Previous studies have used\nlinearization-based methods to implement robust MPC on exoskeletons, but these\ncan degrade performance due to nonlinearities in the robot's dynamics. To\naddress this gap, this paper proposes a Robust Nonlinear Model Predictive\nControl (RNMPC) method, called multi-stage NMPC, to control a\ntwo-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.\nThis method uses multiple scenarios to represent system uncertainties. The\nstudy focuses on minimizing human-robot interaction forces during the swing\nphase, particularly when the robot carries unknown loads. Simulations and\nexperimental tests show that the proposed method significantly improves\nrobustness, outperforming non-robust NMPC. It achieves lower tracking errors\nand interaction forces under various uncertainties. For instance, when a 2 kg\nunknown payload is combined with external disturbances, the RMS values of thigh\nand shank interaction forces for multi-stage NMPC are reduced by 77 and 94\npercent, respectively, compared to non-robust NMPC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RNMPC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u548c\u591a\u91cd\u573a\u666f\u5904\u7406\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u51cf\u5c11\u8ddf\u8e2a\u8bef\u5dee\u548c\u4ea4\u4e92\u529b\u3002", "motivation": "\u7531\u4e8e\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u975e\u7ebf\u6027\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edf\u7ebf\u6027\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RNMPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u5e76\u5229\u7528\u591a\u91cd\u573a\u666f\u4ee3\u8868\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\uff0c\u6765\u4f18\u5316\u5bf9\u5916\u9aa8\u9abc\u7684\u63a7\u5236\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4e0e\u975e\u9c81\u68d2NMPC\u76f8\u6bd4\uff0c\u5728\u672a\u77e5\u8d1f\u8f7d\u548c\u5916\u90e8\u5e72\u6270\u4e0b\uff0c\u9acb\u90e8\u548c\u80eb\u9aa8\u7684\u4ea4\u4e92\u529b\u5206\u522b\u51cf\u5c11\u4e8677%\u548c94%\u3002", "conclusion": "\u591a\u9636\u6bb5RNMPC\u65b9\u6cd5\u5728\u5904\u7406\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u5916\u9aa8\u9abc\u63a7\u5236\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.22149", "pdf": "https://arxiv.org/pdf/2509.22149", "abs": "https://arxiv.org/abs/2509.22149", "authors": ["Haoqi Yuan", "Ziye Huang", "Ye Wang", "Chuan Mao", "Chaoyi Xu", "Zongqing Lu"], "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration", "categories": ["cs.RO"], "comment": null, "summary": "Universal grasping with multi-fingered dexterous hands is a fundamental\nchallenge in robotic manipulation. While recent approaches successfully learn\nclosed-loop grasping policies using reinforcement learning (RL), the inherent\ndifficulty of high-dimensional, long-horizon exploration necessitates complex\nreward and curriculum design, often resulting in suboptimal solutions across\ndiverse objects. We propose DemoGrasp, a simple yet effective method for\nlearning universal dexterous grasping. We start from a single successful\ndemonstration trajectory of grasping a specific object and adapt to novel\nobjects and poses by editing the robot actions in this trajectory: changing the\nwrist pose determines where to grasp, and changing the hand joint angles\ndetermines how to grasp. We formulate this trajectory editing as a single-step\nMarkov Decision Process (MDP) and use RL to optimize a universal policy across\nhundreds of objects in parallel in simulation, with a simple reward consisting\nof a binary success term and a robot-table collision penalty. In simulation,\nDemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow\nHand, outperforming previous state-of-the-art methods. It also shows strong\ntransferability, achieving an average success rate of 84.6% across diverse\ndexterous hand embodiments on six unseen object datasets, while being trained\non only 175 objects. Through vision-based imitation learning, our policy\nsuccessfully grasps 110 unseen real-world objects, including small, thin items.\nIt generalizes to spatial, background, and lighting changes, supports both RGB\nand depth inputs, and extends to language-guided grasping in cluttered scenes.", "AI": {"tldr": "DemoGrasp\u901a\u8fc7\u5b66\u4e60\u5355\u4e2a\u6210\u529f\u6293\u53d6\u6f14\u793a\u8f68\u8ff9\uff0c\u5229\u7528RL\u4f18\u5316\u901a\u7528\u6293\u53d6\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u7269\u4f53\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u7075\u5de7\u624b\u901a\u7528\u6293\u53d6\u7684\u9ad8\u7ef4\u63a2\u7d22\u96be\u9898\uff0c\u907f\u514d\u590d\u6742\u5956\u52b1\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u7f16\u8f91\u6f14\u793a\u8f68\u8ff9\u7684\u624b\u8155\u59ff\u52bf\u548c\u5173\u8282\u89d2\u5ea6\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u5355\u6b65MDP\uff0c\u5e76\u4f7f\u7528RL\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5b9e\u73b095%\u6210\u529f\u7387\uff0c\u5e76\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u5e73\u574784.6%\u6210\u529f\u7387\uff0c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DemoGrasp\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u901a\u7528\u7075\u5de7\u6293\u53d6\u65b9\u6cd5\uff0c\u5177\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.22175", "pdf": "https://arxiv.org/pdf/2509.22175", "abs": "https://arxiv.org/abs/2509.22175", "authors": ["Quanzhou Li", "Zhonghua Wu", "Jingbo Wang", "Chen Change Loy", "Bo Dai"], "title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions", "categories": ["cs.RO"], "comment": null, "summary": "Learning to generate dual-hand grasps that respect object semantics is\nessential for robust hand-object interaction but remains largely underexplored\ndue to dataset scarcity. Existing grasp datasets predominantly focus on\nsingle-hand interactions and contain only limited semantic part annotations. To\naddress these challenges, we introduce a pipeline, SymOpt, that constructs a\nlarge-scale dual-hand grasp dataset by leveraging existing single-hand datasets\nand exploiting object and hand symmetries. Building on this, we propose a\ntext-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand\nAffordance-aware Grasps for unseen objects. Our approach incorporates a novel\ndual-hand affordance representation and follows a two-stage design, which\nenables effective learning from a small set of segmented training objects while\nscaling to a much larger pool of unsegmented data. Extensive experiments\ndemonstrate that our method produces diverse and semantically consistent\ngrasps, outperforming strong baselines in both grasp quality and generalization\nto unseen objects. The project page is at\nhttps://quanzhou-li.github.io/DHAGrasp/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SymOpt\u6d41\u7a0b\u548cDHAGrasp\u751f\u6210\u5668\uff0c\u89e3\u51b3\u4e86\u53cc\u6293\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5bf9\u79f0\u6027\u548c\u4e24\u9636\u6bb5\u8bbe\u8ba1\u751f\u6210\u9ad8\u8d28\u91cf\u8bed\u4e49\u4e00\u81f4\u7684\u6293\u63e1\u3002", "motivation": "\u73b0\u6709\u6293\u63e1\u6570\u636e\u96c6\u591a\u4e3a\u5355\u624b\u4ea4\u4e92\u4e14\u8bed\u4e49\u6807\u6ce8\u6709\u9650\uff0c\u53cc\u6293\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u7a33\u5065\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\u7814\u7a76\u3002", "method": "\u901a\u8fc7SymOpt\u6d41\u7a0b\u5229\u7528\u5355\u624b\u6570\u636e\u96c6\u548c\u5bf9\u79f0\u6027\u6784\u5efa\u5927\u89c4\u6a21\u53cc\u6293\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1DHAGrasp\u751f\u6210\u5668\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u53cc\u6293\u8868\u793a\u548c\u4e24\u9636\u6bb5\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6293\u63e1\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\uff0c\u8d28\u91cf\u548c\u6cdb\u5316\u6027\u5747\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "SymOpt\u548cDHAGrasp\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u6293\u6570\u636e\u96c6\u4e0d\u8db3\u95ee\u9898\uff0c\u4e3a\u624b-\u7269\u4f53\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.22195", "pdf": "https://arxiv.org/pdf/2509.22195", "abs": "https://arxiv.org/abs/2509.22195", "authors": ["Asher J. Hancock", "Xindi Wu", "Lihan Zha", "Olga Russakovsky", "Anirudha Majumdar"], "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting", "categories": ["cs.RO"], "comment": null, "summary": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to\ncreate vision-language-action (VLA) models is a promising paradigm for training\ngeneralist policies, but it suffers from a fundamental tradeoff: learning to\nproduce actions often diminishes the VLM's foundational reasoning and\nmultimodal understanding, hindering generalization to novel scenarios,\ninstruction following, and semantic understanding. We argue that this\ncatastrophic forgetting is due to a distribution mismatch between the VLM's\ninternet-scale pretraining corpus and the robotics fine-tuning data. Inspired\nby this observation, we introduce VLM2VLA: a VLA training paradigm that first\nresolves this mismatch at the data level by representing low-level actions with\nnatural language. This alignment makes it possible to train VLAs solely with\nLow-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and\naverting catastrophic forgetting. As a result, the VLM can be fine-tuned on\nrobot teleoperation data without fundamentally altering the underlying\narchitecture and without expensive co-training on internet-scale VLM datasets.\nThrough extensive Visual Question Answering (VQA) studies and over 800\nreal-world robotics experiments, we demonstrate that VLM2VLA preserves the\nVLM's core capabilities, enabling zero-shot generalization to novel tasks that\nrequire open-world semantic reasoning and multilingual instruction following.", "AI": {"tldr": "VLM2VLA\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4f4e\u7ea7\u52a8\u4f5c\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5fae\u8c03\u4e3a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u65f6\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5fae\u8c03VLM\u7528\u4e8e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u5176\u6838\u5fc3\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u9000\u5316\uff0c\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "VLM2VLA\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u6570\u636e\u5206\u5e03\uff0c\u5e76\u4f7f\u7528LoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u907f\u514d\u5927\u89c4\u6a21\u4fee\u6539VLM\u67b6\u6784\u3002", "result": "VLM2VLA\u5728\u4fdd\u7559VLM\u6838\u5fc3\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65b0\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7800\u591a\u6b21\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "VLM2VLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4fdd\u62a4VLM\u6838\u5fc3\u80fd\u529b\u7684VLA\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2509.22199", "pdf": "https://arxiv.org/pdf/2509.22199", "abs": "https://arxiv.org/abs/2509.22199", "authors": ["Haoyun Li", "Ivan Zhang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Zhiqin Yang", "Zhentao Zhang", "Boyuan Wang", "Chaojun Ni", "Wenkang Qin", "Xinze Chen", "Yun Ye", "Guan Huang", "Zhenbo Song", "Xingang Wang"], "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision Language Action (VLA) models derive their generalization capability\nfrom diverse training data, yet collecting embodied robot interaction data\nremains prohibitively expensive. In contrast, human demonstration videos are\nfar more scalable and cost-efficient to collect, and recent studies confirm\ntheir effectiveness in training VLA models. However, a significant domain gap\npersists between human videos and robot-executed videos, including unstable\ncamera viewpoints, visual discrepancies between human hands and robotic arms,\nand differences in motion dynamics. To bridge this gap, we propose\nMimicDreamer, a framework that turns fast, low-cost human demonstrations into\nrobot-usable supervision by jointly aligning vision, viewpoint, and actions to\ndirectly support policy training. For visual alignment, we propose H2R Aligner,\na video diffusion model that generates high-fidelity robot demonstration videos\nby transferring motion from human manipulation footage. For viewpoint\nstabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos\nvia homography and inpaints occlusions and distortions caused by warping. For\naction alignment, we map human hand trajectories to the robot frame and apply a\nconstrained inverse kinematics solver to produce feasible, low-jitter joint\ncommands with accurate pose tracking. Empirically, VLA models trained purely on\nour synthesized human-to-robot videos achieve few-shot execution on real\nrobots. Moreover, scaling training with human data significantly boosts\nperformance compared to models trained solely on real robot data; our approach\nimproves the average success rate by 14.7\\% across six representative\nmanipulation tasks.", "AI": {"tldr": "MimicDreamer\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u3001\u89c6\u89d2\u548c\u52a8\u4f5c\u5bf9\u9f50\uff0c\u5c06\u4f4e\u6210\u672c\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u7528\u7684\u76d1\u7763\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u6210\u672c\u4f4e\u4e14\u6613\u6536\u96c6\uff0c\u4f46\u4e0e\u673a\u5668\u4eba\u89c6\u9891\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5f25\u5408\u8fd9\u79cd\u5dee\u8ddd\u4ee5\u652f\u6301VLA\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "MimicDreamer\u5305\u542bH2R Aligner\uff08\u89c6\u89c9\u5bf9\u9f50\uff09\u3001EgoStabilizer\uff08\u89c6\u89d2\u7a33\u5b9a\uff09\u548c\u52a8\u4f5c\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u751f\u6210\u673a\u5668\u4eba\u6f14\u793a\u89c6\u9891\u548c\u4f18\u5316\u52a8\u4f5c\uff0c\u63d0\u5347\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u5408\u6210\u7684\u4eba\u7c7b\u8f6c\u673a\u5668\u4eba\u89c6\u9891\u8bad\u7ec3\u7684VLA\u6a21\u578b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86few-shot\u6267\u884c\uff0c\u4e14\u6027\u80fd\u63d0\u534714.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u5728\u8bad\u7ec3VLA\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u4e86\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2509.22205", "pdf": "https://arxiv.org/pdf/2509.22205", "abs": "https://arxiv.org/abs/2509.22205", "authors": ["Ke Ye", "Jiaming Zhou", "Yuanfeng Qiu", "Jiayi Liu", "Shihui Zhou", "Kun-Yu Lin", "Junwei Liang"], "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment", "categories": ["cs.RO"], "comment": null, "summary": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20\\%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems.", "AI": {"tldr": "Super-Mimic \u662f\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u65e0\u811a\u672c\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u76f4\u63a5\u63a8\u65ad\u7a0b\u5e8f\u610f\u56fe\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u673a\u5668\u4eba\u6a21\u4eff\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u57fa\u7840\u65b9\u6cd5\u5728\u9759\u6001\u89c6\u89c9\u8f93\u5165\u4e0b\u65e0\u6cd5\u5206\u89e3\u9ad8\u7ea7\u547d\u4ee4\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u7684\u6311\u6218\u3002", "method": "\u5305\u62ec Human Intent Translator\uff08HIT\uff09\u548c Future Dynamics Predictor\uff08FDP\uff09\uff0c\u5206\u522b\u89e3\u6790\u89c6\u9891\u4e3a\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u52a8\u6001\u9884\u6d4b\u3002", "result": "\u5728\u957f\u5468\u671f\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u6027\u80fd\u63d0\u5347\u8d85\u8fc7 20%\u3002", "conclusion": "\u89c6\u9891\u9a71\u52a8\u7684\u610f\u56fe\u89e3\u6790\u4e0e\u524d\u77bb\u52a8\u6001\u5efa\u6a21\u76f8\u7ed3\u5408\u662f\u5f00\u53d1\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2509.22287", "pdf": "https://arxiv.org/pdf/2509.22287", "abs": "https://arxiv.org/abs/2509.22287", "authors": ["Stina Sundstedt", "Mattias Wingren", "Susanne H\u00e4gglund", "Daniel Ventus"], "title": "Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities", "categories": ["cs.RO", "cs.AI", "cs.HC", "I.2.7; H.5.2; K.3.1; J.4"], "comment": "12 pages, 2 figures, Preprint of: Sundstedt, S., Wingren, M.,\n  H\\\"agglund, S. & Ventus, D. (2025). Leveraging Large Language Models for\n  Robot-Assisted Learning of Morphological Structures in Preschool Children\n  with Language Vulnerabilities. In: Stephanidis, C., Antona, M., Ntoa, S. &\n  Salvendy, G. (eds.), Communications in Computer and Information Science, vol.\n  2523, pp. 415-425. Springer", "summary": "Preschool children with language vulnerabilities -- such as developmental\nlanguage disorders or immigration related language challenges -- often require\nsupport to strengthen their expressive language skills. Based on the principle\nof implicit learning, speech-language therapists (SLTs) typically embed target\nmorphological structures (e.g., third person -s) into everyday interactions or\ngame-based learning activities. Educators are recommended by SLTs to do the\nsame. This approach demands precise linguistic knowledge and real-time\nproduction of various morphological forms (e.g., \"Daddy wears these when he\ndrives to work\"). The task becomes even more demanding when educators or parent\nalso must keep children engaged and manage turn-taking in a game-based\nactivity. In the TalBot project our multiprofessional team have developed an\napplication in which the Furhat conversational robot plays the word retrieval\ngame \"Alias\" with children to improve language skills. Our application\ncurrently employs a large language model (LLM) to manage gameplay, dialogue,\naffective responses, and turn-taking. Our next step is to further leverage the\ncapacity of LLMs so the robot can generate and deliver specific morphological\ntargets during the game. We hypothesize that a robot could outperform humans at\nthis task. Novel aspects of this approach are that the robot could ultimately\nserve as a model and tutor for both children and professionals and that using\nLLM capabilities in this context would support basic communication needs for\nchildren with language vulnerabilities. Our long-term goal is to create a\nrobust LLM-based Robot-Assisted Language Learning intervention capable of\nteaching a variety of morphological structures across different languages.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u6b3e\u4f7f\u7528Furhat\u5bf9\u8bdd\u673a\u5668\u4eba\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u6e38\u620f\u63d0\u5347\u5b66\u524d\u513f\u7ae5\u7684\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\uff0c\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u4eba\u7684\u8bed\u8a00\u5b66\u4e60\u5e72\u9884\u7cfb\u7edf\u3002", "motivation": "\u9488\u5bf9\u8bed\u8a00\u53d1\u5c55\u969c\u788d\u6216\u79fb\u6c11\u76f8\u5173\u8bed\u8a00\u95ee\u9898\u7684\u5b66\u524d\u513f\u7ae5\uff0c\u4f20\u7edf\u8bed\u8a00\u6cbb\u7597\u9700\u8981\u6cbb\u7597\u5e08\u548c\u6559\u80b2\u8005\u5b9e\u65f6\u751f\u6210\u7279\u5b9a\u8bed\u8a00\u7ed3\u6784\uff0c\u4efb\u52a1\u7e41\u91cd\u4e14\u6709\u6311\u6218\u3002\u673a\u5668\u4eba\u8f85\u52a9\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5b8c\u6210\u8fd9\u4e00\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684Furhat\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u5728\u201cAlias\u201d\u5355\u8bcd\u68c0\u7d22\u6e38\u620f\u4e2d\u7ba1\u7406\u5bf9\u8bdd\u3001\u60c5\u611f\u53cd\u9988\u548c\u8f6e\u6d41\u4e92\u52a8\uff0c\u5e76\u8ba1\u5212\u8fdb\u4e00\u6b65\u5229\u7528LLM\u751f\u6210\u7279\u5b9a\u8bed\u8a00\u76ee\u6807\u3002", "result": "\u76ee\u524d\u5e94\u7528\u7a0b\u5e8f\u5df2\u5b8c\u6210\u57fa\u7840\u529f\u80fd\uff0c\u4e0b\u4e00\u6b65\u662f\u4f18\u5316LLM\u751f\u6210\u7279\u5b9a\u8bed\u8a00\u7ed3\u6784\u7684\u80fd\u529b\u3002", "conclusion": "\u673a\u5668\u4eba\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u6709\u671b\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\uff0c\u5e76\u6210\u4e3a\u513f\u7ae5\u548c\u4e13\u4e1a\u4eba\u5458\u7684\u8bed\u8a00\u6a21\u578b\u548c\u5bfc\u5e08\uff0c\u672a\u6765\u76ee\u6807\u662f\u5f00\u53d1\u591a\u8bed\u8a00\u652f\u6301\u7684LLM\u673a\u5668\u4eba\u5e72\u9884\u7cfb\u7edf\u3002"}}
{"id": "2509.22288", "pdf": "https://arxiv.org/pdf/2509.22288", "abs": "https://arxiv.org/abs/2509.22288", "authors": ["Johan Hatleskog", "Morten Nissov", "Kostas Alexis"], "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, accepted by The 22nd International Conference on\n  Advanced Robotics (ICAR 2025). Supplementary video:\n  https://youtu.be/95jeWXBMN7c", "summary": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor\ngraph node per measurement to compensate for the lack of time synchronization\nbetween radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this\nstrategy results in a state creation rate of twice the individual sensor\nfrequencies. This doubling of the number of states per second yields high\noptimization costs, inhibiting real-time performance on resource-constrained\nhardware. We introduce IMU-preintegrated radar factors that use high-rate\ninertial data to propagate the most recent LiDAR state to the radar measurement\ntimestamp. This strategy maintains the node creation rate at the LiDAR\nmeasurement frequency. Assuming equal sensor rates, this lowers the number of\nnodes by 50 % and consequently the computational costs. Experiments on a single\nboard computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB\nRAM) show that our method preserves the absolute pose error of a conventional\nbaseline while simultaneously lowering the aggregated factor graph optimization\ntime by up to 56 %.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u96f7\u8fbe-LiDAR\u4f20\u611f\u5668\u5bf9\u7684\u72b6\u6001\u8282\u70b9\u6570\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u56fa\u5b9a\u6ede\u540e\u96f7\u8fbe-LiDAR-\u60ef\u6027\u5e73\u6ed1\u5668\u7531\u4e8e\u65f6\u95f4\u540c\u6b65\u95ee\u9898\uff0c\u5bfc\u81f4\u72b6\u6001\u8282\u70b9\u6570\u91cf\u7ffb\u500d\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002\u672c\u6587\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4e0d\u5f71\u54cd\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u5f15\u5165IMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\uff0c\u5229\u7528\u9ad8\u9891\u60ef\u6027\u6570\u636e\u5c06LiDAR\u72b6\u6001\u4f20\u64ad\u5230\u96f7\u8fbe\u65f6\u95f4\u6233\uff0c\u4ece\u800c\u51cf\u5c11\u8282\u70b9\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9a\u4f4d\u8bef\u5dee\u4e0e\u4f20\u7edf\u57fa\u7ebf\u4e00\u81f4\u7684\u540c\u65f6\uff0c\u5c06\u56e0\u5b50\u56fe\u4f18\u5316\u65f6\u95f4\u964d\u4f4e\u4e8656%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.22296", "pdf": "https://arxiv.org/pdf/2509.22296", "abs": "https://arxiv.org/abs/2509.22296", "authors": ["Joseph Hunt", "Koyo Fujii", "Aly Magassouba", "Praminda Caleb-Solly"], "title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm", "categories": ["cs.RO"], "comment": "ICSR 2025, 8 pages, 3 figures", "summary": "Hospital patient falls remain a critical and costly challenge worldwide.\nWhile conventional fall prevention systems typically rely on post-fall\ndetection or reactive alerts, they also often suffer from high false positive\nrates and fail to address the underlying patient needs that lead to bed-exit\nattempts. This paper presents a novel system architecture that leverages the\nInternet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction\nfor proactive and personalized patient assistance. The system integrates a\nprivacy-preserving thermal sensing model capable of real-time bed-exit\nprediction, with two coordinated robotic agents that respond dynamically based\non predicted intent and patient input. This orchestrated response could not\nonly reduce fall risk but also attend to the patient's underlying motivations\nfor movement, such as thirst, discomfort, or the need for assistance, before a\nhazardous situation arises. Our contributions with this pilot study are\nthree-fold: (1) a modular IoRT-based framework enabling distributed sensing,\nprediction, and multi-robot coordination; (2) a demonstration of low-resolution\nthermal sensing for accurate, privacy-preserving preemptive bed-exit detection;\nand (3) results from a user study and systematic error analysis that inform the\ndesign of situationally aware, multi-agent interactions in hospital settings.\nThe findings highlight how interactive and connected robotic systems can move\nbeyond passive monitoring to deliver timely, meaningful assistance, empowering\nsafer, more responsive care environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u8054\u7f51\u673a\u5668\u4eba\uff08IoRT\uff09\u7684\u65b0\u578b\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u534f\u8c03\u4eba-\u673a\u5668\u4eba-\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e3b\u52a8\u4e14\u4e2a\u6027\u5316\u7684\u60a3\u8005\u8f85\u52a9\uff0c\u4ee5\u51cf\u5c11\u533b\u9662\u60a3\u8005\u8dcc\u5012\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u8dcc\u5012\u9884\u9632\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4e8b\u540e\u68c0\u6d4b\u6216\u53cd\u5e94\u6027\u8b66\u62a5\uff0c\u4e14\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\uff0c\u672a\u80fd\u89e3\u51b3\u60a3\u8005\u8bd5\u56fe\u79bb\u5e8a\u7684\u6839\u672c\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u9690\u79c1\u4fdd\u62a4\u7684\u70ed\u611f\u6d4b\u6a21\u578b\uff08\u5b9e\u65f6\u9884\u6d4b\u79bb\u5e8a\u884c\u4e3a\uff09\u4e0e\u4e24\u4e2a\u534f\u8c03\u7684\u673a\u5668\u4eba\u4ee3\u7406\uff0c\u6839\u636e\u9884\u6d4b\u610f\u56fe\u548c\u60a3\u8005\u8f93\u5165\u52a8\u6001\u54cd\u5e94\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u5206\u5e03\u5f0f\u611f\u77e5\u3001\u9884\u6d4b\u548c\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u6237\u7814\u7a76\u548c\u7cfb\u7edf\u8bef\u5dee\u5206\u6790\u4e3a\u60c5\u5883\u611f\u77e5\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u8fde\u63a5\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u8d85\u8d8a\u88ab\u52a8\u76d1\u6d4b\uff0c\u63d0\u4f9b\u53ca\u65f6\u6709\u6548\u7684\u8f85\u52a9\uff0c\u521b\u9020\u66f4\u5b89\u5168\u3001\u54cd\u5e94\u66f4\u8fc5\u901f\u7684\u62a4\u7406\u73af\u5883\u3002"}}
{"id": "2509.22356", "pdf": "https://arxiv.org/pdf/2509.22356", "abs": "https://arxiv.org/abs/2509.22356", "authors": ["Enguang Liu", "Siyuan Liang", "Liming Lu", "Xiyu Zeng", "Xiaochun Cao", "Aishan Liu", "Shuchao Pang"], "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The safety and reliability of embodied agents rely on accurate and unbiased\nvisual perception. However, existing benchmarks mainly emphasize generalization\nand robustness under perturbations, while systematic quantification of visual\nbias remains scarce. This gap limits a deeper understanding of how perception\ninfluences decision-making stability. To address this issue, we propose\nRoboView-Bias, the first benchmark specifically designed to systematically\nquantify visual bias in robotic manipulation, following a principle of factor\nisolation. Leveraging a structured variant-generation framework and a\nperceptual-fairness validation protocol, we create 2,127 task instances that\nenable robust measurement of biases induced by individual visual factors and\ntheir interactions. Using this benchmark, we systematically evaluate three\nrepresentative embodied agents across two prevailing paradigms and report three\nkey findings: (i) all agents exhibit significant visual biases, with camera\nviewpoint being the most critical factor; (ii) agents achieve their highest\nsuccess rates on highly saturated colors, indicating inherited visual\npreferences from underlying VLMs; and (iii) visual biases show strong,\nasymmetric coupling, with viewpoint strongly amplifying color-related bias.\nFinally, we demonstrate that a mitigation strategy based on a semantic\ngrounding layer substantially reduces visual bias by approximately 54.5\\% on\nMOKA. Our results highlight that systematic analysis of visual bias is a\nprerequisite for developing safe and reliable general-purpose embodied agents.", "AI": {"tldr": "RoboView-Bias\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u91cf\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u89c6\u89c9\u504f\u89c1\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u56e0\u7d20\u9694\u79bb\u539f\u5219\u751f\u62102127\u4e2a\u4efb\u52a1\u5b9e\u4f8b\uff0c\u8bc4\u4f30\u4e09\u79cd\u4ee3\u8868\u4ee3\u7406\u7684\u89c6\u89c9\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u504f\u89c1\u7684\u7cfb\u7edf\u6027\u91cf\u5316\uff0c\u9650\u5236\u4e86\u7406\u89e3\u611f\u77e5\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u7a33\u5b9a\u6027\u3002", "method": "\u5229\u7528\u7ed3\u6784\u5316\u53d8\u91cf\u751f\u6210\u6846\u67b6\u548c\u611f\u77e5\u516c\u5e73\u9a8c\u8bc1\u534f\u8bae\uff0c\u91cf\u5316\u89c6\u89c9\u56e0\u7d20\u53ca\u5176\u4ea4\u4e92\u5f15\u8d77\u7684\u504f\u89c1\u3002", "result": "\u53d1\u73b0\u4ee3\u7406\u5b58\u5728\u663e\u8457\u89c6\u89c9\u504f\u89c1\uff08\u5982\u76f8\u673a\u89c6\u89d2\u662f\u5173\u952e\u56e0\u7d20\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u53ef\u51cf\u5c11\u504f\u89c1\u7ea654.5%\u7684\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "\u7cfb\u7edf\u6027\u5206\u6790\u89c6\u89c9\u504f\u89c1\u662f\u5f00\u53d1\u5b89\u5168\u53ef\u9760\u901a\u7528\u4ee3\u7406\u7684\u524d\u63d0\u3002"}}
{"id": "2509.22421", "pdf": "https://arxiv.org/pdf/2509.22421", "abs": "https://arxiv.org/abs/2509.22421", "authors": ["Leonel Giacobbe", "Jingdao Chen", "Chuangchuang Sun"], "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping", "categories": ["cs.RO"], "comment": null, "summary": "Grasping is a core task in robotics with various applications. However, most\ncurrent implementations are primarily designed for rigid items, and their\nperformance drops considerably when handling fragile or deformable materials\nthat require real-time feedback. Meanwhile, tactile-reactive grasping focuses\non a single agent, which limits their ability to grasp and manipulate large,\nheavy objects. To overcome this, we propose a learning-based, tactile-reactive\nmulti-agent Model Predictive Controller (MPC) for grasping a wide range of\nobjects with different softness and shapes, beyond the capabilities of\npreexisting single-agent implementations. Our system uses two Gelsight Mini\ntactile sensors [1] to extract real-time information on object texture and\nstiffness. This rich tactile feedback is used to estimate contact dynamics and\nobject compliance in real time, enabling the system to adapt its control policy\nto diverse object geometries and stiffness profiles. The learned controller\noperates in a closed loop, leveraging tactile encoding to predict grasp\nstability and adjust force and position accordingly. Our key technical\ncontributions include a multi-agent MPC formulation trained on real contact\ninteractions, a tactile-data driven method for inferring grasping states, and a\ncoordination strategy that enables collaborative control. By combining tactile\nsensing and a learning-based multi-agent MPC, our method offers a robust,\nintelligent solution for collaborative grasping in complex environments,\nsignificantly advancing the capabilities of multi-agent systems. Our approach\nis validated through extensive experiments against independent PD and MPC\nbaselines. Our pipeline outperforms the baselines regarding success rates in\nachieving and maintaining stable grasps across objects of varying sizes and\nstiffness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u89e6\u89c9\u53cd\u5e94\u591a\u667a\u80fd\u4f53MPC\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u6293\u53d6\u4e0d\u540c\u8f6f\u786c\u5ea6\u548c\u5f62\u72b6\u7684\u7269\u4f53\uff0c\u8d85\u8d8a\u73b0\u6709\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u6293\u53d6\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u7269\u4f53\uff0c\u5bf9\u8106\u5f31\u6216\u53ef\u53d8\u5f62\u6750\u6599\u7684\u5b9e\u65f6\u53cd\u9988\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u5927\u578b\u3001\u91cd\u578b\u7269\u4f53\u3002", "method": "\u5229\u7528\u4e24\u4e2aGelsight Mini\u89e6\u89c9\u4f20\u611f\u5668\u63d0\u53d6\u7269\u4f53\u7684\u5b9e\u65f6\u7eb9\u7406\u548c\u521a\u5ea6\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53MPC\u548c\u89e6\u89c9\u6570\u636e\u9a71\u52a8\u7684\u72b6\u6001\u63a8\u65ad\u65b9\u6cd5\u5b9e\u73b0\u534f\u540c\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u4e0d\u540c\u5927\u5c0f\u548c\u521a\u5ea6\u7684\u7269\u4f53\u65f6\uff0c\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u5747\u4f18\u4e8e\u72ec\u7acb\u7684PD\u548cMPC\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u5408\u89e6\u89c9\u611f\u77e5\u548c\u5b66\u4e60\u578b\u591a\u667a\u80fd\u4f53MPC\u7684\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u540c\u6293\u53d6\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22434", "pdf": "https://arxiv.org/pdf/2509.22434", "abs": "https://arxiv.org/abs/2509.22434", "authors": ["Margherita Martorana", "Francesca Urgese", "Ilaria Tiddi", "Stefan Schlobach"], "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOntoBOT\u7684\u673a\u5668\u4eba\u672c\u4f53\u8bba\uff0c\u65e8\u5728\u7edf\u4e00\u8868\u793a\u4efb\u52a1\u3001\u52a8\u4f5c\u3001\u73af\u5883\u548c\u673a\u5668\u4eba\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u670d\u52a1\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "motivation": "\u89e3\u51b3\u670d\u52a1\u673a\u5668\u4eba\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u5e73\u53f0\u8026\u5408\u6027\u9ad8\u5bfc\u81f4\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u590d\u7528\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u672c\u4f53\u8bba\u5728\u9886\u57df\u8986\u76d6\u548c\u7cfb\u7edf\u96c6\u6210\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u6269\u5c55\u73b0\u6709\u672c\u4f53\u8bba\uff08\u5982SOMA\u548cDOLCE\uff09\uff0c\u63d0\u51faOntoBOT\uff0c\u63d0\u4f9b\u4efb\u52a1\u3001\u52a8\u4f5c\u3001\u73af\u5883\u548c\u80fd\u529b\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u901a\u8fc7TIAGo\u7b49\u56db\u79cd\u673a\u5668\u4eba\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u3002", "result": "OntoBOT\u652f\u6301\u4efb\u52a1\u6267\u884c\u7684\u5f62\u5f0f\u5316\u63a8\u7406\uff0c\u5e76\u5728\u56db\u79cd\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5176\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u4efb\u52a1\u5bfc\u5411\u6267\u884c\u548c\u77e5\u8bc6\u5171\u4eab\u7684\u80fd\u529b\u3002", "conclusion": "OntoBOT\u4e3a\u670d\u52a1\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.22441", "pdf": "https://arxiv.org/pdf/2509.22441", "abs": "https://arxiv.org/abs/2509.22441", "authors": ["Zhangyuan Wang", "Yunpeng Zhu", "Yuqi Yan", "Xiaoyuan Tian", "Xinhao Shao", "Meixuan Li", "Weikun Li", "Guangsheng Su", "Weicheng Cui", "Dixia Fan"], "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation", "categories": ["cs.RO"], "comment": "This paper introduces the first VLA framework for AUVs, featuring a\n  dual-brain architecture and zero-data MPC for real-world underwater\n  navigation", "summary": "This paper presents UnderwaterVLA, a novel framework for autonomous\nunderwater navigation that integrates multimodal foundation models with\nembodied intelligence systems. Underwater operations remain difficult due to\nhydrodynamic disturbances, limited communication bandwidth, and degraded\nsensing in turbid waters. To address these challenges, we introduce three\ninnovations. First, a dual-brain architecture decouples high-level mission\nreasoning from low-level reactive control, enabling robust operation under\ncommunication and computational constraints. Second, we apply\nVision-Language-Action(VLA) models to underwater robotics for the first time,\nincorporating structured chain-of-thought reasoning for interpretable\ndecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)\nscheme compensates for fluid effects in real time without costly task-specific\ntraining. Experimental results in field tests show that UnderwaterVLA reduces\nnavigation errors in degraded visual conditions while maintaining higher task\ncompletion by 19% to 27% over baseline. By minimizing reliance on\nunderwater-specific training data and improving adaptability across\nenvironments, UnderwaterVLA provides a scalable and cost-effective path toward\nthe next generation of intelligent AUVs.", "AI": {"tldr": "UnderwaterVLA\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u6c34\u4e0b\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u8111\u67b6\u6784\u3001VLA\u6a21\u578b\u548c\u6d41\u4f53\u529b\u5b66MPC\u65b9\u6848\u89e3\u51b3\u6c34\u4e0b\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u663e\u8457\u51cf\u5c11\u5bfc\u822a\u8bef\u5dee\u5e76\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u64cd\u4f5c\u4e2d\u56e0\u6d41\u4f53\u6270\u52a8\u3001\u6709\u9650\u901a\u4fe1\u5e26\u5bbd\u548c\u6d51\u6d4a\u6c34\u57df\u611f\u77e5\u9000\u5316\u5e26\u6765\u7684\u56f0\u96be\u3002", "method": "1. \u53cc\u8111\u67b6\u6784\u5206\u79bb\u9ad8\u5c42\u4efb\u52a1\u63a8\u7406\u4e0e\u5e95\u5c42\u53cd\u5e94\u63a7\u5236\uff1b2. \u9996\u6b21\u5c06VLA\u6a21\u578b\u5e94\u7528\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff1b3. \u6d41\u4f53\u529b\u5b66MPC\u65b9\u6848\u5b9e\u65f6\u8865\u507f\u6d41\u4f53\u6548\u5e94\u3002", "result": "\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0cUnderwaterVLA\u51cf\u5c11\u4e86\u5bfc\u822a\u8bef\u5dee\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad819%\u81f327%\u3002", "conclusion": "UnderwaterVLA\u51cf\u5c11\u5bf9\u6c34\u4e0b\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u667a\u80fdAUV\u53d1\u5c55\u8def\u5f84\u3002"}}
{"id": "2509.22469", "pdf": "https://arxiv.org/pdf/2509.22469", "abs": "https://arxiv.org/abs/2509.22469", "authors": ["Ben Rossano", "Jaein Lim", "Jonathan P. How"], "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards", "categories": ["cs.RO"], "comment": "8 pages", "summary": "This paper proposes a task allocation algorithm for teams of heterogeneous\nrobots in environments with uncertain task requirements. We model these\nrequirements as probability distributions over capabilities and use this model\nto allocate tasks such that robots with complementary skills naturally position\nnear uncertain tasks, proactively mitigating task failures without wasting\nresources. We introduce a market-based approach that optimizes the joint team\nobjective while explicitly capturing coupled rewards between robots, offering a\npolynomial-time solution in decentralized settings with strict communication\nassumptions. Comparative experiments against benchmark algorithms demonstrate\nthe effectiveness of our approach and highlight the challenges of incorporating\ncoupled rewards in a decentralized formulation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u5efa\u6a21\u4efb\u52a1\u9700\u6c42\uff0c\u4f18\u5316\u8d44\u6e90\u5229\u7528\u5e76\u51cf\u5c11\u4efb\u52a1\u5931\u8d25\u3002\u91c7\u7528\u57fa\u4e8e\u5e02\u573a\u7684\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u901a\u4fe1\u5047\u8bbe\u4e0b\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u3002", "motivation": "\u7814\u7a76\u5f02\u6784\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u4efb\u52a1\u9700\u6c42\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u4e92\u8865\u6280\u80fd\u4f18\u5316\u56e2\u961f\u534f\u4f5c\uff0c\u51cf\u5c11\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u4f7f\u7528\u6982\u7387\u5206\u5e03\u5efa\u6a21\u4efb\u52a1\u9700\u6c42\uff0c\u63d0\u51fa\u57fa\u4e8e\u5e02\u573a\u7684\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\uff0c\u8003\u8651\u673a\u5668\u4eba\u95f4\u7684\u8026\u5408\u5956\u52b1\uff0c\u63d0\u4f9b\u591a\u9879\u5f0f\u65f6\u95f4\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7b97\u6cd5\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u4f46\u51f8\u663e\u4e86\u5728\u5206\u6563\u5f0f\u73af\u5883\u4e0b\u5904\u7406\u8026\u5408\u5956\u52b1\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4efb\u52a1\u5206\u914d\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5206\u6563\u5f0f\u73af\u5883\u4e2d\u7684\u8026\u5408\u5956\u52b1\u95ee\u9898\u3002"}}
{"id": "2509.22493", "pdf": "https://arxiv.org/pdf/2509.22493", "abs": "https://arxiv.org/abs/2509.22493", "authors": ["Alberto Olivares-Alarcos", "Sergi Foix", "J\u00falia Borr\u00e0s", "Gerard Canal", "Guillem Aleny\u00e0"], "title": "Ontological foundations for contrastive explanatory narration of robot plans", "categories": ["cs.RO", "cs.AI", "cs.IR", "cs.LO"], "comment": "This version was submitted to the journal Information Sciences and is\n  under review since October 2024", "summary": "Mutual understanding of artificial agents' decisions is key to ensuring a\ntrustworthy and successful human-robot interaction. Hence, robots are expected\nto make reasonable decisions and communicate them to humans when needed. In\nthis article, the focus is on an approach to modeling and reasoning about the\ncomparison of two competing plans, so that robots can later explain the\ndivergent result. First, a novel ontological model is proposed to formalize and\nreason about the differences between competing plans, enabling the\nclassification of the most appropriate one (e.g., the shortest, the safest, the\nclosest to human preferences, etc.). This work also investigates the\nlimitations of a baseline algorithm for ontology-based explanatory narration.\nTo address these limitations, a novel algorithm is presented, leveraging\ndivergent knowledge between plans and facilitating the construction of\ncontrastive narratives. Through empirical evaluation, it is observed that the\nexplanations excel beyond the baseline method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u8bba\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u548c\u89e3\u91ca\u673a\u5668\u4eba\u51b3\u7b56\u4e2d\u7ade\u4e89\u8ba1\u5212\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u786e\u4fdd\u4eba\u673a\u4ea4\u4e92\u4e2d\u673a\u5668\u4eba\u51b3\u7b56\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u4fe1\u8d56\u6027\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u89e3\u91ca\u5176\u51b3\u7b56\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u8bba\u6a21\u578b\u6765\u5f62\u5f0f\u5316\u548c\u63a8\u7406\u7ade\u4e89\u8ba1\u5212\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\u751f\u6210\u5bf9\u6bd4\u6027\u89e3\u91ca\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u751f\u6210\u89e3\u91ca\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5c55\u793a\u8ba1\u5212\u7684\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u51b3\u7b56\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.22498", "pdf": "https://arxiv.org/pdf/2509.22498", "abs": "https://arxiv.org/abs/2509.22498", "authors": ["Katrina Ashton", "Chahyon Ku", "Shrey Shah", "Wen Jiang", "Kostas Daniilidis", "Bernadette Bucher"], "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes", "categories": ["cs.RO"], "comment": null, "summary": "Language-specified mobile manipulation tasks in novel environments\nsimultaneously face challenges interacting with a scene which is only partially\nobserved, grounding semantic information from language instructions to the\npartially observed scene, and actively updating knowledge of the scene with new\nobservations. To address these challenges, we propose HELIOS, a hierarchical\nscene representation and associated search objective to perform language\nspecified pick and place mobile manipulation tasks. We construct 2D maps\ncontaining the relevant semantic and occupancy information for navigation while\nsimultaneously actively constructing 3D Gaussian representations of\ntask-relevant objects. We fuse observations across this multi-layered\nrepresentation while explicitly modeling the multi-view consistency of the\ndetections of each object. In order to efficiently search for the target\nobject, we formulate an objective function balancing exploration of unobserved\nor uncertain regions with exploitation of scene semantic information. We\nevaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and\nplace benchmark in which perception is challenging due to large and complex\nscenes with comparatively small target objects. HELIOS achieves\nstate-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also\ntransfer to the real world without requiring additional data, as we illustrate\nby demonstrating it in a real world office environment on a Spot robot.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HELIOS\uff0c\u4e00\u79cd\u5206\u5c42\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u8a00\u6307\u5b9a\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u90e8\u5206\u89c2\u5bdf\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u54082D\u8bed\u4e49\u5730\u56fe\u548c3D\u9ad8\u65af\u8868\u793a\uff0c\u4f18\u5316\u76ee\u6807\u51fd\u6570\u4ee5\u5b9e\u73b0\u9ad8\u6548\u641c\u7d22\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u90e8\u5206\u89c2\u5bdf\u573a\u666f\u4e2d\u8fdb\u884c\u8bed\u8a00\u6307\u5b9a\u79fb\u52a8\u64cd\u4f5c\u65f6\u9762\u4e34\u7684\u4ea4\u4e92\u3001\u8bed\u4e49\u4fe1\u606f\u63a5\u5730\u548c\u52a8\u6001\u66f4\u65b0\u573a\u666f\u77e5\u8bc6\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86HELIOS\u65b9\u6cd5\uff0c\u7ed3\u54082D\u8bed\u4e49\u5730\u56fe\u548c3D\u9ad8\u65af\u8868\u793a\uff0c\u663e\u5f0f\u5efa\u6a21\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u7684\u76ee\u6807\u51fd\u6570\u641c\u7d22\u76ee\u6807\u7269\u4f53\u3002", "result": "\u5728OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684Spot\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "HELIOS\u901a\u8fc7\u5206\u5c42\u8868\u793a\u548c\u76ee\u6807\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u90e8\u5206\u89c2\u5bdf\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u6307\u5b9a\u4efb\u52a1\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.22550", "pdf": "https://arxiv.org/pdf/2509.22550", "abs": "https://arxiv.org/abs/2509.22550", "authors": ["Xiaoyun Qiu", "Haichao Liu", "Yue Pan", "Jun Ma", "Xinhu Zheng"], "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment", "categories": ["cs.RO"], "comment": null, "summary": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u7684\u6362\u9053\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bc6\u522b\u9a7e\u9a76\u98ce\u683c\u548c\u5408\u4f5c\u610f\u8bc6\u51b3\u7b56\uff0c\u63d0\u5347\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u4e0e\u591a\u6837\u5316\u7684\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HVs\uff09\u4e92\u52a8\uff0c\u4e0d\u53ef\u9884\u6d4b\u7684\u610f\u56fe\u548c\u5f02\u6784\u884c\u4e3a\u4f7f\u5f97\u5b89\u5168\u9ad8\u6548\u7684\u6362\u9053\u64cd\u4f5c\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u8fc7\u5ea6\u7b80\u5316\u8fd9\u4e9b\u4e92\u52a8\u3002", "method": "\u7ed3\u5408\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u3001\u5408\u4f5c\u611f\u77e5\u51b3\u7b56\u548c\u534f\u8c03\u8fd0\u52a8\u89c4\u5212\u3002\u4f7f\u7528NGSIM\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u5b9e\u65f6\u8bc6\u522b\u9a7e\u9a76\u98ce\u683c\uff1b\u5408\u4f5c\u8bc4\u5206\u4f30\u8ba1\u5468\u56f4\u9a7e\u9a76\u5458\u7684\u610f\u56fe\u548c\u5408\u4f5c\u610f\u613f\uff1b\u51b3\u7b56\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u4e0e\u9006\u5f3a\u5316\u5b66\u4e60\uff1b\u8f68\u8ff9\u751f\u6210\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548cIRL\u610f\u56fe\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u6362\u9053\u8bc6\u522b\u4e2d\u7684\u51c6\u786e\u7387\u4e3a94.2%\uff0cF1\u5206\u6570\u4e3a94.3%\uff0c\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u548c\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd54-15%\u3002", "conclusion": "\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u9a7e\u9a76\u5458\u95f4\u7684\u5f02\u6784\u6027\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4eba\u672c\u5316\u81ea\u52a8\u9a7e\u9a76\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22573", "pdf": "https://arxiv.org/pdf/2509.22573", "abs": "https://arxiv.org/abs/2509.22573", "authors": ["Farida Mohsen", "Ali Safa"], "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Efficiently detecting human intent to interact with ubiquitous robots is\ncrucial for effective human-robot interaction (HRI) and collaboration. Over the\npast decade, deep learning has gained traction in this field, with most\nexisting approaches relying on multimodal inputs, such as RGB combined with\ndepth (RGB-D), to classify time-sequence windows of sensory data as interactive\nor non-interactive. In contrast, we propose a novel RGB-only pipeline for\npredicting human interaction intent with frame-level precision, enabling faster\nrobot responses and improved service quality. A key challenge in intent\nprediction is the class imbalance inherent in real-world HRI datasets, which\ncan hinder the model's training and generalization. To address this, we\nintroduce MINT-RVAE, a synthetic sequence generation method, along with new\nloss functions and training strategies that enhance generalization on\nout-of-sample data. Our approach achieves state-of-the-art performance (AUROC:\n0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB\ninput and supporting precise frame onset prediction. Finally, to support future\nresearch, we openly release our new dataset with frame-level labeling of human\ninteraction intent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u7684\u65b0\u578b\u6d41\u7a0b\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u4ea4\u4e92\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u548c\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u9ad8\u6548\u68c0\u6d4b\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u610f\u56fe\u5bf9\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u6a21\u6001\u8f93\u5165\uff0c\u800c\u672c\u6587\u63d0\u51fa\u4ec5\u9700RGB\u8f93\u5165\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528RGB-only\u6d41\u7a0b\uff0c\u7ed3\u5408MINT-RVAE\u5408\u6210\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u53ca\u65b0\u635f\u5931\u51fd\u6570\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728AUROC\u6307\u6807\u4e0a\u8fbe\u52300.95\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff080.90-0.912\uff09\uff0c\u5e76\u652f\u6301\u7cbe\u786e\u5e27\u7ea7\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u516c\u5f00\u4e86\u5e26\u5e27\u7ea7\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.22578", "pdf": "https://arxiv.org/pdf/2509.22578", "abs": "https://arxiv.org/abs/2509.22578", "authors": ["Yuan Xu", "Jiabing Yang", "Xiaofeng Wang", "Yixiang Chen", "Zheng Zhu", "Bowen Fang", "Guan Huang", "Xinze Chen", "Yun Ye", "Qiang Zhang", "Peiyan Li", "Xiangnan Wu", "Kai Wang", "Bing Zhan", "Shuo Lu", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning based policies perform well in robotic manipulation, but\nthey often degrade under *egocentric viewpoint shifts* when trained from a\nsingle egocentric viewpoint. To address this issue, we present **EgoDemoGen**,\na framework that generates *paired* novel egocentric demonstrations by\nretargeting actions in the novel egocentric frame and synthesizing the\ncorresponding egocentric observation videos with proposed generative video\nrepair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint\nreprojected scene video and a robot-only video rendered from the retargeted\njoint actions. EgoViewTransfer is finetuned from a pretrained video generation\nmodel using self-supervised double reprojection strategy. We evaluate\nEgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After\ntraining with a mixture of EgoDemoGen-generated novel egocentric demonstrations\nand original standard egocentric demonstrations, policy success rate improves\n**absolutely** by **+17.0%** for standard egocentric viewpoint and by\n**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,\nthe **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,\nperformance continues to improve as the proportion of EgoDemoGen-generated\ndemonstrations increases, with diminishing returns. These results demonstrate\nthat EgoDemoGen provides a practical route to egocentric viewpoint-robust\nrobotic manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEgoDemoGen\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u914d\u5bf9\u7684\u65b0\u89c6\u89d2\u6f14\u793a\u89c6\u9891\uff0c\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5355\u4e00\u89c6\u89d2\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u6807\u51c6\u548c\u65b0\u89c6\u89d2\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5355\u4e00\u89c6\u89d2\u4e0b\u8bad\u7ec3\u65f6\uff0c\u9762\u5bf9\u89c6\u89d2\u53d8\u5316\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u4e3a\u4e86\u589e\u5f3a\u89c6\u89d2\u9c81\u68d2\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u751f\u6210\u591a\u89c6\u89d2\u6f14\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEgoDemoGen\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u4f5c\u91cd\u5b9a\u5411\u548c\u751f\u6210\u89c6\u9891\u4fee\u590d\u6a21\u578bEgoViewTransfer\uff0c\u5408\u6210\u65b0\u89c6\u89d2\u7684\u6f14\u793a\u89c6\u9891\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5fae\u8c03\u548c\u81ea\u76d1\u7763\u91cd\u6295\u5f71\u7b56\u7565\u5b9e\u73b0\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528EgoDemoGen\u751f\u6210\u7684\u6f14\u793a\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u6210\u529f\u7387\uff08\u4eff\u771f\u4e2d\u6807\u51c6\u89c6\u89d2+17.0%\uff0c\u65b0\u89c6\u89d2+17.7%\uff1b\u771f\u5b9e\u673a\u5668\u4eba+18.3%\u548c+25.8%\uff09\u3002", "conclusion": "EgoDemoGen\u4e3a\u63d0\u5347\u673a\u5668\u4eba\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6548\u679c\u968f\u6f14\u793a\u6bd4\u4f8b\u589e\u52a0\u800c\u6301\u7eed\u63d0\u5347\u3002"}}
{"id": "2509.22642", "pdf": "https://arxiv.org/pdf/2509.22642", "abs": "https://arxiv.org/abs/2509.22642", "authors": ["Xiaowei Chi", "Peidong Jia", "Chun-Kai Fan", "Xiaozhu Ju", "Weishi Mi", "Kevin Zhang", "Zhiyuan Qin", "Wanxin Tian", "Kuangzhi Ge", "Hao Li", "Zezhong Qian", "Anthony Chen", "Qiang Zhou", "Yueru Jia", "Jiaming Liu", "Yong Dai", "Qingpo Wuwu", "Chengyu Bai", "Yu-Kai Wang", "Ying Li", "Lizhang Chen", "Yong Bao", "Zhiyuan Jiang", "Jiacheng Zhu", "Kai Tang", "Ruichuan An", "Yulin Luo", "Qiuxuan Feng", "Siyuan Zhou", "Chi-min Chan", "Chengkai Hou", "Wei Xue", "Sirui Han", "Yike Guo", "Shanghang Zhang", "Jian Tang"], "title": "WoW: Towards a World omniscient World model Through Embodied Interaction", "categories": ["cs.RO", "cs.CV", "cs.MM"], "comment": null, "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u4ea4\u4e92\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u751f\u6210\u4e16\u754c\u6a21\u578bWoW\uff0c\u5f3a\u8c03\u7269\u7406\u76f4\u89c9\u9700\u8981\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u4e30\u5bcc\u4ea4\u4e92\u6765\u5efa\u7acb\u3002\u901a\u8fc7SOPHIA\u548c\u9006\u52a8\u529b\u5b66\u6a21\u578b\u4f18\u5316\uff0cWoW\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u6a21\u578b\uff08\u5982Sora\uff09\u4f9d\u8d56\u4e8e\u88ab\u52a8\u89c2\u5bdf\uff0c\u96be\u4ee5\u7406\u89e3\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3002\u4f5c\u8005\u5047\u8bbe\u771f\u5b9e\u7684\u7269\u7406\u76f4\u89c9\u9700\u8981\u57fa\u4e8e\u5927\u91cf\u56e0\u679c\u4e30\u5bcc\u7684\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u751f\u6210\u4e16\u754c\u6a21\u578bWoW\uff0c\u4f7f\u7528200\u4e07\u6761\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u6570\u636e\uff0c\u5e76\u7ed3\u5408SOPHIA\u548c\u9006\u52a8\u529b\u5b66\u6a21\u578b\u4f18\u5316\u751f\u6210\u7ed3\u679c\u3002", "result": "WoW\u5728\u7269\u7406\u4e00\u81f4\u6027\u3001\u78b0\u649e\u52a8\u529b\u5b66\u548c\u7269\u4f53\u6301\u4e45\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728WoWBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u662fAI\u53d1\u5c55\u7269\u7406\u76f4\u89c9\u7684\u57fa\u7840\uff0c\u6a21\u578b\u3001\u6570\u636e\u548c\u57fa\u51c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2509.22643", "pdf": "https://arxiv.org/pdf/2509.22643", "abs": "https://arxiv.org/abs/2509.22643", "authors": ["Wenkai Guo", "Guanxing Lu", "Haoyuan Deng", "Zhenyu Wu", "Yansong Tang", "Ziwei Wang"], "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search", "categories": ["cs.RO"], "comment": "9 pages", "summary": "Vision-Language-Action models (VLAs) achieve strong performance in general\nrobotic manipulation tasks by scaling imitation learning. However, existing\nVLAs are limited to predicting short-sighted next-action, which struggle with\nlong-horizon trajectory tasks due to incremental deviations. To address this\nproblem, we propose a plug-in framework named VLA-Reasoner that effectively\nempowers off-the-shelf VLAs with the capability of foreseeing future states via\ntest-time scaling. Specifically, VLA-Reasoner samples and rolls out possible\naction trajectories where involved actions are rationales to generate future\nstates via a world model, which enables VLA-Reasoner to foresee and reason\npotential outcomes and search for the optimal actions. We further leverage\nMonte Carlo Tree Search (MCTS) to improve search efficiency in large action\nspaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a\nconfidence sampling mechanism based on Kernel Density Estimation (KDE), to\nenable efficient exploration in MCTS without redundant VLA queries. We evaluate\nintermediate states in MCTS via an offline reward shaping strategy, to score\npredicted futures and correct deviations with long-term feedback. We conducted\nextensive experiments in both simulators and the real world, demonstrating that\nour proposed VLA-Reasoner achieves significant improvements over the\nstate-of-the-art VLAs. Our method highlights a potential pathway toward\nscalable test-time computation of robotic manipulation.", "AI": {"tldr": "VLA-Reasoner\u901a\u8fc7\u524d\u77bb\u6027\u63a8\u7406\u548c\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\uff08MCTS\uff09\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4ec5\u80fd\u9884\u6d4b\u77ed\u89c6\u7684\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u56e0\u9010\u6b65\u504f\u5dee\u5bfc\u81f4\u7684\u95ee\u9898\u3002", "method": "VLA-Reasoner\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408MCTS\u548c\u7f6e\u4fe1\u5ea6\u91c7\u6837\u673a\u5236\u4f18\u5316\u52a8\u4f5c\u641c\u7d22\uff0c\u540c\u65f6\u5229\u7528\u79bb\u7ebf\u5956\u52b1\u7b56\u7565\u6821\u6b63\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLA-Reasoner\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLA\u6a21\u578b\u3002", "conclusion": "VLA-Reasoner\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8def\u5f84\u3002"}}
{"id": "2509.22652", "pdf": "https://arxiv.org/pdf/2509.22652", "abs": "https://arxiv.org/abs/2509.22652", "authors": ["E-Ro Nguyen", "Yichi Zhang", "Kanchana Ranasinghe", "Xiang Li", "Michael S. Ryoo"], "title": "Pixel Motion Diffusion is What We Need for Robot Control", "categories": ["cs.RO", "cs.CV"], "comment": "16 pages, 7 figures", "summary": "We present DAWN (Diffusion is All We Need for robot control), a unified\ndiffusion-based framework for language-conditioned robotic manipulation that\nbridges high-level motion intent and low-level robot action via structured\npixel motion representation. In DAWN, both the high-level and low-level\ncontrollers are modeled as diffusion processes, yielding a fully trainable,\nend-to-end system with interpretable intermediate motion abstractions. DAWN\nachieves state-of-the-art results on the challenging CALVIN benchmark,\ndemonstrating strong multi-task performance, and further validates its\neffectiveness on MetaWorld. Despite the substantial domain gap between\nsimulation and reality and limited real-world data, we demonstrate reliable\nreal-world transfer with only minimal finetuning, illustrating the practical\nviability of diffusion-based motion abstractions for robotic control. Our\nresults show the effectiveness of combining diffusion modeling with\nmotion-centric representations as a strong baseline for scalable and robust\nrobot learning. Project page: https://nero1342.github.io/DAWN/", "AI": {"tldr": "DAWN\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u8a00\u6761\u4ef6\u5316\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u50cf\u7d20\u8fd0\u52a8\u8868\u793a\u8fde\u63a5\u9ad8\u5c42\u8fd0\u52a8\u610f\u56fe\u548c\u4f4e\u5c42\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8bed\u8a00\u6761\u4ef6\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9ad8\u5c42\u8fd0\u52a8\u610f\u56fe\u4e0e\u4f4e\u5c42\u52a8\u4f5c\u4e4b\u95f4\u7684\u8fde\u63a5\u95ee\u9898\uff0cDAWN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8fd0\u52a8\u62bd\u8c61\u3002", "method": "DAWN\u5c06\u9ad8\u5c42\u548c\u4f4e\u5c42\u63a7\u5236\u5668\u5efa\u6a21\u4e3a\u6269\u6563\u8fc7\u7a0b\uff0c\u5f62\u6210\u4e00\u4e2a\u5b8c\u5168\u53ef\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u7ed3\u6784\u5316\u50cf\u7d20\u8fd0\u52a8\u8868\u793a\u3002", "result": "DAWN\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728MetaWorld\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002\u5373\u4f7f\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u4e14\u771f\u5b9e\u6570\u636e\u6709\u9650\uff0cDAWN\u4ecd\u80fd\u901a\u8fc7\u5c11\u91cf\u5fae\u8c03\u5b9e\u73b0\u53ef\u9760\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "DAWN\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u4e0e\u8fd0\u52a8\u4e2d\u5fc3\u8868\u793a\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u529b\u57fa\u7ebf\u3002"}}
{"id": "2509.22653", "pdf": "https://arxiv.org/pdf/2509.22653", "abs": "https://arxiv.org/abs/2509.22653", "authors": ["Chih Yao Hu", "Yang-Sen Lin", "Yuna Lee", "Chih-Hai Su", "Jie-Ying Lee", "Shr-Ruei Tsai", "Chin-Yang Lin", "Kuan-Wen Chen", "Tsung-Wei Ke", "Yu-Lun Liu"], "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "CoRL 2025. Project page: https://spf-web.pages.dev", "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev", "AI": {"tldr": "SPF\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u822a\u7a7a\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u52a8\u4f5c\u9884\u6d4b\u89c6\u4e3a2D\u7a7a\u95f4\u5b9a\u4f4d\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5bfc\u822a\u65b9\u6cd5\u5c06\u52a8\u4f5c\u9884\u6d4b\u89c6\u4e3a\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "SPF\u5229\u7528VLM\u5c06\u6a21\u7cca\u7684\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u8f93\u5165\u56fe\u50cf\u4e0a\u76842D\u8def\u5f84\u70b9\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u8ddd\u79bb\u9884\u6d4b\u8f6c\u6362\u4e3a3D\u4f4d\u79fb\u5411\u91cf\u4f5c\u4e3a\u65e0\u4eba\u673a\u52a8\u4f5c\u547d\u4ee4\u3002", "result": "SPF\u5728DRL\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7edd\u5bf9\u4f18\u52bf\u63d0\u534763%\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SPF\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\u548c\u5bf9\u4e0d\u540cVLM\u7684\u6cdb\u5316\u6027\uff0c\u4e3a\u822a\u7a7a\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21386", "pdf": "https://arxiv.org/pdf/2509.21386", "abs": "https://arxiv.org/abs/2509.21386", "authors": ["Anja Sheppard", "Tyler Smithline", "Andrew Scheffer", "David Smith", "Advaith V. Sethuraman", "Ryan Bird", "Sabrina Lin", "Katherine A. Skinner"], "title": "ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Accepted to OCEANS 2025 Great Lakes", "summary": "In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that\ndetects shipwrecks from multibeam sonar data. Shipwrecks are an important\nhistorical marker of maritime history, and can be discovered through manual\ninspection of bathymetric data. However, this is a time-consuming process and\noften requires expert analysis. Our proposed tool allows users to automatically\npreprocess bathymetry data, perform deep learning inference, threshold model\noutputs, and produce either pixel-wise segmentation masks or bounding boxes of\npredicted shipwrecks. The backbone of this open-source tool is a deep learning\nmodel, which is trained on a variety of shipwreck data from the Great Lakes and\nthe coasts of Ireland. Additionally, we employ synthetic data generation in\norder to increase the size and diversity of our dataset. We demonstrate\nsuperior segmentation performance with our open-source tool and training\npipeline as compared to a deep learning-based ArcGIS toolkit and a more\nclassical inverse sinkhole detection method. The open-source tool can be found\nat https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.", "AI": {"tldr": "ShipwreckFinder\u662f\u4e00\u4e2a\u5f00\u6e90\u7684QGIS\u63d2\u4ef6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4ece\u591a\u6ce2\u675f\u58f0\u7eb3\u6570\u636e\u4e2d\u81ea\u52a8\u68c0\u6d4b\u6c89\u8239\u3002", "motivation": "\u6c89\u8239\u662f\u6d77\u6d0b\u5386\u53f2\u7684\u91cd\u8981\u6807\u5fd7\uff0c\u4f46\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u5de5\u5177\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u5206\u5272\u6216\u8fb9\u754c\u6846\u9884\u6d4b\u3002", "result": "\u76f8\u6bd4ArcGIS\u5de5\u5177\u5305\u548c\u4f20\u7edf\u65b9\u6cd5\uff0cShipwreckFinder\u5728\u5206\u5272\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u5f00\u6e90\u5de5\u5177\u4e3a\u6c89\u8239\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5386\u53f2\u548c\u6d77\u6d0b\u7814\u7a76\u3002"}}
{"id": "2509.21405", "pdf": "https://arxiv.org/pdf/2509.21405", "abs": "https://arxiv.org/abs/2509.21405", "authors": ["Nyi Nyi Aung", "Neil Muralles", "Adrian Stein"], "title": "Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification", "categories": ["cs.LG", "cs.RO"], "comment": "2025 International Conference on Machine Learning and Applications\n  (ICMLA)", "summary": "This work addresses object identification under known dynamics in unmanned\naerial vehicle applications, where learning and classification are combined\nthrough a physics-informed residual neural network. The proposed framework\nleverages physics-informed learning for state mapping and state-derivative\nprediction, while a softmax layer enables multi-class confidence estimation.\nQuadcopter, fixed-wing, and helicopter aerial vehicles are considered as case\nstudies. The results demonstrate high classification accuracy with reduced\ntraining time, offering a promising solution for system identification problems\nin domains where the underlying dynamics are well understood.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u7684\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u57fa\u4e8e\u5df2\u77e5\u52a8\u529b\u5b66\u7684\u7269\u4f53\u8bc6\u522b\uff0c\u5e76\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u7269\u4f53\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5df2\u77e5\u52a8\u529b\u5b66\u6a21\u578b\u7684\u573a\u666f\u4e0b\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u7684\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u72b6\u6001\u6620\u5c04\u548c\u72b6\u6001\u5bfc\u6570\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528softmax\u5c42\u8fdb\u884c\u591a\u7c7b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u4e3a\u52a8\u529b\u5b66\u6a21\u578b\u5df2\u77e5\u7684\u7cfb\u7edf\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21464", "pdf": "https://arxiv.org/pdf/2509.21464", "abs": "https://arxiv.org/abs/2509.21464", "authors": ["Dereje Shenkut", "B. V. K Vijaya Kumar"], "title": "Residual Vector Quantization For Communication-Efficient Multi-Agent Perception", "categories": ["cs.CV", "cs.RO"], "comment": "5 pages", "summary": "Multi-agent collaborative perception (CP) improves scene understanding by\nsharing information across connected agents such as autonomous vehicles,\nunmanned aerial vehicles, and robots. Communication bandwidth, however,\nconstrains scalability. We present ReVQom, a learned feature codec that\npreserves spatial identity while compressing intermediate features. ReVQom is\nan end-to-end method that compresses feature dimensions via a simple bottleneck\nnetwork followed by multi-stage residual vector quantization (RVQ). This allows\nonly per-pixel code indices to be transmitted, reducing payloads from 8192 bits\nper pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent\nwith minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves\n273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),\nReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables\nultra-low-bandwidth operation with graceful degradation. ReVQom allows\nefficient and accurate multi-agent collaborative perception with a step toward\npractical V2X deployment.", "AI": {"tldr": "ReVQom\u662f\u4e00\u79cd\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u548c\u74f6\u9888\u7f51\u7edc\u538b\u7f29\u591a\u4ee3\u7406\u534f\u4f5c\u611f\u77e5\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u5927\u5e45\u51cf\u5c11\u5e26\u5bbd\u9700\u6c42\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u4ee3\u7406\u534f\u4f5c\u611f\u77e5\u4e2d\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u7684\u74f6\u9888\u7f51\u7edc\u548c\u591a\u9636\u6bb5\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u538b\u7f29\u7279\u5f81\u7ef4\u5ea6\uff0c\u4ec5\u4f20\u8f93\u6bcf\u50cf\u7d20\u4ee3\u7801\u7d22\u5f15\u3002", "result": "\u5728DAIR-V2X\u6570\u636e\u96c6\u4e0a\uff0cReVQom\u5b9e\u73b0\u4e86273x\u52301365x\u7684\u538b\u7f29\uff0c\u4e14\u572818 bpp\u65f6\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u7279\u5f81\u65b9\u6cd5\u3002", "conclusion": "ReVQom\u4e3a\u591a\u4ee3\u7406\u534f\u4f5c\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4f4e\u5e26\u5bbd\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86V2X\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.21930", "pdf": "https://arxiv.org/pdf/2509.21930", "abs": "https://arxiv.org/abs/2509.21930", "authors": ["Jiahui Wang", "Changhao Chen"], "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted as a poster in NeurIPS 2025", "summary": "Visual navigation is essential for robotics and embodied AI. However,\nexisting foundation models, particularly those with transformer decoders,\nsuffer from high computational overhead and lack interpretability, limiting\ntheir deployment in resource-tight scenarios. To address this, we propose\nDynaNav, a Dynamic Visual Navigation framework that adapts feature and layer\nselection based on scene complexity. It employs a trainable hard feature\nselector for sparse operations, enhancing efficiency and interpretability.\nAdditionally, we integrate feature selection into an early-exit mechanism, with\nBayesian Optimization determining optimal exit thresholds to reduce\ncomputational cost. Extensive experiments in real-world-based datasets and\nsimulated environments demonstrate the effectiveness of DynaNav. Compared to\nViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,\nand 32.8% lower memory usage, while improving navigation performance across\nfour public datasets.", "AI": {"tldr": "DynaNav\u662f\u4e00\u4e2a\u52a8\u6001\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u573a\u666f\u590d\u6742\u5ea6\u9009\u62e9\u7279\u5f81\u548c\u5c42\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff08\u5c24\u5176\u662fTransformer\u89e3\u7801\u5668\uff09\u5728\u8d44\u6e90\u7d27\u5f20\u7684\u573a\u666f\u4e2d\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u4f4e\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faDynaNav\u6846\u67b6\uff0c\u5305\u62ec\u53ef\u8bad\u7ec3\u7684\u786c\u7279\u5f81\u9009\u62e9\u5668\u548c\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65e9\u9000\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u64cd\u4f5c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6a21\u62df\u73af\u5883\u4e2d\uff0cDynaNav\u5b9e\u73b0\u4e862.26\u500d\u7684FLOPs\u51cf\u5c11\u300142.3%\u7684\u63a8\u7406\u65f6\u95f4\u964d\u4f4e\u548c32.8%\u7684\u5185\u5b58\u8282\u7701\uff0c\u4e14\u5bfc\u822a\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DynaNav\u901a\u8fc7\u9ad8\u6548\u7684\u7279\u5f81\u9009\u62e9\u548c\u65e9\u9000\u673a\u5236\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22014", "pdf": "https://arxiv.org/pdf/2509.22014", "abs": "https://arxiv.org/abs/2509.22014", "authors": ["Saurav Jha", "Stefan K. Ehrlich"], "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.RO"], "comment": "11 pages, 3 figures", "summary": "Healthcare robotics requires robust multimodal perception and reasoning to\nensure safety in dynamic clinical environments. Current Vision-Language Models\n(VLMs) demonstrate strong general-purpose capabilities but remain limited in\ntemporal reasoning, uncertainty estimation, and structured outputs needed for\nrobotic planning. We present a lightweight agentic multimodal framework for\nvideo-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model\nwith a SmolAgent-based orchestration layer, it supports chain-of-thought\nreasoning, speech-vision fusion, and dynamic tool invocation. The framework\ngenerates structured scene graphs and leverages a hybrid retrieval module for\ninterpretable and adaptive reasoning. Evaluations on the Video-MME benchmark\nand a custom clinical dataset show competitive accuracy and improved robustness\ncompared to state-of-the-art VLMs, demonstrating its potential for applications\nin robot-assisted surgery, patient monitoring, and decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u573a\u666f\u7406\u89e3\uff0c\u7ed3\u5408\u4e86Qwen2.5-VL-3B-Instruct\u6a21\u578b\u548cSmolAgent\u534f\u8c03\u5c42\uff0c\u5728\u533b\u7597\u673a\u5668\u4eba\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u533b\u7597\u673a\u5668\u4eba\u9700\u8981\u5f3a\u5927\u7684\u591a\u6a21\u6001\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u4ee5\u786e\u4fdd\u5728\u52a8\u6001\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u800c\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7b49\u65b9\u9762\u4ecd\u6709\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e86Qwen2.5-VL-3B-Instruct\u6a21\u578b\u548cSmolAgent\u534f\u8c03\u5c42\uff0c\u652f\u6301\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u8bed\u97f3\u89c6\u89c9\u878d\u5408\u548c\u52a8\u6001\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5728Video-MME\u57fa\u51c6\u548c\u81ea\u5b9a\u4e49\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u3001\u60a3\u8005\u76d1\u62a4\u548c\u51b3\u7b56\u652f\u6301\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.22137", "pdf": "https://arxiv.org/pdf/2509.22137", "abs": "https://arxiv.org/abs/2509.22137", "authors": ["Seoyoung Lee", "Seonbin Yoon", "Seongbeen Lee", "Hyesoo Kim", "Joo Yong Sim"], "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach", "categories": ["cs.AI", "cs.HC", "cs.MA", "cs.RO", "68N19, 68T09", "H.5.2; D.2.2"], "comment": null, "summary": "GUI task automation streamlines repetitive tasks, but existing LLM or\nVLM-based planner-executor agents suffer from brittle generalization, high\nlatency, and limited long-horizon coherence. Their reliance on single-shot\nreasoning or static plans makes them fragile under UI changes or complex tasks.\nLog2Plan addresses these limitations by combining a structured two-level\nplanning framework with a task mining approach over user behavior logs,\nenabling robust and adaptable GUI automation. Log2Plan constructs high-level\nplans by mapping user commands to a structured task dictionary, enabling\nconsistent and generalizable automation. To support personalization and reuse,\nit employs a task mining approach from user behavior logs that identifies\nuser-specific patterns. These high-level plans are then grounded into low-level\naction sequences by interpreting real-time GUI context, ensuring robust\nexecution across varying interfaces. We evaluated Log2Plan on 200 real-world\ntasks, demonstrating significant improvements in task success rate and\nexecution time. Notably, it maintains over 60.0% success rate even on\nlong-horizon task sequences, highlighting its robustness in complex, multi-step\nworkflows.", "AI": {"tldr": "Log2Plan\u901a\u8fc7\u7ed3\u5408\u4e24\u7ea7\u89c4\u5212\u6846\u67b6\u548c\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u7684\u4efb\u52a1\u6316\u6398\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GUI\u81ea\u52a8\u5316\u5de5\u5177\u7684\u901a\u7528\u6027\u3001\u5ef6\u8fdf\u548c\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u6216VLM\u7684\u89c4\u5212\u6267\u884c\u4ee3\u7406\u5728GUI\u4efb\u52a1\u81ea\u52a8\u5316\u4e2d\u5b58\u5728\u901a\u7528\u6027\u5dee\u3001\u5ef6\u8fdf\u9ad8\u548c\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Log2Plan\u91c7\u7528\u4e24\u7ea7\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5b57\u5178\u6620\u5c04\u7528\u6237\u547d\u4ee4\u751f\u6210\u9ad8\u7ea7\u8ba1\u5212\uff0c\u5e76\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u6316\u6398\u4e2a\u6027\u5316\u6a21\u5f0f\uff0c\u6700\u540e\u7ed3\u5408\u5b9e\u65f6GUI\u4e0a\u4e0b\u6587\u751f\u6210\u4f4e\u7ea7\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728200\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLog2Plan\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\uff0c\u5c24\u5176\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u5e8f\u5217\u4e2d\u4fdd\u6301\u4e8660%\u4ee5\u4e0a\u7684\u6210\u529f\u7387\u3002", "conclusion": "Log2Plan\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u4efb\u52a1\u6316\u6398\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u81ea\u52a8\u5316\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u3002"}}
{"id": "2509.22271", "pdf": "https://arxiv.org/pdf/2509.22271", "abs": "https://arxiv.org/abs/2509.22271", "authors": ["Felix Glawe", "Tim Schmeckel", "Philipp Brauner", "Martina Ziefle"], "title": "Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Human autonomy and sense of agency are increasingly recognised as critical\nfor user well-being, motivation, and the ethical deployment of robots in\nhuman-robot interaction (HRI). Given the rapid development of artificial\nintelligence, robot capabilities and their potential to function as colleagues\nand companions are growing. This systematic literature review synthesises 22\nempirical studies selected from an initial pool of 728 articles published\nbetween 2011 and 2024. Articles were retrieved from major scientific databases\nand identified based on empirical focus and conceptual relevance, namely, how\nto preserve and promote human autonomy and sense of agency in HRI. Derived\nthrough thematic synthesis, five clusters of potentially influential factors\nare revealed: robot adaptiveness, communication style, anthropomorphism,\npresence of a robot and individual differences. Measured through psychometric\nscales or the intentional binding paradigm, perceptions of autonomy and agency\nvaried across industrial, educational, healthcare, care, and hospitality\nsettings. The review underscores the theoretical differences between both\nconcepts, but their yet entangled use in HRI. Despite increasing interest, the\ncurrent body of empirical evidence remains limited and fragmented, underscoring\nthe necessity for standardised definitions, more robust operationalisations,\nand further exploratory and qualitative research. By identifying existing gaps\nand highlighting emerging trends, this review contributes to the development of\nhuman-centered, autonomy-supportive robot design strategies that uphold ethical\nand psychological principles, ultimately supporting well-being in human-robot\ninteraction.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u5206\u6790\u4e8622\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u7684\u4fdd\u62a4\u4e0e\u4fc3\u8fdb\u3002\u7814\u7a76\u53d1\u73b0\u4e94\u7c7b\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5b58\u5728\u5b9a\u4e49\u548c\u64cd\u4f5c\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4f26\u7406\u90e8\u7f72\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u9700\u6df1\u5165\u7814\u7a76\u4ee5\u652f\u6301\u8bbe\u8ba1\u548c\u4f26\u7406\u539f\u5219\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u7b5b\u9009728\u7bc7\u6587\u7ae0\u4e2d\u768422\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff0c\u63a2\u8ba8\u5f71\u54cd\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u7684\u56e0\u7d20\u3002", "result": "\u8bc6\u522b\u51fa\u673a\u5668\u4eba\u9002\u5e94\u6027\u3001\u6c9f\u901a\u65b9\u5f0f\u3001\u62df\u4eba\u5316\u3001\u673a\u5668\u4eba\u5b58\u5728\u53ca\u4e2a\u4f53\u5dee\u5f02\u4e94\u5927\u5f71\u54cd\u56e0\u7d20\uff0c\u6307\u51fa\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u7684\u5c40\u9650\u4e0e\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u6807\u51c6\u5316\u5b9a\u4e49\uff0c\u52a0\u5f3a\u63a2\u7d22\u6027\u548c\u5b9a\u6027\u7814\u7a76\uff0c\u4ee5\u5f00\u53d1\u652f\u6301\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u7684\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u7b56\u7565\u3002"}}
{"id": "2509.22281", "pdf": "https://arxiv.org/pdf/2509.22281", "abs": "https://arxiv.org/abs/2509.22281", "authors": ["Jinkun Hao", "Naifu Liang", "Zhen Luo", "Xudong Xu", "Weipeng Zhong", "Ran Yi", "Yichen Jin", "Zhaoyang Lyu", "Feng Zheng", "Lizhuang Ma", "Jiangmiao Pang"], "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by NeurIPS 2025; Project page: https://mesatask.github.io/", "summary": "The ability of robots to interpret human instructions and execute\nmanipulation tasks necessitates the availability of task-relevant tabletop\nscenes for training. However, traditional methods for creating these scenes\nrely on time-consuming manual layout design or purely randomized layouts, which\nare limited in terms of plausibility or alignment with the tasks. In this\npaper, we formulate a novel task, namely task-oriented tabletop scene\ngeneration, which poses significant challenges due to the substantial gap\nbetween high-level task instructions and the tabletop scenes. To support\nresearch on such a challenging task, we introduce MesaTask-10K, a large-scale\ndataset comprising approximately 10,700 synthetic tabletop scenes with manually\ncrafted layouts that ensure realistic layouts and intricate inter-object\nrelations. To bridge the gap between tasks and scenes, we propose a Spatial\nReasoning Chain that decomposes the generation process into object inference,\nspatial interrelation reasoning, and scene graph construction for the final 3D\nlayout. We present MesaTask, an LLM-based framework that utilizes this\nreasoning chain and is further enhanced with DPO algorithms to generate\nphysically plausible tabletop scenes that align well with given task\ndescriptions. Exhaustive experiments demonstrate the superior performance of\nMesaTask compared to baselines in generating task-conforming tabletop scenes\nwith realistic layouts. Project page is at https://mesatask.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u4efb\u52a1\u5bfc\u5411\u7684\u684c\u9762\u573a\u666f\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86MesaTask-10K\u6570\u636e\u96c6\u548cSpatial Reasoning Chain\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u751f\u6210\u8fc7\u7a0b\u63d0\u5347\u573a\u666f\u4e0e\u4efb\u52a1\u7684\u5951\u5408\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210\u4efb\u52a1\u76f8\u5173\u684c\u9762\u573a\u666f\u65f6\u7684\u4e0d\u8db3\uff0c\u5982\u8017\u65f6\u6216\u968f\u673a\u6027\u5bfc\u81f4\u7684\u4e0d\u5408\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51faSpatial Reasoning Chain\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5bf9\u8c61\u63a8\u65ad\u3001\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u548c\u573a\u666f\u56fe\u6784\u5efa\uff0c\u5e76\u57fa\u4e8eLLM\u6846\u67b6\u548cDPO\u7b97\u6cd5\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMesaTask\u5728\u751f\u6210\u7b26\u5408\u4efb\u52a1\u9700\u6c42\u4e14\u5e03\u5c40\u5408\u7406\u7684\u684c\u9762\u573a\u666f\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MesaTask\u901a\u8fc7\u65b0\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4efb\u52a1\u5bfc\u5411\u7684\u684c\u9762\u573a\u666f\u751f\u6210\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.22298", "pdf": "https://arxiv.org/pdf/2509.22298", "abs": "https://arxiv.org/abs/2509.22298", "authors": ["Felix Glawe", "Laura Kremer", "Luisa Vervier", "Philipp Brauner", "Martina Ziefle"], "title": "Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Collaborative robots (cobots) are a core technology of Industry 4.0. Industry\n4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency\nand data-driven decision-making. Cobots, as cyber-physical systems, enable the\nintroduction of lightweight automation to smaller companies through their\nflexibility, low cost and ability to work alongside humans, while keeping\nhumans and their skills in the loop. Industry 5.0, the evolution of Industry\n4.0, places the worker at the centre of its principles: The physical and mental\nwell-being of the worker is the main goal of new technology design, not just\nproductivity, efficiency and safety standards. Within this concept, human trust\nin cobots and human autonomy are important. While trust is essential for\neffective and smooth interaction, the workers' perception of autonomy is key to\nintrinsic motivation and overall well-being. As failures are an inevitable part\nof technological systems, this study aims to answer the question of how system\nfailures affect trust in cobots as well as human autonomy, and how they can be\nrecovered afterwards. Therefore, a VR experiment (n = 39) was set up to\ninvestigate the influence of a cobot failure and its severity on human autonomy\nand trust in the cobot. Furthermore, the influence of transparent communication\nabout the failure and next steps was investigated. The results show that both\ntrust and autonomy suffer after cobot failures, with the severity of the\nfailure having a stronger negative impact on trust, but not on autonomy. Both\ntrust and autonomy can be partially restored by transparent communication.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u534f\u4f5c\u673a\u5668\u4eba\uff08cobot\uff09\u6545\u969c\u5bf9\u5de5\u4eba\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u900f\u660e\u6c9f\u901a\u7684\u6062\u590d\u4f5c\u7528\u3002", "motivation": "\u5de5\u4e1a5.0\u5f3a\u8c03\u5de5\u4eba\u798f\u7949\uff0c\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u662f\u5173\u952e\u3002\u7814\u7a76\u65e8\u5728\u5206\u6790\u6545\u969c\u5bf9\u8fd9\u4e24\u8005\u7684\u5f71\u54cd\u53ca\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7VR\u5b9e\u9a8c\uff08n=39\uff09\u7814\u7a76\u6545\u969c\u4e25\u91cd\u6027\u548c\u900f\u660e\u6c9f\u901a\u7684\u5f71\u54cd\u3002", "result": "\u6545\u969c\u964d\u4f4e\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\uff0c\u5176\u4e2d\u4fe1\u4efb\u53d7\u6545\u969c\u4e25\u91cd\u6027\u66f4\u5927\u5f71\u54cd\uff1b\u900f\u660e\u6c9f\u901a\u53ef\u90e8\u5206\u6062\u590d\u4e24\u8005\u3002", "conclusion": "\u900f\u660e\u6c9f\u901a\u662f\u6062\u590d\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u7684\u6709\u6548\u624b\u6bb5\uff0c\u5bf9\u5de5\u4e1a5.0\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.22379", "pdf": "https://arxiv.org/pdf/2509.22379", "abs": "https://arxiv.org/abs/2509.22379", "authors": ["Stefano Carlo Lambertenghi", "Mirena Flores Valdez", "Andrea Stocco"], "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems", "categories": ["cs.SE", "cs.RO"], "comment": "In proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE '25)", "summary": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)\ndevelopment, offering safe and scalable evaluation across diverse driving\nscenarios. However, discrepancies between simulated and real-world behavior,\nknown as the reality gap, challenge the transferability of test results to\ndeployed systems. In this paper, we present a comprehensive empirical study\ncomparing four representative testing modalities: Software-in-the-Loop (SiL),\nVehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.\nUsing a small-scale physical vehicle equipped with real sensors (camera and\nLiDAR) and its digital twin, we implement each setup and evaluate two ADS\narchitectures (modular and end-to-end) across diverse indoor driving scenarios\ninvolving real obstacles, road topologies, and indoor environments. We\nsystematically assess the impact of each testing modality along three\ndimensions of the reality gap: actuation, perception, and behavioral fidelity.\nOur results show that while SiL and ViL setups simplify critical aspects of\nreal-world dynamics and sensing, MR testing improves perceptual realism without\ncompromising safety or control. Importantly, we identify the conditions under\nwhich failures do not transfer across testing modalities and isolate the\nunderlying dimensions of the gap responsible for these discrepancies. Our\nfindings offer actionable insights into the respective strengths and\nlimitations of each modality and outline a path toward more robust and\ntransferable validation of autonomous driving systems.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u5f0f\u7684\u73b0\u5b9e\u5dee\u8ddd\uff0c\u91cd\u70b9\u5206\u6790\u4e86SiL\u3001ViL\u3001MR\u548c\u771f\u5b9e\u6d4b\u8bd5\u7684\u4f18\u7f3a\u70b9\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u6d4b\u8bd5\u7a33\u5065\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u7684\u5efa\u8bae\u3002", "motivation": "\u63ed\u793a\u4e0d\u540c\u6d4b\u8bd5\u65b9\u5f0f\u5728\u73b0\u5b9e\u5dee\u8ddd\uff08actuation\u3001perception\u3001behavioral fidelity\uff09\u4e0a\u7684\u5dee\u5f02\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9a8c\u8bc1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u914d\u5907\u771f\u5b9e\u4f20\u611f\u5668\u7684\u5c0f\u578b\u7269\u7406\u8f66\u8f86\u53ca\u5176\u6570\u5b57\u5b6a\u751f\uff0c\u5728\u5ba4\u5185\u9a7e\u9a76\u573a\u666f\u4e2d\u8bc4\u4f30SiL\u3001ViL\u3001MR\u548c\u771f\u5b9e\u6d4b\u8bd5\u56db\u79cd\u65b9\u5f0f\u5bf9\u4e24\u79cdADS\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "SiL\u548cViL\u7b80\u5316\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u548c\u611f\u77e5\uff0cMR\u63d0\u5347\u4e86\u611f\u77e5\u771f\u5b9e\u611f\u4e14\u4e0d\u5f71\u54cd\u5b89\u5168\u4e0e\u63a7\u5236\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0d\u8de8\u6d4b\u8bd5\u65b9\u5f0f\u4f20\u9012\u7684\u6545\u969c\u6761\u4ef6\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6bcf\u79cd\u6d4b\u8bd5\u65b9\u5f0f\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u672a\u6765\u66f4\u7a33\u5065\u548c\u53ef\u8fc1\u79fb\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.22402", "pdf": "https://arxiv.org/pdf/2509.22402", "abs": "https://arxiv.org/abs/2509.22402", "authors": ["Nan Tang", "Jing-Cheng Pang", "Guanlin Li", "Chao Qian", "Yang Yu"], "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Reward design remains a critical bottleneck in visual reinforcement learning\n(RL) for robotic manipulation. In simulated environments, rewards are\nconventionally designed based on the distance to a target position. However,\nsuch precise positional information is often unavailable in real-world visual\nsettings due to sensory and perceptual limitations. In this study, we propose a\nmethod that implicitly infers spatial distances through keypoints extracted\nfrom images. Building on this, we introduce Reward Learning with Anticipation\nModel (ReLAM), a novel framework that automatically generates dense, structured\nrewards from action-free video demonstrations. ReLAM first learns an\nanticipation model that serves as a planner and proposes intermediate\nkeypoint-based subgoals on the optimal path to the final goal, creating a\nstructured learning curriculum directly aligned with the task's geometric\nobjectives. Based on the anticipated subgoals, a continuous reward signal is\nprovided to train a low-level, goal-conditioned policy under the hierarchical\nreinforcement learning (HRL) framework with provable sub-optimality bound.\nExtensive experiments on complex, long-horizon manipulation tasks show that\nReLAM significantly accelerates learning and achieves superior performance\ncompared to state-of-the-art methods.", "AI": {"tldr": "Reward design is challenging in visual RL for robotic manipulation, especially in real-world settings where positional data is often unavailable. The proposed ReLAM framework uses keypoints and an anticipation model to generate dense rewards from action-free video demonstrations, improving learning efficiency and performance in complex tasks.", "motivation": "Addressing the bottleneck of reward design in visual RL for robotic manipulation, particularly in real-world settings where precise positional information is scarce.", "method": "ReLAM learns an anticipation model to propose keypoint-based subgoals, creating a structured curriculum, and provides continuous rewards for training a goal-conditioned policy under HRL.", "result": "ReLAM accelerates learning and outperforms state-of-the-art methods in complex, long-horizon manipulation tasks.", "conclusion": "ReLAM offers a novel, effective approach to reward design in visual RL, demonstrating significant improvements in robotic manipulation tasks."}}
{"id": "2509.22407", "pdf": "https://arxiv.org/pdf/2509.22407", "abs": "https://arxiv.org/abs/2509.22407", "authors": ["Zhehao Dong", "Xiaofeng Wang", "Zheng Zhu", "Yirui Wang", "Yang Wang", "Yukun Zhou", "Boyuan Wang", "Chaojun Ni", "Runqi Ouyang", "Wenkang Qin", "Xinze Chen", "Yun Ye", "Guan Huang"], "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models increasingly rely on diverse training\ndata to achieve robust generalization. However, collecting large-scale\nreal-world robot manipulation data across varied object appearances and\nenvironmental conditions remains prohibitively time-consuming and expensive. To\novercome this bottleneck, we propose Embodied Manipulation Media Adaptation\n(EMMA), a VLA policy enhancement framework that integrates a generative data\nengine with an effective training pipeline. We introduce DreamTransfer, a\ndiffusion Transformer-based framework for generating multi-view consistent,\ngeometrically grounded embodied manipulation videos. DreamTransfer enables\ntext-controlled visual editing of robot videos, transforming foreground,\nbackground, and lighting conditions without compromising 3D structure or\ngeometrical plausibility. Furthermore, we explore hybrid training with real and\ngenerated data, and introduce AdaMix, a hard-sample-aware training strategy\nthat dynamically reweights training batches to focus optimization on\nperceptually or kinematically challenging samples. Extensive experiments show\nthat videos generated by DreamTransfer significantly outperform prior video\ngeneration methods in multi-view consistency, geometric fidelity, and\ntext-conditioning accuracy. Crucially, VLAs trained with generated data enable\nrobots to generalize to unseen object categories and novel visual domains using\nonly demonstrations from a single appearance. In real-world robotic\nmanipulation tasks with zero-shot visual domains, our approach achieves over a\n200% relative performance gain compared to training on real data alone, and\nfurther improves by 13% with AdaMix, demonstrating its effectiveness in\nboosting policy generalization.", "AI": {"tldr": "EMMA\u6846\u67b6\u901a\u8fc7DreamTransfer\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u5e76\u7ed3\u5408AdaMix\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u6570\u636e\u589e\u5f3aVLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faEMMA\u6846\u67b6\uff0c\u5305\u62ecDreamTransfer\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u89c6\u9891\u548cAdaMix\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6743\u91cd\u3002", "result": "\u751f\u6210\u7684\u89c6\u9891\u5728\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u673a\u5668\u4eba\u6cdb\u5316\u80fd\u529b\u63d0\u5347200%\u3002", "conclusion": "EMMA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.22442", "pdf": "https://arxiv.org/pdf/2509.22442", "abs": "https://arxiv.org/abs/2509.22442", "authors": ["Pei Xu", "Zhen Wu", "Ruocheng Wang", "Vishnu Sarukkai", "Kayvon Fatahalian", "Ioannis Karamouzas", "Victor Zordan", "C. Karen Liu"], "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves", "categories": ["cs.GR", "cs.AI", "cs.LG", "cs.RO"], "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025).\n  Website: http://pei-xu.github.io/basketball. Video:\n  https://youtu.be/2RBFIjjmR2I. Code: https://github.com/xupei0610/basketball", "summary": "Learning a control policy for a multi-phase, long-horizon task, such as\nbasketball maneuvers, remains challenging for reinforcement learning approaches\ndue to the need for seamless policy composition and transitions between skills.\nA long-horizon task typically consists of distinct subtasks with well-defined\ngoals, separated by transitional subtasks with unclear goals but critical to\nthe success of the entire task. Existing methods like the mixture of experts\nand skill chaining struggle with tasks where individual policies do not share\nsignificant commonly explored states or lack well-defined initial and terminal\nstates between different phases. In this paper, we introduce a novel policy\nintegration framework to enable the composition of drastically different motor\nskills in multi-phase long-horizon tasks with ill-defined intermediate states.\nBased on that, we further introduce a high-level soft router to enable seamless\nand robust transitions between the subtasks. We evaluate our framework on a set\nof fundamental basketball skills and challenging transitions. Policies trained\nby our approach can effectively control the simulated character to interact\nwith the ball and accomplish the long-horizon task specified by real-time user\ncommands, without relying on ball trajectory references.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b56\u7565\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u9636\u6bb5\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u65e0\u7f1d\u7ec4\u5408\u5dee\u5f02\u5927\u7684\u8fd0\u52a8\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u9ad8\u7ea7\u8f6f\u8def\u7531\u5668\u5b9e\u73b0\u5b50\u4efb\u52a1\u95f4\u7684\u7a33\u5065\u8fc7\u6e21\u3002", "motivation": "\u89e3\u51b3\u591a\u9636\u6bb5\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7b56\u7565\u7ec4\u5408\u4e0e\u8fc7\u6e21\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u4e0d\u660e\u786e\u7684\u8fc7\u6e21\u5b50\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u7b56\u7565\u96c6\u6210\u6846\u67b6\u548c\u9ad8\u5c42\u6b21\u8f6f\u8def\u7531\u5668\uff0c\u4ee5\u5b9e\u73b0\u4e0d\u540c\u8fd0\u52a8\u6280\u80fd\u7684\u7ec4\u5408\u548c\u5b50\u4efb\u52a1\u95f4\u7684\u65e0\u7f1d\u8fc7\u6e21\u3002", "result": "\u5728\u7bee\u7403\u6280\u80fd\u548c\u590d\u6742\u8fc7\u6e21\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u65e0\u9700\u4f9d\u8d56\u7403\u7684\u8f68\u8ff9\u53c2\u8003\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u9636\u6bb5\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u7ec4\u5408\u6280\u80fd\u5e76\u5b9e\u73b0\u7a33\u5065\u8fc7\u6e21\u3002"}}
{"id": "2509.22548", "pdf": "https://arxiv.org/pdf/2509.22548", "abs": "https://arxiv.org/abs/2509.22548", "authors": ["Shuang Zeng", "Dekang Qi", "Xinyuan Chang", "Feng Xiong", "Shichao Xie", "Xiaolong Wu", "Shiyi Liang", "Mu Xu", "Xing Wei"], "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/", "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86JanusVLN\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9690\u5f0f\u795e\u7ecf\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u51e0\u4f55\u548c\u89c6\u89c9\u8bed\u4e49\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLN\u65b9\u6cd5\u4e2d\u7684\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u548c\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLN\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u663e\u5f0f\u8bed\u4e49\u8bb0\u5fc6\uff0c\u5bfc\u81f4\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u3001\u8ba1\u7b97\u5197\u4f59\u548c\u5185\u5b58\u81a8\u80c0\u3002\u53d7\u4eba\u7c7b\u5bfc\u822a\u4e2d\u9690\u5f0f\u573a\u666f\u8868\u5f81\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u53cc\u9690\u5f0f\u795e\u7ecf\u8bb0\u5fc6\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "JanusVLN\u901a\u8fc7\u7a7a\u95f4\u51e0\u4f55\u7f16\u7801\u5668\u589e\u5f3a\u4e86MLLM\u76843D\u5148\u9a8c\u77e5\u8bc6\uff0c\u6784\u5efa\u4e86\u7a7a\u95f4\u51e0\u4f55\u548c\u89c6\u89c9\u8bed\u4e49\u7684\u53cc\u9690\u5f0f\u8bb0\u5fc6\uff0c\u4ec5\u4fdd\u7559\u521d\u59cb\u548c\u6ed1\u52a8\u7a97\u53e3\u7684KV\u7f13\u5b58\u4ee5\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJanusVLN\u4f18\u4e8e20\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534735.5%\uff0c\u4e3aVLN\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u53cc\u9690\u5f0f\u795e\u7ecf\u8bb0\u5fc6\u4f5c\u4e3a\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86VLN\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
