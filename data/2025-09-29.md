<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [cs.RO](#cs.RO) [Total: 47]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Hidden Markov Model Decoding for LDPC Codes](https://arxiv.org/abs/2509.21872)
*Jan C Olivier,Etienne Barnard*

Main category: eess.SP

TL;DR: 该论文提出了一种用于解码LDPC码的迭代隐马尔可夫模型（HMM），通过前向后向平滑估计器高效解码，显著提升解码性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用HMM框架改善LDPC码的解码效率，尤其针对短帧长度和规则LDPC码的情况。

Method: 采用一阶隐马尔可夫模型，结合固定转移矩阵和随机游走，通过前向后向平滑估计器实现高效解码。

Result: 相比传统信念传播方法，新方法显著提升了LDPC码的解码阈值，性能接近Polar码的SCL-CRC解码。

Conclusion: 该方法为LDPC码解码提供了一种高效且性能优越的新思路，尤其适用于短帧长度场景。

Abstract: The paper proposes an iterative Hidden Markov Model (HMM) for decoding a Low
Density Parity Check (LDPC) code. It is demonstrated that a first-order HMM
provides a natural framework for the decoder. The HMM is time-homogeneous with
a fixed transition matrix and is based on a random walk through the encoded
frame bits. Each hidden state contains a pair of two encoded bits, and parity
checks are naturally incorporated into the observation model. The paper shows
that by implementing a forward-backward smoothing estimator for the hidden
states, decoding is efficient and requires only a small number of iterations in
most cases. The results show that the LDPC decoding threshold is significantly
improved compared to belief propagation (BP) on a Tanner graph. Numerical
results are presented showing that LDPC codes under the proposed decoder yield
a frame error rate (FER) and decoding threshold comparable to that of a Polar
code where Successive Cancellation List (SCL) - Cyclic Redundancy Check (CRC)
decoding is deployed. This is shown to be achieved even if the frame length is
short (on the order of $512$ bits or less) and a regular LDPC code is used. 1

</details>


### [2] [CRB minimization for PASS Assisted ISAC](https://arxiv.org/abs/2509.22181)
*Haochen Li,Ruikang Zhong,Jiayi Lei,Pan Zhiwen,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种多波导PASS辅助的集成感知与通信系统，通过交替优化方法解决非凸优化问题，性能优于基准方案。


<details>
  <summary>Details</summary>
Motivation: 设计一个集成感知与通信系统，以高效利用硬件资源并满足通信和服务质量要求。

Method: 采用PASS发射天线和ULA接收天线配置，通过波导传输信号，并利用交替优化方法最小化目标感知的CRB。

Result: 仿真结果表明，所提出的系统在性能上优于基准方案。

Conclusion: PASS辅助的ISAC框架提供了高效的集成感知与通信解决方案。

Abstract: A multiple waveguide PASS assisted integrated sensing and communication
(ISAC) system is proposed, where the base station (BS) is equipped with
transmitting pinching antennas (PAs) and receiving uniform linear array (ULA)
antennas. The PASS-transmitting-ULA-receiving (PTUR) BS transmits the
communication and sensing signals through the stretched PAs on waveguides and
collects the echo sensing signals with the mounted ULA. Based on this
configuration, a target sensing Cramer Rao Bound (CRB) minimization problem is
formulated under communication quality-of-service (QoS) constraints, power
budget constraints, and PA deployment constraints. An alternating optimization
(AO) method is employed to address the formulated non-convex optimization
problem. Simulation results demonstrate that the proposed PASS assisted ISAC
framework achieves superior performance over benchmark schemes.

</details>


### [3] [A Deep Neural Network Codebook Approach for Near-Field Nulling Control Beam Focusing](https://arxiv.org/abs/2509.22204)
*Mohammadhossein Karimi,Yuanzhe Gong,Tho Le-Ngoc*

Main category: eess.SP

TL;DR: 提出了一种基于DNN的码本方法，用于XL-MIMO系统中的多用户干扰抑制，通过分区训练轻量级模型，降低复杂度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有DNN方法在XL-MIMO系统中面临可扩展性和复杂度挑战，需要更高效的干扰抑制方案。

Method: 利用相关性采样分割菲涅尔区，为每个子区域训练轻量DNN模型，基于LCMV生成的波束赋形权重进行预测。

Result: 模型预测误差低，干扰抑制优于31.64 dB，性能接近LCMV方法但复杂度更低。

Conclusion: 该方法在降低计算复杂度的同时，有效抑制了多用户干扰。

Abstract: This paper proposes a deep neural network (DNN) codebook approach for
multi-user interference (MUI) mitigation in extremely large multiple-input
multiple-output (XL-MIMO) systems operating in the near-field region. Unlike
existing DNN-based nulling control beamforming (NCBF) methods that face
scalability and complexity challenges, the proposed framework partitions the
Fresnel region using correlation-based sampling and assigns a lightweight fully
connected DNN model to each subsection. Each model is trained on beamforming
weights generated using the linearly constrained minimum variance (LCMV)
method, enabling accurate prediction of nulling control beam-focusing weights
that simultaneously optimize the desired signal strength and suppress potential
interference for both collinear and non-collinear user configurations.
Simulation results show that the trained models achieve average phase and
magnitude prediction errors of 0.085 radians and 0.52 dB, respectively, across
75 sample subsections. Full-wave simulations in Ansys HFSS further demonstrate
that the proposed DNN codebook achieves interference suppression better than
31.64 dB, with a performance gap within 2 dB of the LCMV method, thereby
validating its effectiveness in mitigating MUI while reducing computational
complexity.

</details>


### [4] [Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM Communications](https://arxiv.org/abs/2509.22327)
*Zheao Li,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 该论文提出了一种基于OFDM-IM的SIM增强宽带多用户收发器，通过稀疏激活和深度展开优化框架解决了宽带部署中的静态相位张量和快速重配置问题。


<details>
  <summary>Details</summary>
Motivation: 针对可编程超表面的多层实现（SIM）在宽带部署中面临的静态相位张量适应性和多用户调度快速重配置的挑战。

Method: 开发了一种基于OFDM-IM的收发器，通过稀疏激活限制高保真均衡，并采用max-min SINR作为可靠性优化的替代指标，提出了一种深度展开的UPGD-Net。

Result: 仿真显示快速单调收敛、明显的层深优化点以及最差链路BER和总速率的持续提升。

Conclusion: 该框架通过结合结构稀疏性和BER驱动的深度展开优化，直接解决了SIM在宽带部署中的关键缺陷。

Abstract: Leveraging the multilayer realization of programmable metasurfaces, stacked
intelligent metasurfaces (SIM) enable fine-grained wave-domain control.
However, their wideband deployment is impeded by two structural factors: (i) a
single, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)
multiuser scheduling changes the subcarrier activation pattern frame by frame,
requiring rapid reconfiguration. To address both challenges, we develop a
SIM-enhanced wideband multiuser transceiver built on orthogonal
frequency-division multiplexing with index modulation (OFDM-IM). The sparse
activation of OFDM-IM confines high-fidelity equalization to the active tones,
effectively widening the usable bandwidth. To make the design
reliability-aware, we directly target the worst-link bit-error rate (BER) and
adopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a
principled surrogate, turning the reliability optimization tractable. For
frame-rate inference and interpretability, we propose an unfolded
projected-gradient-descent network (UPGD-Net) that double-unrolls across the
SIM's layers and algorithmic iterations: each cell computes the analytic
gradient from the cascaded precoder with a learnable per-iteration step size.
Simulations on wideband multiuser downlinks show fast, monotone convergence, an
evident layer-depth sweet spot, and consistent gains in worst-link BER and sum
rate. By combining structural sparsity with a BER-driven, deep-unfolded
optimization backbone, the proposed framework directly addresses the key
wideband deficiencies of SIM.

</details>


### [5] [Specific multi-emitter identification via multi-label learning](https://arxiv.org/abs/2509.22396)
*Yuhao Chen,Boxiang He,Shilian Wang,Jing Lei*

Main category: eess.SP

TL;DR: 提出了一个多标签学习的多发射器识别方法（SMEI），通过设计指纹提取器和决策器，有效识别重叠信号中的多个发射器，且复杂度较低。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理多个发射器信号重叠的情况，需要一种能够同时识别多个发射器的低复杂度方法。

Method: 设计了多发射器指纹提取器和决策器，前者减少信号间干扰，后者基于提取的指纹识别所有发射器。

Result: 在不同重叠条件下，SMEI方法达到与基线相当的识别准确率，同时复杂度显著降低。

Conclusion: SMEI方法能够以低复杂度有效识别重叠信号中的多个发射器。

Abstract: Specific emitter identification leverages hardware-induced impairments to
uniquely determine a specific transmitter. However, existing approaches fail to
address scenarios where signals from multiple emitters overlap. In this paper,
we propose a specific multi-emitter identification (SMEI) method via
multi-label learning to determine multiple transmitters. Specifically, the
multi-emitter fingerprint extractor is designed to mitigate the mutual
interference among overlapping signals. Then, the multi-emitter decision maker
is proposed to assign the all emitter identification using the previous
extracted fingerprint. Experimental results demonstrate that, compared with
baseline approach, the proposed SMEI scheme achieves comparable identification
accuracy under various overlapping conditions, while operating at significantly
lower complexity. The significance of this paper is to identify multiple
emitters from overlapped signal with a low complexity.

</details>


### [6] [Approximation of the Range Ambiguity Function in Near-field Sensing Systems](https://arxiv.org/abs/2509.22423)
*Marcin Wachowiak,André Bourdoux,Sofie Pollin*

Main category: eess.SP

TL;DR: 研究了近场系统中范围和带宽联合决定的模糊函数，提出了阵列因子和带宽波形的近似准则，并分析了近场波束聚焦的性能增益。


<details>
  <summary>Details</summary>
Motivation: 探讨近场系统中范围和带宽如何共同影响分辨率的模糊函数。

Method: 推导匹配滤波模糊函数，引入不同天线阵列几何的近场阵列因子，近似为范围和带宽波形的组合，提出孔径-带宽乘积准则。

Result: 近场波束聚焦在分辨率、峰值旁瓣比和积分旁瓣水平上有显著改善，但增益随距离变化且仅在近场显著。

Conclusion: 近场波束聚焦在近距离内能显著改善模糊函数性能，但增益随距离增加而减弱。

Abstract: This paper investigates the range ambiguity function of near-field systems
where bandwidth and near-field beamfocusing jointly determine the resolution.
First, the general matched filter ambiguity function is derived and the
near-field array factors of different antenna array geometries are introduced.
Next, the near-field ambiguity function is approximated as a product of the
range-dependent near-field array factor and the ambiguity function due to the
utilized bandwidth and waveform. An approximation criterion based on the
aperture-bandwidth product is formulated, and its accuracy is examined.
Finally, the improvements to the ambiguity function offered by the near-field
beamfocusing, as compared to the far-field case, are presented. The performance
gains are evaluated in terms of resolution improvement offered by beamfocusing,
peak-to-sidelobe and integrated-sidelobe level improvement. The gains offered
by the near-field regime are shown to be range-dependent and substantial only
in close proximity to the array.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [7] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: VISION系统结合视觉语言模型和自主路径规划，用于运河涵洞的自动检查，通过语言提示提出感兴趣区域，规划动作捕获高清图像，最终评估结果与专家一致率达80%。


<details>
  <summary>Details</summary>
Motivation: 人工检查运河涵洞存在诸多困难（如空间狭小、光照不足），需要自动化解决方案以提高效率和安全性。

Method: 开发了VISION系统，集成视觉语言模型（VLM）、立体深度估计和约束规划，语言提示生成感兴趣区域，规划动作捕获目标图像。

Result: 在实地测试中，初始提案与专家一致率为61.4%，最终提升至80%，系统可生成高清图像用于详细报告。

Conclusion: VISION展示了语言引导自动化检查的潜力，无需领域微调即可生成专家认可的结果。

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [8] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: 本文介绍了一种新型软刚结合的机器人平台'GeoGami'，利用折纸表面的特性实现形状收缩和运动。通过集成表面顺应性解决了折纸表面自由度多的问题，设计了单驱动系统。


<details>
  <summary>Details</summary>
Motivation: 解决折纸表面自由度多且需要多个驱动器的挑战，开发能够通过形状变化适应不同环境并用于运动的机器人。

Method: 结合折纸表面顺应性和几何顺应骨架，设计单驱动移动平台，开发刚度模型和齿轮箱机制，并分析电缆驱动方法。

Result: GeoGami平台展示了形状变换和滚动能力，为机器人提供了新的功能。

Conclusion: GeoGami平台为机器人提供了通过形状变化适应环境和用于运动的新功能，具有广泛的应用潜力。

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [9] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: 论文提出了一种针对四旋翼飞行器在近墙环境中稳定飞行的综合解决方案，包括基于物理的吸力模型和吸力补偿模型预测控制框架，显著提升了轨迹跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 近墙环境下复杂的气动效应会对四旋翼飞行器产生不稳定的吸力，导致危险振动或碰撞，因此需要通过建模和控制策略提升飞行安全性。

Method: 1. 建立基于物理的吸力模型，明确表征转速和距离对吸力的影响；2. 设计吸力补偿模型预测控制（SC-MPC），优化系统动态约束和轨迹跟踪目标。

Result: 实验验证显示，SC-MPC在X轴和Y轴的位置控制误差分别比PID控制提升了74%和79%，比标准MPC提升了60%和53%。

Conclusion: SC-MPC框架有效解决了近墙环境下的飞行稳定性问题，并开源了完整实现以促进社区采用。

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [10] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: DroneFL是一个专为多无人机目标跟踪设计的联邦学习框架，通过轻量级本地模型、位置不变架构和云端多无人机预测融合，显著提升了预测精度和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 多无人机目标跟踪在精准农业、环境监测等领域应用广泛，但现有方法存在计算资源有限、数据异质性高以及轨迹预测与规划紧密耦合等挑战。

Method: DroneFL采用轻量级本地模型（YOLO主干网络+浅层Transformer）进行目标轨迹预测，并通过位置不变架构和自适应实例归一化解决数据异质性，最后在云端融合预测并规划最优轨迹。

Result: 相比非联邦学习框架，DroneFL预测误差降低6%-83%，跟踪距离减少0.4%-4.6%，且在树莓派5上实时运行，云端数据率仅为1.56 KBps。

Conclusion: DroneFL为多无人机目标跟踪提供了一种高效、实时的联邦学习解决方案，显著提升了性能与效率。

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [11] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: Plan2Evolve是一个LLM自我进化框架，通过生成规划领域和问题-计划对，并转化为自然语言解释，以增强LLM的规划能力和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将规划领域作为搜索工具，忽视了其作为可扩展推理数据的潜力。同时，机器人领域的推理LLM依赖于成本高昂的人工标注数据。

Method: 提出Plan2Evolve框架，利用基础模型生成规划领域和问题-计划对，并通过自然语言解释扩展为链式思维轨迹，从而对齐符号规划与自然语言推理。

Result: 生成的数据扩展了模型的规划能力，使其在规划成功率、跨任务泛化和推理成本上均有显著提升。

Conclusion: Plan2Evolve通过自我生成数据的方法，显著增强了LLM的规划性能，并减少了对人工标注的依赖。

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [12] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2是一种结合IMU、车轮编码器和摄像头的视觉-惯性-车轮里程计系统，通过新颖的线特征处理和SE(2)-约束的SE(3)车轮预积分方法，提高了复杂环境下的状态估计精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉里程计在复杂城市环境中的性能下降是一个关键问题，因此需要一种整合多传感器的方案来提升鲁棒性和准确性。

Method: 提出PL-VIWO2系统，利用线特征框架、SE(2)-约束的SE(3)车轮预积分方法和运动一致性检查（MCC）来优化状态估计。

Result: 实验表明PL-VIWO2在精度、效率和鲁棒性上优于现有方法。

Conclusion: PL-VIWO2通过多传感器融合和创新算法，显著提升了复杂环境中的状态估计性能。

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [13] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: 提出了一种自主无人机-四足机器人对接框架，解决了GPS缺失环境下由于地形复杂和姿势变化带来的挑战，结合强化学习和约束感知控制器实现稳定对接。


<details>
  <summary>Details</summary>
Motivation: 异构系统中的无人机与地面机器人自主对接是关键，但现有方法多针对轮式平台，难以适应复杂地形；四足机器人灵活性高但姿势变化频繁，需要稳定着陆面。

Method: 四足侧通过深度强化学习训练的内置模型稳定躯干；无人机侧采用三阶段策略：远距离检测、近距离跟踪（结合NFTSMC和对数障碍函数）及终端下降。

Result: 在模拟和真实场景中验证了框架有效性，成功在17厘米高台阶和30度陡坡上实现对接。

Conclusion: 该框架为复杂地形下的异构机器人系统自主对接提供了可行方案，展示了鲁棒性和适应性。

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [14] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 该论文提出了一种利用大型语言模型（LLMs）获取物体几何属性先验知识的方法，以解决对象级SLAM中稀疏观测导致的优化不足问题。


<details>
  <summary>Details</summary>
Motivation: 对象级SLAM由于稀疏观测面临优化不足的挑战，传统方法依赖人工先验知识，缺乏通用性和效率。

Method: 作者利用LLMs生成物体几何属性（大小和方向）的先验知识，并将其整合到基于图的SLAM框架中。

Result: 在TUM RGB-D和3RScan数据集上，系统映射精度提升了36.8%，并实现实时性能。

Conclusion: 通过LLMs提供的先验知识，论文实现了更高效且通用的对象级SLAM系统。

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [15] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 提出一种将稳定性直接集成到扩散模型采样过程中的方法，用于机器人操作中多目标场景下的物体稳定放置。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖仿真引擎或启发式外观评估，难以高效且准确地评估放置稳定性。

Method: 利用离线采样规划器收集多模态放置标签，训练扩散模型生成稳定放置；结合几何感知先验和稳定性感知损失。

Result: 在四个基准场景中，相比最优几何方法，物理引导模型使放置抗扰动能力提高56%，运行时间减少47%。

Conclusion: 该方法无需额外训练，可直接应用于现成模型，显著提高了放置稳定性和效率。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [16] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: 该论文提出了一种强化学习框架，通过直接映射球的位置观察到全身关节指令，以解决人形机器人乒乓球任务中的快速感知和动作协调问题。


<details>
  <summary>Details</summary>
Motivation: 人形机器人乒乓球需要快速感知、主动全身运动和敏捷步法，传统统一控制器难以实现这些能力。

Method: 使用强化学习框架，结合预测信号和密集物理引导奖励，直接生成全身关节指令。轻量级学习预测器用于估计未来球的状态，辅助决策。

Result: 在模拟中，策略在不同发球范围内表现出色（命中率≥96%，成功率≥92%）。实机测试显示协调的步法和快速回球能力。

Conclusion: 该策略为实现多功能、竞争性的人形机器人乒乓球提供了一条实用路径。

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [17] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan框架通过任务分解和视觉语言基础，从单个人类示例中学习可重用技能，显著减少演示需求，支持长时任务组合泛化，并实现跨平台技能迁移。


<details>
  <summary>Details</summary>
Motivation: 解决现有双手机器人操纵方法在泛化和适应性问题上的不足，如需要大量演示或缺乏动态场景灵活性。

Method: 通过任务感知分解提取不变原语，利用视觉语言基础动态调整适应组件，结合语义解析和几何可行性约束。

Result: 实验显示VLBiMan在减少演示需求、组合泛化、鲁棒性和跨平台技能迁移方面表现优异。

Conclusion: VLBiMan结合人类先验和视觉语言适应机制，为非结构化环境中的双手机器人操纵提供了实用且通用的解决方案。

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [18] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: 研究了基于土耳其冰淇淋销售商互动的游戏性欺骗对用户信任、享受和参与感的影响，发现这种欺骗能提升乐趣和参与度，但会降低感知安全和信任。


<details>
  <summary>Details</summary>
Motivation: 探索人类-机器人交互中游戏性欺骗的作用，尤其是其对用户体验的多维影响。

Method: 设计了配备定制末端执行器的机器人机械臂，实现了五种土耳其冰淇淋风格的欺骗策略，并进行了91名参与者的混合设计用户研究。

Result: 游戏性欺骗显著提升了用户的享受和参与度，但降低了感知安全和信任。

Conclusion: 游戏性欺骗是娱乐和互动导向机器人设计的有效策略，但也需权衡其复杂影响。

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [19] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: 提出了一种基于条件对抗运动先验（CAMP）的多技能学习框架，旨在帮助四足机器人通过专家演示高效学习多样化的运动技能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法通过单一策略学习多种运动技能且缺乏平滑过渡的问题。

Method: 采用技能判别器和技能条件奖励设计，实现精确技能重建和多技能主动控制。

Result: 该框架能够高效学习多样化技能，并支持在复杂环境中学习泛化性策略。

Conclusion: 提出了一种实用的多技能学习解决方案，适用于复杂环境中的运动控制。

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [20] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: 提出一种结合博弈论与传统轨迹分类的方法，提高车辆变道预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有分类模型在缺乏全局场景信息时无法准确预测变道行为，博弈论可模拟多车间的理性决策。

Method: 利用Level-k博弈论建模车辆互动，结合传统运动分类模型，通过在线优化输出最理性预测。

Result: 该方法可更准确地预测目标车辆的变道行为，提升自适应巡航控制等系统的决策质量。

Conclusion: 博弈论与运动分类结合显著提升预测准确性，为智能驾驶系统提供更可靠的决策支持。

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [21] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: WAVE是一种集成蜗轮的非反向驱动可变刚度执行器，通过弹性弹簧吸收外力并调节刚度，验证了其在外力解耦和安全操作中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为提高机械手的安全性和多功能性，设计一种能够同时调节刚度和顺应性的执行器。

Method: 集成非反向驱动蜗轮，通过弹簧存储弹性能量，并调节弹簧预压缩长度实现刚度调制。

Result: 实验验证了刚度模型，电机负载在静止时接近零，展示了在接触密集型任务中的潜力。

Conclusion: WAVE成功解耦外力，适用于挑战性环境中的稳健机器人应用。

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [22] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: SAGE是一种新型框架，利用语义场景图作为结构化表示，结合任务规划与图像编辑，解决长时程操作任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 长时程操作任务涉及复杂的动作序列和对象交互，现有方法在泛化和语义推理上存在不足。

Method: SAGE通过语义场景图连接任务级推理和像素级控制，包括基于场景图的任务规划器和结构图像编辑管道。

Result: 实验表明，SAGE在长时程任务上达到最优性能。

Conclusion: SAGE通过结合语义推理与图像编辑，有效推动了长时程操作任务的研究。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [23] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 本文提出了可学习的共形预测（LCP），通过利用上下文特征改进深度学习的预测可靠性，在分类、检测和路径规划任务中显著优于传统共形预测和基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在未知或噪声输入下预测可靠性不足的问题，传统共形预测（CP）的固定评分无法适应上下文，导致预测区间过保守或不安全。

Method: LCP用轻量级神经网络替代CP的固定评分，结合几何、语义和任务特征生成上下文感知的不确定集。

Result: LCP在分类任务中减少预测集大小18%，检测任务中收紧区间52%，路径规划成功率从72%提升至91%；计算开销低（4.8%运行时开销，42KB内存）。

Conclusion: LCP在保持理论保证的同时显著提升性能，适用于资源受限的自主系统。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [24] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: 论文针对学习型规划器在驾驶数据长尾分布下的性能问题，提出FlowDrive方法，通过轨迹模式重加权和改进的流匹配技术提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 驾驶数据的长尾分布导致学习型规划器对常见驾驶行为过度偏重，而在危险或罕见场景中表现不佳，影响了规划的全面性和安全性。

Method: 采用轨迹模式重加权平衡训练数据，并设计FlowDrive方法，利用流匹配技术直接生成轨迹分布；引入适度引导增加轨迹多样性。

Result: FlowDrive在nuPlan和interPlan基准测试中，性能优于其他学习型规划器，接近基于规则的方法；经过改进后（FlowDrive*）更达到最佳性能。

Conclusion: FlowDrive通过数据平衡和改进的流匹配技术，有效解决了长尾分布问题，显著提升了规划器的性能和多样性。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [25] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 论文探讨了如何通过结合符号计划与连续轨迹生成来解决机器人执行长期复杂任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成方法（如扩散模型）在处理涉及复杂决策的长期任务时表现不佳，容易混淆不同行为模式。

Method: 提出一种结合离散变量扩散和连续扩散的新方法，通过同时生成高层符号计划来增强连续轨迹生成。

Result: 该方法显著优于基线，并支持根据部分或完整符号条件灵活合成轨迹。

Conclusion: 混合扩散过程为解决机器人长期任务提供了有效途径。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [26] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 该论文提出了一种利用EgoScaler框架从原始自我中心视频中提取6DoF物体操作轨迹的方法，显著提高了VLA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用自我中心视频作为低成本、可扩展的资源，替代传统昂贵且依赖专家标注的VLAs训练方法。

Method: 利用EgoScaler框架从四种大规模自我中心视频数据集中提取并自动优化6DoF操作轨迹，构建新的数据集用于VLA预训练。

Result: 实验表明，预训练后的模型任务成功率提高了20%以上，性能接近真实机器人数据集，且结合真实数据后效果更佳。

Conclusion: 自我中心视频是VLA研究中具有潜力且可扩展的资源。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [27] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: 论文提出了一种新颖的计算方法，用于设计单自由度（1-DoF）过约束机器人肢体，以实现高效、无自碰撞的全周期运动。


<details>
  <summary>Details</summary>
Motivation: 尽管自然界中的多自由度肢体设计更具多样性，但单自由度设计因其简单性、鲁棒性和成本效益仍具有重要价值。本研究旨在解决1-DoF系统在运动多样性和自碰撞问题上的挑战。

Method: 研究通过几何优化问题生成无自碰撞的连杆机构，并使用过约束连杆优化空间轨迹生成，同时优化几何形状以确保单驱动下的平滑无碰撞运动。

Result: 实验验证了方法的有效性，包括个性化自动机和仿生六足机器人。结果显示，采用过约束机器人肢体的六足机器人在前进行走时表现出卓越的能源效率。

Conclusion: 研究成功实现了高效、无自碰撞的单自由度机器人肢体设计，为未来机器人系统提供了新的设计思路。

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [28] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种基于自适应ICP的LiDAR测程法，通过可靠的初始姿态和动态调整的自适应阈值，提高了动态环境中的点云配准精度。


<details>
  <summary>Details</summary>
Motivation: 现有的ICP方法在初始姿态可靠性不足且缺乏自适应机制的情况下，容易陷入局部最优且难以应对复杂动态环境。

Method: 采用密度过滤的分布式粗配准获取初始姿态，结合运动预测姿态选择可靠初始姿态；动态调整自适应阈值；基于点对平面自适应ICP进行配准。

Result: 在KITTI数据集上的实验表明，该方法优于现有方法，显著提高了LiDAR测程的精度。

Conclusion: 该方法通过可靠的初始姿态和自适应机制，有效提升了动态环境中的配准精度和稳定性。

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [29] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: 本文研究了足式机器人在松散可变形基底上的步态对地形传感精度的影响，比较了传感导向的Crawl N' Sense步态和运动导向的Trot-Walk步态的效果。结果表明，两种步态均可区分基底强度差异，但Trot-Walk的数据波动更大；Crawl N' Sense能更准确地检测表面脆性断裂。


<details>
  <summary>Details</summary>
Motivation: 为了提高足式机器人在行星探测中的地形传感能力，研究不同步态对地形性质（如强度和纹理）测量的影响。

Method: 通过实验室实验，比较Crawl N' Sense（传感导向）和Trot-Walk（运动导向）两种步态在刚性表面、松散沙地和带合成外壳的沙地上的表现。

Result: 两种步态均能区分基底强度差异，但Trot-Walk测量数据的方差更大；Crawl N' Sense能更准确地检测脆性表面断裂。

Conclusion: 研究表明，传感导向的慢速步态更适合高精度地形测量，为未来行星探测任务的步态设计提供了重要参考。

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [30] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 该论文提出了一种基于视觉-语言-动作（VLA）模型的高效推理方法，通过动态剪枝技术在不同机器人操作阶段调整视觉冗余，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在VLA模型推理中忽略了不同机器人操作阶段视觉冗余的差异性，作者观察到粗粒度操作阶段视觉冗余更高，且与动作动态相关，从而提出动态剪枝框架。

Method: 提出了基于动作感知的动态剪枝（ADP）框架，结合文本驱动的令牌选择和动作感知的轨迹门控，通过门控机制根据近期动作轨迹动态调整令牌保留比例。

Result: 实验表明，ADP显著降低了计算量（FLOPs）和推理延迟（如1.35倍加速），同时保持较高的成功率（如OpenVLA提升25.8%）。

Conclusion: ADP提供了一种简单高效的机器人策略实现路径，平衡了计算效率和感知精度，推动了机器人操作的前沿性能。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [31] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 该论文提出了一种多阶段非线性模型预测控制（RNMPC）方法，用于提高外骨骼机器人的鲁棒性和性能，通过非线性优化和多重场景处理系统不确定性，显著减少跟踪误差和交互力。


<details>
  <summary>Details</summary>
Motivation: 由于外骨骼机器人控制系统中存在的不确定性，特别是在动态非线性条件下，传统线性化方法的性能较差，需要一种更鲁棒的控制策略。

Method: 研究者提出了一种多阶段非线性模型预测控制（RNMPC）方法，通过求解非线性优化问题并利用多重场景代表系统不确定性，来优化对外骨骼的控制。

Result: 模拟和实验表明，该方法显著提高了鲁棒性，与非鲁棒NMPC相比，在未知负载和外部干扰下，髋部和胫骨的交互力分别减少了77%和94%。

Conclusion: 多阶段RNMPC方法在处理外骨骼机器人系统中的不确定性方面表现优异，为未来外骨骼控制系统的设计提供了新的方向。

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [32] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoGrasp通过学习单个成功抓取演示轨迹，利用RL优化通用抓取策略，在不同物体上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多指灵巧手通用抓取的高维探索难题，避免复杂奖励和课程设计。

Method: 通过编辑演示轨迹的手腕姿势和关节角度，将其建模为单步MDP，并使用RL优化策略。

Result: 在仿真中实现95%成功率，并在未见物体上平均84.6%成功率，实际测试中也表现优异。

Conclusion: DemoGrasp是一种简单高效的通用灵巧抓取方法，具强泛化能力。

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [33] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: 论文提出了SymOpt流程和DHAGrasp生成器，解决了双抓数据集稀缺问题，并通过对称性和两阶段设计生成高质量语义一致的抓握。


<details>
  <summary>Details</summary>
Motivation: 现有抓握数据集多为单手交互且语义标注有限，双抓数据集稀缺，阻碍了稳健的手-物体交互研究。

Method: 通过SymOpt流程利用单手数据集和对称性构建大规模双抓数据集，并设计DHAGrasp生成器，结合新颖的双抓表示和两阶段学习。

Result: 实验表明，该方法生成的抓握多样且语义一致，质量和泛化性均优于基线。

Conclusion: SymOpt和DHAGrasp有效解决了双抓数据集不足问题，为手-物体交互研究提供了新工具。

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [34] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: VLM2VLA是一种通过自然语言表示低级动作的方法，解决了视觉语言模型（VLM）在微调为视觉语言动作（VLA）模型时导致的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 微调VLM用于机器人远程操作数据可能导致其核心推理和多模态理解能力退化，影响泛化能力。

Method: VLM2VLA通过自然语言对齐数据分布，并使用LoRA进行微调，避免大规模修改VLM架构。

Result: VLM2VLA在保留VLM核心能力的同时，实现了对新任务的零样本泛化，并通过800多次实验验证。

Conclusion: VLM2VLA提供了一种高效且保护VLM核心能力的VLA训练范式。

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [35] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: MimicDreamer框架通过视觉、视角和动作对齐，将低成本的人类演示视频转化为机器人可用的监督数据，显著提升了VLA模型的性能。


<details>
  <summary>Details</summary>
Motivation: 人类演示视频成本低且易收集，但与机器人视频存在显著的领域差距，需要一种方法来弥合这种差距以支持VLA模型的训练。

Method: MimicDreamer包含H2R Aligner（视觉对齐）、EgoStabilizer（视角稳定）和动作对齐模块，通过生成机器人演示视频和优化动作，提升模型的训练效果。

Result: 实验表明，仅使用合成的人类转机器人视频训练的VLA模型在真实机器人上实现了few-shot执行，且性能提升14.7%。

Conclusion: 该方法证明了人类演示数据在训练VLA模型中的潜力，并通过技术手段解决了领域差距问题。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [36] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: Super-Mimic 是一种分层框架，通过从无脚本的人类演示视频中直接推断程序意图，实现零样本机器人模仿，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态基础方法在静态视觉输入下无法分解高级命令为可执行动作序列的挑战。

Method: 包括 Human Intent Translator（HIT）和 Future Dynamics Predictor（FDP），分别解析视频为子任务并生成动态预测。

Result: 在长周期任务实验中性能提升超过 20%。

Conclusion: 视频驱动的意图解析与前瞻动态建模相结合是开发通用机器人系统的有效策略。

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [37] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 研究开发了一款使用Furhat对话机器人和大语言模型的应用程序，通过游戏提升学前儿童的语言表达能力，目标是建立一个基于机器人的语言学习干预系统。


<details>
  <summary>Details</summary>
Motivation: 针对语言发展障碍或移民相关语言问题的学前儿童，传统语言治疗需要治疗师和教育者实时生成特定语言结构，任务繁重且有挑战。机器人辅助可以更高效地完成这一任务。

Method: 使用大语言模型驱动的Furhat对话机器人，在“Alias”单词检索游戏中管理对话、情感反馈和轮流互动，并计划进一步利用LLM生成特定语言目标。

Result: 目前应用程序已完成基础功能，下一步是优化LLM生成特定语言结构的能力。

Conclusion: 机器人辅助语言学习有望超越人类表现，并成为儿童和专业人员的语言模型和导师，未来目标是开发多语言支持的LLM机器人干预系统。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [38] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: 论文提出了一种基于IMU预积分雷达因子的方法，减少雷达-LiDAR传感器对的状态节点数量，从而降低计算成本，同时保持定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统的固定滞后雷达-LiDAR-惯性平滑器由于时间同步问题，导致状态节点数量翻倍，增加了计算负担。本文旨在降低计算成本同时不影响精度。

Method: 通过引入IMU预积分雷达因子，利用高频惯性数据将LiDAR状态传播到雷达时间戳，从而减少节点数量。

Result: 实验显示，新方法在保持定位误差与传统基线一致的同时，将因子图优化时间降低了56%。

Conclusion: 该方法有效降低了计算成本，适用于资源受限的硬件，同时保持了定位的准确性。

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [39] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 本文提出了一种基于物联网机器人（IoRT）的新型系统架构，通过协调人-机器人-机器人交互，实现主动且个性化的患者辅助，以减少医院患者跌倒风险。


<details>
  <summary>Details</summary>
Motivation: 传统跌倒预防系统通常依赖事后检测或反应性警报，且存在高误报率，未能解决患者试图离床的根本需求。

Method: 系统结合隐私保护的热感测模型（实时预测离床行为）与两个协调的机器人代理，根据预测意图和患者输入动态响应。

Result: 研究展示了分布式感知、预测和多机器人协调的模块化框架，用户研究和系统误差分析为情境感知交互设计提供了依据。

Conclusion: 交互式连接机器人系统能够超越被动监测，提供及时有效的辅助，创造更安全、响应更迅速的护理环境。

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [40] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: RoboView-Bias是首个系统性量化机器人操作中视觉偏见的基准，通过因素隔离原则生成2127个任务实例，评估三种代表代理的视觉偏见，并提出一种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注泛化性和鲁棒性，缺乏对视觉偏见的系统性量化，限制了理解感知如何影响决策稳定性。

Method: 利用结构化变量生成框架和感知公平验证协议，量化视觉因素及其交互引起的偏见。

Result: 发现代理存在显著视觉偏见（如相机视角是关键因素），并验证了一种可减少偏见约54.5%的缓解策略。

Conclusion: 系统性分析视觉偏见是开发安全可靠通用代理的前提。

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [41] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: 提出了一种基于学习的触觉反应多智能体MPC控制器，用于抓取不同软硬度和形状的物体，超越现有单智能体系统的能力，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有抓取技术主要针对刚性物体，对脆弱或可变形材料的实时反馈能力不足，且单智能体系统难以处理大型、重型物体。

Method: 利用两个Gelsight Mini触觉传感器提取物体的实时纹理和刚度信息，通过多智能体MPC和触觉数据驱动的状态推断方法实现协同控制。

Result: 实验表明，该方法在抓取不同大小和刚度的物体时，成功率和稳定性均优于独立的PD和MPC基线。

Conclusion: 结合触觉感知和学习型多智能体MPC的方法，为复杂环境中的协同抓取提供了鲁棒且智能的解决方案。

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [42] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 该论文提出了一种名为OntoBOT的机器人本体论，旨在统一表示任务、动作、环境和机器人能力，以提高服务机器人在动态环境中的推理能力和知识共享。


<details>
  <summary>Details</summary>
Motivation: 解决服务机器人在实际部署中因平台耦合性高导致的互操作性和复用性问题，以及现有本体论在领域覆盖和系统集成上的不足。

Method: 扩展现有本体论（如SOMA和DOLCE），提出OntoBOT，提供任务、动作、环境和能力的统一表示，并通过TIAGo等四种机器人验证其通用性。

Result: OntoBOT支持任务执行的形式化推理，并在四种机器人上展示了其上下文感知推理、任务导向执行和知识共享的能力。

Conclusion: OntoBOT为服务机器人提供了一种通用的知识表示方法，增强了其在实际应用中的灵活性和可扩展性。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [43] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: UnderwaterVLA是一种新型自主水下导航框架，结合多模态基础模型与具身智能系统，通过双脑架构、VLA模型和流体力学MPC方案解决水下操作的挑战，显著减少导航误差并提升任务完成率。


<details>
  <summary>Details</summary>
Motivation: 解决水下操作中因流体扰动、有限通信带宽和浑浊水域感知退化带来的困难。

Method: 1. 双脑架构分离高层任务推理与底层反应控制；2. 首次将VLA模型应用于水下机器人，结合链式思维推理；3. 流体力学MPC方案实时补偿流体效应。

Result: 在实地测试中，UnderwaterVLA减少了导航误差，任务完成率比基线提高19%至27%。

Conclusion: UnderwaterVLA减少对水下特定训练数据的依赖，提供可扩展且经济高效的智能AUV发展路径。

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [44] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: 提出一种针对异构机器人团队的任务分配算法，通过概率分布建模任务需求，优化资源利用并减少任务失败。采用基于市场的方法，在严格通信假设下提供多项式时间解。


<details>
  <summary>Details</summary>
Motivation: 研究异构机器人在不确定任务需求环境中的任务分配问题，旨在通过互补技能优化团队协作，减少资源浪费。

Method: 使用概率分布建模任务需求，提出基于市场的任务分配算法，考虑机器人间的耦合奖励，提供多项式时间解。

Result: 实验证明该算法优于基准方法，但凸显了在分散式环境下处理耦合奖励的挑战。

Conclusion: 该方法有效优化异构机器人团队的任务分配，但需进一步解决分散式环境中的耦合奖励问题。

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [45] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 论文提出了一种新的本体论模型和算法，用于比较和解释机器人决策中竞争计划的差异，并通过实证验证了其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 确保人机交互中机器人决策的可理解性和可信赖性，使机器人能够解释其决策依据。

Method: 提出了一种新的本体论模型来形式化和推理竞争计划之间的差异，并开发了一种新算法生成对比性解释。

Result: 新算法在生成解释方面优于基线方法，能够更有效地展示计划的差异。

Conclusion: 该研究为机器人决策的解释提供了有效工具，增强了人机交互中决策的透明度和可信度。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [46] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: 论文提出了HELIOS，一种分层场景表示方法，用于解决语言指定移动操作任务中的部分观察问题。通过结合2D语义地图和3D高斯表示，优化目标函数以实现高效搜索，并在模拟和真实环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在部分观察场景中进行语言指定移动操作时面临的交互、语义信息接地和动态更新场景知识的挑战。

Method: 提出了HELIOS方法，结合2D语义地图和3D高斯表示，显式建模多视角一致性，并通过平衡探索和利用的目标函数搜索目标物体。

Result: 在OVMM基准测试中取得最佳效果，并在真实世界的Spot机器人上验证了零样本迁移能力。

Conclusion: HELIOS通过分层表示和目标优化，成功解决了部分观察场景中的语言指定任务问题，展示了优越的泛化能力。

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [47] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种基于意图的换道框架，通过实时识别驾驶风格和合作意识决策，提升混合交通环境下自动驾驶车辆的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 在混合交通环境中，自动驾驶车辆（AVs）与多样化的人类驾驶车辆（HVs）互动，不可预测的意图和异构行为使得安全高效的换道操作极具挑战性。现有方法常过度简化这些互动。

Method: 结合驾驶风格识别、合作感知决策和协调运动规划。使用NGSIM数据集训练的深度学习分类器实时识别驾驶风格；合作评分估计周围驾驶员的意图和合作意愿；决策结合行为克隆与逆强化学习；轨迹生成采用模型预测控制和IRL意图推断。

Result: 实验显示，该模型在换道识别中的准确率为94.2%，F1分数为94.3%，优于基于规则和学习的基线方法4-15%。

Conclusion: 模型能有效捕捉驾驶员间的异构性，展示了在复杂交通环境中实现上下文感知和人本化自动驾驶的潜力。

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [48] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 本文提出了一种仅使用RGB输入的新型流程，用于预测人类交互意图，并通过合成序列生成方法和改进的训练策略解决了数据不平衡问题，实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 高效检测人类与机器人交互意图对提升人机交互（HRI）效果至关重要。现有方法多依赖多模态输入，而本文提出仅需RGB输入的解决方案。

Method: 采用RGB-only流程，结合MINT-RVAE合成序列生成方法及新损失函数和训练策略，提升模型泛化能力。

Result: 在AUROC指标上达到0.95，优于现有方法（0.90-0.912），并支持精确帧级预测。

Conclusion: 该方法不仅性能优越，还公开了带帧级标注的新数据集，为未来研究提供支持。

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [49] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: 论文提出EgoDemoGen框架，通过生成配对的新视角演示视频，解决模仿学习策略在单一视角下性能下降的问题，显著提升机器人在标准和新视角下的表现。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在单一视角下训练时，面对视角变化时性能会下降。为了增强视角鲁棒性，需要一种能生成多视角演示的方法。

Method: 提出EgoDemoGen框架，结合动作重定向和生成视频修复模型EgoViewTransfer，合成新视角的演示视频。通过预训练模型的微调和自监督重投影策略实现。

Result: 在仿真和真实机器人实验中，使用EgoDemoGen生成的演示显著提升了策略的成功率（仿真中标准视角+17.0%，新视角+17.7%；真实机器人+18.3%和+25.8%）。

Conclusion: EgoDemoGen为提升机器人在不同视角下的鲁棒性提供了实用的解决方案，效果随演示比例增加而持续提升。

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [50] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 本文提出了一种基于机器人交互训练的大规模生成世界模型WoW，强调物理直觉需要通过真实世界的丰富交互来建立。通过SOPHIA和逆动力学模型优化，WoW在物理一致性和因果推理方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型（如Sora）依赖于被动观察，难以理解物理因果关系。作者假设真实的物理直觉需要基于大量因果丰富的现实世界交互。

Method: 训练了一个140亿参数的生成世界模型WoW，使用200万条机器人交互轨迹数据，并结合SOPHIA和逆动力学模型优化生成结果。

Result: WoW在物理一致性、碰撞动力学和物体持久性方面表现优异，在WoWBench基准测试中达到领先水平。

Conclusion: 研究表明，大规模真实世界交互是AI发展物理直觉的基础，模型、数据和基准将开源。

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [51] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: VLA-Reasoner通过前瞻性推理和蒙特卡罗树搜索（MCTS）解决了现有视觉-语言-动作模型（VLA）在长视野任务中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型仅能预测短视的下一个动作，无法有效处理长视野任务中因逐步偏差导致的问题。

Method: VLA-Reasoner通过世界模型生成未来状态，结合MCTS和置信度采样机制优化动作搜索，同时利用离线奖励策略校正偏差。

Result: 实验表明，VLA-Reasoner在仿真和实际环境中均显著优于现有VLA模型。

Conclusion: VLA-Reasoner为机器人操作提供了可扩展的测试时计算路径。

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [52] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN是一种基于扩散的统一框架，用于语言条件化机器人操作，通过结构化像素运动表示连接高层运动意图和低层机器人动作，展示了扩散模型在机器人学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言条件化机器人操作中高层运动意图与低层动作之间的连接问题，DAWN提出了一种基于扩散的统一框架，旨在实现可解释的运动抽象。

Method: DAWN将高层和低层控制器建模为扩散过程，形成一个完全可训练的端到端系统，并采用结构化像素运动表示。

Result: DAWN在CALVIN基准测试中取得了最先进的结果，展示了多任务性能，并在MetaWorld中验证了有效性。即使仿真与现实之间存在巨大差距且真实数据有限，DAWN仍能通过少量微调实现可靠的实际应用。

Conclusion: DAWN展示了扩散模型与运动中心表示结合的潜力，为可扩展且鲁棒的机器人学习提供了有力基线。

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [53] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF是一种无需训练的航空视觉与语言导航框架，通过将动作预测视为2D空间定位任务，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有基于视觉语言模型（VLM）的导航方法将动作预测视为文本生成任务的局限性，提出了一种更高效的空间定位方法。

Method: SPF利用VLM将模糊的语言指令分解为输入图像上的2D路径点预测，并结合距离预测转换为3D位移向量作为无人机动作命令。

Result: SPF在DRL仿真基准测试中表现优异，绝对优势提升63%，并在实际测试中大幅超越基线方法。

Conclusion: SPF展示了强大的导航能力和对不同VLM的泛化性，为航空导航提供了高效解决方案。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [World's First Authenticated Satellite Pseudorange from Orbit](https://arxiv.org/abs/2509.21601)
*Jason Anderson*

Main category: cs.CR

TL;DR: 介绍了Pulsar认证测距服务的初步结果，包括水印设计、安全性分析及实验结果，展示了其反欺骗检测的有效性。


<details>
  <summary>Details</summary>
Motivation: 提供无需假设加密密钥泄漏的认证测距服务，实现世界上首次认证卫星伪距测距。

Method: 设计Pulsar水印，分析其安全性和检测概率，并通过轨道传输验证其有效性。

Result: 实验结果表明，Pulsar水印具有较高的欺骗检测效能，且无需依赖对称密钥泄漏假设。

Conclusion: Pulsar认证测距服务通过数学证明和实验验证，成功实现了安全的卫星伪距认证。

Abstract: Cryptographic Ranging Authentication is here! We present initial results on
the Pulsar authenticated ranging service broadcast from space with Pulsar-0
utilizing a recording taken at Xona headquarters in Burlingame, CA. No
assumptions pertaining to the ownership or leakage of encryption keys are
required. This work discusses the Pulsar watermark design and security
analysis. We derive the Pulsar watermark's probabilities of missed detection
and false alarm, and we discuss the required receiver processing needed to
utilize the Pulsar watermark. We present validation results of the Pulsar
watermark utilizing the transmissions from orbit. Lastly, we provide results
that demonstrate the spoofing detection efficacy with a spoofing scenario that
incorporates the authentic transmissions from orbit. Because we make no
assumption about the leakage of symmetric encryption keys, this work provides
mathematical justification of the watermark's security, and our July 2025
transmissions from orbit, we claim the world's first authenticated satellite
pseudorange from orbit.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [55] [Transabdominal Fetal Oximetry via Diffuse Optics: Principled Analysis and Demonstration in Pregnant Ovine Models](https://arxiv.org/abs/2509.21594)
*Weitai Qian,Rishad Raiyan Joarder,Randall Fowler,Begum Kasap,Mahya Saffarpour,Kourosh Vali,Tailai Lihe,Aijun Wang,Diana Farmer,Soheil Ghiasi*

Main category: eess.IV

TL;DR: 该论文提出了一种基于扩散光学的无创胎儿血氧饱和度监测方法，通过理论推导和多检测器信息融合，显著提高了测量精度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过扩散光学技术提升胎儿血氧饱和度监测的准确性，并探索其技术极限。

Method: 提出指数搏动比（EPR）作为关键特征，并开发机器学习模型融合多检测器数据。

Result: 在仿真和动物实验中，平均绝对误差分别为4.81%和6.85%，皮尔逊相关系数分别为0.81和0.71。

Conclusion: 该方法优于现有技术，有望成为产时胎儿监测的补充技术。

Abstract: Diffuse optics has the potential to offer a substantial advancement in fetal
health monitoring via enabling continuous measurement of fetal blood oxygen
saturation (fSpO$_2$). Aiming to enhance the sensing accuracy and to elucidate
the foundational limits of Transabdominal Fetal Oximetry (TFO) via diffuse
optics, we introduce a theoretical derivation, and a comprehensive pipeline for
fSpO$_2$ estimation from non-invasively sensed diffuse light intensity values,
which are leveraged to analyze datasets obtained through both simulations and
in-vivo experiments in gold standard large animal model of pregnancy. We
propose the Exponential Pulsation Ratio (EPR) as a key feature, and develop
machine-learning models to fuse the information collected across multiple
detectors. Our proposed method demonstrates a Mean Absolute Error (MAE) of
4.81% and 6.85% with a Pearson's r correlation of 0.81 (p<0.001) and 0.71
(p<0.001) for estimation of fSpO$_2$ in simulated dataset and in-vivo dataset,
respectively. Across both datasets, our method outperforms the existing
approaches, enhancing the accuracy of the fSpO$_2$ estimation and demonstrates
its viability as a supplemental technology for intrapartum fetal monitoring.

</details>


### [56] [Multicollinearity-Aware Parameter-Free Strategy for Hyperspectral Band Selection: A Dependence Measures-Based Approach](https://arxiv.org/abs/2509.21973)
*Dibyabha Deb,Ujjwal Verma*

Main category: eess.IV

TL;DR: 该论文提出了一种结合三种依赖度量（ABC、MI和VIF）的无参数波段选择方法，有效解决了高光谱数据维度高的问题，并在多个数据集上验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱数据的高维度带来了处理效率的挑战，而现有波段选择方法存在初始化敏感、参数调优和高计算成本等问题。

Method: 该方法通过VIF预选波段以减少多重共线性，并基于ABC和MI值的聚类算法选择最优波段子集，完全无需参数调优。

Result: 在四个标准数据集上的实验表明，该方法能有效捕获相关光谱特征，并通过SVM分类验证了其增强分类性能的效果。

Conclusion: 结合ABC和MI的波段选择方法不仅鲁棒且判别性强，且无需参数调优，显著优于现有方法。

Abstract: Hyperspectral bands offer rich spectral and spatial information; however,
their high dimensionality poses challenges for efficient processing. Band
selection (BS) methods aim to extract a smaller subset of bands to reduce
spectral redundancy. Existing approaches, such as ranking-based,
clustering-based, and iterative methods, often suffer from issues like
sensitivity to initialization, parameter tuning, and high computational cost.
This work introduces a BS strategy integrating three dependence measures:
Average Band Correlation (ABC) and Mutual Information (MI), and Variance
Inflation Factor (VIF). ABC quantifies linear correlations between spectral
bands, while MI measures uncertainty reduction relative to ground truth labels.
To address multicollinearity and reduce the search space, the approach first
applies a VIF-based pre-selection of spectral bands. Subsequently, a clustering
algorithm is used to identify the optimal subset of bands based on the ABC and
MI values. Unlike previous methods, this approach is completely parameter-free
for hyperspectral band selection, eliminating the need for optimal parameter
estimation. The proposed method is evaluated on four standard benchmark
datasets: WHU-Hi-LongKou, Pavia University, Salinas, and Oil Spill datasets,
and is compared to existing state-of-the-art approaches. There is significant
overlap between the bands identified by our proposed method and those selected
by other methods, indicating that our approach effectively captures the most
relevant spectral features. Further, support vector machine (SVM)
classification validates that VIF-driven pruning enhances classification by
minimizing multicollinearity. Ablation studies confirm that combining ABC with
MI yields robust, discriminative band subsets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard Overparametrization: The Dynamic Graph Flow Case](https://arxiv.org/abs/2509.22197)
*Duc Thien Nguyen,Konstantinos Slavakis,Eleftherios Kofidis,Dimitris Pados*

Main category: cs.LG

TL;DR: 论文提出了一种名为KReTTaH的回归框架，用于多路数据插值，通过内核回归和张量火车结构实现高效参数化，并在动态图流估计中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决多路数据缺失问题，提出一种高效且可解释的插值方法，特别适用于动态图流等场景。

Method: KReTTaH结合了非参数化的核回归和张量火车结构（固定TT秩），通过哈达玛超参数化提升稀疏性，并在黎曼流形上求解平滑逆问题。

Result: 在真实图数据集上的实验表明，KReTTaH在缺失时间边流插值上优于当前最优方法，包括非参数张量和神经网络方法。

Conclusion: KReTTaH提供了一种高效、灵活且可解释的多路数据插值框架，尤其在动态图流应用中表现突出。

Abstract: A regression-based framework for interpretable multi-way data imputation,
termed Kernel Regression via Tensor Trains with Hadamard overparametrization
(KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting
imputation as regression via reproducing kernel Hilbert spaces. Parameter
efficiency is achieved through tensors of fixed tensor-train (TT) rank, which
reside on low-dimensional Riemannian manifolds, and is further enhanced via
Hadamard overparametrization, which promotes sparsity within the TT parameter
space. Learning is accomplished by solving a smooth inverse problem posed on
the Riemannian manifold of fixed TT-rank tensors. As a representative
application, the estimation of dynamic graph flows is considered. In this
setting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based
(topological) priors via its inverse problem formulation. Numerical tests on
real-world graph datasets demonstrate that KReTTaH consistently outperforms
state-of-the-art alternatives-including a nonparametric tensor- and a
neural-network-based methods-for imputing missing, time-varying edge flows.

</details>


### [58] [Towards a more realistic evaluation of machine learning models for bearing fault diagnosis](https://arxiv.org/abs/2509.22267)
*João Paulo Vieira,Victor Afonso Bauler,Rodrigo Kobashikawa Rosa,Danilo Silva*

Main category: cs.LG

TL;DR: 该论文探讨了轴承故障诊断中数据泄漏问题及其对模型评估的影响，提出了一种无泄漏的评估方法，强调轴承分区策略的重要性，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习在轴承故障诊断中因数据泄漏导致的泛化能力不足问题，研究需要更严谨的评估方法。

Method: 提出基于轴承分区的无泄漏评估方法，并将分类任务重新定义为多标签问题，使用Macro AUROC等指标。

Result: 研究表明，常见的数据分割策略会导致虚假相关性，而轴承分区和数据集多样性对模型泛化能力至关重要。

Conclusion: 论文强调了无泄漏评估协议的重要性，并为工业故障诊断提供了实用的数据集分区和模型选择指南。

Abstract: Reliable detection of bearing faults is essential for maintaining the safety
and operational efficiency of rotating machinery. While recent advances in
machine learning (ML), particularly deep learning, have shown strong
performance in controlled settings, many studies fail to generalize to
real-world applications due to methodological flaws, most notably data leakage.
This paper investigates the issue of data leakage in vibration-based bearing
fault diagnosis and its impact on model evaluation. We demonstrate that common
dataset partitioning strategies, such as segment-wise and condition-wise
splits, introduce spurious correlations that inflate performance metrics. To
address this, we propose a rigorous, leakage-free evaluation methodology
centered on bearing-wise data partitioning, ensuring no overlap between the
physical components used for training and testing. Additionally, we reformulate
the classification task as a multi-label problem, enabling the detection of
co-occurring fault types and the use of prevalence-independent metrics such as
Macro AUROC. Beyond preventing leakage, we also examine the effect of dataset
diversity on generalization, showing that the number of unique training
bearings is a decisive factor for achieving robust performance. We evaluate our
methodology on three widely adopted datasets: CWRU, Paderborn University (PU),
and University of Ottawa (UORED-VAFCLS). This study highlights the importance
of leakage-aware evaluation protocols and provides practical guidelines for
dataset partitioning, model selection, and validation, fostering the
development of more trustworthy ML systems for industrial fault diagnosis
applications.

</details>


### [59] [Distributed Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2509.22321)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种分布式在线梯度下降方法，用于优化局部关联记忆（AM），并通过树形路由通信实现，理论分析和实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络（如Transformers）的运作基于关联记忆（AM），本文研究分布式环境下多智能体通过本地AM进行自我关联记忆和选择性信息共享的问题。

Method: 采用分布式在线梯度下降方法，通过树形路由通信优化各智能体的局部AM。

Result: 理论分析证明该方法具有次线性遗憾保证，实验显示其性能优于现有在线优化基线。

Conclusion: 该方法有效解决了分布式环境下多智能体的关联记忆优化问题，具有理论和实践优势。

Abstract: An associative memory (AM) enables cue-response recall, and associative
memorization has recently been noted to underlie the operation of modern neural
architectures such as Transformers. This work addresses a distributed setting
where agents maintain a local AM to recall their own associations as well as
selective information from others. Specifically, we introduce a distributed
online gradient descent method that optimizes local AMs at different agents
through communication over routing trees. Our theoretical analysis establishes
sublinear regret guarantees, and experiments demonstrate that the proposed
protocol consistently outperforms existing online optimization baselines.

</details>


### [60] [ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models](https://arxiv.org/abs/2509.22556)
*Chenyu Liu,Yuqiu Deng,Tianyu Liu,Jinan Zhou,Xinliang Zhou,Ziyu Jia,Yi Ding*

Main category: cs.LG

TL;DR: ECHO是一种新型的EEG解码器中心大模型范式，通过序列到序列学习提升多任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统EEG大模型缺乏高容量解码器，限制了学习特征的充分利用。

Method: ECHO将EEG建模为序列到序列学习，捕捉信号、标签和任务的层次关系，并结合离散支持样本构建上下文线索。

Result: ECHO在多任务设置中优于现有单任务大模型，表现出更强的泛化和适应能力。

Conclusion: ECHO通过解码器中心设计和上下文学习，显著提升了EEG模型的多任务性能。

Abstract: Electroencephalography (EEG), with its broad range of applications,
necessitates models that can generalize effectively across various tasks and
datasets. Large EEG Models (LEMs) address this by pretraining encoder-centric
architectures on large-scale unlabeled data to extract universal
representations. While effective, these models lack decoders of comparable
capacity, limiting the full utilization of the learned features. To address
this issue, we introduce ECHO, a novel decoder-centric LEM paradigm that
reformulates EEG modeling as sequence-to-sequence learning. ECHO captures
layered relationships among signals, labels, and tasks within sequence space,
while incorporating discrete support samples to construct contextual cues. This
design equips ECHO with in-context learning, enabling dynamic adaptation to
heterogeneous tasks without parameter updates. Extensive experiments across
multiple datasets demonstrate that, even with basic model components, ECHO
consistently outperforms state-of-the-art single-task LEMs in multi-task
settings, showing superior generalization and adaptability.

</details>


### [61] [Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification](https://arxiv.org/abs/2509.21405)
*Nyi Nyi Aung,Neil Muralles,Adrian Stein*

Main category: cs.LG

TL;DR: 该论文提出了一种结合物理信息的残差神经网络，用于无人机应用中基于已知动力学的物体识别，并在分类准确性和训练时间上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过结合物理知识和深度学习，解决无人机应用中物体识别的问题，尤其是在已知动力学模型的场景下。

Method: 方法是通过物理信息的残差神经网络进行状态映射和状态导数预测，并使用softmax层进行多类置信度估计。

Result: 结果表明，该方法在分类准确性上表现优异，同时显著减少了训练时间。

Conclusion: 结论是该框架为动力学模型已知的系统识别问题提供了一种有效的解决方案。

Abstract: This work addresses object identification under known dynamics in unmanned
aerial vehicle applications, where learning and classification are combined
through a physics-informed residual neural network. The proposed framework
leverages physics-informed learning for state mapping and state-derivative
prediction, while a softmax layer enables multi-class confidence estimation.
Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case
studies. The results demonstrate high classification accuracy with reduced
training time, offering a promising solution for system identification problems
in domains where the underlying dynamics are well understood.

</details>


### [62] [ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation](https://arxiv.org/abs/2509.22402)
*Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu*

Main category: cs.LG

TL;DR: Reward design is challenging in visual RL for robotic manipulation, especially in real-world settings where positional data is often unavailable. The proposed ReLAM framework uses keypoints and an anticipation model to generate dense rewards from action-free video demonstrations, improving learning efficiency and performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the bottleneck of reward design in visual RL for robotic manipulation, particularly in real-world settings where precise positional information is scarce.

Method: ReLAM learns an anticipation model to propose keypoint-based subgoals, creating a structured curriculum, and provides continuous rewards for training a goal-conditioned policy under HRL.

Result: ReLAM accelerates learning and outperforms state-of-the-art methods in complex, long-horizon manipulation tasks.

Conclusion: ReLAM offers a novel, effective approach to reward design in visual RL, demonstrating significant improvements in robotic manipulation tasks.

Abstract: Reward design remains a critical bottleneck in visual reinforcement learning
(RL) for robotic manipulation. In simulated environments, rewards are
conventionally designed based on the distance to a target position. However,
such precise positional information is often unavailable in real-world visual
settings due to sensory and perceptual limitations. In this study, we propose a
method that implicitly infers spatial distances through keypoints extracted
from images. Building on this, we introduce Reward Learning with Anticipation
Model (ReLAM), a novel framework that automatically generates dense, structured
rewards from action-free video demonstrations. ReLAM first learns an
anticipation model that serves as a planner and proposes intermediate
keypoint-based subgoals on the optimal path to the final goal, creating a
structured learning curriculum directly aligned with the task's geometric
objectives. Based on the anticipated subgoals, a continuous reward signal is
provided to train a low-level, goal-conditioned policy under the hierarchical
reinforcement learning (HRL) framework with provable sub-optimality bound.
Extensive experiments on complex, long-horizon manipulation tasks show that
ReLAM significantly accelerates learning and achieves superior performance
compared to state-of-the-art methods.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [63] [Snapshot Synthetic Aperture Imaging with Boiling Speckle](https://arxiv.org/abs/2509.21682)
*Janith B. Senanayaka,Christopher A. Metzler*

Main category: physics.optics

TL;DR: 该论文探讨了光基合成孔径(SA)成像方法在远距离成像中的应用问题，特别是散斑噪声的挑战。作者提出了一种快照SA成像方法和孔径相位同步策略，克服了限制，并展示了利用散斑提高SA成像系统分辨率的可能性。


<details>
  <summary>Details</summary>
Motivation: 针对光基SA成像在宏观尺度上面临的散斑噪声问题，研究旨在解决并克服这一技术难点。

Method: 提出快照SA成像方法和孔径相位同步策略，解决了独立散斑导致的成像问题。

Result: 通过仿真证明了散斑可用于恢复SA成像系统中缺失的空间频率信息，从而提高分辨率。

Conclusion: 研究表明，散斑噪声可以转化为优势，提升SA成像系统的性能。

Abstract: Light-based synthetic aperture (SA) imaging methods, such as Fourier
Ptychography, have brought breakthrough high-resolution wide-field-of-view
imaging capabilities to microscopy. While these technologies promise similar
improvements in long-range imaging applications, macroscale light-based SA
imaging is significantly more challenging. In this work, we first demonstrate
that speckle noise is particularly problematic for light-based SA systems.
Specifically, we prove that it is fundamentally impossible to perform SA
imaging of fully diffuse scenes if one captures sequential measurements that
suffer from per-measurement-independent speckle. We then develop a snapshot SA
imaging method and aperture-phase-synchronization strategy that can overcome
this limitation and enable SA imaging. Remarkably, we further demonstrate, in
simulation, that speckle can be exploited to recover missing spatial frequency
information in SA imaging systems with distributed, non-overlapping apertures.
That is, one can use speckle to improve the resolution of an SA imaging system.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [64] [Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review](https://arxiv.org/abs/2509.22271)
*Felix Glawe,Tim Schmeckel,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: 系统综述分析了22项实证研究，探讨了人机交互中人类自主性和代理感的保护与促进。研究发现五类影响因素，并指出当前研究存在定义和操作化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人技术的快速发展，人类自主性和代理感对用户体验和伦理部署的重要性日益凸显，需深入研究以支持设计和伦理原则。

Method: 通过系统性文献综述，筛选728篇文章中的22项实证研究，进行主题分析，探讨影响自主性和代理感的因素。

Result: 识别出机器人适应性、沟通方式、拟人化、机器人存在及个体差异五大影响因素，指出理论和实证上的局限与需求。

Conclusion: 研究强调需标准化定义，加强探索性和定性研究，以开发支持人类自主性和代理感的人机交互设计策略。

Abstract: Human autonomy and sense of agency are increasingly recognised as critical
for user well-being, motivation, and the ethical deployment of robots in
human-robot interaction (HRI). Given the rapid development of artificial
intelligence, robot capabilities and their potential to function as colleagues
and companions are growing. This systematic literature review synthesises 22
empirical studies selected from an initial pool of 728 articles published
between 2011 and 2024. Articles were retrieved from major scientific databases
and identified based on empirical focus and conceptual relevance, namely, how
to preserve and promote human autonomy and sense of agency in HRI. Derived
through thematic synthesis, five clusters of potentially influential factors
are revealed: robot adaptiveness, communication style, anthropomorphism,
presence of a robot and individual differences. Measured through psychometric
scales or the intentional binding paradigm, perceptions of autonomy and agency
varied across industrial, educational, healthcare, care, and hospitality
settings. The review underscores the theoretical differences between both
concepts, but their yet entangled use in HRI. Despite increasing interest, the
current body of empirical evidence remains limited and fragmented, underscoring
the necessity for standardised definitions, more robust operationalisations,
and further exploratory and qualitative research. By identifying existing gaps
and highlighting emerging trends, this review contributes to the development of
human-centered, autonomy-supportive robot design strategies that uphold ethical
and psychological principles, ultimately supporting well-being in human-robot
interaction.

</details>


### [65] [Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0](https://arxiv.org/abs/2509.22298)
*Felix Glawe,Laura Kremer,Luisa Vervier,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: 研究探讨了协作机器人（cobot）故障对工人信任和自主性的影响，以及透明沟通的恢复作用。


<details>
  <summary>Details</summary>
Motivation: 工业5.0强调工人福祉，信任和自主性是关键。研究旨在分析故障对这两者的影响及恢复方法。

Method: 通过VR实验（n=39）研究故障严重性和透明沟通的影响。

Result: 故障降低信任和自主性，其中信任受故障严重性更大影响；透明沟通可部分恢复两者。

Conclusion: 透明沟通是恢复信任和自主性的有效手段，对工业5.0中的人机协作设计有重要意义。

Abstract: Collaborative robots (cobots) are a core technology of Industry 4.0. Industry
4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency
and data-driven decision-making. Cobots, as cyber-physical systems, enable the
introduction of lightweight automation to smaller companies through their
flexibility, low cost and ability to work alongside humans, while keeping
humans and their skills in the loop. Industry 5.0, the evolution of Industry
4.0, places the worker at the centre of its principles: The physical and mental
well-being of the worker is the main goal of new technology design, not just
productivity, efficiency and safety standards. Within this concept, human trust
in cobots and human autonomy are important. While trust is essential for
effective and smooth interaction, the workers' perception of autonomy is key to
intrinsic motivation and overall well-being. As failures are an inevitable part
of technological systems, this study aims to answer the question of how system
failures affect trust in cobots as well as human autonomy, and how they can be
recovered afterwards. Therefore, a VR experiment (n = 39) was set up to
investigate the influence of a cobot failure and its severity on human autonomy
and trust in the cobot. Furthermore, the influence of transparent communication
about the failure and next steps was investigated. The results show that both
trust and autonomy suffer after cobot failures, with the severity of the
failure having a stronger negative impact on trust, but not on autonomy. Both
trust and autonomy can be partially restored by transparent communication.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [66] [General Pruning Criteria for Fast SBL](https://arxiv.org/abs/2509.21572)
*Jakob Möderl,Erik Leitinger,Bernard Henri Fleury*

Main category: stat.ML

TL;DR: 稀疏贝叶斯学习（SBL）通过假设权重为零均值且精度为超参数的高斯分布，估计超参数并实现权重稀疏。本文分析了在减弱高斯假设下单个超参数的边际似然，推导了超参数估计有限或无限的条件，并在高斯情况下与F-SBL的剪枝条件一致。


<details>
  <summary>Details</summary>
Motivation: 研究弱化高斯假设对稀疏贝叶斯学习（SBL）中超参数估计的影响，进一步理解SBL的稀疏性机制。

Method: 分析单超参数的边际似然函数，固定其他超参数，推导超参数估计有限或无限的充分条件。

Result: 在高斯情况下，推导的条件与F-SBL的剪枝条件一致，验证了算法的有效性。

Conclusion: 弱化高斯假设后，SBL的稀疏性条件依然成立，为理解其机制提供了新视角。

Abstract: Sparse Bayesian learning (SBL) associates to each weight in the underlying
linear model a hyperparameter by assuming that each weight is Gaussian
distributed with zero mean and precision (inverse variance) equal to its
associated hyperparameter. The method estimates the hyperparameters by
marginalizing out the weights and performing (marginalized) maximum likelihood
(ML) estimation. SBL returns many hyperparameter estimates to diverge to
infinity, effectively setting the estimates of the corresponding weights to
zero (i.e., pruning the corresponding weights from the model) and thereby
yielding a sparse estimate of the weight vector.
  In this letter, we analyze the marginal likelihood as function of a single
hyperparameter while keeping the others fixed, when the Gaussian assumptions on
the noise samples and the weight distribution that underlies the derivation of
SBL are weakened. We derive sufficient conditions that lead, on the one hand,
to finite hyperparameter estimates and, on the other, to infinite ones.
Finally, we show that in the Gaussian case, the two conditions are
complementary and coincide with the pruning condition of fast SBL (F-SBL),
thereby providing additional insights into this algorithm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan通过结合两级规划框架和用户行为日志的任务挖掘方法，解决了现有GUI自动化工具的通用性、延迟和长时程一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的规划执行代理在GUI任务自动化中存在通用性差、延迟高和长时程一致性不足的问题，需要更鲁棒和自适应的解决方案。

Method: Log2Plan采用两级规划框架，通过任务字典映射用户命令生成高级计划，并基于用户行为日志挖掘个性化模式，最后结合实时GUI上下文生成低级动作序列。

Result: 在200个真实任务上的评估显示，Log2Plan显著提高了任务成功率和执行效率，尤其在长时程任务序列中保持了60%以上的成功率。

Conclusion: Log2Plan通过结构化规划和任务挖掘，显著提升了GUI自动化的鲁棒性和适应性，适用于复杂多步骤任务。

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [68] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: EMMA框架通过DreamTransfer生成多视角一致的机器人操作视频，并结合AdaMix训练策略，显著提升了VLA模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决收集大规模真实机器人操作数据的困难，提出通过生成数据增强VLA模型的泛化能力。

Method: 提出EMMA框架，包括DreamTransfer生成多视角一致的视频和AdaMix动态调整训练权重。

Result: 生成的视频在一致性和几何保真度上优于现有方法，机器人泛化能力提升200%。

Conclusion: EMMA框架有效提升了VLA模型的泛化性能，尤其在零样本视觉任务中表现突出。

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [69] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 论文比较了四种自动驾驶系统测试方式的现实差距，重点分析了SiL、ViL、MR和真实测试的优缺点，提出了改进测试稳健性和可迁移性的建议。


<details>
  <summary>Details</summary>
Motivation: 揭示不同测试方式在现实差距（actuation、perception、behavioral fidelity）上的差异，为自动驾驶系统的验证提供更可靠的测试方法。

Method: 使用配备真实传感器的小型物理车辆及其数字孪生，在室内驾驶场景中评估SiL、ViL、MR和真实测试四种方式对两种ADS架构的影响。

Result: SiL和ViL简化了真实世界的动态和感知，MR提升了感知真实感且不影响安全与控制，并识别了不跨测试方式传递的故障条件。

Conclusion: 研究提供了每种测试方式的优缺点，为未来更稳健和可迁移的自动驾驶系统验证指明了方向。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [70] [A Parallel Ultra-Low Power Silent Speech Interface based on a Wearable, Fully-dry EMG Neckband](https://arxiv.org/abs/2509.21964)
*Fiona Meier,Giusy Spacone,Sebastian Frey,Luca Benini,Andrea Cossettini*

Main category: eess.SY

TL;DR: 本文提出了一种可穿戴、完全干式且超低功耗的EMG系统，用于无声语音识别，集成在纺织颈带中以实现舒适、非侵入性使用。系统在八种语音命令下的分类准确率分别为87%（有声）和68%（无声），并展示了在日常条件下的稳健性。


<details>
  <summary>Details</summary>
Motivation: 设计一种舒适且高效的无声语音识别系统，以满足日常生活中的无干扰使用需求。

Method: 基于BioGAP-Ultra平台，开发了14通道全差分EMG系统，通过纺织颈带实现信号采集和无线传输。采用5折交叉验证评估性能，并引入会话间变异性模拟日常使用条件。

Result: 在有声和无声实验中的平均分类准确率分别为87±3%和68±3%，在会话间变异性测试中准确率分别为64±18%和54±7%。

Conclusion: 该系统展示了高效且稳健的无声语音解码潜力，为未来可穿戴技术提供了实用方向。

Abstract: We present a wearable, fully-dry, and ultra-low power EMG system for silent
speech recognition, integrated into a textile neckband to enable comfortable,
non-intrusive use. The system features 14 fully-differential EMG channels and
is based on the BioGAP-Ultra platform for ultra-low power (22 mW) biosignal
acquisition and wireless transmission. We evaluate its performance on eight
speech commands under both vocalized and silent articulation, achieving average
classification accuracies of 87$\pm$3% and 68$\pm$3% respectively, with a
5-fold CV approach. To mimic everyday-life conditions, we introduce
session-to-session variability by repositioning the neckband between sessions,
achieving leave-one-session-out accuracies of 64$\pm$18% and 54$\pm$7% for the
vocalized and silent experiments, respectively. These results highlight the
robustness of the proposed approach and the promise of energy-efficient
silent-speech decoding.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [71] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: ShipwreckFinder是一个开源的QGIS插件，通过深度学习从多波束声纳数据中自动检测沉船。


<details>
  <summary>Details</summary>
Motivation: 沉船是海洋历史的重要标志，但传统的人工检测方法耗时且需要专业知识。

Method: 工具基于深度学习模型，结合真实数据和合成数据进行训练，提供像素级分割或边界框预测。

Result: 相比ArcGIS工具包和传统方法，ShipwreckFinder在分割性能上表现更优。

Conclusion: 该开源工具为沉船检测提供了高效解决方案，适用于历史和海洋研究。

Abstract: In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.

</details>


### [72] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: ReVQom是一种通过残差向量量化和瓶颈网络压缩多代理协作感知特征的方法，大幅减少带宽需求并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多代理协作感知中通信带宽限制的问题，提高系统的可扩展性和实用性。

Method: 使用端到端的瓶颈网络和多阶段残差向量量化（RVQ）压缩特征维度，仅传输每像素代码索引。

Result: 在DAIR-V2X数据集上，ReVQom实现了273x到1365x的压缩，且在18 bpp时性能优于原始特征方法。

Conclusion: ReVQom为多代理协作感知提供了一种高效且准确的低带宽解决方案，推动了V2X的实际部署。

Abstract: Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.

</details>


### [73] [DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation](https://arxiv.org/abs/2509.21930)
*Jiahui Wang,Changhao Chen*

Main category: cs.CV

TL;DR: DynaNav是一个动态视觉导航框架，通过自适应场景复杂度选择特征和层，显著降低了计算开销和内存使用，同时提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型（尤其是Transformer解码器）在资源紧张的场景中存在高计算开销和低可解释性问题，限制了其应用。

Method: 提出DynaNav框架，包括可训练的硬特征选择器和基于贝叶斯优化的早退机制，以动态调整操作。

Result: 在真实数据集和模拟环境中，DynaNav实现了2.26倍的FLOPs减少、42.3%的推理时间降低和32.8%的内存节省，且导航性能提升。

Conclusion: DynaNav通过高效的特征选择和早退机制，为资源受限场景下的视觉导航提供了可行解决方案。

Abstract: Visual navigation is essential for robotics and embodied AI. However,
existing foundation models, particularly those with transformer decoders,
suffer from high computational overhead and lack interpretability, limiting
their deployment in resource-tight scenarios. To address this, we propose
DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer
selection based on scene complexity. It employs a trainable hard feature
selector for sparse operations, enhancing efficiency and interpretability.
Additionally, we integrate feature selection into an early-exit mechanism, with
Bayesian Optimization determining optimal exit thresholds to reduce
computational cost. Extensive experiments in real-world-based datasets and
simulated environments demonstrate the effectiveness of DynaNav. Compared to
ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,
and 32.8% lower memory usage, while improving navigation performance across
four public datasets.

</details>


### [74] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的多模态框架，用于视频场景理解，结合了Qwen2.5-VL-3B-Instruct模型和SmolAgent协调层，在医疗机器人领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗机器人需要强大的多模态感知和推理能力以确保在动态临床环境中的安全性，而当前的视觉语言模型在时间推理和不确定性估计等方面仍有限制。

Method: 采用了Qwen2.5-VL-3B-Instruct模型和SmolAgent协调层，支持思维链推理、语音视觉融合和动态工具调用。

Result: 在Video-MME基准和自定义临床数据集上表现优于现有视觉语言模型，具有更高的准确性和鲁棒性。

Conclusion: 该框架在机器人辅助手术、患者监护和决策支持等领域具有应用潜力。

Abstract: Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.

</details>


### [75] [MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/abs/2509.22281)
*Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——任务导向的桌面场景生成，并引入了MesaTask-10K数据集和Spatial Reasoning Chain方法，通过分解生成过程提升场景与任务的契合度。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在生成任务相关桌面场景时的不足，如耗时或随机性导致的不合理问题。

Method: 提出Spatial Reasoning Chain，将生成过程分解为对象推断、空间关系推理和场景图构建，并基于LLM框架和DPO算法优化。

Result: 实验表明MesaTask在生成符合任务需求且布局合理的桌面场景上优于基线方法。

Conclusion: MesaTask通过新方法和数据集，成功实现了任务导向的桌面场景生成，具有实际应用价值。

Abstract: The ability of robots to interpret human instructions and execute
manipulation tasks necessitates the availability of task-relevant tabletop
scenes for training. However, traditional methods for creating these scenes
rely on time-consuming manual layout design or purely randomized layouts, which
are limited in terms of plausibility or alignment with the tasks. In this
paper, we formulate a novel task, namely task-oriented tabletop scene
generation, which poses significant challenges due to the substantial gap
between high-level task instructions and the tabletop scenes. To support
research on such a challenging task, we introduce MesaTask-10K, a large-scale
dataset comprising approximately 10,700 synthetic tabletop scenes with manually
crafted layouts that ensure realistic layouts and intricate inter-object
relations. To bridge the gap between tasks and scenes, we propose a Spatial
Reasoning Chain that decomposes the generation process into object inference,
spatial interrelation reasoning, and scene graph construction for the final 3D
layout. We present MesaTask, an LLM-based framework that utilizes this
reasoning chain and is further enhanced with DPO algorithms to generate
physically plausible tabletop scenes that align well with given task
descriptions. Exhaustive experiments demonstrate the superior performance of
MesaTask compared to baselines in generating task-conforming tabletop scenes
with realistic layouts. Project page is at https://mesatask.github.io/

</details>


### [76] [JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/abs/2509.22548)
*Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: 论文提出了JanusVLN，一种新颖的双隐式神经记忆框架，通过结合空间几何和视觉语义记忆，解决了现有VLN方法中的空间信息丢失和计算冗余问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLN方法依赖于显式语义记忆，导致空间信息丢失、计算冗余和内存膨胀。受人类导航中隐式场景表征的启发，提出双隐式神经记忆框架以解决这些问题。

Method: JanusVLN通过空间几何编码器增强了MLLM的3D先验知识，构建了空间几何和视觉语义的双隐式记忆，仅保留初始和滑动窗口的KV缓存以避免冗余计算。

Result: 实验表明，JanusVLN优于20多种现有方法，成功率最高提升35.5%，为VLN研究提供了新方向。

Conclusion: 双隐式神经记忆作为一种新范式，显著提升了VLN的性能，为未来研究提供了有前景的方向。

Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through
unseen environments, guided by natural language instructions and a continuous
video stream. Recent advances in VLN have been driven by the powerful semantic
understanding of Multimodal Large Language Models. However, these methods
typically rely on explicit semantic memory, such as building textual cognitive
maps or storing historical visual frames. This type of method suffers from
spatial information loss, computational redundancy, and memory bloat, which
impede efficient navigation. Inspired by the implicit scene representation in
human navigation, analogous to the left brain's semantic understanding and the
right brain's spatial cognition, we propose JanusVLN, a novel VLN framework
featuring a dual implicit neural memory that models spatial-geometric and
visual-semantic memory as separate, compact, and fixed-size neural
representations. This framework first extends the MLLM to incorporate 3D prior
knowledge from the spatial-geometric encoder, thereby enhancing the spatial
reasoning capabilities of models based solely on RGB input. Then, the
historical key-value caches from the spatial-geometric and visual-semantic
encoders are constructed into a dual implicit memory. By retaining only the KVs
of tokens in the initial and sliding window, redundant computation is avoided,
enabling efficient incremental updates. Extensive experiments demonstrate that
JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For
example, the success rate improves by 10.5-35.5 compared to methods using
multiple data types as input and by 3.6-10.8 compared to methods using more RGB
training data. This indicates that the proposed dual implicit neural memory, as
a novel paradigm, explores promising new directions for future VLN research.
Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [77] [Radio-PPG: photoplethysmogram digital twin synthesis using deep neural representation of 6G/WiFi ISAC signals](https://arxiv.org/abs/2509.22326)
*Israel Jesus Santos Filho,Muhammad Mahboob Ur Rahman,Taous-Meriem Laleg-Kirati,Tareq Al-Naffouri*

Main category: cs.IT

TL;DR: 该论文提出了一种基于6G/WiFi集成感知与通信（ISAC）系统的非接触式数字孪生（DT）光电容积脉搏波（PPG）信号合成方法，利用生成式AI模型实现了高精度的DT-PPG信号合成。


<details>
  <summary>Details</summary>
Motivation: 通过数字孪生技术实时监测生理信号，以实现早期疾病诊断和个性化治疗，同时探索6G/WiFi ISAC技术在非接触医疗筛查中的应用潜力。

Method: 使用5.23 GHz的软件定义无线电（SDR）采集64通道无线电数据，结合PPG数据，测试了离散余弦变换+多层感知机及级联U-NET模型（近似网络与细化网络）。

Result: U-NET模型实现了相对平均绝对误差0.194，ISAC感知开销仅为15.62%，且合成的DT-PPG信号在体征估计和特征提取上与参考PPG信号表现相当。

Conclusion: 该研究展示了生成式AI与6G/WiFi ISAC技术在非接触式医疗筛查（如新冠、心血管疾病）中的潜力，为特殊需求人群的健康评估奠定了基础。

Abstract: Digital twins for 1D bio-signals enable real-time monitoring of physiological
processes of a person, which enables early disease diagnosis and personalized
treatment. This work introduces a novel non-contact method for digital twin
(DT) photoplethysmogram (PPG) signal synthesis under the umbrella of 6G/WiFi
integrated sensing and communication (ISAC) systems. We employ a
software-defined radio (SDR) operating at 5.23 GHz that illuminates the chest
of a nearby person with a wideband 6G/WiFi signal and collects the reflected
signals. This allows us to acquire Radio-PPG dataset that consists of 300
minutes worth of near synchronous 64-channel radio data, PPG data, along with
the labels (three body vitals) of 30 healthy subjects. With this, we test two
artificial intelligence (AI) models for DT-PPG signal synthesis: i) discrete
cosine transform followed by a multi-layer perceptron, ii) two U-NET models
(Approximation network, Refinement network) in cascade, along with a custom
loss function. Experimental results indicate that U-NET model achieves an
impressive relative mean absolute error of 0.194 with a small ISAC sensing
overhead of 15.62%, for DT-PPG synthesis. Furthermore, we performed quality
assessment of the synthetic DT-PPG by computing the accuracy of DT-PPG-based
vitals estimation and feature extraction, which turned out to be at par with
that of reference PPG-based vitals estimation and feature extraction. This work
highlights the potential of generative AI and 6G/WiFi ISAC technologies and
serves as a foundational step towards the development of non-contact screening
tools for covid-19, cardiovascular diseases and well-being assessment of people
with special needs.

</details>


### [78] [UAV-Enabled Fluid Antenna Systems for Multi-Target Wireless Sensing over LAWCNs](https://arxiv.org/abs/2509.22497)
*Xuhui Zhang,Wenchao Liu,Chunjie Wang,Jinke Ren,Huijun Xing,Shuqiang Wang,Yanyan Shen*

Main category: cs.IT

TL;DR: 论文研究了无人机驱动的流体天线系统（FAS）在低空经济任务中的应用，提出了一种高效算法优化无人机轨迹和天线位置，显著提高了多目标无线传感的精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 未来无线系统中，流体天线系统（FAS）因其空间灵活性和传感精度优势成为关键技术，本文针对低空无线消费网络（LAWCNs）中的多目标无线传感需求，提出无人机驱动的FAS解决方案。

Method: 通过交替优化（AO）算法，联合优化无人机轨迹、发射和接收流体天线的位置以及无人机的发射波束成形，以最小化多目标估计的Cramér-Rao界（CRB）。

Result: 仿真结果表明，相较于传统固定天线方案，提出的系统在估计精度和传感可靠性上有显著提升，并通过灵活的FAS天线重定位有效抑制干扰。

Conclusion: 无人机驱动的FAS系统通过自适应轨迹设计和波束成形，在低空无线消费网络中展现出精准传感的实际潜力。

Abstract: Fluid antenna system (FAS) is emerging as a key technology for enhancing
spatial flexibility and sensing accuracy in future wireless systems. This paper
investigates an unmanned aerial vehicle (UAV)-enabled FAS for multi-target
wireless sensing in low-altitude wireless consumer networks (LAWCNs) for
achieving the low-altitude economy (LAE) missions. We formulate an optimization
problem aimed at minimizing the average Cram\'er-Rao bound (CRB) for multiple
target estimations. To tackle this non-convex problem, an efficient alternating
optimization (AO) algorithm is proposed, which jointly optimizes the UAV
trajectory, the antenna position of the transmit fluid antennas (FAs) and the
receive FAs, and the transmit beamforming at the UAV. Simulation results
demonstrate significant performance improvements in estimation accuracy and
sensing reliability compared to conventional schemes, e.g., the fixed position
antenna scheme. The proposed system achieves enhanced sensing performance
through adaptive trajectory design and beamforming, alongside effective
interference suppression via the flexible FAS antenna repositioning,
underscoring its practical potential for precision sensing in the UAV-enabled
LAWCNs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [79] [Learning to Ball: Composing Policies for Long-Horizon Basketball Moves](https://arxiv.org/abs/2509.22442)
*Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu*

Main category: cs.GR

TL;DR: 本文提出了一种新颖的策略集成框架，用于在多阶段长视野任务中无缝组合差异大的运动技能，并通过高级软路由器实现子任务间的稳健过渡。


<details>
  <summary>Details</summary>
Motivation: 解决多阶段长视野任务中策略组合与过渡的挑战，特别是在目标不明确的过渡子任务中。

Method: 提出策略集成框架和高层次软路由器，以实现不同运动技能的组合和子任务间的无缝过渡。

Result: 在篮球技能和复杂过渡任务中验证了框架的有效性，训练出的策略无需依赖球的轨迹参考。

Conclusion: 该方法在多阶段长视野任务中表现出色，能够有效组合技能并实现稳健过渡。

Abstract: Learning a control policy for a multi-phase, long-horizon task, such as
basketball maneuvers, remains challenging for reinforcement learning approaches
due to the need for seamless policy composition and transitions between skills.
A long-horizon task typically consists of distinct subtasks with well-defined
goals, separated by transitional subtasks with unclear goals but critical to
the success of the entire task. Existing methods like the mixture of experts
and skill chaining struggle with tasks where individual policies do not share
significant commonly explored states or lack well-defined initial and terminal
states between different phases. In this paper, we introduce a novel policy
integration framework to enable the composition of drastically different motor
skills in multi-phase long-horizon tasks with ill-defined intermediate states.
Based on that, we further introduce a high-level soft router to enable seamless
and robust transitions between the subtasks. We evaluate our framework on a set
of fundamental basketball skills and challenging transitions. Policies trained
by our approach can effectively control the simulated character to interact
with the ball and accomplish the long-horizon task specified by real-time user
commands, without relying on ball trajectory references.

</details>
