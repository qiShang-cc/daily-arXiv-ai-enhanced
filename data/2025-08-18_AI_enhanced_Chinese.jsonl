{"id": "2508.11029", "pdf": "https://arxiv.org/pdf/2508.11029", "abs": "https://arxiv.org/abs/2508.11029", "authors": ["Yuchen Zhang", "Francis Soualle", "Musa Furkan Keskin", "Yuan Liu", "Linlong Wu", "Jos\u00e9 A. del Peral-Rosado", "Bhavani Shankar M. R.", "Gonzalo Seco-Granados", "Henk Wymeersch", "Tareq Y. Al-Naffouri"], "title": "Distributed Integrated Sensing, Localization, and Communications over LEO Satellite Constellations", "categories": ["eess.SP"], "comment": "This paper has been submitted to IEEE for possible publication", "summary": "Low Earth orbit (LEO) satellite constellations are rapidly becoming essential\nenablers of next-generation wireless systems, offering global broadband access,\nhigh-precision localization, and reliable sensing beyond terrestrial coverage.\nHowever, the inherent limitations of individual LEO satellites, including\nrestricted power, limited antenna aperture, and constrained onboard processing,\nhinder their ability to meet the growing demands of 6G applications. To address\nthese challenges, this article introduces the concept of distributed integrated\nsensing, localization, and communication (DISLAC) over LEO constellations,\ninspired by distributed multiple input multiple output architectures. By\nenabling inter-satellite cooperation through inter-satellite links, DISLAC can\nsubstantially improve throughput, positioning accuracy, and sensing robustness.\nWe present illustrative case studies that quantify these benefits and analyze\nkey system-level considerations, including synchronization, antenna\nreconfigurability, and ISL design. The article concludes by outlining open\nresearch directions to advance the practical deployment of DISLAC in future\nnon-terrestrial networks.", "AI": {"tldr": "LEO\u536b\u661f\u661f\u5ea7\u57286G\u7cfb\u7edf\u4e2d\u9762\u4e34\u529f\u7387\u548c\u5929\u7ebf\u9650\u5236\uff0c\u6587\u7ae0\u63d0\u51faDISLAC\u6982\u5ff5\uff0c\u901a\u8fc7\u536b\u661f\u95f4\u534f\u4f5c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LEO\u536b\u661f\u57286G\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "method": "\u5f15\u5165DISLAC\u6982\u5ff5\uff0c\u5229\u7528\u536b\u661f\u95f4\u94fe\u63a5\u5b9e\u73b0\u534f\u4f5c\u3002", "result": "\u63d0\u5347\u541e\u5410\u91cf\u3001\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u611f\u77e5\u9c81\u68d2\u6027\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63a8\u52a8DISLAC\u5728\u672a\u6765\u975e\u5730\u9762\u7f51\u7edc\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2508.11132", "pdf": "https://arxiv.org/pdf/2508.11132", "abs": "https://arxiv.org/abs/2508.11132", "authors": ["Sangwon Jo", "Seok-Hwan Park"], "title": "Multi-Satellite Cooperative MIMO Transmission: Statistical CSI-Aware RSMA Precoding Design", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "accepted for publication in IEEE Wireless Communications Letters", "summary": "We investigate inter-satellite cooperative transmission in a multiple\nlow-Earth orbit (LEO) satellite communication system to enhance spectral\nefficiency. Specifically, we design multiple-input multipleoutput (MIMO)\nprecoding at LEO satellites for cooperative rate-splitting multiple access\n(RSMA). Given the difficulty of acquiring instantaneous channel state\ninformation (iCSI) due to long delays and Doppler effects, we formulate an\nergodic max-min fairness rate (MMFR) maximization problem based on statistical\nCSI (sCSI). To address the challenge of ergodic rate evaluation, we approximate\nthe problem using closed-form upper bounds and develop a weighted minimum mean\nsquared error-based algorithm to obtain a stationary point. Simulation results\ndemonstrate that the proposed sCSI-based RSMA scheme approaches iCSI-based\nperformance and significantly outperforms conventional space-division multiple\naccess.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u9897\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u4e4b\u95f4\u7684\u534f\u540c\u4f20\u8f93\uff0c\u4ee5\u63d0\u9ad8\u9891\u8c31\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u9053\u4fe1\u606f\uff08sCSI\uff09\u7684\u534f\u540c\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165\uff08RSMA\uff09\u65b9\u6848\u3002", "motivation": "\u7531\u4e8e\u957f\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u6548\u5e94\uff0c\u83b7\u53d6\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08iCSI\uff09\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u9053\u4fe1\u606f\u7684\u65b9\u6848\u6765\u4f18\u5316\u591aLEO\u536b\u661f\u7684\u534f\u540c\u4f20\u8f93\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eMIMO\u9884\u7f16\u7801\u7684\u534f\u540cRSMA\u65b9\u6848\uff0c\u901a\u8fc7\u8fd1\u4f3c\u95ed\u5f0f\u4e0a\u754c\u548c\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u7b97\u6cd5\u6c42\u89e3\u6700\u5927-\u6700\u5c0f\u516c\u5e73\u901f\u7387\uff08MMFR\uff09\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684sCSI-RSMA\u65b9\u6848\u63a5\u8fd1iCSI\u6027\u80fd\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u7a7a\u5206\u591a\u5740\u63a5\u5165\uff08SDMA\uff09\u3002", "conclusion": "\u57fa\u4e8e\u7edf\u8ba1\u4fe1\u9053\u4fe1\u606f\u7684RSMA\u65b9\u6848\u5728\u591aLEO\u536b\u661f\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11178", "pdf": "https://arxiv.org/pdf/2508.11178", "abs": "https://arxiv.org/abs/2508.11178", "authors": ["Yida Zhang", "Qiuyan Liu", "Qiang Wang", "Hongtao Luo", "Yuqi Xia"], "title": "Near-Field Variable-Width Beam Coverage and Codebook Design for XL-RIS", "categories": ["eess.SP"], "comment": null, "summary": "To mitigate the issue of limited base station coverage caused by severe\nhigh-frequency electromagnetic wave attenuation, Extremely Large Reconfigurable\nIntelligent Surface (XL-RIS) has garnered significant attention due to its high\nbeam gain. However, XL-RIS exhibits a narrower beam width compared to\ntraditional RIS, which increases the complexity of beam alignment and\nbroadcast. To address this problem, we propose a variable-width beam generation\nalgorithm under the near-field assumption and apply it to the near-field\ncodebook design for XL-RIS. Our algorithm can achieve beam coverage for\narbitrarily shaped codeword regions and generate a joint codebook for the\nmulti-XL-RIS system. The simulation results demonstrate that our proposed\nscheme enables user equipment (UE) to achieve higher spectral efficiency and\nlower communication outage probability within the codeword region compared to\nexisting works. Furthermore, our scheme exhibits better robustness to codeword\nregion location and area variations.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3XL-RIS\u6ce2\u675f\u5bbd\u5ea6\u8f83\u7a84\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u573a\u5047\u8bbe\u4e0b\u7684\u53ef\u53d8\u5bbd\u5ea6\u6ce2\u675f\u751f\u6210\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eXL-RIS\u7684\u8fd1\u573a\u7801\u672c\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u4efb\u610f\u5f62\u72b6\u7801\u5b57\u533a\u57df\u7684\u6ce2\u675f\u8986\u76d6\uff0c\u5e76\u4e3a\u591aXL-RIS\u7cfb\u7edf\u751f\u6210\u8054\u5408\u7801\u672c\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u4f5c\uff0c\u8be5\u65b9\u6848\u80fd\u63d0\u9ad8\u7528\u6237\u8bbe\u5907\u5728\u7801\u5b57\u533a\u57df\u5185\u7684\u9891\u8c31\u6548\u7387\u5e76\u964d\u4f4e\u901a\u4fe1\u4e2d\u65ad\u6982\u7387\u3002", "motivation": "\u9488\u5bf9XL-RIS\u6ce2\u675f\u5bbd\u5ea6\u8f83\u7a84\u5bfc\u81f4\u6ce2\u675f\u5bf9\u51c6\u548c\u5e7f\u64ad\u590d\u6742\u5ea6\u589e\u52a0\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u573a\u5047\u8bbe\u4e0b\u7684\u53ef\u53d8\u5bbd\u5ea6\u6ce2\u675f\u751f\u6210\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eXL-RIS\u8fd1\u573a\u7801\u672c\u8bbe\u8ba1\uff0c\u652f\u6301\u591aXL-RIS\u7cfb\u7edf\u7684\u8054\u5408\u7801\u672c\u751f\u6210\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u8bbe\u5907\u7684\u9891\u8c31\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u901a\u4fe1\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u5bf9\u7801\u5b57\u533a\u57df\u7684\u4f4d\u7f6e\u548c\u9762\u79ef\u53d8\u5316\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u89e3\u51b3XL-RIS\u6ce2\u675f\u5bbd\u5ea6\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u901a\u4fe1\u573a\u666f\u3002"}}
{"id": "2508.11186", "pdf": "https://arxiv.org/pdf/2508.11186", "abs": "https://arxiv.org/abs/2508.11186", "authors": ["Mohammad Alikhani"], "title": "KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network", "categories": ["eess.SP"], "comment": null, "summary": "Human Activity Recognition (HAR) plays a critical role in numerous\napplications, including healthcare monitoring, fitness tracking, and smart\nenvironments. Traditional deep learning (DL) approaches, while effective, often\nrequire extensive parameter tuning and may lack interpretability. In this work,\nwe investigate the use of a single three-axis accelerometer and the\nKolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model\ncomplex nonlinear relationships with improved interpretability and parameter\nefficiency. The MotionSense dataset, containing smartphone-based motion sensor\nsignals across various physical activities, is employed to evaluate the\nproposed approach. Our methodology involves preprocessing and normalization of\naccelerometer and gyroscope data, followed by KAN-based feature learning and\nclassification. Experimental results demonstrate that the KAN achieves\ncompetitive or superior classification performance compared to conventional\ndeep neural networks, while maintaining a significantly reduced parameter\ncount. This highlights the potential of KAN architectures as an efficient and\ninterpretable alternative for real-world HAR systems. The open-source\nimplementation of the proposed framework is available at the Project's GitHub\nRepository.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5355\u4e09\u8f74\u52a0\u901f\u5ea6\u8ba1\u548cKolmogorov--Arnold\u7f51\u7edc\uff08KAN\uff09\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728HAR\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u91cf\u53c2\u6570\u8c03\u6574\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u4e86KAN\u4f5c\u4e3a\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528MotionSense\u6570\u636e\u96c6\uff0c\u9884\u5904\u7406\u5e76\u5f52\u4e00\u5316\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u6570\u636e\uff0c\u901a\u8fc7KAN\u8fdb\u884c\u7279\u5f81\u5b66\u4e60\u548c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKAN\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u540c\u65f6\u53c2\u6570\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "KAN\u67b6\u6784\u5728HAR\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.10973", "pdf": "https://arxiv.org/pdf/2508.10973", "abs": "https://arxiv.org/abs/2508.10973", "authors": ["Hongchen Wang", "Sima Zeinali Danalou", "Jiahao Zhu", "Kenneth Sulimro", "Chaewon Lim", "Smita Basak", "Aimee Tai", "Usan Siriwardana", "Jason Hattrick-Simpers", "Jay Werber"], "title": "Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "comment": null, "summary": "The development of porous polymeric membranes remains a labor-intensive\nprocess, often requiring extensive trial and error to identify optimal\nfabrication parameters. In this study, we present a fully automated platform\nfor membrane fabrication and characterization via nonsolvent-induced phase\nseparation (NIPS). The system integrates automated solution preparation, blade\ncasting, controlled immersion, and compression testing, allowing precise\ncontrol over fabrication parameters such as polymer concentration and ambient\nhumidity. The modular design allows parallel processing and reproducible\nhandling of samples, reducing experimental time and increasing consistency.\nCompression testing is introduced as a sensitive mechanical characterization\nmethod for estimating membrane stiffness and as a proxy to infer porosity and\nintra-sample uniformity through automated analysis of stress-strain curves. As\na proof of concept to demonstrate the effectiveness of the system, NIPS was\ncarried out with polysulfone, the green solvent PolarClean, and water as the\npolymer, solvent, and nonsolvent, respectively. Experiments conducted with the\nautomated system reproduced expected effects of polymer concentration and\nambient humidity on membrane properties, namely increased stiffness and\nuniformity with increasing polymer concentration and humidity variations in\npore morphology and mechanical response. The developed automated platform\nsupports high-throughput experimentation and is well-suited for integration\ninto self-driving laboratory workflows, offering a scalable and reproducible\nfoundation for data-driven optimization of porous polymeric membranes through\nNIPS.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u5e73\u53f0\u7528\u4e8e\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u5236\u5907\u548c\u8868\u5f81\uff0c\u901a\u8fc7\u975e\u6eb6\u5242\u8bf1\u5bfc\u76f8\u5206\u79bb\uff08NIPS\uff09\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u91cd\u590d\u7684\u819c\u5236\u5907\u4e0e\u6027\u80fd\u6d4b\u8bd5\u3002", "motivation": "\u4f20\u7edf\u591a\u5b54\u805a\u5408\u7269\u819c\u7684\u5236\u5907\u8fc7\u7a0b\u8017\u65f6\u4e14\u4f9d\u8d56\u5927\u91cf\u8bd5\u9519\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u96c6\u6210\u81ea\u52a8\u6eb6\u6db2\u5236\u5907\u3001\u5200\u7247\u6d82\u5e03\u3001\u63a7\u5236\u6d78\u6ca1\u548c\u538b\u7f29\u6d4b\u8bd5\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7NIPS\u6280\u672f\u7cbe\u786e\u63a7\u5236\u5236\u5907\u53c2\u6570\u5982\u805a\u5408\u7269\u6d53\u5ea6\u548c\u73af\u5883\u6e7f\u5ea6\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u590d\u5236\u4e86\u805a\u5408\u7269\u6d53\u5ea6\u548c\u6e7f\u5ea6\u5bf9\u819c\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8868\u660e\u81ea\u52a8\u5316\u5e73\u53f0\u5728\u63d0\u9ad8\u5236\u5907\u6548\u7387\u548c\u6570\u636e\u4e00\u81f4\u6027\u65b9\u9762\u6709\u6548\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u5e73\u53f0\u9002\u7528\u4e8e\u9ad8\u901a\u91cf\u5b9e\u9a8c\u548c\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u819c\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u590d\u7684\u57fa\u7840\u3002"}}
{"id": "2508.11234", "pdf": "https://arxiv.org/pdf/2508.11234", "abs": "https://arxiv.org/abs/2508.11234", "authors": ["Shengheng Liu", "Ningning Fu"], "title": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing", "categories": ["eess.SP"], "comment": "Already published in ACM TOSN. 27 pages, 7 figures", "summary": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e09\u8fdb\u5236ADC\uff08T-ADCs\uff09\u7684\u4f4e\u529f\u8017AIoT\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8054\u5408\u5bfc\u9891\u548c\u6570\u636e\uff08JPD\uff09\u65b9\u6848\u4f18\u5316\u4fe1\u9053\u4f30\u8ba1\uff0c\u964d\u4f4e\u529f\u8017\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u8bbe\u5907\u6570\u91cf\u6fc0\u589e\u548c\u5bf9\u6cdb\u5728\u667a\u80fd\u9700\u6c42\u7684\u589e\u957f\uff0cAIoT\u529f\u8017\u95ee\u9898\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u9ad8\u5206\u8fa8\u7387ADC\u548c\u591a\u5c04\u9891\u94fe\u7684\u9ad8\u529f\u8017\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u91c7\u7528T-ADCs\u548cJPD\u65b9\u6848\u8fdb\u884c\u4fe1\u9053\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u6539\u8fdb\u7684EM\u548c\u53d8\u5206\u63a8\u65adEM\u4f30\u8ba1\u5668\u5904\u7406\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u4fe1\u9053\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0cJPD\u65b9\u6848\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u51cf\u8f7b\u7c97\u91cf\u5316\u5bf9\u4fe1\u9053\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5bfc\u9891\u5f00\u9500\uff0cT-ADCs\u548cJPD\u65b9\u6848\u5728\u7eff\u8272AIoT\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "T-ADCs\u53caJPD\u65b9\u6848\u4e3aAIoT\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u3001\u9ad8\u6548\u7684\u667a\u80fd\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.10999", "pdf": "https://arxiv.org/pdf/2508.10999", "abs": "https://arxiv.org/abs/2508.10999", "authors": ["Yizhi Zhou", "Jie Xu", "Jiawei Xia", "Zechen Hu", "Weizi Li", "Xuan Wang"], "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a novel robust online calibration framework for\nUltra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems\n(VINS). Accurate anchor positioning, a process known as calibration, is crucial\nfor integrating UWB ranging measurements into state estimation. While several\nprior works have demonstrated satisfactory results by using robot-aided systems\nto autonomously calibrate UWB systems, there are still some limitations: 1)\nthese approaches assume accurate robot localization during the initialization\nstep, ignoring localization errors that can compromise calibration robustness,\nand 2) the calibration results are highly sensitive to the initial guess of the\nUWB anchors' positions, reducing the practical applicability of these methods\nin real-world scenarios. Our approach addresses these challenges by explicitly\nincorporating the impact of robot localization uncertainties into the\ncalibration process, ensuring robust initialization. To further enhance the\nrobustness of the calibration results against initialization errors, we propose\na tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,\nmaking the system suitable for practical applications. Simulations and\nreal-world experiments validate the improved accuracy and robustness of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u5728\u7ebf\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8eUWB\u8f85\u52a9\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684UWB\u951a\u70b9\u6821\u51c6\uff0c\u901a\u8fc7\u8003\u8651\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u548c\u4f7f\u7528SKF\u5728\u7ebf\u7ec6\u5316\u65b9\u6cd5\u63d0\u5347\u6821\u51c6\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709UWB\u951a\u70b9\u6821\u51c6\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5ffd\u7565\u673a\u5668\u4eba\u5b9a\u4f4d\u8bef\u5dee\u548c\u521d\u59cb\u731c\u6d4b\u7684\u654f\u611f\u6027\u3002", "method": "\u7ed3\u5408\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u57fa\u4e8eSKF\u7684\u7d27\u8026\u5408\u5728\u7ebf\u7ec6\u5316\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6821\u51c6\u95ee\u9898\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11259", "pdf": "https://arxiv.org/pdf/2508.11259", "abs": "https://arxiv.org/abs/2508.11259", "authors": ["Ryosuke Isono", "Shunsuke Ono"], "title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing.\n  arXiv admin note: text overlap with arXiv:2308.00500", "summary": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u878d\u5408\u6846\u67b6TSSTF\uff0c\u901a\u8fc7TGTV\u548cTGEC\u673a\u5236\u89e3\u51b3\u536b\u661f\u56fe\u50cf\u566a\u58f0\u95ee\u9898\u5e76\u4fdd\u7559\u7cbe\u7ec6\u7a7a\u95f4\u7ed3\u6784\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u56e0\u8bbe\u5907\u548c\u73af\u5883\u566a\u58f0\u800c\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u4e14\u6613\u4e22\u5931\u7a7a\u95f4\u7ec6\u8282\u3002", "method": "\u5f15\u5165TGTV\u548cTGEC\u673a\u5236\uff0c\u5206\u522b\u901a\u8fc7\u65f6\u95f4\u5f15\u5bfc\u7684\u603b\u53d8\u5dee\u548c\u8fb9\u7f18\u7ea6\u675f\u4f18\u5316\u7a7a\u95f4\u7ed3\u6784\u4fdd\u7559\u4e0e\u566a\u58f0\u9c81\u68d2\u6027\u3002", "result": "TSSTF\u5728\u65e0\u566a\u58f0\u6761\u4ef6\u4e0b\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u8350\u53c2\u6570\u4ee5\u589e\u5f3a\u5b9e\u7528\u6027\u3002", "conclusion": "TSSTF\u901a\u8fc7\u521b\u65b0\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u65f6\u7a7a\u878d\u5408\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u7ed3\u6784\u4fdd\u7559\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.11002", "pdf": "https://arxiv.org/pdf/2508.11002", "abs": "https://arxiv.org/abs/2508.11002", "authors": ["Nikolaos Gkanatsios", "Jiahe Xu", "Matthew Bronars", "Arsalan Mousavian", "Tsung-Wei Ke", "Katerina Fragkiadaki"], "title": "3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot\nmanipulation that combines flow matching for trajectory prediction with 3D\npretrained visual scene representations for learning from demonstration. 3DFA\nleverages 3D relative attention between action and visual tokens during action\ndenoising, building on prior work in 3D diffusion-based single-arm policy\nlearning. Through a combination of flow matching and targeted system-level and\narchitectural optimizations, 3DFA achieves over 30x faster training and\ninference than previous 3D diffusion-based policies, without sacrificing\nperformance. On the bimanual PerAct2 benchmark, it establishes a new state of\nthe art, outperforming the next-best method by an absolute margin of 41.4%. In\nextensive real-world evaluations, it surpasses strong baselines with up to\n1000x more parameters and significantly more pretraining. In unimanual\nsettings, it sets a new state of the art on 74 RLBench tasks by directly\npredicting dense end-effector trajectories, eliminating the need for motion\nplanning. Comprehensive ablation studies underscore the importance of our\ndesign choices for both policy effectiveness and efficiency.", "AI": {"tldr": "3DFA\u662f\u4e00\u79cd\u7ed3\u5408\u6d41\u5339\u914d\u548c3D\u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\u76843D\u7b56\u7565\u67b6\u6784\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u63a7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u6d41\u5339\u914d\u548c3D\u89c6\u89c9\u8868\u793a\u6280\u672f\uff0c\u89e3\u51b3\u73b0\u67093D\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6d41\u5339\u914d\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u7ed3\u54083D\u9884\u8bad\u7ec3\u89c6\u89c9\u573a\u666f\u8868\u793a\uff1b\u901a\u8fc73D\u76f8\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u52a8\u4f5c\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5728bimanual PerAct2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u6b21\u4f18\u65b9\u6cd5\u63d0\u534741.4%\uff0c\u5728\u73b0\u5b9e\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u53c2\u6570\u66f4\u591a\u4e14\u9884\u8bad\u7ec3\u66f4\u5f3a\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u5355\u81c2\u4efb\u52a1\u4e2d\u8fbe\u523074\u9879RLBench\u4efb\u52a1\u7684\u65b0\u7eaa\u5f55\u3002", "conclusion": "3DFA\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u63a7\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u5411\u3002"}}
{"id": "2508.11292", "pdf": "https://arxiv.org/pdf/2508.11292", "abs": "https://arxiv.org/abs/2508.11292", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Haijun Zhang"], "title": "Beyond Diagonal Reconfigurable Intelligent Surface Enabled Sensing: Cramer-Rao Bound Optimization", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "to appear in IEEE Wireless Communications Letters", "summary": "Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has\nemerged as a more flexible solution to engineer the wireless propagation\nchannels, thanks to its non-diagonal reflecting matrix. Although the gain of\nthe BD-RIS over the conventional RIS in communication has been revealed in many\nworks, its gain in 6G sensing is still unknown. This motivates us to study the\nBD-RIS assisted sensing in this letter. Specifically, we derive the Cramer-Rao\nbound (CRB) for estimating the angle-of-arrival (AOA) from the target to the\nBD-RIS under the constraint that the BD-RIS scattering matrix is unitary. To\nminimize the CRB, we develop an optimization scheme based on an adaptive\nRiemannian steepest ascent algorithm that can satisfy the non-convex unitary\nconstraint. Numerical results demonstrate that the proposed BD-RIS-assisted\ntarget localization method achieves superior sensing performance.", "AI": {"tldr": "BD-RIS\u4f5c\u4e3a\u4e00\u79cd\u975e\u5bf9\u89d2\u53cd\u5c04\u77e9\u9635\u7684\u667a\u80fd\u8868\u9762\uff0c\u6bd4\u4f20\u7edfRIS\u66f4\u7075\u6d3b\u3002\u672c\u6587\u7814\u7a76\u4e86BD-RIS\u57286G\u611f\u77e5\u4e2d\u7684\u589e\u76ca\uff0c\u63a8\u5bfc\u4e86\u5728\u6563\u5c04\u77e9\u9635\u4e3a\u9149\u77e9\u9635\u7ea6\u675f\u4e0b\u7684AOA\u4f30\u8ba1CRB\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u9ece\u66fc\u6700\u9661\u4e0a\u5347\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRIS\u5728\u901a\u4fe1\u4e2d\u7684\u6027\u80fd\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46BD-RIS\u57286G\u611f\u77e5\u4e2d\u7684\u589e\u76ca\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u672c\u6587\u63a2\u7d22\u4e86BD-RIS\u5728\u76ee\u6807\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63a8\u5bfc\u4e86\u5728BD-RIS\u6563\u5c04\u77e9\u9635\u4e3a\u9149\u77e9\u9635\u7ea6\u675f\u4e0b\u7684AOA\u4f30\u8ba1CRB\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9ece\u66fc\u6700\u9661\u4e0a\u5347\u7b97\u6cd5\u4ee5\u6ee1\u8db3\u975e\u51f8\u9149\u7ea6\u675f\u5e76\u6700\u5c0f\u5316CRB\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684BD-RIS\u8f85\u52a9\u76ee\u6807\u5b9a\u4f4d\u65b9\u6cd5\u5177\u6709\u5353\u8d8a\u7684\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "BD-RIS\u57286G\u611f\u77e5\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u76ee\u6807\u5b9a\u4f4d\u65b9\u9762\uff0c\u5176\u4f18\u5316\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.11049", "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL\u901a\u8fc7\u4ece\u591a\u6837\u5316\u7684\u8de8\u5177\u73b0\u6570\u636e\u96c6\u4e2d\u751f\u6210\u6d41\u6765\u5b66\u4e60\u901a\u7528\u4e14\u9c81\u68d2\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u73af\u5883\u53cd\u9988\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u64cd\u4f5c\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u3002GenFlowRL\u65e8\u5728\u901a\u8fc7\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u4f4e\u7ef4\u7279\u5f81\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GenFlowRL\u4ece\u751f\u6210\u7684\u6d41\u4e2d\u63d0\u53d6\u5f62\u72b6\u5956\u52b1\uff0c\u5229\u7528\u8de8\u5177\u73b0\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5b66\u4e60\u901a\u7528\u7b56\u7565\u3002", "result": "\u572810\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8de8\u5177\u73b0\u4efb\u52a1\u4e2d\uff0cGenFlowRL\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u6837\u5316\u548c\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "GenFlowRL\u901a\u8fc7\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11295", "pdf": "https://arxiv.org/pdf/2508.11295", "abs": "https://arxiv.org/abs/2508.11295", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Weifeng Zhu", "Haijun Zhang"], "title": "Optimizing Rate-CRB Performance for Beyond Diagonal Reconfigurable Intelligent Surface Enabled ISAC", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "to appear in IEEE Communications Letters", "summary": "This letter considers a beyond diagonal reconfigurable intelligent surface\n(BD-RIS) aided integrated sensing and communication (ISAC) system, where the\nBD-RIS can help a multi-antenna base station (BS) serve multiple user\nequipments (UEs) and localize a target simultaneously. We formulate an\noptimization problem that designs the BS beamforming matrix and the BD-RIS\nscattering matrix to maximize UEs' sum rate subject to a localization\nCramer-Rao bound (CRB) constraint and an additional unitary matrix constraint\nfor the scattering matrix. Because unitary matrices form a manifold, our\nproblem belongs to constrained manifold optimization. This letter proposes a\nlog-barrier based Riemannian steepest ascent method to solve this problem\neffectively. Numerical results verify the effectiveness of our algorithm and\nthe performance gain of the BD-RIS aided ISAC systems over the conventional RIS\naided ISAC systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bf9\u89d2\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08BD-RIS\uff09\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86BS\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u548cBD-RIS\u6563\u5c04\u77e9\u9635\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u7528\u6237\u7684\u603b\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9a\u4f4d\u7cbe\u5ea6\u7ea6\u675f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528BD-RIS\u6280\u672f\u540c\u65f6\u63d0\u5347\u901a\u4fe1\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u514b\u670d\u4f20\u7edfRIS\u5728ISAC\u7cfb\u7edf\u4e2d\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u4e86\u5bf9\u6570\u969c\u788d\u6cd5\u4e0e\u6d41\u5f62\u4f18\u5316\u7684Riemannian\u6700\u901f\u4e0a\u5347\u6cd5\u6765\u89e3\u51b3\u7ea6\u675f\u6d41\u5f62\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u6709\u6548\uff0c\u4e14BD-RIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfRIS\u8f85\u52a9\u7684\u7cfb\u7edf\u3002", "conclusion": "BD-RIS\u5728\u652f\u6301\u591a\u5929\u7ebf\u57fa\u7ad9\u540c\u65f6\u670d\u52a1\u7528\u6237\u548c\u5b9a\u4f4d\u76ee\u6807\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3aISAC\u7cfb\u7edf\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.11093", "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u589e\u5f3aGUIDER\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u4eba\u673a\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u5feb\u901f\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u900f\u660e\u5730\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u5b9e\u73b0\u76ee\u6807\u3002\u73b0\u6709\u7684GUIDER\u6846\u67b6\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7VLM\u548cLLM\u6784\u5efa\u8bed\u4e49\u5148\u9a8c\uff0c\u5229\u7528\u89c6\u89c9\u7ba1\u9053\uff08YOLO\u548cSegment Anything Model\uff09\u751f\u6210\u5019\u9009\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u8bc4\u5206\u52a0\u6743GUIDER\u7684\u5bfc\u822a\u548c\u64cd\u4f5c\u5c42\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9009\u62e9\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u76ee\u6807\u5e76\u6291\u5236\u65e0\u5173\u5bf9\u8c61\uff0c\u4e00\u65e6\u4fe1\u5ff5\u8d85\u8fc7\u9608\u503c\uff0c\u673a\u5668\u4eba\u5373\u53ef\u5bfc\u822a\u5230\u76ee\u6807\u533a\u57df\u5e76\u68c0\u7d22\u6240\u9700\u5bf9\u8c61\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u5728Isaac Sim\u4e2d\u4f7f\u7528Franka Emika\u673a\u68b0\u81c2\u548cRidgeback\u5e95\u5ea7\u8fdb\u884c\u5b9e\u65f6\u8f85\u52a9\u8bc4\u4f30\u3002"}}
{"id": "2508.11351", "pdf": "https://arxiv.org/pdf/2508.11351", "abs": "https://arxiv.org/abs/2508.11351", "authors": ["Haonan Lu", "Rui Meng", "Xiaodong Xu", "Yiming Liu", "Ping Zhang", "Dusit Niyato"], "title": "Important Bit Prefix M-ary Quadrature Amplitude Modulation for Semantic Communications", "categories": ["eess.SP"], "comment": null, "summary": "M-ary Quadrature Amplitude Modulation (MQAM) is a commonly used channel\nmodulation technology in wireless communication systems. To achieve dedicated\nchannel modulation for semantic communication (SemCom), we propose an\nImportant-Bit-Prefixed MQAM (IBP-MQAM) scheme and derive its approximate\nexpression of important symbol error rate (ISER) and unimportant symbol error\nrate (USER). By extracting and quantifying text semantics using Latent\nDirichlet Allocation (LDA), we verify that IBP-MQAM achieves improved\nperformance over MQAM in SemCom scenarios and further analyze the effects of\nkey system parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMQAM\u7684IBP-MQAM\u65b9\u6848\uff0c\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7LDA\u9a8c\u8bc1\u5176\u5728\u8bed\u4e49\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u4e3a\u8bed\u4e49\u901a\u4fe1\u8bbe\u8ba1\u4e13\u7528\u4fe1\u9053\u8c03\u5236\u65b9\u6848\uff0c\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u53d6\u6587\u672c\u8bed\u4e49\u5e76\u91cf\u5316\uff0c\u4f7f\u7528IBP-MQAM\u65b9\u6848\uff0c\u5206\u6790\u5173\u952e\u53c2\u6570\u5f71\u54cd\u3002", "result": "IBP-MQAM\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfMQAM\u3002", "conclusion": "IBP-MQAM\u65b9\u6848\u9002\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\uff0c\u53ef\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2508.11117", "pdf": "https://arxiv.org/pdf/2508.11117", "abs": "https://arxiv.org/abs/2508.11117", "authors": ["Xuning Yang", "Clemens Eppner", "Jonathan Tremblay", "Dieter Fox", "Stan Birchfield", "Fabio Ramos"], "title": "Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective", "categories": ["cs.RO"], "comment": "2025 Robot: Science and Systems (RSS) Workshop on Robot Evaluation\n  for the Real World", "summary": "Current vision-based robotics simulation benchmarks have significantly\nadvanced robotic manipulation research. However, robotics is fundamentally a\nreal-world problem, and evaluation for real-world applications has lagged\nbehind in evaluating generalist policies. In this paper, we discuss challenges\nand desiderata in designing benchmarks for generalist robotic manipulation\npolicies for the goal of sim-to-real policy transfer. We propose 1) utilizing\nhigh visual-fidelity simulation for improved sim-to-real transfer, 2)\nevaluating policies by systematically increasing task complexity and scenario\nperturbation to assess robustness, and 3) quantifying performance alignment\nbetween real-world performance and its simulation counterparts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u4e00\u79cd\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u673a\u5668\u4eba\u6a21\u62df\u57fa\u51c6\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u8bc4\u4f30\u6ede\u540e\uff0c\u5c24\u5176\u662f\u5728\u8bc4\u4ef7\u901a\u7528\u7b56\u7565\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e09\u70b9\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u6a21\u62df\u63d0\u5347\u8fc1\u79fb\u6548\u679c\uff1b2) \u901a\u8fc7\u589e\u52a0\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u6270\u52a8\u6765\u8bc4\u4f30\u9c81\u68d2\u6027\uff1b3) \u91cf\u5316\u73b0\u5b9e\u4e0e\u6a21\u62df\u6027\u80fd\u7684\u4e00\u81f4\u6027\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u65b9\u6cd5\u8bbe\u8ba1\u3002", "conclusion": "\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u6539\u8fdb\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6548\u679c\u3002"}}
{"id": "2508.11457", "pdf": "https://arxiv.org/pdf/2508.11457", "abs": "https://arxiv.org/abs/2508.11457", "authors": ["Hui Cao", "Rui Meng", "Xiaodong Xu", "Shujun Han", "Ping Zhang"], "title": "Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground Communication", "categories": ["eess.SP"], "comment": null, "summary": "Satellite-ground semantic communication is anticipated to serve a critical\nrole in the forthcoming 6G era. Nonetheless, task-oriented data transmission in\nsuch systems remains a formidable challenge, primarily due to the dynamic\nnature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth\nlimitations inherent to low Earth orbit (LEO) satellite channels. In response\nto these constraints, we propose an importance-aware robust semantic\ntransmission (IRST) framework, specifically designed for scenarios\ncharacterized by bandwidth scarcity and channel variability. The IRST scheme\nbegins by applying a segmentation model enhancement algorithm to improve the\ngranularity and accuracy of semantic segmentation. Subsequently, a task-driven\nsemantic selection method is employed to prioritize the transmission of\nsemantically vital content based on real-time channel state information.\nFurthermore, the framework incorporates a stack-based, SNR-aware channel codec\ncapable of executing adaptive channel coding in alignment with SNR variations.\nComparative evaluations across diverse operating conditions demonstrate the\nsuperior performance and resilience of the IRST model relative to existing\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u8981\u6027\u611f\u77e5\u7684\u9c81\u68d2\u8bed\u4e49\u4f20\u8f93\u6846\u67b6\uff08IRST\uff09\uff0c\u7528\u4e8e\u89e3\u51b36G\u65f6\u4ee3\u536b\u661f-\u5730\u9762\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u52a8\u6001SNR\u53d8\u5316\u548c\u5e26\u5bbd\u9650\u5236\u95ee\u9898\u3002", "motivation": "6G\u65f6\u4ee3\u536b\u661f-\u5730\u9762\u8bed\u4e49\u901a\u4fe1\u4e2d\uff0c\u52a8\u6001SNR\u6ce2\u52a8\u548c\u5e26\u5bbd\u9650\u5236\u5bfc\u81f4\u4efb\u52a1\u5bfc\u5411\u6570\u636e\u4f20\u8f93\u56f0\u96be\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u6a21\u578b\u589e\u5f3a\u7b97\u6cd5\u63d0\u5347\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff0c\u7ed3\u5408\u4efb\u52a1\u9a71\u52a8\u7684\u8bed\u4e49\u9009\u62e9\u65b9\u6cd5\uff0c\u4f18\u5148\u4f20\u8f93\u5173\u952e\u8bed\u4e49\u5185\u5bb9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5806\u6808\u7684SNR\u611f\u77e5\u4fe1\u9053\u7f16\u89e3\u7801\u5668\u3002", "result": "IRST\u6846\u67b6\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "IRST\u4e3a\u5e26\u5bbd\u7a00\u7f3a\u548c\u4fe1\u9053\u591a\u53d8\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bed\u4e49\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11129", "pdf": "https://arxiv.org/pdf/2508.11129", "abs": "https://arxiv.org/abs/2508.11129", "authors": ["Ryan M. Bena", "Gilbert Bahati", "Blake Werner", "Ryan K. Cosner", "Lizhi Yang", "Aaron D. Ames"], "title": "Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "Autonomous navigation through unstructured and dynamically-changing\nenvironments is a complex task that continues to present many challenges for\nmodern roboticists. In particular, legged robots typically possess manipulable\nasymmetric geometries which must be considered during safety-critical\ntrajectory planning. This work proposes a predictive safety filter: a nonlinear\nmodel predictive control (MPC) algorithm for online trajectory generation with\ngeometry-aware safety constraints based on control barrier functions (CBFs).\nCritically, our method leverages Poisson safety functions to numerically\nsynthesize CBF constraints directly from perception data. We extend the\ntheoretical framework for Poisson safety functions to incorporate temporal\nchanges in the domain by reformulating the static Dirichlet problem for\nPoisson's equation as a parameterized moving boundary value problem.\nFurthermore, we employ Minkowski set operations to lift the domain into a\nconfiguration space that accounts for robot geometry. Finally, we implement our\nreal-time predictive safety filter on humanoid and quadruped robots in various\nsafety-critical scenarios. The results highlight the versatility of Poisson\nsafety functions, as well as the benefit of CBF constrained model predictive\nsafety-critical controllers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u8f68\u8ff9\u751f\u6210\u3002\u65b9\u6cd5\u5229\u7528\u6cca\u677e\u5b89\u5168\u51fd\u6570\u4ece\u611f\u77e5\u6570\u636e\u4e2d\u5408\u6210CBF\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8fb9\u754c\u95ee\u9898\u6269\u5c55\u7406\u8bba\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651\u673a\u5668\u4eba\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u5b9e\u65f6\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u548c\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\uff0c\u817f\u5f0f\u673a\u5668\u4eba\u7684\u4e0d\u5bf9\u79f0\u51e0\u4f55\u5f62\u72b6\u9700\u8981\u88ab\u5b89\u5168\u8003\u8651\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6027\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5c06\u9759\u6001Dirichlet\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u53c2\u6570\u5316\u79fb\u52a8\u8fb9\u754c\u503c\u95ee\u9898\uff0c\u5e76\u7ed3\u5408Minkowski\u96c6\u5408\u64cd\u4f5c\u63d0\u5347\u914d\u7f6e\u7a7a\u95f4\u3002", "result": "\u5728\u53cc\u8db3\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9884\u6d4b\u6027\u5b89\u5168\u63a7\u5236\uff0c\u7ed3\u679c\u5c55\u793a\u4e86\u6cca\u677e\u5b89\u5168\u51fd\u6570\u7684\u591a\u6837\u6027\u548cCBF\u7ea6\u675f\u63a7\u5236\u5668\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u5b89\u5168\u7ea6\u675f\u7684MPC\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.11459", "pdf": "https://arxiv.org/pdf/2508.11459", "abs": "https://arxiv.org/abs/2508.11459", "authors": ["Tzu-Chi Liu", "Po-Lin Chen", "Yi-Chieh Chen", "Po-Hsun Tu", "Chih-Hua Yeh", "Mun-Chun Yeap", "Chiung-Chu Chen", "Hau-Tieng Wu"], "title": "Efficient Artifacts Removal for Adaptive Deep Brain Stimulation and a Temporal Event Localization Analysis", "categories": ["eess.SP"], "comment": "This manuscript is under review at Journal of Neural Engineering", "summary": "Adaptive deep brain stimulation (aDBS) leverages symptom-related biomarkers\nto deliver personalized neuromodulation therapy, with the potential to improve\ntreatment efficacy and reduce power consumption compared to conventional DBS.\nHowever, stimulation-induced signal contamination remains a major technical\nbarrier to advancing its clinical application. Existing artifact removal\nstrategies, both front-end and back-end, face trade-offs between artifact\nsuppression and algorithmic flexibility. Among back-end algorithms, Shrinkage\nand Manifold-based Artifact Removal using Template Adaptation (SMARTA) has\nshown promising performance in mitigating stimulus artifacts with minimal\ndistortion to local field potentials (LFPs), but its high computational demand\nand inability to handle transient direct current (DC) artifacts limit its use\nin real-time applications. To address this, we developed SMARTA+, a\ncomputationally efficient extension of SMARTA capable of suppressing both\nstimulus and transient DC artifacts while supporting flexible algorithmic\ndesign. We evaluated SMARTA+ using semi-real aDBS data and real data from\nParkinson's disease patients. Compared to SMARTA and other established methods,\nSMARTA+ achieved comparable or superior artifact removal while significantly\nreducing computation time. It preserved spectral and temporal structures,\nranging from beta band to high-frequency oscillations, and demonstrated\nrobustness across diverse stimulation protocols. Temporal event localization\nanalysis further showed improved accuracy in detecting beta bursts. These\nfindings support SMARTA+ as a promising tool for advancing real-time,\nclosed-loop aDBS systems.", "AI": {"tldr": "SMARTA+\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u6291\u5236\u523a\u6fc0\u548c\u77ac\u6001\u76f4\u6d41\u4f2a\u8ff9\uff0c\u652f\u6301\u5b9e\u65f6\u95ed\u73afaDBS\u7cfb\u7edf\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfDBS\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u9762\u4e34\u523a\u6fc0\u4fe1\u53f7\u6c61\u67d3\u7684\u6280\u672f\u969c\u788d\uff0c\u73b0\u6709\u4f2a\u8ff9\u53bb\u9664\u65b9\u6cd5\u5728\u6291\u5236\u4f2a\u8ff9\u548c\u7b97\u6cd5\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u5f00\u53d1\u4e86SMARTA+\uff0c\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684SMARTA\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u6291\u5236\u523a\u6fc0\u548c\u77ac\u6001\u76f4\u6d41\u4f2a\u8ff9\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u7684\u7b97\u6cd5\u8bbe\u8ba1\u3002", "result": "SMARTA+\u5728\u4f2a\u8ff9\u53bb\u9664\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4fe1\u53f7\u9891\u8c31\u548c\u65f6\u57df\u7ed3\u6784\u3002", "conclusion": "SMARTA+\u662f\u63a8\u8fdb\u5b9e\u65f6\u95ed\u73afaDBS\u7cfb\u7edf\u7684\u6709\u524d\u9014\u7684\u5de5\u5177\u3002"}}
{"id": "2508.11143", "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3\u662f\u4e00\u79cd\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6548\u7a33\u5b9a\u5730\u5b66\u4e60\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u7684\u957f\u671f\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u3001\u7a00\u758f\u5956\u52b1\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u52a8\u4f5c\u5206\u5757\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "method": "\u5f15\u5165AC3\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u66f4\u65b0\u89c4\u5219\u8bad\u7ec3actor\uff08\u4ec5\u5229\u7528\u6210\u529f\u8f68\u8ff9\uff09\uff0c\u5e76\u5229\u7528n\u6b65\u56de\u62a5\u548c\u81ea\u76d1\u7763\u6a21\u5757\u7a33\u5b9acritic\u5b66\u4e60\u3002", "result": "\u5728BiGym\u548cRLBench\u768425\u9879\u4efb\u52a1\u4e2d\uff0cAC3\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u5373\u53d6\u5f97\u66f4\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "AC3\u901a\u8fc7\u9488\u5bf9\u6027\u7a33\u5b9a\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.11473", "pdf": "https://arxiv.org/pdf/2508.11473", "abs": "https://arxiv.org/abs/2508.11473", "authors": ["Yuqin Liu", "Mona Jaber", "Yan Liu", "Arumugam Nallanathan"], "title": "Reducing AoI and Improving Throughput for NOMA-assisted SGF Systems: A Hierarchical Learning Approach", "categories": ["eess.SP"], "comment": null, "summary": "A non-orthogonal multiple access (NOMA) assisted semi-grant-free (SGF)\nframework is proposed to enable channel access for grant-free users (GFUs) by\nusing residual resources from grant-based users. Under this framework, the\nproblem of joint beamforming design and transmission scheduling is formulated\nto improve the system throughput and reduce the age-of-information of GFUs. The\naforementioned problem is transferred into a Markov Decision Process to model\nthe changing environment with the transmission/ waiting/ retransmission of\nGFUs. In an effort to solve the pertinent problem, firstly, a deep\nreinforcement learning (DRL) based transmission scheduling approach is proposed\nfor determining the optimal transmission probability based on the available\ntransmission slots and transmission status of GFUs. Secondly, a hierarchical\nlearning algorithm is proposed to analyze the channel state information of GBUs\nand the transmission status of GFUs, and to train an upper-level policy based\non this analysis for beamforming to achieve efficient grant-based transmission,\nwhile a lower-level policy adapts to maximize the utilization of transmission\nslots allocated by the upper-level agent. The two policies interact to improve\nchannel access and avoid collisions. Numerical results reveal that 1) The DRL\nbased transmission scheduling outperforms existing adaptive and state-dependent\nbaselines in AoI reduction, where an average\nthree-time-slots-earlier-transmission can be obtained compared to the\nstate-dependent choice, and five time slots earlier can be achieved when\ncomparing to the adaptive choice; 2) The hierarchical learning algorithm is\nable to achieve approximately a 31.82% gain while maintaining the average AoI\nof GFUs within 1.5 time slots. 3) The effectiveness of the hierarchical\nlearning scheme in NOMA-assisted SGF system is validated across scenarios with\nGFUs counts from 1-5 times of GBUs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\uff08NOMA\uff09\u7684\u534a\u65e0\u6388\u6743\uff08SGF\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6388\u6743\u7528\u6237\u7684\u5269\u4f59\u8d44\u6e90\u4e3a\u65e0\u6388\u6743\u7528\u6237\uff08GFUs\uff09\u63d0\u4f9b\u4fe1\u9053\u63a5\u5165\u3002\u901a\u8fc7\u8054\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u548c\u4f20\u8f93\u8c03\u5ea6\u4f18\u5316\u7cfb\u7edf\u541e\u5410\u91cf\u5e76\u964d\u4f4eGFU\u7684\u4fe1\u606f\u5e74\u9f84\u3002\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728AoI\u51cf\u5c11\u548c\u7cfb\u7edf\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u65e0\u6388\u6743\u7528\u6237\uff08GFUs\uff09\u7684\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\uff0c\u540c\u65f6\u5229\u7528\u6388\u6743\u7528\u6237\u7684\u5269\u4f59\u8d44\u6e90\u4e3aGFUs\u63d0\u4f9b\u9ad8\u6548\u7684\u4fe1\u9053\u63a5\u5165\u3002", "method": "1. \u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff1b2. \u63d0\u51fa\u57fa\u4e8eDRL\u7684\u4f20\u8f93\u8c03\u5ea6\u65b9\u6cd5\u4ee5\u4f18\u5316\u4f20\u8f93\u6982\u7387\uff1b3. \u8bbe\u8ba1\u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e0a\u5c42\u7b56\u7565\u8d1f\u8d23\u6ce2\u675f\u6210\u5f62\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6388\u6743\u4f20\u8f93\uff0c\u4e0b\u5c42\u7b56\u7565\u6700\u5927\u5316\u5206\u914d\u65f6\u9699\u7684\u5229\u7528\u7387\u3002", "result": "1. DRL\u8c03\u5ea6\u65b9\u6cd5\u5728AoI\u51cf\u5c11\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff1b2. \u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7ea631.82%\u7684\u6027\u80fd\u589e\u76ca\uff1b3. \u5728\u4e0d\u540cGFUs\u4e0eGBUs\u6bd4\u4f8b\u7684\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "NOMA\u8f85\u52a9\u7684SGF\u6846\u67b6\u7ed3\u5408DRL\u548c\u5206\u5c42\u5b66\u4e60\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u51cf\u5c11GFUs\u7684\u4fe1\u606f\u5e74\u9f84\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u8d44\u6e90\u3002"}}
{"id": "2508.11200", "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u6846\u67b6GASv2\uff0c\u7528\u4e8e\u624b\u672f\u4e2d\u7684\u6293\u53d6\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u95ee\u9898\uff0c\u4ec5\u9700\u5355\u5bf9\u7acb\u4f53\u76f8\u673a\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\uff08RAS\uff09\u4e2d\u7684\u6293\u53d6\u4efb\u52a1\u9700\u8981\u81ea\u52a8\u5316\u4ee5\u63d0\u9ad8\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u4f53\u59ff\u6001\u8ddf\u8e2a\u6216\u624b\u5de5\u89c6\u89c9\u7279\u5f81\u7684\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u6216\u5904\u7406\u53d8\u5f62\u7269\u4f53\uff0c\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "GASv2\u901a\u8fc7\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u67b6\u6784\u548c\u624b\u672f\u611f\u77e5\u6d41\u7a0b\u5904\u7406\u89c6\u89c9\u8f93\u5165\uff0c\u7ed3\u5408\u6df7\u5408\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u5b89\u5168\u6267\u884c\u3002\u5229\u7528\u9886\u57df\u968f\u673a\u5316\u6280\u672f\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5355\u5bf9\u7acb\u4f53\u76f8\u673a\u90e8\u7f72\u5230\u5b9e\u9645\u673a\u5668\u4eba\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGASv2\u5728\u4eff\u771f\u548c\u771f\u5b9e\u624b\u672f\u573a\u666f\u4e2d\u5747\u8fbe\u523065%\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u5939\u5177\uff0c\u9002\u5e94\u591a\u79cd\u5e72\u6270\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3001\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GASv2\u4e3a\u624b\u672f\u6293\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3aRAS\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.11489", "pdf": "https://arxiv.org/pdf/2508.11489", "abs": "https://arxiv.org/abs/2508.11489", "authors": ["Bowu Wang", "Mohamadreza Delbari", "Robin Neuder", "Alejandro Jim\u00e9nez-S\u00e1ez", "Vahid Jamali"], "title": "Liquid Crystal-Based RIS Loss-Trade-Off Analysis", "categories": ["eess.SP"], "comment": null, "summary": "Liquid crystal (LC) technology has emerged as a promising solution for large\nreconfigurable intelligent surfaces (RISs) at millimeter wave (mmWave) bands,\noffering advantages such as low power consumption, scalability, and\ncontinuously tunable phase shifts. For LC-RIS based on the delay-line\narchitecture, i.e., with dedicated phase shifters, there exists a trade-off\nbetween the maximum achievable phase-shift range and the corresponding\ninsertion loss, which has not been studied for LC-RIS-assisted wireless systems\nyet. In this paper, we investigate this trade-off where a base station (BS) and\nan RIS are configured to minimize the transmit power while satisfying a given\nquality of service (QoS) for a number of users. Simulation results reveal a\nfundamental trade-off between the total transmit power and the achievable data\nrate as a function of the LC phase-shift range.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6db2\u6676\u6280\u672f\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u5728\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u76f8\u4f4d\u504f\u79fb\u8303\u56f4\u4e0e\u63d2\u5165\u635f\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u57fa\u7ad9\u548cRIS\u914d\u7f6e\u4ee5\u6700\u5c0f\u5316\u53d1\u5c04\u529f\u7387\u7684\u65b9\u6cd5\u3002", "motivation": "\u6db2\u6676\u6280\u672f\u5728\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u4e2d\u5177\u6709\u4f4e\u529f\u8017\u548c\u53ef\u6269\u5c55\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u5176\u76f8\u4f4d\u504f\u79fb\u8303\u56f4\u4e0e\u63d2\u5165\u635f\u8017\u4e4b\u95f4\u7684\u6743\u8861\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u5ef6\u8fdf\u7ebf\u67b6\u6784\u7684LC-RIS\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u57fa\u7ad9\u548cRIS\u7684\u914d\u7f6e\uff0c\u4ee5\u6700\u5c0f\u5316\u53d1\u5c04\u529f\u7387\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u7684\u670d\u52a1\u8d28\u91cf\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u53d1\u5c04\u529f\u7387\u4e0e\u6570\u636e\u7387\u4e4b\u95f4\u5b58\u5728\u57fa\u4e8eLC\u76f8\u4f4d\u504f\u79fb\u8303\u56f4\u7684\u57fa\u672c\u6743\u8861\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86LC-RIS\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u504f\u79fb\u8303\u56f4\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.11204", "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u65b9\u6cd5\uff08MEA\uff09\uff0c\u901a\u8fc7\u591a\u7fa4\u7b49\u53d8\u6027\u589e\u5f3a\u63d0\u5347\u91c7\u6837\u6548\u7387\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u7b49\u8ddd\u5bf9\u79f0\u6027\uff0c\u9650\u5236\u4e86\u91c7\u6837\u6548\u7387\u7684\u63d0\u5347\u3002", "method": "\u5f15\u5165\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u7684POMDP\u6a21\u578b\uff0c\u7ed3\u5408MEA\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u548c\u4f53\u7d20\u89c6\u89c9\u8868\u793a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u975e\u7b49\u8ddd\u5bf9\u79f0\u6027\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2508.11115", "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "categories": ["cs.CV", "cs.HC", "eess.SP"], "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bbd\u5e26\u6280\u672f\u7684\u9690\u79c1\u4fdd\u62a4\u5750\u59ff\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u975e\u63a5\u89e6\u5f0f\u76d1\u6d4b\u9884\u9632\u5065\u5eb7\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u73af\u5883\u9c81\u68d2\u6027\u3002", "motivation": "\u957f\u65f6\u95f4\u4f7f\u7528\u7535\u8111\u65f6\u4e0d\u826f\u5750\u59ff\u6210\u4e3a\u516c\u5171\u5065\u5eb7\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u548c\u8212\u9002\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u5546\u7528UWB\u8bbe\u5907\u63d0\u53d6\u5750\u59ff\u7279\u5f81\uff0c\u5f00\u53d1PoseGBDT\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u548c19\u79cd\u59ff\u52bf\u7684\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe99.11%\uff0c\u4e14\u5bf9\u73af\u5883\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f4e\u6210\u672c\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u79fb\u52a8\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4e3b\u52a8\u6539\u5584\u5750\u59ff\u7ba1\u7406\u3002"}}
{"id": "2508.11232", "pdf": "https://arxiv.org/pdf/2508.11232", "abs": "https://arxiv.org/abs/2508.11232", "authors": ["Guoliang Li", "Xibin Jin", "Yujie Wan", "Chenxuan Liu", "Tong Zhang", "Shuai Wang", "Chengzhong Xu"], "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification", "categories": ["cs.RO", "cs.NI"], "comment": "9 pages, 6 figures, to appear in IEEE Network", "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fb9\u7f18\u667a\u80fd\u4e0e\u8fd1\u573a\u901a\u4fe1\u7684\u65b0\u8303\u5f0fNEEI\uff0c\u4ee5\u89e3\u51b3\u5927\u6a21\u578b\u5728\u5b9e\u65f6\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u8054\u5408\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u73b0\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u9762\u4e34\u5927\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u6311\u6218\uff0cEEI\u7ed3\u5408NFC\u53ef\u63d0\u4f9b\u9ad8\u6548\u3001\u5b89\u5168\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faNEEI\u8303\u5f0f\uff0c\u8054\u5408\u4f18\u5316EEI\u548cNFC\u529f\u80fd\uff0c\u5305\u62ec\u65e0\u7ebf\u7535\u53cb\u597d\u7684\u89c4\u5212\u3001\u5149\u675f\u805a\u7126\u548c\u534f\u4f5c\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "NEEI\u4e3a\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u4fe1\u4e0e\u8ba1\u7b97\u534f\u540c\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2508.11175", "pdf": "https://arxiv.org/pdf/2508.11175", "abs": "https://arxiv.org/abs/2508.11175", "authors": ["Ali Karimi", "Hadi Zadeh-Haghighi", "Youssef Kora", "Christoph Simon"], "title": "The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators", "categories": ["quant-ph", "cs.LG", "eess.SP"], "comment": null, "summary": "Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently\nprocess temporal data. In this work, we investigate a QRC framework based on\ntwo coupled Kerr nonlinear oscillators, a system well-suited for time-series\nprediction tasks due to its complex nonlinear interactions and potentially\nhigh-dimensional state space. We explore how its performance in time-series\nprediction depends on key physical parameters: input drive strength, Kerr\nnonlinearity, and oscillator coupling, and analyze the role of entanglement in\nimproving the reservoir's computational performance, focusing on its effect on\npredicting non-trivial time series. Using logarithmic negativity to quantify\nentanglement and normalized root mean square error (NRMSE) to evaluate\npredictive accuracy, our results suggest that entanglement provides a\ncomputational advantage on average-up to a threshold in the input\nfrequency-that persists under some levels of dissipation and dephasing. In\nparticular, we find that higher dissipation rates can enhance performance.\nWhile the entanglement advantage manifests as improvements in both average and\nworst-case performance, it does not lead to improvements in the best-case\nerror. These findings contribute to the broader understanding of quantum\nreservoirs for high performance, efficient quantum machine learning and\ntime-series forecasting.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u4e24\u4e2a\u8026\u5408\u514b\u5c14\u975e\u7ebf\u6027\u632f\u8361\u5668\u7684\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u5173\u952e\u7269\u7406\u53c2\u6570\u5bf9\u5176\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u7ea0\u7f20\u5728\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u52a8\u529b\u5b66\u5982\u4f55\u9ad8\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u63a2\u7d22\u7ea0\u7f20\u5728\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u4e2d\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u8026\u5408\u514b\u5c14\u975e\u7ebf\u6027\u632f\u8361\u5668\uff0c\u5206\u6790\u8f93\u5165\u9a71\u52a8\u5f3a\u5ea6\u3001\u514b\u5c14\u975e\u7ebf\u6027\u548c\u632f\u8361\u5668\u8026\u5408\u7b49\u53c2\u6570\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5bf9\u6570\u8d1f\u6027\u548cNRMSE\u91cf\u5316\u7ea0\u7f20\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u7ed3\u679c\u8868\u660e\u7ea0\u7f20\u5728\u8f93\u5165\u9891\u7387\u9608\u503c\u5185\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\uff0c\u4e14\u8f83\u9ad8\u8017\u6563\u7387\u53ef\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7ea0\u7f20\u4e0d\u6539\u5584\u6700\u4f73\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7406\u89e3\u3002"}}
{"id": "2508.11261", "pdf": "https://arxiv.org/pdf/2508.11261", "abs": "https://arxiv.org/abs/2508.11261", "authors": ["Shan Luo", "Nathan F. Lepora", "Wenzhen Yuan", "Kaspar Althoefer", "Gordon Cheng", "Ravinder Dahiya"], "title": "Tactile Robotics: An Outlook", "categories": ["cs.RO"], "comment": "20 pages, 2 figures, accepted to IEEE Transactions on Robotics", "summary": "Robotics research has long sought to give robots the ability to perceive the\nphysical world through touch in an analogous manner to many biological systems.\nDeveloping such tactile capabilities is important for numerous emerging\napplications that require robots to co-exist and interact closely with humans.\nConsequently, there has been growing interest in tactile sensing, leading to\nthe development of various technologies, including piezoresistive and\npiezoelectric sensors, capacitive sensors, magnetic sensors, and optical\ntactile sensors. These diverse approaches utilise different transduction\nmethods and materials to equip robots with distributed sensing capabilities,\nenabling more effective physical interactions. These advances have been\nsupported in recent years by simulation tools that generate large-scale tactile\ndatasets to support sensor designs and algorithms to interpret and improve the\nutility of tactile data. The integration of tactile sensing with other\nmodalities, such as vision, as well as with action strategies for active\ntactile perception highlights the growing scope of this field. To further the\ntransformative progress in tactile robotics, a holistic approach is essential.\nIn this outlook article, we examine several challenges associated with the\ncurrent state of the art in tactile robotics and explore potential solutions to\ninspire innovations across multiple domains, including manufacturing,\nhealthcare, recycling and agriculture.", "AI": {"tldr": "\u6587\u7ae0\u63a2\u8ba8\u4e86\u89e6\u89c9\u4f20\u611f\u6280\u672f\u7684\u591a\u6837\u6027\u4e0e\u5e94\u7528\u524d\u666f\uff0c\u6307\u51fa\u4e86\u591a\u6a21\u6001\u6574\u5408\u53ca\u4eff\u771f\u5de5\u5177\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5f53\u524d\u6311\u6218\u4e0e\u672a\u6765\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8d4b\u4e88\u673a\u5668\u4eba\u7c7b\u4f3c\u751f\u7269\u7cfb\u7edf\u7684\u89e6\u89c9\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u7d27\u5bc6\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u4e86\u591a\u79cd\u4f20\u611f\u6280\u672f\uff08\u5982\u538b\u963b\u3001\u538b\u7535\u3001\u7535\u5bb9\u3001\u78c1\u6027\u548c\u5149\u5b66\u4f20\u611f\u5668\uff09\u53ca\u4eff\u771f\u5de5\u5177\u751f\u6210\u6570\u636e\u96c6\u3002", "result": "\u89e6\u89c9\u4f20\u611f\u6280\u672f\u591a\u6837\u5316\uff0c\u7ed3\u5408\u89c6\u89c9\u7b49\u591a\u6a21\u6001\u53ca\u4e3b\u52a8\u89e6\u89c9\u611f\u77e5\u7b56\u7565\u53d6\u5f97\u8fdb\u5c55\u3002", "conclusion": "\u91c7\u7528\u6574\u4f53\u65b9\u6cd5\u63a8\u52a8\u89e6\u89c9\u673a\u5668\u4eba\u53d1\u5c55\uff0c\u89e3\u51b3\u5f53\u524d\u6311\u6218\u4ee5\u6fc0\u53d1\u591a\u9886\u57df\u521b\u65b0\u3002"}}
{"id": "2508.11312", "pdf": "https://arxiv.org/pdf/2508.11312", "abs": "https://arxiv.org/abs/2508.11312", "authors": ["Ziyi Zeng", "Yun-Hsuan Chen", "Xurong Gao", "Wenyao Zheng", "Hemmings Wu", "Zhoule Zhu", "Jie Yang", "Chengkai Wang", "Lihua Zhong", "Weiwei Cheng", "Mohamad Sawan"], "title": "Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "comment": "10 pages, 9 figures", "summary": "The impact of repetitive transcranial magnetic stimulation (rTMS) on\nmethamphetamine (METH) users' craving levels is often assessed using\nquestionnaires. This study explores the feasibility of using neural signals to\nobtain more objective results. EEG signals recorded from 20 METH-addicted\nparticipants Before and After rTMS (MBT and MAT) and from 20 healthy\nparticipants (HC) are analyzed. In each EEG paradigm, participants are shown 15\nMETH-related and 15 neutral pictures randomly, and the relative band power\n(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31\nchannels, as well as individual brain regions, is analyzed. Statistically,\nMAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as\nindicated by the power topographies. Utilizing a random forest (RF), the gamma\nRBP is identified as the optimal frequency band for distinguishing between MBT\nand HC with a 90% accuracy. The performance of classifying MAT versus HC is\nlower than that of MBT versus HC, suggesting that the efficacy of rTMS can be\nvalidated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the\nTP10 and CP2 channels dominates the classification task of MBT versus HC when\nreceiving METH-related image cues. The gamma RBP during exposure to\nMETH-related cues can serve as a biomarker for distinguishing between MBT and\nHC and for evaluating the effectiveness of rTMS. Therefore, real-time\nmonitoring of gamma RBP variations holds promise as a parameter for\nimplementing a customized closed-loop neuromodulation system for treating METH\naddiction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u795e\u7ecf\u4fe1\u53f7\uff08EEG\uff09\u8bc4\u4f30rTMS\u5bf9\u7532\u57fa\u82ef\u4e19\u80fa\uff08METH\uff09\u6210\u763e\u8005\u6e34\u671b\u6c34\u5e73\u5f71\u54cd\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0gamma\u9891\u6bb5\u7684RBP\u53ef\u4f5c\u4e3a\u533a\u5206\u6210\u763e\u8005\u548c\u5065\u5eb7\u4eba\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u9a8c\u8bc1\u4e86rTMS\u7684\u7597\u6548\u3002", "motivation": "\u4f20\u7edf\u95ee\u5377\u8bc4\u4f30rTMS\u5bf9METH\u6210\u763e\u8005\u6e34\u671b\u6c34\u5e73\u7684\u5f71\u54cd\u5b58\u5728\u4e3b\u89c2\u6027\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7EEG\u4fe1\u53f7\u63d0\u4f9b\u66f4\u5ba2\u89c2\u7684\u7ed3\u679c\u3002", "method": "\u5206\u679020\u540dMETH\u6210\u763e\u8005\u548c20\u540d\u5065\u5eb7\u4eba\u7684EEG\u4fe1\u53f7\uff0c\u5bf9\u6bd4rTMS\u524d\u540e\u7684RBP\u53d8\u5316\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u6570\u636e\u5206\u6790\u3002", "result": "gamma RBP\u662f\u533a\u5206\u6210\u763e\u8005\u548c\u5065\u5eb7\u4eba\u7684\u6700\u4f18\u9891\u6bb5\uff0c\u51c6\u786e\u7387\u8fbe90%\u3002rTMS\u540e\uff0c\u6210\u763e\u8005\u7684RBP\u66f4\u63a5\u8fd1\u5065\u5eb7\u4eba\u3002", "conclusion": "gamma RBP\u53ef\u4f5c\u4e3a\u8bc4\u4f30rTMS\u7597\u6548\u548c\u6210\u763e\u8005\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4e3a\u5b9a\u5236\u5316\u95ed\u73af\u795e\u7ecf\u8c03\u8282\u7cfb\u7edf\u63d0\u4f9b\u53c2\u6570\u3002"}}
{"id": "2508.11275", "pdf": "https://arxiv.org/pdf/2508.11275", "abs": "https://arxiv.org/abs/2508.11275", "authors": ["Masaki Murooka", "Iori Kumagai", "Mitsuharu Morisawa", "Fumio Kanehiro"], "title": "Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation", "categories": ["cs.RO"], "comment": null, "summary": "To reduce the computational cost of humanoid motion generation, we introduce\na new approach to representing robot kinematic reachability: the differentiable\nreachability map. This map is a scalar-valued function defined in the task\nspace that takes positive values only in regions reachable by the robot's\nend-effector. A key feature of this representation is that it is continuous and\ndifferentiable with respect to task-space coordinates, enabling its direct use\nas constraints in continuous optimization for humanoid motion planning. We\ndescribe a method to learn such differentiable reachability maps from a set of\nend-effector poses generated using a robot's kinematic model, using either a\nneural network or a support vector machine as the learning model. By\nincorporating the learned reachability map as a constraint, we formulate\nhumanoid motion generation as a continuous optimization problem. We demonstrate\nthat the proposed approach efficiently solves various motion planning problems,\nincluding footstep planning, multi-contact motion planning, and\nloco-manipulation planning for humanoid robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5730\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u964d\u4f4e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u51cf\u5c11\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5730\u56fe\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6216\u652f\u6301\u5411\u91cf\u673a\u5b66\u4e60\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5730\u56fe\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\u5e94\u7528\u4e8e\u8fde\u7eed\u4f18\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u4e86\u6b65\u6001\u89c4\u5212\u3001\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c-\u79fb\u52a8\u89c4\u5212\u7b49\u591a\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "conclusion": "\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5730\u56fe\u4e3a\u8fde\u7eed\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u6548\u7387\u3002"}}
{"id": "2508.11332", "pdf": "https://arxiv.org/pdf/2508.11332", "abs": "https://arxiv.org/abs/2508.11332", "authors": ["Chris Verhoek", "Ivan Markovsky", "Roland T\u00f3th"], "title": "Direct data-driven interpolation and approximation of linear parameter-varying system trajectories", "categories": ["eess.SY", "cs.SY", "eess.SP", "math.OC"], "comment": "9 pages, 5 figures, submitted for review", "summary": "We consider the problem of estimating missing values in trajectories of\nlinear parameter-varying (LPV) systems. We solve this interpolation problem for\nthe class of shifted-affine LPV systems. Conditions for the existence and\nuniqueness of solutions are given and a direct data-driven algorithm for its\ncomputation is presented, i.e., the data-generating system is not given by a\nparametric model but is implicitly specified by data. We illustrate the\napplicability of the proposed solution on illustrative examples of a\nmass-spring-damper system with exogenous and endogenous parameter variation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u53c2\u6570\u53d8\u5316\uff08LPV\uff09\u7cfb\u7edf\u4e2d\u8f68\u8ff9\u7f3a\u5931\u503c\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u4f4d\u4eff\u5c04LPV\u7cfb\u7edf\u7684\u63d2\u503c\u65b9\u6cd5\uff0c\u7ed9\u51fa\u4e86\u89e3\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u76f4\u63a5\u6570\u636e\u9a71\u52a8\u7684\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3LPV\u7cfb\u7edf\u4e2d\u8f68\u8ff9\u7f3a\u5931\u503c\u7684\u4f30\u8ba1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u751f\u6210\u7cfb\u7edf\u6ca1\u6709\u663e\u5f0f\u53c2\u6570\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u79fb\u4f4d\u4eff\u5c04LPV\u7cfb\u7edf\u7684\u63d2\u503c\u65b9\u6cd5\uff0c\u7ed9\u51fa\u4e86\u89e3\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u76f4\u63a5\u6570\u636e\u9a71\u52a8\u7684\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5f39\u7c27-\u8d28\u91cf-\u963b\u5c3c\u7cfb\u7edf\u7684\u4f8b\u5b50\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1LPV\u7cfb\u7edf\u4e2d\u7684\u7f3a\u5931\u503c\uff0c\u9002\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u573a\u666f\u3002"}}
{"id": "2508.11286", "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5f53\u524d\u573a\u666f\u56fe\u4e0e\u53c2\u8003\u573a\u666f\u56fe\uff0c\u5728\u5b50\u4efb\u52a1\u8fb9\u754c\u68c0\u6d4b\u548c\u4fee\u6b63\u9519\u8bef\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u6839\u636e\u73af\u5883\u72b6\u6001\u5b9e\u65f6\u8c03\u6574\u884c\u52a8\uff0c\u800c\u81ea\u4e3b\u673a\u5668\u4eba\u5e38\u56e0\u7f3a\u4e4f\u9002\u5e94\u6027\u5bfc\u81f4\u5931\u8d25\u3002\u73b0\u6709\u7684\u91cd\u65b0\u89c4\u5212\u65b9\u6cd5\u591a\u5728\u5931\u8d25\u540e\u624d\u54cd\u5e94\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6784\u5efa\u5f53\u524dRGB-D\u89c2\u6d4b\u7684\u573a\u666f\u56fe\u4e0e\u6210\u529f\u6f14\u793a\u7684\u53c2\u8003\u56fe\u5bf9\u6bd4\uff0c\u82e5\u4e0d\u5339\u914d\u5219\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u63a8\u7406\u6a21\u5757\u8bca\u65ad\u5e76\u8c03\u6574\u8ba1\u5212\u3002", "result": "\u5728AI2-THOR\u6a21\u62df\u5668\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u524d\u68c0\u6d4b\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3b\u52a8\u91cd\u65b0\u89c4\u5212\u6846\u67b6\u6709\u6548\u9884\u9632\u6267\u884c\u5931\u8d25\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2508.11289", "pdf": "https://arxiv.org/pdf/2508.11289", "abs": "https://arxiv.org/abs/2508.11289", "authors": ["Lin Li", "Xueming Liu", "Zhoujingzi Qiu", "Tianjiang Hu", "Qingrui Zhang"], "title": "A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation", "categories": ["cs.RO"], "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 6 Pages", "summary": "Bearing-only Target Motion Analysis (TMA) is a promising technique for\npassive tracking in various applications as a bearing angle is easy to measure.\nDespite its advantages, bearing-only TMA is challenging due to the nonlinearity\nof the bearing measurement model and the lack of range information, which\nimpairs observability and estimator convergence. This paper addresses these\nissues by proposing a Recursive Total Least Squares (RTLS) method for online\ntarget localization and tracking using mobile observers. The RTLS approach,\ninspired by previous results on Total Least Squares (TLS), mitigates biases in\nposition estimation and improves computational efficiency compared to\npseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a\ncircumnavigation controller to enhance system observability and estimator\nconvergence by guiding the mobile observer in orbit around the target.\nExtensive simulations and experiments are performed to demonstrate the\neffectiveness and robustness of the proposed method. The proposed algorithm is\nalso compared with the state-of-the-art approaches, which confirms its superior\nperformance in terms of both accuracy and stability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRecursive Total Least Squares (RTLS)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4ec5\u4f7f\u7528\u65b9\u4f4d\u4fe1\u606f\u7684TMA\u4e2d\u7684\u975e\u7ebf\u6027\u95ee\u9898\u548c\u89c2\u6d4b\u6027\u4e0d\u8db3\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed5\u884c\u63a7\u5236\u5668\u4ee5\u589e\u5f3a\u7cfb\u7edf\u89c2\u6d4b\u6027\u548c\u4f30\u8ba1\u5668\u6536\u655b\u6027\u3002", "motivation": "\u4ec5\u4f7f\u7528\u65b9\u4f4d\u4fe1\u606f\u7684TMA\u5728\u88ab\u52a8\u8ddf\u8e2a\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u6d4b\u91cf\u6a21\u578b\u7684\u975e\u7ebf\u6027\u548c\u8ddd\u79bb\u4fe1\u606f\u7684\u7f3a\u5931\uff0c\u5bfc\u81f4\u89c2\u6d4b\u6027\u5dee\u548c\u4f30\u8ba1\u5668\u6536\u655b\u56f0\u96be\u3002", "method": "\u63d0\u51faRTLS\u65b9\u6cd5\u7528\u4e8e\u5728\u7ebf\u76ee\u6807\u5b9a\u4f4d\u548c\u8ddf\u8e2a\uff0c\u5e76\u7ed3\u5408\u7ed5\u884c\u63a7\u5236\u5668\u4ee5\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RTLS\u65b9\u6cd5\u7ed3\u5408\u7ed5\u884c\u63a7\u5236\u5668\u5728\u4ec5\u65b9\u4f4dTMA\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2508.11396", "pdf": "https://arxiv.org/pdf/2508.11396", "abs": "https://arxiv.org/abs/2508.11396", "authors": ["Jingran Zhang", "Zhengzhang Yan", "Yiming Chen", "Zeqiang He", "Jiahao Chen"], "title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728GPS\u7f3a\u5931\u73af\u5883\u4e0b\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u4f4e\u6210\u672c\u60ef\u6027\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7684IMU\u6d4b\u91cf\u548c\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u53cc\u8db3\u673a\u5668\u4eba\u9700\u8981\u53ef\u9760\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8c03\u53c2\u590d\u6742\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6613\u4e8e\u8c03\u53c2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528IMU\u5728\u652f\u6491\u811a\u65f6\u7684\u9759\u6b62\u4f2a\u6d4b\u91cf\u63d0\u4f9b\u521b\u65b0\uff0c\u7ed3\u5408\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\u8fdb\u884c\u9884\u6d4b\u3002\u7406\u8bba\u90e8\u5206\u57fa\u4e8e\u77e9\u9635\u674e\u7fa4\u5c55\u5f00\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInEKF\u5728\u8fd0\u52a8\u6355\u6349\u3001\u5927\u89c4\u6a21\u591a\u697c\u5c42\u6b65\u884c\u53ca\u53cc\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6EKF\uff0c\u4e14\u8c03\u53c2\u66f4\u7b80\u5355\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728GPS\u7f3a\u5931\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u9ad8\u53ef\u884c\u6027\u548c\u6613\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.11404", "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "AI\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7ed3\u5408\u7684HRC\u65b9\u6cd5\u5728\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u67e5\u4e2d\u6bd4\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u68c0\u67e5\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3001\u9ad8\u8ba4\u77e5\u8d1f\u62c5\u548c\u6f5c\u5728\u8bef\u5dee\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u68c0\u67e5\u65b9\u6cd5\u3002", "method": "\u5c06AI\u8f85\u52a9\u89c6\u89c9\u88c2\u7eb9\u68c0\u6d4b\u96c6\u6210\u5230\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0Jackal\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "HRC\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u67e5\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "AI\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u5728\u6838\u8bbe\u65bd\u68c0\u67e5\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u6027\u80fd\u6f5c\u529b\u3002"}}
{"id": "2508.11406", "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u548c\u4e00\u4e2a\u4e91\u5e73\u53f0\uff0c\u7528\u4e8e\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u79d1\u5b66\u5b9e\u9a8c\u3002", "motivation": "\u63a8\u52a8\u673a\u5668\u4eba\u79d1\u5b66\u5b9e\u9a8c\u7684\u900f\u660e\u5ea6\u3001\u53ef\u590d\u73b0\u6027\u548c\u5f00\u653e\u6027\uff0c\u4ee5\u4fbf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u5f00\u53d1\u8bed\u4e49\u6267\u884c\u8ffd\u8e2a\u6846\u67b6\u548cAICOR\u865a\u62df\u7814\u7a76\u5efa\u7b51(VRB)\u5e73\u53f0\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u6570\u636e\u548c\u8bed\u4e49\u6ce8\u91ca\u3002", "result": "\u5b9e\u73b0\u4e86\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u9a71\u52a8\u79d1\u5b66\uff0c\u652f\u6301\u786e\u5b9a\u6027\u6267\u884c\u548c\u5f00\u653e\u77e5\u8bc6\u8868\u793a\u3002", "conclusion": "\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u900f\u660e\u548c\u53ef\u4fe1\u4efb\u7684\u5b9e\u9a8c\u73af\u5883\u3002"}}
{"id": "2508.11453", "pdf": "https://arxiv.org/pdf/2508.11453", "abs": "https://arxiv.org/abs/2508.11453", "authors": ["Jiayue Jin", "Lang Qian", "Jingyu Zhang", "Chuanyu Ju", "Liang Song"], "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback", "categories": ["cs.RO"], "comment": null, "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.", "AI": {"tldr": "\u63d0\u51faEvoPSF\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u89c4\u5212\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u591a\u4e3a\u79bb\u7ebf\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5728\u7ebf\u9002\u5e94\u65b0\u73af\u5883\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u89c4\u5212\u5931\u8d25\u7684\u53cd\u9988\u4fe1\u53f7\uff08\u89c4\u5212\u5668\u4e0d\u786e\u5b9a\u6027\uff09\u89e6\u53d1\u5728\u7ebf\u8fdb\u5316\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u635f\u5931\u9488\u5bf9\u5173\u952e\u5bf9\u8c61\u5fae\u8c03\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvoPSF\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8de8\u533a\u57df\u548c\u5f02\u5e38\u6761\u4ef6\u4e0b\u3002", "conclusion": "EvoPSF\u901a\u8fc7\u5728\u7ebf\u8fdb\u5316\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u52a8\u6001\u73af\u5883\u7684\u9002\u5e94\u80fd\u529b\u548c\u89c4\u5212\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.11479", "pdf": "https://arxiv.org/pdf/2508.11479", "abs": "https://arxiv.org/abs/2508.11479", "authors": ["Tatiana Zemskova", "Aleksei Staroverov", "Dmitry Yudin", "Aleksandr Panov"], "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOVSegDT\u7684\u8f7b\u91cf\u7ea7Transformer\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u652f\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u78b0\u649e\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u7b56\u7565\u5728\u5c0f\u89c4\u6a21\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u4e0d\u5b89\u5168\uff08\u9891\u7e41\u78b0\u649e\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "method": "OVSegDT\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8bed\u4e49\u5206\u652f\uff08\u5305\u62ec\u76ee\u6807\u4e8c\u8fdb\u5236\u63a9\u7801\u7f16\u7801\u5668\u548c\u8f85\u52a9\u5206\u5272\u635f\u5931\u51fd\u6570\uff09\u548c\u71b5\u81ea\u9002\u5e94\u635f\u5931\u8c03\u5236\uff08\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u548c\u5f3a\u5316\u4fe1\u53f7\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u8bad\u7ec3\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u4e8633%\uff0c\u78b0\u649e\u6b21\u6570\u51cf\u5c11\u4e86\u4e00\u534a\uff0c\u5e76\u5728HM3D-OVON\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0840.1% SR\uff0c20.9% SPL\uff09\u3002", "conclusion": "OVSegDT\u5728\u4e0d\u4f7f\u7528\u6df1\u5ea6\u3001\u91cc\u7a0b\u8ba1\u6216\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.11485", "pdf": "https://arxiv.org/pdf/2508.11485", "abs": "https://arxiv.org/abs/2508.11485", "authors": ["Hailiang Tang", "Tisheng Zhang", "Liqiang Wang", "Xin Ding", "Man Yuan", "Zhiyu Xiang", "Jujin Chen", "Yuhan Bian", "Shuangyan Liu", "Yuqing Wang", "Guan Wang", "Xiaoji Niu"], "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping", "categories": ["cs.RO"], "comment": "10 pages, 12 figures", "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.", "AI": {"tldr": "i2Nav-Robot\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5ba4\u5185\u5916\u73af\u5883\u4e2dUGV\u5bfc\u822a\u4e0e\u5730\u56fe\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u914d\u7f6e\u3001\u65f6\u95f4\u540c\u6b65\u548c\u573a\u666f\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709UGV\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u5bfc\u822a\u4e0e\u5730\u56fe\u6784\u5efa\u6280\u672f\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u5f00\u53d1i2Nav-Robot\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6574\u5408\u591a\u79cd\u6700\u65b0\u4f20\u611f\u5668\uff08\u5982\u56fa\u6001LiDAR\u30014D\u96f7\u8fbe\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u540c\u6b65\u4e0e\u79bb\u7ebf\u6821\u51c6\u786e\u4fdd\u65f6\u95f4\u540c\u6b65\uff0c\u8986\u76d6\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b17060\u7c73\u7684\u5e8f\u5217\uff0c\u5398\u7c73\u7ea7\u7cbe\u5ea6\u5730\u9762\u771f\u503c\uff0c\u5e76\u88ab\u591a\u4e2a\u5f00\u6e90\u7cfb\u7edf\u9a8c\u8bc1\u5176\u6570\u636e\u8d28\u91cf\u4f18\u8d8a\u3002", "conclusion": "i2Nav-Robot\u4e3aUGV\u5bfc\u822a\u4e0e\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2508.11492", "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6781\u5750\u6807\u7684\u65b0\u65b9\u6cd5Polaris\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u4e0e\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u5728\u5efa\u6a21\u76f8\u5bf9\u8ddd\u79bb\u548c\u65b9\u5411\u65f6\u7684\u4e0d\u8db3\uff0c\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u7f16\u7801\u5730\u56fe\u548c\u4ee3\u7406\u4f4d\u7f6e\uff0c\u672a\u80fd\u81ea\u7136\u6355\u6349\u4e0d\u540c\u4ea4\u901a\u5143\u7d20\u56e0\u8ddd\u79bb\u548c\u65b9\u5411\u53d8\u5316\u7684\u52a8\u6001\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6781\u5750\u6807\u7cfb\u7edf\uff0c\u63d0\u51faPolaris\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u5f84\u548c\u89d2\u5ea6\u5efa\u6a21\u7a7a\u95f4\u53d8\u5316\u548c\u76f8\u5bf9\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u7f16\u7801\u548c\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728Argoverse 2\u548cnuPlan\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPolaris\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6781\u5750\u6807\u7cfb\u7edf\u66f4\u76f4\u89c2\u4e14\u6709\u6548\u5730\u5efa\u6a21\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.11498", "pdf": "https://arxiv.org/pdf/2508.11498", "abs": "https://arxiv.org/abs/2508.11498", "authors": ["Agnes Bressan de Almeida", "Joao Aires Correa Fernandes Marsicano"], "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language", "categories": ["cs.RO"], "comment": null, "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.", "AI": {"tldr": "Swarm in Blocks \u662f\u4e00\u4e2a\u57fa\u4e8e\u5757\u7684\u8bed\u8a00\u63a5\u53e3\uff0c\u7528\u4e8e\u7b80\u5316\u65e0\u4eba\u673a\u7fa4\u7f16\u7a0b\uff0c\u6700\u521d\u4e3aCopterHack 2022\u5f00\u53d1\uff0c2023\u5e74\u5347\u7ea7\u4e3a2.0\u7248\u672c\uff0c\u65e8\u5728\u4f7f\u521d\u5b66\u8005\u4e5f\u80fd\u7ba1\u7406\u65e0\u4eba\u673a\u7fa4\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7fa4\u5728\u914d\u9001\u3001\u519c\u4e1a\u548c\u76d1\u63a7\u7b49\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7ba1\u7406\u65e0\u4eba\u673a\u7fa4\u7684\u590d\u6742\u6027\u4e5f\u968f\u4e4b\u63d0\u5347\uff0c\u56e2\u961f\u5e0c\u671b\u901a\u8fc7\u5757\u7f16\u7a0b\u964d\u4f4e\u5165\u95e8\u95e8\u69db\u3002", "method": "\u57fa\u4e8eClover\u5e73\u53f0\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u7ec4\u88c5\u4ee3\u7801\u5757\u6765\u5b9e\u73b0\u5faa\u73af\u3001\u6761\u4ef6\u7ed3\u6784\u7b49\u529f\u80fd\uff0c\u65e0\u9700\u6df1\u5165\u4e86\u89e3ROS\u6216\u7f16\u7a0b\u3002", "result": "Swarm in Blocks 2.0\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5e73\u53f0\uff0c\u4f7f\u65e0\u4eba\u673a\u7fa4\u7ba1\u7406\u66f4\u52a0\u7528\u6237\u53cb\u597d\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u7f16\u7a0b\u6559\u80b2\u673a\u4f1a\u3002", "conclusion": "\u8be5\u5de5\u5177\u6210\u529f\u964d\u4f4e\u4e86\u65e0\u4eba\u673a\u7fa4\u7f16\u7a0b\u7684\u96be\u5ea6\uff0c\u4e3a\u521d\u5b66\u8005\u548c\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002"}}
{"id": "2508.11503", "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u6574\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc1\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u52a8\u6001\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5e76\u5728\u771f\u5b9e\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u4e0a\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u591a\u6837\u6027\u548c\u7a0b\u5e8f\u5316\u8bad\u7ec3\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5b66\u4e60\u578b\u63a7\u5236\u5668\u5728\u9065\u8fdc\u884c\u661f\u8868\u9762\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8f6e\u5f0f\u4e0e\u9897\u7c92\u4ecb\u8d28\u590d\u6742\u52a8\u6001\u4ea4\u4e92\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u8986\u76d6\u591a\u6837\u5316\u7684\u7a0b\u5e8f\u5316\u751f\u6210\u73af\u5883\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u540c\u65f6\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u548c\u52a8\u4f5c\u5e73\u6ed1\u6ee4\u6ce2\u5668\u7684\u6548\u679c\u3002", "result": "\u7a0b\u5e8f\u5316\u591a\u6837\u6027\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u4f18\u4e8e\u9759\u6001\u573a\u666f\u8bad\u7ec3\u7684\u4ee3\u7406\uff0c\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u5fae\u8c03\u5e26\u6765\u4f4e\u901f\u7cbe\u5ea6\u7684\u5c0f\u5e45\u63d0\u5347\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u7cfb\u7edf\u5f00\u53d1\u6d41\u7a0b\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.11520", "pdf": "https://arxiv.org/pdf/2508.11520", "abs": "https://arxiv.org/abs/2508.11520", "authors": ["Evangelos Tsiatsianas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": "8 pages, 2 figures, 4 tables, Accepted at Humanoids 2025", "summary": "Automatically generating agile whole-body motions for legged and humanoid\nrobots remains a fundamental challenge in robotics. While numerous trajectory\noptimization approaches have been proposed, there is no clear guideline on how\nthe choice of floating-base space parameterization affects performance,\nespecially for agile behaviors involving complex contact dynamics. In this\npaper, we present a comparative study of different parameterizations for direct\ntranscription-based trajectory optimization of agile motions in legged systems.\nWe systematically evaluate several common choices under identical optimization\nsettings to ensure a fair comparison. Furthermore, we introduce a novel\nformulation based on the tangent space of SE(3) for representing the robot's\nfloating-base pose, which, to our knowledge, has not received attention from\nthe literature. This approach enables the use of mature off-the-shelf numerical\nsolvers without requiring specialized manifold optimization techniques. We hope\nthat our experiments and analysis will provide meaningful insights for\nselecting the appropriate floating-based representation for agile whole-body\nmotion generation.", "AI": {"tldr": "\u5bf9\u8db3\u5f0f\u548c\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u751f\u6210\u7684\u6d6e\u52a8\u57fa\u5ea7\u7a7a\u95f4\u53c2\u6570\u5316\u9009\u62e9\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u654f\u6377\u8fd0\u52a8\u751f\u6210\u4e2d\u6d6e\u52a8\u57fa\u5ea7\u7a7a\u95f4\u53c2\u6570\u5316\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u5e38\u89c1\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSE(3)\u5207\u7a7a\u95f4\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u907f\u514d\u590d\u6742\u7684\u6d41\u5f62\u4f18\u5316\u3002", "result": "\u65b0\u65b9\u6cd5\u4f7f\u7528\u6210\u719f\u6570\u503c\u6c42\u89e3\u5668\uff0c\u65e0\u9700\u4e13\u95e8\u6d41\u5f62\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u654f\u6377\u5168\u8eab\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u6d6e\u52a8\u57fa\u5ea7\u8868\u793a\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u53c2\u8003\u3002"}}
{"id": "2508.11537", "pdf": "https://arxiv.org/pdf/2508.11537", "abs": "https://arxiv.org/abs/2508.11537", "authors": ["Han Zheng", "Zikang Zhou", "Guli Zhang", "Zhepei Wang", "Kaixuan Wang", "Peiliang Li", "Shaojie Shen", "Ming Yang", "Tong Qin"], "title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "categories": ["cs.RO"], "comment": null, "summary": "Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MultiPark\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52Transformer\u7684\u591a\u6a21\u6001\u505c\u8f66\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u5728\u505c\u8f66\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u591a\u6a21\u6001\u884c\u4e3a\u548c\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u505c\u8f66\u884c\u4e3a\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u505c\u8f66\u4efb\u52a1\u9762\u4e34\u9ad8\u5ea6\u7ea6\u675f\u7a7a\u95f4\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4e86\u505c\u8f66\u884c\u4e3a\u7684\u591a\u6a21\u6001\u6027\uff0c\u4e14\u5b58\u5728\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u505c\u8f66\u573a\u666f\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MultiPark\uff0c\u91c7\u7528\u5206\u6bb5\u9884\u6d4b\u8303\u5f0f\u5904\u7406\u6025\u8f6c\u5f2f\u8def\u5f84\uff0c\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u505c\u8f66\u67e5\u8be2\uff08\u5206\u4e3a\u6863\u4f4d\u3001\u7eb5\u5411\u548c\u6a2a\u5411\u5206\u91cf\uff09\uff0c\u5e76\u5229\u7528\u76ee\u6807\u4e2d\u5fc3\u4f4d\u59ff\u548c\u81ea\u4e2d\u5fc3\u78b0\u649e\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u4ee5\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\u3002", "result": "\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMultiPark\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u8f66\u8f86\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "MultiPark\u6210\u529f\u89e3\u51b3\u4e86\u505c\u8f66\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u884c\u4e3a\u548c\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u505c\u8f66\u73af\u5883\u3002"}}
{"id": "2508.11547", "pdf": "https://arxiv.org/pdf/2508.11547", "abs": "https://arxiv.org/abs/2508.11547", "authors": ["Martin Jirou\u0161ek", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u6807\u51c6\u4f20\u611f\u5668\u8ddf\u8e2a\u7f06\u7ef3\u60ac\u6302\u8d1f\u8f7d\u4f4d\u7f6e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u52a8\u529b\u5b66\u5efa\u6a21\u548c\u5148\u8fdb\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u771f\u5b9e\u6d4b\u91cf\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u643a\u5e26\u7f06\u7ef3\u60ac\u6302\u8d1f\u8f7d\u65f6\u7684\u4f4d\u7f6e\u8ddf\u8e2a\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u989d\u5916\u786c\u4ef6\u6216\u590d\u6742\u7cfb\u7edf\u7684\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6807\u51c6\u4f20\u611f\u5668\uff08RTK-GNSS\u548cIMU\uff09\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u8026\u5408\u52a8\u529b\u5b66\u6a21\u578b\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3002", "result": "\u4eff\u771f\u663e\u793a\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6d4b\u91cf\uff08\u8bef\u5dee<6%\uff09\uff0c\u5bf9\u8d1f\u8f7d\u53c2\u6570\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u65e0\u9700\u989d\u5916\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u9002\u5408\u6237\u5916\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.11573", "pdf": "https://arxiv.org/pdf/2508.11573", "abs": "https://arxiv.org/abs/2508.11573", "authors": ["Mogens Plessen"], "title": "Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "14 pages plus 7 pages appendix with additional figures, 18 main\n  figures, 3 tables", "summary": "Automatic Section Control (ASC) is a long-standing trend for spraying in\nagriculture. It promises to minimise spray overlap areas. The core idea is to\n(i) switch off spray nozzles on areas that have already been sprayed, and (ii)\nto dynamically adjust nozzle flow rates along the boom bar that holds the spray\nnozzles when velocities of boom sections vary during turn maneuvers. ASC is not\npossible without sensors, in particular for accurate positioning data. Spraying\nand the movement of modern wide boom bars are highly dynamic processes. In\naddition, many uncertainty factors have an effect such as cross wind drift,\nboom height, nozzle clogging in open-field conditions, and so forth. In view of\nthis complexity, the natural question arises if a simpler alternative exist.\nTherefore, an Automatic Multi-Sections Control method is compared to a proposed\nsimpler one- or two-sections alternative that uses predictive spray switching.\nThe comparison is provided under nominal conditions. Agricultural spraying is\nintrinsically linked to area coverage path planning and spray switching logic.\nCombinations of two area coverage path planning and switching logics as well as\nthree sections-setups are compared. The three sections-setups differ by\ncontrolling 48 sections, 2 sections or controlling all nozzles uniformly with\nthe same control signal as one single section. Methods are evaluated on 10\ndiverse real-world field examples, including non-convex field contours,\nfreeform mainfield lanes and multiple obstacle areas. A preferred method is\nsuggested that (i) minimises area coverage pathlength, (ii) offers intermediate\noverlap, (iii) is suitable for manual driving by following a pre-planned\npredictive spray switching logic for an area coverage path plan, and (iv) and\nin contrast to ASC can be implemented sensor-free and therefore at low cost.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u81ea\u52a8\u591a\u6bb5\u63a7\u5236\u4e0e\u7b80\u5355\u7684\u4e00\u6216\u4e24\u6bb5\u9884\u6d4b\u55b7\u96fe\u5207\u6362\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u9700\u4f20\u611f\u5668\u7684\u4f18\u9009\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u624b\u52a8\u9a7e\u9a76\uff0c\u4f18\u5316\u8def\u5f84\u8986\u76d6\u548c\u55b7\u96fe\u91cd\u53e0\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5bfb\u627e\u4e00\u79cd\u6bd4\u73b0\u6709\u81ea\u52a8\u5206\u6bb5\u63a7\u5236\u66f4\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u55b7\u96fe\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u519c\u4e1a\u55b7\u96fe\u4e2d\u7684\u590d\u6742\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u5206\u6bb5\u8bbe\u7f6e\uff0848\u6bb5\u30012\u6bb5\u548c\u5355\u6bb5\u63a7\u5236\uff09\u4ee5\u53ca\u4e24\u79cd\u8def\u5f84\u89c4\u5212\u548c\u55b7\u96fe\u5207\u6362\u903b\u8f91\uff0c\u8bc4\u4f30\u4e8610\u4e2a\u771f\u5b9e\u519c\u7530\u6848\u4f8b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u9009\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u55b7\u96fe\u5207\u6362\u903b\u8f91\uff0c\u51cf\u5c11\u8def\u5f84\u957f\u5ea6\u548c\u55b7\u96fe\u91cd\u53e0\uff0c\u4e14\u65e0\u9700\u4f20\u611f\u5668\uff0c\u9002\u5408\u624b\u52a8\u64cd\u4f5c\u3002", "conclusion": "\u4f18\u9009\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u548c\u65e0\u4f20\u611f\u5668\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u81ea\u52a8\u5206\u6bb5\u63a7\u5236\u7684\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u519c\u4e1a\u5e94\u7528\u3002"}}
{"id": "2508.11584", "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub \u0141ucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5e76\u884c\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u89c6\u89c9\u591a\u4efb\u52a1\u5904\u7406\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u5e38\u89c1\u7684\u8ba1\u7b97\u5197\u4f59\u3001\u5185\u5b58\u5360\u7528\u5927\u548c\u96c6\u6210\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5171\u4eab\u7684\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u5934\u9ad8\u6548\u5171\u4eab\u7279\u5f81\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684GPU-CPU\u5185\u5b58\u4f20\u8f93\u3002\u7ed3\u5408CUDA MPS\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u4f18\u5148\u7ea7\u8c03\u6574\u3002", "result": "\u5728DINOv2\u57fa\u7840\u4e0a\u5b9e\u73b0\u591a\u4efb\u52a1\uff08\u6df1\u5ea6\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff09\u5904\u7406\uff0c\u901f\u5ea6\u63d0\u53473\u500d\uff0c\u5e76\u5728NVIDIA Jetson Orin AGX\u4e0a\u8fbe\u5230\u226550Hz\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "VPEngine\u5728\u4fdd\u6301\u9ad8\u6548GPU\u5229\u7528\u548c\u52a8\u6001\u4efb\u52a1\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u89c6\u89c9\u5904\u7406\u7684\u6027\u80fd\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2508.11588", "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u519c\u4e1a\u91c7\u6458\u4e2d\u5229\u7528\u591a\u79cd\u4f20\u611f\u5668\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u51c6\u786e\u8bc6\u522b\u6293\u53d6\u72b6\u6001\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6458\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u519c\u4e1a\u91c7\u6458\u73af\u5883\u590d\u6742\u4e14\u6613\u53d7\u906e\u6321\uff0c\u51c6\u786e\u7406\u89e3\u6293\u53d6\u72b6\u6001\u5bf9\u9ad8\u6548\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210IMU\u3001IR\u3001\u5f20\u529b\u3001\u89e6\u89c9\u4f20\u611f\u5668\u548cRGB\u76f8\u673a\uff0c\u6bd4\u8f83\u968f\u673a\u68ee\u6797\u548cLSTM\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u5728\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u6293\u53d6\u72b6\u6001\uff0c\u5c24\u5176\u662fIMU\u548c\u5f20\u529b\u4f20\u611f\u5668\u7ec4\u5408\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u519c\u4e1a\u91c7\u6458\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4f20\u611f\u5668\u7ec4\u5408\u548c\u5206\u7c7b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6458\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.10934", "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u7528\u4e8e\u4ece\u65e0\u7ea6\u675f\u7684\u539f\u59cb\u89c6\u9891\u4e2d\u4f30\u8ba1\u76f8\u673a\u53c2\u6570\u548c\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u7528\u4e8e\u6807\u6ce8\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u83b7\u53d6\u7cbe\u786e3D\u6ce8\u91ca\u7684\u6311\u6218\uff0c\u4e3a\u7a7a\u95f4AI\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u611f\u77e5\u6570\u636e\u3002", "method": "\u5f00\u53d1ViPE\u5f15\u64ce\uff0c\u652f\u6301\u591a\u79cd\u76f8\u673a\u6a21\u578b\u548c\u573a\u666f\uff0c\u9ad8\u6548\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u3001\u8fd0\u52a8\u548c\u6df1\u5ea6\u56fe\u3002", "result": "ViPE\u5728TUM/KITTI\u5e8f\u5217\u4e0a\u6bd4\u73b0\u6709\u57fa\u7ebf\u8868\u73b0\u4f1818%/50%\uff0c\u8fd0\u884c\u901f\u5ea6\u4e3a3-5FPS\uff1b\u6807\u6ce8\u4e8696M\u5e27\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "ViPE\u53ca\u5176\u5f00\u6e90\u6570\u636e\u96c6\u6709\u671b\u63a8\u52a8\u7a7a\u95f4AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.10935", "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HQ-OV3D\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u89e3\u51b3\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u4e2d\u51e0\u4f55\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5c01\u95ed\u5f0f3D\u68c0\u6d4b\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u5f00\u653e\u4e16\u754c\u5e94\u7528\u9700\u6c42\uff0c\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u7684\u51e0\u4f55\u7cbe\u5ea6\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1aIMCV\u63d0\u6848\u751f\u6210\u5668\u5229\u7528\u8de8\u6a21\u6001\u51e0\u4f55\u4e00\u81f4\u6027\u751f\u6210\u9ad8\u8d28\u91cf3D\u63d0\u6848\uff1bACA\u964d\u566a\u5668\u901a\u8fc7DDIM\u673a\u5236\u9010\u6b65\u4f18\u5316\u63d0\u6848\u3002", "result": "\u5728\u65b0\u578b\u7c7b\u522b\u7684mAP\u4e0a\u63d0\u5347\u4e867.37%\uff0c\u8868\u660e\u4f2a\u6807\u7b7e\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HQ-OV3D\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u5668\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u73b0\u6709\u68c0\u6d4b\u6d41\u7a0b\u7684\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2508.10936", "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7a00\u758f3D\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u7684\u534f\u4f5c3D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u4fe1\u6210\u672c\u9ad8\u6216\u4f9d\u8d56\u7cbe\u786e\u6df1\u5ea6\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u5728\u534f\u4f5c\u573a\u666f\u4e2d\u56e0\u9ad8\u901a\u4fe1\u6210\u672c\u6216\u4f9d\u8d56\u6df1\u5ea6\u4f30\u8ba1\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u548c\u878d\u5408\u4e2d\u95f4\u9ad8\u65af\u57fa\u5143\uff0c\u5b9e\u73b0\u4ea4\u53c9\u4ee3\u7406\u878d\u5408\uff0c\u8054\u5408\u7f16\u7801\u51e0\u4f55\u548c\u8bed\u4e49\uff0c\u51cf\u5c11\u901a\u4fe1\u91cf\u3002", "result": "\u5728mIoU\u548cIoU\u4e0a\u5206\u522b\u6bd4\u5355\u4ee3\u7406\u548c\u57fa\u7ebf\u534f\u4f5c\u65b9\u6cd5\u63d0\u53478.42/3.28\u548c5.11/22.41\u5206\uff0c\u901a\u4fe1\u91cf\u51cf\u5c11\u81f334.6%\u65f6\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u901a\u4fe1\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2508.11022", "pdf": "https://arxiv.org/pdf/2508.11022", "abs": "https://arxiv.org/abs/2508.11022", "authors": ["Lauren W. Wang", "Parastoo Abtahi"], "title": "GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Robots are increasingly capable of autonomous operations, yet human\ninteraction remains essential for issuing personalized instructions. Instead of\ndirectly controlling robots through Programming by Demonstration (PbD) or\nteleoperation, we propose giving instructions by interacting with\nGhostObjects-world-aligned, life-size virtual twins of physical objects-in\naugmented reality (AR). By direct manipulation of GhostObjects, users can\nprecisely specify physical goals and spatial parameters, with features\nincluding real-world lasso selection of multiple objects and snapping back to\ndefault positions, enabling tasks beyond simple pick-and-place.", "AI": {"tldr": "\u901a\u8fc7AR\u4e2d\u7684GhostObjects\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\uff0c\u7528\u6237\u53ef\u76f4\u63a5\u64cd\u4f5c\u865a\u62df\u5bf9\u8c61\u6765\u6307\u5b9a\u7269\u7406\u76ee\u6807\uff0c\u63d0\u5347\u673a\u5668\u4eba\u6307\u4ee4\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u3002", "motivation": "\u867d\u7136\u673a\u5668\u4eba\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u4eba\u7c7b\u4ea4\u4e92\u4ecd\u4e0d\u53ef\u6216\u7f3a\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u7f16\u7a0b\u6f14\u793a\u6216\u8fdc\u7a0b\u64cd\u4f5c\uff09\u9650\u5236\u4e86\u6307\u4ee4\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4f7f\u7528AR\u4e2d\u7684GhostObjects\uff08\u7269\u7406\u5bf9\u8c61\u7684\u865a\u62df\u526f\u672c\uff09\uff0c\u7528\u6237\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c\u865a\u62df\u5bf9\u8c61\u6765\u6307\u5b9a\u76ee\u6807\u4f4d\u7f6e\u548c\u7a7a\u95f4\u53c2\u6570\uff0c\u652f\u6301\u591a\u5bf9\u8c61\u9009\u62e9\u548c\u9ed8\u8ba4\u4f4d\u7f6e\u590d\u4f4d\u7b49\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8d8a\u7b80\u5355\u6293\u53d6\u7684\u9ad8\u7ea7\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u3002", "conclusion": "GhostObjects\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\uff0c\u62d3\u5c55\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11366", "pdf": "https://arxiv.org/pdf/2508.11366", "abs": "https://arxiv.org/abs/2508.11366", "authors": ["Sanghoon Lee", "Taehun Kim", "Jiyeong Chae", "Kyung-Joon Park"], "title": "Optimizing ROS 2 Communication for Wireless Robotic Systems", "categories": ["cs.NI", "cs.RO"], "comment": "10 pages, 8 figures", "summary": "Wireless transmission of large payloads, such as high-resolution images and\nLiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source\nrobotics middleware. The default Data Distribution Service (DDS) communication\nstack in ROS 2 exhibits significant performance degradation over lossy wireless\nlinks. Despite the widespread use of ROS 2, the underlying causes of these\nwireless communication challenges remain unexplored. In this paper, we present\nthe first in-depth network-layer analysis of ROS 2's DDS stack under wireless\nconditions with large payloads. We identify the following three key issues:\nexcessive IP fragmentation, inefficient retransmission timing, and congestive\nbuffer bursts. To address these issues, we propose a lightweight and fully\ncompatible DDS optimization framework that tunes communication parameters based\non link and payload characteristics. Our solution can be seamlessly applied\nthrough the standard ROS 2 application interface via simple XML-based QoS\nconfiguration, requiring no protocol modifications, no additional components,\nand virtually no integration efforts. Extensive experiments across various\nwireless scenarios demonstrate that our framework successfully delivers large\npayloads in conditions where existing DDS modes fail, while maintaining low\nend-to-end latency.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86ROS 2\u4e2dDDS\u534f\u8bae\u5728\u65e0\u7ebf\u4f20\u8f93\u5927\u8d1f\u8f7d\u65f6\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u4f18\u5316\u6846\u67b6\u4ee5\u89e3\u51b3\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u65e0\u7ebf\u4f20\u8f93\u5927\u8d1f\u8f7d\uff08\u5982\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\uff09\u662fROS 2\u4e2d\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u5176DDS\u534f\u8bae\u5728\u65e0\u7ebf\u94fe\u8def\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u539f\u56e0\u5c1a\u672a\u88ab\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7f51\u7edc\u5c42\u5206\u6790\uff0c\u8bc6\u522b\u51faIP\u788e\u7247\u8fc7\u591a\u3001\u91cd\u4f20\u65f6\u673a\u4f4e\u6548\u548c\u7f13\u51b2\u533a\u62e5\u585e\u7b49\u5173\u952e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7XML\u914d\u7f6e\u8c03\u6574\u901a\u4fe1\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u73b0\u6709DDS\u534f\u8bae\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u4f20\u8f93\u5927\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u65e0\u9700\u4fee\u6539\u534f\u8bae\u6216\u589e\u52a0\u7ec4\u4ef6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347ROS 2\u5728\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u4f20\u8f93\u6027\u80fd\u3002"}}
{"id": "2508.11426", "pdf": "https://arxiv.org/pdf/2508.11426", "abs": "https://arxiv.org/abs/2508.11426", "authors": ["Steffen Hauck", "Diar Abdlkarim", "John Dudley", "Per Ola Kristensson", "Eyal Ofek", "Jens Grubert"], "title": "ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality", "categories": ["cs.HC", "cs.RO"], "comment": "To appear in Proceedings of IEEE ISMAR 2025", "summary": "Human-Robot-Collaboration can enhance workflows by leveraging the mutual\nstrengths of human operators and robots. Planning and understanding robot\nmovements remain major challenges in this domain. This problem is prevalent in\ndynamic environments that might need constant robot motion path adaptation. In\nthis paper, we investigate whether a minimalistic encoding of the reachability\nof a point near an object of interest, which we call ReachVox, can aid the\ncollaboration between a remote operator and a robotic arm in VR. Through a user\nstudy (n=20), we indicate the strength of the visualization relative to a\npoint-based reachability check-up.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86ReachVox\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u53ef\u8fbe\u6027\u7f16\u7801\u6539\u5584\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u7684\u70b9\u53ef\u8fbe\u6027\u68c0\u67e5\u3002", "motivation": "\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u548c\u52a8\u6001\u9002\u5e94\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6301\u7eed\u8c03\u6574\u8fd0\u52a8\u8def\u5f84\u7684\u52a8\u6001\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aReachVox\u7684\u6700\u5c0f\u5316\u53ef\u8fbe\u6027\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728VR\u73af\u5883\u4e2d\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u8005\u4e0e\u673a\u68b0\u81c2\u7684\u534f\u4f5c\u3002", "result": "\u7528\u6237\u5b9e\u9a8c\uff08n=20\uff09\u8868\u660e\uff0cReachVox\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u5728\u53ef\u8fbe\u6027\u68c0\u67e5\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u70b9\u5f0f\u65b9\u6cd5\u3002", "conclusion": "ReachVox\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u80fd\u591f\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6548\u7387\u3002"}}
