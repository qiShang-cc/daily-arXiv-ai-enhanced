<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Deep Learning-based Human Gesture Channel Modeling for Integrated Sensing and Communication Scenarios](https://arxiv.org/abs/2507.06588)
*Zhengyu Zhang,Neeraj Varshney,Jelena Senic,Raied Caromi,Samuel Berweger,Camillo Gentile,Enrico M. Vitucci,Ruisi He,Vittorio Degli-Esposti*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的手势动作信道建模框架，用于6G无线系统中的集成感知与通信（ISAC）场景，通过分解人体部位并学习多径特性，提高了建模准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线系统中集成感知与通信（ISAC）的发展，如何准确建模由手势动作引起的无线多径传播变化成为一个关键问题。

Method: 使用深度学习框架，将人体分解为多个部位，并通过泊松神经网络预测多径分量数量，同时利用条件变分自编码器生成散射点重建信道响应。

Result: 仿真结果表明，该方法在不同手势和对象上具有高准确性和泛化能力，为数据增强和手势ISAC系统评估提供了可解释的解决方案。

Conclusion: 该框架为手势动作信道建模提供了一种高效且可解释的方法，支持ISAC系统的设计验证。

Abstract: With the development of Integrated Sensing and Communication (ISAC) for
Sixth-Generation (6G) wireless systems, contactless human recognition has
emerged as one of the key application scenarios. Since human gesture motion
induces subtle and random variations in wireless multipath propagation, how to
accurately model human gesture channels has become a crucial issue for the
design and validation of ISAC systems. To this end, this paper proposes a deep
learning-based human gesture channel modeling framework for ISAC scenarios, in
which the human body is decomposed into multiple body parts, and the mapping
between human gestures and their corresponding multipath characteristics is
learned from real-world measurements. Specifically, a Poisson neural network is
employed to predict the number of Multi-Path Components (MPCs) for each human
body part, while Conditional Variational Auto-Encoders (C-VAEs) are reused to
generate the scattering points, which are further used to reconstruct
continuous channel impulse responses and micro-Doppler signatures. Simulation
results demonstrate that the proposed method achieves high accuracy and
generalization across different gestures and subjects, providing an
interpretable approach for data augmentation and the evaluation of
gesture-based ISAC systems.

</details>


### [2] [Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization to Estimation](https://arxiv.org/abs/2507.06612)
*Peng Jiang,Ming Li,Rang Liu,Qian Liu*

Main category: eess.SP

TL;DR: 本文提出了一种基于图学习的端到端方法，用于优化无蜂窝集成传感与通信（ISAC）系统中的AP模式选择、用户关联、预编码和回波信号处理，以提高多目标位置和速度估计的精度。


<details>
  <summary>Details</summary>
Motivation: 为了在6G网络中实现高数据速率传输和高精度雷达感知，需要一种统一的系统优化方法，以桥接系统级优化与多目标参数估计。

Method: 提出了两种图学习框架：动态图学习框架和轻量级镜像图注意力网络（mirror-GAT）框架，分别通过结构/时间注意力机制和双层迭代结构优化。

Result: 仿真结果表明，两种框架在多目标位置和速度估计精度上均优于传统方法，且mirror-GAT显著降低了计算复杂性和信号开销。

Conclusion: 图学习方法在无蜂窝ISAC系统中表现优异，尤其是mirror-GAT框架适合实际部署。

Abstract: Cell-free integrated sensing and communication (ISAC) systems have emerged as
a promising paradigm for sixth-generation (6G) networks, enabling simultaneous
high-rate data transmission and high-precision radar sensing through
cooperative distributed access points (APs). Fully exploiting these
capabilities requires a unified design that bridges system-level optimization
with multi-target parameter estimation. This paper proposes an end-to-end graph
learning approach to close this gap, modeling the entire cell-free ISAC network
as a heterogeneous graph to jointly design the AP mode selection, user
association, precoding, and echo signal processing for multi-target position
and velocity estimation. In particular, we propose two novel heterogeneous
graph learning frameworks: a dynamic graph learning framework and a lightweight
mirror-based graph attention network (mirror-GAT) framework. The dynamic graph
learning framework employs structural and temporal attention mechanisms
integrated with a three-dimensional convolutional neural network (3D-CNN),
enabling superior performance and robustness in cell-free ISAC environments.
Conversely, the mirror-GAT framework significantly reduces computational
complexity and signaling overhead through a bi-level iterative structure with
share adjacency. Simulation results validate that both proposed
graph-learning-based frameworks achieve significant improvements in
multi-target position and velocity estimation accuracy compared to conventional
heuristic and optimization-based designs. Particularly, the mirror-GAT
framework demonstrates substantial reductions in computational time and
signaling overhead, underscoring its suitability for practical deployments.

</details>


### [3] [Wireless Energy Transfer Beamforming Optimization for Intelligent Transmitting Surface](https://arxiv.org/abs/2507.06805)
*Osmel Martínez Rosabal,Onel Alcaraz López,Victoria Dala Pegorara Souto,Richard Demo Souza,Samuel Montejo-Sánchez,Robert Schober,Hirley Alves*

Main category: eess.SP

TL;DR: 该研究提出了一种配备被动智能发射表面（ITS）的数字波束成形供电基站（PB），旨在最小化其功耗，以支持多天线物联网设备充电。通过成功解决非线性非凸优化问题，并与其他架构比较，证明了其高效性。


<details>
  <summary>Details</summary>
Motivation: 物联网设备数量激增，需要高效的无线能量传输技术。智能发射表面（ITS）提供了可重构大型天线阵列的可能性，但其功耗优化是一个挑战。

Method: 采用连续凸近似（SCA）方法，联合优化数字预编码器和相位配置，并通过算法确保初始可行性和收敛速度与解质量的平衡。

Result: 提出的ITS架构在功耗上优于数字和混合模拟-数字波束成形基准架构，并能随着射频链和ITS元件数量的增加高效扩展。

Conclusion: 配备ITS的PB架构在无线能量传输中表现出高效性和可扩展性，非均匀功率分配还能影响波束成形性能。

Abstract: Radio frequency (RF) wireless energy transfer (WET) is a promising technology
for powering the growing ecosystem of Internet of Things (IoT) devices using
power beacons (PBs). Recent research focuses on designing efficient PB
architectures that can support numerous antennas. In this context, PBs equipped
with intelligent surfaces present a promising approach, enabling physically
large, reconfigurable arrays. Motivated by these advantages, this work aims to
minimize the power consumption of a PB equipped with a passive intelligent
transmitting surface (ITS) and a collocated digital beamforming-based feeder to
charge multiple single-antenna devices. To model the PB's power consumption
accurately, we consider power amplifiers nonlinearities, ITS control power, and
feeder-to-ITS air interface losses. The resulting optimization problem is
highly nonlinear and nonconvex due to the high-power amplifier (HPA), the
received power constraints at the devices, and the unit-modulus constraint
imposed by the phase shifter configuration of the ITS. To tackle this issue, we
apply successive convex approximation (SCA) to iteratively solve convex
subproblems that jointly optimize the digital precoder and phase configuration.
Given SCA's sensitivity to initialization, we propose an algorithm that ensures
initialization feasibility while balancing convergence speed and solution
quality. We compare the proposed ITS-equipped PB's power consumption against
benchmark architectures featuring digital and hybrid analog-digital
beamforming. Results demonstrate that the proposed architecture efficiently
scales with the number of RF chains and ITS elements. We also show that
nonuniform ITS power distribution influences beamforming and can shift a device
between near- and far-field regions, even with a constant aperture.

</details>


### [4] [Enhancing Environment Generalizability for Deep Learning-Based CSI Feedback](https://arxiv.org/abs/2507.06833)
*Haoyu Wang,Shuangfeng Han,Xiaoyun Wang,Zhi Sun*

Main category: eess.SP

TL;DR: EG-CsiNet是一种新型CSI反馈学习框架，通过多路径解耦和细粒度对齐模块解决环境通用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习CSI反馈算法在未知环境中泛化能力有限，增加了部署成本。本文旨在解决这一问题。

Method: 提出EG-CsiNet框架，包含多路径解耦和细粒度对齐模块，以处理多路径结构和单一路径的分布偏移。

Result: 实验显示EG-CsiNet在未知环境中表现优于现有技术，尤其是在单源环境的挑战性条件下。

Conclusion: EG-CsiNet显著提升了CSI反馈的环境通用性，为FDD大规模MIMO系统提供了高效解决方案。

Abstract: Accurate and low-overhead channel state information (CSI) feedback is
essential to boost the capacity of frequency division duplex (FDD) massive
multiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback
significantly outperforms conventional approaches. Nevertheless, current deep
learning-based CSI feedback algorithms exhibit limited generalizability to
unseen environments, which obviously increases the deployment cost. In this
paper, we first model the distribution shift of CSI across different
environments, which is composed of the distribution shift of multipath
structure and a single-path. Then, EG-CsiNet is proposed as a novel CSI
feedback learning framework to enhance environment-generalizability.
Explicitly, EG-CsiNet comprises the modules of multipath decoupling and
fine-grained alignment, which can address the distribution shift of multipath
structure and a single path. Based on extensive simulations, the proposed
EG-CsiNet can robustly enhance the generalizability in unseen environments
compared to the state-of-the-art, especially in challenging conditions with a
single source environment.

</details>


### [5] [OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion](https://arxiv.org/abs/2507.06849)
*Yizhuo Wu,Ang Li,Chang Gao*

Main category: eess.SP

TL;DR: OpenDPDv2是一个统一的框架，用于降低功率放大器（PA）的数字预失真（DPD）模型的能耗，同时保持高性能；采用TRes-DeltaGRU算法和两种节能方法，显著减少能耗。


<details>
  <summary>Details</summary>
Motivation: 传统的NN DPD模型需要大量参数，导致能耗高，OpenDPDv2旨在通过优化技术减少能耗，同时保持线性化性能。

Method: 提出TRes-DeltaGRU算法，并结合定点量化和动态时间稀疏性两种节能方法，显著降低推理能耗。

Result: 优化后的模型在FP32下性能为-59.4 dBc ACPR和-42.1 dB EVM；通过节能方法可实现4.5倍能耗降低，同时性能保持在-50.3 dBc ACPR和-35.2 dB EVM。

Conclusion: OpenDPDv2在高性能和低能耗之间取得了平衡，为RF系统的数字预失真提供了一种高效解决方案。

Abstract: Neural network (NN)-based Digital Predistortion (DPD) stands out in improving
signal quality in wideband radio frequency (RF) power amplifiers (PAs)
employing complex modulation. However, NN DPDs usually rely on a large number
of parameters for effective linearization and can significantly contribute to
the energy consumption of the digital back-end in RF systems. This paper
presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and
model optimization to reduce power consumption while maintaining high
linearization performance. The optimization techniques feature a novel DPD
algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The
top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an
Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude
(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal
sparsity of input signals and hidden neurons, the inference energy of our model
can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM
with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth
256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,
datasets, and documentation are publicly accessible at:
https://github.com/lab-emi/OpenDPD.

</details>


### [6] [Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA Assisted Wireless Communication Systems](https://arxiv.org/abs/2507.06904)
*Yu Liu,Qu Luo,Gaojie Chen,Pei Xiao,Ahmed Elzanaty,Mohsen Khalily,Rahim Tafazolli*

Main category: eess.SP

TL;DR: 论文提出了一种新型流体智能表面（FSTAR-RIS）辅助的非正交多址（NOMA）通信系统，通过动态调整表面元素位置增强了系统空间自由度和服务适应性。


<details>
  <summary>Details</summary>
Motivation: 传统可重构智能表面（RIS）在空间控制能力上存在局限性，需要更灵活的系统来提升通信性能。

Method: 引入FSTAR-RIS概念，结合交替优化算法（AO）和多种技术（SCA、SDR、MM）联合优化波束成形、传输/反射系数及元素位置。

Result: 仿真结果表明，该系统比传统STAR-RIS系统总速率提升27%，且所需RIS元素数量减少50%。

Conclusion: FSTAR-RIS系统在高效、低成本大规模部署中展现出显著优势。

Abstract: To address the limitations of traditional reconfigurable intelligent surfaces
(RIS) in spatial control capability, this paper introduces the concept of the
fluid antenna system (FAS) and proposes a fluid simultaneously transmitting and
reflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)
multi-user communication system. In this system, each FSTAR-RIS element is
capable of flexible mobility and can dynamically adjust its position in
response to environmental variations, thereby enabling simultaneous service to
users in both the transmission and reflection zones. This significantly
enhances the system's spatial degrees of freedom (DoF) and service
adaptability. To maximize the system's weighted sum-rate, we formulate a
non-convex optimization problem that jointly optimizes the base station
beamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the
element positions. An alternating optimization (AO) algorithm is developed,
incorporating successive convex approximation (SCA), semi-definite relaxation
(SDR), and majorization-minimization (MM) techniques. In particular, to address
the complex channel coupling introduced by the coexistence of direct and
FSTAR-RIS paths, the MM framework is employed in the element position
optimization subproblem, enabling an efficient iterative solution strategy.
Simulation results validate that the proposed system achieves up to a 27%
increase in total sum rate compared to traditional STAR-RIS systems and
requires approximately 50% fewer RIS elements to attain the same performance,
highlighting its effectiveness for cost-efficient large-scale deployment.

</details>


### [7] [Precise Representation Model of SAR Saturated Interference: Mechanism and Verification](https://arxiv.org/abs/2507.06932)
*Lunhao Duan,Xingyu Lu,Yushuang Liu,Jianchao Yang,Hong Gu*

Main category: eess.SP

TL;DR: 论文提出了一种基于Bessel函数的SAR接收机饱和干扰分析模型，解决了传统双曲正切函数近似方法的误差问题。


<details>
  <summary>Details</summary>
Motivation: SAR接收机易受高功率射频干扰（RFI）影响，导致饱和和非线性失真。传统分析方法因饱和函数非光滑而受限，现有研究基于双曲正切函数的近似存在误差。

Method: 提出基于Bessel函数的饱和干扰分析模型，并通过仿真与传统平滑函数近似模型对比验证其准确性。

Result: 新模型较传统方法更精确，为后续饱和干扰抑制等研究提供了理论支持。

Conclusion: 基于Bessel函数的模型能更准确分析SAR接收机饱和干扰，具有实际应用价值。

Abstract: Synthetic Aperture Radar (SAR) is highly susceptible to Radio Frequency
Interference (RFI). Due to the performance limitations of components such as
gain controllers and analog-to-digital converters in SAR receivers, high-power
interference can easily cause saturation of the SAR receiver, resulting in
nonlinear distortion of the interfered echoes, which are distorted in both the
time domain and frequency domain. Some scholars have analyzed the impact of SAR
receiver saturation on target echoes through simulations. However, the
saturation function has non-smooth characteristics, making it difficult to
conduct accurate analysis using traditional analytical methods. Current related
studies have approximated and analyzed the saturation function based on the
hyperbolic tangent function, but there are approximation errors. Therefore,
this paper proposes a saturation interference analysis model based on Bessel
functions, and verifies the accuracy of the proposed saturation interference
analysis model by simulating and comparing it with the traditional saturation
model based on smooth function approximation. This model can provide certain
guidance for further work such as saturation interference suppression.

</details>


### [8] [Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks](https://arxiv.org/abs/2507.06997)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 该论文研究了联邦学习与多智能体强化学习结合的方法以提升5G之后网络中的物理层安全，比较了DQN和RDPG两种方法，RDPG表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多小区网络中通过联邦学习和强化学习提升物理层安全，以保护合法用户免受窃听者的攻击。

Method: 提出一种联邦学习与多智能体强化学习结合的方法，每个基站作为DRL智能体通过联邦机制共享参数而非用户数据，比较了DQN和RDPG两种策略。

Result: RDPG比DQN收敛更快，且该方法优于分布式DRL方法，但需权衡安全性与复杂度。

Conclusion: 联邦学习与RDPG结合的强化学习方法是提升物理层安全的一种有效途径，但需进一步优化复杂度问题。

Abstract: This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.

</details>


### [9] [How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks](https://arxiv.org/abs/2507.07067)
*Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir M. Al-Hashimi*

Main category: eess.SP

TL;DR: 论文探讨了如何通过数字孪生和模拟到现实的策略解决电信领域AI模型训练中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 由于电信领域部署特定数据的稀缺性，以及现有数据集难以捕捉网络环境的独特条件和多样性，因此需要解决方案来生成更多数据并弥补模拟与现实的差距。

Method: 提出了两种互补策略：1）通过实际测量校准数字孪生；2）使用模拟到现实差距感知的训练策略，包括基于贝叶斯学习的环境建模和基于预测能力推断的训练损失建模。

Result: 两种方法均有效处理了数字孪生生成数据与真实数据之间的差异。

Conclusion: 数字孪生和模拟到现实策略的结合是解决电信领域数据稀缺和模型训练挑战的有效途径。

Abstract: Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.

</details>


### [10] [Joint Target Acquisition and Refined Position Estimation in OFDM-based ISAC Networks](https://arxiv.org/abs/2507.07081)
*Lorenzo Pucci,Andrea Giorgetti*

Main category: eess.SP

TL;DR: 论文提出了一种基于OFDM的ISAC网络中联合目标获取和位置估计的两阶段框架，通过BS协作提高了检测性能和定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决OFDM-based ISAC网络中联合目标获取和位置估计的问题，利用基站协作提升性能。

Method: 采用两阶段框架：第一阶段通过空间多样性计算距离-角度图检测目标并估计粗略位置；第二阶段通过协作ML估计器在全局参考系中精确定位。

Result: 数值结果表明，该方法通过BS协作提升了检测性能，并实现了厘米级定位精度。

Conclusion: 提出的两阶段框架有效提升了ISAC网络的联合目标检测和定位能力。

Abstract: This paper addresses joint target acquisition and position estimation in an
OFDM-based integrated sensing and communication (ISAC) network with base
station (BS) cooperation via a fusion center. A two-stage framework is
proposed: in the first stage, each BS computes range-angle maps to detect
targets and estimate coarse positions, exploiting spatial diversity. In the
second stage, refined localization is performed using a cooperative maximum
likelihood (ML) estimator over predefined regions of interest (RoIs) within a
shared global reference frame. Numerical results demonstrate that the proposed
approach not only improves detection performance through BS cooperation but
also achieves centimeter-level localization accuracy, highlighting the
effectiveness of the refined estimation technique.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 该论文研究了CNN、ResNet及Transformer-CNN混合模型在ECG信号防篡改检测中的性能，并评估了Siamese网络在ECG身份验证中的表现，结果显示高准确性。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统的普及，保护信号完整性免受篡改变得至关重要，尤其是在健康和身份认证应用中。

Method: 通过CWT将一维ECG信号转为二维时频域表示，模拟六种篡改策略，训练并评估多种深度学习模型。

Result: 在高度碎片化篡改场景下，多模型准确率超99.5%；Siamese网络中，CNN-Transformer混合模型实现100%准确率。

Conclusion: 深度学习模型（尤其是混合模型）在ECG防篡改和身份验证中表现出色，具有实际应用潜力。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [12] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: 本文介绍了一种名为eegFloss的开源Python工具包，用于检测睡眠EEG中的伪迹，提高睡眠分析的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪迹干扰，导致自动睡眠分期错误，影响研究结果。

Method: 开发了基于机器学习的eegUsability模型，检测EEG中的伪迹，并提供自动时间检测和数据分析功能。

Result: 模型表现优异（F1-score约0.85，Cohens kappa 0.78），能高召回率识别可用数据。

Conclusion: eegFloss能提升睡眠研究的精度和可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [13] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 该论文提出了一种基于异构图注意力网络的方法，用于多物理域电力系统的短期状态预测，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的增加而变得复杂，需要可靠的短期状态预测以确保稳定性。

Method: 利用异构图注意力网络建模同质域内和异质域间传感器关系，处理液压和电力两个物理域的数据。

Result: 相比传统基线方法，该方法在归一化均方根误差上平均提升了35.5%。

Conclusion: 该方法在多域多速率电力系统状态预测中表现出色，验证了其有效性。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [14] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 论文研究了在资源受限的智能电表上进行的边缘侧模型训练，探讨了电网边缘智能化的动机和设备端训练的概念，并通过光伏功率预测任务验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 探讨电网边缘智能化的可能性，并研究在资源受限的设备上实现模型训练的可行性。

Method: 介绍了设备端训练的技术准备步骤，并采用了梯度提升树模型和循环神经网络模型进行光伏功率预测任务，还设计了混合和降低精度的训练方案以适应资源限制。

Result: 实验结果表明，通过现有先进的计量基础设施，可以经济地实现电网边缘智能化。

Conclusion: 该研究证明了在资源受限的设备上进行模型训练的可行性，为电网边缘智能化提供了经济有效的解决方案。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [15] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）和自监督学习（SSL）在资源受限的边缘设备上部署时面临挑战，本研究探讨了如何通过定制化SSL策略在性能和能耗间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 传统CL和SSL方法需要大量数据和计算资源，难以在资源受限的边缘设备上高效运行，因此需要研究如何在边缘环境中实现高效的SSL。

Method: 分析不同SSL技术如何在有限的计算、数据和能源预算下适应，评估其在资源受限环境中学习鲁棒表征的效果，并探索半监督学习如何降低CL模型的训练能耗。

Result: 实验表明，定制化的SSL策略能实现与现有方法相当的性能，同时将资源消耗降低高达4倍。

Conclusion: 研究表明，通过优化SSL技术，可以在边缘设备上实现高效且节能的学习，具有实际应用潜力。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [16] [Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation](https://arxiv.org/abs/2507.07087)
*Klaus Brümann,Kouei Yamaoka,Nobutaka Ono,Simon Doclo*

Main category: eess.AS

TL;DR: 提出一种基于多CPSD平均的改进GCC-PHAT方法，用于提高在噪声和混响环境下的TDOA估计和声源定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决背景噪声和混响对基于TDOA的声源定位的负面影响。

Method: 通过增量方法计算基于MST的GCC-PHAT函数，使用多个CPSD的平均值。

Result: 相比单CPSD方法，多CPSD方法显著提高了TDOA估计和声源定位精度。

Conclusion: 多CPSD平均的GCC-PHAT方法在噪声和混响环境下更鲁棒。

Abstract: Estimating the position of a speech source based on
time-differences-of-arrival (TDOAs) is often adversely affected by background
noise and reverberation. A popular method to estimate the TDOA between a
microphone pair involves maximizing a generalized cross-correlation with phase
transform (GCC-PHAT) function. Since the TDOAs across different microphone
pairs satisfy consistency relations, generally only a small subset of
microphone pairs are used for source position estimation. Although the set of
microphone pairs is often determined based on a reference microphone, recently
a more robust method has been proposed to determine the set of microphone pairs
by computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT
function reliabilities. To reduce the influence of noise and reverberation on
the TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT
functions of the MST based on an average of multiple cross-power spectral
densities (CPSDs) using an incremental method. In each step of the method, we
increase the number of CPSDs over which we average by considering CPSDs
computed indirectly via other microphones from previous steps. Using signals
recorded in a noisy and reverberant laboratory with an array of spatially
distributed microphones, the performance of the proposed method is evaluated in
terms of TDOA estimation error and 2D source position estimation error.
Experimental results for different source and microphone configurations and
three reverberation conditions show that the proposed method considering
multiple CPSDs improves the TDOA estimation and source position estimation
accuracy compared to the reference microphone- and MST-based methods that rely
on a single CPSD as well as steered-response power-based source position
estimation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G](https://arxiv.org/abs/2507.06911)
*Michele Polese,Niloofar Mohamadi,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种融合O-RAN和AI-RAN的新型架构，支持电信和AI工作负载在共享基础设施上的统一编排与管理，为边缘AI提供了灵活部署和实时处理能力。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型AI应用在网络边缘的普及，需要从单纯利用AI优化网络转向支持分布式AI工作负载，从而为运营商创造新的盈利机会。

Method: 通过扩展O-RAN的模块化和云原生原则，提出了AI-RAN Orchestrator和AI-RAN站点两个关键创新，支持异构AI部署和资源分配。

Result: 该架构能够在不同时间尺度下管理异构工作负载，同时保持开放标准和多厂商互操作性，实现了灵活部署和实时处理。

Conclusion: 融合O-RAN和AI-RAN的架构为边缘AI提供了高效、灵活的解决方案，推动了网络边缘的AI商业化应用。

Abstract: The proliferation of data-intensive Artificial Intelligence (AI) applications
at the network edge demands a fundamental shift in RAN design, from merely
consuming AI for network optimization, to actively enabling distributed AI
workloads. This paradigm shift presents a significant opportunity for network
operators to monetize AI at the edge while leveraging existing infrastructure
investments. To realize this vision, this article presents a novel converged
O-RAN and AI-RAN architecture that unifies orchestration and management of both
telecommunications and AI workloads on shared infrastructure. The proposed
architecture extends the Open RAN principles of modularity, disaggregation, and
cloud-nativeness to support heterogeneous AI deployments. We introduce two key
architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN
Service Management and Orchestration (SMO) to enable integrated resource and
allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide
distributed edge AI platforms with real-time processing capabilities. The
proposed system supports flexible deployment options, allowing AI workloads to
be orchestrated with specific timing requirements (real-time or batch
processing) and geographic targeting. The proposed architecture addresses the
orchestration requirements for managing heterogeneous workloads at different
time scales while maintaining open, standardized interfaces and multi-vendor
interoperability.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Constraint Optimized Multichannel Mixer-limiter Design](https://arxiv.org/abs/2507.06769)
*Yuancheng Luo,Dmitriy Yamkovoy,Guillermo Garcia*

Main category: cs.SD

TL;DR: 提出了一种耦合的混音器-限制器-包络设计，通过线性约束二次规划最小化失真，实验表明其降低了失真并优化了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统多通道音频混音器和限制器设计因计算复杂度和运行成本高而解耦，因此需要一种更高效的耦合方案。

Method: 采用线性约束二次规划优化多通道增益变量，结合非对称常数重叠叠加窗口优化等方法。

Result: 实验显示耦合设计降低了失真，并实现了高效实时处理的权衡。

Conclusion: 耦合设计在降低失真和计算效率方面表现优越，适合实时处理。

Abstract: Multichannel audio mixer and limiter designs are conventionally decoupled for
content reproduction over loudspeaker arrays due to high computational
complexity and run-time costs. We propose a coupled mixer-limiter-envelope
design formulated as an efficient linear-constrained quadratic program that
minimizes a distortion objective over multichannel gain variables subject to
sample mixture constraints. Novel methods for asymmetric constant overlap-add
window optimization, objective function approximation, variable and constraint
reduction are presented. Experiments demonstrate distortion reduction of the
coupled design, and computational trade-offs required for efficient real-time
processing.

</details>


### [19] [Physics-Informed Direction-Aware Neural Acoustic Fields](https://arxiv.org/abs/2507.06826)
*Yoshiki Masuyama,François G. Germain,Gordon Wichern,Christopher Ick,Jonathan Le Roux*

Main category: cs.SD

TL;DR: 本文提出一种物理信息神经网络（PINN）用于建模一阶Ambisonic（FOA）房间脉冲响应（RIR）。通过结合神经网络的强大建模能力和声波传播的物理原理，PINN在声场插值中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在房间声学中主要用于全向麦克风测量的声压建模，而FOA RIRs还提供空间特性，适用于沉浸式音频生成。本文旨在扩展PINN框架以建模FOA RIRs。

Method: 本文推导了基于粒子速度与FOA的(X, Y, Z)通道对应关系的两种物理信息先验，通过这些先验关联W通道与其他通道，并施加物理可行关系。

Result: 实验表明，与未使用物理信息先验的神经网络相比，所提方法更有效。

Conclusion: 通过引入物理信息先验，PINN成功建模了FOA RIRs，验证了其在这种场景中的优势。

Abstract: This paper presents a physics-informed neural network (PINN) for modeling
first-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have
demonstrated promising performance in sound field interpolation by combining
the powerful modeling capability of neural networks and the physical principles
of sound propagation. In room acoustics, PINNs have typically been trained to
represent the sound pressure measured by omnidirectional microphones where the
wave equation or its frequency-domain counterpart, i.e., the Helmholtz
equation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial
characteristics and are useful for immersive audio generation with a wide range
of applications. In this paper, we extend the PINN framework to model FOA RIRs.
We derive two physics-informed priors for FOA RIRs based on the correspondence
between the particle velocity and the (X, Y, Z)-channels of FOA. These priors
associate the predicted W-channel and other channels through their partial
derivatives and impose the physically feasible relationship on the four
channels. Our experiments confirm the effectiveness of the proposed method
compared with a neural network without the physics-informed prior.

</details>


### [20] [Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation](https://arxiv.org/abs/2507.07043)
*Haris Khan,Shumaila Asif,Hassan Nasir*

Main category: cs.SD

TL;DR: 该系统综述探讨了人工智能在助听器中应用的进展，特别是选择性噪声消除技术，总结了技术发展、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 将人工智能整合到助听领域，从传统放大系统转向智能、上下文感知的音频处理，以提升助听器性能。

Method: 通过系统文献综述，评估深度学习架构、硬件部署策略、临床验证研究和以用户为中心的设计。

Result: 研究表明，最新模型在噪声-混响基准测试中取得了显著改进（18.3 dB SI-SDR），并实现了低于10毫秒的实时处理。

Conclusion: 尽管进展显著，但仍需解决功率限制、环境多样性和个性化等问题，未来应关注轻量模型、持续学习和临床转化。

Abstract: The integration of artificial intelligence into hearing assistance marks a
paradigm shift from traditional amplification-based systems to intelligent,
context-aware audio processing. This systematic literature review evaluates
advances in AI-driven selective noise cancellation (SNC) for hearing aids,
highlighting technological evolution, implementation challenges, and future
research directions. We synthesize findings across deep learning architectures,
hardware deployment strategies, clinical validation studies, and user-centric
design. The review traces progress from early machine learning models to
state-of-the-art deep networks, including Convolutional Recurrent Networks for
real-time inference and Transformer-based architectures for high-accuracy
separation. Key findings include significant gains over traditional methods,
with recent models achieving up to 18.3 dB SI-SDR improvement on
noisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and
promising clinical outcomes. Yet, challenges remain in bridging lab-grade
models with real-world deployment - particularly around power constraints,
environmental variability, and personalization. Identified research gaps
include hardware-software co-design, standardized evaluation protocols, and
regulatory considerations for AI-enhanced hearing devices. Future work must
prioritize lightweight models, continual learning, contextual-based
classification and clinical translation to realize transformative hearing
solutions for millions globally.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [21] [Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels](https://arxiv.org/abs/2507.06981)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: cs.ET

TL;DR: 本文提出了一种基于强化学习的方法，利用深度 Q 网络（DQN）优化认知无线电网络的物理层安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决认知车载网络中二次用户的隐私和可靠性问题，防止窃听者拦截通信。

Method: 使用多 DQN 代理，每个代理分配给二次用户发射机，优化传输功率和能量收集决策。

Result: 所提出的 DQN 策略在安全性和可靠性上优于其他基准策略。

Conclusion: 通过 DQN 方法，成功提升了二次用户的吞吐量和安全性。

Abstract: This paper presents a reinforcement learning (RL) based approach to improve
the physical layer security (PLS) of an underlay cognitive radio network (CRN)
over cascaded channels. These channels are utilized in highly mobile networks
such as cognitive vehicular networks (CVN). In addition, an eavesdropper aims
to intercept the communications between secondary users (SUs). The SU receiver
has full-duplex and energy harvesting capabilities to generate jamming signals
to confound the eavesdropper and enhance security. Moreover, the SU transmitter
extracts energy from ambient radio frequency signals in order to power
subsequent transmissions to its intended receiver. To optimize the privacy and
reliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized
where multiple DQN agents are required such that an agent is assigned at each
SU transmitter. The objective for the SUs is to determine the optimal
transmission power and decide whether to collect energy or transmit messages
during each time period in order to maximize their secrecy rate. Thereafter, we
propose a DQN approach to maximize the throughput of the SUs while respecting
the interference threshold acceptable at the receiver of the primary user.
According to our findings, our strategy outperforms two other baseline
strategies in terms of security and reliability.

</details>


### [22] [Maximizing Reliability in Overlay Radio Networks with Time Switching and Power Splitting Energy Harvesting](https://arxiv.org/abs/2507.06983)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: cs.ET

TL;DR: 本文研究了认知无线电网络中的能量效率问题，优化了系统可靠性，并通过时间切换和能量收集协议提升数据速率。


<details>
  <summary>Details</summary>
Motivation: 解决频谱利用率低的问题，并优化覆盖CRN访问模式的系统可靠性。

Method: 使用随机分布的次级用户作为中继，采用时间切换和功率分配协议，利用多天线和最大比率组合技术。

Result: 通过优化时间切换和功率分配参数，提升了次级用户的数据速率和系统能量效率。

Conclusion: 提出的方法有效优化了系统性能，提升了能量效率和可靠性。

Abstract: Cognitive radio networks (CRNs) are acknowledged for their ability to tackle
the issue of spectrum under-utilization. In the realm of CRNs, this paper
investigates the energy efficiency issue and addresses the critical challenge
of optimizing system reliability for overlay CRN access mode. Randomly
dispersed secondary users (SUs) serving as relays for primary users (PUs) are
considered, in which one of these relays is designated to harvest energy
through the time switching-energy harvesting (EH) protocol. Moreover, this
relay amplifies-and-forwards (AF) the PU's messages and broadcasts them along
with its own across cascaded $\kappa$-$\mu$ fading channels. The power
splitting protocol is another EH approach utilized by the SU and PU receivers
to enhance the amount of energy in their storage devices. In addition, the SU
transmitters and the SU receiver are deployed with multiple antennas for
reception and apply the maximal ratio combining approach. The outage
probability is utilized to assess both networks' reliability. Then, an energy
efficiency evaluation is performed to determine the effectiveness of EH on the
system. Finally, an optimization problem is provided with the goal of
maximizing the data rate of the SUs by optimizing the time switching and the
power allocation parameters of the SU relay.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Hybrid Quantum Convolutional Neural Network-Aided Pilot Assignment in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.06585)
*Doan Hieu Nguyen,Xuan Tung Nguyen,Seon-Geun Jeong,Trinh Van Chien,Lajos Hanzo,Won Joo Hwang*

Main category: cs.IT

TL;DR: 论文提出了一种混合量子卷积神经网络（HQCNN），用于优化小区自由大规模MIMO系统中的导频分配任务，显著提升总遍历和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的解决方案效率低下或计算量大，传统深度神经网络在高维输入下表现不佳、架构复杂且收敛慢。

Method: 利用参数化量子电路（PQCs）的叠加特性增强特征提取，并在所有卷积层中重复使用同一PQC以加速收敛。

Result: 数值结果表明，HQCNN的网络吞吐量接近高复杂度穷举搜索，且优于现有基准方法。

Conclusion: HQCNN在性能和计算效率上均优于传统方法，为导频分配问题提供了高效解决方案。

Abstract: A sophisticated hybrid quantum convolutional neural network (HQCNN) is
conceived for handling the pilot assignment task in cell-free massive MIMO
systems, while maximizing the total ergodic sum throughput. The existing
model-based solutions found in the literature are inefficient and/or
computationally demanding. Similarly, conventional deep neural networks may
struggle in the face of high-dimensional inputs, require complex architectures,
and their convergence is slow due to training numerous hyperparameters. The
proposed HQCNN leverages parameterized quantum circuits (PQCs) relying on
superposition for enhanced feature extraction. Specifically, we exploit the
same PQC across all the convolutional layers for customizing the neural network
and for accelerating the convergence. Our numerical results demonstrate that
the proposed HQCNN offers a total network throughput close to that of the
excessive-complexity exhaustive search and outperforms the state-of-the-art
benchmarks.

</details>


### [24] [Soft Robotics-Inspired Flexible Antenna Arrays](https://arxiv.org/abs/2507.06589)
*Elio Faddoul,Andreas Nicolaides,Konstantinos Ntougias,Ioannis Krikidis*

Main category: cs.IT

TL;DR: 提出了一种基于软体连续机器人的新型可变形天线阵列，通过几何连续变形实现可重构性，优于固定几何和逐个单元可重构天线阵列。


<details>
  <summary>Details</summary>
Motivation: 为了提升天线阵列的性能，利用软体连续机器人的灵活性设计新型可变形阵列，避免逐个单元控制的复杂性。

Method: 通过振幅和空间频率参数建模几何变形，采用逐次凸逼近方法优化变形参数以最大化网络和速率。

Result: 数值结果表明，该可变形阵列在和速率上显著优于固定几何和逐个单元可重构阵列。

Conclusion: 结构级灵活性是下一代天线阵列的重要优势。

Abstract: In this work, a novel soft continuum robot-inspired antenna array is
proposed, featuring tentacle-like structures with multiple antenna elements.
The proposed array achieves reconfigurability through continuous deformation of
its geometry, in contrast to reconfigurable antennas which incur a per-element
control. More specifically, the deformation is modeled by amplitude and spatial
frequency parameters. We consider a multi-user multiple-input single-output
downlink system, whereby the optimal deformation parameters are found to
maximize the sum rate in the network. A successive convex approximation method
is adopted to solve the problem. Numerical results show that the proposed
deformable array significantly outperforms fixed geometry and per-element
reconfigurable arrays in sum rate, demonstrating the benefits of
structure-level flexibility for next-generation antenna arrays.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [25] [COS2A: Conversion from Sentinel-2 to AVIRIS Hyperspectral Data Using Interpretable Algorithm With Spectral-Spatial Duality](https://arxiv.org/abs/2507.06575)
*Chia-Hsiang Lin,Jui-Ting Chen,Zi-Chao Leng,Jhao-Ting Lin*

Main category: eess.IV

TL;DR: Sentinel-2卫星的多分辨率12波段图像通过新算法COS2A转换为高分辨率172波段的AVIRIS级高光谱图像，解决了现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2卫星的多分辨率和有限波段限制了其在遥感识别任务中的效果，现有光谱超分辨率方法未能解决多分辨率问题且应用范围有限。

Method: 结合深度展开正则化和Q-二次范数正则化，提出COS2A算法，实现从Sentinel-2到AVIRIS级高光谱图像的转换。

Result: COS2A算法在不同地表类型上表现出优异的光谱超分辨率效果，实验验证了其有效性。

Conclusion: 该研究首次解决了Sentinel-2到AVIRIS的高光谱转换难题，为历史数据提供了高质量的对应版本。

Abstract: The Sentinel-2 satellite, launched by the European Space Agency (ESA), offers
extensive spatial coverage and has become indispensable in a wide range of
remote sensing applications. However, it just has 12 spectral bands, making
substances/objects identification less effective, not mentioning the varying
spatial resolutions (10/20/60 m) across the 12 bands. If such a
multi-resolution 12-band image can be computationally converted into a
hyperspectral image with uniformly high resolution (i.e., 10 m), it
significantly facilitates remote identification tasks. Though there are some
spectral super-resolution methods, they did not address the multi-resolution
issue on one hand, and, more seriously, they mostly focused on the CAVE-level
hyperspectral image reconstruction (involving only 31 visible bands) on the
other hand, greatly limiting their applicability in real-world remote sensing
scenarios. We ambitiously aim to convert Sentinel-2 data directly into NASA's
AVIRIS-level hyperspectral image (encompassing up to 172 visible and
near-infrared (NIR) bands, after ignoring those absorption/corruption ones).
For the first time, this paper solves this specific super-resolution problem
(highly ill-posed), allowing all historical Sentinel-2 data to have their
corresponding high-standard AVIRIS counterparts. We achieve so by customizing a
novel algorithm that introduces deep unfolding regularization and
Q-quadratic-norm regularization into the so-called convex/deep (CODE)
small-data learning criterion. Based on the derived spectral-spatial duality,
the proposed interpretable COS2A algorithm demonstrates superior spectral
super-resolution results across diverse land cover types, as validated through
extensive experiments.

</details>
