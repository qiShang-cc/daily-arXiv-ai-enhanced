{"id": "2508.16601", "pdf": "https://arxiv.org/pdf/2508.16601", "abs": "https://arxiv.org/abs/2508.16601", "authors": ["Marco Donald Migliore"], "title": "Notes on Deterministic and Stochastic Approaches in Electromagnetic Information Theory", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "This paper investigates the relationship between the Number of Degrees of\nFreedom ($N_{\\rm DoF}$) of the field in deterministic and stochastic source\nmodels within Electromagnetic Information Theory (EIT). Our findings\ndemonstrate a fundamental connection between these two approaches.\nSpecifically, we show that a deterministic model and a stochastic model with a\nspatially incoherent and homogeneous source yield not only the same $N_{\\rm\nDoF}$ but also identical eigenvalues and basis functions for field\nrepresentation. This key equivalence not only explains the effectiveness of\ndeterministic approaches in EIT but also corroborates the use of classical\nelectromagnetic methods within this new discipline.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7535\u78c1\u4fe1\u606f\u7406\u8bba\u4e2d\u786e\u5b9a\u6027\u6a21\u578b\u4e0e\u968f\u673a\u6a21\u578b\u7684\u81ea\u7531\u5ea6\u6570\u91cf\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u7a7a\u95f4\u975e\u76f8\u5e72\u4e14\u5747\u5300\u6e90\u6761\u4ef6\u4e0b\u5177\u6709\u76f8\u540c\u7684\u81ea\u7531\u5ea6\u3001\u7279\u5f81\u503c\u548c\u57fa\u51fd\u6570\u3002", "motivation": "\u65e8\u5728\u9a8c\u8bc1\u786e\u5b9a\u6027\u6a21\u578b\u4e0e\u968f\u673a\u6a21\u578b\u5728\u7535\u78c1\u4fe1\u606f\u7406\u8bba\u4e2d\u7684\u7b49\u4ef7\u6027\uff0c\u4ee5\u89e3\u91ca\u786e\u5b9a\u65b9\u6cd5\u5728\u8be5\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "method": "\u6bd4\u8f83\u786e\u5b9a\u6027\u6a21\u578b\u548c\u7a7a\u95f4\u975e\u76f8\u5e72\u5747\u5300\u6e90\u7684\u968f\u673a\u6a21\u578b\u4e2d\u7684\u81ea\u7531\u5ea6\u6570\u91cf\u3001\u7279\u5f81\u503c\u548c\u57fa\u51fd\u6570\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u6a21\u578b\u5728\u6240\u8ff0\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5b8c\u5168\u76f8\u540c\u7684\u81ea\u7531\u5ea6\u3001\u7279\u5f81\u503c\u548c\u57fa\u51fd\u6570\u3002", "conclusion": "\u8fd9\u4e00\u7b49\u4ef7\u6027\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u786e\u5b9a\u6027\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8fd8\u652f\u6301\u4e86\u7ecf\u5178\u7535\u78c1\u65b9\u6cd5\u5728\u65b0\u5b66\u79d1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.16735", "pdf": "https://arxiv.org/pdf/2508.16735", "abs": "https://arxiv.org/abs/2508.16735", "authors": ["Seyed Mohammad Amin Shirinbayan", "Gholamreza Moradi"], "title": "A Practical Approach to the Design of an S-Band Image-Rejecting Dual-Conversion Super-Heterodyne RF Chain of a Receiver Considering Spur Signals", "categories": ["eess.SP"], "comment": "18 pages, 31 figures, 13 tables, This preprint is being submitted\n  with the intention of publication in the Journal of Electrical Engineering &\n  Technology (Springer)", "summary": "This paper presents a typical design of the RF section of a radar receiver,\nthe chain within a superheterodyne dual-conversion architecture. A significant\nchallenge in this framework is the occurrence of spur signals, which negatively\nimpact the dynamic range of the RF chain. When addressing this issue, the paper\nintroduces an innovative approach to mitigate (or even wipe out) these\nundesired effects, utilizing two mutually verifying MATLAB codes. These codes\nhave been tested with two distinct commercial mixers and could be applied to\nany superheterodyne configuration with various mixers. The presented method\nmakes the Spurious-Free Dynamic Range (SFDR) of the chain the least different\nfrom the dynamic range of the chain. Also, the selection of other components\ngets optimized to align with spurious signals consideration, with explanations\nprovided for these choices. Moreover, two filters of the RF chain, the second\nand the third, have been designed to reduce implementation costs. Various\nMicrowave software and full-wave analyses were employed for detailed design and\nanalysis, with the results compared to evaluate their performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u96f7\u8fbe\u63a5\u6536\u673a\u5c04\u9891\u90e8\u5206\u7684\u53cc\u53d8\u9891\u67b6\u6784\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4e24\u79cdMATLAB\u4ee3\u7801\u6d88\u9664\u6742\u6563\u4fe1\u53f7\uff0c\u4f18\u5316SFDR\u4e0e\u52a8\u6001\u8303\u56f4\uff0c\u5e76\u964d\u4f4e\u5b9e\u73b0\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u8d85\u5916\u5dee\u53cc\u53d8\u9891\u67b6\u6784\u4e2d\u6742\u6563\u4fe1\u53f7\u5bf9\u5c04\u9891\u94fe\u52a8\u6001\u8303\u56f4\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u76f8\u4e92\u9a8c\u8bc1\u7684MATLAB\u4ee3\u7801\u4f18\u5316\u6742\u6563\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u5fae\u6ce2\u8f6f\u4ef6\u548c\u5168\u6ce2\u5206\u6790\u8fdb\u884c\u8be6\u7ec6\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u7ec4\u4ef6\u9009\u62e9\u548c\u6ee4\u6ce2\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u94fe\u8def\u7684SFDR\u5e76\u964d\u4f4e\u4e86\u5b9e\u73b0\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u6742\u6563\u4fe1\u53f7\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6df7\u9891\u5668\u7684\u8d85\u5916\u5dee\u914d\u7f6e\u3002"}}
{"id": "2508.16888", "pdf": "https://arxiv.org/pdf/2508.16888", "abs": "https://arxiv.org/abs/2508.16888", "authors": ["Jiazhe Li", "Nicol\u00f2 Decarli", "Francesco Guidi", "Anna Guerra", "Alessandro Bazzi", "Zhuoming Li"], "title": "Dual Orthogonal Projections-Based Multiuser Interference Cancellation for mmWave Beamforming in XL-MIMO Systems", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates multiuser interference (MUI) cancellation for\nmillimeter-wave (mmWave) beamforming in extremely large-scale multiple-input\nmultiple-output (XL-MIMO) communication systems. We propose a linear algorithm,\ntermed iterative dual orthogonal projections (DOP), which alternates between\ntwo orthogonal projections: one to eliminate MUI and the other to refine\ncombiners, ensuring a monotonic increase in spectral efficiency. Theoretical\nanalysis and simulation results show that, with each iteration, the signal\npower for each user increases monotonically, the equivalent noise power after\nreceive combining decreases monotonically, and the spectral efficiency improves\naccordingly and converges rapidly, closely approaching the theoretical optimum\ndetermined by dirty paper coding (DPC), outperforming existing linear\nalgorithms in spectral efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8fed\u4ee3\u53cc\u6b63\u4ea4\u6295\u5f71\uff08DOP\uff09\u7684\u7ebf\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2XL-MIMO\u7cfb\u7edf\u4e2d\u7684\u591a\u7528\u6237\u5e72\u6270\u6d88\u9664\uff0c\u901a\u8fc7\u4ea4\u66ff\u6b63\u4ea4\u6295\u5f71\u63d0\u5347\u9891\u8c31\u6548\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u3002", "motivation": "\u7814\u7a76\u8d85\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08XL-MIMO\uff09\u901a\u4fe1\u7cfb\u7edf\u4e2d\u6beb\u7c73\u6ce2\u6ce2\u675f\u6210\u5f62\u6280\u672f\u7684\u591a\u7528\u6237\u5e72\u6270\uff08MUI\uff09\u6d88\u9664\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u53cc\u6b63\u4ea4\u6295\u5f71\uff08DOP\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u6267\u884c\u6d88\u9664MUI\u548c\u4f18\u5316\u7ec4\u5408\u5668\u7684\u6b63\u4ea4\u6295\u5f71\uff0c\u5355\u8c03\u63d0\u5347\u9891\u8c31\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u663e\u793a\uff0c\u7b97\u6cd5\u6bcf\u6b21\u8fed\u4ee3\u5747\u5355\u8c03\u63d0\u5347\u7528\u6237\u4fe1\u53f7\u529f\u7387\u3001\u964d\u4f4e\u7b49\u6548\u566a\u58f0\uff0c\u9891\u8c31\u6548\u7387\u5feb\u901f\u6536\u655b\u5e76\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7ebf\u6027\u7b97\u6cd5\u3002", "conclusion": "DOP\u7b97\u6cd5\u5728XL-MIMO\u7cfb\u7edf\u4e2d\u6709\u6548\u89e3\u51b3\u4e86MUI\u95ee\u9898\uff0c\u9891\u8c31\u6548\u7387\u663e\u8457\u63d0\u5347\u4e14\u63a5\u8fd1\u7406\u8bba\u6781\u9650\uff0c\u4e3a\u6beb\u7c73\u6ce2\u901a\u4fe1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16946", "pdf": "https://arxiv.org/pdf/2508.16946", "abs": "https://arxiv.org/abs/2508.16946", "authors": ["Rashmi Kumari", "Gourab Ghatak", "Abhishek K. Gupta"], "title": "Spatially Correlated Blockage Aware Placement of RIS in IIoT Networks", "categories": ["eess.SP", "cs.NA", "math.NA", "60D05, 60G55, 68M10", "C.2.1; C.2.3; G.3"], "comment": "13 pages, 21 figures. A preliminary version of this work was accepted\n  in IEEE PIMRC 2025 under the title \"Blockage Aware Placement of RIS in IIoT\n  Networks\"", "summary": "We study the impact of deploying reconfigurable intelligent surfaces (RISs)\nin mitigating coverage gaps and enhancing transmission reliability in an\nindustrial internet of things (IIoT) network. First, we consider a single\nblockage scenario and characterize the correlation between blocking events of\nthe base station (BS)-user and the RIS-user links and study its impact on the\nprobability of establishing a viable reflected link. Then, by considering\nmultiple blockages, we derive the distribution of the signal to noise ratio\n(SNR) as a function of data size, blockage density, the number of RISs, and the\ndeployment area. We analyze the impact of normalized blockage radius and\nidentify the threshold beyond which the assumption of independent blockages\ndeviates from the ground truth of correlated blocking. Finally, we compare the\noutage performance of this RIS-assisted system with that operated with network-\ncontrolled relays, and demonstrate that while the relays provide a higher\nreliability beyond a certain blockage threshold, increasing the number of RISs\nmay help mitigate this effect. These insights offer valuable design guidelines\nfor deploying RIS-aided IIoT networks in dense blockage environments.", "AI": {"tldr": "\u7814\u7a76RIS\u5728IIoT\u7f51\u7edc\u4e2d\u51cf\u5c11\u8986\u76d6\u76f2\u533a\u3001\u63d0\u5347\u4f20\u8f93\u53ef\u9760\u6027\u7684\u4f5c\u7528\uff0c\u5206\u6790\u5355\u5757\u548c\u591a\u5757\u906e\u6321\u573a\u666f\u4e0bSNR\u5206\u5e03\u53ca\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u5728\u5de5\u4e1a\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\uff0c\u906e\u6321\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u8fde\u63a5\u4e2d\u65ad\uff0c\u7814\u7a76RIS\u5982\u4f55\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5355\u5757\u548c\u591a\u5757\u906e\u6321\u573a\u666f\u5efa\u6a21\uff0c\u5206\u6790RIS\u4e0e\u7528\u6237\u95f4\u94fe\u8def\u7684\u906e\u6321\u76f8\u5173\u6027\uff0c\u63a8\u5bfcSNR\u5206\u5e03\uff0c\u5e76\u4e0e\u4e2d\u7ee7\u7cfb\u7edf\u6bd4\u8f83\u3002", "result": "\u591a\u5757\u906e\u6321\u4e0b\uff0cRIS\u80fd\u6709\u6548\u6539\u5584\u6027\u80fd\uff0c\u4f46\u4e2d\u7ee7\u5728\u66f4\u9ad8\u906e\u6321\u9608\u503c\u4e0b\u8868\u73b0\u66f4\u4f18\uff1b\u589e\u52a0RIS\u6570\u91cf\u53ef\u7f13\u89e3\u5176\u52a3\u52bf\u3002", "conclusion": "RIS\u5728\u906e\u6321\u5bc6\u96c6\u7684IIoT\u7f51\u7edc\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u8bbe\u8ba1\u65f6\u9700\u5e73\u8861RIS\u6570\u91cf\u4e0e\u4e2d\u7ee7\u7cfb\u7edf\u7684\u9009\u62e9\u3002"}}
{"id": "2508.16731", "pdf": "https://arxiv.org/pdf/2508.16731", "abs": "https://arxiv.org/abs/2508.16731", "authors": ["Daniel McGann", "Easton R. Potokar", "Michael Kaess"], "title": "COSMO-Bench: A Benchmark for Collaborative SLAM Optimization", "categories": ["cs.RO"], "comment": null, "summary": "Recent years have seen a focus on research into distributed optimization\nalgorithms for multi-robot Collaborative Simultaneous Localization and Mapping\n(C-SLAM). Research in this domain, however, is made difficult by a lack of\nstandard benchmark datasets. Such datasets have been used to great effect in\nthe field of single-robot SLAM, and researchers focused on multi-robot problems\nwould benefit greatly from dedicated benchmark datasets. To address this gap,\nwe design and release the Collaborative Open-Source Multi-robot Optimization\nBenchmark (COSMO-Bench) -- a suite of 24 datasets derived from a\nstate-of-the-art C-SLAM front-end and real-world LiDAR data. Data DOI:\nhttps://doi.org/10.1184/R1/29652158", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9\u591a\u673a\u5668\u4eba\u534f\u4f5cSLAM\uff08C-SLAM\uff09\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\u7814\u7a76\u4e2d\u7f3a\u4e4f\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCOSMO-Bench\u7684\u5f00\u653e\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4ebaC-SLAM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u548c\u53d1\u5e03COSMO-Bench\uff0c\u5305\u542b24\u4e2a\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u5148\u8fdb\u7684C-SLAM\u524d\u7aef\u7b97\u6cd5\u548c\u771f\u5b9eLiDAR\u6570\u636e\u3002", "result": "COSMO-Bench\u4e3a\u591a\u673a\u5668\u4ebaC-SLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "COSMO-Bench\u586b\u8865\u4e86\u591a\u673a\u5668\u4ebaC-SLAM\u7814\u7a76\u4e2d\u7684\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.17051", "pdf": "https://arxiv.org/pdf/2508.17051", "abs": "https://arxiv.org/abs/2508.17051", "authors": ["Christopher Saetia", "Daniel M. Dobkin", "Gregory Durgin"], "title": "Radio Frequency Identification: Decades at a Time", "categories": ["eess.SP"], "comment": null, "summary": "In this article, we briefly review the history of the use of radio signals to\nidentify objects, and of the key Radio Frequency Identification (RFID)\nstandards for ultra-high-frequency (UHF) and near-field communications that\nenabled broad use of these technologies in daily life. We will compare the\nvision for the future presented by the Auto-ID Lab in the early 21st century\nwith the reality we see today, two decades and a little after. We will review\nsome of the applications in which UHF RFID technology has become hugely\nsuccessful, others where High Frequency Near-field Communications (HF NFC) is\npreferred, and applications where optical identification or active wireless\ncommunications are dominant.\n  We will then examine some possible future paths for RFID technology. We\nanticipate that UHF read capability will become widely available for\ncellphones, making it as universal as NFC and Bluetooth are today. We will look\nat more sophisticated radio interfaces, such as multiple-antenna phased arrays\nfor readers, and tunnel diode reflection for tags. We will discuss the\nintegration of information from Artificial Intelligence (AI)-based image\nprocessing, barcodes, NFC and UHF tags, into a digital twin of the real\nenvironment experienced by the human user. We will examine the role of RFID\nwith sensing in improving the management of perishable goods. The role that\nRFID might play in a truly circular economy, with intelligent recycling and\nreuse, will be discussed. Finally, we survey the many hazards and obstacles\nthat obstruct the path to an RF-informed future.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5c04\u9891\u8bc6\u522b\uff08RFID\uff09\u6280\u672f\u7684\u53d1\u5c55\u5386\u53f2\u3001\u5e94\u7528\u73b0\u72b6\u4ee5\u53ca\u672a\u6765\u53ef\u80fd\u7684\u8def\u5f84\uff0c\u5305\u62ecUHF\u548cHF NFC\u6280\u672f\u7684\u6bd4\u8f83\uff0c\u4ee5\u53caRFID\u4e0eAI\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u6280\u672f\u7684\u7ed3\u5408\u524d\u666f\u3002", "motivation": "\u63a2\u8ba8RFID\u6280\u672f\u4ece\u65e9\u671f\u613f\u666f\u5230\u5f53\u524d\u5b9e\u9645\u5e94\u7528\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u53ef\u80fd\u7684\u521b\u65b0\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5386\u53f2\u3001\u6bd4\u8f83\u73b0\u6709\u5e94\u7528\u548c\u6280\u672f\uff0c\u5206\u6790\u672a\u6765RFID\u6280\u672f\u7684\u6f5c\u5728\u53d1\u5c55\u65b9\u5411\u3002", "result": "\u603b\u7ed3\u4e86RFID\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u6210\u529f\u6848\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6280\u672f\u6574\u5408\uff08\u5982AI\u3001\u6570\u5b57\u5b6a\u751f\uff09\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "RFID\u6280\u672f\u5728\u672a\u6765\u4ecd\u6709\u8bb8\u591a\u53d1\u5c55\u6f5c\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002"}}
{"id": "2508.16749", "pdf": "https://arxiv.org/pdf/2508.16749", "abs": "https://arxiv.org/abs/2508.16749", "authors": ["Victor-Louis De Gusseme", "Thomas Lips", "Remko Proesmans", "Julius Hietala", "Giwan Lee", "Jiyoung Choi", "Jeongil Choi", "Geon Kim", "Phayuth Yonrith", "Domen Tabernik", "Andrej Gams", "Peter Nimac", "Matej Urbas", "Jon Muhovi\u010d", "Danijel Sko\u010daj", "Matija Mavsar", "Hyojeong Yu", "Minseo Kwon", "Young J. Kim", "Yang Cong", "Ronghan Chen", "Yu Ren", "Supeng Diao", "Jiawei Weng", "Jiayue Liu", "Haoran Sun", "Linhan Yang", "Zeqing Zhang", "Ning Guo", "Lei Yang", "Fang Wan", "Chaoyang Song", "Jia Pan", "Yixiang Jin", "Yong A", "Jun Shi", "Dingzhe Li", "Yong Yang", "Kakeru Yamasaki", "Takumi Kajiwara", "Yuki Nakadera", "Krati Saxena", "Tomohiro Shibata", "Chongkun Xia", "Kai Mo", "Yanzhao Yu", "Qihao Lin", "Binqiang Ma", "Uihun Sagong", "JungHyun Choi", "JeongHyun Park", "Dongwoo Lee", "Yeongmin Kim", "Myun Joong Hwang", "Yusuke Kuribayashi", "Naoki Hiratsuka", "Daisuke Tanaka", "Solvi Arnold", "Kimitoshi Yamazaki", "Carlos Mateo-Agullo", "Andreas Verleysen", "Francis Wyffels"], "title": "A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition", "categories": ["cs.RO"], "comment": "submitted to IJRR", "summary": "Robotic cloth manipulation suffers from a lack of standardized benchmarks and\nshared datasets for evaluating and comparing different approaches. To address\nthis, we created a benchmark and organized the ICRA 2024 Cloth Competition, a\nunique head-to-head evaluation focused on grasp pose selection for in-air\nrobotic cloth unfolding. Eleven diverse teams participated in the competition,\nutilizing our publicly released dataset of real-world robotic cloth unfolding\nattempts and a variety of methods to design their unfolding approaches.\nAfterwards, we also expanded our dataset with 176 competition evaluation\ntrials, resulting in a dataset of 679 unfolding demonstrations across 34\ngarments. Analysis of the competition results revealed insights about the\ntrade-off between grasp success and coverage, the surprisingly strong\nachievements of hand-engineered methods and a significant discrepancy between\ncompetition performance and prior work, underscoring the importance of\nindependent, out-of-the-lab evaluation in robotic cloth manipulation. The\nassociated dataset is a valuable resource for developing and evaluating grasp\nselection methods, particularly for learning-based approaches. We hope that our\nbenchmark, dataset and competition results can serve as a foundation for future\nbenchmarks and drive further progress in data-driven robotic cloth\nmanipulation. The dataset and benchmarking code are available at\nhttps://airo.ugent.be/cloth_competition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u7684\u6807\u51c6\u5316\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u7ec4\u7ec7\u4e86ICRA 2024\u5e03\u6599\u7ade\u8d5b\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u56e2\u961f\u7684\u8868\u73b0\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u548c\u5171\u4eab\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ec4\u7ec7\u4e86\u7ade\u8d5b\uff0c\u9080\u8bf7\u4e8611\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u5e03\u6599\u5c55\u5f00\u4efb\u52a1\u7684\u6570\u636e\u96c6\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u4e86\u7ade\u8d5b\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u6293\u53d6\u6210\u529f\u7387\u548c\u8986\u76d6\u8303\u56f4\u7684\u6743\u8861\uff0c\u624b\u5de5\u8bbe\u8ba1\u65b9\u6cd5\u7684\u51fa\u8272\u8868\u73b0\uff0c\u4ee5\u53ca\u4e0e\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u7ade\u8d5b\u7ed3\u679c\u4e3a\u672a\u6765\u57fa\u51c6\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u7814\u7a76\u6709\u91cd\u8981\u4ef7\u503c\u3002\u6570\u636e\u53ca\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.17246", "pdf": "https://arxiv.org/pdf/2508.17246", "abs": "https://arxiv.org/abs/2508.17246", "authors": ["Takuma Sumi", "Georgi S. Medvedev"], "title": "Graphon Signal Processing for Spiking and Biological Neural Networks", "categories": ["eess.SP"], "comment": "20 pages, 10 figures", "summary": "Graph Signal Processing (GSP) extends classical signal processing to signals\ndefined on graphs, enabling filtering, spectral analysis, and sampling of data\ngenerated by networks of various kinds. Graphon Signal Processing (GnSP)\ndevelops this framework further by employing the theory of graphons. Graphons\nare measurable functions on the unit square that represent graphs and limits of\nconvergent graph sequences. The use of graphons provides stability of GSP\nmethods to stochastic variability in network data and improves computational\nefficiency for very large networks. We use GnSP to address the stimulus\nidentification problem (SIP) in computational and biological neural networks.\nThe SIP is an inverse problem that aims to infer the unknown stimulus s from\nthe observed network output f. We first validate the approach in spiking neural\nnetwork simulations and then analyze calcium imaging recordings. Graphon-based\nspectral projections yield trial-invariant, lowdimensional embeddings that\nimprove stimulus classification over Principal Component Analysis and discrete\nGSP baselines. The embeddings remain stable under variations in network\nstochasticity, providing robustness to different network sizes and noise\nlevels. To the best of our knowledge, this is the first application of GnSP to\nbiological neural networks, opening new avenues for graphon-based analysis in\nneuroscience.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8bba\u4fe1\u53f7\u5904\u7406\uff08GSP\uff09\u548c\u56fe\u5b50\uff08Graphons\uff09\u7684\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff08GnSP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8ba1\u7b97\u548c\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u523a\u6fc0\u8bc6\u522b\u95ee\u9898\uff08SIP\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u56fe\u4fe1\u53f7\u5904\u7406\uff08GSP\uff09\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5927\u89c4\u6a21\u7f51\u7edc\u548c\u968f\u673a\u6027\u7684\u7a33\u5b9a\u5904\u7406\u3002\u56fe\u5b50\uff08Graphons\uff09\u80fd\u591f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u8bba\u6587\u5f15\u5165GnSP\u6846\u67b6\uff0c\u5e76\u9996\u6b21\u5c06\u5176\u5e94\u7528\u4e8e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002", "method": "\u8bba\u6587\u4f7f\u7528GnSP\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u5b50\u7684\u8c31\u6295\u5f71\u65b9\u6cd5\uff0c\u4ece\u7f51\u7edc\u8f93\u51fa\u4e2d\u63a8\u65ad\u672a\u77e5\u523a\u6fc0\u3002\u65b9\u6cd5\u5206\u522b\u5728\u6a21\u62df\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u9499\u6210\u50cf\u8bb0\u5f55\u4e2d\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u79bb\u6563GSP\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0c\u57fa\u4e8e\u56fe\u5b50\u7684\u4f4e\u7ef4\u5d4c\u5165\u5728\u523a\u6fc0\u5206\u7c7b\u4e0a\u4f18\u4e8ePCA\u548c\u79bb\u6563GSP\uff0c\u4e14\u5728\u7f51\u7edc\u89c4\u6a21\u548c\u566a\u58f0\u53d8\u5316\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5c06GnSP\u5e94\u7528\u4e8e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3SIP\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u7684\u56fe\u5b50\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.16807", "pdf": "https://arxiv.org/pdf/2508.16807", "abs": "https://arxiv.org/abs/2508.16807", "authors": ["Marco S. Tayar", "Lucas K. de Oliveira", "Juliano D. Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Inspecting confined industrial infrastructure, such as ventilation shafts, is\na hazardous and inefficient task for humans. Unmanned Aerial Vehicles (UAVs)\noffer a promising alternative, but GPS-denied environments require robust\ncontrol policies to prevent collisions. Deep Reinforcement Learning (DRL) has\nemerged as a powerful framework for developing such policies, and this paper\nprovides a comparative study of two leading DRL algorithms for this task: the\non-policy Proximal Policy Optimization (PPO) and the off-policy Soft\nActor-Critic (SAC). The training was conducted with procedurally generated duct\nenvironments in Genesis simulation environment. A reward function was designed\nto guide a drone through a series of waypoints while applying a significant\npenalty for collisions. PPO learned a stable policy that completed all\nevaluation episodes without collision, producing smooth trajectories. By\ncontrast, SAC consistently converged to a suboptimal behavior that traversed\nonly the initial segments before failure. These results suggest that, in\nhazard-dense navigation, the training stability of on-policy methods can\noutweigh the nominal sample efficiency of off-policy algorithms. More broadly,\nthe study provides evidence that procedurally generated, high-fidelity\nsimulations are effective testbeds for developing and benchmarking robust\nnavigation policies.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cdDRL\u7b97\u6cd5\uff08PPO\u548cSAC\uff09\u5728GPS-denied\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u8868\u73b0\uff0c\u53d1\u73b0PPO\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "motivation": "\u4eba\u7c7b\u68c0\u67e5\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u901a\u98ce\u4e95\uff09\u6548\u7387\u4f4e\u4e14\u5371\u9669\uff0c\u65e0\u4eba\u673a\u662f\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4fGPS\u65f6\u9700\u8981\u7a33\u5065\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u4f7f\u7528Genesis\u6a21\u62df\u73af\u5883\u751f\u6210\u7ba1\u9053\u73af\u5883\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u65e0\u4eba\u673a\u901a\u8fc7\u8def\u5f84\u70b9\u5e76\u907f\u514d\u78b0\u649e\uff0c\u6bd4\u8f83PPO\u548cSAC\u7684\u8868\u73b0\u3002", "result": "PPO\u5b66\u4f1a\u7684\u7a33\u5b9a\u7b56\u7565\u80fd\u5b8c\u6210\u6240\u6709\u8bc4\u4f30\u4e14\u65e0\u78b0\u649e\uff0c\u800cSAC\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0cPPO\u7684\u7a33\u5b9a\u6027\u4f18\u4e8eSAC\u7684\u6837\u672c\u6548\u7387\uff1b\u9ad8\u4fdd\u771f\u6a21\u62df\u662f\u5f00\u53d1\u5bfc\u822a\u7b56\u7565\u7684\u6709\u6548\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2508.17354", "pdf": "https://arxiv.org/pdf/2508.17354", "abs": "https://arxiv.org/abs/2508.17354", "authors": ["Jun Wu", "Weijie Yuan", "Xiaoqi Zhang", "Yaohuan Yu", "Yuanhao Cui", "Fan Liu", "Geng Sun", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim"], "title": "Toward Multi-Functional LAWNs with ISAC: Opportunities, Challenges, and the Road Ahead", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) has been envisioned as a\nfoundational technology for future low-altitude wireless networks (LAWNs),\nenabling real-time environmental perception and data exchange across\naerial-ground systems. In this article, we first explore the roles of ISAC in\nLAWNs from both node-level and network-level perspectives. We highlight the\nperformance gains achieved through hierarchical integration and cooperation,\nwherein key design trade-offs are demonstrated. Apart from physical-layer\nenhancements, emerging LAWN applications demand broader functionalities. To\nthis end, we propose a multi-functional LAWN framework that extends ISAC with\ncapabilities in control, computation, wireless power transfer, and large\nlanguage model (LLM)-based intelligence. We further provide a representative\ncase study to present the benefits of ISAC-enabled LAWNs and the promising\nresearch directions are finally outlined.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5728\u672a\u6765\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u591a\u529f\u80fd\u6846\u67b6\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76ISAC\u6280\u672f\u5728LAWNs\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u73af\u5883\u611f\u77e5\u548c\u7a7a\u5730\u7cfb\u7edf\u6570\u636e\u4ea4\u6362\u7684\u9700\u6c42\u3002", "method": "\u4ece\u8282\u70b9\u548c\u7f51\u7edc\u5c42\u9762\u5206\u6790ISAC\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u591a\u529f\u80fd\u6846\u67b6\uff08\u5305\u62ec\u63a7\u5236\u3001\u8ba1\u7b97\u3001\u65e0\u7ebf\u80fd\u91cf\u4f20\u8f93\u548cLLM\u667a\u80fd\uff09\u3002", "result": "\u901a\u8fc7\u5206\u5c42\u96c6\u6210\u4e0e\u5408\u4f5c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86ISAC-enabled LAWN\u7684\u6f5c\u529b\u3002", "conclusion": "ISAC\u6280\u672f\u5728LAWNs\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5176\u591a\u529f\u80fd\u6269\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.16856", "pdf": "https://arxiv.org/pdf/2508.16856", "abs": "https://arxiv.org/abs/2508.16856", "authors": ["Zubair Islam", "Ahmaad Ansari", "George Daoud", "Mohamed El-Darieby"], "title": "A Workflow for Map Creation in Autonomous Vehicle Simulations", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": "6 pages, 12 figures. Published in the Proceedings of GEOProcessing\n  2025: The Seventeenth International Conference on Advanced Geographic\n  Information Systems, Applications, and Services (IARIA)", "summary": "The fast development of technology and artificial intelligence has\nsignificantly advanced Autonomous Vehicle (AV) research, emphasizing the need\nfor extensive simulation testing. Accurate and adaptable maps are critical in\nAV development, serving as the foundation for localization, path planning, and\nscenario testing. However, creating simulation-ready maps is often difficult\nand resource-intensive, especially with simulators like CARLA (CAR Learning to\nAct). Many existing workflows require significant computational resources or\nrely on specific simulators, limiting flexibility for developers. This paper\npresents a custom workflow to streamline map creation for AV development,\ndemonstrated through the generation of a 3D map of a parking lot at Ontario\nTech University. Future work will focus on incorporating SLAM technologies,\noptimizing the workflow for broader simulator compatibility, and exploring more\nflexible handling of latitude and longitude values to enhance map generation\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u4e2d\u5730\u56fe\u751f\u6210\u7684\u5b9a\u5236\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5b89\u5927\u7565\u7406\u5de5\u5927\u5b66\u76843D\u505c\u8f66\u573a\u5730\u56fe\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u5173\u6ce8SLAM\u6280\u672f\u7684\u96c6\u6210\u548c\u4f18\u5316\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7814\u7a76\u4e2d\uff0c\u7cbe\u786e\u4e14\u53ef\u9002\u5e94\u7684\u5730\u56fe\u662f\u5f00\u53d1\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u7b80\u5316\u548c\u4f18\u5316\u4eff\u771f\u5c31\u7eea\u5730\u56fe\u7684\u751f\u6210\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5b89\u5927\u7565\u7406\u5de5\u5927\u5b66\u505c\u8f66\u573a\u76843D\u5730\u56fe\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5730\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u589e\u5f3a\u517c\u5bb9\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.17526", "pdf": "https://arxiv.org/pdf/2508.17526", "abs": "https://arxiv.org/abs/2508.17526", "authors": ["Kangda Zhi", "Tianyu Yang", "Shuangyang Li", "Yi Song", "Amir Rezaei", "Giuseppe Caire"], "title": "Near-Field Integrated Imaging and Communication in Distributed MIMO Networks", "categories": ["eess.SP"], "comment": "18 pages, 15 figures", "summary": "In this work, we propose a general framework for wireless imaging in\ndistributed MIMO wideband communication systems, considering multi-view\nnon-isotropic targets and near-field propagation effects. For indoor scenarios\nwhere the objective is to image small-scale objects with high resolution, we\npropose a range migration algorithm (RMA)-based scheme using three kinds of\narray architectures: the full array, boundary array, and distributed boundary\narray. With non-isotropic near-field channels, we establish the Fourier\ntransformation (FT)-based relationship between the imaging reflectivity and the\ndistributed spatial-domain signals and discuss the corresponding theoretical\nproperties. Next, for outdoor scenarios where the objective is to reconstruct\nthe large-scale three-dimensional (3D) environment with coarse resolution, we\npropose a sparse Bayesian learning (SBL)-based algorithm to solve the multiple\nmeasurement vector (MMV) problem, which further addresses the non-isotropic\nreflectivity across different subcarriers. Numerical results demonstrate the\neffectiveness of the proposed algorithms in acquiring high-resolution small\nobjects and accurately reconstructing large-scale environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5206\u5e03\u5f0fMIMO\u5bbd\u5e26\u901a\u4fe1\u7cfb\u7edf\u7684\u65e0\u7ebf\u6210\u50cf\u6846\u67b6\uff0c\u9488\u5bf9\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u5206\u522b\u8bbe\u8ba1\u4e86\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u548c\u7c97\u5206\u8fa8\u73873D\u73af\u5883\u91cd\u6784\u7684\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0fMIMO\u7cfb\u7edf\u4e2d\u975e\u5404\u5411\u540c\u6027\u76ee\u6807\u548c\u8fd1\u573a\u4f20\u64ad\u6548\u5e94\u7684\u65e0\u7ebf\u6210\u50cf\u95ee\u9898\u3002", "method": "\u5ba4\u5185\u573a\u666f\u91c7\u7528\u57fa\u4e8eRMA\u7684\u65b9\u6848\uff1b\u5ba4\u5916\u573a\u666f\u91c7\u7528\u57fa\u4e8eSBL\u7684\u7b97\u6cd5\u89e3\u51b3MMV\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u7b97\u6cd5\u80fd\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u5c0f\u7269\u4f53\u5e76\u51c6\u786e\u91cd\u6784\u5927\u5c3a\u5ea6\u73af\u5883\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u65e0\u7ebf\u6210\u50cf\u9700\u6c42\u3002"}}
{"id": "2508.16901", "pdf": "https://arxiv.org/pdf/2508.16901", "abs": "https://arxiv.org/abs/2508.16901", "authors": ["David Baxter", "Aldo Ter\u00e1n Espinoza", "Antonio Ter\u00e1n Espinoza", "Amy Loutfi", "John Folkesson", "Peter Sigray", "Stephanie Lowry", "Jakob Kuttenkeuler"], "title": "Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY", "I.2.9; I.2.8; F.2.2"], "comment": "10 pages, 7 figures. Equal contribution by David Baxter and Aldo\n  Ter\\'an Espinoza. Supported by SAAB, SMaRC, and WASP. Supported by SAAB and\n  the Swedish Maritime Robotics Centre (SMaRC), and by the Wallenberg AI,\n  Autonomous Systems and Software Program (WASP) funded by the Knut and Alice\n  Wallenberg Foundation", "summary": "Estimating a target's 6-DoF motion in underwater proximity operations is\ndifficult because the chaser lacks target-side proprioception and the available\nrelative observations are sparse, noisy, and often partial (e.g., Ultra-Short\nBaseline (USBL) positions). Without a motion prior, factor-graph maximum a\nposteriori estimation is underconstrained: consecutive target states are weakly\nlinked and orientation can drift. We propose a generalized constant-twist\nmotion prior defined on the tangent space of Lie groups that enforces\ntemporally consistent trajectories across all degrees of freedom; in SE(3) it\ncouples translation and rotation in the body frame. We present a ternary factor\nand derive its closed-form Jacobians based on standard Lie group operations,\nenabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two\ndeployment modes: (A) an SE(3)-only representation that regularizes orientation\neven when only position is measured, and (B) a mode with boundary factors that\nswitches the target representation between SE(3) and 3D position while applying\nthe same generalized constant-twist prior across representation changes.\nValidation on a real-world dynamic docking scenario dataset shows consistent\nego-target trajectory estimation through USBL-only and optical relative\nmeasurement segments with an improved relative tracking accuracy compared to\nthe noisy measurements to the target. Because the construction relies on\nstandard Lie group primitives, it is portable across state manifolds and\nsensing modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u674e\u7fa4\u5207\u7ebf\u7a7a\u95f4\u7684\u5e7f\u4e49\u6052\u5b9a\u626d\u66f2\u8fd0\u52a8\u5148\u9a8c\uff0c\u7528\u4e8e\u6c34\u4e0b\u63a5\u8fd1\u64cd\u4f5c\u4e2d\u76846\u81ea\u7531\u5ea6\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u3001\u566a\u58f0\u548c\u90e8\u5206\u89c2\u6d4b\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6c34\u4e0b\u63a5\u8fd1\u64cd\u4f5c\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u76ee\u6807\u7aef\u7684\u672c\u4f53\u611f\u77e5\uff0c\u4e14\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u3001\u566a\u58f0\u5927\u4e14\u90e8\u5206\u7f3a\u5931\uff08\u5982USBL\u4f4d\u7f6e\uff09\uff0c6\u81ea\u7531\u5ea6\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u6052\u5b9a\u626d\u66f2\u8fd0\u52a8\u5148\u9a8c\uff0c\u57fa\u4e8e\u674e\u7fa4\u5207\u7ebf\u7a7a\u95f4\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u7684\u8f68\u8ff9\uff1b\u8bbe\u8ba1\u4e86\u4e09\u5143\u56e0\u5b50\u53ca\u5176\u95ed\u5f0f\u96c5\u53ef\u6bd4\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u674e\u7fa4\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u52a8\u6001\u505c\u9760\u573a\u666f\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u901a\u8fc7USBL\u548c\u5149\u5b66\u76f8\u5bf9\u6d4b\u91cf\u6bb5\u5b9e\u73b0\u4e00\u81f4\u7684\u76ee\u6807\u8f68\u8ff9\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u76f8\u5bf9\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6807\u51c6\u674e\u7fa4\u64cd\u4f5c\uff0c\u5177\u6709\u53ef\u79fb\u690d\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u72b6\u6001\u6d41\u5f62\u548c\u611f\u77e5\u6a21\u6001\u3002"}}
{"id": "2508.17607", "pdf": "https://arxiv.org/pdf/2508.17607", "abs": "https://arxiv.org/abs/2508.17607", "authors": ["Yankai Zhang", "Jiafeng Ding", "Jingjing Ning", "Qiaoxi Zhu"], "title": "Steerable Invariant Beamformer Using a Differential Line Array of Omnidirectional and Directional Microphones with Null Constraints", "categories": ["eess.SP"], "comment": "12 pages, 15 figures", "summary": "Line differential microphone arrays have attracted attention for their\nability to achieve frequency-invariant beampatterns and high directivity.\nRecently, the Jacobi-Anger expansion-based approach has enabled the design of\nfully steerable-invariant differential beamformers for line arrays combining\nomnidirectional and directional microphones. However, this approach relies on\nthe analytical expression of the ideal beam pattern and the proper selection of\ntruncation order, which is not always practical. This paper introduces a\nnull-constraint-based method for designing frequency- and steerable-invariant\ndifferential beamformers using a line array of omnidirectional and directional\nmicrophones. The approach employs a multi-constraint optimisation framework,\nwhere the reference filter and ideal beam pattern are first determined based on\nspecified nulls and desired direction. Subsequently, the white noise gain\nconstraint is derived from the reference filter, and the beampattern constraint\nis from the ideal beam pattern. The optimal filter is then obtained by\nconsidering constraints related to the beampattern, nulls, and white noise\ngain. This method achieves a balance between white noise gain and mean square\nerror, allowing robust, frequency- and steerableinvariant differential\nbeamforming performance. It addresses limitations in beam pattern flexibility\nand truncation errors, offering greater design freedom and improved practical\napplicability. Simulations and experiments demonstrate that this method\noutperforms the Jacobi-Anger expansion-based approach in three key aspects: an\nextended effective range, improved main lobe and null alignment, and greater\nflexibility in microphone array configuration and beam pattern design,\nrequiring only steering direction and nulls instead of an analytic beam pattern\nexpression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u9891\u7387\u548c\u65b9\u5411\u4e0d\u53d8\u5dee\u5206\u6ce2\u675f\u6210\u5f62\u5668\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8bbe\u8ba1\u81ea\u7531\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684Jacobi-Anger\u5c55\u5f00\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7406\u60f3\u7684\u6ce2\u675f\u6a21\u5f0f\u89e3\u6790\u8868\u8fbe\u548c\u622a\u65ad\u987a\u5e8f\u9009\u62e9\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u57fa\u4e8e\u6307\u5b9a\u7684\u96f6\u70b9\u548c\u671f\u671b\u65b9\u5411\u786e\u5b9a\u53c2\u8003\u6ee4\u6ce2\u5668\u548c\u7406\u60f3\u6ce2\u675f\u6a21\u5f0f\uff0c\u7ed3\u5408\u767d\u566a\u58f0\u589e\u76ca\u7ea6\u675f\u548c\u6ce2\u675f\u6a21\u5f0f\u7ea6\u675f\uff0c\u901a\u8fc7\u4f18\u5316\u6ee4\u6ce2\u5668\u4e2d\u5404\u7ea6\u675f\u6761\u4ef6\uff0c\u5b9e\u73b0\u6027\u80fd\u5e73\u8861\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u6709\u6548\u8303\u56f4\u3001\u4e3b\u74e3\u548c\u96f6\u70b9\u5bf9\u9f50\u4ee5\u53ca\u8bbe\u8ba1\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u65e0\u9700\u89e3\u6790\u6ce2\u675f\u6a21\u5f0f\u8868\u8fbe\u3002", "conclusion": "\u96f6\u7ea6\u675f\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5dee\u5206\u6ce2\u675f\u6210\u5f62\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5927\u81ea\u7531\u5ea6\u3002"}}
{"id": "2508.16943", "pdf": "https://arxiv.org/pdf/2508.16943", "abs": "https://arxiv.org/abs/2508.16943", "authors": ["Haozhuo Zhang", "Jingkai Sun", "Michele Caprio", "Jian Tang", "Shanghang Zhang", "Qiang Zhang", "Wei Pan"], "title": "HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page:\n  https://haozhuo-zhang.github.io/HumanoidVerse-project-page/", "summary": "We introduce HumanoidVerse, a novel framework for vision-language guided\nhumanoid control that enables a single physically simulated robot to perform\nlong-horizon, multi-object rearrangement tasks across diverse scenes. Unlike\nprior methods that operate in fixed settings with single-object interactions,\nour approach supports consecutive manipulation of multiple objects, guided only\nby natural language instructions and egocentric camera RGB observations.\nHumanoidVerse is trained via a multi-stage curriculum using a dual-teacher\ndistillation pipeline, enabling fluid transitions between sub-tasks without\nrequiring environment resets. To support this, we construct a large-scale\ndataset comprising 350 multi-object tasks spanning four room layouts. Extensive\nexperiments in the Isaac Gym simulator demonstrate that our method\nsignificantly outperforms prior state-of-the-art in both task success rate and\nspatial precision, and generalizes well to unseen environments and\ninstructions. Our work represents a key step toward robust, general-purpose\nhumanoid agents capable of executing complex, sequential tasks under real-world\nsensory constraints. The video visualization results can be found on the\nproject page: https://haozhuo-zhang.github.io/HumanoidVerse-project-page/.", "AI": {"tldr": "HumanoidVerse\u662f\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5f15\u5bfc\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u652f\u6301\u591a\u7269\u4f53\u957f\u65f6\u95f4\u91cd\u6392\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u56fa\u5b9a\u573a\u666f\u548c\u5355\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u591a\u7269\u4f53\u8fde\u7eed\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u548c\u53cc\u6559\u5e08\u84b8\u998f\u7ba1\u9053\u8bad\u7ec3\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548cRGB\u89c6\u89c9\u8f93\u5165\u3002", "result": "\u5728Isaac Gym\u6a21\u62df\u5668\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a7a\u95f4\u7cbe\u5ea6\u5747\u6709\u63d0\u5347\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u590d\u6742\u3001\u987a\u5e8f\u4efb\u52a1\u6267\u884c\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4ee3\u7406\u8fc8\u51fa\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2508.17640", "pdf": "https://arxiv.org/pdf/2508.17640", "abs": "https://arxiv.org/abs/2508.17640", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Henk Wymeersch"], "title": "Multimodal Radio and Vision Fusion for Robust Localization in Urban V2I Communications", "categories": ["eess.SP"], "comment": "6 pages, 6 figures, submitted to conference", "summary": "Accurate localization is critical for vehicle-to-infrastructure (V2I)\ncommunication systems, especially in urban areas where GPS signals are often\nobstructed by tall buildings, leading to significant positioning errors,\nnecessitating alternative or complementary techniques for reliable and precise\npositioning in applications like autonomous driving and smart city\ninfrastructure. This paper proposes a multimodal contrastive learning\nregression based localization framework for V2I scenarios that combines channel\nstate information (CSI) with visual information to achieve improved accuracy\nand reliability. The approach leverages the complementary strengths of wireless\nand visual data to overcome the limitations of traditional localization\nmethods, offering a robust solution for V2I applications. Simulation results\ndemonstrate that the proposed CSI and vision fusion model significantly\noutperforms traditional methods and single modal models, achieving superior\nlocalization accuracy and precision in complex urban environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u56de\u5f52\u7684V2I\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408CSI\u4e0e\u89c6\u89c9\u4fe1\u606f\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2dGPS\u4fe1\u53f7\u6613\u53d7\u906e\u6321\uff0c\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u786e\u4fdd\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u6167\u57ce\u5e02\u3002", "method": "\u5229\u7528\u65e0\u7ebf\u548c\u89c6\u89c9\u6570\u636e\u7684\u4e92\u8865\u6027\uff0c\u5efa\u7acb\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u56de\u5f52\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u7ed3\u5408CSI\u4e0e\u89c6\u89c9\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u4e3aV2I\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16947", "pdf": "https://arxiv.org/pdf/2508.16947", "abs": "https://arxiv.org/abs/2508.16947", "authors": ["Fan Ding", "Xuewen Luo", "Hwa Hui Tew", "Ruturaj Reddy", "Xikun Wang", "Junn Yong Loo"], "title": "Drive As You Like: Strategy-Level Motion Planning Based on A Multi-Head Diffusion Model", "categories": ["cs.RO", "cs.AI"], "comment": "Has been submitted to AAAI 2026", "summary": "Recent advances in motion planning for autonomous driving have led to models\ncapable of generating high-quality trajectories. However, most existing\nplanners tend to fix their policy after supervised training, leading to\nconsistent but rigid driving behaviors. This limits their ability to reflect\nhuman preferences or adapt to dynamic, instruction-driven demands. In this\nwork, we propose a diffusion-based multi-head trajectory planner(M-diffusion\nplanner). During the early training stage, all output heads share weights to\nlearn to generate high-quality trajectories. Leveraging the probabilistic\nnature of diffusion models, we then apply Group Relative Policy Optimization\n(GRPO) to fine-tune the pre-trained model for diverse policy-specific\nbehaviors. At inference time, we incorporate a large language model (LLM) to\nguide strategy selection, enabling dynamic, instruction-aware planning without\nswitching models. Closed-loop simulation demonstrates that our post-trained\nplanner retains strong planning capability while achieving state-of-the-art\n(SOTA) performance on the nuPlan val14 benchmark. Open-loop results further\nshow that the generated trajectories exhibit clear diversity, effectively\nsatisfying multi-modal driving behavior requirements. The code and related\nexperiments will be released upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u5934\u8f68\u8ff9\u89c4\u5212\u5668\uff08M-diffusion planner\uff09\uff0c\u901a\u8fc7GRPO\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u6837\u6027\u7b56\u7565\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408LLM\u52a8\u6001\u9009\u62e9\u7b56\u7565\uff0c\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u5668\u5728\u76d1\u7763\u8bad\u7ec3\u540e\u7b56\u7565\u56fa\u5b9a\uff0c\u884c\u4e3a\u50f5\u5316\uff0c\u96be\u4ee5\u53cd\u6620\u4eba\u7c7b\u504f\u597d\u6216\u9002\u5e94\u52a8\u6001\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u591a\u5934\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u524d\u9636\u6bb5\u5171\u4eab\u6743\u91cd\u5b66\u4e60\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u540e\u9636\u6bb5\u901a\u8fc7GRPO\u5fae\u8c03\u5b9e\u73b0\u591a\u6837\u6027\u7b56\u7565\uff0c\u5e76\u5f15\u5165LLM\u52a8\u6001\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728nuPlan val14\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u751f\u6210\u7684\u8f68\u8ff9\u5177\u6709\u660e\u663e\u591a\u6837\u6027\uff0c\u6ee1\u8db3\u591a\u6a21\u6001\u9a7e\u9a76\u9700\u6c42\u3002", "conclusion": "M-diffusion planner\u5728\u4fdd\u6301\u5f3a\u89c4\u5212\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u3001\u6307\u4ee4\u611f\u77e5\u7684\u89c4\u5212\uff0c\u6ee1\u8db3\u591a\u6837\u9a7e\u9a76\u884c\u4e3a\u9700\u6c42\u3002"}}
{"id": "2508.17704", "pdf": "https://arxiv.org/pdf/2508.17704", "abs": "https://arxiv.org/abs/2508.17704", "authors": ["Neil Irwin Bernardo"], "title": "Symbol Detection Using an Integrate-and-Fire Time Encoding Receiver", "categories": ["eess.SP"], "comment": "5 pages, 2 figures. This work has been accepted for publication at\n  the 38th IEEE Workshop on Signal Processing Systems (SiPS 2025)", "summary": "Event-driven sampling is a promising alternative to uniform sampling methods,\nparticularly for systems constrained by power and hardware cost. A notable\nexample of this sampling approach is the integrate-and-fire time encoding\nmachine (IF-TEM), which encodes an analog signal into a sequence of time stamps\nby generating an event each time the integral of the input signal reaches a\nfixed threshold. In this paper, we propose a receiver architecture that\nestimates the sequence of transmitted symbols directly from the encoded time\nstamps, called time encodings, produced by the IF-TEM sampler on the received\nsignal. We show that waveform reconstruction from time encodings is not\nnecessary for symbol detection. We develop an analytical approximation for the\nsymbol error probability (SEP) of the proposed IF-TEM-based receiver and show\nthat it closely matches the SEP results obtained through Monte Carlo\nsimulations. Additionally, we demonstrate that narrowing the 3 dB bandwidth of\nthe transmit pulse shaping filter degrades the proposed IF-TEM receiver's\nperformance, highlighting a trade-off between spectral efficiency and error\nresilience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIF-TEM\u91c7\u6837\u5668\u7684\u63a5\u6536\u673a\u67b6\u6784\uff0c\u76f4\u63a5\u4ece\u65f6\u95f4\u7f16\u7801\u4e2d\u4f30\u8ba1\u7b26\u53f7\u5e8f\u5217\uff0c\u65e0\u9700\u6ce2\u5f62\u91cd\u6784\u3002", "motivation": "\u63a2\u7d22\u4e8b\u4ef6\u9a71\u52a8\u91c7\u6837\uff08\u5982IF-TEM\uff09\u5728\u4f4e\u529f\u8017\u7cfb\u7edf\u4e2d\u7684\u7b26\u53f7\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u65f6\u95f4\u7f16\u7801\u4f30\u8ba1\u7b26\u53f7\u5e8f\u5217\u7684\u63a5\u6536\u673a\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8fd1\u4f3c\u8ba1\u7b97\u7b26\u53f7\u9519\u8bef\u6982\u7387\uff08SEP\uff09\u3002", "result": "\u5206\u6790\u7ed3\u679c\u4e0e\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7684SEP\u5339\u914d\uff0c\u4e14\u53d1\u73b0\u7f29\u5c0f\u4f20\u8f93\u8109\u51b2\u6210\u5f62\u6ee4\u6ce2\u5668\u76843 dB\u5e26\u5bbd\u4f1a\u964d\u4f4e\u63a5\u6536\u673a\u6027\u80fd\u3002", "conclusion": "\u8be5\u63a5\u6536\u673a\u76f4\u63a5\u5229\u7528\u65f6\u95f4\u7f16\u7801\u68c0\u6d4b\u7b26\u53f7\uff0c\u4f46\u9700\u8981\u5728\u9891\u8c31\u6548\u7387\u548c\u9519\u8bef\u6062\u590d\u4e4b\u95f4\u6743\u8861\u3002"}}
{"id": "2508.16962", "pdf": "https://arxiv.org/pdf/2508.16962", "abs": "https://arxiv.org/abs/2508.16962", "authors": ["Wendi Li", "Hao Wu", "Han Gao", "Bing Mao", "Fengyuan Xu", "Sheng Zhong"], "title": "LLM-based Human-like Traffic Simulation for Self-driving Tests", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Ensuring realistic traffic dynamics is a prerequisite for simulation\nplatforms to evaluate the reliability of self-driving systems before deployment\nin the real world. Because most road users are human drivers, reproducing their\ndiverse behaviors within simulators is vital. Existing solutions, however,\ntypically rely on either handcrafted heuristics or narrow data-driven models,\nwhich capture only fragments of real driving behaviors and offer limited\ndriving style diversity and interpretability. To address this gap, we introduce\nHDSim, an HD traffic generation framework that combines cognitive theory with\nlarge language model (LLM) assistance to produce scalable and realistic traffic\nscenarios within simulation platforms. The framework advances the state of the\nart in two ways: (i) it introduces a hierarchical driver model that represents\ndiverse driving style traits, and (ii) it develops a Perception-Mediated\nBehavior Influence strategy, where LLMs guide perception to indirectly shape\ndriver actions. Experiments reveal that embedding HDSim into simulation\nimproves detection of safety-critical failures in self-driving systems by up to\n68% and yields realism-consistent accident interpretability.", "AI": {"tldr": "HDSim\u6846\u67b6\u7ed3\u5408\u8ba4\u77e5\u7406\u8bba\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u66f4\u5177\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u7684\u4ea4\u901a\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u4eba\u5de5\u9a7e\u9a76\u884c\u4e3a\u6a21\u62df\u65b9\u6cd5\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faHDSim\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u9a7e\u9a76\u5458\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u95f4\u63a5\u5f15\u5bfc\u884c\u4e3a\uff0c\u751f\u6210\u591a\u6837\u5316\u4e14\u771f\u5b9e\u7684\u4ea4\u901a\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHDSim\u663e\u8457\u63d0\u9ad8\u5b89\u5168\u5173\u952e\u6545\u969c\u68c0\u6d4b\u7387\uff08\u8fbe68%\uff09\uff0c\u5e76\u63d0\u4f9b\u4e00\u81f4\u7684\u771f\u5b9e\u4e8b\u6545\u89e3\u91ca\u6027\u3002", "conclusion": "HDSim\u901a\u8fc7\u8ba4\u77e5\u7406\u8bba\u548cLLM\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u63d0\u4f9b\u66f4\u771f\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u4ea4\u901a\u573a\u666f\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2508.17710", "pdf": "https://arxiv.org/pdf/2508.17710", "abs": "https://arxiv.org/abs/2508.17710", "authors": ["Dianhao Jia", "Wenqian Shen", "Jianping An", "Byonghyo Shim"], "title": "Blind Channel Estimation for RIS-Assisted Millimeter Wave Communication Systems", "categories": ["eess.SP"], "comment": null, "summary": "In the research of RIS-assisted communication systems, channel estimation is\na problem of vital importance for further performance optimization. In order to\nreduce the pilot overhead to the greatest extent, blind channel estimation\nmethods are required, which can estimate the channel and the transmit signals\nat the same time without transmitting pilot sequence. Different from existing\nresearches in traditional MIMO systems, the RIS-assisted two-hop channel brings\nnew challenges to the blind channel estimation design. Hence, a novel blind\nchannel estimation method based on compressed sensing for RIS-assisted\nmultiuser millimeter wave communication systems is proposed for the first time\nin this paper. Specifically, for accurately estimating the RIS-assisted two-hop\nchannel without transmitting pilots, we propose a block-wise transmission\nscheme. Among different blocks of data transmission, RIS elements are\nreconfigured for better estimating the cascade channel. Inside each block, data\nfor each user are mapped to a codeword for realizing the transmit signal\nrecovery and equivalent channel estimation simultaneously. Simulation results\ndemonstrate that our method can achieve a considerable accuracy of channel\nestimation and transmit signal recovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\u7684\u76f2\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8eRIS\u8f85\u52a9\u7684\u591a\u7528\u6237\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\uff0c\u65e0\u9700\u53d1\u9001\u5bfc\u9891\u5e8f\u5217\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u4fe1\u9053\u4f30\u8ba1\u548c\u4fe1\u53f7\u6062\u590d\u3002", "motivation": "RIS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u4fe1\u9053\u4f30\u8ba1\u5bf9\u6027\u80fd\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u76f2\u4f30\u8ba1\u65b9\u6cd5\u5728RIS\u4e24\u8df3\u4fe1\u9053\u4e2d\u9762\u4e34\u65b0\u6311\u6218\uff0c\u9700\u51cf\u5c11\u5bfc\u9891\u5f00\u9500\u3002", "method": "\u91c7\u7528\u5206\u5757\u4f20\u8f93\u65b9\u6848\uff0c\u901a\u8fc7\u91cd\u65b0\u914d\u7f6eRIS\u5143\u4ef6\u4f18\u5316\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\uff1b\u6bcf\u4e2a\u5757\u5185\u5c06\u7528\u6237\u6570\u636e\u6620\u5c04\u4e3a\u7801\u5b57\uff0c\u540c\u65f6\u5b9e\u73b0\u4fe1\u53f7\u6062\u590d\u548c\u7b49\u6548\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u548c\u4fe1\u53f7\u6062\u590d\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aRIS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u76f2\u4fe1\u9053\u4f30\u8ba1\u65b9\u6848\u3002"}}
{"id": "2508.17034", "pdf": "https://arxiv.org/pdf/2508.17034", "abs": "https://arxiv.org/abs/2508.17034", "authors": ["Jiayi Li", "Yuxin Yao", "Qiuhang Lu", "Juyong Zhang"], "title": "DualReg: Dual-Space Filtering and Reinforcement for Rigid Registration", "categories": ["cs.RO"], "comment": null, "summary": "Rigid registration, aiming to estimate a rigid transformation to align source\nand target data, play a crucial role in applications such as SLAM and 3D\nreconstruction. However, noisy, partially overlapping data and the need for\nreal-time processing pose major challenges for rigid registration. Considering\nthat feature-based matching can handle large transformation differences but\nsuffers from limited accuracy, while local geometry-based matching can achieve\nfine-grained local alignment but relies heavily on a good initial\ntransformation, we propose a novel dual-space paradigm to fully leverage the\nstrengths of both approaches. First, we introduce an efficient filtering\nmechanism that incorporates a computationally lightweight single-point RANSAC\nalgorithm followed by a refinement module to eliminate unreliable feature-based\ncorrespondences. Subsequently, we treat filtered correspondences as anchor\npoints, extract geometric proxies, and formulates an effective objective\nfunction with a tailored solver to estimate the transformation. Experiments\nverify our method's effectiveness, as shown by achieving up to a 32x CPU-time\nspeedup over MAC on KITTI with comparable accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u7a7a\u95f4\u65b9\u6cd5\uff0c\u7ed3\u5408\u7279\u5f81\u5339\u914d\u548c\u51e0\u4f55\u5339\u914d\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u9ad8\u6548\u7b5b\u9009\u673a\u5236\u548c\u4f18\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u521a\u6027\u914d\u51c6\u3002", "motivation": "\u521a\u6027\u914d\u51c6\u5728SLAM\u548c3D\u91cd\u5efa\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u5bf9\u566a\u58f0\u3001\u90e8\u5206\u91cd\u53e0\u6570\u636e\u548c\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff08\u5982\u7279\u5f81\u5339\u914d\u7cbe\u5ea6\u4f4e\u3001\u51e0\u4f55\u5339\u914d\u4f9d\u8d56\u521d\u59cb\u53d8\u6362\uff09\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u53cc\u7a7a\u95f4\u8303\u5f0f\uff0c\u5148\u901a\u8fc7\u5355\u70b9RANSAC\u548c\u7ec6\u5316\u6a21\u5757\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7279\u5f81\u5339\u914d\uff0c\u518d\u57fa\u4e8e\u51e0\u4f55\u4ee3\u7406\u548c\u5b9a\u5236\u6c42\u89e3\u5668\u4f18\u5316\u53d8\u6362\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad832\u500d\u7684CPU\u65f6\u95f4\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u53cc\u7a7a\u95f4\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u7279\u5f81\u548c\u51e0\u4f55\u5339\u914d\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521a\u6027\u914d\u51c6\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2508.17742", "pdf": "https://arxiv.org/pdf/2508.17742", "abs": "https://arxiv.org/abs/2508.17742", "authors": ["Wei Xiong", "Jiangtong Li", "Jie Li", "Kun Zhu"], "title": "EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models", "categories": ["eess.SP", "cs.AI", "cs.HC"], "comment": "17 pages, 7 pages", "summary": "Electroencephalography (EEG) foundation models are poised to significantly\nadvance brain signal analysis by learning robust representations from\nlarge-scale, unlabeled datasets. However, their rapid proliferation has\noutpaced the development of standardized evaluation benchmarks, which\ncomplicates direct model comparisons and hinders systematic scientific\nprogress. This fragmentation fosters scientific inefficiency and obscures\ngenuine architectural advancements. To address this critical gap, we introduce\nEEG-FM-Bench, the first comprehensive benchmark for the systematic and\nstandardized evaluation of EEG foundation models (EEG-FMs). Our contributions\nare threefold: (1) we curate a diverse suite of downstream tasks and datasets\nfrom canonical EEG paradigms, implementing standardized processing and\nevaluation protocols within a unified open-source framework; (2) we benchmark\nprominent state-of-the-art foundation models to establish comprehensive\nbaseline results for a clear comparison of the current landscape; (3) we\nperform qualitative analyses of the learned representations to provide insights\ninto model behavior and inform future architectural design. Through extensive\nexperiments, we find that fine-grained spatio-temporal feature interaction,\nmultitask unified training and neuropsychological priors would contribute to\nenhancing model performance and generalization capabilities. By offering a\nunified platform for fair comparison and reproducible research, EEG-FM-Bench\nseeks to catalyze progress and guide the community toward the development of\nmore robust and generalizable EEG-FMs. Code is released at\nhttps://github.com/xw1216/EEG-FM-Bench.", "AI": {"tldr": "EEG-FM-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u7cfb\u7edf\u5316\u8bc4\u4f30EEG\u57fa\u7840\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u6807\u51c6\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u6a21\u578b\u6bd4\u8f83\u548c\u79d1\u7814\u6548\u7387\u3002", "motivation": "\u5f53\u524dEEG\u57fa\u7840\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u5bfc\u81f4\u6a21\u578b\u6bd4\u8f83\u56f0\u96be\uff0c\u79d1\u7814\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faEEG-FM-Bench\uff0c\u5305\u542b\u4e00\u7cfb\u5217\u6807\u51c6\u5316\u4e0b\u6e38\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u8868\u5f81\u5206\u6790\u3002", "result": "\u53d1\u73b0\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7279\u5f81\u4ea4\u4e92\u3001\u591a\u4efb\u52a1\u7edf\u4e00\u8bad\u7ec3\u548c\u795e\u7ecf\u5fc3\u7406\u5b66\u5148\u9a8c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EEG-FM-Bench\u4e3aEEG\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u7814\u7a76\u7684\u5e73\u53f0\uff0c\u63a8\u52a8\u66f4\u9c81\u68d2\u548c\u901a\u7528\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.17038", "pdf": "https://arxiv.org/pdf/2508.17038", "abs": "https://arxiv.org/abs/2508.17038", "authors": ["Zhouheng Li", "Lei Xie", "Cheng Hu", "Hongye Su"], "title": "A Rapid Iterative Trajectory Planning Method for Automated Parking through Differential Flatness", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Published in the journal Robotics and Autonomous Systems", "summary": "As autonomous driving continues to advance, automated parking is becoming\nincreasingly essential. However, significant challenges arise when implementing\npath velocity decomposition (PVD) trajectory planning for automated parking.\nThe primary challenge is ensuring rapid and precise collision-free trajectory\nplanning, which is often in conflict. The secondary challenge involves\nmaintaining sufficient control feasibility of the planned trajectory,\nparticularly at gear shifting points (GSP). This paper proposes a PVD-based\nrapid iterative trajectory planning (RITP) method to solve the above\nchallenges. The proposed method effectively balances the necessity for time\nefficiency and precise collision avoidance through a novel collision avoidance\nframework. Moreover, it enhances the overall control feasibility of the planned\ntrajectory by incorporating the vehicle kinematics model and including terminal\nsmoothing constraints (TSC) at GSP during path planning. Specifically, the\nproposed method leverages differential flatness to ensure the planned path\nadheres to the vehicle kinematic model. Additionally, it utilizes TSC to\nmaintain curvature continuity at GSP, thereby enhancing the control feasibility\nof the overall trajectory. The simulation results demonstrate superior time\nefficiency and tracking errors compared to model-integrated and other\niteration-based trajectory planning methods. In the real-world experiment, the\nproposed method was implemented and validated on a ROS-based vehicle,\ndemonstrating the applicability of the RITP method for real vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePVD\u7684\u5feb\u901f\u8fed\u4ee3\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff08RITP\uff09\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u65f6\u95f4\u6548\u7387\u4e0e\u7cbe\u786e\u78b0\u649e\u907f\u514d\uff0c\u5e76\u901a\u8fc7\u7ec8\u7aef\u5e73\u6ed1\u7ea6\u675f\u63d0\u5347\u4e86\u8f68\u8ff9\u63a7\u5236\u53ef\u884c\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709PVD\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u5feb\u901f\u3001\u7cbe\u786e\u7684\u78b0\u649e\u907f\u514d\u548c\u8f68\u8ff9\u63a7\u5236\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528RITP\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f66\u8f86\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u7ec8\u7aef\u5e73\u6ed1\u7ea6\u675f\uff0c\u901a\u8fc7\u5dee\u5206\u5e73\u5766\u6027\u548cTSC\u63d0\u5347\u8f68\u8ff9\u89c4\u5212\u6548\u7387\u548c\u63a7\u5236\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u663e\u793a\uff0cRITP\u5728\u65f6\u95f4\u6548\u7387\u548c\u8ddf\u8e2a\u8bef\u5dee\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "RITP\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86PVD\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17852", "pdf": "https://arxiv.org/pdf/2508.17852", "abs": "https://arxiv.org/abs/2508.17852", "authors": ["Hossein Mohammadi Firouzjaei", "Rafaela Scaciota", "Sumudu Samarakoon", "Beatriz Lorenzo"], "title": "Cross-Domain Lifelong Reinforcement Learning for Wireless Sensor Networks", "categories": ["eess.SP"], "comment": null, "summary": "Wireless sensor networks (WSNs) with energy harvesting (EH) are expected to\nplay a vital role in intelligent 6G systems, especially in industrial sensing\nand control, where continuous operation and sustainable energy use are\ncritical. Given limited energy resources, WSNs must operate efficiently to\nensure long-term performance. Their deployment, however, is challenged by\ndynamic environments where EH conditions, network scale, and traffic rates\nchange over time. In this work, we address system dynamics that yield different\nlearning tasks, where decision variables remain fixed but strategies vary, as\nwell as learning domains, where both decision space and strategies evolve. To\nhandle such scenarios, we propose a cross-domain lifelong reinforcement\nlearning (CD-L2RL) framework for energy-efficient WSN design. Our CD-L2RL\nalgorithm leverages prior experience to accelerate adaptation across tasks and\ndomains. Unlike conventional approaches based on Markov decision processes or\nLyapunov optimization, which assume relatively stable environments, our\nsolution achieves rapid policy adaptation by reusing knowledge from past tasks\nand domains to ensure continuous operations. We validate the approach through\nextensive simulations under diverse conditions. Results show that our method\nimproves adaptation speed by up to 35% over standard reinforcement learning and\nup to 70% over Lyapunov-based optimization, while also increasing total\nharvested energy. These findings highlight the strong potential of CD-L2RL for\ndeployment in dynamic 6G WSNs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u57df\u7ec8\u8eab\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08CD-L2RL\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u80fd\u91cf\u91c7\u96c6\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08WSN\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8bbe\u8ba1\u95ee\u9898\u3002", "motivation": "6G\u7cfb\u7edf\u4e2d\u7684WSN\u9700\u8981\u6301\u7eed\u8fd0\u884c\u548c\u9ad8\u6548\u80fd\u6e90\u5229\u7528\uff0c\u4f46\u52a8\u6001\u73af\u5883\uff08\u5982\u80fd\u91cf\u91c7\u96c6\u6761\u4ef6\u3001\u7f51\u7edc\u89c4\u6a21\u548c\u6d41\u91cf\u53d8\u5316\uff09\u7ed9\u90e8\u7f72\u5e26\u6765\u6311\u6218\u3002", "method": "\u63d0\u51faCD-L2RL\u6846\u67b6\uff0c\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u52a0\u901f\u8de8\u4efb\u52a1\u548c\u9886\u57df\u7684\u9002\u5e94\uff0c\u800c\u975e\u4f20\u7edf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u6216\u674e\u96c5\u666e\u8bfa\u592b\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u901f\u5ea6\u5feb35%\uff0c\u6bd4\u674e\u96c5\u666e\u8bfa\u592b\u4f18\u5316\u5feb70%\uff0c\u4e14\u603b\u91c7\u96c6\u80fd\u91cf\u66f4\u9ad8\u3002", "conclusion": "CD-L2RL\u5728\u52a8\u60016G WSN\u4e2d\u5177\u6709\u663e\u8457\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.17070", "pdf": "https://arxiv.org/pdf/2508.17070", "abs": "https://arxiv.org/abs/2508.17070", "authors": ["Halid Abdulrahim Kadi", "Kasim Terzi\u0107"], "title": "LaGarNet: Goal-Conditioned Recurrent State-Space Models for Pick-and-Place Garment Flattening", "categories": ["cs.RO"], "comment": "20 pages, 11 figures and 3 tables", "summary": "We present a novel goal-conditioned recurrent state space (GC-RSSM) model\ncapable of learning latent dynamics of pick-and-place garment manipulation. Our\nproposed method LaGarNet matches the state-of-the-art performance of mesh-based\nmethods, marking the first successful application of state-space models on\ncomplex garments. LaGarNet trains on a coverage-alignment reward and a dataset\ncollected through a general procedure supported by a random policy and a\ndiffusion policy learned from few human demonstrations; it substantially\nreduces the inductive biases introduced in the previous similar methods. We\ndemonstrate that a single-policy LaGarNet achieves flattening on four different\ntypes of garments in both real-world and simulation settings.", "AI": {"tldr": "GC-RSSM\u6a21\u578bLaGarNet\u7528\u4e8e\u5b66\u4e60\u8863\u7269\u6293\u653e\u64cd\u4f5c\u7684\u6f5c\u5728\u52a8\u6001\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u8986\u76d6\u5bf9\u9f50\u5956\u52b1\u548c\u6df7\u5408\u7b56\u7565\u6570\u636e\u8bad\u7ec3\uff0c\u51cf\u5c11\u4e86\u5f52\u7eb3\u504f\u5dee\u3002", "motivation": "\u89e3\u51b3\u8863\u7269\u6293\u653e\u64cd\u4f5c\u7684\u590d\u6742\u52a8\u6001\u5efa\u6a21\u95ee\u9898\uff0c\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5f52\u7eb3\u504f\u5dee\u3002", "method": "\u4f7f\u7528GC-RSSM\u6a21\u578bLaGarNet\uff0c\u7ed3\u5408\u8986\u76d6\u5bf9\u9f50\u5956\u52b1\u548c\u6df7\u5408\u7b56\u7565\uff08\u968f\u673a\u7b56\u7565\u4e0e\u6269\u6563\u7b56\u7565\uff09\u6570\u636e\u8bad\u7ec3\u3002", "result": "LaGarNet\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5bf9\u56db\u79cd\u8863\u7269\u5b9e\u73b0\u5e73\u6574\u64cd\u4f5c\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "LaGarNet\u9996\u6b21\u6210\u529f\u5c06\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5e94\u7528\u4e8e\u590d\u6742\u8863\u7269\u64cd\u4f5c\uff0c\u4e3a\u51cf\u5c11\u5f52\u7eb3\u504f\u5dee\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.17873", "pdf": "https://arxiv.org/pdf/2508.17873", "abs": "https://arxiv.org/abs/2508.17873", "authors": ["Mehdi Abdollahpour", "Carsten Bockelmann", "Tajim Md Hasibur Rahman", "Armin Dekorsy", "Andreas Fischer"], "title": "Compressed Learning for Nanosurface Deficiency Recognition Using Angle-resolved Scatterometry Data", "categories": ["eess.SP", "eess.IV"], "comment": null, "summary": "Nanoscale manufacturing requires high-precision surface inspection to\nguarantee the quality of the produced nanostructures. For production\nenvironments, angle-resolved scatterometry offers a non- invasive and in-line\ncompatible alternative to traditional surface inspection methods, such as\nscanning electron microscopy. However, angle-resolved scatterometry currently\nsuffers from long data acquisition time. Our study addresses the issue of slow\ndata acquisition by proposing a compressed learning framework for the accurate\nrecognition of nanosurface deficiencies using angle-resolved scatterometry\ndata. The framework uses the particle swarm optimization algorithm with a\nsampling scheme customized for scattering patterns. This combination allows the\nidentification of optimal sampling points in scatterometry data that maximize\nthe detection accuracy of five different levels of deficiency in ZnO\nnanosurfaces. The proposed method significantly reduces the amount of sampled\ndata while maintaining a high accuracy in deficiency detection, even in noisy\nenvironments. Notably, by sampling only 1% of the data, the method achieves an\naccuracy of over 86%, which further improves to 94% when the sampling rate is\nincreased to 6%. These results demonstrate a favorable balance between data\nreduction and classification performance. The obtained results also show that\nthe compressed learning framework effectively identifies critical sampling\nareas.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u538b\u7f29\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89d2\u5206\u8fa8\u6563\u5c04\u4eea\u7684\u5feb\u901f\u7eb3\u7c73\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u91c7\u96c6\u65f6\u95f4\u5e76\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u7eb3\u7c73\u5236\u9020\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u8868\u9762\u68c0\u6d4b\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u7535\u5b50\u663e\u5fae\u955c\u8017\u65f6\u4e14\u4e0d\u9002\u5408\u5728\u7ebf\u68c0\u6d4b\uff0c\u89d2\u5206\u8fa8\u6563\u5c04\u4eea\u867d\u9002\u5408\u5728\u7ebf\u4f7f\u7528\u4f46\u6570\u636e\u91c7\u96c6\u901f\u5ea6\u8f83\u6162\u3002", "method": "\u4f7f\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u7ed3\u5408\u9488\u5bf9\u6563\u5c04\u6a21\u5f0f\u7684\u81ea\u5b9a\u4e49\u91c7\u6837\u65b9\u6848\uff0c\u9009\u62e9\u6700\u4f18\u91c7\u6837\u70b9\u4ee5\u6700\u5927\u5316\u7f3a\u9677\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "result": "\u4ec5\u91c7\u68371%\u6570\u636e\u65f6\u51c6\u786e\u7387\u8fbe86%\uff0c\u91c7\u68376%\u65f6\u63d0\u5347\u81f394%\uff0c\u540c\u65f6\u6709\u6548\u8bc6\u522b\u5173\u952e\u91c7\u6837\u533a\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u51cf\u5c11\u548c\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u7eb3\u7c73\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u3002"}}
{"id": "2508.17260", "pdf": "https://arxiv.org/pdf/2508.17260", "abs": "https://arxiv.org/abs/2508.17260", "authors": ["Anurag Maurya", "Tashmoy Ghosh", "Anh Nguyen", "Ravi Prakash"], "title": "OVITA: Open-Vocabulary Interpretable Trajectory Adaptations", "categories": ["cs.RO"], "comment": "Accepted to Robotics and Automation Letters 2025. Code link:\n  https://github.com/anurag1000101/OVITA", "summary": "Adapting trajectories to dynamic situations and user preferences is crucial\nfor robot operation in unstructured environments with non-expert users. Natural\nlanguage enables users to express these adjustments in an interactive manner.\nWe introduce OVITA, an interpretable, open-vocabulary, language-driven\nframework designed for adapting robot trajectories in dynamic and novel\nsituations based on human instructions. OVITA leverages multiple pre-trained\nLarge Language Models (LLMs) to integrate user commands into trajectories\ngenerated by motion planners or those learned through demonstrations. OVITA\nemploys code as an adaptation policy generated by an LLM, enabling users to\nadjust individual waypoints, thus providing flexible control. Another LLM,\nwhich acts as a code explainer, removes the need for expert users, enabling\nintuitive interactions. The efficacy and significance of the proposed OVITA\nframework is demonstrated through extensive simulations and real-world\nenvironments with diverse tasks involving spatiotemporal variations on\nheterogeneous robotic platforms such as a KUKA IIWA robot manipulator,\nClearpath Jackal ground robot, and CrazyFlie drone.", "AI": {"tldr": "OVITA\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u6574\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u4ee3\u7801\u7b56\u7565\uff0c\u652f\u6301\u975e\u4e13\u5bb6\u7528\u6237\u76f4\u89c2\u4ea4\u4e92\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u9002\u5e94\u52a8\u6001\u60c5\u5883\u548c\u7528\u6237\u504f\u597d\u7684\u8f68\u8ff9\u8c03\u6574\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e3a\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002", "method": "OVITA\u7ed3\u5408\u591a\u79cd\u9884\u8bad\u7ec3LLMs\uff0c\u5c06\u7528\u6237\u6307\u4ee4\u878d\u5165\u8f68\u8ff9\u751f\u6210\u6216\u6f14\u793a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4ee3\u7801\u7b56\u7565\u8c03\u6574\u8def\u5f84\u70b9\uff0c\u53e6\u4e00LLM\u4f5c\u4e3a\u4ee3\u7801\u89e3\u91ca\u5668\u7b80\u5316\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5982KUKA IIWA\u3001Clearpath Jackal\u3001CrazyFlie\uff09\u7684\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86OVITA\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "OVITA\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u76f4\u89c2\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u8c03\u6574\u65b9\u6848\u3002"}}
{"id": "2508.17942", "pdf": "https://arxiv.org/pdf/2508.17942", "abs": "https://arxiv.org/abs/2508.17942", "authors": ["Qingtang Jiang", "Shuixin Li", "Jiecheng Chen", "Lin Li"], "title": "Synchrosqueezed X-Ray Wavelet-Chirplet Transform for Accurate Chirp Rate Estimation and Retrieval of Modes from Multicomponent Signals with Crossover Instantaneous Frequencies", "categories": ["eess.SP"], "comment": null, "summary": "Recent advances in the chirplet transform and wavelet-chirplet transform\n(WCT) have enabled the estimation of instantaneous frequencies (IFs) and\nchirprates, as well as mode retrieval from multicomponent signals with\ncrossover IF curves. However, chirprate estimation via these approaches remains\nless accurate than IF estimation, primarily due to the slow decay of the\nchirplet transform or WCT along the chirprate direction. To address this, the\nsynchrosqueezed chirplet transform (SCT) and multiple SCT methods were\nproposed, achieving moderate improvements in IF and chirprate estimation\naccuracy. Nevertheless, a novel approach is still needed to enhance the\ntransform's decay along the chirprate direction.\n  This paper introduces an X-ray transform-based wavelet-chirprate transform,\ntermed the X-ray wavelet-chirplet transform (XWCT), which exhibits superior\ndecay along the chirprate direction compared to the WCT. Furthermore,\nthird-order synchrosqueezed variants of the WCT and XWCT are developed to yield\nsharp time-frequency-chirprate representations of signals. Experimental results\ndemonstrate that the XWCT achieves significantly faster decay along the\nchirprate axis, while the third-order synchrosqueezed XWCT enables accurate IF\nand chirprate estimation, as well as mode retrieval, without requiring multiple\nsynchrosqueezing operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eX\u5c04\u7ebf\u53d8\u6362\u7684XWCT\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cbfchirprate\u65b9\u5411\u7684\u8870\u51cf\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u540c\u6b65\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u77ac\u65f6\u9891\u7387\u548cchirprate\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728chirprate\u4f30\u8ba1\u4e0a\u7684\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u6cbfchirprate\u65b9\u5411\u7684\u8870\u51cf\u8f83\u6162\u3002", "method": "\u5f15\u5165X\u5c04\u7ebf\u53d8\u6362\u7684XWCT\uff0c\u5e76\u5f00\u53d1\u5176\u4e09\u9636\u540c\u6b65\u538b\u7f29\u53d8\u4f53\u3002", "result": "XWCT\u6cbfchirprate\u65b9\u5411\u7684\u8870\u51cf\u663e\u8457\u52a0\u5feb\uff0c\u4e09\u9636\u540c\u6b65\u538b\u7f29XWCT\u65e0\u9700\u591a\u6b21\u64cd\u4f5c\u5373\u53ef\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "conclusion": "XWCT\u53ca\u5176\u4e09\u9636\u540c\u6b65\u538b\u7f29\u53d8\u4f53\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.17449", "pdf": "https://arxiv.org/pdf/2508.17449", "abs": "https://arxiv.org/abs/2508.17449", "authors": ["Zezeng Li", "Alexandre Chapin", "Enda Xiang", "Rui Yang", "Bruno Machado", "Na Lei", "Emmanuel Dellandrea", "Di Huang", "Liming Chen"], "title": "Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges", "categories": ["cs.RO"], "comment": null, "summary": "Robotic Manipulation (RM) is central to the advancement of autonomous robots,\nenabling them to interact with and manipulate objects in real-world\nenvironments. This survey focuses on RM methodologies that leverage imitation\nlearning, a powerful technique that allows robots to learn complex manipulation\nskills by mimicking human demonstrations. We identify and analyze the most\ninfluential studies in this domain, selected based on community impact and\nintrinsic quality. For each paper, we provide a structured summary, covering\nthe research purpose, technical implementation, hierarchical classification,\ninput formats, key priors, strengths and limitations, and citation metrics.\nAdditionally, we trace the chronological development of imitation learning\ntechniques within RM policy (RMP), offering a timeline of key technological\nadvancements. Where available, we report benchmark results and perform\nquantitative evaluations to compare existing methods. By synthesizing these\ninsights, this review provides a comprehensive resource for researchers and\npractitioners, highlighting both the state of the art and the challenges that\nlie ahead in the field of robotic manipulation through imitation learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8c03\u67e5\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\uff08RM\uff09\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u76f8\u5173\u7814\u7a76\u7684\u6280\u672f\u5b9e\u73b0\u3001\u5206\u7c7b\u53ca\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002", "motivation": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8ba9\u673a\u5668\u4eba\u638c\u63e1\u590d\u6742\u64cd\u4f5c\u6280\u80fd\uff0c\u4fc3\u8fdb\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5206\u6790\u5f71\u54cd\u529b\u5927\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u603b\u7ed3\uff0c\u5305\u62ec\u6280\u672f\u5b9e\u73b0\u3001\u5206\u7c7b\u3001\u8f93\u5165\u683c\u5f0f\u7b49\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u4e86\u6a21\u4eff\u5b66\u4e60\u5728RM\u9886\u57df\u7684\u6280\u672f\u8fdb\u5c55\u3001\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u53ca\u6311\u6218\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u6307\u51fa\u4e86\u6a21\u4eff\u5b66\u4e60\u5728RM\u9886\u57df\u7684\u73b0\u72b6\u548c\u672a\u6765\u6311\u6218\u3002"}}
{"id": "2508.17960", "pdf": "https://arxiv.org/pdf/2508.17960", "abs": "https://arxiv.org/abs/2508.17960", "authors": ["Yuto Kawai", "Rajeev Koodli"], "title": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless Signal Processing", "categories": ["eess.SP"], "comment": "10 pages, 8 figures", "summary": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65e0\u7ebf\u4fe1\u53f7\u5904\u7406\u7edf\u4e00\u67b6\u6784\uff0c\u66ff\u4ee3\u4f20\u7edf\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u4fe1\u53f7\u5904\u7406\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5ef6\u8fdf\u9ad8\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684Transformer\u67b6\u6784\u63d0\u5347\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u96c6\u6210\u4fe1\u9053\u4f30\u8ba1\u3001\u63d2\u503c\u548c\u89e3\u8c03\u7684\u7edf\u4e00\u6ce8\u610f\u529b\u9a71\u52a8\u67b6\u6784\uff0c\u652f\u6301\u52a8\u6001\u8f93\u51fa\u683c\u5f0f\u8c03\u6574\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5728\u7aef\u5230\u7aef\u63a5\u6536\u3001\u4fe1\u9053\u9891\u7387\u63d2\u503c\u548c\u4fe1\u9053\u4f30\u8ba1\u4e09\u4e2a\u6838\u5fc3\u7528\u4f8b\u4e2d\uff0c\u5747\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u667a\u80fd\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5960\u5b9a\u4e86\u8f6f\u4ef6\u5b9a\u4e49\u4fe1\u53f7\u5904\u7406\u7684\u57fa\u7840\u3002"}}
{"id": "2508.17464", "pdf": "https://arxiv.org/pdf/2508.17464", "abs": "https://arxiv.org/abs/2508.17464", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Evolutionary Brain-Body Co-Optimization Consistently Fails to Select for Morphological Potential", "categories": ["cs.RO", "cs.NE"], "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "Brain-body co-optimization remains a challenging problem, despite increasing\ninterest from the community in recent years. To understand and overcome the\nchallenges, we propose exhaustively mapping a morphology-fitness landscape to\nstudy it. To this end, we train controllers for each feasible morphology in a\ndesign space of 1,305,840 distinct morphologies, constrained by a computational\nbudget. First, we show that this design space constitutes a good model for\nstudying the brain-body co-optimization problem, and our attempt to\nexhaustively map it roughly captures the landscape. We then proceed to analyze\nhow evolutionary brain-body co-optimization algorithms work in this design\nspace. The complete knowledge of the morphology-fitness landscape facilitates a\nbetter understanding of the results of evolutionary brain-body co-optimization\nalgorithms and how they unfold over evolutionary time in the morphology space.\nThis investigation shows that the experimented algorithms cannot consistently\nfind near-optimal solutions. The search, at times, gets stuck on morphologies\nthat are sometimes one mutation away from better morphologies, and the\nalgorithms cannot efficiently track the fitness gradient in the\nmorphology-fitness landscape. We provide evidence that experimented algorithms\nregularly undervalue the fitness of individuals with newly mutated bodies and,\nas a result, eliminate promising morphologies throughout evolution. Our work\nprovides the most concrete demonstration of the challenges of evolutionary\nbrain-body co-optimization. Our findings ground the trends in the literature\nand provide valuable insights for future work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8be6\u5c3d\u6620\u5c04\u5f62\u6001-\u9002\u5e94\u5ea6\u666f\u89c2\uff0c\u63ed\u793a\u4e86\u5927\u8111-\u8eab\u4f53\u534f\u540c\u4f18\u5316\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u7b97\u6cd5\u96be\u4ee5\u7a33\u5b9a\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u5e76\u63d0\u51fa\u7b97\u6cd5\u4f4e\u4f30\u65b0\u7a81\u53d8\u4f53\u9002\u5e94\u5ea6\u7684\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5927\u8111-\u8eab\u4f53\u534f\u540c\u4f18\u5316\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8be6\u5c3d\u6620\u5c04\u5f62\u6001-\u9002\u5e94\u5ea6\u666f\u89c2\u6765\u6df1\u5165\u7406\u89e3\u7b97\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u65b9\u6cd5\u662f\u5728\u5305\u542b1,305,840\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u4e3a\u6bcf\u79cd\u53ef\u884c\u5f62\u6001\u8bad\u7ec3\u63a7\u5236\u5668\uff0c\u5e76\u5206\u6790\u8fdb\u5316\u7b97\u6cd5\u5728\u5f62\u6001\u7a7a\u95f4\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5b9e\u9a8c\u7b97\u6cd5\u65e0\u6cd5\u7a33\u5b9a\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u4e14\u5728\u5f62\u6001-\u9002\u5e94\u5ea6\u666f\u89c2\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u5e38\u4f4e\u4f30\u65b0\u7a81\u53d8\u4f53\u7684\u9002\u5e94\u5ea6\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u5de5\u4f5c\u4e3a\u5927\u8111-\u8eab\u4f53\u534f\u540c\u4f18\u5316\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u5177\u4f53\u8bc1\u636e\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.18009", "pdf": "https://arxiv.org/pdf/2508.18009", "abs": "https://arxiv.org/abs/2508.18009", "authors": ["Leonardo Tercas", "Markku Juntti"], "title": "Positioning via Probabilistic Graphical Models in RIS-Aided Systems with Channel Estimation Errors", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, 2 tables. Presented at 2025 IEEE International\n  Conference on Communications (ICC), June 2025, Montreal, Canada", "summary": "We propose a 6D Bayesian-based localization framework to estimate the\nposition and rotation angles of a mobile station (MS) within an indoor\nreconfigurable intelligent surface (RIS)-aided system. This framework relies on\na probabilistic graphical model to represent the joint probability distribution\nof random variables through their conditional dependencies and employs the\nNo-U-Turn Sampler (NUTS) to approximate the posterior distribution based on the\nestimated channel parameters. Our framework estimates both the position and\nrotation of the mobile station (MS), in the presence of channel parameter\nestimation errors. We derive the Cramer-Rao lower bound (CRLB) for the proposed\nscenario and use it to evaluate the system's position error bound (PEB) and\nrotation error bound (REB). We compare the system performances with and without\nRIS. The results demonstrate that the RIS can enhance positioning accuracy\nsignificantly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u76846D\u5b9a\u4f4d\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u5ba4\u5185RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u79fb\u52a8\u7ad9\u7684\u4f4d\u7f6e\u548c\u65cb\u8f6c\u89d2\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5ba4\u5185RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\uff0c\u79fb\u52a8\u7ad9\u4f4d\u7f6e\u548c\u65cb\u8f6c\u89d2\u5ea6\u7684\u7cbe\u786e\u4f30\u8ba1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u6982\u7387\u56fe\u6a21\u578b\u8868\u793a\u968f\u673a\u53d8\u91cf\u7684\u8054\u5408\u6982\u7387\u5206\u5e03\uff0c\u5e76\u91c7\u7528NUTS\u91c7\u6837\u5668\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03\uff0c\u540c\u65f6\u63a8\u5bfc\u4e86CRLB\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cRIS\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u7cfb\u7edf\u5728\u6709RIS\u548c\u65e0RIS\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u5bf9\u6bd4\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u63d0\u51fa\u76846D\u5b9a\u4f4d\u6846\u67b6\u5728RIS\u8f85\u52a9\u4e0b\u80fd\u591f\u6709\u6548\u63d0\u5347\u79fb\u52a8\u7ad9\u7684\u5b9a\u4f4d\u548c\u65cb\u8f6c\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2508.17466", "pdf": "https://arxiv.org/pdf/2508.17466", "abs": "https://arxiv.org/abs/2508.17466", "authors": ["Dilermando Almeida", "Guilherme Lazzarini", "Juliano Negri", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Quadruped robots have emerged as highly efficient and versatile platforms,\nexcelling in navigating complex and unstructured terrains where traditional\nwheeled robots might fail. Equipping these robots with manipulator arms unlocks\nthe advanced capability of loco-manipulation to perform complex physical\ninteraction tasks in areas ranging from industrial automation to\nsearch-and-rescue missions. However, achieving precise and adaptable grasping\nin such dynamic scenarios remains a significant challenge, often hindered by\nthe need for extensive real-world calibration and pre-programmed grasp\nconfigurations. This paper introduces a deep learning framework designed to\nenhance the grasping capabilities of quadrupeds equipped with arms, focusing on\nimproved precision and adaptability. Our approach centers on a sim-to-real\nmethodology that minimizes reliance on physical data collection. We developed a\npipeline within the Genesis simulation environment to generate a synthetic\ndataset of grasp attempts on common objects. By simulating thousands of\ninteractions from various perspectives, we created pixel-wise annotated\ngrasp-quality maps to serve as the ground truth for our model. This dataset was\nused to train a custom CNN with a U-Net-like architecture that processes\nmulti-modal input from an onboard RGB and depth cameras, including RGB images,\ndepth maps, segmentation masks, and surface normal maps. The trained model\noutputs a grasp-quality heatmap to identify the optimal grasp point. We\nvalidated the complete framework on a four-legged robot. The system\nsuccessfully executed a full loco-manipulation task: autonomously navigating to\na target object, perceiving it with its sensors, predicting the optimal grasp\npose using our model, and performing a precise grasp. This work proves that\nleveraging simulated training with advanced sensing offers a scalable and\neffective solution for object handling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u589e\u5f3a\u56db\u8db3\u673a\u5668\u4eba\u81c2\u7684\u6293\u53d6\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u64cd\u4f5c\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u7ed3\u5408\u81c2\u90e8\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u6293\u53d6\u4ecd\u9762\u4e34\u6821\u51c6\u548c\u9884\u7f16\u7a0b\u914d\u7f6e\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u65b9\u6cd5\uff0c\u5728Genesis\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3U-Net\u7ed3\u6784CNN\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u751f\u6210\u6293\u53d6\u8d28\u91cf\u70ed\u56fe\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u5b8c\u6574\u7684\u6293\u53d6\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u5bfc\u822a\u3001\u611f\u77e5\u3001\u9884\u6d4b\u6700\u4f18\u6293\u53d6\u59ff\u52bf\u5e76\u7cbe\u786e\u6293\u53d6\u7684\u76ee\u6807\u4efb\u52a1\u3002", "conclusion": "\u6a21\u62df\u8bad\u7ec3\u7ed3\u5408\u5148\u8fdb\u611f\u77e5\u662f\u89e3\u51b3\u673a\u5668\u4eba\u5bf9\u8c61\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.17469", "pdf": "https://arxiv.org/pdf/2508.17469", "abs": "https://arxiv.org/abs/2508.17469", "authors": ["Alican Mertan", "Nick Cheney"], "title": "Morphological Cognition: Classifying MNIST Digits Through Morphological Computation Alone", "categories": ["cs.RO", "cs.NE"], "comment": "Accepted to be presented at ALife 2025 as a talk", "summary": "With the rise of modern deep learning, neural networks have become an\nessential part of virtually every artificial intelligence system, making it\ndifficult even to imagine different models for intelligent behavior. In\ncontrast, nature provides us with many different mechanisms for intelligent\nbehavior, most of which we have yet to replicate. One of such underinvestigated\naspects of intelligence is embodiment and the role it plays in intelligent\nbehavior. In this work, we focus on how the simple and fixed behavior of\nconstituent parts of a simulated physical body can result in an emergent\nbehavior that can be classified as cognitive by an outside observer.\nSpecifically, we show how simulated voxels with fixed behaviors can be combined\nto create a robot such that, when presented with an image of an MNIST digit\nzero, it moves towards the left; and when it is presented with an image of an\nMNIST digit one, it moves towards the right. Such robots possess what we refer\nto as ``morphological cognition'' -- the ability to perform cognitive behavior\nas a result of morphological processes. To the best of our knowledge, this is\nthe first demonstration of a high-level mental faculty such as image\nclassification performed by a robot without any neural circuitry. We hope that\nthis work serves as a proof-of-concept and fosters further research into\ndifferent models of intelligence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6a21\u62df\u7269\u7406\u8eab\u4f53\u7684\u7b80\u5355\u56fa\u5b9a\u90e8\u5206\u884c\u4e3a\u5982\u4f55\u4ea7\u751f\u53ef\u88ab\u5916\u90e8\u89c2\u5bdf\u8005\u5206\u7c7b\u4e3a\u8ba4\u77e5\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u795e\u7ecf\u7f51\u7edc\u5373\u53ef\u5b9e\u73b0\u56fe\u50cf\u5206\u7c7b\u7684\u673a\u5668\u4eba\u3002", "motivation": "\u65e8\u5728\u7814\u7a76\u667a\u80fd\u884c\u4e3a\u7684\u591a\u6837\u5316\u673a\u5236\uff0c\u7279\u522b\u662f\u5173\u6ce8\u88ab\u5ffd\u89c6\u7684\u667a\u80fd\u884c\u4e3a\u4e2d\u7684\u4f53\u73b0\uff08embodiment\uff09\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5177\u6709\u56fa\u5b9a\u884c\u4e3a\u7684\u4f53\u7d20\u7ec4\u5408\u6210\u673a\u5668\u4eba\uff0c\u4f7f\u5176\u5728\u9762\u5bf9MNIST\u6570\u5b57\u56fe\u50cf\u65f6\u8868\u73b0\u51fa\u4e0d\u540c\u65b9\u5411\u7684\u79fb\u52a8\u884c\u4e3a\u3002", "result": "\u9996\u6b21\u5c55\u793a\u4e86\u65e0\u9700\u795e\u7ecf\u7535\u8def\u5373\u80fd\u5b8c\u6210\u56fe\u50cf\u5206\u7c7b\u7b49\u9ad8\u7ea7\u8ba4\u77e5\u529f\u80fd\u7684\u673a\u5668\u4eba\uff0c\u79f0\u4e4b\u4e3a\u2018\u5f62\u6001\u8ba4\u77e5\u2019\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u884c\u4e3a\u7684\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5e0c\u671b\u80fd\u4fc3\u8fdb\u66f4\u591a\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.16908", "pdf": "https://arxiv.org/pdf/2508.16908", "abs": "https://arxiv.org/abs/2508.16908", "authors": ["Amod K. Agrawal"], "title": "Localization using Angle-of-Arrival Triangulation", "categories": ["eess.AS", "cs.HC", "cs.NI", "cs.SD", "eess.SP", "C.3; C.2.1; C.2.4; I.5.4; H.5.2; J.7"], "comment": "6 pages, 5 figures, 1 table. Accepted at the ACM International\n  Workshop on Environmental Sensing Systems for Smart Cities (EnvSys 2025). To\n  appear in the MobiSys 2025 Proceedings", "summary": "Indoor localization is a long-standing challenge in mobile computing, with\nsignificant implications for enabling location-aware and intelligent\napplications within smart environments such as homes, offices, and retail\nspaces. As AI assistants such as Amazon Alexa and Google Nest become\nincreasingly pervasive, microphone-equipped devices are emerging as key\ncomponents of everyday life and home automation. This paper introduces a\npassive, infrastructure-light system for localizing human speakers using speech\nsignals captured by two or more spatially distributed smart devices. The\nproposed approach, GCC+, extends the Generalized Cross-Correlation with Phase\nTransform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio\nsignals at each device and applies robust triangulation techniques to infer the\nspeaker's two-dimensional position. To further improve temporal resolution and\nlocalization accuracy, feature-space expansion and subsample interpolation\ntechniques are employed for precise Time Difference of Arrival (TDoA)\nestimation. The system operates without requiring hardware modifications, prior\ncalibration, explicit user cooperation, or knowledge of the speaker's signal\ncontent, thereby offering a highly practical solution for real-world\ndeployment. Experimental evaluation in a real-world home environment yields a\nmedian AoA estimation error of 2.2 degrees and a median localization error of\n1.25 m, demonstrating the feasibility and effectiveness of audio-based\nlocalization for enabling context-aware, privacy-preserving ambient\nintelligence.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u4fe1\u53f7\u7684\u88ab\u52a8\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edfGCC+\uff0c\u901a\u8fc7\u667a\u80fd\u8bbe\u5907\u6355\u6349\u58f0\u97f3\u5e76\u8ba1\u7b97\u5230\u8fbe\u89d2\u548c\u4f4d\u7f6e\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u6216\u7528\u6237\u914d\u5408\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8ba1\u7b97\u4e2d\u5ba4\u5185\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u652f\u6301\u667a\u80fd\u73af\u5883\u4e0b\u7684\u4f4d\u7f6e\u611f\u77e5\u5e94\u7528\u3002", "method": "\u6269\u5c55GCC-PHAT\u65b9\u6cd5\u4f30\u8ba1\u5230\u8fbe\u89d2\uff0c\u7ed3\u5408\u7a33\u5065\u4e09\u89d2\u6d4b\u91cf\u548c\u5b50\u6837\u672c\u63d2\u503c\u6280\u672f\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e2d\u4f4d\u5230\u8fbe\u89d2\u8bef\u5dee2.2\u5ea6\uff0c\u5b9a\u4f4d\u8bef\u5dee1.25\u7c73\u3002", "conclusion": "\u97f3\u9891\u5b9a\u4f4d\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u53ef\u884c\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u667a\u80fd\u573a\u666f\u3002"}}
{"id": "2508.17482", "pdf": "https://arxiv.org/pdf/2508.17482", "abs": "https://arxiv.org/abs/2508.17482", "authors": ["S. Talha Bukhari", "Kaivalya Agrawal", "Zachary Kingston", "Aniket Bera"], "title": "Variational Shape Inference for Grasp Diffusion on SE(3)", "categories": ["cs.RO"], "comment": null, "summary": "Grasp synthesis is a fundamental task in robotic manipulation which usually\nhas multiple feasible solutions. Multimodal grasp synthesis seeks to generate\ndiverse sets of stable grasps conditioned on object geometry, making the robust\nlearning of geometric features crucial for success. To address this challenge,\nwe propose a framework for learning multimodal grasp distributions that\nleverages variational shape inference to enhance robustness against shape noise\nand measurement sparsity. Our approach first trains a variational autoencoder\nfor shape inference using implicit neural representations, and then uses these\nlearned geometric features to guide a diffusion model for grasp synthesis on\nthe SE(3) manifold. Additionally, we introduce a test-time grasp optimization\ntechnique that can be integrated as a plugin to further enhance grasping\nperformance. Experimental results demonstrate that our shape inference for\ngrasp synthesis formulation outperforms state-of-the-art multimodal grasp\nsynthesis methods on the ACRONYM dataset by 6.3%, while demonstrating\nrobustness to deterioration in point cloud density compared to other\napproaches. Furthermore, our trained model achieves zero-shot transfer to\nreal-world manipulation of household objects, generating 34% more successful\ngrasps than baselines despite measurement noise and point cloud calibration\nerrors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u5f62\u72b6\u63a8\u7406\u548c\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5b66\u4e60\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u591a\u6837\u7a33\u5b9a\u6293\u53d6\u5408\u6210\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u7279\u5f81\u5b66\u4e60\u4e2d\u5bf9\u566a\u58f0\u548c\u7a00\u758f\u6027\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5f62\u72b6\u63a8\u7406\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5728SE(3)\u6d41\u5f62\u4e0a\u751f\u6210\u6293\u53d6\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u6d4b\u8bd5\u65f6\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728ACRONYM\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u53476.3%\uff0c\u4e14\u5728\u70b9\u4e91\u7a00\u758f\u60c5\u51b5\u4e0b\u66f4\u9c81\u68d2\uff1b\u5b9e\u9645\u6293\u53d6\u6210\u529f\u7387\u8fbe34%\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6293\u53d6\u5408\u6210\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2508.16933", "pdf": "https://arxiv.org/pdf/2508.16933", "abs": "https://arxiv.org/abs/2508.16933", "authors": ["Dhandeep Challagundla", "Venkata Krishna Vamsi Sundarapu", "Ignatius Bezzam", "Riadul Islam"], "title": "TSPC-PFD: TSPC-Based Low-Power High-Resolution CMOS Phase Frequency Detector", "categories": ["cs.ET", "eess.SP"], "comment": null, "summary": "Phase Frequency Detectors (PFDs) are essential components in Phase-Locked\nLoop (PLL) and Delay-Locked Loop (DLL) systems, responsible for comparing phase\nand frequency differences and generating up/down signals to regulate charge\npumps and/or, consequently, Voltage-Controlled Oscillators (VCOs). Conventional\nPFD designs often suffer from significant dead zones and blind zones, which\ndegrade phase detection accuracy and increase jitter in high-speed\napplications. This paper addresses PFD design challenges and presents a novel\nlow-power True Single-Phase Clock (TSPC)-based PFD. The proposed design\neliminates the blind zone entirely while achieving a minimal dead zone of 40\nps. The proposed PFD, implemented using TSMC 28 nm technology, demonstrates a\nlow-power consumption of 4.41 uW at 3 GHz input frequency with a layout area of\n$10.42\\mu m^2$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f4e\u529f\u8017\u7684TSPC\u57fa\u76f8\u9891\u68c0\u6d4b\u5668\uff08PFD\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPFD\u7684\u6b7b\u533a\u548c\u76f2\u533a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPFD\u5728\u9ad8\u9891\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6b7b\u533a\u548c\u76f2\u533a\uff0c\u5f71\u54cd\u4e86\u76f8\u4f4d\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u589e\u52a0\u4e86\u6296\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528TSMC 28\u7eb3\u7c73\u6280\u672f\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8eTrue Single-Phase Clock (TSPC)\u7684\u65b0\u578bPFD\u8bbe\u8ba1\u3002", "result": "\u65b0\u578bPFD\u5b8c\u5168\u6d88\u9664\u4e86\u76f2\u533a\uff0c\u6b7b\u533a\u4ec5\u4e3a40 ps\uff0c\u529f\u8017\u4f4e\u81f34.41 uW\uff083 GHz\u8f93\u5165\u9891\u7387\uff09\uff0c\u5e03\u5c40\u9762\u79ef\u4e3a10.42 \u03bcm\u00b2\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u5728\u9ad8\u9891\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPFD\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u4f4e\u529f\u8017\u548c\u5c0f\u9762\u79ef\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.17547", "pdf": "https://arxiv.org/pdf/2508.17547", "abs": "https://arxiv.org/abs/2508.17547", "authors": ["Weikang Wan", "Jiawei Fu", "Xiaodi Yuan", "Yifeng Zhu", "Hao Su"], "title": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "CoRL 2025", "summary": "Developing robotic systems capable of robustly executing long-horizon\nmanipulation tasks with human-level dexterity is challenging, as such tasks\nrequire both physical dexterity and seamless sequencing of manipulation skills\nwhile robustly handling environment variations. While imitation learning offers\na promising approach, acquiring comprehensive datasets is resource-intensive.\nIn this work, we propose a learning framework and system LodeStar that\nautomatically decomposes task demonstrations into semantically meaningful\nskills using off-the-shelf foundation models, and generates diverse synthetic\ndemonstration datasets from a few human demos through reinforcement learning.\nThese sim-augmented datasets enable robust skill training, with a Skill Routing\nTransformer (SRT) policy effectively chaining the learned skills together to\nexecute complex long-horizon manipulation tasks. Experimental evaluations on\nthree challenging real-world long-horizon dexterous manipulation tasks\ndemonstrate that our approach significantly improves task performance and\nrobustness compared to previous baselines. Videos are available at\nlodestar-robot.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLodeStar\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5c06\u4efb\u52a1\u6f14\u793a\u5206\u89e3\u4e3a\u8bed\u4e49\u5316\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u5177\u5907\u4eba\u7c7b\u6c34\u5e73\u7075\u5de7\u6027\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u89e3\u51b3\u7269\u7406\u7075\u5de7\u6027\u548c\u6280\u80fd\u5e8f\u5217\u5316\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u96c6\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faLodeStar\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5206\u89e3\u4efb\u52a1\u6f14\u793a\u4e3a\u8bed\u4e49\u5316\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4f7f\u7528SRT\u7b56\u7565\u94fe\u5f0f\u8c03\u7528\u6280\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LodeStar\u901a\u8fc7\u81ea\u52a8\u5206\u89e3\u6280\u80fd\u548c\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u6311\u6218\u3002"}}
{"id": "2508.16980", "pdf": "https://arxiv.org/pdf/2508.16980", "abs": "https://arxiv.org/abs/2508.16980", "authors": ["Luis C. Mathias", "Atefeh Termehchi", "Taufik Abr\u00e3o", "Ekram Hossain"], "title": "Beamforming Control in RIS-Aided Wireless Communications: A Predictive Physics-Based Approach", "categories": ["eess.SY", "cs.SY", "eess.SP", "physics.app-ph"], "comment": "11 pages, 10 figures, four tables, 29 references. Full paper\n  submitted to IEEE-TWC", "summary": "Integrating reconfigurable intelligent surfaces (RIS) into wireless\ncommunication systems is a promising approach for enhancing coverage and data\nrates by intelligently redirecting signals, through a process known as\nbeamforming. However, the process of RIS beamforming (or passive beamforming)\ncontrol is associated with multiple latency-inducing factors. As a result, by\nthe time the beamforming is effectively updated, the channel conditions may\nhave already changed. For example, the low update rate of localization systems\nbecomes a critical limitation, as a mobile UE's position may change\nsignificantly between two consecutive measurements. To address this issue, this\nwork proposes a practical and scalable physics-based solution that is effective\nacross a wide range of UE movement models. Specifically, we propose a kinematic\nobserver and predictor to enable proactive RIS control. From low-rate position\nestimates provided by a localizer, the kinematic observer infers the UE's speed\nand acceleration. These motion parameters are then used by a predictor to\nestimate the UE's future positions at a higher rate, allowing the RIS to adjust\npromptly and compensate for inherent delays in both the RIS control and\nlocalization systems. Numerical results validate the effectiveness of the\nproposed approach, demonstrating real-time RIS adjustments with low\ncomputational complexity, even in scenarios involving rapid UE movement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b66\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8fd0\u52a8\u89c2\u6d4b\u5668\u548c\u9884\u6d4b\u5668\u5b9e\u73b0RIS\u7684\u4e3b\u52a8\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u79fb\u52a8\u7528\u6237\u4f4d\u7f6e\u53d8\u5316\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRIS\u6ce2\u675f\u6210\u5f62\u63a7\u5236\u56e0\u66f4\u65b0\u5ef6\u8fdf\u5bfc\u81f4\u4fe1\u9053\u6761\u4ef6\u53d8\u5316\uff0c\u5c24\u5176\u5728\u79fb\u52a8\u7528\u6237\u5feb\u901f\u8fd0\u52a8\u65f6\uff0c\u96be\u4ee5\u5b9e\u65f6\u8c03\u6574\u6ce2\u675f\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd0\u52a8\u89c2\u6d4b\u5668\u548c\u9884\u6d4b\u5668\u7684\u65b9\u6848\uff0c\u5229\u7528\u4f4e\u9891\u4f4d\u7f6e\u4f30\u8ba1\u63a8\u65ad\u7528\u6237\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\uff0c\u9884\u6d4b\u672a\u6765\u4f4d\u7f6e\u5e76\u5b9e\u65f6\u8c03\u6574RIS\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6848\u5728\u5feb\u901f\u79fb\u52a8\u573a\u666f\u4e0b\u80fd\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5b9e\u65f6RIS\u8c03\u6574\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u7528\u6237\u4f4d\u7f6e\u53d8\u5316\u5bfc\u81f4\u7684RIS\u63a7\u5236\u5ef6\u8fdf\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.17600", "pdf": "https://arxiv.org/pdf/2508.17600", "abs": "https://arxiv.org/abs/2508.17600", "authors": ["Guanxing Lu", "Baoxiong Jia", "Puhao Li", "Yixin Chen", "Ziwei Wang", "Yansong Tang", "Siyuan Huang"], "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Published at ICCV 2025. Project page:\n  https://gaussian-world-model.github.io/", "summary": "Training robot policies within a learned world model is trending due to the\ninefficiency of real-world interactions. The established image-based world\nmodels and policies have shown prior success, but lack robust geometric\ninformation that requires consistent spatial and physical understanding of the\nthree-dimensional world, even pre-trained on internet-scale video sources. To\nthis end, we propose a novel branch of world model named Gaussian World Model\n(GWM) for robotic manipulation, which reconstructs the future state by\ninferring the propagation of Gaussian primitives under the effect of robot\nactions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D\nvariational autoencoder, enabling fine-grained scene-level future state\nreconstruction with Gaussian Splatting. GWM can not only enhance the visual\nrepresentation for imitation learning agent by self-supervised future\nprediction training, but can serve as a neural simulator that supports\nmodel-based reinforcement learning. Both simulated and real-world experiments\ndepict that GWM can precisely predict future scenes conditioned on diverse\nrobot actions, and can be further utilized to train policies that outperform\nthe state-of-the-art by impressive margins, showcasing the initial data scaling\npotential of 3D world model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGaussian World Model (GWM)\u7684\u65b0\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u9ad8\u65af\u57fa\u5143\u4f20\u64ad\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u7ed3\u5408Diffusion Transformer\u548c3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4e09\u7ef4\u4e16\u754c\u7684\u7a33\u5b9a\u51e0\u4f55\u4fe1\u606f\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u7cbe\u786e\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u6a21\u578b\u3002", "method": "GWM\u4f7f\u7528\u9ad8\u65af\u57fa\u5143\u4f20\u64ad\u548cDiffusion Transformer\u4e0e3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u7684\u672a\u6765\u72b6\u6001\u91cd\u5efa\u3002", "result": "GWM\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u7cbe\u786e\u9884\u6d4b\u672a\u6765\u573a\u666f\uff0c\u5e76\u8bad\u7ec3\u51fa\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u7b56\u7565\u3002", "conclusion": "GWM\u5c55\u793a\u4e863D\u4e16\u754c\u6a21\u578b\u7684\u6570\u636e\u6269\u5c55\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2508.17143", "pdf": "https://arxiv.org/pdf/2508.17143", "abs": "https://arxiv.org/abs/2508.17143", "authors": ["Syed Muhammad Kazim", "Franziska Strasser", "Mia Kv\u00e5le L\u00f8vmo", "Andrii Nehrych", "Simon Moser", "Micha\u0142 Ziemczonok", "Wolfgang Heidrich", "Ivo Ihrke", "Monika Ritsch-Marte"], "title": "Performance Validation of Coded Wavefront Sensing for Quantitative Phase Imaging of Static and Dynamic Specimens Using Digital Holographic Microscopy", "categories": ["physics.optics", "eess.SP"], "comment": "Presented in ISCS25", "summary": "Coded wavefront sensing (Coded-WFS) is a snapshot quantitative phase imaging\n(QPI) technique that has been shown to successfully leverage the memory effect\nto retrieve the phase of biological specimens. In this paper, we perform QPI on\nstatic silica beads and dynamic HEK cells using Coded-WFS. The accuracy of the\nretrieved phase map is validated using digital holographic microscopy (DHM) for\nthe same specimens. We report comparisons of simultaneous bright-field\nintensity and optical path delay.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u79f0\u4e3aCoded-WFS\u7684\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\u6280\u672f\uff0c\u901a\u8fc7\u9759\u6001\u7845\u73e0\u548c\u52a8\u6001HEK\u7ec6\u80de\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u76f8\u4f4d\u6062\u590d\u51c6\u786e\u6027\uff0c\u5e76\u4e0e\u6570\u5b57\u5168\u606f\u663e\u5fae\u955c\uff08DHM\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "motivation": "\u9a8c\u8bc1Coded-WFS\u6280\u672f\u5728\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u751f\u7269\u6837\u672c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528Coded-WFS\u5bf9\u9759\u6001\u7845\u73e0\u548c\u52a8\u6001HEK\u7ec6\u80de\u8fdb\u884c\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\uff0c\u5e76\u4e0eDHM\u7684\u6210\u50cf\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8868\u660eCoded-WFS\u80fd\u591f\u51c6\u786e\u6062\u590d\u76f8\u4f4d\uff0c\u5e76\u4e0eDHM\u7684\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "Coded-WFS\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\u6280\u672f\uff0c\u9002\u5408\u7528\u4e8e\u751f\u7269\u6837\u672c\u7684\u76f8\u4f4d\u6062\u590d\u3002"}}
{"id": "2508.17643", "pdf": "https://arxiv.org/pdf/2508.17643", "abs": "https://arxiv.org/abs/2508.17643", "authors": ["Krishna Vinod", "Prithvi Jai Ramesh", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90v2e ROS\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728Gazebo\u4e2d\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u8bc4\u4f30\u4e8b\u4ef6\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5bfc\u822a\u4e0e\u64cd\u4f5c\u7b56\u7565\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u6a21\u62df\u73af\u5883\u4e2d\u76f8\u5173\u7814\u7a76\u8f83\u5c11\uff0c\u963b\u788d\u4e86\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86v2e ROS\u5de5\u5177\u5305\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u751f\u6210\u4e8b\u4ef6\u6d41\uff0c\u5e76\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u4e8b\u4ef6\u9a71\u52a8\u7b56\u7565\uff08ERP\uff09\uff0c\u7528\u4e8e\u5bfc\u822a\u4e0e\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e8b\u4ef6\u9a71\u52a8\u7b56\u7565\u5728\u79fb\u52a8\u673a\u5668\u4eba\u8ddf\u968f\u548c\u673a\u68b0\u81c2\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eRGB\u65b9\u6cd5\u3002", "conclusion": "\u4e8b\u4ef6\u9a71\u52a8\u611f\u77e5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.17149", "pdf": "https://arxiv.org/pdf/2508.17149", "abs": "https://arxiv.org/abs/2508.17149", "authors": ["Rahman Saadat Yeganeh", "Hamid Behroozi", "Mohammad Javad Omidi", "Mohammad Robat Mili", "Eduard A. Jorswieck", "Symeon Chatzinotas"], "title": "Enhancing Energy and Spectral Efficiency in IoT-Cellular Networks via Active SIM-Equipped LEO Satellites", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "This paper investigates a low Earth orbit (LEO) satellite communication\nsystem enhanced by an active stacked intelligent metasurface (ASIM), mounted on\nthe backplate of the satellite solar panels to efficiently utilize limited\nonboard space and reduce the main satellite power amplifier requirements. The\nsystem serves multiple ground users via rate-splitting multiple access (RSMA)\nand IoT devices through a symbiotic radio network. Multi-layer sequential\nprocessing in the ASIM improves effective channel gains and suppresses\ninter-user interference, outperforming active RIS and beyond-diagonal RIS\ndesigns. Three optimization approaches are evaluated: block coordinate descent\nwith successive convex approximation (BCD-SCA), model-assisted multi-agent\nconstraint soft actor-critic (MA-CSAC), and multi-constraint proximal policy\noptimization (MCPPO). Simulation results show that BCD-SCA converges fast and\nstably in convex scenarios without learning, MCPPO achieves rapid initial\nconvergence with moderate stability, and MA-CSAC attains the highest long-term\nspectral and energy efficiency in large-scale networks. Energy-spectral\nefficiency trade-offs are analyzed for different ASIM elements, satellite\nantennas, and transmit power. Overall, the study demonstrates that integrating\nmulti-layer ASIM with suitable optimization algorithms offers a scalable,\nenergy-efficient, and high-performance solution for next-generation LEO\nsatellite communications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4e3b\u52a8\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\uff08ASIM\uff09\u589e\u5f3a\u7684\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5229\u7528\u536b\u661f\u7a7a\u95f4\u5e76\u964d\u4f4e\u529f\u7387\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u63d0\u5347LEO\u536b\u661f\u901a\u4fe1\u7684\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u89e3\u51b3\u536b\u661f\u7a7a\u95f4\u6709\u9650\u548c\u529f\u7387\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528ASIM\u6280\u672f\u5e76\u7ed3\u5408\u591a\u7528\u6237\u8bbf\u95ee\u7b56\u7565\uff08RSMA\uff09\u548c\u7269\u8054\u7f51\u8bbe\u5907\u5171\u751f\u7f51\u7edc\uff0c\u901a\u8fc7\u4e09\u79cd\u4f18\u5316\u7b97\u6cd5\uff08BCD-SCA\u3001MA-CSAC\u3001MCPPO\uff09\u8fdb\u884c\u6027\u80fd\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u7684\u4f18\u5316\u7b97\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5404\u5f02\uff0c\u4f46\u6574\u4f53\u4e0aASIM\u4e0e\u4f18\u5316\u7b97\u6cd5\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u5347\u9891\u8c31\u548c\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u5c42ASIM\u7ed3\u5408\u5408\u9002\u7684\u4f18\u5316\u7b97\u6cd5\u4e3a\u4e0b\u4e00\u4ee3LEO\u536b\u661f\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17684", "pdf": "https://arxiv.org/pdf/2508.17684", "abs": "https://arxiv.org/abs/2508.17684", "authors": ["Kento Kawaharazuka", "Shogo Sawaguchi", "Ayumu Iwata", "Keita Yoneda", "Temma Suzuki", "Kei Okada"], "title": "MEVITA: Open-Source Bipedal Robot Assembled from E-Commerce Components via Sheet Metal Welding", "categories": ["cs.RO"], "comment": "Accepted at IEEE-RAS Humanoids2025, Website -\n  https://haraduka.github.io/mevita-hardware , YouTube -\n  https://youtu.be/_akfHkCne0s", "summary": "Various bipedal robots have been developed to date, and in recent years,\nthere has been a growing trend toward releasing these robots as open-source\nplatforms. This shift is fostering an environment in which anyone can freely\ndevelop bipedal robots and share their knowledge, rather than relying solely on\ncommercial products. However, most existing open-source bipedal robots are\ndesigned to be fabricated using 3D printers, which limits their scalability in\nsize and often results in fragile structures. On the other hand, some\nmetal-based bipedal robots have been developed, but they typically involve a\nlarge number of components, making assembly difficult, and in some cases, the\nparts themselves are not readily available through e-commerce platforms. To\naddress these issues, we developed MEVITA, an open-source bipedal robot that\ncan be built entirely from components available via e-commerce. Aiming for the\nminimal viable configuration for a bipedal robot, we utilized sheet metal\nwelding to integrate complex geometries into single parts, thereby\nsignificantly reducing the number of components and enabling easy assembly for\nanyone. Through reinforcement learning in simulation and Sim-to-Real transfer,\nwe demonstrated robust walking behaviors across various environments,\nconfirming the effectiveness of our approach. All hardware, software, and\ntraining environments can be obtained from https://github.com/haraduka/mevita .", "AI": {"tldr": "\u5f00\u6e90\u53cc\u8db3\u673a\u5668\u4ebaMEVITA\uff0c\u901a\u8fc7\u7535\u5546\u53ef\u8d2d\u96f6\u4ef6\u548c\u91d1\u5c5e\u677f\u6750\u710a\u63a5\u7b80\u5316\u7ec4\u88c5\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7a33\u5065\u884c\u8d70\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u53cc\u8db3\u673a\u5668\u4eba\u56e03D\u6253\u5370\u9650\u5236\u5bfc\u81f4\u7684\u8106\u5f31\u6027\uff0c\u4ee5\u53ca\u91d1\u5c5e\u673a\u5668\u4eba\u56e0\u96f6\u4ef6\u590d\u6742\u5bfc\u81f4\u7684\u7ec4\u88c5\u96be\u9898\u3002", "method": "\u91c7\u7528\u91d1\u5c5e\u677f\u6750\u710a\u63a5\u6280\u672f\u51cf\u5c11\u7ec4\u4ef6\u6570\u91cf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6280\u672f\u8bad\u7ec3\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u5404\u79cd\u73af\u5883\u4e2d\u7684\u7a33\u5065\u884c\u8d70\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "MEVITA\u4e3a\u5f00\u6e90\u53cc\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u7ec4\u88c5\u4e14\u6027\u80fd\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17210", "pdf": "https://arxiv.org/pdf/2508.17210", "abs": "https://arxiv.org/abs/2508.17210", "authors": ["Ali Zare", "Yao Shi", "Qiyu Sun"], "title": "Blind Deconvolution of Nonstationary Graph Signals over Shift-Invariant Channels", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "In this paper, we investigate blind deconvolution of nonstationary graph\nsignals from noisy observations, transmitted through an unknown shift-invariant\nchannel. The deconvolution process assumes that the observer has access to the\ncovariance structure of the original graph signals. To evaluate the\neffectiveness of our channel estimation and blind deconvolution method, we\nconduct numerical experiments using a temperature dataset in the Brest region\nof France.", "AI": {"tldr": "\u7814\u7a76\u975e\u5e73\u7a33\u56fe\u4fe1\u53f7\u5728\u672a\u77e5\u6052\u5b9a\u4fe1\u9053\u4e2d\u7684\u76f2\u53cd\u5377\u79ef\u95ee\u9898\uff0c\u4f7f\u7528\u6e29\u5ea6\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u5e73\u7a33\u56fe\u4fe1\u53f7\u5728\u672a\u77e5\u4fe1\u9053\u4e2d\u7684\u76f2\u53cd\u5377\u79ef\u95ee\u9898\uff0c\u5047\u8bbe\u5df2\u77e5\u539f\u59cb\u4fe1\u53f7\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u76f2\u53cd\u5377\u79ef\u548c\u4fe1\u9053\u4f30\u8ba1\u6280\u672f\u5904\u7406\u4fe1\u53f7\uff0c\u5229\u7528\u6e29\u5ea6\u6570\u636e\u96c6\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u76f2\u53cd\u5377\u79ef\u65b9\u6cd5\u53ef\u7528\u4e8e\u975e\u5e73\u7a33\u56fe\u4fe1\u53f7\u7684\u6062\u590d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2508.17753", "pdf": "https://arxiv.org/pdf/2508.17753", "abs": "https://arxiv.org/abs/2508.17753", "authors": ["Theresa Pekarek Rosin", "Julia Gachot", "Henri-Leon Kordt", "Matthias Kerzel", "Stefan Wermter"], "title": "Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "comment": "Accepted at the workshop on Foundation Models for Social Robotics\n  (FoMoSR) at ICSR 2025", "summary": "Automatic Speech Recognition (ASR) systems in real-world settings need to\nhandle imperfect audio, often degraded by hardware limitations or environmental\nnoise, while accommodating diverse user groups. In human-robot interaction\n(HRI), these challenges intersect to create a uniquely challenging recognition\nenvironment. We evaluate four state-of-the-art ASR systems on eight publicly\navailable datasets that capture six dimensions of difficulty: domain-specific,\naccented, noisy, age-variant, impaired, and spontaneous speech. Our analysis\ndemonstrates significant variations in performance, hallucination tendencies,\nand inherent biases, despite similar scores on standard benchmarks. These\nlimitations have serious implications for HRI, where recognition errors can\ninterfere with task performance, user trust, and safety.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cdASR\u7cfb\u7edf\u5728\u591a\u79cd\u56f0\u96be\u7ef4\u5ea6\u4e0b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u5176\u5728HRI\u4e2d\u7684\u5c40\u9650\u6027\u548c\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3ASR\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982HRI\uff09\u4e2d\u5bf9\u4e0d\u5b8c\u7f8e\u97f3\u9891\u548c\u591a\u7528\u6237\u7fa4\u4f53\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u5728\u516b\u79cd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56db\u79cd\u524d\u6cbfASR\u7cfb\u7edf\uff0c\u6db5\u76d6\u516d\u79cd\u56f0\u96be\u7ef4\u5ea6\u3002", "result": "\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u5e7b\u89c9\u503e\u5411\u548c\u56fa\u6709\u504f\u89c1\uff0c\u6807\u51c6\u57fa\u51c6\u5f97\u5206\u76f8\u8fd1\u4f46\u5b9e\u9645\u8868\u73b0\u4e0d\u540c\u3002", "conclusion": "ASR\u7cfb\u7edf\u7684\u5c40\u9650\u53ef\u80fd\u5f71\u54cdHRI\u7684\u4efb\u52a1\u6267\u884c\u3001\u4fe1\u4efb\u548c\u5b89\u5168\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2508.17480", "pdf": "https://arxiv.org/pdf/2508.17480", "abs": "https://arxiv.org/abs/2508.17480", "authors": ["Brian Chao", "Jacqueline Yang", "Suyeon Choi", "Manu Gopakumar", "Ryota Koiso", "Gordon Wetzstein"], "title": "Random-phase Gaussian Wave Splatting for Computer-generated Holography", "categories": ["cs.GR", "cs.AR", "eess.IV", "eess.SP", "physics.optics"], "comment": null, "summary": "Holographic near-eye displays offer ultra-compact form factors for virtual\nand augmented reality systems, but rely on advanced computer-generated\nholography (CGH) algorithms to convert 3D scenes into interference patterns\nthat can be displayed on spatial light modulators (SLMs). Gaussian Wave\nSplatting (GWS) has recently emerged as a powerful CGH paradigm that allows for\nthe conversion of Gaussians, a state-of-the-art neural 3D representation, into\nholograms. However, GWS assumes smooth-phase distributions over the Gaussian\nprimitives, limiting their ability to model view-dependent effects and\nreconstruct accurate defocus blur, and severely under-utilizing the\nspace-bandwidth product of the SLM. In this work, we propose random-phase GWS\n(GWS-RP) to improve bandwidth utilization, which has the effect of increasing\neyebox size, reconstructing accurate defocus blur and parallax, and supporting\ntime-multiplexed rendering to suppress speckle artifacts.\n  At the core of GWS-RP are (1) a fundamentally new wavefront compositing\nprocedure and (2) an alpha-blending scheme specifically designed for\nrandom-phase Gaussian primitives, ensuring physically correct color\nreconstruction and robust occlusion handling. Additionally, we present the\nfirst formally derived algorithm for applying random phase to Gaussian\nprimitives, grounded in rigorous statistical optics analysis and validated\nthrough practical near-eye display applications. Through extensive simulations\nand experimental validations, we demonstrate that these advancements,\ncollectively with time-multiplexing, uniquely enables full-bandwith light field\nCGH that supports accurate accurate parallax and defocus, yielding\nstate-of-the-art image quality and perceptually faithful 3D holograms for\nnext-generation near-eye displays.", "AI": {"tldr": "\u968f\u673a\u76f8\u4f4d\u9ad8\u65af\u6ce2\u6492\u5c04\uff08GWS-RP\uff09\u901a\u8fc7\u6539\u8fdb\u5e26\u5bbd\u5229\u7528\u7387\u548c\u65b0\u7684\u6ce2\u524d\u5408\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u773c\u5168\u606f\u663e\u793a\u7684\u8d28\u91cf\u548c\u5e94\u7528\u6548\u679c\u3002", "motivation": "GWS\u65b9\u6cd5\u5728\u5e73\u6ed1\u76f8\u4f4d\u5047\u8bbe\u4e0b\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u89c6\u89d2\u4f9d\u8d56\u6548\u5e94\u548c\u6563\u7126\u6a21\u7cca\uff0c\u9650\u5236\u4e86SLM\u7684\u7a7a\u95f4\u5e26\u5bbd\u5229\u7528\u7387\u3002GWS-RP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faGWS-RP\uff0c\u5305\u62ec\u65b0\u7684\u6ce2\u524d\u5408\u6210\u65b9\u6cd5\u548c\u9488\u5bf9\u968f\u673a\u76f8\u4f4d\u9ad8\u65af\u7684alpha\u6df7\u5408\u65b9\u6848\uff0c\u5e76\u9996\u6b21\u4e25\u683c\u63a8\u5bfc\u4e86\u968f\u673a\u76f8\u4f4d\u9ad8\u65af\u7684\u5e94\u7528\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cGWS-RP\u5b9e\u73b0\u4e86\u5168\u5e26\u5bbd\u5149\u573aCGH\uff0c\u652f\u6301\u51c6\u786e\u7684\u89c6\u5dee\u548c\u6563\u7126\uff0c\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u76843D\u5168\u606f\u56fe\u3002", "conclusion": "GWS-RP\u901a\u8fc7\u968f\u673a\u76f8\u4f4d\u548c\u65f6\u95f4\u590d\u7528\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u773c\u5168\u606f\u663e\u793a\u7684\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17797", "pdf": "https://arxiv.org/pdf/2508.17797", "abs": "https://arxiv.org/abs/2508.17797", "authors": ["Yunxiang Liu", "Hongkuo Niu", "Jianlin Zhu"], "title": "Adaptive Output Steps: FlexiSteps Network for Dynamic Trajectory Prediction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate trajectory prediction is vital for autonomous driving, robotics, and\nintelligent decision-making systems, yet traditional models typically rely on\nfixed-length output predictions, limiting their adaptability to dynamic\nreal-world scenarios. In this paper, we introduce the FlexiSteps Network (FSN),\na novel framework that dynamically adjusts prediction output time steps based\non varying contextual conditions. Inspired by recent advancements addressing\nobservation length discrepancies and dynamic feature extraction, FSN\nincorporates an pre-trained Adaptive Prediction Module (APM) to evaluate and\nadjust the output steps dynamically, ensuring optimal prediction accuracy and\nefficiency. To guarantee the plug-and-play of our FSN, we also design a Dynamic\nDecoder(DD). Additionally, to balance the prediction time steps and prediction\naccuracy, we design a scoring mechanism, which not only introduces the\nFr\\'echet distance to evaluate the geometric similarity between the predicted\ntrajectories and the ground truth trajectories but the length of predicted\nsteps is also considered. Extensive experiments conducted on benchmark datasets\nincluding Argoverse and INTERACTION demonstrate the effectiveness and\nflexibility of our proposed FSN framework.", "AI": {"tldr": "\u63d0\u51fa FlexiSteps \u7f51\u7edc\uff08FSN\uff09\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u8f93\u51fa\u65f6\u95f4\u6b65\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u7684\u56fa\u5b9a\u957f\u5ea6\u9884\u6d4b\u8f93\u51fa\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u9884\u6d4b\u6a21\u5757\uff08APM\uff09\u548c\u52a8\u6001\u89e3\u7801\u5668\uff08DD\uff09\uff0c\u8bbe\u8ba1\u8bc4\u5206\u673a\u5236\u5e73\u8861\u9884\u6d4b\u65f6\u95f4\u6b65\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728 Argoverse \u548c INTERACTION \u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86 FSN \u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "FSN \u6846\u67b6\u80fd\u591f\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u6b65\u957f\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.17820", "pdf": "https://arxiv.org/pdf/2508.17820", "abs": "https://arxiv.org/abs/2508.17820", "authors": ["Tingyu Ding", "Qunsong Zeng", "Kaibin Huang"], "title": "In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications", "categories": ["cs.AR", "eess.SP"], "comment": null, "summary": "The development of sixth-generation (6G) mobile networks imposes\nunprecedented latency and reliability demands on multiple-input multiple-output\n(MIMO) communication systems, a key enabler of high-speed radio access.\nRecently, deep unfolding-based detectors, which map iterative algorithms onto\nneural network architectures, have emerged as a promising approach, combining\nthe strengths of model-driven and data-driven methods to achieve high detection\naccuracy with relatively low complexity. However, algorithmic innovation alone\nis insufficient; software-hardware co-design is essential to meet the extreme\nlatency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to\npropose leveraging in-memory computing, which is an analog computing technology\nthat integrates memory and computation within memristor circuits, to perform\nthe intensive matrix-vector multiplication (MVM) operations inherent in deep\nMIMO detection at the nanosecond scale. Specifically, we introduce a novel\narchitecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized\nby two key features. First, each of its cascaded computational blocks is\ndecomposed into channel-dependent and channel-independent neural network\nmodules. Such a design minimizes the latency of memristor reprogramming in\nresponse to channel variations, which significantly exceeds computation time.\nSecond, we develop a customized detector-training method that exploits prior\nknowledge of memristor-value statistics to enhance robustness against\nprogramming noise. Furthermore, we conduct a comprehensive analysis of the\nIM-MIMO detector's performance, evaluating detection accuracy, processing\nlatency, and hardware complexity. Our study quantifies detection error as a\nfunction of various factors, including channel noise, memristor programming\nnoise, and neural network size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5b58\u8ba1\u7b97\u7684\u6df1\u5ea6MIMO\u68c0\u6d4b\u5668\uff08IM-MIMO\uff09\uff0c\u4ee5\u5e94\u5bf96G\u7f51\u7edc\u5bf9\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5b9a\u5236\u5316\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u5bf9MIMO\u7cfb\u7edf\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\uff0c\u73b0\u6709\u7b97\u6cd5\u548c\u786c\u4ef6\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u7ed3\u5408\u5185\u5b58\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u3002", "method": "\u63d0\u51faIM-MIMO\u68c0\u6d4b\u5668\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u6a21\u5757\u5206\u4e3a\u4fe1\u9053\u76f8\u5173\u548c\u65e0\u5173\u90e8\u5206\u4ee5\u51cf\u5c11\u5ef6\u8fdf\uff0c\u5e76\u5f00\u53d1\u5b9a\u5236\u5316\u8bad\u7ec3\u65b9\u6cd5\u589e\u5f3a\u5bf9\u7f16\u7a0b\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u68c0\u6d4b\u5668\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u786c\u4ef6\u590d\u6742\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u91cf\u5316\u4e86\u8bef\u5dee\u4e0e\u4fe1\u9053\u566a\u58f0\u3001\u7f16\u7a0b\u566a\u58f0\u53ca\u7f51\u7edc\u89c4\u6a21\u7684\u5173\u7cfb\u3002", "conclusion": "IM-MIMO\u68c0\u6d4b\u5668\u7ed3\u5408\u5185\u5b58\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e866G\u7f51\u7edc\u4e0b\u7684MIMO\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17830", "pdf": "https://arxiv.org/pdf/2508.17830", "abs": "https://arxiv.org/abs/2508.17830", "authors": ["Mary Kate Gale", "Kailana Baker-Matsuoka", "Ilana Nisky", "Allison Okamura"], "title": "Effect of Performance Feedback Timing on Motor Learning for a Surgical Training Task", "categories": ["cs.RO"], "comment": "Submitted to IEEE Transactions on Biomedical Engineering", "summary": "Objective: Robot-assisted minimally invasive surgery (RMIS) has become the\ngold standard for a variety of surgical procedures, but the optimal method of\ntraining surgeons for RMIS is unknown. We hypothesized that real-time, rather\nthan post-task, error feedback would better increase learning speed and reduce\nerrors. Methods: Forty-two surgical novices learned a virtual version of the\nring-on-wire task, a canonical task in RMIS training. We investigated the\nimpact of feedback timing with multi-sensory (haptic and visual) cues in three\ngroups: (1) real-time error feedback, (2) trial replay with error feedback, and\n(3) no error feedback. Results: Participant performance was evaluated based on\nthe accuracy of ring position and orientation during the task. Participants who\nreceived real-time feedback outperformed other groups in ring orientation.\nAdditionally, participants who received feedback in replay outperformed\nparticipants who did not receive any error feedback on ring orientation during\nlong, straight path sections. There were no significant differences between\ngroups for ring position overall, but participants who received real-time\nfeedback outperformed the other groups in positional accuracy on tightly curved\npath sections. Conclusion: The addition of real-time haptic and visual error\nfeedback improves learning outcomes in a virtual surgical task over error\nfeedback in replay or no error feedback at all. Significance: This work\ndemonstrates that multi-sensory error feedback delivered in real time leads to\nbetter training outcomes as compared to the same feedback delivered after task\ncompletion. This novel method of training may enable surgical trainees to\ndevelop skills with greater speed and accuracy.", "AI": {"tldr": "\u5b9e\u65f6\u591a\u611f\u5b98\u9519\u8bef\u53cd\u9988\u6bd4\u4efb\u52a1\u56de\u653e\u6216\u65e0\u53cd\u9988\u66f4\u80fd\u63d0\u9ad8\u865a\u62df\u624b\u672f\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\uff08RMIS\uff09\u4e2d\u5b9e\u65f6\u9519\u8bef\u53cd\u9988\u5bf9\u624b\u672f\u65b0\u624b\u5b66\u4e60\u901f\u5ea6\u548c\u9519\u8bef\u7387\u7684\u5f71\u54cd\u3002", "method": "42\u540d\u624b\u672f\u65b0\u624b\u5b8c\u6210\u865a\u62df\u624b\u672f\u4efb\u52a1\uff0c\u5206\u4e3a\u5b9e\u65f6\u53cd\u9988\u3001\u56de\u653e\u53cd\u9988\u548c\u65e0\u53cd\u9988\u4e09\u7ec4\u3002", "result": "\u5b9e\u65f6\u53cd\u9988\u7ec4\u5728\u73af\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u56de\u653e\u53cd\u9988\u7ec4\u5728\u76f4\u7ebf\u8def\u5f84\u4e0a\u4f18\u4e8e\u65e0\u53cd\u9988\u7ec4\u3002", "conclusion": "\u5b9e\u65f6\u591a\u611f\u5b98\u9519\u8bef\u53cd\u9988\u80fd\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u679c\uff0c\u9002\u7528\u4e8e\u624b\u672f\u6280\u80fd\u57f9\u8bad\u3002"}}
{"id": "2508.18045", "pdf": "https://arxiv.org/pdf/2508.18045", "abs": "https://arxiv.org/abs/2508.18045", "authors": ["Xiuheng Wang", "Ricardo Borsoi", "Arnaud Breloy", "C\u00e9dric Richard"], "title": "Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Non-parametric change-point detection in streaming time series data is a\nlong-standing challenge in signal processing. Recent advancements in statistics\nand machine learning have increasingly addressed this problem for data residing\non Riemannian manifolds. One prominent strategy involves monitoring abrupt\nchanges in the center of mass of the time series. Implemented in a streaming\nfashion, this strategy, however, requires careful step size tuning when\ncomputing the updates of the center of mass. In this paper, we propose to\nleverage robust centroid on manifolds from M-estimation theory to address this\nissue. Our proposal consists of comparing two centroid estimates: the classical\nKarcher mean (sensitive to change) versus one defined from Huber's function\n(robust to change). This comparison leads to the definition of a test statistic\nwhose performance is less sensitive to the underlying estimation method. We\npropose a stochastic Riemannian optimization algorithm to estimate both robust\ncentroids efficiently. Experiments conducted on both simulated and real-world\ndata across two representative manifolds demonstrate the superior performance\nof our proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u5316\u7684\u6d41\u5f0f\u65f6\u7a7a\u6570\u636e\u53d8\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u6d41\u5f62\u4e0a\u7684\u9c81\u68d2\u8d28\u5fc3\u6765\u51cf\u5c11\u5bf9\u6b65\u957f\u8c03\u4f18\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u4e0e\u7ecf\u5178Karcher\u5747\u503c\u7684\u5bf9\u6bd4\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6d41\u5f0f\u65f6\u7a7a\u6570\u636e\u4e2d\u4f7f\u7528\u8d28\u5fc3\u53d8\u5316\u68c0\u6d4b\u53d8\u70b9\u65f6\uff0c\u9700\u8981\u7cbe\u7ec6\u7684\u6b65\u957f\u8c03\u4f18\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u9c81\u68d2\u8d28\u5fc3\u6765\u51cf\u5c11\u8fd9\u79cd\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eM\u4f30\u8ba1\u7406\u8bba\u7684\u9c81\u68d2\u8d28\u5fc3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83Karcher\u5747\u503c\uff08\u5bf9\u53d8\u5316\u654f\u611f\uff09\u548c\u57fa\u4e8eHuber\u51fd\u6570\u7684\u9c81\u68d2\u8d28\u5fc3\uff08\u5bf9\u53d8\u5316\u9c81\u68d2\uff09\u6765\u5b9a\u4e49\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u3002", "result": "\u5728\u4e24\u79cd\u5178\u578b\u6d41\u5f62\u4e0a\u7684\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d41\u5f0f\u65f6\u7a7a\u6570\u636e\u4e2d\u7684\u975e\u53c2\u6570\u53d8\u70b9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.17831", "pdf": "https://arxiv.org/pdf/2508.17831", "abs": "https://arxiv.org/abs/2508.17831", "authors": ["Yuan Fang", "Fangzhan Shi", "Xijia Wei", "Qingchao Chen", "Kevin Chetty", "Simon Julier"], "title": "CubeDN: Real-time Drone Detection in 3D Space from Dual mmWave Radar Cubes", "categories": ["cs.RO"], "comment": null, "summary": "As drone use has become more widespread, there is a critical need to ensure\nsafety and security. A key element of this is robust and accurate drone\ndetection and localization. While cameras and other optical sensors like LiDAR\nare commonly used for object detection, their performance degrades under\nadverse lighting and environmental conditions. Therefore, this has generated\ninterest in finding more reliable alternatives, such as millimeter-wave\n(mmWave) radar. Recent research on mmWave radar object detection has\npredominantly focused on 2D detection of road users. Although these systems\ndemonstrate excellent performance for 2D problems, they lack the sensing\ncapability to measure elevation, which is essential for 3D drone detection. To\naddress this gap, we propose CubeDN, a single-stage end-to-end radar object\ndetection network specifically designed for flying drones. CubeDN overcomes\nchallenges such as poor elevation resolution by utilizing a dual radar\nconfiguration and a novel deep learning pipeline. It simultaneously detects,\nlocalizes, and classifies drones of two sizes, achieving decimeter-level\ntracking accuracy at closer ranges with overall $95\\%$ average precision (AP)\nand $85\\%$ average recall (AR). Furthermore, CubeDN completes data processing\nand inference at 10Hz, making it highly suitable for practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCubeDN\uff0c\u4e00\u79cd\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7f51\u7edc\uff0c\u7528\u4e8e\u65e0\u4eba\u673a3D\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u514b\u670d\u4e86\u73b0\u67092D\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7684\u666e\u53ca\uff0c\u5b89\u5168\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5149\u5b66\u4f20\u611f\u5668\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u57282D\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u7f3a\u4e4f3D\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "CubeDN\u91c7\u7528\u53cc\u96f7\u8fbe\u914d\u7f6e\u548c\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u89e3\u51b3\u4f4e\u4ef0\u89d2\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u3002", "result": "\u5728\u8fd1\u8ddd\u79bb\u5b9e\u73b0\u5206\u7c73\u7ea7\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e73\u5747\u7cbe\u5ea695%\uff0c\u5e73\u5747\u53ec\u56de\u738785%\uff0c\u5904\u7406\u901f\u5ea6\u8fbe10Hz\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "CubeDN\u586b\u8865\u4e863D\u65e0\u4eba\u673a\u68c0\u6d4b\u7684\u6280\u672f\u7a7a\u767d\uff0c\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.17922", "pdf": "https://arxiv.org/pdf/2508.17922", "abs": "https://arxiv.org/abs/2508.17922", "authors": ["Bokai Ji", "Jie Gu", "Xiaokang Ma", "Chu Tang", "Jingmin Chen", "Guangxia Li"], "title": "Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Affordance is crucial for intelligent robots in the context of object\nmanipulation. In this paper, we argue that affordance should be\ntask-/instruction-dependent, which is overlooked by many previous works. That\nis, different instructions can lead to different manipulation regions and\ndirections even for the same object. According to this observation, we present\na new dataset comprising fifteen thousand object-instruction-affordance\ntriplets. All scenes in the dataset are from an egocentric viewpoint, designed\nto approximate the perspective of a human-like robot. Furthermore, we\ninvestigate how to enable large multimodal models (LMMs) to serve as affordance\npredictors by implementing a ``search against verifiers'' pipeline. An LMM is\nasked to progressively predict affordances, with the output at each step being\nverified by itself during the iterative process, imitating a reasoning process.\nExperiments show that our method not only unlocks new instruction-oriented\naffordance prediction capabilities, but also achieves outstanding performance\nbroadly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4efb\u52a1/\u6307\u4ee4\u4f9d\u8d56\u7684affordance\u6982\u5ff5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5927\u91cfobject-instruction-affordance\u4e09\u5143\u7ec4\u7684\u6570\u636e\u96c6\uff0c\u540c\u65f6\u901a\u8fc7\u201csearch against verifiers\u201d\u6846\u67b6\u5229\u7528\u5927\u6a21\u578b\u9884\u6d4baffordance\u3002", "motivation": "\u4f20\u7edf\u7684affordance\u7814\u7a76\u5ffd\u7565\u4e86\u4efb\u52a1\u6216\u6307\u4ee4\u7684\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e0d\u7b26\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b15,000\u4e2a\u4e09\u5143\u7ec4\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u201csearch against verifiers\u201d\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u9a8c\u8bc1\u9010\u6b65\u9884\u6d4baffordance\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5b9e\u73b0\u6307\u4ee4\u5bfc\u5411\u7684affordance\u9884\u6d4b\uff0c\u8fd8\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u4efb\u52a1/\u6307\u4ee4\u4f9d\u8d56\u6027\u5bf9affordance\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2508.17969", "pdf": "https://arxiv.org/pdf/2508.17969", "abs": "https://arxiv.org/abs/2508.17969", "authors": ["Alexandros Gkillas", "Christos Anagnostopoulos", "Nikos Piperigkos", "Dimitris Tsiktsiris", "Theofilos Christodoulou", "Theofanis Siamatras", "Dimitrios Triantafyllou", "Christos Basdekis", "Theoktisti Marinopoulou", "Panagiotis Lepentsiotis", "Elefterios Blitsis", "Aggeliki Zacharaki", "Nearchos Stylianidis", "Leonidas Katelaris", "Lamberto Salvan", "Aris S. Lalos", "Christos Laoudias", "Antonios Lalas", "Konstantinos Votis"], "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5185\u5916\u76d1\u6d4b\u611f\u77e5\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u901a\u8fc7AI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u6846\u67b6\u4f18\u5316\u8f66\u5185\u611f\u77e5\u4e0e\u4f53\u9a8c\u7684\u76ee\u6807\u3002", "motivation": "\u901a\u8fc7\u5185\u5916\u76d1\u6d4b\u7cfb\u7edf\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u4e0e\u8212\u9002\u6027\uff0c\u4f18\u5316\u4e58\u5ba2\u4f53\u9a8c\u3002", "method": "\u5185\u90e8\u76d1\u6d4b\u7cfb\u7edf\u91c7\u7528\u591a\u6444\u50cf\u5934\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u9a7e\u9a76\u5458\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u667a\u80fd\u4f20\u611f\u5668\u5206\u6790\u8f66\u5185\u73af\u5883\uff1b\u5916\u90e8\u76d1\u6d4b\u7cfb\u7edf\u901a\u8fc7LiDAR\u8fdb\u884c\u9ad8\u6548\u7684\u8bed\u4e49\u5206\u5272\u548c\u70b9\u4e91\u8d85\u5206\u8fa8\u7387\u5904\u7406\u3002", "result": "\u7cfb\u7edf\u5728\u771f\u5b9e\u7535\u52a8\u8f66\u4e0a\u90e8\u7f72\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u5404\u6a21\u5757\u7684\u6027\u80fd\u548c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.17985", "pdf": "https://arxiv.org/pdf/2508.17985", "abs": "https://arxiv.org/abs/2508.17985", "authors": ["Abu Shad Ahammed", "Md Shahi Amran Hossain", "Sayeri Mukherjee", "Roman Obermaisser", "Md. Ziaur Rahman"], "title": "Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE", "categories": ["cs.RO"], "comment": null, "summary": "Ensuring safety in autonomous driving requires a seamless integration of\nperception and decision making under uncertain conditions. Although computer\nvision (CV) models such as YOLO achieve high accuracy in detecting traffic\nsigns and obstacles, their performance degrades in drift scenarios caused by\nweather variations or unseen objects. This work presents a simulated autonomous\ndriving system that combines a context aware CV model with adaptive control\nusing the ADORE framework. The CARLA simulator was integrated with ADORE via\nthe ROS bridge, allowing real-time communication between perception, decision,\nand control modules. A simulated test case was designed in both clear and drift\nweather conditions to demonstrate the robust detection performance of the\nperception model while ADORE successfully adapted vehicle behavior to speed\nlimits and obstacles with low response latency. The findings highlight the\npotential of coupling deep learning-based perception with rule-based adaptive\ndecision making to improve automotive safety critical system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5CV\u6a21\u578b\u4e0e\u81ea\u9002\u5e94\u63a7\u5236ADORE\u6846\u67b6\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5728CARLA\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u5929\u6c14\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u4f4e\u5ef6\u8fdf\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u5929\u6c14\u53d8\u5316\u6216\u672a\u89c1\u7269\u4f53\u7b49\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u4e0e\u51b3\u7b56\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u4f7f\u7528ADORE\u6846\u67b6\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5CV\u6a21\u578b\uff0c\u5728CARLA\u6a21\u62df\u5668\u4e2d\u901a\u8fc7ROS\u6865\u8fdb\u884c\u5b9e\u65f6\u901a\u4fe1\uff0c\u8bbe\u8ba1\u6d4b\u8bd5\u6848\u4f8b\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u5728\u590d\u6742\u5929\u6c14\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u68c0\u6d4b\u6027\u80fd\uff0cADORE\u6210\u529f\u9002\u5e94\u901f\u5ea6\u9650\u5236\u548c\u969c\u788d\u7269\uff0c\u5177\u6709\u4f4e\u54cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2508.17986", "pdf": "https://arxiv.org/pdf/2508.17986", "abs": "https://arxiv.org/abs/2508.17986", "authors": ["Karel Bartunek", "Lukas Rustler", "Matej Hoffmann"], "title": "No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin", "categories": ["cs.RO"], "comment": "Submitted for review to ICRA 2026", "summary": "Locating and grasping of objects by robots is typically performed using\nvisual sensors. Haptic feedback from contacts with the environment is only\nsecondary if present at all. In this work, we explored an extreme case of\nsearching for and grasping objects in complete absence of visual input, relying\non haptic feedback only. The main novelty lies in the use of contacts over the\ncomplete surface of a robot manipulator covered with sensitive skin. The search\nis divided into two phases: (1) coarse workspace exploration with the complete\nrobot surface, followed by (2) precise localization using the end-effector\nequipped with a force/torque sensor. We systematically evaluated this method in\nsimulation and on the real robot, demonstrating that diverse objects can be\nlocated, grasped, and put in a basket. The overall success rate on the real\nrobot for one object was 85.7\\% with failures mainly while grasping specific\nobjects. The method using whole-body contacts is six times faster compared to a\nbaseline that uses haptic feedback only on the end-effector. We also show\nlocating and grasping multiple objects on the table. This method is not\nrestricted to our specific setup and can be deployed on any platform with the\nability of sensing contacts over the entire body surface. This work holds\npromise for diverse applications in areas with challenging visual perception\n(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or\nvegetables need to be located inside foliage and picked.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5b8c\u5168\u65e0\u89c6\u89c9\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u673a\u5668\u4eba\u5168\u8eab\u89e6\u89c9\u53cd\u9988\u8fdb\u884c\u7269\u4f53\u5b9a\u4f4d\u4e0e\u6293\u53d6\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5206\u9636\u6bb5\u641c\u7d22\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u611f\u77e5\u53d7\u9650\u73af\u5883\uff08\u5982\u5149\u7ebf\u4e0d\u8db3\u3001\u906e\u6321\u7b49\uff09\u4e0b\u7684\u7269\u4f53\u5b9a\u4f4d\u4e0e\u6293\u53d6\u95ee\u9898\uff0c\u63a2\u7d22\u89e6\u89c9\u53cd\u9988\u7684\u6f5c\u529b\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1) \u5229\u7528\u5168\u8eab\u654f\u611f\u76ae\u80a4\u8fdb\u884c\u7c97\u7565\u63a2\u7d22\uff1b2) \u7528\u672b\u7aef\u6267\u884c\u5668\u7684\u529b/\u529b\u77e9\u4f20\u611f\u5668\u7cbe\u786e\u5b9a\u4f4d\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5355\u7269\u4f53\u6210\u529f\u7387\u4e3a85.7%\uff0c\u5168\u8eab\u89e6\u89c9\u65b9\u6cd5\u7684\u6548\u7387\u662f\u4ec5\u7528\u672b\u7aef\u89e6\u89c9\u76846\u500d\u3002\u5c55\u793a\u4e86\u591a\u7269\u4f53\u5b9a\u4f4d\u4e0e\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u5907\u5168\u8eab\u89e6\u89c9\u611f\u77e5\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5728\u89c6\u89c9\u53d7\u9650\u573a\u666f\uff08\u5982\u519c\u4e1a\u91c7\u6458\uff09\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.18039", "pdf": "https://arxiv.org/pdf/2508.18039", "abs": "https://arxiv.org/abs/2508.18039", "authors": ["Diego Quevedo", "Sarah Hudson", "Donghoon Kim"], "title": "Modeling and Control Framework for Autonomous Space Manipulator Handover Operations", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "14 pages, submitted to 2025 Astrodynamics Specialists Conference\n  proceedings", "summary": "Autonomous space robotics is poised to play a vital role in future space\nmissions, particularly for In-space Servicing, Assembly, and Manufacturing\n(ISAM). A key capability in such missions is the Robot-to-Robot (R2R) handover\nof mission-critical objects. This work presents a dynamic model of a dual-arm\nspace manipulator system and compares various tracking control laws. The key\ncontributions of this work are the development of a cooperative manipulator\ndynamic model and the comparative analysis of control laws to support\nautonomous R2R handovers in ISAM scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53cc\u673a\u68b0\u81c2\u7a7a\u95f4\u64cd\u4f5c\u7cfb\u7edf\u7684\u52a8\u6001\u6a21\u578b\u53ca\u5176\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u673a\u5668\u95f4\u4efb\u52a1\u5173\u952e\u5bf9\u8c61\u4ea4\u63a5\u3002", "motivation": "\u672a\u6765\u7a7a\u95f4\u4efb\u52a1\u4e2d\uff0c\u81ea\u4e3b\u7a7a\u95f4\u673a\u5668\u4eba\u5bf9\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u4e0e\u5236\u9020\uff08ISAM\uff09\u81f3\u5173\u91cd\u8981\uff0c\u800c\u673a\u5668\u95f4\u4ea4\u63a5\u4efb\u52a1\u5173\u952e\u5bf9\u8c61\u662f\u5176\u6838\u5fc3\u80fd\u529b\u4e4b\u4e00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u4e86\u534f\u4f5c\u673a\u68b0\u81c2\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u5bf9\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u4e3bR2R\u4ea4\u63a5\u5728ISAM\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.18066", "pdf": "https://arxiv.org/pdf/2508.18066", "abs": "https://arxiv.org/abs/2508.18066", "authors": ["Alberto Silvio Chiappa", "Boshi An", "Merkourios Simos", "Chengkun Li", "Alexander Mathis"], "title": "Arnold: a generalist muscle transformer policy", "categories": ["cs.RO", "cs.AI", "cs.LG", "q-bio.QM"], "comment": "A.S.C. and B.A. contributed equally. Code is available at\n  https://github.com/amathislab/arnold-the-generalist", "summary": "Controlling high-dimensional and nonlinear musculoskeletal models of the\nhuman body is a foundational scientific challenge. Recent machine learning\nbreakthroughs have heralded policies that master individual skills like\nreaching, object manipulation and locomotion in musculoskeletal systems with\nmany degrees of freedom. However, these agents are merely \"specialists\",\nachieving high performance for a single skill. In this work, we develop Arnold,\na generalist policy that masters multiple tasks and embodiments. Arnold\ncombines behavior cloning and fine-tuning with PPO to achieve expert or\nsuper-expert performance in 14 challenging control tasks from dexterous object\nmanipulation to locomotion. A key innovation is Arnold's sensorimotor\nvocabulary, a compositional representation of the semantics of heterogeneous\nsensory modalities, objectives, and actuators. Arnold leverages this vocabulary\nvia a transformer architecture to deal with the variable observation and action\nspaces of each task. This framework supports efficient multi-task,\nmulti-embodiment learning and facilitates rapid adaptation to novel tasks.\nFinally, we analyze Arnold to provide insights into biological motor control,\ncorroborating recent findings on the limited transferability of muscle\nsynergies across tasks.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u540d\u4e3aArnold\u7684\u901a\u7528\u7b56\u7565\uff0c\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u572814\u9879\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u8868\u73b0\u3002\u5173\u952e\u521b\u65b0\u662f\u5176\u611f\u77e5\u8fd0\u52a8\u8bcd\u6c47\u548cTransformer\u67b6\u6784\uff0c\u652f\u6301\u591a\u4efb\u52a1\u548c\u591a\u4f53\u73b0\u5b66\u4e60\u3002", "motivation": "\u76ee\u524d\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u63a7\u5236\u591a\u4e3a\u5355\u4e00\u4efb\u52a1\u4e13\u5bb6\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u80fd\u5904\u7406\u591a\u4efb\u52a1\u548c\u591a\u4f53\u73b0\u7684\u901a\u7528\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u5f15\u5165\u611f\u77e5\u8fd0\u52a8\u8bcd\u6c47\u548cTransformer\u67b6\u6784\uff0c\u5904\u7406\u53ef\u53d8\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "Arnold\u572814\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5e76\u63ed\u793a\u4e86\u808c\u8089\u534f\u540c\u4f5c\u7528\u5728\u4efb\u52a1\u95f4\u7684\u6709\u9650\u53ef\u8f6c\u79fb\u6027\u3002", "conclusion": "Arnold\u4e3a\u901a\u7528\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u751f\u7269\u8fd0\u52a8\u63a7\u5236\u7814\u7a76\u4e2d\u6709\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2508.18074", "pdf": "https://arxiv.org/pdf/2508.18074", "abs": "https://arxiv.org/abs/2508.18074", "authors": ["Zhaokun Chen", "Wenshuo Wang", "Wenzhuo Liu", "Yichen Liu", "Junqiang Xi"], "title": "The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation", "categories": ["cs.RO"], "comment": null, "summary": "Communication delays in mobile robot teleoperation adversely affect\nhuman-machine collaboration. Understanding delay effects on human operational\nperformance and neurocognition is essential for resolving this issue. However,\nno previous research has explored this. To fill this gap, we conduct a\nhuman-in-the-loop experiment involving 10 participants, integrating\nelectroencephalography (EEG) and robot behavior data under varying delays\n(0-500 ms in 100 ms increments) to systematically investigate these effects.\nBehavior analysis reveals significant performance degradation at 200-300 ms\ndelays, affecting both task efficiency and accuracy. EEG analysis discovers\nfeatures with significant delay dependence: frontal $\\theta/\\beta$-band and\nparietal $\\alpha$-band power. We also identify a threshold window (100-200 ms)\nfor early perception of delay in humans, during which these EEG features first\nexhibit significant differences. When delay exceeds 400 ms, all features\nplateau, indicating saturation of cognitive resource allocation at\nphysiological limits. These findings provide the first evidence of perceptual\nand cognitive delay thresholds during teleoperation tasks in humans, offering\ncritical neurocognitive insights for the design of delay compensation\nstrategies.", "AI": {"tldr": "\u79fb\u52a8\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u4eba\u4e0e\u673a\u5668\u534f\u4f5c\u6709\u8d1f\u9762\u5f71\u54cd\u3002\u7814\u7a76\u9996\u6b21\u901a\u8fc7\u4eba\u673a\u5b9e\u9a8c\u7ed3\u5408EEG\u548c\u673a\u5668\u4eba\u884c\u4e3a\u6570\u636e\uff0c\u7cfb\u7edf\u63a2\u7a76\u4e86\u5ef6\u8fdf\u5bf9\u4eba\u7684\u64cd\u4f5c\u8868\u73b0\u548c\u795e\u7ecf\u8ba4\u77e5\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u534f\u4f5c\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u4ee5\u5f80\u7814\u7a76\u5bf9\u5ef6\u8fdf\u6548\u5e94\u7684\u8ba4\u77e5\u7a7a\u767d\u3002", "method": "10\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u673a\u5b9e\u9a8c\uff0cEEG\u548c\u884c\u4e3a\u6570\u636e\u7ed3\u5408\uff0c\u5ef6\u8fdf\u4ece0\u5230500 ms\u4ee5100 ms\u9012\u589e\u3002", "result": "200-300 ms\u5ef6\u8fdf\u663e\u8457\u5f71\u54cd\u4efb\u52a1\u6548\u7387\u548c\u51c6\u786e\u6027\uff1bEEG\u7279\u5f81\u663e\u793a\u5ef6\u8fdf\u4f9d\u8d56\u5173\u7cfb\uff0c\u9608\u503c\u7a97\u53e3\u4e3a100-200 ms\uff1b\u8d85\u8fc7400 ms\u65f6\u8ba4\u77e5\u8d44\u6e90\u5206\u914d\u8fbe\u5230\u751f\u7406\u6781\u9650\u3002", "conclusion": "\u9996\u6b21\u63ed\u793a\u4e86\u4eba\u7c7b\u5728\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u548c\u8ba4\u77e5\u5ef6\u8fdf\u9608\u503c\uff0c\u4e3a\u5ef6\u8fdf\u8865\u507f\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u795e\u7ecf\u8ba4\u77e5\u4f9d\u636e\u3002"}}
{"id": "2508.18139", "pdf": "https://arxiv.org/pdf/2508.18139", "abs": "https://arxiv.org/abs/2508.18139", "authors": ["Prathima Ananda Kumar"], "title": "Analysis of Harpy's Constrained Trotting and Jumping Maneuver", "categories": ["cs.RO"], "comment": "Master's Thesis", "summary": "This study presents an analysis of experimental data from Harpy, a\nthruster-assisted bipedal robot developed at Northeastern University. The study\nexamines data sets from trotting and jumping experiments to understand the\nfundamental principles governing hybrid leg-thruster locomotion. Through data\nanalysis across multiple locomotion modes, this research reveals that Harpy\nachieves stable locomotion with bounded trajectories and consistent foot\nplacement through strategic leg-thruster synergy. The results demonstrate\ncontrolled joint behavior with low torques and symmetric tracking, accurate\nfoot placement within kinematic constraints despite phase-transition\nperturbations, and underactuated degree-of-freedom stability without\ndivergence. Energy level analysis reveals that legs provide primary propulsion,\nwhile the thrusters enable additional aerial phase control. The analysis\nidentifies critical body-leg coupling dynamics during aerial phases that\nrequire phase-specific control strategies. Consistent repeatability and\nsymmetry across experiments validate the robustness of the hybrid actuation\napproach.", "AI": {"tldr": "\u5bf9Harpy\uff08\u4e00\u79cd\u5e26\u63a8\u8fdb\u5668\u7684\u53cc\u8db3\u673a\u5668\u4eba\uff09\u7684\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u901a\u8fc7\u817f\u90e8\u548c\u63a8\u8fdb\u5668\u7684\u534f\u540c\u5b9e\u73b0\u7a33\u5b9a\u8fd0\u52a8\u7684\u5173\u952e\u539f\u7406\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6df7\u5408\u817f-\u63a8\u8fdb\u5668\u8fd0\u52a8\u7684\u57fa\u672c\u539f\u7406\uff0c\u4ee5\u53ca\u5982\u4f55\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u63a7\u7684\u8fd0\u52a8\u3002", "method": "\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u884c\u8d70\u548c\u8df3\u8dc3\u7684\u6570\u636e\uff0c\u7814\u7a76\u5176\u8fd0\u52a8\u6a21\u5f0f\u3001\u5173\u8282\u884c\u4e3a\u548c\u80fd\u91cf\u5206\u914d\u3002", "result": "Harpy\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u5173\u8282\u884c\u4e3a\u3001\u7cbe\u51c6\u7684\u8db3\u90e8\u5b9a\u4f4d\u548c\u5bf9\u79f0\u7684\u8fd0\u52a8\uff0c\u63a8\u8fdb\u5668\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u7a7a\u4e2d\u63a7\u5236\uff0c\u817f\u90e8\u4e3b\u5bfc\u63a8\u8fdb\u3002", "conclusion": "\u6df7\u5408\u9a71\u52a8\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u63a7\u5236\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u817f-\u63a8\u8fdb\u5668\u534f\u540c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18153", "pdf": "https://arxiv.org/pdf/2508.18153", "abs": "https://arxiv.org/abs/2508.18153", "authors": ["Aalok Patwardhan", "Andrew J. Davison"], "title": "DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot Swarms with Gaussian Belief Propagation", "categories": ["cs.RO"], "comment": null, "summary": "Robot swarms require cohesive collective behaviour to address diverse\nchallenges, including shape formation and decision-making. Existing approaches\noften treat consensus in discrete and continuous decision spaces as distinct\nproblems. We present DANCeRS, a unified, distributed algorithm leveraging\nGaussian Belief Propagation (GBP) to achieve consensus in both domains. By\nrepresenting a swarm as a factor graph our method ensures scalability and\nrobustness in dynamic environments, relying on purely peer-to-peer message\npassing. We demonstrate the effectiveness of our general framework through two\napplications where agents in a swarm must achieve consensus on global behaviour\nwhilst relying on local communication. In the first, robots must perform path\nplanning and collision avoidance to create shape formations. In the second, we\nshow how the same framework can be used by a group of robots to form a\nconsensus over a set of discrete decisions. Experimental results highlight our\nmethod's scalability and efficiency compared to recent approaches to these\nproblems making it a promising solution for multi-robot systems requiring\ndistributed consensus. We encourage the reader to see the supplementary video\ndemo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDANCeRS\u7684\u7edf\u4e00\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\uff08GBP\uff09\u5728\u79bb\u6563\u548c\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u8fbe\u6210\u5171\u8bc6\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u5f62\u72b6\u5f62\u6210\u548c\u51b3\u7b56\u5236\u5b9a\u4e2d\u9700\u8981\u7edf\u4e00\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u5c06\u79bb\u6563\u548c\u8fde\u7eed\u51b3\u7b56\u89c6\u4e3a\u72ec\u7acb\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u7fa4\u4f53\u8868\u793a\u4e3a\u56e0\u5b50\u56fe\uff0c\u4f7f\u7528\u7eaf\u5bf9\u7b49\u6d88\u606f\u4f20\u9012\u5b9e\u73b0\u5206\u5e03\u5f0f\u5171\u8bc6\uff0c\u786e\u4fdd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDANCeRS\u5728\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u907f\u969c\u548c\u79bb\u6563\u51b3\u7b56\u5f62\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DANCeRS\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.18249", "pdf": "https://arxiv.org/pdf/2508.18249", "abs": "https://arxiv.org/abs/2508.18249", "authors": ["Zipeng Fang", "Yanbo Wang", "Lei Zhao", "Weidong Chen"], "title": "Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Traversability estimation is critical for enabling robots to navigate across\ndiverse terrains and environments. While recent self-supervised learning\nmethods achieve promising results, they often fail to capture the\ncharacteristics of non-traversable regions. Moreover, most prior works\nconcentrate on a single modality, overlooking the complementary strengths\noffered by integrating heterogeneous sensory modalities for more robust\ntraversability estimation. To address these limitations, we propose a\nmultimodal self-supervised framework for traversability labeling and\nestimation. First, our annotation pipeline integrates footprint, LiDAR, and\ncamera data as prompts for a vision foundation model, generating traversability\nlabels that account for both semantic and geometric cues. Then, leveraging\nthese labels, we train a dual-stream network that jointly learns from different\nmodalities in a decoupled manner, enhancing its capacity to recognize diverse\ntraversability patterns. In addition, we incorporate sparse LiDAR-based\nsupervision to mitigate the noise introduced by pseudo labels. Finally,\nextensive experiments conducted across urban, off-road, and campus environments\ndemonstrate the effectiveness of our approach. The proposed automatic labeling\nmethod consistently achieves around 88% IoU across diverse datasets. Compared\nto existing self-supervised state-of-the-art methods, our multimodal\ntraversability estimation network yields consistently higher IoU, improving by\n1.6-3.5% on all evaluated datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5730\u5f62\u53ef\u7a7f\u8d8a\u6027\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u5347\u4e86\u53ef\u7a7f\u8d8a\u6027\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u4e0d\u53ef\u7a7f\u8d8a\u533a\u57df\u7684\u7279\u5f81\uff0c\u4e14\u591a\u6570\u4ec5\u5173\u6ce8\u5355\u4e00\u6a21\u6001\uff0c\u672a\u80fd\u5229\u7528\u591a\u6a21\u6001\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u6574\u5408\u8db3\u8ff9\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\uff0c\u751f\u6210\u53ef\u7a7f\u8d8a\u6027\u6807\u7b7e\uff1b\u8bad\u7ec3\u53cc\u6d41\u7f51\u7edc\u7ed3\u5408\u591a\u6a21\u6001\u5b66\u4e60\uff1b\u5f15\u5165\u7a00\u758fLiDAR\u76d1\u7763\u4ee5\u51cf\u5c11\u4f2a\u6807\u7b7e\u566a\u58f0\u3002", "result": "\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u5728\u591a\u6837\u6570\u636e\u96c6\u4e2d\u8fbe\u5230\u7ea688% IoU\uff1b\u591a\u6a21\u6001\u7f51\u7edc\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53471.6-3.5% IoU\u3002", "conclusion": "\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\u663e\u8457\u63d0\u5347\u53ef\u7a7f\u8d8a\u6027\u4f30\u8ba1\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73af\u5883\u3002"}}
{"id": "2508.18268", "pdf": "https://arxiv.org/pdf/2508.18268", "abs": "https://arxiv.org/abs/2508.18268", "authors": ["Haoyuan Deng", "Wenkai Guo", "Qianzhun Wang", "Zhenyu Wu", "Ziwei Wang"], "title": "SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Project website is at: https://denghaoyuan123.github.io/SafeBimanip/", "summary": "Bimanual manipulation has been widely applied in household services and\nmanufacturing, which enables the complex task completion with coordination\nrequirements. Recent diffusion-based policy learning approaches have achieved\npromising performance in modeling action distributions for bimanual\nmanipulation. However, they ignored the physical safety constraints of bimanual\nmanipulation, which leads to the dangerous behaviors with damage to robots and\nobjects. To this end, we propose a test-time trajectory optimization framework\nnamed SafeBimanual for any pre-trained diffusion-based bimanual manipulation\npolicies, which imposes the safety constraints on bimanual actions to avoid\ndangerous robot behaviors with improved success rate. Specifically, we design\ndiverse cost functions for safety constraints in different dual-arm cooperation\npatterns including avoidance of tearing objects and collision between arms and\nobjects, which optimizes the manipulator trajectories with guided sampling of\ndiffusion denoising process. Moreover, we employ a vision-language model (VLM)\nto schedule the cost functions by specifying keypoints and corresponding\npairwise relationship, so that the optimal safety constraint is dynamically\ngenerated in the entire bimanual manipulation process. SafeBimanual\ndemonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase\nin success rate and a 18.8% reduction in unsafe interactions over\nstate-of-the-art diffusion-based methods. Extensive experiments on 4 real-world\ntasks further verify its practical value by improving the success rate by\n32.5%.", "AI": {"tldr": "SafeBimanual \u662f\u4e00\u79cd\u9488\u5bf9\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u6d4b\u8bd5\u65f6\u95f4\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5b89\u5168\u7ea6\u675f\u548c\u52a8\u6001\u6210\u672c\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u65b9\u6cd5\u5ffd\u89c6\u4e86\u7269\u7406\u5b89\u5168\u7ea6\u675f\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u4f18\u5316\u8f68\u8ff9\u4ee5\u786e\u4fdd\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u7684\u6210\u672c\u51fd\u6570\u6765\u7ea6\u675f\u4e0d\u540c\u53cc\u673a\u68b0\u81c2\u534f\u4f5c\u6a21\u5f0f\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u751f\u6210\u6700\u4f18\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u4e8613.7%\uff0c\u4e0d\u5b89\u5168\u4ea4\u4e92\u51cf\u5c11\u4e8618.8%\uff1b\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u4e8632.5%\u3002", "conclusion": "SafeBimanual \u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u53cc\u673a\u68b0\u81c2\u64cd\u4f5c\u7684\u6210\u529f\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18269", "pdf": "https://arxiv.org/pdf/2508.18269", "abs": "https://arxiv.org/abs/2508.18269", "authors": ["Zhide Zhong", "Haodong Yan", "Junfeng Li", "Xiangchen Liu", "Xin Gong", "Wenxuan Song", "Jiayi Chen", "Haoang Li"], "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought", "categories": ["cs.RO"], "comment": null, "summary": "Many Vision-Language-Action (VLA) models rely on an internal world model\ntrained via next-frame prediction. This approach, however, struggles with\nphysical reasoning as it entangles static appearance with dynamic motion, often\nresulting in implausible visual forecasts and inefficient policy learning. To\naddress these limitations, we introduce the Visual Chain of Thought (Visual\nCoT): a pre-training framework that encourages a model to reason about how a\nscene evolves before predicting what it will look like. We instantiate this\nprinciple in FlowVLA, which predicts a future frame ($v_{t+1}$) only after\ngenerating an intermediate optical flow representation ($f_t$) that encodes\nmotion dynamics. This ``$v_t \\rightarrow f_t \\rightarrow v_{t+1}$'' reasoning\nprocess is implemented within a single autoregressive Transformer, guiding the\nmodel to learn disentangled dynamics. As a result, FlowVLA produces coherent\nvisual predictions and facilitates more efficient policy learning. Experiments\non challenging robotics manipulation benchmarks demonstrate state-of-the-art\nperformance with substantially improved sample efficiency, pointing toward a\nmore principled foundation for world modeling. Project page:\nhttps://irpn-lab.github.io/FlowVLA/", "AI": {"tldr": "FlowVLA\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u94fe\u5f0f\u601d\u7ef4\uff08Visual CoT\uff09\uff0c\u5728\u9884\u6d4b\u672a\u6765\u5e27\u4e4b\u524d\u5148\u751f\u6210\u4e2d\u95f4\u5149\u6d41\u8868\u793a\uff0c\u4ece\u800c\u89e3\u8026\u9759\u6001\u5916\u89c2\u4e0e\u52a8\u6001\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u9884\u6d4b\u548c\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e0b\u4e00\u5e27\u9884\u6d4b\u7684VLA\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u5c06\u9759\u6001\u5916\u89c2\u4e0e\u52a8\u6001\u8fd0\u52a8\u6df7\u4e3a\u4e00\u8c08\uff0c\u5bfc\u81f4\u89c6\u89c9\u9884\u6d4b\u4e0d\u5408\u7406\u4e14\u7b56\u7565\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u3002", "method": "FlowVLA\u91c7\u7528Visual CoT\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u5f53\u524d\u5e27\u2192\u5149\u6d41\u2192\u672a\u6765\u5e27\u201d\u7684\u94fe\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u5355\u81ea\u56de\u5f52Transformer\u4e2d\u751f\u6210\u89e3\u8026\u7684\u52a8\u6001\u8868\u793a\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowVLA\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "FlowVLA\u901a\u8fc7\u89e3\u8026\u52a8\u6001\u4e0e\u9759\u6001\u4fe1\u606f\uff0c\u4e3a\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u5408\u7406\u7684\u57fa\u7840\u3002"}}
{"id": "2508.14422", "pdf": "https://arxiv.org/pdf/2508.14422", "abs": "https://arxiv.org/abs/2508.14422", "authors": ["Tianhua Gao", "Masashi Izumita", "Kohji Tomita", "Akiya Kamimura"], "title": "Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control with Almost Global Exponential Convergence on SO(3)", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper introduces a lightweight and interpretable online learning\napproach called Dimension-Decomposed Learning (DiD-L) for disturbance\nidentification in quadrotor geometric attitude control. As a module instance of\nDiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to\naddress underlying underfitting problems, the high-dimensional mapping for\nonline identification is axially ``sliced\" into multiple low-dimensional\nsubmappings (slices). In this way, the complex high-dimensional problem is\ndecomposed into a set of simple low-dimensional subtasks addressed by shallow\nneural networks and adaptive laws. These neural networks and adaptive laws are\nupdated online via Lyapunov-based adaptation without the persistent excitation\n(PE) condition. To enhance the interpretability of the proposed approach, we\nprove that the state solution of the rotational error dynamics exponentially\nconverges into an arbitrarily small ball within an almost global attraction\ndomain, despite time-varying disturbances and inertia uncertainties. This\nresult is novel as it demonstrates exponential convergence without requiring\npre-training for unseen disturbances and specific knowledge of the model. To\nour knowledge in the quadrotor control field, DiD-L is the first online\nlearning approach that is lightweight enough to run in real-time at 400 Hz on\nmicrocontroller units (MCUs) such as STM32, and has been validated through\nreal-world experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5DiD-L\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u51e0\u4f55\u59ff\u6001\u63a7\u5236\u4e2d\u7684\u6270\u52a8\u8bc6\u522b\uff0c\u5305\u542b\u6a21\u5757SANM\u3002\u901a\u8fc7\u5206\u89e3\u9ad8\u7ef4\u6620\u5c04\u4e3a\u4f4e\u7ef4\u5b50\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u6b20\u62df\u5408\u95ee\u9898\uff0c\u5e76\u5728\u4e0d\u6ee1\u8db3PE\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6307\u6570\u6536\u655b\u3002", "motivation": "\u9488\u5bf9\u56db\u65cb\u7ffc\u63a7\u5236\u4e2d\u7684\u6270\u52a8\u8bc6\u522b\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u5b9e\u65f6\u6027\u548c\u8f7b\u91cf\u5316\u4e0a\u6ee1\u8db3\u8981\u6c42\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86DiD-L\u6846\u67b6\u53ca\u5176\u6a21\u5757SANM\uff0c\u5c06\u9ad8\u7ef4\u6620\u5c04\u5206\u89e3\u4e3a\u4f4e\u7ef4\u5b50\u4efb\u52a1\uff0c\u5229\u7528\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u5f8b\u5728\u7ebf\u66f4\u65b0\u3002", "result": "\u5728\u4e0d\u6ee1\u8db3PE\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u72b6\u6001\u89e3\u7684\u6307\u6570\u6536\u655b\uff0c\u4e14\u65b9\u6cd5\u8db3\u591f\u8f7b\u91cf\u5316\uff0c\u53ef\u5728MCU\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "DiD-L\u662f\u9996\u4e2a\u5728\u56db\u65cb\u7ffc\u63a7\u5236\u9886\u57df\u540c\u65f6\u6ee1\u8db3\u8f7b\u91cf\u5316\u3001\u5b9e\u65f6\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2508.16609", "pdf": "https://arxiv.org/pdf/2508.16609", "abs": "https://arxiv.org/abs/2508.16609", "authors": ["Katie Seaborn"], "title": "Social Identity in Human-Agent Interaction: A Primer", "categories": ["physics.soc-ph", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": "28 pages", "summary": "Social identity theory (SIT) and social categorization theory (SCT) are two\nfacets of the social identity approach (SIA) to understanding social phenomena.\nSIT and SCT are models that describe and explain how people interact with one\nanother socially, connecting the individual to the group through an\nunderstanding of underlying psychological mechanisms and intergroup behaviour.\nSIT, originally developed in the 1970s, and SCT, a later, more general\noffshoot, have been broadly applied to a range of social phenomena among\npeople. The rise of increasingly social machines embedded in daily life has\nspurned efforts on understanding whether and how artificial agents can and do\nparticipate in SIA activities. As agents like social robots and chatbots\npowered by sophisticated large language models (LLMs) advance, understanding\nthe real and potential roles of these technologies as social entities is\ncrucial. Here, I provide a primer on SIA and extrapolate, through case studies\nand imagined examples, how SIT and SCT can apply to artificial social agents. I\nemphasize that not all human models and sub-theories will apply. I further\nargue that, given the emerging competence of these machines and our tendency to\nbe taken in by them, we experts may need to don the hat of the uncanny killjoy,\nfor our own good.", "AI": {"tldr": "\u6458\u8981\u7efc\u8ff0\u4e86\u793e\u4f1a\u8eab\u4efd\u7406\u8bba\uff08SIT\uff09\u548c\u793e\u4f1a\u5206\u7c7b\u7406\u8bba\uff08SCT\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u7406\u8bba\u5982\u4f55\u5e94\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u4ee3\u7406\u3002\u4f5c\u8005\u8ba4\u4e3a\u5e76\u975e\u6240\u6709\u4eba\u7c7b\u6a21\u578b\u90fd\u9002\u7528\uff0c\u5e76\u547c\u5401\u4e13\u5bb6\u8b66\u60d5\u6280\u672f\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b49\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5982\u4f55\u53c2\u4e0e\u793e\u4f1a\u8eab\u4efd\u6d3b\u52a8\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u60f3\u8c61\u793a\u4f8b\uff0c\u5c06SIT\u548cSCT\u7406\u8bba\u5e94\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u4ee3\u7406\u3002", "result": "\u6307\u51fa\u5e76\u975e\u6240\u6709\u4eba\u9645\u6a21\u578b\u90fd\u9002\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\uff0c\u4e14\u4e13\u5bb6\u9700\u8b66\u60d5\u6280\u672f\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u4f5c\u4e3a\u793e\u4ea4\u5b9e\u4f53\u7684\u89d2\u8272\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4e13\u5bb6\u5e94\u4fdd\u6301\u6279\u5224\u6001\u5ea6\u4ee5\u786e\u4fdd\u6280\u672f\u53d1\u5c55\u7b26\u5408\u4eba\u7c7b\u5229\u76ca\u3002"}}
{"id": "2508.16622", "pdf": "https://arxiv.org/pdf/2508.16622", "abs": "https://arxiv.org/abs/2508.16622", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "title": "Observations of atypical users from a pilot deployment of a public-space social robot in a church", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025", "summary": "Though a goal of HRI is the natural integration of social robots into\neveryday public spaces, real-world studies still occur mostly within controlled\nenvironments with predetermined participants. True public spaces present an\nenvironment which is largely unconstrained and unpredictable, frequented by a\ndiverse range of people whose goals can often conflict with those of the robot.\nWhen combined with the general unfamiliarity most people have with social\nrobots, this leads to unexpected human-robot interactions in these public\nspaces that are rarely discussed or detected in other contexts. In this paper,\nwe describe atypical users we observed interacting with our robot, and those\nwho did not, during a three-day pilot deployment within a large working church\nand visitor attraction. We then discuss theoretical future advances in the\nfield that could address these challenges, as well as immediate practical\nmitigations and strategies to help improve public space human-robot\ninteractions in the present. This work contributes empirical insights into the\ndynamics of human-robot interaction in public environments and offers\nactionable guidance for more effective future deployments for social robot\ndesigners.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u771f\u5b9e\u516c\u5171\u7a7a\u95f4\u4e2d\u793e\u4ea4\u673a\u5668\u4eba\u4e0e\u7528\u6237\u7684\u975e\u5178\u578b\u4e92\u52a8\uff0c\u63d0\u51fa\u4e86\u6539\u8fdbHRI\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u89e3\u51b3\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u516c\u5171\u7a7a\u95f4\u4e2d\u4e0e\u591a\u6837\u6027\u7528\u6237\u4e92\u52a8\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u53ca\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e09\u5929\u5728\u6559\u5802\u548c\u6e38\u5ba2\u666f\u70b9\u7684\u8bd5\u70b9\u90e8\u7f72\uff0c\u89c2\u5bdf\u975e\u5178\u578b\u7528\u6237\u4e0e\u673a\u5668\u4eba\u7684\u4e92\u52a8\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u4e86\u8bb8\u591a\u672a\u5728\u5176\u4ed6\u60c5\u5883\u4e2d\u8ba8\u8bba\u7684\u610f\u5916\u4e92\u52a8\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6539\u8fdb\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u89c1\u89e3\u548c\u53ef\u884c\u6027\u5efa\u8bae\u3002"}}
{"id": "2508.17012", "pdf": "https://arxiv.org/pdf/2508.17012", "abs": "https://arxiv.org/abs/2508.17012", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "Fiducial Marker Splatting for High-Fidelity Robotics Simulations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "High-fidelity 3D simulation is critical for training mobile robots, but its\ntraditional reliance on mesh-based representations often struggle in complex\nenvironments, such as densely packed greenhouses featuring occlusions and\nrepetitive structures. Recent neural rendering methods, like Gaussian Splatting\n(GS), achieve remarkable visual realism but lack flexibility to incorporate\nfiducial markers, which are essential for robotic localization and control. We\npropose a hybrid framework that combines the photorealism of GS with structured\nmarker representations. Our core contribution is a novel algorithm for\nefficiently generating GS-based fiducial markers (e.g., AprilTags) within\ncluttered scenes. Experiments show that our approach outperforms traditional\nimage-fitting techniques in both efficiency and pose-estimation accuracy. We\nfurther demonstrate the framework's potential in a greenhouse simulation. This\nagricultural setting serves as a challenging testbed, as its combination of\ndense foliage, similar-looking elements, and occlusions pushes the limits of\nperception, thereby highlighting the framework's value for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u548c\u7ed3\u6784\u5316\u6807\u8bb0\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f3D\u4eff\u771f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u5982\u6e29\u5ba4\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7f51\u683c\u7684\u4eff\u771f\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u867d\u771f\u5b9e\u4f46\u7f3a\u4e4f\u6807\u8bb0\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u5b9a\u4f4d\u9700\u6c42\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6cfc\u6e85\u4e0e\u7ed3\u6784\u5316\u6807\u8bb0\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u751f\u6210\u9ad8\u65af\u6cfc\u6e85\u6807\u8bb0\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u56fe\u50cf\u62df\u5408\u6280\u672f\uff0c\u5e76\u5728\u6e29\u5ba4\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u519c\u4e1a\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.17044", "pdf": "https://arxiv.org/pdf/2508.17044", "abs": "https://arxiv.org/abs/2508.17044", "authors": ["Dmitry Yudin"], "title": "M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": "29 pages, 3 figures, 13 tables. Preprint of the accepted article in\n  Optical Memory and Neural Network Journal", "summary": "3D mapping in dynamic environments poses a challenge for modern researchers\nin robotics and autonomous transportation. There are no universal\nrepresentations for dynamic 3D scenes that incorporate multimodal data such as\nimages, point clouds, and text. This article takes a step toward solving this\nproblem. It proposes a taxonomy of methods for constructing multimodal 3D maps,\nclassifying contemporary approaches based on scene types and representations,\nlearning methods, and practical applications. Using this taxonomy, a brief\nstructured analysis of recent methods is provided. The article also describes\nan original modular method called M3DMap, designed for object-aware\nconstruction of multimodal 3D maps for both static and dynamic scenes. It\nconsists of several interconnected components: a neural multimodal object\nsegmentation and tracking module; an odometry estimation module, including\ntrainable algorithms; a module for 3D map construction and updating with\nvarious implementations depending on the desired scene representation; and a\nmultimodal data retrieval module. The article highlights original\nimplementations of these modules and their advantages in solving various\npractical tasks, from 3D object grounding to mobile manipulation. Additionally,\nit presents theoretical propositions demonstrating the positive effect of using\nmultimodal data and modern foundational models in 3D mapping methods. Details\nof the taxonomy and method implementation are available at\nhttps://yuddim.github.io/M3DMap.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6784\u5efa\u591a\u6a21\u60013D\u5730\u56fe\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u65b9\u6cd5M3DMap\uff0c\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u76843D\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d3D\u5730\u56fe\u6784\u5efa\u7684\u6311\u6218\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\u7684\u901a\u7528\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\uff08taxonomy\uff09\u548c\u6a21\u5757\u5316\u65b9\u6cd5M3DMap\uff0c\u5305\u542b\u76ee\u6807\u5206\u5272\u4e0e\u8ddf\u8e2a\u3001\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u3001\u5730\u56fe\u6784\u5efa\u4e0e\u66f4\u65b0\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u68c0\u7d22\u6a21\u5757\u3002", "result": "\u5c55\u793a\u4e86M3DMap\u5728\u591a\u79cd\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u6570\u636e\u548c\u57fa\u7840\u6a21\u578b\u5bf93D\u5730\u56fe\u6784\u5efa\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e0b\u591a\u6a21\u60013D\u5730\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2508.17054", "pdf": "https://arxiv.org/pdf/2508.17054", "abs": "https://arxiv.org/abs/2508.17054", "authors": ["Qingwen Zhang", "Xiaomeng Zhu", "Yushan Zhang", "Yixi Cai", "Olov Andersson", "Patric Jensfelt"], "title": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method", "categories": ["cs.CV", "cs.RO"], "comment": "17 pages (9 main pages + 8 supp materail), 11 figures, code at\n  https://github.com/Kin-Zhang/DeltaFlow", "summary": "Previous dominant methods for scene flow estimation focus mainly on input\nfrom two consecutive frames, neglecting valuable information in the temporal\ndomain. While recent trends shift towards multi-frame reasoning, they suffer\nfrom rapidly escalating computational costs as the number of frames grows. To\nleverage temporal information more efficiently, we propose DeltaFlow\n($\\Delta$Flow), a lightweight 3D framework that captures motion cues via a\n$\\Delta$ scheme, extracting temporal features with minimal computational cost,\nregardless of the number of frames. Additionally, scene flow estimation faces\nchallenges such as imbalanced object class distributions and motion\ninconsistency. To tackle these issues, we introduce a Category-Balanced Loss to\nenhance learning across underrepresented classes and an Instance Consistency\nLoss to enforce coherent object motion, improving flow accuracy. Extensive\nevaluations on the Argoverse 2 and Waymo datasets show that $\\Delta$Flow\nachieves state-of-the-art performance with up to 22% lower error and $2\\times$\nfaster inference compared to the next-best multi-frame supervised method, while\nalso demonstrating a strong cross-domain generalization ability. The code is\nopen-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model\nweights.", "AI": {"tldr": "DeltaFlow\uff08\u0394Flow\uff09\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u76843D\u6846\u67b6\uff0c\u901a\u8fc7\u0394\u65b9\u6848\u9ad8\u6548\u6355\u6349\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5f15\u5165\u7c7b\u522b\u5e73\u8861\u635f\u5931\u548c\u5b9e\u4f8b\u4e00\u81f4\u6027\u635f\u5931\u89e3\u51b3\u573a\u666f\u6d41\u4f30\u8ba1\u4e2d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u6027\u80fd\u548c\u6548\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u573a\u666f\u6d41\u4f30\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e24\u5e27\u8f93\u5165\uff0c\u5ffd\u7565\u65f6\u95f4\u57df\u4fe1\u606f\uff1b\u591a\u5e27\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u0394Flow\u6846\u67b6\uff0c\u5229\u7528\u0394\u65b9\u6848\u4f4e\u6210\u672c\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u7c7b\u522b\u5e73\u8861\u635f\u5931\u548c\u5b9e\u4f8b\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728Argoverse 2\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bef\u5dee\u964d\u4f4e22%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u4e14\u5177\u6709\u5f3a\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DeltaFlow\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u5e27\u8ba1\u7b97\u6210\u672c\u548c\u573a\u666f\u6d41\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u4e0e\u4e00\u81f4\u6027\u6311\u6218\u3002"}}
{"id": "2508.17173", "pdf": "https://arxiv.org/pdf/2508.17173", "abs": "https://arxiv.org/abs/2508.17173", "authors": ["Chao Ning", "Han Wang", "Longyan Li", "Yang Shi"], "title": "Collaborative-Online-Learning-Enabled Distributionally Robust Motion Control for Multi-Robot Systems", "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper develops a novel COllaborative-Online-Learning (COOL)-enabled\nmotion control framework for multi-robot systems to avoid collision amid\nrandomly moving obstacles whose motion distributions are partially observable\nthrough decentralized data streams. To address the notable challenge of data\nacquisition due to occlusion, a COOL approach based on the Dirichlet process\nmixture model is proposed to efficiently extract motion distribution\ninformation by exchanging among robots selected learning structures. By\nleveraging the fine-grained local-moment information learned through COOL, a\ndata-stream-driven ambiguity set for obstacle motion is constructed. We then\nintroduce a novel ambiguity set propagation method, which theoretically admits\nthe derivation of the ambiguity sets for obstacle positions over the entire\nprediction horizon by utilizing obstacle current positions and the ambiguity\nset for obstacle motion. Additionally, we develop a compression scheme with its\nsafety guarantee to automatically adjust the complexity and granularity of the\nambiguity set by aggregating basic ambiguity sets that are close in a measure\nspace, thereby striking an attractive trade-off between control performance and\ncomputation time. Then the probabilistic collision-free trajectories are\ngenerated through distributionally robust optimization problems. The\ndistributionally robust obstacle avoidance constraints based on the compressed\nambiguity set are equivalently reformulated by deriving separating hyperplanes\nthrough tractable semi-definite programming. Finally, we establish the\nprobabilistic collision avoidance guarantee and the long-term tracking\nperformance guarantee for the proposed framework. The numerical simulations are\nused to demonstrate the efficacy and superiority of the proposed approach\ncompared with state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534f\u4f5c\u5728\u7ebf\u5b66\u4e60\uff08COOL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u4ee5\u907f\u514d\u5728\u968f\u673a\u79fb\u52a8\u969c\u788d\u7269\u4e2d\u53d1\u751f\u78b0\u649e\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u4ea4\u6362\u548c\u4f18\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u65f6\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u968f\u673a\u79fb\u52a8\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u78b0\u649e\u907f\u514d\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u969c\u788d\u7269\u8fd0\u52a8\u5206\u5e03\u90e8\u5206\u53ef\u89c1\u4e14\u6570\u636e\u83b7\u53d6\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u8fc7\u7a0b\u6df7\u5408\u6a21\u578b\u7684COOL\u65b9\u6cd5\u63d0\u53d6\u8fd0\u52a8\u5206\u5e03\u4fe1\u606f\uff0c\u6784\u5efa\u6570\u636e\u6d41\u9a71\u52a8\u7684\u969c\u788d\u7269\u8fd0\u52a8\u6a21\u7cca\u96c6\uff0c\u5e76\u63d0\u51fa\u6a21\u7cca\u96c6\u4f20\u64ad\u65b9\u6cd5\u548c\u538b\u7f29\u65b9\u6848\u4ee5\u4f18\u5316\u63a7\u5236\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u78b0\u649e\u907f\u514d\u548c\u8ddf\u8e2a\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u548c\u53ef\u9760\u7684\u78b0\u649e\u907f\u514d\u3002"}}
{"id": "2508.17255", "pdf": "https://arxiv.org/pdf/2508.17255", "abs": "https://arxiv.org/abs/2508.17255", "authors": ["Yuzhi Lai", "Shenghai Yuan", "Peizheng Li", "Jun Lou", "Andreas Zell"], "title": "SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present SEER-VAR, a novel framework for egocentric vehicle-based augmented\nreality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches\n(CASB), and LLM-driven recommendation. Unlike existing systems that assume\nstatic or single-view settings, SEER-VAR dynamically separates cabin and road\nscenes via depth-guided vision-language grounding. Two SLAM branches track\negocentric motion in each context, while a GPT-based module generates\ncontext-aware overlays such as dashboard cues and hazard alerts. To support\nevaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring\nsynchronized egocentric views, 6DoF ground-truth poses, and AR annotations\nacross diverse driving scenarios. Experiments demonstrate that SEER-VAR\nachieves robust spatial alignment and perceptually coherent AR rendering across\nvaried environments. As one of the first to explore LLM-based AR recommendation\nin egocentric driving, we address the lack of comparable systems through\nstructured prompting and detailed user studies. Results show that SEER-VAR\nenhances perceived scene understanding, overlay relevance, and driver ease,\nproviding an effective foundation for future research in this direction. Code\nand dataset will be made open source.", "AI": {"tldr": "SEER-VAR\u662f\u4e00\u79cd\u7528\u4e8e\u8f66\u8f86AR\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u89e3\u3001\u4e0a\u4e0b\u6587\u611f\u77e5SLAM\u5206\u652f\u548cLLM\u9a71\u52a8\u7684\u63a8\u8350\uff0c\u63d0\u5347\u9a7e\u9a76\u573a\u666f\u7684AR\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684AR\u7cfb\u7edf\u5047\u8bbe\u9759\u6001\u6216\u5355\u89c6\u56fe\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u63d0\u51faSEER-VAR\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u52a8\u6001\u5206\u79bb\u9a7e\u9a76\u8231\u548c\u9053\u8def\u573a\u666f\uff0c\u4f7f\u7528\u4e24\u6761SLAM\u5206\u652f\u8ddf\u8e2a\u8fd0\u52a8\uff0c\u5e76\u5229\u7528GPT\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684AR\u53e0\u52a0\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEER-VAR\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u7a7a\u95f4\u5bf9\u9f50\u548c\u611f\u77e5\u4e00\u81f4\u7684AR\u6e32\u67d3\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "SEER-VAR\u4e3a\u672a\u6765\u57fa\u4e8eLLM\u7684AR\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.17427", "pdf": "https://arxiv.org/pdf/2508.17427", "abs": "https://arxiv.org/abs/2508.17427", "authors": ["Zhao Zheng", "Jingfan Fan", "Long Shao", "Hong Song", "Danni Ai", "Tianyu Fu", "Deqiang Xiao", "Yongtian Wang", "Jian Yang"], "title": "Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Point cloud registration based on correspondences computes the rigid\ntransformation that maximizes the number of inliers constrained within the\nnoise threshold. Current state-of-the-art (SOTA) methods employing spatial\ncompatibility graphs or branch-and-bound (BnB) search mainly focus on\nregistration under high outlier ratios. However, graph-based methods require at\nleast quadratic space and time complexity for graph construction, while\nmulti-stage BnB search methods often suffer from inaccuracy due to local optima\nbetween decomposed stages. This paper proposes a geometric maximum overlapping\nregistration framework via rotation-only BnB search. The rigid transformation\nis decomposed using Chasles' theorem into a translation along rotation axis and\na 2D rigid transformation. The optimal rotation axis and angle are searched via\nBnB, with residual parameters formulated as range maximum query (RMQ) problems.\nFirstly, the top-k candidate rotation axes are searched within a hemisphere\nparameterized by cube mapping, and the translation along each axis is estimated\nthrough interval stabbing of the correspondences projected onto that axis.\nSecondly, the 2D registration is relaxed to 1D rotation angle search with 2D\nRMQ of geometric overlapping for axis-aligned rectangles, which is solved\ndeterministically in polynomial time using sweep line algorithm with segment\ntree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets\ndemonstrate superior accuracy and efficiency over SOTA methods, while the time\ncomplexity is polynomial and the space complexity increases linearly with the\nnumber of points, even in the worst case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c-only BnB\u641c\u7d22\u7684\u51e0\u4f55\u6700\u5927\u91cd\u53e0\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u521a\u6027\u53d8\u6362\u5e76\u5229\u7528\u533a\u95f4\u67e5\u8be2\u548c\u7ebf\u6bb5\u6811\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u79bb\u7fa4\u6bd4\u60c5\u51b5\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u6216\u56e0\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u800c\u96be\u4ee5\u5b9e\u7528\u3002", "method": "\u4f7f\u7528Chasles\u5b9a\u7406\u5206\u89e3\u521a\u6027\u53d8\u6362\uff0c\u901a\u8fc7BnB\u641c\u7d22\u6700\u4f18\u65cb\u8f6c\u8f74\u548c\u89d2\u5ea6\uff0c\u5e76\u5c06\u6b8b\u4f59\u53c2\u6570\u4f5c\u4e3a\u533a\u95f4\u67e5\u8be2\u95ee\u9898\u89e3\u51b3\u3002", "result": "\u57283DMatch\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8eSOTA\uff0c\u4e14\u590d\u6742\u5ea6\u66f4\u4f4e\u3002"}}
{"id": "2508.17468", "pdf": "https://arxiv.org/pdf/2508.17468", "abs": "https://arxiv.org/abs/2508.17468", "authors": ["Pedro Antonio Rabelo Saraiva", "Enzo Ferreira de Souza", "Joao Manoel Herrera Pinheiro", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "A Synthetic Dataset for Manometry Recognition in Robotic Applications", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This work addresses the challenges of data scarcity and high acquisition\ncosts for training robust object detection models in complex industrial\nenvironments, such as offshore oil platforms. The practical and economic\nbarriers to collecting real-world data in these hazardous settings often hamper\nthe development of autonomous inspection systems. To overcome this, in this\nwork we propose and validate a hybrid data synthesis pipeline that combines\nprocedural rendering with AI-driven video generation. Our methodology leverages\nBlenderProc to create photorealistic images with precise annotations and\ncontrolled domain randomization, and integrates NVIDIA's Cosmos-Predict2\nworld-foundation model to synthesize physically plausible video sequences with\ntemporal diversity, capturing rare viewpoints and adverse conditions. We\ndemonstrate that a YOLO-based detection network trained on a composite dataset,\nblending real images with our synthetic data, achieves superior performance\ncompared to models trained exclusively on real-world data. Notably, a 1:1\nmixture of real and synthetic data yielded the highest accuracy, surpassing the\nreal-only baseline. These findings highlight the viability of a synthetic-first\napproach as an efficient, cost-effective, and safe alternative for developing\nreliable perception systems in safety-critical and resource-constrained\nindustrial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a0b\u5e8f\u6e32\u67d3\u4e0eAI\u9a71\u52a8\u89c6\u9891\u751f\u6210\u7684\u6df7\u5408\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5de5\u4e1a\u73af\u5883\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\uff08\u5982\u6d77\u4e0a\u77f3\u6cb9\u5e73\u53f0\uff09\u4e2d\u6536\u96c6\u771f\u5b9e\u6570\u636e\u7684\u56f0\u96be\u548c\u6210\u672c\u9ad8\u6602\u95ee\u9898\uff0c\u63a8\u52a8\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528BlenderProc\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u7ed3\u5408NVIDIA\u7684Cosmos-Predict2\u6a21\u578b\u5408\u6210\u7269\u7406\u5408\u7406\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u901a\u8fc7\u57df\u968f\u673a\u5316\u548c\u6df7\u5408\u771f\u5b9e\u6570\u636e\u8bad\u7ec3YOLO\u6a21\u578b\u3002", "result": "\u6df7\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u7684\u8bad\u7ec3\u663e\u8457\u4f18\u4e8e\u4ec5\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5c24\u5176\u662f1:1\u6bd4\u4f8b\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4f18\u5148\u65b9\u6cd5\u4e3a\u5b89\u5168\u5173\u952e\u4e14\u8d44\u6e90\u6709\u9650\u7684\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17481", "pdf": "https://arxiv.org/pdf/2508.17481", "abs": "https://arxiv.org/abs/2508.17481", "authors": ["Priyanka Prakash Surve", "Asaf Shabtai", "Yuval Elovici"], "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem", "categories": ["cs.CR", "cs.RO"], "comment": null, "summary": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5b89\u5168\u6a21\u578b\uff0c\u6574\u5408\u4e8639\u79cd\u653b\u51fb\u548c35\u79cd\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u8bc4\u4f30\u4e86\u4e09\u79cd\u771f\u5b9e\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u72ec\u7279\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5c40\u9650\u4e8e\u7279\u5b9a\u5a01\u80c1\uff0c\u7f3a\u4e4f\u5bf9\u653b\u51fb\u5728\u7cfb\u7edf\u4e2d\u8fde\u9501\u6548\u5e94\u7684\u5168\u9762\u5206\u6790\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u7cfb\u7edf\u5316\uff08SoK\uff09\u6784\u5efa\u4e86\u4e03\u5c42\u5b89\u5168\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e8639x35\u7684\u653b\u51fb-\u9632\u5fa1\u77e9\u9635\uff0c\u5e76\u91c7\u7528\u8499\u7279\u5361\u6d1b\u5206\u6790\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u673a\u5668\u4eba\uff08Pepper\u3001G1 EDU\u3001Digit\uff09\uff0c\u5b89\u5168\u6210\u719f\u5ea6\u5f97\u5206\u8303\u56f4\u4e3a39.9%\u81f379.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u548c\u6295\u8d44\u4f18\u5148\u7ea7\u8bbe\u5b9a\u3002"}}
{"id": "2508.17921", "pdf": "https://arxiv.org/pdf/2508.17921", "abs": "https://arxiv.org/abs/2508.17921", "authors": ["Diptabrata Paul", "Nikola Milosevic", "Nico Scherf", "Frank Cichos"], "title": "Physical Embodiment Enables Information Processing Beyond Explicit Sensing in Active Matter", "categories": ["cond-mat.soft", "cs.RO"], "comment": null, "summary": "Living microorganisms have evolved dedicated sensory machinery to detect\nenvironmental perturbations, processing these signals through biochemical\nnetworks to guide behavior. Replicating such capabilities in synthetic active\nmatter remains a fundamental challenge. Here, we demonstrate that synthetic\nactive particles can adapt to hidden hydrodynamic perturbations through\nphysical embodiment alone, without explicit sensing mechanisms. Using\nreinforcement learning to control self-thermophoretic particles, we show that\nthey learn navigation strategies to counteract unobserved flow fields by\nexploiting information encoded in their physical dynamics. Remarkably,\nparticles successfully navigate perturbations that are not included in their\nstate inputs, revealing that embodied dynamics can serve as an implicit sensing\nmechanism. This discovery establishes physical embodiment as a computational\nresource for information processing in active matter, with implications for\nautonomous microrobotic systems and bio-inspired computation.", "AI": {"tldr": "\u5408\u6210\u6d3b\u6027\u7c92\u5b50\u901a\u8fc7\u7269\u7406\u4f53\u73b0\uff08\u65e0\u9700\u660e\u786e\u611f\u77e5\u673a\u5236\uff09\u9002\u5e94\u9690\u85cf\u7684\u6d41\u4f53\u52a8\u529b\u5b66\u6270\u52a8\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\uff0c\u63ed\u793a\u7269\u7406\u52a8\u6001\u53ef\u4f5c\u4e3a\u9690\u542b\u611f\u77e5\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5408\u6210\u6d3b\u6027\u7269\u8d28\u4e2d\u590d\u5236\u751f\u7269\u4f53\u7684\u73af\u5883\u611f\u77e5\u4e0e\u9002\u5e94\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u81ea\u4e3b\u5fae\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4fe1\u606f\u5904\u7406\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u81ea\u70ed\u6cf3\u7c92\u5b50\uff0c\u8bad\u7ec3\u5b83\u4eec\u5728\u672a\u89c2\u6d4b\u5230\u7684\u6d41\u573a\u4e2d\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u7c92\u5b50\u80fd\u591f\u6210\u529f\u5e94\u5bf9\u672a\u5305\u542b\u5728\u8f93\u5165\u72b6\u6001\u4e2d\u7684\u6270\u52a8\uff0c\u9a8c\u8bc1\u7269\u7406\u52a8\u6001\u53ef\u4f5c\u4e3a\u9690\u542b\u611f\u77e5\u673a\u5236\u3002", "conclusion": "\u7269\u7406\u4f53\u73b0\u4e3a\u6d3b\u6027\u7269\u8d28\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u5904\u7406\u8d44\u6e90\uff0c\u5bf9\u81ea\u4e3b\u5fae\u673a\u5668\u4eba\u548c\u4eff\u751f\u8ba1\u7b97\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.17971", "pdf": "https://arxiv.org/pdf/2508.17971", "abs": "https://arxiv.org/abs/2508.17971", "authors": ["Pu Feng", "Size Wang", "Yuhong Cao", "Junkang Liang", "Rongye Shi", "Wenjun Wu"], "title": "Neural Algorithmic Reasoners informed Large Language Model for Multi-Agent Path Finding", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted by IJCNN 2025", "summary": "The development and application of large language models (LLM) have\ndemonstrated that foundational models can be utilized to solve a wide array of\ntasks. However, their performance in multi-agent path finding (MAPF) tasks has\nbeen less than satisfactory, with only a few studies exploring this area. MAPF\nis a complex problem requiring both planning and multi-agent coordination. To\nimprove the performance of LLM in MAPF tasks, we propose a novel framework,\nLLM-NAR, which leverages neural algorithmic reasoners (NAR) to inform LLM for\nMAPF. LLM-NAR consists of three key components: an LLM for MAPF, a pre-trained\ngraph neural network-based NAR, and a cross-attention mechanism. This is the\nfirst work to propose using a neural algorithmic reasoner to integrate GNNs\nwith the map information for MAPF, thereby guiding LLM to achieve superior\nperformance. LLM-NAR can be easily adapted to various LLM models. Both\nsimulation and real-world experiments demonstrate that our method significantly\noutperforms existing LLM-based approaches in solving MAPF problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u5668\uff08NAR\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u6846\u67b6LLM-NAR\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u4e00\u79cd\u7ed3\u5408\u89c4\u5212\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u8c03\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86LLM-NAR\u6846\u67b6\uff0c\u5305\u542bLLM\u3001\u56fe\u795e\u7ecf\u7f51\u7edcNAR\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9996\u6b21\u5c06GNN\u4e0e\u5730\u56fe\u4fe1\u606f\u7ed3\u5408\u7528\u4e8e\u6307\u5bfcLLM\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "LLM-NAR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7b97\u6cd5\u63a8\u7406\u5668\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u6613\u4e8e\u9002\u914d\u4e0d\u540cLLM\u6a21\u578b\u3002"}}
{"id": "2508.18136", "pdf": "https://arxiv.org/pdf/2508.18136", "abs": "https://arxiv.org/abs/2508.18136", "authors": ["Nico Klar", "Nizam Gifary", "Felix P. G. Ziegler", "Frank Sehnke", "Anton Kaifel", "Eric Price", "Aamir Ahmad"], "title": "BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": "18 pages, 1 figures, to appear in Proceedings of the 19th\n  International Conference on Intelligent Autonomous Systems (IAS-19), Genoa,\n  Italy, 2025", "summary": "The urgent need for renewable energy expansion, particularly wind power, is\nhindered by conflicts with wildlife conservation. To address this, we developed\nBirdRecorder, an advanced AI-based anti-collision system to protect endangered\nbirds, especially the red kite (Milvus milvus). Integrating robotics,\ntelemetry, and high-performance AI algorithms, BirdRecorder aims to detect,\ntrack, and classify avian species within a range of 800 m to minimize\nbird-turbine collisions.\n  BirdRecorder integrates advanced AI methods with optimized hardware and\nsoftware architectures to enable real-time image processing. Leveraging Single\nShot Detector (SSD) for detection, combined with specialized hardware\nacceleration and tracking algorithms, our system achieves high detection\nprecision while maintaining the speed necessary for real-time decision-making.\nBy combining these components, BirdRecorder outperforms existing approaches in\nboth accuracy and efficiency.\n  In this paper, we summarize results on field tests and performance of the\nBirdRecorder system. By bridging the gap between renewable energy expansion and\nwildlife conservation, BirdRecorder contributes to a more sustainable\ncoexistence of technology and nature.", "AI": {"tldr": "\u5f00\u53d1\u4e86AI\u9a71\u52a8\u7684\u9632\u78b0\u649e\u7cfb\u7edfBirdRecorder\uff0c\u4ee5\u51cf\u5c11\u98ce\u529b\u53d1\u7535\u5bf9\u6fd2\u5371\u9e1f\u7c7b\u7684\u5a01\u80c1\u3002", "motivation": "\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\uff08\u5c24\u5176\u662f\u98ce\u80fd\uff09\u6269\u5f20\u4e0e\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4fdd\u62a4\u6fd2\u5371\u9e1f\u7c7b\u5982\u7ea2\u9e22\u3002", "method": "\u7ed3\u5408\u673a\u5668\u4eba\u6280\u672f\u3001\u9065\u6d4b\u6280\u672f\u548c\u9ad8\u6027\u80fdAI\u7b97\u6cd5\uff0c\u91c7\u7528SSD\u68c0\u6d4b\u5668\u548c\u786c\u4ef6\u52a0\u901f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u56fe\u50cf\u5904\u7406\u4e0e\u9e1f\u7c7b\u8ffd\u8e2a\u3002", "result": "BirdRecorder\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u98ce\u673a\u4e0e\u9e1f\u7c7b\u7684\u78b0\u649e\u3002", "conclusion": "BirdRecorder\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u4e0e\u81ea\u7136\u4fdd\u62a4\u7684\u53ef\u6301\u7eed\u5171\u5b58\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
