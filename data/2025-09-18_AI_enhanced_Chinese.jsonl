{"id": "2509.13328", "pdf": "https://arxiv.org/pdf/2509.13328", "abs": "https://arxiv.org/abs/2509.13328", "authors": ["Danish Rizvi", "David Boyle"], "title": "Dual Actor DDPG for Airborne STAR-RIS Assisted Communications", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "This study departs from the prevailing assumption of independent Transmission\nand Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect\nReconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a\nnovel multi-user downlink communication system that leverages a UAV-mounted\nSTAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key\ncontributions include the joint optimization of UAV trajectory, active\nbeamforming vectors at the base station, and passive RIS TRCs to enhance\ncommunication efficiency, while considering UAV energy constraints. We design\nthe TRC as a combination of discrete and continuous actions, and propose a\nnovel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The\nalgorithm relies on two separate actor networks for high-dimensional hybrid\naction space. We also propose a novel harmonic mean index (HFI)-based reward\nfunction to ensure communication fairness amongst users. For comprehensive\nanalysis, we study the impact of RIS size on UAV aerodynamics showing that it\nincreases drag and energy demand. Simulation results demonstrate that the\nproposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based\nsolutions by 24% and 97%, respectively, in accumulated reward.\nThree-dimensional UAV trajectory optimization achieves 28% higher communication\nefficiency compared to two-dimensional and altitude optimization. The HFI based\nreward function provides 41% lower QoS denial rates as compared to other\nbenchmarks. The mobile Aerial-STAR system shows superior performance over fixed\ndeployed counterparts, with the coupled phase STAR-RIS outperforming dual\nTransmit/Reflect RIS and conventional RIS setups. These findings highlight the\npotential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG\napproach in optimizing their performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u642d\u8f7d\u7684STAR-RIS\uff08Aerial-STAR\uff09\u7684\u591a\u7528\u6237\u4e0b\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8026\u5408TRC\u76f8\u79fb\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u57fa\u7ad9\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u548c\u88ab\u52a8RIS TRC\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684DA-DDPG\u7b97\u6cd5\u548cHFI\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2dTRC\u901a\u5e38\u5047\u8bbe\u4e3a\u72ec\u7acb\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u8026\u5408TRC\u76f8\u79fb\u6a21\u578b\u5728\u65e0\u4eba\u673a\u642d\u8f7dSTAR-RIS\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u591a\u7528\u6237\u901a\u4fe1\u7cfb\u7edf\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u540c\u65f6\u8003\u8651\u65e0\u4eba\u673a\u80fd\u91cf\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86DA-DDPG\u7b97\u6cd5\uff0c\u4f7f\u7528\u53cc\u52a8\u4f5c\u7f51\u7edc\u5904\u7406\u9ad8\u7ef4\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u4e86HFI\u5956\u52b1\u51fd\u6570\u786e\u4fdd\u7528\u6237\u516c\u5e73\u6027\uff0c\u5e76\u8054\u5408\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u548c\u88ab\u52a8RIS TRC\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cDA-DDPG\u7b97\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u63d0\u534724%-97%\uff0c3D\u8f68\u8ff9\u4f18\u5316\u901a\u4fe1\u6548\u7387\u63d0\u534728%\uff0cHFI\u5956\u52b1\u51fd\u6570\u964d\u4f4eQoS\u62d2\u7edd\u738741%\uff0cAerial-STAR\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u56fa\u5b9a\u90e8\u7f72\u65b9\u6848\u3002", "conclusion": "Aerial-STAR\u7cfb\u7edf\u548cDA-DDPG\u7b97\u6cd5\u5728\u4f18\u5316\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.13559", "pdf": "https://arxiv.org/pdf/2509.13559", "abs": "https://arxiv.org/abs/2509.13559", "authors": ["Yuan Liu", "Linlong Wu", "Xuesong Cai", "M. R. Bhavani Shankar"], "title": "Environment Reconstruction in Multi-Bounce Channels with Array Partial Blockage", "categories": ["eess.SP"], "comment": "Presented in EUSIPCO2025", "summary": "Extremely-large antenna arrays (ELAA) are important in applications requiring\nhigh angular resolution. However, a prominent issue is the spatial\nnon-stationary (SNS) channels due to partial blockage to the ELAA. In this\npaper, we address the scatterer localization and subsequent environment\nreconstruction considering partially blocked SNS channels. Specifically, the\nSNS effects are parametrically modeled through spatial-varying amplitudes with\nsparsity. Based on the established signal model, the graph-based\ndictionary-aided multi-bounce space-alternating generalized\nexpectation-maximization (GM-SAGE) algorithm is applied to estimate the channel\nparameters and the channel sparsity is empirically detected along with\namplitude estimation. To validate the proposed approach, we generate\nmulti-bounce paths through ray tracing (RT) simulations, where the SNS channels\ncaused by partial blockage could be configured flexibly. The simulation results\ndemonstrate the robustness of the proposed approach in dealing with the SNS\nchannels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u90e8\u5206\u963b\u585e\u5f15\u8d77\u7684\u7a7a\u95f4\u975e\u5e73\u7a33\uff08SNS\uff09\u4fe1\u9053\u7684\u6563\u5c04\u4f53\u5b9a\u4f4d\u548c\u73af\u5883\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7GM-SAGE\u7b97\u6cd5\u4f30\u8ba1\u4fe1\u9053\u53c2\u6570\u5e76\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u6781\u5927\u578b\u5929\u7ebf\u9635\u5217\uff08ELAA\uff09\u5728\u9ad8\u89d2\u5ea6\u5206\u8fa8\u7387\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u90e8\u5206\u963b\u585e\u4f1a\u5bfc\u81f4SNS\u4fe1\u9053\u95ee\u9898\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u5b57\u5178\u8f85\u52a9\u591a\u5f39\u8df3\u7a7a\u95f4\u4ea4\u66ff\u5e7f\u4e49\u671f\u671b\u6700\u5927\u5316\uff08GM-SAGE\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u7a7a\u95f4\u53d8\u5e45\u548c\u7a00\u758f\u6027\u5efa\u6a21SNS\u6548\u5e94\u3002", "result": "\u901a\u8fc7\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406SNS\u4fe1\u9053\u65f6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u90e8\u5206\u963b\u585e\u5f15\u8d77\u7684SNS\u4fe1\u9053\u95ee\u9898\uff0c\u4e3aELAA\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2509.13592", "pdf": "https://arxiv.org/pdf/2509.13592", "abs": "https://arxiv.org/abs/2509.13592", "authors": ["Youval Klioui"], "title": "Fast Single-Snapshot Harmonic Recovery with 2D Sparse Arrays using BCCB Matrices", "categories": ["eess.SP"], "comment": null, "summary": "We introduce an efficient implementation of sparse recovery methods for the\nproblem of harmonic estimation with 2D sparse arrays using a single snapshot.\nBy imposing a uniformity constraint on the harmonic grids of the\nsubdictionaries used in the sparse recovery problem, in addition to a mild\nconstraint on the array topology that consists in having the elements lie on a\ngrid specified in half-wavelength units, we show that the Gram matrices that\nappear in these sparse recovery methods exhibit a block-circulant with\ncirculant blocks (BCCB) structure. The BCCB structure is then exploited to\nreduce the computational complexity of the matrix-vector products that appear\nin these methods through the use of 2D fast Fourier transforms (FFT) from\nO((L1L2)^2) down to O(L1L2 log(L1L2)) operations per iterations, where L1, L2\nare the lengths of the subdictionaries used for estimating the harmonics in the\nfirst and second dimension, respectively. We experimentally verify the proposed\nimplementation using the iterative shrinkage thresholding algorithm (ISTA), the\nfast iterative shrinkage-thresholding algorithm (FISTA), and the alternating\ndirection method of multipliers (ADMM) where we observe improvements", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u6062\u590d\u65b9\u6cd5\u5b9e\u73b0\uff0c\u7528\u4e8e\u901a\u8fc7\u5355\u6b21\u5feb\u7167\u8fdb\u884c\u4e8c\u7ef4\u7a00\u758f\u9635\u5217\u7684\u8c10\u6ce2\u4f30\u8ba1\u3002\u901a\u8fc7\u65bd\u52a0\u5747\u5300\u6027\u7ea6\u675f\u548c\u6e29\u548c\u7684\u9635\u5217\u62d3\u6251\u7ea6\u675f\uff0c\u5c55\u793a\u4e86\u7a00\u758f\u6062\u590d\u65b9\u6cd5\u4e2dGram\u77e9\u9635\u7684BCCB\u7ed3\u6784\uff0c\u5e76\u5229\u75282D FFT\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e8c\u7ef4\u7a00\u758f\u9635\u5217\u8c10\u6ce2\u4f30\u8ba1\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u5229\u7528\u77e9\u9635\u7ed3\u6784\u4f18\u5316\u52a0\u901f\u7a00\u758f\u6062\u590d\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u5728\u7a00\u758f\u6062\u590d\u95ee\u9898\u4e2d\u5bf9\u5b50\u5b57\u5178\u7684\u8c10\u6ce2\u7f51\u683c\u65bd\u52a0\u5747\u5300\u6027\u7ea6\u675f\uff0c\u5e76\u5229\u7528BCCB\u7ed3\u6784\uff0c\u901a\u8fc72D FFT\u5c06\u77e9\u9635\u64cd\u4f5c\u7684\u590d\u6742\u5ea6\u4eceO((L1L2)^2)\u964d\u81f3O(L1L2 log(L1L2))\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ISTA\u3001FISTA\u548cADMM\u7b97\u6cd5\u7684\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5229\u7528BCCB\u7ed3\u6784\u548cFFT\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7a00\u758f\u6062\u590d\u4e2d\u5229\u7528BCCB\u7ed3\u6784\u548cFFT\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u5927\u5e45\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u4e8c\u7ef4\u7a00\u758f\u9635\u5217\u7684\u8c10\u6ce2\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2509.13600", "pdf": "https://arxiv.org/pdf/2509.13600", "abs": "https://arxiv.org/abs/2509.13600", "authors": ["Argyris Kriezis", "Yu-Hsuan Chen", "Dennis Akos", "Sherman Lo", "Todd Walter"], "title": "GNSS Jamming and Spoofing Monitoring Using Low-Cost COTS Receivers", "categories": ["eess.SP"], "comment": "Submitted to ION NAVIGATION Journal", "summary": "The Global Navigation Satellite System (GNSS) is increasingly vulnerable to\nradio frequency interference (RFI), including jamming and spoofing, which\nthreaten the integrity of navigation and timing services. This paper presents a\nmethodology for detecting and classifying RFI events using low-cost commercial\noff-the-shelf (COTS) GNSS receivers. By combining carrier-to-noise ratio (C/N0)\nmeasurements with a calibrated received power metric, a two-dimensional\ndetection space is constructed to identify and distinguish nominal, jammed,\nspoofed, and blocked signal conditions. The method is validated through both\ncontrolled jamming tests in Norway and real-world deployments in Poland, and\nthe Southeast Mediterranean which have experienced such conditions. Results\ndemonstrate that COTS-based detection, when properly calibrated, offers a\nviable and effective approach for GNSS RFI monitoring.", "AI": {"tldr": "\u5229\u7528\u4f4e\u6210\u672c\u5546\u7528GNSS\u63a5\u6536\u673a\u68c0\u6d4b\u548c\u5206\u7c7b\u5c04\u9891\u5e72\u6270\uff08RFI\uff09\u4e8b\u4ef6\uff0c\u7ed3\u5408\u8f7d\u566a\u6bd4\u548c\u6821\u51c6\u63a5\u6536\u529f\u7387\uff0c\u6784\u5efa\u4e8c\u7ef4\u68c0\u6d4b\u7a7a\u95f4\uff0c\u6709\u6548\u533a\u5206\u5e72\u6270\u548c\u6b3a\u9a97\u4fe1\u53f7\u3002", "motivation": "\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff08GNSS\uff09\u6613\u53d7\u5c04\u9891\u5e72\u6270\uff08RFI\uff09\u5a01\u80c1\uff0c\u5f71\u54cd\u5bfc\u822a\u548c\u8ba1\u65f6\u670d\u52a1\u5b8c\u6574\u6027\uff0c\u9700\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8f7d\u566a\u6bd4\uff08C/N0\uff09\u548c\u6821\u51c6\u63a5\u6536\u529f\u7387\uff0c\u6784\u5efa\u4e8c\u7ef4\u68c0\u6d4b\u7a7a\u95f4\uff0c\u5206\u7c7b\u6b63\u5e38\u3001\u5e72\u6270\u3001\u6b3a\u9a97\u548c\u963b\u585e\u4fe1\u53f7\u3002", "result": "\u5728\u632a\u5a01\u3001\u6ce2\u5170\u548c\u4e1c\u5357\u5730\u4e2d\u6d77\u7684\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u5546\u7528\u63a5\u6536\u673a\u5728RFI\u76d1\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ecf\u8fc7\u6821\u51c6\u7684\u4f4e\u6210\u672c\u5546\u7528GNSS\u63a5\u6536\u673a\u53ef\u6709\u6548\u7528\u4e8eGNSS RFI\u76d1\u6d4b\u3002"}}
{"id": "2509.13336", "pdf": "https://arxiv.org/pdf/2509.13336", "abs": "https://arxiv.org/abs/2509.13336", "authors": ["Mehran Behjati", "Rosdiadee Nordin", "Nor Fadzilah Abdullah"], "title": "Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning", "categories": ["cs.RO", "cs.LG"], "comment": "Submitted to an IEEE Conference", "summary": "This paper presents a reinforcement learning (RL) based approach for path\nplanning of cellular connected unmanned aerial vehicles (UAVs) operating beyond\nvisual line of sight (BVLoS). The objective is to minimize travel distance\nwhile maximizing the quality of cellular link connectivity by considering real\nworld aerial coverage constraints and employing an empirical aerial channel\nmodel. The proposed solution employs RL techniques to train an agent, using the\nquality of communication links between the UAV and base stations (BSs) as the\nreward function. Simulation results demonstrate the effectiveness of the\nproposed method in training the agent and generating feasible UAV path plans.\nThe proposed approach addresses the challenges due to limitations in UAV\ncellular communications, highlighting the need for investigations and\nconsiderations in this area. The RL algorithm efficiently identifies optimal\npaths, ensuring maximum connectivity with ground BSs to ensure safe and\nreliable BVLoS flight operation. Moreover, the solution can be deployed as an\noffline path planning module that can be integrated into future ground control\nsystems (GCS) for UAV operations, enhancing their capabilities and safety. The\nmethod holds potential for complex long range UAV applications, advancing the\ntechnology in the field of cellular connected UAV path planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u98de\u884c\u8ddd\u79bb\u5e76\u6700\u5927\u5316\u8702\u7a9d\u8fde\u63a5\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728BVLoS\u98de\u884c\u4e2d\u8702\u7a9d\u901a\u4fe1\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u98de\u884c\u5b89\u5168\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u4ee5\u65e0\u4eba\u673a\u4e0e\u57fa\u7ad9\u7684\u901a\u4fe1\u94fe\u8def\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bad\u7ec3\u667a\u80fd\u4f53\u5e76\u751f\u6210\u53ef\u884c\u7684\u8def\u5f84\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6574\u5408\u5230\u672a\u6765\u7684\u5730\u9762\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u590d\u6742\u957f\u8ddd\u79bb\u65e0\u4eba\u673a\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.13745", "pdf": "https://arxiv.org/pdf/2509.13745", "abs": "https://arxiv.org/abs/2509.13745", "authors": ["Hiroki Kuroda", "Renato Luis Garrido Cavalcante", "Masahiro Yukawa"], "title": "Theoretical Validation of the Latent Optimally Partitioned-$\\ell_2/\\ell_1$ Penalty with Application to Angular Power Spectrum Estimation", "categories": ["eess.SP"], "comment": null, "summary": "This paper demonstrates that, in both theory and practice, the latent\noptimally partitioned (LOP)-$\\ell_2/\\ell_1$ penalty is effective for exploiting\nblock-sparsity without the knowledge of the concrete block structure. More\nprecisely, we first present a novel theoretical result showing that the\noptimized block partition in the LOP-$\\ell_2/\\ell_1$ penalty satisfy a\ncondition required for accurate recovery of block-sparse signals. Motivated by\nthis result, we present a new application of the LOP-$\\ell_2/\\ell_1$ penalty to\nestimation of angular power spectrum, which is block-sparse with unknown block\npartition, in MIMO communication systems. Numerical simulations show that the\nproposed use of block-sparsity with the LOP-$\\ell_2/\\ell_1$ penalty\nsignificantly improves the estimation accuracy of the angular power spectrum.", "AI": {"tldr": "LOP-$\u2113_2/\u2113_1$\u60e9\u7f5a\u65e0\u9700\u5df2\u77e5\u5757\u7ed3\u6784\u5373\u53ef\u6709\u6548\u5229\u7528\u5757\u7a00\u758f\u6027\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u4f18\u5316\u5206\u533a\u6ee1\u8db3\u4fe1\u53f7\u6062\u590d\u6761\u4ef6\uff0c\u5e94\u7528\u4e8eMIMO\u7cfb\u7edf\u4e2d\u7684\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u5757\u7ed3\u6784\u4e0b\u5757\u7a00\u758f\u4fe1\u53f7\u7684\u51c6\u786e\u6062\u590d\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5176\u5728MIMO\u7cfb\u7edf\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faLOP-$\u2113_2/\u2113_1$\u60e9\u7f5a\u4f18\u5316\u5757\u5206\u533a\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u5757\u7a00\u758f\u7684\u89d2\u529f\u7387\u8c31\u4f30\u8ba1\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u89d2\u529f\u7387\u8c31\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "LOP-$\u2113_2/\u2113_1$\u60e9\u7f5a\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u80fd\u6709\u6548\u5229\u7528\u5757\u7a00\u758f\u6027\uff0c\u9002\u7528\u4e8e\u672a\u77e5\u5757\u7ed3\u6784\u7684\u573a\u666f\u3002"}}
{"id": "2509.13342", "pdf": "https://arxiv.org/pdf/2509.13342", "abs": "https://arxiv.org/abs/2509.13342", "authors": ["Isaac Ronald Ward"], "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments", "categories": ["cs.RO", "cs.AI"], "comment": "This report is submitted as partial fulfilment of the requirements\n  for the Honours Programme of the Department of Computer Science and Software\n  Engineering, The University of Western Australia, 2019", "summary": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of", "AI": {"tldr": "\u6539\u8fdb\u73b0\u6709\u89c6\u89c9RGB\u56fe\u50cf\u7684\u673a\u5668\u4eba\u59ff\u6001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6269\u5c55\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u63d0\u5347\u3002", "motivation": "\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u89c6\u89c9\u4fe1\u606f\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u6269\u5c55\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff0c\u5e76\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u751f\u6210\u5e26\u6807\u7b7e\u7684\u6570\u636e\u96c6\u3002", "result": "\u5ba4\u5185\u573a\u666f\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u4e2d\u4f4d\u6570\u5206\u522b\u964d\u4f4e\u4e869.64%\uff08\u4f4d\u7f6e\uff09\u548c2.99%\uff08\u65cb\u8f6c\uff09\uff0c\u5e76\u5b9e\u73b0\u4e860.11\u7c73\u548c0.89\u5ea6\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u9c81\u68d2\u5bfc\u822a\u7b97\u6cd5\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5ba4\u5185\u573a\u666f\u3002"}}
{"id": "2509.13786", "pdf": "https://arxiv.org/pdf/2509.13786", "abs": "https://arxiv.org/abs/2509.13786", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization", "categories": ["eess.SP"], "comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026", "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. We extend Post-Training Quantization (PTQ)\nbaselines with Quantization-Aware Training (QAT), which incorporates\nlow-precision simulation during training for robustness at ultra-low bitwidths.\nOur study applies QAT/PTQ to a neural receiver architecture and evaluates\nacross 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS\nenvironments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit\nQAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT\nmodels are also shown to outperform PTQ models by up to 3 dB, and yield 8x\ncompression. These results demonstrate that QAT is a key enabler of\nlow-complexity and latency-constrained inference at the PHY layer, facilitating\nreal-time processing in 6G edge devices", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57286G\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u9ad8\u6548\u7269\u7406\u5c42\u5904\u7406\u3002", "motivation": "\u968f\u77406G\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u4e3a\u57fa\u7840\u7684\u795e\u7ecf\u63a5\u6536\u5668\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u7136\u800c\uff0c\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u90e8\u7f72\u9700\u8981\u9ad8\u6548\u7684\u91cf\u5316\u6280\u672f\u4ee5\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u80fd\u8017\u3002", "method": "\u8bba\u6587\u7ed3\u5408\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4f4e\u7cbe\u5ea6\u6a21\u62df\uff0c\u4ee5\u786e\u4fdd\u5728\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u57283GPP CDL-B/D\u4fe1\u9053\u73af\u5883\u4e0b\u8bc4\u4f30\u4e86QAT/PTQ\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c4\u4f4d\u548c8\u4f4dQAT\u6a21\u578b\u572810%\u76ee\u6807BLER\u4e0b\u6027\u80fd\u63a5\u8fd1FP32\u6a21\u578b\u3002QAT\u6a21\u578b\u6bd4PTQ\u6a21\u578b\u8868\u73b0\u9ad8\u51fa3 dB\uff0c\u5e76\u5b9e\u73b0\u4e868\u500d\u538b\u7f29\u3002", "conclusion": "QAT\u662f\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u3001\u4f4e\u5ef6\u8fdf6G\u8fb9\u7f18\u8bbe\u5907\u7269\u7406\u5c42\u5904\u7406\u7684\u5173\u952e\u6280\u672f\uff0c\u4e3a\u5b9e\u65f6\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.13349", "pdf": "https://arxiv.org/pdf/2509.13349", "abs": "https://arxiv.org/abs/2509.13349", "authors": ["Jed Guzelkabaagac", "Boris Petrovi\u0107"], "title": "Label-Efficient Grasp Joint Prediction with Point-JEPA", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "4 pages, 5 figures. Submitted to IROS 2025 Workshop", "summary": "We investigate whether 3D self-supervised pretraining with a Joint-Embedding\nPredictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle\nprediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained\nPoint-JEPA encoder, we train a lightweight multi-hypothesis head with\nwinner-takes-all and evaluate by top-logit selection. On DLR-Hand II with\nobject-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes\nand reaches parity with full supervision. These results suggest JEPA-style\npretraining is a practical approach for data-efficient grasp learning.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u57fa\u4e8ePoint-JEPA\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4f4e\u6807\u7b7e\u6570\u636e\u4e0b\u9ad8\u6548\u9884\u6d4b\u6293\u53d6\u5173\u8282\u89d2\u5ea6\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6548\u679c\u4e0e\u5168\u76d1\u7763\u76f8\u5f53\u3002", "motivation": "\u63a2\u7d22\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08Point-JEPA\uff09\u5728\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u4f4e\u6807\u7b7e\u6570\u636e\u4e0b\u3002", "method": "\u5229\u7528mesh\u751f\u6210\u7684\u70b9\u4e91\u4ee4\u724c\u5316\u548c\u9884\u8bad\u7ec3\u7684Point-JEPA\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u591a\u5047\u8bbe\u5934\u90e8\uff0c\u901a\u8fc7top-logit\u9009\u62e9\u8bc4\u4f30\u3002", "result": "\u5728DLR-Hand II\u6570\u636e\u96c6\u4e2d\uff0cPoint-JEPA\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u964d\u4f4e\u4e8626%\u7684RMSE\uff0c\u5e76\u4e0e\u5168\u76d1\u7763\u6548\u679c\u76f8\u5f53\u3002", "conclusion": "JEPA\u5f0f\u9884\u8bad\u7ec3\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u6293\u53d6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.13807", "pdf": "https://arxiv.org/pdf/2509.13807", "abs": "https://arxiv.org/abs/2509.13807", "authors": ["Ruiqi Kong", "He Chen"], "title": "Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing", "categories": ["eess.SP", "cs.NI"], "comment": "5 pages, 5 figures", "summary": "WiFi sensing faces a critical reliability challenge due to hardware-induced\nRF distortions, especially with modern, market-dominant WiFi cards supporting\n802.11ac/ax protocols. These cards employ sensitive automatic gain control and\nseparate RF chains, introducing complex and dynamic distortions that render\nexisting compensation methods ineffective. In this paper, we introduce Domino,\na new framework that transforms channel state information (CSI) into channel\nimpulse response (CIR) and leverages it for precise distortion compensation.\nDomino is built on the key insight that hardware-induced distortions impact all\nsignal paths uniformly, allowing the dominant static path to serve as a\nreliable reference for effective compensation through delay-domain processing.\nReal-world respiration monitoring experiments show that Domino achieves at\nleast 2x higher mean accuracy over existing methods, maintaining robust\nperformance with a median error below 0.24 bpm, even using a single antenna in\nboth direct line-of-sight and obstructed scenarios.", "AI": {"tldr": "Domino\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684WiFi\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u8f6c\u6362\u4e3a\u4fe1\u9053\u8109\u51b2\u54cd\u5e94\uff08CIR\uff09\uff0c\u6709\u6548\u8865\u507f\u786c\u4ef6\u5f15\u5165\u7684\u5c04\u9891\u5931\u771f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u547c\u5438\u76d1\u6d4b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709WiFi\u611f\u77e5\u7cfb\u7edf\u56e0802.11ac/ax\u534f\u8bae\u7684\u786c\u4ef6\u5c04\u9891\u5931\u771f\u95ee\u9898\u800c\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u8865\u507f\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u52a8\u6001\u5931\u771f\u3002", "method": "Domino\u5229\u7528\u4fe1\u53f7\u8def\u5f84\u7684\u5747\u5300\u6027\uff0c\u4ee5\u4e3b\u5bfc\u9759\u6001\u8def\u5f84\u4e3a\u53c2\u8003\uff0c\u901a\u8fc7\u5ef6\u8fdf\u57df\u5904\u7406\u7cbe\u786e\u8865\u507f\u5931\u771f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDomino\u5728\u547c\u5438\u76d1\u6d4b\u4e2d\u7684\u5e73\u5747\u7cbe\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u81f3\u5c11\u63d0\u9ad82\u500d\uff0c\u5355\u5929\u7ebf\u6761\u4ef6\u4e0b\u4e2d\u4f4d\u8bef\u5dee\u4f4e\u4e8e0.24 bpm\u3002", "conclusion": "Domino\u901a\u8fc7\u521b\u65b0\u7684\u8def\u5f84\u8865\u507f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86WiFi\u611f\u77e5\u7684\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.13378", "pdf": "https://arxiv.org/pdf/2509.13378", "abs": "https://arxiv.org/abs/2509.13378", "authors": ["Mattias Wingren", "S\u00f6ren Andersson", "Sara Rosenberg", "Malin Andtfolk", "Susanne H\u00e4gglund", "Prashani Jayasingha Arachchige", "Linda Nyholm"], "title": "Using role-play and Hierarchical Task Analysis for designing human-robot interaction", "categories": ["cs.RO"], "comment": "11 pages. This is a preprint version of the published paper in the\n  International Conference on Social Robotics:\n  https://link.springer.com/chapter/10.1007/978-981-96-3522-1_28", "summary": "We present the use of two methods we believe warrant more use than they\ncurrently have in the field of human-robot interaction: role-play and\nHierarchical Task Analysis. Some of its potential is showcased through our use\nof them in an ongoing research project which entails developing a robot\napplication meant to assist at a community pharmacy. The two methods have\nprovided us with several advantages. The role-playing provided a controlled and\nadjustable environment for understanding the customers' needs where pharmacists\ncould act as models for the robot's behavior; and the Hierarchical Task\nAnalysis ensured the behavior displayed was modelled correctly and aided\ndevelopment through facilitating co-design. Future research could focus on\ndeveloping task analysis methods especially suited for social robot\ninteraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u5021\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u66f4\u5e7f\u6cdb\u5e94\u7528\u89d2\u8272\u626e\u6f14\u548c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\uff0c\u5e76\u4ee5\u793e\u533a\u836f\u623f\u673a\u5668\u4eba\u9879\u76ee\u4e3a\u4f8b\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u89d2\u8272\u626e\u6f14\u548c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4ee5\u6539\u5584\u4ea4\u4e92\u8bbe\u8ba1\u548c\u5f00\u53d1\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u89d2\u8272\u626e\u6f14\u548c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u793e\u533a\u836f\u623f\u673a\u5668\u4eba\u9879\u76ee\u8fdb\u884c\u5b9e\u8df5\u548c\u5e94\u7528\u3002", "result": "\u89d2\u8272\u626e\u6f14\u63d0\u4f9b\u4e86\u53ef\u63a7\u73af\u5883\u4ee5\u7406\u89e3\u7528\u6237\u9700\u6c42\uff0c\u5c42\u6b21\u4efb\u52a1\u5206\u6790\u786e\u4fdd\u4e86\u884c\u4e3a\u5efa\u6a21\u7684\u6b63\u786e\u6027\u5e76\u4fc3\u8fdb\u4e86\u5171\u540c\u8bbe\u8ba1\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u53ef\u4e13\u6ce8\u4e8e\u5f00\u53d1\u66f4\u9002\u5408\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u4efb\u52a1\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2509.13822", "pdf": "https://arxiv.org/pdf/2509.13822", "abs": "https://arxiv.org/abs/2509.13822", "authors": ["Hao Sun", "Shicong Liu", "Xianghao Yu", "Ying Sun"], "title": "Flow Matching-Based Active Learning for Radio Map Construction with Low-Altitude UAVs", "categories": ["eess.SP"], "comment": null, "summary": "The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy\nnecessitates precise and real-time radio maps for reliable communication and\nsafe navigation. However, constructing such maps is hindered by the\ninfeasibility of exhaustive measurements due to UAVs' limited flight endurance.\nTo address this, we propose a novel active learning framework for low-altitude\nradio map construction based on limited measurements. First, a Plug-and-Play\n(PnP)-refined flow matching algorithm is introduced, which leverages flow\nmatching as a powerful generative prior within a PnP scheme to reconstruct\nhigh-fidelity radio maps. Second, the generative nature of flow matching is\nexploited to quantify uncertainty by generating an ensemble of radio maps and\ncomputing the location-wise variance. The resulting uncertainty map guides a\nmulti-objective candidate selection and then a trajectory is planned via\nutility-aware path search (UAPS), directing the UAV to the most informative\nlocations while taking travel costs into account. Simulation results\ndemonstrate that our method significantly outperforms the baselines, achieving\nmore than a 70% reduction in normalized mean squared error (NMSE).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6709\u9650\u6d4b\u91cf\u6570\u636e\u6784\u5efa\u4f4e\u7a7a\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8def\u5f84\u89c4\u5212\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u56e0\u98de\u884c\u65f6\u95f4\u6709\u9650\u65e0\u6cd5\u8fdb\u884c\u8be6\u5c3d\u6d4b\u91cf\uff0c\u5bfc\u81f4\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408PnP\u6539\u8fdb\u7684\u6d41\u5339\u914d\u7b97\u6cd5\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5019\u9009\u9009\u62e9\u548c\u8def\u5f84\u89c4\u5212\u4f18\u5316\u65e0\u4eba\u673a\u6d4b\u91cf\u8def\u7ebf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u9ad8\u6548\u6784\u5efa\u9ad8\u4fdd\u771f\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u4e3a\u65e0\u4eba\u673a\u901a\u4fe1\u548c\u5bfc\u822a\u63d0\u4f9b\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2509.13380", "pdf": "https://arxiv.org/pdf/2509.13380", "abs": "https://arxiv.org/abs/2509.13380", "authors": ["Alejandro D. Mousist"], "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "comment": "This preprint presents ASTREA, a multi-agent architecture combining\n  LLM-guided semantic modulation with reinforcement learning for autonomous\n  satellite operations. The system is validated in hardware orbital\n  environments", "summary": "This paper presents ASTREA, the first agentic system deployed on\nflight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using\nthermal control as a representative use case, we integrate a\nresource-constrained Large Language Model (LLM) agent with a reinforcement\nlearning controller in an asynchronous architecture tailored for\nspace-qualified platforms. Ground experiments show that LLM-guided supervision\nimproves thermal stability and reduces violations, confirming the feasibility\nof combining semantic reasoning with adaptive control under hardware\nconstraints. However, on-orbit validation aboard the International Space\nStation (ISS) reveals performance degradation caused by inference latency\nmismatched with the rapid thermal cycles characteristic of Low Earth Orbit\n(LEO) satellites. These results highlight both the opportunities and current\nlimitations of agentic LLM-based systems in real flight environments, providing\npractical design guidelines for future space autonomy.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ASTREA\uff0c\u9996\u4e2a\u5728\u98de\u884c\u786c\u4ef6\u4e0a\u90e8\u7f72\u7684\u81ea\u4e3b\u822a\u5929\u5668\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5730\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u70ed\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f46\u5728\u8f68\u9a8c\u8bc1\u4e2d\u56e0\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u822a\u5929\u5668\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u7684\u70ed\u63a7\u5236\u95ee\u9898\uff0c\u63a2\u7d22LLM\u4e0e\u81ea\u9002\u5e94\u63a7\u5236\u7ed3\u5408\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u67b6\u6784\uff0c\u5c06\u8d44\u6e90\u53d7\u9650\u7684LLM\u4e0e\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u5e94\u7528\u4e8e\u7a7a\u95f4\u5e73\u53f0\u3002", "result": "\u5730\u9762\u5b9e\u9a8c\u663e\u793aLLM\u76d1\u7763\u63d0\u9ad8\u4e86\u70ed\u7a33\u5b9a\u6027\uff0c\u4f46\u5728\u8f68\u9a8c\u8bc1\u4e2d\u56e0\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "LLM\u4ee3\u7406\u7cfb\u7edf\u5728\u822a\u5929\u5668\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u4f18\u5316\u63a8\u7406\u5ef6\u8fdf\u4ee5\u9002\u5408\u5feb\u901f\u70ed\u5faa\u73af\u73af\u5883\u3002"}}
{"id": "2509.13851", "pdf": "https://arxiv.org/pdf/2509.13851", "abs": "https://arxiv.org/abs/2509.13851", "authors": ["Hao Su", "Jiangtao Wang", "Yongchao Wang"], "title": "FFT-Free PAPR Reduction Methods for OFDM Signals", "categories": ["eess.SP"], "comment": "6 page, 7 figures", "summary": "In this paper, we propose two low-complexity peak to average power\nratio(PAPR) reduction algorithms for orthogonal frequency division\nmultiplexing(OFDM) signals. The main content is as follows: First, a non-convex\noptimization model is established by minimizing the signal distortion power.\nThen, a customized alternating direction method of multipliers(ADMM) algorithm\nis proposed to solve the problem, named time domain ADMM(T-ADMM) along with an\nimproved version called T-ADMM with constrain update(TCU-ADMM). In the\nalgorithms, all subproblems can be solved analytically, and each iteration has\nlinear computational complexity. These algorithms circumvents the challenges\nposed by repeated fast Fourier transform(FFT) and inverse FFT(IFFT) operations\nin traditional PAPR reduction algorithms. Additionally, we prove that the\nT-ADMM algorithm is theoretically guaranteed convergent if proper parameter is\nchosen. Finally, simulation results demonstrate the effectiveness of the\nproposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u7684PAPR\u964d\u4f4e\u7b97\u6cd5\uff0c\u7528\u4e8eOFDM\u4fe1\u53f7\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u91cd\u590dFFT\u548cIFFT\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfPAPR\u964d\u4f4e\u7b97\u6cd5\u7531\u4e8e\u91cd\u590d\u7684FFT\u548cIFFT\u64cd\u4f5c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u3002", "method": "\u5efa\u7acb\u975e\u51f8\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u51fa\u5b9a\u5236\u5316\u7684ADMM\u7b97\u6cd5\uff08T-ADMM\u53ca\u5176\u6539\u8fdb\u7248\u672cTCU-ADMM\uff09\uff0c\u6240\u6709\u5b50\u95ee\u9898\u5747\u53ef\u89e3\u6790\u6c42\u89e3\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14T-ADMM\u5728\u9002\u5f53\u53c2\u6570\u4e0b\u5177\u6709\u7406\u8bba\u6536\u655b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684T-ADMM\u548cTCU-ADMM\u7b97\u6cd5\u5728\u964d\u4f4ePAPR\u65f6\u6709\u6548\u4e14\u9ad8\u6548\u3002"}}
{"id": "2509.13381", "pdf": "https://arxiv.org/pdf/2509.13381", "abs": "https://arxiv.org/abs/2509.13381", "authors": ["Zhang Xueyao", "Yang Bo", "Yu Zhiwen", "Cao Xuelin", "George C. Alexandropoulos", "Merouane Debbah", "Chau Yuen"], "title": "Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": "6 pages", "summary": "Autonomous Underwater Vehicles (AUVs) have shown great potential for\ncooperative detection and reconnaissance. However, collaborative AUV\ncommunications introduce risks of exposure. In adversarial environments,\nachieving efficient collaboration while ensuring covert operations becomes a\nkey challenge for underwater cooperative missions. In this paper, we propose a\nnovel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization\n(H-MAPPO) framework. The high-level component determines the individuals\nparticipating in the task based on a central AUV, while the low-level component\nreduces exposure probabilities through power and trajectory control by the\nparticipating AUVs. Simulation results show that the proposed framework\nachieves rapid convergence, outperforms benchmark algorithms in terms of\nperformance, and maximizes long-term cooperative efficiency while ensuring\ncovert operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u4ee3\u7406\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08H-MAPPO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9690\u853d\u7684\u6c34\u4e0b\u534f\u4f5c\u4efb\u52a1\u3002", "motivation": "\u5728\u654c\u5bf9\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u5728\u786e\u4fdd\u9690\u853d\u64cd\u4f5c\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u662f\u6c34\u4e0b\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u5206\u5c42\u6846\u67b6\uff0c\u9ad8\u5c42\u51b3\u5b9a\u4efb\u52a1\u53c2\u4e0e\u8005\uff0c\u4f4e\u5c42\u901a\u8fc7\u529f\u7387\u548c\u8f68\u8ff9\u63a7\u5236\u51cf\u5c11\u66b4\u9732\u6982\u7387\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u6846\u67b6\u6536\u655b\u5feb\u3001\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\uff0c\u5e76\u80fd\u5728\u9690\u853d\u64cd\u4f5c\u7684\u540c\u65f6\u6700\u5927\u5316\u957f\u671f\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "H-MAPPO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6c34\u4e0b\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u9690\u853d\u4e0e\u6548\u7387\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2509.13940", "pdf": "https://arxiv.org/pdf/2509.13940", "abs": "https://arxiv.org/abs/2509.13940", "authors": ["Weifeng Zhu", "Junyuan Gao", "Shuowen Zhang", "Liang Liu"], "title": "Reconfigurable Intelligent Surface-Assisted Multiuser Tracking and Signal Detection in ISAC", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "6 pages, 6 figures, accepted by IEEE conference", "summary": "This paper investigates the multiuser tracking and signal detection problem\nin integrated sensing and communication (ISAC) systems with the assistance of\nreconfigurable intelligent surfaces (RISs). Due to the diverse and high user\nmobility, the tracking and signal detection performance can be significantly\ndeteriorated without choreographed user state (position and velocity) updating\nprinciple. To tackle this challenge, we manage to establish a comprehensive\nprobabilistic signal model to characterize the interdependencies among user\nstates, transmit signals, and received signals during the tracking procedure.\nBased on the Bayesian problem formulation, we further propose a novel hybrid\nvariational message passing algorithm for the online estimation of user states,\nwhich can iteratively update the posterior probabilities of user states during\neach tracking frame with computational efficiency. Numerical results are\nprovided to demonstrate that the proposed algorithm can significantly improve\nboth of the tracking and signal detection performance over the representative\nBayesian estimation counterparts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u4e0b\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\u591a\u7528\u6237\u8ddf\u8e2a\u4e0e\u4fe1\u53f7\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u7528\u6237\u79fb\u52a8\u6027\u9ad8\u4e14\u591a\u6837\u5316\uff0c\u7f3a\u4e4f\u534f\u8c03\u7684\u7528\u6237\u72b6\u6001\u66f4\u65b0\u673a\u5236\u4f1a\u663e\u8457\u964d\u4f4e\u8ddf\u8e2a\u4e0e\u4fe1\u53f7\u68c0\u6d4b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u6982\u7387\u4fe1\u53f7\u6a21\u578b\u6765\u8868\u5f81\u7528\u6237\u72b6\u6001\u3001\u53d1\u5c04\u4fe1\u53f7\u4e0e\u63a5\u6536\u4fe1\u53f7\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u95ee\u9898\u8868\u8ff0\u7684\u6df7\u5408\u53d8\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u8ddf\u8e2a\u548c\u4fe1\u53f7\u68c0\u6d4b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4ee3\u8868\u6027\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u79fb\u52a8\u6027\u7528\u6237\u72b6\u6001\u66f4\u65b0\u7684\u6311\u6218\uff0c\u4e3a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13386", "pdf": "https://arxiv.org/pdf/2509.13386", "abs": "https://arxiv.org/abs/2509.13386", "authors": ["Hansol Lim", "Minhyeok Im", "Jonathan Boyack", "Jee Won Lee", "Jongseong Brad Choi"], "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization", "categories": ["cs.RO", "cs.LG"], "comment": "This work has been submitted to the 2026 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication", "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles\n(EVs) are increasingly being equipped with powerful computers. This enables\nonboard AI systems to optimize charge-aware path optimization customized to\nreflect vehicle's current condition and environment. We present VEGA, a\ncharge-aware EV navigation agent that plans over a charger-annotated road graph\nusing Proximal Policy Optimization (PPO) with budgeted A* teacher-student\nguidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.\nFirst, a physics-informed neural operator (PINO), trained on real vehicle speed\nand battery-power logs, uses recent vehicle speed logs to estimate aerodynamic\ndrag, rolling resistance, mass, motor and regenerative-braking efficiencies,\nand auxiliary load by learning a vehicle-custom dynamics. Second, a\nReinforcement Learning (RL) agent uses these dynamics to optimize a path with\noptimal charging stops and dwell times under SoC constraints. VEGA requires no\nadditional sensors and uses only vehicle speed signals. It may serve as a\nvirtual sensor for power and efficiency to potentially reduce EV cost. In\nevaluation on long routes like San Francisco to New York, VEGA's stops, dwell\ntimes, SoC management, and total travel time closely track Tesla Trip Planner\nwhile being slightly more conservative, presumably due to real vehicle\nconditions such as vehicle parameter drift due to deterioration. Although\ntrained only in U.S. regions, VEGA was able to compute optimal charge-aware\npaths in France and Japan, demonstrating generalizability. It achieves\npractical integration of physics-informed learning and RL for EV eco-routing.", "AI": {"tldr": "VEGA\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u7535\u52a8\u6c7d\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5145\u7535\u8def\u5f84\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\uff0c\u8868\u73b0\u63a5\u8fd1Tesla Trip Planner\uff0c\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u9700\u6c42\u589e\u957f\uff0cSDV\u6280\u672f\u53d1\u5c55\u50ac\u751f\u4e86\u66f4\u667a\u80fd\u7684\u5145\u7535\u8def\u5f84\u4f18\u5316\u9700\u6c42\uff0cVEGA\u65e8\u5728\u901a\u8fc7AI\u548c\u5b9e\u65f6\u8f66\u8f86\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "method": "VEGA\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINO\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6a21\u5757\uff0c\u524d\u8005\u5b66\u4e60\u8f66\u8f86\u52a8\u6001\uff0c\u540e\u8005\u4f18\u5316\u5145\u7535\u8def\u5f84\u3002\u4f7f\u7528PPO\u7b97\u6cd5\u548cA*\u6307\u5bfc\u3002", "result": "VEGA\u5728\u957f\u9014\u8def\u7ebf\uff08\u5982\u7ebd\u7ea6\u5230\u65e7\u91d1\u5c71\uff09\u4e2d\u8868\u73b0\u63a5\u8fd1Tesla Trip Planner\uff0c\u5145\u7535\u7b56\u7565\u4fdd\u5b88\u4f46\u9002\u5e94\u6027\u5f3a\uff0c\u4e14\u5728\u6cd5\u56fd\u548c\u65e5\u672c\u4e5f\u9002\u7528\u3002", "conclusion": "VEGA\u6210\u529f\u878d\u5408\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u7535\u52a8\u6c7d\u8f66\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u751f\u6001\u8def\u5f84\u4f18\u5316\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.13961", "pdf": "https://arxiv.org/pdf/2509.13961", "abs": "https://arxiv.org/abs/2509.13961", "authors": ["Lorenza Angelini", "Dimitar Stanev", "Marta P\u0142onka", "Rafa\u0142 Klimas", "Natan Napi\u00f3rkowski", "Gabriela Gonz\u00e1lez Chan", "Lisa Bunn", "Paul S Glazier", "Richard Hosking", "Jenny Freeman", "Jeremy Hobart", "Jonathan Marsden", "Licinio Craveiro", "Mike D Rinderknecht", "Mattia Zanon"], "title": "Adaptive and robust smartphone-based step detection in multiple sclerosis", "categories": ["eess.SP", "92C55, 68T10, 93C85", "I.5.4; J.3; H.1.2"], "comment": "66 pages total, 6 figures, 1 table, 23 supplementary appendix pages,\n  2 supplementary figures, 6 supplementary tables", "summary": "Background: Many attempts to validate gait pipelines that process sensor data\nto detect gait events have focused on the detection of initial contacts only in\nsupervised settings using a single sensor. Objective: To evaluate the\nperformance of a gait pipeline in detecting initial/final contacts using a step\ndetection algorithm adaptive to different test settings, smartphone wear\nlocations, and gait impairment levels. Methods: In GaitLab (ISRCTN15993728),\nhealthy controls (HC) and people with multiple sclerosis (PwMS; Expanded\nDisability Status Scale 0.0-6.5) performed supervised Two-Minute Walk Test\n[2MWT] (structured in-lab overground and treadmill 2MWT) during two on-site\nvisits carrying six smartphones and unsupervised walking activities (structured\nand unstructured real-world walking) daily for 10-14 days using a single\nsmartphone. Reference gait data were collected with a motion capture system or\nGait Up sensors. The pipeline's performance in detecting initial/final contacts\nwas evaluated through F1 scores and absolute temporal error with respect to\nreference measurement systems. Results: We studied 35 HC and 93 PwMS.\nInitial/final contacts were accurately detected across all smartphone wear\nlocations. Median F1 scores for initial/final contacts on in-lab 2MWT were\n>=98.2%/96.5% in HC and >=98.5%/97.7% in PwMS. F1 scores remained high on\nstructured (HC: 100% [0.3%]/100% [0.2%]; PwMS: 99.5% [1.9%]/99.4% [2.5%]) and\nunstructured real-world walking (HC: 97.8% [2.6%]/97.8% [2.8%]; PwMS: 94.4%\n[6.2%]/94.0% [6.5%]). Median temporal errors were <=0.08 s. Neither age, sex,\ndisease severity, walking aid use, nor setting (outdoor/indoor) impacted\npipeline performance (all p>0.05). Conclusion: This gait pipeline accurately\nand consistently detects initial and final contacts in PwMS across different\nsmartphone locations and environments, highlighting its potential for\nreal-world gait assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u6b65\u6001\u68c0\u6d4b\u7b97\u6cd5\u5728\u4e0d\u540c\u667a\u80fd\u624b\u673a\u4f69\u6234\u4f4d\u7f6e\u548c\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5bf9\u521d\u59cb\u548c\u7ec8\u6b62\u6b65\u6001\u4e8b\u4ef6\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u5f88\u9ad8\u3002", "motivation": "\u9a8c\u8bc1\u4e00\u79cd\u6b65\u6001\u68c0\u6d4b\u7b97\u6cd5\u5728\u4e0d\u540c\u6d4b\u8bd5\u8bbe\u7f6e\u3001\u667a\u80fd\u624b\u673a\u4f69\u6234\u4f4d\u7f6e\u4ee5\u53ca\u6b65\u6001\u969c\u788d\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6b65\u6001\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u7814\u7a76\u62db\u52df\u4e86\u5065\u5eb7\u5bf9\u7167\u7ec4\u548c\u591a\u53d1\u6027\u786c\u5316\u75c7\u60a3\u8005\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6b65\u884c\u6d4b\u8bd5\uff0c\u4f7f\u7528\u667a\u80fd\u624b\u673a\u548c\u53c2\u8003\u4f20\u611f\u5668\u6536\u96c6\u6570\u636e\uff0c\u8bc4\u4f30\u7b97\u6cd5\u7684F1\u5206\u6570\u548c\u7edd\u5bf9\u65f6\u95f4\u8bef\u5dee\u3002", "result": "\u7b97\u6cd5\u5728\u6240\u6709\u667a\u80fd\u624b\u673a\u4f69\u6234\u4f4d\u7f6e\u548c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u521d\u59cb/\u7ec8\u6b62\u6b65\u6001\u4e8b\u4ef6\u7684F1\u5206\u6570\u5728\u5065\u5eb7\u7ec4\u548c\u591a\u53d1\u6027\u786c\u5316\u75c7\u60a3\u8005\u4e2d\u5747\u5f88\u9ad8\uff0c\u65f6\u95f4\u8bef\u5dee\u5c0f\u3002\u6027\u80fd\u4e0d\u53d7\u5e74\u9f84\u3001\u6027\u522b\u3001\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u7b49\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6b65\u6001\u7b97\u6cd5\u5728\u591a\u53d1\u6027\u786c\u5316\u75c7\u60a3\u8005\u4e2d\u8868\u73b0\u51fa\u51c6\u786e\u548c\u4e00\u81f4\u7684\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u6b65\u6001\u8bc4\u4f30\u3002"}}
{"id": "2509.13434", "pdf": "https://arxiv.org/pdf/2509.13434", "abs": "https://arxiv.org/abs/2509.13434", "authors": ["Wei-Chen Li", "Glen Chou"], "title": "A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies", "categories": ["cs.RO"], "comment": null, "summary": "We present a computational framework for simulating filaments interacting\nwith rigid bodies through contact. Filaments are challenging to simulate due to\ntheir codimensionality, i.e., they are one-dimensional structures embedded in\nthree-dimensional space. Existing methods often assume that filaments remain\npermanently attached to rigid bodies. Our framework unifies discrete elastic\nrod (DER) modeling, a pressure field patch contact model, and a convex contact\nformulation to accurately simulate frictional interactions between slender\nfilaments and rigid bodies - capabilities not previously achievable. Owing to\nthe convex formulation of contact, each time step can be solved to global\noptimality, guaranteeing complementarity between contact velocity and impulse.\nWe validate the framework by assessing the accuracy of frictional forces and\ncomparing its physical fidelity against baseline methods. Finally, we\ndemonstrate its applicability in both soft robotics, such as a stochastic\nfilament-based gripper, and deformable object manipulation, such as shoelace\ntying, providing a versatile simulator for systems involving complex\nfilament-filament and filament-rigid body interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u7ec6\u4e1d\u4e0e\u521a\u4f53\u63a5\u89e6\u4ea4\u4e92\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u6469\u64e6\u4ea4\u4e92\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7ec6\u4e1d\u7684\u4f4e\u7ef4\u7279\u6027\uff0c\u73b0\u6709\u6a21\u62df\u65b9\u6cd5\u5e38\u5047\u8bbe\u5176\u6c38\u4e45\u9644\u7740\u4e8e\u521a\u4f53\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u6469\u64e6\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u79bb\u6563\u5f39\u6027\u6746\u6a21\u578b\u3001\u538b\u529b\u573a\u63a5\u89e6\u6a21\u578b\u548c\u51f8\u63a5\u89e6\u516c\u5f0f\uff0c\u5b9e\u73b0\u7ec6\u4e1d\u4e0e\u521a\u4f53\u7684\u7cbe\u786e\u6469\u64e6\u6a21\u62df\u3002", "result": "\u901a\u8fc7\u51f8\u63a5\u89e6\u516c\u5f0f\uff0c\u6bcf\u6b65\u6c42\u89e3\u53ef\u8fbe\u5230\u5168\u5c40\u6700\u4f18\uff0c\u9a8c\u8bc1\u4e86\u6469\u64e6\u529b\u51c6\u786e\u6027\u5e76\u5c55\u793a\u4e86\u8f6f\u673a\u5668\u4eba\u548c\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u7ec6\u4e1d\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u6a21\u62df\u5668\uff0c\u5177\u5907\u9ad8\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13975", "pdf": "https://arxiv.org/pdf/2509.13975", "abs": "https://arxiv.org/abs/2509.13975", "authors": ["Ilker Bayram"], "title": "Classification Filtering", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "We consider a streaming signal in which each sample is linked to a latent\nclass. We assume that multiple classifiers are available, each providing class\nprobabilities with varying degrees of accuracy. These classifiers are employed\nfollowing a straightforward and fixed policy. In this setting, we consider the\nproblem of fusing the output of the classifiers while incorporating the\ntemporal aspect to improve classification accuracy. We propose a state-space\nmodel and develop a filter tailored for realtime execution. We demonstrate the\neffectiveness of the proposed filter in an activity classification application\nbased on inertial measurement unit (IMU) data from a wearable device.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u591a\u5206\u7c7b\u5668\u8f93\u51fa\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u63d0\u9ad8\u6d41\u4fe1\u53f7\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u6d41\u4fe1\u53f7\u4e2d\u6bcf\u4e2a\u6837\u672c\u4e0e\u9690\u85cf\u7c7b\u522b\u76f8\u5173\uff0c\u73b0\u6709\u56fa\u5b9a\u7b56\u7565\u7684\u591a\u5206\u7c7b\u5668\u8f93\u51fa\u878d\u5408\u672a\u5145\u5206\u5229\u7528\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5e76\u8bbe\u8ba1\u5b9e\u65f6\u6ee4\u6ce2\u5668\uff0c\u7ed3\u5408\u65f6\u5e8f\u4fe1\u606f\u4f18\u5316\u5206\u7c7b\u3002", "result": "\u5728\u57fa\u4e8eIMU\u6570\u636e\u7684\u6d3b\u52a8\u5206\u7c7b\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6ee4\u6ce2\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002"}}
{"id": "2509.13501", "pdf": "https://arxiv.org/pdf/2509.13501", "abs": "https://arxiv.org/abs/2509.13501", "authors": ["Hossein Gholampour", "Logan E. Beaver"], "title": "Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume", "categories": ["cs.RO"], "comment": null, "summary": "Many robotic systems must follow planned paths yet pause safely and resume\nwhen people or objects intervene. We present an output-space method for systems\nwhose tracked output can be feedback-linearized to a double integrator (e.g.,\nmanipulators). The approach has two parts. Offline, we perform a pre-run\nreachability check to verify that the motion plan respects speed and\nacceleration magnitude limits. Online, we apply a quadratic program to track\nthe motion plan under the same limits. We use a one-step reachability test to\nbound the maximum disturbance the system is capable of rejecting. When the\nstate coincides with the reference path we recover perfect tracking in the\ndeterministic case, and we correct errors using a KKT-inspired weight. We\ndemonstrate that safety stops and unplanned deviations are handled efficiently,\nand the system returns to the motion plan without replanning. We demonstrate\nour system's improved performance over pure pursuit in simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53cd\u9988\u7ebf\u6027\u5316\u4e3a\u53cc\u79ef\u5206\u5668\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8f93\u51fa\u7a7a\u95f4\u65b9\u6cd5\uff0c\u786e\u4fdd\u5176\u5728\u9075\u5faa\u8def\u5f84\u65f6\u80fd\u5b89\u5168\u6682\u505c\u5e76\u6062\u590d\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8def\u5f84\u8ddf\u8e2a\u4e2d\u9700\u8981\u5b89\u5168\u5e94\u5bf9\u4eba\u4e3a\u6216\u7269\u4f53\u5e72\u9884\uff0c\u540c\u65f6\u786e\u4fdd\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u9650\u5236\u5185\u8fd0\u884c\u3002", "method": "\u79bb\u7ebf\u8fdb\u884c\u53ef\u8fbe\u6027\u68c0\u67e5\u9a8c\u8bc1\u8fd0\u52a8\u8ba1\u5212\uff0c\u5728\u7ebf\u4f7f\u7528\u4e8c\u6b21\u89c4\u5212\u8ddf\u8e2a\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u4e00\u6b65\u53ef\u8fbe\u6027\u6d4b\u8bd5\u9650\u5236\u6270\u52a8\u3002", "result": "\u7cfb\u7edf\u80fd\u9ad8\u6548\u5904\u7406\u5b89\u5168\u505c\u6b62\u548c\u975e\u8ba1\u5212\u504f\u79bb\uff0c\u65e0\u9700\u91cd\u65b0\u89c4\u5212\u5373\u53ef\u6062\u590d\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u7eaf\u8ffd\u8e2a\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5b89\u5168\u8def\u5f84\u8ddf\u8e2a\u7684\u573a\u666f\u3002"}}
{"id": "2509.13984", "pdf": "https://arxiv.org/pdf/2509.13984", "abs": "https://arxiv.org/abs/2509.13984", "authors": ["Drake Silbernagel", "Yu Rong", "Isabella Lenz", "Prithvi Hemanth", "Carl Morgenstern", "Owen Ma", "Nolan Matthews", "Nader Zaki", "Kyle W. Martin", "John D. Elgin", "Jacob Holtom", "Daniel W. Bliss", "Kimberly Frey"], "title": "Distributed Coherent Beamforming at 60 GHz Enabled by Optically-Established Coherence", "categories": ["eess.SP", "physics.optics"], "comment": null, "summary": "We implement and experimentally demonstrate a 60 GHz distributed system\nleveraging an optical time synchronization system that provides precise time\nand frequency alignment between independent elements of the distributed mesh.\nUtilizing such accurate coherence, we perform receive beamforming with\ninterference rejection and transmit nulling. In these configurations, the\nsystem achieves a coherent gain over an incoherent network of N nodes,\nsignificantly improving the relevant signal power ratios. Our system\ndemonstrates extended array phase coherence times, enabling advanced\ntechniques. Results from over-the-air experiments demonstrate a 14.3 dB\nsignal-to-interference-plus-noise improvement in interference-laden scenarios\nwith a contributing 13.5 dB null towards interference in receive beamforming.\nIn transmit nulling, a signal-to-noise ratio (SNR) gain of 7.9 dB is measured\ntowards an intended receiver while maintaining an SNR reduction of 8.9 dB at\nanother receiver. These findings represent the use of distributed coherence in\nthe V band without the use of GPS timing.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd60 GHz\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5229\u7528\u5149\u5b66\u65f6\u95f4\u540c\u6b65\u5b9e\u73b0\u8282\u70b9\u95f4\u7cbe\u786e\u65f6\u95f4\u4e0e\u9891\u7387\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u529f\u7387\u6bd4\u548c\u5e72\u6270\u6291\u5236\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65f6\u95f4\u4e0e\u9891\u7387\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u4fe1\u53f7\u5904\u7406\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5e72\u6270\u73af\u5883\u4e0b\u3002", "method": "\u91c7\u7528\u5149\u5b66\u65f6\u95f4\u540c\u6b65\u7cfb\u7edf\u5b9e\u73b0\u8282\u70b9\u95f4\u7684\u65f6\u95f4\u4e0e\u9891\u7387\u5bf9\u9f50\uff0c\u8fdb\u884c\u63a5\u6536\u6ce2\u675f\u8d4b\u5f62\u548c\u53d1\u5c04\u96f6\u9677\uff0c\u6d4b\u8bd5\u4e86\u5e72\u6270\u73af\u5883\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u63a5\u6536\u6ce2\u675f\u8d4b\u5f62\u4e2d\u5b9e\u73b0\u4e8614.3 dB\u7684\u4fe1\u5e72\u566a\u6bd4\u63d0\u5347\uff0c13.5 dB\u7684\u5e72\u6270\u6291\u5236\uff1b\u5728\u53d1\u5c04\u96f6\u9677\u4e2d\u5b9e\u73b0\u4e867.9 dB\u7684\u4fe1\u566a\u6bd4\u589e\u76ca\u548c8.9 dB\u7684\u5e72\u6270\u6291\u5236\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u4e86\u5728V\u6ce2\u6bb5\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u65e0\u9700GPS\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u540c\u6b65\u548c\u5e72\u6270\u6291\u5236\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.13534", "pdf": "https://arxiv.org/pdf/2509.13534", "abs": "https://arxiv.org/abs/2509.13534", "authors": ["Chunxin Zheng", "Kai Chen", "Zhihai Bi", "Yulin Li", "Liang Pan", "Jinni Zhou", "Haoang Li", "Jun Ma"], "title": "Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Whole-body manipulation (WBM) for humanoid robots presents a promising\napproach for executing embracing tasks involving bulky objects, where\ntraditional grasping relying on end-effectors only remains limited in such\nscenarios due to inherent stability and payload constraints. This paper\nintroduces a reinforcement learning framework that integrates a pre-trained\nhuman motion prior with a neural signed distance field (NSDF) representation to\nachieve robust whole-body embracing. Our method leverages a teacher-student\narchitecture to distill large-scale human motion data, generating kinematically\nnatural and physically feasible whole-body motion patterns. This facilitates\ncoordinated control across the arms and torso, enabling stable multi-contact\ninteractions that enhance the robustness in manipulation and also the load\ncapacity. The embedded NSDF further provides accurate and continuous geometric\nperception, improving contact awareness throughout long-horizon tasks. We\nthoroughly evaluate the approach through comprehensive simulations and\nreal-world experiments. The results demonstrate improved adaptability to\ndiverse shapes and sizes of objects and also successful sim-to-real transfer.\nThese indicate that the proposed framework offers an effective and practical\nsolution for multi-contact and long-horizon WBM tasks of humanoid robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548c\u795e\u7ecf\u7b26\u53f7\u8ddd\u79bb\u573a\uff08NSDF\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u5305\u88f9\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6293\u53d6\u65b9\u6cd5\u5728\u6d89\u53ca\u5927\u578b\u7269\u4f53\u65f6\u53d7\u9650\u4e8e\u7a33\u5b9a\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\uff0c\u800c\u5168\u8eab\u64cd\u4f5c\uff08WBM\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u4ece\u5927\u89c4\u6a21\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u4e2d\u84b8\u998f\u51fa\u81ea\u7136\u4e14\u7269\u7406\u53ef\u884c\u7684\u5168\u8eab\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408NSDF\u63d0\u4f9b\u7cbe\u786e\u7684\u51e0\u4f55\u611f\u77e5\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9002\u5e94\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u591a\u63a5\u89e6\u548c\u957f\u65f6\u7a0b\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.14062", "pdf": "https://arxiv.org/pdf/2509.14062", "abs": "https://arxiv.org/abs/2509.14062", "authors": ["Saifur Rahman", "Syed Luqman Shah", "Salman Khan", "Jalal Khan", "Muhammad Irfan", "Maaz Shafi", "Said Muhammad", "Fazal Muhammad", "Mohammad Shahed Akond"], "title": "Distributed Deep Learning with RIS Grouping for Accurate Cascaded Channel Estimation", "categories": ["eess.SP"], "comment": "Submitted for Publication", "summary": "Reconfigurable Intelligent Surface (RIS) panels are envisioned as a key\ntechnology for sixth-generation (6G) wireless networks, providing a\ncost-effective means to enhance coverage and spectral efficiency. A critical\nchallenge is the estimation of the cascaded base station (BS)-RIS-user channel,\nsince the passive nature of RIS elements prevents direct channel acquisition,\nincurring prohibitive pilot overhead, computational complexity, and energy\nconsumption. To address this, we propose a deep learning (DL)-based channel\nestimation framework that reduces pilot overhead by grouping RIS elements and\nreconstructing the cascaded channel from partial pilot observations.\nFurthermore, conventional DL models trained under single-user settings suffer\nfrom poor generalization across new user locations and propagation scenarios.\nWe develop a distributed machine learning (DML) strategy in which the BS and\nusers collaboratively train a shared neural network using diverse channel\ndatasets collected across the network, thereby achieving robust generalization.\nBuilding on this foundation, we design a hierarchical DML neural architecture\nthat first classifies propagation conditions and then employs scenario-specific\nfeature extraction to further improve estimation accuracy. Simulation results\nconfirm that the proposed framework substantially reduces pilot overhead and\ncomplexity while outperforming conventional methods and single-user models in\nchannel estimation accuracy. These results demonstrate the practicality and\neffectiveness of the proposed approach for 6G RIS-assisted systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684RIS\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4RIS\u5143\u7d20\u548c\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5bfc\u9891\u5f00\u9500\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "RIS\u9762\u677f\u57286G\u7f51\u7edc\u4e2d\u867d\u7136\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u4f46\u88ab\u52a8\u7279\u6027\u5bfc\u81f4\u4fe1\u9053\u4f30\u8ba1\u56f0\u96be\uff0c\u5bfc\u9891\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5206\u7ec4RIS\u5143\u7d20\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u7b56\u7565\uff0c\u534f\u540c\u8bad\u7ec3\u5171\u4eab\u795e\u7ecf\u7f51\u7edc\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42DML\u67b6\u6784\u8fdb\u4e00\u6b65\u4f18\u5316\u4f30\u8ba1\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u5bfc\u9891\u5f00\u9500\u548c\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u7528\u6237\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57286G RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.13541", "pdf": "https://arxiv.org/pdf/2509.13541", "abs": "https://arxiv.org/abs/2509.13541", "authors": ["Ayberk Acar", "Fangjie Li", "Hao Li", "Lidia Al-Zogbi", "Kanyifeechukwu Jane Oguine", "Susheela Sharma Stern", "Jesse F. d'Almeida", "Robert J. Webster III", "Ipek Oguz", "Jie Ying Wu"], "title": "Semantic 3D Reconstructions with SLAM for Central Airway Obstruction", "categories": ["cs.RO", "cs.CV"], "comment": "5 pages, 2 figures, 1 table", "summary": "Central airway obstruction (CAO) is a life-threatening condition with\nincreasing incidence, caused by tumors in and outside of the airway.\nTraditional treatment methods such as bronchoscopy and electrocautery can be\nused to remove the tumor completely; however, these methods carry a high risk\nof complications. Recent advances allow robotic interventions with lesser risk.\nThe combination of robot interventions with scene understanding and mapping\nalso opens up the possibilities for automation. We present a novel pipeline\nthat enables real-time, semantically informed 3D reconstructions of the central\nairway using monocular endoscopic video.\n  Our approach combines DROID-SLAM with a segmentation model trained to\nidentify obstructive tissues. The SLAM module reconstructs the 3D geometry of\nthe airway in real time, while the segmentation masks guide the annotation of\nobstruction regions within the reconstructed point cloud. To validate our\npipeline, we evaluate the reconstruction quality using ex vivo models.\n  Qualitative and quantitative results show high similarity between ground\ntruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By\nintegrating segmentation directly into the SLAM workflow, our system produces\nannotated 3D maps that highlight clinically relevant regions in real time.\nHigh-speed capabilities of the pipeline allows quicker reconstructions compared\nto previous work, reflecting the surgical scene more accurately.\n  To the best of our knowledge, this is the first work to integrate semantic\nsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our\nframework is modular and can generalize to other anatomies or procedures with\nminimal changes, offering a promising step toward autonomous robotic\ninterventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9e\u65f6\u5355\u76eeSLAM\u548c\u8bed\u4e49\u5206\u5272\uff0c\u7528\u4e8e\u4e2d\u592e\u6c14\u9053\u963b\u585e\u76843D\u91cd\u5efa\uff0c\u63d0\u9ad8\u4e86\u624b\u672f\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6cbb\u7597\u4e2d\u592e\u6c14\u9053\u963b\u585e\u7684\u65b9\u6cd5\u98ce\u9669\u9ad8\uff0c\u65b0\u65b9\u6cd5\u901a\u8fc7\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u4e3a\u624b\u672f\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408DROID-SLAM\u548c\u5206\u5272\u6a21\u578b\uff0c\u5b9e\u65f6\u91cd\u5efa\u6c14\u90533D\u51e0\u4f55\u5e76\u6807\u6ce8\u963b\u585e\u533a\u57df\uff0c\u9a8c\u8bc1\u901a\u8fc7\u4f53\u5916\u6a21\u578b\u4e0eCT\u626b\u63cf\u5bf9\u6bd4\u3002", "result": "\u91cd\u5efa\u7ed3\u679c\u4e0eCT\u626b\u63cf\u9ad8\u5ea6\u76f8\u4f3c\uff080.62 mm Chamfer\u8ddd\u79bb\uff09\uff0c\u5b9e\u65f6\u751f\u6210\u6807\u6ce8\u76843D\u5730\u56fe\uff0c\u901f\u5ea6\u5feb\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "\u9996\u6b21\u5c06\u8bed\u4e49\u5206\u5272\u4e0e\u5b9e\u65f6\u5355\u76eeSLAM\u7ed3\u5408\u7528\u4e8e\u5185\u955cCAO\uff0c\u6846\u67b6\u6a21\u5757\u5316\uff0c\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u89e3\u5256\u6216\u624b\u672f\uff0c\u8fc8\u5411\u81ea\u4e3b\u673a\u5668\u4eba\u5e72\u9884\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.14072", "pdf": "https://arxiv.org/pdf/2509.14072", "abs": "https://arxiv.org/abs/2509.14072", "authors": ["Vincent Lauinger", "Lennart Schmitz", "Patrick Matalla", "Andrej Rode", "Sebastian Randel", "Laurent Schmalen"], "title": "Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission", "categories": ["eess.SP"], "comment": "Accepted and to be presented at the European Conference on Optical\n  Communication (ECOC) 2025", "summary": "We demonstrate the effectiveness of a novel phase-noise-tolerant,\nvariational-autoencoder-based equalization scheme for\nspace-division-multiplexed (SDM) transmission in an experiment over 150km of\nrandomly-coupled multi-core fibers.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u76f8\u4f4d\u566a\u58f0\u5bb9\u5fcd\u5747\u8861\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u82af\u5149\u7ea4\u7684\u7a7a\u95f4\u5206\u5272\u590d\u7528\u4f20\u8f93\u3002", "motivation": "\u89e3\u51b3\u591a\u82af\u5149\u7ea4\u4f20\u8f93\u4e2d\u76f8\u4f4d\u566a\u58f0\u5bf9\u4fe1\u53f7\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5747\u8861\u5316\u5904\u7406\uff0c\u5e76\u5728150\u516c\u91cc\u968f\u673a\u8026\u5408\u591a\u82af\u5149\u7ea4\u4e2d\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u82af\u5149\u7ea4\u4f20\u8f93\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u6291\u5236\u4e86\u76f8\u4f4d\u566a\u58f0\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u957f\u8ddd\u79bb\u591a\u82af\u5149\u7ea4\u4f20\u8f93\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u76f8\u4f4d\u566a\u58f0\u6291\u5236\u65b9\u6cd5\u3002"}}
{"id": "2509.13572", "pdf": "https://arxiv.org/pdf/2509.13572", "abs": "https://arxiv.org/abs/2509.13572", "authors": ["Ozan Karaali", "Hossam Farag", "Strahinja Dosen", "Cedomir Stefanovic"], "title": "Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference", "categories": ["cs.RO"], "comment": "ICAT 2025", "summary": "This study examines the potential of utilizing Vision Language Models (VLMs)\nto improve the perceptual capabilities of semi-autonomous prosthetic hands. We\nintroduce a unified benchmark for end-to-end perception and grasp inference,\nevaluating a single VLM to perform tasks that traditionally require complex\npipelines with separate modules for object detection, pose estimation, and\ngrasp planning. To establish the feasibility and current limitations of this\napproach, we benchmark eight contemporary VLMs on their ability to perform a\nunified task essential for bionic grasping. From a single static image, they\nshould (1) identify common objects and their key properties (name, shape,\norientation, and dimensions), and (2) infer appropriate grasp parameters (grasp\ntype, wrist rotation, hand aperture, and number of fingers). A corresponding\nprompt requesting a structured JSON output was employed with a dataset of 34\nsnapshots of common objects. Key performance metrics, including accuracy for\ncategorical attributes (e.g., object name, shape) and errors in numerical\nestimates (e.g., dimensions, hand aperture), along with latency and cost, were\nanalyzed. The results demonstrated that most models exhibited high performance\nin object identification and shape recognition, while accuracy in estimating\ndimensions and inferring optimal grasp parameters, particularly hand rotation\nand aperture, varied more significantly. This work highlights the current\ncapabilities and limitations of VLMs as advanced perceptual modules for\nsemi-autonomous control of bionic limbs, demonstrating their potential for\neffective prosthetic applications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u63d0\u5347\u534a\u81ea\u4e3b\u5047\u624b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u611f\u77e5\u548c\u6293\u53d6\u63a8\u7406\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30VLM\u5728\u5355\u5e45\u56fe\u50cf\u4e2d\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u5047\u624b\u7684\u611f\u77e5\u4e0e\u63a7\u5236\u6d41\u7a0b\uff0c\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u4e2d\u591a\u4e2a\u72ec\u7acb\u6a21\u5757\u7684\u9700\u6c42\uff0c\u63d0\u5347\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6d4b\u8bd5\u4e86\u516b\u79cd\u73b0\u4ee3VLM\u5728\u5b8c\u6210\u7edf\u4e00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4efb\u52a1\u5305\u62ec\u8bc6\u522b\u7269\u4f53\u5c5e\u6027\uff08\u540d\u79f0\u3001\u5f62\u72b6\u7b49\uff09\u548c\u63a8\u65ad\u6293\u53d6\u53c2\u6570\uff08\u6293\u63e1\u7c7b\u578b\u3001\u624b\u90e8\u65cb\u8f6c\u7b49\uff09\uff0c\u4f7f\u7528\u7ed3\u6784\u5316JSON\u8f93\u51fa\u548c34\u5f20\u5e38\u89c1\u7269\u4f53\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u548c\u5f62\u72b6\u5224\u65ad\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5c3a\u5bf8\u4f30\u8ba1\u548c\u6293\u53d6\u53c2\u6570\u63a8\u65ad\uff08\u5982\u624b\u90e8\u65cb\u8f6c\u548c\u5f00\u5408\u5ea6\uff09\u4e0a\u51c6\u786e\u7387\u6ce2\u52a8\u8f83\u5927\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVLM\u5728\u5047\u80a2\u611f\u77e5\u6a21\u5757\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u5728\u590d\u6742\u53c2\u6570\u63a8\u65ad\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2509.14160", "pdf": "https://arxiv.org/pdf/2509.14160", "abs": "https://arxiv.org/abs/2509.14160", "authors": ["Adam Umra", "Aya Mostafa Ahmed", "Stefan Roth", "Aydin Sezgin"], "title": "Hardware-Efficient Cognitive Radar: Multi-Target Detection with RL-Driven Transmissive RIS", "categories": ["eess.SP"], "comment": "5 pages, 3 figures, submitted to ICASSP 2026", "summary": "Cognitive radar has emerged as a key paradigm for next-generation sensing,\nenabling adaptive, intelligent operation in dynamic and complex environments.\nYet, conventional cognitive multiple-input multiple-output (MIMO) radars offer\nstrong detection performance but suffer from high hardware complexity and power\ndemands. To overcome these limitations, we develop a reinforcement learning\n(RL)-based framework that leverages a transmissive reconfigurable intelligent\nsurface (TRIS) for adaptive beamforming. A state-action-reward-state-action\n(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low\nsignal-to-noise ratio (SNR) conditions while operating with far fewer radio\nfrequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar\nmatches or, for large number of elements, even surpasses MIMO performance with\nreduced cost and energy requirements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u900f\u5c04\u578b\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08TRIS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u96f7\u8fbe\u7684\u6ce2\u675f\u6210\u5f62\u6027\u80fd\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u63d0\u5347\u591a\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u529f\u8017\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5MIMO\u96f7\u8fbe\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u529f\u8017\u9ad8\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u5229\u7528TRIS\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u96f7\u8fbe\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u72b6\u6001-\u52a8\u4f5c-\u5956\u52b1-\u72b6\u6001-\u52a8\u4f5c\uff08SARSA\uff09\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574TRIS\u7684\u76f8\u4f4d\u504f\u79fb\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u6ce2\u675f\u6210\u5f62\uff0c\u51cf\u5c11\u5c04\u9891\u94fe\u7684\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cTRIS-RL\u96f7\u8fbe\u5728\u591a\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u4e0a\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u4f20\u7edfMIMO\u96f7\u8fbe\uff0c\u5c24\u5176\u662f\u5728\u5143\u4ef6\u6570\u91cf\u8f83\u591a\u65f6\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6210\u672c\u548c\u80fd\u8017\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u96f7\u8fbe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7ed3\u5408TRIS\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u590d\u6742\u5ea6\u548c\u529f\u8017\u7684\u663e\u8457\u964d\u4f4e\u3002"}}
{"id": "2509.13574", "pdf": "https://arxiv.org/pdf/2509.13574", "abs": "https://arxiv.org/abs/2509.13574", "authors": ["Zidong Chen", "Zihao Guo", "Peng Wang", "ThankGod Itua Egbe", "Yan Lyu", "Chenghao Qian"], "title": "Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Flow matching has emerged as a competitive framework for learning\nhigh-quality generative policies in robotics; however, we find that\ngeneralisation arises and saturates early along the flow trajectory, in\naccordance with recent findings in the literature. We further observe that\nincreasing the number of Euler integration steps during inference\ncounter-intuitively and universally degrades policy performance. We attribute\nthis to (i) additional, uniformly spaced integration steps oversample the\nlate-time region, thereby constraining actions towards the training\ntrajectories and reducing generalisation; and (ii) the learned velocity field\nbecoming non-Lipschitz as integration time approaches 1, causing instability.\nTo address these issues, we propose a novel policy that utilises non-uniform\ntime scheduling (e.g., U-shaped) during training, which emphasises both early\nand late temporal stages to regularise policy training, and a dense-jump\nintegration schedule at inference, which uses a single-step integration to\nreplace the multi-step integration beyond a jump point, to avoid unstable areas\naround 1. Essentially, our policy is an efficient one-step learner that still\npushes forward performance through multi-step integration, yielding up to 23.7%\nperformance gains over state-of-the-art baselines across diverse robotic tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u673a\u5668\u4eba\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u548c\u63a8\u65ad\u6027\u80fd\u4e0b\u964d\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u548c\u5bc6\u96c6\u8df3\u8dc3\u79ef\u5206\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u6cdb\u5316\u80fd\u529b\u65e9\u671f\u9971\u548c\uff0c\u4e14\u589e\u52a0\u79ef\u5206\u6b65\u6570\u4f1a\u964d\u4f4e\u7b56\u7565\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u79ef\u5206\u6b65\u6570\u8bbe\u8ba1\u4e0e\u573a\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u8bad\u7ec3\u7b56\u7565\u4ee5\u52a0\u5f3a\u65e9\u671f\u548c\u665a\u671f\u65f6\u95f4\u9636\u6bb5\u7684\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u5bc6\u96c6\u8df3\u8dc3\u79ef\u5206\u7b56\u7565\u907f\u514d\u4e0d\u7a33\u5b9a\u533a\u57df\u3002", "result": "\u65b0\u7b56\u7565\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u534723.7%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u65f6\u95f4\u8c03\u5ea6\u548c\u79ef\u5206\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6d41\u5339\u914d\u6846\u67b6\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.14186", "pdf": "https://arxiv.org/pdf/2509.14186", "abs": "https://arxiv.org/abs/2509.14186", "authors": ["Patrick Vincent N. Lubenia", "Taposh Banerjee"], "title": "Quickest Change Detection with Cost-Constrained Experiment Design", "categories": ["eess.SP", "cs.SY", "eess.SY", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "In the classical quickest change detection problem, an observer performs only\none experiment to monitor a stochastic process. This paper considers the case\nwhere, at each observation time, the decision-maker needs to choose between\nmultiple experiments with different information qualities and costs. The goal\nis to minimize the worst-case average detection delay subject to false alarm\nand cost constraints. An algorithm called the 2E-CUSUM Algorithm has been\ndeveloped to achieve this goal for the two-experiment case. Extensions to\nmultiple-experiment designs are also studied, and 2E-CUSUM is extended\naccordingly. Data efficiency, where the observer has the choice not to perform\nan experiment, is explored as well. The proposed algorithms are analyzed and\nshown to be asymptotically optimal.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u591a\u5b9e\u9a8c\u9009\u62e9\u4e0b\u7684\u6700\u5feb\u53d8\u5316\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa2E-CUSUM\u7b97\u6cd5\u4ee5\u6700\u5c0f\u5316\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u5e76\u62d3\u5c55\u5230\u591a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8bc1\u660e\u5176\u6e10\u8fd1\u6700\u4f18\u3002", "motivation": "\u4f20\u7edf\u7684\u5feb\u901f\u53d8\u5316\u68c0\u6d4b\u95ee\u9898\u4e2d\u89c2\u5bdf\u8005\u4ec5\u8fdb\u884c\u4e00\u6b21\u5b9e\u9a8c\uff0c\u800c\u73b0\u5b9e\u4e2d\u51b3\u7b56\u8005\u9700\u5728\u6bcf\u6b21\u89c2\u5bdf\u65f6\u9009\u62e9\u4e0d\u540c\u4fe1\u606f\u8d28\u91cf\u548c\u6210\u672c\u7684\u5b9e\u9a8c\uff0c\u9700\u4f18\u5316\u68c0\u6d4b\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa2E-CUSUM\u7b97\u6cd5\u5904\u7406\u53cc\u5b9e\u9a8c\u60c5\u51b5\uff0c\u5e76\u6269\u5c55\u5230\u591a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u540c\u65f6\u63a2\u7d22\u6570\u636e\u6548\u7387\uff08\u53ef\u9009\u62e9\u4e0d\u5b9e\u9a8c\uff09\u3002", "result": "\u7b97\u6cd5\u5728\u6ee1\u8db3\u8bef\u62a5\u548c\u6210\u672c\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u6700\u574f\u60c5\u51b5\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u5e76\u8bc1\u660e\u6e10\u8fd1\u6700\u4f18\u6027\u3002", "conclusion": "2E-CUSUM\u53ca\u5176\u6269\u5c55\u7b97\u6cd5\u5728\u591a\u5b9e\u9a8c\u9009\u62e9\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u68c0\u6d4b\u5ef6\u8fdf\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.13579", "pdf": "https://arxiv.org/pdf/2509.13579", "abs": "https://arxiv.org/abs/2509.13579", "authors": ["Momchil S. Tomov", "Sang Uk Lee", "Hansford Hendrago", "Jinwook Huh", "Teawon Han", "Forbes Howington", "Rafael da Silva", "Gianmarco Bernasconi", "Marc Heim", "Samuel Findler", "Xiaonan Ji", "Alexander Boule", "Michael Napoli", "Kuo Chen", "Jesse Miller", "Boaz Floor", "Yunqing Hu"], "title": "TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.", "AI": {"tldr": "TreeIRL\u662f\u4e00\u79cd\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u6a21\u62df\u548c\u73b0\u5b9e\u9a7e\u9a76\u3002\u5b83\u901a\u8fc7MCTS\u751f\u6210\u5b89\u5168\u5019\u9009\u8f68\u8ff9\uff0c\u518d\u901a\u8fc7\u6df1\u5ea6IRL\u8bc4\u5206\u9009\u62e9\u6700\u63a5\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u7684\u65b9\u6848\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u6837\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u590d\u6742\u6027\u548c\u5b89\u5168\u95ee\u9898\uff0c\u7814\u7a76\u7ed3\u5408\u4e86\u7ecf\u5178\u7684MCTS\u548c\u57fa\u4e8e\u5b66\u4e60\u7684IRL\uff0c\u65e8\u5728\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u63a5\u8fd1\u4eba\u7c7b\u9a7e\u9a76\u7684\u884c\u4e3a\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8eMCTS\u751f\u6210\u5b89\u5168\u5019\u9009\u8f68\u8ff9\uff0c\u7ed3\u5408\u6df1\u5ea6IRL\u8bc4\u5206\u9009\u62e9\u6700\u4f18\u65b9\u6848\u3002", "result": "\u5728\u6a21\u62df\u548c\u62c9\u65af\u7ef4\u52a0\u65af500+\u82f1\u91cc\u5b9e\u9645\u9a7e\u9a76\u6d4b\u8bd5\u4e2d\uff0cTreeIRL\u5728\u5b89\u5168\u3001\u8fdb\u5ea6\u3001\u8212\u9002\u6027\u548c\u4eba\u7c7b\u76f8\u4f3c\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86MCTS\u5728\u516c\u5171\u9053\u8def\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u7ecf\u5178\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14201", "pdf": "https://arxiv.org/pdf/2509.14201", "abs": "https://arxiv.org/abs/2509.14201", "authors": ["Guangjin Pan", "Liping Bai", "Zhuojun Tian", "Hui Chen", "Mehdi Bennis", "Henk Wymeersch"], "title": "Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems", "categories": ["eess.SP", "cs.NI", "cs.SY", "eess.SY"], "comment": "5 pages, 2 figures", "summary": "Integrated sensing and communication (ISAC) is a core technology for 6G, and\nits application to closed-loop sensing, communication, and control (SCC)\nenables various services. Existing SCC solutions often treat sensing and\ncontrol separately, leading to suboptimal performance and resource usage. In\nthis work, we introduce the active inference framework (AIF) into SCC-enabled\nunmanned aerial vehicle (UAV) systems for joint state estimation, control, and\nsensing resource allocation. By formulating a unified generative model, the\nproblem reduces to minimizing variational free energy for inference and\nexpected free energy for action planning. Simulation results show that both\ncontrol cost and sensing cost are reduced relative to baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u4e0e\u63a7\u5236\uff08SCC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u6a21\u578b\u4f18\u5316\u72b6\u6001\u4f30\u8ba1\u3001\u63a7\u5236\u548c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u73b0\u6709SCC\u65b9\u6848\u5e38\u5c06\u611f\u77e5\u4e0e\u63a7\u5236\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0e\u8d44\u6e90\u4f7f\u7528\u6b20\u4f73\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7AIF\u6846\u67b6\u5b9e\u73b0\u8054\u5408\u4f18\u5316\u3002", "method": "\u5f15\u5165\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u6a21\u578b\u6700\u5c0f\u5316\u53d8\u5206\u81ea\u7531\u80fd\uff08\u63a8\u65ad\uff09\u548c\u671f\u671b\u81ea\u7531\u80fd\uff08\u52a8\u4f5c\u89c4\u5212\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a7\u5236\u6210\u672c\u548c\u611f\u77e5\u6210\u672c\u5747\u6709\u6240\u964d\u4f4e\u3002", "conclusion": "AIF\u6846\u67b6\u5728SCC\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a6G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.13591", "pdf": "https://arxiv.org/pdf/2509.13591", "abs": "https://arxiv.org/abs/2509.13591", "authors": ["Amir-Hossein Shahidzadeh", "Jiyue Zhu", "Kezhou Chen", "Sha Yi", "Cornelia Ferm\u00fcller", "Yiannis Aloimonos", "Xiaolong Wang"], "title": "Object Pose Estimation through Dexterous Touch", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c6\u89c9\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4f30\u8ba1\u7269\u4f53\u4f4d\u59ff\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u7269\u4f53\u4f4d\u59ff\u7684\u9c81\u68d2\u4f30\u8ba1\u662f\u5173\u952e\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u6570\u636e\u53d7\u9650\u6216\u6613\u53d7\u5149\u7167\u3001\u906e\u6321\u548c\u5916\u89c2\u5f71\u54cd\u65f6\u3002\u89e6\u89c9\u4f20\u611f\u5668\u63d0\u4f9b\u7684\u4fe1\u606f\u6709\u9650\u4e14\u5c40\u90e8\uff0c\u96be\u4ee5\u4ece\u90e8\u5206\u6570\u636e\u91cd\u5efa\u4f4d\u59ff\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e3b\u52a8\u63a7\u5236\u673a\u5668\u4eba\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\uff0c\u6536\u96c6\u89e6\u89c9\u6570\u636e\u5e76\u751f\u62103D\u70b9\u4e91\uff0c\u8fed\u4ee3\u4f18\u5316\u7269\u4f53\u5f62\u72b6\u548c\u4f4d\u59ff\u3002\u5b9e\u9a8c\u91c7\u7528\u53cc\u624b\u534f\u4f5c\uff0c\u4e00\u53ea\u624b\u56fa\u5b9a\u7269\u4f53\uff0c\u53e6\u4e00\u53ea\u624b\u4e3b\u52a8\u63a2\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u65e0\u7269\u4f53\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u7269\u4f53\u8868\u9762\u8bc6\u522b\u5173\u952e\u4f4d\u59ff\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u4f4d\u59ff\u7684\u6709\u6548\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2509.14217", "pdf": "https://arxiv.org/pdf/2509.14217", "abs": "https://arxiv.org/abs/2509.14217", "authors": ["Andriy Enttsel", "Weichen Wang", "Mauro Mangia", "Riccardo Rovatti", "Deniz G\u00fcnd\u00fcz"], "title": "Goal-Oriented Joint Source-Channel Coding: Distortion-Classification-Power Trade-off", "categories": ["eess.SP"], "comment": "13 pages, 3 figures", "summary": "Joint source-channel coding is a compelling paradigm when low-latency and\nlow-complexity communication is required. This work proposes a theoretical\nframework that integrates classification and anomaly detection within the\nconventional signal reconstruction objective. Assuming a Gaussian scalar source\nand constraining the encoder to piecewise linear mappings, we derive tractable\ndesign rules and explicitly characterize the trade-offs between distortion,\nclassification error, and transmission power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u878d\u5165\u4fe1\u53f7\u91cd\u5efa\u76ee\u6807\u4e2d\uff0c\u9002\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u590d\u6742\u5ea6\u901a\u4fe1\u573a\u666f\u3002", "motivation": "\u5728\u9700\u8981\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u590d\u6742\u5ea6\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\uff0c\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u662f\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u529f\u80fd\u6574\u5408\u5230\u4f20\u7edf\u7684\u4fe1\u53f7\u91cd\u5efa\u6846\u67b6\u4e2d\u3002", "method": "\u5047\u8bbe\u9ad8\u65af\u6807\u91cf\u6e90\uff0c\u5e76\u9650\u5236\u7f16\u7801\u5668\u4e3a\u5206\u6bb5\u7ebf\u6027\u6620\u5c04\uff0c\u63a8\u5bfc\u51fa\u53ef\u5904\u7406\u7684\u8bbe\u8ba1\u89c4\u5219\uff0c\u660e\u786e\u8868\u5f81\u5931\u771f\u3001\u5206\u7c7b\u8bef\u5dee\u548c\u4f20\u8f93\u529f\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u4fe1\u53f7\u91cd\u5efa\u3001\u5206\u7c7b\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u590d\u6742\u5ea6\u901a\u4fe1\u4e2d\u7684\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.13595", "pdf": "https://arxiv.org/pdf/2509.13595", "abs": "https://arxiv.org/abs/2509.13595", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Leg-Arm Coordinated Operation for Curtain Wall Installation", "categories": ["cs.RO"], "comment": null, "summary": "With the acceleration of urbanization, the number of high-rise buildings and\nlarge public facilities is increasing, making curtain walls an essential\ncomponent of modern architecture with widespread applications. Traditional\ncurtain wall installation methods face challenges such as variable on-site\nterrain, high labor intensity, low construction efficiency, and significant\nsafety risks. Large panels often require multiple workers to complete\ninstallation. To address these issues, based on a hexapod curtain wall\ninstallation robot, we design a hierarchical optimization-based whole-body\ncontrol framework for coordinated arm-leg planning tailored to three key tasks:\nwall installation, ceiling installation, and floor laying. This framework\nintegrates the motion of the hexapod legs with the operation of the folding arm\nand the serial-parallel manipulator. We conduct experiments on the hexapod\ncurtain wall installation robot to validate the proposed control method,\ndemonstrating its capability in performing curtain wall installation tasks. Our\nresults confirm the effectiveness of the hierarchical optimization-based\narm-leg coordination framework for the hexapod robot, laying the foundation for\nits further application in complex construction site environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u4f18\u5316\u7684\u516d\u8db3\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u5e55\u5899\u5b89\u88c5\u65b9\u6cd5\u7684\u52b3\u52a8\u5f3a\u5ea6\u5927\u3001\u6548\u7387\u4f4e\u548c\u5b89\u5168\u98ce\u9669\u9ad8\u7b49\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u5316\u52a0\u901f\u5bfc\u81f4\u9ad8\u5c42\u5efa\u7b51\u548c\u5927\u516c\u5171\u8bbe\u65bd\u589e\u591a\uff0c\u4f20\u7edf\u5e55\u5899\u5b89\u88c5\u65b9\u6cd5\u9762\u4e34\u5730\u5f62\u591a\u53d8\u3001\u6548\u7387\u4f4e\u548c\u5b89\u5168\u98ce\u9669\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u516d\u8db3\u673a\u5668\u4eba\u5206\u5c42\u4f18\u5316\u7684\u5168\u8eab\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u817f\u90e8\u8fd0\u52a8\u4e0e\u6298\u53e0\u81c2\u548c\u4e32\u5e76\u8054\u673a\u68b0\u81c2\u64cd\u4f5c\uff0c\u89e3\u51b3\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u63a7\u5236\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u516d\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5b8c\u6210\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u3002", "conclusion": "\u5206\u5c42\u4f18\u5316\u7684\u81c2\u817f\u534f\u8c03\u6846\u67b6\u4e3a\u516d\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u65bd\u5de5\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.13661", "pdf": "https://arxiv.org/pdf/2509.13661", "abs": "https://arxiv.org/abs/2509.13661", "authors": ["Kareem M. Attiah", "Wei Yu"], "title": "Uplink-Downlink Duality for Beamforming in Integrated Sensing and Communications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "18 pages, 5 figures, submitted to an IEEE journal for possible\n  publication", "summary": "This paper considers the beamforming and power optimization problem for a\nclass of integrated sensing and communications (ISAC) problems that utilize the\ncommunication signals simultaneously for sensing. We formulate the problem of\nminimizing the Bayesian Cram\\'er-Rao bound (BCRB) on the mean-squared error of\nestimating a vector of parameters, while satisfying downlink\nsignal-to-interference-and-noise-ratio constraints for a set of communication\nusers at the same time. The proposed optimization framework comprises two key\nnew ingredients. First, we show that the BCRB minimization problem corresponds\nto maximizing beamforming power along certain sensing directions of interest.\nSecond, the classical uplink-downlink duality for multiple-input\nmultiple-output communications can be extended to the ISAC setting, but unlike\nthe classical communication problem, the dual uplink problem for ISAC may\nentail negative noise power and needs to include an extra condition on the\nuplink beamformers. This new duality theory opens doors for an efficient\niterative algorithm for optimizing power and beamformers for ISAC.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u901a\u4fe1\u4fe1\u53f7\u540c\u65f6\u8fdb\u884c\u4f20\u611f\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u95ee\u9898\u4e2d\u7684\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u514b\u62c9\u7f8e-\u7f57\u754c\uff08BCRB\uff09\u7684\u6846\u67b6\uff0c\u540c\u65f6\u6ee1\u8db3\u901a\u4fe1\u7528\u6237\u7684\u4fe1\u53f7\u5e72\u6270\u52a0\u566a\u58f0\u6bd4\u7ea6\u675f\u3002", "motivation": "\u89e3\u51b3ISAC\u7cfb\u7edf\u4e2d\u5982\u4f55\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u5206\u914d\uff0c\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f20\u611f\u548c\u9ad8\u6548\u901a\u4fe1\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eBCRB\u6700\u5c0f\u5316\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6cbf\u7279\u5b9a\u4f20\u611f\u65b9\u5411\u7684\u6ce2\u675f\u6210\u5f62\u529f\u7387\uff0c\u5e76\u6269\u5c55\u7ecf\u5178\u7684\u4e0a\u884c-\u4e0b\u884c\u5bf9\u5076\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0ISAC\u7684\u53cc\u4e0a\u884c\u95ee\u9898\u53ef\u80fd\u6d89\u53ca\u8d1f\u566a\u58f0\u529f\u7387\uff0c\u5e76\u63d0\u51fa\u4e0a\u884c\u6ce2\u675f\u6210\u5f62\u5668\u7684\u989d\u5916\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u6ce2\u675f\u6210\u5f62\u548c\u529f\u7387\u7684\u8054\u5408\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u5076\u7406\u8bba\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u611f\u548c\u901a\u4fe1\u7684\u8054\u5408\u6027\u80fd\u3002"}}
{"id": "2509.13649", "pdf": "https://arxiv.org/pdf/2509.13649", "abs": "https://arxiv.org/abs/2509.13649", "authors": ["M\u00e9lon\u00e9 Nyoba Tchonkeu", "Soulaimane Berkane", "Tarek Hamel"], "title": "Barometer-Aided Attitude Estimation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "6 pages, 4 figures. this manuscript is submitted to IEEE Control\n  Systems Letters (L-CSS) with American Control Conference (ACC) option", "summary": "Accurate and robust attitude estimation is a central challenge for autonomous\nvehicles operating in GNSS-denied or highly dynamic environments. In such\ncases, Inertial Measurement Units (IMUs) alone are insufficient for reliable\ntilt estimation due to the ambiguity between gravitational and inertial\naccelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,\nDoppler radar, or visual odometry, are often used, they can be unavailable,\nintermittent, or costly. This work introduces a barometer-aided attitude\nestimation architecture that leverages barometric altitude measurements to\ninfer vertical velocity and attitude within a nonlinear observer on SO(3). The\ndesign cascades a deterministic Riccati observer with a complementary filter,\nensuring Almost Global Asymptotic Stability (AGAS) under a uniform\nobservability condition while maintaining geometric consistency. The analysis\nhighlights barometer-aided estimation as a lightweight and effective\ncomplementary modality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u538b\u8ba1\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u6216\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u9ad8\u81ea\u4e3b\u8f66\u8f86\u7684\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u6216\u9ad8\u5ea6\u52a8\u6001\u7684\u73af\u5883\u4e2d\uff0c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMUs\uff09\u7684\u59ff\u6001\u4f30\u8ba1\u56e0\u91cd\u529b\u4e0e\u60ef\u6027\u52a0\u901f\u5ea6\u7684\u6a21\u7cca\u6027\u800c\u4e0d\u591f\u53ef\u9760\u3002\u8f85\u52a9\u4f20\u611f\u5668\u6216\u6210\u672c\u9ad8\u6216\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6c14\u538b\u8ba1\u8f85\u52a9\u7684\u975e\u7ebf\u6027\u89c2\u6d4b\u5668\u8bbe\u8ba1\uff0c\u7ed3\u5408\u786e\u5b9a\u6027Riccati\u89c2\u6d4b\u5668\u548c\u4e92\u8865\u6ee4\u6ce2\u5668\uff0c\u786e\u4fdd\u5728\u5747\u5300\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u51e0\u4e4e\u5168\u5c40\u6e10\u8fdb\u7a33\u5b9a\u6027\uff08AGAS\uff09\u3002", "result": "\u6c14\u538b\u8ba1\u8f85\u52a9\u7684\u59ff\u6001\u4f30\u8ba1\u88ab\u8bc1\u660e\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u8865\u5145\u65b9\u5f0f\uff0c\u80fd\u591f\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aGNSS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13955", "pdf": "https://arxiv.org/pdf/2509.13955", "abs": "https://arxiv.org/abs/2509.13955", "authors": ["Zheyu Wu", "Junjie Ma", "Ya-Feng Liu", "Bruno Clerckx"], "title": "Asymptotic Analysis of Nonlinear One-Bit Precoding in Massive MIMO Systems via Approximate Message Passing", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "39 pages, 6 figures, submitted for possible publication", "summary": "Massive multiple-input multiple-output (MIMO) systems employing one-bit\ndigital-to-analog converters offer a hardware-efficient solution for wireless\ncommunications. However, the one-bit constraint poses significant challenges\nfor precoding design, as it transforms the problem into a discrete and\nnonconvex optimization task. In this paper, we investigate a widely adopted\n``convex-relaxation-then-quantization\" approach for nonlinear symbol-level\none-bit precoding. Specifically, we first solve a convex relaxation of the\ndiscrete minimum mean square error precoding problem, and then quantize the\nsolution to satisfy the one-bit constraint. To analyze the high-dimensional\nasymptotic performance of this scheme, we develop a novel analytical framework\nbased on approximate message passing (AMP). This framework enables us to derive\na closed-form expression for the symbol error probability (SEP) at the receiver\nside in the large-system limit, which provides a quantitative characterization\nof how model and system parameters affect the SEP performance. Our empirical\nresults suggest that the $\\ell_\\infty^2$ regularizer, when paired with an\noptimally chosen regularization parameter, achieves optimal SEP performance\nwithin a broad class of convex regularization functions. As a first step\ntowards a theoretical justification, we prove the optimality of the\n$\\ell_\\infty^2$ regularizer within the mixed $\\ell_\\infty^2$-$\\ell_2^2$\nregularization functions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u677e\u5f1b\u548c\u91cf\u5316\u7684\u975e\u7ebf\u6027\u7b26\u53f7\u7ea7\u4e00\u4f4d\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\u6846\u67b6\u5206\u6790\u4e86\u5176\u9ad8\u7ef4\u6e10\u8fd1\u6027\u80fd\uff0c\u5e76\u63a8\u5bfc\u4e86\u7b26\u53f7\u9519\u8bef\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "motivation": "\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u91c7\u7528\u4e00\u4f4d\u6570\u6a21\u8f6c\u6362\u5668\u53ef\u5b9e\u73b0\u786c\u4ef6\u9ad8\u6548\u7684\u65e0\u7ebf\u901a\u4fe1\uff0c\u4f46\u4e00\u4f4d\u7ea6\u675f\u4f7f\u9884\u7f16\u7801\u8bbe\u8ba1\u6210\u4e3a\u79bb\u6563\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u9700\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\"\u51f8\u677e\u5f1b\u540e\u91cf\u5316\"\u65b9\u6cd5\uff0c\u9996\u5148\u6c42\u89e3\u79bb\u6563\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u9884\u7f16\u7801\u95ee\u9898\u7684\u51f8\u677e\u5f1b\uff0c\u518d\u91cf\u5316\u89e3\u4ee5\u6ee1\u8db3\u4e00\u4f4d\u7ea6\u675f\uff1b\u57fa\u4e8eAMP\u6846\u67b6\u5206\u6790\u9ad8\u7ef4\u6e10\u8fd1\u6027\u80fd\u3002", "result": "\u63a8\u5bfc\u4e86\u5927\u7cfb\u7edf\u6781\u9650\u4e0b\u63a5\u6536\u7aef\u7b26\u53f7\u9519\u8bef\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u53d1\u73b0$\backslash ell_\backslash infty^2$\u6b63\u5219\u5668\u5728\u6700\u4f18\u53c2\u6570\u4e0b\u53ef\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "$\backslash ell_\backslash infty^2$\u6b63\u5219\u5668\u5728\u6df7\u5408\u6b63\u5219\u51fd\u6570\u7c7b\u4e2d\u6700\u4f18\uff0c\u4e3a\u7406\u8bba\u652f\u6301\u63d0\u4f9b\u4e86\u521d\u6b65\u8bc1\u660e\u3002"}}
{"id": "2509.13666", "pdf": "https://arxiv.org/pdf/2509.13666", "abs": "https://arxiv.org/abs/2509.13666", "authors": ["Zhenqi Wu", "Abhinav Modi", "Angelos Mavrogiannis", "Kaustubh Joshi", "Nikhil Chopra", "Yiannis Aloimonos", "Nare Karapetyan", "Ioannis Rekleitis", "Xiaomin Lin"], "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring", "categories": ["cs.RO", "cs.AI"], "comment": "submitted to ICRA 2026", "summary": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.", "AI": {"tldr": "DREAM\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u81ea\u4e3b\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u671f\u6c34\u4e0b\u63a2\u7d22\u548c\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u7269\uff08\u5982\u7261\u86ce\u3001\u6c89\u8239\uff09\u7684\u53d1\u73b0\u548c\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u6d77\u6d0b\u53d8\u6696\u548c\u9178\u5316\u589e\u52a0\u4e86\u5bf9\u6e29\u5ea6\u654f\u611f\u7684\u8d1d\u7c7b\uff08\u5982\u7261\u86ce\uff09\u5927\u89c4\u6a21\u6b7b\u4ea1\u7684\u98ce\u9669\uff0c\u9700\u8981\u957f\u671f\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f46\u4eba\u5de5\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u66f4\u5b89\u5168\u9ad8\u6548\u3002", "method": "\u63d0\u51faDREAM\u6846\u67b6\uff0c\u5229\u7528VLM\u6307\u5bfc\u6c34\u4e0b\u673a\u5668\u4eba\u81ea\u4e3b\u51b3\u7b56\uff0c\u5b9e\u73b0\u65e0\u4eba\u5de5\u5e72\u9884\u7684\u5b9e\u65f6\u73af\u5883\u611f\u77e5\u3002", "result": "\u5728\u7261\u86ce\u76d1\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6846\u67b6\u6bd4\u57fa\u7ebf\u8282\u770131.5%\u65f6\u95f4\uff1b\u6bd4\u666e\u901aVLM\u5c11\u752823%\u6b65\u9aa4\uff0c\u8986\u76d6\u8303\u56f4\u589e\u52a08.88%\u3002\u5728\u6c89\u8239\u573a\u666f\u4e2d\uff0c\u6846\u67b6\u5b9e\u73b0100%\u8986\u76d6\u7387\uff0c\u666e\u901aVLM\u4ec5\u4e3a60.23%\u3002", "conclusion": "DREAM\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u76d1\u6d4b\u7684\u6548\u7387\u548c\u8986\u76d6\u7387\uff0c\u4e3a\u957f\u671f\u3001\u5927\u9762\u79ef\u3001\u4f4e\u6210\u672c\u7684\u6d77\u5e95\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.13974", "pdf": "https://arxiv.org/pdf/2509.13974", "abs": "https://arxiv.org/abs/2509.13974", "authors": ["Amirhossein Shahbazinia", "Jonathan Dan", "Jose A. Miranda", "Giovanni Ansaloni", "David Atienza"], "title": "Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Objective: Epilepsy, a prevalent neurological disease, demands careful\ndiagnosis and continuous care. Seizure detection remains challenging, as\ncurrent clinical practice relies on expert analysis of electroencephalography,\nwhich is a time-consuming process and requires specialized knowledge.\nAddressing this challenge, this paper explores automated epileptic seizure\ndetection using deep learning, focusing on personalized continual learning\nmodels that adapt to each patient's unique electroencephalography signal\nfeatures, which evolve over time. Methods: In this context, our approach\naddresses the challenge of integrating new data into existing models without\ncatastrophic forgetting, a common issue in static deep learning models. We\npropose EpiSMART, a continual learning framework for seizure detection that\nuses a size-constrained replay buffer and an informed sample selection strategy\nto incrementally adapt to patient-specific electroencephalography signals. By\nselectively retaining high-entropy and seizure-predicted samples, our method\npreserves critical past information while maintaining high performance with\nminimal memory and computational requirements. Results: Validation on the\nCHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score\nover a trained baseline without updates in all other patients. On average,\nEpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,\nmaking it suitable for real-time deployment in wearable systems.\nConclusion:EpiSMART enables robust and personalized seizure detection under\nrealistic and resource-constrained conditions by effectively integrating new\ndata into existing models without degrading past knowledge. Significance: This\nframework advances automated seizure detection by providing a continual\nlearning approach that supports patient-specific adaptation and practical\ndeployment in wearable healthcare systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpiSMART\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u51cf\u5c11\u5bf9\u4e13\u5bb6\u5206\u6790\u548c\u9759\u6001\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u9002\u5e94\u60a3\u8005\u4e2a\u6027\u5316\u7684\u8111\u7535\u56fe\u4fe1\u53f7\u53d8\u5316\u3002", "method": "\u91c7\u7528\u5e26\u7ea6\u675f\u7684\u91cd\u653e\u7f13\u51b2\u533a\u548c\u667a\u80fd\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u9010\u6b65\u9002\u5e94\u60a3\u8005\u7279\u5f02\u6027\u8111\u7535\u56fe\u4fe1\u53f7\u3002", "result": "\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\uff0cEpiSMART\u7684F1\u5206\u6570\u6bd4\u57fa\u7ebf\u63d0\u534721%\uff0c\u4e14\u6bcf\u65e5\u4ec5\u97006.46\u5206\u949f\u6807\u8bb0\u6570\u636e\u548c6.28\u6b21\u66f4\u65b0\uff0c\u9002\u5408\u53ef\u7a7f\u6234\u7cfb\u7edf\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "EpiSMART\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u766b\u75eb\u68c0\u6d4b\uff0c\u6709\u6548\u6574\u5408\u65b0\u6570\u636e\u4e14\u4e0d\u7834\u574f\u5386\u53f2\u77e5\u8bc6\u3002"}}
{"id": "2509.13691", "pdf": "https://arxiv.org/pdf/2509.13691", "abs": "https://arxiv.org/abs/2509.13691", "authors": ["Songhao Huang", "Yuwei Wu", "Guangyao Shi", "Gaurav S. Sukhatme", "Vijay Kumar"], "title": "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics", "categories": ["cs.RO"], "comment": null, "summary": "We investigate the problem of automatic domain generation for the Planning\nDomain Definition Language (PDDL) using Large Language Models (LLMs), with a\nparticular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a\nwidely adopted standard in robotic planning, manually designing domains for\ndiverse applications such as surveillance, delivery, and inspection is\nlabor-intensive and error-prone, which hinders adoption and real-world\ndeployment. To address these challenges, we propose SPAR, a framework that\nleverages the generative capabilities of LLMs to automatically produce valid,\ndiverse, and semantically accurate PDDL domains from natural language input. To\nthis end, we first introduce a systematically formulated and validated UAV\nplanning dataset, consisting of ground-truth PDDL domains and associated\nproblems, each paired with detailed domain and action descriptions. Building on\nthis dataset, we design a prompting framework that generates high-quality PDDL\ndomains from language input. The generated domains are evaluated through syntax\nvalidation, executability, feasibility, and interpretability. Overall, this\nwork demonstrates that LLMs can substantially accelerate the creation of\ncomplex planning domains, providing a reproducible dataset and evaluation\npipeline that enables application experts without prior experience to leverage\nit for practical tasks and advance future research in aerial robotics and\nautomated planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u81ea\u52a8\u751f\u6210\u89c4\u5212\u9886\u57df\u5b9a\u4e49\u8bed\u8a00(PDDL)\u7684\u6846\u67b6SPAR\uff0c\u4e13\u6ce8\u4e8e\u65e0\u4eba\u673a(UAV)\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cfPDDL\u9886\u57df\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u8bbe\u8ba1\u590d\u6742\u95ee\u9898\u3002", "motivation": "PDDL\u662f\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7684\u901a\u7528\u8bed\u8a00\uff0c\u4f46\u624b\u52a8\u8bbe\u8ba1\u9886\u57df\u65e2\u8017\u65f6\u53c8\u6613\u51fa\u9519\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u548c\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u9a8c\u8bc1\u7684UAV\u89c4\u5212\u6570\u636e\u96c6\uff0c\u7ed3\u5408LLMs\u751f\u6210\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d0\u793a\u6846\u67b6\u6765\u81ea\u52a8\u751f\u6210PDDL\u9886\u57df\u3002", "result": "\u751f\u6210\u7684\u9886\u57df\u901a\u8fc7\u8bed\u6cd5\u9a8c\u8bc1\u3001\u53ef\u6267\u884c\u6027\u3001\u53ef\u884c\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86LLMs\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u590d\u6742\u89c4\u5212\u9886\u57df\u7684\u521b\u5efa\u3002", "conclusion": "SPAR\u6846\u67b6\u4e3a\u65e0\u7ecf\u9a8c\u7684\u4e13\u5bb6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u65e0\u4eba\u673a\u4efb\u52a1\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.14029", "pdf": "https://arxiv.org/pdf/2509.14029", "abs": "https://arxiv.org/abs/2509.14029", "authors": ["Samuel Tovey", "Julian Ho\u00dfbach", "Sandro Kuppel", "Tobias Ensslen", "Jan C. Behrends", "Christian Holm"], "title": "Deep Learning-Driven Peptide Classification in Biological Nanopores", "categories": ["cs.LG", "eess.SP", "physics.comp-ph", "q-bio.BM"], "comment": "29 pages (incl. references) 7 figures", "summary": "A device capable of performing real time classification of proteins in a\nclinical setting would allow for inexpensive and rapid disease diagnosis. One\nsuch candidate for this technology are nanopore devices. These devices work by\nmeasuring a current signal that arises when a protein or peptide enters a\nnanometer-length-scale pore. Should this current be uniquely related to the\nstructure of the peptide and its interactions with the pore, the signals can be\nused to perform identification. While such a method would allow for real time\nidentification of peptides and proteins in a clinical setting, to date, the\ncomplexities of these signals limit their accuracy. In this work, we tackle the\nissue of classification by converting the current signals into scaleogram\nimages via wavelet transforms, capturing amplitude, frequency, and time\ninformation in a modality well-suited to machine learning algorithms. When\ntested on 42 peptides, our method achieved a classification accuracy of\n~$81\\,\\%$, setting a new state-of-the-art in the field and taking a step toward\npractical peptide/protein diagnostics at the point of care. In addition, we\ndemonstrate model transfer techniques that will be critical when deploying\nthese models into real hardware, paving the way to a new method for real-time\ndisease diagnosis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5c06\u7eb3\u7c73\u5b54\u7535\u6d41\u4fe1\u53f7\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u80bd\u548c\u86cb\u767d\u8d28\u7684\u5b9e\u65f6\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u65b0\u6280\u672f\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u65f6\u5206\u7c7b\u86cb\u767d\u8d28\u7684\u8bbe\u5907\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u5feb\u901f\u7684\u75be\u75c5\u8bca\u65ad\uff0c\u7eb3\u7c73\u5b54\u6280\u672f\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u6f5c\u5728\u5019\u9009\u65b9\u6848\u3002", "method": "\u5c06\u7eb3\u7c73\u5b54\u7535\u6d41\u4fe1\u53f7\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u4ece\u800c\u6355\u6349\u632f\u5e45\u3001\u9891\u7387\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u572842\u79cd\u80bd\u7684\u6d4b\u8bd5\u4e2d\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u5230\u7ea681%\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u8fc1\u79fb\u6280\u672f\uff0c\u4e3a\u5b9e\u9645\u786c\u4ef6\u90e8\u7f72\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u80bd\u548c\u86cb\u767d\u8d28\u5b9e\u65f6\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13692", "pdf": "https://arxiv.org/pdf/2509.13692", "abs": "https://arxiv.org/abs/2509.13692", "authors": ["Yadan Zeng", "Jiadong Zhou", "Xiaohan Li", "I-Ming Chen"], "title": "HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "Point cloud completion is essential for robotic perception, object\nreconstruction and supporting downstream tasks like grasp planning, obstacle\navoidance, and manipulation. However, incomplete geometry caused by\nself-occlusion and sensor limitations can significantly degrade downstream\nreasoning and interaction. To address these challenges, we propose HGACNet, a\nnovel framework that reconstructs complete point clouds of individual objects\nby hierarchically encoding 3D geometric features and fusing them with\nimage-guided priors from a single-view RGB image. At the core of our approach,\nthe Hierarchical Graph Attention (HGA) encoder adaptively selects critical\nlocal points through graph attention-based downsampling and progressively\nrefines hierarchical geometric features to better capture structural continuity\nand spatial relationships. To strengthen cross-modal interaction, we further\ndesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performs\nattention-based feature alignment between hierarchical geometric features and\nstructured visual representations, enabling fine-grained semantic guidance for\ncompletion. In addition, we proposed the contrastive loss (C-Loss) to\nexplicitly align the feature distributions across modalities, improving\ncompletion fidelity under modality discrepancy. Finally, extensive experiments\nconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset\nconfirm the effectiveness of HGACNet, demonstrating state-of-the-art\nperformance as well as strong applicability in real-world robotic manipulation\ntasks.", "AI": {"tldr": "HGACNet\u662f\u4e00\u79cd\u7528\u4e8e\u70b9\u4e91\u8865\u5168\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u78013D\u51e0\u4f55\u7279\u5f81\u5e76\u4e0e\u5355\u89c6RGB\u56fe\u50cf\u7684\u56fe\u50cf\u5f15\u5bfc\u5148\u9a8c\u878d\u5408\uff0c\u5b9e\u73b0\u5bf9\u5355\u4e2a\u7269\u4f53\u5b8c\u6574\u70b9\u4e91\u7684\u91cd\u5efa\u3002", "motivation": "\u70b9\u4e91\u8865\u5168\u5bf9\u4e8e\u673a\u5668\u4eba\u611f\u77e5\u3001\u7269\u4f53\u91cd\u5efa\u4ee5\u53ca\u652f\u6301\u6293\u53d6\u89c4\u5212\u3001\u907f\u969c\u548c\u64cd\u4f5c\u7b49\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u81ea\u906e\u6321\u548c\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u51e0\u4f55\u4f1a\u663e\u8457\u964d\u4f4e\u8fd9\u4e9b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faHGACNet\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\u5206\u5c42\u56fe\u6ce8\u610f\u529b\uff08HGA\uff09\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7684\u4e0b\u91c7\u6837\u81ea\u9002\u5e94\u9009\u62e9\u5173\u952e\u5c40\u90e8\u70b9\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff08MSCF\uff09\u5bf9\u9f50\u51e0\u4f55\u7279\u5f81\u4e0e\u89c6\u89c9\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u5bf9\u6bd4\u635f\u5931\uff08C-Loss\uff09\u4ee5\u663e\u5f0f\u5bf9\u9f50\u8de8\u6a21\u6001\u7279\u5f81\u5206\u5e03\u3002", "result": "\u5728ShapeNet-ViPC\u548cYCB-Complete\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGACNet\u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u9002\u7528\u6027\u3002", "conclusion": "HGACNet\u901a\u8fc7\u5206\u5c42\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.14052", "pdf": "https://arxiv.org/pdf/2509.14052", "abs": "https://arxiv.org/abs/2509.14052", "authors": ["Junan Zhang", "Yunjia Zhang", "Xueyao Zhang", "Zhizheng Wu"], "title": "AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic Bottleneck", "categories": ["cs.SD", "eess.SP"], "comment": "Demo audio and code: https://anyaccomp.github.io", "summary": "Singing Accompaniment Generation (SAG) is the process of generating\ninstrumental music for a given clean vocal input. However, existing SAG\ntechniques use source-separated vocals as input and overfit to separation\nartifacts. This creates a critical train-test mismatch, leading to failure on\nclean, real-world vocal inputs. We introduce AnyAccomp, a framework that\nresolves this by decoupling accompaniment generation from source-dependent\nartifacts. AnyAccomp first employs a quantized melodic bottleneck, using a\nchromagram and a VQ-VAE to extract a discrete and timbre-invariant\nrepresentation of the core melody. A subsequent flow-matching model then\ngenerates the accompaniment conditioned on these robust codes. Experiments show\nAnyAccomp achieves competitive performance on separated-vocal benchmarks while\nsignificantly outperforming baselines on generalization test sets of clean\nstudio vocals and, notably, solo instrumental tracks. This demonstrates a\nqualitative leap in generalization, enabling robust accompaniment for\ninstruments - a task where existing models completely fail - and paving the way\nfor more versatile music co-creation tools. Demo audio and code:\nhttps://anyaccomp.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAnyAccomp\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4f34\u594f\u751f\u6210\u4e0e\u6e90\u4f9d\u8d56\u4f2a\u5f71\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6b4c\u5531\u4f34\u594f\u751f\u6210\u6280\u672f\u5bf9\u5206\u79bb\u4f2a\u5f71\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6b4c\u5531\u4f34\u594f\u751f\u6210\u6280\u672f\u4f9d\u8d56\u4e8e\u6e90\u5206\u79bb\u7684\u4eba\u58f0\u8f93\u5165\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u5206\u79bb\u4f2a\u5f71\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u5e72\u51c0\u7684\u58f0\u4e50\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "AnyAccomp\u91c7\u7528\u91cf\u5316\u65cb\u5f8b\u74f6\u9888\uff08\u5982\u8272\u5ea6\u56fe\u548cVQ-VAE\uff09\u63d0\u53d6\u79bb\u6563\u4e14\u97f3\u8272\u4e0d\u53d8\u7684\u6838\u5fc3\u65cb\u5f8b\u8868\u793a\uff0c\u968f\u540e\u901a\u8fc7\u6d41\u5339\u914d\u6a21\u578b\u751f\u6210\u4f34\u594f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnyAccomp\u5728\u5206\u79bb\u4eba\u58f0\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5e72\u51c0\u5f55\u97f3\u5ba4\u4eba\u58f0\u548c\u72ec\u594f\u4e50\u5668\u97f3\u8f68\u7684\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AnyAccomp\u5b9e\u73b0\u4e86\u6cdb\u5316\u7684\u8d28\u7684\u98de\u8dc3\uff0c\u80fd\u591f\u4e3a\u4e50\u5668\u751f\u6210\u4f34\u594f\uff0c\u4e3a\u66f4\u901a\u7528\u7684\u97f3\u4e50\u534f\u540c\u521b\u4f5c\u5de5\u5177\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13720", "pdf": "https://arxiv.org/pdf/2509.13720", "abs": "https://arxiv.org/abs/2509.13720", "authors": ["Tianle Zeng", "Jianwei Peng", "Hanjing Ye", "Guangcheng Chen", "Senzi Luo", "Hong Zhang"], "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility", "categories": ["cs.RO"], "comment": "Page:https://tianlezeng.github.io/EzReal/", "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces\nmany challenges; we specifically address a coupled one: long-range targets that\nreduce to tiny projections and intermittent visibility due to partial or\ncomplete occlusion. We present a unified, lightweight closed-loop system built\non an aligned multi-scale image tile hierarchy. Through hierarchical\ntarget-saliency fusion, it summarizes localized semantic contrast into a stable\ncoarse-layer regional saliency that provides the target direction and indicates\ntarget visibility. This regional saliency supports visibility-aware heading\nmaintenance through keyframe memory, saliency-weighted fusion of historical\nheadings, and active search during temporary invisibility. The system avoids\nwhole-image rescaling, enables deterministic bottom-up aggregation, supports\nzero-shot navigation, and runs efficiently on a mobile robot. Across simulation\nand real-world outdoor trials, the system detects semantic targets beyond 150m,\nmaintains a correct heading through visibility changes with 82.6% probability,\nand improves overall task success by 17.5% compared with the SOTA methods,\ndemonstrating robust ZSON toward distant and intermittently observable targets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\uff08ZSON\uff09\u7684\u957f\u8ddd\u79bb\u76ee\u6807\u548c\u95f4\u6b47\u6027\u53ef\u89c1\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u56fe\u50cf\u5757\u5c42\u6b21\u7ed3\u6784\u548c\u76ee\u6807\u663e\u8457\u6027\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\uff0c\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u9762\u4e34\u957f\u8ddd\u79bb\u76ee\u6807\u7f29\u5c0f\u4e3a\u6781\u5c0f\u6295\u5f71\u4ee5\u53ca\u95f4\u6b47\u6027\u53ef\u89c1\u6027\u7684\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5bf9\u9f50\u7684\u591a\u5c3a\u5ea6\u56fe\u50cf\u5757\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u76ee\u6807\u663e\u8457\u6027\u878d\u5408\uff0c\u5c06\u5c40\u90e8\u8bed\u4e49\u5bf9\u6bd4\u6c47\u603b\u4e3a\u7a33\u5b9a\u7684\u7c97\u5c42\u533a\u57df\u663e\u8457\u6027\uff0c\u4ece\u800c\u63d0\u4f9b\u76ee\u6807\u65b9\u5411\u548c\u53ef\u89c1\u6027\u6307\u793a\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u901a\u8fc7\u5173\u952e\u5e27\u8bb0\u5fc6\u3001\u663e\u8457\u6027\u52a0\u6743\u7684\u5386\u53f2\u822a\u5411\u878d\u5408\u4ee5\u53ca\u5728\u6682\u65f6\u4e0d\u53ef\u89c1\u65f6\u7684\u4e3b\u52a8\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u53ef\u89c1\u6027\u611f\u77e5\u7684\u822a\u5411\u7ef4\u6301\u3002", "result": "\u7cfb\u7edf\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6237\u5916\u8bd5\u9a8c\u4e2d\uff0c\u80fd\u591f\u68c0\u6d4b150\u7c73\u4ee5\u4e0a\u7684\u8bed\u4e49\u76ee\u6807\uff0c\u901a\u8fc7\u53ef\u89c1\u6027\u53d8\u5316\u65f6\u4fdd\u6301\u6b63\u786e\u822a\u5411\u7684\u6982\u7387\u4e3a82.6%\uff0c\u5e76\u5c06\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u4e8617.5%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u5bf9\u8fdc\u8ddd\u79bb\u548c\u95f4\u6b47\u6027\u53ef\u89c1\u76ee\u6807\u7684\u7a33\u5065\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.13731", "pdf": "https://arxiv.org/pdf/2509.13731", "abs": "https://arxiv.org/abs/2509.13731", "authors": ["Jeongwoo Park", "Seabin Lee", "Changmin Park", "Wonjong Lee", "Changjoo Nam"], "title": "Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings", "categories": ["cs.RO"], "comment": null, "summary": "The industrial insertion of flexible flat cables (FFCs) into receptacles\npresents a significant challenge owing to the need for submillimeter precision\nwhen handling the deformable cables. In manufacturing processes, FFC insertion\nwith robotic manipulators often requires laborious human-guided trajectory\ngeneration. While Reinforcement Learning (RL) offers a solution to automate\nthis task without modeling complex properties of FFCs, the nondeterminism\ncaused by the deformability of FFCs requires significant efforts and time on\ntraining. Moreover, training directly in a real environment is dangerous as\nindustrial robots move fast and possess no safety measure. We propose an RL\nalgorithm for FFC insertion that leverages a foundation model-based real-to-sim\napproach to reduce the training time and eliminate the risk of physical damages\nto robots and surroundings. Training is done entirely in simulation, allowing\nfor random exploration without the risk of physical damages. Sim-to-real\ntransfer is achieved through semantic segmentation masks which leave only those\nvisual features relevant to the insertion tasks such as the geometric and\nspatial information of the cables and receptacles. To enhance generality, we\nuse a foundation model, Segment Anything Model 2 (SAM2). To eleminate human\nintervention, we employ a Vision-Language Model (VLM) to automate the initial\nprompting of SAM2 to find segmentation masks. In the experiments, our method\nexhibits zero-shot capabilities, which enable direct deployments to real\nenvironments without fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67d4\u6027\u6241\u5e73\u7535\u7f06\u63d2\u5165\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u6a21\u62df\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u7269\u7406\u98ce\u9669\u3002", "motivation": "\u5de5\u4e1a\u4e2d\u67d4\u6027\u6241\u5e73\u7535\u7f06\u63d2\u5165\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u6709\u98ce\u9669\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\uff08SAM2\uff09\uff0c\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5206\u5272\u63a9\u7801\u3002", "result": "\u65b9\u6cd5\u5c55\u793a\u4e86\u96f6\u6837\u672c\u80fd\u529b\uff0c\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u73af\u5883\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u67d4\u6027\u6241\u5e73\u7535\u7f06\u63d2\u5165\u7684\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.13733", "pdf": "https://arxiv.org/pdf/2509.13733", "abs": "https://arxiv.org/abs/2509.13733", "authors": ["Xiaolin Zhou", "Tingyang Xiao", "Liu Liu", "Yucheng Wang", "Maiyue Chen", "Xinrui Meng", "Xinjie Wang", "Wei Feng", "Wei Sui", "Zhizhong Su"], "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic\nsystems, with broad applications for the deployment of embodied agents in\nreal-world environments. Despite recent advances, existing approaches are\nlimited in long-range spatial reasoning, often exhibiting low success rates and\nhigh inference latency, particularly in long-range navigation tasks. To address\nthese limitations, we propose FSR-VLN, a vision-language navigation system that\ncombines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow\nNavigation Reasoning (FSR). The HMSG provides a multi-modal map representation\nsupporting progressive retrieval, from coarse room-level localization to\nfine-grained goal view and object identification. Building on HMSG, FSR first\nperforms fast matching to efficiently select candidate rooms, views, and\nobjects, then applies VLM-driven refinement for final goal selection. We\nevaluated FSR-VLN across four comprehensive indoor datasets collected by\nhumanoid robots, utilizing 87 instructions that encompass a diverse range of\nobject categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all\ndatasets, measured by the retrieval success rate (RSR), while reducing the\nresponse time by 82% compared to VLM-based methods on tour videos by activating\nslow reasoning only when fast intuition fails. Furthermore, we integrate\nFSR-VLN with speech interaction, planning, and control modules on a Unitree-G1\nhumanoid robot, enabling natural language interaction and real-time navigation.", "AI": {"tldr": "FSR-VLN\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5c42\u591a\u6a21\u6001\u573a\u666f\u56fe\u548c\u5feb\u6162\u5bfc\u822a\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u8ddd\u79bb\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\u5728\u957f\u8ddd\u79bb\u7a7a\u95f4\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u7a81\u51fa\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u6a21\u6001\u573a\u666f\u56fe\uff08HMSG\uff09\u63d0\u4f9b\u591a\u6a21\u6001\u5730\u56fe\u8868\u793a\uff0c\u5e76\u5f15\u5165\u5feb\u6162\u5bfc\u822a\u63a8\u7406\uff08FSR\uff09\u673a\u5236\uff0c\u5148\u5feb\u901f\u5339\u914d\u5019\u9009\u76ee\u6807\u518d\u7cbe\u7ec6\u7b5b\u9009\u3002", "result": "FSR-VLN\u5728\u56db\u4e2a\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u68c0\u7d22\u6210\u529f\u7387\u9ad8\uff0c\u54cd\u5e94\u65f6\u95f4\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1182%\u3002", "conclusion": "FSR-VLN\u901a\u8fc7\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.13736", "pdf": "https://arxiv.org/pdf/2509.13736", "abs": "https://arxiv.org/abs/2509.13736", "authors": ["Muyuan Ma", "Long Cheng", "Lijun Han", "Xiuze Xia", "Houcheng Li"], "title": "Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning", "categories": ["cs.RO"], "comment": null, "summary": "Wearable exoskeletons can augment human strength and reduce muscle fatigue\nduring specific tasks. However, developing personalized and task-generalizable\nassistance algorithms remains a critical challenge. To address this, a\nmeta-imitation learning approach is proposed. This approach leverages a\ntask-specific neural network to predict human elbow joint movements, enabling\neffective assistance while enhancing generalization to new scenarios. To\naccelerate data collection, full-body keypoint motions are extracted from\npublicly available RGB video and motion-capture datasets across multiple tasks,\nand subsequently retargeted in simulation. Elbow flexion trajectories generated\nin simulation are then used to train the task-specific neural network within\nthe model-agnostic meta-learning (MAML) framework, which allows the network to\nrapidly adapt to novel tasks and unseen users with only a few gradient updates.\nThe adapted network outputs personalized references tracked by a\ngravity-compensated PD controller to ensure stable assistance. Experimental\nresults demonstrate that the exoskeleton significantly reduces both muscle\nactivation and metabolic cost for new users performing untrained tasks,\ncompared to performing without exoskeleton assistance. These findings suggest\nthat the proposed framework effectively improves task generalization and user\nadaptability for wearable exoskeleton systems.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u5143\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u8f85\u52a9\u7b97\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8098\u90e8\u8fd0\u52a8\uff0c\u7ed3\u5408\u516c\u5f00\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u65b0\u7528\u6237\u5728\u672a\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6fc0\u6d3b\u548c\u4ee3\u8c22\u6210\u672c\u3002", "motivation": "\u5f00\u53d1\u4e2a\u6027\u5316\u7684\u4efb\u52a1\u901a\u7528\u8f85\u52a9\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u5728\u589e\u5f3a\u4eba\u529b\u548c\u51cf\u5c11\u75b2\u52b3\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5143\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u516c\u5f00RGB\u89c6\u9891\u548c\u52a8\u4f5c\u6355\u6349\u6570\u636e\u8bad\u7ec3\u4efb\u52a1\u7279\u5b9a\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7MAML\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u7528\u6237\u3002", "result": "\u5916\u9aa8\u9abc\u663e\u8457\u51cf\u5c11\u65b0\u7528\u6237\u5728\u672a\u8bad\u7ec3\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6fc0\u6d3b\u548c\u4ee3\u8c22\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u53ef\u7a7f\u6234\u5916\u9aa8\u9abc\u7cfb\u7edf\u7684\u4efb\u52a1\u901a\u7528\u6027\u548c\u7528\u6237\u9002\u5e94\u6027\u3002"}}
{"id": "2509.13737", "pdf": "https://arxiv.org/pdf/2509.13737", "abs": "https://arxiv.org/abs/2509.13737", "authors": ["Renjie Wang", "Shangke Lyu", "Donglin Wang"], "title": "Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control", "categories": ["cs.RO"], "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable progress in legged\nlocomotion control, it often suffers from performance degradation in\nout-of-distribution (OOD) conditions and discrepancies between the simulation\nand the real environments. Instead of mainly relying on domain randomization\n(DR) to best cover the real environments and thereby close the sim-to-real gap\nand enhance robustness, this work proposes an emerging decoupled framework that\nacquires fast online adaptation ability and mitigates the sim-to-real problems\nin unfamiliar environments by isolating stance-leg control and swing-leg\ncontrol. Various simulation and real-world experiments demonstrate its\neffectiveness against horizontal force disturbances, uneven terrains, heavy and\nbiased payloads, and sim-to-real gap.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\u7684\u63a7\u5236\uff0c\u589e\u5f3aRL\u5728\u817f\u5f0f\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5916\u6761\u4ef6\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u73af\u5883\u5dee\u5f02\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u817f\u5f0f\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6761\u4ef6\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u73af\u5883\u5dee\u5f02\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u968f\u673a\u5316\uff0c\u4f46\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6846\u67b6\uff0c\u5206\u79bb\u7ad9\u7acb\u817f\u548c\u6446\u52a8\u817f\u7684\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u7f13\u89e3\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u5f02\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u6709\u6548\u5e94\u5bf9\u4e86\u6c34\u5e73\u529b\u5e72\u6270\u3001\u4e0d\u5e73\u5730\u9762\u3001\u5927\u8d1f\u8f7d\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "conclusion": "\u89e3\u8026\u6846\u67b6\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u817f\u5f0f\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13771", "pdf": "https://arxiv.org/pdf/2509.13771", "abs": "https://arxiv.org/abs/2509.13771", "authors": ["Mengzhu Li", "Yunyu Zhou", "He Ying", "F. Richard Yu"], "title": "CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs", "categories": ["cs.RO"], "comment": null, "summary": "Signed Distance Fields (SDFs) are a fundamental representation in robot\nmotion planning. Their configuration-space counterpart, the Configuration Space\nDistance Field (CDF), directly encodes distances in joint space, offering a\nunified representation for optimization and control. However, existing CDF\nformulations face two major challenges in high-degree-of-freedom (DoF) robots:\n(1) they effectively return only a single nearest collision configuration,\nneglecting the multi-modal nature of minimal-distance collision configurations\nand leading to gradient ambiguity; and (2) they rely on sparse sampling of the\ncollision boundary, which often fails to identify the true closest\nconfigurations, producing oversmoothed approximations and geometric distortion\nin high-dimensional spaces. We propose CDFlow, a novel framework that addresses\nthese limitations by learning a continuous flow in configuration space via\nNeural Ordinary Differential Equations (Neural ODEs). We redefine the problem\nfrom finding a single nearest point to modeling the distribution of\nminimal-distance collision configurations. We also introduce an adaptive\nrefinement sampling strategy to generate high-fidelity training data for this\ndistribution. The resulting Neural ODE implicitly models this multi-modal\ndistribution and produces a smooth, consistent gradient field-derived as the\nexpected direction towards the distribution-that mitigates gradient ambiguity\nand preserves sharp geometric features. Extensive experiments on high-DoF\nmotion planning tasks demonstrate that CDFlow significantly improves planning\nefficiency, trajectory quality, and robustness compared to existing CDF-based\nmethods, enabling more robust and efficient planning for collision-aware robots\nin complex environments.", "AI": {"tldr": "CDFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfODE\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a\uff08CDF\uff09\u5728\u591a\u6a21\u6001\u548c\u51e0\u4f55\u5931\u771f\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CDF\u65b9\u6cd5\u5728\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e2d\u5b58\u5728\u591a\u6a21\u6001\u68af\u5ea6\u6a21\u7cca\u548c\u51e0\u4f55\u5931\u771f\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u795e\u7ecfODE\u5b66\u4e60\u914d\u7f6e\u7a7a\u95f4\u7684\u8fde\u7eed\u6d41\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7ec6\u5316\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCDFlow\u5728\u8fd0\u52a8\u89c4\u5212\u6548\u7387\u3001\u8f68\u8ff9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709CDF\u65b9\u6cd5\u3002", "conclusion": "CDFlow\u901a\u8fc7\u5efa\u6a21\u591a\u6a21\u6001\u5206\u5e03\u548c\u4e00\u81f4\u6027\u68af\u5ea6\u573a\uff0c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u89c4\u5212\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.13774", "pdf": "https://arxiv.org/pdf/2509.13774", "abs": "https://arxiv.org/abs/2509.13774", "authors": ["Piaopiao Jin", "Qi Wang", "Guokang Sun", "Ziwen Cai", "Pinjia He", "Yangwei You"], "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models demonstrate strong generalization in\nrobotic manipulation but face challenges in complex, real-world tasks. While\nsupervised fine-tuning with demonstrations is constrained by data quality,\nreinforcement learning (RL) offers a promising alternative. We propose a\nhuman-in-the-loop dual-actor fine-tuning framework grounded in RL. The\nframework integrates a primary actor for robust multi-task performance with a\nrefinement actor for latent-space adaptation. Beyond standard physical\ninterventions, we introduce a lightweight talk-and-tweak scheme that converts\nhuman corrections into semantically grounded language commands, thereby\ngenerating a new dataset for policy learning. In real-world multi-task\nexperiments, our approach achieves 100% success across three tasks within 101\nminutes of online fine-tuning. For long-horizon tasks, it sustains a 50%\nsuccess rate over 12 consecutive operations. Furthermore, the framework scales\neffectively to multi-robot training, achieving up to a 2 times improvement in\nefficiency when using dual robots. The experiment videos are available at\nhttps://sites.google.com/view/hil-daft/.", "AI": {"tldr": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u673a\u534f\u4f5c\u53cc\u6267\u884c\u5668\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u547d\u4ee4\u751f\u6210\u65b0\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u548c\u957f\u65f6\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u6267\u884c\u5668\u6846\u67b6\uff0c\u5305\u62ec\u4e3b\u6267\u884c\u5668\u548c\u5fae\u8c03\u6267\u884c\u5668\uff0c\u7ed3\u5408\u8bed\u8a00\u547d\u4ee4\u751f\u6210\u65b0\u6570\u636e\u96c6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u771f\u5b9e\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff0c\u957f\u65f6\u4efb\u52a1\u4fdd\u6301\u4e8650%\u6210\u529f\u7387\uff0c\u591a\u673a\u5668\u4eba\u8bad\u7ec3\u6548\u7387\u63d0\u53472\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13780", "pdf": "https://arxiv.org/pdf/2509.13780", "abs": "https://arxiv.org/abs/2509.13780", "authors": ["Weishuai Zeng", "Shunlin Lu", "Kangning Yin", "Xiaojie Niu", "Minyue Dai", "Jingbo Wang", "Jiangmiao Pang"], "title": "Behavior Foundation Model for Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff08BFM\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5168\u8eab\u4f53\u63a7\u5236\uff08WBC\uff09\u6846\u67b6\u7684\u4efb\u52a1\u7279\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002BFM\u901a\u8fc7\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u63a9\u7801\u5728\u7ebf\u84b8\u998f\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\uff0c\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u548c\u6280\u80fd\u7684\u9ad8\u6548\u6cdb\u5316\u548c\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5168\u8eab\u4f53\u63a7\u5236\u6846\u67b6\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\u573a\u666f\u3002\u8bba\u6587\u901a\u8fc7\u8bc6\u522b\u8de8\u4efb\u52a1\u7684\u5171\u540c\u76ee\u6807\uff08\u751f\u6210\u5f15\u5bfc\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u72b6\u6001\u7684\u884c\u4e3a\uff09\uff0c\u63d0\u51faBFM\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "BFM\u7ed3\u5408\u63a9\u7801\u5728\u7ebf\u84b8\u998f\u6846\u67b6\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\uff0c\u9884\u8bad\u7ec3\u4e8e\u5927\u89c4\u6a21\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u5efa\u6a21\u884c\u4e3a\u5206\u5e03\uff0c\u652f\u6301\u7075\u6d3b\u63a7\u5236\u6a21\u5f0f\u548c\u9ad8\u6548\u5b66\u4e60\u65b0\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBFM\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5747\u80fd\u6cdb\u5316\u591a\u6837WBC\u4efb\u52a1\uff0c\u5e76\u5feb\u901f\u9002\u5e94\u65b0\u884c\u4e3a\u3002", "conclusion": "BFM\u4e3a\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6548\u7387\u3002"}}
{"id": "2509.13802", "pdf": "https://arxiv.org/pdf/2509.13802", "abs": "https://arxiv.org/abs/2509.13802", "authors": ["Takuya Kiyokawa", "Ryunosuke Takebayashi", "Kensuke Harada"], "title": "Shell-Type Soft Jig for Holding Objects during Disassembly", "categories": ["cs.RO"], "comment": "6 pages, 8 figures", "summary": "This study addresses a flexible holding tool for robotic disassembly. We\npropose a shell-type soft jig that securely and universally holds objects,\nmitigating the risk of component damage and adapting to diverse shapes while\nenabling soft fixation that is robust to recognition, planning, and control\nerrors. The balloon-based holding mechanism ensures proper alignment and stable\nholding performance, thereby reducing the need for dedicated jig design, highly\naccurate perception, precise grasping, and finely tuned trajectory planning\nthat are typically required with conventional fixtures. Our experimental\nresults demonstrate the practical feasibility of the proposed jig through\nperformance comparisons with a vise and a jamming-gripper-inspired soft jig.\nTests on ten different objects further showed representative successes and\nfailures, clarifying the jig's limitations and outlook.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u67d4\u6027\u5939\u5177\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u62c6\u5378\uff0c\u80fd\u591f\u5b89\u5168\u3001\u901a\u7528\u5730\u56fa\u5b9a\u7269\u4f53\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u4e13\u7528\u5939\u5177\u8bbe\u8ba1\u548c\u7cbe\u5bc6\u64cd\u4f5c\u7684\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5939\u5177\u5728\u62c6\u5378\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5bfc\u81f4\u7684\u90e8\u4ef6\u635f\u574f\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u7269\u4f53\u5f62\u72b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u7403\u7684\u8f6f\u6027\u5939\u5177\uff0c\u901a\u8fc7\u67d4\u6027\u56fa\u5b9a\u673a\u5236\u51cf\u5c11\u5bf9\u7cbe\u786e\u8bc6\u522b\u3001\u89c4\u5212\u548c\u63a7\u5236\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u5939\u5177\u7684\u53ef\u884c\u6027\uff0c\u4e0e\u4f20\u7edf\u5939\u5177\u76f8\u6bd4\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4e5f\u660e\u786e\u4e86\u5176\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u67d4\u6027\u5939\u5177\u4e3a\u673a\u5668\u4eba\u62c6\u5378\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u66f4\u590d\u6742\u7684\u573a\u666f\u3002"}}
{"id": "2509.13815", "pdf": "https://arxiv.org/pdf/2509.13815", "abs": "https://arxiv.org/abs/2509.13815", "authors": ["Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "Soft Regrasping Tool Inspired by Jamming Gripper", "categories": ["cs.RO"], "comment": "6 pages, 9 figures", "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u5939\u5177\u7684\u91cd\u65b0\u6293\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u7684\u5939\u5177\u51cf\u5c11\u673a\u5668\u4eba\u88c5\u914d\u4e2d\u7684\u4f4d\u59ff\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u7684\u653e\u7f6e\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u5939\u5177\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u96f6\u4ef6\u4e13\u95e8\u8bbe\u8ba1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u53d8\u5f62\u8f6f\u5939\u5177\uff0c\u4ee5\u66f4\u7075\u6d3b\u5730\u9002\u5e94\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u96f6\u4ef6\u3002", "method": "\u5229\u7528\u4e09\u89d2\u9525\u5f62\u5de5\u5177\u538b\u5165\u8f6f\u5939\u5177\u7684\u819c\u4e2d\u5e76\u62bd\u51fa\u7a7a\u6c14\uff0c\u5f62\u6210\u7a33\u5b9a\u7684\u653e\u7f6e\u7a7a\u95f4\u3002\u4f18\u5316\u538b\u5165\u6df1\u5ea6\u4ee5\u5e73\u8861\u653e\u7f6e\u7a33\u5b9a\u6027\u548c\u6293\u624b\u53ef\u53ca\u6027\u3002", "result": "\u5728\u5341\u79cd\u4e0d\u540c\u5f62\u72b6\u7684\u673a\u68b0\u96f6\u4ef6\u4e0a\u8fdb\u884c\u4e86\u653e\u7f6e\u5b9e\u9a8c\uff0c\u5927\u591a\u6570\u96f6\u4ef6\u7684\u6210\u529f\u7387\u8d85\u8fc780%\uff0c\u5706\u67f1\u5f62\u96f6\u4ef6\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u5931\u8d25\u4e3b\u8981\u7531\u51e0\u4f55\u7ea6\u675f\u548c\u819c\u7279\u6027\u5f15\u8d77\u3002", "conclusion": "\u8f6f\u5939\u5177\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u7cbe\u786e\u4e14\u53ef\u91cd\u590d\u7684\u91cd\u65b0\u6293\u53d6\u65b9\u6848\uff0c\u672a\u6765\u6709\u671b\u6210\u4e3a\u521a\u6027\u5939\u5177\u7684\u5b9e\u7528\u66ff\u4ee3\u54c1\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2509.13816", "pdf": "https://arxiv.org/pdf/2509.13816", "abs": "https://arxiv.org/abs/2509.13816", "authors": ["Yude Li", "Zhexuan Zhou", "Huizhe Li", "Youmin Gong", "Jie Mei"], "title": "Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex\nenvironments is a critical capability. However, modern end-to-end navigation\nfaces a key challenge: the high-frequency control loop needed for agile flight\nconflicts with low-frequency perception streams, which are limited by sensor\nupdate rates and significant computational cost. This mismatch forces\nconventional synchronous models into undesirably low control rates. To resolve\nthis, we propose an asynchronous reinforcement learning framework that\ndecouples perception and control, enabling a high-frequency policy to act on\nthe latest IMU state for immediate reactivity, while incorporating perception\nfeatures asynchronously. To manage the resulting data staleness, we introduce a\ntheoretically-grounded Temporal Encoding Module (TEM) that explicitly\nconditions the policy on perception delays, a strategy complemented by a\ntwo-stage curriculum to ensure stable and efficient training. Validated in\nextensive simulations, our method was successfully deployed in zero-shot\nsim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate\nand demonstrates robust, agile navigation in cluttered real-world environments.\nOur source code will be released for community reference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u81ea\u4e3b\u98de\u884c\u5668\u4e2d\u611f\u77e5\u4e0e\u63a7\u5236\u9891\u7387\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u4e0e\u63a7\u5236\u5e76\u5f15\u5165TEM\u6a21\u5757\u5904\u7406\u6570\u636e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9891\u7387\u63a7\u5236\u548c\u9c81\u68d2\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u9891\u7387\u63a7\u5236\u4e0e\u4f4e\u9891\u7387\u611f\u77e5\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u5bfc\u822a\u7684\u654f\u6377\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u8026\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u5f15\u5165Temporal Encoding Module (TEM) \u5904\u7406\u611f\u77e5\u5ef6\u8fdf\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\u786e\u4fdd\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6210\u529f\u90e8\u7f72\u5230\u5b9e\u9645\u8bbe\u5907\uff0c\u5b9e\u73b0\u4e86100Hz\u7684\u63a7\u5236\u9891\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f02\u6b65\u5904\u7406\u548c\u5ef6\u8fdf\u8865\u507f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2509.13827", "pdf": "https://arxiv.org/pdf/2509.13827", "abs": "https://arxiv.org/abs/2509.13827", "authors": ["Renyuan Liu", "Haoting Zhou", "Chuankai Fang", "Qinbing Fu"], "title": "How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots", "categories": ["cs.RO", "cs.NE"], "comment": "9 pages, 6 figures", "summary": "Anyone who has tried to swat a fly has likely been frustrated by its\nremarkable agility.This ability stems from its visual neural perception system,\nparticularly the collision-selective neurons within its small brain.For\nautonomous robots operating in complex and unfamiliar environments, achieving\nsimilar agility is highly desirable but often constrained by the trade-off\nbetween computational cost and performance.In this context, insect-inspired\nintelligence offers a parsimonious route to low-power, computationally\nefficient frameworks.In this paper, we propose an attention-driven visuomotor\ncontrol strategy inspired by a specific class of fly visual projection\nneurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated\nescape behaviors.To our knowledge, this represents the first embodiment of an\nLPLC2 neural model in the embedded vision of a physical mobile robot, enabling\ncollision perception and reactive evasion.The model was simplified and\noptimized at 70KB in memory to suit the computational constraints of a\nvision-based micro robot, the Colias, while preserving key neural perception\nmechanisms.We further incorporated multi-attention mechanisms to emulate the\ndistributed nature of LPLC2 responses, allowing the robot to detect and react\nto approaching targets both rapidly and selectively.We systematically evaluated\nthe proposed method against a state-of-the-art locust-inspired collision\ndetection model.Results showed that the fly-inspired visuomotor model achieved\ncomparable robustness, at success rate of 96.1% in collision detection while\nproducing more adaptive and elegant evasive maneuvers.Beyond demonstrating an\neffective collision-avoidance strategy, this work highlights the potential of\nfly-inspired neural models for advancing research into collective behaviors in\ninsect intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u82cd\u8747\u89c6\u89c9\u795e\u7ecf\u5143\u542f\u53d1\u7684\u6ce8\u610f\u529b\u9a71\u52a8\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7684\u78b0\u649e\u611f\u77e5\u548c\u53cd\u5e94\u8eb2\u907f\uff0c\u5e76\u5728\u5fae\u578b\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u548c\u4f18\u96c5\u7684\u907f\u969c\u3002", "motivation": "\u82cd\u8747\u7684\u654f\u6377\u6027\u6e90\u4e8e\u5176\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf\uff0c\u8fd9\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bf9\u673a\u5668\u4eba\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u6709\u91cd\u8981\u542f\u53d1\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4eff\u751f\u65b9\u6cd5\u89e3\u51b3\u673a\u5668\u4eba\u8ba1\u7b97\u6210\u672c\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u82cd\u8747\u7684LPLC2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u7b80\u5316\u548c\u4f18\u5316\u768470KB\u5185\u5b58\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u591a\u6ce8\u610f\u529b\u673a\u5236\u6a21\u62df\u5206\u5e03\u5f0f\u54cd\u5e94\u3002", "result": "\u5728\u5fae\u578b\u673a\u5668\u4ebaColias\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u78b0\u649e\u68c0\u6d4b\u6210\u529f\u7387\u8fbe96.1%\uff0c\u4e14\u907f\u969c\u52a8\u4f5c\u66f4\u81ea\u9002\u5e94\u548c\u4f18\u96c5\uff0c\u6027\u80fd\u5ab2\u7f8e\u8757\u866b\u542f\u53d1\u7684\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u907f\u969c\u7b56\u7565\uff0c\u8fd8\u7a81\u663e\u4e86\u82cd\u8747\u542f\u53d1\u7684\u795e\u7ecf\u6a21\u578b\u5728\u6606\u866b\u667a\u80fd\u96c6\u4f53\u884c\u4e3a\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13832", "pdf": "https://arxiv.org/pdf/2509.13832", "abs": "https://arxiv.org/abs/2509.13832", "authors": ["Teng Wang", "Haojun Jiang", "Yuxuan Wang", "Zhenguo Sun", "Xiangjie Yan", "Xiang Li", "Gao Huang"], "title": "UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography", "categories": ["cs.RO"], "comment": null, "summary": "Carotid ultrasound is crucial for the assessment of cerebrovascular health,\nparticularly the internal carotid artery (ICA). While previous research has\nexplored automating carotid ultrasound, none has tackled the challenging ICA.\nThis is primarily due to its deep location, tortuous course, and significant\nindividual variations, which greatly increase scanning complexity. To address\nthis, we propose a Hierarchical Transformer-based decision architecture, namely\nUltraHiT, that integrates high-level variation assessment with low-level action\ndecision. Our motivation stems from conceptualizing individual vascular\nstructures as morphological variations derived from a standard vascular model.\nThe high-level module identifies variation and switches between two low-level\nmodules: an adaptive corrector for variations, or a standard executor for\nnormal cases. Specifically, both the high-level module and the adaptive\ncorrector are implemented as causal transformers that generate predictions\nbased on the historical scanning sequence. To ensure generalizability, we\ncollected the first large-scale ICA scanning dataset comprising 164\ntrajectories and 72K samples from 28 subjects of both genders. Based on the\nabove innovations, our approach achieves a 95% success rate in locating the ICA\non unseen individuals, outperforming baselines and demonstrating its\neffectiveness. Our code will be released after acceptance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42Transformer\u7684\u51b3\u7b56\u67b6\u6784UltraHiT\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9888\u52a8\u8109\u8d85\u58f0\u68c0\u67e5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u9888\u5185\u52a8\u8109(ICA)\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9ad8\u7ea7\u53d8\u5f02\u8bc4\u4f30\u548c\u4f4e\u7ea7\u52a8\u4f5c\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86ICA\u5b9a\u4f4d\u7684\u6210\u529f\u7387\u3002", "motivation": "\u52a8\u673a\u6e90\u4e8e\u5c06\u4e2a\u4f53\u8840\u7ba1\u7ed3\u6784\u89c6\u4e3a\u6807\u51c6\u8840\u7ba1\u6a21\u578b\u7684\u5f62\u6001\u53d8\u5f02\uff0c\u4ece\u800c\u89e3\u51b3ICA\u7531\u4e8e\u4f4d\u7f6e\u6df1\u3001\u8def\u5f84\u66f2\u6298\u548c\u4e2a\u4f53\u5dee\u5f02\u5927\u800c\u5e26\u6765\u7684\u626b\u63cf\u590d\u6742\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5206\u5c42Transformer\u67b6\u6784\uff0c\u9ad8\u7ea7\u6a21\u5757\u8bc6\u522b\u53d8\u5f02\u5e76\u5207\u6362\u81f3\u4f4e\u7ea7\u6a21\u5757\uff08\u81ea\u9002\u5e94\u6821\u6b63\u5668\u6216\u6807\u51c6\u6267\u884c\u5668\uff09\u3002\u9ad8\u7ea7\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6821\u6b63\u5668\u5747\u91c7\u7528\u56e0\u679cTransformer\uff0c\u57fa\u4e8e\u5386\u53f2\u626b\u63cf\u5e8f\u5217\u751f\u6210\u9884\u6d4b\u3002", "result": "\u572828\u540d\u53d7\u8bd5\u8005\u7684164\u4e2a\u8f68\u8ff9\u548c72K\u6837\u672c\u7684\u5927\u89c4\u6a21ICA\u626b\u63cf\u6570\u636e\u96c6\u4e0a\uff0cUltraHiT\u5728\u672a\u89c1\u4e2a\u4f53\u4e0a\u7684ICA\u5b9a\u4f4d\u6210\u529f\u7387\u8fbe95%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u662fUltraHiT\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86ICA\u81ea\u52a8\u5316\u626b\u63cf\u7684\u6311\u6218\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.13833", "pdf": "https://arxiv.org/pdf/2509.13833", "abs": "https://arxiv.org/abs/2509.13833", "authors": ["Zhikai Zhang", "Jun Guo", "Chao Chen", "Jilong Wang", "Chenghuai Lin", "Yunrui Lian", "Han Xue", "Zhenrong Wang", "Maoqi Liu", "Huaping Liu", "He Wang", "Li Yi"], "title": "Track Any Motions under Any Disturbances", "categories": ["cs.RO"], "comment": null, "summary": "A foundational humanoid motion tracker is expected to be able to track\ndiverse, highly dynamic, and contact-rich motions. More importantly, it needs\nto operate stably in real-world scenarios against various dynamics\ndisturbances, including terrains, external forces, and physical property\nchanges for general practical use. To achieve this goal, we propose Any2Track\n(Track Any motions under Any disturbances), a two-stage RL framework to track\nvarious motions under multiple disturbances in the real world. Any2Track\nreformulates dynamics adaptability as an additional capability on top of basic\naction execution and consists of two key components: AnyTracker and AnyAdapter.\nAnyTracker is a general motion tracker with a series of careful designs to\ntrack various motions within a single policy. AnyAdapter is a history-informed\nadaptation module that endows the tracker with online dynamics adaptability to\novercome the sim2real gap and multiple real-world disturbances. We deploy\nAny2Track on Unitree G1 hardware and achieve a successful sim2real transfer in\na zero-shot manner. Any2Track performs exceptionally well in tracking various\nmotions under multiple real-world disturbances.", "AI": {"tldr": "Any2Track\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u8ffd\u8e2a\u591a\u6837\u5316\u8fd0\u52a8\u5e76\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u52a8\u6001\u5e72\u6270\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u8fd0\u52a8\u8ffd\u8e2a\u5668\u5728\u52a8\u6001\u5e72\u6270\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5RL\u6846\u67b6\uff0c\u5305\u62ec\u901a\u7528\u8fd0\u52a8\u8ffd\u8e2a\u5668AnyTracker\u548c\u5386\u53f2\u4fe1\u606f\u9002\u5e94\u6a21\u5757AnyAdapter\u3002", "result": "\u5728Unitree G1\u786c\u4ef6\u4e0a\u6210\u529f\u5b9e\u73b0\u96f6\u6837\u672csim2real\u8fc1\u79fb\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Any2Track\u80fd\u6709\u6548\u8ffd\u8e2a\u591a\u6837\u5316\u8fd0\u52a8\u5e76\u5e94\u5bf9\u591a\u79cd\u73b0\u5b9e\u5e72\u6270\u3002"}}
{"id": "2509.13839", "pdf": "https://arxiv.org/pdf/2509.13839", "abs": "https://arxiv.org/abs/2509.13839", "authors": ["Motonari Kambara", "Komei Sugiura"], "title": "Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models", "categories": ["cs.RO"], "comment": "Published in Advanced Robotics", "summary": "In this work, we address the problem of predicting the future success of\nopen-vocabulary object manipulation tasks. Conventional approaches typically\ndetermine success or failure after the action has been carried out. However,\nthey make it difficult to prevent potential hazards and rely on failures to\ntrigger replanning, thereby reducing the efficiency of object manipulation\nsequences. To overcome these challenges, we propose a model, which predicts the\nalignment between a pre-manipulation egocentric image with the planned\ntrajectory and a given natural language instruction. We introduce a Multi-Level\nTrajectory Fusion module, which employs a state-of-the-art deep state-space\nmodel and a transformer encoder in parallel to capture multi-level time-series\nself-correlation within the end effector trajectory. Our experimental results\nindicate that the proposed method outperformed existing methods, including\nfoundation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u672a\u6765\u6210\u529f\u7387\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cTransformer\u7f16\u7801\u5668\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u64cd\u4f5c\u5b8c\u6210\u540e\u624d\u5224\u65ad\u6210\u529f\u4e0e\u5426\uff0c\u65e0\u6cd5\u9884\u9632\u6f5c\u5728\u5371\u9669\u4e14\u4f9d\u8d56\u5931\u8d25\u89e6\u53d1\u91cd\u89c4\u5212\uff0c\u964d\u4f4e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u9884\u6d4b\u64cd\u4f5c\u524d\u56fe\u50cf\u4e0e\u8ba1\u5212\u8f68\u8ff9\u53ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u5bf9\u9f50\uff0c\u91c7\u7528\u591a\u7ea7\u8f68\u8ff9\u878d\u5408\u6a21\u5757\u7ed3\u5408\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cTransformer\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u63d0\u9ad8\u4e86\u7269\u4f53\u64cd\u4f5c\u5e8f\u5217\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.13857", "pdf": "https://arxiv.org/pdf/2509.13857", "abs": "https://arxiv.org/abs/2509.13857", "authors": ["Nguyen Hoang Khoi Tran", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 5 figures", "summary": "Reliable global localization is critical for autonomous vehicles, especially\nin environments where GNSS is degraded or unavailable, such as urban canyons\nand tunnels. Although high-definition (HD) maps provide accurate priors, the\ncost of data collection, map construction, and maintenance limits scalability.\nOpenStreetMap (OSM) offers a free and globally available alternative, but its\ncoarse abstraction poses challenges for matching with sensor data. We propose\nInterKey, a cross-modal framework that leverages road intersections as\ndistinctive landmarks for global localization. Our method constructs compact\nbinary descriptors by jointly encoding road and building imprints from point\nclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,\norientation determination, and area-equalized sampling strategies, enabling\nrobust cross-modal matching. Experiments on the KITTI dataset demonstrate that\nInterKey achieves state-of-the-art accuracy, outperforming recent baselines by\na large margin. The framework generalizes to sensors that can produce dense\nstructural point clouds, offering a scalable and cost-effective solution for\nrobust vehicle localization.", "AI": {"tldr": "InterKey\u662f\u4e00\u79cd\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u9053\u8def\u4ea4\u53c9\u53e3\u4f5c\u4e3a\u663e\u8457\u5730\u6807\u8fdb\u884c\u5168\u7403\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u548cOSM\u6570\u636e\u6784\u5efa\u7d27\u51d1\u4e8c\u8fdb\u5236\u63cf\u8ff0\u7b26\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728KITTI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5168\u7403\u5b9a\u4f4d\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728GNSS\u4fe1\u53f7\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002\u5c3d\u7ba1\u9ad8\u6e05\u5730\u56fe\u63d0\u4f9b\u51c6\u786e\u5148\u9a8c\uff0c\u4f46\u6210\u672c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002OSM\u867d\u514d\u8d39\u4f46\u62bd\u8c61\u7c97\u7cd9\uff0c\u96be\u4ee5\u4e0e\u4f20\u611f\u5668\u6570\u636e\u5339\u914d\u3002", "method": "\u63d0\u51faInterKey\u6846\u67b6\uff0c\u5229\u7528\u9053\u8def\u4ea4\u53c9\u53e3\u4f5c\u4e3a\u5730\u6807\uff0c\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u548cOSM\u6570\u636e\u6784\u5efa\u4e8c\u8fdb\u5236\u63cf\u8ff0\u7b26\uff0c\u5e76\u5f15\u5165\u8de8\u6a21\u6001\u7b56\u7565\uff08\u5982\u5dee\u5f02\u7f13\u89e3\u548c\u65b9\u5411\u786e\u5b9a\uff09\u5b9e\u73b0\u9c81\u68d2\u5339\u914d\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\uff0cInterKey\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u80fd\u751f\u6210\u5bc6\u96c6\u70b9\u4e91\u7684\u4f20\u611f\u5668\u3002", "conclusion": "InterKey\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5b9a\u4f4d\u3002"}}
{"id": "2509.13861", "pdf": "https://arxiv.org/pdf/2509.13861", "abs": "https://arxiv.org/abs/2509.13861", "authors": ["G\u00f6rkem K\u0131l\u0131n\u00e7 Soylu", "Neziha Akalin", "Maria Riveiro"], "title": "Using Petri Nets for Context-Adaptive Robot Explanations", "categories": ["cs.RO", "I.6.0; A.0"], "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://www.ro-man2025.org/", "summary": "In human-robot interaction, robots must communicate in a natural and\ntransparent manner to foster trust, which requires adapting their communication\nto the context. In this paper, we propose using Petri nets (PNs) to model\ncontextual information for adaptive robot explanations. PNs provide a formal,\ngraphical method for representing concurrent actions, causal dependencies, and\nsystem states, making them suitable for analyzing dynamic interactions between\nhumans and robots. We demonstrate this approach through a scenario involving a\nrobot that provides explanations based on contextual cues such as user\nattention and presence. Model analysis confirms key properties, including\ndeadlock-freeness, context-sensitive reachability, boundedness, and liveness,\nshowing the robustness and flexibility of PNs for designing and verifying\ncontext-adaptive explanations in human-robot interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Petri\u7f51\uff08PNs\uff09\u6765\u5efa\u6a21\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u81ea\u9002\u5e94\u89e3\u91ca\u529f\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5f62\u5f0f\u5316\u3001\u56fe\u5f62\u5316\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u573a\u666f\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5728\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u548c\u81ea\u7136\u6c9f\u901a\u65b9\u9762\uff0c\u673a\u5668\u4eba\u9700\u8981\u6839\u636e\u4e0a\u4e0b\u6587\u8c03\u6574\u5176\u901a\u4fe1\u65b9\u5f0f\uff0c\u800cPetri\u7f51\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u8fd9\u79cd\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u5229\u7528Petri\u7f51\uff08PNs\uff09\u5efa\u6a21\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5206\u6790\u7528\u6237\u6ce8\u610f\u529b\u3001\u5b58\u5728\u611f\u7b49\u52a8\u6001\u56e0\u7d20\uff0c\u5e76\u8bbe\u8ba1\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u89e3\u91ca\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u5206\u6790\u9a8c\u8bc1\u4e86Petri\u7f51\u5728\u6b7b\u9501\u81ea\u7531\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u53ef\u8fbe\u6027\u3001\u6709\u754c\u6027\u548c\u6d3b\u6027\u7b49\u5173\u952e\u6027\u8d28\u4e0a\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u89e3\u91ca\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "Petri\u7f51\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u4eba\u673a\u4ea4\u4e92\u4e2d\u673a\u5668\u4eba\u7684\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u89e3\u91ca\u884c\u4e3a\u3002"}}
{"id": "2509.13882", "pdf": "https://arxiv.org/pdf/2509.13882", "abs": "https://arxiv.org/abs/2509.13882", "authors": ["Junhwa Hong", "Beomjoon Lee", "Woojin Lee", "Changjoo Nam"], "title": "Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning", "categories": ["cs.RO", "cs.MA"], "comment": "7 pages", "summary": "We propose an efficient motion planning method designed to efficiently find\ncollision-free trajectories for multiple manipulators. While multi-manipulator\nsystems offer significant advantages, coordinating their motions is\ncomputationally challenging owing to the high dimensionality of their composite\nconfiguration space. Conflict-Based Search (CBS) addresses this by decoupling\nmotion planning, but suffers from subsequent conflicts incurred by resolving\nexisting conflicts, leading to an exponentially growing constraint tree of CBS.\nOur proposed method is based on repulsive trajectory modification within the\ntwo-level structure of CBS. Unlike conventional CBS variants, the low-level\nplanner applies a gradient descent approach using an Artificial Potential\nField. This field generates repulsive forces that guide the trajectory of the\nconflicting manipulator away from those of other robots. As a result,\nsubsequent conflicts are less likely to occur. Additionally, we develop a\nstrategy that, under a specific condition, directly attempts to find a\nconflict-free solution in a single step without growing the constraint tree.\nThrough extensive tests including physical robot experiments, we demonstrate\nthat our method consistently reduces the number of expanded nodes in the\nconstraint tree, achieves a higher success rate, and finds a solution faster\ncompared to Enhanced CBS and other state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b2\u7a81\u7684\u641c\u7d22\uff08CBS\uff09\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6392\u65a5\u8f68\u8ff9\u4fee\u6539\u548c\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u51cf\u5c11\u51b2\u7a81\uff0c\u63d0\u9ad8\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u3002", "motivation": "\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u56e0\u9ad8\u7ef4\u5ea6\u914d\u7f6e\u7a7a\u95f4\u800c\u8ba1\u7b97\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u5982CBS\u5b58\u5728\u51b2\u7a81\u589e\u52a0\u7684\u7f3a\u9677\u3002", "method": "\u5728CBS\u7684\u4e24\u5c42\u7ed3\u6784\u4e2d\uff0c\u4f4e\u5c42\u89c4\u5212\u5668\u4f7f\u7528\u4eba\u5de5\u52bf\u573a\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u751f\u6210\u6392\u65a5\u529b\u907f\u514d\u51b2\u7a81\uff0c\u5e76\u5f00\u53d1\u5355\u6b65\u89e3\u51b3\u51b2\u7a81\u7684\u7b56\u7565\u3002", "result": "\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea6\u675f\u6811\u4e2d\u7684\u8282\u70b9\u6269\u5c55\uff0c\u63d0\u9ad8\u6210\u529f\u7387\u548c\u6c42\u89e3\u901f\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6392\u65a5\u8f68\u8ff9\u548c\u5355\u6b65\u51b2\u7a81\u89e3\u51b3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13903", "pdf": "https://arxiv.org/pdf/2509.13903", "abs": "https://arxiv.org/abs/2509.13903", "authors": ["Artem Lykov", "Jeffrin Sam", "Hung Khang Nguyen", "Vladislav Kozlovskiy", "Yara Mahmoud", "Valerii Serpiva", "Miguel Altamirano Cabrera", "Mikhail Konenkov", "Dzmitry Tsetserukou"], "title": "PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models", "categories": ["cs.RO"], "comment": "submitted to IEEE conference", "summary": "We introduce PhysicalAgent, an agentic framework for robotic manipulation\nthat integrates iterative reasoning, diffusion-based video generation, and\nclosed-loop execution. Given a textual instruction, our method generates short\nvideo demonstrations of candidate trajectories, executes them on the robot, and\niteratively re-plans in response to failures. This approach enables robust\nrecovery from execution errors. We evaluate PhysicalAgent across multiple\nperceptual modalities (egocentric, third-person, and simulated) and robotic\nembodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing\nagainst state-of-the-art task-specific baselines. Experiments demonstrate that\nour method consistently outperforms prior approaches, achieving up to 83%\nsuccess on human-familiar tasks. Physical trials reveal that first-attempt\nsuccess is limited (20-30%), yet iterative correction increases overall success\nto 80% across platforms. These results highlight the potential of video-based\ngenerative reasoning for general-purpose robotic manipulation and underscore\nthe importance of iterative execution for recovering from initial failures. Our\nframework paves the way for scalable, adaptable, and robust robot control.", "AI": {"tldr": "PhysicalAgent\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u8fed\u4ee3\u63a8\u7406\u3001\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u548c\u95ed\u73af\u6267\u884c\uff0c\u901a\u8fc7\u751f\u6210\u5019\u9009\u8f68\u8ff9\u89c6\u9891\u5e76\u8fed\u4ee3\u8c03\u6574\u4ee5\u5e94\u5bf9\u5931\u8d25\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u751f\u6210\u548c\u8fed\u4ee3\u6267\u884c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6267\u884c\u9519\u8bef\u65f6\u5b9e\u73b0\u6062\u590d\u3002", "method": "\u4f7f\u7528\u6587\u672c\u6307\u4ee4\u751f\u6210\u5019\u9009\u8f68\u8ff9\u89c6\u9891\uff0c\u6267\u884c\u5e76\u8fed\u4ee3\u91cd\u89c4\u5212\u4ee5\u5e94\u5bf9\u5931\u8d25\u3002", "result": "\u5728\u591a\u79cd\u611f\u77e5\u6a21\u6001\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0cPhysicalAgent\u5e73\u5747\u6210\u529f\u7387\u9ad8\u8fbe83%\uff0c\u9996\u6b21\u5c1d\u8bd5\u6210\u529f\u7387\u4e3a20-30%\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u6b63\u63d0\u5347\u81f380%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u89c6\u9891\u751f\u6210\u63a8\u7406\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fed\u4ee3\u6267\u884c\u5bf9\u6062\u590d\u5931\u8d25\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.13926", "pdf": "https://arxiv.org/pdf/2509.13926", "abs": "https://arxiv.org/abs/2509.13926", "authors": ["Huilin Yin", "Yiming Kan", "Daniel Watzenig"], "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10"], "comment": "8 pages, 2 figures, accepted by ICCVW Author list updated to match\n  the camera-ready version, in compliance with conference policy", "summary": "In recent years, end-to-end autonomous driving has attracted increasing\nattention for its ability to jointly model perception, prediction, and planning\nwithin a unified framework. However, most existing approaches underutilize the\nonline mapping module, leaving its potential to enhance trajectory planning\nlargely untapped. This paper proposes MAP (Map-Assisted Planning), a novel\nmap-assisted end-to-end trajectory planning framework. MAP explicitly\nintegrates segmentation-based map features and the current ego status through a\nPlan-enhancing Online Mapping module, an Ego-status-guided Planning module, and\na Weight Adapter based on current ego status. Experiments conducted on the\nDAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%\nreduction in L2 displacement error, a 56.2% reduction in off-road rate, and a\n44.5% improvement in overall score compared to the UniV2X baseline, even\nwithout post-processing. Furthermore, it achieves top ranking in Track 2 of the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS\nWorkshop @CVPR2025, outperforming the second-best model by 39.5% in terms of\noverall score. These results highlight the effectiveness of explicitly\nleveraging semantic map features in planning and suggest new directions for\nimproving structure design in end-to-end autonomous driving systems. Our code\nis available at https://gitee.com/kymkym/map.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMAP\u7684\u65b0\u578b\u5730\u56fe\u8f85\u52a9\u7aef\u5230\u7aef\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u663e\u5f0f\u6574\u5408\u5730\u56fe\u7279\u5f81\u548c\u8f66\u8f86\u72b6\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5728\u7ebf\u5730\u56fe\u6a21\u5757\u7684\u6f5c\u529b\uff0c\u672a\u80fd\u6709\u6548\u7ed3\u5408\u8bed\u4e49\u5730\u56fe\u7279\u5f81\u4e0e\u8f68\u8ff9\u89c4\u5212\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u6a21\u5757\uff1a\u7528\u4e8e\u589e\u5f3a\u89c4\u5212\u7684\u5728\u7ebf\u5730\u56fe\u6a21\u5757\u3001\u5f15\u5bfc\u89c4\u5212\u7684\u8f66\u8f86\u72b6\u6001\u6a21\u5757\u4ee5\u53ca\u57fa\u4e8e\u72b6\u6001\u7684\u6743\u91cd\u9002\u914d\u5668\u3002", "result": "\u5728DAIR-V2X-seq-SPD\u6570\u636e\u96c6\u4e0a\uff0cMAP\u6bd4\u57fa\u7ebfUniV2X\u51cf\u5c11\u4e8616.6%\u7684L2\u4f4d\u79fb\u8bef\u5dee\u548c56.2%\u7684\u504f\u79bb\u9053\u8def\u7387\uff0c\u6574\u4f53\u8bc4\u5206\u63d0\u534744.5%\uff0c\u5e76\u5728CVPR2025\u7ade\u8d5b\u4e2d\u9886\u5148\u7b2c\u4e8c\u540d39.5%\u3002", "conclusion": "\u663e\u5f0f\u5229\u7528\u8bed\u4e49\u5730\u56fe\u7279\u5f81\u53ef\u6709\u6548\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6027\u80fd\uff0c\u4e3a\u7cfb\u7edf\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.13943", "pdf": "https://arxiv.org/pdf/2509.13943", "abs": "https://arxiv.org/abs/2509.13943", "authors": ["Salim Oyinlola", "Nitesh Subedi", "Soumik Sarkar"], "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Presented at the Research Experience for Undergraduates (REU)\n  Symposium at the Translational AI Centre in Iowa State University", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u8bad\u7ec3\u65e0\u4eba\u673a\u9ad8\u6548\u907f\u969c\u5e76\u5230\u8fbe\u76ee\u6807\u70b9\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5982\u4f55\u5b9e\u73b0\u53ef\u9760\u81ea\u4e3b\u5bfc\u822a\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u672c\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u70b9\u5bf9\u70b9\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u4eba\u673a\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u8bd5\u9519\u4ea4\u4e92\u5b66\u4e60\u5bfc\u822a\u7b56\u7565\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u4ee5\u63d0\u5347\u6548\u7387\u548c\u5b89\u5168\u6027\u3002\u63a7\u5236\u7cfb\u7edf\u96c6\u6210ROS\u548cGym\u517c\u5bb9\u7684\u8bad\u7ec3\u73af\u5883\u3002", "result": "\u7ecf\u8fc7\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u5b9e\u9645\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65e0\u4eba\u673a\u80fd\u591f\u6210\u529f\u81ea\u4e3b\u5bfc\u822a\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\uff0c\u80fd\u591f\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.13948", "pdf": "https://arxiv.org/pdf/2509.13948", "abs": "https://arxiv.org/abs/2509.13948", "authors": ["Benedict Barrow", "Roger K. Moore"], "title": "The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot", "categories": ["cs.RO"], "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://ro-man2025.org/", "summary": "Trust and the perception of trustworthiness play an important role in\ndecision-making and our behaviour towards others, and this is true not only of\nhuman-human interactions but also of human-robot interactions. While\nsignificant advances have been made in recent years in the field of social\nrobotics, there is still some way to go before we fully understand the factors\nthat influence human trust in robots. This paper presents the results of a\nstudy into the first impressions created by a social robot's facial features,\nbased on the hypothesis that a `babyface' engenders trust. By manipulating the\nback-projected face of a Furhat robot, the study confirms that eye shape and\nsize have a significant impact on the perception of trustworthiness. The work\nthus contributes to an understanding of the design choices that need to be made\nwhen developing social robots so as to optimise the effectiveness of\nhuman-robot interaction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u9762\u90e8\u7279\u5f81\u4e2d\u7684\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u611f\u6709\u663e\u8457\u5f71\u54cd\uff0c\u652f\u6301\u201c\u5a74\u513f\u8138\u201d\u5047\u8bbe\u3002", "motivation": "\u4fe1\u4efb\u5728\u4eba\u4e0e\u4eba\u53ca\u4eba\u673a\u4e92\u52a8\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u5982\u4f55\u589e\u5f3a\u4eba\u7c7b\u4fe1\u4efb\u7684\u7406\u89e3\u4ecd\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8c03\u6574Furhat\u673a\u5668\u4eba\u7684\u9762\u90e8\u6295\u5f71\uff08\u5c24\u5176\u662f\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\uff09\uff0c\u7814\u7a76\u5176\u5bf9\u4fe1\u4efb\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u773c\u775b\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u53d8\u5316\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u53ef\u4fe1\u5ea6\u7684\u611f\u77e5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u6548\u679c\u3002"}}
{"id": "2509.13949", "pdf": "https://arxiv.org/pdf/2509.13949", "abs": "https://arxiv.org/abs/2509.13949", "authors": ["Jannick Strangh\u00f6ner", "Philipp Hartmann", "Marco Braun", "Sebastian Wrede", "Klaus Neumann"], "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks", "categories": ["cs.RO", "I.2.9"], "comment": "8 pages, 5 figures, submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86SHaRe-RL\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u9ad8\u6df7\u5408\u4f4e\u6279\u91cf\u5de5\u4e1a\u88c5\u914d\u4e2d\u673a\u5668\u4eba\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9ad8\u6df7\u5408\u4f4e\u6279\u91cf\u5de5\u4e1a\u88c5\u914d\u4e2d\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u7684\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "SHaRe-RL\u7ed3\u5408\u4e86\u9884\u5b9a\u4e49\u6280\u80fd\u3001\u4eba\u7c7b\u793a\u8303\u548c\u5728\u7ebf\u4fee\u6b63\uff0c\u5e76\u901a\u8fc7\u8f74\u5411\u5408\u89c4\u6027\u9650\u5236\u4ea4\u4e92\u529b\u3002", "result": "\u5728\u5de5\u4e1a\u8fde\u63a5\u5668\u63d2\u5165\u4efb\u52a1\u4e2d\uff0cSHaRe-RL\u57280.2-0.4\u6beb\u7c73\u95f4\u9699\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u5728\u7ebf\u5b66\u4e60\u3002", "conclusion": "SHaRe-RL\u5c55\u793a\u4e86\u8fc7\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u5982\u4f55\u6709\u6548\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4e1a\u88c5\u914d\u4e2d\u7684\u5e94\u7528\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u7a33\u5065\u548c\u7ecf\u6d4e\u53ef\u884c\u7684\u90e8\u7f72\u3002"}}
{"id": "2509.13956", "pdf": "https://arxiv.org/pdf/2509.13956", "abs": "https://arxiv.org/abs/2509.13956", "authors": ["Zewei Yang", "Zengqi Peng", "Jun Ma"], "title": "SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous parking is a critical component for achieving safe and efficient\nurban autonomous driving. However, unstructured environments and dynamic\ninteractions pose significant challenges to autonomous parking tasks. To\naddress this problem, we propose SEG-Parking, a novel end-to-end offline\nreinforcement learning (RL) framework to achieve interaction-aware autonomous\nparking. Notably, a specialized parking dataset is constructed for parking\nscenarios, which include those without interference from the opposite vehicle\n(OV) and complex ones involving interactions with the OV. Based on this\ndataset, a goal-conditioned state encoder is pretrained to map the fused\nperception information into the latent space. Then, an offline RL policy is\noptimized with a conservative regularizer that penalizes out-of-distribution\nactions. Extensive closed-loop experiments are conducted in the high-fidelity\nCARLA simulator. Comparative results demonstrate the superior performance of\nour framework with the highest success rate and robust generalization to\nout-of-distribution parking scenarios. The related dataset and source code will\nbe made publicly available after the paper is accepted.", "AI": {"tldr": "SEG-Parking\u662f\u4e00\u79cd\u65b0\u578b\u7684\u7aef\u5230\u7aef\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u65e0\u7ed3\u6784\u5316\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u4e2d\u7684\u81ea\u4e3b\u505c\u8f66\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u505c\u8f66\u573a\u6570\u636e\u96c6\u548c\u4fdd\u5b88\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u89e3\u51b3\u65e0\u7ed3\u6784\u5316\u73af\u5883\u548c\u52a8\u6001\u4ea4\u4e92\u5bf9\u81ea\u4e3b\u505c\u8f66\u7684\u6311\u6218\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86SEG-Parking\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u6784\u5efa\u4e13\u7528\u505c\u8f66\u573a\u6570\u636e\u96c6\uff0c\u9884\u8bad\u7ec3\u76ee\u6807\u6761\u4ef6\u72b6\u6001\u7f16\u7801\u5668\uff0c\u5e76\u5229\u7528\u4fdd\u5b88\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u7684\u95ed\u73af\u5b9e\u9a8c\u663e\u793a\uff0cSEG-Parking\u5177\u6709\u6700\u9ad8\u6210\u529f\u7387\u548c\u4f18\u79c0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SEG-Parking\u6846\u67b6\u5728\u81ea\u4e3b\u505c\u8f66\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5c06\u516c\u5f00\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.13965", "pdf": "https://arxiv.org/pdf/2509.13965", "abs": "https://arxiv.org/abs/2509.13965", "authors": ["Abhijeet Nayak", "D\u00e9bora N. P. Oliveira", "Samiran Gode", "Cordelia Schmid", "Wolfram Burgard"], "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Generative navigation policies have made rapid progress in improving\nend-to-end learned navigation. Despite their promising results, this paradigm\nhas two structural problems. First, the sampled trajectories exist in an\nabstract, unscaled space without metric grounding. Second, the control strategy\ndiscards the full path, instead moving directly towards a single waypoint. This\nleads to short-sighted and unsafe actions, moving the robot towards obstacles\nthat a complete and correctly scaled path would circumvent. To address these\nissues, we propose MetricNet, an effective add-on for generative navigation\nthat predicts the metric distance between waypoints, grounding policy outputs\nin real-world coordinates. We evaluate our method in simulation with a new\nbenchmarking framework and show that executing MetricNet-scaled waypoints\nsignificantly improves both navigation and exploration performance. Beyond\nsimulation, we further validate our approach in real-world experiments.\nFinally, we propose MetricNav, which integrates MetricNet into a navigation\npolicy to guide the robot away from obstacles while still moving towards the\ngoal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMetricNet\uff0c\u89e3\u51b3\u751f\u6210\u5f0f\u5bfc\u822a\u7b56\u7565\u4e2d\u8f68\u8ff9\u65e0\u5ea6\u91cf\u57fa\u7840\u548c\u77ed\u89c6\u63a7\u5236\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u548c\u63a2\u7d22\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0f\u5bfc\u822a\u7b56\u7565\u5b58\u5728\u8f68\u8ff9\u65e0\u5ea6\u91cf\u57fa\u7840\u548c\u77ed\u89c6\u63a7\u5236\u7684\u7f3a\u70b9\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "method": "\u63d0\u51faMetricNet\u9884\u6d4b\u8def\u6807\u95f4\u7684\u771f\u5b9e\u8ddd\u79bb\uff0c\u5e76\u6574\u5408\u5230\u5bfc\u822a\u7b56\u7565MetricNav\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMetricNet\u663e\u8457\u63d0\u5347\u5bfc\u822a\u548c\u63a2\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "MetricNet\u548cMetricNav\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u5bfc\u822a\u7684\u7f3a\u9677\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.13972", "pdf": "https://arxiv.org/pdf/2509.13972", "abs": "https://arxiv.org/abs/2509.13972", "authors": ["Asier Bikandi", "Miguel Fernandez-Cortizas", "Muhammad Shaheer", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM Informed Visual SLAM for Construction Monitoring", "categories": ["cs.RO"], "comment": "8 pages, 5 tables, 4 figures", "summary": "Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring\nconstruction sites, where aligning the evolving as-built state with the\nas-planned design enables early error detection and reduces costly rework.\nLiDAR-based SLAM achieves high geometric precision, but its sensors are\ntypically large and power-demanding, limiting their use on portable platforms.\nVisual SLAM offers a practical alternative with lightweight cameras already\nembedded in most mobile devices. however, visually mapping construction\nenvironments remains challenging: repetitive layouts, occlusions, and\nincomplete or low-texture structures often cause drift in the trajectory map.\nTo mitigate this, we propose an RGB-D SLAM system that incorporates the\nBuilding Information Model (BIM) as structural prior knowledge. Instead of\nrelying solely on visual cues, our system continuously establishes\ncorrespondences between detected wall and their BIM counterparts, which are\nthen introduced as constraints in the back-end optimization. The proposed\nmethod operates in real time and has been validated on real construction sites,\nreducing trajectory error by an average of 23.71% and map RMSE by 7.14%\ncompared to visual SLAM baselines. These results demonstrate that BIM\nconstraints enable reliable alignment of the digital plan with the as-built\nscene, even under partially constructed conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BIM\u5148\u9a8c\u77e5\u8bc6\u7684RGB-D SLAM\u7cfb\u7edf\uff0c\u7528\u4e8e\u65bd\u5de5\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff0c\u663e\u8457\u51cf\u5c11\u8f68\u8ff9\u8bef\u5dee\u548c\u5730\u56feRMSE\u3002", "motivation": "\u65bd\u5de5\u73af\u5883\u4e2d\u89c6\u89c9SLAM\u56e0\u91cd\u590d\u5e03\u5c40\u548c\u4f4e\u7eb9\u7406\u7ed3\u6784\u6613\u4ea7\u751f\u6f02\u79fb\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u5b9e\u73b0\u6570\u5b57\u8ba1\u5212\u4e0e\u5b9e\u9645\u573a\u666f\u7684\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7RGB-D SLAM\u7cfb\u7edf\uff0c\u5229\u7528BIM\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u6301\u7eed\u68c0\u6d4b\u5899\u9762\u5e76\u4e0e\u5176BIM\u5bf9\u5e94\u7269\u5efa\u7acb\u7ea6\u675f\uff0c\u4f18\u5316\u540e\u7aef\u3002", "result": "\u5728\u771f\u5b9e\u65bd\u5de5\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u5e73\u5747\u51cf\u5c11\u8f68\u8ff9\u8bef\u5dee23.71%\uff0c\u5730\u56feRMSE\u964d\u4f4e7.14%\u3002", "conclusion": "BIM\u7ea6\u675f\u80fd\u591f\u5728\u90e8\u5206\u65bd\u5de5\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6570\u5b57\u8ba1\u5212\u4e0e\u5b9e\u9645\u573a\u666f\u7684\u53ef\u9760\u5bf9\u9f50\uff0c\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9SLAM\u3002"}}
{"id": "2509.13998", "pdf": "https://arxiv.org/pdf/2509.13998", "abs": "https://arxiv.org/abs/2509.13998", "authors": ["Bailey Dacre", "Rodrigo Moreno", "Serhat Demirtas", "Ziqiao Wang", "Yuhao Jiang", "Jamie Paik", "Kasper Stoy", "Andr\u00e9s Fa\u00ed\u00f1a"], "title": "Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array", "categories": ["cs.RO"], "comment": null, "summary": "Object manipulation is a fundamental challenge in robotics, where systems\nmust balance trade-offs among manipulation capabilities, system complexity, and\nthroughput. Distributed manipulator systems (DMS) use the coordinated motion of\nactuator arrays to perform complex object manipulation tasks, seeing widespread\nexploration within the literature and in industry. However, existing DMS\ndesigns typically rely on high actuator densities and impose constraints on\nobject-to-actuator scale ratios, limiting their adaptability. We present a\nnovel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles\ninterconnected by a compliant surface layer. Unlike conventional DMS, our\napproach enables manipulation not only at the actuator end effectors but also\nacross a flexible surface connecting all actuators; creating a continuous,\ncontrollable manipulation surface. We analyse the combined workspace of such a\nsystem, derive simple motion primitives, and demonstrate its capabilities to\ntranslate simple geometric objects across an array of tiles. By leveraging the\ninter-tile connective material, our approach significantly reduces actuator\ndensity, increasing the area over which an object can be manipulated by x1.84\nwithout an increase in the number of actuators. This design offers a lower cost\nand complexity alternative to traditional high-density arrays, and introduces\nnew opportunities for manipulation strategies that leverage the flexibility of\nthe interconnected surface.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u64cd\u7eb5\u5668\u7cfb\u7edf\uff0c\u5229\u75283\u81ea\u7531\u5ea6\u6298\u7eb8\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u74e6\u7247\u548c\u67d4\u6027\u8868\u9762\u5c42\uff0c\u964d\u4f4e\u4e86\u6267\u884c\u5668\u5bc6\u5ea6\u5e76\u63d0\u9ad8\u64cd\u4f5c\u8303\u56f4\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5206\u5e03\u5f0f\u64cd\u7eb5\u5668\u7cfb\u7edf\u9ad8\u6267\u884c\u5668\u5bc6\u5ea6\u548c\u5bf9\u7269\u4f53\u6bd4\u4f8b\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u91c7\u75283\u81ea\u7531\u5ea6\u6298\u7eb8\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u74e6\u7247\u548c\u67d4\u6027\u8868\u9762\u5c42\uff0c\u5b9e\u73b0\u8fde\u7eed\u53ef\u63a7\u7684\u64cd\u4f5c\u8868\u9762\u3002", "result": "\u7cfb\u7edf\u53ef\u5c06\u7269\u4f53\u64cd\u4f5c\u8303\u56f4\u63d0\u9ad81.84\u500d\uff0c\u65e0\u9700\u589e\u52a0\u6267\u884c\u5668\u6570\u91cf\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u4f20\u7edf\u9ad8\u5bc6\u5ea6\u9635\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5f00\u521b\u4e86\u5229\u7528\u67d4\u6027\u8868\u9762\u7684\u65b0\u64cd\u4f5c\u7b56\u7565\u3002"}}
{"id": "2509.14010", "pdf": "https://arxiv.org/pdf/2509.14010", "abs": "https://arxiv.org/abs/2509.14010", "authors": ["Zong Chen", "Shaoyang Li", "Ben Liu", "Min Li", "Zhouping Yin", "Yiqun Li"], "title": "Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization", "categories": ["cs.RO"], "comment": null, "summary": "Wheel-legged robots with integrated manipulators hold great promise for\nmobile manipulation in logistics, industrial automation, and human-robot\ncollaboration. However, unified control of such systems remains challenging due\nto the redundancy in degrees of freedom, complex wheel-ground contact dynamics,\nand the need for seamless coordination between locomotion and manipulation. In\nthis work, we present the design and whole-body motion control of an\nomnidirectional wheel-legged quadrupedal robot equipped with a dexterous\nmanipulator. The proposed platform incorporates independently actuated steering\nmodules and hub-driven wheels, enabling agile omnidirectional locomotion with\nhigh maneuverability in structured environments. To address the challenges of\ncontact-rich interaction, we develop a contact-aware whole-body dynamic\noptimization framework that integrates point-contact modeling for manipulation\nwith line-contact modeling for wheel-ground interactions. A warm-start strategy\nis introduced to accelerate online optimization, ensuring real-time feasibility\nfor high-dimensional control. Furthermore, a unified kinematic model tailored\nfor the robot's 4WIS-4WID actuation scheme eliminates the need for mode\nswitching across different locomotion strategies, improving control consistency\nand robustness. Simulation and experimental results validate the effectiveness\nof the proposed framework, demonstrating agile terrain traversal, high-speed\nomnidirectional mobility, and precise manipulation under diverse scenarios,\nunderscoring the system's potential for factory automation, urban logistics,\nand service robotics in semi-structured environments.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u4e00\u79cd\u914d\u5907\u7075\u5de7\u673a\u68b0\u624b\u7684\u5168\u5411\u8f6e\u817f\u56db\u8db3\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u4e0e\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u5197\u4f59\u81ea\u7531\u5ea6\u3001\u590d\u6742\u8f6e\u5730\u63a5\u89e6\u52a8\u6001\u53ca\u8fd0\u52a8\u4e0e\u64cd\u7eb5\u534f\u8c03\u7684\u6311\u6218\u3002", "motivation": "\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u53ca\u5176\u673a\u68b0\u624b\u7684\u7edf\u4e00\u63a7\u5236\u5728\u7269\u6d41\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u5177\u6709\u91cd\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u81ea\u7531\u5ea6\u5197\u4f59\u3001\u590d\u6742\u63a5\u89e6\u52a8\u6001\u53ca\u534f\u8c03\u96be\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u72ec\u7acb\u9a71\u52a8\u8f6c\u5411\u6a21\u5757\u548c\u8f6e\u6bc2\u9a71\u52a8\u8f6e\u7684\u5168\u5411\u673a\u5668\u4eba\uff0c\u5f00\u53d1\u4e86\u63a5\u89e6\u611f\u77e5\u7684\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u63a5\u89e6\u548c\u7ebf\u63a5\u89e6\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u70ed\u542f\u52a8\u7b56\u7565\u52a0\u901f\u4f18\u5316\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u654f\u6377\u5730\u5f62\u7a7f\u8d8a\u3001\u9ad8\u901f\u5168\u5411\u79fb\u52a8\u548c\u7cbe\u786e\u64cd\u7eb5\u80fd\u529b\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5728\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u5de5\u5382\u81ea\u52a8\u5316\u3001\u57ce\u5e02\u7269\u6d41\u548c\u670d\u52a1\u673a\u5668\u4eba\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14025", "pdf": "https://arxiv.org/pdf/2509.14025", "abs": "https://arxiv.org/abs/2509.14025", "authors": ["Rui Huang", "Zhiyu Gao", "Siyu Tang", "Jialin Zhang", "Lei He", "Ziqian Zhang", "Lin Zhao"], "title": "TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Modular Aerial Robot Systems (MARS) consist of multiple drone modules that\nare physically bound together to form a single structure for flight. Exploiting\nstructural redundancy, MARS can be reconfigured into different formations to\nmitigate unit or rotor failures and maintain stable flight. Prior work on MARS\nself-reconfiguration has solely focused on maximizing controllability margins\nto tolerate a single rotor or unit fault for rectangular-shaped MARS. We\npropose TransforMARS, a general fault-tolerant reconfiguration framework that\ntransforms arbitrarily shaped MARS under multiple rotor and unit faults while\nensuring continuous in-air stability. Specifically, we develop algorithms to\nfirst identify and construct minimum controllable assemblies containing faulty\nunits. We then plan feasible disassembly-assembly sequences to transport MARS\nunits or subassemblies to form target configuration. Our approach enables more\nflexible and practical feasible reconfiguration. We validate TransforMARS in\nchallenging arbitrarily shaped MARS configurations, demonstrating substantial\nimprovements over prior works in both the capacity of handling diverse\nconfigurations and the number of faults tolerated. The videos and source code\nof this work are available at the anonymous repository:\nhttps://anonymous.4open.science/r/TransforMARS-1030/", "AI": {"tldr": "TransforMARS\u662f\u4e00\u79cd\u901a\u7528\u7684\u6545\u969c\u5bb9\u5fcd\u91cd\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff08MARS\uff09\uff0c\u652f\u6301\u591a\u8f6c\u5b50\u6216\u5355\u5143\u6545\u969c\u4e0b\u7684\u7a33\u5b9a\u98de\u884c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u9488\u5bf9\u77e9\u5f62MARS\u7684\u5355\u6545\u969c\u5bb9\u5fcd\u91cd\u6784\uff0c\u800cTransforMARS\u65e8\u5728\u89e3\u51b3\u4efb\u610f\u5f62\u72b6MARS\u5728\u591a\u6545\u969c\u4e0b\u7684\u91cd\u6784\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u7b97\u6cd5\u8bc6\u522b\u6700\u5c0f\u53ef\u63a7\u7ec4\u4ef6\u5e76\u89c4\u5212\u62c6\u5378-\u7ec4\u88c5\u5e8f\u5217\uff0c\u76ee\u6807\u662f\u5728\u7a7a\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cTransforMARS\u5728\u591a\u6837\u914d\u7f6e\u548c\u5bb9\u5fcd\u591a\u6545\u969c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TransforMARS\u4e3aMARS\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u7684\u6545\u969c\u5bb9\u5fcd\u91cd\u6784\u80fd\u529b\uff0c\u62d3\u5c55\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.14040", "pdf": "https://arxiv.org/pdf/2509.14040", "abs": "https://arxiv.org/abs/2509.14040", "authors": ["Zewen Yang", "Xiaobing Dai", "Dongfa Zhang", "Yu Li", "Ziyang Meng", "Bingkun Huang", "Hamid Sadeghian", "Sami Haddadin"], "title": "Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u4e0d\u53d8\u6027\u7684\u4e00\u6b65\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u6846\u67b6Prompt2Auto\uff0c\u901a\u8fc7\u5355\u6b21\u8fd0\u52a8\u63d0\u793a\u5b9e\u73b0\u673a\u5668\u4eba\u81ea\u52a8\u5316\u63a7\u5236\uff0c\u51cf\u5c11\u4e86\u6f14\u793a\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5b66\u4e60\u6f14\u793a\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u96be\u4ee5\u8de8\u5750\u6807\u53d8\u6362\u6cdb\u5316\uff0cPrompt2Auto\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u4e0d\u53d8\u6027\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u5750\u6807\u53d8\u6362\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\uff0c\u652f\u6301\u591a\u6b65\u9884\u6d4b\u548c\u591a\u6280\u80fd\u81ea\u4e3b\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u6709\u6548\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u663e\u8457\u51cf\u5c11\u6f14\u793a\u9700\u6c42\u3002", "conclusion": "Prompt2Auto\u5728\u673a\u5668\u4eba\u81ea\u52a8\u5316\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2509.14063", "pdf": "https://arxiv.org/pdf/2509.14063", "abs": "https://arxiv.org/abs/2509.14063", "authors": ["Sundhar Vinodh Sangeetha", "Chih-Yuan Chiu", "Sarah H. Q. Li", "Shreyas Kousik"], "title": "Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace", "categories": ["cs.RO"], "comment": "The last two authors advised equally. Submitted to the 2026 IEEE\n  International Conference on Robotics and Automation. 8 pages, 6 figures", "summary": "Autonomous aircraft must safely operate in untowered airspace, where\ncoordination relies on voice-based communication among human pilots. Safe\noperation requires an aircraft to predict the intent, and corresponding goal\nlocation, of other aircraft. This paper introduces a multimodal framework for\naircraft goal prediction that integrates natural language understanding with\nspatial reasoning to improve autonomous decision-making in such environments.\nWe leverage automatic speech recognition and large language models to\ntranscribe and interpret pilot radio calls, identify aircraft, and extract\ndiscrete intent labels. These intent labels are fused with observed\ntrajectories to condition a temporal convolutional network and Gaussian mixture\nmodel for probabilistic goal prediction. Our method significantly reduces goal\nprediction error compared to baselines that rely solely on motion history,\ndemonstrating that language-conditioned prediction increases prediction\naccuracy. Experiments on a real-world dataset from an untowered airport\nvalidate the approach and highlight its potential to enable socially aware,\nlanguage-conditioned robotic motion planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\uff0c\u4ee5\u63d0\u5347\u65e0\u4eba\u822a\u7a7a\u5668\u5728\u975e\u5854\u53f0\u7a7a\u57df\u4e2d\u7684\u76ee\u6807\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5728\u975e\u5854\u53f0\u7a7a\u57df\u4e2d\uff0c\u822a\u7a7a\u5668\u7684\u5b89\u5168\u64cd\u4f5c\u4f9d\u8d56\u4e8e\u98de\u884c\u5458\u4e4b\u95f4\u7684\u8bed\u97f3\u901a\u4fe1\uff0c\u56e0\u6b64\u9700\u8981\u9884\u6d4b\u5176\u4ed6\u822a\u7a7a\u5668\u7684\u610f\u56fe\u548c\u76ee\u6807\u4f4d\u7f6e\u4ee5\u652f\u6301\u81ea\u4e3b\u51b3\u7b56\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u98de\u884c\u5458\u65e0\u7ebf\u7535\u901a\u4fe1\uff0c\u63d0\u53d6\u610f\u56fe\u6807\u7b7e\uff0c\u5e76\u5c06\u5176\u4e0e\u89c2\u6d4b\u8f68\u8ff9\u878d\u5408\uff0c\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u6982\u7387\u76ee\u6807\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u76ee\u6807\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5386\u53f2\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6761\u4ef6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bed\u8a00\u6761\u4ef6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14075", "pdf": "https://arxiv.org/pdf/2509.14075", "abs": "https://arxiv.org/abs/2509.14075", "authors": ["Yu Li", "Hamid Sadeghian", "Zewen Yang", "Valentin Le Mesle", "Sami Haddadin"], "title": "Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Robotic-assisted minimally invasive surgery (RAMIS) requires precise\nenforcement of the remote center of motion (RCM) constraint to ensure safe tool\nmanipulation through a trocar. Achieving this constraint under dynamic and\ninteractive conditions remains challenging, as existing control methods either\nlack robustness at the torque level or do not guarantee consistent RCM\nconstraint satisfaction. This paper proposes a constraint-consistent torque\ncontroller that treats the RCM as a rheonomic holonomic constraint and embeds\nit into a projection-based inverse-dynamics framework. The method unifies\ntask-level and kinematic formulations, enabling accurate tool-tip tracking\nwhile maintaining smooth and efficient torque behavior. The controller is\nvalidated both in simulation and on a RAMIS training platform, and is\nbenchmarked against state-of-the-art approaches. Results show improved RCM\nconstraint satisfaction, reduced required torque, and robust performance by\nimproving joint torque smoothness through the consistency formulation under\nclinically relevant scenarios, including spiral trajectories, variable\ninsertion depths, moving trocars, and human interaction. These findings\ndemonstrate the potential of constraint-consistent torque control to enhance\nsafety and reliability in surgical robotics. The project page is available at:\nhttps://rcmpc-cube.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea6\u675f\u4e00\u81f4\u626d\u77e9\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u786e\u4fddRAMIS\u4e2d\u7684RCM\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u52a8\u6001\u548c\u4ea4\u4e92\u6761\u4ef6\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5728RAMIS\u4e2d\uff0c\u7cbe\u786e\u6267\u884cRCM\u7ea6\u675f\u5bf9\u5de5\u5177\u7684\u5b89\u5168\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u626d\u77e9\u7ea7\u522b\u7f3a\u4e4f\u9c81\u68d2\u6027\u6216\u65e0\u6cd5\u4fdd\u8bc1\u4e00\u81f4\u7684\u7ea6\u675f\u6ee1\u8db3\u3002", "method": "\u901a\u8fc7\u5c06RCM\u89c6\u4e3a\u6d41\u5f62\u7ea6\u675f\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u57fa\u4e8e\u6295\u5f71\u7684\u9006\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5b9e\u73b0\u4efb\u52a1\u7ea7\u548c\u8fd0\u52a8\u5b66\u8868\u8ff0\u7684\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728RCM\u7ea6\u675f\u6ee1\u8db3\u3001\u626d\u77e9\u9700\u6c42\u51cf\u5c11\u548c\u626d\u77e9\u5e73\u6ed1\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u4e34\u5e8a\u573a\u666f\u3002", "conclusion": "\u7ea6\u675f\u4e00\u81f4\u626d\u77e9\u63a7\u5236\u6709\u671b\u63d0\u5347\u624b\u672f\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.14082", "pdf": "https://arxiv.org/pdf/2509.14082", "abs": "https://arxiv.org/abs/2509.14082", "authors": ["Valerii Serpiva", "Artem Lykov", "Faryal Batool", "Vladislav Kozlovskiy", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video", "categories": ["cs.RO"], "comment": "Submitted to conference", "summary": "We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.", "AI": {"tldr": "FlightDiffusion\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff08FPV\uff09\u89c6\u9891\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u3002\u8be5\u6a21\u578b\u80fd\u4ece\u5355\u5e27\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u9644\u5e26\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u5bfc\u822a\u3002", "motivation": "\u901a\u8fc7\u751f\u6210\u903c\u771f\u7684FPV\u89c6\u9891\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u6536\u96c6\u7684\u9ad8\u6210\u672c\uff0c\u5e76\u4e3a\u65e0\u4eba\u673a\u7814\u7a76\u63d0\u4f9b\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684FPV\u8f68\u8ff9\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u5728\u7269\u7406\u4e0a\u53ef\u884c\uff0c\u4f4d\u7f6e\u548c\u65b9\u5411\u8bef\u5dee\u8f83\u4f4e\uff08RMSE\u5206\u522b\u4e3a0.28 m\u548c0.24 rad\uff09\uff0c\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u6027\u80fd\u65e0\u663e\u8457\u5dee\u5f02\uff08p=0.541\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u4eba\u673a\u5bfc\u822a\u3001\u52a8\u4f5c\u751f\u6210\u548c\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.14117", "pdf": "https://arxiv.org/pdf/2509.14117", "abs": "https://arxiv.org/abs/2509.14117", "authors": ["Ali Abouzeid", "Malak Mansour", "Zezhou Sun", "Dezhen Song"], "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model", "categories": ["cs.RO"], "comment": "Under Review", "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.", "AI": {"tldr": "GeoAware-VLA\u901a\u8fc7\u96c6\u6210\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u63d0\u5347\u4e86VLA\u6a21\u578b\u5bf9\u65b0\u76f8\u673a\u89c6\u89d2\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "VLA\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u76f8\u673a\u89c6\u89d2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u96be\u4ee5\u4ece2D\u56fe\u50cf\u63a8\u65ad\u7a33\u5065\u76843D\u51e0\u4f55\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u5148\u9a8c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faGeoAware-VLA\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u51e0\u4f55\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u5c06\u8fd9\u4e9b\u7279\u5f81\u8c03\u6574\u4e3a\u9002\u5408\u7b56\u7565\u89e3\u7801\u5668\u7684\u5f62\u5f0f\uff0c\u65e0\u9700\u4ece\u5934\u5b66\u4e603D\u4e00\u81f4\u6027\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4eff\u771f\u73af\u5883\u4e0b\u6210\u529f\u7387\u63d0\u53472\u500d\u4ee5\u4e0a\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u8fc7\u7684\u76f8\u673a\u89d2\u5ea6\u4e0b\u3002", "conclusion": "\u51e0\u4f55\u5148\u9a8c\u662f\u63d0\u5347\u673a\u5668\u4eba\u4ee3\u7406\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\uff0cGeoAware-VLA\u5728\u8fde\u7eed\u548c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.14126", "pdf": "https://arxiv.org/pdf/2509.14126", "abs": "https://arxiv.org/abs/2509.14126", "authors": ["Viktor Lorentz", "Khaled Wahba", "Sayantan Auddy", "Marc Toussaint", "Wolfgang H\u00f6nig"], "title": "CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads", "categories": ["cs.RO", "cs.MA"], "comment": "This work has been submitted to IEEE for possible publication", "summary": "Collaborative transportation of cable-suspended payloads by teams of Unmanned\nAerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to\ndifferent payload shapes, and provide built-in compliance, making it attractive\nfor applications ranging from disaster relief to precision logistics. However,\nmulti-UAV coordination under disturbances, nonlinear payload dynamics, and\nslack--taut cable modes remains a challenging control problem. To our\nknowledge, no prior work has addressed these cable mode transitions in the\nmulti-UAV context, instead relying on simplifying rigid-link assumptions. We\npropose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for\nmulti-UAV cable-suspended payload transport. Simulation results demonstrate\nthat the learned policies can outperform classical decentralized controllers in\nterms of disturbance rejection and tracking precision, achieving an 80%\nrecovery rate from harsh conditions compared to 44% for the baseline method. We\nalso achieve successful zero-shot sim-to-real transfer and demonstrate that our\npolicies are highly robust under harsh conditions, including wind, random\nexternal disturbances, and transitions between slack and taut cable dynamics.\nThis work paves the way for autonomous, resilient UAV teams capable of\nexecuting complex payload missions in unstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrazyMARL\u7684\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u534f\u540c\u8fd0\u8f93\u7535\u7f06\u60ac\u6302\u8d1f\u8f7d\u7684\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u534f\u540c\u8fd0\u8f93\u8d1f\u8f7d\u5728\u707e\u5bb3\u6551\u63f4\u548c\u7cbe\u51c6\u7269\u6d41\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8d1f\u8f7d\u52a8\u529b\u5b66\u548c\u7535\u7f06\u6a21\u5f0f\u5207\u6362\u7b49\u6311\u6218\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6CrazyMARL\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\u521a\u6027\u5047\u8bbe\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6297\u5e72\u6270\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u5668\uff0c\u4e14\u5728\u4e25\u82db\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u7387\u4e3a80%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u56e2\u961f\u7684\u81ea\u4e3b\u6027\u548c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.14127", "pdf": "https://arxiv.org/pdf/2509.14127", "abs": "https://arxiv.org/abs/2509.14127", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Philip Dames"], "title": "Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "We consider the problem of delivering multiple packages from a single pickup\ndepot to distinct goal locations using a homogeneous fleet of robots with\nlimited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner\nTree Relay Coordination Planning framework that constructs sparse relay trunks\nusing Steiner tree optimization and then synthesizes robot-level pickup, relay,\nand delivery schedules. This framework reframes relays from incidental\nbyproducts into central elements of coordination, offering a contrast with\ntraditional delivery methods that rely on direct source-to-destination\ntransport. Extensive experiments show consistent improvements of up to 34%\ncompared to conventional baselines, underscoring the benefits of incorporating\nrelays into the delivery process. These improvements translate directly to\nenhanced energy efficiency in multi-robot delivery under capacity constraints,\nproviding a scalable framework for real-world logistics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVCST-RCP\u6846\u67b6\uff0c\u901a\u8fc7Steiner\u6811\u4f18\u5316\u6784\u5efa\u7a00\u758f\u4e2d\u7ee7\u4e3b\u5e72\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u4eba\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u591a\u673a\u5668\u4eba\u914d\u9001\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u914d\u9001\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u8fd0\u8f93\uff0c\u800c\u672c\u6587\u5c06\u4e2d\u7ee7\u4f5c\u4e3a\u534f\u8c03\u6838\u5fc3\uff0c\u65e8\u5728\u89e3\u51b3\u6709\u9650\u8fd0\u529b\u4e0b\u591a\u673a\u5668\u4eba\u914d\u9001\u7684\u6548\u7387\u548c\u80fd\u8017\u95ee\u9898\u3002", "method": "\u91c7\u7528Voronoi\u7ea6\u675f\u7684Steiner\u6811\u4f18\u5316\u6784\u5efa\u4e2d\u7ee7\u4e3b\u5e72\uff0c\u5e76\u5408\u6210\u673a\u5668\u4eba\u7ea7\u522b\u7684\u62fe\u53d6\u3001\u4e2d\u7ee7\u548c\u4ea4\u4ed8\u8c03\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0cVCST-RCP\u6548\u7387\u63d0\u5347\u8fbe34%\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u591a\u673a\u5668\u4eba\u914d\u9001\u7684\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "VCST-RCP\u4e3a\u4e2d\u7ee7\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u914d\u9001\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7269\u6d41\u573a\u666f\u3002"}}
{"id": "2509.14138", "pdf": "https://arxiv.org/pdf/2509.14138", "abs": "https://arxiv.org/abs/2509.14138", "authors": ["Ran Yang", "Zijian An", "Lifeng ZHou", "Yiming Feng"], "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model", "categories": ["cs.RO", "68T40"], "comment": "8 pages, 9 figures, 1 table", "summary": "Long-horizon robotic manipulation tasks require executing multiple\ninterdependent subtasks in strict sequence, where errors in detecting subtask\ncompletion can cascade into downstream failures. Existing\nVision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous\nlow-level control but lack an internal signal for identifying when a subtask\nhas finished, making them brittle in sequential settings. We propose SeqVLA, a\ncompletion-aware extension of $\\pi_0$ that augments the base architecture with\na lightweight detection head perceiving whether the current subtask is\ncomplete. This dual-head design enables SeqVLA not only to generate\nmanipulation actions but also to autonomously trigger transitions between\nsubtasks. We investigate four finetuning strategies that vary in how the action\nand detection heads are optimized (joint vs. sequential finetuning) and how\npretrained knowledge is preserved (full finetuning vs. frozen backbone).\nExperiments are performed on two multi-stage tasks: salad packing with seven\ndistinct subtasks and candy packing with four distinct subtasks. Results show\nthat SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong\nbaselines in overall success rate. In particular, joint finetuning with an\nunfrozen backbone yields the most decisive and statistically reliable\ncompletion predictions, eliminating sequence-related failures and enabling\nrobust long-horizon execution. Our results highlight the importance of coupling\naction generation with subtask-aware detection for scalable sequential\nmanipulation.", "AI": {"tldr": "SeqVLA \u662f\u4e00\u79cd\u57fa\u4e8e $\u03c0_0$ \u7684\u5b8c\u6210\u611f\u77e5\u6269\u5c55\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5934\u5b9e\u73b0\u5b50\u4efb\u52a1\u5b8c\u6210\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u9636\u6bb5\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684 VLA \u6a21\u578b\uff08\u5982 $\u03c0_0$\uff09\u5728\u8fde\u7eed\u4f4e\u5c42\u6b21\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5b50\u4efb\u52a1\u5b8c\u6210\u68c0\u6d4b\u7684\u5185\u90e8\u4fe1\u53f7\uff0c\u5bfc\u81f4\u5728\u987a\u5e8f\u4efb\u52a1\u4e2d\u8868\u73b0\u8106\u5f31\u3002", "method": "SeqVLA \u91c7\u7528\u53cc\u5934\u8bbe\u8ba1\uff0c\u5305\u542b\u52a8\u4f5c\u751f\u6210\u548c\u5b50\u4efb\u52a1\u5b8c\u6210\u68c0\u6d4b\u5934\uff0c\u7814\u7a76\u4e86\u56db\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u63a2\u8ba8\u8054\u5408\u4f18\u5316\u4e0e\u987a\u5e8f\u4f18\u5316\u4ee5\u53ca\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4fdd\u7559\u65b9\u5f0f\u3002", "result": "\u5728\u6c99\u62c9\u5305\u88c5\u548c\u7cd6\u679c\u5305\u88c5\u4efb\u52a1\u4e2d\uff0cSeqVLA \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b $\u03c0_0$\uff0c\u8054\u5408\u4f18\u5316\u4e14\u4e0d\u51bb\u7ed3\u4e3b\u5e72\u7f51\u7edc\u7684\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u52a8\u4f5c\u751f\u6210\u4e0e\u5b50\u4efb\u52a1\u611f\u77e5\u68c0\u6d4b\u7ed3\u5408\u5bf9\u4e8e\u53ef\u6269\u5c55\u7684\u987a\u5e8f\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.14143", "pdf": "https://arxiv.org/pdf/2509.14143", "abs": "https://arxiv.org/abs/2509.14143", "authors": ["Zijian An", "Ran Yang", "Yiming Feng", "Lifeng Zhou"], "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping", "categories": ["cs.RO", "68T40"], "comment": "8 pages, 5 figures, 1 table", "summary": "Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.", "AI": {"tldr": "CLAW\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u6761\u4ef6\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u751f\u6210\uff0c\u7ed3\u5408\u7b26\u53f7\u5316\u63a8\u7406\u4e0e\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6267\u884c\u7cbe\u786e\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u6ee1\u8db3\u7cbe\u786e\u4efb\u52a1\u7ea6\u675f\uff08\u5982\u57fa\u4e8e\u6570\u503c\u9608\u503c\u7684\u505c\u6b62\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u663e\u5f0f\u7684\u6761\u4ef6\u76d1\u6d4b\u673a\u5236\u3002", "method": "CLAW\u6846\u67b6\u5229\u7528\u5fae\u8c03\u7684CLIP\u6a21\u578b\u751f\u6210\u79bb\u6563\u6307\u4ee4\uff0c\u7ed3\u5408VLA\u7b56\u7565\u8fdb\u884c\u9ad8\u9891\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u3002", "result": "\u5728\u5355\u7269\u4f53\u6293\u53d6\u548c\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cCLAW\u8868\u73b0\u4f18\u4e8e\u539f\u59cb\u548c\u5fae\u8c03\u7684VLA\u6a21\u578b\u3002", "conclusion": "CLAW\u901a\u8fc7\u663e\u5f0f\u6761\u4ef6\u76d1\u6d4b\u4e0e\u7b26\u53f7\u5316\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2509.14147", "pdf": "https://arxiv.org/pdf/2509.14147", "abs": "https://arxiv.org/abs/2509.14147", "authors": ["Fanxing Li", "Shengyang Wang", "Fangyu Sun", "Shuyu Wu", "Dexin Zuo", "Wenxian Yu", "Danping Zou"], "title": "StableTracker: Learning to Stably Track Target via Differentiable Simulation", "categories": ["cs.RO"], "comment": null, "summary": "FPV object tracking methods heavily rely on handcraft modular designs,\nresulting in hardware overload and cumulative error, which seriously degrades\nthe tracking performance, especially for rapidly accelerating or decelerating\ntargets. To address these challenges, we present \\textbf{StableTracker}, a\nlearning-based control policy that enables quadrotors to robustly follow the\nmoving target from arbitrary perspectives. The policy is trained using\nbackpropagation-through-time via differentiable simulation, allowing the\nquadrotor to maintain the target at the center of the visual field in both\nhorizontal and vertical directions, while keeping a fixed relative distance,\nthereby functioning as an autonomous aerial camera. We compare StableTracker\nagainst both state-of-the-art traditional algorithms and learning baselines.\nSimulation experiments demonstrate that our policy achieves superior accuracy,\nstability and generalization across varying safe distances, trajectories, and\ntarget velocities. Furthermore, a real-world experiment on a quadrotor with an\nonboard computer validated practicality of the proposed approach.", "AI": {"tldr": "StableTracker\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6a21\u62df\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86FPV\u8ddf\u8e2a\u4e2d\u786c\u4ef6\u8fc7\u8f7d\u548c\u7d2f\u8ba1\u8bef\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfFPV\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u6a21\u5757\uff0c\u5bfc\u81f4\u786c\u4ef6\u8fc7\u8f7d\u548c\u7d2f\u8ba1\u8bef\u5dee\uff0c\u5c24\u5176\u5728\u76ee\u6807\u5feb\u901f\u52a0\u51cf\u901f\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u65f6\u95f4\u7684\u53cd\u5411\u4f20\u64ad\u548c\u53ef\u5fae\u5206\u6a21\u62df\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u4fdd\u6301\u76ee\u6807\u5728\u89c6\u91ce\u4e2d\u5fc3\u5e76\u56fa\u5b9a\u76f8\u5bf9\u8ddd\u79bb\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0cStableTracker\u5728\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u53ca\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "StableTracker\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3FPV\u8ddf\u8e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14159", "pdf": "https://arxiv.org/pdf/2509.14159", "abs": "https://arxiv.org/abs/2509.14159", "authors": ["Dayi Dong", "Maulik Bhatt", "Seoyeon Choi", "Negar Mehr"], "title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies", "categories": ["cs.RO"], "comment": "9 pages, 4 figures, 5 tables", "summary": "As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u65b9\u6cd5MIMIC-D\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u4e2d\u534f\u8c03\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u793e\u4f1a\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e0e\u5176\u4ed6\u673a\u5668\u4eba\u6216\u4eba\u7c7b\u534f\u8c03\u7684\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u7684\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6837\u5316\u7b56\u7565\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faMIMIC-D\u65b9\u6cd5\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u7684\u8303\u5f0f\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u591a\u6a21\u6001\u8f68\u8ff9\u5206\u5e03\uff0c\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u6210\u529f\u6062\u590d\u4e86\u591a\u667a\u80fd\u4f53\u7684\u534f\u8c03\u884c\u4e3a\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MIMIC-D\u4e3a\u591a\u6a21\u6001\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65e0\u6cd5\u76f4\u63a5\u901a\u4fe1\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2509.14178", "pdf": "https://arxiv.org/pdf/2509.14178", "abs": "https://arxiv.org/abs/2509.14178", "authors": ["Kai Ye", "Yuhang Wu", "Shuyuan Hu", "Junliang Li", "Meng Liu", "Yongquan Chen", "Rui Huang"], "title": "\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous manipulation remains a challenging robotics problem, largely due to\nthe difficulty of collecting extensive human demonstrations for learning. In\nthis paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos\nwith one generated video and drives robot skill from it: it combines\ndemonstration generation that leverages video generation with pose and depth\nestimation to yield hand-object trajectories, trajectory optimization that uses\nPhysics-aware Interaction Optimization Model (PIOM) to impose physics\nconsistency, and demonstration learning that retargets human motions to a robot\nhand and stabilizes control with an anchor-based residual Proximal Policy\nOptimization (PPO) policy. Using only generated videos, the learned policy\nachieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates\ncoherent executions on a real robot. We also conduct ablation studies to\nvalidate the contribution of each component and demonstrate the ability to\ndirectly specify tasks using natural language, highlighting the flexibility and\nrobustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined\nvideos to real-world execution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGen2Real\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u9891\u800c\u975e\u4eba\u5de5\u6f14\u793a\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u6293\u53d6\u6280\u80fd\uff0c\u6210\u529f\u7387\u8fbe\u523077.3%\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u6f14\u793a\u6570\u636e\u6536\u96c6\u96be\u9898\uff0c\u63d0\u51fa\u5229\u7528\u751f\u6210\u89c6\u9891\u66ff\u4ee3\u4eba\u5de5\u6f14\u793a\u7684\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u751f\u6210\u3001\u8f68\u8ff9\u4f18\u5316\u548c\u57fa\u4e8e\u951a\u70b9\u7684\u6b8b\u5deePPO\u7b56\u7565\uff0c\u751f\u6210\u5e76\u4f18\u5316\u624b-\u7269\u4f53\u8f68\u8ff9\uff0c\u518d\u5c06\u5176\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u6210\u529f\u738777.3%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u4efb\u52a1\u63cf\u8ff0\u7684\u7075\u6d3b\u6027\u548c\u6280\u80fd\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Gen2Real\u5c55\u793a\u4e86\u4ece\u751f\u6210\u89c6\u9891\u5230\u771f\u5b9e\u6267\u884c\u7684\u7075\u6d3b\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.14191", "pdf": "https://arxiv.org/pdf/2509.14191", "abs": "https://arxiv.org/abs/2509.14191", "authors": ["Zhihao Cao", "Hanyu Wu", "Li Wa Tang", "Zizhou Luo", "Zihan Zhu", "Wei Zhang", "Marc Pollefeys", "Martin R. Oswald"], "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.", "AI": {"tldr": "MCGS-SLAM\u662f\u57fa\u4e8e3D\u9ad8\u65af\u70b9\u4e91\u7684\u9996\u4e2a\u7eafRGB\u591a\u6444\u50cf\u5934SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u7a20\u5bc6\u8f93\u5165\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u7cbe\u5ea6\u5efa\u56fe\uff0c\u4f18\u4e8e\u5355\u76ee\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5355\u76eeSLAM\u5728\u9c81\u68d2\u6027\u548c\u51e0\u4f55\u8986\u76d6\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u81ea\u4e3b\u9a7e\u9a76\u7b49\u573a\u666f\u7684\u5b89\u5168\u6027\u548c\u91cd\u5efa\u6548\u679c\u3002", "method": "\u4f7f\u7528\u591a\u6444\u50cf\u5934\u675f\u8c03\u6574\uff08MCBA\uff09\u8054\u5408\u4f18\u5316\u4f4d\u59ff\u548c\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5c3a\u5ea6\u4e00\u81f4\u6027\u6a21\u5757\u786e\u4fdd\u591a\u89c6\u56fe\u5ea6\u91cf\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u76ee\u57fa\u7ebf\uff0c\u80fd\u91cd\u5efa\u5355\u76ee\u7cfb\u7edf\u9057\u6f0f\u7684\u4fa7\u89c6\u533a\u57df\u3002", "conclusion": "\u591a\u6444\u50cf\u5934\u9ad8\u65af\u70b9\u4e91SLAM\u5728\u9ad8\u4fdd\u771f\u5efa\u56fe\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u9002\u5408\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u3002"}}
{"id": "2509.14210", "pdf": "https://arxiv.org/pdf/2509.14210", "abs": "https://arxiv.org/abs/2509.14210", "authors": ["Seth Farrell", "Chenghao Li", "Hongzhan Yu", "Hesam Mojtahedi", "Sicun Gao", "Henrik I. Christensen"], "title": "GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "We present a cooperative aerial-ground search-and-rescue (SAR) framework that\npairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)\nto achieve rapid victim localization and obstacle-aware navigation in unknown\nenvironments. We dub this framework Guided Long-horizon Integrated Drone Escort\n(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon\nplanning. In our framework, a goal-searching UAV executes real-time onboard\nvictim detection and georeferencing to nominate goals for the ground platform,\nwhile a terrain-scouting UAV flies ahead of the UGV's planned route to provide\nmid-level traversability updates. The UGV fuses aerial cues with local sensing\nto perform time-efficient A* planning and continuous replanning as information\narrives. Additionally, we present a hardware demonstration (using a GEM e6 golf\ncart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission\nperformance and include simulation ablations to assess the planning stack in\nisolation from detection. Empirical results demonstrate that explicit role\nseparation across UAVs, coupled with terrain scouting and guided planning,\nimproves reach time and navigation safety in time-critical SAR missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u7a7a\u4e2d-\u5730\u9762\u641c\u6551\u6846\u67b6GLIDE\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u5f15\u5bfc\u5730\u9762\u8f66\u8f86\u5b9e\u73b0\u5feb\u901f\u53d7\u5bb3\u8005\u5b9a\u4f4d\u548c\u969c\u788d\u7269\u611f\u77e5\u5bfc\u822a\u3002", "motivation": "\u63d0\u9ad8\u65f6\u95f4\u7d27\u8feb\u7684\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u5bfc\u822a\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u65e0\u4eba\u673a\uff08\u76ee\u6807\u641c\u7d22\u548c\u5730\u5f62\u4fa6\u5bdf\uff09\u4e0e\u5730\u9762\u8f66\u8f86\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408A*\u89c4\u5212\u548c\u5b9e\u65f6\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u660e\u786e\u7684\u89d2\u8272\u5206\u914d\u548c\u5730\u5f62\u4fa6\u5bdf\u63d0\u5347\u4e86\u5230\u8fbe\u65f6\u95f4\u548c\u5bfc\u822a\u5b89\u5168\u6027\u3002", "conclusion": "GLIDE\u6846\u67b6\u5728\u641c\u6551\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u5b89\u5168\u7684\u7279\u70b9\u3002"}}
{"id": "2509.14228", "pdf": "https://arxiv.org/pdf/2509.14228", "abs": "https://arxiv.org/abs/2509.14228", "authors": ["Benjamin Shaffer", "Victoria Edwards", "Brooks Kinch", "Nathaniel Trask", "M. Ani Hsieh"], "title": "Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u79fb\u52a8\u4f20\u611f\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u6d41\u52a8\u73af\u5883\u4e2d\u7684\u6e90\u5b9a\u4f4d\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7684\u6709\u9650\u5143\u6a21\u578b\u548c\u4fe1\u606f\u7a0e\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u590d\u6742\u6d41\u52a8\u73af\u5883\uff08\u5982\u5316\u5b66\u6cc4\u6f0f\u6216\u6cb9\u6c61\u6269\u6563\uff09\u4e2d\u7684\u6e90\u5b9a\u4f4d\u5bf9\u591a\u673a\u5668\u4eba\u56e2\u961f\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u6a21\u578b\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u6bcf\u4e2a\u673a\u5668\u4eba\u643a\u5e26\u673a\u5668\u5b66\u4e60\u7684\u6709\u9650\u5143\u6a21\u578b\uff0c\u5229\u7528\u8fd1\u4f3c\u4e92\u4fe1\u606f\u51c6\u5219\u9a71\u52a8\u4fe1\u606f\u7a0e\u63a7\u5236\u7b56\u7565\uff0c\u9009\u62e9\u9884\u671f\u4fe1\u606f\u91cf\u6700\u5927\u7684\u91c7\u6837\u533a\u57df\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u4f20\u611f\u7b56\u7565\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u5feb\u964d\u4f4e\u8bef\u5dee\u5e76\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u6e90\u5b9a\u4f4d\u3002", "conclusion": "\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6709\u6548\u63d0\u5347\u590d\u6742\u6d41\u52a8\u6e90\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.13331", "pdf": "https://arxiv.org/pdf/2509.13331", "abs": "https://arxiv.org/abs/2509.13331", "authors": ["Reza Pirayeshshirazinezhad"], "title": "Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation", "categories": ["astro-ph.IM", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We use artificial intelligence (AI) and supervisory adaptive control systems\nto plan and optimize the mission of precise spacecraft formation. Machine\nlearning and robust control enhance the efficiency of spacecraft precision\nformation of the Virtual Telescope for X-ray Observation (VTXO) space mission.\nVTXO is a precise formation of two separate spacecraft making a virtual\ntelescope with a one-kilometer focal length. One spacecraft carries the lens\nand the other spacecraft holds the camera to observe high-energy space objects\nin the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed\nautomata for supervisory control, Monte Carlo simulations for stability and\nrobustness evaluation, and integration of deep neural networks for optimal\nestimation of mission parameters, satisfy the high precision mission criteria.\nWe integrate deep neural networks with a constrained, non-convex dynamic\noptimization pipeline to predict optimal mission parameters, ensuring precision\nmission criteria are met. AI framework provides explainability by predicting\nthe resulting energy consumption and mission error for a given set of mission\nparameters. It allows for transparent, justifiable, and real-time trade-offs, a\ncapability not present in traditional adaptive controllers. The results show\nreductions in energy consumption and improved mission accuracy, demonstrating\nthe capability of the system to address dynamic uncertainties and disturbances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u7684\u7cbe\u786e\u822a\u5929\u5668\u7f16\u961f\u4efb\u52a1\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0X\u5c04\u7ebf\u89c2\u6d4b\u865a\u62df\u671b\u8fdc\u955c\uff08VTXO\uff09\u7684\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u9c81\u68d2\u63a7\u5236\u6280\u672f\uff0c\u63d0\u9ad8VTXO\u7a7a\u95f4\u4efb\u52a1\u4e2d\u822a\u5929\u5668\u7f16\u961f\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4ee5\u89c2\u6d4b\u9ad8\u80fdX\u5c04\u7ebf\u5929\u4f53\u7684\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u3002", "method": "\u91c7\u7528\u5b9a\u65f6\u81ea\u52a8\u673a\u8fdb\u884c\u76d1\u7763\u63a7\u5236\uff0c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8bc4\u4f30\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4efb\u52a1\u53c2\u6570\uff0c\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u8981\u6c42\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u80fd\u591f\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u9ad8\u4efb\u52a1\u7cbe\u5ea6\uff0c\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AI\u6846\u67b6\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4efb\u52a1\u53c2\u6570\u7684\u4f18\u5316\u548c\u900f\u660e\u5316\u51b3\u7b56\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u822a\u5929\u5668\u7f16\u961f\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.13352", "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65e0\u4eba\u673a\u6846\u67b6\uff08Agentic UAVs\uff09\uff0c\u901a\u8fc7\u4e94\u5c42\u67b6\u6784\u589e\u5f3a\u65e0\u4eba\u673a\u7684\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u548c\u96c6\u6210\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u4e0e\u6551\u63f4\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165LLM\u9a71\u52a8\u7684\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e94\u5c42\u67b6\u6784\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u3001\u5b66\u4e60\uff09\uff0c\u7ed3\u5408ROS2\u3001Gazebo\u3001YOLOv11\u548cGPT-4\u7b49\u6280\u672f\u5b9e\u73b0\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cAgentic UAVs\u5728\u76ee\u6807\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff080.79 vs. 0.72\uff09\u3001\u4eba\u5458\u68c0\u6d4b\u7387\uff0891% vs. 75%\uff09\u548c\u884c\u52a8\u63a8\u8350\u7387\uff0892% vs. 4.5%\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5982\u4f55\u5728\u6709\u9650\u8ba1\u7b97\u5f00\u9500\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6c34\u5e73\u7684\u81ea\u4e3b\u6027\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u3002"}}
{"id": "2509.13414", "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u524d\u9988\u6a21\u578b\uff0c\u901a\u8fc7\u8f93\u5165\u56fe\u50cf\u548c\u53ef\u9009\u51e0\u4f55\u4fe1\u606f\uff0c\u76f4\u63a5\u56de\u5f523D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u901a\u8fc7\u7edf\u4e00\u7684\u6a21\u578b\u89e3\u51b3\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5206\u89e3\u8868\u793a\u65b9\u6cd5\u5904\u7406\u591a\u89c6\u89d2\u573a\u666f\u51e0\u4f55\uff0c\u7ed3\u5408\u6807\u51c6\u5316\u76d1\u7763\u548c\u7075\u6d3b\u8f93\u5165\u589e\u5f3a\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u9988\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMapAnything\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u7528\u6a21\u578b\uff0c\u540c\u65f6\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "MapAnything\u4e3a\u901a\u75283D\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2509.13564", "pdf": "https://arxiv.org/pdf/2509.13564", "abs": "https://arxiv.org/abs/2509.13564", "authors": ["Arman Pourghorban", "Dipankar Maity"], "title": "Multi-Attacker Single-Defender Target Defense in Conical Environments", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "We consider a variant of the target defense problem in a planar conical\nenvironment where a single defender is tasked to capture a sequence of incoming\nattackers. The attackers' objective is to breach the target boundary without\nbeing captured by the defender. As soon as the current attacker breaches the\ntarget or gets captured by the defender, the next attacker appears at the\nboundary of the environment and moves radially toward the target with maximum\nspeed. Therefore, the defender's final location at the end of the current game\nbecomes its initial location for the next game. The attackers pick strategies\nthat are advantageous for the current as well as for future engagements between\nthe defender and the remaining attackers. The attackers have their own sensors\nwith limited range, using which they can perfectly detect if the defender is\nwithin their sensing range. We derive equilibrium strategies for all the\nplayers to optimize the capture percentage using the notions of capture\ndistribution. Finally, the theoretical results are verified through numerical\nexamples using Monte Carlo type random trials of experiments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5e73\u9762\u4e0a\u5706\u9525\u73af\u5883\u4e2d\u7684\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u9632\u5fa1\u8005\u5982\u4f55\u62e6\u622a\u4e00\u7cfb\u5217\u653b\u51fb\u8005\u4ee5\u4fdd\u62a4\u76ee\u6807\u3002\u901a\u8fc7\u5206\u6790\u535a\u5f08\u5747\u8861\u7b56\u7565\uff0c\u7406\u8bba\u7ed3\u679c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u591a\u653b\u51fb\u8005\u5e8f\u5217\u4e0b\u7684\u76ee\u6807\u9632\u5fa1\u95ee\u9898\uff0c\u653b\u51fb\u8005\u80fd\u6839\u636e\u9632\u5fa1\u8005\u7684\u4f4d\u7f6e\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u8fd9\u5bf9\u5b9e\u9645\u9632\u5fa1\u7cfb\u7edf\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63a8\u5bfc\u653b\u9632\u53cc\u65b9\u7684\u535a\u5f08\u5747\u8861\u7b56\u7565\uff0c\u7279\u522b\u662f\u5229\u7528\u6355\u6349\u5206\u5e03\u6982\u5ff5\u4f18\u5316\u9632\u5fa1\u8005\u7684\u62e6\u622a\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u7f57\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u653b\u9632\u53cc\u65b9\u7684\u6700\u4f18\u7b56\u7565\u80fd\u663e\u8457\u5f71\u54cd\u6355\u83b7\u7387\uff0c\u7406\u8bba\u5206\u6790\u4e0e\u6570\u503c\u5b9e\u9a8c\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u901a\u8fc7\u5747\u8861\u7b56\u7565\u8bbe\u8ba1\u548c\u52a8\u6001\u611f\u77e5\u4f18\u5316\uff0c\u9632\u5fa1\u8005\u53ef\u4ee5\u6709\u6548\u62e6\u622a\u653b\u51fb\u8005\u5e8f\u5217\uff0c\u4fdd\u62a4\u76ee\u6807\u5b89\u5168\u3002"}}
{"id": "2509.13577", "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f1a\u9047\u5230\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u573a\u666f\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u7684OOD\u68c0\u6d4b\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u8f68\u8ff9\u5c42\u9762\u7684OOD\u68c0\u6d4b\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u9884\u6d4b\u9519\u8bef\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u4ee5\u5b9e\u73b0\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8f68\u8ff9\u9884\u6d4bOOD\u68c0\u6d4b\u3002"}}
{"id": "2509.13605", "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "CLAP\u7b97\u6cd5\u662f\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u76842D\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ee5\u5176\u6297\u566a\u6027\u548c\u9c81\u68d2\u6027\u8457\u79f0\uff0c\u672c\u6587\u5c06\u5176\u6269\u5c55\u81f33D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7684\u5173\u8054\u3002", "motivation": "\u4e3a\u4e86\u5c06CLAP\u7b97\u6cd5\u7684\u4f18\u52bf\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u9886\u57df\uff0c\u6269\u5c55\u5176\u9002\u7528\u8303\u56f4\u5e76\u63d0\u5347\u5904\u7406\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\u3002", "method": "\u5c06CLAP\u7b97\u6cd5\u4ece2D\u5b9a\u4f4d\u63a8\u5e7f\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\uff0c\u540c\u65f6\u5206\u6790\u5176\u4e0eRANSAC\u548cHough\u53d8\u6362\u7684\u5173\u8054\u3002", "result": "CLAP\u7b97\u6cd5\u7684\u5e7f\u4e49\u5316\u4f7f\u5176\u9002\u7528\u4e8e\u591a\u4e2a\u9886\u57df\uff0c\u6210\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u5de5\u5177\u3002", "conclusion": "CLAP\u7684\u6269\u5c55\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5728\u591a\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u601d\u8def\u3002"}}
