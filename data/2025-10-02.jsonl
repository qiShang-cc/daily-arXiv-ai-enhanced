{"id": "2510.00032", "pdf": "https://arxiv.org/pdf/2510.00032", "abs": "https://arxiv.org/abs/2510.00032", "authors": ["Ziyi Zeng", "Zhenyang Cai", "Yixi Cai", "Xidong Wang", "Junying Chen", "Rongsheng Wang", "Yipeng Liu", "Siqi Cai", "Benyou Wang", "Zhiguo Zhang", "Haizhou Li"], "title": "WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Electroencephalography (EEG) interpretation using multimodal large language\nmodels (MLLMs) offers a novel approach for analyzing brain signals. However,\nthe complex nature of brain activity introduces critical challenges: EEG\nsignals simultaneously encode both cognitive processes and intrinsic neural\nstates, creating a mismatch in EEG paired-data modality that hinders effective\ncross-modal representation learning. Through a pivot investigation, we uncover\ncomplementary relationships between these modalities. Leveraging this insight,\nwe propose mapping EEG signals and their corresponding modalities into a\nunified semantic space to achieve generalized interpretation. To fully enable\nconversational capabilities, we further introduce WaveMind-Instruct-338k, the\nfirst cross-task EEG dataset for instruction tuning. The resulting model\ndemonstrates robust classification accuracy while supporting flexible,\nopen-ended conversations across four downstream tasks, thereby offering\nvaluable insights for both neuroscience research and the development of\ngeneral-purpose EEG models."}
{"id": "2510.00141", "pdf": "https://arxiv.org/pdf/2510.00141", "abs": "https://arxiv.org/abs/2510.00141", "authors": ["Dipankar Shakya", "Naveed A. Abbasi", "Mingjun Ying", "Isha Jariwala", "Jason J. Qin", "Ishaan S. Gupte", "Bridget Meier", "Guanyue Qian", "Daniel Abraham", "Theodore S. Rappaport", "Andreas F. Molisch"], "title": "Standardized Machine-Readable Point-Data Format for Consolidating Wireless Propagation Across Environments, Frequencies, and Institutions", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, 4 tables, IEEE MILCOM 2025 conference", "summary": "The necessity of new spectrum for 6G has intensified global interest in radio\npropagation measurements across emerging frequency bands, use cases, and\nantenna types. These measurements are vital for understanding radio channel\nproperties in diverse environments, and involve time-consuming and expensive\ncampaigns. A major challenge for the effective utilization of propagation\nmeasurement data has been the lack of a standardized format for reporting and\narchiving results. Although organizations such as NIST, NGA, and 3GPP have made\ncommendable efforts for data pooling, a unified machine-readable data format\nfor consolidating measurements across different institutions and frequencies\nremains a missing piece in advancing global standardization efforts. This paper\nintroduces a standardized point-data format for radio propagation measurements\nand demonstrates how institutions may merge disparate campaigns into a common\nformat. This data format, alongside an environmental map and a measurement\nsummary metadata table, enables integration of data from disparate sources by\nusing a structured representation of key parameters. Here, we show the efficacy\nof the point-data format standard using data gathered from two independent\nsub-THz urban microcell (UMi) campaigns: 142 GHz measurements at New York\nUniversity (NYU) and 145 GHz measurements at the University of Southern\nCalifornia (USC). A joint path loss analysis using the close-in path loss model\n(1 m ref. distance) yields a refined estimate of the path loss exponent (PLE)\nemploying the proposed standard to pool measurements. Other statistics such as\nRMS delay spread and angular spread are also determined using a joint\npoint-data table. Adopting this simple, unified format will accelerate channel\nmodel development, build multi-institutional datasets, and feed AI/ML\napplications with reliable training data in a common format from many sources."}
{"id": "2510.00342", "pdf": "https://arxiv.org/pdf/2510.00342", "abs": "https://arxiv.org/abs/2510.00342", "authors": ["Samuel Li", "Ian P. Roberts"], "title": "Site-Specific Beam Learning for Full-Duplex Massive MIMO Wireless Systems", "categories": ["eess.SP"], "comment": null, "summary": "Existing beamforming-based full-duplex solutions for multi-antenna wireless\nsystems often rely on explicit estimation of the self-interference channel. The\npilot overhead of such estimation, however, can be prohibitively high in\nmillimeter-wave and massive MIMO systems, thus limiting the practicality of\nexisting solutions, especially in fast-fading conditions. In this work, we\npresent a novel beam learning framework that bypasses explicit\nself-interference channel estimation by designing beam codebooks to efficiently\nobtain implicit channel knowledge that can then be processed by a deep learning\nnetwork to synthesize transmit and receive beams for full-duplex operation.\nSimulation results using ray-tracing illustrate that our proposed technique can\nallow a full-duplex base station to craft serving beams that couple low\nself-interference while delivering high SNR, with 75-97% fewer measurements\nthan would be required for explicit estimation of the self-interference\nchannel."}
{"id": "2510.00422", "pdf": "https://arxiv.org/pdf/2510.00422", "abs": "https://arxiv.org/abs/2510.00422", "authors": ["Kleanthis Avramidis", "Myzelle Hughes", "Idan A Blank", "Dani Byrd", "Assal Habibi", "Takfarinas Medani", "Richard M Leahy", "Shrikanth Narayanan"], "title": "A Point Process Model of Skin Conductance Responses in a Stroop Task for Predicting Depression and Suicidal Ideation", "categories": ["eess.SP"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Accurate identification of mental health biomarkers can enable earlier\ndetection and objective assessment of compromised mental well-being. In this\nstudy, we analyze electrodermal activity recorded during an Emotional Stroop\ntask to capture sympathetic arousal dynamics associated with depression and\nsuicidal ideation. We model the timing of skin conductance responses as a point\nprocess whose conditional intensity is modulated by task-based covariates,\nincluding stimulus valence, reaction time, and response accuracy. The resulting\nsubject-specific parameter vector serves as input to a machine learning\nclassifier for distinguishing individuals with and without depression. Our\nresults show that the model parameters encode meaningful physiological\ndifferences associated with depressive symptomatology and yield superior\nclassification performance compared to conventional feature extraction methods."}
{"id": "2510.00154", "pdf": "https://arxiv.org/pdf/2510.00154", "abs": "https://arxiv.org/abs/2510.00154", "authors": ["Xinyi Liu", "Mohammadreza Fani Sani", "Zewei Zhou", "Julius Wirbel", "Bahram Zarrin", "Roberto Galeazzi"], "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Despite rapid progress in autonomous robotics, executing complex or\nlong-horizon tasks remains a fundamental challenge. Most current approaches\nfollow an open-loop paradigm with limited reasoning and no feedback, resulting\nin poor robustness to environmental changes and severe error accumulation. We\npresent RoboPilot, a dual-thinking closed-loop framework for robotic\nmanipulation that supports adaptive reasoning for complex tasks in real-world\ndynamic environments. RoboPilot leverages primitive actions for structured task\nplanning and flexible action generation, while introducing feedback to enable\nreplanning from dynamic changes and execution errors. Chain-of-Thought\nreasoning further enhances high-level task planning and guides low-level action\ngeneration. The system dynamically switches between fast and slow thinking to\nbalance efficiency and accuracy. To systematically evaluate the robustness of\nRoboPilot in diverse robot manipulation scenarios, we introduce\nRoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including\ninfeasible-task recognition and failure recovery. Experiments show that\nRoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success\nrate, and the real-world deployment on an industrial robot further demonstrates\nits robustness in real-world settings."}
{"id": "2510.00550", "pdf": "https://arxiv.org/pdf/2510.00550", "abs": "https://arxiv.org/abs/2510.00550", "authors": ["Tai Le", "Hau Luu", "Loan Pham-Nguyen", "Hung Viet-Dao", "Duc Nguyen Minh", "Afshan B. Hameed", "Hoang Nguyen", "Liem Thanh Nguyen", "Huy-Dung Han", "Hung Cao"], "title": "Investigation of Using Non-Contact Electrodes for Fetal ECG Monitoring", "categories": ["eess.SP"], "comment": null, "summary": "Regular physiological monitoring of maternal and fetal parameters is\nindispensable for ensuring safe outcomes during pregnancy and parturition.\nFetal electrocardiogram (fECG) assessment is crucial to detect fetal distress\nand developmental anomalies. Given challenges of prenatal care due to the lack\nof medical professionals and the limit of accessibility, especially in remote\nand resource-poor areas, we develop a fECG monitoring system using novel\nnon-contact electrodes (NCE) to record the fetal/maternal ECG (f/mECG) signals\nthrough clothes, thereby improving the comfort during measurement. The system\nis designed to be incorporated inside a maternity belt with data acquisition,\ndata transmission module as well as novel NCEs. Thorough characterizations were\ncarried out to evaluate the novel NCE against traditional wet electrodes (i.e.,\nAg/AgCl electrodes), showing comparable performance. A successful {preliminary\npilot feasibility study} conducted with pregnant women (n = 10) between 25 and\n32 weeks of gestation demonstrates the system's performance, usability and\nsafety."}
{"id": "2510.00182", "pdf": "https://arxiv.org/pdf/2510.00182", "abs": "https://arxiv.org/abs/2510.00182", "authors": ["Jorge Mendez-Mendez"], "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Using large language models (LLMs) to solve complex robotics problems\nrequires understanding their planning capabilities. Yet while we know that LLMs\ncan plan on some problems, the extent to which these planning capabilities\ncover the space of robotics tasks is unclear. One promising direction is to\nintegrate the semantic knowledge of LLMs with the formal reasoning of task and\nmotion planning (TAMP). However, the myriad of choices for how to integrate\nLLMs within TAMP complicates the design of such systems. We develop 16\nalgorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our\nzero-shot experiments across 4,950 problems and three domains reveal that the\nGemini-based planners exhibit lower success rates and higher planning times\nthan their engineered counterparts. We show that providing geometric details\nincreases the number of task-planning errors compared to pure PDDL\ndescriptions, and that (faster) non-reasoning LLM variants outperform (slower)\nreasoning variants in most cases, since the TAMP system can direct the LLM to\ncorrect its mistakes."}
{"id": "2510.00562", "pdf": "https://arxiv.org/pdf/2510.00562", "abs": "https://arxiv.org/abs/2510.00562", "authors": ["Shingo Takemoto", "Shunsuke Ono"], "title": "Geometric Spatio-Spectral Total Variation for Hyperspectral Image Denoising and Destriping", "categories": ["eess.SP"], "comment": "Submitted to IEEE Open Journal of Signal Processing. The source code\n  is available at\n  https://github.com/MDI-TokyoTech/Geometric-Spatio-Spectral-Total-Variation", "summary": "This article proposes a novel regularization method, named Geometric\nSpatio-Spectral Total Variation (GeoSSTV), for hyperspectral (HS) image\ndenoising and destriping. HS images are inevitably affected by various types of\nnoise due to the measurement equipment and environment. Total Variation\n(TV)-based regularization methods that model the spatio-spectral piecewise\nsmoothness inherent in HS images are promising approaches for HS image\ndenoising and destriping. However, existing TV-based methods are based on\nclassical anisotropic and isotropic TVs, which cause staircase artifacts and\nlack rotation invariance, respectively, making it difficult to accurately\nrecover round structures and oblique edges. To address this issue, GeoSSTV\nintroduces a geometrically consistent formulation of TV that measures\nvariations across all directions in a Euclidean manner. Through this\nformulation, GeoSSTV removes noise while preserving round structures and\noblique edges. Furthermore, we formulate the HS image denoising problem as a\nconstrained convex optimization problem involving GeoSSTV and develop an\nefficient algorithm based on a preconditioned primal-dual splitting method.\nExperimental results on HS images contaminated with mixed noise demonstrate the\nsuperiority of the proposed method over existing approaches."}
{"id": "2510.00188", "pdf": "https://arxiv.org/pdf/2510.00188", "abs": "https://arxiv.org/abs/2510.00188", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its\nheavy computational load often prevents application in robotic systems. Some\nstudies have attempted to approximate NMPC using deep neural networks\n(NMPC-DNN). However, in the presence of unexpected disturbances or when\noperating conditions differ from training data, this approach lacks robustness,\nleading to large tracking errors. To address this issue, for the first time,\nthe NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The\nproposed controller is validated by applying it to an exoskeleton robot during\nsquat movement, which has a complex dynamic model and has received limited\nattention regarding robust nonlinear control design. A human-robot dynamic\nmodel with three active joints (ankle, knee, hip) is developed, and more than\n5.3 million training samples are used to train the DNN. The results show that,\nunder unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI\nis significantly lower compared to NMPC-DNN. Moreover, human joint torques are\ngreatly reduced with the use of the exoskeleton, with RMS values for the\nstudied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,\nrespectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is\n99.93% lower than that of NMPC."}
{"id": "2510.00581", "pdf": "https://arxiv.org/pdf/2510.00581", "abs": "https://arxiv.org/abs/2510.00581", "authors": ["Zhuoran Li", "Zhen Gao", "Boyu Ning", "Zhaocheng Wang"], "title": "Radiation Pattern Reconfigurable FAS-Empowered Interference-Resilient UAV Communication", "categories": ["eess.SP"], "comment": "Simulation codes are provided to reproduce the results in this paper:\n  \\href{https://github.com/LiZhuoRan0/2025-JSAC-RadiationPatternReconfigurableAntenna}{https://github.com/LiZhuoRan0}", "summary": "The widespread use of uncrewed aerial vehicles (UAVs) has propelled the\ndevelopment of advanced techniques on countering unauthorized UAV flights.\nHowever, the resistance of legal UAVs to illegal interference remains\nunder-addressed. This paper proposes radiation pattern reconfigurable fluid\nantenna systems (RPR-FAS)-empowered interference-resilient UAV communication\nscheme. This scheme integrates the reconfigurable pixel antenna technology,\nwhich provides each antenna with an adjustable radiation pattern. Therefore,\nRPR-FAS can enhance the angular resolution of a UAV with a limited number of\nantennas, thereby improving spectral efficiency (SE) and interference\nresilience. Specifically, we first design dedicated radiation pattern adapted\nfrom 3GPP-TR-38.901, where the beam direction and half power beamwidth are\ntailored for UAV communications. Furthermore, we propose a low-storage-overhead\northogonal matching pursuit multiple measurement vectors algorithm, which\naccurately estimates the angle-of-arrival (AoA) of the communication link, even\nin the single antenna case. Particularly, by utilizing the Fourier transform to\nthe radiation pattern gain matrix, we design a dimension-reduction technique to\nachieve 1--2 order-of-magnitude reduction in storage requirements. Meanwhile,\nwe propose a maximum likelihood interference AoA estimation method based on the\nlaw of large numbers, so that the SE can be further improved. Finally,\nalternating optimization is employed to obtain the optimal uplink radiation\npattern and combiner, while an exhaustive search is applied to determine the\noptimal downlink pattern, complemented by the water-filling algorithm for\nbeamforming. Comprehensive simulations demonstrate that the proposed schemes\noutperform traditional methods in terms of angular sensing precision and\nspectral efficiency."}
{"id": "2510.00225", "pdf": "https://arxiv.org/pdf/2510.00225", "abs": "https://arxiv.org/abs/2510.00225", "authors": ["Yue Meng", "Fei Chen", "Chuchu Fan"], "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO"}
{"id": "2510.00696", "pdf": "https://arxiv.org/pdf/2510.00696", "abs": "https://arxiv.org/abs/2510.00696", "authors": ["Ferdaous Tarhouni", "Muneer AlZubi", "Mohamed-Slim Alouini"], "title": "Machine Learning-based Path Loss Prediction in Suburban Environment in the Sub-6 GHz Band", "categories": ["eess.SP"], "comment": null, "summary": "Accurate path loss (PL) prediction is crucial for successful network\nplanning, antenna design, and performance optimization in wireless\ncommunication systems. Several conventional approaches for PL prediction have\nbeen adopted, but they have been demonstrated to lack flexibility and accuracy.\nIn this work, we investigate the effectiveness of Machine Learning (ML) models\nin predicting PL, particularly for the sub-6 GHz band in a suburban campus of\nKing Abdullah University of Science and Technology (KAUST). For training\npurposes, we generate synthetic datasets using the ray-tracing simulation\ntechnique. The feasibility and accuracy of the ML-based PL models are verified\nand validated using both synthetic and measurement datasets. The random forest\nregression (RFR) and the K-nearest neighbors (KNN) algorithms provide the best\nPL prediction accuracy compared to other ML models. In addition, we compare the\nperformance of the developed ML-based PL models with the traditional\npropagation models, including COST-231 Hata, Longley-Rice, and Close-in models.\nThe results show the superiority of the ML-based PL models compared to\nconventional models. Therefore, the ML approach using the ray-tracing technique\ncan provide a promising and cost-effective solution for predicting and modeling\nradio wave propagation in various scenarios in a flexible manner."}
{"id": "2510.00272", "pdf": "https://arxiv.org/pdf/2510.00272", "abs": "https://arxiv.org/abs/2510.00272", "authors": ["Odichimnma Ezeji", "Michael Ziegltrum", "Giulio Turrisi", "Tommaso Belvedere", "Valerio Modugno"], "title": "BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control", "categories": ["cs.RO"], "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has recently emerged as a fast,\ngradient-free alternative to model-predictive control in highly non-linear\nrobotic tasks, yet it offers no hard guarantees on constraint satisfaction. We\nintroduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that\nattaches a probabilistic surrogate to every state and input constraint. At each\nre-planning step the surrogate returns the probability that a candidate\ntrajectory is feasible; this joint probability scales the weight given to a\ncandidate, automatically down-weighting rollouts likely to collide or exceed\nlimits and pushing the sampling distribution toward the safe subset; no\nhand-tuned penalty costs or explicit sample rejection required. We train the\nsurrogate from 1000 offline simulations and deploy the controller on a\nquadrotor in MuJoCo with both static and moving obstacles. Across K in\n[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the\nprescribed probability of violation. Because the surrogate is a stand-alone,\nversion-controlled artefact and the runtime safety score is a single scalar,\nthe approach integrates naturally with verification-and-validation pipelines\nfor certifiable autonomous systems."}
{"id": "2510.00816", "pdf": "https://arxiv.org/pdf/2510.00816", "abs": "https://arxiv.org/abs/2510.00816", "authors": ["Fernando Moya Caceres", "Akram Al-Hourani", "Saman Atapattu", "Kandeepan Sithamparanathan"], "title": "Null-Shaping for Interference Mitigation in LEO Satellites Under Location Uncertainty", "categories": ["eess.SP"], "comment": "6 pages, 8 figures, GLOBECOM 2025", "summary": "Radio frequency interference (RFI) poses a growing challenge to satellite\ncommunications, particularly in uplink channels of Low Earth Orbit (LEO)\nsystems, due to increasing spectrum congestion and uncertainty in the location\nof terrestrial interferers. This paper addresses the impact of RFI source\nposition uncertainty on beamforming-based interference mitigation. First, we\nanalytically characterize how geographic uncertainty in RFI location translates\ninto angular deviation as observed from the satellite. Building on this, we\npropose a robust null-shaping framework to increase resilience in the\ncommunication links by incorporating the probability density function (PDF) of\nthe RFI location uncertainty into the beamforming design via stochastic\noptimization. This allows adaptive shaping of the antenna array's nulling\npattern to enhance interference suppression under uncertainty. Extensive Monte\nCarlo simulations, incorporating realistic satellite orbital dynamics and\nvarious RFI scenarios, demonstrate that the proposed approach achieves\nsignificantly improved mitigation performance compared to conventional\ndeterministic designs."}
{"id": "2510.00329", "pdf": "https://arxiv.org/pdf/2510.00329", "abs": "https://arxiv.org/abs/2510.00329", "authors": ["Sarmad Mehrdad", "Maxime Sabbah", "Vincent Bonnet", "Ludovic Righetti"], "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "This paper investigates the application of Minimal Observation Inverse\nReinforcement Learning (MO-IRL) to model and predict human arm-reaching\nmovements with time-varying cost weights. Using a planar two-link biomechanical\nmodel and high-resolution motion-capture data from subjects performing a\npointing task, we segment each trajectory into multiple phases and learn\nphase-specific combinations of seven candidate cost functions. MO-IRL\niteratively refines cost weights by scaling observed and generated trajectories\nin the maximum entropy IRL formulation, greatly reducing the number of required\ndemonstrations and convergence time compared to classical IRL approaches.\nTraining on ten trials per posture yields average joint-angle Root Mean Squared\nErrors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight\ndivisions, respectively, versus 10.4 deg using a single static weight.\nCross-validation on remaining trials and, for the first time, inter-subject\nvalidation on an unseen subject's 20 trials, demonstrates comparable predictive\naccuracy, around 8 deg RMSE, indicating robust generalization. Learned weights\nemphasize joint acceleration minimization during movement onset and\ntermination, aligning with smoothness principles observed in biological motion.\nThese results suggest that MO-IRL can efficiently uncover dynamic,\nsubject-independent cost structures underlying human motor control, with\npotential applications for humanoid robots."}
{"id": "2510.00838", "pdf": "https://arxiv.org/pdf/2510.00838", "abs": "https://arxiv.org/abs/2510.00838", "authors": ["Hasnul Hashim"], "title": "Effectiveness of Reconfigurable Intelligent Surface in Multipath Fading Channel", "categories": ["eess.SP"], "comment": "8 pages, 16 figures", "summary": "A method of simulating a single-input single-output reconfigurable\nintelligent surface (RIS) assisted channel is presented using three channel\nblack boxes to represent the direct signal path, the transmit path to the RIS\nand the reflected path from the RIS. The complex coefficients for each channel\nbox is obtained by ray tracing in a scenario with geographic terrain\ninformation that also contains approximate building shapes. The electrical\ncharacteristics of the ground and building walls were also accounted for in the\nray tracing function. Simulations were conducted with reflected rays only and\nreflected rays together with diffracted rays. The received power exhibits\nvariations typical of multipath fading environments. In the best locations, the\nRIS-assisted channel simulation result agrees well with theoretical models, the\nperformance increasing by the RIS size squared as the number of RIS elements is\nincreased. In the simplified theoretical model where the transmitter and\nreceiver are inline and the RIS orthogonal but much closer than the distance\nbetween the former elements, the simulation results also corroborate best\ndeployment close the transmitter or the receiver with a U-shaped drop between\nthem."}
{"id": "2510.00358", "pdf": "https://arxiv.org/pdf/2510.00358", "abs": "https://arxiv.org/abs/2510.00358", "authors": ["Linjin He", "Xinda Qi", "Dong Chen", "Zhaojian Li", "Xiaobo Tan"], "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control."}
{"id": "2510.00851", "pdf": "https://arxiv.org/pdf/2510.00851", "abs": "https://arxiv.org/abs/2510.00851", "authors": ["Abdelaziz Salama", "Mohammed M. H. Qazzaz", "Zeinab Nezami", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Agentic AI meets Neural Architecture Search: Proactive Traffic Prediction for AI-RAN", "categories": ["eess.SP"], "comment": null, "summary": "Next-generation wireless networks require intelligent traffic prediction to\nenable autonomous resource management and handle diverse, dynamic service\ndemands. The Open Radio Access Network (O-RAN) framework provides a promising\nfoundation for embedding machine learning intelligence through its\ndisaggregated architecture and programmable interfaces. This work applies a\nNeural Architecture Search (NAS)-based framework that dynamically selects and\norchestrates efficient Long Short-Term Memory (LSTM) architectures for traffic\nprediction in O-RAN environments. Our approach leverages the O-RAN paradigm by\nseparating architecture optimisation (via non-RT RIC rApps) from real-time\ninference (via near-RT RIC xApps), enabling adaptive model deployment based on\ntraffic conditions and resource constraints. Experimental evaluation across six\nLSTM architectures demonstrates that lightweight models achieve $R^2 \\approx\n0.91$--$0.93$ with high efficiency for regular traffic, while complex models\nreach near-perfect accuracy ($R^2 = 0.989$--$0.996$) during critical scenarios.\nOur NAS-based orchestration achieves a 70-75\\% reduction in computational\ncomplexity compared to static high-performance models, while maintaining high\nprediction accuracy when required, thereby enabling scalable deployment in\nreal-world edge environments."}
{"id": "2510.00401", "pdf": "https://arxiv.org/pdf/2510.00401", "abs": "https://arxiv.org/abs/2510.00401", "authors": ["Shounak Sural", "Charles Kekeh", "Wenliang Liu", "Federico Pecora", "Mouhacine Benosman"], "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models."}
{"id": "2510.00896", "pdf": "https://arxiv.org/pdf/2510.00896", "abs": "https://arxiv.org/abs/2510.00896", "authors": ["Romina Garcia Camargo", "Zhiyang Wang", "Alejandro Ribeiro"], "title": "Graph Neural Networks in Large Scale Wireless Communication Networks: Scalability Across Random Geometric Graphs", "categories": ["eess.SP"], "comment": null, "summary": "The growing complexity of wireless systems has accelerated the move from\ntraditional methods to learning-based solutions. Graph Neural Networks (GNNs)\nare especially well-suited here, since wireless networks can be naturally\nrepresented as graphs. A key property of GNNs is transferability: models\ntrained on one graph often generalize to much larger graphs with little\nperformance loss. While empirical studies have shown that GNN-based wireless\npolicies transfer effectively, existing theoretical guarantees do not capture\nthis phenomenon. Most works focus on dense graphs where node degrees scale with\nnetwork size, an assumption that fails in wireless systems. In this work, we\nprovide a formal theoretical foundation for transferability on Random Geometric\nGraphs (RGGs), a sparse and widely used model of wireless networks. We further\nvalidate our results through numerical experiments on power allocation, a\nfundamental resource management task."}
{"id": "2510.00406", "pdf": "https://arxiv.org/pdf/2510.00406", "abs": "https://arxiv.org/abs/2510.00406", "authors": ["Hengtao Li", "Pengxiang Ding", "Runze Suo", "Yihao Wang", "Zirui Ge", "Dongyuan Zang", "Kexian Yu", "Mingyang Sun", "Hongyin Zhang", "Donglin Wang", "Weihua Su"], "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/."}
{"id": "2510.00934", "pdf": "https://arxiv.org/pdf/2510.00934", "abs": "https://arxiv.org/abs/2510.00934", "authors": ["Junwei Ji", "Dongyuan Shi", "Zhengding Luo", "Boxiang Wang", "Ziyi Yang", "Haowen Li", "Woon-Seng Gan"], "title": "A Robust Proactive Communication Strategy for Distributed Active Noise Control Systems", "categories": ["eess.SP", "eess.AS"], "comment": null, "summary": "Distributed multichannel active noise control (DMCANC) systems assign the\nhigh computational load of conventional centralized algorithms across multiple\nprocessing nodes, leveraging inter-node communication to collaboratively\nsuppress unwanted noise. However, communication overhead can undermine\nalgorithmic stability and degrade overall performance. To address this\nchallenge, we propose a robust communication framework that integrates\nadaptive-fixed-filter switching and the mixed-gradient combination strategy. In\nthis approach, each node independently executes a single-channel filtered\nreference least mean square (FxLMS) algorithm while monitoring real-time noise\nreduction levels. When the current noise reduction performance degrades\ncompared to the previous state, the node halts its adaptive algorithm, switches\nto a fixed filter, and simultaneously initiates a communication request. The\nexchanged information comprises the difference between the current control\nfilter and the filter at the time of the last communication, equivalent to the\naccumulated gradient sum during non-communication intervals. Upon receiving\nneighboring cumulative gradients, the node employs a mixed-gradient combination\nmethod to update its control filter, subsequently reverting to the adaptive\nmode. This proactive communication strategy and adaptive-fixed switching\nmechanism ensure system robustness by mitigating instability risks caused by\ncommunication issues. Simulations demonstrate that the proposed method achieves\nnoise reduction performance comparable to centralized algorithms while\nmaintaining stability under communication constraints, highlighting its\npractical applicability in real-world distributed ANC scenarios."}
{"id": "2510.00441", "pdf": "https://arxiv.org/pdf/2510.00441", "abs": "https://arxiv.org/abs/2510.00441", "authors": ["Yiyuan Pan", "Yunzhe Xu", "Zhe Liu", "Hesheng Wang"], "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents."}
{"id": "2510.00148", "pdf": "https://arxiv.org/pdf/2510.00148", "abs": "https://arxiv.org/abs/2510.00148", "authors": ["Abu Hasnat Mohammad Rubaiyat", "Jordan Vincent", "Colin Olson"], "title": "Improved Hyperspectral Anomaly Detection via Unsupervised Subspace Modeling in the Signed Cumulative Distribution Transform Domain", "categories": ["cs.CV", "eess.SP"], "comment": "8 pages, 8 figures", "summary": "Hyperspectral anomaly detection (HAD), a crucial approach for many civilian\nand military applications, seeks to identify pixels with spectral signatures\nthat are anomalous relative to a preponderance of background signatures.\nSignificant effort has been made to improve HAD techniques, but challenges\narise due to complex real-world environments and, by definition, limited prior\nknowledge of potential signatures of interest. This paper introduces a novel\nHAD method by proposing a transport-based mathematical model to describe the\npixels comprising a given hyperspectral image. In this approach, hyperspectral\npixels are viewed as observations of a template pattern undergoing unknown\ndeformations that enables their representation in the signed cumulative\ndistribution transform (SCDT) domain. An unsupervised subspace modeling\ntechnique is then used to construct a model of abundant background signals in\nthis domain, whereupon anomalous signals are detected as deviations from the\nlearned model. Comprehensive evaluations across five distinct datasets\nillustrate the superiority of our approach compared to state-of-the-art\nmethods."}
{"id": "2510.00466", "pdf": "https://arxiv.org/pdf/2510.00466", "abs": "https://arxiv.org/abs/2510.00466", "authors": ["Run Su", "Hao Fu", "Shuai Zhou", "Yingao Fu"], "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications."}
{"id": "2510.00180", "pdf": "https://arxiv.org/pdf/2510.00180", "abs": "https://arxiv.org/abs/2510.00180", "authors": ["Amit Milstein", "Nir Shlezinger", "Boaz Rafaely"], "title": "DiffAU: Diffusion-Based Ambisonics Upscaling", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": null, "summary": "Spatial audio enhances immersion by reproducing 3D sound fields, with\nAmbisonics offering a scalable format for this purpose. While first-order\nAmbisonics (FOA) notably facilitates hardware-efficient acquisition and storage\nof sound fields as compared to high-order Ambisonics (HOA), its low spatial\nresolution limits realism, highlighting the need for Ambisonics upscaling (AU)\nas an approach for increasing the order of Ambisonics signals. In this work we\npropose DiffAU, a cascaded AU method that leverages recent developments in\ndiffusion models combined with novel adaptation to spatial audio to generate\n3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides\na principled approach that rapidly and reliably reproduces HOA in various\nsettings. Experiments in anechoic conditions with multiple speakers, show\nstrong objective and perceptual performance."}
{"id": "2510.00491", "pdf": "https://arxiv.org/pdf/2510.00491", "abs": "https://arxiv.org/abs/2510.00491", "authors": ["Han Zhou", "Jinjin Cao", "Liyuan Ma", "Xueji Fang", "Guo-jun Qi"], "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/."}
{"id": "2510.00381", "pdf": "https://arxiv.org/pdf/2510.00381", "abs": "https://arxiv.org/abs/2510.00381", "authors": ["Kaiwen Yu", "Mengying Sun", "Zhijin Qin", "Xiaodong Xu", "Ping Yang", "Yue Xiao", "Gang Wu"], "title": "Semantic-Driven AI Agent Communications: Challenges and Solutions", "categories": ["cs.AI", "eess.SP"], "comment": null, "summary": "With the rapid growth of intelligent services, communication targets are\nshifting from humans to artificial intelligent (AI) agents, which require new\nparadigms to enable real-time perception, decision-making, and collaboration.\nSemantic communication, which conveys task-relevant meaning rather than raw\ndata, offers a promising solution. However, its practical deployment remains\nconstrained by dynamic environments and limited resources. To address these\nissues, this article proposes a semantic-driven AI agent communication\nframework and develops three enabling techniques. First, semantic adaptation\ntransmission applies fine-tuning with real or generative samples to efficiently\nadapt models to varying environments. Second, semantic lightweight transmission\nincorporates pruning, quantization, and perception-aware sampling to reduce\nmodel complexity and alleviate computational burden on edge agents. Third,\nsemantic self-evolution control employs distributed hierarchical\ndecision-making to optimize multi-dimensional resources, enabling robust\nmulti-agent collaboration in dynamic environments. Simulation results show that\nthe proposed solutions achieve faster convergence and stronger robustness,\nwhile the proposed distributed hierarchical optimization method significantly\noutperforms conventional decision-making schemes, highlighting its potential\nfor AI agent communication networks."}
{"id": "2510.00524", "pdf": "https://arxiv.org/pdf/2510.00524", "abs": "https://arxiv.org/abs/2510.00524", "authors": ["Baoshan Song", "Penggao Yan", "Xiao Xia", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion", "categories": ["cs.RO"], "comment": null, "summary": "Reliable GNSS positioning in complex environments remains a critical\nchallenge due to non-line-of-sight (NLOS) propagation, multipath effects, and\nfrequent signal blockages. These effects can easily introduce large outliers\ninto the raw pseudo-range measurements, which significantly degrade the\nperformance of global navigation satellite system (GNSS) real-time kinematic\n(RTK) positioning and limit the effectiveness of tightly coupled GNSS-based\nintegrated navigation system. To address this issue, we propose a two-stage\noutlier detection method and apply the method in a tightly coupled GNSS-RTK,\ninertial navigation system (INS), and odometer integration based on factor\ngraph optimization (FGO). In the first stage, Doppler measurements are employed\nto detect pseudo-range outliers in a GNSS-only manner, since Doppler is less\nsensitive to multipath and NLOS effects compared with pseudo-range, making it a\nmore stable reference for detecting sudden inconsistencies. In the second\nstage, pre-integrated inertial measurement units (IMU) and odometer constraints\nare used to generate predicted double-difference pseudo-range measurements,\nwhich enable a more refined identification and rejection of remaining outliers.\nBy combining these two complementary stages, the system achieves improved\nrobustness against both gross pseudo-range errors and degraded satellite\nmeasuring quality. The experimental results demonstrate that the two-stage\ndetection framework significantly reduces the impact of pseudo-range outliers,\nand leads to improved positioning accuracy and consistency compared with\nrepresentative baseline approaches. In the deep urban canyon test, the outlier\nmitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52\nm to 0.30 m, with 42.3% improvement."}
{"id": "2510.00463", "pdf": "https://arxiv.org/pdf/2510.00463", "abs": "https://arxiv.org/abs/2510.00463", "authors": ["Daofu Zhang", "Mehrdad Pournaderi", "Hanne M. Clifford", "Yu Xiang", "Pramod K. Varshney"], "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "comment": null, "summary": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."}
{"id": "2510.00573", "pdf": "https://arxiv.org/pdf/2510.00573", "abs": "https://arxiv.org/abs/2510.00573", "authors": ["Yen-Ling Tai", "Yi-Ru Yang", "Kuan-Ting Yu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Robotic food scooping is a critical manipulation skill for food preparation\nand service robots. However, existing robot learning algorithms, especially\nlearn-from-demonstration methods, still struggle to handle diverse and dynamic\nfood states, which often results in spillage and reduced reliability. In this\nwork, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot\nFood Scooping Tasks. This framework leverages guided diffusion policy to\nminimize food spillage during scooping and to ensure reliable transfer of food\nitems from the initial to the target location. Specifically, we design a\nspillage predictor that estimates the probability of spillage given current\nobservation and action rollout. The predictor is trained on a simulated dataset\nwith food spillage scenarios, constructed from four primitive shapes (spheres,\ncubes, cones, and cylinders) with varied physical properties such as mass,\nfriction, and particle size. At inference time, the predictor serves as a\ndifferentiable guidance signal, steering the diffusion sampling process toward\nsafer trajectories while preserving task success. We validate GRITS on a\nreal-world robotic food scooping platform. GRITS is trained on six food\ncategories and evaluated on ten unseen categories with different shapes and\nquantities. GRITS achieves an 82% task success rate and a 4% spillage rate,\nreducing spillage by over 40% compared to baselines without guidance, thereby\ndemonstrating its effectiveness."}
{"id": "2510.00638", "pdf": "https://arxiv.org/pdf/2510.00638", "abs": "https://arxiv.org/abs/2510.00638", "authors": ["Wing Chau Ng", "Scott Yam"], "title": "On the Achievable Performance in the presence of Multiple Path Interference for Intra Data Center applications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "Submitted to European Conference on Optical Communications (ECOC)\n  2025", "summary": "An accurate analytical form of the achievable bit error rate in the presence\nof multipath interference (MPI) is proposed for PAM4 for the first time, taking\ninto account an ideal MPI estimate and compensation."}
{"id": "2510.00600", "pdf": "https://arxiv.org/pdf/2510.00600", "abs": "https://arxiv.org/abs/2510.00600", "authors": ["Pietro Mazzaglia", "Cansu Sancaktar", "Markus Peschl", "Daniel Dijkman"], "title": "Hybrid Training for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments."}
{"id": "2510.00771", "pdf": "https://arxiv.org/pdf/2510.00771", "abs": "https://arxiv.org/abs/2510.00771", "authors": ["Woongjib Choi", "Sangmin Lee", "Hyungseob Lim", "Hong-Goo Kang"], "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets."}
{"id": "2510.00619", "pdf": "https://arxiv.org/pdf/2510.00619", "abs": "https://arxiv.org/abs/2510.00619", "authors": ["Michiel Braat", "Maren Buermann", "Marijke van Weperen", "Jan-Pieter Paardekooper"], "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication in proceedings of the 2025 IEEE\n  International Automated Vehicle Validation Conference", "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving."}
{"id": "2510.00831", "pdf": "https://arxiv.org/pdf/2510.00831", "abs": "https://arxiv.org/abs/2510.00831", "authors": ["Julian Oelhaf", "Georg Kordowich", "Changhun Kim", "Paula Andrea Pérez-Toro", "Christian Bergler", "Andreas Maier", "Johann Jäger", "Siming Bayer"], "title": "Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": "Submitted to ICASSP 2026; under review", "summary": "The increasing integration of distributed energy resources (DERs),\nparticularly renewables, poses significant challenges for power system\nprotection, with fault classification (FC) and fault localization (FL) being\namong the most critical tasks. Conventional protection schemes, based on fixed\nthresholds, cannot reliably identify and localize short circuits with the\nincreasing complexity of the grid under dynamic conditions. Machine learning\n(ML) offers a promising alternative; however, systematic benchmarks across\nmodels and settings remain limited. This work presents, for the first time, a\ncomparative benchmarking study of classical ML models for FC and FL in power\nsystem protection based on EMT data. Using voltage and current waveforms\nsegmented into sliding windows of 10 ms to 50 ms, we evaluate models under\nrealistic real-time constraints. Performance is assessed in terms of accuracy,\nrobustness to window size, and runtime efficiency. The best-performing FC model\nachieved an F1 score of 0.992$\\pm$0.001, while the top FL model reached an R2\nof 0.806$\\pm$0.008 with a mean processing time of 0.563 ms."}
{"id": "2510.00630", "pdf": "https://arxiv.org/pdf/2510.00630", "abs": "https://arxiv.org/abs/2510.00630", "authors": ["Federico Oliva", "Tom Shaked", "Daniele Carnevale", "Amir Degani"], "title": "Trajectory Based Observer Design: A Framework for Lightweight Sensor Fusion", "categories": ["cs.RO"], "comment": null, "summary": "Efficient observer design and accurate sensor fusion are key in state\nestimation. This work proposes an optimization-based methodology, termed\nTrajectory Based Optimization Design (TBOD), allowing the user to easily design\nobservers for general nonlinear systems and multi-sensor setups. Starting from\nparametrized observer dynamics, the proposed method considers a finite set of\npre-recorded measurement trajectories from the nominal plant and exploits them\nto tune the observer parameters through numerical optimization. This research\nhinges on the classic observer's theory and Moving Horizon Estimators\nmethodology. Optimization is exploited to ease the observer's design, providing\nthe user with a lightweight, general-purpose sensor fusion methodology. TBOD's\nmain characteristics are the capability to handle general sensors efficiently\nand in a modular way and, most importantly, its straightforward tuning\nprocedure. The TBOD's performance is tested on a terrestrial rover localization\nproblem, combining IMU and ranging sensors provided by Ultra Wide Band\nantennas, and validated through a motion-capture system. Comparison with an\nExtended Kalman Filter is also provided, matching its position estimation\naccuracy and significantly improving in the orientation."}
{"id": "2510.01022", "pdf": "https://arxiv.org/pdf/2510.01022", "abs": "https://arxiv.org/abs/2510.01022", "authors": ["David R. Johnson", "Rishabh Anand", "Smita Krishnaswamy", "Michael Perlmutter"], "title": "Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "Accepted for presentation at the NeurIPS workshop on New Perspectives\n  in Advancing Graph Machine Learning", "summary": "We introduce a novel version of the geometric scattering transform for\ngeometric graphs containing scalar and vector node features. This new\nscattering transform has desirable symmetries with respect to rigid-body\nroto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into a\ngeometric GNN framework. We empirically show that our equivariant\nscattering-based GNN achieves comparable performance to other equivariant\nmessage-passing-based GNNs at a fraction of the parameter count."}
{"id": "2510.00646", "pdf": "https://arxiv.org/pdf/2510.00646", "abs": "https://arxiv.org/abs/2510.00646", "authors": ["Haoyang Wang", "Xinyu Luo", "Wenhua Ding", "Jingao Xu", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Haitao Zhang", "Yunhao Liu", "Xinlei Chen"], "title": "Enabling High-Frequency Cross-Modality Visual Positioning Service for Accurate Drone Landing", "categories": ["cs.RO"], "comment": "15 pages, 23 figures", "summary": "After years of growth, drone-based delivery is transforming logistics. At its\ncore, real-time 6-DoF drone pose tracking enables precise flight control and\naccurate drone landing. With the widespread availability of urban 3D maps, the\nVisual Positioning Service (VPS), a mobile pose estimation system, has been\nadapted to enhance drone pose tracking during the landing phase, as\nconventional systems like GPS are unreliable in urban environments due to\nsignal attenuation and multi-path propagation. However, deploying the current\nVPS on drones faces limitations in both estimation accuracy and efficiency. In\nthis work, we redesign drone-oriented VPS with the event camera and introduce\nEV-Pose to enable accurate, high-frequency 6-DoF pose tracking for accurate\ndrone landing. EV-Pose introduces a spatio-temporal feature-instructed pose\nestimation module that extracts a temporal distance field to enable 3D point\nmap matching for pose estimation; and a motion-aware hierarchical fusion and\noptimization scheme to enhance the above estimation in accuracy and efficiency,\nby utilizing drone motion in the \\textit{early stage} of event filtering and\nthe \\textit{later stage} of pose optimization. Evaluation shows that EV-Pose\nachieves a rotation accuracy of 1.34$\\degree$ and a translation accuracy of\n6.9$mm$ with a tracking latency of 10.08$ms$, outperforming baselines by\n$>$50\\%, \\tmcrevise{thus enabling accurate drone landings.} Demo:\nhttps://ev-pose.github.io/"}
{"id": "2510.01175", "pdf": "https://arxiv.org/pdf/2510.01175", "abs": "https://arxiv.org/abs/2510.01175", "authors": ["Yudong Wei", "Liang Zhang", "Bingcong Li", "Niao He"], "title": "On the Benefits of Weight Normalization for Overparameterized Matrix Sensing", "categories": ["cs.LG", "eess.SP", "math.OC", "stat.ML"], "comment": null, "summary": "While normalization techniques are widely used in deep learning, their\ntheoretical understanding remains relatively limited. In this work, we\nestablish the benefits of (generalized) weight normalization (WN) applied to\nthe overparameterized matrix sensing problem. We prove that WN with Riemannian\noptimization achieves linear convergence, yielding an exponential speedup over\nstandard methods that do not use WN. Our analysis further demonstrates that\nboth iteration and sample complexity improve polynomially as the level of\noverparameterization increases. To the best of our knowledge, this work\nprovides the first characterization of how WN leverages overparameterization\nfor faster convergence in matrix sensing."}
{"id": "2510.00682", "pdf": "https://arxiv.org/pdf/2510.00682", "abs": "https://arxiv.org/abs/2510.00682", "authors": ["Shengzhi Wang", "Niels Dehio", "Xuanqi Zeng", "Xian Yang", "Lingwei Zhang", "Yun-Hui Liu", "K. W. Samuel Au"], "title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "comment": "8 pages, 9 figures, submitted to The 2026 American Control Conference", "summary": "Utilizing teams of multiple robots is advantageous for handling bulky\nobjects. Many related works focus on multi-manipulator systems, which are\nlimited by workspace constraints. In this paper, we extend a classical hybrid\nmotion-force controller to a team of legged manipulator systems, enabling\ncollaborative loco-manipulation of rigid objects with a force-closed grasp. Our\nnovel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport, validated\nthrough extensive simulations and real-world experiments."}
{"id": "2510.00695", "pdf": "https://arxiv.org/pdf/2510.00695", "abs": "https://arxiv.org/abs/2510.00695", "authors": ["Myungkyu Koo", "Daewon Choi", "Taeyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Youngyo Seo", "Jinwoo Shin"], "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://myungkyukoo.github.io/hamlet/", "summary": "Inherently, robotic manipulation tasks are history-dependent: leveraging past\ncontext could be beneficial. However, most existing Vision-Language-Action\nmodels (VLAs) have been designed without considering this aspect, i.e., they\nrely solely on the current observation, ignoring preceding context. In this\npaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the\nhistorical context during action prediction. Specifically, we introduce moment\ntokens that compactly encode perceptual information at each timestep. Their\nrepresentations are initialized with time-contrastive learning, allowing them\nto better capture temporally distinctive aspects. Next, we employ a lightweight\nmemory module that integrates the moment tokens across past timesteps into\nmemory features, which are then leveraged for action prediction. Through\nempirical evaluation, we show that HAMLET successfully transforms a\nstate-of-the-art VLA into a history-aware policy, especially demonstrating\nsignificant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of\n76.4% on history-dependent real-world tasks, surpassing the baseline\nperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from\n64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on\nLIBERO, highlighting its effectiveness even under generic robot-manipulation\nbenchmarks."}
{"id": "2510.00703", "pdf": "https://arxiv.org/pdf/2510.00703", "abs": "https://arxiv.org/abs/2510.00703", "authors": ["Andrea Bussolan", "Stefano Baraldo", "Oliver Avram", "Pablo Urcola", "Luis Montesano", "Luca Maria Gambardella", "Anna Valente"], "title": "MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration", "categories": ["cs.RO"], "comment": null, "summary": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to\nenhance worker productivity while ensuring well-being. The ability to perceive\nhuman psycho-physical states, such as stress and cognitive load, is crucial for\nadaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a\nmultimodal dataset containing physiological, audio, and facial data collected\nduring real-world HRC scenarios. The dataset includes electroencephalography\n(EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration\n(RESP), electromyography (EMG), voice recordings, and facial action units. The\ndataset integrates controlled cognitive tasks, immersive virtual reality\nexperiences, and industrial disassembly activities performed manually and with\nrobotic assistance, to capture a holistic view of the participants' mental\nstates. Rich ground truth annotations were obtained using validated\npsychological self-assessment questionnaires. Baseline models were evaluated\nfor stress and cognitive load classification, demonstrating the dataset's\npotential for affective computing and human-aware robotics research.\nMultiPhysio-HRC is publicly available to support research in human-centered\nautomation, workplace well-being, and intelligent robotic systems."}
{"id": "2510.00726", "pdf": "https://arxiv.org/pdf/2510.00726", "abs": "https://arxiv.org/abs/2510.00726", "authors": ["Giovanni Minelli", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Code and data available at https://github.com/iit-DLSLab/croSTAta", "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks."}
{"id": "2510.00770", "pdf": "https://arxiv.org/pdf/2510.00770", "abs": "https://arxiv.org/abs/2510.00770", "authors": ["Tianle Ni", "Xiao Chen", "Hamid Sadeghian", "Sami Haddadin"], "title": "Tele-rehabilitation with online skill transfer and adaptation in $\\mathbb{R}^3 \\times \\mathit{S}^3$", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes a tele-teaching framework for the domain of\nrobot-assisted tele-rehabilitation. The system connects two robotic\nmanipulators on therapist and patient side via bilateral teleoperation,\nenabling a therapist to remotely demonstrate rehabilitation exercises that are\nexecuted by the patient-side robot. A 6-DoF Dynamical Movement Primitives\nformulation is employed to jointly encode translational and rotational motions\nin $\\mathbb{R}^3 \\times \\mathit{S}^3$ space, ensuring accurate trajectory\nreproduction. The framework supports smooth transitions between therapist-led\nguidance and patient passive training, while allowing adaptive adjustment of\nmotion. Experiments with 7-DoF manipulators demonstrate the feasibility of the\napproach, highlighting its potential for personalized and remotely supervised\nrehabilitation."}
{"id": "2510.00783", "pdf": "https://arxiv.org/pdf/2510.00783", "abs": "https://arxiv.org/abs/2510.00783", "authors": ["Thanh Nguyen Canh", "Haolan Zhang", "Xiem HoangVan", "Nak Young Chong"], "title": "Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions", "categories": ["cs.RO"], "comment": null, "summary": "Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of\nresearch within robotics and computer vision, focusing on the simultaneous\nlocalization of robotic systems and associating semantic information to\nconstruct the most accurate and complete comprehensive model of the surrounding\nenvironment. Since the first foundational work in Semantic SLAM appeared more\nthan two decades ago, this field has received increasing attention across\nvarious scientific communities. Despite its significance, the field lacks\ncomprehensive surveys encompassing recent advances and persistent challenges.\nIn response, this study provides a thorough examination of the state-of-the-art\nof Semantic SLAM techniques, with the aim of illuminating current trends and\nkey obstacles. Beginning with an in-depth exploration of the evolution of\nvisual SLAM, this study outlines its strengths and unique characteristics,\nwhile also critically assessing previous survey literature. Subsequently, a\nunified problem formulation and evaluation of the modular solution framework is\nproposed, which divides the problem into discrete stages, including visual\nlocalization, semantic feature extraction, mapping, data association, and loop\nclosure optimization. Moreover, this study investigates alternative\nmethodologies such as deep learning and the utilization of large language\nmodels, alongside a review of relevant research about contemporary SLAM\ndatasets. Concluding with a discussion on potential future research directions,\nthis study serves as a comprehensive resource for researchers seeking to\nnavigate the complex landscape of Semantic SLAM."}
{"id": "2510.00814", "pdf": "https://arxiv.org/pdf/2510.00814", "abs": "https://arxiv.org/abs/2510.00814", "authors": ["Kai Tang", "Dipankar Bhattacharya", "Hang Xu", "Fuyuki Tokuda", "Norman C. Tien", "Kazuhiro Kosuge"], "title": "RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator", "categories": ["cs.RO"], "comment": "9 pages, 6 figures, conference", "summary": "Robotic fabric manipulation in garment production for sewing, cutting, and\nironing requires reliable flattening and alignment, yet remains challenging due\nto fabric deformability, effectively infinite degrees of freedom, and frequent\nocclusions from wrinkles, folds, and the manipulator's End-Effector (EE) and\narm. To address these issues, this paper proposes the first Random-to-Target\nFabric Flattening (RTFF) policy, which aligns a random wrinkled fabric state to\nan arbitrary wrinkle-free target state. The proposed policy adopts a hybrid\nImitation Learning-Visual Servoing (IL-VS) framework, where IL learns with\nexplicit fabric models for coarse alignment of the wrinkled fabric toward a\nwrinkle-free state near the target, and VS ensures fine alignment to the\ntarget. Central to this framework is a template-based mesh that offers precise\ntarget state representation, wrinkle-aware geometry prediction, and consistent\nvertex correspondence across RTFF manipulation steps, enabling robust\nmanipulation and seamless IL-VS switching. Leveraging the power of mesh, a\nnovel IL solution for RTFF-Mesh Action Chunking Transformer (MACT)-is then\nproposed by conditioning the mesh information into a Transformer-based policy.\nThe RTFF policy is validated on a real dual-arm tele-operation system, showing\nzero-shot alignment to different targets, high accuracy, and strong\ngeneralization across fabrics and scales. Project website:\nhttps://kaitang98.github.io/RTFF_Policy/"}
{"id": "2510.00933", "pdf": "https://arxiv.org/pdf/2510.00933", "abs": "https://arxiv.org/abs/2510.00933", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication. 8\n  pages, 6 figures", "summary": "Current products, especially in the automotive sector, pose complex technical\nsystems having a multi-disciplinary mechatronic nature. Industrial standards\nsupporting system engineering and production typically (i) address the\nproduction phase only, but do not cover the complete product life cycle, and\n(ii) focus on production processes and resources rather than the products\nthemselves. The presented approach is motivated by incorporating impacts of\nend-of-life phase of the product life cycle into the engineering phase. This\npaper proposes a modelling approach coming up from the Product-Process-Resource\n(PPR) modeling paradigm. It combines requirements on (i) respecting the product\nstructure as a basis for the model, and (ii) it incorporates repairing,\nremanufacturing, or upcycling within cyber-physical production systems. The\nproposed model called PoPAN should accompany the product during the entire life\ncycle as a digital shadow encapsulated within the Asset Administration Shell of\na product. To facilitate the adoption of the proposed paradigm, the paper also\nproposes serialization of the model in the AutomationML data format. The model\nis demonstrated on a use-case for disassembling electric vehicle batteries to\nsupport their remanufacturing for stationary battery applications."}
{"id": "2510.00942", "pdf": "https://arxiv.org/pdf/2510.00942", "abs": "https://arxiv.org/abs/2510.00942", "authors": ["Reza Vafaee", "Kian Behzad", "Milad Siami", "Luca Carlone", "Ali Jadbabaie"], "title": "Non-submodular Visual Attention for Robot Navigation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "22 pages; Accepted to appear in IEEE Transactions on Robotics (T-RO)", "summary": "This paper presents a task-oriented computational framework to enhance\nVisual-Inertial Navigation (VIN) in robots, addressing challenges such as\nlimited time and energy resources. The framework strategically selects visual\nfeatures using a Mean Squared Error (MSE)-based, non-submodular objective\nfunction and a simplified dynamic anticipation model. To address the\nNP-hardness of this problem, we introduce four polynomial-time approximation\nalgorithms: a classic greedy method with constant-factor guarantees; a low-rank\ngreedy variant that significantly reduces computational complexity; a\nrandomized greedy sampler that balances efficiency and solution quality; and a\nlinearization-based selector based on a first-order Taylor expansion for\nnear-constant-time execution. We establish rigorous performance bounds by\nleveraging submodularity ratios, curvature, and element-wise curvature\nanalyses. Extensive experiments on both standardized benchmarks and a custom\ncontrol-aware platform validate our theoretical results, demonstrating that\nthese methods achieve strong approximation guarantees while enabling real-time\ndeployment."}
{"id": "2510.00995", "pdf": "https://arxiv.org/pdf/2510.00995", "abs": "https://arxiv.org/abs/2510.00995", "authors": ["Jacob Moore", "Phil Tokumaru", "Ian Reid", "Brandon Sutherland", "Joseph Ritchie", "Gabe Snow", "Tim McLain"], "title": "ROSflight 2.0: Lean ROS 2-Based Autopilot for Unmanned Aerial Vehicles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "To be submitted to the 2026 IEEE International Conference on Robotics\n  and Automation in Vienna, Austria", "summary": "ROSflight is a lean, open-source autopilot ecosystem for unmanned aerial\nvehicles (UAVs). Designed by researchers for researchers, it is built to lower\nthe barrier to entry to UAV research and accelerate the transition from\nsimulation to hardware experiments by maintaining a lean (not full-featured),\nwell-documented, and modular codebase. This publication builds on previous\ntreatments and describes significant additions to the architecture that improve\nthe modularity and usability of ROSflight, including the transition from ROS 1\nto ROS 2, supported hardware, low-level actuator mixing, and the simulation\nenvironment. We believe that these changes improve the usability of ROSflight\nand enable ROSflight to accelerate research in areas like advanced-air\nmobility. Hardware results are provided, showing that ROSflight is able to\ncontrol a multirotor over a serial connection at 400 Hz while closing all\ncontrol loops on the companion computer."}
{"id": "2510.01023", "pdf": "https://arxiv.org/pdf/2510.01023", "abs": "https://arxiv.org/abs/2510.01023", "authors": ["S. Satsevich", "A. Bazhenov", "S. Egorov", "A. Erkhov", "M. Gromakov", "A. Fedoseev", "D. Tsetserukou"], "title": "Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel teleoperation system with force feedback,\nutilizing consumer-grade HTC Vive Trackers 2.0. The system integrates a\ncustom-built controller, a UR3 robotic arm, and a Robotiq gripper equipped with\ncustom-designed fingers to ensure uniform pressure distribution on an embedded\nforce sensor. Real-time compression force data is transmitted to the\ncontroller, enabling operators to perceive the gripping force applied to\nobjects. Experimental results demonstrate that the system enhances task success\nrates and provides a low-cost solution for large-scale imitation learning data\ncollection without compromising affordability."}
{"id": "2510.01041", "pdf": "https://arxiv.org/pdf/2510.01041", "abs": "https://arxiv.org/abs/2510.01041", "authors": ["Ian Reid", "Joseph Ritchie", "Jacob Moore", "Brandon Sutherland", "Gabe Snow", "Phillip Tokumaru", "Tim McLain"], "title": "ROSplane 2.0: A Fixed-Wing Autopilot for Research", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) research requires the integration of\ncutting-edge technology into existing autopilot frameworks. This process can be\narduous, requiring extensive resources, time, and detailed knowledge of the\nexisting system. ROSplane is a lean, open-source fixed-wing autonomy stack\nbuilt by researchers for researchers. It is designed to accelerate research by\nproviding clearly defined interfaces with an easily modifiable framework.\nPowered by ROS 2, ROSplane allows for rapid integration of low or high-level\ncontrol, path planning, or estimation algorithms. A focus on lean, easily\nunderstood code and extensive documentation lowers the barrier to entry for\nresearchers. Recent developments to ROSplane improve its capacity to accelerate\nUAV research, including the transition from ROS 1 to ROS 2, enhanced estimation\nand control algorithms, increased modularity, and an improved aerodynamic\nmodeling pipeline. This aerodynamic modeling pipeline significantly reduces the\neffort of transitioning from simulation to real-world testing without requiring\nexpensive system identification or computational fluid dynamics tools.\nROSplane's architecture reduces the effort required to integrate new research\ntools and methods, expediting hardware experimentation."}
{"id": "2510.01068", "pdf": "https://arxiv.org/pdf/2510.01068", "abs": "https://arxiv.org/abs/2510.01068", "authors": ["Jiahang Cao", "Yize Huang", "Hanzhong Guo", "Rui Zhang", "Mu Nan", "Weijian Mai", "Jiaxu Wang", "Hao Cheng", "Jingkai Sun", "Gang Han", "Wen Zhao", "Qiang Zhang", "Yijie Guo", "Qihao Zheng", "Chunfeng Song", "Xiao Li", "Ping Luo", "Andrew F. Luo"], "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition", "categories": ["cs.RO", "cs.LG"], "comment": "Project Page: https://sagecao1125.github.io/GPC-Site/", "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies."}
{"id": "2510.01138", "pdf": "https://arxiv.org/pdf/2510.01138", "abs": "https://arxiv.org/abs/2510.01138", "authors": ["Matthew Woodward"], "title": "Real-Time Trajectory Generation and Hybrid Lyapunov-Based Control for Hopping Robots", "categories": ["cs.RO"], "comment": "7 pages, 4 figures, 4 tables", "summary": "The advent of rotor-based hopping robots has created very capable hopping\nplatforms with high agility and efficiency, and similar controllability, as\ncompared to their purely flying quadrotor counterparts. Advances in robot\nperformance have increased the hopping height to greater than 4 meters and\nopened up the possibility for more complex aerial trajectories (i.e.,\nbehaviors). However, currently hopping robots do not directly control their\naerial trajectory or transition to flight, eliminating the efficiency benefits\nof a hopping system. Here we show a real-time, computationally efficiency,\nnon-linear drag compensated, trajectory generation methodology and accompanying\nLyapunov-based controller. The combined system can create and follow complex\naerial trajectories from liftoff to touchdown on horizontal and vertical\nsurfaces, while maintaining strick control over the orientation at touchdown.\nThe computational efficiency provides broad applicability across all size\nscales of hopping robots while maintaining applicability to quadrotors in\ngeneral."}
{"id": "2510.00060", "pdf": "https://arxiv.org/pdf/2510.00060", "abs": "https://arxiv.org/abs/2510.00060", "authors": ["Sheng Yang", "Tong Zhan", "Guancheng Chen", "Yanfeng Lu", "Jian Wang"], "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In this work, we reconceptualize autonomous driving as a generalized language\nand formulate the trajectory planning task as next waypoint prediction. We\nintroduce Max-V1, a novel framework for one-stage end-to-end autonomous\ndriving. Our framework presents a single-pass generation paradigm that aligns\nwith the inherent sequentiality of driving. This approach leverages the\ngenerative capacity of the VLM (Vision-Language Model) to enable end-to-end\ntrajectory prediction directly from front-view camera input. The efficacy of\nthis method is underpinned by a principled supervision strategy derived from\nstatistical modeling. This provides a well-defined learning objective, which\nmakes the framework highly amenable to master complex driving policies through\nimitation learning from large-scale expert demonstrations. Empirically, our\nmethod achieves the state-of-the-art performance on the nuScenes dataset,\ndelivers an overall improvement of over 30% compared to prior baselines.\nFurthermore, it exhibits superior generalization performance on cross-domain\ndatasets acquired from diverse vehicles, demonstrating notable potential for\ncross-vehicle robustness and adaptability. Due to these empirical strengths,\nthis work introduces a model enabling fundamental driving behaviors, laying the\nfoundation for the development of more capable self-driving agents. Code will\nbe available upon publication."}
{"id": "2510.00120", "pdf": "https://arxiv.org/pdf/2510.00120", "abs": "https://arxiv.org/abs/2510.00120", "authors": ["Xiang Chang", "Zhijie Yi", "Yichang Liu", "Hongling Sheng", "Dengbo He"], "title": "The Formation of Trust in Autonomous Vehicles after Interacting with Robotaxis on Public Roads", "categories": ["cs.HC", "cs.RO"], "comment": "Proceedings of the 69th HFES International Annual Meeting", "summary": "This study investigates how pedestrian trust, receptivity, and behavior\nevolve during interactions with Level-4 autonomous vehicles (AVs) at\nuncontrolled urban intersections in a naturalistic setting. While public\nacceptance is critical for AV adoption, most prior studies relied on simplified\nsimulations or field tests. We conducted a real-world experiment in a\ncommercial Robotaxi operation zone, where 33 participants repeatedly crossed an\nuncontrolled intersection with frequent Level-4 Robotaxi traffic. Participants\ncompleted the Pedestrian Behavior Questionnaire (PBQ), Pedestrian Receptivity\nQuestionnaire for Fully AVs (PRQF), pre- and post-experiment Trust in AVs\nScale, and Personal Innovativeness Scale (PIS). Results showed that trust in\nAVs significantly increased post-experiment, with the increase positively\nassociated with the Interaction component of PRQF. Additionally, both the\nPositive and Error subscales of the PBQ significantly influenced trust change.\nThis study reveals how trust forms in real-world pedestrian-AV encounters,\noffering insights beyond lab-based research by accounting for population\nheterogeneity."}
{"id": "2510.00167", "pdf": "https://arxiv.org/pdf/2510.00167", "abs": "https://arxiv.org/abs/2510.00167", "authors": ["Diego Ortiz Barbosa", "Mohit Agrawal", "Yash Malegaonkar", "Luis Burbano", "Axel Andersson", "György Dán", "Henrik Sandberg", "Alvaro A. Cardenas"], "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI", "categories": ["cs.AI", "cs.CR", "cs.RO"], "comment": null, "summary": "Autonomous drones must often respond to sudden events, such as alarms,\nfaults, or unexpected changes in their environment, that require immediate and\nadaptive decision-making. Traditional approaches rely on safety engineers\nhand-coding large sets of recovery rules, but this strategy cannot anticipate\nthe vast range of real-world contingencies and quickly becomes incomplete.\nRecent advances in embodied AI, powered by large visual language models,\nprovide commonsense reasoning to assess context and generate appropriate\nactions in real time. We demonstrate this capability in a simulated urban\nbenchmark in the Unreal Engine, where drones dynamically interpret their\nsurroundings and decide on sudden maneuvers for safe landings. Our results show\nthat embodied AI makes possible a new class of adaptive recovery and\ndecision-making pipelines that were previously infeasible to design by hand,\nadvancing resilience and safety in autonomous aerial systems."}
{"id": "2510.00208", "pdf": "https://arxiv.org/pdf/2510.00208", "abs": "https://arxiv.org/abs/2510.00208", "authors": ["Tanay Kumar", "Raktim Bhattacharya"], "title": "Robust Attitude Control of Nonlinear Multi-Rotor Dynamics with LFT Models and $\\mathcal{H}_\\infty$ Performance", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": "6 pages, 6 figures, 3 tables, submitted to ACC 2026", "summary": "Attitude stabilization of unmanned aerial vehicles in uncertain environments\npresents significant challenges due to nonlinear dynamics, parameter\nvariations, and sensor limitations. This paper presents a comparative study of\n$\\mathcal{H}_\\infty$ and classical PID controllers for multi-rotor attitude\nregulation in the presence of wind disturbances and gyroscope noise. The flight\ndynamics are modeled using a linear parameter-varying (LPV) framework, where\nnonlinearities and parameter variations are systematically represented as\nstructured uncertainties within a linear fractional transformation formulation.\nA robust controller based on $\\mathcal{H}_\\infty$ formulation is designed using\nonly gyroscope measurements to ensure guaranteed performance bounds. Nonlinear\nsimulation results demonstrate the effectiveness of the robust controllers\ncompared to classical PID control, showing significant improvement in attitude\nregulation under severe wind disturbances."}
{"id": "2510.00259", "pdf": "https://arxiv.org/pdf/2510.00259", "abs": "https://arxiv.org/abs/2510.00259", "authors": ["Ethan Herron", "Xian Yeow Lee", "Gregory Sin", "Teresa Gonzalez Diaz", "Ahmed Farahat", "Chetan Gupta"], "title": "A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection", "categories": ["cs.MA", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Autonomous inspection systems are essential for ensuring the performance and\nlongevity of industrial assets. Recently, agentic frameworks have demonstrated\nsignificant potential for automating inspection workflows but have been limited\nto digital tasks. Their application to physical assets in real-world\nenvironments, however, remains underexplored. In this work, our contributions\nare two-fold: first, we propose a hierarchical agentic framework for autonomous\ndrone control, and second, a reasoning methodology for individual function\nexecutions which we refer to as ReActEval. Our framework focuses on visual\ninspection tasks in indoor industrial settings, such as interpreting industrial\nreadouts or inspecting equipment. It employs a multi-agent system comprising a\nhead agent and multiple worker agents, each controlling a single drone. The\nhead agent performs high-level planning and evaluates outcomes, while worker\nagents implement ReActEval to reason over and execute low-level actions.\nOperating entirely in natural language, ReActEval follows a plan, reason, act,\nevaluate cycle, enabling drones to handle tasks ranging from simple navigation\n(e.g., flying forward 10 meters and land) to complex high-level tasks (e.g.,\nlocating and reading a pressure gauge). The evaluation phase serves as a\nfeedback and/or replanning stage, ensuring actions align with user objectives\nwhile preventing undesirable outcomes. We evaluate the framework in a simulated\nenvironment with two worker agents, assessing performance qualitatively and\nquantitatively based on task completion across varying complexity levels and\nworkflow efficiency. By leveraging natural language processing for agent\ncommunication, our approach offers a novel, flexible, and user-accessible\nalternative to traditional drone-based solutions, enabling autonomous\nproblem-solving for industrial inspection without extensive user intervention."}
{"id": "2510.00405", "pdf": "https://arxiv.org/pdf/2510.00405", "abs": "https://arxiv.org/abs/2510.00405", "authors": ["Jiayi Liu", "Jiaming Zhou", "Ke Ye", "Kun-Yu Lin", "Allan Wang", "Junwei Liang"], "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception."}
{"id": "2510.00425", "pdf": "https://arxiv.org/pdf/2510.00425", "abs": "https://arxiv.org/abs/2510.00425", "authors": ["Rishi Veerapaneni", "Alvin Tang", "Haodong He", "Sophia Zhao", "Viraj Shah", "Yidai Cen", "Ziteng Ji", "Gabriel Olin", "Jon Arrizabalaga", "Yorai Shaoul", "Jiaoyang Li", "Maxim Likhachev"], "title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks", "categories": ["cs.MA", "cs.RO"], "comment": "Project webpage: https://rishi-v.github.io/CBS-Protocol/", "summary": "Imagine the future construction site, hospital, office, or even sophisticated\nhousehold with dozens of robots bought from different manufacturers. How can we\nenable these different systems to effectively move in a shared environment,\ngiven that each robot may have its own independent motion planning system? This\nwork shows how we can get efficient collision-free movements between\nalgorithmically heterogeneous agents by using Conflict-Based Search (Sharon et\nal. 2015) as a protocol. At its core, the CBS Protocol requires one specific\nsingle-agent motion planning API; finding a collision-free path that satisfies\ncertain space-time constraints. Given such an API, CBS uses a central planner\nto find collision-free paths - independent of how the API is implemented. We\nshow how this protocol enables multi-agent motion planning for a heterogeneous\nteam of agents completing independent tasks with a variety of single-agent\nplanners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g.,\nRRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement\nLearning."}
{"id": "2510.01049", "pdf": "https://arxiv.org/pdf/2510.01049", "abs": "https://arxiv.org/abs/2510.01049", "authors": ["Abdelrhman Werby", "Dennis Rotondi", "Fabio Scaparro", "Kai O. Arras"], "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In recent years, 3D scene graphs have emerged as a powerful world\nrepresentation, offering both geometric accuracy and semantic richness.\nCombining 3D scene graphs with large language models enables robots to reason,\nplan, and navigate in complex human-centered environments. However, current\napproaches for constructing 3D scene graphs are semantically limited to a\npredefined set of relationships, and their serialization in large environments\ncan easily exceed an LLM's context window. We introduce KeySG, a framework that\nrepresents 3D scenes as a hierarchical graph consisting of floors, rooms,\nobjects, and functional elements, where nodes are augmented with multi-modal\ninformation extracted from keyframes selected to optimize geometric and visual\ncoverage. The keyframes allow us to efficiently leverage VLM to extract scene\ninformation, alleviating the need to explicitly model relationship edges\nbetween objects, enabling more general, task-agnostic reasoning and planning.\nOur approach can process complex and ambiguous queries while mitigating the\nscalability issues associated with large scene graphs by utilizing a\nhierarchical retrieval-augmented generation (RAG) pipeline to extract relevant\ncontext from the graph. Evaluated across four distinct benchmarks -- including\n3D object segmentation and complex query retrieval -- KeySG outperforms prior\napproaches on most metrics, demonstrating its superior semantic richness and\nefficiency."}
{"id": "2510.01059", "pdf": "https://arxiv.org/pdf/2510.01059", "abs": "https://arxiv.org/abs/2510.01059", "authors": ["Juan Augusto Paredes Salazar", "James Usevitch", "Ankit Goel"], "title": "Predictive Control Barrier Functions for Discrete-Time Linear Systems with Unmodeled Delays", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": "8 pages, 7 figures, submitted to ACC 2026", "summary": "This paper introduces a predictive control barrier function (PCBF) framework\nfor enforcing state constraints in discrete-time systems with unknown relative\ndegree, which can be caused by input delays or unmodeled input dynamics.\nExisting discrete-time CBF formulations typically require the construction of\nauxiliary barrier functions when the relative degree is greater than one, which\ncomplicates implementation and may yield conservative safe sets. The proposed\nPCBF framework addresses this challenge by extending the prediction horizon to\nconstruct a CBF for an associated system with relative degree one. As a result,\nthe superlevel set of the PCBF coincides with the safe set, simplifying\nconstraint enforcement and eliminating the need for auxiliary functions. The\neffectiveness of the proposed method is demonstrated on a discrete-time double\nintegrator with input delay and a bicopter system with position constraints."}
{"id": "2510.01126", "pdf": "https://arxiv.org/pdf/2510.01126", "abs": "https://arxiv.org/abs/2510.01126", "authors": ["Yuxiang Feng", "Keyang Zhang", "Hassane Ouchouid", "Ashwil Kaniamparambil", "Ioannis Souflas", "Panagiotis Angeloudis"], "title": "Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages", "summary": "Large vision-language models (VLMs) are increasingly used in\nautonomous-vehicle (AV) stacks, but hallucination limits their reliability in\nsafety-critical pipelines. We present Shapley-credited Context-Aware\nDawid-Skene with Agreement, a game-theoretic fusion method for multi-label\nunderstanding of ego-view dashcam video. It learns per-model, per-label,\ncontext-conditioned reliabilities from labelled history and, at inference,\nconverts each model's report into an agreement-guardrailed log-likelihood ratio\nthat is combined with a contextual prior and a public reputation state updated\nvia Shapley-based team credit. The result is calibrated, thresholdable\nposteriors that (i) amplify agreement among reliable models, (ii) preserve\nuniquely correct single-model signals, and (iii) adapt to drift. To specialise\ngeneral VLMs, we curate 1,000 real-world dashcam clips with structured\nannotations (scene description, manoeuvre recommendation, rationale) via an\nautomatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11\n+ BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three\nheterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming\ndistance, Micro-Macro-F1, and average per-video latency. Empirically, the\nproposed method achieves a 23% reduction in Hamming distance, 55% improvement\nin Macro-F1, and 47% improvement in Micro-F1 when comparing with the best\nsingle model, supporting VLM fusion as a calibrated, interpretable, and robust\ndecision-support component for AV pipelines."}
