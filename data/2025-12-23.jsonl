{"id": "2512.17919", "pdf": "https://arxiv.org/pdf/2512.17919", "abs": "https://arxiv.org/abs/2512.17919", "authors": ["Marie-Dominique van Damme", "Yann Méneroux", "Ana-Maria Olteanu-Raimond"], "title": "An extensive analysis and calibration of the Modular Aggregation Algorithm across three categories of for GNSS trajectories data sources", "categories": ["eess.SP"], "comment": null, "summary": "This technical report aims to complement the conference paper (https://doi.org/10.1145/3678717.3691325) by providing additional experiments or further details that could not be included in the paper."}
{"id": "2512.17928", "pdf": "https://arxiv.org/pdf/2512.17928", "abs": "https://arxiv.org/abs/2512.17928", "authors": ["Dongdong Yang", "Bin Li", "Jiguang He", "Yicheng Yan", "Xiaoyu Zhang", "Chongwen Huang"], "title": "Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG"], "comment": null, "summary": "Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications."}
{"id": "2512.18071", "pdf": "https://arxiv.org/pdf/2512.18071", "abs": "https://arxiv.org/abs/2512.18071", "authors": ["Meysam Ghanbari", "Mohammad Taghi Dabiri", "Mazen Hasna", "Tanvir Alam", "Khalid Qaraqe"], "title": "Deep Learning Surrogate for Fast CIR Prediction in Reactive Molecular Diffusion Advection Channels", "categories": ["eess.SP"], "comment": "Conference paper, proposes a deep-learning surrogate for fast prediction of channel impulse responses in reactive molecular diffusion advection channels", "summary": "Accurate channel impulse response (CIR) modeling in molecular communication (MC) often requires solving coupled reactive diffusion-advection equations, which is computationally expensive for large parameter sweeps or design loops. We develop a deep-learning surrogate for a three-dimensional duct MC channel with reactive diffusion-advection transport and reversible ligand-receptor binding on a finite ring receiver. Using a physics-based partial differential equation (PDE)-ordinary differential equation (ODE) model, we generate a large CIR dataset across broad transport, reaction, and geometric ranges and train a neural network that maps these parameters directly to the CIR. On an independent test set, the surrogate closely matches reference CIRs both qualitatively and quantitatively: the empirical cumulative distribution function (CDF) of the normalized root mean square error (NRMSE) shows that 90% of test channels are predicted with error below 0.15, with only weak dependence on individual parameters. The surrogate therefore offers an accurate and computationally efficient replacement for repeated PDE-based CIR evaluations in MC system analysis and design."}
{"id": "2512.18075", "pdf": "https://arxiv.org/pdf/2512.18075", "abs": "https://arxiv.org/abs/2512.18075", "authors": ["Mingjun Sun", "Chongjun Ouyang", "Shaochuan Wu", "Yuanwei Liu"], "title": "Robust Beamforming for Pinching-Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "Pinching-antenna system (PASS) mitigates large-scale path loss by enabling flexible placement of pinching antennas (PAs) along the dielectric waveguide. However, most existing studies assume perfect channel state information (CSI), overlooking the impact of channel uncertainty. This paper addresses this gap by proposing a robust beamforming framework for both lossy and lossless waveguides. For baseband beamforming, the lossy case yields an second-order cone programming-based solution, while the lossless case admits a closed-form solution via maximum ratio transmission. The PAs' positions in both cases are optimized through the Gauss-Seidel-based method. Numerical results validate the effectiveness of the proposed algorithm and demonstrate that PASS exhibits superior robustness against channel uncertainty compared with conventional fixed-antenna systems. Notably, its worst-case achievable rate can even exceed the fixed-antenna baseline under perfect CSI."}
{"id": "2512.17940", "pdf": "https://arxiv.org/pdf/2512.17940", "abs": "https://arxiv.org/abs/2512.17940", "authors": ["Xi Wang", "Jing Liu", "Siqian Li", "Hengtai Dai", "Jung-Che Chang", "Adam Rushworth", "Xin Dong"], "title": "Untethered thin dielectric elastomer actuated soft robot", "categories": ["cs.RO"], "comment": null, "summary": "Thin dielectric elastomer actuator (DEA) features a unique in-plane configuration, enabling low-profile designs capable of accessing millimetre-scale narrow spaces. However, most existing DEA-powered soft robots require high voltages and wired power connections, limiting their ability to operate in confined environments. This study presents an untethered thin soft robot (UTS-Robot) powered by thin dielectric elastomer actuators (TS-DEA). The robot measures 38 mm in length, 6 mm in height, and weighs just 2.34 grams, integrating flexible onboard electronics to achieve fully untethered actuation. The TS-DEA, operating at resonant frequencies of 86 Hz under a low driving voltage of 220 V, adopts a dual-actuation sandwiched structure, comprising four dielectric elastomer layers bonded to a compressible tensioning mechanism at its core. This design enables high power density actuation and locomotion via three directional friction pads. The low-voltage actuation is achieved by fabricating each elastomer layer via spin coating to an initial thickness of 50 um, followed by biaxial stretching to 8 um. A comprehensive design and modelling framework has been developed to optimise TS-DEA performance. Experimental evaluations demonstrate that the bare TS-DEA achieves a locomotion speed of 12.36 mm/s at resonance, the untethered configuration achieves a locomotion speed of 0.5 mm/s, making it highly suitable for navigating confined and complex environments."}
{"id": "2512.18087", "pdf": "https://arxiv.org/pdf/2512.18087", "abs": "https://arxiv.org/abs/2512.18087", "authors": ["Meysam Ghanbari", "Mohammad Taghi Dabiri", "Rula Ammuri", "Mazen Hasna", "Khalid Qaraqe"], "title": "AI Assisted Next Gen Outdoor Optical Networks: Camera Sensing for Monitoring and User Localization", "categories": ["eess.SP"], "comment": null, "summary": "We consider outdoor optical access points (OAPs), which, enabled by recent advances in metasurface technology, have attracted growing interest. While OAPs promise high data rates and strong physical-layer security, practical deployments still expose vulnerabilities and misuse patterns that necessitate a dedicated monitoring layer - the focus of this work. We therefore propose a user positioning and monitoring system that infers locations from spatial intensity measurements on a photodetector (PD) array. Specifically, our hybrid approach couples an optics-informed forward model and sparse, model-based inversion with a lightweight data-driven calibration stage, yielding high accuracy at low computational cost. This design preserves the interpretability and stability of model-based reconstruction while leveraging learning to absorb residual nonidealities and device-specific distortions. Under identical hardware and training conditions (both with 5 x 10^5 samples), the hybrid method attains consistently lower mean-squared error than a generic deep-learning baseline while using substantially less training time and compute. Accuracy improves with array resolution and saturates around 60 x 60-80 x 80, indicating a favorable accuracy-complexity trade-off for real-time deployment. The resulting position estimates can be cross-checked with real-time network logs to enable continuous monitoring, anomaly detection (e.g., potential eavesdropping), and access control in outdoor optical access networks."}
{"id": "2512.17958", "pdf": "https://arxiv.org/pdf/2512.17958", "abs": "https://arxiv.org/abs/2512.17958", "authors": ["Farida Mohsen", "Ali Safa"], "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots."}
{"id": "2512.18097", "pdf": "https://arxiv.org/pdf/2512.18097", "abs": "https://arxiv.org/abs/2512.18097", "authors": ["Mohammad Taghi Dabiri", "Meysam Ghanbari", "Rula Ammuri", "Saif Al-Kuwari", "Mazen Hasna", "Khalid Qaraqe"], "title": "CV Quantum Communications with Angular Rejection Filtering: Modeling and Security Analysis", "categories": ["eess.SP"], "comment": null, "summary": "Continuous-variable quantum key distribution (CVQKD) over free-space optical links is a promising approach for secure communication, but its performance is limited by turbulence, pointing errors, and angular leakage that can be exploited by an eavesdropper. To mitigate this, we consider an angular rejection filter that defines a safe-zone at the receiver and blocks signals from outside the desired cone. A system and channel model is developed including turbulence, misalignment, and safe-zone effects, and information theoretic metrics are derived to evaluate security. Simulation results show that the safe zone significantly reduces information leakage and that careful tuning of beam waist, angular threshold, and aperture size is essential for maximizing the secret key rate. Larger apertures improve performance but increase receiver size, while longer links require sub 100 urad alignment accuracy. These results highlight safe-zone enforcement and parameter optimization as effective strategies for practical and secure CV-QKD."}
{"id": "2512.17992", "pdf": "https://arxiv.org/pdf/2512.17992", "abs": "https://arxiv.org/abs/2512.17992", "authors": ["Qianwei Wang", "Bowen Li", "Zhanpeng Luo", "Yifan Xu", "Alexander Gray", "Tom Silver", "Sebastian Scherer", "Katia Sycara", "Yaqi Xie"], "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models", "categories": ["cs.RO"], "comment": "18 pages, 11 figures", "summary": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics."}
{"id": "2512.18326", "pdf": "https://arxiv.org/pdf/2512.18326", "abs": "https://arxiv.org/abs/2512.18326", "authors": ["Meidong Xia", "Min Fan", "Wei Xu", "Haiming Wang", "Xiaohu You"], "title": "Two-Stage Signal Reconstruction for Amplitude-Phase-Time Block Modulation-based Communications", "categories": ["eess.SP"], "comment": null, "summary": "Operating power amplifiers (PAs) at lower input back-off (IBO) levels is an effective way to improve PA efficiency, but often introduces severe nonlinear distortion that degrades transmission performance. Amplitude-phase-time block modulation (APTBM) has recently emerged as an effective solution to this problem. By leveraging the intrinsic amplitude and phase constraints of each APTBM block, PA-induced nonlinear distortion can be mitigated through constraint-guided signal reconstruction. However, existing reconstruction methods apply these constraints only heuristically and statistically, limiting the achievable IBO reduction and PA efficiency improvement. This paper addresses this limitation by decomposing the nonlinear distortion into dominant and residual components, and accordingly develops a novel two-stage signal reconstruction algorithm consisting of coarse and fine reconstruction stages. The coarse reconstruction stage eliminates the dominant distortion by jointly exploiting the APTBM block structure and PA nonlinear characteristics. The fine reconstruction stage minimizes the residual distortion by formulating a nonconvex optimization problem that explicitly enforces the APTBM constraints. To handle this problem efficiently, a low-complexity iterative variable substitution method is introduced, which relaxes the problem into a sequence of trust-region subproblems, each solvable in closed form. The proposed algorithm is validated through comprehensive numerical simulations and testbed experiments. Results show that it achieves up to 4 dB IBO reduction in simulations and up to 2 dB IBO reduction in experiments while maintaining transmission performance, corresponding to PA efficiency improvements of 59.1\\% and 33.9\\%, respectively, over existing methods."}
{"id": "2512.18007", "pdf": "https://arxiv.org/pdf/2512.18007", "abs": "https://arxiv.org/abs/2512.18007", "authors": ["Yu Fang", "Kanchana Ranasinghe", "Le Xue", "Honglu Zhou", "Juntao Tan", "Ran Xu", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Daniel Szafir", "Mingyu Ding", "Michael S. Ryoo", "Juan Carlos Niebles"], "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion", "categories": ["cs.RO", "cs.CV"], "comment": "Website: https://vla-motion.github.io/", "summary": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs."}
{"id": "2512.18346", "pdf": "https://arxiv.org/pdf/2512.18346", "abs": "https://arxiv.org/abs/2512.18346", "authors": ["Vishesh Bhardwaj", "Aman Yadav", "Srikireddy Dhanunjay Reddy", "Tharun Kumar Reddy Bollu"], "title": "Cognitive Inference based Feature Pyramid Network for Sentimental Analysis using EEG Signals", "categories": ["eess.SP"], "comment": null, "summary": "Sentiment analysis using Electroencephalography (EEG) sensor signals provides a deeper behavioral understanding of a person's emotional state, offering insights into real-time mood fluctuations. This approach takes advantage of brain electrical activity, making it a promising tool for various applications, including mental health monitoring, affective computing, and personalised user experiences. An encoder-based model for EEG-to-sentiment analysis, utilizing the ZUCO 2.0 dataset and incorporating a Feature Pyramid Network (FPN), is proposed to enhance this process. FPNs are adapted here for EEG sensor data, enabling multiscale feature extraction to capture local and global sentiment-related patterns. The raw EEG sensor data from the ZUCO 2.0 dataset is pre-processed and passed through the FPN, which extracts hierarchical features. In addition, extracted features are passed to a Gated Recurrent Unit (GRU) to model temporal dependencies, thereby enhancing the accuracy of sentiment classification. The ZUCO 2.0 dataset is utilized for its clear and detailed representation in 128 channels, offering rich spatial and temporal resolution. The experimental metric results show that the proposed architecture achieves a 6.88\\% performance gain compared to the existing methods. Furthermore, the proposed framework demonstrated its efficacy on the validation datasets DEAP and SEED."}
{"id": "2512.18028", "pdf": "https://arxiv.org/pdf/2512.18028", "abs": "https://arxiv.org/abs/2512.18028", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence."}
{"id": "2512.18426", "pdf": "https://arxiv.org/pdf/2512.18426", "abs": "https://arxiv.org/abs/2512.18426", "authors": ["Xinrui Li", "R. Michael Buehrer", "Steven W. Ellingson"], "title": "RIS-Aided Spatial Nulling: Algorithms, Analysis, and Nulling Limits", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have recently gained attention as a means to dynamically shape the wireless propagation environment through programmable reflection control. Among the numerous applications, an important emerging use case is employing RIS as an auxiliary mechanism for spatial interference nulling, particularly in large ground-based reflector antennas where sidelobe interference can significantly degrade the system performance. With the growing density of satellites and terrestrial emitters, algorithms with faster convergence speed and better performance are needed. This work investigates RIS-equipped reflector antennas as a representative example of RIS-assisted spatial nulling and develop algorithms for sidelobe cancellation at specific directions and frequencies under various constraints. For the continuous-phase case, we adapt the gradient projection (GP) and alternating projection (AP) algorithms for scalability and propose a closed-form near-optimal solution that achieves satisfactory nulling performance with significantly reduced complexity. For the discrete-phase case, we reformulate the problem using a penalty method and solve it via majorization-minimization, outperforming the heuristic methods from our earlier work. Further, we analyze the electric field characteristics across multiple interference directions and frequencies to quantify the nulling capability of the RIS-aided reflectors, and identify a simple criterion for the existence of unimodular weights enabling perfect nulls. Simulation results demonstrate the effectiveness of the proposed methods and confirm the theoretical nulling limits."}
{"id": "2512.18032", "pdf": "https://arxiv.org/pdf/2512.18032", "abs": "https://arxiv.org/abs/2512.18032", "authors": ["Jacqueline Borgstedt", "Jake Bhattacharyya", "Matteo Iovino", "Frank E. Pollick", "Stephen Brewster"], "title": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Zoomorphic Socially Assistive Robots (SARs) offer an alternative source of social touch for individuals who cannot access animal companionship. However, current SARs provide only limited, passive touch-based interactions and lack the rich haptic cues, such as warmth, heartbeat or purring, that are characteristic of human-animal touch. This limits their ability to evoke emotionally engaging, life-like physical interactions.\n  We present a multimodal tactile prototype, which was used to augment the established PARO robot, integrating thermal and vibrotactile feedback to simulate feeling biophysiological signals. A flexible heating interface delivers body-like warmth, while embedded actuators generate heartbeat-like rhythms and continuous purring sensations. These cues were iteratively designed and calibrated with input from users and haptics experts. We outline the design process and offer reproducible guidelines to support the development of emotionally resonant and biologically plausible touch interactions with SARs."}
{"id": "2512.18427", "pdf": "https://arxiv.org/pdf/2512.18427", "abs": "https://arxiv.org/abs/2512.18427", "authors": ["Xinrui Li", "R. Michael Buehrer"], "title": "On the Limits of Coherent Time-Domain Cancellation of Radio Frequency Interference", "categories": ["eess.SP"], "comment": null, "summary": "In many sensing (viz., radio astronomy) and radar applications, the received signal of interest (SOI) exhibits a significantly wider bandwidth or weaker power than the interference signal, rendering it indistinguishable from the background noise. Such scenarios arise frequently in applications such as passive radar, cognitive radio, low-probability-of-intercept (LPI) radar, and planetary radar for radio astronomy, where canceling the radio frequency interference (RFI) is critical for uncovering the SOI. In this work, we examine the Demodulation-Remodulation (Demod-Remod) based interference cancellation framework for the RFI. This approach demodulates the unknown interference, creates a noise-free interference replica, and coherently subtracts it from the received signal. To evaluate the performance limits, we employ the performance metric termed \\textit{interference rejection ratio} (IRR), which quantifies the interference canceled. We derive the analytical expressions of IRR as a function of the optimal estimation variances of the signal parameters. Simulation results confirm the accuracy of the analytical expression for both single-carrier and multi-carrier interference signals and demonstrate that the method can substantially suppress the interference at a sufficient interference-to-noise ratio (INR), enabling enhanced detection and extraction of the SOI. We further extend the analysis to the scenario where the SOI is above the noise floor, and confirm the validity of the theoretical IRR expression in this scenario. Lastly, we compare the Demod-Remod technique to other time-domain cancellation methods. The result of the comparison identifies the conditions under which each method is preferred, offering practical guidelines for interference mitigation under different scenarios."}
{"id": "2512.18048", "pdf": "https://arxiv.org/pdf/2512.18048", "abs": "https://arxiv.org/abs/2512.18048", "authors": ["Nidhi Malhotra", "Amber K. Rothe", "Revanth Konda", "Jaydev P. Desai"], "title": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications", "categories": ["cs.RO"], "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "Robotically steerable compliant surgical tools offer several advantages over rigid tools, including enhanced dexterity, reduced tissue damage, and the ability to generate non-linear trajectories in minimally invasive neurosurgical procedures. Many existing robotic neurosurgical tools are designed using stainless steel or nitinol materials. Using polymer-based materials instead can offer advantages such as reduced interference in magnetic resonance imaging, enhanced safety for guiding electrically powered instruments, and reduced tissue damage due to inherent compliance. Several polymer materials have been used in robotic surgical applications, such as polyimide, polycarbonate, and elastic resin. Various fabrication strategies have also been proposed, including standard microfabrication techniques, thermal drawing, and 3-D printing. In our previous work, a tendon-driven, notched-tube was designed for several neurosurgical robotic tools, utilizing laser micromachining to reduce the stiffness of the tube in certain directions. This fabrication method is desirable because it has a single-step process, has high precision, and does not require a cleanroom or harsh chemicals. Past studies have explored laser-micromachining of polymer material for surgical applications such as stent fabrication. In this work, we explore extending the use of the laser micromachining approach to the fabrication of polyimide (PI) robotically steerable cannulas for neurosurgical applications. Utilizing the method presented in this work, we fabricated joints as small as 1.5 mm outer diameter (OD). Multiple joints were fabricated using PI tubes of different ODs, and the loading behavior of the fabricated joints was experimentally characterized."}
{"id": "2512.18641", "pdf": "https://arxiv.org/pdf/2512.18641", "abs": "https://arxiv.org/abs/2512.18641", "authors": ["Ziad Hatab", "Michael Gadringer", "Wolfgang Bösch"], "title": "The Choice of Line Lengths in Multiline Thru-Reflect-Line Calibration", "categories": ["eess.SP", "physics.ins-det"], "comment": "https://github.com/ZiadHatab/line-length-multiline-trl-calibration", "summary": "This paper presents an analysis and rigorous procedure for determining the optimal lengths of line standards in multiline thru-reflect-line (TRL) calibration of vector network analyzers (VNAs). The solution is obtained through nonlinear constrained optimization of the eigenvalue problem in multiline TRL calibration. Additionally, we propose a simplified approach for near-optimal length selection based on predefined sparse rulers. Alongside the length calculation, we discuss the required number of lines to meet bandwidth requirements. The sensitivity of the proposed procedure is evaluated numerically via Monte Carlo simulations, demonstrating that the derived lengths have lower uncertainty than those from existing industry standards. Practical examples are provided for various applications, including lossy and dispersive lines, as well as banded solutions for waveguides."}
{"id": "2512.18068", "pdf": "https://arxiv.org/pdf/2512.18068", "abs": "https://arxiv.org/abs/2512.18068", "authors": ["Juo-Tung Chen", "XinHao Chen", "Ji Woong Kim", "Paul Maria Scheikl", "Richard Jaepyeong Cha", "Axel Krieger"], "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning", "categories": ["cs.RO"], "comment": "8 pages, 6 figures, 2 tables", "summary": "Imitation learning (IL) has shown immense promise in enabling autonomous dexterous manipulation, including learning surgical tasks. To fully unlock the potential of IL for surgery, access to clinical datasets is needed, which unfortunately lack the kinematic data required for current IL approaches. A promising source of large-scale surgical demonstrations is monocular surgical videos available online, making monocular pose estimation a crucial step toward enabling large-scale robot learning. Toward this end, we propose SurgiPose, a differentiable rendering based approach to estimate kinematic information from monocular surgical videos, eliminating the need for direct access to ground truth kinematics. Our method infers tool trajectories and joint angles by optimizing tool pose parameters to minimize the discrepancy between rendered and real images. To evaluate the effectiveness of our approach, we conduct experiments on two robotic surgical tasks: tissue lifting and needle pickup, using the da Vinci Research Kit Si (dVRK Si). We train imitation learning policies with both ground truth measured kinematics and estimated kinematics from video and compare their performance. Our results show that policies trained on estimated kinematics achieve comparable success rates to those trained on ground truth data, demonstrating the feasibility of using monocular video based kinematic estimation for surgical robot learning. By enabling kinematic estimation from monocular surgical videos, our work lays the foundation for large scale learning of autonomous surgical policies from online surgical data."}
{"id": "2512.18711", "pdf": "https://arxiv.org/pdf/2512.18711", "abs": "https://arxiv.org/abs/2512.18711", "authors": ["Yue Zhang", "Yaru Fu", "Pei Liu", "Yalin Liu", "Kevin Hung"], "title": "Multi-Waveguide Pinching Antenna Placement Optimization for Rate Maximization", "categories": ["eess.SP"], "comment": null, "summary": "Pinching antenna systems (PASS) have emerged as a technology that enables the large-scale movement of antenna elements, offering significant potential for performance gains in next-generation wireless networks. This paper investigates the problem of maximizing the average per-user data rate by optimizing the antenna placement of a multi-waveguide PASS, subject to a stringent physical minimum spacing constraint. To address this complex challenge, which involves a coupled fractional objective and a non-convex constraint, we employ the fractional programming (FP) framework to transform the non-convex rate maximization problem into a more tractable one, and devise a projected gradient ascent (PGA)-based algorithm to iteratively solve the transformed problem. Simulation results demonstrate that our proposed scheme significantly outperforms various geometric placement baselines, achieving superior per-user data rates by actively mitigating multi-user interference."}
{"id": "2512.18081", "pdf": "https://arxiv.org/pdf/2512.18081", "abs": "https://arxiv.org/abs/2512.18081", "authors": ["Tudor Jianu", "Anh Nguyen", "Sebastiano Fichera", "Pierre Berthet-Rayne"], "title": "Towards Autonomous Navigation in Endovascular Interventions", "categories": ["cs.RO"], "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality, with minimally invasive treatment options offered through endovascular interventions. However, the precision and adaptability of current robotic systems for endovascular navigation are limited by heuristic control, low autonomy, and the absence of haptic feedback. This thesis presents an integrated AI-driven framework for autonomous guidewire navigation in complex vascular environments, addressing key challenges in data availability, simulation fidelity, and navigational accuracy.\n  A high-fidelity, real-time simulation platform, CathSim, is introduced for reinforcement learning based catheter navigation, featuring anatomically accurate vascular models and contact dynamics. Building on CathSim, the Expert Navigation Network is developed, a policy that fuses visual, kinematic, and force feedback for autonomous tool control. To mitigate data scarcity, the open-source, bi-planar fluoroscopic dataset Guide3D is proposed, comprising more than 8,700 annotated images for 3D guidewire reconstruction. Finally, SplineFormer, a transformer-based model, is introduced to directly predict guidewire geometry as continuous B-spline parameters, enabling interpretable, real-time navigation.\n  The findings show that combining high-fidelity simulation, multimodal sensory fusion, and geometric modelling substantially improves autonomous endovascular navigation and supports safer, more precise minimally invasive procedures."}
{"id": "2512.18715", "pdf": "https://arxiv.org/pdf/2512.18715", "abs": "https://arxiv.org/abs/2512.18715", "authors": ["Kaiyi Chi", "Yinghui He", "Qianqian Yang", "Yuanchao Shu", "Zhiqin Wang", "Jun Luo", "Jiming Chen"], "title": "DeepGuard: Defending Deep Joint Source-Channel Coding Against Eavesdropping at Physical-Layer", "categories": ["eess.SP"], "comment": "16 pages, 34 figures", "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a promising paradigm for efficient and robust information transmission. However, its intrinsic characteristics also pose new security challenges, notably an increased vulnerability to eavesdropping attacks. Existing studies on defending against eavesdropping attacks in DeepJSCC, while demonstrating certain effectiveness, often incur considerable computational overhead or introduce performance trade-offs that may adversely affect legitimate users. In this paper, we present DeepGuard, to the best of our knowledge, the first physical-layer defense framework for DeepJSCC against eavesdropping attacks, validated through over-the-air experiments using software-defined radios (SDRs). Considering that existing eavesdropping attacks against DeepJSCC are limited to simulation under ideal channels, we take a step further by identifying and implementing four representative types of attacks under various configurations in orthogonal frequency-division multiplexing systems. These attacks are evaluated over-the-air under diverse scenarios, allowing us to comprehensively characterize the real-world threat landscape. To mitigate these threats, DeepGuard introduces a novel preamble perturbation mechanism that modifies the preamble shared only between legitimate transceivers. To realize it, we first conduct a theoretical analysis of the perturbation's impact on the signals intercepted by the eavesdropper. Building upon this, we develop an end-to-end perturbation optimization algorithm that significantly degrades eavesdropping performance while preserving reliable communication for legitimate users. We prototype DeepGuard using SDRs and conduct extensive over-the-air experiments in practical scenarios. Extensive experiments demonstrate that DeepGuard effectively mitigates eavesdropping threats."}
{"id": "2512.18146", "pdf": "https://arxiv.org/pdf/2512.18146", "abs": "https://arxiv.org/abs/2512.18146", "authors": ["Stergios E. Bachoumas", "Panagiotis Artemiadis"], "title": "On Swarm Leader Identification using Probing Policies", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "13 pages, journal", "summary": "Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections."}
{"id": "2512.18773", "pdf": "https://arxiv.org/pdf/2512.18773", "abs": "https://arxiv.org/abs/2512.18773", "authors": ["Xue Xian Zheng", "Xing Liu", "Tareq Y. Al-Naffouri"], "title": "Decentralized GNSS at Global Scale via Graph-Aware Diffusion Adaptation", "categories": ["eess.SP"], "comment": null, "summary": "Network-based Global Navigation Satellite Systems (GNSS) underpin critical infrastructure and autonomous systems, yet typically rely on centralized processing hubs that limit scalability, resilience, and latency. Here we report a global-scale, decentralized GNSS architecture spanning hundreds of ground stations. By modeling the receiver network as a time-varying graph, we employ a deep linear neural network approach to learn topology-aware mixing schedules that optimize information exchange. This enables a gradient tracking diffusion strategy wherein stations execute local inference and exchange succinct messages to achieve two concurrent objectives: centimeter-level self-localization and network-wide consensus on satellite correction products. The consensus products are broadcast to user receivers as corrections, supporting precise point positioning (PPP) and precise point positioning-real-time kinematic (PPP-RTK). Numerical results demonstrate that our method matches the accuracy of centralized baselines while significantly outperforming existing decentralized methods in convergence speed and communication overhead. By reframing decentralized GNSS as a networked signal processing problem, our results pave the way for integrating decentralized optimization, consensus-based inference, and graph-aware learning as effective tools in operational satellite navigation."}
{"id": "2512.18206", "pdf": "https://arxiv.org/pdf/2512.18206", "abs": "https://arxiv.org/abs/2512.18206", "authors": ["Trevor Stepp", "Parthan Olikkal", "Ramana Vinjamuri", "Rajasekhar Anguluri"], "title": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination", "categories": ["cs.RO", "math.OC"], "comment": "7 pages, 5 figures", "summary": "Identifying motor synergies -- coordinated hand joint patterns activated at task-dependent time shifts -- from kinematic data is central to motor control and robotics. Existing two-stage methods first extract candidate waveforms (via SVD) and then select shifted templates using sparse optimization, requiring at least two datasets and complicating data collection. We introduce an optimization-based framework that jointly learns a small set of synergies and their sparse activation coefficients. The formulation enforces group sparsity for synergy selection and element-wise sparsity for activation timing. We develop an alternating minimization method in which coefficient updates decouple across tasks and synergy updates reduce to regularized least-squares problems. Our approach requires only a single data set, and simulations show accurate velocity reconstruction with compact, interpretable synergies."}
{"id": "2512.18780", "pdf": "https://arxiv.org/pdf/2512.18780", "abs": "https://arxiv.org/abs/2512.18780", "authors": ["Yifeng Zhang", "Xiao Liang"], "title": "Domain Adaptation in Structural Health Monitoring of Civil Infrastructure: A Systematic Review", "categories": ["eess.SP"], "comment": null, "summary": "This study provides a comprehensive review of domain adaptation (DA) techniques in vibration-based structural health monitoring (SHM). As data-driven models increasingly support the assessment of civil structures, the persistent challenge of transferring knowledge across varying geometries, materials, and environmental conditions remains a major obstacle. DA offers a systematic approach to mitigate these discrepancies by aligning feature distributions between simulated, laboratory, and field domains while preserving the sensitivity of damage-related information. Drawing on more than sixty representative studies, this paper analyzes the evolution of DA methods for SHM, including statistical alignment, adversarial and subdomain learning, physics-informed adaptation, and generative modeling for simulation-to-real transfer. The review summarizes their contributions and limitations across bridge and building applications, revealing that while DA has improved generalization significantly, key challenges persist: managing domain discrepancy, addressing data scarcity, enhancing model interpretability, and enabling adaptability to multiple sources and time-varying conditions. Future research directions emphasize integrating physical constraints into learning objectives, developing physics-consistent generative frameworks to enhance data realism, establishing interpretable and certifiable DA systems for engineering practice, and advancing multi-source and lifelong adaptation for scalable monitoring. Overall, this review consolidates the methodological foundation of DA for SHM, identifies existing barriers to generalization and trust, and outlines the technological trajectory toward transparent, physics-aware, and adaptive monitoring systems that support the long-term resilience of civil infrastructure."}
{"id": "2512.18211", "pdf": "https://arxiv.org/pdf/2512.18211", "abs": "https://arxiv.org/abs/2512.18211", "authors": ["Yudong Liu", "Spencer Hallyburton", "Jiwoo Kim", "Yueqian Lin", "Yiming Li", "Qinsi Wang", "Hui Ye", "Jingwei Sun", "Miroslav Pajic", "Yiran Chen", "Hai Li"], "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful \"VLM Trajectory Planner for Autonomous Driving.\" On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub."}
{"id": "2512.18788", "pdf": "https://arxiv.org/pdf/2512.18788", "abs": "https://arxiv.org/abs/2512.18788", "authors": ["George C. Alexandropoulos", "Kostantinos D. Katsanos", "George Stamatelis", "Ioannis Gavras"], "title": "RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization", "categories": ["eess.SP", "cs.IT", "cs.LG"], "comment": "48 pages; 12 figures; book chapter", "summary": "This chapter overviews the concept of Smart Wireless Environments (SWEs) motivated by the emerging technology of Reconfigurable Intelligent Surfaces (RISs). The operating principles and state-of-the-art hardware architectures of programmable metasurfaces are first introduced. Subsequently, key performance objectives and use cases of RIS-enabled SWEs, including spectral and energy efficiency, physical-layer security, integrated sensing and communications, as well as the emerging paradigm of over-the-air computing, are discussed. Focusing on the recent trend of Beyond-Diagonal (BD) RISs, two distributed designs of respective SWEs are presented. The first deals with a multi-user Multiple-Input Single-Output (MISO) system operating within the area of influence of a SWE comprising multiple BD-RISs. A hybrid distributed and fusion machine learning framework based on multi-branch attention-based convolutional Neural Networks (NNs), NN parameter sharing, and neuroevolutionary training is presented, which enables online mapping of channel realizations to the BD-RIS configurations as well as the multi-user transmit precoder. Performance evaluation results showcase that the distributedly optimized RIS-enabled SWE achieves near-optimal sum-rate performance with low online computational complexity. The second design focuses on the wideband interference MISO broadcast channel, where each base station exclusively controls one BD-RIS to serve its assigned group of users. A cooperative optimization framework that jointly designs the base station transmit precoders as well as the tunable capacitances and switch matrices of all metasurfaces is presented. Numerical results demonstrating the superior sum-rate performance of the designed RIS-enabled SWE for multi-cell MISO networks over benchmark schemes, considering non-cooperative configuration and conventional diagonal metasurfaces, are presented."}
{"id": "2512.18213", "pdf": "https://arxiv.org/pdf/2512.18213", "abs": "https://arxiv.org/abs/2512.18213", "authors": ["Wu-Te Yang", "Masayoshi Tomizuka"], "title": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization", "categories": ["cs.RO"], "comment": "6 pages, 4 figures", "summary": "Modeling soft pneumatic actuators with high precision remains a fundamental challenge due to their highly nonlinear and compliant characteristics. This paper proposes an innovative modeling framework based on fractional-order differential equations (FODEs) to accurately capture the dynamic behavior of soft materials. The unknown parameters within the fractional-order model are identified using particle swarm optimization (PSO), enabling parameter estimation directly from experimental data without reliance on pre-established material databases or empirical constitutive laws. The proposed approach effectively represents the complex deformation phenomena inherent in soft actuators. Experimental results validate the accuracy and robustness of the developed model, demonstrating improvement in predictive performance compared to conventional modeling techniques. The presented framework provides a data-efficient and database-independent solution for soft actuator modeling, advancing the precision and adaptability of soft robotic system design."}
{"id": "2512.18854", "pdf": "https://arxiv.org/pdf/2512.18854", "abs": "https://arxiv.org/abs/2512.18854", "authors": ["Xiarui Su", "Xihui Teng", "Yiyang Yu", "Yiming Yang", "Atif Shamim"], "title": "A 100-GHz CMOS-Compatible RIS-on-Chip Based on Phase-Delay Lines for 6G Applications", "categories": ["eess.SP"], "comment": null, "summary": "On-chip reconfigurable intelligent surfaces (RIS) are expected to play a vital role in future 6G communication systems. This work proposed a CMOS-compatible on-chip RIS capable of achieving beam steering for the first time. The proposed unit cell design is a combination of a slot, a phase-delay line with VO2, and a ground. Under the two states of the VO2, the unit cell has a 180 deg phase difference at the center frequency, while maintaining reflection magnitudes better than -1.2 dB. Moreover, a 60by60 RIS array based on the present novel unit is designed, demonstrating the beam-steering capability. Finally, to validate the design concept, a prototype is fabricated, and the detailed fabrication process is presented. The measurement result demonstrates a 27.1 dB enhancement between ON and OFF states. The proposed RIS has the advantages of low loss, CMOS-compatibility, providing a foundation for future 6G applications."}
{"id": "2512.18268", "pdf": "https://arxiv.org/pdf/2512.18268", "abs": "https://arxiv.org/abs/2512.18268", "authors": ["Si Wei Feng"], "title": "On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region", "categories": ["cs.RO", "cs.CG"], "comment": null, "summary": "With the popularity of drone technologies, aerial photography have become prevalent in many daily scenarios such as environment monitoring, structure inspection, law enforcement etc. A central challenge in this domain is the efficient coverage of a target area with photographs that can entirely capture the region, while respecting constraints such as the image resolution, and limited number of pictures that can be taken. This work investigates the computational complexity of several fundamental problems arised from this challenge. By abstracting the aerial photography problem into the coverage problems in computational geometry, we demonstrate that most of these problems are in fact computationally intractable, with the implication that traditional algorithms cannot solve them efficiently. The intuitions of this work can extend beyond aerial photography to broader applications such as pesticide spraying, and strategic sensor placement."}
{"id": "2512.18890", "pdf": "https://arxiv.org/pdf/2512.18890", "abs": "https://arxiv.org/abs/2512.18890", "authors": ["Yuchen Zhang", "Eva Lagunas", "Xue Xian Zheng", "Symeon Chatzinotas", "Tareq Y. Al-Naffouri"], "title": "Decentralized Cooperative Beamforming for Networked LEO Satellites with Statistical CSI", "categories": ["eess.SP"], "comment": "This paper has been submitted to IEEE for peer review", "summary": "Inter-satellite-link-enabled low-Earth-orbit (LEO) satellite constellations are evolving toward networked architectures that support constellation-level cooperation, enabling multiple satellites to jointly serve user terminals through cooperative beamforming. While such cooperation can substantially enhance link budgets and achievable rates, its practical realization is challenged by the scalability limitations of centralized beamforming designs and the stringent computational and signaling constraints of large LEO constellations. This paper develops a fully decentralized cooperative beamforming framework for networked LEO satellite downlinks. Using an ergodic-rate-based formulation, we first derive a centralized weighted minimum mean squared error (WMMSE) solution as a performance benchmark. Building on this formulation, we propose a topology-agnostic decentralized beamforming algorithm by localizing the benchmark and exchanging a set of globally coupled variables whose dimensions are independent of the antenna number and enforcing consensus over arbitrary connected inter-satellite networks. The resulting algorithm admits fully parallel execution across satellites. To further enhance scalability, we eliminate the consensus-related auxiliary variables in closed form and derive a low-complexity per-satellite update rule that is optimal to local iteration and admits a quasi-closed-form solution via scalar line search. Simulation results show that the proposed decentralized schemes closely approach centralized performance under practical inter-satellite topologies, while significantly reducing computational complexity and signaling overhead, enabling scalable cooperative beamforming for large LEO constellations."}
{"id": "2512.18333", "pdf": "https://arxiv.org/pdf/2512.18333", "abs": "https://arxiv.org/abs/2512.18333", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($φ$) and Pitch ($θ$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($ψ$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller."}
{"id": "2512.18970", "pdf": "https://arxiv.org/pdf/2512.18970", "abs": "https://arxiv.org/abs/2512.18970", "authors": ["Tuo Wu", "Xiazhi Lai", "Maged Elkashlan", "Naofal Al-Dhahir", "Matthew C. Valenti", "Fumiyuki Adachi"], "title": "FAS-RIS for V2X: Unlocking Realistic Performance Analysis with Finite Elements", "categories": ["eess.SP"], "comment": "Accepted by IEEE TVT", "summary": "The synergy of fluid antenna systems (FAS) and reconfigurable intelligent surfaces (RIS) is poised to unlock robust Vehicle-to-Everything (V2X) communications. However, a critical gap persists between theoretical predictions and real-world performance. Existing analyses predominantly rely on the Central Limit Theorem (CLT), an assumption valid only for a large number of RIS elements, which fails to represent practical, finite-sized deployments constrained by cost and urban infrastructure. This paper bridges this gap by presenting a novel framework that unlocks a realistic performance analysis for FAS-RIS systems with finite elements. Leveraging a Gamma distribution approximation, we derive a new, tractable closed-form expression for the outage probability. Numerical results validate our approach, demonstrating that it offers a significantly more accurate performance characterization than conventional CLT-based methods, particularly in the practical regime of small-scale RIS. This work provides a crucial foundation for the design and deployment of reliable FAS-RIS-aided vehicular networks."}
{"id": "2512.18336", "pdf": "https://arxiv.org/pdf/2512.18336", "abs": "https://arxiv.org/abs/2512.18336", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency."}
{"id": "2512.18981", "pdf": "https://arxiv.org/pdf/2512.18981", "abs": "https://arxiv.org/abs/2512.18981", "authors": ["Jiaji Ren", "Ye Tian", "Baiyang Liu", "Tuo Wu", "Wei Liu", "Kai-Kit Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "An Fluid Antenna Array-Enabled DOA Estimation Method: End-Fire Effect Suppression", "categories": ["eess.SP"], "comment": null, "summary": "Direction of Arrival (DOA) estimation serves as a critical sensing technology poised to play a vital role in future intelligent and ubiquitous communication systems. Despite the development of numerous mature super-resolution algorithms, the inherent end-fire effect problem in fixed antenna arrays remains inadequately addressed. This work proposed a novel array architecture composed of fluid antennas. By exploiting the spatial reconfigurability of their positions to equivalently modulate the array steering vector and integrating it with the classical MUSIC algorithm, this approach achieved high-precision DOA estimation. Simulation results demonstrated that the proposed method delivers outstanding estimation performance even in highly challenging end-fire regions."}
{"id": "2512.18368", "pdf": "https://arxiv.org/pdf/2512.18368", "abs": "https://arxiv.org/abs/2512.18368", "authors": ["Yihang Zhu", "Weiqing Wang", "Shijie Wu", "Ye Shi", "Jingya Wang"], "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "While imitation learning has shown impressive results in single-task robot manipulation, scaling it to multi-task settings remains a fundamental challenge due to issues such as suboptimal demonstrations, trajectory noise, and behavioral multi-modality. Existing skill-based methods attempt to address this by decomposing actions into reusable abstractions, but they often rely on fixed-length segmentation or environmental priors that limit semantic consistency and cross-task generalization. In this work, we propose AtomSkill, a novel multi-task imitation learning framework that learns and leverages a structured Atomic Skill Space for composable robot manipulation. Our approach is built on two key technical contributions. First, we construct a Semantically Grounded Atomic Skill Library by partitioning demonstrations into variable-length skills using gripper-state keyframe detection and vision-language model annotation. A contrastive learning objective ensures the resulting skill embeddings are both semantically consistent and temporally coherent. Second, we propose an Action Generation module with Keypose Imagination, which jointly predicts a skill's long-horizon terminal keypose and its immediate action sequence. This enables the policy to reason about overarching motion goals and fine-grained control simultaneously, facilitating robust skill chaining. Extensive experiments in simulated and real-world environments show that AtomSkill consistently outperforms state-of-the-art methods across diverse manipulation tasks."}
{"id": "2512.18982", "pdf": "https://arxiv.org/pdf/2512.18982", "abs": "https://arxiv.org/abs/2512.18982", "authors": ["Tuo Wu", "Kai-Kit Wong", "Jie Tang", "Junteng Yao", "Baiyang Liu", "Kin-Fai Tong", "Chan-Byoung Chae", "Matthew C. Valenti", "Kwai-Man Luk"], "title": "Reimagining Wireless Connectivity: The FAS-RIS Synergy for 6G Smart Cities", "categories": ["eess.SP"], "comment": null, "summary": "Fluid antenna system (FAS) represents the concept of treating antenna as a reconfigurable physical-layer resource to broaden system design and network optimization and inspire next-generation reconfigurable antennas. FAS can unleash new degree of freedom (DoF) via antenna reconfigurations for novel spatial diversity. Reconfigurable intelligent surfaces (RISs) on the other hand can reshape wireless propagation environments but often face limitations from double path-loss and minimal signal processing capability when operating independently. This article envisions a transformative FAS-RIS integrated architecture for future smart city networks, uniting the adaptability of FAS with the environmental control of RIS. The proposed framework has five key applications: FAS-enabled base stations (BSs) for large-scale beamforming, FAS-equipped user devices with finest spatial diversity, and three novel RIS paradigms -- fluid RIS (FRIS) with reconfigurable elements, FAS-embedded RIS as active relays, and enormous FAS (E-FAS) exploiting surface waves on facades to re-establish line-of-sight (LoS) communication. A two-timescale control mechanism coordinates network-level beamforming with rapid, device-level adaptation. Applications spanning from simultaneous wireless information and power transfer (SWIPT) to integrated sensing and communications (ISAC), with challenges in co-design, channel modeling, and optimization, are discussed. This article concludes with simulation results demonstrating the robustness and effectiveness of the FAS-RIS system."}
{"id": "2512.18396", "pdf": "https://arxiv.org/pdf/2512.18396", "abs": "https://arxiv.org/abs/2512.18396", "authors": ["Yulu Wu", "Jiujun Cheng", "Haowen Wang", "Dengyang Suo", "Pei Ren", "Qichao Mao", "Shangce Gao", "Yakun Huang"], "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts."}
{"id": "2512.19010", "pdf": "https://arxiv.org/pdf/2512.19010", "abs": "https://arxiv.org/abs/2512.19010", "authors": ["Devi Yuliarti", "Ravi Prakash", "Hiu Ching Cheung", "Amy Strong", "Patrick J. Codd", "Shan Lin"], "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation", "categories": ["eess.SP", "cs.RO"], "comment": null, "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making."}
{"id": "2512.18474", "pdf": "https://arxiv.org/pdf/2512.18474", "abs": "https://arxiv.org/abs/2512.18474", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI 2026). This is a preprint of the author-accepted manuscript", "summary": "Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted."}
{"id": "2512.19013", "pdf": "https://arxiv.org/pdf/2512.19013", "abs": "https://arxiv.org/abs/2512.19013", "authors": ["Seongkyu Jung", "Namyoon Lee", "Jeonghun Park"], "title": "The MIMO-ME-MS Channel: Analysis and Algorithm for Secure MIMO Integrated Sensing and Communications", "categories": ["eess.SP"], "comment": "16 pages, 5 figures. Submitted to an IEEE journal", "summary": "This paper studies precoder design for secure MIMO integrated sensing and communications (ISAC) by introducing the MIMO-ME-MS channel, where a multi-antenna transmitter serves a legitimate multi-antenna receiver in the presence of a multi-antenna eavesdropper while simultaneously enabling sensing via a multi-antenna sensing receiver. Using sensing mutual information as the sensing metric, we formulate a nonconvex weighted objective that jointly captures secure communication (via secrecy rate) and sensing performance. A high-SNR analysis based on subspace decomposition characterizes the maximum achievable weighted degrees of freedom and reveals that a quasi-optimal precoder must span a \"useful subspace,\" highlighting why straightforward extensions of classical wiretap/ISAC precoders can be suboptimal in this tripartite setting. Motivated by these insights, we develop a practical two-stage iterative algorithm that alternates between sequential basis construction and power allocation via a difference-of-convex program. Numerical results show that the proposed approach captures the desirable precoding structure predicted by the analysis and yields substantial gains in the MIMO-ME-MS channel."}
{"id": "2512.18477", "pdf": "https://arxiv.org/pdf/2512.18477", "abs": "https://arxiv.org/abs/2512.18477", "authors": ["Wenjun Lin", "Jensen Zhang", "Kaitong Cai", "Keze Wang"], "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Under submission", "summary": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation."}
{"id": "2512.19054", "pdf": "https://arxiv.org/pdf/2512.19054", "abs": "https://arxiv.org/abs/2512.19054", "authors": ["Chengyong Jiang", "Jiajia Guo", "Yuqing Hua", "Chao-Kai Wen", "Shi Jin"], "title": "AI-Driven Subcarrier-Level CQI Feedback", "categories": ["eess.SP"], "comment": null, "summary": "The Channel Quality Indicator (CQI) is a fundamental component of channel state information (CSI) that enables adaptive modulation and coding by selecting the optimal modulation and coding scheme to meet a target block error rate. While AI-enabled CSI feedback has achieved significant advances, especially in precoding matrix index feedback, AI-based CQI feedback remains underexplored. Conventional subband-based CQI approaches, due to coarse granularity, often fail to capture fine frequency-selective variations and thus lead to suboptimal resource allocation. In this paper, we propose an AI-driven subcarrier-level CQI feedback framework tailored for 6G and NextG systems. First, we introduce CQInet, an autoencoder-based scheme that compresses per-subcarrier CQI at the user equipment and reconstructs it at the base station, significantly reducing feedback overhead without compromising CQI accuracy. Simulation results show that CQInet increases the effective data rate by 7.6% relative to traditional subband CQI under equivalent feedback overhead. Building on this, we develop SR-CQInet, which leverages super-resolution to infer fine-grained subcarrier CQI from sparsely reported CSI reference signals (CSI-RS). SR-CQInet reduces CSI-RS overhead to 3.5% of CQInet's requirements while maintaining comparable throughput. These results demonstrate that AI-driven subcarrier-level CQI feedback can substantially enhance spectral efficiency and reliability in future wireless networks."}
{"id": "2512.18537", "pdf": "https://arxiv.org/pdf/2512.18537", "abs": "https://arxiv.org/abs/2512.18537", "authors": ["Erdao Liang"], "title": "Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators", "categories": ["cs.RO"], "comment": "Source code is available at https://github.com/LuminousLamp/SUMO-Benchmark", "summary": "This paper presents a systematic benchmarking of the model-based microscopic traffic simulator SUMO against state-of-the-art data-driven traffic simulators using large-scale real-world datasets. Using the Waymo Open Motion Dataset (WOMD) and the Waymo Open Sim Agents Challenge (WOSAC), we evaluate SUMO under both short-horizon (8s) and long-horizon (60s) closed-loop simulation settings. To enable scalable evaluation, we develop Waymo2SUMO, an automated pipeline that converts WOMD scenarios into SUMO simulations. On the WOSAC benchmark, SUMO achieves a realism meta metric of 0.653 while requiring fewer than 100 tunable parameters. Extended rollouts show that SUMO maintains low collision and offroad rates and exhibits stronger long-horizon stability than representative data-driven simulators. These results highlight complementary strengths of model-based and data-driven approaches for autonomous driving simulation and benchmarking."}
{"id": "2512.19089", "pdf": "https://arxiv.org/pdf/2512.19089", "abs": "https://arxiv.org/abs/2512.19089", "authors": ["Marie Jose Perez Peralta", "Daniela Flores Casillas", "Benjamin Wilson", "Cristian Aviles Medina", "Yira Itzae Rendon Hernandez", "Vladimir Orrante Bracho"], "title": "Wireless sEMG-IMU Wearable for Real-Time Squat Kinematics and Muscle Activation", "categories": ["eess.SP"], "comment": "6 pages, 9 figures. Technical report / preprint (wearable sEMG + IMU system for squat analysis)", "summary": "This work presents the design and implementation of a wireless, wearable system that combines surface electromyography (sEMG) and inertial measurement units (IMUs) to analyze a single lower-limb functional task: the free bodyweight squat in a healthy adult. The system records bipolar EMG from one agonist and one antagonist muscle of the dominant leg (vastus lateralis and semitendinosus) while simultaneously estimating knee joint angle, angular velocity, and angular acceleration using two MPU6050 IMUs. A custom dual-channel EMG front end with differential instrumentation preamplification, analog filtering (5-500 Hz band-pass and 60 Hz notch), high final gain, and rectified-integrated output was implemented on a compact 10 cm x 12 cm PCB. Data are digitized by an ESP32 microcontroller and transmitted wirelessly via ESP-NOW to a second ESP32 connected to a PC. A Python-based graphical user interface (GUI) displays EMG and kinematic signals in real time, manages subject metadata, and exports a summary of each session to Excel. The complete system is battery-powered to reduce electrical risk during human use. The resulting prototype demonstrates the feasibility of low-cost, portable EMG-IMU instrumentation for integrated analysis of muscle activation and squat kinematics and provides a platform for future biomechanical applications in sports performance and rehabilitation."}
{"id": "2512.18662", "pdf": "https://arxiv.org/pdf/2512.18662", "abs": "https://arxiv.org/abs/2512.18662", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "Offline Reinforcement Learning for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages", "summary": "End-to-end (E2E) autonomous driving models that take only camera images as input and directly predict a future trajectory are appealing for their computational efficiency and potential for improved generalization via unified optimization; however, persistent failure modes remain due to reliance on imitation learning (IL). While online reinforcement learning (RL) could mitigate IL-induced issues, the computational burden of neural rendering-based simulation and large E2E networks renders iterative reward and hyperparameter tuning costly. We introduce a camera-only E2E offline RL framework that performs no additional exploration and trains solely on a fixed simulator dataset. Offline RL offers strong data efficiency and rapid experimental iteration, yet is susceptible to instability from overestimation on out-of-distribution (OOD) actions. To address this, we construct pseudo ground-truth trajectories from expert driving logs and use them as a behavior regularization signal, suppressing imitation of unsafe or suboptimal behavior while stabilizing value learning. Training and closed-loop evaluation are conducted in a neural rendering environment learned from the public nuScenes dataset. Empirically, the proposed method achieves substantial improvements in collision rate and route completion compared with IL baselines. Our code will be available at [URL]."}
{"id": "2512.19109", "pdf": "https://arxiv.org/pdf/2512.19109", "abs": "https://arxiv.org/abs/2512.19109", "authors": ["Sai Zhao", "Fanjin Kong", "Dong Tang", "Tuo Wu", "Shunxing Yang", "Kai-Kit Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "Intelligent Sky Mirrors: SAC-Driven MF-RIS Optimization for Secure NOMA in Low-Altitude Economy", "categories": ["eess.SP"], "comment": null, "summary": "Low-altitude economy (LAE) has become a key driving force for smart cities and economic growth. To address spectral efficiency and communication security challenges in LAE, this paper investigates secure energy efficiency (SEE) maximization using intelligent sky mirrors, UAV-mounted multifunctional reconfigurable intelligent surfaces (MF-RIS) assisting nonorthogonal multiple access (NOMA) systems. These aerial mirrors intelligently amplify legitimate signals while simultaneously generating jamming against eavesdroppers. We formulate a joint optimization problem encompassing UAV trajectory, base station power allocation, RIS phase shifts, amplification factors, and scheduling matrices. Given the fractional SEE objective and dynamic UAV scenarios, we propose a two-layer optimization scheme: SAC-driven first layer for trajectory and power management, and channel alignment-based second layer for phase optimization. Simulations demonstrate that our proposed scheme significantly outperforms benchmark approaches."}
{"id": "2512.18703", "pdf": "https://arxiv.org/pdf/2512.18703", "abs": "https://arxiv.org/abs/2512.18703", "authors": ["Cailin Lei", "Haiyang Wu", "Yuxiong Ji", "Xiaoyu Cai", "Yuchuan Du"], "title": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Enhancing the performance of trajectory planners for lane - changing vehicles is one of the key challenges in autonomous driving within human - machine mixed traffic. Most existing studies have not incorporated human drivers' prior knowledge when designing trajectory planning models. To address this issue, this study proposes a novel trajectory planning framework that integrates causal prior knowledge into the control process. Both longitudinal and lateral microscopic behaviors of vehicles are modeled to quantify interaction risk, and a staged causal graph is constructed to capture causal dependencies in lane-changing scenarios. Causal effects between the lane-changing vehicle and surrounding vehicles are then estimated using causal inference, including average causal effects (ATE) and conditional average treatment effects (CATE). These causal priors are embedded into a model predictive control (MPC) framework to enhance trajectory planning. The proposed approach is validated on naturalistic vehicle trajectory datasets. Experimental results show that: (1) causal inference provides interpretable and stable quantification of vehicle interactions; (2) individual causal effects reveal driver heterogeneity; and (3) compared with the baseline MPC, the proposed method achieves a closer alignment with human driving behaviors, reducing maximum trajectory deviation from 1.2 m to 0.2 m, lateral velocity fluctuation by 60%, and yaw angle variability by 50%. These findings provide methodological support for human-like trajectory planning and practical value for improving safety, stability, and realism in autonomous vehicle testing and traffic simulation platforms."}
{"id": "2512.19127", "pdf": "https://arxiv.org/pdf/2512.19127", "abs": "https://arxiv.org/abs/2512.19127", "authors": ["Yuhao Chen", "Boxiang He", "Junshan Luo", "Shilian Wang", "Lei Yao", "Jing Lei"], "title": "Specific Multi-emitter Identification: Theoretical Limits and Low-complexity Design", "categories": ["eess.SP"], "comment": null, "summary": "Specific emitter identification (SEI) distinguishes emitters by utilizing hardware-induced signal imperfections. However, conventional SEI techniques are primarily designed for single-emitter scenarios. This poses a fundamental limitation in distributed wireless networks, where simultaneous transmissions from multiple emitters result in overlapping signals that conventional single-emitter identification methods cannot effectively handle. To overcome this limitation, we present a specific multi-emitter identification (SMEI) framework via multi-label learning, treating identification as a problem of directly decoding emitter states from overlapping signals. Theoretically, we establish performance bounds using Fano's inequality. Methodologically, the multi-label formulation reduces output dimensionality from exponential to linear scale, thereby substantially decreasing computational complexity. Additionally, we propose an improved SMEI (I-SMEI), which incorporates multi-head attention to effectively capture features in correlated signal combinations. Experimental results demonstrate that SMEI achieves high identification accuracy with a linear computational complexity. Furthermore, the proposed I-SMEI scheme significantly improves identification accuracy across various overlapping scenarios compared to the proposed SMEI and other advanced methods."}
{"id": "2512.18712", "pdf": "https://arxiv.org/pdf/2512.18712", "abs": "https://arxiv.org/abs/2512.18712", "authors": ["Maozeng Zhang", "Ke Shi", "Huijun Li", "Tongshu Chen", "Jiejun Yan", "Aiguo Song"], "title": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Stroke-induced motor impairment often results in substantial loss of upper-limb function, creating a strong demand for rehabilitation robots that enable safe and transparent physical human-robot interaction (pHRI). Variable stiffness actuators are well suited for such applications. However, in most existing designs, stiffness is coupled with the deflection angle, complicating both modeling and control. To address this limitation, this paper presents a variable stiffness actuator featuring decoupled stiffness and output behavior for rehabilitation robotics. The system integrates a variable stiffness mechanism that combines a variable-length lever with a hypocycloidal straight-line mechanism to achieve a linear torque-deflection relationship and continuous stiffness modulation from near zero to theoretically infinite. It also incorporates a differential transmission mechanism based on a planetary gear system that enables dual-motor load sharing. A cascade PI controller is further developed on the basis of the differential configuration, in which the position-loop term jointly regulates stiffness and deflection angle, effectively suppressing stiffness fluctuations and output disturbances. The performance of prototype was experimentally validated through stiffness calibration, stiffness regulation, torque control, decoupled characteristics, and dual-motor load sharing, indicating the potential for rehabilitation exoskeletons and other pHRI systems."}
{"id": "2512.19166", "pdf": "https://arxiv.org/pdf/2512.19166", "abs": "https://arxiv.org/abs/2512.19166", "authors": ["Luca Reggiani", "Arnaldo Spalvieri"], "title": "Energy Optimization for Time-of-Arrival Based Tracking", "categories": ["eess.SP"], "comment": null, "summary": "The paper analyzes energy allocation in a scenario where the position of a moving target is tracked by exploiting the Time-of-Arrivals of bandwidth-constrained signals received by or transmitted from a fixed number of anchors located at known positions. The signal of each anchor is generated by transmitting a sequence of known symbols, allowing for amplitude and duration (number of symbols) to be different from anchor to anchor. The problem is the minimization of the sum of the energies of the transmitted signals imposing a constraint on the performance of the tracking procedure. Specifically, the constraint is the Posterior Cramer-Rao Bound, below the mean square error achieved by any unbiased estimator. The main improvement over the previous literature is the derivation of a formula that, at each step of the tracking, allows to calculate in closed form the first-order variation of the Posterior Cramer-Rao Bound as a function of the variation of the total energy. To concretely show the application of our approach, we present also two numerical algorithms that implement the constrained optimization in the case of signals of fixed amplitude and variable duration transmitted from the anchors in a time division multiplexing scheme."}
{"id": "2512.18836", "pdf": "https://arxiv.org/pdf/2512.18836", "abs": "https://arxiv.org/abs/2512.18836", "authors": ["Jingjia Teng", "Yang Li", "Jianqiang Wang", "Yingbai Hu", "Songyuan Tang", "Manjiang Hu"], "title": "Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes", "categories": ["cs.RO"], "comment": null, "summary": "Four-wheel Independent Steering (4WIS) vehicles have attracted increasing attention for their superior maneuverability. Human drivers typically choose to cross or drive over the low-profile obstacles (e.g., plastic bags) to efficiently navigate through narrow spaces, while existing planners neglect obstacle attributes, causing inefficiency or path-finding failures. To address this, we propose a trajectory planning framework integrating the 4WIS hybrid A* and Optimal Control Problem (OCP), in which the hybrid A* provides an initial path to enhance the OCP solution. Specifically, a multimodal classification network is introduced to assess scene complexity (hard/easy task) by fusing image and vehicle state data. For hard tasks, guided points are set to decompose complex tasks into local subtasks, improving the search efficiency of 4WIS hybrid A*. The multiple steering modes of 4WIS vehicles (Ackermann, diagonal, and zero-turn) are also incorporated into node expansion and heuristic designs. Moreover, a hierarchical obstacle handling strategy is designed to guide the node expansion considering obstacle attributes, i.e., 'non-traversable', 'crossable', and 'drive-over' obstacles. It allows crossing or driving over obstacles instead of the 'avoid-only' strategy, greatly enhancing success rates of pathfinding. We also design a logical constraint for the 'drive-over' obstacle by limiting its velocity to ensure safety. Furthermore, to address dynamic obstacles with motion uncertainty, we introduce a probabilistic risk field model, constructing risk-aware driving corridors that serve as linear collision constraints in OCP. Experimental results demonstrate the proposed framework's effectiveness in generating safe, efficient, and smooth trajectories for 4WIS vehicles, especially in constrained environments."}
{"id": "2512.19220", "pdf": "https://arxiv.org/pdf/2512.19220", "abs": "https://arxiv.org/abs/2512.19220", "authors": ["Bob Aubouin-Pairault", "Mazen Alamir", "Benjamin Meyer", "Rémi Wolf", "Kaouther Moussa"], "title": "How is remifentanil dosed without dedicated indicator?", "categories": ["eess.SP"], "comment": null, "summary": "This study investigates the paradigm of intraoperative analgesic dosage using a data-driven approach based on retrospective clinical data. Remifentanil, an analgesic widely used during anesthesia, presents a dosing challenge due to the absence of an universally accepted indicator of analgesia. To examine how changes in patient state correlate with adjustments in remifentanil target concentration triggered by the practitioner, we analyzed data from two sources: VitalDB (Seoul, Korea) and PREDIMED (Grenoble, France). Results show that only features derived from arterial pressure are consistently associated with changes in remifentanil targets. This finding is robust across both datasets despite variations in specific thresholds. In particular, increases in remifentanil targets are associated with high or rising arterial pressure over short periods (1--2 minutes), whereas decreases are linked to low, stable, or declining arterial pressure over longer periods (5--7 minutes). By capturing anesthesiologists' dosing strategies we provide a foundation for the future development of closed-loop control algorithms. Beyond the specific example of remifentanil's change prediction, the proposed feature generation and associated sparse fitting approach can be applied to other domain where human decision can be viewed as sensors interpretation."}
{"id": "2512.18850", "pdf": "https://arxiv.org/pdf/2512.18850", "abs": "https://arxiv.org/abs/2512.18850", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement", "categories": ["cs.RO"], "comment": null, "summary": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models."}
{"id": "2512.19263", "pdf": "https://arxiv.org/pdf/2512.19263", "abs": "https://arxiv.org/abs/2512.19263", "authors": ["Zonghan Wang", "Zahra Mobini", "Hien Quoc Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "Anti-Malicious ISAC: How to Jointly Monitor and Disrupt Your Foes?", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) systems are key enablers of future networks but raise significant security concerns. In this realm, the emergence of malicious ISAC systems has amplified the need for authorized parties to legitimately monitor suspicious communication links and protect legitimate targets from potential detection or exploitation by malicious foes. In this paper, we propose a new wireless proactive monitoring paradigm, where a legitimate monitor intercepts a suspicious communication link while performing cognitive jamming to enhance the monitoring success probability (MSP) and simultaneously safeguard the target. To this end, we derive closed-form expressions of the signal-to-interference-plus-noise-ratio (SINR) at the user (UE), sensing access points (S-APs), and an approximating expression of the SINR at the proactive monitor. Moreover, we propose an optimization technique under which the legitimate monitor minimizes the success detection probability (SDP) of the legitimate target, by optimizing the jamming power allocation over both communication and sensing channels subject to total power constraints and monitoring performance requirement. To enhance the monitor's longevity and reduce the risk of detection by malicious ISAC systems, we further propose an adaptive power allocation scheme aimed at minimizing the total transmit power at the monitor while meeting a pre-selected sensing SINR threshold and ensuring successful monitoring. Our numerical results show that the proposed algorithm significantly compromises the sensing and communication performance of malicious ISAC."}
{"id": "2512.18869", "pdf": "https://arxiv.org/pdf/2512.18869", "abs": "https://arxiv.org/abs/2512.18869", "authors": ["Georg Nawratil"], "title": "Construction and deformation of P-hedra using control polylines", "categories": ["cs.RO", "cs.CG"], "comment": "8 pages, 4 figures", "summary": "In the 19th International Symposium on Advances in Robot Kinematics the author introduced a novel class of continuous flexible discrete surfaces and mentioned that these so-called P-hedra (or P-nets) allow direct access to their spatial shapes by three control polylines. In this follow-up paper we study this intuitive method, which makes these flexible planar quad surfaces suitable for transformable design tasks by means of interactive tools. The construction of P-hedra from the control polylines can also be used for an efficient algorithmic computation of their isometric deformations. In addition we discuss flexion limits, bifurcation configurations, developable/flat-foldable pattern and tubular P-hedra."}
{"id": "2512.19442", "pdf": "https://arxiv.org/pdf/2512.19442", "abs": "https://arxiv.org/abs/2512.19442", "authors": ["Simon Welker", "Bunlong Lay", "Maris Hillemann", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching", "categories": ["eess.SP", "cs.LG", "cs.SD"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency."}
{"id": "2512.18922", "pdf": "https://arxiv.org/pdf/2512.18922", "abs": "https://arxiv.org/abs/2512.18922", "authors": ["Tianyuan Liu", "Richard Dazeley", "Benjamin Champion", "Akan Cosgun"], "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction", "categories": ["cs.RO"], "comment": "Accepted in ACRA 2025", "summary": "In this paper, we study whether inexpensive, physics-free supervision can reliably prioritize grasp-place candidates for budget-aware pick-and-place. From an object's initial pose, target pose, and a candidate grasp, we generate two path-aware geometric labels: path-wise inverse kinematics (IK) feasibility across a fixed approach-grasp-lift waypoint template, and a transit collision flag from mesh sweeps along the same template. A compact dual-output MLP learns these signals from pose encodings, and at test time its scores rank precomputed candidates for a rank-then-plan policy under the same IK gate and planner as the baseline. Although learned from cheap labels only, the scores transfer to physics-enabled executed trajectories: at a fixed planning budget the policy finds successful paths sooner with fewer planner calls while keeping final success on par or better. This work targets a single rigid cuboid with side-face grasps and a fixed waypoint template, and we outline extensions to varied objects and richer waypoint schemes."}
{"id": "2512.19639", "pdf": "https://arxiv.org/pdf/2512.19639", "abs": "https://arxiv.org/abs/2512.19639", "authors": ["Alejandro Ramírez-Arroyo", "O. S. Peñaherrera-Pulla", "Preben Mogensen"], "title": "Towards Reliable Connectivity: Measurement-Driven Assessment of Starlink and OneWeb Non-Terrestrial and 5G Terrestrial Networks", "categories": ["eess.SP"], "comment": "15 pages, 12 figures", "summary": "The emergence of commercial satellite communications networks, such as Starlink and OneWeb, has significantly transformed the communications landscape over the last years. As a complement to terrestrial cellular networks, non-terrestrial systems enable coverage extension and reliability enhancement beyond the limits of conventional infrastructure. Currently, the high reliance on terrestrial networks exposes communications to vulnerabilities in the event of terrestrial infrastructure failures, e.g., due to natural disasters. Therefore, this work proposes the joint evaluation of Key Performance Indicators (KPIs) for two non-terrestrial satellite networks (Starlink and OneWeb) and two terrestrial cellular networks to assess the current performance of these technologies across three different environments: (i) urban, (ii) suburban, and (iii) forest scenarios. Additionally, multi-connectivity techniques are explored to determine the benefits in connectivity when two technologies are used simultaneously. For instance, the outage probability of Starlink and OneWeb in urban areas is reduced from approximately 12-21\\% to 2\\% when both solutions are employed together. Finally, the joint analysis of KPIs in both terrestrial and non-terrestrial networks demonstrates that their integration enhances coverage, improves performance, and increases reliability, highlighting the benefits of combining satellite and terrestrial systems in the analyzed environments."}
{"id": "2512.18938", "pdf": "https://arxiv.org/pdf/2512.18938", "abs": "https://arxiv.org/abs/2512.18938", "authors": ["Yadong Liu", "Jianwei Liu", "He Liang", "Dimitrios Kanoulas"], "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research."}
{"id": "2512.18057", "pdf": "https://arxiv.org/pdf/2512.18057", "abs": "https://arxiv.org/abs/2512.18057", "authors": ["Sabri Mustafa Kahya", "Muhammet Sami Yavuz", "Boran Hamdi Sivrikaya", "Eckehard Steinbach"], "title": "FOODER: Real-time Facial Authentication and Expression Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "comment": "Book chapter", "summary": "Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time."}
{"id": "2512.18987", "pdf": "https://arxiv.org/pdf/2512.18987", "abs": "https://arxiv.org/abs/2512.18987", "authors": ["Ryosuke Korekata", "Quanting Xie", "Yonatan Bisk", "Komei Sugiura"], "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026", "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success."}
{"id": "2512.18207", "pdf": "https://arxiv.org/pdf/2512.18207", "abs": "https://arxiv.org/abs/2512.18207", "authors": ["Kanishka Roy", "Tahsin Fuad Hasan", "Chenfeng Wu", "Eshwar Vangala", "Roshan Ayyalasomayajula"], "title": "FedWiLoc: Federated Learning for Privacy-Preserving WiFi Indoor Localization", "categories": ["cs.CR", "eess.SP", "eess.SY"], "comment": null, "summary": "Current data-driven Wi-Fi-based indoor localization systems face three critical challenges: protecting user privacy, achieving accurate predictions in dynamic multipath environments, and generalizing across different deployments. Traditional Wi-Fi localization systems often compromise user privacy, particularly when facing compromised access points (APs) or man-in-the-middle attacks. As IoT devices proliferate in indoor environments, developing solutions that deliver accurate localization while robustly protecting privacy has become imperative. We introduce FedWiLoc, a privacy-preserving indoor localization system that addresses these challenges through three key innovations. First, FedWiLoc employs a split architecture where APs process Channel State Information (CSI) locally and transmit only privacy-preserving embedding vectors to user devices, preventing raw CSI exposure. Second, during training, FedWiLoc uses federated learning to collaboratively train the model across APs without centralizing sensitive user data. Third, we introduce a geometric loss function that jointly optimizes angle-of-arrival predictions and location estimates, enforcing geometric consistency to improve accuracy in challenging multipath conditions. Extensive evaluation across six diverse indoor environments spanning over 2,000 sq. ft. demonstrates that FedWiLoc outperforms state-of-the-art methods by up to 61.9% in median localization error while maintaining strong privacy guarantees throughout both training and inference."}
{"id": "2512.18988", "pdf": "https://arxiv.org/pdf/2512.18988", "abs": "https://arxiv.org/abs/2512.18988", "authors": ["Yanding Yang", "Weitao Zhou", "Jinhai Wang", "Xiaomin Guo", "Junze Wen", "Xiaolong Liu", "Lang Ding", "Zheng Fu", "Jinyu Miao", "Kun Jiang", "Diange Yang"], "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport."}
{"id": "2512.18210", "pdf": "https://arxiv.org/pdf/2512.18210", "abs": "https://arxiv.org/abs/2512.18210", "authors": ["Wen Huang", "Yuchen Mao", "Yanmin Qian"], "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection", "categories": ["cs.SD", "eess.SP"], "comment": null, "summary": "Achieving robust generalization in speech deepfake detection (SDD) remains a primary challenge, as models often fail to detect unseen forgery methods. While research has focused on model-centric and algorithm-centric solutions, the impact of data composition is often underexplored. This paper proposes a data-centric approach, analyzing the SDD data landscape from two practical perspectives: constructing a single dataset and aggregating multiple datasets. To address the first perspective, we conduct a large-scale empirical study to characterize the data scaling laws for SDD, quantifying the impact of source and generator diversity. To address the second, we propose the Diversity-Optimized Sampling Strategy (DOSS), a principled framework for mixing heterogeneous data with two implementations: DOSS-Select (pruning) and DOSS-Weight (re-weighting). Our experiments show that DOSS-Select outperforms the naive aggregation baseline while using only 3% of the total available data. Furthermore, our final model, trained on a 12k-hour curated data pool using the optimal DOSS-Weight strategy, achieves state-of-the-art performance, outperforming large-scale baselines with greater data and model efficiency on both public benchmarks and a new challenge set of various commercial APIs."}
{"id": "2512.19024", "pdf": "https://arxiv.org/pdf/2512.19024", "abs": "https://arxiv.org/abs/2512.19024", "authors": ["Xu Liu", "Yu Liu", "Hanshuo Qiu", "Yang Qirong", "Zhouhui Lian"], "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain."}
{"id": "2512.18222", "pdf": "https://arxiv.org/pdf/2512.18222", "abs": "https://arxiv.org/abs/2512.18222", "authors": ["Evangelos Vlachos"], "title": "Regularized Distributed MPC for UAV Networks: Stabilizing Coupled Motion and Hybrid Beam Alignment", "categories": ["eess.SY", "eess.SP"], "comment": "Submitted to IEEE Control Systems Letters (LCSS). 6 pages, 3 figures", "summary": "This letter investigates the coupled control problem in UAV networks utilizing high-frequency hybrid beamsteering. While phased arrays enable rapid electronic scanning, their finite Field of View (FoV) imposes a fundamental constraint that necessitates active mechanical steering of the airframe to maintain connectivity. We propose a decentralized Model Predictive Control (MPC) framework that jointly optimizes trajectory and heading to maximize network sum-capacity subject to safety constraints. Addressing the numerical instability caused by fast-fading channel nulls, we introduce a regularized surrogate cost function based on discrete spatial smoothing. We analytically prove that this approximation bounds the cost curvature, restoring the Lipschitz continuity of the gradient. Crucially, we derive a sufficient condition linking this Lipschitz constant to the controller gain, guaranteeing the contraction and linear convergence of the distributed best-response dynamics. Simulation results demonstrate that the proposed algorithm effectively navigates the trade-off between electronic beam tracking and kinematic safety, significantly systematically outperforming velocity-aligned baselines."}
{"id": "2512.19043", "pdf": "https://arxiv.org/pdf/2512.19043", "abs": "https://arxiv.org/abs/2512.19043", "authors": ["Chao Yang", "Yingkai Sun", "Peng Ye", "Xin Chen", "Chong Yu", "Tao Chen"], "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control", "categories": ["cs.RO"], "comment": null, "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks."}
{"id": "2512.18508", "pdf": "https://arxiv.org/pdf/2512.18508", "abs": "https://arxiv.org/abs/2512.18508", "authors": ["Barak Or"], "title": "The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics", "categories": ["stat.ME", "cs.AI", "eess.SP", "eess.SY"], "comment": "8 pages, preprint", "summary": "Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance."}
{"id": "2512.19083", "pdf": "https://arxiv.org/pdf/2512.19083", "abs": "https://arxiv.org/abs/2512.19083", "authors": ["Pengyu Chen", "Tao Ouyang", "Ke Luo", "Weijie Hong", "Xu Chen"], "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models", "categories": ["cs.RO"], "comment": "This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on \"Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems\"", "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation."}
{"id": "2512.18761", "pdf": "https://arxiv.org/pdf/2512.18761", "abs": "https://arxiv.org/abs/2512.18761", "authors": ["Dimitrios Tyrovolas", "Sotiris A. Tegos", "Yue Xiao", "Panagiotis D. Diamantoulakis", "Sotiris Ioannidis", "Christos K. Liaskos", "George K. Karagiannidis", "Stylianos D. Asimonis"], "title": "How Many Pinching Antennas Are Enough?", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Programmable wireless environments (PWEs) have emerged as a key paradigm for next-generation communication networks, aiming to transform wireless propagation from an uncontrollable phenomenon into a reconfigurable process that can adapt to diverse service requirements. In this framework, pinching-antenna systems (PASs) have recently been proposed as a promising enabling technology, as they allow the radiation location and effective propagation distance to be adjusted by selectively exciting radiating points along a dielectric waveguide. However, most existing studies on PASs rely on the idealized assumption that pinching-antenna (PA) positions can be continuously adjusted along the waveguide, while realistically only a finite set of pinching locations is available. Motivated by this, this paper analyzes the performance of two-state PASs, where the PA positions are fixed and only their activation state can be controlled. By explicitly accounting for the spatial discreteness of the available pinching points, closed-form analytical expressions for the outage probability and the ergodic achievable data rate are derived. In addition, we introduce the pinching discretization efficiency to quantify the performance gap between discrete and continuous pinching configurations, enabling a direct assessment of the number of PAs required to approximate the ideal continuous case. Finally, numerical results validate the analytical framework and show that near-continuous performance can be achieved with a limited number of PAs, offering useful insights for the design and deployment of PASs in PWEs."}
{"id": "2512.19133", "pdf": "https://arxiv.org/pdf/2512.19133", "abs": "https://arxiv.org/abs/2512.19133", "authors": ["Pengxuan Yang", "Ben Lu", "Zhongpu Xia", "Chao Han", "Yinfeng Gao", "Teng Zhang", "Kun Zhan", "XianPeng Lang", "Yupeng Zheng", "Qichao Zhang"], "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "AAAI 2026, first version", "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS)."}
{"id": "2512.19177", "pdf": "https://arxiv.org/pdf/2512.19177", "abs": "https://arxiv.org/abs/2512.19177", "authors": ["Guangjin Pan", "Zhixing Li", "Ayça Özçelikkale", "Christian Häger", "Musa Furkan Keskin", "Henk Wymeersch"], "title": "Semantic Communication for Rate-Limited Closed-Loop Distributed Communication-Sensing-Control Systems", "categories": ["eess.SY", "cs.NI", "eess.SP"], "comment": "13 pages, 18 figures. This work has been submitted to the IEEE for possible publication", "summary": "The growing integration of distributed integrated sensing and communication (ISAC) with closed-loop control in intelligent networks demands efficient information transmission under stringent bandwidth constraints. To address this challenge, this paper proposes a unified framework for goal-oriented semantic communication in distributed SCC systems. Building upon Weaver's three-level model, we establish a hierarchical semantic formulation with three error levels (L1: observation reconstruction, L2: state estimation, and L3: control) to jointly optimize their corresponding objectives. Based on this formulation, we propose a unified goal-oriented semantic compression and rate adaptation framework that is applicable to different semantic error levels and optimization goals across the SCC loop. A rate-limited multi-sensor LQR system is used as a case study to validate the proposed framework. We employ a GRU-based AE for semantic compression and a PPO-based rate adaptation algorithm that dynamically allocates transmission rates across sensors. Results show that the proposed framework effectively captures task-relevant semantics and adapts its resource allocation strategies across different semantic levels, thereby achieving level-specific performance gains under bandwidth constraints."}
{"id": "2512.19148", "pdf": "https://arxiv.org/pdf/2512.19148", "abs": "https://arxiv.org/abs/2512.19148", "authors": ["Jose Gustavo Buenaventura Carreon", "Floris Erich", "Roman Mykhailyshyn", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors", "categories": ["cs.RO"], "comment": "6 pages, 7 figures, conference: SII 2026. Cancun, Mexico", "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization."}
{"id": "2512.19309", "pdf": "https://arxiv.org/pdf/2512.19309", "abs": "https://arxiv.org/abs/2512.19309", "authors": ["Keivan Faghih Niresi", "Jun Qing", "Mengjie Zhao", "Olga Fink"], "title": "Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement."}
{"id": "2512.19178", "pdf": "https://arxiv.org/pdf/2512.19178", "abs": "https://arxiv.org/abs/2512.19178", "authors": ["Jin Wang", "Kim Tien Ly", "Jacques Cloete", "Nikos Tsagarakis", "Ioannis Havoutis"], "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Manuscript under review", "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/"}
{"id": "2512.19468", "pdf": "https://arxiv.org/pdf/2512.19468", "abs": "https://arxiv.org/abs/2512.19468", "authors": ["Mert Ozates", "Mohammad Kazemi", "Gianluigi Liva", "Deniz Gündüz"], "title": "Fully Asynchronous Unsourced Random Access over Fading Channels", "categories": ["cs.IT", "eess.SP"], "comment": null, "summary": "We examine unsourced random access in a fully asynchronous setup, where active users transmit their data without restriction on the start time over a fading channel. In the proposed scheme, the transmitted signal consists of a pilot sequence and a polar codeword, with the polar codeword distributed across the data part of the packet in an on-off pattern. The receiver uses a double sliding-window decoder, where the inner window employs iterative decoding with joint timing and pilot detection, channel estimation, single-user decoding, and successive interference cancellation to recover the message bits, while the outer window enhances interference cancellation. The numerical results indicate that the proposed scheme exhibits only a slight performance loss compared to the synchronous benchmark while being more applicable in practice."}
{"id": "2512.19269", "pdf": "https://arxiv.org/pdf/2512.19269", "abs": "https://arxiv.org/abs/2512.19269", "authors": ["Yitian Zheng", "Zhangchen Ye", "Weijun Dong", "Shengjie Wang", "Yuyang Liu", "Chongjie Zhang", "Chuan Wen", "Yang Gao"], "title": "Translating Flow to Policy via Hindsight Online Imitation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning."}
{"id": "2512.19270", "pdf": "https://arxiv.org/pdf/2512.19270", "abs": "https://arxiv.org/abs/2512.19270", "authors": ["Zhaoyang Liu", "Weitao Zhou", "Junze Wen", "Cheng Jing", "Qian Cheng", "Kun Jiang", "Diange Yang"], "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization", "categories": ["cs.RO"], "comment": "7 pages, 4 figures", "summary": "Collecting large-scale naturalistic driving data is essential for training robust autonomous driving planners. However, real-world datasets often contain a substantial amount of repetitive and low-value samples, which lead to excessive storage costs and bring limited benefits to policy learning. To address this issue, we propose an information-theoretic data pruning method that effectively reduces the training data volume without compromising model performance. Our approach evaluates the trajectory distribution information entropy of driving data and iteratively selects high-value samples that preserve the statistical characteristics of the original dataset in a model-agnostic manner. From a theoretical perspective, we show that maximizing trajectory entropy effectively constrains the Kullback-Leibler divergence between the pruned subset and the original data distribution, thereby maintaining generalization ability. Comprehensive experiments on the NuPlan benchmark with a large-scale imitation learning framework demonstrate that the proposed method can reduce the dataset size by up to 40% while maintaining closed-loop performance. This work provides a lightweight and theoretically grounded approach for scalable data management and efficient policy learning in autonomous driving systems."}
{"id": "2512.19289", "pdf": "https://arxiv.org/pdf/2512.19289", "abs": "https://arxiv.org/abs/2512.19289", "authors": ["Longxiang Shao", "Ulrich Dahmen", "Juergen Rossmann"], "title": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems", "categories": ["cs.RO"], "comment": "Accepted at the 10th MHI-Fachkolloquium", "summary": "Rigid body dynamics simulators are important tools for the design, analysis and optimization of mechanical systems in a variety of technical and scientific applications. This study examines four different simulation environments (Adams, Simscape, OpenModelica, and VEROSIM), focusing in particular on the comparison of the modeling methods, the numerical solvers, and the treatment of numerical problems that arise especially in closed-loop kinematics (esp. redundant boundary conditions and static equilibrium problem). A novel and complex crane boom of a real forestry machine serves as a practical benchmark application example. The direct comparison of the different solution approaches in the examined simulation tools supports the user in selecting the most suitable tool for his application."}
{"id": "2512.19347", "pdf": "https://arxiv.org/pdf/2512.19347", "abs": "https://arxiv.org/abs/2512.19347", "authors": ["Han Fang", "Yize Huang", "Yuheng Zhao", "Paul Weng", "Xiao Li", "Yutong Ban"], "title": "OMP: One-step Meanflow Policy with Directional Alignment", "categories": ["cs.RO"], "comment": null, "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation."}
{"id": "2512.19390", "pdf": "https://arxiv.org/pdf/2512.19390", "abs": "https://arxiv.org/abs/2512.19390", "authors": ["Hongwei Fan", "Hang Dai", "Jiyao Zhang", "Jinzhou Li", "Qiyang Yan", "Yujie Zhao", "Mingju Gao", "Jinghang Wu", "Hao Tang", "Hao Dong"], "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation", "categories": ["cs.RO", "cs.CV", "cs.GR"], "comment": null, "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io"}
{"id": "2512.19402", "pdf": "https://arxiv.org/pdf/2512.19402", "abs": "https://arxiv.org/abs/2512.19402", "authors": ["Yujie Zhao", "Hongwei Fan", "Di Chen", "Shengcong Chen", "Liliang Chen", "Xiaoqi Li", "Guanghui Ren", "Hao Dong"], "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface", "categories": ["cs.RO", "cs.CV", "cs.GR"], "comment": null, "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework."}
{"id": "2512.19453", "pdf": "https://arxiv.org/pdf/2512.19453", "abs": "https://arxiv.org/abs/2512.19453", "authors": ["Zhenglong Guo", "Yiming Zhao", "Feng Jiang", "Heng Jin", "Zongbao Feng", "Jianbin Zhou", "Siyuan Xu"], "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, This work was completed in December 2024", "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/."}
{"id": "2512.19562", "pdf": "https://arxiv.org/pdf/2512.19562", "abs": "https://arxiv.org/abs/2512.19562", "authors": ["Martin Sedlacek", "Pavlo Yefanov", "Georgy Ponimatkin", "Jai Bardhan", "Simon Pilc", "Mederic Fourmy", "Evangelos Kazakos", "Cees G. M. Snoek", "Josef Sivic", "Vladimir Petrik"], "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm"}
{"id": "2512.19564", "pdf": "https://arxiv.org/pdf/2512.19564", "abs": "https://arxiv.org/abs/2512.19564", "authors": ["Yanliang Huang", "Xia Yan", "Peiran Yin", "Zhenduo Zhang", "Zeyan Shao", "Youran Wang", "Haoliang Huang", "Matthias Althoff"], "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions."}
{"id": "2512.19567", "pdf": "https://arxiv.org/pdf/2512.19567", "abs": "https://arxiv.org/abs/2512.19567", "authors": ["Carlos Pérez-Ruiz", "Joan Solà"], "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry", "categories": ["cs.RO"], "comment": null, "summary": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello."}
{"id": "2512.19576", "pdf": "https://arxiv.org/pdf/2512.19576", "abs": "https://arxiv.org/abs/2512.19576", "authors": ["Kirill Djebko", "Tom Baumann", "Erik Dilger", "Frank Puppe", "Sergio Montenegro"], "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data", "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers."}
{"id": "2512.19583", "pdf": "https://arxiv.org/pdf/2512.19583", "abs": "https://arxiv.org/abs/2512.19583", "authors": ["Yinhuai Wang", "Runyi Yu", "Hok Wai Tsui", "Xiaoyi Lin", "Hui Zhang", "Qihan Zhao", "Ke Fan", "Miao Li", "Jie Song", "Jingbo Wang", "Qifeng Chen", "Ping Tan"], "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation."}
{"id": "2512.19629", "pdf": "https://arxiv.org/pdf/2512.19629", "abs": "https://arxiv.org/abs/2512.19629", "authors": ["Jiaqi Peng", "Wenzhe Cai", "Yuqiang Yang", "Tai Wang", "Yuan Shen", "Jiangmiao Pang"], "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "categories": ["cs.RO", "cs.CV"], "comment": "Project page:https://steinate.github.io/logoplanner.github.io/", "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}."}
{"id": "2512.18237", "pdf": "https://arxiv.org/pdf/2512.18237", "abs": "https://arxiv.org/abs/2512.18237", "authors": ["Shahram Najam Syed", "Yitian Hu", "Yuchao Yao"], "title": "Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 2 figures, 2 tables", "summary": "Photorealistic 3-D reconstruction from monocular video collapses in large-scale scenes when depth, pose, and radiance are solved in isolation: scale-ambiguous depth yields ghost geometry, long-horizon pose drift corrupts alignment, and a single global NeRF cannot model hundreds of metres of content. We introduce a joint learning framework that couples all three factors and demonstrably overcomes each failure case. Our system begins with a Vision-Transformer (ViT) depth network trained with metric-scale supervision, giving globally consistent depths despite wide field-of-view variations. A multi-scale feature bundle-adjustment (BA) layer refines camera poses directly in feature space--leveraging learned pyramidal descriptors instead of brittle keypoints--to suppress drift on unconstrained trajectories. For scene representation, we deploy an incremental local-radiance-field hierarchy: new hash-grid NeRFs are allocated and frozen on-the-fly when view overlap falls below a threshold, enabling city-block-scale coverage on a single GPU. Evaluated on the Tanks and Temples benchmark, our method reduces Absolute Trajectory Error to 0.001-0.021 m across eight indoor-outdoor sequences--up to 18x lower than BARF and 2x lower than NoPe-NeRF--while maintaining sub-pixel Relative Pose Error. These results demonstrate that metric-scale, drift-free 3-D reconstruction and high-fidelity novel-view synthesis are achievable from a single uncalibrated RGB camera."}
{"id": "2512.18320", "pdf": "https://arxiv.org/pdf/2512.18320", "abs": "https://arxiv.org/abs/2512.18320", "authors": ["Sun-Hyun Youn"], "title": "Deterministic Reconstruction of Tennis Serve Mechanics: From Aerodynamic Constraints to Internal Torques via Rigid-Body Dynamics", "categories": ["physics.app-ph", "cs.RO"], "comment": "9 figures", "summary": "Most conventional studies on tennis serve biomechanics rely on phenomenological observations comparing professional and amateur players or, more recently, on AI-driven statistical analyses of motion data. While effective at describing \\textit{what} elite players do, these approaches often fail to explain \\textit{why} such motions are physically necessary from a mechanistic perspective. This paper proposes a deterministic, physics-based approach to the tennis serve using a 12-degree-of-freedom multi-segment model of the human upper body. Rather than fitting the model to motion capture data, we solve the inverse kinematics problem via trajectory optimization to rigorously satisfy the aerodynamic boundary conditions required for Flat, Slice, and Kick serves. We subsequently perform an inverse dynamics analysis based on the Principle of Virtual Power to compute the net joint torques. The simulation results reveal that while the kinematic trajectories for different serves may share visual similarities, the underlying kinetic profiles differ drastically. A critical finding is that joints exhibiting minimal angular displacement (kinematically ``quiet'' phases), particularly at the wrist, require substantial and highly time-varying torques to counteract gravitational loading and dynamic coupling effects. By elucidating the dissociation between visible kinematics and internal kinetics, this study provides a first-principles framework for understanding the mechanics of the tennis serve, moving beyond simple imitation of elite techniques."}
{"id": "2512.18583", "pdf": "https://arxiv.org/pdf/2512.18583", "abs": "https://arxiv.org/abs/2512.18583", "authors": ["Pengcheng Li", "Qiang Fang", "Tong Zhao", "Yixing Lan", "Xin Xu"], "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL."}
{"id": "2512.18619", "pdf": "https://arxiv.org/pdf/2512.18619", "abs": "https://arxiv.org/abs/2512.18619", "authors": ["Zhenhao Zhou", "Dan Negrut"], "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories."}
{"id": "2512.18640", "pdf": "https://arxiv.org/pdf/2512.18640", "abs": "https://arxiv.org/abs/2512.18640", "authors": ["Kai Kohyama", "Yoshimitsu Aoki", "Guillermo Gallego", "Shintaro Shiba"], "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "15 pages, 10 figures, 5 tables", "summary": "Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released."}
{"id": "2512.18933", "pdf": "https://arxiv.org/pdf/2512.18933", "abs": "https://arxiv.org/abs/2512.18933", "authors": ["Hang Yu", "Juntu Zhao", "Yufeng Liu", "Kaiyu Li", "Cheng Ma", "Di Zhang", "Yingdong Hu", "Guang Chen", "Junyuan Xie", "Junliang Guo", "Junqiao Zhao", "Yang Gao"], "title": "Point What You Mean: Visually Grounded Instruction Policy", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control."}
{"id": "2512.19010", "pdf": "https://arxiv.org/pdf/2512.19010", "abs": "https://arxiv.org/abs/2512.19010", "authors": ["Devi Yuliarti", "Ravi Prakash", "Hiu Ching Cheung", "Amy Strong", "Patrick J. Codd", "Shan Lin"], "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation", "categories": ["eess.SP", "cs.RO"], "comment": null, "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making."}
{"id": "2512.19021", "pdf": "https://arxiv.org/pdf/2512.19021", "abs": "https://arxiv.org/abs/2512.19021", "authors": ["Sihao Lin", "Zerui Li", "Xunyi Zhao", "Gengze Zhou", "Liuyi Wang", "Rong Wei", "Rui Tang", "Juncheng Li", "Hanqing Wang", "Jiangmiao Pang", "Anton van den Hengel", "Jiajun Liu", "Qi Wu"], "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents."}
{"id": "2512.19245", "pdf": "https://arxiv.org/pdf/2512.19245", "abs": "https://arxiv.org/abs/2512.19245", "authors": ["Tarek Bouazza", "Alessandro Melis", "Soulaimane Berkane", "Robert Mahony", "Tarek Hamel"], "title": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements", "categories": ["eess.SY", "cs.RO"], "comment": "13 pages, 4 figures. Submitted to IFAC World Congress 2026", "summary": "This paper tackles the problem of estimating the relative position, orientation, and velocity between a UAV and a planar platform undergoing arbitrary 3D motion during approach and landing. The estimation relies on measurements from Inertial Measurement Units (IMUs) mounted on both systems, assuming there is a suitable communication channel to exchange data, together with visual information provided by an onboard monocular camera, from which the bearing (line-of-sight direction) to the platform's center and the normal vector of its planar surface are extracted. We propose a cascade observer with a complementary filter on SO(3) to reconstruct the relative attitude, followed by a linear Riccati observer for relative position and velocity estimation. Convergence of both observers is established under persistently exciting conditions, and the cascade is shown to be almost globally asymptotically and locally exponentially stable. We further extend the design to the case where the platform's rotation is restricted to its normal axis and show that its measured linear acceleration can be exploited to recover the remaining unobservable rotation angle. A sufficient condition to ensure local exponential convergence in this setting is provided. The performance of the proposed observers is validated through extensive simulations."}
{"id": "2512.19408", "pdf": "https://arxiv.org/pdf/2512.19408", "abs": "https://arxiv.org/abs/2512.19408", "authors": ["Philipp L. Kinon", "Simon R. Eugster", "Peter Betsch"], "title": "Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework", "categories": ["math.NA", "cs.CE", "cs.RO", "eess.SY", "math.DS"], "comment": "37 pages, 16 figures, currently under review", "summary": "An energy-based modeling framework for the nonlinear dynamics of spatial Cosserat rods undergoing large displacements and rotations is proposed. The mixed formulation features independent displacement, velocity and stress variables and is further objective and locking-free. Finite rotations are represented using a director formulation that avoids singularities and yields a constant mass matrix. This results in an infinite-dimensional nonlinear port-Hamiltonian (PH) system governed by partial differential-algebraic equations with a quadratic energy functional. Using a time-differentiated compliance form of the stress-strain relations allows for the imposition of kinematic constraints, such as inextensibility or shear-rigidity. A structure-preserving finite element discretization leads to a finite-dimensional system with PH structure, thus facilitating the design of an energy-momentum consistent integration scheme. Dissipative material behavior (via the generalized-Maxwell model) and non-standard actuation approaches (via pneumatic chambers or tendons) integrate naturally into the framework. As illustrated by selected numerical examples, the present framework establishes a new approach to energy-momentum consistent formulations in computational mechanics involving finite rotations."}
{"id": "2512.19451", "pdf": "https://arxiv.org/pdf/2512.19451", "abs": "https://arxiv.org/abs/2512.19451", "authors": ["Nitin Kumar Singh", "Arie Rachmad Syulistyo", "Yuichiro Tanaka", "Hakaru Tamukoh"], "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices."}
{"id": "2512.19684", "pdf": "https://arxiv.org/pdf/2512.19684", "abs": "https://arxiv.org/abs/2512.19684", "authors": ["Dixuan Lin", "Tianyou Wang", "Zhuoyang Pan", "Yufu Wang", "Lingjie Liu", "Kostas Daniilidis"], "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video."}
