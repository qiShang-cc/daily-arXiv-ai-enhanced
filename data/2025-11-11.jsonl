{"id": "2511.05522", "pdf": "https://arxiv.org/pdf/2511.05522", "abs": "https://arxiv.org/abs/2511.05522", "authors": ["Ali Saeizadeh", "Miead Tehrani-Moayyed", "Davide Villa", "J. Gordon Beattie", "Pedram Johari", "Stefano Basagni", "Tommaso Melodia"], "title": "AIRMap - AI-Generated Radio Maps for Wireless Digital Twins", "categories": ["eess.SP", "cs.AI"], "comment": "13 pages, 17 figures, This paper has been submitted to the IEEE Transactions for possible publication", "summary": "Accurate, low-latency channel modeling is essential for real-time wireless network simulation and digital-twin applications. Traditional modeling methods like ray tracing are however computationally demanding and unsuited to model dynamic conditions. In this paper, we propose AIRMap, a deep-learning framework for ultra-fast radio-map estimation, along with an automated pipeline for creating the largest radio-map dataset to date. AIRMap uses a single-input U-Net autoencoder that processes only a 2D elevation map of terrain and building heights. Trained and evaluated on 60,000 Boston-area samples, spanning coverage areas from 500 m to 3 km per side, AIRMap predicts path gain with under 5 dB RMSE in 4 ms per inference on an NVIDIA L40S -over 7000x faster than GPU-accelerated ray tracing based radio maps. A lightweight transfer learning calibration using just 20% of field measurements reduces the median error to approximately 10%, significantly outperforming traditional simulators, which exceed 50% error. Integration into the Colosseum emulator and the Sionna SYS platform demonstrate near-zero error in spectral efficiency and block-error rate compared to measurement-based channels. These findings validate AIRMap's potential for scalable, accurate, and real-time radio map estimation in wireless digital twins."}
{"id": "2511.05642", "pdf": "https://arxiv.org/pdf/2511.05642", "abs": "https://arxiv.org/abs/2511.05642", "authors": ["Justin Williams", "Kishor Datta Gupta", "Roy George", "Mrinmoy Sarkar"], "title": "Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots", "categories": ["cs.RO", "cs.AR", "cs.CV", "eess.SY"], "comment": null, "summary": "The deployment of artificial intelligence models at the edge is increasingly critical for autonomous robots operating in GPS-denied environments where local, resource-efficient reasoning is essential. This work demonstrates the feasibility of deploying small Vision-Language Models (VLMs) on mobile robots to achieve real-time scene understanding and reasoning under strict computational constraints. Unlike prior approaches that separate perception from mobility, the proposed framework enables simultaneous movement and reasoning in dynamic environments using only on-board hardware. The system integrates a compact VLM with multimodal perception to perform contextual interpretation directly on embedded hardware, eliminating reliance on cloud connectivity. Experimental validation highlights the balance between computational efficiency, task accuracy, and system responsiveness. Implementation on a mobile robot confirms one of the first successful deployments of small VLMs for concurrent reasoning and mobility at the edge. This work establishes a foundation for scalable, assured autonomy in applications such as service robotics, disaster response, and defense operations."}
{"id": "2511.05680", "pdf": "https://arxiv.org/pdf/2511.05680", "abs": "https://arxiv.org/abs/2511.05680", "authors": ["Jeong-Jung Kim", "Doo-Yeol Koh", "Chang-Hyun Kim"], "title": "VLM-driven Skill Selection for Robotic Assembly Tasks", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition."}
{"id": "2511.05785", "pdf": "https://arxiv.org/pdf/2511.05785", "abs": "https://arxiv.org/abs/2511.05785", "authors": ["Lianhao Yin", "Haiping Yu", "Pascal Spino", "Daniela Rus"], "title": "A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms", "categories": ["cs.RO"], "comment": null, "summary": "Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \\textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics."}
{"id": "2511.06035", "pdf": "https://arxiv.org/pdf/2511.06035", "abs": "https://arxiv.org/abs/2511.06035", "authors": ["Vincenzo Mottola", "Alessandro Sardellitti", "Filippo Milano", "Luigi Ferrigno", "Marco Laracca", "Antonello Tamburrino"], "title": "Invariants in Eddy Current Testing via Dimensional Analysis", "categories": ["eess.SP"], "comment": null, "summary": "The Buckingham's $π$, theorem has been recently introduced in the context of Non destructive Testing \\& Evaluation (NdT\\&E) , giving a theoretical basis for developing simple but effective methods for multi-parameter estimation via dimensional analysis. Dimensional groups, or $π-$groups, allow for the reduction of the number of parameters affecting the dimensionless measured quantities.\n  In many real-world applications, the main interest is in estimating only a subset of the variables affecting the measurements. An example is estimating the thickness and electrical conductivity of a plate from Eddy Current Testing data, regardless of the lift-off of the probe, which may be either uncertain and/or variable. Alternatively, one may seek to estimate thickness and lift-off while neglecting the influence of the electrical conductivity, or to estimate the electrical conductivity and the lift-off, neglecting the thickness.\n  This is where the concept of invariants becomes crucial. An invariant transformation is a mathematical mapping that makes the measured signal independent of one or more of these uncertain parameters. Invariant transformations provide a way to isolate useful signals from uncertain ones, improving the accuracy and reliability of the NdT results.\n  The main contribution of this paper is a systematic method to derive \\emph{invariant} transformations for frequency domain Eddy Current Testing data, via dimensional analysis. The proposed method is compatible with real-time and in-line operations.\n  After its theoretical foundation is introduced, the method is validated by means of experimental data, with reference to configurations consisting of plates with different thicknesses, electrical conductivity, and lift-off. The experimental validation proves the effectiveness of the method in achieving excellent accuracy on a wide range of parameters of interest."}
{"id": "2511.05791", "pdf": "https://arxiv.org/pdf/2511.05791", "abs": "https://arxiv.org/abs/2511.05791", "authors": ["Manav Kulshrestha", "S. Talha Bukhari", "Damon Conover", "Aniket Bera"], "title": "VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "8 pages, 4 figures, under review", "summary": "Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod \"impales\" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation."}
{"id": "2511.06036", "pdf": "https://arxiv.org/pdf/2511.06036", "abs": "https://arxiv.org/abs/2511.06036", "authors": ["Hassan Hizeh", "Rim Chighri", "Muhammad Mahboob Ur Rahman", "Mohamed A. Bahloul", "Ali Muqaibel", "Tareq Y. Al-Naffouri"], "title": "Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook", "categories": ["eess.SP", "cs.RO"], "comment": "20 pages, 5 figures, 4 tables, under review with a journal", "summary": "The current body of research on Parkinson's disease (PD) screening, monitoring, and management has evolved along two largely independent trajectories. The first research community focuses on multimodal sensing of PD-related biomarkers using noninvasive technologies such as inertial measurement units (IMUs), force/pressure insoles, electromyography (EMG), electroencephalography (EEG), speech and acoustic analysis, and RGB/RGB-D motion capture systems. These studies emphasize data acquisition, feature extraction, and machine learning-based classification for PD screening, diagnosis, and disease progression modeling. In parallel, a second research community has concentrated on robotic intervention and rehabilitation, employing socially assistive robots (SARs), robot-assisted rehabilitation (RAR) systems, and virtual reality (VR)-integrated robotic platforms for improving motor and cognitive function, enhancing social engagement, and supporting caregivers. Despite the complementary goals of these two domains, their methodological and technological integration remains limited, with minimal data-level or decision-level coupling between the two. With the advent of advanced artificial intelligence (AI), including large language models (LLMs), agentic AI systems, a unique opportunity now exists to unify these research streams. We envision a closed-loop sensor-AI-robot framework in which multimodal sensing continuously guides the interaction between the patient, caregiver, humanoid robot (and physician) through AI agents that are powered by a multitude of AI models such as robotic and wearables foundation models, LLM-based reasoning, reinforcement learning, and continual learning. Such closed-loop system enables personalized, explainable, and context-aware intervention, forming the basis for digital twin of the PD patient that can adapt over time to deliver intelligent, patient-centered PD care."}
{"id": "2511.05798", "pdf": "https://arxiv.org/pdf/2511.05798", "abs": "https://arxiv.org/abs/2511.05798", "authors": ["William R. Johnson", "Patrick Meng", "Nelson Chen", "Luca Cimatti", "Augustin Vercoutere", "Mridul Aanjaneya", "Rebecca Kramer-Bottiglio", "Kostas E. Bekris"], "title": "An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles", "categories": ["cs.RO"], "comment": null, "summary": "Tensegrity robots, composed of rigid struts and elastic tendons, provide impact resistance, low mass, and adaptability to unstructured terrain. Their compliance and complex, coupled dynamics, however, present modeling and control challenges, hindering path planning and obstacle avoidance. This paper presents a complete, open-source, and reproducible system that enables navigation for a 3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source hardware design, and (ii) an integrated, open-source software stack for physics-based modeling, system identification, state estimation, path planning, and control. All hardware and software are publicly available at https://sites.google.com/view/tensegrity-navigation/. The proposed system tracks the robot's pose and executes collision-free paths to a specified goal among known obstacle locations. System robustness is demonstrated through experiments involving unmodeled environmental challenges, including a vertical drop, an incline, and granular media, culminating in an outdoor field demonstration. To validate reproducibility, experiments were conducted using robot instances at two different laboratories. This work provides the robotics community with a complete navigation system for a compliant, impact-resistant, and shape-morphing robot. This system is intended to serve as a springboard for advancing the navigation capabilities of other unconventional robotic platforms."}
{"id": "2511.06045", "pdf": "https://arxiv.org/pdf/2511.06045", "abs": "https://arxiv.org/abs/2511.06045", "authors": ["Yakov Gusakov", "Osvaldo Simeone", "Tirza Routtenberg", "Nir Shlezinger"], "title": "Online Learning of Modular Bayesian Deep Receivers: Single-Step Adaptation with Streaming Data", "categories": ["eess.SP", "cs.IT"], "comment": "Under review for publication in the IEEE", "summary": "Deep neural network (DNN)-based receivers offer a powerful alternative to classical model-based designs for wireless communication, especially in complex and nonlinear propagation environments. However, their adoption is challenged by the rapid variability of wireless channels, which makes pre-trained static DNN-based receivers ineffective, and by the latency and computational burden of online stochastic gradient descent (SGD)-based learning. In this work, we propose an online learning framework that enables rapid low-complexity adaptation of DNN-based receivers. Our approach is based on two main tenets. First, we cast online learning as Bayesian tracking in parameter space, enabling a single-step adaptation, which deviates from multi-epoch SGD . Second, we focus on modular DNN architectures that enable parallel, online, and localized variational Bayesian updates. Simulations with practical communication channels demonstrate that our proposed online learning framework can maintain a low error rate with markedly reduced update latency and increased robustness to channel dynamics as compared to traditional gradient descent based method."}
{"id": "2511.05809", "pdf": "https://arxiv.org/pdf/2511.05809", "abs": "https://arxiv.org/abs/2511.05809", "authors": ["Yu Chen", "Botao He", "Yuemin Mao", "Arthur Jakobsson", "Jeffrey Ke", "Yiannis Aloimonos", "Guanya Shi", "Howie Choset", "Jiayuan Mao", "Jeffrey Ichnowski"], "title": "Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "For many complex tasks, multi-finger robot hands are poised to revolutionize how we interact with the world, but reliably grasping objects remains a significant challenge. We focus on the problem of synthesizing grasps for multi-finger robot hands that, given a target object's geometry and pose, computes a hand configuration. Existing approaches often struggle to produce reliable grasps that sufficiently constrain object motion, leading to instability under disturbances and failed grasps. A key reason is that during grasp generation, they typically focus on resisting a single wrench, while ignoring the object's potential for adversarial movements, such as escaping. We propose a new grasp-synthesis approach that explicitly captures and leverages the adversarial object motion in grasp generation by formulating the problem as a two-player game. One player controls the robot to generate feasible grasp configurations, while the other adversarially controls the object to seek motions that attempt to escape from the grasp. Simulation experiments on various robot platforms and target objects show that our approach achieves a success rate of 75.78%, up to 19.61% higher than the state-of-the-art baseline. The two-player game mechanism improves the grasping success rate by 27.40% over the method without the game formulation. Our approach requires only 0.28-1.04 seconds on average to generate a grasp configuration, depending on the robot platform, making it suitable for real-world deployment. In real-world experiments, our approach achieves an average success rate of 85.0% on ShadowHand and 87.5% on LeapHand, which confirms its feasibility and effectiveness in real robot setups."}
{"id": "2511.06060", "pdf": "https://arxiv.org/pdf/2511.06060", "abs": "https://arxiv.org/abs/2511.06060", "authors": ["Jie Ma", "Pinjun Zheng", "Xing Liu", "Yuchen Zhang", "Ali A. Nasir", "Tareq Y. Al-Naffouri"], "title": "Positioning Using LEO Satellite Communication Signals Under Orbital Errors", "categories": ["eess.SP"], "comment": null, "summary": "Low Earth orbit (LEO) satellites offer a promising alternative to global navigation satellite systems for precise positioning; however, their relatively low altitudes make them more susceptible to orbital perturbations, which in turn degrade positioning accuracy. In this work, we study LEO-based positioning under orbital errors within a signal-of-opportunity framework. First, we introduce a LEO orbit model that accounts for Earth's non-sphericity and derive a wideband communication model that captures fast- and slow-time Doppler effects and multipath propagation. Subsequently, we perform a misspecified Cramér-Rao bound (MCRB) analysis to evaluate the impact of orbital errors on positioning performance. Then, we propose a two-stage positioning method starting with a (i) MCRB-based weighted orbit calibration, followed by (ii) least-squares user positioning using the corrected orbit. The MCRB analysis indicates that orbital errors can induce kilometer-level position biases. Extensive simulations show that the proposed estimator can considerably enhance the positioning accuracy relative to the orbit-mismatched baseline, yielding errors on the order of a few meters."}
{"id": "2511.05816", "pdf": "https://arxiv.org/pdf/2511.05816", "abs": "https://arxiv.org/abs/2511.05816", "authors": ["Taku Okawara", "Ryo Nishibe", "Mao Kasano", "Kentaro Uno", "Kazuya Yoshida"], "title": "3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots", "categories": ["cs.RO"], "comment": "International Conference on Space Robotics (iSpaRo)", "summary": "Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars. In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views. While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.\n  This work presents a 3D terrain mapping system designed for space exploration using limbed climbing robots equipped with a monocular hand-eye camera. Compared to RGB-D cameras, monocular cameras are more lightweight, compact structures, and have lower power consumption. Although monocular SLAM can be used to construct 3D maps, it suffers from scale ambiguity. To address this limitation, we propose a SLAM method that fuses monocular visual constraints with limb forward kinematics. The proposed method jointly estimates time-series gripper poses and the global metric scale of the 3D map based on factor graph optimization.\n  We validate the proposed framework through both physics-based simulations and real-world experiments. The results demonstrate that our framework constructs a metrically scaled 3D terrain map in real-time and enables autonomous grasping of convex terrain surfaces using a monocular hand-eye camera, without relying on RGB-D cameras. Our method contributes to scalable and energy-efficient perception for future space missions involving limbed climbing robots. See the video summary here: https://youtu.be/fMBrrVNKJfc"}
{"id": "2511.06173", "pdf": "https://arxiv.org/pdf/2511.06173", "abs": "https://arxiv.org/abs/2511.06173", "authors": ["Liyang Lu", "Haochen Wu", "Wenbo Xu", "Zhaocheng Wang", "H. Vincent Poor"], "title": "Hierarchically Block-Sparse Recovery With Prior Support Information", "categories": ["eess.SP"], "comment": "This manuscript has been accepted by IEEE Transactions on Information Theory (IEEE TIT) in Nov. 2025", "summary": "We provide new recovery bounds for hierarchical compressed sensing (HCS) based on prior support information (PSI). A detailed PSI-enabled reconstruction model is formulated using various forms of PSI. The hierarchical block orthogonal matching pursuit with PSI (HiBOMP-P) algorithm is designed in a recursive form to reliably recover hierarchically block-sparse signals. We derive exact recovery conditions (ERCs) measured by the mutual incoherence property (MIP), wherein hierarchical MIP concepts are proposed, and further develop reconstructible sparsity levels to reveal sufficient conditions for ERCs. Leveraging these MIP analyses, we present several extended insights, including reliable recovery conditions in noisy scenarios and the optimal hierarchical structure for cases where sparsity is not equal to zero. Our results further confirm that HCS offers improved recovery performance even when the prior information does not overlap with the true support set, whereas existing methods heavily rely on this overlap, thereby compromising performance if it is absent."}
{"id": "2511.05855", "pdf": "https://arxiv.org/pdf/2511.05855", "abs": "https://arxiv.org/abs/2511.05855", "authors": ["Jiayu Zhou", "Qiwei Wu", "Jian Li", "Zhe Chen", "Xiaogang Xiong", "Renjing Xu"], "title": "Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills", "categories": ["cs.RO"], "comment": "Accepted for the 40th Annual AAAI Conference on Artificial Intelligence (2026)", "summary": "Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks."}
{"id": "2511.06188", "pdf": "https://arxiv.org/pdf/2511.06188", "abs": "https://arxiv.org/abs/2511.06188", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "title": "Meta-Learning-Driven GFlowNets for 3D Directional Modulation in Mobile Wireless Systems", "categories": ["eess.SP"], "comment": "Submitted to IEEE ICC 2026", "summary": "In our prior work we have proposed the use of GFlowNets, a generative AI (GenAI) framework, for designing a secure communication system comprising a time-modulated intelligent reflecting surface (TM-IRS). However, GFlowNet-based approaches assume static environments, limiting their applicability in mobile wireless networks. In this paper, we proposes a novel Meta-GFlowNet framework that achieves rapid adaptation to dynamic conditions using model-agnostic meta-learning. As the communication user is moving, the framework learns a direction-general prior across user directions via inner trajectory-balance updates and outer meta-updates, enabling quick convergence to new user directions. The approach requires no labeled data, employing a pseudo-supervised consistency objective derived from the learned reward by GFlowNet and the actual sum-rate reward of the TM-IRS system. Simulation results show that the proposed method attains faster adaptation and higher secrecy performance than retrained GFlowNets, offering an efficient GenAI framework for dynamic wireless environments. Although the scenario considered here focuses on directional modulation-based physical-layer security, the proposed framework can also be applied to other mobile wireless systems, such as joint sensing-communication networks, that utilize GFlowNets."}
{"id": "2511.05858", "pdf": "https://arxiv.org/pdf/2511.05858", "abs": "https://arxiv.org/abs/2511.05858", "authors": ["Chuanyu Li", "Chaoyi Liu", "Daotan Wang", "Shuyu Zhang", "Lusong Li", "Zecui Zeng", "Fangchen Liu", "Jing Xu", "Rui Chen"], "title": "ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface", "categories": ["cs.RO"], "comment": null, "summary": "Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems."}
{"id": "2511.06257", "pdf": "https://arxiv.org/pdf/2511.06257", "abs": "https://arxiv.org/abs/2511.06257", "authors": ["Yimeng Lin", "Nan Wang", "Daniel Abraham", "Daniel Polak", "Xiaozhi Cao", "Stephen Cauley", "Kawin Setsompop"], "title": "Fast Reconstruction of Motion-Corrupted Data with Mobile-GRAPPA: Motion and dB0 Inhomogeneity Correction Leveraging Efficient GRAPPA", "categories": ["eess.SP"], "comment": "8 pages, 5 figures", "summary": "Advanced motion navigations now enable rapid tracking of subject motion and dB0-induced phase, but accurately incorporating this high-temporal-resolution information into SENSE (Aligned-SENSE) is often computationally prohibitive. We propose \"Mobile-GRAPPA\", a k-space \"cleaning\" approach that uses local GRAPPA operators to remove motion and dB0 related corruption so that the resulting data can be reconstructed with standard SENSE. We efficiently train a family of k-space-position-specific Mobile-GRAPPA kernels via a lightweight multilayer perceptron (MLP) and apply them across k-space to generate clean data. In experiments on highly motion-corrupted 1-mm whole-brain GRE (Tacq = 10 min; 1,620 motion/dB0 trackings) and EPTI (Tacq = 2 min; 544 trackings), Mobile-GRAPPA enabled accurate reconstruction with negligible time penalty, whereas full Aligned-SENSE was impractical (reconstruction times > 10 h for GRE and > 10 days for EPTI). These results show that Mobile-GRAPPA incorporates detailed motion and dB0 tracking into SENSE with minimal computational overhead, enabling fast, high-quality reconstructions of challenging data."}
{"id": "2511.05886", "pdf": "https://arxiv.org/pdf/2511.05886", "abs": "https://arxiv.org/abs/2511.05886", "authors": ["Lei Shi", "Yongju Kim", "Xinzhi Zhong", "Wissam Kontar", "Qichao Liu", "Soyoung Ahn"], "title": "Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections", "categories": ["cs.RO"], "comment": null, "summary": "Ensuring fairness in the coordination of connected and automated vehicles at intersections is essential for equitable access, social acceptance, and long-term system efficiency, yet it remains underexplored in safety-critical, real-time traffic control. This paper proposes a fairness-aware hierarchical control framework that explicitly integrates inequity aversion into intersection management. At the top layer, a centralized allocation module assigns control authority (i.e., selects a single vehicle to execute its trajectory) by maximizing a utility that accounts for waiting time, urgency, control history, and velocity deviation. At the bottom layer, the authorized vehicle executes a precomputed trajectory using a Linear Quadratic Regulator (LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety filter for real-time collision avoidance. Simulation results across varying traffic demands and demand distributions demonstrate that the proposed framework achieves near-perfect fairness, eliminates collisions, reduces average delay, and maintains real-time feasibility. These results highlight that fairness can be systematically incorporated without sacrificing safety or performance, enabling scalable and equitable coordination for future autonomous traffic systems."}
{"id": "2511.06270", "pdf": "https://arxiv.org/pdf/2511.06270", "abs": "https://arxiv.org/abs/2511.06270", "authors": ["Abdulahi Abiodun Badrudeen", "Nakyung Lee", "Adam Dubs", "Sunwoo Kim"], "title": "Blocker-Aware Beamforming and Dynamic Power Allocation for Multicarrier ISAC-NOMA Systems", "categories": ["eess.SP"], "comment": "6 pages, 5 figures, conference", "summary": "This paper proposes a blocker-aware multicarrier integrated sensing and communication (ISAC)-non orthogonal multiple access (NOMA) system, leveraging hybrid beamforming and dynamic power allocation to enhance spectrum efficiency in 6G networks. Recognizing the performance degradation caused by environmental blockers, the system introduces a joint waveform design that ensures robust operation under varying channel conditions. A channel switching mechanism is deployed to reroute communication through alternative non-line-of-sight paths when the primary line-of-sight links are obstructed. Moreover, a dynamic power allocation strategy enforces a minimum rate constraint for the weak NOMA user, ensuring consistent quality of service. Extensive simulations over multiple blockage scenarios and signal to noise (SNR) conditions validate the effectiveness of the proposed solution. Notably, under severe blockage, the system achieves up to a 400% sensing rate enhancement at 15 dB SNR, with only a 20% reduction in communication rate. These results corroborate the system's ability to adapt and optimize joint sensing-communication performance in practical deployment environments."}
{"id": "2511.05889", "pdf": "https://arxiv.org/pdf/2511.05889", "abs": "https://arxiv.org/abs/2511.05889", "authors": ["Zeyuan Feng", "Haimingyue Zhang", "Somil Bansal"], "title": "From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios."}
{"id": "2511.06339", "pdf": "https://arxiv.org/pdf/2511.06339", "abs": "https://arxiv.org/abs/2511.06339", "authors": ["Shasha Liu", "Hayssam Dahrouj", "Abla Kammoun", "Mohamed-Slim Alouini"], "title": "Sum Rate and Worst Case SINR Optimization in Multi HAPS Ground Integrated Networks", "categories": ["eess.SP"], "comment": null, "summary": "Balancing throughput and fairness promises to be a key enabler for achieving large-scale digital inclusion in future vertical heterogeneous networks (VHetNets). In an attempt to address the global digital divide problem, this paper explores a multi-high-altitude platform system (HAPS)-ground integrated network, in which multiple HAPSs collaborate with ground base stations (BSs) to enhance the users' quality of service on the ground to achieve the highly sought-after digital equity. To this end, this paper considers maximizing both the network-wide weighted sum rate function and the worst-case signal-to-interference-plus-noise ratio (SINR) function subject to the same system level constraints. More specifically, the paper tackles the two different optimization problems so as to balance throughput and fairness, by accounting for the individual HAPS payload connectivity constraints, HAPS and BS distinct power limitations, and per-user rate requirements. This paper solves the considered problems using techniques from optimization theory by adopting a generalized assignment problem (GAP)-based methodology to determine the user association variables, jointly with successive convex approximation (SCA)-based iterative algorithms for optimizing the corresponding beamforming vectors. One of the main advantages of the proposed algorithms is their amenability for distributed implementation across the multiple HAPSs and BSs. The simulation results particularly validate the performance of the presented algorithms, demonstrating the capability of multi-HAPS networks to boost-up the overall network digital inclusion toward democratizing future digital services."}
{"id": "2511.05936", "pdf": "https://arxiv.org/pdf/2511.05936", "abs": "https://arxiv.org/abs/2511.05936", "authors": ["Soujanya Poria", "Navonil Majumder", "Chia-Yu Hung", "Amir Ali Bagherzadeh", "Chuan Li", "Kenneth Kwok", "Ziwei Wang", "Cheston Tan", "Jiajun Wu", "David Hsu"], "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI"], "comment": "AAAI 2026 (Senior Track)", "summary": "Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability."}
{"id": "2511.06359", "pdf": "https://arxiv.org/pdf/2511.06359", "abs": "https://arxiv.org/abs/2511.06359", "authors": ["Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato", "Geng Sun", "Weijie Yuan", "Abbas Jamalipour", "Tao Xiang"], "title": "Stackelberg Game-Driven Defense for ISAC Against Channel Attacks in Low-Altitude Networks", "categories": ["eess.SP", "cs.GT"], "comment": "6 pages, 4 figures", "summary": "The increasing saturation of terrestrial resources has driven economic activities into low-altitude airspace. These activities, such as air taxis, rely on low-altitude wireless networks, and one key enabling technology is integrated sensing and communication (ISAC). However, in low-altitude airspace, ISAC is vulnerable to channel-access attacks, thereby degrading performance and threatening safety. To address this, we propose a defense framework based on a Stackelberg game. Specifically, we first model the system under attack, deriving metrics for the communication and the sensing to quantify performance. Then, we formulate the interaction as a three-player game where a malicious attacker acts as the leader, while the legitimate drone and ground base station act as followers. Using a backward induction algorithm, we obtain the Stackelberg equilibrium, allowing the defenders to dynamically adjust their strategies to mitigate the attack. Simulation results verify that the proposed algorithm converges to a stable solution and outperforms existing baselines, ensuring reliable ISAC performance for critical low-altitude applications."}
{"id": "2511.05995", "pdf": "https://arxiv.org/pdf/2511.05995", "abs": "https://arxiv.org/abs/2511.05995", "authors": ["Jianbo Yuan", "Jing Dai", "Yerui Fan", "Yaxiong Wu", "Yunpeng Liang", "Weixin Yan"], "title": "Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm", "categories": ["cs.RO"], "comment": "20 pages, 13 figures", "summary": "The human arm exhibits remarkable capabilities, including both explosive power and precision, which demonstrate dexterity, compliance, and robustness in unstructured environments. Developing robotic systems that emulate human-like operational characteristics through musculoskeletal structures has long been a research focus. In this study, we designed a novel lightweight tendon-driven musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF) skeletal joint system and a modularized artificial muscular system (MAMS) with 15 actuators. Additionally, we employed a Hilly-type muscle model and data-driven iterative learning control (DDILC) to learn and refine activation signals for repetitive tasks within a finite time frame. We validated the anti-interference capabilities of the musculoskeletal system through both simulations and experiments. The results show that the LTDM-Arm system can effectively achieve desired trajectory tracking tasks, even under load disturbances of 20 % in simulation and 15 % in experiments. This research lays the foundation for developing advanced robotic systems with human-like operational performance."}
{"id": "2511.06369", "pdf": "https://arxiv.org/pdf/2511.06369", "abs": "https://arxiv.org/abs/2511.06369", "authors": ["Wonseok Choi", "Jeongjae Lee", "Songnam Hong"], "title": "CSIT-Free Multi-Group Multicast Transmission in Overloaded mmWave Systems", "categories": ["eess.SP"], "comment": "Submitted to IEEE ICC 2026", "summary": "In this paper, we investigate the downlink multi-group multicast (MGM) transmission problem in overloaded mmWave systems. In particular, the conventional MGM beamforming requires substantial computational complexity and feedback (or pilot) overhead for acquisition of channel state information at the transmitter (CSIT), while simultaneous interference management and multicast beamforming optimization across multi-group inevitably incurs a significant rate loss. To address this, we propose a CSIT-free MGM (CF-MGM) transmission that eliminates the need for a complex CSIT acquisition. A deterministic CSIT-free precoding and proposed closed-form power allocation based on max-min fairness (MMF) allow each user to detect the common multicast stream completely canceling the inter-group interference with a significantly low complexity. Simulation results demonstrate the superiority and scalability of the proposed CF-MGM for the achievable rate and increase of users in a group outperforming the existing CSIT-based methods."}
{"id": "2511.06102", "pdf": "https://arxiv.org/pdf/2511.06102", "abs": "https://arxiv.org/abs/2511.06102", "authors": ["Mohammed Abboodi"], "title": "Development and testing of novel soft sleeve actuators", "categories": ["cs.RO"], "comment": "PhD thesis", "summary": "Aging populations and the rising prevalence of neurological and musculoskeletal disorders increase the demand for wearable mobility assistive devices that are effective, comfortable, and anatomically compatible. Many existing systems use rigid mechanisms and bulky interfaces that impede force transmission and reduce wearability. This study introduces a soft sleeve actuation architecture that conforms to the limb while transmitting forces and moments efficiently. We develop three soft sleeve actuators that produce linear, bending, and twisting motion, and an omnidirectional design that combines these motions in one device. Actuators are fabricated from thermoplastic elastomers using a customized fused filament fabrication process that produces airtight and compliant structures and resolves leakage observed with conventional methods. A dedicated experimental platform quantifies kinematic outputs such as displacement, angle, and twist, and kinetic outputs such as force and torque under low pneumatic pressures. A parametric study varies geometric features and material properties to determine their influence on performance. Results show reproducible multi axis motion with improved transfer of force to the limb and reduced need for complex attachment hardware. The work establishes a unified and manufacturable framework for soft sleeve actuation that enables compact and user centered assistive technologies with enhanced kinematic and kinetic performance."}
{"id": "2511.06383", "pdf": "https://arxiv.org/pdf/2511.06383", "abs": "https://arxiv.org/abs/2511.06383", "authors": ["Khalid A. Alshumayri", "Mudassir Masood", "Ali. A. Nasir"], "title": "Near-Field Velocity Estimation and Predictive Beamforming with Modular Linear Array", "categories": ["eess.SP"], "comment": "7 pages, 6 figures", "summary": "Velocity estimation is a cornerstone of recently introduced near-field predictive beamforming. This paper derives the closed-form Cramer-Rao bounds (CRBs) for joint velocity estimation using a modular linear array (MLA) within a predictive-beamforming framework. The analysis shows that increasing inter-module separation enlarges the effective aperture and reduces the transverse-velocity CRB, whereas the radial-velocity CRB is largely insensitive to separation. We further obtain a simple closed-form relation linking the achievable antenna savings to the inter-module separation while preserving the same transverse accuracy of a uniform linear array (ULA). We further investigate how velocity mismatch affects array gain and show that transverse-velocity errors cause more severe performance degradation than radial-velocity errors. Simulations show that predictive beamforming with MLAs maintains high localization accuracy for target tracking."}
{"id": "2511.06141", "pdf": "https://arxiv.org/pdf/2511.06141", "abs": "https://arxiv.org/abs/2511.06141", "authors": ["Marc Duclusaud", "Grégoire Passault", "Vincent Padois", "Olivier Ly"], "title": "PlaCo: a QP-based robot planning and control framework", "categories": ["cs.RO"], "comment": null, "summary": "This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance."}
{"id": "2511.06395", "pdf": "https://arxiv.org/pdf/2511.06395", "abs": "https://arxiv.org/abs/2511.06395", "authors": ["Hung D. Nguyen", "Jeongseok Ha"], "title": "UAV-Assisted Downlink Satellite Covert Communication", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates the use of an unmanned aerial vehicle (UAV) to assist covert communication between a low-Earth orbit (LEO) satellite and a ground user under the surveillance of a passive warden. The UAV simultaneously serves its own ground network and acts as a friendly jammer to enhance the covertness of satellite transmissions. We derive a closed-form lower bound on the warden's average minimum detection error probability which is then used to define the covert constraint. Building on this, we formulate an optimization problem to jointly design the UAV's 3D placement, its power allocation, and the satellite's transmit power to maximize the system's covert rate. To solve the resulting non-convex problem, we propose an algorithm based on the block coordinate descent (BCD) and successive convex approximation (SCA) techniques, and further develop a Dinkelbach's algorithm for a special case. Numerical results validate the tightness of the derived bound and demonstrate the effectiveness of the proposed algorithms in configuring optimal system parameters."}
{"id": "2511.06182", "pdf": "https://arxiv.org/pdf/2511.06182", "abs": "https://arxiv.org/abs/2511.06182", "authors": ["Peican Lin", "Gan Sun", "Chenxi Liu", "Fazeng Li", "Weihong Ren", "Yang Cong"], "title": "OpenVLN: Open-world aerial Vision-Language Navigation", "categories": ["cs.RO"], "comment": "Content: 8 pages 4 figures, conference under review", "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments."}
{"id": "2511.06440", "pdf": "https://arxiv.org/pdf/2511.06440", "abs": "https://arxiv.org/abs/2511.06440", "authors": ["Yingjie Xu", "Xuesong Cai", "Ali Al-Ameri", "Sara Willhammar", "Fredrik Tufvesson"], "title": "Distributed MIMO Positioning: Fundamental Limit Analysis and User Tracking Framework Design", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper presents a comprehensive study on the 3D positioning capabilities of users in distributed multiple-input multiple-output (MIMO) systems. Unlike previous studies that mainly rely on idealized isotropic antenna models, we adopt a polarimetric model that takes advantage of effective aperture distribution functions to characterize realistic antenna patterns, placements, and polarization effects. Based on this model, we analyze the fundamental limits of UE positioning using the Fisher information matrix (FIM) and the position error bound (PEB). The FIM is shown to be expressed as a weighted sum of the information contributions from individual access point (AP)-UE pairs, with each contribution interpreted geometrically across distance, azimuth, and elevation dimensions. The impact of the UE tilt and the spatial distribution of APs on the PEBs is further analyzed. As a further advancement, we propose a complete positioning framework from a UE tracking perspective. By integrating a global probability hypothesis density filter and a PEB-aware AP management strategy, the framework enables accurate tracking while optimizing AP scheduling. Finally, we present a distributed MIMO channel measurement campaign to validate the proposed framework. The results demonstrate a centimeter-level tracking accuracy. In addition, the PEB-aware AP management strategy is shown to maintain robust tracking performance while significantly reducing the number of concurrently active APs, thus lowering the overall system overhead."}
{"id": "2511.06202", "pdf": "https://arxiv.org/pdf/2511.06202", "abs": "https://arxiv.org/abs/2511.06202", "authors": ["Shahram Najam Syed", "Yatharth Ahuja", "Arthur Jakobsson", "Jeff Ichnowski"], "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval", "categories": ["cs.RO"], "comment": "10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors", "summary": "Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment."}
{"id": "2511.06621", "pdf": "https://arxiv.org/pdf/2511.06621", "abs": "https://arxiv.org/abs/2511.06621", "authors": ["Antonina Kosikova", "Apostolos Psaros", "Andrew Smyth"], "title": "Spectrum and Physics-Informed Neural Networks (SaPINNs) for Input-State-Parameter Estimation in Dynamic Systems Subjected to Natural Hazards-Induced Excitation", "categories": ["eess.SP"], "comment": "Preprint submitted to Mechanical Systems and Signal Processing", "summary": "System identification under unknown external excitation is an inherently ill-posed problem, typically requiring additional knowledge or simplifying assumptions to enable reliable state and parameter estimation. The difficulty of the problem is further amplified in structural systems subjected to natural hazards such as earthquakes or windstorms, where responses are often highly transient, nonlinear, and spatially distributed. To address this challenge, we introduce Spectrum and Physics-Informed Neural Networks (SaPINNs) for efficient input--state--parameter estimation in systems under complex excitations characteristic of natural hazards. The proposed model enhances the neural network with governing physics of the system dynamics and incorporates spectral information of natural hazards by using empirically derived spectra as priors on the unknown excitations. This integration improves inference of unmeasured inputs, system states, and parameters without imposing restrictive assumptions on their dynamics. The performance of the proposed framework is demonstrated through comparative studies on both linear and nonlinear systems under various types of excitation, including the El Centro earthquake, where the seismic spectrum is assumed to be not precisely known. To account for predictive uncertainty, the proposed architecture is embedded within a Deep Ensemble (DEns) networks architecture, providing distributions over possible solutions. The results demonstrate that the proposed approach outperforms conventional PINNs, as the incorporation of spectral information introduces an inductive bias that guides the network more effectively through the solution space and enhances its ability to recover physically consistent state and parameter estimates with realistic uncertainty levels."}
{"id": "2511.06240", "pdf": "https://arxiv.org/pdf/2511.06240", "abs": "https://arxiv.org/abs/2511.06240", "authors": ["Tzu-Jung Lin", "Jia-Fong Yeh", "Hung-Ting Su", "Chung-Yi Lin", "Yi-Ting Chen", "Winston H. Hsu"], "title": "Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to AAAI 2026", "summary": "In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM."}
{"id": "2511.06710", "pdf": "https://arxiv.org/pdf/2511.06710", "abs": "https://arxiv.org/abs/2511.06710", "authors": ["Hao Sun", "Xianghao Yu", "Junting Chen"], "title": "Structure-Aware Near-Field Radio Map Recovery via RBF-Assisted Matrix Completion", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a novel structure-aware matrix completion framework assisted by radial basis function (RBF) interpolation for near-field radio map construction in extremely large multiple-input multiple-output (XL-MIMO) systems. Unlike the far-field scenario, near-field wavefronts exhibit strong dependencies on both angle and distance due to spherical wave propagation, leading to complicated variations in received signal strength (RSS). To effectively capture the intricate spatial variations structure inherent in near-field environments, a regularized RBF interpolation method is developed to enhance radio map reconstruction accuracy. Leveraging theoretical insights from interpolation error analysis of RBF, an inverse μ-law-inspired nonuniform sampling strategy is introduced to allocate measurements adaptively, emphasizing regions with rapid RSS variations near the transmitter. To further exploit the global low-rank structure in the near-field radio map, we integrate RBF interpolation with nuclear norm minimization (NNM)-based matrix completion. A robust Huberized leave-one-out cross-validation (LOOCV) scheme is then proposed for adaptive selection of the tolerance parameter, facilitating optimal fusion between RBF interpolation and matrix completion. The integration of local variation structure modeling via RBF interpolation and global low-rank structure exploitation via matrix completion yields a structure-aware framework that substantially improves the accuracy of near-field radio map reconstruction. Extensive simulations demonstrate that the proposed approach achieves over 10% improvement in normalized mean squared error (NMSE) compared to standard interpolation and matrix completion methods under varying sampling densities and shadowing conditions."}
{"id": "2511.06267", "pdf": "https://arxiv.org/pdf/2511.06267", "abs": "https://arxiv.org/abs/2511.06267", "authors": ["Jiayi Chen", "Wei Zhao", "Liangwang Ruan", "Baoquan Chen", "He Wang"], "title": "Robust Differentiable Collision Detection for General Objects", "categories": ["cs.RO"], "comment": null, "summary": "Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision."}
{"id": "2511.06806", "pdf": "https://arxiv.org/pdf/2511.06806", "abs": "https://arxiv.org/abs/2511.06806", "authors": ["Zhiyuan Zhai", "Wei Ni", "Xin Wang"], "title": "Learning Performance Optimization for Edge AI System with Time and Energy Constraints", "categories": ["eess.SP"], "comment": null, "summary": "Edge AI, which brings artificial intelligence to the edge of the network for real-time processing and decision-making, has emerged as a transformative technology across various applications. However, the deployment of Edge AI systems faces significant challenges due to high energy consumption and extended operation time.\n  In this paper, we consider an Edge AI system which integrates the data acquisition, computation and communication processes, and focus on improving learning performance of this system. We model the time and energy consumption of different processes and perform a rigorous convergence analysis to quantify the impact of key system parameters, such as the amount of collected data and the number of training rounds, on the learning performance. Based on this analysis, we formulate a system-wide optimization problem that seeks to maximize learning performance under given time and energy constraints. We explore both homogeneous and heterogeneous device scenarios, developing low-complexity algorithms based on one-dimensional search and alternating optimization to jointly optimize data collection time and training rounds. Simulation results validate the accuracy of our convergence analysis and demonstrate the effectiveness of the proposed algorithms, providing valuable insights into designing energy-efficient Edge AI systems under real-world conditions."}
{"id": "2511.06311", "pdf": "https://arxiv.org/pdf/2511.06311", "abs": "https://arxiv.org/abs/2511.06311", "authors": ["Seiichi Yamamoto", "Hiroki Ishizuka", "Takumi Kawasetsu", "Koh Hosoda", "Takayuki Kameoka", "Kango Yanagida", "Takato Horii", "Sei Ikeda", "Osamu Oshiro"], "title": "External Photoreflective Tactile Sensing Based on Surface Deformation Measurement", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration."}
{"id": "2511.06866", "pdf": "https://arxiv.org/pdf/2511.06866", "abs": "https://arxiv.org/abs/2511.06866", "authors": ["Ahmet Kaplan", "Diana P. M. Osorio", "Erik G. Larsson"], "title": "Joint Access Point Selection and Beamforming Design for Bistatic Backscatter Communication", "categories": ["eess.SP"], "comment": null, "summary": "Future Internet-of-Things networks are envisioned to use small and cheap sensor nodes with extremely low power consumption to avoid the extensive use of batteries. To provide connectivity to a massive number of these nodes, backscatter communication (BC) is emerging as an energy- and cost-efficient technology exploiting the reflection of radio frequency signals. However, challenges such as round-trip path loss and direct link interference (DLI) between the carrier emitter and the reader limit its performance. To tackle these limitations, this paper proposes a joint access point role selection and a novel beamforming technique for bistatic BC in a distributed multiple-input multiple- output setup. The proposed approach boosts the received backscattered energy while effectively mitigating DLI, thereby reducing the error probability. We also propose a channel estimation method tailored to operate under DLI conditions and propose a mismatch detector using estimated channel coefficients. Furthermore, we derive a closed-form expression for the probability of error for the detectors and model the quantization noise caused by DLI. Finally, comprehensive simulation results show that the proposed method with 1-bit analog-to-digital converters (ADCs) effectively mitigates DLI, reduces the quantization noise, and enhances backscattered signal energy, achieving performance comparable to the benchmark scenario with 8-bit ADCs."}
{"id": "2511.06371", "pdf": "https://arxiv.org/pdf/2511.06371", "abs": "https://arxiv.org/abs/2511.06371", "authors": ["Yingnan Zhao", "Xinmiao Wang", "Dewei Wang", "Xinzhe Liu", "Dan Lu", "Qilong Han", "Peng Liu", "Chenjia Bai"], "title": "Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io."}
{"id": "2511.06874", "pdf": "https://arxiv.org/pdf/2511.06874", "abs": "https://arxiv.org/abs/2511.06874", "authors": ["Giuseppe Baruffa", "Luca Rugini", "Francesco Binucci", "Fabrizio Frescura", "Paolo Banelli", "Renzo Perfetti"], "title": "Radio-Coverage-Aware Path Planning for Cooperative Autonomous Vehicles", "categories": ["eess.SP", "eess.SY"], "comment": "11 pages, 19 figures", "summary": "Fleets of autonomous vehicles (AV) often are at the core of intelligent transportation scenarios for smart cities, and may require a wireless Internet connection to offload computer vision tasks to data centers located either in the edge or the cloud section of the network. Cooperation among AVs is successful when the environment is unknown, or changes dynamically, so as to improve coverage and trip time, and minimize the traveled distance. The AVs, while mapping the environment with range-based sensors, move across the wireless coverage areas, with consequences on the achieved access bit rate, latency, and handover rate. In this paper, we propose to modify the cost of path planning algorithms such as Dijkstra and A*, so that not only the traveled distance is considered in the best path solution, but also the radio coverage experience. To this aim, several radio-related cost-weighting functions are introduced and tested, to assess the performance of the proposed techniques with extensive simulations. The proposed mapping algorithm can achieve a mapping error probability below 2%, while the proposed path-planning algorithms extend the experienced radio coverage of the AVs, with limited distance increase with respect to shortest-path existing methods, such as conventional Dijkstra and A* algorithms."}
{"id": "2511.06378", "pdf": "https://arxiv.org/pdf/2511.06378", "abs": "https://arxiv.org/abs/2511.06378", "authors": ["Prajval Kumar Murali", "Mohsen Kaboli"], "title": "ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects", "categories": ["cs.RO", "cs.CV"], "comment": "Under review", "summary": "Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints)."}
{"id": "2511.06922", "pdf": "https://arxiv.org/pdf/2511.06922", "abs": "https://arxiv.org/abs/2511.06922", "authors": ["Konstantinos Alexoudis", "Jasper Müller", "Sai Kireet Patri", "Vincent A. J. M. Sleiffer", "Vishal Chandraprakash Rai", "André Sandmann", "Sander Jansen", "Thomas Bradley", "Chigo Okonkwo"], "title": "Real-Time Diverse Fiber Sensing Multi-Event Detection using Phase OTDR Measurements", "categories": ["eess.SP"], "comment": "This work has been partially funded by the German Federal Ministry of Education and Research in the project HYPERCORE (#16KIS2098). We also acknowledge the Bilateral Project \"DistraSignalSense\" between the Eindhoven University of Technology, The Netherlands, and Adtran Networks SE", "summary": "We demonstrate an experimental phase optical time-domain reflectometry (OTDR) system capable of simultaneous detection and classification of various environmental events, such as wind-induced fiber movement, vehicle movement, and audio signatures, with real-time visualization."}
{"id": "2511.06385", "pdf": "https://arxiv.org/pdf/2511.06385", "abs": "https://arxiv.org/abs/2511.06385", "authors": ["Ralf Römer", "Julian Balletshofer", "Jakob Thumm", "Marco Pavone", "Angela P. Schoellig", "Matthias Althoff"], "title": "From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies", "categories": ["cs.RO", "eess.SY"], "comment": "Project page: https://tum-lsy.github.io/pacs/. 8 pages, 4 figures", "summary": "Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/."}
{"id": "2511.06971", "pdf": "https://arxiv.org/pdf/2511.06971", "abs": "https://arxiv.org/abs/2511.06971", "authors": ["Qiushi Liang", "Yeyue Cai", "Jianhua Mo", "Meixia Tao"], "title": "MARBLE-Net: Learning to Localize in Multipath Environment with Adaptive Rainbow Beams", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) systems demand precise and efficient target localization, a task challenged by rich multipath propagation in complex wireless environments. This paper introduces MARBLE-Net (Multipath-Aware Rainbow Beam Learning Network), a deep learning framework that jointly optimizes the analog beamforming parameters of a frequency-dependent rainbow beam and a neural localization network for high-accuracy position estimation. By treating the phase-shifter (PS) and true-time-delay (TTD) parameters as learnable weights, the system adaptively refines its sensing beam to exploit environment-specific multipath characteristics. A structured multi-stage training strategy is proposed to ensure stable convergence and effective end-to-end optimization. Simulation results show that MARBLE-Net outperforms both a fixed-beam deep learning baseline (RaiNet) and a traditional k-nearest neighbors (k-NN) method, reducing localization error by more than 50\\% in a multipath-rich scene. Moreover, the results reveal a nuanced interaction with multipath propagation: while confined uni-directional multipath degrades accuracy, structured and directional multipath can be effectively exploited to achieve performance surpassing even line-of-sight (LoS) conditions."}
{"id": "2511.06397", "pdf": "https://arxiv.org/pdf/2511.06397", "abs": "https://arxiv.org/abs/2511.06397", "authors": ["Cong Wen", "Yunfei Li", "Kexin Liu", "Yixin Qiu", "Xuanhong Liao", "Tianyu Wang", "Dingchuan Liu", "Tao Zhang", "Ximin Lyu"], "title": "Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments."}
{"id": "2511.07026", "pdf": "https://arxiv.org/pdf/2511.07026", "abs": "https://arxiv.org/abs/2511.07026", "authors": ["Mikhail Krasnov", "Ljupcho Milosheski", "Mihael Mohorčič", "Carolina Fortuna"], "title": "Design Principles of Zero-Shot Self-Supervised Unknown Emitter Detectors", "categories": ["eess.SP"], "comment": null, "summary": "The proliferation of wireless devices necessitates more robust and reliable emitter detection and identification for critical tasks such as spectrum management and network security. Existing studies exploring methods for unknown emitters identification, however, are typically hindered by their dependence on labeled or proprietary datasets, unrealistic assumptions (e.g. all samples with identical transmitted messages), or deficiency of systematic evaluations across different architectures and design dimensions. In this work, we present a comprehensive evaluation of unknown emitter detection systems across key aspects of the design space, focusing on data modality, learning approaches, and feature learn- ing modules. We demonstrate that prior self-supervised, zero-shot emitter detection approaches commonly use datasets with identical transmitted messages. To address this limitation, we propose a 2D- Constellation data modality for scenarios with varying messages, achieving up to a 40\\% performance improvement in ROC-AUC, NMI, and F1 metrics compared to conventional raw I/Q data. Furthermore, we introduce interpretable Kolmogorov--Arnold Net- works (KANs) to enhance model transparency, and a Singular Value Decomposition (SVD)-based initialization procedure for feature learning modules operating on sparse 2D-Constellation data, which improves the performance of Deep Clustering approaches by up to 40\\% across the same metrics comparing to the modules without SVD initialization. We evaluate all data modalities and learning modules across three learning approaches: Deep Clustering, Auto Encoder and Contrastive Learning."}
{"id": "2511.06434", "pdf": "https://arxiv.org/pdf/2511.06434", "abs": "https://arxiv.org/abs/2511.06434", "authors": ["Wenkang Hu", "Xincheng Tang", "Yanzhi E", "Yitong Li", "Zhengjie Shu", "Wei Li", "Huamin Wang", "Ruigang Yang"], "title": "Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator", "categories": ["cs.RO"], "comment": "2026 AAAI Accept", "summary": "While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/"}
{"id": "2511.07178", "pdf": "https://arxiv.org/pdf/2511.07178", "abs": "https://arxiv.org/abs/2511.07178", "authors": ["Zhiyuan Zhai", "Yuan Gao", "Wei Ni", "Xiaojun Yuan", "Xin Wang"], "title": "Trajectory Design for UAV-Assisted Logistics Collection in Low-Altitude Economy", "categories": ["eess.SP"], "comment": null, "summary": "Low-altitude economy (LAE) is rapidly emerging as a key driver of innovation, encompassing economic activities taking place in airspace below 500 meters. Unmanned aerial vehicles (UAVs) provide valuable tools for logistics collection within LAE systems, offering the ability to navigate through complex environments, avoid obstacles, and improve operational efficiency. However, logistics collection tasks involve UAVs flying through complex three-dimensional (3D) environments while avoiding obstacles, where traditional UAV trajectory design methods,typically developed under free-space conditions without explicitly accounting for obstacles, are not applicable. This paper presents, we propose a novel algorithm that combines the Lin-Kernighan-Helsgaun (LKH) and Deep Deterministic Policy Gradient (DDPG) methods to minimize the total collection time. Specifically, the LKH algorithm determines the optimal order of item collection, while the DDPG algorithm designs the flight trajectory between collection points. Simulations demonstrate that the proposed LKH-DDPG algorithm significantly reduces collection time by approximately 49 percent compared to baseline approaches, thereby highlighting its effectiveness in optimizing UAV trajectories and enhancing operational efficiency for logistics collection tasks in the LAE paradigm."}
{"id": "2511.06465", "pdf": "https://arxiv.org/pdf/2511.06465", "abs": "https://arxiv.org/abs/2511.06465", "authors": ["Lingfan Bao", "Tianhu Peng", "Chengxu Zhou"], "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion", "categories": ["cs.RO"], "comment": "Sim-to-real for bipedal locomotion chapter", "summary": "This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions."}
{"id": "2511.07310", "pdf": "https://arxiv.org/pdf/2511.07310", "abs": "https://arxiv.org/abs/2511.07310", "authors": ["Mahmoud Zaher", "Emil Björnson"], "title": "Low-Complexity ADMM-Based Multicast Beamforming in Cell-Free Massive MIMO Systems", "categories": ["eess.SP"], "comment": "13 pages, submitted to IEEE Transactions on Wireless Communications", "summary": "The growing demand for efficient delivery of common content to multiple user equipments (UEs) has motivated significant research in physical-layer multicasting. By exploiting the beamforming capabilities of massive MIMO, multicasting provides a spectrum-efficient solution that avoids unnecessary intra-group interference. A key challenge, however, is solving the max-min fair (MMF) and quality-of-service (QoS) multicast beamforming optimization problems, which are NP-hard due to the non-convex structure and the requirement for rank-1 solutions. Traditional approaches based on semidefinite relaxation (SDR) followed by randomization exhibit poor scalability with system size, while state-of-the-art successive convex approximation (SCA) methods only guarantee convergence to stationary points. In this paper, we propose an alternating direction method of multipliers (ADMM)-based framework for MMF and QoS multicast beamforming in cell-free massive MIMO networks. The algorithm leverages SDR but incorporates a novel iterative elimination strategy within the ADMM updates to efficiently obtain near-global optimal rank-1 beamforming solutions with reduced computational complexity compared to standard SDP solvers and randomization methods. Numerical evaluations demonstrate that the proposed ADMM-based procedure not only achieves superior spectral efficiency but also scales favorably with the number of antennas and UEs compared to state-of-the-art SCA-based algorithms, making it a practical tool for next-generation multicast systems."}
{"id": "2511.06496", "pdf": "https://arxiv.org/pdf/2511.06496", "abs": "https://arxiv.org/abs/2511.06496", "authors": ["Keke Long", "Jiacheng Guo", "Tianyun Zhang", "Hongkai Yu", "Xiaopeng Li"], "title": "A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications."}
{"id": "2511.07376", "pdf": "https://arxiv.org/pdf/2511.07376", "abs": "https://arxiv.org/abs/2511.07376", "authors": ["Jiewei Feng", "Ken R. Duffy", "Muriel Médard"], "title": "Enhanced GCD through ORBGRAND-AI: Exploiting Partial and Total Correlation in Noise", "categories": ["eess.SP"], "comment": null, "summary": "There have been significant advances in recent years in the development of forward error correction decoders that can decode codes of any structure, including practical realizations in synthesized circuits and taped out chips. While essentially all soft-decision decoders assume that bits have been impacted independently on the channel, for one of these new approaches it has been established that channel dependencies can be exploited to achieve superior decoding accuracy, resulting in Ordered Reliability Bits Guessing Random Additive Noise Decoding Approximate Independence (ORBGRAND-AI). Building on that capability, here we consider the integration of ORBGRAND-AI as a pattern generator for Guessing Codeword Decoding (GCD). We first establish that a direct approach delivers mildly degraded block error rate (BLER) but with reduced number of queried patterns when compared to ORBGRAND-AI. We then show that with a more nuanced approach it is possible to leverage total correlation to deliver an additional BLER improvement of around 0.75 dB while retaining reduced query numbers."}
{"id": "2511.06500", "pdf": "https://arxiv.org/pdf/2511.06500", "abs": "https://arxiv.org/abs/2511.06500", "authors": ["JiaHao Wu", "ShengWen Yu"], "title": "Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation", "categories": ["cs.RO"], "comment": "21 pages,12 tables, 6 figures", "summary": "Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \\textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\\% improvement from 12.36° to 2.42°). Critically, this work discovers the \\textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\\%, no disturbance: +16.6\\%, average: +10.0\\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems."}
{"id": "2511.05536", "pdf": "https://arxiv.org/pdf/2511.05536", "abs": "https://arxiv.org/abs/2511.05536", "authors": ["Bakytzhan Alibekov", "Alina Gutoreva", "Elisa Raffaella-Ferre"], "title": "Gravity-Awareness: Deep Learning Models and LLM Simulation of Human Awareness in Altered Gravity", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "eess.SP"], "comment": "64 pages, 8 figures, 2 datasets, 1 protocol", "summary": "Earth's gravity has fundamentally shaped human development by guiding the brain's integration of vestibular, visual, and proprioceptive inputs into an internal model of gravity: a dynamic neural representation enabling prediction and interpretation of gravitational forces. This work presents a dual computational framework to quantitatively model these adaptations. The first component is a lightweight Multi-Layer Perceptron (MLP) that predicts g-load-dependent changes in key electroencephalographic (EEG) frequency bands, representing the brain's cortical state. The second component utilizes a suite of independent Gaussian Processes (GPs) to model the body's broader physiological state, including Heart Rate Variability (HRV), Electrodermal Activity (EDA), and motor behavior. Both models were trained on data derived from a comprehensive review of parabolic flight literature, using published findings as anchor points to construct robust, continuous functions. To complement this quantitative analysis, we simulated subjective human experience under different gravitational loads, ranging from microgravity (0g) and partial gravity (Moon 0.17g, Mars 0.38g) to hypergravity associated with spacecraft launch and re-entry (1.8g), using a large language model (Claude 3.5 Sonnet). The model was prompted with physiological parameters to generate introspective narratives of alertness and self-awareness, which closely aligned with the quantitative findings from both the EEG and physiological models. This combined framework integrates quantitative physiological modeling with generative cognitive simulation, offering a novel approach to understanding and predicting human performance in altered gravity"}
{"id": "2511.06515", "pdf": "https://arxiv.org/pdf/2511.06515", "abs": "https://arxiv.org/abs/2511.06515", "authors": ["Cormac O'Neill", "Jasmine Terrones", "H. Harry Asada"], "title": "Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control", "categories": ["cs.RO", "math.DS"], "comment": null, "summary": "Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics."}
{"id": "2511.05730", "pdf": "https://arxiv.org/pdf/2511.05730", "abs": "https://arxiv.org/abs/2511.05730", "authors": ["Amin Golnari", "Jamileh Yousefi", "Reza Moheimani", "Saeid Sanei"], "title": "QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "This work introduces the quantum-inspired variational convolution (QiVC) framework, a novel learning paradigm that integrates principles of probabilistic inference, variational optimization, and quantum-inspired transformations within convolutional architectures. The central innovation of QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE performs differentiable low-dimensional subspace rotations of convolutional weights, analogously to quantum state evolution. This approach enables structured uncertainty modeling while preserving the intrinsic geometry of the parameter space, resulting in more expressive, stable, and uncertainty-aware representations. To demonstrate its practical potential, the concept is instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in the context of biosignal classification, focusing on phonocardiogram (PCG) recordings, a challenging domain characterized by high noise, inter-subject variability, and often imbalanced data. The proposed QiVC-Net integrates an architecture in which the QiVC layer does not introduce additional parameters, instead performing an ensemble rotation of the convolutional weights through a structured mechanism ensuring robustness without added highly computational burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art performance, reaching accuracies of 97.84% and 97.89%, respectively. These findings highlight the versatility of the QiVC framework and its promise for advancing uncertainty-aware modeling in real-world biomedical signal analysis. The implementation of the QiVConv layer is openly available in GitHub."}
{"id": "2511.06575", "pdf": "https://arxiv.org/pdf/2511.06575", "abs": "https://arxiv.org/abs/2511.06575", "authors": ["Jun Wang", "Yevgeniy Vorobeychik", "Yiannis Kantaros"], "title": "CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments."}
{"id": "2511.05777", "pdf": "https://arxiv.org/pdf/2511.05777", "abs": "https://arxiv.org/abs/2511.05777", "authors": ["Jianqing Jia", "Ashley Prater-Bennette", "Lixin Shen"], "title": "Signal and Image Recovery with Scale and Signed Permutation Invariant Sparsity-Promoting Functions", "categories": ["math.OC", "eess.SP"], "comment": null, "summary": "Sparse signal recovery has been a cornerstone of advancements in data processing and imaging. Recently, the squared ratio of $\\ell_1$ to $\\ell_2$ norms, $(\\ell_1/\\ell_2)^2$, has been introduced as a sparsity-prompting function, showing superior performance compared to traditional $\\ell_1$ minimization, particularly in challenging scenarios with high coherence and dynamic range. This paper explores the integration of the proximity operator of $(\\ell_1/\\ell_2)^2$ and $\\ell_1/\\ell_2$ into efficient optimization frameworks, including the Accelerated Proximal Gradient (APG) and Alternating Direction Method of Multipliers (ADMM). We rigorously analyze the convergence properties of these algorithms and demonstrate their effectiveness in compressed sensing and image restoration applications. Numerical experiments highlight the advantages of our proposed methods in terms of recovery accuracy and computational efficiency, particularly under noise and high-coherence conditions."}
{"id": "2511.06578", "pdf": "https://arxiv.org/pdf/2511.06578", "abs": "https://arxiv.org/abs/2511.06578", "authors": ["Kaustubh Singh", "Shivam Kumar", "Shashikant Pawar", "Sandeep Manjanna"], "title": "Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring", "categories": ["cs.RO", "eess.SY"], "comment": "ICRA RUNE 2024 Workshop Paper", "summary": "In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on"}
{"id": "2511.05804", "pdf": "https://arxiv.org/pdf/2511.05804", "abs": "https://arxiv.org/abs/2511.05804", "authors": ["Valentin Noël"], "title": "Catching Contamination Before Generation: Spectral Kill Switches for Agents", "categories": ["cs.LG", "eess.SP", "eess.SY", "stat.ML"], "comment": "Preprint under review (2025). 9 pages, 2 figures. Code and scripts: to be released", "summary": "Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain."}
{"id": "2511.06619", "pdf": "https://arxiv.org/pdf/2511.06619", "abs": "https://arxiv.org/abs/2511.06619", "authors": ["Chuheng Zhang", "Rushuai Yang", "Xiaoyu Chen", "Kaixin Wang", "Li Zhao", "Yi Chen", "Jiang Bian"], "title": "How Do VLAs Effectively Inherit from VLMs?", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems."}
{"id": "2511.06313", "pdf": "https://arxiv.org/pdf/2511.06313", "abs": "https://arxiv.org/abs/2511.06313", "authors": ["Stef Cuyckens", "Xiaoling Yi", "Robin Geens", "Joren Dumoulin", "Martin Wiesner", "Chao Fang", "Marian Verhelst"], "title": "Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.SP"], "comment": "To appear in the 31st Asia and South Pacific Design Automation Conference (ASP-DAC 2026, Invited Paper)", "summary": "Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS."}
{"id": "2511.06667", "pdf": "https://arxiv.org/pdf/2511.06667", "abs": "https://arxiv.org/abs/2511.06667", "authors": ["Andrew Choi", "Dezhong Tong"], "title": "Rapidly Learning Soft Robot Control via Implicit Time-Stepping", "categories": ["cs.RO", "cs.AI"], "comment": "Code: https://github.com/QuantuMope/dismech-rl", "summary": "With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy."}
{"id": "2511.06409", "pdf": "https://arxiv.org/pdf/2511.06409", "abs": "https://arxiv.org/abs/2511.06409", "authors": ["Vishal Cholapadi Ravindra"], "title": "Sensor Importance towards Observability Degree via Shapley Values", "categories": ["eess.SY", "eess.SP"], "comment": null, "summary": "Sensor selection is an often under-appreciated aspect of state estimator or Kalman filter design. The basic minimum requirement for the choice of a sensor set while designing Kalman filters is that all states are observable. In addition, the sensors should be chosen with a view towards estimating the states with a desired accuracy. Often observability is treated as true/false check during filter design. Beyond observability -- the observability degree -- which measures \\emph{how observable} the states are, has been used as the metric of choice to for sensor selection or placement applications. The higher the degree of observability, the better the possibility of designing Kalman filters that achieve the desired state estimation accuracy and consistency requirements. When a wide variety of sensors are available, sometimes with cost and physical constraints involved, sensor selection plays a crucial role in filter design. In such situations it is important to know the expected contribution of each sensor towards observability degree. Shapley values, developed in cooperative game theory for fair allocation of the payout of a multi-player game to individual players, are widely used in machine learning to assess feature importance. This paper shows that Shapley values can indeed be leveraged to quantify the expected marginal contribution of each sensor in any given sensor set towards the observability degree. This quantification of the fair contribution of each sensor towards the observability degree can be leveraged by filter designers for sensor selection, placement and filter (state estimator) design."}
{"id": "2511.06673", "pdf": "https://arxiv.org/pdf/2511.06673", "abs": "https://arxiv.org/abs/2511.06673", "authors": ["Joel Kemp", "Andre Farinha", "David Howard", "Krishna Manaswi Digumarti", "Josh Pinskier"], "title": "Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, Submitted to Robosoft 2026", "summary": "Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required."}
{"id": "2511.06424", "pdf": "https://arxiv.org/pdf/2511.06424", "abs": "https://arxiv.org/abs/2511.06424", "authors": ["Amit Vaisman", "Guy Ohayon", "Hila Manor", "Michael Elad", "Tomer Michaeli"], "title": "Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP", "stat.ML"], "comment": "Code is available at https://amitvaisman.github.io/turbo_ddcm/", "summary": "While zero-shot diffusion-based compression methods have seen significant progress in recent years, they remain notoriously slow and computationally demanding. This paper presents an efficient zero-shot diffusion-based compression method that runs substantially faster than existing methods, while maintaining performance that is on par with the state-of-the-art techniques. Our method builds upon the recently proposed Denoising Diffusion Codebook Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by sequentially choosing the diffusion noise vectors from reproducible random codebooks, guiding the denoiser's output to reconstruct the target image. We modify this framework with Turbo-DDCM, which efficiently combines a large number of noise vectors at each denoising step, thereby significantly reducing the number of required denoising operations. This modification is also coupled with an improved encoding protocol. Furthermore, we introduce two flexible variants of Turbo-DDCM, a priority-aware variant that prioritizes user-specified regions and a distortion-controlled variant that compresses an image based on a target PSNR rather than a target BPP. Comprehensive experiments position Turbo-DDCM as a compelling, practical, and flexible image compression scheme."}
{"id": "2511.06745", "pdf": "https://arxiv.org/pdf/2511.06745", "abs": "https://arxiv.org/abs/2511.06745", "authors": ["Lan Thi Ha Nguyen", "Kien Ton Manh", "Anh Do Duc", "Nam Pham Hai"], "title": "Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios."}
{"id": "2511.06493", "pdf": "https://arxiv.org/pdf/2511.06493", "abs": "https://arxiv.org/abs/2511.06493", "authors": ["Sivaram Krishnan", "Jinho Choi", "Jihong Park"], "title": "Learning Time-Varying Graph Signals via Koopman", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "A wide variety of real-world data, such as sea measurements, e.g., temperatures collected by distributed sensors and multiple unmanned aerial vehicles (UAV) trajectories, can be naturally represented as graphs, often exhibiting non-Euclidean structures. These graph representations may evolve over time, forming time-varying graphs. Effectively modeling and analyzing such dynamic graph data is critical for tasks like predicting graph evolution and reconstructing missing graph data. In this paper, we propose a framework based on the Koopman autoencoder (KAE) to handle time-varying graph data. Specifically, we assume the existence of a hidden non-linear dynamical system, where the state vector corresponds to the graph embedding of the time-varying graph signals. To capture the evolving graph structures, the graph data is first converted into a vector time series through graph embedding, representing the structural information in a finite-dimensional latent space. In this latent space, the KAE is applied to learn the underlying non-linear dynamics governing the temporal evolution of graph features, enabling both prediction and reconstruction tasks."}
{"id": "2511.06749", "pdf": "https://arxiv.org/pdf/2511.06749", "abs": "https://arxiv.org/abs/2511.06749", "authors": ["Weining Lu", "Deer Bin", "Lian Ma", "Ming Ma", "Zhihao Ma", "Xiangyang Chen", "Longfei Wang", "Yixiao Feng", "Zhouxian Jiang", "Yongliang Shi", "Bin Liang"], "title": "Semi-distributed Cross-modal Air-Ground Relative Localization", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 3 figures. Accepted by IROS 2025", "summary": "Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git."}
{"id": "2511.06510", "pdf": "https://arxiv.org/pdf/2511.06510", "abs": "https://arxiv.org/abs/2511.06510", "authors": ["Marx M. M. Freitas", "Giovanni Interdonato", "Stefano Buzzi"], "title": "Differential Space-Time Block Coding for Phase-Unsynchronized Cell-Free MIMO Downlink", "categories": ["cs.IT", "cs.ET", "eess.SP"], "comment": "Submitted to IEEE for possible publication. This manuscript builds upon and significantly extends our prior works: https://ieeexplore.ieee.org/document/11027597/ and https://ieeexplore.ieee.org/document/11143305 | © 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "In the downlink of a cell-free massive multiple-input multiple-output (CF-mMIMO) system, spectral efficiency gains critically rely on joint coherent transmission, as all access points (APs) must align their transmitted signals in phase at the user equipment (UE). Achieving such phase alignment is technically challenging, as it requires tight synchronization among geographically distributed APs. In this paper, we address this issue by introducing a differential space-time block coding (DSTBC) approach that bypasses the need for AP phase synchronization. We first provide analytic bounds to the achievable spectral efficiency of CF-mMIMO with phase-unsynchronized APs. Then, we propose a DSTBC-based transmission scheme specifically tailored to CF-mMIMO, which operates without channel state information and does not require any form of phase synchronization among the APs. We derive a closed-form expression for the resulting signal-to-interference-plus-noise ratio (SINR), enabling quantitative comparisons among different DSTBC schemes. Numerical simulations confirm that phase misalignments can significantly impair system performance. In contrast, the proposed DSTBC scheme successfully mitigates these effects, achieving performance comparable to that of fully synchronized systems."}
{"id": "2511.06754", "pdf": "https://arxiv.org/pdf/2511.06754", "abs": "https://arxiv.org/abs/2511.06754", "authors": ["Taisei Hanyu", "Nhat Chung", "Huy Le", "Toan Nguyen", "Yuki Ikebe", "Anthony Gunderman", "Duy Nguyen Ho Minh", "Khoa Vo", "Tung Kieu", "Kashu Yamazaki", "Chase Rainwater", "Anh Nguyen", "Ngan Le"], "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "under review", "summary": "Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation."}
{"id": "2511.06735", "pdf": "https://arxiv.org/pdf/2511.06735", "abs": "https://arxiv.org/abs/2511.06735", "authors": ["Raya Majid Alsharfa", "Mahmood Mohassel Feghhi", "Majid Hameed Majeed"], "title": "Wireless Sensor Networks Nodes Clustering and Optimization Based on Fuzzy C-Means and Water Strider Algorithms", "categories": ["cs.DC", "eess.SP", "math.OC"], "comment": "15 pages, 9 figures, Published in International Journal of Intelligent Engineering and Systems, 2025", "summary": "Wireless sensor networks (WSNs) face critical challenges in energy management and network lifetime optimization due to limited battery resources and communication overhead. This study introduces a novel hybrid clustering protocol that integrates the Water Strider Algorithm (WSA) with Fuzzy C-Means (FCM) clustering to achieve superior energy efficiency and network longevity. The proposed WSA-FCM method employs WSA for global optimization of cluster- head positions and FCM for refined node membership assignment with fuzzy boundaries. Through extensive experimentation across networks of 200-800 nodes with 10 independent simulation runs, the method demonstrates significant improvements: First Node Death (FND) delayed by 16.1% ($678\\pm12$ vs $584\\pm18$ rounds), Last Node Death (LND) extended by 11.9% ($1,262\\pm8$ vs $1,128\\pm11$ rounds), and 37.4% higher residual energy retention ($5.47\\pm0.09$ vs $3.98\\pm0.11$ J) compared to state-of-the-art hybrid methods. Intra-cluster distances are reduced by 19.4% with statistical significance (p < 0.001). Theoretical analysis proves convergence guarantees and complexity bounds of $O(n\\times c\\times T)$, while empirical scalability analysis demonstrates near-linear scaling behaviour. The method outperforms recent hybrid approaches including MOALO-FCM, MSSO-MST, Fuzzy+HHO, and GWO-FCM across all performance metrics with rigorous statistical validation."}
{"id": "2511.06796", "pdf": "https://arxiv.org/pdf/2511.06796", "abs": "https://arxiv.org/abs/2511.06796", "authors": ["MD-Nazmus Sunbeam"], "title": "Human-Level Actuation for Humanoids", "categories": ["cs.RO"], "comment": "61 pages, 8 figures, 7 tables, and 12 numbered equations", "summary": "Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \\emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \\emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \\emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \\emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data."}
{"id": "2511.06950", "pdf": "https://arxiv.org/pdf/2511.06950", "abs": "https://arxiv.org/abs/2511.06950", "authors": ["M. Doostmohammadian", "U. A. Khan", "N. Meskin"], "title": "On the Redundant Distributed Observability of Mixed Traffic Transportation Systems", "categories": ["eess.SY", "cs.MA", "eess.SP", "math.OC"], "comment": "EURASIP journal on advances in signal processing", "summary": "In this paper, the problem of distributed state estimation of human-driven vehicles (HDVs) by connected autonomous vehicles (CAVs) is investigated in mixed traffic transportation systems. Toward this, a distributed observable state-space model is derived, which paves the way for estimation and observability analysis of HDVs in mixed traffic scenarios. In this direction, first, we obtain the condition on the network topology to satisfy the distributed observability, i.e., the condition such that each HDV state is observable to every CAV via information-exchange over the network. It is shown that strong connectivity of the network, along with the proper design of the observer gain, is sufficient for this. A distributed observer is then designed by locally sharing estimates/observations of each CAV with its neighborhood. Second, in case there exist faulty sensors or unreliable observation data, we derive the condition for redundant distributed observability as a $q$-node/link-connected network design. This redundancy is achieved by extra information-sharing over the network and implies that a certain number of faulty sensors and unreliable links can be isolated/removed without losing the observability. Simulation results are provided to illustrate the effectiveness of the proposed approach."}
{"id": "2511.06801", "pdf": "https://arxiv.org/pdf/2511.06801", "abs": "https://arxiv.org/abs/2511.06801", "authors": ["Praveen Kumar", "Tushar Sandhan"], "title": "Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots", "categories": ["cs.RO"], "comment": "10 pages", "summary": "The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners."}
{"id": "2511.06994", "pdf": "https://arxiv.org/pdf/2511.06994", "abs": "https://arxiv.org/abs/2511.06994", "authors": ["Emil Björnson", "Murat Babek Salman"], "title": "Experimental Validation of Reflective Near-Field Beamfocusing using a b-bit RIS", "categories": ["cs.IT", "eess.SP"], "comment": "6 pages, 6 figures, Submitted for publication", "summary": "This paper presents the first experimental validation of reflective near-field beamfocusing using a reconfigurable intelligent surface (RIS). While beamfocusing has been theoretically established as a key feature of large-aperture RISs, its practical realization has remained unexplored. We derive new analytical expressions for the array gain achieved with a $b$-bit RIS in near-field line-of-sight scenarios, characterizing both the finite depth and angular width of the focal region. The theoretical results are validated through a series of measurements in an indoor office environment at 28 GHz using a one-bit 1024-element RIS. The experiments confirm that near-field beamfocusing can be dynamically achieved and accurately predicted by the proposed analytical model, despite the presence of hardware imperfections and multipath propagation. These findings demonstrate that near-field beamfocusing is a robust and practically viable feature of RIS-assisted wireless communications."}
{"id": "2511.06839", "pdf": "https://arxiv.org/pdf/2511.06839", "abs": "https://arxiv.org/abs/2511.06839", "authors": ["Selim Ahmet Iz", "Mustafa Unel"], "title": "Vision-Based System Identification of a Quadrotor", "categories": ["cs.RO", "cs.CV", "eess.SY", "math.DS"], "comment": null, "summary": "This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes."}
{"id": "2511.07109", "pdf": "https://arxiv.org/pdf/2511.07109", "abs": "https://arxiv.org/abs/2511.07109", "authors": ["Junjun Pan", "Valentin Leplat", "Michael Ng", "Nicolas Gillis"], "title": "A Provably-Correct and Robust Convex Model for Smooth Separable NMF", "categories": ["math.NA", "cs.LG", "eess.SP", "math.OC", "stat.ML"], "comment": "30 pages, 10 figures, code available from https://github.com/vleplat/ConvexSmoothSeparableNMF.git", "summary": "Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets."}
{"id": "2511.06892", "pdf": "https://arxiv.org/pdf/2511.06892", "abs": "https://arxiv.org/abs/2511.06892", "authors": ["Kailin Tong", "Selim Solmaz", "Kenan Mujkic", "Gottfried Allmer", "Bo Leng"], "title": "Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation", "categories": ["cs.RO"], "comment": "submitted to TRA 2026", "summary": "Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications."}
{"id": "2511.06919", "pdf": "https://arxiv.org/pdf/2511.06919", "abs": "https://arxiv.org/abs/2511.06919", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Integration of Visual SLAM into Consumer-Grade Automotive Localization", "categories": ["cs.RO"], "comment": "This manuscript has been submitted to the IEEE for possible publication", "summary": "Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods."}
{"id": "2511.06998", "pdf": "https://arxiv.org/pdf/2511.06998", "abs": "https://arxiv.org/abs/2511.06998", "authors": ["Jin Huang", "Yingqiang Wang", "Ying Chen"], "title": "Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics."}
{"id": "2511.07081", "pdf": "https://arxiv.org/pdf/2511.07081", "abs": "https://arxiv.org/abs/2511.07081", "authors": ["Guanghu Xie", "Mingxu Li", "Songwei Wu", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects", "categories": ["cs.RO"], "comment": null, "summary": "Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase."}
{"id": "2511.07155", "pdf": "https://arxiv.org/pdf/2511.07155", "abs": "https://arxiv.org/abs/2511.07155", "authors": ["Thomas Steinecker", "Alexander Bienemann", "Denis Trescher", "Thorsten Luettel", "Mirko Maehlisch"], "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control."}
{"id": "2511.07175", "pdf": "https://arxiv.org/pdf/2511.07175", "abs": "https://arxiv.org/abs/2511.07175", "authors": ["Marvin Rüdt", "Constantin Enke", "Kai Furmans"], "title": "Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets", "categories": ["cs.RO"], "comment": "submitted to the IEEE for possible publication; 8 pages, 6 figures, 2 tables", "summary": "Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets."}
{"id": "2511.07275", "pdf": "https://arxiv.org/pdf/2511.07275", "abs": "https://arxiv.org/abs/2511.07275", "authors": ["David Black", "Septimiu Salcudean"], "title": "Robotic versus Human Teleoperation for Remote Ultrasound", "categories": ["cs.RO", "cs.HC"], "comment": "Under review at IEEE TMRB. Extended version of a paper presented at the Hamlyn Symposium for Medical Robotics, 2025", "summary": "Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible."}
{"id": "2511.07292", "pdf": "https://arxiv.org/pdf/2511.07292", "abs": "https://arxiv.org/abs/2511.07292", "authors": ["Simon Gerstenecker", "Andreas Geiger", "Katrin Renz"], "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2."}
{"id": "2511.07375", "pdf": "https://arxiv.org/pdf/2511.07375", "abs": "https://arxiv.org/abs/2511.07375", "authors": ["Shaohang Han", "Joris Verhagen", "Jana Tumova"], "title": "Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications", "categories": ["cs.RO"], "comment": null, "summary": "We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance."}
{"id": "2511.07381", "pdf": "https://arxiv.org/pdf/2511.07381", "abs": "https://arxiv.org/abs/2511.07381", "authors": ["Yizhe Zhu", "Zhang Ye", "Boce Hu", "Haibo Zhao", "Yu Qi", "Dian Wang", "Robert Platt"], "title": "Residual Rotation Correction using Tactile Equivariance", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks."}
{"id": "2511.07407", "pdf": "https://arxiv.org/pdf/2511.07407", "abs": "https://arxiv.org/abs/2511.07407", "authors": ["Zhengjie Xu", "Ye Li", "Kwan-yee Lin", "Stella X. Yu"], "title": "Unified Humanoid Fall-Safety Policy from a Few Demonstrations", "categories": ["cs.RO"], "comment": null, "summary": "Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/."}
{"id": "2511.07410", "pdf": "https://arxiv.org/pdf/2511.07410", "abs": "https://arxiv.org/abs/2511.07410", "authors": ["Hao Wang", "Sathwik Karnik", "Bea Lim", "Somil Bansal"], "title": "Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners."}
{"id": "2511.07416", "pdf": "https://arxiv.org/pdf/2511.07416", "abs": "https://arxiv.org/abs/2511.07416", "authors": ["Jiageng Mao", "Sicheng He", "Hao-Ning Wu", "Yang You", "Shuyang Sun", "Zhicheng Wang", "Yanan Bao", "Huizhong Chen", "Leonidas Guibas", "Vitor Guizilini", "Howard Zhou", "Yue Wang"], "title": "Robot Learning from a Physical World Model", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://pointscoder.github.io/PhysWorld_Web/", "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details."}
{"id": "2511.07418", "pdf": "https://arxiv.org/pdf/2511.07418", "abs": "https://arxiv.org/abs/2511.07418", "authors": ["Zhao-Heng Yin", "Pieter Abbeel"], "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.DC", "cs.GR"], "comment": "Code: https://github.com/zhaohengyin/lightning-grasp", "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation."}
{"id": "2511.05540", "pdf": "https://arxiv.org/pdf/2511.05540", "abs": "https://arxiv.org/abs/2511.05540", "authors": ["Shiyao Sang"], "title": "Token Is All You Need: Cognitive Planning through Sparse Intent Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "6 pages, 2 figures. Preprint exploring a new cognitive paradigm for autonomous planning", "summary": "We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Unlike world-model approaches that rely on computationally intensive future scene generation or vision-language-action (VLA) systems constrained by Markov assumptions, we show that a minimal set of semantically rich tokens is sufficient for effective planning. Experiments on the nuPlan benchmark (720 scenarios, over 11,000 samples) using perception-informed BEV representations yield three key findings: (1) even without future prediction, our sparse representation achieves 0.548 m ADE, comparable to or surpassing prior methods reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over current-state baselines; and (3) explicit reconstruction loss offers no benefit and may degrade performance under reliable perception inputs. Notably, we observe the emergence of temporal fuzziness, where the model adaptively attends to task-relevant semantics rather than aligning rigidly to fixed timestamps, providing a cognitive advantage for planning under uncertainty. Our \"token is all you need\" principle marks a paradigm shift from reconstructing the world to understanding it, laying a foundation for cognitively inspired systems that plan through imagination rather than reaction."}
{"id": "2511.05604", "pdf": "https://arxiv.org/pdf/2511.05604", "abs": "https://arxiv.org/abs/2511.05604", "authors": ["Subash Gautam", "Alejandro Vargas-Uscategui", "Peter King", "Hans Lohr", "Alireza Bab-Hadiashar", "Ivan Cole", "Ehsan Asadi"], "title": "In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Additive manufacturing (AM) is an emerging digital manufacturing technology to produce complex and freeform objects through a layer-wise deposition. High deposition rate robotic AM (HDRRAM) processes, such as cold spray additive manufacturing (CSAM), offer significantly increased build speeds by delivering large volumes of material per unit time. However, maintaining shape accuracy remains a critical challenge, particularly due to process instabilities in current open-loop systems. Detecting these deviations as they occur is essential to prevent error propagation, ensure part quality, and minimize post-processing requirements. This study presents a real-time monitoring system to acquire and reconstruct the growing part and directly compares it with a near-net reference model to detect the shape deviation during the manufacturing process. The early identification of shape inconsistencies, followed by segmenting and tracking each deviation region, paves the way for timely intervention and compensation to achieve consistent part quality."}
{"id": "2511.05622", "pdf": "https://arxiv.org/pdf/2511.05622", "abs": "https://arxiv.org/abs/2511.05622", "authors": ["Nicholas Babey", "Tiffany Gu", "Yiheng Li", "Cristian Meo", "Kevin Zhu"], "title": "Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Accepted at NeurIPS 2025 SpaVLE, for code see https://github.com/nbabey20/groundactrec , 9 pages, 1 figure", "summary": "For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition."}
{"id": "2511.05683", "pdf": "https://arxiv.org/pdf/2511.05683", "abs": "https://arxiv.org/abs/2511.05683", "authors": ["Eric Godden", "Jacquie Groenewegen", "Michael Wheeler", "Matthew K. X. J. Pan"], "title": "Social-Physical Interactions with Virtual Characters: Evaluating the Impact of Physicality through Encountered-Type Haptics", "categories": ["cs.HC", "cs.RO"], "comment": "9 pages", "summary": "This work investigates how robot-mediated physicality influences the perception of social-physical interactions with virtual characters. ETHOS (Encountered-Type Haptics for On-demand Social interaction) is an encountered-type haptic display that integrates a torque-controlled manipulator and interchangeable props with a VR headset to enable three gestures: object handovers, fist bumps, and high fives. We conducted a user study to examine how ETHOS adds physicality to virtual character interactions and how this affects presence, realism, enjoyment, and connection metrics. Each participant experienced one interaction under three conditions: no physicality (NP), static physicality (SP), and dynamic physicality (DP). SP extended the purely virtual baseline (NP) by introducing tangible props for direct contact, while DP further incorporated motion and impact forces to emulate natural touch. Results show presence increased stepwise from NP to SP to DP. Realism, enjoyment, and connection also improved with added physicality, though differences between SP and DP were not significant. Comfort remained consistent across conditions, indicating no added psychological friction. These findings demonstrate the experiential value of ETHOS and motivate the integration of encountered-type haptics into socially meaningful VR experiences."}
{"id": "2511.05873", "pdf": "https://arxiv.org/pdf/2511.05873", "abs": "https://arxiv.org/abs/2511.05873", "authors": ["Tong Chen", "Xinyu Ma", "Long Bai", "Wenyang Wang", "Sun Yue", "Luping Zhou"], "title": "EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Endoscopic images often suffer from diverse and co-occurring degradations such as low lighting, smoke, and bleeding, which obscure critical clinical details. Existing restoration methods are typically task-specific and often require prior knowledge of the degradation type, limiting their robustness in real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic diffusion-based framework that restores multiple degradation types using a single model. EndoIR introduces a Dual-Domain Prompter that extracts joint spatial-frequency features, coupled with an adaptive embedding that encodes both shared and task-specific cues as conditioning for denoising. To mitigate feature confusion in conventional concatenation-based conditioning, we design a Dual-Stream Diffusion architecture that processes clean and degraded inputs separately, with a Rectified Fusion Block integrating them in a structured, degradation-aware manner. Furthermore, Noise-Aware Routing Block improves efficiency by dynamically selecting only noise-relevant features during denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR achieves state-of-the-art performance across multiple degradation scenarios while using fewer parameters than strong baselines, and downstream segmentation experiments confirm its clinical utility."}
{"id": "2511.05900", "pdf": "https://arxiv.org/pdf/2511.05900", "abs": "https://arxiv.org/abs/2511.05900", "authors": ["Ruoyu Lin", "Gennaro Notomista", "Magnus Egerstedt"], "title": "Disentangled Control of Multi-Agent Systems", "categories": ["eess.SY", "cs.RO"], "comment": "This work has been submitted to IEEE Transactions on Control of Network Systems for possible publication", "summary": "This paper develops a general framework for multi-agent control synthesis, which applies to a wide range of problems with convergence guarantees, regardless of the complexity of the underlying graph topology and the explicit time dependence of the objective function. The proposed framework systematically addresses a particularly challenging problem in multi-agent systems, i.e., decentralization of entangled dynamics among different agents, and it naturally supports multi-objective robotics and real-time implementations. To demonstrate its generality and effectiveness, the framework is implemented across three experiments, namely time-varying leader-follower formation control, decentralized coverage control for time-varying density functions without any approximations, which is a long-standing open problem, and safe formation navigation in dense environments."}
{"id": "2511.05982", "pdf": "https://arxiv.org/pdf/2511.05982", "abs": "https://arxiv.org/abs/2511.05982", "authors": ["Albert Schotschneider", "Svetlana Pavlitska", "J. Marius Zöllner"], "title": "Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "6 pages, 1 figure, 2 tables, accepted at IEEE SMC 2025 in Vienna, presented on 8th October 2025", "summary": "Deep neural networks (DNNs) are widely used in perception systems for safety-critical applications, such as autonomous driving and robotics. However, DNNs remain vulnerable to various safety concerns, including generalization errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can lead to hazardous failures. This survey provides a comprehensive overview of runtime safety monitoring approaches, which operate in parallel to DNNs during inference to detect these safety concerns without modifying the DNN itself. We categorize existing methods into three main groups: Monitoring inputs, internal representations, and outputs. We analyze the state-of-the-art for each category, identify strengths and limitations, and map methods to the safety concerns they address. In addition, we highlight open challenges and future research directions."}
{"id": "2511.06036", "pdf": "https://arxiv.org/pdf/2511.06036", "abs": "https://arxiv.org/abs/2511.06036", "authors": ["Hassan Hizeh", "Rim Chighri", "Muhammad Mahboob Ur Rahman", "Mohamed A. Bahloul", "Ali Muqaibel", "Tareq Y. Al-Naffouri"], "title": "Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook", "categories": ["eess.SP", "cs.RO"], "comment": "20 pages, 5 figures, 4 tables, under review with a journal", "summary": "The current body of research on Parkinson's disease (PD) screening, monitoring, and management has evolved along two largely independent trajectories. The first research community focuses on multimodal sensing of PD-related biomarkers using noninvasive technologies such as inertial measurement units (IMUs), force/pressure insoles, electromyography (EMG), electroencephalography (EEG), speech and acoustic analysis, and RGB/RGB-D motion capture systems. These studies emphasize data acquisition, feature extraction, and machine learning-based classification for PD screening, diagnosis, and disease progression modeling. In parallel, a second research community has concentrated on robotic intervention and rehabilitation, employing socially assistive robots (SARs), robot-assisted rehabilitation (RAR) systems, and virtual reality (VR)-integrated robotic platforms for improving motor and cognitive function, enhancing social engagement, and supporting caregivers. Despite the complementary goals of these two domains, their methodological and technological integration remains limited, with minimal data-level or decision-level coupling between the two. With the advent of advanced artificial intelligence (AI), including large language models (LLMs), agentic AI systems, a unique opportunity now exists to unify these research streams. We envision a closed-loop sensor-AI-robot framework in which multimodal sensing continuously guides the interaction between the patient, caregiver, humanoid robot (and physician) through AI agents that are powered by a multitude of AI models such as robotic and wearables foundation models, LLM-based reasoning, reinforcement learning, and continual learning. Such closed-loop system enables personalized, explainable, and context-aware intervention, forming the basis for digital twin of the PD patient that can adapt over time to deliver intelligent, patient-centered PD care."}
{"id": "2511.06084", "pdf": "https://arxiv.org/pdf/2511.06084", "abs": "https://arxiv.org/abs/2511.06084", "authors": ["Juan Augusto Paredes Salazar", "Ankit Goel"], "title": "Model-free Adaptive Output Feedback Vibration Suppression in a Cantilever Beam", "categories": ["eess.SY", "cs.RO"], "comment": "16 pages, 14 figures, to be presented at Scitech 2026", "summary": "This paper presents a model-free adaptive control approach to suppress vibrations in a cantilevered beam excited by an unknown disturbance. The cantilevered beam under harmonic excitation is modeled using a lumped parameter approach. Based on retrospective cost optimization, a sampled-data adaptive controller is developed to suppress vibrations caused by external disturbances. Both displacement and acceleration measurements are considered for feedback. Since acceleration measurements are more sensitive to spillover, which excites higher frequency modes, a filter is developed to extract key displacement information from the acceleration data and enhance suppression performance. The vibration suppression performance is compared using both displacement and acceleration measurements."}
{"id": "2511.06341", "pdf": "https://arxiv.org/pdf/2511.06341", "abs": "https://arxiv.org/abs/2511.06341", "authors": ["Nikolaus Vertovec", "Frederik Baymler Mathiesen", "Thom Badings", "Luca Laurenti", "Alessandro Abate"], "title": "Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation", "categories": ["cs.LG", "cs.RO", "eess.SY", "math.OC"], "comment": null, "summary": "Control barrier functions (CBFs) are a popular tool for safety certification of nonlinear dynamical control systems. Recently, CBFs represented as neural networks have shown great promise due to their expressiveness and applicability to a broad class of dynamics and safety constraints. However, verifying that a trained neural network is indeed a valid CBF is a computational bottleneck that limits the size of the networks that can be used. To overcome this limitation, we present a novel framework for verifying neural CBFs based on piecewise linear upper and lower bounds on the conditions required for a neural network to be a CBF. Our approach is rooted in linear bound propagation (LBP) for neural networks, which we extend to compute bounds on the gradients of the network. Combined with McCormick relaxation, we derive linear upper and lower bounds on the CBF conditions, thereby eliminating the need for computationally expensive verification procedures. Our approach applies to arbitrary control-affine systems and a broad range of nonlinear activation functions. To reduce conservatism, we develop a parallelizable refinement strategy that adaptively refines the regions over which these bounds are computed. Our approach scales to larger neural networks than state-of-the-art verification procedures for CBFs, as demonstrated by our numerical experiments."}
{"id": "2511.06471", "pdf": "https://arxiv.org/pdf/2511.06471", "abs": "https://arxiv.org/abs/2511.06471", "authors": ["Jingtao Tang", "Hang Ma"], "title": "GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted to AAAI-2026", "summary": "We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS."}
{"id": "2511.06611", "pdf": "https://arxiv.org/pdf/2511.06611", "abs": "https://arxiv.org/abs/2511.06611", "authors": ["Jiajun Jiang", "Xiao Hu", "Wancheng Liu", "Wei Jiang"], "title": "On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Circular targets are widely used in LiDAR-camera extrinsic calibration due to their geometric consistency and ease of detection. However, achieving accurate 3D-2D circular center correspondence remains challenging. Existing methods often fail due to decoupled 3D fitting and erroneous 2D ellipse-center estimation. To address this, we propose a geometrically principled framework featuring two innovations: (i) a robust 3D circle center estimator based on conformal geometric algebra and RANSAC; and (ii) a chord-length variance minimization method to recover the true 2D projected center, resolving its dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback. Evaluated on synthetic and real-world datasets, our framework significantly outperforms state-of-the-art approaches. It reduces extrinsic estimation error and enables robust calibration across diverse sensors and target types, including natural circular objects. Our code will be publicly released for reproducibility."}
{"id": "2511.06840", "pdf": "https://arxiv.org/pdf/2511.06840", "abs": "https://arxiv.org/abs/2511.06840", "authors": ["Qunchao Jin", "Yilin Wu", "Changhao Chen"], "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted as a poster in AAAI 2026", "summary": "Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics."}
{"id": "2511.06841", "pdf": "https://arxiv.org/pdf/2511.06841", "abs": "https://arxiv.org/abs/2511.06841", "authors": ["Selim Ahmet Iz", "Mustafa Unel"], "title": "Aerial Image Stitching Using IMU Data from a UAV", "categories": ["cs.CV", "cs.RO", "eess.SY", "math.DS"], "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and remote sensing applications. One of the main challenges is to stitch together multiple images into a single high-resolution image that covers a large area. Featurebased image stitching algorithms are commonly used but can suffer from errors and ambiguities in feature detection and matching. To address this, several approaches have been proposed, including using bundle adjustment techniques or direct image alignment. In this paper, we present a novel method that uses a combination of IMU data and computer vision techniques for stitching images captured by a UAV. Our method involves several steps such as estimating the displacement and rotation of the UAV between consecutive images, correcting for perspective distortion, and computing a homography matrix. We then use a standard image stitching algorithm to align and blend the images together. Our proposed method leverages the additional information provided by the IMU data, corrects for various sources of distortion, and can be easily integrated into existing UAV workflows. Our experiments demonstrate the effectiveness and robustness of our method, outperforming some of the existing feature-based image stitching algorithms in terms of accuracy and reliability, particularly in challenging scenarios such as large displacements, rotations, and variations in camera pose."}
{"id": "2511.07071", "pdf": "https://arxiv.org/pdf/2511.07071", "abs": "https://arxiv.org/abs/2511.07071", "authors": ["Marcel Müller"], "title": "Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots", "categories": ["cs.MA", "cs.RO"], "comment": "for associated repositories, see https://github.com/Nerozud/dl_reference_models and https://github.com/Nerozud/FTS_simpel", "summary": "This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions.\n  To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes.\n  Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context."}
{"id": "2511.07377", "pdf": "https://arxiv.org/pdf/2511.07377", "abs": "https://arxiv.org/abs/2511.07377", "authors": ["June Moh Goo", "Zichao Zeng", "Jan Boehm"], "title": "Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems."}
{"id": "2511.07412", "pdf": "https://arxiv.org/pdf/2511.07412", "abs": "https://arxiv.org/abs/2511.07412", "authors": ["Han Zhang", "Yiqing Shen", "Roger D. Soberanis-Mukul", "Ankita Ghosh", "Hao Ding", "Lalithkumar Seenivasan", "Jose L. Porras", "Zhekai Mao", "Chenjia Li", "Wenjie Xiao", "Lonny Yarmus", "Angela Christine Argento", "Masaru Ishii", "Mathias Unberath"], "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real."}
