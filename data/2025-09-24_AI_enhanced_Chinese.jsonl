{"id": "2509.18282", "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "PEEK\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7edf\u4e00\u7684\u57fa\u4e8e\u70b9\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u5b66\u4e60\u6ce8\u610f\u529b\u3001\u52a8\u4f5c\u9009\u62e9\u548c\u6267\u884c\u65f6\u7684\u6cdb\u5316\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u9ad8\u7ea7\u63a8\u7406\u4efb\u52a1\u5378\u8f7d\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u4f7f\u7528PEEK\u6846\u67b6\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u548c\u4efb\u52a1\u76f8\u5173\u63a9\u7801\uff0c\u751f\u6210\u7b56\u7565\u65e0\u5173\u7684\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\uff0cPEEK\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f8b\u5982\u5728\u4eff\u771f\u8bad\u7ec3\u76843D\u7b56\u7565\u4e0a\u5b9e\u73b0\u4e8641.4\u500d\u7684\u6539\u8fdb\u3002", "conclusion": "PEEK\u901a\u8fc7\u7b80\u5316\u673a\u5668\u4eba\u7b56\u7565\u7684\u5b66\u4e60\u4efb\u52a1\uff0c\u4f7f\u5176\u4e13\u6ce8\u4e8e\u6267\u884c\u52a8\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2509.18311", "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "categories": ["cs.RO"], "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "PRoP\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u901a\u8fc7\u7528\u6237\u5bc6\u94a5\u5b9e\u73b0\u7b56\u7565\u8f6c\u6362\uff0c\u4fdd\u62a4\u7528\u6237\u504f\u597d\u9690\u79c1\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u4e2a\u6027\u5316\u65f6\u53ef\u80fd\u6cc4\u9732\u7528\u6237\u504f\u597d\u6570\u636e\uff0c\u9700\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "method": "\u4e3a\u6bcf\u4e2a\u7528\u6237\u5206\u914d\u552f\u4e00\u5bc6\u94a5\uff0c\u901a\u8fc7\u6570\u5b66\u53d8\u6362\u6539\u53d8\u673a\u5668\u4eba\u7f51\u7edc\u6743\u91cd\uff0c\u4ee5\u5339\u914d\u7528\u6237\u504f\u597d\u3002", "result": "PRoP\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4fdd\u7559\u539f\u59cb\u7b56\u7565\u67b6\u6784\u548c\u884c\u4e3a\u3002", "conclusion": "PRoP\u6709\u6548\u5e73\u8861\u4e86\u673a\u5668\u4eba\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u9700\u6c42\u3002"}}
{"id": "2509.18327", "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "categories": ["cs.RO"], "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0e\u4eba\u7c7b\u53ca\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u5171\u540c\u64cd\u7eb5\u7269\u4f53\u65f6\u7684\u8fd0\u52a8\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u534f\u4f5c\u66f4\u6d41\u7545\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u673a\u5668\u4eba\u53ef\u901a\u8fc7\u6a21\u62df\u89e6\u89c9\u4fe1\u53f7\u6539\u8fdb\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u534f\u4f5c\u7684\u5dee\u5f02\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u52a9\u624b\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7IMU\u8ffd\u8e2a\u4eba\u7c7b\u4e0e\u4eba\u7c7b\u53ca\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u65f6\u7684\u7269\u4f53\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u95ee\u5377\u53cd\u9988\u5206\u6790\u3002", "result": "\u4eba\u7c7b\u534f\u4f5c\u66f4\u6d41\u7545\uff0cIMU\u6570\u636e\u63ed\u793a\u4e86\u8fd0\u52a8\u5dee\u5f02\uff0c\u4e3b\u89c2\u53cd\u9988\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u6539\u8fdb\u673a\u5668\u4eba\u89e6\u89c9\u4fe1\u53f7\u4f20\u9012\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2509.18330", "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "categories": ["cs.RO"], "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "\u5229\u7528\u706b\u661f2020\u63a2\u6d4b\u8f66\u56fe\u50cf\u53ca\u706b\u661f\u52d8\u6d4b\u8f68\u9053\u98de\u884c\u5668\u6570\u636e\u81ea\u52a8\u6784\u5efa\u5730\u5f62\u53ef\u89c6\u5316\u7f51\u683c\uff0c\u652f\u6301\u4efb\u52a1\u79d1\u5b66\u5bb6\u548c\u516c\u4f17\u4f7f\u7528\u3002", "motivation": "\u76ee\u7684\u662f\u901a\u8fc7\u878d\u54082D\u548c3D\u6570\u636e\uff0c\u4e3a\u706b\u661f\u63a2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5730\u5f62\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u652f\u6301\u4efb\u52a1\u89c4\u5212\u548c\u516c\u4f17\u79d1\u666e\u3002", "method": "\u5229\u7528\u63a2\u6d4b\u8f66\u56fe\u50cf\u548c\u8f68\u9053\u98de\u884c\u5668\u7684\u6570\u636e\uff0c\u81ea\u52a8\u6784\u5efa\u5730\u5f62\u7f51\u683c\uff0c\u96c6\u6210\u5230\u4efb\u52a1\u89c4\u5212\u5de5\u5177\u548c\u516c\u4f17\u7f51\u7ad9\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u4ea4\u4e92\u5f0f3D\u5730\u5f62\u7f51\u683c\uff0c\u5e76\u4e3a\u4efb\u52a1\u79d1\u5b66\u5bb6\u548c\u516c\u4f17\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5730\u5f62\u6570\u636e\u5904\u7406\u4e0e\u5171\u4eab\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u89c4\u5212\u548c\u79d1\u666e\u6548\u7387\u3002"}}
{"id": "2509.18310", "pdf": "https://arxiv.org/pdf/2509.18310", "abs": "https://arxiv.org/abs/2509.18310", "authors": ["Bahar Kor", "Bipin Gaikwad", "Abani Patra", "Eric L. Miller"], "title": "On Multi-entity, Multivariate Quickest Change Point Detection", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "We propose a framework for online Change Point Detection (CPD) from\nmulti-entity, multivariate time series data, motivated by applications in crowd\nmonitoring where traditional sensing methods (e.g., video surveillance) may be\ninfeasible. Our approach addresses the challenge of detecting system-wide\nbehavioral shifts in complex, dynamic environments where the number and\nbehavior of individual entities may be uncertain or evolve. We introduce the\nconcept of Individual Deviation from Normality (IDfN), computed via a\nreconstruction-error-based autoencoder trained on normal behavior. We aggregate\nthese individual deviations using mean, variance, and Kernel Density Estimates\n(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or\nabrupt changes, we apply statistical deviation metrics and the Cumulative Sum\n(CUSUM) technique to these scores. Our unsupervised approach eliminates the\nneed for labeled data or feature extraction, enabling real-time operation on\nstreaming input. Evaluations on both synthetic datasets and crowd simulations,\nexplicitly designed for anomaly detection in group behaviors, demonstrate that\nour method accurately detects significant system-level changes, offering a\nscalable and privacy-preserving solution for monitoring complex multi-agent\nsystems. In addition to this methodological contribution, we introduce new,\nchallenging multi-entity multivariate time series datasets generated from crowd\nsimulations in Unity and coupled nonlinear oscillators. To the best of our\nknowledge, there is currently no publicly available dataset of this type\ndesigned explicitly to evaluate CPD in complex collective and interactive\nsystems, highlighting an essential gap that our work addresses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u53d8\u5316\u70b9\u68c0\u6d4b\uff08CPD\uff09\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u5b9e\u4f53\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7cfb\u7edf\u7ea7\u884c\u4e3a\u53d8\u5316\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\uff08\u5982\u89c6\u9891\u76d1\u63a7\uff09\u53ef\u80fd\u4e0d\u53ef\u884c\uff0c\u9700\u5728\u4e0d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u7279\u5f81\u63d0\u53d6\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u7cfb\u7edf\u8303\u56f4\u5185\u7684\u884c\u4e3a\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u91cd\u6784\u8bef\u5dee\u7684\u81ea\u7f16\u7801\u5668\u8ba1\u7b97\u4e2a\u4f53\u504f\u79bb\u6b63\u5e38\u503c\uff08IDfN\uff09\uff0c\u805a\u5408\u4e3a\u7cfb\u7edf\u7ea7\u5f02\u5e38\u5206\u6570\uff08SWAS\uff09\uff0c\u5e76\u4f7f\u7528\u7edf\u8ba1\u504f\u5dee\u5ea6\u91cf\u548cCUSUM\u6280\u672f\u68c0\u6d4b\u53d8\u5316\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u4eba\u7fa4\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u51c6\u786e\u68c0\u6d4b\u7cfb\u7edf\u7ea7\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u586b\u8865\u4e86\u516c\u5f00\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.18342", "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "categories": ["cs.RO", "cs.CV"], "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5728\u4f3c\u7136\u4f30\u8ba1\u4e2d\u878d\u5408\u7a33\u5b9a\u7269\u4f53\u68c0\u6d4b\uff08\u5982\u8461\u8404\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\uff0c\u5e76\u7ed3\u5408\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u751f\u6210\u8bed\u4e49\u89c2\u6d4b\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5b9a\u4f4d\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\uff08\u5982\u8461\u8404\u56ed\uff09\u4e2d\uff0c\u6fc0\u5149\u96f7\u8fbe\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u91cd\u590d\u7684\u884c\u51e0\u4f55\u548c\u611f\u77e5\u6df7\u6dc6\u800c\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u7269\u4f53\u7ea7\u68c0\u6d4b\uff08\u5982\u8461\u8404\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u6295\u5f71\u5230\u4fef\u89c6\u56fe\u5e76\u4e0e\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u878d\u5408\uff0c\u540c\u65f6\u5f15\u5165\u8bed\u4e49\u5899\u548c\u566a\u58f0GPS\u5148\u9a8c\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u771f\u5b9e\u8461\u8404\u56ed\u4e2d\u51c6\u786e\u4fdd\u6301\u5728\u6b63\u786e\u884c\u5185\u5b9a\u4f4d\uff0c\u4ece\u504f\u5dee\u4e2d\u6062\u590d\uff0c\u5e76\u4f18\u4e8e\u89c6\u89c9SLAM\u65b9\u6cd5\uff08\u5982RTAB-Map\uff09\u3002", "conclusion": "\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\u7ed3\u5408\u7269\u4f53\u68c0\u6d4b\u548c\u6fc0\u5149\u96f7\u8fbe\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8461\u8404\u56ed\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18381", "pdf": "https://arxiv.org/pdf/2509.18381", "abs": "https://arxiv.org/abs/2509.18381", "authors": ["Nicholas L. K. Goradia", "Harpreet S. Dhillon", "R. Michael Buehrer"], "title": "Multi-Target Detection for Cognitive MIMO Radar Networks", "categories": ["eess.SP"], "comment": "12 pages, 16 figures", "summary": "In this work, we develop centralized and decentralized signal fusion\ntechniques for constant false alarm rate (CFAR) multi-target detection with a\ncognitive radar network in unknown noise and clutter distributions. Further, we\nfirst develop a detection statistic for co-located monostatic MIMO radar in\nunknown noise and clutter distributions which is asymptotically CFAR as the\nnumber of received pulses over all antennas grows large, and we provide\nconditions under which this detection statistic is valid. We leverage\nreinforcement learning (RL) for improved multi-target detection performance,\nwhere the radar learns likely target locations in a search area. These results\nare then generalized to the setting of cognitive radar networks, where radars\ncollaborate to learn where targets are likely to appear in a search area. We\nshow a fundamental tradeoff between the spatial and temporal domain for CFAR\ndetection in unknown noise and clutter distributions; in other words, we show a\ntradeoff between the number of radar antennas and the number of temporal\nsamples. We show the benefits and tradeoffs with centralized and decentralized\ndetection with a network of cognitive radars.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u4fe1\u53f7\u878d\u5408\u6280\u672f\uff0c\u7528\u4e8e\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u7684CFAR\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5171\u7f6e\u5355\u57fa\u5730MIMO\u96f7\u8fbe\u7684\u68c0\u6d4b\u7edf\u8ba1\u91cf\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u591a\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63a8\u5e7f\u5230\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u4e2d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u7684\u591a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u96f7\u8fbe\u7f51\u7edc\u4e2d\u7684\u534f\u4f5c\u68c0\u6d4b\u6027\u80fd\u5e73\u8861\u3002", "method": "\u5f00\u53d1\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u4fe1\u53f7\u878d\u5408\u6280\u672f\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u68c0\u6d4b\u7edf\u8ba1\u91cf\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff1b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u76ee\u6807\u68c0\u6d4b\uff1b\u63a8\u5e7f\u5230\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u4e2d\u3002", "result": "\u8bc1\u660e\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5c55\u793a\u4e86\u8ba4\u77e5\u96f7\u8fbe\u7f51\u7edc\u4e2d\u96c6\u4e2d\u5f0f\u548c\u5206\u6563\u5f0f\u68c0\u6d4b\u7684\u4f18\u52bf\u4e0e\u6743\u8861\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u7406\u8bba\u4e3a\u672a\u77e5\u566a\u58f0\u548c\u6742\u6ce2\u5206\u5e03\u4e0b\u7684CFAR\u591a\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u96f7\u8fbe\u7f51\u7edc\u534f\u4f5c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.18384", "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "categories": ["cs.RO", "cs.FL"], "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLAD-VF\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u81ea\u52a8\u4f18\u5316\u63d0\u793a\uff0c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347LLM\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u7684\u89c4\u8303\u9075\u4ece\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u89c4\u5212\u5728\u7269\u7406\u4e16\u754c\u4e2d\u5e38\u56e0\u5e7b\u89c9\u6216\u5f31\u5bf9\u9f50\u800c\u8fdd\u53cd\u5b89\u5168\u548c\u76d1\u7ba1\u7ea6\u675f\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLAD-VF\u6846\u67b6\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u4e0eLLM-AutoDiff\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u800c\u975e\u6a21\u578b\u53c2\u6570\u5b9e\u73b0\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLAD-VF\u5c06\u89c4\u8303\u9075\u4ece\u6027\u4ece60%\u63d0\u5347\u81f390%\u4ee5\u4e0a\u3002", "conclusion": "LAD-VF\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u7684LLM\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u3002"}}
{"id": "2509.18426", "pdf": "https://arxiv.org/pdf/2509.18426", "abs": "https://arxiv.org/abs/2509.18426", "authors": ["Ziad Hatab", "Michael Ernst Gadringer", "Arash Arsanjani", "Wolfgang Boesch"], "title": "Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration", "categories": ["eess.SP", "physics.ins-det"], "comment": "https://github.com/ZiadHatab/srm-calibration", "summary": "This paper addresses the modeling of parasitics of the match standard in the\nsymmetric-reciprocal-match (SRM) calibration method of vector network analyzers\n(VNAs). In the general SRM procedure, the match standard is assumed to be fully\nknown. Here, we demonstrate that the match can be modeled with an arbitrary\nfrequency-dependent model using a non-linear global optimization procedure. To\nhighlight the validity of the suggested approach, numerical tests were\nconducted, demonstrating the ability to recover the match standard parasitic\nmodel down to software numerical precision. Additionally, we performed\nmicrostrip line measurements to compare the SRM calibration with match modeling\nto the multiline thru-reflect-line (TRL) calibration one, showing that\nautomatic model extraction can achieve accuracy similar to using a match\nstandard defined through multiline TRL calibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u77e2\u91cf\u7f51\u7edc\u5206\u6790\u4eea\u5bf9\u79f0\u4e92\u6613\u5339\u914d\u6821\u51c6\u4e2d\u5bc4\u751f\u53c2\u6570\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u5168\u5c40\u4f18\u5316\u5b9e\u73b0\u9891\u7387\u76f8\u5173\u6a21\u578b\u3002\u6570\u503c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "motivation": "\u7814\u7a76\u5bf9\u79f0\u4e92\u6613\u5339\u914d\u6821\u51c6\u4e2d\u5bc4\u751f\u53c2\u6570\u5efa\u6a21\u95ee\u9898\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u5339\u914d\u6807\u51c6\u5b8c\u5168\u5df2\u77e5\u7684\u5047\u8bbe\u9650\u5236\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u5168\u5c40\u4f18\u5316\u7a0b\u5e8f\u5bf9\u5339\u914d\u6807\u51c6\u8fdb\u884c\u4efb\u610f\u9891\u7387\u76f8\u5173\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6d4b\u8bd5\u548c\u5fae\u5e26\u7ebf\u6d4b\u91cf\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u6d4b\u8bd5\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u7cbe\u786e\u6062\u590d\u5bc4\u751f\u6a21\u578b\uff1b\u5b9e\u6d4b\u6570\u636e\u8868\u660e\u5176\u7cbe\u5ea6\u4e0e\u4f20\u7edfTRL\u6821\u51c6\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u4f20\u7edfTRL\u6821\u51c6\u76f8\u5f53\uff0c\u4e3a\u5bc4\u751f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18407", "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8ePOMDP\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u7684\u901a\u884c\u6743\u51b3\u7b56\uff0c\u901a\u8fc7\u6a21\u62df\u6d4b\u8bd5\u8bc1\u660e\u6982\u7387\u89c4\u5212\u5668\u4f18\u4e8e\u89c4\u5219\u57fa\u7ebf\u3002", "motivation": "\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u56e0\u901a\u884c\u89c4\u5219\u6a21\u7cca\u3001\u906e\u6321\u548c\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u5bfc\u81f4\u4e8b\u6545\u9891\u53d1\uff0c\u4e9f\u9700\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u652f\u6301\u3002", "method": "\u6846\u67b6\u4ee5POMDP\u5efa\u6a21\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u56db\u79cd\u65b9\u6cd5\uff1a\u786e\u5b9a\u6027FSM\u53ca\u4e09\u79cd\u6982\u7387\u89c4\u5212\u5668\uff08QMDP\u3001POMCP\u3001DESPOT\uff09\u3002", "result": "\u6982\u7387\u89c4\u5212\u5668\u8868\u73b0\u4f18\u4e8e\u89c4\u5219\u57fa\u7ebf\uff0cPOMCP\u5b89\u5168\u6027\u6700\u4f73\uff0cDESPOT\u5728\u6548\u7387\u4e0e\u53ef\u884c\u6027\u95f4\u5e73\u8861\uff0c\u6700\u9ad8\u5b9e\u73b097.5%\u65e0\u78b0\u649e\u5bfc\u822a\u3002", "conclusion": "\u5f3a\u8c03\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u672a\u6765\u6574\u5408\u4f20\u611f\u5668\u878d\u5408\u4e0e\u73af\u5883\u611f\u77e5\u6a21\u5757\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2509.18555", "pdf": "https://arxiv.org/pdf/2509.18555", "abs": "https://arxiv.org/abs/2509.18555", "authors": ["Ping Wang", "Zulin Wang", "Yuanfang Ma", "Xiaosi Tian", "Yuanhan Ni"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "categories": ["eess.SP"], "comment": "6 pages, 5 figures, 2025 IEEE International Conference on\n  Communications", "summary": "This paper introduces a secure affine frequency division multiplexing\n(SE-AFDM) for wireless communication systems to enhance communication security.\nBesides configuring the parameter c1 to obtain communication reliability under\ndoubly selective channels, we also utilize the time-varying parameter c2 to\nimprove the security of the communications system. The derived input-output\nrelation shows that the legitimate receiver can eliminate the nonlinear impact\nintroduced by the time-varying c2 without losing the bit error rate (BER)\nperformance. Moreover, it is theoretically proved that the eavesdropper cannot\nseparate the time-varying c2 and random information symbols, such that the BER\nperformance of the eavesdropper is severely deteriorated. Meanwhile, the\nanalysis of the effective signal-to-interference-plus-noise ratio (SINR) of the\neavesdropper illustrates that the SINR decreases as the value range of c2\nexpands. Numerical results verify that the proposed SE-AFDM waveform has\nsignificant security while maintaining good BER performance in high-mobility\nscenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u7684\u4eff\u5c04\u9891\u5206\u590d\u7528\uff08SE-AFDM\uff09\u6280\u672f\uff0c\u7528\u4e8e\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002\u901a\u8fc7\u914d\u7f6e\u53c2\u6570c1\u548c\u65f6\u53d8\u53c2\u6570c2\uff0c\u65e2\u4fdd\u8bc1\u4e86\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u53c8\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u7a83\u542c\u8005\u65e0\u6cd5\u5206\u79bb\u65f6\u53d8\u7684c2\u548c\u968f\u673a\u4fe1\u606f\u7b26\u53f7\uff0c\u5bfc\u81f4\u5176\u8bef\u7801\u7387\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86SE-AFDM\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u65e2\u6709\u826f\u597d\u7684\u8bef\u7801\u7387\u6027\u80fd\uff0c\u53c8\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u9762\u4e34\u5b89\u5168\u6027\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5728\u4e0d\u727a\u7272\u53ef\u9760\u6027\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u901a\u4fe1\u5b89\u5168\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cdSE-AFDM\u6280\u672f\uff0c\u7ed3\u5408\u53c2\u6570c1\uff08\u786e\u4fdd\u53ef\u9760\u6027\uff09\u548c\u65f6\u53d8\u53c2\u6570c2\uff08\u589e\u5f3a\u5b89\u5168\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5408\u6cd5\u63a5\u6536\u8005\u53ef\u4ee5\u6d88\u9664c2\u7684\u975e\u7ebf\u6027\u5f71\u54cd\uff0c\u800c\u7a83\u542c\u8005\u56e0\u65e0\u6cd5\u5206\u79bbc2\u548c\u4fe1\u606f\u7b26\u53f7\uff0c\u8bef\u7801\u7387\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7a83\u542c\u8005\u7684\u4fe1\u5e72\u566a\u6bd4\uff08SINR\uff09\u968fc2\u503c\u8303\u56f4\u7684\u6269\u5927\u800c\u964d\u4f4e\u3002", "conclusion": "SE-AFDM\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u65e2\u80fd\u4fdd\u6301\u4f4e\u8bef\u7801\u7387\uff0c\u53c8\u80fd\u663e\u8457\u63d0\u5347\u901a\u4fe1\u5b89\u5168\u6027\uff0c\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18428", "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6LAWM\uff0c\u901a\u8fc7\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u9884\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8de8\u4efb\u52a1\u3001\u73af\u5883\u548c\u5b9e\u4f53\u8fc1\u79fb\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u867d\u7136\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u5927\u4e14\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u66f4\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86LAWM\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\uff08\u5982\u673a\u5668\u4eba\u8bb0\u5f55\u6216\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\uff09\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u573a\u666f\u4e2d\uff0cLAWM\u4f18\u4e8e\u57fa\u4e8e\u771f\u5b9e\u673a\u5668\u4eba\u52a8\u4f5c\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u5176\u4ed6\u7c7b\u4f3c\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u66f4\u9ad8\u6548\u5b9e\u7528\u3002", "conclusion": "LAWM\u4e3a\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u5347\u4e86\u8de8\u4efb\u52a1\u548c\u573a\u666f\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.18727", "pdf": "https://arxiv.org/pdf/2509.18727", "abs": "https://arxiv.org/abs/2509.18727", "authors": ["Yasaman Ettefagh", "Sharief Saleh", "Musa Furkan Keskin", "Hui Chen", "Gonzalo Seco-Granados", "Henk Wymeersch"], "title": "Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates the localization, synchronization, and speed\nestimation of a mobile user equipment (UE) leveraging integrated terrestrial\nand non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)\nsatellites. We focus on a minimal setup in which the UE received signal from\nonly one base station (BS) and one LEO satellite. We derive a generic signal\nmodel accounting for mobility, clock and frequency offsets, based on which a\nhierarchy of simplified models are proposed and organized by computational\ncomplexity. Estimation algorithms are developed for each model to facilitate\nefficient and accurate parameter recovery. Rigorous simulations validate the\neffectiveness of the proposed models, demonstrating their suitability across\ndiverse scenarios. The findings highlight how the trade-off between complexity\nand performance can be optimized for varying deployment environments and\napplication requirements, offering valuable insights for 6G positioning and\nsynchronization systems under user mobility.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6574\u5408\u5730\u9762\u4e0e\u975e\u5730\u9762\u7f51\u7edc\uff08\u5982\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u79fb\u52a8\u7528\u6237\u8bbe\u5907\u8fdb\u884c\u5b9a\u4f4d\u3001\u540c\u6b65\u548c\u901f\u5ea6\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4ec5\u4ece\u4e00\u4e2a\u57fa\u7ad9\u548c\u4e00\u9897LEO\u536b\u661f\u63a5\u6536\u4fe1\u53f7\u7684\u7b80\u5355\u573a\u666f\u3002", "motivation": "\u968f\u77406G\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6574\u5408\u5730\u9762\u4e0e\u975e\u5730\u9762\u7f51\u7edc\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u4e0e\u540c\u6b65\u6027\u80fd\u4f18\u5316\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u8003\u8651\u79fb\u52a8\u6027\u3001\u65f6\u949f\u548c\u9891\u7387\u504f\u79fb\u7684\u901a\u7528\u4fe1\u53f7\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e0d\u540c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7b80\u5316\u6a21\u578b\u548c\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u590d\u6742\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u53ef\u4ee5\u9488\u5bf9\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u548c\u5e94\u7528\u9700\u6c42\u8fdb\u884c\u4f18\u5316\uff0c\u4e3a6G\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u4e0e\u540c\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.18447", "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "categories": ["cs.RO", "cs.AI"], "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "PrioriTouch\u662f\u4e00\u4e2a\u7528\u4e8e\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u6267\u884c\u591a\u63a5\u89e6\u63a7\u5236\u76ee\u6807\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u62a4\u7406\u548c\u591a\u63a5\u89e6\u573a\u666f\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u6392\u5e8f\u548c\u5206\u5c42\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u9002\u5e94\u4e2a\u4f53\u63a5\u89e6\u504f\u597d\uff08\u5982\u529b\u548c\u4f4d\u7f6e\uff09\u662f\u4e00\u4e2a\u590d\u6742\u95ee\u9898\uff0c\u5c24\u5176\u5728\u591a\u63a5\u89e6\u573a\u666f\u4e2d\u56e0\u51b2\u7a81\u9700\u6c42\u800c\u96be\u4ee5\u89e3\u51b3\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u548c\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff0c\u5229\u7528\u4eff\u771f\u6570\u636e\u9ad8\u6548\u4e14\u5b89\u5168\u5730\u63a2\u7d22\u504f\u597d\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u3001\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cPrioriTouch\u80fd\u591f\u9002\u5e94\u7528\u6237\u504f\u597d\u3001\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u5e76\u63d0\u5347\u5b89\u5168\u6027\u548c\u8212\u9002\u5ea6\u3002", "conclusion": "PrioriTouch\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u591a\u63a5\u89e6\u4ea4\u4e92\u4e2d\u7684\u51b2\u7a81\u9700\u6c42\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18753", "pdf": "https://arxiv.org/pdf/2509.18753", "abs": "https://arxiv.org/abs/2509.18753", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong", "Kaibin Huang"], "title": "Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors", "categories": ["eess.SP"], "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, their unique physical\ncharacteristics enable two fundamental signal readout schemes: intensity-based\ndetection and splitting-based detection. The former measures the electric\nfields through laser intensity, while the latter utilizes Autler-Townes\nsplitting. In this work, we systematically categorize and model existing signal\nreadout methods, classifying them into these two paradigms. Then, we derive the\nmaximum likelihood estimation procedures and corresponding Cram\\'er-Rao lower\nbounds (CRLB) for each detection modality. Through the analysis of the CRLB, we\npropose strategy for both readout schemes to enhance sensitivity and minimize\nestimation variance: acquiring data in regions with maximal slope magnitudes.\nWhile this approach has been implemented in intensity-based detection (e.g.,\nsuperheterodyne schemes), its application to splitting-based detection remains\nunexplored. Implementation of non-uniform frequency scanning, with preferential\nsampling at regions exhibiting maximum peak slopes combined with our proposed\nmaximum likelihood splitting estimation method, achieves significantly reduced\nestimation variance compared to conventional polynomial fitting. The\ncomparative analysis reveals the optimal detection performance of the two\ndetection schemes. This work also contributes to enhancing the accuracy of\nmicrowave calibration. Numerical results reveal that both fundamental signal\nreadout methods achieve lower estimation variance based on our proposed maximum\nlikelihood estimation approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5730\u5206\u7c7b\u548c\u5efa\u6a21\u4e86Rydberg\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u7684\u4e24\u79cd\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6848\uff08\u5f3a\u5ea6\u68c0\u6d4b\u548c\u5206\u88c2\u68c0\u6d4b\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f30\u8ba1\u65b9\u5dee\u3002", "motivation": "Rydberg\u539f\u5b50\u91cf\u5b50\u63a5\u6536\u5668\u56e0\u5176\u9ad8\u7075\u654f\u5ea6\u5728\u591a\u9891\u6bb5\u901a\u4fe1\u63a5\u6536\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u72ec\u7279\u7684\u7269\u7406\u7279\u6027\u9700\u8981\u66f4\u6df1\u5165\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u4f18\u5316\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u548cCram\u00e9r-Rao\u4e0b\u754c\uff08CRLB\uff09\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6848\u7684\u4f18\u5316\u7b56\u7565\uff1a\u5728\u659c\u7387\u6700\u5927\u533a\u57df\u91c7\u96c6\u6570\u636e\u3002", "result": "\u975e\u5747\u5300\u9891\u7387\u626b\u63cf\u7ed3\u5408\u6700\u5927\u4f3c\u7136\u5206\u88c2\u4f30\u8ba1\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f30\u8ba1\u65b9\u5dee\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u591a\u9879\u5f0f\u62df\u5408\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u4f18\u5316\u4e86\u4e24\u79cd\u4fe1\u53f7\u8bfb\u51fa\u65b9\u6848\u7684\u6027\u80fd\uff0c\u8fd8\u63d0\u5347\u4e86\u5fae\u6ce2\u6821\u51c6\u7684\u51c6\u786e\u6027\uff0c\u4e3aRydberg\u63a5\u6536\u5668\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.18455", "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "categories": ["cs.RO"], "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGD2P\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7075\u5de7\u624b\u8fdb\u884c\u975e\u6293\u53d6\u64cd\u4f5c\uff08\u5982\u63a8\u62c9\uff09\uff0c\u901a\u8fc7\u591a\u6837\u5316\u624b\u90e8\u59ff\u6001\u751f\u6210\u548c\u7269\u7406\u6a21\u62df\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7269\u4f53\u64cd\u4f5c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u4e0d\u540c\u624b\u5f62\u6001\u7684\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u975e\u6293\u53d6\u64cd\u4f5c\u591a\u4f9d\u8d56\u5e73\u884c\u5939\u722a\u6216\u7b80\u5355\u5de5\u5177\uff0c\u7075\u5de7\u624b\u867d\u63a5\u89e6\u6a21\u5f0f\u66f4\u4e30\u5bcc\u4f46\u96be\u4ee5\u5efa\u6a21\u52a8\u6001\u8fc7\u7a0b\u3002GD2P\u65e8\u5728\u5229\u7528\u7075\u5de7\u624b\u7684\u7075\u6d3b\u6027\u63d0\u5347\u975e\u6293\u53d6\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u624b\u90e8\u59ff\u6001\uff0c\u5229\u7528\u7269\u7406\u6a21\u62df\u7b5b\u9009\u5e76\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9884\u6d4b\u53ef\u884c\u59ff\u6001\u3002\u6d4b\u8bd5\u65f6\u7ed3\u5408\u8fd0\u52a8\u89c4\u5212\u6267\u884c\u63a8\u62c9\u52a8\u4f5c\u3002", "result": "840\u6b21\u771f\u5b9e\u5b9e\u9a8c\u8868\u660eGD2P\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u624b\u5f62\u6001\uff08\u5982Allegro Hand\u548cLEAP Hand\uff09\u3002", "conclusion": "GD2P\u4e3a\u7075\u5de7\u975e\u6293\u53d6\u64cd\u4f5c\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u5f00\u6e90\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u8fdb\u4e00\u6b65\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2509.18799", "pdf": "https://arxiv.org/pdf/2509.18799", "abs": "https://arxiv.org/abs/2509.18799", "authors": ["Sijia Cheng", "Liang Liu", "Ove Edfors", "Juan Vidal Alegria"], "title": "Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing", "categories": ["eess.SP"], "comment": "5 pages, 6 figures, accepted to SiPS2025", "summary": "Singular value decomposition (SVD) is widely used in wireless systems,\nincluding multiple-input multiple-output (MIMO) processing and dimension\nreduction in distributed MIMO (D-MIMO). However, the iterative nature of\ndecomposition methods results in increased execution time as system size grows,\nposing challenges for real-time and low-latency applications. To address this,\nwe analyze the latency of state-of-art SVD methods, and highlight the\nefficiency of a 4-step highly parallel method based on Gram matrix\ntridiagonalization. Furthermore, we develop a time complexity (processing\nlatency) analysis framework with hardware profiling, allowing scalable and\nrealistic evaluation without full implementation. The numerical results\ndemonstrate the superior time efficiency of the selected parallel method,\nparticularly in massive MIMO scenarios.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86SVD\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u8ba1\u7b97\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u8bc4\u6d4b\u9a8c\u8bc1\u5176\u5728\u5927\u89c4\u6a21MIMO\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u7684SVD\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7cfb\u7edf\u65f6\u5ef6\u8fdf\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u548c\u4f4e\u5ef6\u8fdf\u5e94\u7528\u7684\u9700\u6c42\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGram\u77e9\u9635\u4e09\u5bf9\u89d2\u5316\u76844\u6b65\u9ad8\u6548\u5e76\u884c\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u786c\u4ef6\u8bc4\u6d4b\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u6790\u6846\u67b6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21MIMO\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u7684\u65f6\u95f4\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "\u8be5\u5e76\u884c\u65b9\u6cd5\u4e3aSVD\u5728\u5b9e\u65f6\u548c\u4f4e\u5ef6\u8fdf\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18460", "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\uff08FDI\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5197\u4f59\u800c\u975e\u7269\u7406\u5197\u4f59\u6765\u63d0\u5347\u53ef\u9760\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u63a2\u7d22\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u611f\u77e5\u7cfb\u7edf\u7684\u6545\u969c\u5bf9\u73af\u5883\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u5176\u590d\u6742\u6027\u548c\u591a\u9636\u6bb5\u7279\u6027\uff0c\u96be\u4ee5\u68c0\u6d4b\u548c\u9694\u79bb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c\u533a\u5206\u88ab\u52a8\u548c\u4e3b\u52a8FDI\u65b9\u6cd5\u3002\u4e3b\u52a8FDI\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u548c\u4e0a\u7f6e\u4fe1\u754c\uff08UCB\uff09\u4f18\u5316\u63a7\u5236\u8f93\u5165\uff0c\u6700\u5927\u5316\u6709\u6548\u4fe1\u606f\uff08EI\uff09\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u9694\u79bb\u4f20\u611f\u5668\u635f\u574f\u3001\u52a8\u6001\u573a\u666f\u548c\u611f\u77e5\u9000\u5316\u7b49\u6545\u969c\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u7ed3\u5408\u4e3b\u52a8FDI\u65b9\u6cd5\u4e3a\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18853", "pdf": "https://arxiv.org/pdf/2509.18853", "abs": "https://arxiv.org/abs/2509.18853", "authors": ["Xiaolei Li", "Pengyu Wang", "Wenhua Song", "Yangjin Xu", "Wei Gao"], "title": "Normal mode parameters estimation by a VLA in single-shooting", "categories": ["eess.SP", "physics.ao-ph"], "comment": null, "summary": "This paper proposes an orthogonality-constrained modal search (OCMS) method\nfor estimating modal wavenumbers and modal depth functions using a vertical\nlinear array (VLA). Under the assumption of a known sound speed profile, OCMS\nleverages the orthogonality of distinct modal depth functions to extract both\nthe modal depth functions and their corresponding wavenumbers, even when the\nVLA and a monochromatic sound source remain stationary.The performance of OCMS\nis evaluated through numerical simulations under varying signal-to-noise ratios\n(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and\nsound speed profile (SSP) uncertainty. The results demonstrate that OCMS is\nrobust against noise, VLA aperture variations, and changes in the number of VLA\nelements, meanwhile, the algorithm maintains reliable performance when SSP\nuncertainty < 1 m/s and VLA tilt angle <5{\\deg}. Furthermore, the effectiveness\nof OCMS is validated using SwellEx96 experimental data. The relative error\nbetween the modal wavenumbers derived from experimental data and those computed\nvia Kraken is on the order of $10^{-4}$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u6027\u7ea6\u675f\u7684\u6a21\u6001\u641c\u7d22\u65b9\u6cd5\uff08OCMS\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u5782\u76f4\u7ebf\u6027\u9635\u5217\uff08VLA\uff09\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u53ca\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u3002\u8be5\u65b9\u6cd5\u5728\u5df2\u77e5\u58f0\u901f\u5256\u9762\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u5df2\u77e5\u58f0\u901f\u5256\u9762\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u4e14\u9c81\u68d2\u5730\u4f30\u8ba1\u6a21\u6001\u6ce2\u6570\u548c\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\uff0c\u540c\u65f6\u89e3\u51b3VLA\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u6311\u6218\u3002", "method": "\u5229\u7528\u6a21\u6001\u6df1\u5ea6\u51fd\u6570\u7684\u6b63\u4ea4\u6027\u7ea6\u675f\uff0c\u63d0\u51faOCMS\u65b9\u6cd5\uff0c\u901a\u8fc7VLA\u63d0\u53d6\u6a21\u6001\u53c2\u6570\uff0c\u5e76\u5728\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "OCMS\u5bf9\u566a\u58f0\u3001VLA\u5b54\u5f84\u53d8\u5316\u53ca\u5143\u7d20\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5728SSP\u4e0d\u786e\u5b9a\u5ea6\u5c0f\u4e8e1 m/s\u4e14VLA\u503e\u89d2\u5c0f\u4e8e5\u5ea6\u65f6\u8868\u73b0\u53ef\u9760\uff1b\u5b9e\u9a8c\u6570\u636e\u4e0e\u7406\u8bba\u8ba1\u7b97\u7ed3\u679c\u8bef\u5dee\u4e3a10^-4\u91cf\u7ea7\u3002", "conclusion": "OCMS\u65b9\u6cd5\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u5747\u53ef\u6709\u6548\u4f30\u8ba1\u6a21\u6001\u53c2\u6570\uff0c\u4e3a\u6d77\u6d0b\u58f0\u5b66\u4e2d\u7684\u6a21\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2509.18463", "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6545\u610f\u7a81\u53d8\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u6765\u4ea7\u751f\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\uff0c\u5e76\u4ee5\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u4e3a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u7a81\u53d8\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u6280\u80fd\uff0c\u4e3a\u65b0\u4efb\u52a1\u63d0\u4f9b\u6f5c\u5728\u7684\u6709\u7528\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u566a\u58f0\u7684\u5956\u52b1\u51fd\u6570\u7a81\u53d8\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b\u51c6\u786e\u6027\u3001\u65f6\u95f4\u548c\u52aa\u529b\u7b49\u5173\u952e\u9879\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4f7f\u7528PPO\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7a81\u53d8\u5956\u52b1\u51fd\u6570\u6743\u91cd\u4ea7\u751f\u7684\u7b56\u7565\u5c55\u793a\u4e86\u4ece\u539f\u59cb\u503e\u5012\u4efb\u52a1\u5230\u65b0\u6280\u80fd\uff08\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05\u6d01\u3001\u6db2\u4f53\u6df7\u5408\u548c\u6d47\u6c34\uff09\u7684\u591a\u6837\u5316\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u591a\u6837\u5316\u5b66\u4e60\u4ee5\u53ca\u672a\u6765\u4efb\u52a1\u4e2d\u6709\u610f\u4e49\u6280\u80fd\u7684\u884d\u751f\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.18918", "pdf": "https://arxiv.org/pdf/2509.18918", "abs": "https://arxiv.org/abs/2509.18918", "authors": ["Hamideh-Sadat Fazael-Ardekani", "Hadi Zayyani", "Hamid Soltanian-Zadeh"], "title": "Quaternion LMS for Graph Signal Recovery", "categories": ["eess.SP"], "comment": null, "summary": "This letter generalizes the Graph Signal Recovery (GSR) problem in Graph\nSignal Processing (GSP) to the Quaternion domain. It extends the Quaternion\nLeast Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)\nalgorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).\nThe basic adaptation formula using Quaternion-based algebra is derived.\nMoreover, mean convergence analysis and mean-square convergence analysis are\nmathematically performed. Hence, a sufficient condition on the step-size\nparameter of QGLMS is suggested. Also, simulation results demonstrate the\neffectiveness of the proposed algorithm in graph signal reconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQGLMS\u7684\u65b0\u7b97\u6cd5\uff0c\u5c06\u56fe\u4fe1\u53f7\u6062\u590d\u95ee\u9898\u6269\u5c55\u5230\u4e86\u56db\u5143\u6570\u9886\u57df\uff0c\u5e76\u8fdb\u884c\u4e86\u6536\u655b\u6027\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4fe1\u53f7\u6062\u590d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u56db\u5143\u6570\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6269\u5c55QLMS\u548cGLMS\u7b97\u6cd5\u6765\u5f00\u53d1\u65b0\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u56db\u5143\u6570\u4ee3\u6570\u7684QGLMS\u7b97\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u9002\u5e94\u516c\u5f0f\uff0c\u540c\u65f6\u8fdb\u884c\u4e86\u5747\u503c\u548c\u5747\u65b9\u6536\u655b\u5206\u6790\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u4e86QGLMS\u7b97\u6cd5\u5728\u56fe\u4fe1\u53f7\u91cd\u5efa\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u6b65\u957f\u53c2\u6570\u7684\u5145\u5206\u6761\u4ef6\u3002", "conclusion": "QGLMS\u7b97\u6cd5\u5728\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u56db\u5143\u6570\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.18466", "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "categories": ["cs.RO"], "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cdRL\u589e\u5f3a\u7684MPC\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u7ed3\u5408\u4e86MPC\u548cRL\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MPC\u5728\u5efa\u6a21\u5730\u5f62\u4ea4\u4e92\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53caRL\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u5956\u52b1\u8bbe\u8ba1\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316MPC\u7684\u5173\u952e\u7ec4\u4ef6\uff08\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u52a8\u817f\u63a7\u5236\u5668\u548c\u6b65\u9891\uff09\uff0c\u5e76\u7ed3\u5408RL\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e0a\u7684\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6bd4\u57fa\u7ebfMPC\u548cRL\u66f4\u5177\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "RL\u4e0eMPC\u7684\u7ed3\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2509.19056", "pdf": "https://arxiv.org/pdf/2509.19056", "abs": "https://arxiv.org/abs/2509.19056", "authors": ["Razieh Torkamani", "Arash Amini", "Hadi Zayyani", "Mehdi Korki"], "title": "Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery", "categories": ["eess.SP"], "comment": null, "summary": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u56fe\u4fe1\u53f7\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u56fe\u4fe1\u53f7\u6062\u590d\u3002", "motivation": "\u56fe\u4fe1\u53f7\u6062\u590d\u7684\u6838\u5fc3\u6311\u6218\u662f\u4fe1\u53f7\u7edf\u8ba1\u6a21\u578b\u5f80\u5f80\u672a\u77e5\u6216\u590d\u6742\u96be\u63cf\u8ff0\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u590d\u6742\u7684\u975e\u9ad8\u65af\u4fe1\u53f7\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u7684\u56fe\u611f\u77e5\u6ee4\u6ce2\u5668\u6784\u5efa\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08BCNN\uff09\uff0c\u5c06\u9690\u85cf\u5c42\u89e3\u91ca\u4e3a\u5409\u5e03\u65af\u5206\u5e03\u5e76\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u975e\u7ebf\u6027\uff0c\u5f62\u6210\u95ed\u5408\u5f62\u5f0f\u7684\u5148\u9a8c\u8868\u8fbe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBCNN-GSR\u7b97\u6cd5\u5728\u4e0d\u540c\u4fe1\u53f7\u5206\u5e03\u4e0b\u5747\u80fd\u5b9e\u73b0\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u6062\u590d\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u975e\u9ad8\u65af\u4fe1\u53f7\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u9645\u56fe\u6062\u590d\u4efb\u52a1\u3002"}}
{"id": "2509.18506", "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u5305\u7edc\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u5728\u65e0\u9884\u5b9a\u4e49\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u9a7e\u9a76\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53c2\u8003\u7684\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\u5728\u8f66\u8f86\u52a8\u6001\u6781\u9650\u4e0b\u8868\u73b0\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u9884\u5b9a\u4e49\u53c2\u8003\u7684\u5b9e\u65f6\u89c4\u5212\u4e0e\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u9ad8\u6548\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u3001\u53ef\u5fae\u6570\u5b66\u516c\u5f0f\u53ca\u5f3a\u5316\u5b66\u4e60\u4e0e\u4f18\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u52a8\u6001\u53ef\u884c\u6027\u4e0e\u5b89\u5168\u7ea6\u675f\u7684\u76f4\u63a5\u6574\u5408\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u5728\u8d5b\u8f66\u3001\u7d27\u6025\u907f\u969c\u548c\u8d8a\u91ce\u5bfc\u822a\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2509.19092", "pdf": "https://arxiv.org/pdf/2509.19092", "abs": "https://arxiv.org/abs/2509.19092", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems", "categories": ["eess.SP"], "comment": "Submitted for possible publication", "summary": "Multimodal sensing reduces beam training overhead but is constrained by\nmachine learning complexity and dataset demands. To address this, we propose a\ndata-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided\nmmWave beam tracking, i.e., predicting the best current and future beams.\nSpecifically, we propose a knowledge inversion framework, where a generator\nsynthesizes LiDAR input data from random noise, guided by a loss function\ndefined on the features and outputs of a pre-trained teacher model. The student\nmodel is then trained using the synthetic data and knowledge distilled from the\nteacher. The generator loss combines three terms, called metadata loss,\nactivation loss, and entropy loss. For student training, in addition to the\nstandard Kullback-Leibler divergence loss, we also consider a mean-squared\nerror (MSE) loss between the teacher and student logits. Simulation results\nshow that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and\nTop-5 accuracies. Moreover, we observe that the metadata loss contributes\nsignificantly to the generator performance, and that the MSE loss for the\nstudent can effectively replace the standard KD loss while requiring fewer\nfine-tuned hyperparameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6570\u636e\uff08DF\uff09\u7684\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u4f18\u5316LiDAR\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2\u6ce2\u675f\u8ddf\u8e2a\uff0c\u51cf\u5c11\u4e86\u673a\u5668\u5b66\u4e60\u590d\u6742\u6027\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u591a\u6a21\u6001\u611f\u77e5\u867d\u7136\u80fd\u51cf\u5c11\u6ce2\u675f\u8bad\u7ec3\u7684\u5f00\u9500\uff0c\u4f46\u53d7\u9650\u4e8e\u673a\u5668\u5b66\u4e60\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u9700\u6c42\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u65e0\u9700\u771f\u5b9e\u6570\u636e\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u53cd\u8f6c\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668\u4ece\u968f\u673a\u566a\u58f0\u5408\u6210LiDAR\u8f93\u5165\u6570\u636e\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u548c\u7279\u5f81\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u3002\u5b66\u751f\u6a21\u578b\u5219\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DF-KD\u6846\u67b6\u5728Top-1\u548cTop-5\u51c6\u786e\u7387\u4e0a\u7565\u4f18\u4e8e\u6559\u5e08\u6a21\u578b\uff0c\u4e14\u5143\u6570\u636e\u635f\u5931\u5bf9\u751f\u6210\u5668\u6027\u80fd\u8d21\u732e\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u6570\u636e\u4f9d\u8d56\u6027\uff0c\u8fd8\u4f18\u5316\u4e86\u5b66\u751f\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86\u5728\u6ce2\u675f\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18576", "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8f7b\u91cf\u7ea7LCMF\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5f02\u6784\u6570\u636e\u878d\u5408\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5f02\u6784\u6570\u636e\u878d\u5408\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u667a\u80fd\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51faLCMF\u6846\u67b6\uff0c\u7ed3\u5408Cross-Attention\u548c\u9009\u62e9\u6027\u53c2\u6570\u5171\u4eabSSMs\uff0c\u5b9e\u73b0\u5f02\u6784\u6a21\u6001\u7684\u9ad8\u6548\u878d\u5408\u4e0e\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728VQA\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8fbe74.29%\uff0cEQA\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e2d\u6e38\u6027\u80fd\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c114.35\u500d\uff0c\u53c2\u6570\u89c4\u6a21\u8f7b\u91cf\uff08166.51M-219M\uff09\u3002", "conclusion": "LCMF\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5f3a\u5927\u7684\u591a\u6a21\u6001\u51b3\u7b56\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.19119", "pdf": "https://arxiv.org/pdf/2509.19119", "abs": "https://arxiv.org/abs/2509.19119", "authors": ["Palatip Jopanya", "Diana P. M. Osorio"], "title": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC", "categories": ["eess.SP"], "comment": "5 pages, 2 figures", "summary": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u90e8\u7f72\u4e2d\u7ee7\u5668\u7fa4\u7ec4\u5b9e\u73b0\u6210\u672c\u9ad8\u6548\u7684\u8702\u7a9d\u7f51\u7edc\u5bc6\u96c6\u5316\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728MIMO ISAC\u7cfb\u7edf\u4e2d\u589e\u5f3a\u96f7\u8fbe\u63a2\u6d4b\u80fd\u529b\u7684\u6548\u679c\u3002", "motivation": "\u968f\u7740ISAC\u7684\u65b0\u67b6\u6784\u3001\u7528\u4f8b\u548c\u6807\u51c6\u7684\u51fa\u73b0\uff0c\u8702\u7a9d\u7cfb\u7edf\u901a\u8fc7\u65b0\u578b\u7f51\u7edc\u7ec4\u4ef6\u7684\u96c6\u6210\u652f\u6301\u65b0\u5174\u7528\u4f8b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e2d\u7ee7\u5668\u7fa4\u7ec4\u5728\u589e\u5f3a\u7cfb\u7edf\u611f\u77e5\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u4e2d\u7ee7\u5668\u5373\u65f6\u91cd\u4f20\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u7814\u7a76\u5176\u5728MIMO ISAC\u7cfb\u7edf\u4e2d\u5982\u4f55\u4f18\u5316\u4e2d\u7ee7\u5668\u589e\u76ca\u4ee5\u63d0\u5347\u65e0\u4eba\u673a\u63a2\u6d4b\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8db3\u591f\u5927\u7684\u6700\u5927\u589e\u76ca\u4e0b\uff0c\u589e\u52a0\u4e2d\u7ee7\u5668\u6570\u91cf\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7684\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4e2d\u7ee7\u5668\u589e\u76ca\u548c\u6570\u91cf\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347MIMO ISAC\u7cfb\u7edf\u7684\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u5bc6\u96c6\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18592", "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "VLN-Zero\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a2\u7d22\u9636\u6bb5\u9ad8\u6548\u6784\u5efa\u7b26\u53f7\u573a\u666f\u56fe\uff0c\u5e76\u5728\u90e8\u7f72\u9636\u6bb5\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u548c\u7f13\u5b58\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u9002\u5e94\u548c\u9ad8\u6548\u5bfc\u822a\u7684\u65b0\u6846\u67b6\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a\u63a2\u7d22\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7b26\u53f7\u573a\u666f\u56fe\uff1b\u90e8\u7f72\u9636\u6bb5\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u548c\u7f13\u5b58\u6267\u884c\u6a21\u5757\u751f\u6210\u53ef\u6267\u884c\u8ba1\u5212\u3002", "result": "VLN-Zero\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u53472\u500d\uff0c\u65f6\u95f4\u51cf\u534a\uff0c\u4e14\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u51cf\u5c1155%\u3002", "conclusion": "VLN-Zero\u901a\u8fc7\u7ed3\u5408\u5feb\u901f\u63a2\u7d22\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7f13\u5b58\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u672a\u77e5\u73af\u5883\u3002"}}
{"id": "2509.19130", "pdf": "https://arxiv.org/pdf/2509.19130", "abs": "https://arxiv.org/abs/2509.19130", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Deep Reinforcement Learning for Dynamic Sensing and Communications", "categories": ["eess.SP"], "comment": "Under review for possible publication", "summary": "Environmental sensing can significantly enhance mmWave communications by\nassisting beam training, yet its benefits must be balanced against the\nassociated sensing costs. To this end, we propose a unified machine learning\nframework that dynamically determines when to sense and leverages sensory data\nfor beam prediction. Specifically, we formulate a joint sensing and beamforming\nproblem that maximizes the av- erage signal-to-noise ratio under an average\nsensing budget. Lyapunov optimization is employed to enforce the sensing\nconstraint, while a deep Q-Network determines the sensing slots. A pretrained\ndeep neural network then maps the sens- ing data to optimal beams in the\ncodebook. Simulations based on the real-world DeepSense dataset demonstrate\nthat the pro- posed approach substantially reduces sensing overhead while\nmaintaining satisfactory communications performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u73af\u5883\u611f\u77e5\uff0c\u5e76\u5229\u7528\u611f\u77e5\u6570\u636e\u8fdb\u884c\u6ce2\u675f\u9884\u6d4b\uff0c\u4ee5\u51cf\u5c11\u611f\u77e5\u5f00\u9500\u5e76\u4fdd\u6301\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u73af\u5883\u611f\u77e5\u8f85\u52a9\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6ce2\u675f\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5728\u611f\u77e5\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u8054\u5408\u611f\u77e5\u548c\u6ce2\u675f\u6210\u5f62\u95ee\u9898\uff0c\u4f7f\u7528Lyapunov\u4f18\u5316\u5f3a\u5236\u611f\u77e5\u7ea6\u675f\uff0c\u6df1\u5ea6Q\u7f51\u7edc\u51b3\u5b9a\u611f\u77e5\u65f6\u673a\uff0c\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6620\u5c04\u611f\u77e5\u6570\u636e\u5230\u6700\u4f18\u6ce2\u675f\u3002", "result": "\u57fa\u4e8eDeepSense\u6570\u636e\u96c6\u7684\u4eff\u771f\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u611f\u77e5\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6ee1\u610f\u7684\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u611f\u77e5\u6210\u672c\u548c\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u52a8\u6001\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.18597", "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "categories": ["cs.RO"], "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u5916\u90e8\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u65f6\u4efb\u52a1\u548c\u901a\u7528\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5b66\u4e60\u53ef\u91cd\u7528\u6280\u80fd\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u6846\u67b6\u91c7\u7528\u4eba\u7c7b\u53c2\u4e0e\u5faa\u73af\u8bbe\u8ba1\uff0c\u5c06\u6821\u6b63\u4fe1\u606f\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5b9e\u73b0\u52a8\u6001\u590d\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u8fbe0.93\uff0c\u6821\u6b63\u6548\u7387\u63d0\u534742%\uff0c\u5e76\u80fd\u89e3\u51b320\u4e2a\u539f\u59cb\u4efb\u52a1\u7684\u957f\u65f6\u4efb\u52a1\u3002", "conclusion": "\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u53ef\u91cd\u7528\u6280\u80fd\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\u548c\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2509.19235", "pdf": "https://arxiv.org/pdf/2509.19235", "abs": "https://arxiv.org/abs/2509.19235", "authors": ["Wamberto J. L. Queiroz", "Hugerles S. Silva", "Higo T. P. Silva", "Alexandros-Apostolos A. Boulogeorgos"], "title": "On the Performance of THz Wireless Systems over $\u03b1$-$\\mathcal{F}$ Channels with Beam Misalignment and Mobility", "categories": ["eess.SP", "math.ST", "stat.TH"], "comment": null, "summary": "This paper investigates the performance of terahertz~(THz) wireless systems\nover the $\\alpha$-$\\mathcal{F}$ fading channels with beam misalignment and\nmobility. New expressions are derived for the probability density, cumulative\ndistribution, and moment generating functions, as well as higher-order moments\nof the instantaneous signal-to-noise ratio. Building upon the aforementioned\nexpressions, we extract novel formulas for the outage probability, symbol error\nprobability, and average channel capacity. Asymptotic metrics are also deduced,\nwhich provide useful insights. Monte Carlo simulations results are presented to\nsupport the derived analytical framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u03b1-F\u8870\u843d\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u6ce2\u675f\u672a\u5bf9\u51c6\u548c\u79fb\u52a8\u6027\u7684\u5f71\u54cd\uff0c\u63a8\u5bfc\u4e86\u65b0\u7684\u7edf\u8ba1\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6027\u80fd\u6307\u6807\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u590d\u6742\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5c24\u5176\u662f\u6ce2\u675f\u672a\u5bf9\u51c6\u548c\u79fb\u52a8\u6027\u5bf9\u901a\u4fe1\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u63a8\u5bfc\u4e86\u4fe1\u53f7\u566a\u58f0\u6bd4\u7684\u6982\u7387\u5bc6\u5ea6\u3001\u7d2f\u79ef\u5206\u5e03\u53ca\u77e9\u751f\u6210\u51fd\u6570\u7b49\u65b0\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u8868\u8fbe\u5f0f\u8ba1\u7b97\u4e86\u4e2d\u65ad\u6982\u7387\u3001\u7b26\u53f7\u9519\u8bef\u6982\u7387\u548c\u5e73\u5747\u4fe1\u9053\u5bb9\u91cf\u7b49\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u6e10\u8fd1\u6027\u80fd\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u4e86\u5206\u6790\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u592a\u8d6b\u5179\u65e0\u7ebf\u7cfb\u7edf\u5728\u03b1-F\u8870\u843d\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5bf9\u5b9e\u9645\u7cfb\u7edf\u8bbe\u8ba1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2509.18608", "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc73D LiDAR\u6570\u636e\u76f4\u63a5\u751f\u6210\u63a7\u5236\u547d\u4ee4\uff0c\u89e3\u51b3\u4e86GNSS\u4e0d\u53ef\u9760\u3001\u590d\u6742\u73af\u5883\u548c\u5149\u7167\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u51a0\u5c42\u4e0b\u519c\u4e1a\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5982GNSS\u4e0d\u53ef\u9760\u3001\u5bc6\u96c6\u884c\u548c\u53ef\u53d8\u5149\u7167\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u5c06\u539f\u59cb3D LiDAR\u6570\u636e\u76f4\u63a5\u6620\u5c04\u5230\u63a7\u5236\u547d\u4ee4\uff0c\u91c7\u7528\u57fa\u4e8e\u4f53\u7d20\u7684\u4e0b\u91c7\u6837\u7b56\u7565\u51cf\u5c11\u8f93\u5165\u6570\u636e\u91cf\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0c\u76f4\u7ebf\u884c\u79cd\u690d\u7684\u6210\u529f\u7387\u4e3a100%\uff0c\u4f46\u968f\u7740\u884c\u5f2f\u66f2\u5ea6\u7684\u589e\u52a0\uff0c\u6027\u80fd\u9010\u6e10\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u8bb0\u6570\u636e\u96c6\u6216\u624b\u52a8\u8bbe\u8ba1\u63a7\u5236\u63a5\u53e3\uff0c\u5c55\u793a\u51fa\u5728\u590d\u6742\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6f5c\u529b\u3002"}}
{"id": "2509.19272", "pdf": "https://arxiv.org/pdf/2509.19272", "abs": "https://arxiv.org/abs/2509.19272", "authors": ["Sathwik Chadaga"], "title": "Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity", "categories": ["eess.SP"], "comment": null, "summary": "Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme\nthat violates the Nyquist zero-ISI criterion providing higher throughput and\nbetter spectral efficiency than a Nyquist transmission scheme. In this thesis,\nthe inter symbol interference (ISI) introduced by FTN signalling is studied,\nand conditions on pulse shapes and $\\tau$ (time acceleration factor) are\nderived so that the ISI can be avoided completely. Further, these conditions\nare reinforced by investigating the theoretical limits on the capacities of FTN\nsystems. Finally, the use of power allocation and adaptive loading techniques\nare explored in reducing the effect of ISI and increasing the throughput of\northogonal frequency division multiplexing (OFDM) FTN systems. The\nimplementation of these techniques and simulation results are also\ndemonstrated.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86FTN\u4fe1\u53f7\u4e2d\u7684ISI\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u907f\u514dISI\u7684\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5bb9\u91cf\u754c\u9650\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728OFDM FTN\u7cfb\u7edf\u4e2d\u964d\u4f4eISI\u548c\u63d0\u9ad8\u541e\u5410\u91cf\u7684\u5e94\u7528\u3002", "motivation": "FTN\u4fe1\u53f7\u4f20\u8f93\u65b9\u6848\u56e0\u5176\u975e\u6b63\u4ea4\u6027\u53ef\u4ee5\u63d0\u4f9b\u6bd4Nyquist\u65b9\u6848\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\uff0c\u4f46\u5176\u5e26\u6765\u7684ISI\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u7814\u7a76\u4e86FTN\u4fe1\u53f7\u4e2dISI\u7684\u6761\u4ef6\uff0c\u63a8\u5bfc\u4e86\u907f\u514dISI\u7684\u8109\u51b2\u5f62\u72b6\u548c\u65f6\u95f4\u52a0\u901f\u56e0\u5b50\u8981\u6c42\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5bb9\u91cf\u5206\u6790\u9a8c\u8bc1\u3002\u540c\u65f6\u63a2\u7d22\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728OFDM FTN\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u907f\u514dISI\u7684\u5177\u4f53\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5c55\u793a\u4e86\u529f\u7387\u5206\u914d\u548c\u81ea\u9002\u5e94\u52a0\u8f7d\u6280\u672f\u5728\u51cf\u5c11ISI\u548c\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "FTN\u4fe1\u53f7\u4f20\u8f93\u65b9\u6848\u5728\u5408\u7406\u8bbe\u8ba1\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6709\u6548\u907f\u514dISI\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.18609", "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "PIE\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u610f\u56fe\u5efa\u6a21\u4f18\u5316\u81ea\u4e3b\u9a7e\u9a76\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7b80\u5316\u590d\u6742\u81ea\u4e3b\u9a7e\u9a76\u6d41\u7a0b\uff0c\u514b\u670d\u573a\u666f\u7406\u89e3\u548c\u51b3\u7b56\u9884\u6d4b\u7b49\u6311\u6218\u3002", "method": "\u96c6\u6210\u53cc\u5411Mamba\u878d\u5408\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u538b\u7f29\u635f\u5931\uff0c\u63a8\u7406\u589e\u5f3a\u89e3\u7801\u5668\u4f18\u5316\u8f68\u8ff9\u63a8\u65ad\uff0c\u52a8\u4f5c-\u8fd0\u52a8\u4ea4\u4e92\u6a21\u5757\u63d0\u5347\u89c4\u5212\u3002", "result": "NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2dPDM\u548cEPDM\u5206\u6570\u5206\u522b\u4e3a88.9\u548c85.6\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "PIE\u80fd\u53ef\u9760\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.19275", "pdf": "https://arxiv.org/pdf/2509.19275", "abs": "https://arxiv.org/abs/2509.19275", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Xinwen Chen", "Xiaoying Zhang", "Bo Ai"], "title": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling", "categories": ["eess.SP"], "comment": null, "summary": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u7279\u5b9a\u7ad9\u70b9\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\uff0c\u901a\u8fc7\u4e9a6GHz\u4fe1\u9053\u6d4b\u91cf\u53c2\u6570\u5316\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4ea4\u901a\u548c\u667a\u6167\u57ce\u5e02\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57ce\u5e02\u5ce1\u8c37\u6210\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7684\u91cd\u8981\u573a\u666f\u3002\u7531\u4e8e\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u672a\u5145\u5206\u6355\u6349\u5176\u72ec\u7279\u7684\u4fe1\u9053\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u51e0\u4f55\u7684\u4fe1\u9053\u63a8\u65ad\u6a21\u578b\uff0c\u901a\u8fc7\u4e9a6GHz\u4fe1\u9053\u6d4b\u91cf\u53c2\u6570\u5316\uff0c\u591a\u5f84\u5206\u91cf\uff08MPCs\uff09\u6839\u636e\u51e0\u4f55\u4f20\u64ad\u63d0\u53d6\u548c\u805a\u7c7b\uff0c\u5efa\u7acb\u7269\u7406\u73af\u5883\u4e0eMPCs\u7edf\u8ba1\u7279\u6027\u7684\u53ef\u89e3\u91ca\u6620\u5c04\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u548c\u5b9e\u6d4b\u4fe1\u9053\u7684\u4e8c\u9636\u7edf\u8ba1\u7279\u6027\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57ce\u5e02\u5ce1\u8c37\u573a\u666f\u7684\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.18610", "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "categories": ["cs.RO"], "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "SINGER\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u6a21\u62df\u5668\u548c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u548c\u76ee\u6807\u5bf9\u8c61\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u7684\u7a00\u7f3a\u3001\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u548c\u7f3a\u4e4f\u53ef\u9760\u7684\u5916\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6cfc\u6e85\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\uff0cRRT\u4e13\u5bb6\u751f\u6210\u591a\u8f68\u8ff9\u6f14\u793a\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "result": "SINGER\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u7a81\u51fa\uff0c\u8fbe\u5230\u76ee\u6807\u7684\u6b21\u6570\u6bd4\u57fa\u7ebf\u9ad823.33%\uff0c\u89c6\u91ce\u4fdd\u6301\u7387\u63d0\u9ad816.67%\uff0c\u78b0\u649e\u51cf\u5c1110%\u3002", "conclusion": "SINGER\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u62df\u5668\u548c\u7b56\u7565\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u3002"}}
{"id": "2509.19281", "pdf": "https://arxiv.org/pdf/2509.19281", "abs": "https://arxiv.org/abs/2509.19281", "authors": ["Xiyang Lan", "Xin Li"], "title": "STFT-AECNN: An Attention-Enhanced CNN for Efficient \u03a6-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing", "categories": ["eess.SP"], "comment": null, "summary": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) has emerged\nas a promising sensing technology in Internet of Things (IoT) infrastructures,\nenabling large-scale distributed acoustic sensing (DAS) for smart city\nsurveillance, industrial pipeline monitoring, and critical infrastructure\nprotection. However, accurately recognizing events from massive {\\Phi}-OTDR\ndata streams remains challenging, as existing deep learning methods either\ndisrupt the inherent spatiotemporal structure of signals or incur prohibitive\ncomputational costs, limiting their applicability in resource-constrained IoT\nscenarios. To overcome these challenges, we propose a novel STFT-based\nAttention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents\nmulti-channel time-series data as stacked spectrograms to fully exploit their\nspatiotemporal characteristics while enabling efficient 2D CNN processing. A\nSpatial Efficient Attention Module (SEAM) is further introduced to adaptively\nemphasize the most informative channels, and a joint Cross-Entropy and Triplet\nloss is adopted to enhance the discriminability of the learned feature space.\nExtensive experiments on the public BJTU {\\Phi}-OTDR dataset demonstrate that\nSTFT-AECNN achieves a peak accuracy of 99.94% while maintaining high\ncomputational efficiency. These results highlight its potential for real-time,\nscalable, and robust event recognition in IoT-enabled DAS systems, paving the\nway for reliable and intelligent IoT sensing applications.", "AI": {"tldr": "\u63d0\u51faSTFT-AECNN\u6a21\u578b\uff0c\u7ed3\u5408STFT\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u03a6-OTDR\u6570\u636e\u7684\u4e8b\u4ef6\u8bc6\u522b\uff0c\u51c6\u786e\u7387\u8fbe99.94%\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u03a6-OTDR\u6570\u636e\u4e2d\u7834\u574f\u65f6\u7a7a\u7ed3\u6784\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684IoT\u573a\u666f\u3002", "method": "\u4f7f\u7528STFT\u5c06\u591a\u901a\u9053\u65f6\u5e8f\u6570\u636e\u8f6c\u4e3a\u9891\u8c31\u56fe\uff0c\u5f15\u5165SEAM\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8054\u5408\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3a\u7279\u5f81\u5224\u522b\u6027\u3002", "result": "\u5728BJTU \u03a6-OTDR\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.94%\u51c6\u786e\u7387\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "STFT-AECNN\u9002\u7528\u4e8eIoT-DAS\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4e8b\u4ef6\u8bc6\u522b\u3002"}}
{"id": "2509.18626", "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e8b\u6545\u62a5\u544a\u4e0e\u5e38\u89c4\u9a7e\u9a76\u6570\u636e\u7edf\u4e00\u8868\u793a\u4e3a\u573a\u666f-\u52a8\u4f5c\u5f62\u5f0f\uff0c\u5e76\u7ed3\u5408\u68c0\u7d22\u4e0e\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u65e0\u4e8b\u6545\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u5904\u7406\u5b89\u5168\u8fb9\u754c\u60c5\u51b5\u3002\u4e8b\u6545\u62a5\u544a\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u53cd\u4e8b\u5b9e\u8bc1\u636e\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u7ed3\u6784\u5316\u548c\u7b2c\u4e09\u4eba\u79f0\u7279\u70b9\uff0c\u96be\u4ee5\u76f4\u63a5\u5229\u7528\u3002", "method": "\u5c06\u4e8b\u6545\u62a5\u544a\u8f6c\u5316\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8bed\u8a00\uff0c\u5e76\u4e0e\u9a7e\u9a76\u65e5\u5fd7\u7edf\u4e00\u8868\u793a\u4e3a\u573a\u666f-\u52a8\u4f5c\u5f62\u5f0f\u3002\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u5148\u4f8b\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5148\u4f8b\u68c0\u7d22\u5c06\u4e0a\u4e0b\u6587\u4e2d\u66f4\u4f18\u52a8\u4f5c\u7684\u53ec\u56de\u7387\u4ece24%\u63d0\u5347\u523053%\u3002\u53cd\u4e8b\u5b9e\u53d8\u4f53\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u98ce\u9669\u51b3\u7b56\u3002", "conclusion": "\u91c7\u7528\u7edf\u4e00\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u6821\u51c6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18149", "pdf": "https://arxiv.org/pdf/2509.18149", "abs": "https://arxiv.org/abs/2509.18149", "authors": ["Shakir Showkat Sofi", "Lieven De Lathauwer"], "title": "Tensor Train Completion from Fiberwise Observations Along a Single Mode", "categories": ["math.NA", "cs.LG", "cs.NA", "eess.SP", "math.OC", "stat.CO", "stat.ML"], "comment": "Submitted to Numerical Algorithms (28 pages)", "summary": "Tensor completion is an extension of matrix completion aimed at recovering a\nmultiway data tensor by leveraging a given subset of its entries (observations)\nand the pattern of observation. The low-rank assumption is key in establishing\na relationship between the observed and unobserved entries of the tensor. The\nlow-rank tensor completion problem is typically solved using numerical\noptimization techniques, where the rank information is used either implicitly\n(in the rank minimization approach) or explicitly (in the error minimization\napproach). Current theories concerning these techniques often study\nprobabilistic recovery guarantees under conditions such as random uniform\nobservations and incoherence requirements. However, if an observation pattern\nexhibits some low-rank structure that can be exploited, more efficient\nalgorithms with deterministic recovery guarantees can be designed by leveraging\nthis structure. This work shows how to use only standard linear algebra\noperations to compute the tensor train decomposition of a specific type of\n``fiber-wise\" observed tensor, where some of the fibers of a tensor (along a\nsingle specific mode) are either fully observed or entirely missing, unlike the\nusual entry-wise observations. From an application viewpoint, this setting is\nrelevant when it is easier to sample or collect a multiway data tensor along a\nspecific mode (e.g., temporal). The proposed completion method is fast and is\nguaranteed to work under reasonable deterministic conditions on the observation\npattern. Through numerical experiments, we showcase interesting applications\nand use cases that illustrate the effectiveness of the proposed approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7279\u5b9a\u201c\u7ea4\u7ef4\u5f0f\u201d\u89c2\u6d4b\u5f20\u91cf\u7684\u5feb\u901f\u5f20\u91cf\u8865\u5168\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u51c6\u7ebf\u6027\u4ee3\u6570\u64cd\u4f5c\u8ba1\u7b97\u5f20\u91cf\u5e8f\u5217\u5206\u89e3\uff0c\u5e76\u5728\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u4fdd\u8bc1\u6062\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5f20\u91cf\u8865\u5168\u65b9\u6cd5\u591a\u57fa\u4e8e\u968f\u673a\u5747\u5300\u89c2\u6d4b\u548c\u4f4e\u79e9\u5047\u8bbe\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u89c2\u6d4b\u6a21\u5f0f\u4e2d\u7684\u4f4e\u79e9\u7ed3\u6784\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6cbf\u7279\u5b9a\u6a21\u5f0f\uff08\u5982\u65f6\u95f4\uff09\u91c7\u6837\u7684\u591a\u8def\u6570\u636e\u3002", "method": "\u901a\u8fc7\u6807\u51c6\u7ebf\u6027\u4ee3\u6570\u64cd\u4f5c\u8ba1\u7b97\u7279\u5b9a\u201c\u7ea4\u7ef4\u5f0f\u201d\u89c2\u6d4b\u5f20\u91cf\u7684\u5f20\u91cf\u5e8f\u5217\u5206\u89e3\uff0c\u63d0\u51fa\u4e86\u5feb\u901f\u5b8c\u6210\u8865\u5168\u7684\u65b9\u6cd5\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7279\u5b9a\u89c2\u6d4b\u6a21\u5f0f\u7684\u5f20\u91cf\u8865\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u5f20\u91cf\u8865\u5168\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.18631", "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6f14\u793a\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u800c\u4eff\u771f\u6570\u636e\u867d\u53ef\u5927\u89c4\u6a21\u751f\u6210\uff0c\u4f46\u5b58\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u9886\u57df\u5dee\u5f02\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u548c\u5927\u91cf\u4eff\u771f\u6570\u636e\uff0c\u5b9e\u73b0\u653f\u7b56\u7684\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u8fd0\u8f93\uff08OT\uff09\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u89c2\u5bdf\u4e0e\u52a8\u4f5c\u7684\u8054\u5408\u5206\u5e03\uff08\u800c\u975e\u4ec5\u89c2\u5bdf\uff09\uff0c\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u6269\u5c55\u4e3a\u4e0d\u5e73\u8861OT\u4ee5\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe30%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4ec5\u5728\u4eff\u771f\u4e2d\u89c1\u8fc7\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u4eff\u771f\u6570\u636e\u7684\u89c4\u6a21\u4f18\u52bf\uff0c\u7ed3\u5408\u5c11\u91cf\u771f\u5b9e\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u9645\u6027\u80fd\u3002"}}
{"id": "2509.18535", "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "categories": ["cs.CL", "eess.SP"], "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u5185\u90e8\u7ed3\u6784\uff08\u5bf9\u8bcd\u6c47\u53d8\u5316\u4e0d\u53d8\uff09\u6765\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "ChatGPT\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u6ee5\u7528\u62c5\u5fe7\uff0c\u73b0\u6709\u8bcd\u6c47\u7ea7\u68c0\u6d4b\u5668\u6613\u53d7\u6539\u5199\u6216\u7b80\u5355\u63d0\u793a\u5f71\u54cd\uff0c\u4e14\u5b58\u5728\u504f\u89c1\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7f16\u7801\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u53e5\u5b50\u5d4c\u5165\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5efa\u6a21\u5173\u7cfb\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u56e0\u679c\u56fe\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u51cf\u5c11\u504f\u89c1\u3002", "result": "\u5728\u4e24\u4e2a\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u6458\u8981\u5bf9\u6bd4\u548c\u4fee\u8ba2\u7684\u751f\u6d3b\u95ee\u7b54\u3002", "conclusion": "\u65b0\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u89e3\u51b3\u4e86\u8bcd\u6c47\u7ea7\u68c0\u6d4b\u5668\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u8f7b\u91cf\u5316\u548c\u9c81\u68d2\u6027\u4f18\u52bf\u3002"}}
{"id": "2509.18636", "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "categories": ["cs.RO"], "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u7684\u65e0\u4eba\u673a\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u65f6\u7f16\u961f\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u533a\u548c\u5206\u914d\u7b97\u6cd5\uff08PAAS\uff09\u4ee5\u53ca\u65f6\u7a7a\u8f68\u8ff9\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7f16\u961f\u6062\u590d\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u5bfc\u81f4\u7f16\u961f\u914d\u7f6e\u96be\u4ee5\u6536\u655b\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u5e76\u4fdd\u6301\u7f16\u961f\u5b8c\u6574\u6027\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Lloyd\u7b97\u6cd5\u7684\u5747\u5300\u5206\u533a\u548c\u5308\u7259\u5229\u7b97\u6cd5\u7684\u5206\u914d\uff08PAAS\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u539f\u59cb\u8def\u5f84\u641c\u7d22\u548c\u975e\u7ebf\u6027\u8f68\u8ff9\u4f18\u5316\u7684DVS\u65f6\u7a7a\u8f68\u8ff9\u89c4\u5212\uff0c\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u8f68\u8ff9\u89c4\u5212\u548c\u907f\u78b0\u3002", "result": "\u4eff\u771f\u4e2d\u652f\u630115%\u7684\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\uff0c\u5e76\u5feb\u901f\u6062\u590d\u7f16\u961f\u5f62\u72b6\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7f16\u961f\u6062\u590d\u901f\u5ea6\u548c\u73af\u5883\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18653", "pdf": "https://arxiv.org/pdf/2509.18653", "abs": "https://arxiv.org/abs/2509.18653", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering", "categories": ["cs.LG", "eess.SP"], "comment": "13 pages, Submitted to IEEE Transactions on Signal Processing", "summary": "We introduce a novel framework for clustering a collection of tall matrices\nbased on their column spaces, a problem we term Subspace Clustering of\nSubspaces (SCoS). Unlike traditional subspace clustering methods that assume\nvectorized data, our formulation directly models each data sample as a matrix\nand clusters them according to their underlying subspaces. We establish\nconceptual links to Subspace Clustering and Generalized Canonical Correlation\nAnalysis (GCCA), and clarify key differences that arise in this more general\nsetting. Our approach is based on a Block Term Decomposition (BTD) of a\nthird-order tensor constructed from the input matrices, enabling joint\nestimation of cluster memberships and partially shared subspaces. We provide\nthe first identifiability results for this formulation and propose scalable\noptimization algorithms tailored to large datasets. Experiments on real-world\nhyperspectral imaging datasets demonstrate that our method achieves superior\nclustering accuracy and robustness, especially under high noise and\ninterference, compared to existing subspace clustering techniques. These\nresults highlight the potential of the proposed framework in challenging\nhigh-dimensional applications where structure exists beyond individual data\nvectors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u5217\u7a7a\u95f4\u7684\u65b0\u578b\u805a\u7c7b\u6846\u67b6SCoS\uff0c\u5229\u7528\u5f20\u91cf\u5206\u89e3\u5b9e\u73b0\u9ad8\u7ef4\u5ea6\u6570\u636e\u7684\u9c81\u68d2\u805a\u7c7b\u3002", "motivation": "\u4f20\u7edf\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u4e3a\u5411\u91cf\u5f62\u5f0f\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u5e38\u4e3a\u77e9\u9635\u5f62\u5f0f\uff0c\u9700\u76f4\u63a5\u5efa\u6a21\u77e9\u9635\u6570\u636e\u8fdb\u884c\u66f4\u51c6\u786e\u805a\u7c7b\u3002", "method": "\u57fa\u4e8e\u5757\u9879\u5206\u89e3\uff08BTD\uff09\u6784\u5efa\u4e09\u9636\u5f20\u91cf\uff0c\u8054\u5408\u4f30\u8ba1\u805a\u7c7b\u6210\u5458\u548c\u90e8\u5206\u5171\u4eab\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u548c\u5e72\u6270\u4e0b\u4f18\u4e8e\u73b0\u6709\u5b50\u7a7a\u95f4\u805a\u7c7b\u6280\u672f\u3002", "conclusion": "SCoS\u6846\u67b6\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d85\u8d8a\u5355\u4e2a\u6570\u636e\u5411\u91cf\u7684\u7ed3\u6784\u573a\u666f\u3002"}}
{"id": "2509.18644", "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "categories": ["cs.RO", "cs.AI"], "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u611f\u89c9\u8f93\u5165\uff0c\u5bfc\u81f4\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u63d0\u51fa\u4ec5\u4f9d\u8d56\u89c6\u89c9\u89c2\u5bdf\u7684\u65e0\u72b6\u6001\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u672c\u4f53\u611f\u89c9\u8f93\u5165\u5bf9\u7a7a\u95f4\u6cdb\u5316\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u65e0\u72b6\u6001\u7b56\u7565\uff0c\u4ec5\u57fa\u4e8e\u89c6\u89c9\u89c2\u5bdf\u9884\u6d4b\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u53cc\u8155\u5bbd\u89d2\u76f8\u673a\u63d0\u4f9b\u5168\u9762\u7684\u4efb\u52a1\u76f8\u5173\u89c6\u89c9\u89c2\u5bdf\u3002", "result": "\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u65e0\u72b6\u6001\u7b56\u7565\u5728\u9ad8\u5ea6\u548c\u6c34\u5e73\u6cdb\u5316\u4e2d\u7684\u6210\u529f\u7387\u5206\u522b\u4ece0%\u63d0\u5347\u81f385%\u548c\u4ece6%\u63d0\u5347\u81f364%\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u548c\u8de8\u673a\u5668\u4eba\u9002\u5e94\u6027\u3002", "conclusion": "\u65e0\u72b6\u6001\u7b56\u7565\u901a\u8fc7\u53bb\u9664\u672c\u4f53\u611f\u89c9\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.18780", "pdf": "https://arxiv.org/pdf/2509.18780", "abs": "https://arxiv.org/abs/2509.18780", "authors": ["Thi Hong Thai Nguyen", "Ba-Ngu Vo", "Ba-Tuong Vo"], "title": "Tractable Approximation of Labeled Multi-Object Posterior Densities", "categories": ["stat.ME", "eess.SP"], "comment": null, "summary": "Multi-object estimation in state-space models (SSMs) wherein the system state\nis represented as a finite set has attracted significant interest in recent\nyears. In Bayesian inference, the posterior density captures all information on\nthe system trajectory since it considers the past history of states. In most\nmulti-object SSM applications, closed-form multi-object posteriors are not\navailable for non-standard multi-object models. Thus, functional approximation\nis necessary because these posteriors are very high-dimensional. This work\nprovides a tractable multi-scan Generalized Labeled Multi-Bernoulli (GLMB)\napproximation that matches the trajectory cardinality distribution of the\nlabeled multi-object posterior density. The proposed approximation is also\nproven to minimize the Kullback-Leibler divergence over a special class of\nmulti-scan GLMB model. Additionally, we develop a tractable algorithm for\ncomputing the approximate multi-object posteriors over finite windows.\nNumerical experiments, including simulation results on a multi-object SSM with\nsocial force model and uninformative observations, are presented to validate\nthe applicability of the approximation method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5904\u7406\u7684\u5e7f\u4e49\u6807\u8bb0\u591a\u4f2f\u52aa\u5229\uff08GLMB\uff09\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u591a\u76ee\u6807\u4f30\u8ba1\uff0c\u4ee5\u5339\u914d\u6807\u8bb0\u591a\u76ee\u6807\u540e\u9a8c\u5bc6\u5ea6\u7684\u8f68\u8ff9\u57fa\u6570\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u8fd8\u6700\u5c0f\u5316\u4e86Kullback-Leibler\u6563\u5ea6\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u76ee\u6807\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u975e\u6807\u51c6\u7279\u6027\u4f7f\u5f97\u95ed\u5f0f\u591a\u76ee\u6807\u540e\u9a8c\u4e0d\u53ef\u884c\uff0c\u800c\u529f\u80fd\u6027\u8fd1\u4f3c\u9700\u6c42\u8feb\u5207\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u626b\u63cfGLMB\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5339\u914d\u8f68\u8ff9\u57fa\u6570\u5206\u5e03\u5e76\u4f18\u5316Kullback-Leibler\u6563\u5ea6\uff0c\u5f00\u53d1\u4e86\u8ba1\u7b97\u6709\u9650\u7a97\u53e3\u5185\u8fd1\u4f3c\u540e\u9a8c\u7684\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\uff08\u5305\u62ec\u793e\u4ea4\u529b\u6a21\u578b\u548c\u4fe1\u606f\u4e0d\u8db3\u89c2\u6d4b\u7684\u591a\u76ee\u6807SSM\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684GLMB\u8fd1\u4f3c\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u6709\u6548\u8fd1\u4f3c\u9ad8\u7ef4\u540e\u9a8c\u5bc6\u5ea6\u3002"}}
{"id": "2509.18648", "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "SPiDR\u662f\u4e00\u79cd\u901a\u8fc7\u60b2\u89c2\u57df\u968f\u673a\u5316\u5b9e\u73b0\u5b89\u5168\u6a21\u62df\u5230\u73b0\u5b9e\u4f20\u8f93\u7684\u53ef\u6269\u5c55\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u7ea6\u675f\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5931\u6548\uff0c\u73b0\u6709\u7684\u9c81\u68d2\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u4e0e\u6807\u51c6\u53ef\u6269\u5c55\u8bad\u7ec3\u6d41\u7a0b\u517c\u5bb9\uff0c\u4fc3\u4f7f\u5f00\u53d1\u4e00\u79cd\u66f4\u517c\u5bb9\u4e14\u5b89\u5168\u7684\u65b0\u65b9\u6cd5\u3002", "method": "SPiDR\u5229\u7528\u57df\u968f\u673a\u5316\u6280\u672f\uff0c\u5c06\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u4e0d\u786e\u5b9a\u6027\u878d\u5165\u5b89\u5168\u7ea6\u675f\u4e2d\uff0c\u786e\u4fdd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8003\u8651\u73b0\u5b9e\u73af\u5883\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u5230\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u79cd\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0cSPiDR\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "conclusion": "SPiDR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u4f20\u8f93\u4e2d\u6709\u6548\u4fdd\u969c\u5b89\u5168\u6027\uff0c\u4e14\u6613\u4e8e\u4e0e\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u96c6\u6210\u3002"}}
{"id": "2509.18906", "pdf": "https://arxiv.org/pdf/2509.18906", "abs": "https://arxiv.org/abs/2509.18906", "authors": ["Kyriakos Stylianopoulos", "George C. Alexandropoulos"], "title": "Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks", "categories": ["cs.ET", "cs.LG", "eess.SP"], "comment": "Submitted to IEEE ICASSP 2026", "summary": "This paper introduces a novel framework for Edge Inference (EI) that bypasses\nthe conventional practice of treating the wireless channel as noise. We utilize\nStacked Intelligent Metasurfaces (SIMs) to control wireless propagation,\nenabling the channel itself to perform over-the-air computation. This\neliminates the need for symbol estimation at the receiver, significantly\nreducing computational and communication overhead. Our approach models the\ntransmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)\nwhere the response of the SIM elements are trainable parameters. To address\nchannel variability, we incorporate a dedicated DNN module responsible for\ndynamically adjusting transmission power leveraging user location information.\nOur performance evaluations showcase that the proposed metasurfaces-integrated\nDNN framework with deep SIM architectures are capable of balancing\nclassification accuracy and power consumption under diverse scenarios, offering\nsignificant energy efficiency improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u8d85\u8868\u9762\u7684\u8fb9\u7f18\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u65e0\u7ebf\u4fe1\u9053\u8fdb\u884c\u7a7a\u4e2d\u8ba1\u7b97\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u65e0\u7ebf\u4fe1\u9053\u89c6\u4e3a\u566a\u58f0\uff0c\u800c\u8be5\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u8d85\u8868\u9762\u63a7\u5236\u65e0\u7ebf\u4f20\u64ad\uff0c\u5229\u7528\u4fe1\u9053\u76f4\u63a5\u8fdb\u884c\u8ba1\u7b97\u3002", "method": "\u5c06\u53d1\u5c04-\u4fe1\u9053-\u63a5\u6536\u7cfb\u7edf\u5efa\u6a21\u4e3a\u7aef\u5230\u7aef\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u667a\u80fd\u8d85\u8868\u9762\u7684\u54cd\u5e94\u4e3a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u529f\u7387\u7684DNN\u6a21\u5757\u3002", "result": "\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\uff0c\u8be5\u6846\u67b6\u80fd\u5e73\u8861\u5206\u7c7b\u7cbe\u5ea6\u548c\u529f\u8017\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u8d85\u8868\u9762\u548cDNN\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fb9\u7f18\u63a8\u7406\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5f00\u9500\u3002"}}
{"id": "2509.18666", "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7RKHS\u5d4c\u5165\u969c\u788d\u7269\u672a\u6765\u8f68\u8ff9\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u5e76\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u672a\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528RKHS\u5d4c\u5165\u6761\u4ef6\u5206\u5e03\uff0c\u6784\u5efa\u5206\u5e03\u6a21\u7cca\u96c6\uff0c\u5e76\u7eb3\u5165\u8fd0\u52a8\u89c4\u5212\u7684\u6eda\u52a8\u65f6\u57df\u6846\u67b6\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4e0d\u5305\u542b\u4e0a\u4e0b\u6587\u6216\u5206\u5e03\u9c81\u68d2\u6027\u7684\u573a\u666f\u4e2d\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u5206\u5e03\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u4e86\u78b0\u649e\u907f\u514d\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2509.19064", "pdf": "https://arxiv.org/pdf/2509.19064", "abs": "https://arxiv.org/abs/2509.19064", "authors": ["Renaud-Alexandre Pitaval", "Fredrik Berggren", "Branislav M. Popovic"], "title": "Optimum Spectrum Extension for PAPR Reduction of DFT-s-OFDM", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Uplink coverage in cellular networks is constrained by the maximum UE\ntransmit power, making peak-to-average power ratio (PAPR) reduction essential.\nWhile DFT-s-OFDM with frequency-domain spectral shaping (FDSS) achieves\nsignificantly lower PAPR than OFDM, especially with pi/2-BPSK, the PAPR remains\ntoo high for higher-rate transmission. Spectrum extension (SE) combined with\nFDSS (FDSS-SE) can further reduce the PAPR for higher-order QAM. This paper\nconsiders FDSS-SE with parametrized FDSS windows spanning a range of possible\npower ripples, as well as arbitrary circular shifts of the subcarrier\ncoefficients. We optimize both the frequency shift and the SE size, and show\nthat there exists an optimal SE size for reducing the PAPR and another one for\nincreasing the rate. Analysis and simulations reveal that both optima largely\ndepend on the window attenuation but are nearly invariant in proportion to the\nbandwidth. While the PAPR-optimal SE size is nearly invariant to the\nconstellation order of regular QAM, the rate-optimal SE size depends also on\nthe SNR. These insights provide practical guidelines for beyond-5G uplink\ncoverage enhancement, highlighting that SE size should be individually\nconfigured according to the user's FDSS window and link quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u9891\u8c31\u6269\u5c55\uff08FDSS-SE\uff09\u7684\u53c2\u6570\u5316FDSS\u7a97\u53e3\u548c\u5b50\u8f7d\u6ce2\u7cfb\u6570\u5faa\u73af\u79fb\u4f4d\u6280\u672f\uff0c\u4f18\u5316PAPR\u548c\u901f\u7387\uff0c\u63ed\u793a\u4e86\u6700\u4f73SE\u5927\u5c0f\u4e0e\u7a97\u53e3\u8870\u51cf\u548c\u5e26\u5bbd\u7684\u5173\u7cfb\uff0c\u4e3a5G\u4ee5\u540e\u7684\u4e0a\u884c\u8986\u76d6\u589e\u5f3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8702\u7a9d\u7f51\u7edc\u4e2d\u7531\u4e8e\u6700\u5927UE\u53d1\u5c04\u529f\u7387\u9650\u5236\u5bfc\u81f4\u7684PAPR\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u4f20\u8f93\u901f\u7387\u4e0bPAPR\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408FDSS-SE\u7684\u6280\u672f\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86FDSS-SE\u4e2d\u53c2\u6570\u5316FDSS\u7a97\u53e3\u548c\u5b50\u8f7d\u6ce2\u7cfb\u6570\u5faa\u73af\u79fb\u4f4d\u7684\u4f18\u5316\uff0c\u5305\u62ec\u9891\u79fb\u548cSE\u5927\u5c0f\u7684\u8c03\u6574\uff0c\u4ee5\u964d\u4f4ePAPR\u5e76\u63d0\u9ad8\u901f\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cPAPR\u548c\u901f\u7387\u7684\u6700\u4f73SE\u5927\u5c0f\u4e3b\u8981\u53d6\u51b3\u4e8e\u7a97\u53e3\u8870\u51cf\uff0c\u800c\u4e0e\u5e26\u5bbd\u7684\u6bd4\u4f8b\u5173\u7cfb\u51e0\u4e4e\u4e0d\u53d8\u3002\u6b64\u5916\uff0cPAPR\u6700\u4f18SE\u5927\u5c0f\u51e0\u4e4e\u4e0d\u53d7\u5e38\u89c4QAM\u661f\u5ea7\u9636\u6570\u7684\u5f71\u54cd\uff0c\u800c\u901f\u7387\u6700\u4f18SE\u5927\u5c0f\u8fd8\u4f9d\u8d56\u4e8eSNR\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u5f3a\u8c03\uff0c\u5e94\u6839\u636e\u7528\u6237\u7684FDSS\u7a97\u53e3\u548c\u94fe\u8def\u8d28\u91cf\u5355\u72ec\u914d\u7f6eSE\u5927\u5c0f\uff0c\u8fd9\u4e3a5G\u4ee5\u540e\u7684\u4e0a\u884c\u8986\u76d6\u589e\u5f3a\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.18671", "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "categories": ["cs.RO"], "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faN2M\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u5230\u8fbe\u4efb\u52a1\u533a\u57df\u540e\u5f15\u5bfc\u5176\u8c03\u6574\u5230\u66f4\u6709\u5229\u7684\u521d\u59cb\u4f4d\u59ff\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5bfc\u822a\u6a21\u5757\u4e0e\u64cd\u4f5c\u7b56\u7565\u4e4b\u95f4\u7684\u4f4d\u59ff\u504f\u597d\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u3002", "method": "N2M\u6a21\u5757\u4ec5\u4f9d\u8d56\u673a\u5668\u4eba\u81ea\u8eab\u89c2\u6d4b\uff0c\u5b9e\u65f6\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "result": "\u5728PnPCounterToCab\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u4ece3%\u63d0\u5347\u523054%\uff1b\u5728Toybox Handover\u4efb\u52a1\u4e2d\uff0c\u4ec5\u970015\u4e2a\u6570\u636e\u6837\u672c\u5373\u53ef\u5728\u672a\u89c1\u73af\u5883\u4e2d\u53ef\u9760\u9884\u6d4b\u3002", "conclusion": "N2M\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6570\u636e\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.19073", "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.", "AI": {"tldr": "WaveletGaussian\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u66f4\u9ad8\u6548\u76843D\u9ad8\u65af\u5bf9\u8c61\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u8fc7\u7a0b\u79fb\u81f3\u5c0f\u6ce2\u57df\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u6269\u6563\u4fee\u590d\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5c06\u6269\u6563\u8fc7\u7a0b\u5e94\u7528\u4e8e\u5c0f\u6ce2\u57df\u7684\u4f4e\u5206\u8fa8\u7387LL\u5b50\u5e26\uff0c\u9ad8\u9891\u5b50\u5e26\u5219\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f51\u7edc\u7ec6\u5316\uff1b\u63d0\u51fa\u5728\u7ebf\u968f\u673a\u63a9\u7801\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u5bf9\u751f\u6210\u3002", "result": "\u5728Mip-NeRF 360\u548cOmniObject3D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6e32\u67d3\u8d28\u91cf\u63a5\u8fd1\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "WaveletGaussian\u6846\u67b6\u901a\u8fc7\u5c0f\u6ce2\u57df\u6269\u6563\u548c\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2509.18676", "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D Flow Diffusion Policy\uff083D FDP\uff09\u7684\u65b0\u6846\u67b6\uff0c\u5229\u75283D\u6d41\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u6355\u6349\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\uff0c\u800c\u7cbe\u786e\u7684\u63a5\u89e6\u5f0f\u64cd\u4f5c\u9700\u8981\u8fd9\u4e9b\u7ebf\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u573a\u666f\u7ea73D\u6d41\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u9884\u6d4b\u91c7\u6837\u67e5\u8be2\u70b9\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u67b6\u6784\u751f\u6210\u52a8\u4f5c\uff0c3D FDP\u80fd\u591f\u5728\u5c40\u90e8\u52a8\u529b\u5b66\u7684\u57fa\u7840\u4e0a\u63a8\u7406\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\u5f71\u54cd\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c3D FDP\u572850\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u4e2d\u7b49\u548c\u56f0\u96be\u4efb\u52a1\u4e2d\u3002\u5b9e\u7269\u673a\u5668\u4eba\u6d4b\u8bd5\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5728\u63a5\u89e6\u5f0f\u548c\u975e\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "3D\u6d41\u662f\u5b66\u4e60\u6cdb\u5316\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6709\u6548\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u591a\u529f\u80fd\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.19234", "pdf": "https://arxiv.org/pdf/2509.19234", "abs": "https://arxiv.org/abs/2509.19234", "authors": ["Hesam Hosseini", "Ying Cao", "Ali H. Sayed"], "title": "Stability and Generalization of Adversarial Diffusion Training", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7b97\u6cd5\u7a33\u5b9a\u6027\u5206\u6790\u4e86\u5206\u6563\u5f0f\u7f51\u7edc\u4e2d\u5bf9\u6297\u8bad\u7ec3\u5728\u51f8\u635f\u5931\u4e0b\u7684\u6cdb\u5316\u6027\u8d28\uff0c\u53d1\u73b0\u6cdb\u5316\u8bef\u5dee\u968f\u5bf9\u6297\u6270\u52a8\u529b\u5ea6\u548c\u8bad\u7ec3\u6b65\u6570\u589e\u52a0\u800c\u589e\u957f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5206\u6563\u5f0f\u7f51\u7edc\u4e2d\u5bf9\u6297\u8bad\u7ec3\u7684\u6cdb\u5316\u6027\u8d28\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u9488\u5bf9\u51f8\u635f\u5931\u4e0b\u7684\u5bf9\u6297\u8bad\u7ec3\u8fdb\u884c\u7406\u8bba\u63a8\u5bfc\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u56de\u5f52\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u8868\u660e\u6cdb\u5316\u8bef\u5dee\u4e0e\u5bf9\u6297\u6270\u52a8\u529b\u5ea6\u548c\u8bad\u7ec3\u6b65\u6570\u6b63\u76f8\u5173\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u7406\u8bba\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u5bf9\u6297\u8bad\u7ec3\u5728\u5206\u6563\u5f0f\u7f51\u7edc\u4e2d\u7684\u6cdb\u5316\u6027\u8d28\u4e0e\u5355\u667a\u80fd\u4f53\u60c5\u51b5\u76f8\u4f3c\uff0c\u4f46\u8fd9\u662f\u9996\u6b21\u5728\u5206\u6563\u5f0f\u73af\u5883\u4e2d\u8bc1\u660e\u8fd9\u4e00\u73b0\u8c61\u3002"}}
{"id": "2509.18686", "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67e5\u8be2\u7684\u5c42\u6b21\u5316\u7b56\u7565\uff08QDP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u56e0\u90e8\u4ef6\u590d\u6742\u6027\u548c\u566a\u58f0\u6270\u52a8\u5bfc\u81f4\u7684\u5c42\u6b21\u5316\u7b56\u7565\u5b9e\u65bd\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u67e5\u8be2\u4e3a\u4e2d\u5fc3\u7684\u6269\u6563\u7b56\u7565\uff08QDP\uff09\uff0c\u901a\u8fc7\u5bf9\u8c61\u3001\u63a5\u89e6\u70b9\u548c\u6280\u80fd\u4fe1\u606f\u7684\u67e5\u8be2\uff0c\u6307\u5bfc\u5e95\u5c42\u63a7\u5236\uff0c\u5e76\u5229\u7528\u70b9\u4e91\u89c2\u6d4b\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cQDP\u5728\u6280\u80fd\u7cbe\u5ea6\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d2\u5165\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad850%\u4ee5\u4e0a\u3002", "conclusion": "QDP\u901a\u8fc7\u67e5\u8be2\u673a\u5236\u6709\u6548\u8fde\u63a5\u4e86\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.19268", "pdf": "https://arxiv.org/pdf/2509.19268", "abs": "https://arxiv.org/abs/2509.19268", "authors": ["Xiyu Chen", "Junxiang Cai", "Rui Zheng", "Tao Wu", "Fei Gao"], "title": "A Low-cost Quasi-planar Array Probe for Photoacoustic Imaging", "categories": ["physics.med-ph", "eess.SP"], "comment": "4 pages, 4 figures", "summary": "Photoacoustic imaging (PAI) is a novel hybrid imaging technique that combines\nthe benefits of both optical and acoustic imaging modalities, which provides\nfunctional and molecular optical contrasts of deep tissue. Commonly used\nultrasound transducers for PAI include linear and planar arrays, which can\nprovide two-dimensional (2D) and three-dimensional (3D) image reconstruction,\nrespectively. However, linear arrays cannot provide reconstruction of 3D\nimages, which makes it impossible to locate chromophores in 3D space. Although\nplanar array can provide fast 3D imaging in real time, it usually requires\nthousands of analog-to-digital conversion channels for data acquisition, which\nis costly. To fill the gap between 2D and 3D PAI, we propose a quasi-planar\narray that uses double 16-elements-linear arrays arranged in parallel to\nachieve real-time 3D imaging. We first conducted simulation studies to prove\nthat the quasi-planar probe can perform 3D imaging to localize simple\nchromophores. Then, the agarose phantom experiment demonstrated that the probe\ncan reconstruct 3D imaging of multiple absorbers in different depths. A\npotential application of this device is to provide a low-cost 3D PAI solution\nfor fast tracking of needle tip during needle biopsy, which will be further\nexplored in our future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51c6\u5e73\u9762\u9635\u5217\u6280\u672f\uff0c\u7528\u4e8e\u5149\u58f0\u6210\u50cf\uff08PAI\uff09\uff0c\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u5b9e\u65f63D\u6210\u50cf\uff0c\u586b\u8865\u4e862D\u548c3D PAI\u4e4b\u95f4\u7684\u6280\u672f\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u9635\u5217\u65e0\u6cd5\u5b9e\u73b03D\u6210\u50cf\uff0c\u800c\u5e73\u9762\u9635\u5217\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u76843D PAI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc16\u5355\u5143\u7ebf\u6027\u9635\u5217\u5e76\u884c\u6392\u5217\u7684\u51c6\u5e73\u9762\u9635\u5217\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u743c\u8102\u7cd6\u6a21\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u51763D\u6210\u50cf\u80fd\u529b\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u63a2\u5934\u80fd\u5b9e\u73b03D\u6210\u50cf\uff0c\u5b9a\u4f4d\u4e0d\u540c\u6df1\u5ea6\u7684\u5438\u6536\u4f53\u3002", "conclusion": "\u51c6\u5e73\u9762\u9635\u5217\u4e3a\u4f4e\u6210\u672c\u5b9e\u65f63D PAI\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u5176\u5728\u6d3b\u68c0\u9488\u5c16\u8ddf\u8e2a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.18734", "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "categories": ["cs.RO"], "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u865a\u62df\u56db\u65cb\u7ffc\u673a\u5668\u4eba\u907f\u969c\u65b9\u6cd5\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u7531\u4e8eGPS\u7cbe\u5ea6\u4e0b\u964d\u3001\u72ed\u7a84\u7a7a\u95f4\u548c\u52a8\u6001\u969c\u788d\u7269\u7b49\u56e0\u7d20\uff0c\u98de\u884c\u5668\u7684\u8def\u5f84\u89c4\u5212\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u6709\u6548\u7684\u907f\u969c\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u865a\u62df\u56db\u65cb\u7ffc\u673a\u5668\u4eba\uff0c\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4efb\u52a1\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u907f\u969c\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18757", "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "categories": ["cs.RO", "cs.AI"], "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "MV-UMI\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u63d0\u9ad8\u4e86\u624b\u6301\u8bbe\u5907\u6570\u636e\u91c7\u96c6\u7684\u591a\u6837\u6027\uff0c\u51cf\u5c11\u4e86\u9886\u57df\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u53d7\u9650\u4e8e\u7279\u5b9a\u673a\u5668\u4eba\u672c\u4f53\u3002\u624b\u6301\u8bbe\u5907\u867d\u4fbf\u6377\uff0c\u4f46\u4ec5\u4f9d\u9760\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\u96be\u4ee5\u6355\u6349\u5b8c\u6574\u573a\u666f\u4fe1\u606f\u3002", "method": "\u63d0\u51faMV-UMI\u6846\u67b6\uff0c\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u5f25\u8865\u624b\u6301\u8bbe\u5907\u5728\u573a\u666f\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMV-UMI\u5728\u9700\u8981\u5e7f\u6cdb\u573a\u666f\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u7ea647%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MV-UMI\u6210\u529f\u6269\u5c55\u4e86\u624b\u6301\u8bbe\u5907\u7684\u5b66\u4e60\u4efb\u52a1\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u8de8\u672c\u4f53\u4f18\u52bf\u3002"}}
{"id": "2509.18778", "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "categories": ["cs.RO", "cs.AI"], "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "VGGT-DP \u662f\u4e00\u4e2a\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u7b56\u7565\u8bbe\u8ba1\uff0c\u800c\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ed3\u6784\u548c\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa VGGT-DP \u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684 3D \u611f\u77e5\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff0c\u91c7\u7528 VGGT \u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u672c\u4f53\u611f\u53d7\u5f15\u5bfc\u7684\u89c6\u89c9\u5b66\u4e60\u7b56\u7565\u3002\u8bbe\u8ba1\u4e86\u5e27\u7ea7\u6807\u8bb0\u91cd\u7528\u673a\u5236\u548c\u968f\u673a\u6807\u8bb0\u526a\u679d\u4ee5\u63d0\u5347\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728 MetaWorld \u4efb\u52a1\u4e2d\uff0cVGGT-DP \u663e\u8457\u4f18\u4e8e DP \u548c DP3 \u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VGGT-DP \u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u53d7\u53cd\u9988\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2509.18786", "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGP-CA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u70b9\u4e91\u914d\u51c6\u95ee\u9898\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u8bc6\u522b\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u6837\u672c\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u70b9\u4e91\u914d\u51c6\u95ee\u9898\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u548c\u906e\u6321\u7b49\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982ICP\uff09\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u548c\u89e3\u91ca\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u9ad8\u65af\u8fc7\u7a0b\u6982\u5ff5\u5f52\u56e0\uff08GP-CA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u67e5\u8be2\u4fe1\u606f\u6027\u5b9e\u4f8b\u6765\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "GP-CA\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u6837\u672c\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u5931\u8d25\u6062\u590d\u884c\u4e3a\u3002", "conclusion": "GP-CA\u4e3a\u70b9\u4e91\u914d\u51c6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18793", "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "categories": ["cs.RO", "cs.MA", "cs.SE"], "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u534f\u8c03\u8f66\u8f86\u4e92\u8054\u7cfb\u7edf\u4e2d\u7684\u5fae\u670d\u52a1\u90e8\u7f72\u4e0e\u8d44\u6e90\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u548c\u4e92\u8054\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0c\u5927\u89c4\u6a21\u534f\u540c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08C-ITS\uff09\u7684\u5e94\u7528\u7f16\u6392\u9762\u4e34\u52a8\u6001\u73af\u5883\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u7684\u6311\u6218\u3002", "method": "\u5229\u7528Kubernetes\u548cROS 2\u6784\u5efa\u7684\u5e94\u7528\u7ba1\u7406\u6846\u67b6\uff0c\u52a8\u6001\u54cd\u5e94C-ITS\u4e2d\u4e0d\u540c\u5b9e\u4f53\u7684\u9700\u6c42\uff0c\u81ea\u52a8\u5316\u5fae\u670d\u52a1\u7684\u90e8\u7f72\u3001\u91cd\u6784\u3001\u66f4\u65b0\u548c\u6269\u5c55\u3002", "result": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u7f51\u7edc\u6d41\u91cf\uff0c\u5e76\u5728\u96c6\u4f53\u73af\u5883\u611f\u77e5\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u9700\u6c42\u9a71\u52a8\u7684\u4e91\u539f\u751f\u6280\u672f\u80fd\u6709\u6548\u89e3\u51b3C-ITS\u4e2d\u7684\u52a8\u6001\u5e94\u7528\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2509.18830", "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u67d4\u8f6f\u7684\u7535\u5bb9\u5f0f\u7535\u5b50\u76ae\u80a4\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u624b\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u652f\u6301\u5b66\u4e60\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "motivation": "\u590d\u5236\u4eba\u7c7b\u76ae\u80a4\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1DexSkin\u7535\u5b50\u76ae\u80a4\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5e73\u884c\u5939\u722a\u624b\u6307\uff0c\u8fdb\u884c\u89e6\u89c9\u8986\u76d6\u3002\u901a\u8fc7\u793a\u8303\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "DexSkin\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u7269\u4f53\u91cd\u65b0\u5b9a\u5411\u548c\u5f39\u6027\u5e26\u7f20\u7ed5\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u8de8\u4f20\u611f\u5668\u5b9e\u4f8b\u8fdb\u884c\u6a21\u578b\u8f6c\u79fb\u3002", "conclusion": "DexSkin\u9002\u7528\u4e8e\u5b66\u4e60\u771f\u5b9e\u4e16\u754c\u4e2d\u63a5\u89e6\u5bc6\u96c6\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18865", "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "Bi-VLA\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u6269\u5c55\u4e86\u53cc\u8fb9\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u6a21\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002Bi-VLA\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "Bi-VLA\u5229\u7528\u673a\u5668\u4eba\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u3001\u626d\u77e9\u6570\u636e\uff0c\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u8fc7SigLIP\u548cFiLM\u878d\u5408\u5904\u7406\u591a\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cBi-VLA\u80fd\u6709\u6548\u89e3\u91ca\u89c6\u89c9-\u8bed\u8a00\u7ec4\u5408\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Bi-VLA\u7a81\u7834\u4e86\u4f20\u7edf\u53cc\u8fb9\u65b9\u6cd5\u7684\u5355\u4efb\u52a1\u9650\u5236\uff0c\u8bc1\u5b9e\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u7ed3\u5408\u5bf9\u589e\u5f3a\u901a\u7528\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.18937", "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "categories": ["cs.RO"], "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u673a\u5668\u4eba\u624b\u90e8\u5f62\u6001\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7b26\u53f7\u7ed3\u6784\u548c\u53c2\u6570\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8bbe\u8ba1\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u542f\u53d1\u5f0f\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\uff0c\u800cLLM\u56e0\u5176\u5e7f\u6cdb\u7684\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u77e5\u8bc6\u548c\u751f\u6210\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u53ef\u80fd\u6027\u3002", "method": "Lang2Morph\u5206\u4e3a\u5f62\u6001\u8bbe\u8ba1\uff08\u4efb\u52a1\u6620\u5c04\u5230\u8bed\u4e49\u6807\u7b7e\u3001\u7ed3\u6784\u8bed\u6cd5\u548c\u53c2\u6570\uff09\u548c\u9009\u62e9\u4e0e\u7ec6\u5316\uff08\u8bc4\u4f30\u8bed\u4e49\u5bf9\u9f50\u548c\u5c3a\u5bf8\u517c\u5bb9\u6027\uff0c\u53ef\u9009LLM\u5f15\u5bfc\u7ec6\u5316\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLang2Morph\u80fd\u751f\u6210\u591a\u6837\u4e14\u4efb\u52a1\u76f8\u5173\u7684\u624b\u90e8\u5f62\u6001\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u6761\u4ef6\u5316\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u3002", "conclusion": "Lang2Morph\u4e3a\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u7684\u65b0\u6846\u67b6\uff0c\u5c55\u793a\u4e86LLM\u5728\u5f62\u6001\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.18953", "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "Eva-VLA\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f30Vision-Language-Action\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u7406\u53d8\u5316\u7684\u8106\u5f31\u6027\u3002", "motivation": "Vision-Language-Action\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u7eb5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u7406\u53d8\u5316\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51faEva-VLA\u6846\u67b6\u3002", "method": "\u5c06\u7269\u7406\u53d8\u5316\u5206\u89e3\u4e3a\u5bf9\u8c613D\u53d8\u6362\u3001\u5149\u7167\u53d8\u5316\u548c\u5bf9\u6297\u6027\u8865\u4e01\u4e09\u4e2a\u9886\u57df\uff0c\u5e76\u5f15\u5165\u8fde\u7eed\u9ed1\u76d2\u4f18\u5316\u6846\u67b6\uff0c\u63a2\u7d22\u6700\u574f\u60c5\u51b5\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cOpenVLA\u6a21\u578b\u5bf9\u6240\u6709\u53d8\u5316\u7684\u6545\u969c\u7387\u8d85\u8fc760%\uff0c\u5bf9\u8c61\u53d8\u6362\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u6545\u969c\u7387\u9ad8\u8fbe97.8%\u3002", "conclusion": "Eva-VLA\u6846\u67b6\u4e3a\u63d0\u5347VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.18954", "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u53c2\u8003\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u9884\u6d4bICP\u7684\u914d\u51c6\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8LiDAR\u5b9a\u4f4d\u548cSLAM\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "ICP\u7b97\u6cd5\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u6216\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u4e14\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\u6216\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u9884\u5148\u4f30\u8ba1LiDAR\u626b\u63cf\u76846\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u5730\u56fe\uff0c\u5e76\u5c06\u7ed3\u679c\u65e0\u7f1d\u96c6\u6210\u5230\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u534f\u65b9\u5dee\uff0c\u663e\u8457\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u6846\u67b6\u4e3aICP\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86SLAM\u548c\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18979", "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u5c40\u90e8\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u4ec5\u9700\u7c7b\u522b\u7ea7\u522b\u7684\u7269\u4f53\u5148\u9a8c\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u7684\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u660e\u3002\u901a\u8fc7RGB-D\u56fe\u50cf\u548c\u5b66\u4e60\u7684\u8bed\u4e49\u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u7ed3\u5408\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u59ff\u6001\u548c\u5f62\u72b6\u7684\u540c\u65f6\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u57fa\u7840\u95ee\u9898\uff0c\u5bf9\u64cd\u7eb5\u3001\u573a\u666f\u7406\u89e3\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u7684\u524d\u7aef\u68c0\u6d4b\u7c7b\u522b\u7ea7\u522b\u7684\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u901a\u8fc7\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u7269\u4f53\u5f62\u72b6\uff0c\u5e76\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u57fa\u4e8e\u5355\u4f4d\u56db\u5143\u6570\u7684\u6700\u5927\u540e\u9a8c\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u81ea\u6d3d\u573a\u8fed\u4ee3\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u573a\u666f\u4e2d\uff08\u5305\u62ec\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\uff09\u8868\u73b0\u51fa\u8272\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u9700\u7ea6100\u5fae\u79d2\uff0c\u5e76\u80fd\u5feb\u901f\u5254\u9664\u5f02\u5e38\u503c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6700\u4f18\u6027\u8bc1\u660e\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.19012", "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ece\u4f20\u7edf\u7b56\u7565\u63a7\u5236\u8f6c\u5411\u901a\u7528\u673a\u5668\u4eba\u6280\u672f\uff0c\u5e76\u8be6\u7ec6\u5206\u7c7b\u4e86VLA\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\u3002", "motivation": "\u7814\u7a76VLA\u6a21\u578b\u7684\u52a8\u673a\u662f\u4ece\u88ab\u52a8\u5e8f\u5217\u751f\u6210\u8f6c\u5411\u4e3b\u52a8\u51b3\u7b56\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9300\u591a\u9879\u7814\u7a76\u7684\u7efc\u5408\u5206\u6790\uff0c\u5c06VLA\u65b9\u6cd5\u5206\u4e3a\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u548c\u4e13\u7528\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u5176\u6838\u5fc3\u7b56\u7565\u4e0e\u5b9e\u73b0\u3002", "result": "\u63d0\u51fa\u4e86VLA\u5728\u5404\u4e2a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5206\u6790\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u7840\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u4eff\u771f\u5e73\u53f0\u3002", "conclusion": "\u7efc\u8ff0\u6307\u51fa\u4e86VLA\u7814\u7a76\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u901a\u7528VLA\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u89c6\u89d2\u3002"}}
{"id": "2509.19023", "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ROM-GRL\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\uff0c\u65e0\u9700\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7ROM\u751f\u6210\u80fd\u91cf\u9ad8\u6548\u7684\u6b65\u6001\u6a21\u677f\uff0c\u518d\u5f15\u5bfc\u9ad8\u7ef4\u7b56\u7565\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u5bf9\u79f0\u7684\u6b65\u6001\u8868\u73b0\u4f18\u4e8e\u7eaf\u5956\u52b1\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u81ea\u7136\u6027\u3002", "method": "ROM-GRL\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u4f7f\u7528PPO\u8bad\u7ec34-DOF\u7684\u8f7b\u91cf\u7ea7ROM\u751f\u6210\u6b65\u6001\u6a21\u677f\uff1b2) \u901a\u8fc7\u5bf9\u6297\u5224\u522b\u5668\u5f15\u5bfcSAC\u8bad\u7ec3\u5168\u8eab\u4f53\u7b56\u7565\uff0c\u786e\u4fdd\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0eROM\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aROM-GRL\u57281-4\u7c73/\u79d2\u901f\u5ea6\u4e0b\u751f\u6210\u4e86\u7a33\u5b9a\u3001\u5bf9\u79f0\u7684\u6b65\u6001\uff0c\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u7eaf\u5956\u52b1\u57fa\u7ebf\u3002", "conclusion": "ROM-GRL\u7ed3\u5408\u4e86\u5956\u52b1\u548c\u6a21\u4eff\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u81ea\u7136\u3001\u901a\u7528\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u63a7\u5236\u3002"}}
{"id": "2509.19037", "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "categories": ["cs.RO"], "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTacEva\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u91cf\u5206\u6790\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VBTS\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VBTS\u5728\u4f20\u611f\u673a\u5236\u548c\u7ed3\u6784\u53c2\u6570\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u6807\u4f7f\u5176\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u548c\u9009\u62e9\u3002", "method": "TacEva\u6846\u67b6\u5b9a\u4e49\u4e86\u4e00\u5957\u6027\u80fd\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540cVBTS\u7684\u6027\u80fd\u3002", "result": "\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cdVBTS\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u6309\u4efb\u52a1\u9009\u62e9\u5408\u9002\u7684\u4f20\u611f\u5668\u5e76\u4f18\u5316\u8bbe\u8ba1\u3002", "conclusion": "TacEva\u4e3aVBTS\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u4f20\u611f\u5668\u9009\u62e9\u548c\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2509.19047", "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "ManipForce\u7cfb\u7edf\u901a\u8fc7\u6355\u6349\u9ad8\u9891\u529b\u626d\u77e9\u548cRGB\u6570\u636e\uff0c\u7ed3\u5408FMT\u6a21\u578b\u63d0\u5347\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6f14\u793a\uff0c\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u4ea4\u4e92\u529b\uff0c\u5c24\u5176\u662f\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u5982\u7cbe\u5bc6\u88c5\u914d\u4e2d\u3002", "method": "\u8bbe\u8ba1\u4e86ManipForce\u624b\u6301\u7cfb\u7edf\uff0c\u6355\u6349\u9ad8\u9891\u529b\u626d\u77e9\u548cRGB\u6570\u636e\uff1b\u63d0\u51faFMT\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u7387\u548c\u6a21\u6001\u611f\u77e5\u7684\u5d4c\u5165\u4ee5\u53ca\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u516d\u4e2a\u73b0\u5b9e\u4efb\u52a1\u4e2d\uff0cFMT\u7684\u5e73\u5747\u6210\u529f\u7387\u4e3a83%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8eRGB\u7684\u57fa\u7ebf\u3002", "conclusion": "\u9ad8\u9891\u529b\u626d\u77e9\u6570\u636e\u548c\u8de8\u6a21\u6001\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u9700\u6c42\u7684\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2509.19076", "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "categories": ["cs.RO"], "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "SlicerROS2\u662f\u4e00\u4e2a\u7ed3\u54083D Slicer\u548cROS\u7684\u8f6f\u4ef6\u6a21\u5757\uff0c\u65e8\u5728\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u96c6\u6210\u65b9\u6848\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5176\u65b0\u8bbe\u8ba1\u53ca\u56db\u4e2a\u5e94\u7528\u6848\u4f8b\u3002", "motivation": "\u901a\u8fc7\u6574\u54083D Slicer\u548cROS\uff0c\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u96c6\u6210\u5e73\u53f0\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u548c\u6539\u8fdb\u4e86SlicerROS2\u6a21\u5757\uff0c\u589e\u52a0\u4e86\u6a21\u5757\u5316\u3001\u4f4e\u5c42\u6b21\u529f\u80fd\u8bbf\u95ee\u3001Python API\u652f\u6301\u53ca\u6570\u636e\u4f20\u8f93\u534f\u8bae\u4f18\u5316\u3002", "result": "\u5c55\u793a\u4e86SlicerROS2\u6a21\u5757\u7684\u6838\u5fc3\u529f\u80fd\u5728\u56db\u4e2a\u771f\u5b9e\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "SlicerROS2\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6807\u51c6\u5316\u7684\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2509.19080", "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "World4RL\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4eff\u771f\u73af\u5883\u4e2d\u4f18\u5316\u9884\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u907f\u514d\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u6210\u672c\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u901a\u5e38\u4f9d\u8d56\u4e8e\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u4e13\u5bb6\u6570\u636e\u7684\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u6027\u80fd\uff1b\u5f3a\u5316\u5b66\u4e60\u867d\u53ef\u4f18\u5316\u7b56\u7565\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff0c\u4eff\u771f\u8bad\u7ec3\u5219\u5b58\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86World4RL\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728\u51bb\u7ed3\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5b8c\u5168\u4f18\u5316\u7b56\u7565\u3002\u8bbe\u8ba1\u4e86\u9002\u5408\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53cc\u70ed\u70b9\u52a8\u4f5c\u7f16\u7801\u65b9\u6848\u548c\u6269\u6563\u6a21\u578b\u4e3b\u5e72\u4ee5\u63d0\u9ad8\u5efa\u6a21\u4fdd\u771f\u5ea6\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWorld4RL\u80fd\u591f\u63d0\u4f9b\u9ad8\u4fdd\u771f\u7684\u73af\u5883\u5efa\u6a21\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u7b56\u7565\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u6a21\u4eff\u5b66\u4e60\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "World4RL\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4eff\u771f\u73af\u5883\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u7b56\u7565\u6539\u8fdb\u3002"}}
{"id": "2509.19102", "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon\u6846\u67b6\u901a\u8fc7\u5c06\u957f\u5468\u671f\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\uff0c\u5229\u7528\u529f\u80fd\u5bf9\u8c61\u5bf9\u9f50\u548c\u6269\u6563\u7b56\u7565\u63d0\u5347\u673a\u5668\u4eba\u6280\u80fd\u7684\u901a\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6f14\u793a\u4e2d\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u529f\u80fd\u5bf9\u8c61\u5bf9\u9f50\u548cFuncDiffuser\u6269\u6563\u7b56\u7565\uff0c\u5c06\u5bf9\u8c61\u6620\u5c04\u5230\u5171\u4eab\u529f\u80fd\u6846\u67b6\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u5883\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7c7b\u522b\u7ea7\u6cdb\u5316\u548c\u4efb\u52a1\u95f4\u884c\u4e3a\u91cd\u7528\u80fd\u529b\u3002", "conclusion": "\u529f\u80fd\u5bf9\u9f50\u4e3a\u590d\u6742\u64cd\u4f5c\u9886\u57df\u7684\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2509.19105", "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RS-Net\uff0c\u4e00\u79cd\u4eceRGB\u56fe\u50cf\u9884\u6d4b\u5149\u8c31\u7279\u5f81\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u6237\u5916\u5bfc\u822a\u4e2d\u5730\u5f62\u5206\u7c7b\u548c\u6469\u64e6\u7cfb\u6570\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u5149\u8c31\u4f20\u611f\u5668\u3002", "motivation": "\u6237\u5916\u5bfc\u822a\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u6216\u8bed\u4e49\u6807\u7b7e\u5206\u7c7b\u53ef\u901a\u884c\u8868\u9762\uff0c\u4f46\u65e0\u6cd5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6750\u8d28\u4e0d\u540c\u7684\u8868\u9762\u3002\u5149\u8c31\u4f20\u611f\u5668\u80fd\u63d0\u4f9b\u6750\u8d28\u4fe1\u606f\uff0c\u4f46\u53d7\u9650\u4e8e\u786c\u4ef6\u96c6\u6210\u548c\u9ad8\u6210\u672c\u3002", "method": "\u63d0\u51faRS-Net\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4eceRGB\u56fe\u50cf\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u8fdb\u4e00\u6b65\u6620\u5c04\u4e3a\u5730\u5f62\u6807\u7b7e\u548c\u6469\u64e6\u7cfb\u6570\uff0c\u5e94\u7528\u4e8e\u8f6e\u5f0f\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5b9e\u73b0\u4e86\u4ec5\u4f9d\u8d56RGB\u56fe\u50cf\u7684\u6750\u8d28\u4fe1\u606f\u9884\u6d4b\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "RS-Net\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u6237\u5916\u73af\u5883\u4e2d\u66f4\u6613\u83b7\u53d6\u6750\u8d28\u4fe1\u606f\u3002"}}
{"id": "2509.19142", "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "BiGraspFormer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefTransformer\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u7269\u4f53\u70b9\u4e91\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u534f\u8c03\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u81c2\u6293\u53d6\u65b9\u6cd5\u8981\u4e48\u4ec5\u5173\u6ce8\u5355\u81c2\u6293\u53d6\uff0c\u8981\u4e48\u91c7\u7528\u5206\u79bb\u7684\u6293\u53d6\u751f\u6210\u548c\u53cc\u81c2\u8bc4\u4f30\u9636\u6bb5\uff0c\u5bfc\u81f4\u78b0\u649e\u98ce\u9669\u548c\u4e0d\u5e73\u8861\u529b\u5206\u5e03\u7b49\u534f\u8c03\u95ee\u9898\u3002", "method": "BiGraspFormer\u901a\u8fc7Single-Guided Bimanual\uff08SGB\uff09\u7b56\u7565\uff0c\u9996\u5148\u751f\u6210\u591a\u6837\u7684\u5355\u6293\u53d6\u5019\u9009\uff0c\u7136\u540e\u5229\u7528\u5176\u5b66\u4e60\u7279\u5f81\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u9884\u6d4b\u53cc\u81c2\u59ff\u6001\u548c\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBiGraspFormer\u5728\u6027\u80fd\u548c\u901f\u5ea6\uff08<0.05\u79d2\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "BiGraspFormer\u80fd\u591f\u9ad8\u6548\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u4e3a\u590d\u6742\u7269\u4f53\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.19168", "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "categories": ["cs.RO"], "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u548c\u6eda\u52a8\u65f6\u57df\u7684\u591a\u6a21\u6001\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u4f18\u5316\u63d0\u5347\u9c81\u68d2\u6027\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u78b0\u649e\u907f\u514d\u548c\u5168\u5c40\u76ee\u6807\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u5728\u5c40\u90e8\u6700\u5c0f\u503c\u548c\u63a2\u7d22\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u6269\u5c55\u5230\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7684\u573a\u666f\u3002", "method": "\u4f7f\u7528\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u548c\u6eda\u52a8\u65f6\u57df\u89c4\u5212\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "result": "\u4eff\u771f\u8868\u660e\u591a\u6a21\u6001\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u9677\u9631\u73af\u5883\u548c\u591a\u673a\u5668\u4eba\u907f\u969c\u7684\u6210\u529f\u7387\uff1b\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7b56\u7565\u4f18\u5316\u548c\u591a\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.19169", "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "categories": ["cs.RO"], "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "MagiClaw\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u53cc\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u901a\u8fc7\u8f6f\u591a\u9762\u4f53\u7f51\u7edc\u548c\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u5b9e\u73b06\u81ea\u7531\u5ea6\u529b\u53ca\u63a5\u89e6\u5f62\u53d8\u7684\u89c6\u89c9\u4f30\u8ba1\uff0c\u7ed3\u5408iPhone\u7684\u73af\u5883\u611f\u77e5\uff0c\u964d\u4f4e\u63a5\u89e6\u5bc6\u96c6\u578b\u6570\u636e\u96c6\u6536\u96c6\u95e8\u69db\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u6267\u884c\u4e4b\u95f4\u56e0\u4f20\u611f\u548c\u5f62\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u9886\u57df\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1MagiClaw\u672b\u7aef\u6267\u884c\u5668\uff0c\u7ed3\u5408\u8f6f\u591a\u9762\u4f53\u7f51\u7edc\u3001\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u548ciPhone\u7684\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\uff0c\u652f\u6301\u5b9e\u65f6\u9065\u64cd\u4f5c\u548c\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u6210\u529f\u6784\u5efa\u7edf\u4e00\u7cfb\u7edf\u67b6\u6784\uff0c\u63d0\u5347\u9ad8\u4fdd\u771f\u63a5\u89e6\u5bc6\u96c6\u578b\u6570\u636e\u96c6\u7684\u6536\u96c6\u6548\u7387\uff0c\u52a0\u901f\u901a\u7528\u64cd\u4f5c\u7b56\u7565\u5f00\u53d1\u3002", "conclusion": "MagiClaw\u901a\u8fc7\u786c\u4ef6\u4e00\u81f4\u6027\u548c\u591a\u6a21\u6001\u611f\u77e5\uff0c\u6709\u6548\u5f25\u5408\u4e86\u4eba\u673a\u6280\u80fd\u8f6c\u79fb\u7684\u9886\u57df\u9e3f\u6c9f\u3002"}}
{"id": "2509.19246", "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u65b0\u578b\u4e3b\u52a8-\u88ab\u52a8\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u5229\u7528\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u8def\u590d\u7528\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6765\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u95f4\u6b47\u6027\u6545\u969c\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u53ef\u9760\u6027\u548c\u534f\u8c03\u6027\u6784\u6210\u6311\u6218\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\uff0c\u800c\u95f4\u6b47\u6027\u6545\u969c\u68c0\u6d4b\u5728\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u5c24\u4e3a\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8-\u88ab\u52a8\u7b56\u7565\uff1a\u4e3b\u52a8\u81ea\u7ec4\u7ec7\u52a8\u6001\u5907\u4efd\u8def\u5f84\uff0c\u88ab\u52a8\u4f7f\u7528\u5355\u6b21\u4f3c\u7136\u6bd4\u6d4b\u8bd5\u68c0\u6d4b\u6545\u969c\uff0c\u5e76\u901a\u8fc7\u81ea\u7ec4\u7ec7\u65b9\u5f0f\u4e34\u65f6\u91cd\u5b9a\u5411\u901a\u4fe1\u3002", "result": "\u5728\u4ee3\u8868\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u95f4\u6b47\u6027\u6545\u969c\u672a\u7834\u574f\u7fa4\u4f53\u5f62\u6210\u63a7\u5236\u7684\u6536\u655b\u6027\uff0c\u4e14\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\u3001\u8bef\u62a5\u7387\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u95f4\u6b47\u6027\u6545\u969c\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u6301\u4e45\u7f51\u7edc\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.19261", "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u81c2\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u6548\u6293\u53d6\u8fc7\u6e21\u7b56\u7565\u548c\u8fd0\u52a8\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8981\u65e0\u7f1d\u5207\u6362\u6293\u53d6\u65b9\u5f0f\u4ee5\u4fdd\u6301\u7a33\u5b9a\u548c\u9ad8\u6548\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u5916\u529b\u548c\u590d\u6742\u8fd0\u52a8\u7ea6\u675f\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5f15\u5bfc\u7684\u7b56\u7565\u91c7\u6837\u7a33\u5b9a\u6293\u53d6\u6d41\u5f62\u4ea4\u70b9\uff0c\u7ed3\u5408\u5206\u5c42\u53cc\u9636\u6bb5\u8fd0\u52a8\u67b6\u6784\uff08\u5168\u5c40\u8def\u5f84\u751f\u6210\u5668\u548c\u5c40\u90e8\u89c4\u5212\u5668\uff09\u3002", "result": "\u5728\u529b\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u8fc7\u6e21\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u6293\u53d6\u8fc7\u6e21\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.19292", "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "SOE \u662f\u4e00\u4e2a\u901a\u8fc7\u9650\u5236\u63a2\u7d22\u5728\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\uff0c\u63d0\u5347\u673a\u5668\u4eba\u653f\u7b56\u63a2\u7d22\u548c\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u786e\u4fdd\u5b89\u5168\u3001\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u9f13\u52b1\u63a2\u7d22\uff0c\u4e0d\u5b89\u5168\u4e14\u4e0d\u7a33\u5b9a\u3002SOE \u65e8\u5728\u901a\u8fc7\u6d41\u5f62\u7ea6\u675f\u63a2\u7d22\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SOE \u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u56e0\u7d20\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u63a2\u7d22\u9650\u5236\u5728\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u610f\u653f\u7b56\u6a21\u578b\u4e2d\u3002", "result": "SOE \u5728\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u5b89\u5168\u7684\u63a2\u7d22\u548c\u66f4\u4f18\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "SOE \u8bc1\u660e\u4e86\u6d41\u5f62\u63a2\u7d22\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6837\u672c\u653f\u7b56\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.19301", "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6b8b\u5dee\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3RL\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u4e2d\u7684\u6548\u7387\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3\u884c\u4e3a\u514b\u9686\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u3001\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528BC\u7b56\u7565\u4f5c\u4e3a\u57fa\u7840\uff0c\u5b66\u4e60\u8f7b\u91cf\u7ea7\u7684\u6b8b\u5dee\u4fee\u6b63\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u79bb\u7b56\u7565RL\u8bad\u7ec3\u3002", "result": "\u65b9\u6cd5\u4ec5\u9700\u7a00\u758f\u4e8c\u8fdb\u5236\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9996\u6b21\u5b9e\u73b0\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u771f\u5b9e\u4e16\u754cRL\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u5b9e\u7528\u7684RL\u90e8\u7f72\u8def\u5f84\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18198", "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6MMCD\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u534f\u4f5c\u51b3\u7b56\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4f20\u611f\u5668\u6216\u534f\u4f5c\u8f66\u8f86\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e8b\u6545\u591a\u53d1\u73af\u5883\u4e2d\u9762\u4e34\u4f20\u611f\u5668\u89c6\u91ce\u53d7\u9650\u548c\u534f\u4f5c\u8f66\u8f86\u7f3a\u5931\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6240\u6709\u6a21\u6001\u6570\u636e\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e2d\u5747\u53ef\u7528\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u7684MMCD\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u8f66\u8f86\u7684\u6570\u636e\u878d\u5408\u589e\u5f3a\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u5e08\u751f\u6a21\u578b\u7ed3\u6784\uff0c\u786e\u4fdd\u5728\u90e8\u5206\u6570\u636e\u7f3a\u5931\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u7a7a\u5730\u8f66\u8f86\u534f\u4f5c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u9a7e\u9a76\u5b89\u5168\u6027\u63d0\u9ad8\u4e8620.7%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "MMCD\u6846\u67b6\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2509.18200", "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u7a7a\u95f4\u5b9a\u5411\u7684MCoT\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5728\u65e0GPS\u548c\u8be6\u7ec6\u5730\u56fe\u7684\u590d\u6742\u73af\u5883\u4e2d\u7531\u81ea\u6211\u4e2d\u5fc3\u5230\u4ed6\u4eba\u4e2d\u5fc3\u7684\u5b9a\u5411\u95ee\u9898\uff0c\u5e76\u5728\u53f0\u6e7eLLM-13B\u6a21\u578b\u4e0a\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5728GPS\u4fe1\u53f7\u5f31\u4e14\u7f3a\u4e4f\u8be6\u7ec6\u5730\u56fe\u7684\u5ba4\u5185\u6216\u590d\u6742\u73af\u5883\u4e2d\uff0c\u4f1a\u8bdd\u4ee3\u7406\u9700\u5c06\u81ea\u6211\u4e2d\u5fc3\u8868\u8fbe\u8f6c\u6362\u4e3a\u4ed6\u4eba\u4e2d\u5fc3\u5b9a\u5411\uff0c\u4f46\u76ee\u524d\u591a\u6a21\u6001\u7a7a\u95f4\u5b9a\u5411\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u7ef4\uff08MCoT\uff09\u6846\u67b6\uff0c\u7ed3\u5408ASR\u8f6c\u5f55\u8bed\u97f3\u548c\u5730\u6807\u5750\u6807\uff0c\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\u8fc7\u7a0b\uff08\u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb\u3001\u6620\u5c04\u5750\u6807\u5230\u7edd\u5bf9\u65b9\u5411\u3001\u63a8\u65ad\u7528\u6237\u65b9\u5411\uff09\u5b9e\u73b0\u5b9a\u5411\u3002", "result": "MCoT\u5728\u5e72\u51c0\u8f6c\u5f55\u672c\u4e0a\u5b9e\u73b0100%\u5b9a\u5411\u51c6\u786e\u7387\uff0cASR\u8f6c\u5f55\u672c\u4e0a\u8fbe98.1%\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u975e\u7ed3\u6784\u5316\u57fa\u7ebf\uff0c\u4e14\u5728\u566a\u58f0\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316MCoT\u7a7a\u95f4\u63a8\u7406\u4e3a\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2509.18224", "pdf": "https://arxiv.org/pdf/2509.18224", "abs": "https://arxiv.org/abs/2509.18224", "authors": ["Svyatoslav Covanov", "Cedric Pradalier"], "title": "Reversible Kalman Filter for state estimation with Manifold", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "This work introduces an algorithm for state estimation on manifolds within\nthe framework of the Kalman filter. Its primary objective is to provide a\nmethodology enabling the evaluation of the precision of existing Kalman filter\nvariants with arbitrary accuracy on synthetic data, something that, to the best\nof our knowledge, has not been addressed in prior work. To this end, we develop\na new filter that exhibits favorable numerical properties, thereby correcting\nthe divergences observed in previous Kalman filter variants. In this\nformulation, the achievable precision is no longer constrained by the\nsmall-velocity assumption and is determined solely by sensor noise. In\naddition, this new filter assumes high precision on the sensors, which, in real\nscenarios require a detection step that we define heuristically, allowing one\nto extend this approach to scenarios, using either a 9-axis IMU or a\ncombination of odometry, accelerometer, and pressure sensors. The latter\nconfiguration is designed for the reconstruction of trajectories in underwater\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\u4e0b\u5904\u7406\u6d41\u5f62\u72b6\u6001\u4f30\u8ba1\u7684\u7b97\u6cd5\uff0c\u65e8\u5728\u4e3a\u5408\u6210\u6570\u636e\u4e2d\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u7684\u7cbe\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u65b9\u6cd5\uff0c\u5e76\u4fee\u6b63\u4e86\u5148\u524d\u53d8\u4f53\u7684\u53d1\u6563\u95ee\u9898\u3002", "motivation": "\u8be5\u5de5\u4f5c\u7684\u4e3b\u8981\u52a8\u673a\u662f\u4e3a\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u7cbe\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002\u6b64\u5916\uff0c\u8fd8\u5e0c\u671b\u89e3\u51b3\u5148\u524d\u53d8\u4f53\u4e2d\u7684\u6570\u503c\u53d1\u6563\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ee4\u6ce2\u5668\uff0c\u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u7279\u6027\uff0c\u6d88\u9664\u4e86\u5148\u524d\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u4e2d\u7684\u53d1\u6563\u95ee\u9898\u3002\u65b0\u6ee4\u6ce2\u5668\u4e0d\u518d\u53d7\u9650\u4e8e\u5c0f\u901f\u5ea6\u5047\u8bbe\uff0c\u7cbe\u5ea6\u4ec5\u7531\u4f20\u611f\u5668\u566a\u58f0\u51b3\u5b9a\u3002\u6b64\u5916\uff0c\u6ee4\u6ce2\u5668\u5047\u8bbe\u4f20\u611f\u5668\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u68c0\u6d4b\u6b65\u9aa4\u6269\u5c55\u81f3\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "result": "\u65b0\u6ee4\u6ce2\u5668\u5728\u5408\u6210\u6570\u636e\u4e0a\u80fd\u591f\u4ee5\u4efb\u610f\u7cbe\u5ea6\u8bc4\u4f30\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u6539\u5584\u4e86\u6570\u503c\u7a33\u5b9a\u6027\u3002\u9002\u7528\u4e8e9\u8f74IMU\u6216\u7ec4\u5408\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u5c24\u5176\u662f\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u91cd\u5efa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\u7684\u7cbe\u5ea6\u8bc4\u4f30\u95ee\u9898\uff0c\u8fd8\u901a\u8fc7\u6539\u8fdb\u6570\u503c\u7279\u6027\u548c\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18350", "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "OrthoLoC\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u56fe\u50cf\u4e0e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u4e4b\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u6a21\u6001\u768416245\u5f20\u56fe\u50cf\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff08\u5982\u65e0\u4e92\u8054\u7f51\u6216GNSS/GPS\u652f\u6301\uff09\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u89c6\u89c9\u5b9a\u4f4d\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u578b\u56fe\u50cf\u6570\u636e\u5e93\u6216\u91cd\u578b3D\u6a21\u578b\uff0c\u800c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u4e86OrthoLoC\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e86\u6765\u81ea\u5fb7\u56fd\u548c\u7f8e\u56fd\u768416245\u5f20\u65e0\u4eba\u673a\u56fe\u50cf\uff0c\u652f\u6301\u591a\u6a21\u6001\u3002\u901a\u8fc7\u89e3\u8026\u56fe\u50cf\u68c0\u7d22\u548c\u7279\u5f81\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u5b9a\u4f4d\u548c\u6821\u51c6\u6027\u80fd\u7684\u72ec\u7acb\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86AdHoP\u7ec6\u5316\u6280\u672f\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u7279\u5f81\u5339\u914d\u5668\u4e2d\u3002", "result": "AdHoP\u6280\u672f\u63d0\u9ad8\u4e86\u5339\u914d\u6027\u80fd\uff08\u9ad8\u8fbe95%\uff09\u5e76\u964d\u4f4e\u4e86\u5e73\u79fb\u8bef\u5dee\uff08\u9ad8\u8fbe63%\uff09\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "OrthoLoC\u4e3a\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.18371", "pdf": "https://arxiv.org/pdf/2509.18371", "abs": "https://arxiv.org/abs/2509.18371", "authors": ["Eduardo Sebasti\u00e1n", "Maitrayee Keskar", "Eeman Iqbal", "Eduardo Montijano", "Carlos Sag\u00fc\u00e9s", "Nikolay Atanasov"], "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": null, "summary": "Multi-agent games in dynamic nonlinear settings are challenging due to the\ntime-varying interactions among the agents and the non-stationarity of the\n(potential) Nash equilibria. In this paper we consider model-free games, where\nagent transitions and costs are observed without knowledge of the transition\nand cost functions that generate them. We propose a policy gradient approach to\nlearn distributed policies that follow the communication structure in\nmulti-team games, with multiple agents per team. Our formulation is inspired by\nthe structure of distributed policies in linear quadratic games, which take the\nform of time-varying linear feedback gains. In the nonlinear case, we model the\npolicies as nonlinear feedback gains, parameterized by self-attention layers to\naccount for the time-varying multi-agent communication topology. We demonstrate\nthat our distributed policy gradient approach achieves strong performance in\nseveral settings, including distributed linear and nonlinear regulation, and\nsimulated and real multi-robot pursuit-and-evasion games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5206\u5e03\u5f0f\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u56e2\u961f\u6e38\u620f\u4e2d\u975e\u7ebf\u6027\u52a8\u6001\u73af\u5883\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u975e\u7ebf\u6027\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u6e38\u620f\u56e0\u65f6\u95f4\u53d8\u5316\u7684\u4ea4\u4e92\u548c\u975e\u7a33\u6001\u7eb3\u4ec0\u5747\u8861\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u81ea\u6ce8\u610f\u529b\u5c42\u53c2\u6570\u5316\u975e\u7ebf\u6027\u53cd\u9988\u589e\u76ca\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u6a21\u62df\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u62d3\u6251\u3002", "result": "\u5728\u5206\u5e03\u5f0f\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8c03\u8282\u4ee5\u53ca\u591a\u673a\u5668\u4eba\u8ffd\u6355-\u9003\u907f\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e2d\u6709\u6548\uff0c\u4e3a\u975e\u7ebf\u6027\u591a\u667a\u80fd\u4f53\u6e38\u620f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18566", "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u52a8\u6001\u4eba\u7c7b\u4e0e\u9759\u6001\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u8054\u5408\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e8b\u4ef6\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u5347\u6a21\u7cca\u533a\u57df\u7684\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u76ee\u89c6\u9891\u5728\u5feb\u901f\u8fd0\u52a8\u4e0b\u6613\u53d7\u8fd0\u52a8\u6a21\u7cca\u5f71\u54cd\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u66f4\u9002\u5408\u52a8\u6001\u91cd\u5efa\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u51fa\u4e86\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u76843D\u9ad8\u65af\u96c6\u5408\uff0c\u5176\u4e2d\u4eba\u7c7b\u9ad8\u65af\u5177\u6709\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\u5e76\u901a\u8fc7\u53d8\u5f62\u5b9e\u73b0\u52a8\u753b\uff1b\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u635f\u5931\u4ee5\u5339\u914d\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cPSNR/SSIM\u6307\u6807\u663e\u8457\u63d0\u5347\uff0cLPIPS\u964d\u4f4e\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u901f\u8fd0\u52a8\u5bf9\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u7c7b\u63a9\u7801\uff0c\u7b80\u5316\u4e86\u9ad8\u65af\u96c6\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u91cd\u5efa\u4e2d\u7684\u6a21\u7cca\u95ee\u9898\u3002"}}
{"id": "2509.18723", "pdf": "https://arxiv.org/pdf/2509.18723", "abs": "https://arxiv.org/abs/2509.18723", "authors": ["Jan-Hendrik Ewering", "Alessandro Papa", "Simon F. G. Ehlers", "Thomas Seel", "Michael Meindl"], "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "11 pages, 4 figures", "summary": "Solving motion tasks autonomously and accurately is a core ability for\nintelligent real-world systems. To achieve genuine autonomy across multiple\nsystems and tasks, key challenges include coping with unknown dynamics and\novercoming the need for manual parameter tuning, which is especially crucial in\ncomplex Multiple-Input Multiple-Output (MIMO) systems.\n  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel\ndata-driven iterative learning scheme for simultaneous tracking control and\nmodel learning, without requiring any prior system knowledge or manual\nparameter tuning. The method is designed for repetitive MIMO systems and\nintegrates seamlessly with established iterative learning control methods. We\nprovide monotonic convergence conditions for both reference tracking error and\nmodel error in linear time-invariant systems.\n  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in\nhigh-fidelity simulations of an industrial robot and in multiple nonlinear\nreal-world MIMO systems, without requiring model knowledge or manually tuning\nthe algorithm. In our experiments, many reference tracking tasks are solved\nwithin 10-20 trials, and even complex motions are learned in less than 100\niterations. We believe that, because of its rapid and autonomous learning\ncapabilities, DILC has the potential to serve as an efficient building block\nwithin complex learning frameworks for intelligent real-world systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIMO Dual Iterative Learning Control (DILC)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u9700\u5148\u9a8c\u7cfb\u7edf\u77e5\u8bc6\u6216\u624b\u52a8\u8c03\u53c2\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8ddf\u8e2a\u63a7\u5236\u548c\u6a21\u578b\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u7cfb\u7edf\u5728\u590d\u6742MIMO\u7cfb\u7edf\u4e2d\u672a\u77e5\u52a8\u6001\u548c\u624b\u52a8\u8c03\u53c2\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u6027\u3002", "method": "DILC\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u91cd\u590d\u6027MIMO\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u4e2d\u8bef\u5dee\u5355\u8c03\u6536\u655b\u7684\u6761\u4ef6\u3002", "result": "\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4eff\u771f\u548c\u5b9e\u9645\u975e\u7ebf\u6027MIMO\u7cfb\u7edf\u4e2d\uff0cDILC\u80fd\u572810-20\u6b21\u8bd5\u9a8c\u5185\u5b8c\u6210\u4efb\u52a1\uff0c\u590d\u6742\u4efb\u52a1\u4e5f\u5c11\u4e8e100\u6b21\u8fed\u4ee3\u3002", "conclusion": "DILC\u56e0\u5176\u5feb\u901f\u81ea\u4e3b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u6709\u671b\u6210\u4e3a\u590d\u6742\u5b66\u4e60\u6846\u67b6\u7684\u9ad8\u6548\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2509.18749", "pdf": "https://arxiv.org/pdf/2509.18749", "abs": "https://arxiv.org/abs/2509.18749", "authors": ["Maxwell M. Varley", "Timothy L. Molloy", "Girish N. Nair"], "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "8 pages", "summary": "This article examines state estimation in discrete-time nonlinear stochastic\nsystems with finite-dimensional states and infinite-dimensional measurements,\nmotivated by real-world applications such as vision-based localization and\ntracking. We develop an extended Kalman filter (EKF) for real-time state\nestimation, with the measurement noise modeled as an infinite-dimensional\nrandom field. When applied to vision-based state estimation, the measurement\nJacobians required to implement the EKF are shown to correspond to image\ngradients. This result provides a novel system-theoretic justification for the\nuse of image gradients as features for vision-based state estimation,\ncontrasting with their (often heuristic) introduction in many computer-vision\npipelines. We demonstrate the practical utility of the EKF on a public\nreal-world dataset involving the localization of an aerial drone using video\nfrom a downward-facing monocular camera. The EKF is shown to outperform\nVINS-MONO, an established visual-inertial odometry algorithm, in some cases\nachieving mean squared error reductions of up to an order of magnitude.", "AI": {"tldr": "\u7814\u7a76\u4e86\u79bb\u6563\u65f6\u95f4\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u5e94\u7528\u4e8e\u89c6\u89c9\u5b9a\u4f4d\u65f6\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u548c\u8ddf\u8e2a\uff09\uff0c\u7814\u7a76\u6709\u9650\u7ef4\u72b6\u6001\u4e0e\u65e0\u9650\u7ef4\u6d4b\u91cf\u7684\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\uff0c\u5c06\u6d4b\u91cf\u566a\u58f0\u5efa\u6a21\u4e3a\u65e0\u9650\u7ef4\u968f\u673a\u573a\uff0c\u5e76\u8bc1\u660e\u6d4b\u91cf\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u5e94\u4e8e\u56fe\u50cf\u68af\u5ea6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEKF\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8eVINS-MONO\u7b97\u6cd5\uff0c\u8bef\u5dee\u51cf\u5c11\u4e86\u82e5\u5e72\u6570\u91cf\u7ea7\u3002", "conclusion": "EKF\u4e3a\u56fe\u50cf\u68af\u5ea6\u5728\u89c6\u89c9\u72b6\u6001\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2509.18760", "pdf": "https://arxiv.org/pdf/2509.18760", "abs": "https://arxiv.org/abs/2509.18760", "authors": ["Antoine P. Leeman", "Johannes K\u00f6hler", "Melanie N. Zeilinger"], "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback", "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "comment": "Code: https://github.com/antoineleeman/robust-nonlinear-mpc", "summary": "Robots must satisfy safety-critical state and input constraints despite\ndisturbances and model mismatch. We introduce a robust model predictive control\n(RMPC) formulation that is fast, scalable, and compatible with real-time\nimplementation. Our formulation guarantees robust constraint satisfaction,\ninput-to-state stability (ISS) and recursive feasibility. The key idea is to\ndecompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic\nmodel, (ii) disturbance-feedback controllers, and (iii) bounds on the model\nerror. These components are optimized jointly using sequential convex\nprogramming. The resulting convex subproblems are solved efficiently using a\nrecent disturbance-feedback MPC solver. The approach is validated across\nmultiple dynamics, including a rocket-landing problem with steerable thrust. An\nopen-source implementation is available at\nhttps://github.com/antoineleeman/robust-nonlinear-mpc.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u4e14\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u7684\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RMPC\uff09\u65b9\u6cd5\uff0c\u786e\u4fdd\u5728\u5e72\u6270\u548c\u6a21\u578b\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u5e72\u6270\u548c\u6a21\u578b\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u4ecd\u9700\u6ee1\u8db3\u5173\u952e\u7684\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u5c06\u4e0d\u786e\u5b9a\u975e\u7ebf\u6027\u7cfb\u7edf\u5206\u89e3\u4e3a\u540d\u4e49\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u3001\u5e72\u6270\u53cd\u9988\u63a7\u5236\u5668\u548c\u6a21\u578b\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u987a\u5e8f\u51f8\u89c4\u5212\u8054\u5408\u4f18\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u5305\u62ec\u706b\u7bad\u7740\u9646\u95ee\u9898\u3002\u5f00\u6e90\u5b9e\u73b0\u5df2\u53d1\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fdd\u8bc1\u4e86\u9c81\u68d2\u7ea6\u675f\u6ee1\u8db3\u3001\u8f93\u5165-\u72b6\u6001\u7a33\u5b9a\u6027\uff08ISS\uff09\u548c\u9012\u5f52\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.19110", "pdf": "https://arxiv.org/pdf/2509.19110", "abs": "https://arxiv.org/abs/2509.19110", "authors": ["Chenxu Ke", "Congling Tian", "Kaichen Xu", "Ye Li", "Lingcong Bao"], "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "comment": null, "summary": "Reinforcement learning-based controller design methods often require\nsubstantial data in the initial training phase. Moreover, the training process\ntends to exhibit strong randomness and slow convergence. It often requires\nconsiderable time or high computational resources. Another class of\nlearning-based method incorporates Lyapunov stability theory to obtain a\ncontrol policy with stability guarantees. However, these methods generally\nrequire an initially stable neural network control policy at the beginning of\ntraining. Evidently, a stable neural network controller can not only serve as\nan initial policy for reinforcement learning, allowing the training to focus on\nimproving controller performance, but also act as an initial state for\nlearning-based Lyapunov control methods. Although stable controllers can be\ndesigned using traditional control theory, designers still need to have a great\ndeal of control design knowledge to address increasingly complicated control\nproblems. The proposed neural network rapid initialization method in this paper\nachieves the initial training of the neural network control policy by\nconstructing datasets that conform to the stability conditions based on the\nsystem model. Furthermore, using the image-based visual servoing control for\nmulticopter interception as a case study, simulations and experiments were\nconducted to validate the effectiveness and practical performance of the\nproposed method. In the experiment, the trained control policy attains a final\ninterception velocity of 15 m/s.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u521d\u59cb\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7b26\u5408\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6570\u636e\u96c6\u6765\u8bbe\u8ba1\u521d\u59cb\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u521d\u671f\u6570\u636e\u9700\u6c42\u5927\u3001\u6536\u655b\u6162\u7b49\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\u5728\u521d\u59cb\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u968f\u673a\u6027\u9ad8\u3001\u6536\u655b\u6162\u3002\u53e6\u5916\uff0c\u57fa\u4e8eLyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u7684\u65b9\u6cd5\u9700\u8981\u521d\u59cb\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u5feb\u901f\u521d\u59cb\u5316\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5feb\u901f\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7b26\u5408\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u521d\u59cb\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u7cfb\u7edf\u6a21\u578b\uff0c\u786e\u4fdd\u7b56\u7565\u6ee1\u8db3\u7a33\u5b9a\u6027\u8981\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8bad\u7ec3\u7684\u63a7\u5236\u7b56\u7565\u5728\u65e0\u4eba\u673a\u62e6\u622a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8615 m/s\u7684\u6700\u7ec8\u62e6\u622a\u901f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u521d\u59cb\u5316\u7a33\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u63a7\u5236\u8bbe\u8ba1\u77e5\u8bc6\u7684\u9ad8\u8981\u6c42\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u548cLyapunov\u63a7\u5236\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
