{"id": "2510.09940", "pdf": "https://arxiv.org/pdf/2510.09940", "abs": "https://arxiv.org/abs/2510.09940", "authors": ["Haytham Albousayri", "Bechir Hamdaoui", "Weng-Keen Wong", "Nora Basha"], "title": "Bluetooth Fingerprint Identification Under Domain Shift Through Transient Phase Derivative", "categories": ["eess.SP", "cs.CR"], "comment": "9 pages, IEEE CNS 2025", "summary": "Deep learning-based radio frequency fingerprinting (RFFP) has become an\nenabling physical-layer security technology, allowing device identification and\nauthentication through received RF signals. This technology, however, faces\nsignificant challenges when it comes to adapting to domain variations, such as\ntime, location, environment, receiver and channel. For Bluetooth Low Energy\n(BLE) devices, addressing these challenges is particularly crucial due to the\nBLE protocol's frequency-hopping nature. In this work, and for the first time,\nwe investigated the frequency hopping effect on RFFP of BLE devices, and\nproposed a novel, low-cost, domain-adaptive feature extraction method. Our\napproach improves the classification accuracy by up to 58\\% across environments\nand up to 80\\% across receivers compared to existing benchmarks."}
{"id": "2510.09949", "pdf": "https://arxiv.org/pdf/2510.09949", "abs": "https://arxiv.org/abs/2510.09949", "authors": ["Ran Yang", "Zheng Dong", "Peng Cheng", "Lin Zhang", "Wanting Lyu", "Yue Xiu", "Ning Wei", "Chadi Assi"], "title": "Movable Antenna Enhanced Covert Dual-Functional Radar-Communication: Joint Beamforming and Antenna Position Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Movable antenna (MA) has emerged as a promising technology to flexibly\nreconfigure wireless channels by adjusting antenna placement. In this paper, we\nstudy a dual-functional radar-communication (DFRC) system enhanced with movable\nantennas. To ensure communication security, we aim to maximize the achievable\nsum rate by jointly optimizing the transmit beamforming vectors, receiving\nfilter, and antenna placement, subject to radar signal-to-noise ratio (SNR)\nperformance and transmission covertness constraints. To tackle this challenging\noptimization problem, we first employ a Lagrangian dual transformation process\nto reformulate it into a more tractable form. Subsequently, the problem is\nsolved by introducing a block coordinate descent (BCD) algorithm, incorporating\nsemidefinite relaxation (SDR), projected gradient descent (PGD), and successive\nconvex approximation (SCA) techniques. Simulation results demonstrate that the\nproposed method can significantly improve the covert sum rate, and achieve a\nsatisfactory balance between the communication and radar performance compared\nwith existing benchmark schemes by leveraging the flexibility of movable\nantennas."}
{"id": "2510.10045", "pdf": "https://arxiv.org/pdf/2510.10045", "abs": "https://arxiv.org/abs/2510.10045", "authors": ["Qiaoyan Peng", "Qingqing Wu", "Guangji Chen", "Wen Chen", "Shaodan Ma"], "title": "Active IRS Assisted Joint Uplink and Downlink Communications", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we investigate an intelligent reflecting surface (IRS) aided\nwireless communication system, where active IRSs (AIRSs) are deployed to assist\ncommunication between a base station (BS) and users of both the uplink (UL) and\ndownlink (DL). We aim to maximize the weighted sum rate (WSR) of UL and DL\ncommunications through joint optimization of BS, AIRS beamforming, and AIRS\nelement allocation. First, we study three deployment schemes, namely\ndistributed AIRSs, BS-side AIRS, and user-side AIRS. For distributed AIRSs,\nboth optimal and near-optimal solutions are derived in closed form. To draw\nuseful insights, we analytically compare the deployment schemes in terms of the\nrate performance under the single-user setup. For the multi-user case, we\nconsider two beamforming setups at the distributed AIRSs to balance performance\nand complexity tradeoffs. Regarding the user-adaptive AIRS beamforming,\ndifferent AIRS beamforming vectors are adopted for each user; while for the\nstatic AIRS beamforming, all users share the same beamforming vectors, with\nidentical phase shifts but different amplitudes for UL and DL. With the\nuser-adaptive AIRS beamforming, we focus on the optimization of element\nallocation for rate maximization. With static AIRS beamforming, we solve the\nrate maximization problem by optimizing the BS transmit/receive beamformers,\nuser beamforming, and AIRS beamforming. Despite its non-convexity, we develop\nan efficient alternating optimization (AO) based algorithm that solves each\nsub-problem optimally. Numerical results validate the practical advantages of\ndistributed AIRSs compared to passive IRS (PIRS), BS-side AIRS, and user-side\nAIRS, and highlight the benefits of dynamic IRS beamforming."}
{"id": "2510.10235", "pdf": "https://arxiv.org/pdf/2510.10235", "abs": "https://arxiv.org/abs/2510.10235", "authors": ["Jinpeng Xu", "Shuowen Zhang"], "title": "MIMO Radar Meets Polarization-Reconfigurable Antennas: A BCRB Perspective", "categories": ["eess.SP"], "comment": "To appear in Proc. IEEE Global Communications Conference (Globecom)\n  Workshops, 2025", "summary": "In this paper, we investigate a novel multiple-input multiple-output (MIMO)\nradar system aided by phase shifter based polarization-reconfigurable antennas\n(PRAs). Specifically, a base station (BS) equipped with multiple PRAs at both\nthe transmitter and the receiver aims to sense the unknown and random angular\nlocation parameter of a point target via sending wireless signals and\nprocessing the received echo signals reflected by the target, where only prior\ndistribution information about the location parameter is available for\nexploitation. Firstly, we characterize the sensing performance of this novel\nPRA-based MIMO radar system by deriving the Bayesian Cram\\'er-Rao bound (BCRB)\nof the mean-squared error (MSE) in estimating the desired location parameter\nwith prior distribution information. Then, to fully exploit the new design\ndegrees-of-freedom (DoF) empowered by PRAs, we study the joint optimization of\nthe transmit sample covariance matrix as well as the transmit and receive phase\nshift vectors to minimize the sensing BCRB subject to a transmit power\nconstraint. This problem is non-convex and difficult to solve due to the\ncoupling among optimization variables. To resolve this issue, we develop an\nalternating optimization (AO) based algorithm which iteratively obtains the\nclosed-form optimal solution to each variable with the others being fixed at\neach time, thus being guaranteed to converge to at least a stationary point of\nthe joint optimization problem. Numerical results validate the effectiveness of\nthe proposed algorithm."}
{"id": "2510.09786", "pdf": "https://arxiv.org/pdf/2510.09786", "abs": "https://arxiv.org/abs/2510.09786", "authors": ["Yuang Lu", "Song Wang", "Xiao Han", "Xuri Zhang", "Yucong Wu", "Zhicheng He"], "title": "Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks", "categories": ["cs.RO"], "comment": "7 pages, 7 figures", "summary": "Temporal sequential tasks challenge humanoid robots, as existing Diffusion\nPolicy (DP) and Action Chunking with Transformers (ACT) methods often lack\ntemporal context, resulting in local optima traps and excessive repetitive\nactions. To address these issues, this paper introduces a Classifier-Free\nGuidance-Based Diffusion Policy (CFG-DP), a novel framework to enhance DP by\nintegrating Classifier-Free Guidance (CFG) with conditional and unconditional\nmodels. Specifically, CFG leverages timestep inputs to track task progression\nand ensure precise cycle termination. It dynamically adjusts action predictions\nbased on task phase, using a guidance factor tuned to balance temporal\ncoherence and action accuracy. Real-world experiments on a humanoid robot\ndemonstrate high success rates and minimal repetitive actions. Furthermore, we\nassessed the model's ability to terminate actions and examined how different\ncomponents and parameter adjustments affect its performance. This framework\nsignificantly enhances deterministic control and execution reliability for\nsequential robotic tasks."}
{"id": "2510.10438", "pdf": "https://arxiv.org/pdf/2510.10438", "abs": "https://arxiv.org/abs/2510.10438", "authors": ["Shuixin Li", "Jiecheng Chen", "Qingtang Jiang", "Jian Lu"], "title": "Synchrosqueezed windowed linear canonical transform: A method for mode retrieval from multicomponent signals with crossing instantaneous frequencies", "categories": ["eess.SP", "cs.NA", "math.NA"], "comment": null, "summary": "In nature, signals often appear in the form of the superposition of multiple\nnon-stationary signals. The overlap of signal components in the time-frequency\ndomain poses a significant challenge for signal analysis. One approach to\naddressing this problem is to introduce an additional chirprate parameter and\nuse the chirplet transform (CT) to elevate the two-dimensional time-frequency\nrepresentation to a three-dimensional time-frequency-chirprate representation.\nFrom a certain point of view, the CT of a signal can be regarded as a windowed\nspecial linear canonical transform of that signal, undergoing a shift and a\nmodulation.\n  In this paper, we develop this idea to propose a novel windowed linear\ncanonical transform (WLCT), which provides a new time-frequency-chirprate\nrepresentation. We discuss four types of WLCTs. In addition, we use a special\nX-ray transform to further sharpen the time-frequency-chirprate representation.\nFurthermore, we derive the corresponding three-dimensional synchrosqueezed\ntransform, demonstrating that the WLCTs have great potential for\nthree-dimensional signal separation."}
{"id": "2510.09817", "pdf": "https://arxiv.org/pdf/2510.09817", "abs": "https://arxiv.org/abs/2510.09817", "authors": ["Samanta Rodriguez", "Yiming Dou", "Miquel Oller", "Andrew Owens", "Nima Fazeli"], "title": "Cross-Sensor Touch Generation", "categories": ["cs.RO", "cs.CV"], "comment": "CoRL 2025", "summary": "Today's visuo-tactile sensors come in many shapes and sizes, making it\nchallenging to develop general-purpose tactile representations. This is because\nmost models are tied to a specific sensor design. To address this challenge, we\npropose two approaches to cross-sensor image generation. The first is an\nend-to-end method that leverages paired data (Touch2Touch). The second method\nbuilds an intermediate depth representation and does not require paired data\n(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific\nmodels across multiple sensors via the cross-sensor touch generation process.\nTogether, these models offer flexible solutions for sensor translation,\ndepending on data availability and application needs. We demonstrate their\neffectiveness on downstream tasks such as in-hand pose estimation and behavior\ncloning, successfully transferring models trained on one sensor to another.\nProject page: https://samantabelen.github.io/cross_sensor_touch_generation."}
{"id": "2510.10473", "pdf": "https://arxiv.org/pdf/2510.10473", "abs": "https://arxiv.org/abs/2510.10473", "authors": ["Huizhi Wang", "Tierui Gong", "Emil Björnson", "Chau Yuen"], "title": "Multi-Carrier Rydberg Atomic Quantum Receivers with Enhanced Bandwidth Feature for Communication and Sensing", "categories": ["eess.SP"], "comment": null, "summary": "Rydberg atomic quantum receivers (RAQRs) have attracted significant attention\nin recent years due to their ultra-high sensitivity. Although capable of\nprecisely detecting the amplitude and phase of weak signals, conventional RAQRs\nface inherent limitations in accurately receiving wideband RF signals, due to\nthe discrete nature of atomic energy levels and their intrinsic instantaneous\nbandwidth constraints. These limitations hinder their direct application to\nmulti-carrier communication and sensing. To address this issue, this paper\nproposes a multi-carrier Rydberg atomic quantum receiver (MC-RAQR) structure\nwith five energy levels. We derive the amplitude and phase of the MC-RAQR and\nextract the baseband electrical signal for signal processing. In terms of\nmulti-carrier communication and sensing, we analyze the channel capacity and\naccuracy of angle of arrival (AoA) and distance parameters, respectively.\nNumerical results validate our proposed model, showing that the MC-RAQR can\nachieve up to a bandwidth of 14 MHz, which is 56-fold larger than the\nconventional RAQRs. As a result, the channel capacity and the resolution for\nmulti-target sensing are improved significantly. Specifically, the channel\ncapacity of MC-RAQR is 22-fold and 3-fold larger than the conventional antennas\nand RAQRs, respectively. For sensing performance, the MSE of AoA estimation for\nMC-RAQR is 0.16% of the conventional RAQR and the MSE of distance estimation is\n0.01% of the CRB of conventional antennas, showing the superior performance of\nthe MC-RAQR. This demonstrates its compatibility with waveforms such as\northogonal frequency-division multiplexing (OFDM) and its significant\nadvantages for multi-carrier signal reception."}
{"id": "2510.09962", "pdf": "https://arxiv.org/pdf/2510.09962", "abs": "https://arxiv.org/abs/2510.09962", "authors": ["Yicheng He", "Jingwen Yu", "Guangcheng Chen", "Hong Zhang"], "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Maintaining an up-to-date map that accurately reflects recent changes in the\nenvironment is crucial, especially for robots that repeatedly traverse the same\nspace. Failing to promptly update the changed regions can degrade map quality,\nresulting in poor localization, inefficient operations, and even lost robots.\n3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online\nmap reconstruction due to its dense, differentiable, and photorealistic\nproperties, yet accurately and efficiently updating the regions of change\nremains a challenge. In this paper, we propose VG-Mapping, a novel online\n3DGS-based mapping system tailored for such semi-static scenes. Our approach\nintroduces a hybrid representation that augments 3DGS with a TSDF-based voxel\nmap to efficiently identify changed regions in a scene, along with a\nvariation-aware density control strategy that inserts or deletes Gaussian\nprimitives in regions undergoing change. Furthermore, to address the absence of\npublic benchmarks for this task, we construct a RGB-D dataset comprising both\nsynthetic and real-world semi-static environments. Experimental results\ndemonstrate that our method substantially improves the rendering quality and\nmap update efficiency in semi-static scenes. The code and dataset are available\nat https://github.com/heyicheng-never/VG-Mapping."}
{"id": "2510.10512", "pdf": "https://arxiv.org/pdf/2510.10512", "abs": "https://arxiv.org/abs/2510.10512", "authors": ["Xiaopeng Cheng", "Zhichao Zhang"], "title": "Graph Signal Wiener Filtering in the Linear Canonical Domain: Theory and Method Design", "categories": ["eess.SP"], "comment": null, "summary": "The graph linear canonical transform (GLCT)-based filtering methods often\noptimize transform parameters and filters separately, which results in high\ncomputational costs and limited stability. To address this issue, this paper\nproposes a trainable joint optimization framework that combines GLCT parameters\nand Wiener filtering into an end-to-end learning process, allowing for\nsynergistic optimization between transform domain construction and filtering\noperations. The proposed method not only eliminates the cumbersome grid search\nrequired by traditional strategies but also significantly enhances the\nflexibility and training stability of the filtering system. Experimental\nresults on real-world graph data show the proposed method outperforms existing\nmethods in denoising tasks, featuring superior denoising performance, higher\nrobustness and lower computational complexity."}
{"id": "2510.09963", "pdf": "https://arxiv.org/pdf/2510.09963", "abs": "https://arxiv.org/abs/2510.09963", "authors": ["Chaoran Wang", "Jingyuan Sun", "Yanhui Zhang", "Mingyu Zhang", "Changju Wu"], "title": "LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots", "categories": ["cs.RO"], "comment": "It contains 8 pages, 7 figures and 4 tables. This paper is submitted\n  to ICRA 2026", "summary": "We introduce a novel framework for automatic behavior tree (BT) construction\nin heterogeneous multi-robot systems, designed to address the challenges of\nadaptability and robustness in dynamic environments. Traditional robots are\nlimited by fixed functional attributes and cannot efficiently reconfigure their\nstrategies in response to task failures or environmental changes. To overcome\nthis limitation, we leverage large language models (LLMs) to generate and\nextend BTs dynamically, combining the reasoning and generalization power of\nLLMs with the modularity and recovery capability of BTs. The proposed framework\nconsists of four interconnected modules task initialization, task assignment,\nBT update, and failure node detection which operate in a closed loop. Robots\ntick their BTs during execution, and upon encountering a failure node, they can\neither extend the tree locally or invoke a centralized virtual coordinator\n(Alex) to reassign subtasks and synchronize BTs across peers. This design\nenables long-term cooperative execution in heterogeneous teams. We validate the\nframework on 60 tasks across three simulated scenarios and in a real-world cafe\nenvironment with a robotic arm and a wheeled-legged robot. Results show that\nour method consistently outperforms baseline approaches in task success rate,\nrobustness, and scalability, demonstrating its effectiveness for multi-robot\ncollaboration in complex scenarios."}
{"id": "2510.10532", "pdf": "https://arxiv.org/pdf/2510.10532", "abs": "https://arxiv.org/abs/2510.10532", "authors": ["Guoyun Xie", "Zhichao Zhang"], "title": "SVD-based ugmt-gft on directed product graphs", "categories": ["eess.SP"], "comment": null, "summary": "Traditional directed graph signal processing generally depends on fixed\nrepresentation matrices, whose rigid structures limit the model's ability to\nadapt to complex graph topologies. To address this issue, this study employed\nthe unified graph representation matrix (UGRM) to propose a generalized graph\nFourier transform (UGRM-GFT) method based on singular value decomposition (SVD)\nfor signal analysis on directed graphs and Cartesian product graphs. We defined\nUGRM-GFT for general directed graphs by introducing a parameterized UGRM that\nincorporates traditional representations such as the Laplacian matrix and\nadjacency matrix. The SVD is used to construct spectral transform pairs with\nboth left and right singular vectors. We extended this approach to two types of\nUGRM-GFTs applied to directed Cartesian product graphs. UGRM-GFT-I performs SVD\ndirectly on the composite UGRM matrix of the two-dimensional graph structure,\nsuitable for globally coupled graph signals. UGRM-GFT-II separately applies SVD\nto the UGRMs of the two-factor graphs and then combines the results,\nsignificantly reducing computational complexity while preserving spectral\nexpressiveness. Theoretical analysis confirmed the monotonicity of the proposed\nmethod with respect to the parameters alpha and k embedded in the UGRM.\nExperimental results on real-world datasets demonstrated that the proposed\nmethod significantly outperforms traditional fixed-matrix approaches in\ndenoising tasks, with a particular emphasis on signal-to-noise ratio and\nbandwidth efficiency."}
{"id": "2510.09966", "pdf": "https://arxiv.org/pdf/2510.09966", "abs": "https://arxiv.org/abs/2510.09966", "authors": ["Easton R. Potokar", "Taylor Pool", "Daniel McGann", "Michael Kaess"], "title": "FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Light Detection and Ranging (LiDAR) sensors have become a de-facto sensor for\nmany robot state estimation tasks, spurring development of many LiDAR Odometry\n(LO) methods in recent years. While some smoothing-based LO methods have been\nproposed, most require matching against multiple scans, resulting in\nsub-real-time performance. Due to this, most prior works estimate a single\nstate at a time and are ``submap''-based. This architecture propagates any\nerror in pose estimation to the fixed submap and can cause jittery trajectories\nand degrade future registrations. We propose Fixed-Lag Odometry with Reparative\nMapping (FORM), a LO method that performs smoothing over a densely connected\nfactor graph while utilizing a single iterative map for matching. This allows\nfor both real-time performance and active correction of the local map as pose\nestimates are further refined. We evaluate on a wide variety of datasets to\nshow that FORM is robust, accurate, real-time, and provides smooth trajectory\nestimates when compared to prior state-of-the-art LO methods."}
{"id": "2510.10542", "pdf": "https://arxiv.org/pdf/2510.10542", "abs": "https://arxiv.org/abs/2510.10542", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Data Integration Using Multivariate Mode Decomposition for Physiological Sensing with Multiple Millimeter-Wave Radar Systems", "categories": ["eess.SP"], "comment": "7 pages, 4 figures, 5 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a multi-radar system for non-contact physiological\nsensing across arbitrary body orientations. In integrating signals obtained\nfrom different radar viewpoints, we adopt a multivariate variational mode\ndecomposition method to extract the common respiratory component. Experiments\nconducted with six subjects under varying distances and orientations\ndemonstrate that, compared with a single-radar setup, the proposed system\nreduced the root mean square error of the respiratory interval by 35.5%,\ndecreased the mean absolute error of the respiratory rate by 30.8%, and\nimproved accuracy by 9.4 percentage points. These results highlight that\ncombining multiple radar viewpoints with signal integration enables stable\nrespiratory measurement regardless of body orientation."}
{"id": "2510.09980", "pdf": "https://arxiv.org/pdf/2510.09980", "abs": "https://arxiv.org/abs/2510.09980", "authors": ["Jingyuan Sun", "Hongyu Ji", "Zihan Qu", "Chaoran Wang", "Mingyu Zhang"], "title": "ATRos: Learning Energy-Efficient Agile Locomotion for Wheeled-legged Robots", "categories": ["cs.RO"], "comment": "4 pages, 2 figures, submitted to IROS 2025 wheeled-legged workshop", "summary": "Hybrid locomotion of wheeled-legged robots has recently attracted increasing\nattention due to their advantages of combining the agility of legged locomotion\nand the efficiency of wheeled motion. But along with expanded performance, the\nwhole-body control of wheeled-legged robots remains challenging for hybrid\nlocomotion. In this paper, we present ATRos, a reinforcement learning\n(RL)-based hybrid locomotion framework to achieve hybrid walking-driving\nmotions on the wheeled-legged robot. Without giving predefined gait patterns,\nour planner aims to intelligently coordinate simultaneous wheel and leg\nmovements, thereby achieving improved terrain adaptability and improved energy\nefficiency. Based on RL techniques, our approach constructs a prediction policy\nnetwork that could estimate external environmental states from proprioceptive\nsensory information, and the outputs are then fed into an actor critic network\nto produce optimal joint commands. The feasibility of the proposed framework is\nvalidated through both simulations and real-world experiments across diverse\nterrains, including flat ground, stairs, and grassy surfaces. The hybrid\nlocomotion framework shows robust performance over various unseen terrains,\nhighlighting its generalization capability."}
{"id": "2510.10561", "pdf": "https://arxiv.org/pdf/2510.10561", "abs": "https://arxiv.org/abs/2510.10561", "authors": ["Zhixiong Chen", "Hyundong Shin", "Arumugam Nallanathan", "Jonathon Chambers"], "title": "Large Language Model-Empowered Channel Prediction and Predictive Beamforming for LEO Satellite Communications", "categories": ["eess.SP"], "comment": "14 pages, 13 figures", "summary": "Accurate channel prediction and effective beamforming are essential for low\nEarth orbit (LEO) satellite communications to enhance system capacity and\nenable high-speed connectivity. Most existing channel prediction and predictive\nbeamforming methods are limited by model generalization capabilities and\nstruggle to adapt to time-varying wireless propagation environments. Inspired\nby the remarkable generalization and reasoning capabilities of large language\nmodels (LLMs), this work proposes an LLM-based channel prediction framework,\nnamely CPLLM, to forecast future channel state information (CSI) for LEO\nsatellites based on historical CSI data. In the proposed CPLLM, a dedicated CSI\nencoder is designed to map raw CSI data into the textual embedding space,\neffectively bridging the modality gap and enabling the LLM to perform reliable\nreasoning over CSI data. Additionally, a CSI decoder is introduced to\nsimultaneously predict CSI for multiple future time slots, substantially\nreducing the computational burden and inference latency associated with the\ninherent autoregressive decoding process of LLMs. Then, instead of training the\nLLM from scratch, we adopt a parameter-efficient fine-tuning strategy, i.e.,\nLoRA, for CPLLM, where the pretrained LLM remains frozen and trainable low-rank\nmatrices are injected into each Transformer decoder layer to enable effective\nfine-tuning. Furthermore, we extend CPLLM to directly generate beamforming\nstrategies for future time slots based on historical CSI data, namely BFLLM.\nThis extended framework retains the same architecture as CPLLM, while\nintroducing a dedicated beamforming decoder to output beamforming strategies.\nFinally, extensive simulation results validate the effectiveness of the\nproposed approaches in channel prediction and predictive beamforming for LEO\nsatellite communications."}
{"id": "2510.10016", "pdf": "https://arxiv.org/pdf/2510.10016", "abs": "https://arxiv.org/abs/2510.10016", "authors": ["Shahid Ansari", "Vivek Gupta", "Bishakh Bhattacharya"], "title": "Hybrid Robotic Meta-gripper for Tomato Harvesting: Analysis of Auxetic Structures with Lattice Orientation Variations", "categories": ["cs.RO"], "comment": null, "summary": "The agricultural sector is rapidly evolving to meet growing global food\ndemands, yet tasks like fruit and vegetable handling remain labor-intensive,\ncausing inefficiencies and post-harvest losses. Automation, particularly\nselective harvesting, offers a viable solution, with soft robotics emerging as\na key enabler. This study introduces a novel hybrid gripper for tomato\nharvesting, incorporating a rigid outer frame with a soft auxetic internal\nlattice. The six-finger, 3D caging-effect design enables gentle yet secure\ngrasping in unstructured environments. Uniquely, the work investigates the\neffect of auxetic lattice orientation on grasping conformability, combining\nexperimental validation with 2D Digital Image Correlation (DIC) and nonlinear\nfinite element analysis (FEA). Auxetic configurations with unit cell\ninclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their\ngrasping forces, deformation responses, and motor torque requirements are\nsystematically compared. Results demonstrate that lattice orientation strongly\ninfluences compliance, contact forces, and energy efficiency, with distinct\nadvantages across configurations. This comparative framework highlights the\nnovelty of tailoring auxetic geometries to optimize robotic gripper\nperformance. The findings provide new insights into soft-rigid hybrid gripper\ndesign, advancing automation strategies for precision agriculture while\nminimizing crop damage."}
{"id": "2510.10563", "pdf": "https://arxiv.org/pdf/2510.10563", "abs": "https://arxiv.org/abs/2510.10563", "authors": ["Xuyang Zhao", "Jiangtao Wang", "Xinyu Zhang"], "title": "Covert Waveform Design for Integrated Sensing and Communication System in Clutter Environment", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes an integrated sensing and communication (ISAC) system\ncovert waveform design method for complex clutter environments, with the core\nobjective of maximizing the signal-to-clutter-plus-noise ratio (SCNR). The\ndesign achieves efficient clutter suppression while meeting the covertness\nrequirement through joint optimization of the transmit waveform and receive\nfilter, enabling cooperative radar detection and wireless communication. This\nstudy presents key innovations that explicitly address target Doppler shift\nuncertainty, significantly enhancing system robustness against Doppler effects.\nTo ensure communication reliability, the method incorporates phase difference\nconstraints between communication signal elements in the waveform design, along\nwith energy constraint, covert constraint, and peak-to-average power ratio\n(PAPR) constraint. The original non-convex optimization problem is transformed\ninto a tractable convex optimization form through convex optimization\ntechnique. Simulation results demonstrate that the optimized waveform not only\nsatisfies the covertness requirement in complex clutter environment, but also\nachieves superior target detection performance. It also ensures reliable\ncommunication and confirms the effectiveness of propose method."}
{"id": "2510.10046", "pdf": "https://arxiv.org/pdf/2510.10046", "abs": "https://arxiv.org/abs/2510.10046", "authors": ["Mingke Lu", "Shuaikang Wang", "Meng Guo"], "title": "LOMORO: Long-term Monitoring of Dynamic Targets with Minimum Robotic Fleet under Resource Constraints", "categories": ["cs.RO"], "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025)", "summary": "Long-term monitoring of numerous dynamic targets can be tedious for a human\noperator and infeasible for a single robot, e.g., to monitor wild flocks,\ndetect intruders, search and rescue. Fleets of autonomous robots can be\neffective by acting collaboratively and concurrently. However, the online\ncoordination is challenging due to the unknown behaviors of the targets and the\nlimited perception of each robot. Existing work often deploys all robots\navailable without minimizing the fleet size, or neglects the constraints on\ntheir resources such as battery and memory. This work proposes an online\ncoordination scheme called LOMORO for collaborative target monitoring, path\nrouting and resource charging. It includes three core components: (I) the\nmodeling of multi-robot task assignment problem under the constraints on\nresources and monitoring intervals; (II) the resource-aware task coordination\nalgorithm iterates between the high-level assignment of dynamic targets and the\nlow-level multi-objective routing via the Martin's algorithm; (III) the online\nadaptation algorithm in case of unpredictable target behaviors and robot\nfailures. It ensures the explicitly upper-bounded monitoring intervals for all\ntargets and the lower-bounded resource levels for all robots, while minimizing\nthe average number of active robots. The proposed methods are validated\nextensively via large-scale simulations against several baselines, under\ndifferent road networks, robot velocities, charging rates and monitoring\nintervals."}
{"id": "2510.10647", "pdf": "https://arxiv.org/pdf/2510.10647", "abs": "https://arxiv.org/abs/2510.10647", "authors": ["Emanuele Peschiera", "Sangbu Yun", "Youngjoo Lee", "Liesbet Van der Perre", "François Rottenberg"], "title": "A Parametric Power Model of Upper Mid-Band (FR3) Base Stations for 6G", "categories": ["eess.SP"], "comment": null, "summary": "Increasing attention is given to the upper mid-band or Frequency Range 3\n(FR3), from 7 to 24 GHz, in the research towards sixth-generation (6G)\nnetworks. Promises of offering large data rates at favorable propagation\nconditions are leading to novel FR3 base station (BS) architectures, with up to\nthousands of antenna elements and radio-frequency (RF) chains. This work\ninvestigates the power consumption of prospective FR3 BSs and its relation to\nthe delivered data rates. We model the power consumed by digital and analog\nsignal processing, power amplifiers (PAs), and supply and cooling during four\nphases (data, signaling, micro-sleep, and idle) in downlink and uplink. Hybrid\npartially-connected beamforming is compared to fully-digital one. Results show\nthat, for BS arrays with $1024$ antennas at $30\\%$ of load, the PA consumes\nmost of the power when $64$ or less RF chains are utilized, while the digital\nand analog processing consumption takes over when the number of RF chains is\n$512$ or more. The digital plus analog processing consumes $2\\times$ to\n$4\\times$ more than the PA for fully-digital beamforming. Hybrid beamforming\nachieves $1.3$ Gbit/s/user in downlink while improving the energy efficiency by\n$1.4\\times$ compared to fully-digital beamforming."}
{"id": "2510.10059", "pdf": "https://arxiv.org/pdf/2510.10059", "abs": "https://arxiv.org/abs/2510.10059", "authors": ["Keidai Iiyama", "Grace Gao"], "title": "Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model", "categories": ["cs.RO"], "comment": "Submitted NAVIGATION: Journal of the Institute of Navigation", "summary": "Recent advancements in lunar positioning, navigation, and timing (PNT) have\ndemonstrated that terrestrial GNSS signals, including weak sidelobe\ntransmissions, can be exploited for lunar spacecraft positioning and timing.\nWhile GNSS-based navigation at the Moon has been validated recently, unmodeled\nionospheric and plasmaspheric delays remain a significant error source,\nparticularly given the unique signal geometry and extended propagation paths.\nThis paper characterizes these delays using the Global Core Plasma Model (GCPM)\nand a custom low-cost ray-tracing algorithm that iteratively solves for bent\nsignal paths. We simulate first-, second-, and third-order group delays, as\nwell as excess path length from ray bending, for GNSS signals received at both\nlunar orbit and the lunar south pole under varying solar and geomagnetic\nconditions. Results show that mean group delays are typically on the order of 1\nm, but can exceed 100 m for low-altitude ray paths during high solar activity,\nwhile bending delays are generally smaller but non-negligible for low-altitude\nray paths. We also quantify the influence of signal frequency, geomagnetic\n$K_p$ index, and solar R12 index. These findings inform the design of robust\npositioning and timing algorithms that utilize terrestrial GNSS signals."}
{"id": "2510.10718", "pdf": "https://arxiv.org/pdf/2510.10718", "abs": "https://arxiv.org/abs/2510.10718", "authors": ["Rajat Bhattacharjya", "Woohyeok Park", "Arnab Sarkar", "Hyunwoo Oh", "Mohsen Imani", "Nikil Dutt"], "title": "HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing", "categories": ["eess.SP", "cs.AI", "cs.AR", "cs.SC"], "comment": "3 figures, 5 pages. Authors' version posted for personal use and not\n  for redistribution", "summary": "Direction of Arrival (DoA) estimation techniques face a critical trade-off,\nas classical methods often lack accuracy in challenging, low signal-to-noise\nratio (SNR) conditions, while modern deep learning approaches are too\nenergy-intensive and opaque for resource-constrained, safety-critical systems.\nWe introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing\n(HDC). The framework introduces two distinct feature extraction strategies --\nMean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline,\nand then reframes DoA estimation as a pattern recognition problem. This\napproach leverages HDC's inherent robustness to noise and its transparent\nalgebraic operations to bypass the expensive matrix decompositions and\n``black-box'' nature of classical and deep learning methods, respectively. Our\nevaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than\nstate-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it\nalso consumes ~93% less energy than competing neural baselines on an embedded\nNVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and\nefficiency establishes HYPERDOA as a robust and viable solution for\nmission-critical applications on edge devices."}
{"id": "2510.10086", "pdf": "https://arxiv.org/pdf/2510.10086", "abs": "https://arxiv.org/abs/2510.10086", "authors": ["Feifei Liu", "Haozhe Wang", "Zejun Wei", "Qirong Lu", "Yiyang Wen", "Xiaoyu Tang", "Jingyan Jiang", "Zhijian He"], "title": "Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios", "categories": ["cs.RO"], "comment": null, "summary": "Current evaluation methods for autonomous driving prediction models rely\nheavily on simplistic metrics such as Average Displacement Error (ADE) and\nFinal Displacement Error (FDE). While these metrics offer basic performance\nassessments, they fail to capture the nuanced behavior of prediction modules\nunder complex, interactive, and safety-critical driving scenarios. For\ninstance, existing benchmarks do not distinguish the influence of nearby versus\ndistant agents, nor systematically test model robustness across varying\nmulti-agent interactions. This paper addresses this critical gap by proposing a\nnovel testing framework that evaluates prediction performance under diverse\nscene structures, saying, map context, agent density and spatial distribution.\nThrough extensive empirical analysis, we quantify the differential impact of\nagent proximity on target trajectory prediction and identify scenario-specific\nfailure cases that are not exposed by traditional metrics. Our findings\nhighlight key vulnerabilities in current state-of-the-art prediction models and\ndemonstrate the importance of scenario-aware evaluation. The proposed framework\nlays the groundwork for rigorous, safety-driven prediction validation,\ncontributing significantly to the identification of failure-prone corner cases\nand the development of robust, certifiable prediction systems for autonomous\nvehicles."}
{"id": "2510.10796", "pdf": "https://arxiv.org/pdf/2510.10796", "abs": "https://arxiv.org/abs/2510.10796", "authors": ["R. Maydani", "Y. Wang", "J. Sarrazin", "B. Ma"], "title": "Spatially Filtered Sparse Bayesian Learning for Direction-of-Arrival Estimation with Leaky-Wave Antennas", "categories": ["eess.SP"], "comment": "Preprint submitted to ICASSP 2026. 4 pages, 3 figures", "summary": "Direction-of-arrival (DoA) estimation with leaky-wave antennas (LWAs) offers\na compact and cost-effective alternative to conventional antenna arrays but\nremains challenging in the presence of coherent sources. To address this issue,\nwe propose a spatially filtered sparse Bayesian learning (SF-SBL) framework.\nFirstly, the field of view (FoV) is divided into angular sectors according to\nthe frequency beam-scanning property of LWAs, and Bayesian inverse problems are\nthen solved within each sector to improve efficiency and reduce computational\ncost. Both on-grid SBL and off-grid SBL formulations are developed. Simulation\nresults show that the proposed approach achieves robust and accurate DoA\nestimation, even with coherent sources."}
{"id": "2510.10125", "pdf": "https://arxiv.org/pdf/2510.10125", "abs": "https://arxiv.org/abs/2510.10125", "authors": ["Yanjiang Guo", "Lucy Xiaoyang Shi", "Jianyu Chen", "Chelsea Finn"], "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "17 pages", "summary": "Generalist robot policies can now perform a wide range of manipulation\nskills, but evaluating and improving their ability with unfamiliar objects and\ninstructions remains a significant challenge. Rigorous evaluation requires a\nlarge number of real-world rollouts, while systematic improvement demands\nadditional corrective data with expert labels. Both of these processes are\nslow, costly, and difficult to scale. World models offer a promising, scalable\nalternative by enabling policies to rollout within imagination space. However,\na key challenge is building a controllable world model that can handle\nmulti-step interactions with generalist robot policies. This requires a world\nmodel compatible with modern generalist policies by supporting multi-view\nprediction, fine-grained action control, and consistent long-horizon\ninteractions, which is not achieved by previous works. In this paper, we make a\nstep forward by introducing a controllable multi-view world model that can be\nused to evaluate and improve the instruction-following ability of generalist\nrobot policies. Our model maintains long-horizon consistency with a\npose-conditioned memory retrieval mechanism and achieves precise action control\nthrough frame-level action conditioning. Trained on the DROID dataset (95k\ntrajectories, 564 scenes), our model generates spatially and temporally\nconsistent trajectories under novel scenarios and new camera placements for\nover 20 seconds. We show that our method can accurately rank policy performance\nwithout real-world robot rollouts. Moreover, by synthesizing successful\ntrajectories in imagination and using them for supervised fine-tuning, our\napproach can improve policy success by 44.7\\%."}
{"id": "2510.10923", "pdf": "https://arxiv.org/pdf/2510.10923", "abs": "https://arxiv.org/abs/2510.10923", "authors": ["Xuyao Deng", "Yong Dou", "Kele Xu"], "title": "Spatial Signal Focusing and Noise Suppression for Direction-of-Arrival Estimation in Large-Aperture 2D Arrays under Demanding Conditions", "categories": ["eess.SP"], "comment": null, "summary": "Direction-of-Arrival (DOA) estimation in sensor arrays faces limitations\nunder demanding conditions, including low signal-to-noise ratio,\nsingle-snapshot scenarios, coherent sources, and unknown source counts.\nConventional beamforming suffers from sidelobe interference, adaptive methods\n(e.g., MVDR) and subspace algorithms (e.g., MUSIC) degrade with limited\nsnapshots or coherent signals, while sparse-recovery approaches (e.g., L1-SVD)\nincur high computational complexity for large arrays. In this article, we\nconstruct the concept of the optimal spatial filter to solve the DOA estimation\nproblem under demanding conditions by utilizing the sparsity of spatial\nsignals. By utilizing the concept of the optimal spatial filter, we have\ntransformed the DOA estimation problem into a solution problem for the optimal\nspatial filter. We propose the Spatial Signal Focusing and Noise Suppression\n(SSFNS) algorithm, which is a novel DOA estimation framework grounded in the\ntheoretical existence of an optimal spatial filter, to solve for the optimal\nspatial filter and obtain DOA. Through experiments, it was found that the\nproposed algorithm is suitable for large aperture two-dimensional arrays and\nexperiments have shown that our proposed algorithm performs better than other\nalgorithms in scenarios with few snapshots or even a single snapshot, low\nsignal-to-noise ratio, coherent signals, and unknown signal numbers in\ntwo-dimensional large aperture arrays."}
{"id": "2510.10154", "pdf": "https://arxiv.org/pdf/2510.10154", "abs": "https://arxiv.org/abs/2510.10154", "authors": ["LinFeng Li", "Jian Zhao", "Yuan Xie", "Xin Tan", "Xuelong Li"], "title": "CompassNav: Steering From Path Imitation To Decision Understanding In Navigation", "categories": ["cs.RO"], "comment": null, "summary": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in\nnavigation relies on imitating expert trajectories. This approach reduces the\ncomplex navigation task to a sequence-to-sequence replication of a single\ncorrect path, fundamentally limiting the agent's ability to explore and\ngeneralize. In this work, we argue for and introduce a new paradigm: a shift\nfrom Path Imitation to Decision Understanding. The goal of this paradigm is to\nbuild agents that do not just follow, but truly understand how to navigate. We\nmaterialize this through two core contributions: first, we introduce\nCompass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning\n(RFT) subset provides a panoramic view of the decision landscape by annotating\nall feasible actions with A* geodesic distances. Second, we design a novel\ngap-aware hybrid reward function that dynamically adapts its feedback to\ndecision certainty, shifting between decisive signals for optimal actions and\nnuanced scores to encourage exploration. Integrated into an SFT-then-RFT\nrecipe, our CompassNav agent is trained not to memorize static routes, but to\ndevelop an internal ``compass'' that constantly intuits the direction to the\ngoal by evaluating the relative quality of all possible moves. This approach\nenables our 7B agent to set a new state-of-the-art on Goal navigation\nbenchmarks, outperforming even larger proprietary models, and achieve robust\nreal-world goal navigation on a physical robot."}
{"id": "2510.11044", "pdf": "https://arxiv.org/pdf/2510.11044", "abs": "https://arxiv.org/abs/2510.11044", "authors": ["Yang Lu", "Xinke Xie", "Yanqing Xu", "Bo Ai", "Octavia A. Dobre", "Dusit Niyato"], "title": "Dual-Waveguide Pinching Antennas for PLS: Parallel Placement or Orthogonal Placement?", "categories": ["eess.SP"], "comment": null, "summary": "Pinching antennas (PAs), as an emerging flexible-antenna technology, enables\nmovable PAs deployed along waveguides to customize channel conditions over a\nlarge scale. This paper investigates an application of PAs to enable\nphysical-layer security (PLS) by enlarging the channel condition diversity\nbetween legitimate users (LUs) and eavesdroppers (Eves). Particularly, we focus\non the dual-waveguide scenario, where the two waveguides employs multiple PAs\nto serve multiple LUs in the presence of an Eve. Specifically, we consider two\nwaveguide placement strategies, i.e., parallel placement and orthogonal\nplacement. Meanwhile, we incorporate two channel models, i.e., in-waveguide\nphase shifts, and in-waveguide phase shifts and attenuation. We formulate the\nsecure sum rate (SSR) and secure energy efficiency (SEE) maximization problems,\nand propose a two-stage algorithm to solve them. The first stage adopts a\nparticle swarm optimization (PSO) method with an improved feasibility module,\ntermed FeaPSO, for PA placement, and the second stage employs the successive\nconvex approximate (SCA) method to optimize beamforming and artificial noise\nvectors. Furthermore, we conduct numerical comparisons between the two\nplacement strategies in terms of average performance and a special case where\nan Eve is positioned in front of LUs. Numerical results validate the\neffectiveness of the proposed algorithm and demonstrate that PAs can\nsignificantly improve both SSR and SEE. Additionally, the necessity of\northogonal waveguide placement is explicitly verified."}
{"id": "2510.10181", "pdf": "https://arxiv.org/pdf/2510.10181", "abs": "https://arxiv.org/abs/2510.10181", "authors": ["Shaokai Wu", "Yanbiao Ji", "Qiuchang Li", "Zhiyi Zhang", "Qichen He", "Wenyuan Xie", "Guodong Zhang", "Bayram Bayramli", "Yue Ding", "Hongtao Lu"], "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment."}
{"id": "2510.11097", "pdf": "https://arxiv.org/pdf/2510.11097", "abs": "https://arxiv.org/abs/2510.11097", "authors": ["Shumaila Javaid", "Nasir Saeed"], "title": "The Post-Electromagnetic Era: A Vision for Wireless Communication Beyond 6G", "categories": ["eess.SP"], "comment": "Magazine Article", "summary": "Electromagnetic (EM) communication is nearing its physical and thermodynamic\nlimits, where further performance gains through spectrum optimization alone\nhave become increasingly unsustainable. Finite bandwidth, propagation loss at\nhigher frequencies, and the inherent trade-offs between energy and information\nconstrain the scalability of 6G and beyond systems. These limitations drive the\nsearch for alternative mechanisms for information transfer beyond conventional\nEM propagation. This work introduces a state-centric framework for post-6G\ncommunication, in which information is conveyed by manipulating physical,\nbiological, and cognitive states rather than EM waves. It identifies ten\nfoundational paradigms that define potential carriers and interaction\nmechanisms for the post-electromagnetic era and outlines a research roadmap\ntoward self-organizing, cognitively integrated networks. Together, these\ndevelopments envision a new class of communication systems that are\nenergy-aware, adaptive, and capable of uniting matter, life, and intelligence\nwithin a single informational continuum. By establishing the conceptual basis\nfor this transition, the work provides a foundation for future research aimed\nat realizing communication paradigms that transcend the limitations of\nspectrum-bound systems."}
{"id": "2510.10206", "pdf": "https://arxiv.org/pdf/2510.10206", "abs": "https://arxiv.org/abs/2510.10206", "authors": ["Zuhong Liu", "Junhao Ge", "Minhao Xiong", "Jiahao Gu", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "The true promise of humanoid robotics lies beyond single-agent autonomy: two\nor more humanoids must engage in physically grounded, socially meaningful\nwhole-body interactions that echo the richness of human social interaction.\nHowever, single-humanoid methods suffer from the isolation issue, ignoring\ninter-agent dynamics and causing misaligned contacts, interpenetrations, and\nunrealistic motions. To address this, we present Harmanoid , a dual-humanoid\nmotion imitation framework that transfers interacting human motions to two\nrobots while preserving both kinematic fidelity and physical realism. Harmanoid\ncomprises two key components: (i) contact-aware motion retargeting, which\nrestores inter-body coordination by aligning SMPL contacts with robot vertices,\nand (ii) interaction-driven motion controller, which leverages\ninteraction-specific rewards to enforce coordinated keypoints and physically\nplausible contacts. By explicitly modeling inter-agent contacts and\ninteraction-aware dynamics, Harmanoid captures the coupled behaviors between\nhumanoids that single-humanoid frameworks inherently overlook. Experiments\ndemonstrate that Harmanoid significantly improves interactive motion imitation,\nsurpassing existing single-humanoid frameworks that largely fail in such\nscenarios."}
{"id": "2510.11113", "pdf": "https://arxiv.org/pdf/2510.11113", "abs": "https://arxiv.org/abs/2510.11113", "authors": ["Hetong Wang", "Tiejun Lv", "Yashuai Cao", "Weicai Li", "Jie Zeng", "Pingmu Huang", "Muhammad Khurram Khan"], "title": "Navigating the Dual-Use Nature and Security Implications of Reconfigurable Intelligent Surfaces in Next-Generation Wireless Systems", "categories": ["eess.SP"], "comment": "This manuscript has been accepted for publication in IEEE\n  Communications Surveys and Tutorials. It was received on January 17, 2025,\n  and revised on July 1 and September 16, 2025. This version was accepted on\n  October 10, 2025", "summary": "Reconfigurable intelligent surface (RIS) technology offers significant\npromise in enhancing wireless communication systems, but its dual-use potential\nalso introduces substantial security risks. This survey explores the security\nimplications of RIS in next-generation wireless networks. We first highlight\nthe dual-use nature of RIS, demonstrating how its communication-enhancing\ncapabilities can be exploited by adversaries to compromise legitimate users. We\nidentify a new class of security vulnerabilities termed ``passive-active hybrid\nattacks,'' where RIS, despite passively handling signals, can be reconfigured\nto actively engage in malicious activities, enabling various RIS-assisted\nattacks, such as eavesdropping, man-in-the-middle (MITM), replay, reflection\njamming, and side-channel attacks. Furthermore, we reveal how adversaries can\nexploit the openness of wireless channels to introduce adversarial\nperturbations in artificial intelligence-driven RIS networks, disrupting\ncommunication terminals and causing misclassifications or errors in RIS\nreflection predictions. Despite these risks, RIS technology also plays a\ncritical role in enhancing security and privacy across radio frequency (RF) and\nvisible light communication (VLC) systems. By synthesizing current insights and\nhighlighting emerging threats, we provide actionable insights into cross-layer\ncollaboration, advanced adversarial defenses, and the balance between security\nand cost. This survey provides a comprehensive overview of RIS technology's\nsecurity landscape and underscores the urgent need for robust security\nframeworks in the development of future wireless systems."}
{"id": "2510.10217", "pdf": "https://arxiv.org/pdf/2510.10217", "abs": "https://arxiv.org/abs/2510.10217", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Tetsuya Ogata"], "title": "UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 6 figures", "summary": "Training robots to operate effectively in environments with uncertain states,\nsuch as ambiguous object properties or unpredictable interactions, remains a\nlongstanding challenge in robotics. Imitation learning methods typically rely\non successful examples and often neglect failure scenarios where uncertainty is\nmost pronounced. To address this limitation, we propose the Uncertainty-driven\nForesight Recurrent Neural Network (UF-RNN), a model that combines standard\ntime-series prediction with an active \"Foresight\" module. This module performs\ninternal simulations of multiple future trajectories and refines the hidden\nstate to minimize predicted variance, enabling the model to selectively explore\nactions under high uncertainty. We evaluate UF-RNN on a door-opening task in\nboth simulation and a real-robot setting, demonstrating that, despite the\nabsence of explicit failure demonstrations, the model exhibits robust\nadaptation by leveraging self-induced chaotic dynamics in its latent space.\nWhen guided by the Foresight module, these chaotic properties stimulate\nexploratory behaviors precisely when the environment is ambiguous, yielding\nimproved success rates compared to conventional stochastic RNN baselines. These\nfindings suggest that integrating uncertainty-driven foresight into imitation\nlearning pipelines can significantly enhance a robot's ability to handle\nunpredictable real-world conditions."}
{"id": "2510.11150", "pdf": "https://arxiv.org/pdf/2510.11150", "abs": "https://arxiv.org/abs/2510.11150", "authors": ["Sai Xu", "Yanan Du"], "title": "WiNPA: Wireless Neural Processing Architecture", "categories": ["eess.SP"], "comment": null, "summary": "This article presents a wireless neural processing architecture (WiNPA),\nproviding a novel perspective for accelerating edge inference of deep neural\nnetwork (DNN) workloads via joint optimization of wireless and computing\nresources. WiNPA enables fine-grained integration of wireless communication and\nedge computing, bridging the research gap between wireless and edge\nintelligence and significantly improving DNN inference performance. To fully\nrealize its potential, we explore a set of fundamental research issues,\nincluding mathematical modeling, optimization, and unified hardware--software\nplatforms. Additionally, key research directions are discussed to guide future\ndevelopment and practical implementation. A case study demonstrates WiNPA's\nworkflow and effectiveness in accelerating DNN inference through simulations."}
{"id": "2510.10221", "pdf": "https://arxiv.org/pdf/2510.10221", "abs": "https://arxiv.org/abs/2510.10221", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Hiroki Mori", "Tetsuya Ogata"], "title": "A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "This study investigates the developmental interaction between top-down (TD)\nand bottom-up (BU) visual attention in robotic learning. Our goal is to\nunderstand how structured, human-like attentional behavior emerges through the\nmutual adaptation of TD and BU mechanisms over time. To this end, we propose a\nnovel attention model $A^3 RNN$ that integrates predictive TD signals and\nsaliency-based BU cues through a bi-directional attention architecture.\n  We evaluate our model in robotic manipulation tasks using imitation learning.\nExperimental results show that attention behaviors evolve throughout training,\nfrom saliency-driven exploration to prediction-driven direction. Initially, BU\nattention highlights visually salient regions, which guide TD processes, while\nas learning progresses, TD attention stabilizes and begins to reshape what is\nperceived as salient. This trajectory reflects principles from cognitive\nscience and the free-energy framework, suggesting the importance of\nself-organizing attention through interaction between perception and internal\nprediction. Although not explicitly optimized for stability, our model exhibits\nmore coherent and interpretable attention patterns than baselines, supporting\nthe idea that developmental mechanisms contribute to robust attention\nformation."}
{"id": "2510.11214", "pdf": "https://arxiv.org/pdf/2510.11214", "abs": "https://arxiv.org/abs/2510.11214", "authors": ["Mehdi Sattari", "Javad Aliakbari", "Alexandre Graell i Amat", "Tommy Svensson"], "title": "CSI Prediction Using Diffusion Models", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Acquiring accurate channel state information (CSI) is critical for reliable\nand efficient wireless communication, but challenges such as high pilot\noverhead and channel aging hinder timely and accurate CSI acquisition. CSI\nprediction, which forecasts future CSI from historical observations, offers a\npromising solution. Recent deep learning approaches, including recurrent neural\nnetworks and Transformers, have achieved notable success but typically learn\ndeterministic mappings, limiting their ability to capture the stochastic and\nmultimodal nature of wireless channels. In this paper, we introduce a novel\nprobabilistic framework for CSI prediction based on diffusion models, offering\na flexible design that supports integration of diverse prediction schemes. We\ndecompose the CSI prediction task into two components: a temporal encoder,\nwhich extracts channel dynamics, and a diffusion-based generator, which\nproduces future CSI samples. We investigate two inference\nschemes-autoregressive and sequence-to-sequence- and explore multiple diffusion\nbackbones, including U-Net and Transformer-based architectures. Furthermore, we\nexamine a diffusion-based approach without an explicit temporal encoder and\nutilize the DDIM scheduling to reduce model complexity. Extensive simulations\ndemonstrate that our diffusion-based models significantly outperform\nstate-of-the-art baselines."}
{"id": "2510.10273", "pdf": "https://arxiv.org/pdf/2510.10273", "abs": "https://arxiv.org/abs/2510.10273", "authors": ["Vincent Schoenbach", "Marvin Wiedemann", "Raphael Memmesheimer", "Malte Mosbach", "Sven Behnke"], "title": "Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive Modeling and Learned S-Curve Velocity Profiles", "categories": ["cs.RO"], "comment": "In Proceedings of IEEE 21st International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, USA, August 2025", "summary": "Efficient physics simulation has significantly accelerated research progress\nin robotics applications such as grasping and assembly. The advent of\nGPU-accelerated simulation frameworks like Isaac Sim has particularly empowered\nlearning-based methods, enabling them to tackle increasingly complex tasks. The\nPAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a\nmecanum-wheeled base, allowing omnidirectional movement and a wide range of\ntask capabilities. However, until now, no model of the robot has been available\nin Isaac Sim. In this paper, we introduce such a model, calibrated to\napproximate the behavior of the real robot, with a focus on its omnidirectional\ndrive dynamics. We present two control models for the omnidirectional drive: a\nphysically accurate model that replicates real-world wheel dynamics and a\nlightweight velocity-based model optimized for learning-based applications.\nWith these models, we introduce a learning-based calibration approach to\napproximate the real robot's S-shaped velocity profile using minimal trajectory\ndata recordings. This simulation should allow researchers to experiment with\nthe robot and perform efficient learning-based control in diverse environments.\nWe provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac."}
{"id": "2510.11216", "pdf": "https://arxiv.org/pdf/2510.11216", "abs": "https://arxiv.org/abs/2510.11216", "authors": ["Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "Normalized Ambiguity Function Characteristics of OFDM, OTFS, AFDM, and CP-AFDM for ISAC", "categories": ["eess.SP"], "comment": "Submitted to the IEEE ICC 2025", "summary": "This paper presents a unified and system-agnostic analysis of the ambiguity\nfunction (AF) characteristics of four representative multicarrier waveforms,\northogonal frequency division multiplexing (OFDM), orthogonal time frequency\nspace (OTFS), affine frequency division multiplexing (AFDM), and chirp-permuted\nAFDM (CP-AFDM), which are considered as key candidates for enabling integrated\nsensing and communications (ISAC) in future sixth generation (6G) networks. The\nAF of each waveform is obtained directly from its discrete-time definition and\nenhanced via ideal fractional interpolation, enabling precise characterization\nof its continuous-time delay-Doppler response. Two signaling modes are\nexamined: a communication-oriented case with random information symbols\nsuitable only for monostatic scenarios, and a sensing-oriented case with fixed\nunimodular symbols suitable for general multi-static scenarios. Furthermore,\nthe AFs and the ambiguity metrics including the 3dB mainlobe width,\npeak-to-sidelobe ratio (PSLR), and integrated sidelobe ratio (ISLR), are\nevaluated in normalized delay-Doppler units, enabling direct translation to any\nphysical system configuration defined by bandwidth, sampling frequency, or\nsymbol duration, while ensuring straightforward and consistent comparison\nacross waveforms. The results establish a consistent benchmark for comparing\nwaveform sensing capabilities in ISAC design, consolidating known behaviors:\nOFDM exhibits excellent delay resolution and sidelobe behavior but poor Doppler\nresponse, whereas advanced waveforms achieve improved balance between delay and\nDoppler resolution with varying sidelobe characteristics. The simulation code\nof the smooth AFs, is openly shared to promote reproducibility and support\nfuture ISAC waveform research."}
{"id": "2510.10274", "pdf": "https://arxiv.org/pdf/2510.10274", "abs": "https://arxiv.org/abs/2510.10274", "authors": ["Jinliang Zheng", "Jianxiong Li", "Zhihao Wang", "Dongxiu Liu", "Xirui Kang", "Yuchun Feng", "Yinan Zheng", "Jiayin Zou", "Yilun Chen", "Jia Zeng", "Ya-Qin Zhang", "Jiangmiao Pang", "Jingjing Liu", "Tai Wang", "Xianyuan Zhan"], "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "preprint, technical report, 33 pages", "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/"}
{"id": "2510.11279", "pdf": "https://arxiv.org/pdf/2510.11279", "abs": "https://arxiv.org/abs/2510.11279", "authors": ["Mingzhi Wang", "Zhichao Zhang"], "title": "Two-Dimensional Graph Bi-Fractional Fourier Transform", "categories": ["eess.SP"], "comment": null, "summary": "Graph signal processing (GSP) advances spectral analysis on irregular\ndomains. However, existing two-dimensional graph fractional Fourier transform\n(2D-GFRFT) employs a single fractional order for both factor graphs, thereby\nlimiting its adaptability to heterogeneous signals. We proposed the\ntwo-dimensional graph bi-fractional Fourier transform (2D-GBFRFT), which\nassigns independent fractional orders to the factor graphs of a Cartesian\nproduct while preserving separability. We established invertibility, unitarity,\nand index additivity, and developed two filtering schemes: a Wiener-style\ndesign through grid search and a differentiable framework that jointly\noptimizes transform orders and diagonal spectral filters. We further introduced\na hybrid interpolation with the joint time-vertex fractional Fourier transform\n(JFRFT), controlled by a tunable parameter that balances the two methods. In\nthe domains of synthetic Cartesian product graph signals, authentic temporal\ngraph datasets, and dynamic image deblurring, 2D-GBFRFT consistently surpasses\n2D-GFRFT and enhances JFRFT. Experimental results confirmed the versatility and\nsuperior performance of 2D-GBFRFT for filtering in GSP."}
{"id": "2510.10332", "pdf": "https://arxiv.org/pdf/2510.10332", "abs": "https://arxiv.org/abs/2510.10332", "authors": ["Kohio Deflesselle", "Mélodie Daniel", "Aly Magassouba", "Miguel Aranda", "Olivier Ly"], "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework", "categories": ["cs.RO", "cs.AI"], "comment": "4 pages, 3 figures, 2 tables, Accepted for Safety of Intelligent and\n  Autonomous Vehicles: Formal Methods vs. Machine Learning approaches for\n  reliable navigation (SIAV-FM2L) an IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025) workshop", "summary": "We present a deep reinforcement learning framework based on Soft Actor-Critic\n(SAC) for safe and precise maneuvering of double-Ackermann-steering mobile\nrobots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as\ndifferential-drive robots, DASMRs face strong kinematic constraints that make\nclassical planners brittle in cluttered environments. Our framework leverages\nthe Hindsight Experience Replay (HER) and the CrossQ overlay to encourage\nmaneuvering efficiency while avoiding obstacles. Simulation results with a\nheavy four-wheel-steering rover show that the learned policy can robustly reach\nup to 97% of target positions while avoiding obstacles. Our framework does not\nrely on handcrafted trajectories or expert demonstrations."}
{"id": "2510.11294", "pdf": "https://arxiv.org/pdf/2510.11294", "abs": "https://arxiv.org/abs/2510.11294", "authors": ["Run Gu", "Renjie Xie", "Wei Xu", "Zhaohui Yang", "Kaibin Huang"], "title": "Channel-Aware Deep Learning for Superimposed Pilot Power Allocation and Receiver Design", "categories": ["eess.SP"], "comment": null, "summary": "Superimposed pilot (SIP) schemes face significant challenges in effectively\nsuperimposing and separating pilot and data signals, especially in multiuser\nmobility scenarios with rapidly varying channels. To address these challenges,\nwe propose a novel channel-aware learning framework for SIP schemes, termed\nCaSIP, that jointly optimizes pilot-data power (PDP) allocation and a receiver\nnetwork for pilot-data interference (PDI) elimination, by leveraging channel\npath gain information, a form of large-scale channel state information (CSI).\nThe proposed framework identifies user-specific, resource element-wise PDP\nfactors and develops a deep neural network-based SIP receiver comprising\nexplicit channel estimation and data detection components. To properly leverage\npath gain data, we devise an embedding generator that projects it into\nembeddings, which are then fused with intermediate feature maps of the channel\nestimation network. Simulation results demonstrate that CaSIP efficiently\noutperforms traditional pilot schemes and state-of-the-art SIP schemes in terms\nof sum throughput and channel estimation accuracy, particularly under\nhigh-mobility and low signal-to-noise ratio (SNR) conditions."}
{"id": "2510.10337", "pdf": "https://arxiv.org/pdf/2510.10337", "abs": "https://arxiv.org/abs/2510.10337", "authors": ["Jihong Zhu", "Kefeng Huang", "Jonathon Pipe", "Chris Horbaczewsky", "Andy Tyrrell", "Ian J. S. Fairlamb"], "title": "Rise of the Robochemist", "categories": ["cs.RO"], "comment": "This article was originally published in the IEEE Systems, Man, and\n  Cybernetics Society eNewsletter, September 2025 issue:\n  https://www.ieeesmc.org/wp-content/uploads/2024/10/FeatureArticle_Sept25.pdf", "summary": "Chemistry, a long-standing discipline, has historically relied on manual and\noften time-consuming processes. While some automation exists, the field is now\non the cusp of a significant evolution driven by the integration of robotics\nand artificial intelligence (AI), giving rise to the concept of the\nrobochemist: a new paradigm where autonomous systems assist in designing,\nexecuting, and analyzing experiments. Robochemists integrate mobile\nmanipulators, advanced perception, teleoperation, and data-driven protocols to\nexecute experiments with greater adaptability, reproducibility, and safety.\nRather than a fully automated replacement for human chemists, we envisioned the\nrobochemist as a complementary partner that works collaboratively to enhance\ndiscovery, enabling a more efficient exploration of chemical space and\naccelerating innovation in pharmaceuticals, materials science, and sustainable\nmanufacturing. This article traces the technologies, applications, and\nchallenges that define this transformation, highlighting both the opportunities\nand the responsibilities that accompany the emergence of the robochemist.\nUltimately, the future of chemistry is argued to lie in a symbiotic partnership\nwhere human intuition and expertise is amplified by robotic precision and\nAI-driven insight."}
{"id": "2510.11353", "pdf": "https://arxiv.org/pdf/2510.11353", "abs": "https://arxiv.org/abs/2510.11353", "authors": ["Woo-Hyun Ko", "Jaewon Kim", "Tzu-Hsiang Lin", "Samin Moosavi", "P. R. Kumar"], "title": "A Dynamic Watermarking Technique for Matching Communication Addresses with Cars in a Visual Field", "categories": ["eess.SP"], "comment": null, "summary": "We consider a problem faced by an intelligent roadside unit (RSU) monitoring\na roadway by a video camera. Suppose the RSU notices that a particular car in\nits visual field needs to execute a specific evasive maneuver to avoid danger.\nIt would like to send a packet addressed to that particular car with this\nsuggestion. The problem is that while all the cars are communicating with the\nRSU, the RSU does not know which car in the video is associated with what IP\naddress. So, it does not know which IP address to send the packet to. Indeed,\nthe problem of matching addresses with cars in the visual field is a\nfundamental open problem. We provide an active solution employing dynamic\nwatermarking that was originally developed for the security of cyber-physical\nsystems. This technique calls for a car to superpose a small random excitation\nonto its actuation commands for steering angle or throttle/brake positions. The\ncar sends this random waveform to the RSU in a packet containing its IP\naddress. By signal processing of the video stream of a car at the RSU it can\nverify whether it matches with the waveform in the packet and thereby\nassociates that the IP address of the packet with that car in the visual field.\nThe RSU thereby determines which IP address is associated with which car in its\nvisual field. We present two demonstrations of performance. We demonstrate\nexperimental results on a laboratory transportation automated vehicles, a\nvision system, and a network, as well as on the field with two passenger sedans\nin practice. The results demonstrate that employing the dynamic watermarking\nmethod enables an RSU to distinguish the communication of a target vehicle from\nthat of other IP addresses of nearby vehicles."}
{"id": "2510.10346", "pdf": "https://arxiv.org/pdf/2510.10346", "abs": "https://arxiv.org/abs/2510.10346", "authors": ["Yuxiang Peng", "Chuchu Chen", "Kejian Wu", "Guoquan Huang"], "title": "sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we develop and open-source, for the first time, a square-root\nfilter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS,\nwhich is ultra-fast, numerically stable, and capable of dynamic initialization\neven under extreme conditions (i.e., extremely small time window). Despite\nrecent advancements in VINS, resource constraints and numerical instability on\nembedded (robotic) systems with limited precision remain critical challenges. A\nsquare-root covariance-based filter offers a promising solution by providing\nnumerical stability, efficient memory usage, and guaranteed positive\nsemi-definiteness. However, canonical SRFs suffer from inefficiencies caused by\ndisruptions in the triangular structure of the covariance matrix during\nupdates. The proposed method significantly improves VINS efficiency with a\nnovel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the\nsystem structure to preserve the structure. Moreover, we design a fast, robust,\ndynamic initialization method, which first recovers the minimal states without\ntriangulating 3D features and then efficiently performs iterative SRF update to\nrefine the full states, enabling seamless VINS operation. The proposed\nLLT-based SRF is extensively verified through numerical studies, demonstrating\nsuperior numerical stability and achieving robust efficient performance on\n32-bit single-precision floats, operating at twice the speed of\nstate-of-the-art (SOTA) methods. Our initialization method, tested on both\nmobile workstations and Jetson Nano computers, achieving a high success rate of\ninitialization even within a 100 ms window under minimal conditions. Finally,\nthe proposed sqrtVINS is extensively validated across diverse scenarios,\ndemonstrating strong efficiency, robustness, and reliability. The full\nopen-source implementation is released to support future research and\napplications."}
{"id": "2510.11374", "pdf": "https://arxiv.org/pdf/2510.11374", "abs": "https://arxiv.org/abs/2510.11374", "authors": ["Ruiqi Kong", "He Chen"], "title": "CIRSense: Rethinking WiFi Sensing with Channel Impulse Response", "categories": ["eess.SP", "cs.NI"], "comment": "16 pages, 15 figures", "summary": "WiFi sensing based on channel state information (CSI) collected from\ncommodity WiFi devices has shown great potential across a wide range of\napplications, including vital sign monitoring and indoor localization. Existing\nWiFi sensing approaches typically estimate motion information directly from\nCSI. However, they often overlook the inherent advantages of channel impulse\nresponse (CIR), a delay-domain representation that enables more intuitive and\nprincipled motion sensing by naturally concentrating motion energy and\nseparating multipath components. Motivated by this, we revisit WiFi sensing and\nintroduce CIRSense, a new framework that enhances the performance and\ninterpretability of WiFi sensing with CIR. CIRSense is built upon a new motion\nmodel that characterizes fractional delay effects, a fundamental challenge in\nCIR-based sensing. This theoretical model underpins technical advances for the\nthree challenges in WiFi sensing: hardware distortion compensation,\nhigh-resolution distance estimation, and subcarrier aggregation for extended\nrange sensing. CIRSense, operating with a 160 MHz channel bandwidth,\ndemonstrates versatile sensing capabilities through its dual-mode design,\nachieving a mean error of approximately 0.25 bpm in respiration monitoring and\n0.09 m in distance estimation. Comprehensive evaluations across residential\nspaces, far-range scenarios, and multi-target settings demonstrate CIRSense's\nsuperior performance over state-of-the-art CSI-based baselines. Notably, at a\nchallenging sensing distance of 20 m, CIRSense achieves at least 3x higher\naverage accuracy with more than 4.5x higher computational efficiency."}
{"id": "2510.10357", "pdf": "https://arxiv.org/pdf/2510.10357", "abs": "https://arxiv.org/abs/2510.10357", "authors": ["Yang Liu", "Bruno Da Costa", "Aude Billard"], "title": "Learning to Throw-Flip", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted to IROS 2025. Video Summary: https://youtu.be/txYc9b1oflU", "summary": "Dynamic manipulation, such as robot tossing or throwing objects, has recently\ngained attention as a novel paradigm to speed up logistic operations. However,\nthe focus has predominantly been on the object's landing location, irrespective\nof its final orientation. In this work, we present a method enabling a robot to\naccurately \"throw-flip\" objects to a desired landing pose (position and\norientation). Conventionally, objects thrown by revolute robots suffer from\nparasitic rotation, resulting in highly restricted and uncontrollable landing\nposes. Our approach is based on two key design choices: first, leveraging the\nimpulse-momentum principle, we design a family of throwing motions that\neffectively decouple the parasitic rotation, significantly expanding the\nfeasible set of landing poses. Second, we combine a physics-based model of free\nflight with regression-based learning methods to account for unmodeled effects.\nReal robot experiments demonstrate that our framework can learn to throw-flip\nobjects to a pose target within ($\\pm$5 cm, $\\pm$45 degrees) threshold in\ndozens of trials. Thanks to data assimilation, incorporating projectile\ndynamics reduces sample complexity by an average of 40% when throw-flipping to\nunseen poses compared to end-to-end learning methods. Additionally, we show\nthat past knowledge on in-hand object spinning can be effectively reused,\naccelerating learning by 70% when throwing a new object with a Center of Mass\n(CoM) shift. A video summarizing the proposed method and the hardware\nexperiments is available at https://youtu.be/txYc9b1oflU."}
{"id": "2510.11384", "pdf": "https://arxiv.org/pdf/2510.11384", "abs": "https://arxiv.org/abs/2510.11384", "authors": ["Jennie Couchman", "Phillip Stanley-Marbell"], "title": "Uncertainty Propagation in Finite Impulse Response Filters: Evaluating the Gaussian Assumption", "categories": ["eess.SP"], "comment": "5 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "A common assumption in signal processing is that underlying data numerically\nconforms to a Gaussian distribution. It is commonly utilized in signal\nprocessing to describe unknown additive noise in a system and is often\njustified by citing the central limit theorem for sums of random variables,\nalthough the central limit theorem applies only to sums of independent\nidentically distributed random variables. However, many linear operations in\nsignal processing take the form of weighted sums, which transforms the random\nvariables such that their distributions are no longer identical. One such\noperation is a finite impulse response (FIR) filter. FIR filters are commonly\nused in signal processing applications as a pre-processing step. FIR output\nnoise is generally assumed to be Gaussian. This article examines the FIR output\nresponse in the presence of uniformly distributed quantization noise. We\nexpress the FIR output uncertainty in terms of the input quantization\nuncertainty and filter coefficients. We show that the output uncertainty cannot\nbe assumed to be Gaussian, but depending on the application a Gaussian\nestimation may still be useful. Then, we show through detailed numerical\nsimulations that the output uncertainty distribution of the filter can be\nestimated through its most dominant coefficients."}
{"id": "2510.10379", "pdf": "https://arxiv.org/pdf/2510.10379", "abs": "https://arxiv.org/abs/2510.10379", "authors": ["Rohan Gupta", "Trevor Asbery", "Zain Merchant", "Abrar Anwar", "Jesse Thomason"], "title": "RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Coordinating heterogeneous robot fleets to achieve multiple goals is\nchallenging in multi-robot systems. We introduce an open-source and extensible\nframework for centralized multi-robot task planning and scheduling that\nleverages LLMs to enable fleets of heterogeneous robots to accomplish multiple\ntasks. RobotFleet provides abstractions for planning, scheduling, and execution\nacross robots deployed as containerized services to simplify fleet scaling and\nmanagement. The framework maintains a shared declarative world state and\ntwo-way communication for task execution and replanning. By modularizing each\nlayer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet\nlowers the barrier to building scalable multi-robot systems. The code can be\nfound here: https://github.com/therohangupta/robot-fleet."}
{"id": "2510.11461", "pdf": "https://arxiv.org/pdf/2510.11461", "abs": "https://arxiv.org/abs/2510.11461", "authors": ["Eric Han Wang", "Weijia Yan", "Ruihong Huang"], "title": "Thermal Analysis of 3D GPU-Memory Architectures with Boron Nitride Interposer", "categories": ["eess.SP"], "comment": null, "summary": "As artificial intelligence (AI) chips become more powerful, the thermal\nmanagement capabilities of conventional silicon (Si) substrates become\ninsufficient for 3D-stacked designs. This work integrates electrically\ninsulative and thermally conductive hexagonal boron nitride (h-BN) interposers\ninto AI chips for effective thermal management. Using COMSOL Multiphysics, the\neffects of High-Bandwidth Memory (HBM) distributions and thermal interface\nmaterial configurations on heat dissipation and hotspot mitigation were\nstudied. A 20 {\\deg}C reduction in hot spots was achieved using h-BN\ninterposers compared to Si interposers. Such an improvement could reduce AI\nchips' power leakage by 22% and significantly enhance their thermal\nperformance."}
{"id": "2510.10392", "pdf": "https://arxiv.org/pdf/2510.10392", "abs": "https://arxiv.org/abs/2510.10392", "authors": ["Max Sokolich", "Yanda Yang", "Subrahmanyam Cherukumilli", "Fatma Ceren Kirmizitas", "Sambeeta Das"], "title": "MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents MicroRoboScope, a portable, compact, and versatile\nmicrorobotic experimentation platform designed for real-time, closed-loop\ncontrol of both magnetic and acoustic microrobots. The system integrates an\nembedded computer, microscope, power supplies, and control circuitry into a\nsingle, low-cost and fully integrated apparatus. Custom control software\ndeveloped in Python and Arduino C++ handles live video acquisition, microrobot\ntracking, and generation of control signals for electromagnetic coils and\nacoustic transducers. The platform's multi-modal actuation, accessibility, and\nportability make it suitable not only for specialized research laboratories but\nalso for educational and outreach settings. By lowering the barrier to entry\nfor microrobotic experimentation, this system enables new opportunities for\nresearch, education, and translational applications in biomedicine, tissue\nengineering, and robotics."}
{"id": "2510.11465", "pdf": "https://arxiv.org/pdf/2510.11465", "abs": "https://arxiv.org/abs/2510.11465", "authors": ["Diego Tuzi", "Thomas Delamotte", "Andreas Knopp"], "title": "Control Requirements for Robust Beamforming in Multi-Satellite Systems", "categories": ["eess.SP"], "comment": "6 pages, 3 figures, IFAC Workshop on Control Aspects of\n  Multi-Satellite Systems (CAMSAT) 2025", "summary": "This work investigates the impact of position and attitude perturbations on\nthe beamforming performance of multi-satellite systems. The system under\nanalysis is a formation of small satellites equipped with direct radiating\narrays that synthesise a large virtual antenna aperture. The results show that\nperformance is highly sensitive to the considered perturbations. However, by\nincorporating position and attitude information into the beamforming process,\nnominal performance can be effectively restored. These findings support the\ndevelopment of control-aware beamforming strategies that tightly integrate the\nattitude and orbit control system with signal processing to enable robust\nbeamforming and autonomous coordination."}
{"id": "2510.10421", "pdf": "https://arxiv.org/pdf/2510.10421", "abs": "https://arxiv.org/abs/2510.10421", "authors": ["Junbin Yuan", "Brady Moon", "Muqing Cao", "Sebastian Scherer"], "title": "Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty", "categories": ["cs.RO"], "comment": "8 pages, 7 figures. Accepted to IEEE Robotics and Automation Letters\n  (RAL), 2025", "summary": "Achieving persistent tracking of multiple dynamic targets over a large\nspatial area poses significant challenges for a single-robot system with\nconstrained sensing capabilities. As the robot moves to track different\ntargets, the ones outside the field of view accumulate uncertainty, making them\nprogressively harder to track. An effective path planning algorithm must manage\nuncertainty over a long horizon and account for the risk of permanently losing\ntrack of targets that remain unseen for too long. However, most existing\napproaches rely on short planning horizons and assume small, bounded\nenvironments, resulting in poor tracking performance and target loss in\nlarge-scale scenarios. In this paper, we present a hierarchical planner for\ntracking multiple moving targets with an aerial vehicle. To address the\nchallenge of tracking non-static targets, our method incorporates motion models\nand uncertainty propagation during path execution, allowing for more informed\ndecision-making. We decompose the multi-target tracking task into sub-tasks of\nsingle target search and detection, and our proposed pipeline consists a novel\nlow-level coverage planner that enables searching for a target in an evolving\nbelief area, and an estimation method to assess the likelihood of success for\neach sub-task, making it possible to convert the active target tracking task to\na Markov decision process (MDP) that we solve with a tree-based algorithm to\ndetermine the sequence of sub-tasks. We validate our approach in simulation,\ndemonstrating its effectiveness compared to existing planners for active target\ntracking tasks, and our proposed planner outperforms existing approaches,\nachieving a reduction of 11-70% in final uncertainty across different\nenvironments."}
{"id": "2510.11514", "pdf": "https://arxiv.org/pdf/2510.11514", "abs": "https://arxiv.org/abs/2510.11514", "authors": ["Yinchao Yang", "Yahao Ding", "Zhaohui Yang", "Chongwen Huang", "Zhaoyang Zhang", "Dusit Niyato", "Mohammad Shikh-Bahaei"], "title": "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated Sensing, Computing, and Semantic Communication Approach", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "Accepted by the IEEE Internet of Things Journal", "summary": "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods."}
{"id": "2510.10455", "pdf": "https://arxiv.org/pdf/2510.10455", "abs": "https://arxiv.org/abs/2510.10455", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design."}
{"id": "2510.11582", "pdf": "https://arxiv.org/pdf/2510.11582", "abs": "https://arxiv.org/abs/2510.11582", "authors": ["Renato Luis Garrido Cavalcante", "Noor Ul Ain", "Lorenzo Miretti", "Slawomir Stanczak"], "title": "Beyond the Use-and-then-Forget (UatF) Bound: Fixed Point Algorithms for Statistical Max-Min Power Control", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "We introduce mathematical tools and fixed point algorithms for optimal\nstatistical max-min power control in cellular and cell-less massive MIMO\nsystems. Unlike previous studies that rely on the use-and-then-forget (UatF)\nlower bound on Shannon achievable (ergodic) rates, our proposed framework can\ndeal with alternative bounds that explicitly consider perfect or imperfect\nchannel state information (CSI) at the decoder. In doing so, we address\nlimitations of UatF-based algorithms, which inherit the shortcomings of the\nUatF bound. For example, the UatF bound can be overly conservative: in extreme\ncases, under fully statistical (nonadaptive) beamforming in zero-mean channels,\nthe UatF bound produces trivial (zero) rate bounds. It also lacks scale\ninvariance: merely scaling the beamformers can change the bound drastically,\nespecially when simple beamforming strategies are employed. In contrast, our\nframework is compatible with information-theoretic bounds that do not suffer\nfrom the above drawbacks. We illustrate the framework by solving a max-min\npower control problem considering a standard bound that exploits instantaneous\nCSI at the decoder."}
{"id": "2510.10468", "pdf": "https://arxiv.org/pdf/2510.10468", "abs": "https://arxiv.org/abs/2510.10468", "authors": ["Robert Mahony", "Jonathan Kelly", "Stephan Weiss"], "title": "Galilean Symmetry in Robotics", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Under Review", "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins\nNewtonian physics. Although rigid-body symmetry is one of the most established\nand fundamental tools in robotics, there appears to be no comparable treatment\nof Galilean symmetry for a robotics audience. In this paper, we present a\nrobotics-tailored exposition of Galilean symmetry that leverages the\ncommunity's familiarity with and understanding of rigid-body transformations\nand pose representations. Our approach contrasts with common treatments in the\nphysics literature that introduce Galilean symmetry as a stepping stone to\nEinstein's relativity. A key insight is that the Galilean matrix Lie group can\nbe used to describe two different pose representations, Galilean frames, that\nuse inertial velocity in the state definition, and extended poses, that use\ncoordinate velocity. We provide three examples where applying the Galilean\nmatrix Lie-group algebra to robotics problems is straightforward and yields\nsignificant insights: inertial navigation above the rotating Earth, manipulator\nkinematics, and sensor data fusion under temporal uncertainty. We believe that\nthe time is right for the robotics community to benefit from rediscovering and\nextending this classical material and applying it to modern problems."}
{"id": "2510.11628", "pdf": "https://arxiv.org/pdf/2510.11628", "abs": "https://arxiv.org/abs/2510.11628", "authors": ["Patrick Hödl", "Jakob Möderl", "Erik Leitinger", "Klaus Witrisal"], "title": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G Antenna Arrays", "categories": ["eess.SP"], "comment": null, "summary": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation."}
{"id": "2510.10506", "pdf": "https://arxiv.org/pdf/2510.10506", "abs": "https://arxiv.org/abs/2510.10506", "authors": ["Kush Garg", "Akshat Dave"], "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 Figures , Project webpage: https://super-ex.github.io/", "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility."}
{"id": "2510.11666", "pdf": "https://arxiv.org/pdf/2510.11666", "abs": "https://arxiv.org/abs/2510.11666", "authors": ["Natalie Lang", "Atsutse K. Kludze", "Nir Shlezinger", "Yasaman Ghasempour", "Tirza Routtenberg", "George C. Alexandropoulos", "Yonina C. Eldar"], "title": "Leaky Wave Antennas for Next Generation Wireless Applications in sub-THz Frequencies: Current Status and Research Challenges", "categories": ["eess.SP"], "comment": null, "summary": "The ever-growing demand for ultra-high data rates, massive connectivity, and\njoint communication-sensing capabilities in future wireless networks is driving\nresearch into sub-terahertz (sub-THz) communications. While these frequency\nbands offer abundant spectrum, they also pose severe propagation and hardware\ndesign challenges, motivating the search for alternative antenna solutions\nbeyond conventional antenna arrays. Leaky-wave antennas (LWAs) have emerged as\na promising candidate for sub-THz systems due to their simple feed structure,\nlow fabrication cost, and inherent angle-frequency coupling, which enables\nfrequency-controlled beamsteering with simple hardware. In this article, we\nreview the fundamentals of the LWA technology, highlight their unique\nproperties, and showcase their potential in multi-user wideband sub-THz\nwireless communications. We present representative studies demonstrating that\nLWAs can simultaneously support high-rate multi-user communications and\naccurate localization using only a single antenna element. Finally, several key\nopen challenges are outlined, spanning algorithm design, signal processing,\ninformation theory, standardization, and hardware implementation, that need to\nbe addressed to fully harness LWAs as a cost-effective and scalable enabler of\nnext generations of wireless systems."}
{"id": "2510.10516", "pdf": "https://arxiv.org/pdf/2510.10516", "abs": "https://arxiv.org/abs/2510.10516", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems."}
{"id": "2510.09657", "pdf": "https://arxiv.org/pdf/2510.09657", "abs": "https://arxiv.org/abs/2510.09657", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Fabrizio Frezza", "Aurelio Uncini", "Danilo Comminiello"], "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials", "categories": ["cs.LG", "cs.AI", "cs.NA", "eess.SP", "math.NA"], "comment": "Accepted at EUSIPCO 2025", "summary": "Accurate simulation of wave propagation in complex acoustic materials is\ncrucial for applications in sound design, noise control, and material\nengineering. Traditional numerical solvers, such as finite element methods, are\ncomputationally expensive, especially when dealing with large-scale or\nreal-time scenarios. In this work, we introduce a dataset of 31,000 acoustic\nmaterials, named HA30K, designed and simulated solving the Helmholtz equations.\nFor each material, we provide the geometric configuration and the corresponding\npressure field solution, enabling data-driven approaches to learn Helmholtz\nequation solutions. As a baseline, we explore a deep learning approach based on\nStable Diffusion with ControlNet, a state-of-the-art model for image\ngeneration. Unlike classical solvers, our approach leverages GPU\nparallelization to process multiple simulations simultaneously, drastically\nreducing computation time. By representing solutions as images, we bypass the\nneed for complex simulation software and explicit equation-solving.\nAdditionally, the number of diffusion steps can be adjusted at inference time,\nbalancing speed and quality. We aim to demonstrate that deep learning-based\nmethods are particularly useful in early-stage research, where rapid\nexploration is more critical than absolute accuracy."}
{"id": "2510.10545", "pdf": "https://arxiv.org/pdf/2510.10545", "abs": "https://arxiv.org/abs/2510.10545", "authors": ["Koki Yamane", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Decoupled Scaling 4ch Bilateral Control on the Cartesian coordinate by 6-DoF Manipulator using Rotation Matrix", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "6 pages, 4 figures, Accepted at SAMCON 2025", "summary": "Four-channel bilateral control is a method for achieving remote control with\nforce feedback and adjustment operability by synchronizing the positions and\nforces of two manipulators. This is expected to significantly improve the\noperability of the remote control in contact-rich tasks. Among these, 4-channel\nbilateral control on the Cartesian coordinate system is advantageous owing to\nits suitability for manipulators with different structures and because it\nallows the dynamics in the Cartesian coordinate system to be adjusted by\nadjusting the control parameters, thus achieving intuitive operability for\nhumans. This paper proposes a 4-channel bilateral control method that achieves\nthe desired dynamics by decoupling each dimension in the Cartesian coordinate\nsystem regardless of the scaling factor."}
{"id": "2510.09773", "pdf": "https://arxiv.org/pdf/2510.09773", "abs": "https://arxiv.org/abs/2510.09773", "authors": ["Nora Basha", "Bechir Hamdaoui", "Attila A. Yavuz", "Thang Hoang", "Mehran Mozaffari Kermani"], "title": "Secret-Key Agreement Through Hidden Markov Modeling of Wavelet Scattering Embeddings", "categories": ["cs.CR", "eess.SP"], "comment": "Preprint-Final version accepted for publication in IEEE CNS 2025\n  proceedings", "summary": "Secret-key generation and agreement based on wireless channel reciprocity\noffers a promising avenue for securing IoT networks. However, existing\napproaches predominantly rely on the similarity of instantaneous channel\nmeasurement samples between communicating devices. This narrow view of\nreciprocity is often impractical, as it is highly susceptible to noise,\nasynchronous sampling, channel fading, and other system-level imperfections --\nall of which significantly impair key generation performance. Furthermore, the\nquantization step common in traditional schemes introduces irreversible errors,\nfurther limiting efficiency. In this work, we propose a novel approach for\nsecret-key generation by using wavelet scattering networks to extract robust\nand reciprocal CSI features. Dimensionality reduction is applied to uncover\nhidden cluster structures, which are then used to build hidden Markov models\nfor efficient key agreement. Our approach eliminates the need for quantization\nand effectively captures channel randomness. It achieves a 5x improvement in\nkey generation rate compared to traditional benchmarks, providing a secure and\nefficient solution for key generation in resource-constrained IoT environments."}
{"id": "2510.10567", "pdf": "https://arxiv.org/pdf/2510.10567", "abs": "https://arxiv.org/abs/2510.10567", "authors": ["Alexander Langmann", "Yevhenii Tokarev", "Mattia Piccinini", "Korbinian Moller", "Johannes Betz"], "title": "Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving", "categories": ["cs.RO"], "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based trajectory planners are widely used for agile autonomous\ndriving due to their ability to generate fast, smooth, and kinodynamically\nfeasible trajectories. However, their behavior is often governed by a cost\nfunction with manually tuned, static weights, which forces a tactical\ncompromise that is suboptimal across the wide range of scenarios encountered in\na race. To address this shortcoming, we propose using a Reinforcement Learning\n(RL) agent as a high-level behavioral selector that dynamically switches the\ncost function parameters of an analytical, low-level trajectory planner during\nruntime. We show the effectiveness of our approach in simulation in an\nautonomous racing environment where our RL-based planner achieved 0% collision\nrate while reducing overtaking time by up to 60% compared to state-of-the-art\nstatic planners. Our new agent now dynamically switches between aggressive and\nconservative behaviors, enabling interactive maneuvers unattainable with static\nconfigurations. These results demonstrate that integrating reinforcement\nlearning as a high-level selector resolves the inherent trade-off between\nsafety and competitiveness in autonomous racing planners. The proposed\nmethodology offers a pathway toward adaptive yet interpretable motion planning\nfor broader autonomous driving applications."}
{"id": "2510.10752", "pdf": "https://arxiv.org/pdf/2510.10752", "abs": "https://arxiv.org/abs/2510.10752", "authors": ["Tonghe Bai", "Ayush Kapoor", "Na Young Kim"], "title": "A High-Performance Training-Free Pipeline for Robust Random Telegraph Signal Characterization via Adaptive Wavelet-Based Denoising and Bayesian Digitization Methods", "categories": ["physics.app-ph", "eess.SP"], "comment": "18 pages, 8 figures", "summary": "Random telegraph signal (RTS) analysis is increasingly important for\ncharacterizing meaningful temporal fluctuations in physical, chemical, and\nbiological systems. The simplest RTS arises from discrete stochastic switching\nevents between two binary states, quantified by their transition amplitude and\ndwell times in each state. Quantitative analysis of RTSs provides valuable\ninsights into microscopic processes such as charge trapping in semiconductors.\nHowever, analyzing RTS becomes considerably complex when signals exhibit\nmulti-level structures or are corrupted by background white or pink noise. To\naddress these challenges and support high-throughput RTS analysis, we introduce\na modular and scalable signal processing pipeline combining dual-tree complex\nwavelet transform (DTCWT) denoising with a Bayesian digitization strategy. The\nadaptive DTCWT-based denoiser incorporates autonomous parameter selection rules\nfor its decomposition level and thresholds, optimizing white noise suppression\nwithout manual tuning. Complementing this denoiser, our probabilistic digitizer\neffectively resolves binary trap states even under residual notorious\nbackground pink noise. The overall approach enables robust performance across\nvarying noise levels and multi-trap scenarios, improving mean dwell time\nestimation and RTS reconstruction over classical and neural baselines. The\nmethod is up to 83x faster, training-free, and suitable for real-time or\nlarge-scale analysis. Evaluations confirm its generalizability, speed, and\nreliability, providing a strong foundation for future fully adaptive and\nautomated RTS pipelines."}
{"id": "2510.10597", "pdf": "https://arxiv.org/pdf/2510.10597", "abs": "https://arxiv.org/abs/2510.10597", "authors": ["David Rodríguez-Martínez", "C. J. Pérez del Pulgar"], "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation", "categories": ["cs.RO"], "comment": "9 pages, 6 figures, conference paper", "summary": "Improving robotic navigation is critical for extending exploration range and\nenhancing operational efficiency. Vision-based navigation relying on\ntraditional CCD or CMOS cameras faces major challenges when complex\nillumination conditions are paired with motion, limiting the range and\naccessibility of mobile planetary robots. In this study, we propose a novel\napproach to planetary navigation that leverages the unique imaging capabilities\nof Single-Photon Avalanche Diode (SPAD) cameras. We present the first\ncomprehensive evaluation of single-photon imaging as an alternative passive\nsensing technology for robotic exploration missions targeting perceptually\nchallenging locations, with a special emphasis on high-latitude lunar regions.\nWe detail the operating principles and performance characteristics of SPAD\ncameras, assess their advantages and limitations in addressing key perception\nchallenges of upcoming exploration missions to the Moon, and benchmark their\nperformance under representative illumination conditions."}
{"id": "2510.10820", "pdf": "https://arxiv.org/pdf/2510.10820", "abs": "https://arxiv.org/abs/2510.10820", "authors": ["Maarten van der Hulst", "Rodrigo A. González", "Koen Classens", "Paul Tacx", "Nick Dirkx", "Jeroen van de Wijdeven", "Tom Oomen"], "title": "Structured identification of multivariable modal systems", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "20 pages, 12 figures", "summary": "Physically interpretable models are essential for next-generation industrial\nsystems, as these representations enable effective control, support design\nvalidation, and provide a foundation for monitoring strategies. The aim of this\npaper is to develop a system identification framework for estimating modal\nmodels of complex multivariable mechanical systems from frequency response\ndata. To achieve this, a two-step structured identification algorithm is\npresented, where an additive model is first estimated using a refined\ninstrumental variable method and subsequently projected onto a modal form. The\ndeveloped identification method provides accurate, physically-relevant,\nminimal-order models, for both generally-damped and proportionally damped modal\nsystems. The effectiveness of the proposed method is demonstrated through\nexperimental validation on a prototype wafer-stage system, which features a\nlarge number of spatially distributed actuators and sensors and exhibits\ncomplex flexible dynamics."}
{"id": "2510.10602", "pdf": "https://arxiv.org/pdf/2510.10602", "abs": "https://arxiv.org/abs/2510.10602", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects."}
{"id": "2510.11049", "pdf": "https://arxiv.org/pdf/2510.11049", "abs": "https://arxiv.org/abs/2510.11049", "authors": ["Sonakshi Dua", "Gonzalo Mateos", "Sundeep Prabhakar Chepuri"], "title": "Conformal Inference for Time Series over Graphs", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Trustworthy decision making in networked, dynamic environments calls for\ninnovative uncertainty quantification substrates in predictive models for graph\ntime series. Existing conformal prediction (CP) methods have been applied\nseparately to multivariate time series and static graphs, but they either\nignore the underlying graph topology or neglect temporal dynamics. To bridge\nthis gap, here we develop a CP-based sequential prediction region framework\ntailored for graph time series. A key technical innovation is to leverage the\ngraph structure and thus capture pairwise dependencies across nodes, while\nproviding user-specified coverage guarantees on the predictive outcomes. We\nformally establish that our scheme yields an exponential shrinkage in the\nvolume of the ellipsoidal prediction set relative to its graph-agnostic\ncounterpart. Using real-world datasets, we demonstrate that the novel\nuncertainty quantification framework maintains desired empirical coverage while\nachieving markedly smaller (up to 80% reduction) prediction regions than\nexisting approaches."}
{"id": "2510.10637", "pdf": "https://arxiv.org/pdf/2510.10637", "abs": "https://arxiv.org/abs/2510.10637", "authors": ["Haoyu Zhao", "Cheng Zeng", "Linghao Zhuang", "Yaxi Zhao", "Shengke Xue", "Hao Wang", "Xingyue Zhao", "Zhongyu Li", "Kehan Li", "Siteng Huang", "Mingxiu Chen", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting", "categories": ["cs.RO"], "comment": "13 pages, 6 figures", "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap."}
{"id": "2510.11058", "pdf": "https://arxiv.org/pdf/2510.11058", "abs": "https://arxiv.org/abs/2510.11058", "authors": ["I Chiu", "Yu-Tung Liu", "Kuan-Chen Wang", "Hung-Yu Wei", "Yu Tsao"], "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks", "categories": ["cs.LG", "eess.SP"], "comment": "5 pages, 2 figures", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems."}
{"id": "2510.10642", "pdf": "https://arxiv.org/pdf/2510.10642", "abs": "https://arxiv.org/abs/2510.10642", "authors": ["Jianke Zhang", "Yucheng Hu", "Yanjiang Guo", "Xiaoyu Chen", "Yichen Liu", "Wenna Chen", "Chaochao Lu", "Jianyu Chen"], "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Building generalist robot policies that can handle diverse tasks in\nopen-ended environments is a central challenge in robotics. To leverage\nknowledge from large-scale pretraining, prior work has typically built\ngeneralist policies either on top of vision-language understanding models\n(VLMs) or generative models. However, both semantic understanding from\nvision-language pretraining and visual dynamics modeling from visual-generation\npretraining are crucial for embodied robots. Recent unified models of\ngeneration and understanding have demonstrated strong capabilities in both\ncomprehension and generation through large-scale pretraining. We posit that\nrobotic policy learning can likewise benefit from the combined strengths of\nunderstanding, planning and continuous future representation learning. Building\non this insight, we introduce UniCoD, which acquires the ability to dynamically\nmodel high-dimensional visual features through pretraining on over 1M\ninternet-scale instructional manipulation videos. Subsequently, UniCoD is\nfine-tuned on data collected from the robot embodiment, enabling the learning\nof mappings from predictive representations to action tokens. Extensive\nexperiments show our approach consistently outperforms baseline methods in\nterms of 9\\% and 12\\% across simulation environments and real-world\nout-of-distribution tasks."}
{"id": "2510.11060", "pdf": "https://arxiv.org/pdf/2510.11060", "abs": "https://arxiv.org/abs/2510.11060", "authors": ["Reinhard Fuchs", "Nathalie Sumrah", "Johannes Schwerdt", "Michael Unger", "Georg Stachel", "Michael Schultz", "Karsten Lenk", "Thomas Neumuth"], "title": "Basis for a hands free blood flow measurement with automated vessel focus", "categories": ["physics.med-ph", "eess.SP"], "comment": null, "summary": "Cardiopulmonary resuscitation (CPR) is one of the essential tools to ensure\noxygen supply during cardiac arrest. However, the precise effects of chest\ncompression are not quantifiable to this day. This often results in a low\nquality of chest compressions even if performed by health-care professionals.\nOne solution could be provided by quantification of blood flow via ultrasonic\nDoppler measurements, to guide first responders in their efforts. This paper\npresents an approach to address the issue of limited time, anatomical know how\nand limitations of system configuration during emergency scenarios. An approach\nfor automated vessel identification with three different phases was developed,\nfeaturing a new sensor probe for ultrasonic measurements with non symmetrically\nangled piezo ceramics. The probe was used with prototype ultrasound hardware in\na laboratory setup for Pulsed Wave Doppler (PW Doppler). In an initial\nmeasurement a qualitative flow was approximated to examine valuable measurement\npositions on a phantom. Afterwards an iterative mode was used for depth\ndepending frequency measurements with score calculation of flow periodicity and\nsignal power. The configuration with the best score was used for a prolonged\nmonitoring mode. Flow values were compared to data of an industrial\nflow-sensor. Flow-sensor data showed an average coefficient of determination of\n0.97 with an average root mean square error of 3.84 ml/s. With the proposed\nhardware and software solutions a basis for future developments was made, which\ncould lead to a fully automated vessel identification during CPR. This device\ncould provide first responders as well as clinical staff with vital information\nabout CPR efficiency that has yet to be included into the therapy of people\nduring cardiac arrest."}
{"id": "2510.10716", "pdf": "https://arxiv.org/pdf/2510.10716", "abs": "https://arxiv.org/abs/2510.10716", "authors": ["Christopher Thierauf"], "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy", "categories": ["cs.RO"], "comment": null, "summary": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined."}
{"id": "2510.11245", "pdf": "https://arxiv.org/pdf/2510.11245", "abs": "https://arxiv.org/abs/2510.11245", "authors": ["Leonardo Di Nino", "Gabriele D'Acunto", "Sergio Barbarossa", "Paolo Di Lorenzo"], "title": "Learning the Structure of Connection Graphs", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Connection graphs (CGs) extend traditional graph models by coupling network\ntopology with orthogonal transformations, enabling the representation of global\ngeometric consistency. They play a key role in applications such as\nsynchronization, Riemannian signal processing, and neural sheaf diffusion. In\nthis work, we address the inverse problem of learning CGs directly from\nobserved signals. We propose a principled framework based on maximum\npseudo-likelihood under a consistency assumption, which enforces spectral\nproperties linking the connection Laplacian to the underlying combinatorial\nLaplacian. Based on this formulation, we introduce the Structured Connection\nGraph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian\nmanifolds that jointly infers network topology, edge weights, and geometric\nstructure. Our experiments show that SCGL consistently outperforms existing\nbaselines in both topological recovery and geometric fidelity, while remaining\ncomputationally efficient."}
{"id": "2510.10731", "pdf": "https://arxiv.org/pdf/2510.10731", "abs": "https://arxiv.org/abs/2510.10731", "authors": ["Yongxi Cao", "Julian F. Schumann", "Jens Kober", "Joni Pajarinen", "Arkady Zgonnikov"], "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models."}
{"id": "2510.11445", "pdf": "https://arxiv.org/pdf/2510.11445", "abs": "https://arxiv.org/abs/2510.11445", "authors": ["Renaud-Alexandre Pitaval"], "title": "Repeated-and-Offset QPSK for DFT-s-OFDM in Satellite Access", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Motivated by the convergence of terrestrial cellular networks with satellite\nnetworks, we consider an adaptation of offset quadrature phase shift keying\n(OQPSK), used with single-carrier waveform in traditional satellite systems, to\ndiscrete Fourier transform spread (DFT-s-) orthogonal frequency-division\nmultiplexed (OFDM) waveform employed in the uplink of terrestrial systems. We\nintroduce a new order-one constellation modulation, termed repeated-and-offset\nQPSK (RO-QPSK), derive its basic properties, and compare it with pi/2-BPSK with\nfrequency-domain spectral shaping (FDSS), as supported in 5G. RO-QPSK naturally\nproduces a Hann-window-shaped spectrum, resulting in a very low maximum\npeak-to-average power ratio (PAPR) on the order of 2 dB. Moreover, with\nsingle-tap equalization and symbol combining at the receiver, RO-QSPK can\nimprove the signal-to-interference-plus-noise (SINR) compared to pi/2-BPSK with\nFDSS, in narrowband and/or moderately frequency-selective channels, as\nencountered in satellite communications. A moderate FDSS can also be combined\nwith RO-QSPK to further reduce the PAPR while providing similar performance. Of\nindependent interest, general SINR expressions for DFT-s-OFDM are also\nprovided."}
{"id": "2510.10759", "pdf": "https://arxiv.org/pdf/2510.10759", "abs": "https://arxiv.org/abs/2510.10759", "authors": ["Arthicha Srisuchinnawong", "Poramate Manoonpong"], "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning", "categories": ["cs.RO"], "comment": "RSS 2025", "summary": "Existing robot locomotion learning techniques rely heavily on the offline\nselection of proper reward weighting gains and cannot guarantee constraint\nsatisfaction (i.e., constraint violation) during training. Thus, this work aims\nto address both issues by proposing Reward-Oriented Gains via Embodied\nRegulation (ROGER), which adapts reward-weighting gains online based on\npenalties received throughout the embodied interaction process. The ratio\nbetween the positive reward (primary reward) and negative reward (penalty)\ngains is automatically reduced as the learning approaches the constraint\nthresholds to avoid violation. Conversely, the ratio is increased when learning\nis in safe states to prioritize performance. With a 60-kg quadruped robot,\nROGER achieved near-zero constraint violation throughout multiple learning\ntrials. It also achieved up to 50% more primary reward than the equivalent\nstate-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,\nincluding a single-leg hopper, ROGER exhibited comparable or up to 100% higher\nperformance and 60% less torque usage and orientation deviation compared to\nthose trained with the default reward function. Finally, real-world locomotion\nlearning of a physical quadruped robot was achieved from scratch within one\nhour without any falls. Therefore, this work contributes to\nconstraint-satisfying real-world continual robot locomotion learning and\nsimplifies reward weighting gain tuning, potentially facilitating the\ndevelopment of physical robots and those that learn in the real world."}
{"id": "2510.11458", "pdf": "https://arxiv.org/pdf/2510.11458", "abs": "https://arxiv.org/abs/2510.11458", "authors": ["Soubhagya Ranjan Hota", "Arka Roy", "Udit Satija"], "title": "ILD-VIT: A Unified Vision Transformer Architecture for Detection of Interstitial Lung Disease from Respiratory Sounds", "categories": ["eess.AS", "eess.SP"], "comment": null, "summary": "Interstitial lung disease (ILD) represents a group of restrictive chronic\npulmonary diseases that impair oxygen acquisition by causing irreversible\nchanges in the lungs such as fibrosis, scarring of parenchyma, etc. ILD\nconditions are often diagnosed by various clinical modalities such as\nspirometry, high-resolution lung imaging techniques, crackling respiratory\nsounds (RSs), etc. In this letter, we develop a novel vision transformer\n(VIT)-based deep learning framework namely, ILD-VIT, to detect the ILD\ncondition using the RS recordings. The proposed framework comprises three major\nstages: pre-processing, mel spectrogram extraction, and classification using\nthe proposed VIT architecture using the mel spectrogram image patches.\nExperimental results using the publicly available BRACETS and KAUH databases\nshow that our proposed ILD-VIT achieves an accuracy, sensitivity, and\nspecificity of 84.86%, 82.67%, and 86.91%, respectively, for\nsubject-independent blind testing. The successful onboard implantation of the\nproposed framework on a Raspberry-pi-4 microcontroller indicates its potential\nas a standalone clinical system for ILD screening in a real clinical scenario."}
{"id": "2510.10778", "pdf": "https://arxiv.org/pdf/2510.10778", "abs": "https://arxiv.org/abs/2510.10778", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "title": "Real2USD: Scene Representations in Universal Scene Description Language", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, 1 table", "summary": "Large Language Models (LLMs) can help robots reason about abstract task\nspecifications. This requires augmenting classical representations of the\nenvironment used by robots with natural language-based priors. There are a\nnumber of existing approaches to doing so, but they are tailored to specific\ntasks, e.g., visual-language models for navigation, language-guided neural\nradiance fields for mapping, etc. This paper argues that the Universal Scene\nDescription (USD) language is an effective and general representation of\ngeometric, photometric and semantic information in the environment for\nLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene\ngraph, readable by LLMs and humans alike, and rich enough to support\nessentially any task -- Pixar developed this language to store assets, scenes\nand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2\nquadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD\nrepresentation of indoor environments with diverse objects and challenging\nsettings with lots of glass, and (ii) parses the USD using Google's Gemini to\ndemonstrate scene understanding, complex inferences, and planning. We also\nstudy different aspects of this system in simulated warehouse and hospital\nsettings using Nvidia's Issac Sim. Code is available at\nhttps://github.com/grasp-lyrl/Real2USD ."}
{"id": "2510.10781", "pdf": "https://arxiv.org/pdf/2510.10781", "abs": "https://arxiv.org/abs/2510.10781", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B."}
{"id": "2510.10804", "pdf": "https://arxiv.org/pdf/2510.10804", "abs": "https://arxiv.org/abs/2510.10804", "authors": ["Alessandro Albini", "Mohsen Kaboli", "Giorgio Cannata", "Perla Maiolino"], "title": "Representing Data in Robotic Tactile Perception -- A Review", "categories": ["cs.RO"], "comment": null, "summary": "Robotic tactile perception is a complex process involving several\ncomputational steps performed at different levels. Tactile information is\nshaped by the interplay of robot actions, the mechanical properties of its\nbody, and the software that processes the data. In this respect, high-level\ncomputation, required to process and extract information, is commonly performed\nby adapting existing techniques from other domains, such as computer vision,\nwhich expects input data to be properly structured. Therefore, it is necessary\nto transform tactile sensor data to match a specific data structure. This\noperation directly affects the tactile information encoded and, as a\nconsequence, the task execution. This survey aims to address this specific\naspect of the tactile perception pipeline, namely Data Representation. The\npaper first clearly defines its contributions to the perception pipeline and\nthen reviews how previous studies have dealt with the problem of representing\ntactile information, investigating the relationships among hardware,\nrepresentations, and high-level computation methods. The analysis has led to\nthe identification of six structures commonly used in the literature to\nrepresent data. The manuscript provides discussions and guidelines for properly\nselecting a representation depending on operating conditions, including the\navailable hardware, the tactile information required to be encoded, and the\ntask at hand."}
{"id": "2510.10843", "pdf": "https://arxiv.org/pdf/2510.10843", "abs": "https://arxiv.org/abs/2510.10843", "authors": ["Jared Grinberg", "Yanran Ding"], "title": "Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots", "categories": ["cs.RO"], "comment": "Proc. IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, Aug. 17-21, 2025, pp. 1-7,\n  doi:10.1109/CASE58245.2025.11164031", "summary": "This paper presents a method for detecting and localizing contact along robot\nlegs using distributed joint torque sensors and a single hip-mounted\nforce-torque (FT) sensor using a generalized momentum-based observer framework.\nWe designed a low-cost strain-gauge-based joint torque sensor that can be\ninstalled on every joint to provide direct torque measurements, eliminating the\nneed for complex friction models and providing more accurate torque readings\nthan estimation based on motor current. Simulation studies on a floating-based\n2-DoF robot leg verified that the proposed framework accurately recovers\ncontact force and location along the thigh and shin links. Through a\ncalibration procedure, our torque sensor achieved an average 96.4% accuracy\nrelative to ground truth measurements. Building upon the torque sensor, we\nperformed hardware experiments on a 2-DoF manipulator, which showed\nsub-centimeter contact localization accuracy and force errors below 0.2 N."}
{"id": "2510.10851", "pdf": "https://arxiv.org/pdf/2510.10851", "abs": "https://arxiv.org/abs/2510.10851", "authors": ["Tingxuan Leng", "Yushi Wang", "Tinglong Zheng", "Changsheng Luo", "Mingguo Zhao"], "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid locomotion requires not only accurate command tracking for\nnavigation but also compliant responses to external forces during human\ninteraction. Despite significant progress, existing RL approaches mainly\nemphasize robustness, yielding policies that resist external forces but lack\ncompliance-particularly challenging for inherently unstable humanoids. In this\nwork, we address this by formulating humanoid locomotion as a multi-objective\noptimization problem that balances command tracking and external force\ncompliance. We introduce a preference-conditioned multi-objective RL (MORL)\nframework that integrates rigid command following and compliant behaviors\nwithin a single omnidirectional locomotion policy. External forces are modeled\nvia velocity-resistance factor for consistent reward design, and training\nleverages an encoder-decoder structure that infers task-relevant privileged\nfeatures from deployable observations. We validate our approach in both\nsimulation and real-world experiments on a humanoid robot. Experimental results\nindicate that our framework not only improves adaptability and convergence over\nstandard pipelines, but also realizes deployable preference-conditioned\nhumanoid locomotion."}
{"id": "2510.10865", "pdf": "https://arxiv.org/pdf/2510.10865", "abs": "https://arxiv.org/abs/2510.10865", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "comment": "17 pages, 5 figures, 8 tables", "summary": "Robots navigating dynamic, cluttered, and semantically complex environments\nmust integrate perception, symbolic reasoning, and spatial planning to\ngeneralize across diverse layouts and object categories. Existing methods often\nrely on static priors or limited memory, constraining adaptability under\npartial observability and semantic ambiguity. We present GRIP, Grid-based Relay\nwith Intermediate Planning, a unified, modular framework with three scalable\nvariants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic\noccupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and\nLLM-based introspection; and GRIP-R (Real-World), enabling physical robot\ndeployment under perceptual uncertainty. GRIP integrates dynamic 2D grid\nconstruction, open-vocabulary object grounding, co-occurrence-aware symbolic\nplanning, and hybrid policy execution using behavioral cloning, D* search, and\ngrid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks\nshow that GRIP achieves up to 9.6% higher success rates and over $2\\times$\nimprovement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative\nanalyses reveal interpretable symbolic plans in ambiguous scenes. Real-world\ndeployment on a Jetbot further validates GRIP's generalization under sensor\nnoise and environmental variation. These results position GRIP as a robust,\nscalable, and explainable framework bridging simulation and real-world\nnavigation."}
{"id": "2510.10886", "pdf": "https://arxiv.org/pdf/2510.10886", "abs": "https://arxiv.org/abs/2510.10886", "authors": ["Yashom Dighe", "Youngjin Kim", "Karthik Dantu"], "title": "QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Autonomous racing requires tight integration between perception, planning and\ncontrol to minimize latency as well as timely decision making. A standard\nautonomy pipeline comprising a global planner, local planner, and controller\nloses information as the higher-level racing context is sequentially propagated\ndownstream into specific task-oriented context. In particular, the global\nplanner's understanding of optimality is typically reduced to a sparse set of\nwaypoints, leaving the local planner to make reactive decisions with limited\ncontext. This paper investigates whether additional global insights,\nspecifically time-optimality information, can be meaningfully passed to the\nlocal planner to improve downstream decisions. We introduce a framework that\npreserves essential global knowledge and conveys it to the local planner\nthrough QuayPoints regions where deviations from the optimal raceline result in\nsignificant compromises to optimality. QuayPoints enable local planners to make\nmore informed global decisions when deviating from the raceline, such as during\nstrategic overtaking. To demonstrate this, we integrate QuayPoints into an\nexisting planner and show that it consistently overtakes opponents traveling at\nup to 75% of the ego vehicle's speed across four distinct race tracks."}
{"id": "2510.10893", "pdf": "https://arxiv.org/pdf/2510.10893", "abs": "https://arxiv.org/abs/2510.10893", "authors": ["Dikshant Shehmar", "Matthew E. Taylor", "Ehsan Hashemi"], "title": "An Adaptive Transition Framework for Game-Theoretic Based Takeover", "categories": ["cs.RO"], "comment": null, "summary": "The transition of control from autonomous systems to human drivers is\ncritical in automated driving systems, particularly due to the out-of-the-loop\n(OOTL) circumstances that reduce driver readiness and increase reaction times.\nExisting takeover strategies are based on fixed time-based transitions, which\nfail to account for real-time driver performance variations. This paper\nproposes an adaptive transition strategy that dynamically adjusts the control\nauthority based on both the time and tracking ability of the driver trajectory.\nShared control is modeled as a cooperative differential game, where control\nauthority is modulated through time-varying objective functions instead of\nblending control torques directly. To ensure a more natural takeover, a\ndriver-specific state-tracking matrix is introduced, allowing the transition to\nalign with individual control preferences. Multiple transition strategies are\nevaluated using a cumulative trajectory error metric. Human-in-the-loop control\nscenarios of the standardized ISO lane change maneuvers demonstrate that\nadaptive transitions reduce trajectory deviations and driver control effort\ncompared to conventional strategies. Experiments also confirm that continuously\nadjusting control authority based on real-time deviations enhances vehicle\nstability while reducing driver effort during takeover."}
{"id": "2510.10903", "pdf": "https://arxiv.org/pdf/2510.10903", "abs": "https://arxiv.org/abs/2510.10903", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Wei Zhao", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Haoang Li", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Shanghang Zhang", "Badong Chen"], "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey", "categories": ["cs.RO"], "comment": null, "summary": "Embodied intelligence has witnessed remarkable progress in recent years,\ndriven by advances in computer vision, natural language processing, and the\nrise of large-scale multimodal models. Among its core challenges, robot\nmanipulation stands out as a fundamental yet intricate problem, requiring the\nseamless integration of perception, planning, and control to enable interaction\nwithin diverse and unstructured environments. This survey presents a\ncomprehensive overview of robotic manipulation, encompassing foundational\nbackground, task-organized benchmarks and datasets, and a unified taxonomy of\nexisting methods. We extend the classical division between high-level planning\nand low-level control by broadening high-level planning to include language,\ncode, motion, affordance, and 3D representations, while introducing a new\ntaxonomy of low-level learning-based control grounded in training paradigms\nsuch as input modeling, latent learning, and policy learning. Furthermore, we\nprovide the first dedicated taxonomy of key bottlenecks, focusing on data\ncollection, utilization, and generalization, and conclude with an extensive\nreview of real-world applications. Compared with prior surveys, our work offers\nboth a broader scope and deeper insight, serving as an accessible roadmap for\nnewcomers and a structured reference for experienced researchers. All related\nresources, including research papers, open-source datasets, and projects, are\ncurated for the community at\nhttps://github.com/BaiShuanghao/Awesome-Robotics-Manipulation."}
{"id": "2510.10912", "pdf": "https://arxiv.org/pdf/2510.10912", "abs": "https://arxiv.org/abs/2510.10912", "authors": ["Xinyu Shao", "Yanzhe Tang", "Pengwei Xie", "Kaiwen Zhou", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Long Zeng", "Xiu Li"], "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks", "categories": ["cs.RO"], "comment": "More details and videos can be found at https://robo-map.github.io.\n  Xiu Li (Corresponding author: Xiu Li)", "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io."}
{"id": "2510.10960", "pdf": "https://arxiv.org/pdf/2510.10960", "abs": "https://arxiv.org/abs/2510.10960", "authors": ["Dong Hu", "Fenqing Hu", "Lidong Yang", "Chao Huang"], "title": "Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Ensuring safety in autonomous driving (AD) remains a significant challenge,\nespecially in highly dynamic and complex traffic environments where diverse\nagents interact and unexpected hazards frequently emerge. Traditional\nreinforcement learning (RL) methods often struggle to balance safety,\nefficiency, and adaptability, as they primarily focus on reward maximization\nwithout explicitly modeling risk or safety constraints. To address these\nlimitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)\nframework for safe AD. GTR2L incorporates a multi-level game-theoretic world\nmodel that jointly predicts the interactive behaviors of surrounding vehicles\nand their associated risks, along with an adaptive rollout horizon that adjusts\ndynamically based on predictive uncertainty. Furthermore, an uncertainty-aware\nbarrier mechanism enables flexible modulation of safety boundaries. A dedicated\nrisk modeling approach is also proposed, explicitly capturing both epistemic\nand aleatoric uncertainty to guide constrained policy optimization and enhance\ndecision-making in complex environments. Extensive evaluations across diverse\nand safety-critical traffic scenarios show that GTR2L significantly outperforms\nstate-of-the-art baselines, including human drivers, in terms of success rate,\ncollision and violation reduction, and driving efficiency. The code is\navailable at https://github.com/DanielHu197/GTR2L."}
{"id": "2510.10975", "pdf": "https://arxiv.org/pdf/2510.10975", "abs": "https://arxiv.org/abs/2510.10975", "authors": ["Mingtong Dai", "Lingbo Liu", "Yongjie Bai", "Yang Liu", "Zhouxia Wang", "Rui SU", "Chunjie Chen", "Liang Lin", "Xinyu Wu"], "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs.We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."}
{"id": "2510.10979", "pdf": "https://arxiv.org/pdf/2510.10979", "abs": "https://arxiv.org/abs/2510.10979", "authors": ["Qizhi Guo", "Siyuan Yang", "Junning Lyu", "Jianjun Sun", "Defu Lin", "Shaoming He"], "title": "AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances", "categories": ["cs.RO"], "comment": null, "summary": "Accurate and robust heading estimation is crucial for unmanned aerial\nvehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered\nnature of indoor environments often introduces severe magnetic disturbances,\nwhich can significantly degrade heading accuracy. To address this challenge,\nthis paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation\napproach for UAVs operating in magnetically disturbed environments. AMO-HEAD is\na lightweight and computationally efficient Extended Kalman Filter (EKF)\nframework that leverages inertial and magnetic sensors to achieve reliable\nheading estimation. In the proposed approach, gyroscope angular rate\nmeasurements are integrated to propagate the quaternion state, which is\nsubsequently corrected using accelerometer and magnetometer data. The corrected\nquaternion is then used to compute the UAV's heading. An adaptive process noise\ncovariance method is introduced to model and compensate for gyroscope\nmeasurement noise, bias drift, and discretization errors arising from the Euler\nmethod integration. To mitigate the effects of external magnetic disturbances,\na scaling factor is applied based on real-time magnetic deviation detection. A\ntheoretical observability analysis of the proposed AMO-HEAD is performed using\nthe Lie derivative. Extensive experiments were conducted in real world indoor\nenvironments with customized UAV platforms. The results demonstrate the\neffectiveness of the proposed algorithm in providing precise heading estimation\nunder magnetically disturbed conditions."}
{"id": "2510.11014", "pdf": "https://arxiv.org/pdf/2510.11014", "abs": "https://arxiv.org/abs/2510.11014", "authors": ["Subhransu S. Bhattacharjee", "Hao Lu", "Dylan Campbell", "Rahul Shome"], "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning."}
{"id": "2510.11019", "pdf": "https://arxiv.org/pdf/2510.11019", "abs": "https://arxiv.org/abs/2510.11019", "authors": ["Bingjie Tang", "Iretiayo Akinola", "Jie Xu", "Bowen Wen", "Dieter Fox", "Gaurav S. Sukhatme", "Fabio Ramos", "Abhishek Gupta", "Yashraj Narang"], "title": "Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies", "categories": ["cs.RO"], "comment": "in submission. 8 pages, 6 figures. Website:\n  https://refinery-2025.github.io/refinery/", "summary": "Simulation-based learning has enabled policies for precise, contact-rich\ntasks (e.g., robotic assembly) to reach high success rates (~80%) under high\nlevels of observation noise and control error. Although such performance may be\nsufficient for research applications, it falls short of industry standards and\nmakes policy chaining exceptionally brittle. A key limitation is the high\nvariance in individual policy performance across diverse initial conditions. We\nintroduce Refinery, an effective framework that bridges this performance gap,\nrobustifying policy performance across initial conditions. We propose Bayesian\nOptimization-guided fine-tuning to improve individual policies, and Gaussian\nMixture Model-based sampling during deployment to select initializations that\nmaximize execution success. Using Refinery, we improve mean success rates by\n10.98% over state-of-the-art methods in simulation-based learning for robotic\nassembly, reaching 91.51% in simulation and comparable performance in the real\nworld. Furthermore, we demonstrate that these fine-tuned policies can be\nchained to accomplish long-horizon, multi-part\nassembly$\\unicode{x2013}$successfully assembling up to 8 parts without\nrequiring explicit multi-step training."}
{"id": "2510.11036", "pdf": "https://arxiv.org/pdf/2510.11036", "abs": "https://arxiv.org/abs/2510.11036", "authors": ["Yeonseo Lee", "Jungwook Mun", "Hyosup Shin", "Guebin Hwang", "Junhee Nam", "Taeyeop Lee", "Sungho Jo"], "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Most robotic grasping methods are typically designed for single gripper\ntypes, which limits their applicability in real-world scenarios requiring\ndiverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp\ndetection framework that efficiently handles multiple gripper configurations.\nThe proposed method addresses data scarcity by systematically augmenting\nexisting datasets with multi-gripper annotations. XGrasp employs a hierarchical\ntwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)\nidentifies optimal locations using global scene information and gripper\nspecifications. In the second stage, an Angle-Width Predictor (AWP) refines the\ngrasp angle and width using local features. Contrastive learning in the AWP\nmodule enables zero-shot generalization to unseen grippers by learning\nfundamental grasping characteristics. The modular framework integrates\nseamlessly with vision foundation models, providing pathways for future\nvision-language capabilities. The experimental results demonstrate competitive\ngrasp success rates across various gripper types, while achieving substantial\nimprovements in inference speed compared to existing gripper-aware methods.\nProject page: https://sites.google.com/view/xgrasp"}
{"id": "2510.11041", "pdf": "https://arxiv.org/pdf/2510.11041", "abs": "https://arxiv.org/abs/2510.11041", "authors": ["Shiyao Zhang", "Liwei Deng", "Shuyu Zhang", "Weijie Yuan", "Hong Zhang"], "title": "Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy", "categories": ["cs.RO"], "comment": "Accepted by IEEE RA-L", "summary": "In future intelligent transportation systems, autonomous cooperative planning\n(ACP), becomes a promising technique to increase the effectiveness and security\nof multi-vehicle interactions. However, multiple uncertainties cannot be fully\naddressed for existing ACP strategies, e.g. perception, planning, and\ncommunication uncertainties. To address these, a novel deep reinforcement\nlearning-based autonomous cooperative planning (DRLACP) framework is proposed\nto tackle various uncertainties on cooperative motion planning schemes.\nSpecifically, the soft actor-critic (SAC) with the implementation of gate\nrecurrent units (GRUs) is adopted to learn the deterministic optimal\ntime-varying actions with imperfect state information occurred by planning,\ncommunication, and perception uncertainties. In addition, the real-time actions\nof autonomous vehicles (AVs) are demonstrated via the Car Learning to Act\n(CARLA) simulation platform. Evaluation results show that the proposed DRLACP\nlearns and performs cooperative planning effectively, which outperforms other\nbaseline methods under different scenarios with imperfect AV state information."}
{"id": "2510.11072", "pdf": "https://arxiv.org/pdf/2510.11072", "abs": "https://arxiv.org/abs/2510.11072", "authors": ["Huayi Wang", "Wentao Zhang", "Runyi Yu", "Tao Huang", "Junli Ren", "Feiyu Jia", "Zirui Wang", "Xiaojie Niu", "Xiao Chen", "Jiahe Chen", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang"], "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project website: https://why618188.github.io/physhsi/", "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns."}
{"id": "2510.11083", "pdf": "https://arxiv.org/pdf/2510.11083", "abs": "https://arxiv.org/abs/2510.11083", "authors": ["Tianyi Tan", "Yinan Zheng", "Ruiming Liang", "Zexu Wang", "Kexin Zheng", "Jinliang Zheng", "Jianxiong Li", "Xianyuan Zhan", "Jingjing Liu"], "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling", "categories": ["cs.RO", "cs.AI"], "comment": "26 pages, 6 figures. Accepted at NeurIPS 2025", "summary": "Modeling interactive driving behaviors in complex scenarios remains a\nfundamental challenge for autonomous driving planning. Learning-based\napproaches attempt to address this challenge with advanced generative models,\nremoving the dependency on over-engineered architectures for representation\nfusion. However, brute-force implementation by simply stacking transformer\nblocks lacks a dedicated mechanism for modeling interactive behaviors that are\ncommon in real driving scenarios. The scarcity of interactive driving data\nfurther exacerbates this problem, leaving conventional imitation learning\nmethods ill-equipped to capture high-value interactive behaviors. We propose\nFlow Planner, which tackles these problems through coordinated innovations in\ndata modeling, model architecture, and learning scheme. Specifically, we first\nintroduce fine-grained trajectory tokenization, which decomposes the trajectory\ninto overlapping segments to decrease the complexity of whole trajectory\nmodeling. With a sophisticatedly designed architecture, we achieve efficient\ntemporal and spatial fusion of planning and scene information, to better\ncapture interactive behaviors. In addition, the framework incorporates flow\nmatching with classifier-free guidance for multi-modal behavior generation,\nwhich dynamically reweights agent interactions during inference to maintain\ncoherent response strategies, providing a critical boost for interactive\nscenario understanding. Experimental results on the large-scale nuPlan dataset\nand challenging interactive interPlan dataset demonstrate that Flow Planner\nachieves state-of-the-art performance among learning-based approaches while\neffectively modeling interactive behaviors in complex driving scenarios."}
{"id": "2510.11094", "pdf": "https://arxiv.org/pdf/2510.11094", "abs": "https://arxiv.org/abs/2510.11094", "authors": ["Junxiang Wang", "Han Zhang", "Zehao Wang", "Huaiyuan Chen", "Pu Wang", "Weidong Chen"], "title": "Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation", "categories": ["cs.RO"], "comment": null, "summary": "Effective rehabilitation methods are essential for the recovery of lower limb\ndysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great\npotentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are\nusually heavy and need a lot of work to help the patients to put them on.\nMoreover, it also requires extra compliance control to guarantee the safety. In\ncontrast, soft exoskeletons are easy and comfortable to wear and have intrinsic\ncompliance, but their complex nonlinear human-robot interaction dynamics would\npose significant challenges for control. In this work, based on the pneumatic\nactuators inspired by origami, we design a rehabilitation exoskeleton for knee\nthat is easy and comfortable to wear. To guarantee the control performance and\nenable a nice human-robot interaction, we first use Deep Koopman Network to\nmodel the human-robot interaction dynamics. In particular, by viewing the\nelectromyography (EMG) signals and the duty cycle of the PWM wave that controls\nthe pneumatic robot's valves and pump as the inputs, the linear Koopman model\naccurately captures the complex human-robot interaction dynamics. Next, based\non the obtained Koopman model, we further use Model Predictive Control (MPC) to\ncontrol the soft robot and help the user to do rehabilitation training in\nreal-time. The goal of the rehabilitation training is to track a given\nreference signal shown on the screen. Experiments show that by integrating the\nEMG signals into the Koopman model, we have improved the model accuracy to\ngreat extent. In addition, a personalized Koopman model trained from the\nindividual's own data performs better than the non-personalized model.\nConsequently, our control framework outperforms the traditional PID control in\nboth passive and active training modes. Hence the proposed method provides a\nnew control framework for soft rehabilitation robots."}
{"id": "2510.11103", "pdf": "https://arxiv.org/pdf/2510.11103", "abs": "https://arxiv.org/abs/2510.11103", "authors": ["Martin Schuck", "Sherif Samy", "Angela P. Schoellig"], "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Many robotic control tasks require policies to act on orientations, yet the\ngeometry of SO(3) makes this nontrivial. Because SO(3) admits no global,\nsmooth, minimal parameterization, common representations such as Euler angles,\nquaternions, rotation matrices, and Lie algebra coordinates introduce distinct\nconstraints and failure modes. While these trade-offs are well studied for\nsupervised learning, their implications for actions in reinforcement learning\nremain unclear. We systematically evaluate SO(3) action representations across\nthree standard continuous control algorithms, PPO, SAC, and TD3, under dense\nand sparse rewards. We compare how representations shape exploration, interact\nwith entropy regularization, and affect training stability through empirical\nstudies and analyze the implications of different projections for obtaining\nvalid rotations from Euclidean network outputs. Across a suite of robotics\nbenchmarks, we quantify the practical impact of these choices and distill\nsimple, implementation-ready guidelines for selecting and using rotation\nactions. Our results highlight that representation-induced geometry strongly\ninfluences exploration and optimization and show that representing actions as\ntangent vectors in the local frame yields the most reliable results across\nalgorithms."}
{"id": "2510.11258", "pdf": "https://arxiv.org/pdf/2510.11258", "abs": "https://arxiv.org/abs/2510.11258", "authors": ["Yuhui Fu", "Feiyang Xie", "Chaoyi Xu", "Jing Xiong", "Haoqi Yuan", "Zongqing Lu"], "title": "DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks."}
{"id": "2510.11306", "pdf": "https://arxiv.org/pdf/2510.11306", "abs": "https://arxiv.org/abs/2510.11306", "authors": ["Xiaobin Zhou", "Miao Wang", "Chengao Li", "Can Cui", "Ruibin Zhang", "Yongchao Wang", "Chao Xu", "Fei Gao"], "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration\ndue to rotor imbalance, which introduces significant challenges for autonomous\nflight in unknown environments. The mainstream approaches against rotor\nfailures rely on fault-tolerant control (FTC) and predefined trajectory\ntracking. To the best of our knowledge, online failure detection and diagnosis\n(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown\nand complex environments have not yet been achieved. This paper presents a\nrotor-failure-aware quadrotor navigation system designed to mitigate the\nimpacts of rotor imbalance. First, a composite FDD-based nonlinear model\npredictive controller (NMPC), incorporating motor dynamics, is designed to\nensure fast failure detection and flight stability. Second, a\nrotor-failure-aware planner is designed to leverage FDD results and\nspatial-temporal joint optimization, while a LiDAR-based quadrotor platform\nwith four anti-torque plates is designed to enable reliable perception under\nhigh-speed rotation. Lastly, extensive benchmarks against state-of-the-art\nmethods highlight the superior performance of the proposed approach in\naddressing rotor failures, including propeller unloading and motor stoppage.\nThe experimental results demonstrate, for the first time, that our approach\nenables autonomous quadrotor flight with rotor failures in challenging\nenvironments, including cluttered rooms and unknown forests."}
{"id": "2510.11308", "pdf": "https://arxiv.org/pdf/2510.11308", "abs": "https://arxiv.org/abs/2510.11308", "authors": ["Weixi Situ", "Hanjing Ye", "Jianwei Peng", "Yu Zhan", "Hong Zhang"], "title": "Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments", "categories": ["cs.RO"], "comment": "https://adap-rpf.github.io/", "summary": "Robot person following (RPF) is a core capability in human-robot interaction,\nenabling robots to assist users in daily activities, collaborative work, and\nother service scenarios. However, achieving practical RPF remains challenging\ndue to frequent occlusions, particularly in dynamic and crowded environments.\nExisting approaches often rely on fixed-point following or sparse\ncandidate-point selection with oversimplified heuristics, which cannot\nadequately handle complex occlusions caused by moving obstacles such as\npedestrians. To address these limitations, we propose an adaptive trajectory\nsampling method that generates dense candidate points within socially aware\nzones and evaluates them using a multi-objective cost function. Based on the\noptimal point, a person-following trajectory is estimated relative to the\npredicted motion of the target. We further design a prediction-aware model\npredictive path integral (MPPI) controller that simultaneously tracks this\ntrajectory and proactively avoids collisions using predicted pedestrian\nmotions. Extensive experiments show that our method outperforms\nstate-of-the-art baselines in smoothness, safety, robustness, and human\ncomfort, with its effectiveness further demonstrated on a mobile robot in\nreal-world scenarios."}
{"id": "2510.11321", "pdf": "https://arxiv.org/pdf/2510.11321", "abs": "https://arxiv.org/abs/2510.11321", "authors": ["Ruizhe Liu", "Pei Zhou", "Qian Luo", "Li Sun", "Jun Cen", "Yibing Song", "Yanchao Yang"], "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data", "categories": ["cs.RO"], "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios."}
{"id": "2510.11401", "pdf": "https://arxiv.org/pdf/2510.11401", "abs": "https://arxiv.org/abs/2510.11401", "authors": ["Jiayang Wu", "Jiongye Li", "Shibowen Zhang", "Zhicheng He", "Zaijin Wang", "Xiaokun Leng", "Hangxin Liu", "Jingwen Zhang", "Jiayi Wang", "Song-Chun Zhu", "Yao Su"], "title": "Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations."}
{"id": "2510.11421", "pdf": "https://arxiv.org/pdf/2510.11421", "abs": "https://arxiv.org/abs/2510.11421", "authors": ["Shih-Chieh Sun", "Yun-Cheng Tsai"], "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability."}
{"id": "2510.11448", "pdf": "https://arxiv.org/pdf/2510.11448", "abs": "https://arxiv.org/abs/2510.11448", "authors": ["Yuankai He", "Hanlin Chen", "Weisong Shi"], "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems", "categories": ["cs.RO", "cs.SY", "eess.SY", "C.3; D.4.1; D.4.4; D.4.8; I.2.9"], "comment": "8 pages,7 figures, 8 tables", "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m)."}
{"id": "2510.11474", "pdf": "https://arxiv.org/pdf/2510.11474", "abs": "https://arxiv.org/abs/2510.11474", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": "2025 IEEE International Conference on Agentic AI (ICA)", "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios."}
{"id": "2510.11491", "pdf": "https://arxiv.org/pdf/2510.11491", "abs": "https://arxiv.org/abs/2510.11491", "authors": ["Murad Dawood", "Usama Ahmed Siddiquie", "Shahram Khorshidi", "Maren Bennewitz"], "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods."}
{"id": "2510.11525", "pdf": "https://arxiv.org/pdf/2510.11525", "abs": "https://arxiv.org/abs/2510.11525", "authors": ["Luis F. Recalde", "Dhruv Agrawal", "Jon Arrizabalaga", "Guanrui Li"], "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters", "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails."}
{"id": "2510.11534", "pdf": "https://arxiv.org/pdf/2510.11534", "abs": "https://arxiv.org/abs/2510.11534", "authors": ["Enli Lin", "Ziyuan Yang", "Qiujing Lu", "Jianming Hu", "Shuo Feng"], "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted by ITSC 2025", "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics."}
{"id": "2510.11539", "pdf": "https://arxiv.org/pdf/2510.11539", "abs": "https://arxiv.org/abs/2510.11539", "authors": ["Denglin Cheng", "Jiarong Kang", "Xiaobin Xiong"], "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms."}
{"id": "2510.11542", "pdf": "https://arxiv.org/pdf/2510.11542", "abs": "https://arxiv.org/abs/2510.11542", "authors": ["Neil C. Janwani", "Varun Madabushi", "Maegan Tucker"], "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion."}
{"id": "2510.11552", "pdf": "https://arxiv.org/pdf/2510.11552", "abs": "https://arxiv.org/abs/2510.11552", "authors": ["Gregoire Passault", "Clement Gaspard", "Olivier Ly"], "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education", "categories": ["cs.RO"], "comment": null, "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable."}
{"id": "2510.11566", "pdf": "https://arxiv.org/pdf/2510.11566", "abs": "https://arxiv.org/abs/2510.11566", "authors": ["Kuanning Wang", "Yongchong Gu", "Yuqian Fu", "Zeyu Shangguan", "Sicheng He", "Xiangyang Xue", "Yanwei Fu", "Daniel Seita"], "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy", "categories": ["cs.RO", "cs.CV"], "comment": "Project page is at https://scoopdiff.github.io/", "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/."}
{"id": "2510.11574", "pdf": "https://arxiv.org/pdf/2510.11574", "abs": "https://arxiv.org/abs/2510.11574", "authors": ["Lennart Werner", "Pol Eyschen", "Sean Costello", "Pierluigi Micarelli", "Marco Hutter"], "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery", "categories": ["cs.RO"], "comment": null, "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision."}
{"id": "2510.11660", "pdf": "https://arxiv.org/pdf/2510.11660", "abs": "https://arxiv.org/abs/2510.11660", "authors": ["Yi Yang", "Kefan Gu", "Yuqing Wen", "Hebei Li", "Yucheng Zhao", "Tiancai Wang", "Xudong Liu"], "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 6 figures, conference", "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/."}
{"id": "2510.11682", "pdf": "https://arxiv.org/pdf/2510.11682", "abs": "https://arxiv.org/abs/2510.11682", "authors": ["Hang Liu", "Yuman Gao", "Sangli Teng", "Yufeng Chi", "Yakun Sophia Shao", "Zhongyu Li", "Maani Ghaffari", "Koushil Sreenath"], "title": "Ego-Vision World Model for Humanoid Contact Planning", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/"}
{"id": "2510.11689", "pdf": "https://arxiv.org/pdf/2510.11689", "abs": "https://arxiv.org/abs/2510.11689", "authors": ["Maggie Wang", "Stephen Tian", "Aiden Swann", "Ola Shorinwa", "Jiajun Wu", "Mac Schwager"], "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ ."}
{"id": "2510.09667", "pdf": "https://arxiv.org/pdf/2510.09667", "abs": "https://arxiv.org/abs/2510.09667", "authors": ["Huaihai Lyu", "Chaofan Chen", "Senwei Xie", "Pengwei Wang", "Xiansheng Chen", "Shanghang Zhang", "Changsheng Xu"], "title": "OmniSAT: Compact Action Token, Faster Auto Regression", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Existing Vision-Language-Action (VLA) models can be broadly categorized into\ndiffusion-based and auto-regressive (AR) approaches: diffusion models capture\ncontinuous action distributions but rely on computationally heavy iterative\ndenoising. In contrast, AR models enable efficient optimization and flexible\nsequence construction, making them better suited for large-scale pretraining.\nTo further improve AR efficiency, particularly when action chunks induce\nextended and high-dimensional sequences, prior work applies entropy-guided and\ntoken-frequency techniques to shorten the sequence length. However, such\ncompression struggled with \\textit{poor reconstruction or inefficient\ncompression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,\nwhich learns a compact, transferable action representation. Specifically, we\nfirst normalize value ranges and temporal horizons to obtain a consistent\nrepresentation with B-Spline encoding. Then, we apply multi-stage residual\nquantization to the position, rotation, and gripper subspaces, producing\ncompressed discrete tokens with coarse-to-fine granularity for each part. After\npre-training on the large-scale dataset Droid, the resulting discrete\ntokenization shortens the training sequence by 6.8$\\times$, and lowers the\ntarget entropy. To further explore the potential of OmniSAT, we develop a\ncross-embodiment learning strategy that builds on the unified action-pattern\nspace and jointly leverages robot and human demonstrations. It enables scalable\nauxiliary supervision from heterogeneous egocentric videos. Across diverse\nreal-robot and simulation experiments, OmniSAT encompasses higher compression\nwhile preserving reconstruction quality, enabling faster AR training\nconvergence and model performance."}
{"id": "2510.09925", "pdf": "https://arxiv.org/pdf/2510.09925", "abs": "https://arxiv.org/abs/2510.09925", "authors": ["James Usevitch", "Juan Augusto Paredes Salazar", "Ankit Goel"], "title": "Computing Safe Control Inputs using Discrete-Time Matrix Control Barrier Functions via Convex Optimization", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "17 pages, 8 figures", "summary": "Control barrier functions (CBFs) have seen widespread success in providing\nforward invariance and safety guarantees for dynamical control systems. A\ncrucial limitation of discrete-time formulations is that CBFs that are\nnonconcave in their argument require the solution of nonconvex optimization\nproblems to compute safety-preserving control inputs, which inhibits real-time\ncomputation of control inputs guaranteeing forward invariance. This paper\npresents a novel method for computing safety-preserving control inputs for\ndiscrete-time systems with nonconvex safety sets, utilizing convex optimization\nand the recently developed class of matrix control barrier function techniques.\nThe efficacy of our methods is demonstrated through numerical simulations on a\nbicopter system."}
{"id": "2510.09976", "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL."}
{"id": "2510.10287", "pdf": "https://arxiv.org/pdf/2510.10287", "abs": "https://arxiv.org/abs/2510.10287", "authors": ["Markus Käppeler", "Özgün Çiçek", "Daniele Cattaneo", "Claudius Gläser", "Yakov Miron", "Abhinav Valada"], "title": "Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Camera-based 3D object detection and tracking are essential for perception in\nautonomous driving. Current state-of-the-art approaches often rely exclusively\non either perspective-view (PV) or bird's-eye-view (BEV) features, limiting\ntheir ability to leverage both fine-grained object details and spatially\nstructured scene representations. In this work, we propose DualViewDistill, a\nhybrid detection and tracking framework that incorporates both PV and BEV\ncamera image features to leverage their complementary strengths. Our approach\nintroduces BEV maps guided by foundation models, leveraging descriptive DINOv2\nfeatures that are distilled into BEV representations through a novel\ndistillation process. By integrating PV features with BEV maps enriched with\nsemantic and geometric features from DINOv2, our model leverages this hybrid\nrepresentation via deformable aggregation to enhance 3D object detection and\ntracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks\ndemonstrate that DualViewDistill achieves state-of-the-art performance. The\nresults showcase the potential of foundation model BEV maps to enable more\nreliable perception for autonomous driving. We make the code and pre-trained\nmodels available at https://dualviewdistill.cs.uni-freiburg.de ."}
{"id": "2510.10325", "pdf": "https://arxiv.org/pdf/2510.10325", "abs": "https://arxiv.org/abs/2510.10325", "authors": ["Walid Abdela"], "title": "KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "The seamless integration of physical and digital environments in\nCyber-Physical Systems(CPS), particularly within Industry 4.0, presents\nsignificant challenges stemming from system heterogeneity and complexity.\nTraditional approaches often rely on rigid, data-centric solutions like\nco-simulation frameworks or brittle point-to-point middleware bridges, which\nlack the semantic richness and flexibility required for intelligent, autonomous\ncoordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent\nInfrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS\nleverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,\nproviding a common semantic foundation for a Multi-Agent System(MAS).\nAutonomous agents, representing both physical and digital components, query\nthis KG for decision-making and update it with real-time state information. The\ninfrastructure features a model-driven architecture which facilitates the\nautomatic generation of agents from semantic descriptions, thereby simplifying\nsystem extension and maintenance. By abstracting away underlying communication\nprotocols and providing a unified, intelligent coordination mechanism, KG-MAS\noffers a robust, scalable, and flexible solution for coupling heterogeneous\nphysical and digital robotic environments."}
{"id": "2510.10434", "pdf": "https://arxiv.org/pdf/2510.10434", "abs": "https://arxiv.org/abs/2510.10434", "authors": ["Kangjian Zhu", "Haobo Jiang", "Yigong Zhang", "Jianjun Qian", "Jian Yang", "Jin Xie"], "title": "MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that\nformulates markerless, image-based robot pose estimation as a conditional\ndenoising diffusion process. The framework consists of two processes: a\nvisibility-constrained diffusion process for diverse pose augmentation and a\ntimestep-aware reverse process for progressive pose refinement. The diffusion\nprocess progressively perturbs ground-truth poses to noisy transformations for\ntraining a pose denoising network. Importantly, we integrate visibility\nconstraints into the process, ensuring the transformations remain within the\ncamera field of view. Compared to the fixed-scale perturbations used in current\nmethods, the diffusion process generates in-view and diverse training poses,\nthereby improving the network generalization capability. Furthermore, the\nreverse process iteratively predicts the poses by the denoising network and\nrefines pose estimates by sampling from the diffusion posterior of current\ntimestep, following a scheduled coarse-to-fine procedure. Moreover, the\ntimestep indicates the transformation scales, which guide the denoising network\nto achieve more accurate pose predictions. The reverse process demonstrates\nhigher robustness than direct prediction, benefiting from its timestep-aware\nrefinement scheme. Our approach demonstrates improvements across two benchmarks\n(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most\nchallenging dataset, representing a 32.3% gain over the state-of-the-art."}
{"id": "2510.10503", "pdf": "https://arxiv.org/pdf/2510.10503", "abs": "https://arxiv.org/abs/2510.10503", "authors": ["Kanishkha Jaisankar", "Sunidhi Tandel"], "title": "Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion planning in complex scenarios is a core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto generate trajectories, while recent approaches leverage large language\nmodels (LLMs) for decision-making. However, it remains unclear whether LLMs\ntruly capture human driving logic. We propose Align2Act, a motion planning\nframework that transforms instruction-tuned LLMs into interpretable planners\naligned with human behavior. We derive structured driving instructions based on\nhuman reasoning patterns (e.g., anticipate hazards, yield at intersections) and\ntraffic rules (e.g., stop at red lights, maintain lane boundaries). Our\nAlign2ActChain module guides step-by-step reasoning to produce both an\ninterpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with\nLoRA on one million scenarios from the nuPlan dataset, our method achieves an\nopen-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and\n66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or\nopen-loop settings, we demonstrate improved planning quality and human-likeness\non the real-world nuPlan closed-loop benchmark. Ablation studies confirm that\nstructured reasoning significantly improves performance over baseline LLM\nplanners."}
{"id": "2510.10520", "pdf": "https://arxiv.org/pdf/2510.10520", "abs": "https://arxiv.org/abs/2510.10520", "authors": ["Fuze Sun", "Paul Craig", "Lingyu Li", "Shixiangyue Meng", "Chuxi Nan"], "title": "AI-Agents for Culturally Diverse Online Higher Education Environments", "categories": ["cs.CY", "cs.RO"], "comment": null, "summary": "As the global reach of online higher education continues to grow,\nuniversities are increasingly accommodating students from diverse cultural\nbackgrounds \\parencite{tereshko2024culturally}. This can present a number of\nchallenges including linguistic barriers \\parencite{ullah2021linguistic},\ncultural differences in learning style \\parencite{omidvar2012cultural},\ncultural sensitivity in course design \\parencite{nguyen2022cultural} and\nperceived isolation when students feel their perspectives or experiences are\nnot reflected or valued in the learning environment\n\\parencite{hansen2022belonging}. Ensuring active engagement and reasonable\nlearning outcomes in such a environments requires distance educational systems\nthat are not only adaptive but also culturally resonant\n\\parencite{dalle2024cultural}. Both embodied and virtual AI-Agents have great\npotential in this regard as they can facilitate personalized learning and adapt\ntheir interactions and content delivery to align with students' cultural\ncontext. In addition Generative AI (GAI), such as, Large Language Models (LLMs)\ncan amplify the potential for these culturally aware AI agents to address\neducational challenges due to their advanced capacity for understanding and\ngenerating contextually relevant content \\parencite{wang2024large}. This\nchapter reviews existing research and suggests the usage of culturally aware\nAI-Agents, powered by GAI, to foster engagement and improve learning outcomes\nin culturally diverse online higher education environments."}
{"id": "2510.10676", "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers."}
{"id": "2510.10823", "pdf": "https://arxiv.org/pdf/2510.10823", "abs": "https://arxiv.org/abs/2510.10823", "authors": ["Daniel Howard"], "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety", "categories": ["cs.AI", "cs.NE", "cs.RO"], "comment": "41 pages, 17 figures, 5 tables", "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors\nthat are internally coherent yet misaligned with reality, arising from\ninteractions among planning, uncertainty handling, and aversive memory. In a\ngrid navigation stack we catalogue recurrent modalities including flip-flop,\nplan churn, perseveration loops, paralysis and hypervigilance, futile search,\nbelief incoherence, tie break thrashing, corridor thrashing, optimality\ncompulsion, metric mismatch, policy oscillation, and limited-visibility\nvariants. For each we give lightweight online detectors and reusable escape\npolicies (short commitments, a margin to switch, smoothing, principled\narbitration). We then show that durable phobic avoidance can persist even under\nfull visibility when learned aversive costs dominate local choice, producing\nlong detours despite globally safe routes. Using First/Second/Third Law as\nengineering shorthand for safety latency, command compliance, and resource\nefficiency, we argue that local fixes are insufficient; global failures can\nremain. To surface them, we propose genetic-programming based destructive\ntesting that evolves worlds and perturbations to maximize law pressure and\nneurosis scores, yielding adversarial curricula and counterfactual traces that\nexpose where architectural revision, not merely symptom-level patches, is\nrequired."}
{"id": "2510.10932", "pdf": "https://arxiv.org/pdf/2510.10932", "abs": "https://arxiv.org/abs/2510.10932", "authors": ["Zonghuan Xu", "Xiang Zheng", "Xingjun Ma", "Yu-Gang Jiang"], "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models", "categories": ["cs.CR", "cs.AI", "cs.RO"], "comment": "8 pages, 8 tables, 1 figure. Under review", "summary": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses."}
{"id": "2510.10933", "pdf": "https://arxiv.org/pdf/2510.10933", "abs": "https://arxiv.org/abs/2510.10933", "authors": ["Jiahong Chen", "Jinghao Wang", "Zi Wang", "Ziwen Wang", "Banglei Guan", "Qifeng Yu"], "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects", "categories": ["cs.CV", "cs.RO"], "comment": "12 pages, 9 figures, submitted to ICRA 2026", "summary": "6D pose estimation of textureless objects is valuable for industrial robotic\napplications, yet remains challenging due to the frequent loss of depth\ninformation. Current multi-view methods either rely on depth data or\ninsufficiently exploit multi-view geometric cues, limiting their performance.\nIn this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level\nfusion using only multi-view RGB images as input. We design a three-stage\nprogressive pose optimization strategy that leverages dense multi-view keypoint\ngeometry information. To enable effective dense keypoint fusion, we enhance the\nkeypoint network with attentional aggregation and symmetry-aware training,\nimproving prediction accuracy and resolving ambiguities on symmetric objects.\nExtensive experiments on the ROBI dataset demonstrate that DKPMV outperforms\nstate-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods\nin the majority of cases. The code will be available soon."}
{"id": "2510.11340", "pdf": "https://arxiv.org/pdf/2510.11340", "abs": "https://arxiv.org/abs/2510.11340", "authors": ["Zhao Huang", "Boyang Sun", "Alexandros Delitzas", "Jiaqi Chen", "Marc Pollefeys"], "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages", "summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet\nexisting datasets remain limited due to the labor-intensive process of\nannotating part segmentation, kinematic types, and motion trajectories. We\npresent REACT3D, a scalable zero-shot framework that converts static 3D scenes\ninto simulation-ready interactive replicas with consistent geometry, enabling\ndirect use in diverse downstream tasks. Our contributions include: (i)\nopenable-object detection and segmentation to extract candidate movable parts\nfrom static scenes, (ii) articulation estimation that infers joint types and\nmotion parameters, (iii) hidden-geometry completion followed by interactive\nobject assembly, and (iv) interactive scene integration in widely supported\nformats to ensure compatibility with standard simulation platforms. We achieve\nstate-of-the-art performance on detection/segmentation and articulation metrics\nacross diverse indoor scenes, demonstrating the effectiveness of our framework\nand providing a practical foundation for scalable interactive scene generation,\nthereby lowering the barrier to large-scale research on articulated scene\nunderstanding. Our project page is\n\\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}."}
{"id": "2510.11501", "pdf": "https://arxiv.org/pdf/2510.11501", "abs": "https://arxiv.org/abs/2510.11501", "authors": ["Emran Yasser Moustafa", "Ivana Dusparic"], "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing", "categories": ["cs.LG", "cs.RO"], "comment": "Accepted to IEEE ICAR 2025", "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors."}
{"id": "2510.11583", "pdf": "https://arxiv.org/pdf/2510.11583", "abs": "https://arxiv.org/abs/2510.11583", "authors": ["Siddhartha Upadhyay", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time Reach-Avoid-Stay Control", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods."}
