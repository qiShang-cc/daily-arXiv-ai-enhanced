{"id": "2509.16223", "pdf": "https://arxiv.org/pdf/2509.16223", "abs": "https://arxiv.org/abs/2509.16223", "authors": ["Huaiyu Chen", "Fahed Hassanat", "Robert Laganiere", "Martin Bouchard"], "title": "MRADNET: a Compact Radar Object Detector with MetaFormer", "categories": ["eess.SP", "cs.CV"], "comment": "5 pages, 2 figures, submitted to IEEE Icassp 2026", "summary": "Frequency-modulated continuous wave radars have gained increasing popularity\nin the automotive industry. Its robustness against adverse weather conditions\nmakes it a suitable choice for radar object detection in advanced driver\nassistance systems. These real-time embedded systems have requirements for the\ncompactness and efficiency of the model, which have been largely overlooked in\nprevious work. In this work, we propose mRadNet, a novel radar object detection\nmodel with compactness in mind. mRadNet employs a U-net style architecture with\nMetaFormer blocks, in which separable convolution and attention token mixers\nare used to capture both local and global features effectively. More efficient\ntoken embedding and merging strategies are introduced to further facilitate the\nlightweight design of the model. The performance of mRadNet is validated on the\nCRUW dataset, improving state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3amRadNet\u7684\u65b0\u578b\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u6ce8\u91cd\u6a21\u578b\u7684\u7d27\u51d1\u6027\u548c\u6548\u7387\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u9891\u8c03\u8fde\u7eed\u6ce2\u96f7\u8fbe\u5728\u6c7d\u8f66\u884c\u4e1a\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u7d27\u51d1\u6027\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cmRadNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "mRadNet\u91c7\u7528U-net\u67b6\u6784\u548cMetaFormer\u6a21\u5757\uff0c\u7ed3\u5408\u53ef\u5206\u5377\u79ef\u548c\u6ce8\u610f\u529b\u4ee4\u724c\u6df7\u5408\u5668\uff0c\u63d0\u5347\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728CRUW\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cmRadNet\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "mRadNet\u901a\u8fc7\u9ad8\u6548\u7684\u8bbe\u8ba1\u548c\u7ed3\u6784\u4f18\u5316\uff0c\u4e3a\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16359", "pdf": "https://arxiv.org/pdf/2509.16359", "abs": "https://arxiv.org/abs/2509.16359", "authors": ["David Campos Anchieta", "John R. Buck"], "title": "Power Spectral Density Estimation via Universal Truncated Order Statistics Filtering", "categories": ["eess.SP"], "comment": null, "summary": "Loud transient signals in underwater acoustic data increase the bias and\nvariance of background noise power spectral density (PSD) estimates based on\nsample mean. Recently, two PSD estimators mitigated the loud transient impact\non PSD estimates by applying order statistics filtering (OSF). The first, the\nSchwock and Abadi Welch Percentile, scales a single rank order statistic (OS)\nof consecutive periodograms. The second, the truncated linear order statistics\nfilter, is a weighted sum of OS up to a chosen rank. In order to minimize\nvariance, both OSFs must carefully choose the highest rank that still\neliminates the loud transients. However, in real-time applications in dynamic\nenvironments, loud transients occur at unpredictable rates, requiring dynamic\nadjustment of the OSF ranks to keep low bias and variance. To circumvent the\nchallenges of real-time rank selection, this paper proposes a convex sum of\nOSFs across ranks with blending weights that are sequentially adjusted to favor\nthe lowest variance OSFs over a recent time window. The performance of the\nblended sum provably approaches the performance of the best fixed rank OSF.\nSimulations and real data confirm the blended OSFs effectively filter loud\ntransients out of spectrograms without explicitly choosing a threshold rank.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6df7\u5408\u6743\u91cd\u6765\u4f18\u5316\u6c34\u4e0b\u58f0\u5b66\u6570\u636e\u4e2d\u77ac\u6001\u4fe1\u53f7\u5904\u7406\u7684PSD\u4f30\u8ba1\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u56fa\u5b9a\u79e9\u9009\u62e9\u7684\u4f9d\u8d56\u3002", "motivation": "\u77ac\u6001\u4fe1\u53f7\u4f1a\u589e\u52a0\u80cc\u666f\u566a\u58f0PSD\u4f30\u8ba1\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\u79e9\u9009\u62e9\uff0c\u64cd\u4f5c\u590d\u6742\u3002", "method": "\u91c7\u7528\u4e0d\u540c\u79e9\u7684OSF\u51f8\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6df7\u5408\u6743\u91cd\uff0c\u4f18\u5148\u9009\u62e9\u65b9\u5dee\u6700\u5c0f\u7684OSF\u3002", "result": "\u6df7\u5408OSF\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u6709\u6548\u8fc7\u6ee4\u77ac\u6001\u4fe1\u53f7\uff0c\u6027\u80fd\u63a5\u8fd1\u56fa\u5b9a\u79e9\u7684\u6700\u4f73OSF\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u663e\u5f0f\u9009\u62e9\u9608\u503c\u79e9\u7684\u590d\u6742\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.16417", "pdf": "https://arxiv.org/pdf/2509.16417", "abs": "https://arxiv.org/abs/2509.16417", "authors": ["Ayla Eftekhari", "Maryam Cheraghy", "Armin Farhadi", "Mohammad Robat Mili", "Qingqing Wu"], "title": "Hybrid FIM and STAR-BD-RIS-Aided Wireless Communications with Short Packet Length: A Meta-TD3 Approach", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable intelligent surfaces (RIS) and flexible intelligent\nmetasurfaces (FIM) have been widely adopted in multi-user wireless\ncommunication systems to enhance channel quality through simultaneous\ntransmission and reflection of signals and three-dimensional reconfiguration of\nantennas. In this paper, we propose a novel system architecture that integrates\nthe benefits of both technologies by deploying an FIM antenna at the base\nstation (BS) and a simultaneously transmitting and reflecting beyond diagonal\nRIS (STAR-BD-RIS) along the transmission path to ensure sufficient received\npower for single-antenna users. The objective is to maximize the achievable sum\nrate considering the short block length by jointly optimizing the FIM surface\nconfiguration, the transmit beamforming vector, and STAR-BD-RIS phase shift\nmatrix subject to practical constraints including minimum\nsignal-to-interference-plus-noise ratio (SINR), power limitations, FIM\nconstraint, and the STAR-BD-RIS phase-shift matrix. To solve the resulting\nnon-convex optimization problem, we develop a learning-based approach that\nincorporates meta-learning into the twin delayed deep deterministic policy\ngradient (TD3) algorithm, referred to as Meta-TD3. The simulation results\ndemonstrate that the proposed hybrid system outperforms conventional\nconfigurations employing either FIM or RIS alone, while the Meta-TD3 algorithm\nachieves superior performance compared to classic learning techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408FIM\u548cSTAR-BD-RIS\u7684\u65b0\u578b\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u914d\u7f6e\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u63d0\u5347\u591a\u7528\u6237\u65e0\u7ebf\u901a\u4fe1\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408FIM\u548cRIS\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4fe1\u9053\u8d28\u91cf\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u6ee1\u8db3\u77ed\u5757\u957f\u5ea6\u6761\u4ef6\u4e0b\u7684\u9ad8\u63a5\u6536\u529f\u7387\u9700\u6c42\u3002", "method": "\u90e8\u7f72FIM\u5929\u7ebf\u548cSTAR-BD-RIS\uff0c\u4f18\u5316FIM\u914d\u7f6e\u3001\u6ce2\u675f\u8d4b\u5f62\u5411\u91cf\u548cRIS\u76f8\u79fb\u77e9\u9635\uff0c\u91c7\u7528\u57fa\u4e8e\u5143\u5b66\u4e60\u7684Meta-TD3\u7b97\u6cd5\u3002", "result": "\u6df7\u5408\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfFIM\u6216RIS\u5355\u72ec\u914d\u7f6e\uff0cMeta-TD3\u7b97\u6cd5\u4f18\u4e8e\u7ecf\u5178\u5b66\u4e60\u6280\u672f\u3002", "conclusion": "\u65b0\u578b\u67b6\u6784\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16536", "pdf": "https://arxiv.org/pdf/2509.16536", "abs": "https://arxiv.org/abs/2509.16536", "authors": ["Johannes Mootz", "Reza Akhavian"], "title": "Advancing Accessible Hand-Arm Vibration Safety Monitoring: ISO-Compliance with Wearable Sensors and Transfer Functions", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "Field workers are frequently exposed to hazardous vibrations, increasing the\nrisk of Hand-Arm Vibration Syndrome (HAVS) and other long-term health problems.\nISO 5349-1 provides guidelines for measuring vibration exposure. However, this\nstandard was established in controlled conditions using high-quality\naccelerometers directly attached to power tool handles. This study investigates\nan alternative, wearable sensor-based data collection process and develops an\nerror-minimization transfer function that derives values comparable to ISO\nbenchmarks for safety monitoring. Experiments are performed with subjects\nhammer drilling into concrete while vibrations are measured using three\naccelerometers at different sampling frequencies. The transfer function maps\nvibration data across sensor positions by accounting for damping effects. The\nfindings indicate a significant reduction in acceleration between the palm and\nupper arm, highlight the impact of sampling frequency on data accuracy, and\nenable accurate comparison of true hand-arm vibration levels with existing\nstandard limits to allow accessible, real-time, and cost-effective HAVS\nprevention.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u632f\u52a8\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u8bef\u5dee\u6700\u5c0f\u5316\u4f20\u9012\u51fd\u6570\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u5de5\u4eba\u624b\u90e8\u632f\u52a8\u66b4\u9732\uff0c\u4ee5\u51cf\u5c11\u5065\u5eb7\u98ce\u9669\u3002", "motivation": "\u73b0\u573a\u5de5\u4eba\u5e38\u66b4\u9732\u4e8e\u6709\u5bb3\u632f\u52a8\uff0c\u589e\u52a0\u624b-\u81c2\u632f\u52a8\u7efc\u5408\u5f81\uff08HAVS\uff09\u7b49\u5065\u5eb7\u95ee\u9898\u7684\u98ce\u9669\u3002\u73b0\u6709\u6807\u51c6ISO 5349-1\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u6761\u4ef6\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u4f20\u611f\u5668\u76f4\u63a5\u56fa\u5b9a\u5728\u5de5\u5177\u4e0a\uff0c\u800c\u5b9e\u9645\u73b0\u573a\u6761\u4ef6\u590d\u6742\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u5728\u4e0d\u540c\u91c7\u6837\u9891\u7387\u4e0b\u6d4b\u91cf\u632f\u52a8\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4f20\u9012\u51fd\u6570\u4ee5\u6821\u6b63\u4f20\u611f\u5668\u4f4d\u7f6e\u5dee\u5f02\u548c\u963b\u5c3c\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u9524\u51fb\u6df7\u51dd\u571f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u624b\u638c\u4e0e\u4e0a\u81c2\u95f4\u632f\u52a8\u663e\u8457\u51cf\u5f31\uff0c\u91c7\u6837\u9891\u7387\u5bf9\u6570\u636e\u51c6\u786e\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f20\u9012\u51fd\u6570\u6210\u529f\u5b9e\u73b0\u4e86\u4e0eISO\u6807\u51c6\u7684\u53ef\u6bd4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u3001\u4f4e\u6210\u672c\u76d1\u6d4bHAVS\u98ce\u9669\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u53ef\u5e2e\u52a9\u9884\u9632\u957f\u671f\u5065\u5eb7\u95ee\u9898\u3002"}}
{"id": "2509.16261", "pdf": "https://arxiv.org/pdf/2509.16261", "abs": "https://arxiv.org/abs/2509.16261", "authors": ["Shuocheng Yang", "Zikun Xu", "Jiahao Wang", "Shahid Nawaz", "Jianqiang Wang", "Shaobing Xu"], "title": "RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Radar has shown strong potential for robust perception in autonomous driving;\nhowever, raw radar images are frequently degraded by noise and \"ghost\"\nartifacts, making object detection based solely on semantic features highly\nchallenging. To address this limitation, we introduce RaFD, a radar-based\nobject detection framework that estimates inter-frame bird's-eye-view (BEV)\nflow and leverages the resulting geometric cues to enhance detection accuracy.\nSpecifically, we design a supervised flow estimation auxiliary task that is\njointly trained with the detection network. The estimated flow is further\nutilized to guide feature propagation from the previous frame to the current\none. Our flow-guided, radar-only detector achieves achieves state-of-the-art\nperformance on the RADIATE dataset, underscoring the importance of\nincorporating geometric information to effectively interpret radar signals,\nwhich are inherently ambiguous in semantics.", "AI": {"tldr": "RaFD\u6846\u67b6\u901a\u8fc7\u4f30\u8ba1BEV\u6d41\u5e76\u7ed3\u5408\u51e0\u4f55\u7ebf\u7d22\uff0c\u63d0\u5347\u4e86\u96f7\u8fbe\u56fe\u50cf\u7684\u7269\u4f53\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u96f7\u8fbe\u56fe\u50cf\u5e38\u53d7\u566a\u58f0\u548c\u2018\u9b3c\u5f71\u2019\u5e72\u6270\uff0c\u4ec5\u4f9d\u8d56\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u76d1\u7763\u6d41\u4f30\u8ba1\u8f85\u52a9\u4efb\u52a1\uff0c\u4e0e\u68c0\u6d4b\u7f51\u7edc\u8054\u5408\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u6d41\u5f15\u5bfc\u524d\u4e00\u5e27\u7279\u5f81\u4f20\u64ad\u81f3\u5f53\u524d\u5e27\u3002", "result": "\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u80fd\u6709\u6548\u89e3\u51b3\u96f7\u8fbe\u4fe1\u53f7\u7684\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2509.16570", "pdf": "https://arxiv.org/pdf/2509.16570", "abs": "https://arxiv.org/abs/2509.16570", "authors": ["Rohit Kumar Singh", "Subrata Kumar", "Shovan Bhaumik"], "title": "Bearing-only Tracking using Towed Sensor-Array with Non-Gaussian Measurement Noise Statistics", "categories": ["eess.SP"], "comment": null, "summary": "Passive bearing-only tracking (BOT) estimates the target states by utilising\nnoisy bearing measurements captured by a sensor array. The sensor array is\noften towed behind the ship, using a long flexible cable to reduce interference\nfrom the own-ship's inherent noises. This forms a towed cable sensor-array\nsystem (TCSAS). During BOT, the tow-ship has to perform a manoeuvre to make the\ntracking system observable. Such a manoeuvre destabilises the TCSAS, thus\nmaking its exact location unknown \\emph{w.r.t.} tow-ship. However, it is very\ncrucial to know the exact location of the towed sensor-array to perform\nefficient and reliable target state estimation. The existing BOT approaches\nperform TMA during own-ship manoeuvre either by pausing the measurement\nupdation step of the estimation algorithm or assuming a fixed aft position for\nthe towed sensor-array. These assumptions lead to unreliable state estimation.\nTo address this, we propose a dynamic model for TCSAS, using a lumped mass\napproach, which will provide the location of the sensor array during the\nown-ship manoeuvre. This location will be fed to the state estimation\nalgorithm. The dynamic of TCSAS in 3D space is obtained by solving the\nequations obtained from the moment balance condition and quasi-static\nequilibrium condition at the lumped mass points. Moreover, the bearing data\ncaptured by the towed sensor-array is corrupted with non-Gaussian noise. It is\nhandled using the maximum correntropy criterion based Kalman filter with a\nkernel bandwidth selection technique, proposed in this paper. The proposed\nsensor-array dynamic model is verified for a real-world BOT engagement\nscenario.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u62d6\u66f3\u7535\u7f06\u4f20\u611f\u5668\u9635\u5217\u7cfb\u7edf\uff08TCSAS\uff09\u7684\u52a8\u6001\u6a21\u578b\uff0c\u7ed3\u5408\u975e\u9ad8\u65af\u566a\u58f0\u7684\u6700\u5927\u76f8\u5173\u71b5\u51c6\u5219\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u4ee5\u63d0\u9ad8\u88ab\u52a8\u4ec5\u65b9\u4f4d\u8ddf\u8e2a\uff08BOT\uff09\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709BOT\u65b9\u6cd5\u5728\u62d6\u8239\u673a\u52a8\u65f6\u6682\u505c\u6d4b\u91cf\u66f4\u65b0\u6216\u5047\u8bbe\u4f20\u611f\u5668\u9635\u5217\u4f4d\u7f6e\u56fa\u5b9a\uff0c\u5bfc\u81f4\u72b6\u6001\u4f30\u8ba1\u4e0d\u53ef\u9760\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u52a8\u6001\u6a21\u578b\u6765\u51c6\u786e\u8ddf\u8e2a\u4f20\u611f\u5668\u9635\u5217\u4f4d\u7f6e\u3002", "method": "\u91c7\u7528\u96c6\u603b\u8d28\u91cf\u65b9\u6cd5\u5efa\u7acbTCSAS\u7684\u52a8\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u6c42\u89e3\u529b\u77e9\u5e73\u8861\u548c\u51c6\u9759\u6001\u5e73\u8861\u6761\u4ef6\u786e\u5b9a3D\u7a7a\u95f4\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u6700\u5927\u76f8\u5173\u71b5\u51c6\u5219\u5361\u5c14\u66fc\u6ee4\u6ce2\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u6a21\u578b\u5728\u5b9e\u9645BOT\u573a\u666f\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u4f20\u611f\u5668\u9635\u5217\u4f4d\u7f6e\uff0c\u6539\u5584\u76ee\u6807\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u5efa\u6a21\u6539\u8fdb\u4f20\u611f\u5668\u9635\u5217\u5b9a\u4f4d\uff0c\u7ed3\u5408\u566a\u58f0\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86BOT\u7684\u53ef\u9760\u6027\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2509.16353", "pdf": "https://arxiv.org/pdf/2509.16353", "abs": "https://arxiv.org/abs/2509.16353", "authors": ["Shaoting Peng", "Dakarai Crowder", "Wenzhen Yuan", "Katherine Driggs-Campbell"], "title": "Tactile-Based Human Intent Recognition for Robot Assistive Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Robot assistive navigation (RAN) is critical for enhancing the mobility and\nindependence of the growing population of mobility-impaired individuals.\nHowever, existing systems often rely on interfaces that fail to replicate the\nintuitive and efficient physical communication observed between a person and a\nhuman caregiver, limiting their effectiveness. In this paper, we introduce\nTac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a\nStretch 3 mobile manipulator to provide a more natural and efficient interface\nfor human navigational intent recognition. To robustly classify the tactile\ndata, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an\nalgorithm that explicitly models the sensor's cylindrical geometry and is\nconsequently robust to the natural rotational shifts present in a user's grasp.\nComprehensive experiments were conducted to demonstrate the effectiveness of\nour classification algorithm and the overall system. Results show that CK-SVM\nachieved superior classification accuracy on both simulated (97.1%) and\nreal-world (90.8%) datasets compared to four baseline models. Furthermore, a\npilot study confirmed that users more preferred the Tac-Nav tactile interface\nover conventional joystick and voice-based controls.", "AI": {"tldr": "Tac-Nav\u662f\u4e00\u79cd\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u63d0\u5347\u5bfc\u822a\u610f\u56fe\u8bc6\u522b\u7684\u81ea\u7136\u6027\u548c\u6548\u7387\uff0c\u91c7\u7528\u7684CK-SVM\u7b97\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u754c\u9762\u4e0d\u591f\u76f4\u89c2\uff0c\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u4e0e\u62a4\u7406\u8005\u4e4b\u95f4\u7684\u9ad8\u6548\u7269\u7406\u4ea4\u6d41\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86Tac-Nav\u7cfb\u7edf\uff0c\u4f7f\u7528\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684CK-SVM\u7b97\u6cd5\u6765\u8bc6\u522b\u7528\u6237\u5bfc\u822a\u610f\u56fe\u3002", "result": "CK-SVM\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523097.1%\u548c90.8%\uff0c\u7528\u6237\u66f4\u504f\u597d\u89e6\u89c9\u754c\u9762\u800c\u975e\u4f20\u7edf\u64cd\u63a7\u65b9\u5f0f\u3002", "conclusion": "Tac-Nav\u901a\u8fc7\u89e6\u89c9\u754c\u9762\u548cCK-SVM\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.16580", "pdf": "https://arxiv.org/pdf/2509.16580", "abs": "https://arxiv.org/abs/2509.16580", "authors": ["Dilshara Herath", "Chinthaka Abeyrathne", "Chamindu Adithya", "Chathura Seneviratne"], "title": "Fusing Spectral Correlation Density Imaging with Deep Learning for Intelligent Fault Diagnosis in Rotating Machinery", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Bearing fault diagnosis in rotating machinery is critical for ensuring\noperational reliability, therefore early fault detection is essential to avoid\ncatastrophic failures and expensive emergency repairs. Traditional methods like\nFast Fourier Transform (FFT) often fail to capture the complex, non-stationary\nnature of vibration signals. This study leverages the cyclostationary\nproperties of vibration data through Spectral Correlation Density (SCD) images\nto enhance fault detection and apply deep learning for classification. Using a\npublicly available dataset with bearing faults seeded in two distinct housings\n(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed\nvibration signals into 2D SCD images to reveal fault-specific periodicities,\nsuch as broadband spectra (2000--8000 Hz) for larger faults. Three\nconvolutional neural network (CNN) models, Custom CNN, ResNet152V2, and\nEfficientNetB0, were developed to classify seven bearing conditions. The custom\nCNN achieved the highest accuracies of 96.58\\% and 94.95\\% on Housing A and B,\nrespectively, followed by ResNet152V2 at 96.49\\% and 95.35\\%, and\nEfficientNetB0 at 94.16\\% and 91.65\\%, respectively. The models' high\naccuracies across different housings demonstrate a robust solution suitable for\ncost-effective condition monitoring deployable near sensing platforms,\ncontributing to applied machine learning for edge intelligence and showcasing\neffective signal processing strategies for handling complex, potentially\nlarge-scale vibration data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u76f8\u5173\u5bc6\u5ea6\uff08SCD\uff09\u56fe\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8f74\u627f\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7CNN\u6a21\u578b\u5728\u590d\u6742\u632f\u52a8\u4fe1\u53f7\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6545\u969c\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u7684\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff08\u5982FFT\uff09\u96be\u4ee5\u5904\u7406\u975e\u5e73\u7a33\u632f\u52a8\u4fe1\u53f7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u65e9\u671f\u68c0\u6d4b\u8f74\u627f\u6545\u969c\u3002", "method": "\u5229\u7528SCD\u56fe\u50cf\u8bc6\u522b\u6545\u969c\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u4e86\u4e09\u79cdCNN\u6a21\u578b\uff08Custom CNN\u3001ResNet152V2\u548cEfficientNetB0\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "Custom CNN\u5728\u4e24\u4e2a\u8f74\u627f\u58f3\uff08A\u548cB\uff09\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\uff08\u5206\u522b\u8fbe96.58%\u548c94.95%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u632f\u52a8\u6570\u636e\u7684\u5904\u7406\u548c\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16398", "pdf": "https://arxiv.org/pdf/2509.16398", "abs": "https://arxiv.org/abs/2509.16398", "authors": ["Francesco Argenziano", "Miguel Saavedra-Ruiz", "Sacha Morin", "Daniele Nardi", "Liam Paull"], "title": "Dynamic Objects Relocalization in Changing Environments with Flow Matching", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Task and motion planning are long-standing challenges in robotics, especially\nwhen robots have to deal with dynamic environments exhibiting long-term\ndynamics, such as households or warehouses. In these environments, long-term\ndynamics mostly stem from human activities, since previously detected objects\ncan be moved or removed from the scene. This adds the necessity to find such\nobjects again before completing the designed task, increasing the risk of\nfailure due to missed relocalizations. However, in these settings, the nature\nof such human-object interactions is often overlooked, despite being governed\nby common habits and repetitive patterns. Our conjecture is that these cues can\nbe exploited to recover the most likely objects' positions in the scene,\nhelping to address the problem of unknown relocalization in changing\nenvironments. To this end we propose FlowMaps, a model based on Flow Matching\nthat is able to infer multimodal object locations over space and time. Our\nresults present statistical evidence to support our hypotheses, opening the way\nto more complex applications of our approach. The code is publically available\nat https://github.com/Fra-Tsuna/flowmaps", "AI": {"tldr": "FlowMaps\u5229\u7528Flow Matching\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u73af\u5883\u4e2d\u7269\u4f53\u7684\u591a\u6a21\u6001\u4f4d\u7f6e\uff0c\u89e3\u51b3\u672a\u77e5\u91cd\u5b9a\u4f4d\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7269\u4f53\u7684\u9891\u7e41\u79fb\u52a8\u589e\u52a0\u4efb\u52a1\u5931\u8d25\u98ce\u9669\uff0c\u5229\u7528\u4eba\u7c7b\u6d3b\u52a8\u6a21\u5f0f\u53ef\u9884\u6d4b\u7269\u4f53\u4f4d\u7f6e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFlow Matching\u7684FlowMaps\u6a21\u578b\uff0c\u63a8\u65ad\u7269\u4f53\u5728\u65f6\u7a7a\u4e2d\u7684\u591a\u6a21\u6001\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u5047\u8bbe\uff0c\u663e\u793a\u6a21\u578b\u6709\u6548\u6027\u3002", "conclusion": "FlowMaps\u4e3a\u590d\u6742\u5e94\u7528\u5960\u5b9a\u57fa\u7840\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16585", "pdf": "https://arxiv.org/pdf/2509.16585", "abs": "https://arxiv.org/abs/2509.16585", "authors": ["Ta Giang Thuy Loan", "Hoang-Lan Nguyen", "Nguyen Thi Ngoc Lan", "Do Hai Son", "Tran Thi Thuy Quynh", "Nguyen Linh Trung", "Karim Abed-Meraim", "Thanh Trung Le"], "title": "Robust Sparse Subspace Tracking from Corrupted Data Observations", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "Subspace tracking is a fundamental problem in signal processing, where the\ngoal is to estimate and track the underlying subspace that spans a sequence of\ndata streams over time. In high-dimensional settings, data samples are often\ncorrupted by non-Gaussian noises and may exhibit sparsity. This paper explores\nthe alpha divergence for sparse subspace estimation and tracking, offering\nrobustness to data corruption. The proposed method outperforms the\nstate-of-the-art robust subspace tracking methods while achieving a low\ncomputational complexity and memory storage. Several experiments are conducted\nto demonstrate its effectiveness in robust subspace tracking and\ndirection-of-arrival (DOA) estimation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u03b1\u6563\u5ea6\u8fdb\u884c\u7a00\u758f\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6570\u636e\u5728\u975e\u9ad8\u65af\u566a\u58f0\u548c\u7a00\u758f\u6027\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\uff0c\u6570\u636e\u6837\u672c\u5e38\u53d7\u5230\u975e\u9ad8\u65af\u566a\u58f0\u5f71\u54cd\u4e14\u53ef\u80fd\u5448\u73b0\u7a00\u758f\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u4e0d\u591f\u9c81\u68d2\u3002", "method": "\u91c7\u7528\u03b1\u6563\u5ea6\u8fdb\u884c\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4e0e\u8ddf\u8e2a\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u9c81\u68d2\u5b50\u7a7a\u95f4\u8ddf\u8e2a\u548cDOA\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6570\u636e\u73af\u5883\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.16412", "pdf": "https://arxiv.org/pdf/2509.16412", "abs": "https://arxiv.org/abs/2509.16412", "authors": ["Zihao Deng", "Peng Gao", "Williard Joshua Jose", "Maggie Wigness", "John Rogers", "Brian Reily", "Christopher Reardon", "Hao Zhang"], "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Coordinated multi-robot navigation is essential for robots to operate as a\nteam in diverse environments. During navigation, robot teams usually need to\nmaintain specific formations, such as circular formations to protect human\nteammates at the center. However, in complex scenarios such as narrow\ncorridors, rigidly preserving predefined formations can become infeasible.\nTherefore, robot teams must be capable of dynamically splitting into smaller\nsubteams and adaptively controlling the subteams to navigate through such\nscenarios while preserving formations. To enable this capability, we introduce\na novel method for SubTeaming and Adaptive Formation (STAF), which is built\nupon a unified hierarchical learning framework: (1) high-level deep graph cut\nfor team splitting, (2) intermediate-level graph learning for facilitating\ncoordinated navigation among subteams, and (3) low-level policy learning for\ncontrolling individual mobile robots to reach their goal positions while\navoiding collisions. To evaluate STAF, we conducted extensive experiments in\nboth indoor and outdoor environments using robotics simulations and physical\nrobot teams. Experimental results show that STAF enables the novel capability\nfor subteaming and adaptive formation control, and achieves promising\nperformance in coordinated multi-robot navigation through challenging\nscenarios. More details are available on the project website:\nhttps://hcrlab.gitlab.io/project/STAF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b50\u56e2\u961f\u4e0e\u81ea\u9002\u5e94\u7f16\u961f\uff08STAF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u5206\u5272\u673a\u5668\u4eba\u56e2\u961f\u5e76\u901a\u8fc7\u590d\u6742\u573a\u666f\u3002", "motivation": "\u9700\u5728\u590d\u6742\u73af\u5883\u4e2d\u4fdd\u6301\u673a\u5668\u4eba\u56e2\u961f\u7f16\u961f\uff0c\u4f46\u521a\u6027\u7f16\u961f\u5728\u72ed\u7a84\u8d70\u5eca\u7b49\u573a\u666f\u4e2d\u4e0d\u53ef\u884c\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff1a\u9ad8\u5c42\u6df1\u5ea6\u56fe\u5207\u5272\u5206\u5272\u56e2\u961f\uff0c\u4e2d\u5c42\u56fe\u5b66\u4e60\u534f\u8c03\u5b50\u56e2\u961f\u5bfc\u822a\uff0c\u4f4e\u5c42\u7b56\u7565\u5b66\u4e60\u63a7\u5236\u5355\u4e2a\u673a\u5668\u4eba\u3002", "result": "\u5728\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0cSTAF\u5c55\u793a\u4e86\u5b50\u56e2\u961f\u548c\u81ea\u9002\u5e94\u7f16\u961f\u63a7\u5236\u7684\u65b0\u80fd\u529b\u3002", "conclusion": "STAF\u5728\u591a\u673a\u5668\u4eba\u534f\u540c\u5bfc\u822a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6311\u6218\u6027\u573a\u666f\u3002"}}
{"id": "2509.16643", "pdf": "https://arxiv.org/pdf/2509.16643", "abs": "https://arxiv.org/abs/2509.16643", "authors": ["Yu Zhou", "Chao Zou", "Nanhao Zhou", "Yanqun Tang", "Xiaoying Zhang", "Haoran Yin", "Xiaoran Liu", "Ruisi He", "Pan Tang", "Weijie Yuan", "Yong Zeng"], "title": "Affine Frequency Division Multiplexing for Communication and Channel Sounding: Requirements, Challenges, and Key Technologies", "categories": ["eess.SP"], "comment": "Under revision in an IEEE Magazine", "summary": "Channel models are crucial for theoretical analysis, performance evaluation,\nand deployment of wireless communication systems. Traditional channel sounding\nsystems are insufficient for handling the dynamic changes of channels in the\nnext-generation space-air-ground-sea integrated networks (SAGSIN), which often\nresults in outdated channel models that fail to provide reliable prior\ninformation for communication systems. To address this challenge, this paper\nproposes an integrated channel sounding and communication (ICSC) method as a\npractical solution. Unlike orthogonal frequency division multiplexing, affine\nfrequency division multiplexing (AFDM) provides a full delay-Doppler\nrepresentation of the channel, achieving optimal diversity in time-frequency\ndoubly dispersive channels and effectively addressing the aforementioned\nchallenges. Thus, we investigate the fundamental principles of AFDM, showing\nhow it enables simultaneous communication and channel sounding, and explore key\nperformance metrics for both functionalities. We also clarify the distinction\nand relationship between channel sounding, estimation, tracking and scatterer\nsensing. Additionally, several potential application scenarios for AFDM-ICSC\nare explored. Finally, we highlight the key challenges in implementing\nAFDM-ICSC, outline future research directions, and provide valuable insights\nfor the continued development of this technology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4fe1\u9053\u63a2\u6d4b\u4e0e\u901a\u4fe1\uff08ICSC\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u4fe1\u9053\u6a21\u578b\u5728\u4e0b\u4e00\u4ee3\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\uff08SAGSIN\uff09\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4eff\u9891\u5206\u591a\u8def\u590d\u7528\uff08AFDM\uff09\u5b9e\u73b0\u901a\u4fe1\u4e0e\u4fe1\u9053\u63a2\u6d4b\u7684\u53cc\u91cd\u529f\u80fd\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u63a2\u6d4b\u7cfb\u7edf\u65e0\u6cd5\u9002\u5e94SAGSIN\u4e2d\u52a8\u6001\u4fe1\u9053\u53d8\u5316\uff0c\u5bfc\u81f4\u4fe1\u9053\u6a21\u578b\u8fc7\u65f6\uff0c\u65e0\u6cd5\u4e3a\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u5148\u9a8c\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eAFDM\u7684ICSC\u65b9\u6cd5\uff0c\u8be5\u6280\u672f\u80fd\u591f\u63d0\u4f9b\u4fe1\u9053\u7684\u5168\u65f6\u5ef6-\u591a\u666e\u52d2\u8868\u793a\uff0c\u5b9e\u73b0\u901a\u4fe1\u4e0e\u4fe1\u9053\u63a2\u6d4b\u7684\u540c\u65f6\u8fdb\u884c\u3002", "result": "\u63a2\u7d22\u4e86AFDM-ICSC\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u9610\u660e\u4e86\u4fe1\u9053\u63a2\u6d4b\u3001\u4f30\u8ba1\u3001\u8ddf\u8e2a\u4e0e\u6563\u5c04\u4f53\u611f\u77e5\u7684\u533a\u522b\u4e0e\u5173\u7cfb\u3002", "conclusion": "\u603b\u7ed3\u4e86AFDM-ICSC\u7684\u5e94\u7528\u573a\u666f\u4e0e\u5b9e\u73b0\u6311\u6218\uff0c\u4e3a\u672a\u6765\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.16434", "pdf": "https://arxiv.org/pdf/2509.16434", "abs": "https://arxiv.org/abs/2509.16434", "authors": ["Ritvik Singh", "Karl Van Wyk", "Pieter Abbeel", "Jitendra Malik", "Nathan Ratliff", "Ankur Handa"], "title": "End-to-end RL Improves Dexterous Grasping Policies", "categories": ["cs.RO", "cs.LG"], "comment": "See our blog post: https://e2e4robotics.com/", "summary": "This work explores techniques to scale up image-based end-to-end learning for\ndexterous grasping with an arm + hand system. Unlike state-based RL,\nvision-based RL is much more memory inefficient, resulting in relatively low\nbatch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is\nstill an attractive method as unlike the more commonly used techniques which\ndistill state-based policies into vision networks, end-to-end RL can allow for\nemergent active vision behaviors. We identify a key bottleneck in training\nthese policies is the way most existing simulators scale to multiple GPUs using\ntraditional data parallelism techniques. We propose a new method where we\ndisaggregate the simulator and RL (both training and experience buffers) onto\nseparate GPUs. On a node with four GPUs, we have the simulator running on three\nof them, and PPO running on the fourth. We are able to show that with the same\nnumber of GPUs, we can double the number of existing environments compared to\nthe previous baseline of standard data parallelism. This allows us to train\nvision-based environments, end-to-end with depth, which were previously\nperforming far worse with the baseline. We train and distill both depth and\nstate-based policies into stereo RGB networks and show that depth distillation\nleads to better results, both in simulation and reality. This improvement is\nlikely due to the observability gap between state and vision policies which\ndoes not exist when distilling depth policies into stereo RGB. We further show\nthat the increased batch size brought about by disaggregated simulation also\nimproves real world performance. When deploying in the real world, we improve\nupon the previous state-of-the-art vision-based results using our end-to-end\npolicies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6a21\u62df\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u8026\u5230\u4e0d\u540cGPU\u4e0a\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u6269\u5c55\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u6570\u636e\u5e76\u884c\u6280\u672f\u5728\u8bad\u7ec3\u57fa\u4e8e\u89c6\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u65f6\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c06\u6a21\u62df\u5668\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08\u5305\u62ec\u8bad\u7ec3\u548c\u7ecf\u9a8c\u7f13\u51b2\u533a\uff09\u89e3\u8026\u5230\u4e0d\u540cGPU\u4e0a\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u591aGPU\u8282\u70b9\u63d0\u9ad8\u73af\u5883\u6570\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u901a\u8fc7\u89e3\u8026\u65b9\u6cd5\uff0c\u73af\u5883\u6570\u91cf\u76f8\u8f83\u4f20\u7edf\u6570\u636e\u5e76\u884c\u6280\u672f\u7ffb\u500d\uff0c\u6df1\u5ea6\u84b8\u998f\u81f3RGB\u7f51\u7edc\u7684\u7b56\u7565\u8868\u73b0\u66f4\u4f18\uff0c\u4eff\u771f\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u89e3\u8026\u6a21\u62df\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6548\u679c\u3002"}}
{"id": "2509.16688", "pdf": "https://arxiv.org/pdf/2509.16688", "abs": "https://arxiv.org/abs/2509.16688", "authors": ["\u00d6zlem Tu\u011ffe Demir", "Emil Bj\u00f6rnson"], "title": "Near-Field Channel Estimation with ELAA Modular Arrays Under Hardware Impairments", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "7 pages, 5 figures, IEEE PIMRC 2025", "summary": "Extremely large-scale antenna arrays (ELAAs) enable high spatial resolution\nand multiplexing, especially for user equipments (UEs) in the radiative\nnear-field. To reduce hardware cost, modular ELAA architectures with\ndistributed baseband units (BBUs) are gaining traction. This paper addresses\nnear-field line-of-sight (LOS) channel estimation under low noise amplifier\n(LNA)-induced hardware impairments in such modular systems. We propose\ncomputationally efficient estimators that exploit the array geometry and\nconstant-modulus structure of near-field LOS channels, including a novel\ntwo-dimensional (2D) discrete Fourier transform (DFT) masking technique that\nimproves estimation accuracy and significantly reduces fronthaul signaling.\nNumerical results show that the proposed methods significantly outperform the\nconventional least squares (LS) method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u5757\u5316\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u7684\u8fd1\u573a\u89c6\u8ddd\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9635\u5217\u51e0\u4f55\u548c\u6052\u6a21\u7ed3\u6784\uff0c\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u524d\u4f20\u4fe1\u53f7\u3002", "motivation": "\u6a21\u5757\u5316\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u867d\u7136\u80fd\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u786c\u4ef6\u635f\u4f24\uff08\u5982\u4f4e\u566a\u58f0\u653e\u5927\u5668\uff09\u4f1a\u5f71\u54cd\u4fe1\u9053\u4f30\u8ba1\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9635\u5217\u51e0\u4f55\u548c\u6052\u6a21\u7ed3\u6784\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5305\u62ec\u65b0\u578b\u4e8c\u7ef4DFT\u63a9\u819c\u6280\u672f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4fe1\u9053\u4f30\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.16445", "pdf": "https://arxiv.org/pdf/2509.16445", "abs": "https://arxiv.org/abs/2509.16445", "authors": ["Naoki Yokoyama", "Sehoon Ha"], "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning", "categories": ["cs.RO"], "comment": null, "summary": "Enabling robotic assistants to navigate complex environments and locate\nobjects described in free-form language is a critical capability for real-world\ndeployment. While foundation models, particularly Vision-Language Models\n(VLMs), offer powerful semantic understanding, effectively adapting their\nweb-scale knowledge for embodied decision-making remains a key challenge. We\npresent FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that\ndirectly fine-tunes pre-trained VLM as the navigation policy. In contrast to\nmethods that use foundation models primarily in a zero-shot manner or for map\nannotation, FiLM-Nav learns to select the next best exploration frontier by\nconditioning directly on raw visual trajectory history and the navigation goal.\nLeveraging targeted simulated embodied experience allows the VLM to ground its\npowerful pre-trained representations in the specific dynamics and visual\npatterns relevant to goal-driven navigation. Critically, fine-tuning on a\ndiverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary\nspatial reasoning task proves essential for achieving robustness and broad\ngeneralization. FiLM-Nav sets a new state-of-the-art in both SPL and success\nrate on HM3D ObjectNav among open-vocabulary methods, and sets a\nstate-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating\nstrong generalization to unseen object categories. Our work validates that\ndirectly fine-tuning VLMs on diverse simulated embodied data is a highly\neffective pathway towards generalizable and efficient semantic navigation\ncapabilities.", "AI": {"tldr": "FiLM-Nav\u662f\u4e00\u79cd\u901a\u8fc7\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u5b9e\u73b0\u5bfc\u822a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u57fa\u4e8e\u8f68\u8ff9\u5386\u53f2\u548c\u5bfc\u822a\u76ee\u6807\u9009\u62e9\u6700\u4f73\u63a2\u7d22\u8def\u5f84\uff0c\u5728HM3D ObjectNav\u548cHM3D-OVON\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u4f7f\u673a\u5668\u4eba\u52a9\u624b\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u5e76\u5b9a\u4f4d\u81ea\u7531\u8bed\u8a00\u63cf\u8ff0\u7684\u5bf9\u8c61\uff0c\u5229\u7528VLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bfc\u822a\u6311\u6218\u3002", "method": "FiLM-Nav\u76f4\u63a5\u5fae\u8c03\u9884\u8bad\u7ec3VLM\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\uff0c\u7ed3\u5408\u6a21\u62df\u6570\u636e\uff08ObjectNav\u3001OVON\u3001ImageNav\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u9002\u914d\u5177\u4f53\u7684\u5bfc\u822a\u52a8\u6001\u548c\u89c6\u89c9\u6a21\u5f0f\u3002", "result": "FiLM-Nav\u5728HM3D ObjectNav\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684SPL\u548c\u6210\u529f\u7387\uff0c\u5728HM3D-OVON\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8fbe\u5230\u4e86SPL\u7684\u6700\u4f18\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u5bf9\u8c61\u7c7b\u522b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u76f4\u63a5\u5728\u591a\u6837\u5316\u7684\u6a21\u62df\u6570\u636e\u4e0a\u5fae\u8c03VLM\u662f\u5b9e\u73b0\u901a\u7528\u4e14\u9ad8\u6548\u8bed\u4e49\u5bfc\u822a\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.16776", "pdf": "https://arxiv.org/pdf/2509.16776", "abs": "https://arxiv.org/abs/2509.16776", "authors": ["Hassaan Hashmi", "Spyridon Pougkakiotis", "Dionysis Kalogerias"], "title": "Data-Driven Two-Stage IRS-Aided Sumrate Maximization with Inexact Precoding", "categories": ["eess.SP", "math.OC"], "comment": null, "summary": "We propose iZoSGA, a data-driven learning algorithm for joint passive\nlong-term intelligent reflective surface (IRS)-aided beamforming and active\nshort-term precoding in wireless networks. iZoSGA is based on a zeroth-order\nstochastic quasigradient ascent methodology designed for tackling two-stage\nnonconvex stochastic programs with continuous uncertainty and objective\nfunctions with \"black-box\" terms, and where second-stage optimization is\ninexact. As such, iZoSGA utilizes inexact precoding oracles, enabling practical\nimplementation when short-term (e.g., WMMSE-based) beamforming is solved\napproximately. The proposed method is agnostic to channel models or statistics,\nand applies to arbitrary IRS/network configurations. We prove non-asymptotic\nconvergence of iZoSGA to a neighborhood of a stationary solution of the\noriginal exact problem under minimal assumptions. Our numerics confirm the\nefficacy iZoSGA in several \"inexact regimes\", enabling passive yet fully\neffective IRS operation in diverse and realistic IRS-aided scenarios.", "AI": {"tldr": "iZoSGA\u662f\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8054\u5408\u88ab\u52a8\u957f\u671f\u667a\u80fd\u53cd\u5c04\u9762(IRS)\u6ce2\u675f\u6210\u5f62\u548c\u4e3b\u52a8\u77ed\u671f\u9884\u7f16\u7801\u3002", "motivation": "\u89e3\u51b3\u5177\u6709\u8fde\u7eed\u4e0d\u786e\u5b9a\u6027\u548c", "method": "\u57fa\u4e8e\u96f6\u9636\u968f\u673a\u62df\u68af\u5ea6\u4e0a\u5347\u65b9\u6cd5\uff0c\u5904\u7406\u5177\u6709", "result": "\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86iZoSGA\u6536\u655b\u5230\u539f\u59cb\u95ee\u9898\u7684\u7a33\u5b9a\u89e3\u90bb\u57df\u3002", "conclusion": "iZoSGA\u5728\u591a\u79cd\u4e0d\u7cbe\u786e\u573a\u666f\u4e2d\u6709\u6548\uff0c\u652f\u6301IRS\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u9ad8\u6548\u8fd0\u884c\u3002"}}
{"id": "2509.16469", "pdf": "https://arxiv.org/pdf/2509.16469", "abs": "https://arxiv.org/abs/2509.16469", "authors": ["Guglielmo Cervettini", "Roberto Mauceri", "Alex Coppola", "Fabio Bergonti", "Luca Fiorio", "Marco Maggiali", "Daniele Pucci"], "title": "A Framework for Optimal Ankle Design of Humanoid Robots", "categories": ["cs.RO"], "comment": "This paper has been accepted for publication at the 2025 IEEE-RAS\n  24th International Conference on Humanoid Robots (Humanoids), Seoul, 2025", "summary": "The design of the humanoid ankle is critical for safe and efficient ground\ninteraction. Key factors such as mechanical compliance and motor mass\ndistribution have driven the adoption of parallel mechanism architectures.\nHowever, selecting the optimal configuration depends on both actuator\navailability and task requirements. We propose a unified methodology for the\ndesign and evaluation of parallel ankle mechanisms. A multi-objective\noptimization synthesizes the mechanism geometry, the resulting solutions are\nevaluated using a scalar cost function that aggregates key performance metrics\nfor cross-architecture comparison. We focus on two representative\narchitectures: the Spherical-Prismatic-Universal (SPU) and the\nRevolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and\nfor the RSU, introduce a parameterization that ensures workspace feasibility\nand accelerates optimization. We validate our approach by redesigning the ankle\nof an existing humanoid robot. The optimized RSU consistently outperforms both\nthe original serial design and a conventionally engineered RSU, reducing the\ncost function by up to 41% and 14%, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5e76\u8054\u8e1d\u5173\u8282\u673a\u6784\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u751f\u6210\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u6807\u91cf\u6210\u672c\u51fd\u6570\u8bc4\u4f30\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4f18\u5316\u8bbe\u8ba1\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u8e1d\u5173\u8282\u7684\u8bbe\u8ba1\u5bf9\u5b89\u5168\u9ad8\u6548\u7684\u5730\u9762\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u8bbe\u8ba1\u4f9d\u8d56\u4e8e\u6267\u884c\u5668\u53ef\u7528\u6027\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u7684\u5e76\u8054\u8e1d\u5173\u8282\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5206\u6790\u4e86SPU\u548cRSU\u4e24\u79cd\u67b6\u6784\uff0c\u6539\u8fdb\u4e86RSU\u7684\u53ef\u884c\u5de5\u4f5c\u7a7a\u95f4\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u4f18\u5316\u540e\u7684RSU\u8bbe\u8ba1\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u539f\u59cb\u4e32\u884c\u8bbe\u8ba1\u548c\u4f20\u7edfRSU\uff0c\u6210\u672c\u51fd\u6570\u5206\u522b\u964d\u4f4e41%\u548c14%\u3002", "conclusion": "\u7edf\u4e00\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u5e76\u8054\u8e1d\u5173\u8282\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u652f\u6301\u3002"}}
{"id": "2509.16854", "pdf": "https://arxiv.org/pdf/2509.16854", "abs": "https://arxiv.org/abs/2509.16854", "authors": ["Nianzu Li", "Weidong Mei", "Lipeng Zhu", "Peiran Wu", "Boyu Ning"], "title": "On the Secrecy Performance of Pinching-Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "Pinching-antenna systems have recently gained significant attention as a\nnovel reconfigurable-antenna technology due to its exceptional capability of\nmitigating signal-propagation path loss. In this letter, we investigate the\nsecrecy performance of a pinching-antenna system in the presence of an\neavesdropper. In particular, we derive an approximate expression of the\nsystem's secrecy outage probability (SOP) with respect to the random locations\nof the legitimate user and eavesdropper and analyze its asymptotic behavior.\nMoreover, we derive a constant performance lower bound on the SOP of the\nconsidered system, i.e., $\\frac{2\\pi-1}{24}$, which is significantly lower than\nthat of conventional fixed-position antenna systems, i.e., $0.5$. Finally,\nsimulation results are provided to validate the correctness of our analytical\nresults.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5939\u6301\u5929\u7ebf\u7684\u4fdd\u5bc6\u6027\u80fd\uff0c\u5206\u6790\u4e86\u5176\u5728\u7a83\u542c\u8005\u5b58\u5728\u65f6\u7684\u4fdd\u5bc6\u4e2d\u65ad\u6982\u7387\uff08SOP\uff09\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u6e10\u8fd1\u884c\u4e3a\u548c\u6027\u80fd\u4e0b\u9650\u3002", "motivation": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u56e0\u5176\u5728\u51cf\u5c11\u4fe1\u53f7\u4f20\u64ad\u8def\u5f84\u635f\u8017\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u7a83\u542c\u73af\u5883\u4e0b\u7684\u4fdd\u5bc6\u6027\u80fd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u63a8\u5bfc\u4e86\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u7684\u4fdd\u5bc6\u4e2d\u65ad\u6982\u7387\u7684\u8fd1\u4f3c\u8868\u8fbe\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6e10\u8fd1\u884c\u4e3a\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u6027\u80fd\u4e0b\u9650\u3002", "result": "\u63a8\u5bfc\u7684\u6027\u80fd\u4e0b\u9650\u4e3a$\frac{2\u03c0\u22121}{24}$\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u7cfb\u7edf\u76840.5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u5728\u4fdd\u5bc6\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5929\u7ebf\u7cfb\u7edf\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16482", "pdf": "https://arxiv.org/pdf/2509.16482", "abs": "https://arxiv.org/abs/2509.16482", "authors": ["Pranav Tiwari", "Soumyodipta Nath"], "title": "Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.DS", "physics.app-ph", "49", "I.2.9"], "comment": "6 Pages, 8 Figures. First two authors have contributed equally", "summary": "Coordinated path following in multi-agent systems is a key challenge in\nrobotics, with applications in automated logistics, surveillance, and\ncollaborative exploration. Traditional formation control techniques often rely\non time-parameterized trajectories and path integrals, which can result in\nsynchronization issues and rigid behavior. In this work, we address the problem\nof sequential path following, where agents maintain fixed spatial separation\nalong a common trajectory, guided by a leader under centralized control. We\nintroduce Robot Conga, a leader-follower control strategy that updates each\nagent's desired state based on the leader's spatial displacement rather than\ntime, assuming access to a global position reference, an assumption valid in\nindoor environments equipped with motion capture, vision-based tracking, or UWB\nlocalization systems. The algorithm was validated in simulation using both\nTurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate\ntrajectory tracking, stable inter-agent spacing, and fast convergence, with all\nagents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped\ncase, and almost instantaneously in the TurtleBot3 implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u4f4d\u79fb\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u63a7\u5236\u7b56\u7565Robot Conga\uff0c\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u8def\u5f84\u8ddf\u968f\u4e2d\u7684\u540c\u6b65\u548c\u521a\u6027\u884c\u4e3a\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u65f6\u95f4\u7684\u8def\u5f84\u8ddf\u968f\u65b9\u6cd5\u5728\u540c\u6b65\u548c\u7075\u6d3b\u6027\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u7b56\u7565\u6765\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8def\u5f84\u8ddf\u968f\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9886\u5bfc\u8005-\u8ddf\u968f\u7b56\u7565\uff0c\u901a\u8fc7\u7a7a\u95f4\u4f4d\u79fb\u800c\u975e\u65f6\u95f4\u53c2\u6570\u66f4\u65b0\u8ddf\u968f\u8005\u7684\u671f\u671b\u72b6\u6001\uff0c\u4f9d\u8d56\u5168\u5c40\u4f4d\u7f6e\u53c2\u8003\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0cTurtleBot3\u548c\u56db\u8db3\u673a\u5668\u4eba\u90fd\u80fd\u5feb\u901f\u6536\u655b\u5e76\u4fdd\u6301\u7a33\u5b9a\u7684\u95f4\u8ddd\u3002", "conclusion": "Robot Conga\u7b97\u6cd5\u5728\u591a\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u968f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u73af\u5883\u3002"}}
{"id": "2509.16910", "pdf": "https://arxiv.org/pdf/2509.16910", "abs": "https://arxiv.org/abs/2509.16910", "authors": ["Daxiang Li", "Zhichao Zhang"], "title": "Graph Fractional Hilbert Transform: Theory and Application", "categories": ["eess.SP", "eess.IV"], "comment": "32 pages, 6 figures", "summary": "The graph Hilbert transform (GHT) is a key tool in constructing analytic\nsignals and extracting envelope and phase information in graph signal\nprocessing. However, its utility is limited by confinement to the graph Fourier\ndomain, a fixed phase shift, information loss for real-valued spectral\ncomponents, and the absence of tunable parameters. The graph fractional Fourier\ntransform introduces domain flexibility through a fractional order parameter\n$\\alpha$ but does not resolve the issues of phase rigidity and information\nloss. Inspired by the dual-parameter fractional Hilbert transform (FRHT) in\nclassical signal processing, we propose the graph FRHT (GFRHT). The GFRHT\nincorporates a dual-parameter framework: the fractional order $\\alpha$ enables\nanalysis across arbitrary fractional domains, interpolating between vertex and\nspectral spaces, while the angle parameter $\\beta$ provides adjustable phase\nshifts and a non-zero real-valued response ($\\cos\\beta$) for real eigenvalues,\nthereby eliminating information loss. We formally define the GFRHT, establish\nits core properties, and design a method for graph analytic signal\nconstruction, enabling precise envelope extraction and demodulation.\nExperiments on edge detection, anomaly identification, and speech\nclassification demonstrate that GFRHT outperforms GHT, offering greater\nflexibility and superior performance in graph signal processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u56fe\u4fe1\u53f7\u5904\u7406\u5de5\u5177\u2014\u2014\u56fe\u5206\u6570\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362\uff08GFRHT\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u56fe\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362\uff08GHT\uff09\u5728\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u5305\u62ec\u56fa\u5b9a\u7684\u76f8\u4f4d\u504f\u79fb\u3001\u4fe1\u606f\u4e22\u5931\u4ee5\u53ca\u5bf9\u56fe\u5085\u91cc\u53f6\u57df\u7684\u4f9d\u8d56\u3002\u8bba\u6587\u76ee\u6807\u662f\u63d0\u51fa\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u56fe\u5206\u6570\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362\uff08GFRHT\uff09\uff0c\u91c7\u7528\u53cc\u53c2\u6570\u6846\u67b6\uff08\u5206\u6570\u9636\u03b1\u548c\u89d2\u5ea6\u03b2\uff09\uff0c\u901a\u8fc7\u53ef\u53d8\u76f8\u4f4d\u548c\u5206\u6570\u57df\u5206\u6790\uff0c\u89e3\u51b3\u4e86GHT\u7684\u4e0d\u8db3\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u4fe1\u53f7\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGFRHT\u5728\u56fe\u4fe1\u53f7\u7684\u8fb9\u7f18\u68c0\u6d4b\u3001\u5f02\u5e38\u8bc6\u522b\u548c\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGHT\uff0c\u5c55\u73b0\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "GFRHT\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u56fe\u4fe1\u53f7\u5904\u7406\u5de5\u5177\uff0c\u89e3\u51b3\u4e86GHT\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u652f\u6301\u3002"}}
{"id": "2509.16492", "pdf": "https://arxiv.org/pdf/2509.16492", "abs": "https://arxiv.org/abs/2509.16492", "authors": ["Tinapat Limsila", "Mehul Sharma", "Paulo Garcia"], "title": "Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms", "categories": ["cs.RO"], "comment": null, "summary": "Emergent properties in distributed systems arise due to timing\nunpredictability; asynchronous state evolution within each sub-system may lead\nthe macro-system to faulty meta-states. Empirical validation of correctness is\noften prohibitively expensive, as the size of the state-space is too large to\nbe tractable. In robotic swarms this problem is exacerbated, when compared to\nsoftware systems, by the variability of the implementation substrate across the\ndesign, or even the deployment, process. We present an approach for formally\nreasoning about the correctness of robotic swarm design in a\nsubstrate-timing-independent way. By leveraging concurrent process calculi\n(namely, Communicating Sequential Processes), we introduce a methodology that\ncan automatically identify possible causes of faulty meta-states and correct\nsuch designs such that meta-states are consistently stable, even in the\npresence of timing variability due to substrate changes. We evaluate this\napproach on a robotic swarm with a clearly identified fault, realized in both\nsimulation and reality. Results support the research hypothesis, showing that\nthe swarm reaches an illegal meta-state before the correction is applied, but\nbehaves consistently correctly after the correction. Our techniques are\ntransferable across different design methodologies, contributing to the toolbox\nof formal methods for roboticists.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u53d1\u8fc7\u7a0b\u6f14\u7b97\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u673a\u5668\u4eba\u7fa4\u4f53\u8bbe\u8ba1\u4e2d\u7684\u65f6\u95f4\u65e0\u5173\u6027\u6b63\u786e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u7ea0\u6b63\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u5143\u72b6\u6001\u7684\u8bbe\u8ba1\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u65f6\u5e8f\u4e0d\u53ef\u9884\u6d4b\u6027\u4f1a\u5bfc\u81f4\u9519\u8bef\u5143\u72b6\u6001\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\uff0c\u800c\u5b9e\u8bc1\u9a8c\u8bc1\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u72ec\u7acb\u4e8e\u5e95\u5c42\u65f6\u5e8f\u8fdb\u884c\u5206\u6790\u3002", "method": "\u5229\u7528\u5e76\u53d1\u8fc7\u7a0b\u6f14\u7b97\uff08\u5982\u901a\u4fe1\u987a\u5e8f\u8fdb\u7a0b\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc6\u522b\u548c\u7ea0\u6b63\u6f5c\u5728\u9519\u8bef\u5143\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u8bbe\u8ba1\u5728\u65f6\u5e8f\u53d8\u5316\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5e76\u7ea0\u6b63\u95ee\u9898\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u4fee\u6b63\u540e\u884c\u4e3a\u4e00\u81f4\u6b63\u786e\u3002", "conclusion": "\u8be5\u6280\u672f\u53ef\u63a8\u5e7f\u81f3\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u5de5\u5177\u3002"}}
{"id": "2509.16919", "pdf": "https://arxiv.org/pdf/2509.16919", "abs": "https://arxiv.org/abs/2509.16919", "authors": ["Huong Hoang", "Keito Suzuki", "Truong Nguyen", "Pamela Cosman"], "title": "Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics", "categories": ["eess.SP", "cs.MM"], "comment": null, "summary": "For dynamic human motion sequences, the original KeyNode-Driven codec often\nstruggles to retain compression efficiency when confronted with rapid movements\nor strong non-rigid deformations. This paper proposes a novel Bi-modal coding\nframework that enhances the flexibility of motion representation by integrating\nsemantic segmentation and region-specific transformation modeling. The rigid\ntransformation model (rotation & translation) is extended with a hybrid scheme\nthat selectively applies affine transformations-rotation, translation, scaling,\nand shearing-only to deformation-rich regions (e.g., the torso, where loose\nclothing induces high variability), while retaining rigid models elsewhere. The\naffine model is decomposed into minimal parameter sets for efficient coding and\ncombined through a component selection strategy guided by a Lagrangian\nRate-Distortion optimization. The results show that the Bi-modal method\nachieves more accurate mesh deformation, especially in sequences involving\ncomplex non-rigid motion, without compromising compression efficiency in\nsimpler regions, with an average bit-rate saving of 33.81% compared to the\nbaseline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u6001\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u533a\u57df\u7279\u5b9a\u53d8\u6362\u5efa\u6a21\uff0c\u63d0\u5347\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u5e8f\u5217\u7684\u538b\u7f29\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5feb\u901f\u8fd0\u52a8\u548c\u975e\u521a\u6027\u53d8\u5f62\u573a\u666f\u4e0b\u3002", "motivation": "\u539f\u59cb\u7684KeyNode-Driven\u7f16\u89e3\u7801\u5668\u5728\u5904\u7406\u5feb\u901f\u8fd0\u52a8\u6216\u5f3a\u70c8\u975e\u521a\u6027\u53d8\u5f62\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u6001\u7f16\u7801\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u548c\u533a\u57df\u7279\u5b9a\u53d8\u6362\u5efa\u6a21\uff0c\u6df7\u5408\u5e94\u7528\u521a\u6027\u548c\u4eff\u5c04\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u7387-\u5931\u771f\u4f18\u5316\u6307\u5bfc\u53c2\u6570\u9009\u62e9\u548c\u7f16\u7801\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u53cc\u6a21\u6001\u65b9\u6cd5\u5728\u590d\u6742\u975e\u521a\u6027\u8fd0\u52a8\u4e2d\u80fd\u66f4\u51c6\u786e\u5730\u5b9e\u73b0\u7f51\u683c\u53d8\u5f62\uff0c\u540c\u65f6\u5728\u4e0d\u589e\u52a0\u7b80\u5355\u533a\u57df\u538b\u7f29\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u6bd4\u7279\u7387\u8282\u7701\u4e8633.81%\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u5e8f\u5217\u7684\u538b\u7f29\u6548\u7387\u548c\u53d8\u5f62\u8868\u73b0\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u521a\u6027\u53d8\u5f62\u65f6\u6548\u679c\u7a81\u51fa\u3002"}}
{"id": "2509.16532", "pdf": "https://arxiv.org/pdf/2509.16532", "abs": "https://arxiv.org/abs/2509.16532", "authors": ["Run Yu", "Yangdi Liu", "Wen-Da Wei", "Chen Li"], "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recently,vision-based robotic manipulation has garnered significant attention\nand witnessed substantial advancements. 2D image-based and 3D point cloud-based\npolicy learning represent two predominant paradigms in the field, with recent\nstudies showing that the latter consistently outperforms the former in terms of\nboth policy performance and generalization, thereby underscoring the value and\nsignificance of 3D information. However, 3D point cloud-based approaches face\nthe significant challenge of high data acquisition costs, limiting their\nscalability and real-world deployment. To address this issue, we propose a\nnovel framework NoReal3D: which introduces the 3DStructureFormer, a learnable\n3D perception module capable of transforming monocular images into\ngeometrically meaningful pseudo-point cloud features, effectively fused with\nthe 2D encoder output features. Specially, the generated pseudo-point clouds\nretain geometric and topological structures so we design a pseudo-point cloud\nencoder to preserve these properties, making it well-suited for our framework.\nWe also investigate the effectiveness of different feature fusion\nstrategies.Our framework enhances the robot's understanding of 3D spatial\nstructures while completely eliminating the substantial costs associated with\n3D point cloud acquisition.Extensive experiments across various tasks validate\nthat our framework can achieve performance comparable to 3D point cloud-based\nmethods, without the actual point cloud data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNoReal3D\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc73DStructureFormer\u6a21\u5757\u5c06\u5355\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u51e0\u4f55\u4e0a\u6709\u610f\u4e49\u7684\u4f2a\u70b9\u4e91\u7279\u5f81\uff0c\u89e3\u51b3\u4e863D\u70b9\u4e91\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf93D\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u3002", "motivation": "\u89e3\u51b33D\u70b9\u4e91\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u63013D\u4fe1\u606f\u5bf9\u6027\u80fd\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa3DStructureFormer\u6a21\u5757\uff0c\u751f\u6210\u4f2a\u70b9\u4e91\u7279\u5f81\u5e76\u4e0e2D\u7f16\u7801\u5668\u8f93\u51fa\u7279\u5f81\u878d\u5408\uff0c\u8bbe\u8ba1\u4f2a\u70b9\u4e91\u7f16\u7801\u5668\u4fdd\u7559\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u57fa\u4e8e\u771f\u5b9e\u70b9\u4e91\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "NoReal3D\u6846\u67b6\u6709\u6548\u66ff\u4ee3\u4e86\u4f20\u7edf\u70b9\u4e91\u6570\u636e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002"}}
{"id": "2509.16921", "pdf": "https://arxiv.org/pdf/2509.16921", "abs": "https://arxiv.org/abs/2509.16921", "authors": ["Seyong Kim", "Jeonghun Park"], "title": "Asymptotic Scaling Law Analysis of Multicast Satellite Communications with Massive MIMO", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we consider a geostationary orbit (GEO) satellite\ncommunication system that employs massive multiple-input multiple-output (MIMO)\nfor multicast transmission. By modeling the spatial distribution of ground\nusers using a Poisson point process (PPP) and assuming a fixed-beam precoding\nis adopted, we find a closed-form expression for the asymptotical rate scaling\nlaw as a function of the number of antennas and the scaling factors of user\ndensity and multicast users. From the derived analytical expression, we reveal\nthat the rate degradation caused by multicast transmission can be precisely\ncompensated by increasing the user density accordingly.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728GEO\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u4f7f\u7528\u5927\u89c4\u6a21MIMO\u8fdb\u884c\u591a\u64ad\u4f20\u8f93\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u7528\u6237\u5bc6\u5ea6\u7684\u589e\u52a0\u53ef\u4ee5\u7cbe\u786e\u8865\u507f\u591a\u64ad\u4f20\u8f93\u5e26\u6765\u7684\u901f\u7387\u4e0b\u964d\u3002", "motivation": "\u7814\u7a76\u5728GEO\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u91c7\u7528\u5927\u89c4\u6a21MIMO\u548c\u591a\u64ad\u4f20\u8f93\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u7528\u6237\u5bc6\u5ea6\u7684\u8c03\u6574\u6765\u4f18\u5316\u901f\u7387\u3002", "method": "\u901a\u8fc7\u6cca\u677e\u70b9\u8fc7\u7a0b\u5efa\u6a21\u5730\u9762\u7528\u6237\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u56fa\u5b9a\u6ce2\u675f\u9884\u7f16\u7801\uff0c\u63a8\u5bfc\u51fa\u901f\u7387\u7f29\u653e\u89c4\u5f8b\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u589e\u52a0\u7528\u6237\u5bc6\u5ea6\uff0c\u53ef\u4ee5\u7cbe\u786e\u62b5\u6d88\u591a\u64ad\u4f20\u8f93\u5e26\u6765\u7684\u901f\u7387\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u4e3aGEO\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21MIMO\u591a\u64ad\u4f20\u8f93\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5c55\u793a\u4e86\u7528\u6237\u5bc6\u5ea6\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16550", "pdf": "https://arxiv.org/pdf/2509.16550", "abs": "https://arxiv.org/abs/2509.16550", "authors": ["Yinghao Wu", "Shuhong Hou", "Haowen Zheng", "Yichen Li", "Weiyi Lu", "Xun Zhou", "Yitian Shao"], "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 7 figures", "summary": "Robotic manipulation tasks such as inserting a key into a lock or plugging a\nUSB device into a port can fail when visual perception is insufficient to\ndetect misalignment. In these situations, touch sensing is crucial for the\nrobot to monitor the task's states and make precise, timely adjustments.\nCurrent touch sensing solutions are either insensitive to detect subtle changes\nor demand excessive sensor data. Here, we introduce TranTac, a data-efficient\nand low-cost tactile sensing and control framework that integrates a single\ncontact-sensitive 6-axis inertial measurement unit within the elastomeric tips\nof a robotic gripper for completing fine insertion tasks. Our customized\nsensing system can detect dynamic translational and torsional deformations at\nthe micrometer scale, enabling the tracking of visually imperceptible pose\nchanges of the grasped object. By leveraging transformer-based encoders and\ndiffusion policy, TranTac can imitate human insertion behaviors using transient\ntactile cues detected at the gripper's tip during insertion processes. These\ncues enable the robot to dynamically control and correct the 6-DoF pose of the\ngrasped object. When combined with vision, TranTac achieves an average success\nrate of 79% on object grasping and insertion tasks, outperforming both\nvision-only policy and the one augmented with end-effector 6D force/torque\nsensing. Contact localization performance is also validated through\ntactile-only misaligned insertion tasks, achieving an average success rate of\n88%. We assess the generalizability by training TranTac on a single prism-slot\npair and testing it on unseen data, including a USB plug and a metal key, and\nfind that the insertion tasks can still be completed with an average success\nrate of nearly 70%. The proposed framework may inspire new robotic tactile\nsensing systems for delicate manipulation tasks.", "AI": {"tldr": "TranTac\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u6570\u636e\u89e6\u89c9\u611f\u5e94\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5355\u70b9\u89e6\u654f6\u8f74IMU\u548c\u53d8\u538b\u5668\u7f16\u7801\u5668\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cbe\u7ec6\u63d2\u5165\u4efb\u52a1\u7684\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u65f6\u673a\u5668\u4eba\u63d2\u5165\u4efb\u52a1\u5931\u8d25\u7684\u95ee\u9898\uff0c\u5229\u7528\u89e6\u89c9\u611f\u5e94\u5b9e\u73b0\u7cbe\u786e\u8c03\u6574\u3002", "method": "\u96c6\u62106\u8f74IMU\u4e8e\u5f39\u6027\u4f53\u5939\u6301\u5668\u5c16\u7aef\uff0c\u7ed3\u5408\u53d8\u538b\u5668\u7f16\u7801\u5668\u548c\u6269\u6563\u7b56\u7565\uff0c\u6a21\u4eff\u4eba\u7c7b\u63d2\u5165\u884c\u4e3a\u3002", "result": "TranTac\u5728\u7ed3\u5408\u89c6\u89c9\u7684\u60c5\u51b5\u4e0b\u63d2\u5165\u4efb\u52a1\u6210\u529f\u7387\u8fbe79%\uff0c\u4f18\u4e8e\u7eaf\u89c6\u89c9\u548c6D\u529b\u4f20\u611f\u65b9\u6cd5\uff1b\u89e6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u6210\u529f\u7387\u8fbe88%\u3002", "conclusion": "TranTac\u5c55\u793a\u4e86\u5728\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u63a8\u5e7f\u81f3\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u4efb\u52a1\u3002"}}
{"id": "2509.17101", "pdf": "https://arxiv.org/pdf/2509.17101", "abs": "https://arxiv.org/abs/2509.17101", "authors": ["Shiyong Chen"], "title": "Functional WMMSE Algorithm for Continuous Aperture Array Systems", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we propose a functional extension of the weighted minimum\nmean-squared error (WMMSE) algorithm for downlink beamforming in multiuser\nmultiple-input multiple-output (MU-MIMO) systems where both the base station\n(BS) and the users employ continuous-aperture arrays (CAPAs). The method lifts\nthe matrices and vectors in the classical discrete WMMSE recursion to\ncontinuous functions by replacing matrix products and inner products with\nintegrals over the apertures. In practice, we apply a Galerkin projection to\nmap functions to coefficient matrices, solve the resulting discrete WMMSE\nproblem via closed-form updates, and then lift these updates back to the\nfunctional domain. All integrals are implemented using Gauss-Legendre\nquadrature, which preserves the closed-form structure through weighted matrix\nproducts. Simulations show that the proposed method outperforms baselines in\nboth spectral efficiency (SE) and computational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8fde\u7eed\u5b54\u5f84\u9635\u5217MU-MIMO\u7cfb\u7edf\u7684\u529f\u80fd\u6269\u5c55WMMSE\u7b97\u6cd5\uff0c\u901a\u8fc7\u79ef\u5206\u66ff\u4ee3\u77e9\u9635\u8fd0\u7b97\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7684MU-MIMO\u7cfb\u7edf\uff0c\u4f20\u7edf\u79bb\u6563WMMSE\u7b97\u6cd5\u4e0d\u9002\u7528\uff0c\u9700\u6269\u5c55\u4e3a\u8fde\u7eed\u51fd\u6570\u5f62\u5f0f\u4ee5\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u3002", "method": "\u5c06\u79bb\u6563WMMSE\u7b97\u6cd5\u4e2d\u7684\u77e9\u9635\u548c\u5411\u91cf\u63d0\u5347\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u7528Galerkin\u6295\u5f71\u548cGauss-Legendre\u79ef\u5206\u5b9e\u73b0\u95ed\u5f0f\u66f4\u65b0\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9891\u8c31\u6548\u7387\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u529f\u80fd\u6269\u5c55WMMSE\u7b97\u6cd5\u4e3a\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16611", "pdf": "https://arxiv.org/pdf/2509.16611", "abs": "https://arxiv.org/abs/2509.16611", "authors": ["Xiwei Zhao", "Yiwei Wang", "Yansong Wu", "Fan Wu", "Teng Sun", "Zhonghua Miao", "Sami Haddadin", "Alois Knoll"], "title": "Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly", "categories": ["cs.RO"], "comment": null, "summary": "Modern manufacturing demands robotic assembly systems with enhanced\nflexibility and reliability. However, traditional approaches often rely on\nprogramming tailored to each product by experts for fixed settings, which are\ninherently inflexible to product changes and lack the robustness to handle\nvariations. As Behavior Trees (BTs) are increasingly used in robotics for their\nmodularity and reactivity, we propose a novel hierarchical framework,\nVideo-to-BT, that seamlessly integrates high-level cognitive planning with\nlow-level reactive control, with BTs serving both as the structured output of\nplanning and as the governing structure for execution. Our approach leverages a\nVision-Language Model (VLM) to decompose human demonstration videos into\nsubtasks, from which Behavior Trees are generated. During the execution, the\nplanned BTs combined with real-time scene interpretation enable the system to\noperate reactively in the dynamic environment, while VLM-driven replanning is\ntriggered upon execution failure. This closed-loop architecture ensures\nstability and adaptivity. We validate our framework on real-world assembly\ntasks through a series of experiments, demonstrating high planning reliability,\nrobust performance in long-horizon assembly tasks, and strong generalization\nacross diverse and perturbed conditions. Project website:\nhttps://video2bt.github.io/video2bt_page/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideo-to-BT\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u6811\uff08BTs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5c06\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u884c\u4e3a\u6811\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u53ef\u9760\u7684\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u4ea7\u54c1\u53d8\u5316\u548c\u73af\u5883\u6ce2\u52a8\u3002\u4e3a\u63d0\u5347\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u5408\u884c\u4e3a\u6811\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5206\u6790\u4eba\u7c7b\u793a\u8303\u89c6\u9891\uff0c\u751f\u6210\u884c\u4e3a\u6811\uff08BTs\uff09\u3002\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u7ed3\u5408\u5b9e\u65f6\u573a\u666f\u89e3\u8bfb\u548cVLM\u9a71\u52a8\u7684\u91cd\u65b0\u89c4\u5212\uff0c\u5f62\u6210\u4e00\u4e2a\u95ed\u73af\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u771f\u5b9e\u88c5\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u89c4\u5212\u53ef\u9760\u6027\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u7a33\u5065\u6027\u80fd\uff0c\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u548c\u5e72\u6270\u6761\u4ef6\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Video-to-BT\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u6811\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2509.17181", "pdf": "https://arxiv.org/pdf/2509.17181", "abs": "https://arxiv.org/abs/2509.17181", "authors": ["Mahdi Shamsi", "Hadi Zayyani", "Farokh Marvasti"], "title": "Resilient Signal Reflection under CSI Perturbations: A Robust Approach for Secure RIS Communication", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative\ntechnology in wireless communication, enabling dynamic control over signal\npropagation. This paper tackles the challenge of mitigating Channel State\nInformation (CSI) perturbations in RIS-aided systems, particularly for secure\ncommunication scenarios. Leveraging a first-order approximation technique, we\ndevelop a robust approach that strengthens the resilience of RIS configurations\nagainst CSI imperfections. The study considers both untrusted user interception\nand stealth radar applications, focusing on optimizing signal reflection and\ntransmission in the presence of eavesdroppers. Simulation results demonstrate\nnotable gains in security and efficiency while maintaining low computational\ncomplexity. By extending the stability range, the proposed method updates RIS\nelements using only a few matrix-vector multiplications, eliminating the need\nfor repeated inverse or pseudo-inverse computations under small channel\nperturbations. Additionally, the framework provides a baseline for quantifying\nalgorithmic sensitivity to CSI variations. Overall, the findings underscore the\npotential of RIS to enable secure and reliable communication in next-generation\nnetworks such as 6G.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\u6280\u672f\u589e\u5f3aRIS\u914d\u7f6e\u5bf9CSI\u6270\u52a8\u7684\u6297\u5e72\u6270\u80fd\u529b\uff0c\u4f18\u5316\u4fe1\u53f7\u53cd\u5c04\u548c\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3RIS\u8f85\u52a9\u7cfb\u7edf\u4e2dCSI\u6270\u52a8\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u901a\u4fe1\u573a\u666f\u4e2d\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u4e00\u9636\u8fd1\u4f3c\u6280\u672f\u5f00\u53d1\u9c81\u68d2\u65b9\u6cd5\uff0c\u4f18\u5316RIS\u914d\u7f6e\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u5b89\u5168\u6027\u548c\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86RIS\u5728\u4e0b\u4e00\u4ee3\u7f51\u7edc\uff08\u59826G\uff09\u4e2d\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u901a\u4fe1\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16614", "pdf": "https://arxiv.org/pdf/2509.16614", "abs": "https://arxiv.org/abs/2509.16614", "authors": ["Bojan Deraji\u0107", "Sebastian Bernhard", "Wolfgang H\u00f6nig"], "title": "ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Control barrier functions (CBFs) have been demonstrated as an effective\nmethod for safety-critical control of autonomous systems. Although CBFs are\nsimple to deploy, their design remains challenging, motivating the development\nof learning-based approaches. Yet, issues such as suboptimal safe sets,\napplicability in partially observable environments, and lack of rigorous safety\nguarantees persist. In this work, we propose observation-conditioned neural\nCBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately\nrecover the maximal safe sets. We exploit certain mathematical properties of\nthe HJ value function, ensuring that the predicted safe set never intersects\nwith the observed failure set. Moreover, we leverage a hypernetwork-based\narchitecture that is particularly suitable for the design of\nobservation-conditioned safety filters. The proposed method is examined both in\nsimulation and hardware experiments for a ground robot and a quadcopter. The\nresults show improved success rates and generalization to out-of-domain\nenvironments compared to the baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u7684\u89c2\u6d4b\u6761\u4ef6\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u5b66\u4e60\u578bCBFs\u4e2d\u7684\u6b21\u4f18\u5b89\u5168\u96c6\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u9002\u7528\u6027\u548c\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CBFs\u8bbe\u8ba1\u590d\u6742\uff0c\u5b66\u4e60\u578b\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u96c6\u4e0d\u8db3\u3001\u90e8\u5206\u89c2\u6d4b\u73af\u5883\u4e0d\u9002\u7528\u4e14\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u4fdd\u8bc1\u7684\u7f3a\u9677\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u57fa\u4e8eHamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u8bbe\u8ba1\u89c2\u6d4b\u6761\u4ef6\u795e\u7ecfCBFs\uff0c\u5229\u7528HJ\u503c\u51fd\u6570\u7684\u6570\u5b66\u7279\u6027\u786e\u4fdd\u5b89\u5168\u96c6\u4e0d\u4e0e\u5931\u8d25\u96c6\u76f8\u4ea4\uff0c\u5e76\u91c7\u7528\u8d85\u7f51\u7edc\u67b6\u6784\u9002\u914d\u5b89\u5168\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u3002", "result": "\u5728\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5730\u9762\u673a\u5668\u4eba\u548c\u56db\u8f74\u98de\u884c\u5668\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709CBFs\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b89\u5168\u63a7\u5236\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.17267", "pdf": "https://arxiv.org/pdf/2509.17267", "abs": "https://arxiv.org/abs/2509.17267", "authors": ["Taorui Chen", "Yuki Gao", "Yi Wang", "Hai-Han Sun"], "title": "Estimation of Specific Gravity of Potato Tubers Using Dielectric Properties", "categories": ["eess.SP"], "comment": null, "summary": "Potatoes are an economically important crop, and their quality is closely\nrelated to the starch content, which is typically inferred from specific\ngravity (SG). Although microwave sensing technologies have been increasingly\ndeveloped for underground potato detection and quality assessment in recent\nyears, no accurate model has yet been established to link the dielectric\nproperties of potatoes with their key agronomic traits. To address this gap, we\ndeveloped a model for estimating potato tubers' SG based on their dielectric\nconstant. To construct and validate the model, we conducted SG measurements and\ndielectric spectroscopy measurements in the frequency range of 0.3 GHz to 3.0\nGHz on 250 potatoes of five different types (red, russet, yellow, purple, and\nchipping potatoes, with 50 samples per type). Out of the 250 data sets, 200\ndata sets were used for model development, and 50 data sets were used for model\nvalidation. A linear regression model was used to summarize the relationship\nbetween SG and dielectric constant, where the regression coefficients are\nexpressed as fourth-order polynomial functions of frequency. Experimental\nresults on the 50 validation data sets show that the model achieves high\nestimation accuracy with mean absolute errors (MAE) less than\n\\(4.8\\times10^{-3}\\) and mean absolute percentage errors (MAPE) less than\n0.45\\%. The study of the dielectric properties of potatoes, along with the\nderived SG estimation model, provides a foundation for the future development\nof microwave sensing technologies for agronomic trait assessment in the potato\nproduction and processing industries. All measured data will be made publicly\navailable upon acceptance of the paper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ecb\u7535\u5e38\u6570\u4f30\u8ba1\u571f\u8c46\u6bd4\u91cd\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u6ce2\u4f20\u611f\u6280\u672f\u4e3a\u571f\u8c46\u751f\u4ea7\u548c\u52a0\u5de5\u4e2d\u7684\u519c\u827a\u6027\u72b6\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u76ee\u524d\u5fae\u6ce2\u4f20\u611f\u6280\u672f\u7528\u4e8e\u5730\u4e0b\u571f\u8c46\u68c0\u6d4b\u548c\u8d28\u91cf\u8bc4\u4f30\uff0c\u4f46\u7f3a\u4e4f\u5c06\u571f\u8c46\u4ecb\u7535\u7279\u6027\u4e0e\u5176\u5173\u952e\u519c\u827a\u6027\u72b6\uff08\u5982\u6bd4\u91cd\uff09\u5173\u8054\u7684\u51c6\u786e\u6a21\u578b\u3002", "method": "\u57280.3 GHz\u81f33.0 GHz\u9891\u7387\u8303\u56f4\u5185\uff0c\u5bf95\u79cd\u7c7b\u578b\u7684250\u4e2a\u571f\u8c46\u8fdb\u884c\u4e86\u6bd4\u91cd\u6d4b\u91cf\u548c\u4ecb\u7535\u5149\u8c31\u6d4b\u91cf\uff0c\u5e76\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u6784\u5efa\u4e86\u6bd4\u91cd\u4e0e\u4ecb\u7535\u5e38\u6570\u7684\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u5728\u9a8c\u8bc1\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee(MAE)\u5c0f\u4e8e4.8\u00d710\u22123\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee(MAPE)\u5c0f\u4e8e0.45%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fae\u6ce2\u4f20\u611f\u6280\u672f\u5728\u571f\u8c46\u751f\u4ea7\u548c\u52a0\u5de5\u4e2d\u7684\u519c\u827a\u6027\u72b6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u6d4b\u91cf\u6570\u636e\u3002"}}
{"id": "2509.16615", "pdf": "https://arxiv.org/pdf/2509.16615", "abs": "https://arxiv.org/abs/2509.16615", "authors": ["Jelle Luijkx", "Runyu Ma", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning", "categories": ["cs.RO", "68T40", "I.2.9; I.2.6; I.2.7"], "comment": "8 pages, 7 figures", "summary": "Reinforcement learning (RL) is a promising approach for robotic manipulation,\nbut it can suffer from low sample efficiency and requires extensive exploration\nof large state-action spaces. Recent methods leverage the commonsense knowledge\nand reasoning abilities of large language models (LLMs) to guide exploration\ntoward more meaningful states. However, LLMs can produce plans that are\nsemantically plausible yet physically infeasible, yielding unreliable behavior.\nWe introduce LLM-TALE, a framework that uses LLMs' planning to directly steer\nRL exploration. LLM-TALE integrates planning at both the task level and the\naffordance level, improving learning efficiency by directing agents toward\nsemantically meaningful actions. Unlike prior approaches that assume optimal\nLLM-generated plans or rewards, LLM-TALE corrects suboptimality online and\nexplores multimodal affordance-level plans without human supervision. We\nevaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing\nimprovements in both sample efficiency and success rates over strong baselines.\nReal-robot experiments indicate promising zero-shot sim-to-real transfer. Code\nand supplementary material are available at https://llm-tale.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM-TALE\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5212\u80fd\u529b\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u63a2\u7d22\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6837\u672c\u6548\u7387\u4f4e\u4e14\u63a2\u7d22\u7a7a\u95f4\u5927\uff0c\u800cLLM\u751f\u6210\u7684\u4efb\u52a1\u89c4\u5212\u53ef\u80fd\u5b58\u5728\u7269\u7406\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "method": "LLM-TALE\u5728\u4efb\u52a1\u5c42\u9762\u548c\u53ef\u64cd\u4f5c\u6027\u5c42\u9762\u6574\u5408LLM\u7684\u89c4\u5212\uff0c\u52a8\u6001\u7ea0\u6b63\u6b21\u4f18\u8ba1\u5212\u5e76\u5b9e\u73b0\u591a\u6a21\u5f0f\u63a2\u7d22\u3002", "result": "\u5728\u6807\u51c6RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-TALE\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "LLM-TALE\u901a\u8fc7\u6709\u6548\u5229\u7528LLM\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17344", "pdf": "https://arxiv.org/pdf/2509.17344", "abs": "https://arxiv.org/abs/2509.17344", "authors": ["Sven Hinderer", "Manuel Buchfink", "Bin Yang"], "title": "On Mutual Information Neural Estimation for Localization", "categories": ["eess.SP"], "comment": null, "summary": "Mutual information (MI) is a promising candidate measure for the assessment\nand optimization of localization systems, as it captures nonlinear dependencies\nbetween random variables. However, the high cost of computing MI, especially\nfor high-dimensional problems, prohibits its application for many real-world\nlocalization systems. We evaluate an algorithm from a new class of neural MI\nestimators called Mutual Information Neural Estimation (MINE) to approximate\nthe MI between the set of feasible user element (UE) locations and the\ncorresponding set of measurements from said UE locations used for positioning.\nWe apply this estimator to a simulated multilateration (MLAT) system, where the\ntrue MI for benchmarking can be approximated by Monte Carlo simulation. The\nestimator is experimentally evaluated w.r.t. its convergence and consistency\nand we investigate the usefulness of MI for assessing simple MLAT systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u4e92\u4fe1\u606f\u4f30\u8ba1\u7b97\u6cd5\uff08MINE\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5728\u6a21\u62df\u7684\u591a\u8fb9\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6536\u655b\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e92\u4fe1\u606f\u662f\u8bc4\u4f30\u548c\u4f18\u5316\u5b9a\u4f4d\u7cfb\u7edf\u7684\u7406\u60f3\u6307\u6807\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u5728\u591a\u7ef4\u95ee\u9898\u4e2d\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u4e92\u4fe1\u606f\u4f30\u8ba1\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528MINE\u7b97\u6cd5\u4f30\u7b97\u7528\u6237\u8bbe\u5907\u4f4d\u7f6e\u4e0e\u76f8\u5173\u6d4b\u91cf\u6570\u636e\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u5728\u6a21\u62df\u591a\u8fb9\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u5e94\u7528\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MINE\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e92\u4fe1\u606f\u5728\u8bc4\u4f30\u7b80\u5355\u591a\u8fb9\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "MINE\u7b97\u6cd5\u4e3a\u89e3\u51b3\u4e92\u4fe1\u606f\u8ba1\u7b97\u7684\u9ad8\u6210\u672c\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u5b9a\u4f4d\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u3002"}}
{"id": "2509.16638", "pdf": "https://arxiv.org/pdf/2509.16638", "abs": "https://arxiv.org/abs/2509.16638", "authors": ["Jinrui Han", "Weiji Xie", "Jiakun Zheng", "Jiyuan Shi", "Weinan Zhang", "Ting Xiao", "Chenjia Bai"], "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning versatile whole-body skills by tracking various human motions is a\nfundamental step toward general-purpose humanoid robots. This task is\nparticularly challenging because a single policy must master a broad repertoire\nof motion skills while ensuring stability over long-horizon sequences. To this\nend, we present VMS, a unified whole-body controller that enables humanoid\nrobots to learn diverse and dynamic behaviors within a single policy. Our\nframework integrates a hybrid tracking objective that balances local motion\nfidelity with global trajectory consistency, and an Orthogonal\nMixture-of-Experts (OMoE) architecture that encourages skill specialization\nwhile enhancing generalization across motions. A segment-level tracking reward\nis further introduced to relax rigid step-wise matching, enhancing robustness\nwhen handling global displacements and transient inaccuracies. We validate VMS\nextensively in both simulation and real-world experiments, demonstrating\naccurate imitation of dynamic skills, stable performance over minute-long\nsequences, and strong generalization to unseen motions. These results highlight\nthe potential of VMS as a scalable foundation for versatile humanoid whole-body\ncontrol. The project page is available at\nhttps://kungfubot2-humanoid.github.io.", "AI": {"tldr": "VMS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5168\u8eab\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6df7\u5408\u8ddf\u8e2a\u76ee\u6807\u548cOMoE\u67b6\u6784\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u8fd0\u52a8\u6280\u80fd\u7684\u6a21\u4eff\u4e0e\u7a33\u5b9a\u6027\u3002", "motivation": "\u901a\u8fc7\u5b66\u4e60\u591a\u6837\u5316\u7684\u5168\u8eab\u8fd0\u52a8\u6280\u80fd\uff0c\u63d0\u5347\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u7684\u591a\u529f\u80fd\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u6df7\u5408\u8ddf\u8e2a\u76ee\u6807\u548cOMoE\u67b6\u6784\uff0c\u5f15\u5165\u4e86\u5206\u6bb5\u7ea7\u522b\u8ddf\u8e2a\u5956\u52b1\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\uff0cVMS\u80fd\u591f\u51c6\u786e\u6a21\u4eff\u52a8\u6001\u6280\u80fd\uff0c\u5e76\u5728\u957f\u65f6\u95f4\u5e8f\u5217\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VMS\u4f5c\u4e3a\u591a\u529f\u80fd\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7684\u6269\u5c55\u57fa\u7840\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.17483", "pdf": "https://arxiv.org/pdf/2509.17483", "abs": "https://arxiv.org/abs/2509.17483", "authors": ["Qianqian Li", "Lintao Li", "Lixiang Liu", "Lei Yang", "Caihong Gong", "Hua Li", "Shiya Hao", "Xiaoming Dai"], "title": "On the Design of Capacity-Achieving Distributions for Discrete-Time Poisson Channel with Low-Precision ADCs", "categories": ["eess.SP", "cs.PF"], "comment": null, "summary": "This paper investigates the design of the capacity-achieving input\ndistribution for the discrete-time Poisson channel (DTPC) under dark current\neffects with low-precision analog-to-digital converters (ADCs). This study\nintroduces an efficient optimization algorithm that integrates the\nNewton-Raphson and Blahut-Arimoto (BA) methods to determine the\ncapacity-achieving input distribution and the corresponding amplitudes of input\nmass points for the DTPC, subject to both peak and average power constraints.\nAdditionally, the Karush-Kuhn-Tucker (KKT) conditions are established to\nprovide necessary and sufficient conditions for the optimality of the obtained\ncapacity-achieving distribution. Simulation results illustrate that the\nproposed algorithm attains $72\\%$ and $83\\%$ of the theoretical capacity at 5\ndB for 1-bit and 2-bit quantized DTPC, respectively. Furthermore, for a\nfinite-precision quantized DTPC (i.e., ${\\log _2}K$ bits), the capacity can be\nachieved by a non-uniform discrete input distribution with support for $K$ mass\npoints, under the given power constraints.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6697\u7535\u6d41\u6548\u5e94\u7684\u79bb\u6563\u65f6\u95f4\u6cca\u677e\u4fe1\u9053\uff08DTPC\uff09\u5728\u4f4e\u7cbe\u5ea6\u6a21\u6570\u8f6c\u6362\u5668\uff08ADC\uff09\u4e0b\u7684\u5bb9\u91cf\u8fbe\u5230\u8f93\u5165\u5206\u5e03\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u725b\u987f-\u62c9\u592b\u900a\u548cBlahut-Arimoto\uff08BA\uff09\u65b9\u6cd5\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u4f4e\u7cbe\u5ea6ADC\u548c\u6697\u7535\u6d41\u6548\u5e94\u4e0b\uff0c\u79bb\u6563\u65f6\u95f4\u6cca\u677e\u4fe1\u9053\uff08DTPC\uff09\u7684\u5bb9\u91cf\u8fbe\u5230\u8f93\u5165\u5206\u5e03\uff0c\u4ee5\u4f18\u5316\u4fe1\u9053\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u725b\u987f-\u62c9\u592b\u900a\u548cBlahut-Arimoto\uff08BA\uff09\u65b9\u6cd5\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5229\u7528KKT\u6761\u4ef6\u9a8c\u8bc1\u6700\u4f18\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c1-bit\u548c2-bit\u91cf\u5316DTPC\u57285 dB\u4e0b\u5206\u522b\u8fbe\u5230\u7406\u8bba\u5bb9\u91cf\u768472%\u548c83%\u3002", "conclusion": "\u6709\u9650\u7cbe\u5ea6\u91cf\u5316\u7684DTPC\u53ef\u4ee5\u901a\u8fc7\u975e\u5747\u5300\u79bb\u6563\u8f93\u5165\u5206\u5e03\u5b9e\u73b0\u5bb9\u91cf\u6700\u5927\u5316\u3002"}}
{"id": "2509.16757", "pdf": "https://arxiv.org/pdf/2509.16757", "abs": "https://arxiv.org/abs/2509.16757", "authors": ["Haoyang Weng", "Yitang Li", "Nikhil Sobanbabu", "Zihan Wang", "Zhengyi Luo", "Tairan He", "Deva Ramanan", "Guanya Shi"], "title": "HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos", "categories": ["cs.RO"], "comment": "website: hdmi-humanoid.github.io", "summary": "Enabling robust whole-body humanoid-object interaction (HOI) remains\nchallenging due to motion data scarcity and the contact-rich nature. We present\nHDMI (HumanoiD iMitation for Interaction), a simple and general framework that\nlearns whole-body humanoid-object interaction skills directly from monocular\nRGB videos. Our pipeline (i) extracts and retargets human and object\ntrajectories from unconstrained videos to build structured motion datasets,\n(ii) trains a reinforcement learning (RL) policy to co-track robot and object\nstates with three key designs: a unified object representation, a residual\naction space, and a general interaction reward, and (iii) zero-shot deploys the\nRL policies on real humanoid robots. Extensive sim-to-real experiments on a\nUnitree G1 humanoid demonstrate the robustness and generality of our approach:\nHDMI achieves 67 consecutive door traversals and successfully performs 6\ndistinct loco-manipulation tasks in the real world and 14 tasks in simulation.\nOur results establish HDMI as a simple and general framework for acquiring\ninteractive humanoid skills from human videos.", "AI": {"tldr": "HDMI\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u5355\u76eeRGB\u89c6\u9891\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u6570\u636e\u7a00\u7f3a\u548c\u63a5\u89e6\u5bc6\u96c6\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba-\u7269\u4f53\u4ea4\u4e92(WOI)\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u63a5\u89e6\u5bc6\u96c6\u95ee\u9898\uff0c\u63d0\u5347\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5e76\u91cd\u5b9a\u5411\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u548c\u7269\u4f53\u8f68\u8ff9\u6784\u5efa\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7edf\u4e00\u7269\u4f53\u8868\u793a\u3001\u6b8b\u5dee\u52a8\u4f5c\u7a7a\u95f4\u548c\u4ea4\u4e92\u5956\u52b1\u8bad\u7ec3RL\u7b56\u7565\uff0c\u5e76\u76f4\u63a5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e8667\u6b21\u8fde\u7eed\u95e8\u7a7f\u8d8a\u548c6\u79cd\u771f\u5b9e\u4e16\u754c\u7684\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u4ee5\u53ca14\u79cd\u6a21\u62df\u4efb\u52a1\u3002", "conclusion": "HDMI\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u4ea4\u4e92\u6280\u80fd\u3002"}}
{"id": "2509.17511", "pdf": "https://arxiv.org/pdf/2509.17511", "abs": "https://arxiv.org/abs/2509.17511", "authors": ["Yunqiao Hu", "Xuesu Xiao", "Steven Jones", "Shunqiao Sun"], "title": "Single-Snapshot Localization Using Sparse Extremely Large Aperture Arrays", "categories": ["eess.SP"], "comment": "ICASSP 2026 manuscript under review", "summary": "This paper investigates single-snapshot direction-of-arrival (DOA) estimation\nand target localization with coherent sparse extremely large aperture arrays\n(ELAAs) in automotive radar applications. Far-field and near-field signal\nmodels are formulated for distributed bistatic configurations. To enable\nnoncoherent processing, a single-snapshot MUSIC (SS-MUSIC) algorithm is\nproposed to fuse local spectra from individual subarrays and extended to\nnear-field localization via geometric intersection. For coherent processing, a\nsingle-snapshot ESPRIT (SS-ESPRIT) method with ambiguity dealiasing is\ndeveloped to fully exploit the aperture of sparse ELAAs for high-resolution\nangle estimation. Simulation results demonstrate that SS-ESPRIT provides\nsuperior angular resolution for closely spaced far-field targets, while\nSS-MUSIC offers robustness in near-field localization and flexibility in hybrid\nscenarios.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6c7d\u8f66\u96f7\u8fbe\u4e2d\u5355\u5feb\u62cdDOA\u4f30\u8ba1\u548c\u76ee\u6807\u5b9a\u4f4d\uff0c\u63d0\u51fa\u4e86SS-MUSIC\u548cSS-ESPRIT\u65b9\u6cd5\uff0c\u5206\u522b\u9488\u5bf9\u975e\u76f8\u5e72\u548c\u76f8\u5e72\u5904\u7406\uff0c\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u6c7d\u8f66\u96f7\u8fbe\u4e2d\u7a00\u758f\u8d85\u5927\u5b54\u5f84\u9635\u5217\uff08ELAAs\uff09\u7684\u76f8\u5e72\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u63d0\u9ad8\u76ee\u6807\u5b9a\u4f4d\u548c\u89d2\u5ea6\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faSS-MUSIC\u7b97\u6cd5\u7528\u4e8e\u975e\u76f8\u5e72\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u4ea4\u6269\u5c55\u81f3\u8fd1\u573a\u5b9a\u4f4d\uff1b\u5f00\u53d1SS-ESPRIT\u65b9\u6cd5\u7528\u4e8e\u76f8\u5e72\u5904\u7406\uff0c\u5229\u7528\u7a00\u758fELAAs\u5b54\u5f84\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u89d2\u5ea6\u4f30\u8ba1\u3002", "result": "SS-ESPRIT\u5728\u8fdc\u573a\u76ee\u6807\u89d2\u5ea6\u5206\u8fa8\u7387\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0cSS-MUSIC\u5728\u8fd1\u573a\u5b9a\u4f4d\u548c\u6df7\u5408\u573a\u666f\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "SS-ESPRIT\u548cSS-MUSIC\u65b9\u6cd5\u5206\u522b\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u6c7d\u8f66\u96f7\u8fbe\u4e2d\u7684\u76ee\u6807\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16773", "pdf": "https://arxiv.org/pdf/2509.16773", "abs": "https://arxiv.org/abs/2509.16773", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Improve bounding box in Carla Simulator", "categories": ["cs.RO", "cs.GR"], "comment": "9 pages, 12 figures,VEHITS Conference 2024", "summary": "The CARLA simulator (Car Learning to Act) serves as a robust platform for\ntesting algorithms and generating datasets in the field of Autonomous Driving\n(AD). It provides control over various environmental parameters, enabling\nthorough evaluation. Development bounding boxes are commonly utilized tools in\ndeep learning and play a crucial role in AD applications. The predominant\nmethod for data generation in the CARLA Simulator involves identifying and\ndelineating objects of interest, such as vehicles, using bounding boxes. The\noperation in CARLA entails capturing the coordinates of all objects on the map,\nwhich are subsequently aligned with the sensor's coordinate system at the ego\nvehicle and then enclosed within bounding boxes relative to the ego vehicle's\nperspective. However, this primary approach encounters challenges associated\nwith object detection and bounding box annotation, such as ghost boxes.\nAlthough these procedures are generally effective at detecting vehicles and\nother objects within their direct line of sight, they may also produce false\npositives by identifying objects that are obscured by obstructions. We have\nenhanced the primary approach with the objective of filtering out unwanted\nboxes. Performance analysis indicates that the improved approach has achieved\nhigh accuracy.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CARLA\u6a21\u62df\u5668\u4e2d\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u7684\u8fb9\u754c\u6846\u65b9\u6cd5\uff0c\u5e76\u9488\u5bf9\u5176\u4e3b\u8981\u95ee\u9898\uff08\u5982\u5e7d\u7075\u6846\u548c\u8bef\u68c0\uff09\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\uff0c\u7ed3\u679c\u8868\u660e\u6539\u8fdb\u65b9\u6cd5\u5177\u6709\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u8fb9\u754c\u6846\u6807\u6ce8\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bef\u68c0\u548c\u5e7d\u7075\u6846\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5728CARLA\u6a21\u62df\u5668\u4e2d\u6355\u83b7\u7269\u4f53\u5750\u6807\uff0c\u5e76\u76f8\u5bf9\u4e8e\u81ea\u8f66\u7684\u5750\u6807\u7cfb\u751f\u6210\u8fb9\u754c\u6846\uff0c\u6539\u8fdb\u65b9\u6cd5\u8fdb\u4e00\u6b65\u8fc7\u6ee4\u4e86\u4e0d\u5fc5\u8981\u7684\u6846\u3002", "result": "\u6027\u80fd\u5206\u6790\u663e\u793a\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8fb9\u754c\u6846\u6807\u6ce8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e7d\u7075\u6846\u548c\u8bef\u68c0\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17674", "pdf": "https://arxiv.org/pdf/2509.17674", "abs": "https://arxiv.org/abs/2509.17674", "authors": ["Julia Matejas", "Olaf \u017burawski", "Nils Strodthoff", "Juan Miguel Lopez Alcaraz"], "title": "Predicting Chest Radiograph Findings from Electrocardiograms Using Interpretable Machine Learning", "categories": ["eess.SP", "cs.LG"], "comment": "19 pages, 3 figures, source code under\n  https://github.com/UOLMDA2025/CardioCXR", "summary": "Purpose: Chest X-rays are essential for diagnosing pulmonary conditions, but\nlimited access in resource-constrained settings can delay timely diagnosis.\nElectrocardiograms (ECGs), in contrast, are widely available, non-invasive, and\noften acquired earlier in clinical workflows. This study aims to assess whether\nECG features and patient demographics can predict chest radiograph findings\nusing an interpretable machine learning approach.\n  Methods: Using the MIMIC-IV database, Extreme Gradient Boosting (XGBoost)\nclassifiers were trained to predict diverse chest radiograph findings from\nECG-derived features and demographic variables. Recursive feature elimination\nwas performed independently for each target to identify the most predictive\nfeatures. Model performance was evaluated using the area under the receiver\noperating characteristic curve (AUROC) with bootstrapped 95% confidence\nintervals. Shapley Additive Explanations (SHAP) were applied to interpret\nfeature contributions.\n  Results: Models successfully predicted multiple chest radiograph findings\nwith varying accuracy. Feature selection tailored predictors to each target,\nand including demographic variables consistently improved performance. SHAP\nanalysis revealed clinically meaningful contributions from ECG features to\nradiographic predictions.\n  Conclusion: ECG-derived features combined with patient demographics can serve\nas a proxy for certain chest radiograph findings, enabling early triage or\npre-screening in settings where radiographic imaging is limited. Interpretable\nmachine learning demonstrates potential to support radiology workflows and\nimprove patient care.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5fc3\u7535\u56fe\uff08ECG\uff09\u7279\u5f81\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u53ef\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u80f8\u90e8X\u5149\u68c0\u67e5\u7ed3\u679c\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u5730\u533a\u63d0\u4f9b\u65e9\u671f\u5206\u8bca\u6216\u9884\u7b5b\u67e5\u7684\u53ef\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5730\u533a\uff0c\u80f8\u90e8X\u5149\u68c0\u67e5\uff08CXR\uff09\u96be\u4ee5\u53ca\u65f6\u83b7\u53d6\uff0c\u800cECG\u8bbe\u5907\u5e7f\u6cdb\u53ef\u7528\u4e14\u975e\u4fb5\u5165\u6027\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22ECG\u662f\u5426\u80fd\u66ff\u4ee3CXR\u8fdb\u884c\u65e9\u671f\u8bca\u65ad\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u901a\u8fc7XGBoost\u5206\u7c7b\u5668\u8bad\u7ec3\u6a21\u578b\uff0c\u5229\u7528ECG\u7279\u5f81\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u9884\u6d4bCXR\u7ed3\u679c\u3002\u91c7\u7528\u9012\u5f52\u7279\u5f81\u6d88\u9664\u548cSHAP\u5206\u6790\u89e3\u91ca\u7279\u5f81\u8d21\u732e\u3002", "result": "\u6a21\u578b\u5bf9\u4e0d\u540cCXR\u7ed3\u679c\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u4e00\u3002\u7279\u5f81\u9009\u62e9\u548c\u52a0\u5165\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u63d0\u5347\u4e86\u6027\u80fd\u3002SHAP\u5206\u6790\u63ed\u793a\u4e86ECG\u7279\u5f81\u7684\u4e34\u5e8a\u610f\u4e49\u3002", "conclusion": "ECG\u7279\u5f81\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u53ef\u4f5c\u4e3aCXR\u67d0\u4e9b\u7ed3\u679c\u7684\u66ff\u4ee3\u6307\u6807\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u60c5\u51b5\u4e0b\u3002\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6709\u671b\u4f18\u5316\u653e\u5c04\u5b66\u5de5\u4f5c\u6d41\u7a0b\u548c\u60a3\u8005\u62a4\u7406\u3002"}}
{"id": "2509.16812", "pdf": "https://arxiv.org/pdf/2509.16812", "abs": "https://arxiv.org/abs/2509.16812", "authors": ["Priyanshu Agrawal", "Shalabh Gupta", "Zongyuan Shen"], "title": "SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents SMART-3D, an extension of the SMART algorithm to 3D\nenvironments. SMART-3D is a tree-based adaptive replanning algorithm for\ndynamic environments with fast moving obstacles. SMART-3D morphs the underlying\ntree to find a new path in real-time whenever the current path is blocked by\nobstacles. SMART-3D removed the grid decomposition requirement of the SMART\nalgorithm by replacing the concept of hot-spots with that of hot-nodes, thus\nmaking it computationally efficient and scalable to 3D environments. The\nhot-nodes are nodes which allow for efficient reconnections to morph the\nexisting tree to find a new safe and reliable path. The performance of SMART-3D\nis evaluated by extensive simulations in 2D and 3D environments populated with\nrandomly moving dynamic obstacles. The results show that SMART-3D achieves high\nsuccess rates and low replanning times, thus highlighting its suitability for\nreal-time onboard applications.", "AI": {"tldr": "SMART-3D\u662fSMART\u7b97\u6cd5\u76843D\u6269\u5c55\uff0c\u901a\u8fc7\u6811\u5f62\u7ed3\u6784\u548c\u70ed\u70b9\u8282\u70b9\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u5feb\u901f\u969c\u788d\u7269\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6811\u5f62\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u7b97\u6cd5\uff0c\u5229\u7528\u70ed\u70b9\u8282\u70b9\u66ff\u4ee3\u7f51\u683c\u5206\u89e3\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u6210\u529f\u7387\u9ad8\u4e14\u91cd\u89c4\u5212\u65f6\u95f4\u77ed\u3002", "conclusion": "SMART-3D\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.17797", "pdf": "https://arxiv.org/pdf/2509.17797", "abs": "https://arxiv.org/abs/2509.17797", "authors": ["Yuan Gao", "Yiming Liu", "Runze Yu", "Shengli Liu", "Yanliang Jin", "Shunqing Zhang", "Shugong Xu", "Xiaoli Chu"], "title": "SSNet: Flexible and robust channel extrapolation for fluid antenna systems enabled by an self-supervised learning framework", "categories": ["eess.SP"], "comment": null, "summary": "Fluid antenna systems (FAS) signify a pivotal advancement in 6G communication\nby enhancing spectral efficiency and robustness. However, obtaining accurate\nchannel state information (CSI) in FAS poses challenges due to its complex\nphysical structure. Traditional methods, such as pilot-based interpolation and\ncompressive sensing, are not only computationally intensive but also lack\nadaptability. Current extrapolation techniques relying on rigid parametric\nmodels do not accommodate the dynamic environment of FAS, while data-driven\ndeep learning approaches demand extensive training and are vulnerable to noise\nand hardware imperfections. To address these challenges, this paper introduces\na novel self-supervised learning network (SSNet) designed for efficient and\nadaptive channel extrapolation in FAS. We formulate the problem of channel\nextrapolation in FAS as an image reconstruction task. Here, a limited number of\nunmasked pixels (representing the known CSI of the selected ports) are used to\nextrapolate the masked pixels (the CSI of unselected ports). SSNet capitalizes\non the intrinsic structure of FAS channels, learning generalized\nrepresentations from raw CSI data, thus reducing dependency on large labelled\ndatasets. For enhanced feature extraction and noise resilience, we propose a\nmix-of-expert (MoE) module. In this setup, multiple feedforward neural networks\n(FFNs) operate in parallel. The outputs of the MoE module are combined using a\nweighted sum, determined by a gating function that computes the weights of each\nFFN using a softmax function. Extensive simulations validate the superiority of\nthe proposed model. Results indicate that SSNet significantly outperforms\nbenchmark models, such as AGMAE and long short-term memory (LSTM) networks by\nusing a much smaller labelled dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7f51\u7edc\uff08SSNet\uff09\uff0c\u7528\u4e8e\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u4e2d\u7684\u9ad8\u6548\u81ea\u9002\u5e94\u4fe1\u9053\u5916\u63a8\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "FAS\u57286G\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u9891\u8c31\u5229\u7528\u548c\u9c81\u68d2\u6027\u4f18\u52bf\uff0c\u4f46\u5176\u590d\u6742\u7269\u7406\u7ed3\u6784\u5bfc\u81f4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u83b7\u53d6\u56f0\u96be\u3002\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5219\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u3002", "method": "SSNet\u5c06\u4fe1\u9053\u5916\u63a8\u95ee\u9898\u5efa\u6a21\u4e3a\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u5757\u4ee5\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u548c\u6297\u566a\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660eSSNet\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u663e\u8457\u4f18\u4e8eAGMAE\u548cLSTM\u7f51\u7edc\u7b49\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "SSNet\u4e3aFAS\u4e2d\u7684\u9ad8\u6548\u4fe1\u9053\u5916\u63a8\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u9002\u5e94\u6027\u548c\u6297\u566a\u6027\u80fd\u3002"}}
{"id": "2509.16830", "pdf": "https://arxiv.org/pdf/2509.16830", "abs": "https://arxiv.org/abs/2509.16830", "authors": ["Omkar Patil", "Prabin Rath", "Kartikay Pangaonkar", "Eric Rosen", "Nakul Gopalan"], "title": "Factorizing Diffusion Policies for Observation Modality Prioritization", "categories": ["cs.RO"], "comment": "14 pages; website: https://fdp-policy.github.io/fdp-policy/", "summary": "Diffusion models have been extensively leveraged for learning robot skills\nfrom demonstrations. These policies are conditioned on several observational\nmodalities such as proprioception, vision and tactile. However, observational\nmodalities have varying levels of influence for different tasks that diffusion\npolices fail to capture. In this work, we propose 'Factorized Diffusion\nPolicies' abbreviated as FDP, a novel policy formulation that enables\nobservational modalities to have differing influence on the action diffusion\nprocess by design. This results in learning policies where certain observations\nmodalities can be prioritized over the others such as $\\texttt{vision>tactile}$\nor $\\texttt{proprioception>vision}$. FDP achieves modality prioritization by\nfactorizing the observational conditioning for diffusion process, resulting in\nmore performant and robust policies. Our factored approach shows strong\nperformance improvements in low-data regimes with $15\\%$ absolute improvement\nin success rate on several simulated benchmarks when compared to a standard\ndiffusion policy that jointly conditions on all input modalities. Moreover, our\nbenchmark and real-world experiments show that factored policies are naturally\nmore robust with $40\\%$ higher absolute success rate across several visuomotor\ntasks under distribution shifts such as visual distractors or camera\nocclusions, where existing diffusion policies fail catastrophically. FDP thus\noffers a safer and more robust alternative to standard diffusion policies for\nreal-world deployment. Videos are available at\nhttps://fdp-policy.github.io/fdp-policy/ .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u56e0\u5b50\u5316\u6269\u6563\u7b56\u7565'(FDP)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4f7f\u4e0d\u540c\u89c2\u6d4b\u6a21\u6001\u5728\u52a8\u4f5c\u6269\u6563\u8fc7\u7a0b\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u5f71\u54cd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7b56\u7565\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u5904\u7406\u591a\u6a21\u6001\u89c2\u6d4b\u65f6\uff0c\u672a\u80fd\u6355\u6349\u4e0d\u540c\u6a21\u6001\u5bf9\u4efb\u52a1\u7684\u4e0d\u540c\u5f71\u54cd\u529b\uff0c\u5bfc\u81f4\u7b56\u7565\u6027\u80fd\u4e0d\u8db3\u6216\u9c81\u68d2\u6027\u5dee\u3002", "method": "FDP\u901a\u8fc7\u56e0\u5b50\u5316\u89c2\u6d4b\u6761\u4ef6\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u67d0\u4e9b\u6a21\u6001\uff08\u5982\u89c6\u89c9\u6216\u89e6\u89c9\uff09\u80fd\u591f\u4f18\u5148\u5f71\u54cd\u6269\u6563\u8fc7\u7a0b\u3002", "result": "FDP\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff08\u6210\u529f\u7387\u9ad815%\uff09\uff0c\u5e76\u4e14\u5728\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\uff08\u5982\u89c6\u89c9\u5e72\u6270\u6216\u76f8\u673a\u906e\u6321\uff09\u9c81\u68d2\u6027\u66f4\u5f3a\uff08\u6210\u529f\u7387\u9ad840%\uff09\u3002", "conclusion": "FDP\u4e3a\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6bd4\u6807\u51c6\u6269\u6563\u7b56\u7565\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.17804", "pdf": "https://arxiv.org/pdf/2509.17804", "abs": "https://arxiv.org/abs/2509.17804", "authors": ["Xiaohua Zhou", "Tianyu Fang", "Yijie Mao", "Bruno Clerckx"], "title": "Generalized Beyond-Diagonal RIS Architectures: Theory and Design via Structure-oriented Symmetric Unitary Projection", "categories": ["eess.SP"], "comment": null, "summary": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS), which enables\nadvanced wave control through interconnection of RIS elements, are gaining\ngrowing recognition as a promising technology for 6G and beyond. However, the\nenhanced flexibility of BD-RIS in controlling the phase and amplitude of\nreflected signals comes at the cost of high circuit complexity. In this paper,\nwe propose two novel BD-RIS architectures, namely, the stem-connected RIS and\ncluster-connected RIS, to explore trade-off between circuit complexity and\nperformance. Specifically, the proposed stem-connected RIS is capable of\nachieving the same performance as fully-connected RIS while significantly\nreducing circuit complexity. The proposed cluster-connected RIS offers a\nunified framework that generalizes existing BD-RIS architectures--including\nsingle-connected, fully-connected, group-connected, tree-connected (arrowhead),\nand forest-connected (arrowhead) RISs--as special cases. This framework enables\na much more flexible trade-offs between circuit complexity and system\nperformance than existing ones. Based on the proposed BD-RIS architectures, we\nintroduce a novel and generalized structure-oriented symmetric unitary\nprojection method for designing the scattering matrix across all BD-RIS\nconfigurations. This method is effectively applied to solve the sum channel\ngain maximization problem and other utility-based optimization problems.\nNumerical results demonstrate that the proposed stem-connected RIS is the\nsimplest architecture that achieves optimal BD-RIS performance, while the\ncluster-connected RIS further enlarges the performance-complexity trade-off\nrange. Furthermore, the proposed projection-based algorithms demonstrate high\nefficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u578bBD-RIS\u67b6\u6784\uff08stem-connected\u548ccluster-connected\uff09\uff0c\u5728\u7535\u8def\u590d\u6742\u6027\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u8bbe\u8ba1\u4e86\u901a\u7528\u7684\u6563\u5c04\u77e9\u9635\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "BD-RIS\u6280\u672f\u57286G\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u7535\u8def\u590d\u6742\u5ea6\u9ad8\uff0c\u4e9f\u9700\u65b0\u578b\u67b6\u6784\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u4e86stem-connected\u548ccluster-connected RIS\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u5bfc\u5411\u7684\u5bf9\u79f0\u9149\u6295\u5f71\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u6563\u5c04\u77e9\u9635\u3002", "result": "stem-connected RIS\u80fd\u4ee5\u66f4\u4f4e\u590d\u6742\u5ea6\u8fbe\u5230\u4e0e\u5168\u8fde\u63a5RIS\u76f8\u540c\u7684\u6027\u80fd\uff1bcluster-connected RIS\u6846\u67b6\u66f4\u5177\u7075\u6d3b\u6027\u3002", "conclusion": "\u4e24\u79cd\u65b0\u578bBD-RIS\u67b6\u6784\u663e\u8457\u4f18\u5316\u4e86\u6027\u80fd\u4e0e\u590d\u6742\u6027\u7684\u6743\u8861\u8303\u56f4\uff0c\u7b97\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\u3002"}}
{"id": "2509.16834", "pdf": "https://arxiv.org/pdf/2509.16834", "abs": "https://arxiv.org/abs/2509.16834", "authors": ["Jingxi Xu"], "title": "Robot Learning with Sparsity and Scarcity", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Unlike in language or vision, one of the fundamental challenges in robot\nlearning is the lack of access to vast data resources. We can further break\ndown the problem into (1) data sparsity from the angle of data representation\nand (2) data scarcity from the angle of data quantity. In this thesis, I will\ndiscuss selected works on two domains: (1) tactile sensing and (2)\nrehabilitation robots, which are exemplars of data sparsity and scarcity,\nrespectively. Tactile sensing is an essential modality for robotics, but\ntactile data are often sparse, and for each interaction with the physical\nworld, tactile sensors can only obtain information about the local area of\ncontact. I will discuss my work on learning vision-free tactile-only\nexploration and manipulation policies through model-free reinforcement learning\nto make efficient use of sparse tactile information. On the other hand,\nrehabilitation robots are an example of data scarcity to the extreme due to the\nsignificant challenge of collecting biosignals from disabled-bodied subjects at\nscale for training. I will discuss my work in collaboration with the medical\nschool and clinicians on intent inferral for stroke survivors, where a hand\northosis developed in our lab collects a set of biosignals from the patient and\nuses them to infer the activity that the patient intends to perform, so the\northosis can provide the right type of physical assistance at the right moment.\nMy work develops machine learning algorithms that enable intent inferral with\nminimal data, including semi-supervised, meta-learning, and generative AI\nmethods.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u6570\u636e\u7a00\u758f\u6027\u548c\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u5e76\u901a\u8fc7\u89e6\u89c9\u4f20\u611f\u548c\u5eb7\u590d\u673a\u5668\u4eba\u4e24\u4e2a\u9886\u57df\u7684\u7814\u7a76\u5c55\u793a\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u8d44\u6e90\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u6570\u636e\u7a00\u758f\u6027\u548c\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5728\u89e6\u89c9\u4f20\u611f\u9886\u57df\uff0c\u4f7f\u7528\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4ece\u7a00\u758f\u7684\u89e6\u89c9\u4fe1\u606f\u4e2d\u5b66\u4e60\u63a2\u7d22\u548c\u64cd\u63a7\u7b56\u7565\uff1b\u5728\u5eb7\u590d\u673a\u5668\u4eba\u9886\u57df\uff0c\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u548c\u751f\u6210AI\u65b9\u6cd5\uff0c\u4ece\u5c11\u91cf\u751f\u7269\u4fe1\u53f7\u4e2d\u63a8\u65ad\u60a3\u8005\u610f\u56fe\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u5229\u7528\u7a00\u758f\u7684\u89e6\u89c9\u6570\u636e\uff0c\u5e76\u5728\u6570\u636e\u6781\u5c11\u7684\u5eb7\u590d\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6210\u529f\u63a8\u65ad\u60a3\u8005\u610f\u56fe\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6570\u636e\u7a00\u758f\u6027\u548c\u7a00\u7f3a\u6027\u7684\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89e6\u89c9\u4f20\u611f\u548c\u5eb7\u590d\u673a\u5668\u4eba\u9886\u57df\u3002"}}
{"id": "2509.17916", "pdf": "https://arxiv.org/pdf/2509.17916", "abs": "https://arxiv.org/abs/2509.17916", "authors": ["Kabuto Arai", "Koji Ishibashi", "Hiroki Iimori", "Yuto Hama", "Paulo Valente Klaine", "Szabolcs Malomsoky"], "title": "Joint Pilot Allocation and Sequence Design for MIMO-OFDM Systems With Channel Sparsity", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a joint optimization of pilot subcarrier allocation and\nnon-orthogonal sequence for multiple-input-multiple-output (MIMO)-orthogonal\nfrequency-division multiplexing (OFDM) systems under compressed sensing\n(CS)-based channel estimation exploiting delay and angle sparsity. Since the\nperformance of CS-based approaches depends on a coherence metric of the sensing\nmatrix in the measurement process, we formulate a joint optimization problem to\nminimize this coherence. Due to the discrete nature of subcarrier allocation, a\nstraightforward formulation of the joint optimization results in a\nmixed-integer nonlinear program (MINLP), which is computationally intractable\ndue to the combinatorial explosion of allocation candidates. To overcome the\nintractability of discrete variables, we introduce a block sparse penalty for\npilots across all subcarriers, which ensures that the power of some unnecessary\npilots approaches zero. This framework enables joint optimization using only\ncontinuous variables. In addition, we propose an efficient computation method\nfor the coherence metric by exploiting the structure of the sensing matrix,\nwhich allows its gradient to be derived in closed form, making the joint\noptimization problem solvable in an efficient way via a gradient descent\napproach. Numerical results confirm that the proposed pilot sequence exhibits\nsuperior coherence properties and enhances the CS-based channel estimation\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8eMIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u5bfc\u9891\u5b50\u8f7d\u6ce2\u5206\u914d\u4e0e\u975e\u6b63\u4ea4\u5e8f\u5217\u8bbe\u8ba1\uff0c\u4ee5\u538b\u7f29\u611f\u77e5\u4e3a\u57fa\u7840\u7684\u4fe1\u9053\u4f30\u8ba1\u4e3a\u76ee\u6807\uff0c\u6700\u5c0f\u5316\u611f\u77e5\u77e9\u9635\u7684\u76f8\u5e72\u6027\u3002", "motivation": "\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\u7684\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u4f9d\u8d56\u4e8e\u611f\u77e5\u77e9\u9635\u7684\u76f8\u5e72\u6027\u6307\u6807\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u8fd9\u4e00\u6307\u6807\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5757\u7a00\u758f\u60e9\u7f5a\uff0c\u5c06\u79bb\u6563\u53d8\u91cf\u95ee\u9898\u8f6c\u5316\u4e3a\u8fde\u7eed\u53d8\u91cf\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5229\u7528\u611f\u77e5\u77e9\u9635\u7684\u7ed3\u6784\u8ba1\u7b97\u76f8\u5e72\u6027\u6307\u6807\u53ca\u5176\u68af\u5ea6\uff0c\u91c7\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u5bfc\u9891\u5e8f\u5217\u5177\u6709\u4f18\u8d8a\u7684\u76f8\u5e72\u6027\uff0c\u63d0\u5347\u4e86\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\u7684\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5bfc\u9891\u5206\u914d\u4e0e\u975e\u6b63\u4ea4\u5e8f\u5217\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86MINLP\u7684\u8ba1\u7b97\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u9053\u4f30\u8ba1\u7684\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2509.16858", "pdf": "https://arxiv.org/pdf/2509.16858", "abs": "https://arxiv.org/abs/2509.16858", "authors": ["Soon Jynn Chu", "Raju Gottumukkala", "Alan Barhorst"], "title": "Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics", "categories": ["cs.RO"], "comment": "Submitted to conference", "summary": "The ability of social robots to respond to human emotions is crucial for\nbuilding trust and acceptance in human-robot collaborative environments.\nHowever, developing such capabilities through online reinforcement learning is\nsometimes impractical due to the prohibitive cost of data collection and the\nrisk of generating unsafe behaviors. In this paper, we study the use of offline\nreinforcement learning as a practical and efficient alternative. This technique\nuses pre-collected data to enable emotion-adaptive social robots. We present a\nsystem architecture that integrates multimodal sensing and recognition,\ndecision-making, and adaptive responses. Using a limited dataset from a\nhuman-robot game-playing scenario, we establish a benchmark for comparing\noffline reinforcement learning algorithms that do not require an online\nenvironment. Our results show that BCQ and CQL are more robust to data\nsparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.\nThis work establishes a foundation for benchmarking offline RL in\nemotion-adaptive robotics and informs future deployment in real-world HRI. Our\nfindings provide empirical insight into the performance of offline\nreinforcement learning algorithms in data-constrained HRI. This work\nestablishes a foundation for benchmarking offline RL in emotion-adaptive\nrobotics and informs its future deployment in real-world HRI, such as in\nconversational agents, educational partners, and personal assistants, require\nreliable emotional responsiveness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u60c5\u611f\u81ea\u9002\u5e94\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u9884\u6536\u96c6\u6570\u636e\u89e3\u51b3\u4e86\u5728\u7ebf\u5b66\u4e60\u7684\u9ad8\u6210\u672c\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u54cd\u5e94\u4eba\u7c7b\u60c5\u611f\u7684\u793e\u4ea4\u673a\u5668\u4eba\u9700\u8981\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u65b9\u6cd5\uff0c\u800c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u548c\u6f5c\u5728\u98ce\u9669\u4e0d\u9002\u7528\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528\u9884\u6536\u96c6\u6570\u636e\u6784\u5efa\u7cfb\u7edf\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u3001\u51b3\u7b56\u548c\u81ea\u9002\u5e94\u54cd\u5e94\uff0c\u5e76\u5728\u4eba\u673a\u6e38\u620f\u573a\u666f\u4e2d\u8fdb\u884c\u7b97\u6cd5\u6bd4\u8f83\u3002", "result": "BCQ\u548cCQL\u7b97\u6cd5\u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u76f8\u6bd4NFQ\u3001DQN\u548cDDQN\u5177\u6709\u66f4\u9ad8\u7684\u72b6\u6001-\u52a8\u4f5c\u4ef7\u503c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u60c5\u611f\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u5b9e\u9645\u5e94\u7528\uff08\u5982\u5bf9\u8bdd\u4ee3\u7406\u3001\u6559\u80b2\u4f19\u4f34\u7b49\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.17953", "pdf": "https://arxiv.org/pdf/2509.17953", "abs": "https://arxiv.org/abs/2509.17953", "authors": ["Kathrin Klein", "Benedikt B\u00f6ck", "Nurettin Turan", "Wolfgang Utschick"], "title": "Autoregressive-Gaussian Mixture Models: Efficient Generative Modeling of WSS Signals", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work addresses the challenge of making generative models suitable for\nresource-constrained environments like mobile wireless communication systems.\nWe propose a generative model that integrates Autoregressive (AR)\nparameterization into a Gaussian Mixture Model (GMM) for modeling Wide-Sense\nStationary (WSS) processes. By exploiting model-based insights allowing for\nstructural constraints, the approach significantly reduces parameters while\nmaintaining high modeling accuracy. Channel estimation experiments show that\nthe model can outperform standard GMMs and variants using Toeplitz or circulant\ncovariances, particularly with small sample sizes. For larger datasets, it\nmatches the performance of conventional methods while improving computational\nefficiency and reducing the memory requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u56de\u5f52\u53c2\u6570\u5316\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u4f46\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u79fb\u52a8\u901a\u4fe1\u7cfb\u7edf\uff09\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\u3002", "method": "\u5c06\u81ea\u56de\u5f52\u53c2\u6570\u5316\u96c6\u6210\u5230\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e\u5efa\u6a21\u5bbd\u5e73\u7a33\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u7ea6\u675f\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6807\u51c6\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u53ca\u5176\u53d8\u4f53\uff1b\u5728\u5927\u6570\u636e\u96c6\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u4f46\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u8d1f\u62c5\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u9ad8\u5efa\u6a21\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2509.16871", "pdf": "https://arxiv.org/pdf/2509.16871", "abs": "https://arxiv.org/abs/2509.16871", "authors": ["Yitian Shi", "Zicheng Guo", "Rosa Wolf", "Edgar Welte", "Rania Rayyes"], "title": "HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness", "categories": ["cs.RO"], "comment": "under review", "summary": "We propose Hand-Object\\emph{(HO)GraspFlow}, an affordance-centric approach\nthat retargets a single RGB with hand-object interaction (HOI) into multi-modal\nexecutable parallel jaw grasps without explicit geometric priors on target\nobjects. Building on foundation models for hand reconstruction and vision, we\nsynthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned\non the following three complementary cues: RGB foundation features as visual\nsemantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.\nOur approach demonstrates high fidelity in grasp synthesis without explicit HOI\ncontact input or object geometry, while maintaining strong contact and taxonomy\nrecognition. Another controlled comparison shows that \\emph{HOGraspFlow}\nconsistently outperforms diffusion-based variants (\\emph{HOGraspDiff}),\nachieving high distributional fidelity and more stable optimization in $SE(3)$.\nWe demonstrate a reliable, object-agnostic grasp synthesis from human\ndemonstrations in real-world experiments, where an average success rate of over\n$83\\%$ is achieved.", "AI": {"tldr": "HO-GraspFlow\u901a\u8fc7\u7ed3\u5408RGB\u57fa\u7840\u7279\u5f81\u3001\u624b-\u7269\u63a5\u89e6\u91cd\u5efa\u548c\u6293\u53d6\u7c7b\u578b\u5206\u7c7b\u5148\u9a8c\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\u7684\u591a\u6a21\u6001\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u6548\u679c\u4f18\u4e8e\u6269\u6563\u53d8\u4f53\u3002", "motivation": "\u76ee\u6807\u662f\u4eceRGB\u56fe\u50cf\u4e2d\u5b66\u4e60\u591a\u6a21\u6001\u7684\u53ef\u6267\u884c\u6293\u53d6\u59ff\u6001\uff0c\u800c\u4e0d\u4f9d\u8d56\u663e\u5f0f\u7684\u624b-\u7269\u63a5\u89e6\u8f93\u5165\u6216\u5bf9\u8c61\u51e0\u4f55\u4fe1\u606f\u3002", "method": "\u5229\u7528\u624b\u91cd\u5efa\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408RGB\u7279\u5f81\u3001\u63a5\u89e6\u91cd\u5efa\u548c\u6293\u53d6\u7c7b\u578b\u5206\u7c7b\u5148\u9a8c\uff0c\u901a\u8fc7\u53bb\u566a\u6d41\u5339\u914d\u5408\u6210SE(3)\u6293\u53d6\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6293\u53d6\u5408\u6210\u4fdd\u771f\u5ea6\u9ad8\uff0c\u5206\u5e03\u4e00\u81f4\u6027\u5f3a\uff0c\u4f18\u5316\u66f4\u7a33\u5b9a\uff0c\u771f\u5b9e\u573a\u666f\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8d85\u8fc783%\u3002", "conclusion": "HO-GraspFlow\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.17983", "pdf": "https://arxiv.org/pdf/2509.17983", "abs": "https://arxiv.org/abs/2509.17983", "authors": ["Boxuan Sun", "Hongliang Luo", "Shaodan Ma", "Feifei Gao"], "title": "Bridge Micro-Deformation Monitoring Scheme with Integrated Sensing and Communications", "categories": ["eess.SP"], "comment": "The manuscript was submitted to IEEE Transactions on Wireless\n  Communications (IEEE TWC) on April 22, 2025, and a major revision request was\n  received on August 4, 2025", "summary": "In this paper, we propose a novel integrated sensing and communications\n(ISAC) scheme to perform bridge micro-deformation monitoring (BMDM) in complex\nenvironments. We first provide an excitation-bridge coupling model to represent\nthe micro-deformation process of the bridge. Next, we design a novel frame\nstructure for BMDM applications, and construct the OFDM echo channel model for\nbasic scene of BMDM, including micro-deformation, dynamic objects, and static\nenvironment. Then, we develop a phasor statistical analysis method based on\naverage cancellation algorithm to suppress the interference of dynamic objects,\nas well as a circle fitting method based on least squares algorithm to remove\nthe interference of static environment near the monitoring area. Furthermore,\nwe extract the micro-deformation feature vector from the OFDM echo signals\nafter inverse discrete fourier transform (IDFT), and derive vertical\nmicro-deformation value with the time-frequency phase resources. Simulation\nresults demonstrate the effectiveness of the proposed BMDM scheme and its\nrobustness against both dynamic interferences and static interferences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u6865\u6881\u5fae\u53d8\u5f62\u76d1\u6d4b\uff08BMDM\uff09\u3002", "motivation": "\u4e3a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u6865\u6881\u5fae\u53d8\u5f62\u7684\u7cbe\u786e\u76d1\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u6fc0\u632f-\u6865\u6881\u8026\u5408\u6a21\u578b\u3001OFDM\u56de\u6ce2\u4fe1\u9053\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5e73\u5747\u6d88\u9664\u7b97\u6cd5\u548c\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u7684\u76f8\u4f4d\u7edf\u8ba1\u5206\u6790\u4e0e\u5706\u62df\u5408\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6848\u80fd\u6709\u6548\u6291\u5236\u52a8\u6001\u548c\u9759\u6001\u5e72\u6270\uff0c\u5e76\u51c6\u786e\u63d0\u53d6\u5fae\u53d8\u5f62\u7279\u5f81\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BMDM\u65b9\u6848\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16894", "pdf": "https://arxiv.org/pdf/2509.16894", "abs": "https://arxiv.org/abs/2509.16894", "authors": ["Zhijie Qiao", "Haowei Li", "Zhong Cao", "Henry X. Liu"], "title": "End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing", "categories": ["cs.RO"], "comment": null, "summary": "F1Tenth is a widely adopted reduced-scale platform for developing and testing\nautonomous racing algorithms, hosting annual competitions worldwide. With high\noperating speeds, dynamic environments, and head-to-head interactions,\nautonomous racing requires algorithms that diverge from those in classical\nautonomous driving. Training such algorithms is particularly challenging: the\nneed for rapid decision-making at high speeds severely limits model capacity.\nTo address this, we propose End2Race, a novel end-to-end imitation learning\nalgorithm designed for head-to-head autonomous racing. End2Race leverages a\nGated Recurrent Unit (GRU) architecture to capture continuous temporal\ndependencies, enabling both short-term responsiveness and long-term strategic\nplanning. We also adopt a sigmoid-based normalization function that transforms\nraw LiDAR scans into spatial pressure tokens, facilitating effective model\ntraining and convergence. The algorithm is extremely efficient, achieving an\ninference time of less than 0.5 milliseconds on a consumer-class GPU.\nExperiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%\nsafety rate across 2,400 overtaking scenarios, each with an 8-second time\nlimit, and successfully completes overtakes in 59.2% of cases. This surpasses\nprevious methods and establishes ours as a leading solution for the F1Tenth\nracing testbed. Code is available at\nhttps://github.com/michigan-traffic-lab/End2Race.", "AI": {"tldr": "End2Race\u662f\u4e00\u79cd\u7528\u4e8eF1Tenth\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7684\u65b0\u578b\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u91c7\u7528GRU\u67b6\u6784\u548cLiDAR\u6570\u636e\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u5feb\u901f\u63a8\u7406\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u9700\u8981\u5728\u9ad8\u901f\u4e0b\u5feb\u901f\u51b3\u7b56\uff0c\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u4e3a\u8d5b\u8f66\u4f18\u5316\u7684\u65b0\u7b97\u6cd5\u3002", "method": "\u63d0\u51faEnd2Race\u7b97\u6cd5\uff0c\u4f7f\u7528GRU\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u91c7\u7528\u57fa\u4e8esigmoid\u7684LiDAR\u6570\u636e\u6807\u51c6\u5316\u65b9\u6cd5\u3002", "result": "\u5728F1Tenth\u6a21\u62df\u5668\u4e2d\uff0cEnd2Race\u5b89\u5168\u6027\u8fbe94.2%\uff0c\u8d85\u8f66\u6210\u529f\u738759.2%\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e0.5\u6beb\u79d2\u3002", "conclusion": "End2Race\u662fF1Tinth\u8d5b\u8f66\u5e73\u53f0\u7684\u9886\u5148\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "1705.03366", "pdf": "https://arxiv.org/pdf/1705.03366", "abs": "https://arxiv.org/abs/1705.03366", "authors": ["Dogay Altinel", "Gunes Karabulut Kurt"], "title": "Frequency Switching for Simultaneous Wireless Information and Power Transfer", "categories": ["cs.IT", "cs.SY", "eess.SP", "eess.SY", "math.IT", "physics.data-an"], "comment": null, "summary": "A new frequency switching receiver structure is proposed for simultaneous\nwireless information and power transfer in multi-carrier communication systems.\nEach subcarrier is switched to either the energy harvesting unit or the\ninformation decoding unit, according to the optimal subcarrier allocation. To\nimplement the system, one-bit feedback is required for each subcarrier. Two\noptimization problems are defined, converted to binary knapsack problems, and\nsolved using dynamic programming approaches. Upper bounds are obtained using\ncontinuous relaxations. Power allocation is integrated to further increase the\nperformance. Numerical studies show that the proposed frequency switching based\nmodel is better than existing models in a wide range of parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9891\u7387\u5207\u6362\u63a5\u6536\u5668\u7ed3\u6784\uff0c\u7528\u4e8e\u591a\u8f7d\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u540c\u65f6\u8fdb\u884c\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\uff0c\u901a\u8fc7\u52a8\u6001\u7f16\u7a0b\u4f18\u5316\u5b50\u8f7d\u6ce2\u5206\u914d\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8f7d\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u540c\u65f6\u8fdb\u884c\u65e0\u7ebf\u4fe1\u606f\u4f20\u8f93\u548c\u80fd\u91cf\u91c7\u96c6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u5b50\u8f7d\u6ce2\u5206\u914d\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u9891\u7387\u5207\u6362\u63a5\u6536\u5668\u7ed3\u6784\uff0c\u52a8\u6001\u7f16\u7a0b\u89e3\u51b3\u5b50\u8f7d\u6ce2\u5206\u914d\u7684\u4e8c\u8fdb\u5236\u80cc\u5305\u95ee\u9898\uff0c\u7ed3\u5408\u529f\u7387\u5206\u914d\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u53c2\u6570\u8303\u56f4\u5185\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u9891\u7387\u5207\u6362\u7ed3\u6784\u5728\u591a\u8f7d\u6ce2\u7cfb\u7edf\u4e2d\u9ad8\u6548\u5b9e\u73b0\u4fe1\u606f\u548c\u80fd\u91cf\u540c\u6b65\u4f20\u8f93\uff0c\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2509.16920", "pdf": "https://arxiv.org/pdf/2509.16920", "abs": "https://arxiv.org/abs/2509.16920", "authors": ["Ettilla Mohiuddin Eumi", "Hussein Abbass", "Nadine Marcus"], "title": "SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms", "categories": ["cs.RO", "cs.HC"], "comment": "This paper has been accepted and presented at the 16th International\n  Conference on Swarm Intelligence (ICSI 2025), held on July 11-15, 2025, in\n  Yokohama, Japan", "summary": "Traditional Human-Swarm Interaction (HSI) methods often lack intuitive\nreal-time adaptive interfaces, making decision making slower and increasing\ncognitive load while limiting command flexibility. To solve this, we present\nSwarmChat, a context-aware, multimodal interaction system powered by Large\nLanguage Models (LLMs). SwarmChat enables users to issue natural language\ncommands to robotic swarms using multiple modalities, such as text, voice, or\nteleoperation. The system integrates four LLM-based modules: Context Generator,\nIntent Recognition, Task Planner, and Modality Selector. These modules\ncollaboratively generate context from keywords, detect user intent, adapt\ncommands based on real-time robot state, and suggest optimal communication\nmodalities. Its three-layer architecture offers a dynamic interface with both\nfixed and customizable command options, supporting flexible control while\noptimizing cognitive effort. The preliminary evaluation also shows that the\nSwarmChat's LLM modules provide accurate context interpretation, relevant\nintent recognition, and effective command delivery, achieving high user\nsatisfaction.", "AI": {"tldr": "SwarmChat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\uff0c\u63d0\u5347\u51b3\u7b56\u901f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u754c\u9762\uff0c\u5bfc\u81f4\u51b3\u7b56\u6162\u3001\u8ba4\u77e5\u8d1f\u8377\u9ad8\u4e14\u547d\u4ee4\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u56db\u6a21\u5757\u7ed3\u6784\uff08Context Generator\u3001Intent Recognition\u3001Task Planner\u3001Modality Selector\uff09\u548c\u4e09\u5c42\u6b21\u67b6\u6784\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u5e76\u52a8\u6001\u4f18\u5316\u547d\u4ee4\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0cSwarmChat\u80fd\u51c6\u786e\u89e3\u6790\u4e0a\u4e0b\u6587\u3001\u8bc6\u522b\u610f\u56fe\u5e76\u9ad8\u6548\u4f20\u9012\u547d\u4ee4\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\u3002", "conclusion": "SwarmChat\u901a\u8fc7LLM\u548c\u591a\u6a21\u6001\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4eba\u673a\u4ea4\u4e92\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "1705.10514", "pdf": "https://arxiv.org/pdf/1705.10514", "abs": "https://arxiv.org/abs/1705.10514", "authors": ["Dogay Altinel", "Gunes Karabulut Kurt"], "title": "Diversity Combining for RF Energy Harvesting", "categories": ["cs.IT", "cs.SY", "eess.SP", "eess.SY", "math.IT"], "comment": null, "summary": "RF energy harvesting (RFEH) is a promising technology for energy requirements\nof wireless communication nodes. However, providing sufficient amount of energy\nto ensure self-sufficient devices based on RFEH may be challenging. In this\npaper, the use of diversity combining in RFEH systems is proposed to increase\nthe amount of harvested energy. The power consumption of diversity combining\nprocess is also taken into account to analyze the net benefit of diversity\ncombining. Performances of RFEH systems are investigated for selection\ncombining (SC), equal gain combining (EGC), and maximal ratio combining (MRC)\ntechniques. Simulations are conducted to compare the numerical results of SC,\nEGC, and MRC, and the results show that although the diversity combining\ntechniques can improve the energy harvesting performance, the power consumption\nparameters have a critical importance while determining the suitable technique.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728RF\u80fd\u91cf\u6536\u96c6\u7cfb\u7edf\u4e2d\u4f7f\u7528\u591a\u6837\u6027\u7ec4\u5408\u6280\u672f\u4ee5\u63d0\u9ad8\u80fd\u91cf\u6536\u96c6\u6548\u7387\uff0c\u5e76\u8003\u8651\u4e86\u5176\u529f\u8017\u5bf9\u51c0\u6536\u76ca\u7684\u5f71\u54cd\u3002", "motivation": "RF\u80fd\u91cf\u6536\u96c6\u6280\u672f\u4e3a\u65e0\u7ebf\u901a\u4fe1\u8282\u70b9\u63d0\u4f9b\u80fd\u6e90\uff0c\u4f46\u5176\u80fd\u91cf\u4f9b\u5e94\u53ef\u80fd\u4e0d\u8db3\uff0c\u591a\u6837\u6027\u7ec4\u5408\u6280\u672f\u6709\u671b\u63d0\u5347\u80fd\u91cf\u6536\u96c6\u91cf\u3002", "method": "\u7814\u7a76\u4e86\u9009\u62e9\u7ec4\u5408(SC)\u3001\u7b49\u589e\u76ca\u7ec4\u5408(EGC)\u548c\u6700\u5927\u6bd4\u7387\u7ec4\u5408(MRC)\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002", "result": "\u591a\u6837\u6027\u7ec4\u5408\u6280\u672f\u80fd\u63d0\u5347\u80fd\u91cf\u6536\u96c6\u6027\u80fd\uff0c\u4f46\u529f\u8017\u53c2\u6570\u5728\u786e\u5b9a\u5408\u9002\u6280\u672f\u65f6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u867d\u7136\u591a\u6837\u6027\u7ec4\u5408\u6280\u672f\u80fd\u6539\u5584RF\u80fd\u91cf\u6536\u96c6\uff0c\u4f46\u9700\u6743\u8861\u529f\u8017\u4ee5\u9009\u62e9\u6700\u4f18\u65b9\u6848\u3002"}}
{"id": "2509.16963", "pdf": "https://arxiv.org/pdf/2509.16963", "abs": "https://arxiv.org/abs/2509.16963", "authors": ["Chengjin Wang", "Yanmin Zhou", "Zhipeng Wang", "Zheng Yan", "Feng Luan", "Shuo Jiang", "Runjie Shen", "Hongrui Sang", "Bin He"], "title": "A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Humans and animals can make real-time adjustments to movements by imagining\ntheir action outcomes to prevent unanticipated or even catastrophic motion\nfailures in unknown unstructured environments. Action imagination, as a refined\nsensorimotor strategy, leverages perception-action loops to handle physical\ninteraction-induced uncertainties in perception and system modeling within\ncomplex systems. Inspired by the action-awareness capability of animal\nintelligence, this study proposes an imagination-inspired motion planner (I-MP)\nframework that specifically enhances robots' action reliability by imagining\nplausible spatial states for approaching. After topologizing the workspace,\nI-MP build perception-action loop enabling robots autonomously build contact\nmodels. Leveraging fixed-point theory and Hausdorff distance, the planner\ncomputes convergent spatial states under interaction characteristics and\nmission constraints. By homogenously representing multi-dimensional\nenvironmental characteristics through work, the robot can approach the imagined\nspatial states via real-time computation of energy gradients. Consequently,\nexperimental results demonstrate the practicality and robustness of I-MP in\ncomplex cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u751f\u7269\u542f\u53d1\u7684\u60f3\u8c61\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff08I-MP\uff09\uff0c\u901a\u8fc7\u6a21\u62df\u7a7a\u95f4\u72b6\u6001\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u3002", "motivation": "\u53d7\u4eba\u7c7b\u548c\u52a8\u7269\u901a\u8fc7\u60f3\u8c61\u52a8\u4f5c\u7ed3\u679c\u5b9e\u65f6\u8c03\u6574\u8fd0\u52a8\u7684\u80fd\u529b\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u5728\u672a\u77e5\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faI-MP\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u5316\u5de5\u4f5c\u7a7a\u95f4\u3001\u6784\u5efa\u611f\u77e5-\u52a8\u4f5c\u5faa\u73af\u3001\u5229\u7528\u4e0d\u52a8\u70b9\u7406\u8bba\u548cHausdorff\u8ddd\u79bb\u8ba1\u7b97\u6536\u655b\u7a7a\u95f4\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u80fd\u91cf\u68af\u5ea6\u5b9e\u65f6\u8ba1\u7b97\u8d8b\u8fd1\u76ee\u6807\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eI-MP\u5728\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "I-MP\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u65f6\u8ba1\u7b97\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.16259", "pdf": "https://arxiv.org/pdf/2509.16259", "abs": "https://arxiv.org/abs/2509.16259", "authors": ["Rozita Teymourzadeh", "Yuya Nakazawa"], "title": "A Scalable and Interoperable Platform for Transforming Building Information with Brick Ontology", "categories": ["cs.CY", "eess.SP"], "comment": null, "summary": "In the digital twin and building information era, many building automation\ncompanies searched for scalable methods to extract and analyze different\nbuilding data, including Internet of Things (IoT) sensors, actuators, layout\nsections, zones, etc. The necessity for engineers to continuously manage the\nentire process for each new building creates scalability challenges.\nFurthermore, because construction information is sensitive, transferring data\non vendor platforms via the cloud creates problems. This paper introduces a\nplatform designed to address some of the common challenges in building\nautomation. This is a smart platform designed for the transformation of\nbuilding information into Brick ontology (Brick 2020) and graph formats. This\ntechnology makes it easy to retrieve historical data and converts the building\npoint list into a Brick schema model for use in digital twin applications. The\noverarching goal of the proposed platform development is semi-automate the\nprocess while offering adaptability to various building configurations. This\nplatform uses Brick schema and graph data structure techniques to minimize\ncomplexity, offering a semi-automated approach through its use of a tree-based\ngraph structure. Moreover, the integration of Brick ontology creates a common\nlanguage for interoperability and improves building information management. The\nseamless and offline integration of historical data within the developed\nplatform minimizes data security risks when handling building information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u5e73\u53f0\uff0c\u7528\u4e8e\u5c06\u5efa\u7b51\u4fe1\u606f\u8f6c\u6362\u4e3aBrick\u672c\u4f53\u548c\u56fe\u683c\u5f0f\uff0c\u4ee5\u89e3\u51b3\u5efa\u7b51\u81ea\u52a8\u5316\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u3002", "motivation": "\u5efa\u7b51\u81ea\u52a8\u5316\u516c\u53f8\u5728\u6570\u5b57\u5316\u548c\u5efa\u7b51\u4fe1\u606f\u5316\u65f6\u4ee3\u9762\u4e34\u6570\u636e\u63d0\u53d6\u548c\u5206\u6790\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e14\u7531\u4e8e\u5efa\u7b51\u4fe1\u606f\u654f\u611f\uff0c\u4e91\u4f20\u8f93\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5e73\u53f0\u91c7\u7528Brick\u672c\u4f53\u548c\u56fe\u6570\u636e\u7ed3\u6784\u6280\u672f\uff0c\u901a\u8fc7\u6811\u5f62\u56fe\u7ed3\u6784\u548c\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\u8f6c\u6362\u5efa\u7b51\u4fe1\u606f\u3002", "result": "\u8be5\u5e73\u53f0\u7b80\u5316\u4e86\u5386\u53f2\u6570\u636e\u68c0\u7d22\uff0c\u5e76\u5b9e\u73b0\u4e86\u5efa\u7b51\u4fe1\u606f\u7684\u5b89\u5168\u79bb\u7ebf\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u7ba1\u7406\u6548\u7387\u3002", "conclusion": "\u5f00\u53d1\u7684\u5e73\u53f0\u901a\u8fc7Brick\u672c\u4f53\u548c\u56fe\u7ed3\u6784\u6280\u672f\uff0c\u4e3a\u5efa\u7b51\u4fe1\u606f\u7ba1\u7406\u63d0\u4f9b\u4e86\u534a\u81ea\u52a8\u5316\u548c\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16966", "pdf": "https://arxiv.org/pdf/2509.16966", "abs": "https://arxiv.org/abs/2509.16966", "authors": ["Andreas Mueller"], "title": "Geometric Interpolation of Rigid Body Motions", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA", "math.OC"], "comment": null, "summary": "The problem of interpolating a rigid body motion is to find a spatial\ntrajectory between a prescribed initial and terminal pose. Two variants of this\ninterpolation problem are addressed. The first is to find a solution that\nsatisfies initial conditions on the k-1 derivatives of the rigid body twist.\nThis is called the kth-order initial value trajectory interpolation problem\n(k-IV-TIP). The second is to find a solution that satisfies conditions on the\nrigid body twist and its k-1 derivatives at the initial and terminal pose. This\nis called the kth-order boundary value trajectory interpolation problem\n(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and\nup to the 4th time derivative are prescribed. Further, a solution to the\n1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The\nlatter is a novel cubic interpolation between two spatial configurations with\ngiven initial and terminal twist. This interpolation is automatically identical\nto the minimum acceleration curve when the twists are set to zero. The general\napproach to derive higher-order solutions is presented. Numerical results are\nshown for two examples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u95ee\u9898\uff1ak\u9636\u521d\u59cb\u503c\u8f68\u8ff9\u63d2\u503c\uff08k-IV-TIP\uff09\u548ck\u9636\u8fb9\u754c\u503c\u8f68\u8ff9\u63d2\u503c\uff08k-BV-TIP\uff09\uff0c\u5e76\u5206\u522b\u7ed9\u51fa\u89e3\u51b3\u65b9\u6848\u548c\u6570\u503c\u793a\u4f8b\u3002", "motivation": "\u7814\u7a76\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u95ee\u9898\uff0c\u6ee1\u8db3\u4e0d\u540c\u9636\u6570\u7684\u521d\u59cb\u6761\u4ef6\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u8f68\u8ff9\u3002", "method": "\u63d0\u51fak-IV-TIP\u548ck-BV-TIP\u4e24\u79cd\u63d2\u503c\u65b9\u6cd5\uff0c\u5206\u522b\u89e3\u51b3\u521d\u59cb\u548c\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u9ad8\u9636\u5bfc\u6570\u63d2\u503c\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e861\u52304\u9636\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e861-IV-TBP\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u6b21\u63d2\u503c\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u503c\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u9636\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u95ee\u9898\uff0c\u4e14\u65b0\u63d0\u51fa\u7684\u4e09\u6b21\u63d2\u503c\u6cd5\u5728\u96f6\u626d\u529b\u6761\u4ef6\u4e0b\u4e0e\u6700\u5c0f\u52a0\u901f\u5ea6\u66f2\u7ebf\u4e00\u81f4\u3002"}}
{"id": "2509.16358", "pdf": "https://arxiv.org/pdf/2509.16358", "abs": "https://arxiv.org/abs/2509.16358", "authors": ["Jesper Brunnstr\u00f6m", "Martin Bo M\u00f8ller", "Jan \u00d8stergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Sound field estimation with moving microphones using kernel ridge regression", "categories": ["eess.AS", "eess.SP"], "comment": null, "summary": "Sound field estimation with moving microphones can increase flexibility,\ndecrease measurement time, and reduce equipment constraints compared to using\nstationary microphones. In this paper a sound field estimation method based on\nkernel ridge regression (KRR) is proposed for moving microphones. The proposed\nKRR method is constructed using a discrete time continuous space sound field\nmodel based on the discrete Fourier transform and the Herglotz wave function.\nThe proposed method allows for the inclusion of prior knowledge as a\nregularization penalty, similar to kernel-based methods with stationary\nmicrophones, which is novel for moving microphones. Using a directional\nweighting for the proposed method, the sound field estimates are improved,\nwhich is demonstrated on both simulated and real data. Due to the high\ncomputational cost of sound field estimation with moving microphones, an\napproximate KRR method is proposed, using random Fourier features (RFF) to\napproximate the kernel. The RFF method is shown to decrease computational cost\nwhile obtaining less accurate estimates compared to KRR, providing a trade-off\nbetween cost and performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u9ea6\u514b\u98ce\u7684\u58f0\u573a\u4f30\u8ba1\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u9a8c\u77e5\u8bc6\u548c\u65b9\u5411\u52a0\u6743\u6539\u8fdb\u7cbe\u5ea6\uff0c\u5e76\u63d0\u51fa\u8fd1\u4f3cKRR\u65b9\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f7f\u7528\u79fb\u52a8\u9ea6\u514b\u98ce\u8fdb\u884c\u58f0\u573a\u4f30\u8ba1\u53ef\u4ee5\u63d0\u9ad8\u7075\u6d3b\u6027\u3001\u51cf\u5c11\u6d4b\u91cf\u65f6\u95f4\u548c\u8bbe\u5907\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u548cHerglotz\u6ce2\u51fd\u6570\u7684\u79bb\u6563\u65f6\u95f4\u8fde\u7eed\u7a7a\u95f4\u6a21\u578b\u6784\u5efaKRR\u65b9\u6cd5\uff0c\u5f15\u5165\u65b9\u5411\u52a0\u6743\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08RFF\uff09\u7684\u8fd1\u4f3cKRR\u65b9\u6cd5\u3002", "result": "\u65b9\u5411\u52a0\u6743\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0cRFF\u65b9\u6cd5\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u4f46\u727a\u7272\u4e86\u90e8\u5206\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79fb\u52a8\u9ea6\u514b\u98ce\u58f0\u573a\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6210\u672c\u4e0e\u6027\u80fd\u3002"}}
{"id": "2509.16998", "pdf": "https://arxiv.org/pdf/2509.16998", "abs": "https://arxiv.org/abs/2509.16998", "authors": ["Nishka Khendry", "Christos Margadji", "Sebastian W. Pattinson"], "title": "IDfRA: Self-Verification for Iterative Design in Robotic Assembly", "categories": ["cs.RO"], "comment": null, "summary": "As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),\nwhich is designing products for efficient automated assembly, is increasingly\nimportant. Traditional approaches to DfRA rely on manual planning, which is\ntime-consuming, expensive and potentially impractical for complex objects.\nLarge language models (LLM) have exhibited proficiency in semantic\ninterpretation and robotic task planning, stimulating interest in their\napplication to the automation of DfRA. But existing methodologies typically\nrely on heuristic strategies and rigid, hard-coded physics simulators that may\nnot translate into real-world assembly contexts. In this work, we present\nIterative Design for Robotic Assembly (IDfRA), a framework using iterative\ncycles of planning, execution, verification, and re-planning, each informed by\nself-assessment, to progressively enhance design quality within a fixed yet\ninitially under-specified environment, thereby eliminating the physics\nsimulation with the real world itself. The framework accepts as input a target\nstructure together with a partial environmental representation. Through\nsuccessive refinement, it converges toward solutions that reconcile semantic\nfidelity with physical feasibility. Empirical evaluation demonstrates that\nIDfRA attains 73.3\\% top-1 accuracy in semantic recognisability, surpassing the\nbaseline on this metric. Moreover, the resulting assembly plans exhibit robust\nphysical feasibility, achieving an overall 86.9\\% construction success rate,\nwith design quality improving across iterations, albeit not always\nmonotonically. Pairwise human evaluation further corroborates the advantages of\nIDfRA relative to alternative approaches. By integrating self-verification with\ncontext-aware adaptation, the framework evidences strong potential for\ndeployment in unstructured manufacturing scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8fed\u4ee3\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\uff08IDfRA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u8bc4\u4f30\u8fed\u4ee3\u4f18\u5316\u8bbe\u8ba1\u65b9\u6848\uff0c\u53d6\u4ee3\u4f20\u7edf\u4f9d\u8d56\u7269\u7406\u4eff\u771f\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u88c5\u914d\u6210\u529f\u7387\u548c\u8bed\u4e49\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\u65b9\u6848\u4f9d\u8d56\u8017\u65f6\u4e14\u4e0d\u7075\u6d3b\u7684\u624b\u52a8\u89c4\u5212\u6216\u7269\u7406\u4eff\u771f\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u4efb\u52a1\u548c\u771f\u5b9e\u73af\u5883\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002", "method": "IDfRA\u6846\u67b6\u7ed3\u5408\u89c4\u5212\u3001\u6267\u884c\u3001\u9a8c\u8bc1\u548c\u91cd\u65b0\u89c4\u5212\u7684\u8fed\u4ee3\u5faa\u73af\uff0c\u5229\u7528\u81ea\u6211\u8bc4\u4f30\u9010\u6b65\u4f18\u5316\u8bbe\u8ba1\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u7269\u7406\u4eff\u771f\u3002\u8f93\u5165\u4e3a\u7ed3\u6784\u548c\u90e8\u5206\u73af\u5883\u63cf\u8ff0\uff0c\u8f93\u51fa\u4e3a\u517c\u987e\u8bed\u4e49\u548c\u7269\u7406\u53ef\u884c\u6027\u7684\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cIDfRA\u5728\u8bed\u4e49\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u523073.3%\uff0c\u88c5\u914d\u6210\u529f\u7387\u4e3a86.9%\uff0c\u4e14\u8bbe\u8ba1\u8d28\u91cf\u968f\u8fed\u4ee3\u63d0\u5347\u3002\u4eba\u5de5\u5bf9\u6bd4\u4e5f\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IDfRA\u901a\u8fc7\u81ea\u6211\u9a8c\u8bc1\u548c\u73af\u5883\u9002\u5e94\uff0c\u5728\u975e\u7ed3\u6784\u5316\u5236\u9020\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5b9e\u7528\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u88c5\u914d\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16577", "pdf": "https://arxiv.org/pdf/2509.16577", "abs": "https://arxiv.org/abs/2509.16577", "authors": ["Antonio Tarizzo", "Mohammad Kazemi", "Deniz G\u00fcnd\u00fcz"], "title": "Learned Digital Codes for Over-the-Air Federated Learning", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Federated edge learning (FEEL) enables distributed model training across\nwireless devices without centralising raw data, but deployment is constrained\nby the wireless uplink. A promising direction is over-the-air (OTA)\naggregation, which merges communication with computation. Existing digital OTA\nmethods can achieve either strong convergence or robustness to noise, but\nstruggle to achieve both simultaneously, limiting performance in low\nsignal-to-noise ratios (SNRs) where many IoT devices operate. This work\nproposes a learnt digital OTA framework that extends reliable operation into\nlow-SNR conditions while maintaining the same uplink overhead as\nstate-of-the-art. The proposed method combines an unrolled decoder with a\njointly learnt unsourced random access codebook. Results show an extension of\nreliable operation by more than 7 dB, with improved global model convergence\nacross all SNR levels, highlighting the potential of learning-based design for\nFEEL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u6570\u5b57OTA\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u90a6\u8fb9\u7f18\u5b66\u4e60\uff08FEEL\uff09\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u6269\u5c55\u53ef\u9760\u64cd\u4f5c\uff0c\u4e14\u4e0d\u589e\u52a0\u4e0a\u884c\u94fe\u8def\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b57OTA\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u6536\u655b\u6027\u548c\u6297\u566a\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\uff08\u5982\u7269\u8054\u7f51\u8bbe\u5907\uff09\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5c55\u5f00\u89e3\u7801\u5668\u548c\u8054\u5408\u5b66\u4e60\u7684\u65e0\u6e90\u968f\u673a\u8bbf\u95ee\u7801\u672c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u578b\u6570\u5b57OTA\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u53ef\u9760\u64cd\u4f5c\u8303\u56f4\u6269\u5c55\u4e867 dB\u4ee5\u4e0a\uff0c\u5e76\u5728\u6240\u6709\u4fe1\u566a\u6bd4\u6c34\u5e73\u4e0a\u6539\u5584\u4e86\u5168\u5c40\u6a21\u578b\u6536\u655b\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u8bbe\u8ba1\u5728FEEL\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u5347\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17010", "pdf": "https://arxiv.org/pdf/2509.17010", "abs": "https://arxiv.org/abs/2509.17010", "authors": ["Rajpal Singh", "Aditya Singh", "Chidre Shravista Kashyap", "Jishnu Keshavan"], "title": "Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a novel Koopman operator formulation for Euler Lagrangian\ndynamics that employs an implicit generalized momentum-based state space\nrepresentation, which decouples a known linear actuation channel from state\ndependent dynamics and makes the system more amenable to linear Koopman\nmodeling. By leveraging this structural separation, the proposed formulation\nonly requires to learn the unactuated dynamics rather than the complete\nactuation dependent system, thereby significantly reducing the number of\nlearnable parameters, improving data efficiency, and lowering overall model\ncomplexity. In contrast, conventional explicit formulations inherently couple\ninputs with the state dependent terms in a nonlinear manner, making them more\nsuitable for bilinear Koopman models, which are more computationally expensive\nto train and deploy. Notably, the proposed scheme enables the formulation of\nlinear models that achieve superior prediction performance compared to\nconventional bilinear models while remaining substantially more efficient. To\nrealize this framework, we present two neural network architectures that\nconstruct Koopman embeddings from actuated or unactuated data, enabling\nflexible and efficient modeling across different tasks. Robustness is ensured\nthrough the integration of a linear Generalized Extended State Observer (GESO),\nwhich explicitly estimates disturbances and compensates for them in real time.\nThe combined momentum-based Koopman and GESO framework is validated through\ncomprehensive trajectory tracking simulations and experiments on robotic\nmanipulators, demonstrating superior accuracy, robustness, and learning\nefficiency relative to state of the art alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5e7f\u4e49\u52a8\u91cf\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u89e3\u8026\u7ebf\u6027\u9a71\u52a8\u901a\u9053\u4e0e\u72b6\u6001\u4f9d\u8d56\u52a8\u6001\uff0c\u4f18\u5316\u4e86\u7ebf\u6027Koopman\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0f\u65b9\u6cd5\u5c06\u8f93\u5165\u4e0e\u72b6\u6001\u4f9d\u8d56\u52a8\u6001\u975e\u7ebf\u6027\u8026\u5408\uff0c\u9002\u5408\u53cc\u7ebf\u6027Koopman\u6a21\u578b\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u65b0\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5206\u79bb\uff0c\u4ec5\u9700\u5b66\u4e60\u975e\u9a71\u52a8\u52a8\u6001\uff0c\u964d\u4f4e\u53c2\u6570\u6570\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u5e7f\u4e49\u52a8\u91cf\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6784\u5efaKoopman\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5e7f\u4e49\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\uff08GESO\uff09\u5b9e\u65f6\u4f30\u8ba1\u548c\u8865\u507f\u6270\u52a8\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u548c\u6a21\u578b\u7b80\u5316\uff0c\u5176\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u53cc\u7ebf\u6027\u6a21\u578b\u4e14\u66f4\u9ad8\u6548\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5b66\u4e60\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.16756", "pdf": "https://arxiv.org/pdf/2509.16756", "abs": "https://arxiv.org/abs/2509.16756", "authors": ["Yuchen Liang", "Yingbin Liang", "Lifeng Lai", "Ness Shroff"], "title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Discrete diffusion models have recently gained significant prominence in\napplications involving natural language and graph data. A key factor\ninfluencing their effectiveness is the efficiency of discretized samplers.\nAmong these, $\\tau$-leaping samplers have become particularly popular due to\ntheir empirical success. However, existing theoretical analyses of\n$\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify\nregularity assumptions, and their convergence bounds contain quadratic\ndependence on the vocabulary size. In this work, we introduce a new analytical\napproach for discrete diffusion models that removes the need for such\nassumptions. For the standard $\\tau$-leaping method, we establish convergence\nguarantees in KL divergence that scale linearly with vocabulary size, improving\nupon prior results with quadratic dependence. Our approach is also more broadly\napplicable: it provides the first convergence guarantees for other widely used\nsamplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our\napproach is a novel technique based on differential inequalities, offering a\nmore flexible alternative to the traditional Girsanov change-of-measure\nmethods. This technique may also be of independent interest for the analysis of\nother stochastic processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u4f20\u7edf$\tau$-leaping\u91c7\u6837\u5668\u7406\u8bba\u5206\u6790\u4e2d\u7684\u9650\u5236\u6027\u5047\u8bbe\uff0c\u5e76\u6539\u8fdb\u4e86\u6536\u655b\u754c\uff0c\u4f7f\u5176\u4e0e\u8bcd\u6c47\u91cf\u7ebf\u6027\u76f8\u5173\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u548c\u56fe\u6570\u636e\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5206\u6790\u4f9d\u8d56\u4e25\u683c\u5047\u8bbe\u4e14\u6536\u655b\u754c\u4e0d\u7406\u60f3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5fae\u5206\u4e0d\u7b49\u5f0f\u7684\u65b0\u6280\u672f\uff0c\u5206\u6790$\tau$-leaping\u3001Euler\u65b9\u6cd5\u548cTweedie $\tau$-leaping\u7b49\u91c7\u6837\u5668\u7684\u6536\u655b\u6027\u3002", "result": "\u6210\u529f\u5c06$\tau$-leaping\u91c7\u6837\u5668\u7684KL\u6563\u5ea6\u6536\u655b\u754c\u4ece\u8bcd\u6c47\u91cf\u7684\u4e8c\u6b21\u4f9d\u8d56\u6539\u8fdb\u4e3a\u7ebf\u6027\u4f9d\u8d56\uff0c\u5e76\u9996\u6b21\u4e3a\u5176\u4ed6\u5e38\u7528\u91c7\u6837\u5668\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8fd8\u63d0\u5347\u4e86\u9002\u7528\u6027\uff0c\u5bf9\u66f4\u5e7f\u6cdb\u7684\u968f\u673a\u8fc7\u7a0b\u5206\u6790\u53ef\u80fd\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2509.17042", "pdf": "https://arxiv.org/pdf/2509.17042", "abs": "https://arxiv.org/abs/2509.17042", "authors": ["Zengqi Peng", "Yusen Xie", "Yubin Wang", "Rui Yang", "Qifeng Chen", "Jun Ma"], "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning", "categories": ["cs.RO"], "comment": null, "summary": "The advancement of foundation models fosters new initiatives for policy\nlearning in achieving safe and efficient autonomous driving. However, a\ncritical bottleneck lies in the manual engineering of reward functions and\ntraining curricula for complex and dynamic driving tasks, which is a\nlabor-intensive and time-consuming process. To address this problem, we propose\nOGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning\nframework that leverages vision-language model (VLM)-based multi-agent\ncollaboration. Our framework capitalizes on advanced reasoning and multimodal\nunderstanding capabilities of VLMs to construct a hierarchical agent system.\nSpecifically, a centralized orchestrator plans high-level training objectives,\nwhile a generation module employs a two-step analyze-then-generate process for\nefficient generation of reward-curriculum pairs. A reflection module then\nfacilitates iterative optimization based on the online evaluation. Furthermore,\na dedicated memory module endows the VLM agents with the capabilities of\nlong-term memory. To enhance robustness and diversity of the generation\nprocess, we introduce a parallel generation scheme and a human-in-the-loop\ntechnique for augmentation of the reward observation space. Through efficient\nmulti-agent cooperation and leveraging rich multimodal information, OGR enables\nthe online evolution of reinforcement learning policies to acquire\ninteraction-aware driving skills. Extensive experiments in the CARLA simulator\ndemonstrate the superior performance, robust generalizability across distinct\nurban scenarios, and strong compatibility with various RL algorithms. Further\nreal-world experiments highlight the practical viability and effectiveness of\nour framework. The source code will be available upon acceptance of the paper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOGR\u7684\u81ea\u52a8\u5316\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u8bfe\u7a0b\u624b\u52a8\u8bbe\u8ba1\u7684\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u8fd9\u4e0d\u4ec5\u8017\u65f6\u8017\u529b\uff0c\u8fd8\u9650\u5236\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "method": "OGR\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u7ea7\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u7684\u5956\u52b1\u8bfe\u7a0b\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOGR\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u9002\u914d\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "OGR\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16843", "pdf": "https://arxiv.org/pdf/2509.16843", "abs": "https://arxiv.org/abs/2509.16843", "authors": ["Bowen Liu", "Takasumi Tanabe"], "title": "Tolerance of Phase Noise in Single-Carrier M-Ary QAM Terahertz Wireless Communications", "categories": ["physics.optics", "eess.SP"], "comment": null, "summary": "Terahertz wireless communications offer abundant untapped spectrum and are\nregarded as a promising playground for next-generation high-throughput links.\nYet oscillator phase noise becomes the dominant impairment at such high\nfrequencies, severely limiting the reliability of high-order QAM transmission.\nHere, phase noise is reconstructed from measured spectra and embedded into a\nsingle-carrier link model to evaluate its impact. Distinct distortion\nmechanisms are identified, with slow common phase error and instantaneous phase\njitter, where the latter remains as the residual impairment after carrier phase\nrecovery. We further adopt 3{\\sigma} error criterion that maps residual\ndistortions onto the constellation, offering a clear and practical indicator of\nsystem robustness. The results indicate that modest improvements in oscillator\nstability translate into significant BER gains without proportional power\nincrease. These findings provide intuitive tolerance of phase noise in M-QAM\nsystems and emphasizes the importance of integrating low-noise photonic\noscillators.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u592a\u8d6b\u5179\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u76f8\u4f4d\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u548c\u5b9e\u9a8c\u8bc4\u4f30\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u592a\u8d6b\u5179\u901a\u4fe1\u56e0\u5176\u9ad8\u5e26\u5bbd\u6f5c\u529b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u901f\u94fe\u8def\u7684\u5019\u9009\u6280\u672f\uff0c\u4f46\u632f\u8361\u5668\u76f8\u4f4d\u566a\u58f0\u6210\u4e3a\u4e86\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002", "method": "\u4ece\u5b9e\u6d4b\u9891\u8c31\u91cd\u5efa\u76f8\u4f4d\u566a\u58f0\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5355\u8f7d\u6ce2\u94fe\u8def\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u5bf9\u9ad8\u9636QAM\u4f20\u8f93\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u632f\u8361\u5668\u7a33\u5b9a\u6027\u7684\u5c0f\u5e45\u63d0\u5347\u80fd\u663e\u8457\u6539\u5584\u8bef\u7801\u7387\uff0c\u4e14\u65e0\u9700\u6210\u6bd4\u4f8b\u589e\u52a0\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3aM-QAM\u7cfb\u7edf\u4e2d\u7684\u76f8\u4f4d\u566a\u58f0\u5bb9\u5fcd\u5ea6\u63d0\u4f9b\u4e86\u76f4\u89c2\u53c2\u8003\uff0c\u5e76\u5f3a\u8c03\u4e86\u96c6\u6210\u4f4e\u566a\u58f0\u5149\u5b50\u632f\u8361\u5668\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.17053", "pdf": "https://arxiv.org/pdf/2509.17053", "abs": "https://arxiv.org/abs/2509.17053", "authors": ["Haizhou Ge", "Yufei Jia", "Zheng Li", "Yue Li", "Zhixing Chen", "Ruqi Huang", "Guyue Zhou"], "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks", "categories": ["cs.RO", "68T40, 93C85", "I.2.9"], "comment": null, "summary": "Contact-rich manipulation is crucial for robots to perform tasks requiring\nprecise force control, such as insertion, assembly, and in-hand manipulation.\nHowever, most imitation learning (IL) policies remain position-centric and lack\nexplicit force awareness, and adding force/torque sensors to collaborative\nrobot arms is often costly and requires additional hardware design. To overcome\nthese issues, we propose FILIC, a Force-guided Imitation Learning framework\nwith impedance torque control. FILIC integrates a Transformer-based IL policy\nwith an impedance controller in a dual-loop structure, enabling compliant\nforce-informed, force-executed manipulation. For robots without force/torque\nsensors, we introduce a cost-effective end-effector force estimator using joint\ntorque measurements through analytical Jacobian-based inversion while\ncompensating with model-predicted torques from a digital twin. We also design\ncomplementary force feedback frameworks via handheld haptics and VR\nvisualization to improve demonstration quality. Experiments show that FILIC\nsignificantly outperforms vision-only and joint-torque-based methods, achieving\nsafer, more compliant, and adaptable contact-rich manipulation. Our code can be\nfound in https://github.com/TATP-233/FILIC.", "AI": {"tldr": "FILIC\u662f\u4e00\u4e2a\u529b\u5f15\u5bfc\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u963b\u6297\u626d\u77e9\u63a7\u5236\u548c\u529b\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7f3a\u4e4f\u529b\u611f\u77e5\u80fd\u529b\uff0c\u800c\u914d\u5907\u529b/\u626d\u77e9\u4f20\u611f\u5668\u7684\u534f\u4f5c\u673a\u68b0\u81c2\u6210\u672c\u9ad8\u3002FILIC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4e86Transformer\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4e0e\u963b\u6297\u63a7\u5236\u5668\uff0c\u63d0\u51fa\u4f4e\u6210\u672c\u672b\u7aef\u529b\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8fdb\u884c\u8865\u507f\u3002", "result": "FILIC\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u89c6\u89c9\u548c\u5173\u8282\u626d\u77e9\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u67d4\u987a\u7684\u64cd\u4f5c\u3002", "conclusion": "FILIC\u4e3a\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16852", "pdf": "https://arxiv.org/pdf/2509.16852", "abs": "https://arxiv.org/abs/2509.16852", "authors": ["Zhen Qin", "Zhihui Zhu"], "title": "Quantum State Tomography for Tensor Networks in Two Dimensions", "categories": ["quant-ph", "cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Recent work has shown that for one-dimensional quantum states that can be\neffectively approximated by matrix product operators (MPOs), a polynomial\nnumber of copies of the state suffices for reconstruction. Compared to MPOs in\none dimension, projected entangled-pair states (PEPSs) and projected\nentangled-pair operators (PEPOs), which represent typical low-dimensional\nstructures in two dimensions, are more prevalent as a looped tensor network.\nHowever, a formal analysis of the sample complexity required for estimating\nPEPS or PEPO has yet to be established. In this paper, we aim to address this\ngap by providing theoretical guarantees for the stable recovery of PEPS and\nPEPO. Our analysis primarily focuses on two quantum measurement schemes: $(i)$\ninformationally complete positive operator valued measures (IC-POVMs),\nspecifically the spherical $t$-designs ($t \\geq 3$), and $(ii)$ projective\nrank-one measurements, in particular Haar random projective measurements. We\nfirst establish stable embeddings for PEPSs (or PEPOs) to ensure that the\ninformation contained in the states can be preserved under these two\nmeasurement schemes. We then show that a constrained least-squares estimator\nachieves stable recovery for PEPSs (or PEPOs), with the recovery error bounded\nwhen the number of state copies scales linearly under spherical $t$-designs and\npolynomially under Haar-random projective measurements with respect to the\nnumber of qudits. These results provide theoretical support for the reliable\nuse of PEPS and PEPO in practical quantum information processing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4e8c\u7ef4\u91cf\u5b50\u6001\uff08PEPS/PEPO\uff09\u6837\u672c\u590d\u6742\u5ea6\u7406\u8bba\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u5728\u4e24\u79cd\u6d4b\u91cf\u65b9\u6848\u4e0b\u80fd\u591f\u7a33\u5b9a\u6062\u590d\u91cf\u5b50\u6001\u3002", "motivation": "\u5c3d\u7ba1\u4e00\u7ef4\u91cf\u5b50\u6001\uff08MPOs\uff09\u7684\u6062\u590d\u5df2\u6709\u7406\u8bba\u652f\u6301\uff0c\u4f46\u4e8c\u7ef4\u91cf\u5b50\u6001\uff08PEPS/PEPO\uff09\u7684\u6837\u672c\u590d\u6742\u5ea6\u5c1a\u672a\u660e\u786e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e24\u79cd\u91cf\u5b50\u6d4b\u91cf\u65b9\u6848\uff08IC-POVMs\u548cHaar\u968f\u673a\u6295\u5f71\u6d4b\u91cf\uff09\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\u5b9e\u73b0\u7a33\u5b9a\u6062\u590d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cPEPS/PEPO\u7684\u6062\u590d\u8bef\u5dee\u5728\u7403\u5f62t-design\u4e0b\u7ebf\u6027\u6536\u655b\uff0c\u5728Haar\u968f\u673a\u6295\u5f71\u6d4b\u91cf\u4e0b\u591a\u9879\u5f0f\u6536\u655b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aPEPS/PEPO\u5728\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.17057", "pdf": "https://arxiv.org/pdf/2509.17057", "abs": "https://arxiv.org/abs/2509.17057", "authors": ["Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Hanbit Oh", "Koshi Makihara", "Keisuke Shirai", "Yukiyasu Domae"], "title": "RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments", "categories": ["cs.RO"], "comment": null, "summary": "RoboManipBaselines is an open framework for robot imitation learning that\nunifies data collection, training, and evaluation across simulation and real\nrobots. We introduce it as a platform enabling systematic benchmarking of\ndiverse tasks, robots, and multimodal policies with emphasis on integration,\ngenerality, extensibility, and reproducibility.", "AI": {"tldr": "RoboManipBaselines\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7684\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u7b56\u7565\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5f00\u653e\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u96c6\u6210\u6027\u3001\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u4eff\u771f\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u7684\u65e0\u7f1d\u8854\u63a5\u548c\u9ad8\u6548\u8bc4\u4f30\u3002", "conclusion": "RoboManipBaselines\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u7075\u6d3b\u7684\u57fa\u51c6\u5de5\u5177\u3002"}}
{"id": "2509.17197", "pdf": "https://arxiv.org/pdf/2509.17197", "abs": "https://arxiv.org/abs/2509.17197", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "11 pages", "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.", "AI": {"tldr": "SignalLLM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u901a\u7528\u4fe1\u53f7\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u548c\u6709\u9650\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u548c\u624b\u52a8\u5de5\u7a0b\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u800cLLM\u5177\u6709\u63a8\u7406\u80fd\u529b\u548c\u591a\u6a21\u6001\u8fc1\u79fb\u80fd\u529b\uff0c\u9002\u5408\u81ea\u52a8\u5316\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u3002", "method": "SignalLLM\u901a\u8fc7\u5206\u89e3\u9ad8\u5c42\u76ee\u6807\u4e3a\u7ed3\u6784\u5316\u5b50\u4efb\u52a1\uff0c\u7ed3\u5408RAG\u548c\u5206\u5c42\u89c4\u5212\uff0c\u91c7\u7528\u591a\u6a21\u6001\u63a8\u7406\u3001\u4ee3\u7801\u5408\u6210\u7b49\u65b9\u5f0f\u6267\u884c\u4efb\u52a1\u3002", "result": "\u5728\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\uff0cSignalLLM\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "SignalLLM\u5c55\u793a\u4e86LLM\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u81ea\u52a8\u5316SP\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.17080", "pdf": "https://arxiv.org/pdf/2509.17080", "abs": "https://arxiv.org/abs/2509.17080", "authors": ["Ruiguo Zhong", "Ruoyu Yao", "Pei Liu", "Xiaolong Chen", "Rui Yang", "Jun Ma"], "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Accurate trajectory prediction and motion planning are crucial for autonomous\ndriving systems to navigate safely in complex, interactive environments\ncharacterized by multimodal uncertainties. However, current\ngeneration-then-evaluation frameworks typically construct multiple plausible\ntrajectory hypotheses but ultimately adopt a single most likely outcome,\nleading to overconfident decisions and a lack of fallback strategies that are\nvital for safety in rare but critical scenarios. Moreover, the usual decoupling\nof prediction and planning modules could result in socially inconsistent or\nunrealistic joint trajectories, especially in highly interactive traffic. To\naddress these challenges, we propose a contingency-aware diffusion planner\n(CoPlanner), a unified framework that jointly models multi-agent interactive\ntrajectory generation and contingency-aware motion planning. Specifically, the\npivot-conditioned diffusion mechanism anchors trajectory sampling on a\nvalidated, shared short-term segment to preserve temporal consistency, while\nstochastically generating diverse long-horizon branches that capture multimodal\nmotion evolutions. In parallel, we design a contingency-aware multi-scenario\nscoring strategy that evaluates candidate ego trajectories across multiple\nplausible long-horizon evolution scenarios, balancing safety, progress, and\ncomfort. This integrated design preserves feasible fallback options and\nenhances robustness under uncertainty, leading to more realistic\ninteraction-aware planning. Extensive closed-loop experiments on the nuPlan\nbenchmark demonstrate that CoPlanner consistently surpasses state-of-the-art\nmethods on both Val14 and Test14 datasets, achieving significant improvements\nin safety and comfort under both reactive and non-reactive settings. Code and\nmodel will be made publicly available upon acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPlanner\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u8f68\u8ff9\u9884\u6d4b\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u4ee3\u7406\u4ea4\u4e92\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u8fd0\u52a8\u89c4\u5212\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u51b3\u7b56\u548c\u7f3a\u4e4f\u540e\u5907\u7b56\u7565\u7684\u95ee\u9898\uff0c\u4e14\u9884\u6d4b\u4e0e\u89c4\u5212\u6a21\u5757\u901a\u5e38\u89e3\u8026\uff0c\u5bfc\u81f4\u793e\u4ea4\u4e0d\u4e00\u81f4\u6216\u4e0d\u73b0\u5b9e\u7684\u8f68\u8ff9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "CoPlanner\u91c7\u7528\u4e86\u4e00\u79cd\u951a\u5b9a\u5171\u4eab\u77ed\u671f\u6bb5\u5e76\u968f\u673a\u751f\u6210\u957f\u65f6\u6bb5\u5206\u652f\u7684\u6269\u6563\u673a\u5236\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u591a\u60c5\u666f\u8bc4\u5206\u7b56\u7565\uff0c\u7efc\u5408\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\u7684\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoPlanner\u5728\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u53cd\u5e94\u548c\u975e\u53cd\u5e94\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CoPlanner\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u8f68\u8ff9\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u4ea4\u4e92\u611f\u77e5\u89c4\u5212\uff0c\u5e76\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u540e\u5907\u9009\u9879\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17250", "pdf": "https://arxiv.org/pdf/2509.17250", "abs": "https://arxiv.org/abs/2509.17250", "authors": ["Yigit Berkay Uslu", "Samar Hadou", "Sergio Rozada", "Shirin Saeedi Bidokhti", "Alejandro Ribeiro"], "title": "Graph Signal Generative Diffusion Models", "categories": ["cs.LG", "eess.SP"], "comment": "Submitted to 2026 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2026)", "summary": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for\nstochastic graph signal generation using denoising diffusion processes. The\narchitecture learns node features at different resolutions with skip\nconnections between the encoder and decoder paths, analogous to the\nconvolutional U-Net for image generation. The U-GNN is prominent for a pooling\noperation that leverages zero-padding and avoids arbitrary graph coarsening,\nwith graph convolutions layered on top to capture local dependencies. This\ntechnique permits learning feature embeddings for sampled nodes at deeper\nlevels of the architecture that remain convolutional with respect to the\noriginal graph. Applied to stock price prediction -- where deterministic\nforecasts struggle to capture uncertainties and tail events that are paramount\n-- we demonstrate the effectiveness of the diffusion model in probabilistic\nforecasting of stock prices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdU\u5f62\u7f16\u7801\u5668-\u89e3\u7801\u5668\u56fe\u795e\u7ecf\u7f51\u7edc\uff08U-GNN\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u968f\u673a\u56fe\u4fe1\u53f7\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u80a1\u7968\u4ef7\u683c\u7684\u6982\u7387\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u80a1\u7968\u4ef7\u683c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5c3e\u90e8\u4e8b\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u6982\u7387\u6027\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86U\u5f62\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5229\u7528\u8df3\u8fc7\u8fde\u63a5\u548c\u96f6\u586b\u5145\u6c60\u5316\u64cd\u4f5c\u6765\u907f\u514d\u4efb\u610f\u56fe\u7c97\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cU-GNN\u5728\u80a1\u7968\u4ef7\u683c\u7684\u968f\u673a\u9884\u6d4b\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u80fd\u591f\u6355\u6349\u5230\u4e0d\u786e\u5b9a\u6027\u548c\u5c3e\u90e8\u4e8b\u4ef6\u3002", "conclusion": "U-GNN\u662f\u4e00\u79cd\u6709\u6548\u7684\u56fe\u4fe1\u53f7\u751f\u6210\u548c\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u5c3e\u90e8\u4e8b\u4ef6\u7684\u573a\u666f\u3002"}}
{"id": "2509.17125", "pdf": "https://arxiv.org/pdf/2509.17125", "abs": "https://arxiv.org/abs/2509.17125", "authors": ["Liang Heng", "Jiadong Xu", "Yiwen Wang", "Xiaoqi Li", "Muhe Cai", "Yan Shen", "Juan Zhu", "Guanghui Ren", "Hao Dong"], "title": "Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)\nrequire a robot to manipulate objects with precise semantic and geometric\nreasoning. Existing approaches either rely on pre-collected demonstrations that\nstruggle to capture complex geometric constraints or generate goal-state\nobservations to capture semantic and geometric knowledge, but fail to\nexplicitly couple object transformation with action prediction, resulting in\nerrors due to generative noise. To address these limitations, we propose\nImagine2Act, a 3D imitation-learning framework that incorporates semantic and\ngeometric constraints of objects into policy learning to tackle high-precision\nmanipulation tasks. We first generate imagined goal images conditioned on\nlanguage instructions and reconstruct corresponding 3D point clouds to provide\nrobust semantic and geometric priors. These imagined goal point clouds serve as\nadditional inputs to the policy model, while an object-action consistency\nstrategy with soft pose supervision explicitly aligns predicted end-effector\nmotion with generated object transformation. This design enables Imagine2Act to\nreason about semantic and geometric relationships between objects and predict\naccurate actions across diverse tasks. Experiments in both simulation and the\nreal world demonstrate that Imagine2Act outperforms previous state-of-the-art\npolicies. More visualizations can be found at\nhttps://sites.google.com/view/imagine2act.", "AI": {"tldr": "Imagine2Act\u662f\u4e00\u4e2a3D\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ea6\u675f\u6765\u63d0\u5347\u673a\u5668\u4eba\u5728\u5173\u7cfb\u5bf9\u8c61\u91cd\u6392\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9884\u6536\u96c6\u7684\u6f14\u793a\uff08\u96be\u4ee5\u6355\u6349\u590d\u6742\u51e0\u4f55\u7ea6\u675f\uff09\uff0c\u8981\u4e48\u901a\u8fc7\u751f\u6210\u76ee\u6807\u72b6\u6001\u89c2\u5bdf\u6355\u83b7\u8bed\u4e49\u548c\u51e0\u4f55\u77e5\u8bc6\uff0c\u4f46\u672a\u660e\u786e\u8026\u5408\u5bf9\u8c61\u53d8\u6362\u4e0e\u52a8\u4f5c\u9884\u6d4b\uff0c\u5bfc\u81f4\u751f\u6210\u566a\u58f0\u8bef\u5dee\u3002", "method": "Imagine2Act\u901a\u8fc7\u751f\u6210\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u76ee\u6807\u56fe\u50cf\u5e76\u91cd\u5efa3D\u70b9\u4e91\uff0c\u5c06\u8fd9\u4e9b\u70b9\u4e91\u4f5c\u4e3a\u7b56\u7565\u6a21\u578b\u7684\u8f93\u5165\uff0c\u540c\u65f6\u91c7\u7528\u5bf9\u8c61-\u52a8\u4f5c\u4e00\u81f4\u6027\u7b56\u7565\u548c\u8f6f\u59ff\u6001\u76d1\u7763\uff0c\u663e\u5f0f\u5bf9\u9f50\u9884\u6d4b\u7684\u52a8\u4f5c\u4e0e\u5bf9\u8c61\u53d8\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cImagine2Act\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b56\u7565\u3002", "conclusion": "Imagine2Act\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.17293", "pdf": "https://arxiv.org/pdf/2509.17293", "abs": "https://arxiv.org/abs/2509.17293", "authors": ["Ryan Chappell", "Chayan Banerjee", "Kien Nguyen", "Clinton Fookes"], "title": "Physics-Informed Operator Learning for Hemodynamic Modeling", "categories": ["cs.LG", "eess.SP"], "comment": "To appear in the proceedings of DICTA 2025", "summary": "Accurate modeling of personalized cardiovascular dynamics is crucial for\nnon-invasive monitoring and therapy planning. State-of-the-art physics-informed\nneural network (PINN) approaches employ deep, multi-branch architectures with\nadversarial or contrastive objectives to enforce partial differential equation\nconstraints. While effective, these enhancements introduce significant training\nand implementation complexity, limiting scalability and practical deployment.\nWe investigate physics-informed neural operator learning models as efficient\nsupervisory signals for training simplified architectures through knowledge\ndistillation. Our approach pre-trains a physics-informed DeepONet (PI-DeepONet)\non high-fidelity cuffless blood pressure recordings to learn operator mappings\nfrom raw wearable waveforms to beat-to-beat pressure signals under embedded\nphysics constraints. This pre-trained operator serves as a frozen supervisor in\na lightweight knowledge-distillation pipeline, guiding streamlined base models\nthat eliminate complex adversarial and contrastive learning components while\nmaintaining performance. We characterize the role of physics-informed\nregularization in operator learning and demonstrate its effectiveness for\nsupervisory guidance. Through extensive experiments, our operator-supervised\napproach achieves performance parity with complex baselines (correlation: 0.766\nvs. 0.770, RMSE: 4.452 vs. 4.501), while dramatically reducing architectural\ncomplexity from eight critical hyperparameters to a single regularization\ncoefficient and decreasing training overhead by 4%. Our results demonstrate\nthat operator-based supervision effectively replaces intricate multi-component\ntraining strategies, offering a more scalable and interpretable approach to\nphysiological modeling with reduced implementation burden.", "AI": {"tldr": "\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u6a21\u578b\uff08PI-DeepONet\uff09\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u7b80\u5316\u5fc3\u8840\u7ba1\u52a8\u6001\u5efa\u6a21\uff0c\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u3002", "motivation": "\u7cbe\u786e\u7684\u5fc3\u8840\u7ba1\u52a8\u6001\u5efa\u6a21\u5bf9\u65e0\u521b\u76d1\u6d4b\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u65b9\u6cd5\u56e0\u590d\u6742\u67b6\u6784\u548c\u8bad\u7ec3\u9650\u5236\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u9884\u8bad\u7ec3PI-DeepONet\u5b66\u4e60\u7a7f\u6234\u8bbe\u5907\u6ce2\u5f62\u5230\u8840\u538b\u4fe1\u53f7\u7684\u6620\u5c04\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u84b8\u998f\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u7b80\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u590d\u6742\u57fa\u7ebf\u76f8\u5f53\uff08\u76f8\u5173\u60270.766 vs. 0.770\uff0cRMSE 4.452 vs. 4.501\uff09\uff0c\u8bad\u7ec3\u5f00\u9500\u51cf\u5c114%\u3002", "conclusion": "\u7b97\u5b50\u76d1\u7763\u53ef\u66ff\u4ee3\u590d\u6742\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u4f9b\u66f4\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u751f\u7406\u5efa\u6a21\u65b9\u6cd5\uff0c\u964d\u4f4e\u5b9e\u73b0\u8d1f\u62c5\u3002"}}
{"id": "2509.17141", "pdf": "https://arxiv.org/pdf/2509.17141", "abs": "https://arxiv.org/abs/2509.17141", "authors": ["Jingjing Chen", "Hongjie Fang", "Chenxi Wang", "Shiquan Wang", "Cewu Lu"], "title": "History-Aware Visuomotor Policy Learning via Point Tracking", "categories": ["cs.RO"], "comment": null, "summary": "Many manipulation tasks require memory beyond the current observation, yet\nmost visuomotor policies rely on the Markov assumption and thus struggle with\nrepeated states or long-horizon dependencies. Existing methods attempt to\nextend observation horizons but remain insufficient for diverse memory\nrequirements. To this end, we propose an object-centric history representation\nbased on point tracking, which abstracts past observations into a compact and\nstructured form that retains only essential task-relevant information. Tracked\npoints are encoded and aggregated at the object level, yielding a compact\nhistory representation that can be seamlessly integrated into various\nvisuomotor policies. Our design provides full history-awareness with high\ncomputational efficiency, leading to improved overall task performance and\ndecision accuracy. Through extensive evaluations on diverse manipulation tasks,\nwe show that our method addresses multiple facets of memory requirements - such\nas task stage identification, spatial memorization, and action counting, as\nwell as longer-term demands like continuous and pre-loaded memory - and\nconsistently outperforms both Markovian baselines and prior history-based\napproaches. Project website: http://tonyfang.net/history", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u8ffd\u8e2a\u7684\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u5386\u53f2\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u56e0\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u800c\u96be\u4ee5\u5904\u7406\u91cd\u590d\u72b6\u6001\u6216\u957f\u65f6\u4f9d\u8d56\u6027\u4efb\u52a1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u8bb0\u5fc6\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u91cd\u590d\u72b6\u6001\u6216\u957f\u65f6\u4f9d\u8d56\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u70b9\u8ffd\u8e2a\u5c06\u8fc7\u53bb\u89c2\u6d4b\u62bd\u8c61\u4e3a\u7d27\u51d1\u4e14\u7ed3\u6784\u5316\u7684\u7269\u4f53\u7ea7\u5386\u53f2\u8868\u793a\uff0c\u5e76\u5c06\u5176\u65e0\u7f1d\u6574\u5408\u5230\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u9a6c\u5c14\u53ef\u592b\u57fa\u51c6\u548c\u5148\u524d\u7684\u5386\u53f2\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u9636\u6bb5\u8bc6\u522b\u3001\u7a7a\u95f4\u8bb0\u5fc6\u7b49\u591a\u79cd\u8bb0\u5fc6\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u5386\u53f2\u8868\u793a\u65b9\u6cd5\u5728\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u51b3\u7b56\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.17490", "pdf": "https://arxiv.org/pdf/2509.17490", "abs": "https://arxiv.org/abs/2509.17490", "authors": ["Yuseon Choi", "Hyeonseung Kim", "Jewoo Jun", "Jong Won Shin"], "title": "FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for Multiple Moving Sound Source Localization", "categories": ["eess.AS", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Dual-path processing along the temporal and spectral dimensions has shown to\nbe effective in various speech processing applications. While the sound source\nlocalization (SSL) models utilizing dual-path processing such as the FN-SSL and\nIPDnet demonstrated impressive performances in localizing multiple moving\nsources, they require significant amount of computation. In this paper, we\npropose an architecture for SSL which introduces a U-Net to perform narrow-band\nprocessing in multiple resolutions to reduce computational complexity. The\nproposed model replaces the full-narrow network block in the IPDnet consisting\nof one full-band LSTM layer along the spectral dimension followed by one\nnarrow-band LSTM layer along the temporal dimension with the FUN block composed\nof one Full-band layer followed by a U-net with Narrow-band layers in multiple\nscales. On top of the skip connections within each U-Net, we also introduce the\nskip connections between FUN blocks to enrich information. Experimental results\nshowed that the proposed FUN-SSL outperformed previously proposed approaches\nwith computational complexity much lower than that of the IPDnet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684SSL\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u7a84\u5e26\u5904\u7406\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53cc\u8def\u5f84\u5904\u7406\u6a21\u578b\uff08\u5982FN-SSL\u548cIPDnet\uff09\u5728\u591a\u6e90\u5b9a\u4f4d\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4f18\u5316\u3002", "method": "\u7528FUN\u5757\uff08\u5168\u9891\u5c42+\u591a\u5c3a\u5ea6\u7a84\u5e26U-Net\uff09\u66ff\u4ee3IPDnet\u7684\u5168\u7a84\u7f51\u7edc\u5757\uff0c\u5e76\u5f15\u5165\u8de8\u5757\u8df3\u8dc3\u8fde\u63a5\u3002", "result": "FUN-SSL\u6027\u80fd\u4f18\u4e8eIPDnet\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u591a\u5206\u8fa8\u7387\u7a84\u5e26U-Net\u7ed3\u6784\u53ef\u6709\u6548\u63d0\u5347SSL\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2509.17195", "pdf": "https://arxiv.org/pdf/2509.17195", "abs": "https://arxiv.org/abs/2509.17195", "authors": ["Damian Owerko", "Frederic Vatnsdal", "Saurav Agarwal", "Vijay Kumar", "Alejandro Ribeiro"], "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate", "categories": ["cs.RO"], "comment": null, "summary": "This article presents a novel multi-agent spatial transformer (MAST) for\nlearning communication policies in large-scale decentralized and collaborative\nmulti-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:\n(i) partial observable states as robots make only localized perception, (ii)\nlimited communication range with no central server, and (iii) independent\nexecution of actions. The robots need to optimize a common task-specific\nobjective, which, under the restricted setting, must be done using a\ncommunication policy that exhibits the desired collaborative behavior. The\nproposed MAST is a decentralized transformer architecture that learns\ncommunication policies to compute abstract information to be shared with other\nagents and processes the received information with the robot's own\nobservations. The MAST extends the standard transformer with new positional\nencoding strategies and attention operations that employ windowing to limit the\nreceptive field for MRS. These are designed for local computation,\nshift-equivariance, and permutation equivariance, making it a promising\napproach for DC-MRS. We demonstrate the efficacy of MAST on decentralized\nassignment and navigation (DAN) and decentralized coverage control. Efficiently\ntrained using imitation learning in a centralized setting, the decentralized\nMAST policy is robust to communication delays, scales to large teams, and\nperforms better than the baselines and other learning-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u7a7a\u95f4\u53d8\u6362\u5668\uff08MAST\uff09\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u6563\u534f\u4f5c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08DC-MRS\uff09\u4e2d\u7684\u901a\u4fe1\u7b56\u7565\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u72b6\u6001\u3001\u6709\u9650\u901a\u4fe1\u8303\u56f4\u548c\u72ec\u7acb\u6267\u884c\u52a8\u4f5c\u7b49\u6311\u6218\u3002", "motivation": "\u5728DC-MRS\u4e2d\uff0c\u673a\u5668\u4eba\u534f\u4f5c\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u72b6\u6001\u3001\u6709\u9650\u901a\u4fe1\u8303\u56f4\u548c\u72ec\u7acb\u6267\u884c\u52a8\u4f5c\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f18\u5316\u5171\u540c\u4efb\u52a1\u7684\u901a\u4fe1\u7b56\u7565\u3002", "method": "MAST\u662f\u4e00\u79cd\u5206\u6563\u5f0f\u53d8\u6362\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u548c\u6ce8\u610f\u529b\u64cd\u4f5c\uff08\u5982\u7a97\u53e3\u5316\uff09\u5b9e\u73b0\u5c40\u90e8\u8ba1\u7b97\u548c\u7f6e\u6362\u7b49\u4ef7\u6027\u3002", "result": "MAST\u5728\u5206\u6563\u5206\u914d\u5bfc\u822a\u548c\u5206\u6563\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u548c\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MAST\u662fDC-MRS\u4e2d\u4e00\u79cd\u6709\u524d\u666f\u7684\u901a\u4fe1\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u534f\u4f5c\u6311\u6218\u5e76\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.17549", "pdf": "https://arxiv.org/pdf/2509.17549", "abs": "https://arxiv.org/abs/2509.17549", "authors": ["Keita Kume", "Isao Yamada"], "title": "Minimization of Nonsmooth Weakly Convex Function over Prox-regular Set for Robust Low-rank Matrix Recovery", "categories": ["math.OC", "eess.SP"], "comment": "5 pages, 1 table", "summary": "We propose a prox-regular-type low-rank constrained nonconvex nonsmooth\noptimization model for Robust Low-Rank Matrix Recovery (RLRMR), i.e., estimate\nproblem of low-rank matrix from an observed signal corrupted by outliers. For\nRLRMR, the $\\ell_{1}$-norm has been utilized as a convex loss to detect\noutliers as well as to keep tractability of optimization models. Nevertheless,\nthe $\\ell_{1}$-norm is not necessarily an ideal robust loss because the\n$\\ell_{1}$-norm tends to overpenalize entries corrupted by outliers of large\nmagnitude. In contrast, the proposed model can employ a weakly convex function\nas a more robust loss, against outliers, than the $\\ell_{1}$-norm. For the\nproposed model, we present (i) a projected variable smoothing-type algorithm\napplicable for the minimization of a nonsmooth weakly convex function over a\nprox-regular set, and (ii) a convergence analysis of the proposed algorithm in\nterms of stationary point. Numerical experiments demonstrate the effectiveness\nof the proposed model compared with the existing models that employ the\n$\\ell_{1}$-norm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u9c81\u68d2\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u7684\u975e\u51f8\u975e\u5149\u6ed1\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u4f20\u7edfL1\u8303\u6570\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfL1\u8303\u6570\u5728\u5904\u7406\u5927\u5f02\u5e38\u503c\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u635f\u5931\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f31\u51f8\u51fd\u6570\u7684\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6295\u5f71\u53d8\u91cf\u5e73\u6ed1\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6a21\u578b\u5728\u5904\u7406\u5f02\u5e38\u503c\u65f6\u6bd4L1\u8303\u6570\u66f4\u6709\u6548\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u9c81\u68d2\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.17198", "pdf": "https://arxiv.org/pdf/2509.17198", "abs": "https://arxiv.org/abs/2509.17198", "authors": ["Baoshan Song", "Weisong Wen", "Qi Zhang", "Bing Xu", "Li-Ta Hsu"], "title": "Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This manuscript has been submitted to IEEE Transactions on Aerospace\n  and Electronic Systems (TAES). The current version is uploaded to arXiv for\n  open access and reference purposes only", "summary": "To provide backup and augmentation to global navigation satellite system\n(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as\nsignals of opportunity (SOP) for position, navigation and timing (PNT). Since\nthe Doppler positioning problem is non-convex, local searching methods may\nproduce two types of estimates: a global optimum without notice or a local\noptimum given an inexact initial estimate. As exact initialization is\nunavailable in some unknown environments, a guaranteed global optimization\nmethod in no need of initialization becomes necessary. To achieve this goal, we\npropose a certifiably optimal LEO Doppler positioning method by utilizing\nconvex optimization. In this paper, the certifiable positioning method is\nimplemented through a graduated weight approximation (GWA) algorithm and\nsemidefinite programming (SDP) relaxation. To guarantee the optimality, we\nderive the necessary conditions for optimality in ideal noiseless cases and\nsufficient noise bounds conditions in noisy cases. Simulation and real tests\nare conducted to evaluate the effectiveness and robustness of the proposed\nmethod. Specially, the real test using Iridium-NEXT satellites shows that the\nproposed method estimates an certifiably optimal solution with an 3D\npositioning error of 140 m without initial estimates while Gauss-Newton and\nDog-Leg are trapped in local optima when the initial point is equal or larger\nthan 1000 km away from the ground truth. Moreover, the certifiable estimation\ncan also be used as initialization in local searching methods to lower down the\n3D positioning error to 130 m.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u51f8\u4f18\u5316\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u7684\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u591a\u666e\u52d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u56e0\u521d\u59cb\u4f30\u8ba1\u4e0d\u51c6\u786e\u800c\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff08GNSS\uff09\u65e0\u6cd5\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7684\u591a\u666e\u52d2\u9891\u79fb\u53ef\u4f5c\u4e3a\u673a\u4f1a\u4fe1\u53f7\uff08SOP\uff09\u7528\u4e8e\u5b9a\u4f4d\u5bfc\u822a\u4e0e\u6388\u65f6\uff08PNT\uff09\u3002\u7531\u4e8e\u591a\u666e\u52d2\u5b9a\u4f4d\u95ee\u9898\u662f\u975e\u51f8\u7684\uff0c\u4f20\u7edf\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u6216\u65e0\u6cd5\u5168\u5c40\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u4e86\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd5\u4e1a\u6743\u91cd\u8fd1\u4f3c\uff08GWA\uff09\u7b97\u6cd5\u548c\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u677e\u5f1b\u5b9e\u73b0\uff0c\u5e76\u63a8\u5bfc\u4e86\u7406\u60f3\u65e0\u566a\u60c5\u51b5\u548c\u6709\u566a\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u6d4b\u8bd5\uff08\u5982Iridium-NEXT\u536b\u661f\uff09\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u521d\u59cb\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\u80fd\u63d0\u4f9b140\u7c73\u76843D\u5b9a\u4f4d\u8bef\u5dee\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u521d\u59cb\u70b9\u8fdc\u79bb\u771f\u5b9e\u503c\u65f6\u4f1a\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u5b9a\u4f4d\u8bef\u5dee\u81f3130\u7c73\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18011", "pdf": "https://arxiv.org/pdf/2509.18011", "abs": "https://arxiv.org/abs/2509.18011", "authors": ["Fernando Llorente", "Daniel Waxman", "Sanket Jantre", "Nathan M. Urban", "Susan E. Minkoff"], "title": "Robust, Online, and Adaptive Decentralized Gaussian Processes", "categories": ["stat.ML", "cs.LG", "cs.MA", "eess.SP"], "comment": "Submitted to Icassp 2026 Special Session on \"Bridging Signal\n  Processing and Machine Learning with Gaussian Processes.\"", "summary": "Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for\nmodeling complex signals, but scale cubically with data, assume static targets,\nand are brittle to outliers, limiting their applicability in large-scale\nproblems with dynamic and noisy environments. Recent work introduced\ndecentralized random Fourier feature Gaussian processes (DRFGP), an online and\ndistributed algorithm that casts GPs in an information-filter form, enabling\nexact sequential inference and fully distributed computation without reliance\non a fusion center. In this paper, we extend DRFGP along two key directions:\nfirst, by introducing a robust-filtering update that downweights the impact of\natypical observations; and second, by incorporating a dynamic adaptation\nmechanism that adapts to time-varying functions. The resulting algorithm\nretains the recursive information-filter structure while enhancing stability\nand accuracy. We demonstrate its effectiveness on a large-scale Earth system\napplication, underscoring its potential for in-situ modeling.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86DRFGP\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9c81\u68d2\u6ee4\u6ce2\u5668\u66f4\u65b0\u548c\u52a8\u6001\u9002\u5e94\u673a\u5236\uff0c\u63d0\u5347\u4e86\u5728\u9ad8\u566a\u58f0\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "Gaussian processes\uff08GPs\uff09\u5728\u5927\u89c4\u6a21\u52a8\u6001\u4e14\u566a\u58f0\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u5bf9\u5f02\u5e38\u503c\u654f\u611f\u7b49\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u9002\u5e94\u8fd9\u7c7b\u573a\u666f\u3002", "method": "\u6269\u5c55\u4e86DRFGP\u7b97\u6cd5\uff0c\u52a0\u5165\u9c81\u68d2\u6ee4\u6ce2\u5668\u66f4\u65b0\u4ee5\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u9002\u5e94\u673a\u5236\u4ee5\u5e94\u5bf9\u65f6\u53d8\u51fd\u6570\u3002", "result": "\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u4fdd\u6301\u4e86\u4fe1\u606f\u6ee4\u6ce2\u7684\u9012\u5f52\u7ed3\u6784\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728\u5927\u89c4\u6a21\u5730\u7403\u7cfb\u7edf\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6269\u5c55\u540e\u7684DRFGP\u7b97\u6cd5\u5728\u9ad8\u566a\u58f0\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u9002\u7528\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2509.17204", "pdf": "https://arxiv.org/pdf/2509.17204", "abs": "https://arxiv.org/abs/2509.17204", "authors": ["James R. Han", "Mithun Vanniasinghe", "Hshmat Sahak", "Nicholas Rhinehart", "Timothy D. Barfoot"], "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation", "categories": ["cs.RO"], "comment": "8 pages. Under review at ICRA 2026", "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both\ndata-intensive and unsafe, since policies must learn through direct interaction\nand inevitably encounter collisions. Offline Imitation learning (IL) avoids\nthese risks by collecting expert demonstrations safely, training entirely\noffline, and deploying policies zero-shot. However, we find that naively\napplying Behaviour Cloning (BC) to social navigation is insufficient; achieving\nstrong performance requires careful architectural and training choices. We\npresent Ratatouille, a pipeline and model architecture that, without changing\nthe data, reduces collisions per meter by 6 times and improves success rate by\n3 times compared to naive BC. We validate our approach in both simulation and\nthe real world, where we collected over 11 hours of data on a dense university\ncampus. We further demonstrate qualitative results in a public food court. Our\nfindings highlight that thoughtful IL design, rather than additional data, can\nsubstantially improve safety and reliability in real-world social navigation.\nVideo: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRatatouille\uff0c\u901a\u8fc7\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6210\u529f\u7387\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u5bc6\u96c6\u6027\u548c\u5b89\u5168\u98ce\u9669\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff08Ratatouille\uff09\uff0c\u4f18\u5316\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5373\u53ef\u51cf\u5c11\u78b0\u649e\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u78b0\u649e\u7387\u964d\u4f4e6\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad83\u500d\uff0c\u5e76\u5728\u5927\u5b66\u6821\u56ed\u548c\u516c\u5171\u533a\u57df\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u6570\u636e\u3002"}}
{"id": "2509.18046", "pdf": "https://arxiv.org/pdf/2509.18046", "abs": "https://arxiv.org/abs/2509.18046", "authors": ["Yinuo Wang", "Yuanyang Qi", "Jinzhao Zhou", "Gavin Tao"], "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.SY", "eess.SP", "eess.SY"], "comment": "10 pages", "summary": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing\nfor its compact perception-action mapping, yet practical policies often suffer\nfrom training instability, inefficient feature fusion, and high actuation cost.\nWe present HuMam, a state-centric end-to-end RL framework that employs a\nsingle-layer Mamba encoder to fuse robot-centric states with oriented footstep\ntargets and a continuous phase clock. The policy outputs joint position targets\ntracked by a low-level PD loop and is optimized with PPO. A concise six-term\nreward balances contact quality, swing smoothness, foot placement, posture, and\nbody stability while implicitly promoting energy saving. On the JVRC-1 humanoid\nin mc-mujoco, HuMam consistently improves learning efficiency, training\nstability, and overall task performance over a strong feedforward baseline,\nwhile reducing power consumption and torque peaks. To our knowledge, this is\nthe first end-to-end humanoid RL controller that adopts Mamba as the fusion\nbackbone, demonstrating tangible gains in efficiency, stability, and control\neconomy.", "AI": {"tldr": "HuMam\u662f\u4e00\u4e2a\u57fa\u4e8eMamba\u7684\u5355\u5c42\u7f16\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u63a7\u5236\u7ecf\u6d4e\u6027\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u548c\u9ad8\u9a71\u52a8\u6210\u672c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u5c42Mamba\u7f16\u7801\u5668\u878d\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u8db3\u90e8\u76ee\u6807\u4e0e\u76f8\u4f4d\u65f6\u949f\uff0c\u7ed3\u5408PPO\u4f18\u5316\u7b56\u7565\uff0c\u516d\u9879\u5956\u52b1\u51fd\u6570\u5e73\u8861\u591a\u76ee\u6807\u3002", "result": "\u5728JVRC-1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u8868\u73b0\uff0c\u964d\u4f4e\u529f\u8017\u548c\u626d\u77e9\u5cf0\u503c\u3002", "conclusion": "HuMam\u662f\u9996\u4e2a\u91c7\u7528Mamba\u4f5c\u4e3a\u878d\u5408\u4e3b\u5e72\u7684\u7aef\u5230\u7aef\u4eba\u5f62RL\u63a7\u5236\u5668\uff0c\u5c55\u793a\u4e86\u5728\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u7ecf\u6d4e\u6027\u4e0a\u7684\u5b9e\u8d28\u6027\u6539\u8fdb\u3002"}}
{"id": "2509.17210", "pdf": "https://arxiv.org/pdf/2509.17210", "abs": "https://arxiv.org/abs/2509.17210", "authors": ["Shaunak A. Mehta", "Dylan P. Losey"], "title": "Combining Performance and Passivity in Linear Control of Series Elastic Actuators", "categories": ["cs.RO"], "comment": null, "summary": "When humans physically interact with robots, we need the robots to be both\nsafe and performant. Series elastic actuators (SEAs) fundamentally advance\nsafety by introducing compliant actuation. On the one hand, adding a spring\nmitigates the impact of accidental collisions between human and robot; but on\nthe other hand, this spring introduces oscillations and fundamentally decreases\nthe robot's ability to perform precise, accurate motions. So how should we\ntrade off between physical safety and performance? In this paper, we enumerate\nthe different linear control and mechanical configurations for series elastic\nactuators, and explore how each choice affects the rendered compliance,\npassivity, and tracking performance. While prior works focus on load side\ncontrol, we find that actuator side control has significant benefits. Indeed,\nsimple PD controllers on the actuator side allow for a much wider range of\ncontrol gains that maintain safety, and combining these with a damper in the\nelastic transmission yields high performance. Our simulations and real world\nexperiments suggest that, by designing a system with low physical stiffness and\nhigh controller gains, this solution enables accurate performance while also\nensuring user safety during collisions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u4eba\u7cfb\u5217\u5f39\u6027\u6267\u884c\u5668\uff08SEAs\uff09\u4e2d\u5982\u4f55\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6027\u80fd\uff0c\u53d1\u73b0\u901a\u8fc7\u6267\u884c\u5668\u4fa7\u7684\u7b80\u6613PD\u63a7\u5236\u53ca\u4f20\u8f93\u963b\u5c3c\u8bbe\u8ba1\uff0c\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u5e76\u5b58\u3002", "motivation": "\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u7269\u7406\u4ea4\u4e92\u65f6\u9700\u8981\u673a\u5668\u4eba\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u3002SEAs\u901a\u8fc7\u5f39\u6027\u9a71\u52a8\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f46\u540c\u65f6\u4e5f\u964d\u4f4e\u4e86\u7cbe\u786e\u8fd0\u52a8\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u6743\u8861\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4e86SEAs\u7684\u4e0d\u540c\u7ebf\u6027\u63a7\u5236\u53ca\u673a\u68b0\u914d\u7f6e\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u67d4\u987a\u6027\u3001\u88ab\u52a8\u6027\u53ca\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u63a2\u7d22\u6267\u884c\u5668\u4fa7\u63a7\u5236\u7684\u4f18\u52bf\u3002", "result": "\u6267\u884c\u5668\u4fa7\u7684\u7b80\u6613PD\u63a7\u5236\u7ed3\u5408\u5f39\u6027\u4f20\u8f93\u963b\u5c3c\u8bbe\u8ba1\uff0c\u65e2\u80fd\u6269\u5927\u5b89\u5168\u589e\u76ca\u8303\u56f4\u53c8\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002\u4eff\u771f\u4e0e\u5b9e\u9a8c\u8868\u660e\uff0c\u4f4e\u7269\u7406\u521a\u5ea6\u548c\u9ad8\u63a7\u5236\u589e\u76ca\u7684\u8bbe\u8ba1\u53ef\u5b9e\u73b0\u7cbe\u786e\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316SEAs\u7684\u63a7\u5236\u4e0e\u673a\u68b0\u8bbe\u8ba1\uff0c\u53ef\u517c\u987e\u9ad8\u6027\u80fd\u4e0e\u4eba\u673a\u4ea4\u4e92\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2509.18068", "pdf": "https://arxiv.org/pdf/2509.18068", "abs": "https://arxiv.org/abs/2509.18068", "authors": ["Bin Zhao", "Nakul Garg"], "title": "RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds", "categories": ["cs.RO", "eess.SP"], "comment": null, "summary": "Millimeter-wave radar provides perception robust to fog, smoke, dust, and low\nlight, making it attractive for size, weight, and power constrained robotic\nplatforms. Current radar imaging methods, however, rely on synthetic aperture\nor multi-frame aggregation to improve resolution, which is impractical for\nsmall aerial, inspection, or wearable systems. We present RadarSFD, a\nconditional latent diffusion framework that reconstructs dense LiDAR-like point\nclouds from a single radar frame without motion or SAR. Our approach transfers\ngeometric priors from a pretrained monocular depth estimator into the diffusion\nbackbone, anchors them to radar inputs via channel-wise latent concatenation,\nand regularizes outputs with a dual-space objective combining latent and\npixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer\nDistance and 28 cm Modified Hausdorff Distance, improving over the single-frame\nRadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame\nmethods using 5-41 frames. Qualitative results show recovery of fine walls and\nnarrow gaps, and experiments across new environments confirm strong\ngeneralization. Ablation studies highlight the importance of pretrained\ninitialization, radar BEV conditioning, and the dual-space loss. Together,\nthese results establish the first practical single-frame, no-SAR mmWave radar\npipeline for dense point cloud perception in compact robotic systems.", "AI": {"tldr": "RadarSFD\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u5e27\u96f7\u8fbe\u6570\u636e\u91cd\u5efa\u9ad8\u5bc6\u5ea6LiDAR\u6837\u70b9\u4e91\u3002\u4e0e\u9700\u8981\u591a\u5e27\u6216\u5408\u6210\u5b54\u5f84\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5728\u7d27\u51d1\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u5355\u5e27\u96f7\u8fbe\u611f\u77e5\u3002", "motivation": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u96fe\u3001\u70df\u3001\u5c18\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5c0f\u578b\u673a\u5668\u4eba\u5e73\u53f0\u3002\u7136\u800c\uff0c\u73b0\u6709\u96f7\u8fbe\u6210\u50cf\u65b9\u6cd5\u9700\u8981\u591a\u5e27\u6216\u5408\u6210\u5b54\u5f84\u6765\u63d0\u9ad8\u5206\u8fa8\u7387\uff0c\u9650\u5236\u4e86\u5176\u5728\u5c0f\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "RadarSFD\u901a\u8fc7\u4ece\u5355\u5e27\u96f7\u8fbe\u6570\u636e\u4e2d\u91cd\u5efa\u70b9\u4e91\uff0c\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u901a\u8fc7\u901a\u9053\u7ea7\u6f5c\u5728\u8fde\u63a5\u4e0e\u96f7\u8fbe\u8f93\u5165\u7ed3\u5408\u3002\u4f18\u5316\u91c7\u7528\u53cc\u7a7a\u95f4\u76ee\u6807\uff0c\u7ed3\u5408\u6f5c\u5728\u548c\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u3002", "result": "\u5728RadarHD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRadarSFD\u5728\u5355\u5e27\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u4e0e\u591a\u5e27\u65b9\u6cd5\u7ade\u4e89\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u5176\u5728\u72ed\u7a84\u95f4\u9699\u548c\u7cbe\u7ec6\u7ed3\u6784\u6062\u590d\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u6027\u3002", "conclusion": "RadarSFD\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u8fd0\u52a8\u6216\u5408\u6210\u5b54\u5f84\u7684\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5355\u5e27\u9ad8\u5bc6\u5ea6\u70b9\u4e91\u611f\u77e5\uff0c\u4e3a\u7d27\u51d1\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17213", "pdf": "https://arxiv.org/pdf/2509.17213", "abs": "https://arxiv.org/abs/2509.17213", "authors": ["Yassine Kebbati", "Naima Ait-Oufroukh", "Vincent Vigneron", "Dalil Ichala"], "title": "Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "Self-driving cars operate in constantly changing environments and are exposed\nto a variety of uncertainties and disturbances. These factors render classical\ncontrollers ineffective, especially for lateral control. Therefore, an adaptive\nMPC controller is designed in this paper for the path tracking task, tuned by\nan improved particle swarm optimization algorithm. Online parameter adaptation\nis performed using Neural Networks and ANFIS. The designed controller showed\npromising results compared to standard MPC in triple lane change and trajectory\ntracking scenarios. Code can be found here:\nhttps://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC", "AI": {"tldr": "\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8def\u5f84\u8ddf\u8e2a\u4efb\u52a1\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u5728\u7ebf\u53c2\u6570\u9002\u5e94\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6MPC\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\uff0c\u4f20\u7edf\u63a7\u5236\u5668\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u6a2a\u5411\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\uff0c\u5229\u7528\u6539\u8fdb\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8c03\u53c2\uff0c\u5e76\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u5b9e\u73b0\u5728\u7ebf\u53c2\u6570\u9002\u5e94\u3002", "result": "\u5728\u4e09\u8f66\u9053\u53d8\u6362\u548c\u8f68\u8ff9\u8ddf\u8e2a\u573a\u666f\u4e2d\uff0c\u8be5\u63a7\u5236\u5668\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MPC\u3002", "conclusion": "\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17244", "pdf": "https://arxiv.org/pdf/2509.17244", "abs": "https://arxiv.org/abs/2509.17244", "authors": ["Frederic Vatnsdal", "Romina Garcia Camargo", "Saurav Agarwal", "Alejandro Ribeiro"], "title": "Scalable Multi Agent Diffusion Policies for Coverage Control", "categories": ["cs.RO"], "comment": null, "summary": "We propose MADP, a novel diffusion-model-based approach for collaboration in\ndecentralized robot swarms. MADP leverages diffusion models to generate samples\nfrom complex and high-dimensional action distributions that capture the\ninterdependencies between agents' actions. Each robot conditions policy\nsampling on a fused representation of its own observations and perceptual\nembeddings received from peers. To evaluate this approach, we task a team of\nholonomic robots piloted by MADP to address coverage control-a canonical multi\nagent navigation problem. The policy is trained via imitation learning from a\nclairvoyant expert on the coverage control problem, with the diffusion process\nparameterized by a spatial transformer architecture to enable decentralized\ninference. We evaluate the system under varying numbers, locations, and\nvariances of importance density functions, capturing the robustness demands of\nreal-world coverage tasks. Experiments demonstrate that our model inherits\nvaluable properties from diffusion models, generalizing across agent densities\nand environments, and consistently outperforming state-of-the-art baselines.", "AI": {"tldr": "MADP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5206\u6563\u5f0f\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u4f5c\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u7ef4\u52a8\u4f5c\u5206\u5e03\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5206\u6563\u5f0f\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u9ad8\u7ef4\u590d\u6742\u52a8\u4f5c\u5206\u5e03\u4e0b\u7684\u534f\u4f5c\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684MADP\u65b9\u6cd5\uff0c\u4ee5\u6355\u6349\u673a\u5668\u4eba\u52a8\u4f5c\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002", "method": "MADP\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u7ef4\u52a8\u4f5c\u5206\u5e03\u6837\u672c\uff0c\u6bcf\u4e2a\u673a\u5668\u4eba\u57fa\u4e8e\u81ea\u8eab\u89c2\u5bdf\u548c\u540c\u4f34\u611f\u77e5\u5d4c\u5165\u7684\u7b56\u7565\u91c7\u6837\u3002\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u6269\u6563\u8fc7\u7a0b\u901a\u8fc7\u7a7a\u95f4\u53d8\u6362\u5668\u67b6\u6784\u5b9e\u73b0\u5206\u6563\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMADP\u5728\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5bc6\u5ea6\u548c\u73af\u5883\u53d8\u5316\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MADP\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5206\u6563\u5f0f\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1\u5e76\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2509.17274", "pdf": "https://arxiv.org/pdf/2509.17274", "abs": "https://arxiv.org/abs/2509.17274", "authors": ["Alexandros Ntagkas", "Constantinos Tsakonas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "Learning and Optimization with 3D Orientations", "categories": ["cs.RO", "cs.LG", "math.OC"], "comment": "9 pages, 11 figures", "summary": "There exist numerous ways of representing 3D orientations. Each\nrepresentation has both limitations and unique features. Choosing the best\nrepresentation for one task is often a difficult chore, and there exist\nconflicting opinions on which representation is better suited for a set of\nfamily of tasks. Even worse, when dealing with scenarios where we need to learn\nor optimize functions with orientations as inputs and/or outputs, the set of\npossibilities (representations, loss functions, etc.) is even larger and it is\nnot easy to decide what is best for each scenario. In this paper, we attempt to\na) present clearly, concisely and with unified notation all available\nrepresentations, and \"tricks\" related to 3D orientations (including Lie Group\nalgebra), and b) benchmark them in representative scenarios. The first part\nfeels like it is missing from the robotics literature as one has to read many\ndifferent textbooks and papers in order have a concise and clear understanding\nof all possibilities, while the benchmark is necessary in order to come up with\nrecommendations based on empirical evidence. More precisely, we experiment with\nthe following settings that attempt to cover most widely used scenarios in\nrobotics: 1) direct optimization, 2) imitation/supervised learning with a\nneural network controller, 3) reinforcement learning, and 4) trajectory\noptimization using differential dynamic programming. We finally provide\nguidelines depending on the scenario, and make available a reference\nimplementation of all the orientation math described.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u5e76\u5bf9\u6bd4\u4e863D\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u57fa\u51c6\u548c\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u73b0\u67093D\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u591a\u6837\u4e14\u5404\u6709\u4f18\u52a3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u603b\u7ed3\u548c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u6240\u6709\u53ef\u7528\u7684\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u53ca\u76f8\u5173\u6280\u5de7\uff0c\u5e76\u5728\u56db\u79cd\u5178\u578b\u673a\u5668\u4eba\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u4e3a\u4e0d\u540c\u573a\u666f\u63d0\u4f9b\u4e86\u6700\u4f73\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u7684\u63a8\u8350\u3002", "conclusion": "\u6587\u7ae0\u586b\u8865\u4e86\u673a\u5668\u4eba\u6587\u732e\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u660e\u786e\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.17287", "pdf": "https://arxiv.org/pdf/2509.17287", "abs": "https://arxiv.org/abs/2509.17287", "authors": ["Gokul B. Nair", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation", "categories": ["cs.RO", "cs.CV"], "comment": "8 Pages, 4 Figures, Under Review", "summary": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u6559\u5b66\u4e0e\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u9891\u57df\u4e92\u76f8\u5173\u6846\u67b6\u5b9e\u73b0\u4e86300Hz\u4ee5\u4e0a\u7684\u5904\u7406\u901f\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e27\u76f8\u673a\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u5e27\u76f8\u673a\u7531\u4e8e\u5176\u56fa\u5b9a\u5e27\u7387\uff0830-60Hz\uff09\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u54cd\u5e94\u901f\u5ea6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9891\u57df\u4e92\u76f8\u5173\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u6d41\u5339\u914d\u95ee\u9898\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u5085\u91cc\u53f6\u7a7a\u95f4\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5e76\u7ed3\u5408\u4e8b\u4ef6\u5e27\u7684\u4e8c\u8fdb\u5236\u7279\u6027\u548c\u56fe\u50cf\u538b\u7f29\u6280\u672f\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u57284000\u591a\u7c73\u7684\u5ba4\u5185\u5916\u8f68\u8ff9\u4e0a\u5b9e\u73b0\u4e86\u81ea\u4e3b\u5bfc\u822a\uff0c\u5e73\u5747\u8ddf\u8e2a\u8bef\u5dee\u4f4e\u4e8e24\u5398\u7c73\uff0c\u63a7\u5236\u66f4\u65b0\u9891\u7387\u8fdc\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u672c\u6587\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.17299", "pdf": "https://arxiv.org/pdf/2509.17299", "abs": "https://arxiv.org/abs/2509.17299", "authors": ["Dorian Tsai", "Christopher A. Brunner", "Riki Lamont", "F. Mikaela Nordborg", "Andrea Severati", "Java Terry", "Karen Jackel", "Matthew Dunbabin", "Tobias Fischer", "Scarlett Raine"], "title": "Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 7 figures", "summary": "Coral aquaculture for reef restoration requires accurate and continuous spawn\ncounting for resource distribution and larval health monitoring, but current\nmethods are labor-intensive and represent a critical bottleneck in the coral\nproduction pipeline. We propose the Coral Spawn and Larvae Imaging Camera\nSystem (CSLICS), which uses low cost modular cameras and object detectors\ntrained using human-in-the-loop labeling approaches for automated spawn\ncounting in larval rearing tanks. This paper details the system engineering,\ndataset collection, and computer vision techniques to detect, classify and\ncount coral spawn. Experimental results from mass spawning events demonstrate\nan F1 score of 82.4\\% for surface spawn detection at different embryogenesis\nstages, 65.3\\% F1 score for sub-surface spawn detection, and a saving of 5,720\nhours of labor per spawning event compared to manual sampling methods at the\nsame frequency. Comparison of manual counts with CSLICS monitoring during a\nmass coral spawning event on the Great Barrier Reef demonstrates CSLICS'\naccurate measurement of fertilization success and sub-surface spawn counts.\nThese findings enhance the coral aquaculture process and enable upscaling of\ncoral reef restoration efforts to address climate change threats facing\necosystems like the Great Barrier Reef.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u73ca\u745a\u4ea7\u5375\u8ba1\u6570\u7cfb\u7edfCSLICS\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u6a21\u5757\u5316\u6444\u50cf\u5934\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u73b0\u81ea\u52a8\u5316\u8ba1\u6570\uff0c\u663e\u8457\u8282\u7701\u4eba\u5de5\u65f6\u95f4\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73ca\u745a\u517b\u6b96\u4e2d\u7684\u4eba\u5de5\u4ea7\u5375\u8ba1\u6570\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u73ca\u745a\u751f\u4ea7\u548c\u751f\u6001\u4fee\u590d\u7684\u89c4\u6a21\u3002", "method": "\u5f00\u53d1\u4e86CSLICS\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f4e\u6210\u672c\u6444\u50cf\u5934\u548c\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u8bad\u7ec3\u7684\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5b9e\u73b0\u73ca\u745a\u4ea7\u5375\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u8ba1\u6570\u3002", "result": "\u7cfb\u7edf\u5728\u8868\u9762\u4ea7\u5375\u68c0\u6d4b\u7684F1\u5f97\u5206\u4e3a82.4%\uff0c\u6c34\u4e0b\u4ea7\u5375\u68c0\u6d4b\u4e3a65.3%\uff0c\u5355\u6b21\u4ea7\u5375\u4e8b\u4ef6\u53ef\u8282\u77015720\u5c0f\u65f6\u4eba\u5de5\u3002", "conclusion": "CSLICS\u663e\u8457\u63d0\u5347\u4e86\u73ca\u745a\u517b\u6b96\u6548\u7387\uff0c\u4e3a\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u751f\u6001\u5a01\u80c1\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.17308", "pdf": "https://arxiv.org/pdf/2509.17308", "abs": "https://arxiv.org/abs/2509.17308", "authors": ["Kazutoshi Tanaka", "Tomoya Takahashi", "Masashi Hamaya"], "title": "Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing", "categories": ["cs.RO"], "comment": "9 pages, 7 figures. Accepted at IROS 2025. This is the preprint\n  version", "summary": "Cable-driven serpentine manipulators hold great potential in unstructured\nenvironments, offering obstacle avoidance, multi-directional force application,\nand a lightweight design. By placing all motors and sensors at the base and\nemploying plastic links, we can further reduce the arm's weight. To demonstrate\nthis concept, we developed a 9-degree-of-freedom cable-driven serpentine\nmanipulator with an arm length of 545 mm and a total mass of only 308 g.\nHowever, this design introduces flexibility-induced variations, such as cable\nslack, elongation, and link deformation. These variations result in\ndiscrepancies between analytical predictions and actual link positions, making\npose estimation more challenging. To address this challenge, we propose a\nphysical reservoir computing based pose estimation method that exploits the\nmanipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.\nExperimental results show a mean pose error of 4.3 mm using our method,\ncompared to 4.4 mm with a baseline long short-term memory network and 39.5 mm\nwith an analytical approach. This work provides a new direction for control and\nperception strategies in lightweight cable-driven serpentine manipulators\nleveraging their intrinsic dynamics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u8f7b\u578b\u7535\u7f06\u9a71\u52a8\u86c7\u5f62\u673a\u68b0\u81c2\u7684\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f4d\u59ff\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "motivation": "\u7535\u7f06\u9a71\u52a8\u86c7\u5f62\u673a\u68b0\u81c2\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff0c\u4f46\u5176\u8f7b\u91cf\u5316\u8bbe\u8ba1\u5bfc\u81f4\u7684\u67d4\u6027\u53d8\u5316\uff08\u5982\u7535\u7f06\u677e\u5f1b\u3001\u4f38\u957f\u548c\u8fde\u6746\u53d8\u5f62\uff09\u4f7f\u5f97\u4f4d\u59ff\u4f30\u8ba1\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u50a8\u5907\u6c60\u8ba1\u7b97\u7684\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u68b0\u81c2\u56fa\u6709\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4f5c\u4e3a\u9ad8\u7ef4\u50a8\u5907\u6c60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u4f4d\u59ff\u8bef\u5dee\u4e3a4.3\u6beb\u7c73\uff0c\u4f18\u4e8e\u57fa\u7ebfLSTM\u7f51\u7edc\u76844.4\u6beb\u7c73\u548c\u89e3\u6790\u65b9\u6cd5\u768439.5\u6beb\u7c73\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f7b\u578b\u7535\u7f06\u9a71\u52a8\u86c7\u5f62\u673a\u68b0\u81c2\u7684\u63a7\u5236\u548c\u611f\u77e5\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u5229\u7528\u5176\u56fa\u6709\u7684\u52a8\u529b\u5b66\u7279\u6027\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.17321", "pdf": "https://arxiv.org/pdf/2509.17321", "abs": "https://arxiv.org/abs/2509.17321", "authors": ["Pawe\u0142 Budzianowski", "Emilia Wi\u015bnios", "Gracjan G\u00f3ral", "Igor Kulakov", "Viktor Petrenko", "Krzysztof Walas"], "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.", "AI": {"tldr": "OpenGVL\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u52a1\u8fdb\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eGVL\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6570\u636e\u81ea\u52a8\u6807\u6ce8\u548c\u7b5b\u9009\u3002\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u8f83\u5dee\uff0c\u4ec5\u4e3a\u95ed\u6e90\u6a21\u578b\u768470%\uff0c\u4f46OpenGVL\u80fd\u9ad8\u6548\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u6570\u636e\u7a00\u7f3a\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5448\u6307\u6570\u589e\u957f\uff0c\u9700\u8981\u53ef\u9760\u7684\u4efb\u52a1\u8fdb\u5c55\u9884\u6d4b\u65b9\u6cd5\u6765\u81ea\u52a8\u5316\u6807\u6ce8\u548c\u7b5b\u9009\u6570\u636e\u3002", "method": "\u57fa\u4e8eGVL\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ece\u89c6\u89c9\u89c2\u6d4b\u4e2d\u9884\u6d4b\u4efb\u52a1\u8fdb\u5c55\uff0c\u6784\u5efaOpenGVL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u4efb\u52a1\u8fdb\u5c55\u9884\u6d4b\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4ec5\u4e3a\u95ed\u6e90\u6a21\u578b\u768470%\uff0c\u4f46OpenGVL\u80fd\u6709\u6548\u7528\u4e8e\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u7684\u81ea\u52a8\u7b5b\u9009\u548c\u8d28\u91cf\u8bc4\u4f30\u3002", "conclusion": "OpenGVL\u4e3a\u673a\u5668\u4eba\u6570\u636e\u7684\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u7684\u6027\u80fd\u4ecd\u9700\u63d0\u5347\u4ee5\u63d0\u9ad8\u4efb\u52a1\u8fdb\u5c55\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.17340", "pdf": "https://arxiv.org/pdf/2509.17340", "abs": "https://arxiv.org/abs/2509.17340", "authors": ["Xin Chen", "Rui Huang", "Longbin Tang", "Lin Zhao"], "title": "AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Agile mapless navigation in cluttered 3D environments poses significant\nchallenges for autonomous drones. Conventional mapping-planning-control\npipelines incur high computational cost and propagate estimation errors. We\npresent AERO-MPPI, a fully GPU-accelerated framework that unifies perception\nand planning through an anchor-guided ensemble of Model Predictive Path\nIntegral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR\npoint-cloud representation that rapidly extracts spatially distributed\n\"anchors\" as look-ahead intermediate endpoints, from which we construct\npolynomial trajectory guides to explore distinct homotopy path classes. At each\nplanning step, we run multiple MPPI instances in parallel and evaluate them\nwith a two-stage multi-objective cost that balances collision avoidance and\ngoal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI\nachieves real-time onboard operation and mitigates the local-minima failures of\nsingle-MPPI approaches. Extensive simulations in forests, verticals, and\ninclines demonstrate sustained reliable flight above 7 m/s, with success rates\nabove 80% and smoother trajectories compared to state-of-the-art baselines.\nReal-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX\n16G confirm that AERO-MPPI runs in real time onboard and consistently achieves\nsafe, agile, and robust flight in complex cluttered environments. The code will\nbe open-sourced upon acceptance of the paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u6846\u67b6AERO-MPPI\uff0c\u7528\u4e8e\u65e0\u5730\u56fe\u5bfc\u822a\u76843D\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\uff0c\u7ed3\u5408\u611f\u77e5\u4e0e\u89c4\u5212\u5e76\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u67423D\u73af\u5883\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8bef\u5dee\u4f20\u64ad\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u7684\u9ad8\u6548\u3001\u5b9e\u65f6\u5bfc\u822a\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u8fa8\u7387LiDAR\u70b9\u4e91\u8868\u793a\uff0c\u63d0\u53d6\u2018\u951a\u70b9\u2019\u4f5c\u4e3a\u4e2d\u95f4\u7aef\u70b9\uff0c\u901a\u8fc7\u5e76\u884cMPPI\u4f18\u5316\u5668\u751f\u6210\u591a\u9879\u5f0f\u8f68\u8ff9\u7ebf\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u8d85\u8fc77 m/s\u7684\u7a33\u5b9a\u98de\u884c\uff0c\u6210\u529f\u7387\u9ad8\u4e8e80%\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u3002", "conclusion": "AERO-MPPI\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u5b89\u5168\u548c\u654f\u6377\u7684\u98de\u884c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17350", "pdf": "https://arxiv.org/pdf/2509.17350", "abs": "https://arxiv.org/abs/2509.17350", "authors": ["Haoran Zhou", "Yangwei You", "Shuaijun Wang"], "title": "DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "Dynamic in air handover is a fundamental challenge for dual-arm robots,\nrequiring accurate perception, precise coordination, and natural motion. Prior\nmethods often rely on dynamics models, strong priors, or depth sensing,\nlimiting generalization and naturalness. We present DyDexHandover, a novel\nframework that employs multi-agent reinforcement learning to train an end to\nend RGB based policy for bimanual object throwing and catching. To achieve more\nhuman-like behavior, the throwing policy is guided by a human policy\nregularization scheme, encouraging fluid and natural motion, and enhancing the\ngeneralization capability of the policy. A dual arm simulation environment was\nbuilt in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly\n99 percent success on training objects and 75 percent on unseen objects, while\ngenerating human-like throwing and catching behaviors. To our knowledge, it is\nthe first method to realize dual-arm in-air handover using only raw RGB\nperception.", "AI": {"tldr": "DyDexHandover \u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u7528 RGB \u56fe\u50cf\u8f93\u5165\u5b9e\u73b0\u53cc\u673a\u68b0\u81c2\u7a7a\u4e2d\u7269\u4f53\u4ea4\u63a5\uff0c\u751f\u6210\u4eba\u7c7b\u5316\u52a8\u4f5c\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u89e3\u51b3\u53cc\u673a\u68b0\u81c2\u7a7a\u4e2d\u7269\u4f53\u4ea4\u63a5\u4e2d\u7684\u52a8\u6001\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u52a8\u529b\u5b66\u6a21\u578b\u6216\u6df1\u5ea6\u4f20\u611f\uff0c\u6cdb\u5316\u6027\u548c\u81ea\u7136\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7aef\u5230\u7aef RGB \u7b56\u7565\uff0c\u5f15\u5165\u4eba\u7c7b\u7b56\u7565\u6b63\u5219\u5316\u4ee5\u751f\u6210\u6d41\u7545\u52a8\u4f5c\u3002", "result": "\u5728\u8bad\u7ec3\u5bf9\u8c61\u4e0a\u8fbe\u5230 99% \u6210\u529f\u7387\uff0c\u672a\u89c1\u5bf9\u8c61 75% \u6210\u529f\u7387\uff0c\u9996\u6b21\u4ec5\u7528 RGB \u5b9e\u73b0\u53cc\u673a\u68b0\u81c2\u7a7a\u4e2d\u4ea4\u63a5\u3002", "conclusion": "DyDexHandover \u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u53cc\u673a\u68b0\u81c2\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17381", "pdf": "https://arxiv.org/pdf/2509.17381", "abs": "https://arxiv.org/abs/2509.17381", "authors": ["Yongliang Wang", "Hamidreza Kasaei"], "title": "Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators", "categories": ["cs.RO"], "comment": "Project page available at: https://sites.google.com/view/ftp4rm/home", "summary": "Generating obstacle-free trajectories for robotic manipulators in\nunstructured and cluttered environments remains a significant challenge.\nExisting motion planning methods often require additional computational effort\nto generate the final trajectory by solving kinematic or dynamic equations.\nThis paper highlights the strong potential of model-free reinforcement learning\nmethods over model-based approaches for obstacle-free trajectory planning in\njoint space. We propose a fast trajectory planning system for manipulators that\ncombines vision-based path planning in task space with reinforcement\nlearning-based obstacle avoidance in joint space. We divide the framework into\ntwo key components. The first introduces an innovative vision-based trajectory\nplanner in task space, leveraging the large-scale fast segment anything (FSA)\nmodel in conjunction with basis spline (B-spline)-optimized kinodynamic path\nsearching. The second component enhances the proximal policy optimization (PPO)\nalgorithm by integrating action ensembles (AE) and policy feedback (PF), which\ngreatly improve precision and stability in goal-reaching and obstacle avoidance\nwithin the joint space. These PPO enhancements increase the algorithm's\nadaptability across diverse robotic tasks, ensuring consistent execution of\ncommands from the first component by the manipulator, while also enhancing both\nobstacle avoidance efficiency and reaching accuracy. The experimental results\ndemonstrate the effectiveness of PPO enhancements, as well as\nsimulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)\ntransfer, in improving model robustness and planner efficiency in complex\nscenarios. These enhancements allow the robot to perform obstacle avoidance and\nreal-time trajectory planning in obstructed environments. Project page\navailable at: https://sites.google.com/view/ftp4rm/home", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8def\u5f84\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdbPPO\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u907f\u969c\u548c\u8f68\u8ff9\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u68b0\u81c2\u5728\u975e\u7ed3\u6784\u5316\u548c\u6742\u4e71\u73af\u5883\u4e2d\u751f\u6210\u65e0\u969c\u788d\u8f68\u8ff9\u7684\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u91cf\u95ee\u9898\u3002", "method": "\u5206\u4e3a\u4e24\u90e8\u5206\uff1a1) \u57fa\u4e8e\u89c6\u89c9\u7684\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u7ed3\u5408FSA\u6a21\u578b\u548cB\u6837\u6761\u4f18\u5316\u7684\u52a8\u529b\u5b66\u8def\u5f84\u641c\u7d22\uff1b2) \u6539\u8fdb\u7684PPO\u7b97\u6cd5\uff08\u96c6\u6210AE\u548cPF\uff09\uff0c\u63d0\u5347\u5173\u8282\u7a7a\u95f4\u7684\u907f\u969c\u548c\u76ee\u6807\u5230\u8fbe\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u7684PPO\u7b97\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u548c\u89c4\u5212\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u907f\u969c\u548c\u8f68\u8ff9\u89c4\u5212\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\uff0c\u652f\u6301\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5e94\u7528\u3002"}}
{"id": "2509.17387", "pdf": "https://arxiv.org/pdf/2509.17387", "abs": "https://arxiv.org/abs/2509.17387", "authors": ["Ziqing Zou", "Cong Wang", "Yue Hu", "Xiao Liu", "Bowen Xu", "Rong Xiong", "Changjie Fan", "Yingfeng Chen", "Yue Wang"], "title": "High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics", "categories": ["cs.RO"], "comment": null, "summary": "The complex nonlinear dynamics of hydraulic excavators, such as time delays\nand control coupling, pose significant challenges to achieving high-precision\ntrajectory tracking. Traditional control methods often fall short in such\napplications due to their inability to effectively handle these nonlinearities,\nwhile commonly used learning-based methods require extensive interactions with\nthe environment, leading to inefficiency. To address these issues, we introduce\nEfficientTrack, a trajectory tracking method that integrates model-based\nlearning to manage nonlinear dynamics and leverages closed-loop dynamics to\nimprove learning efficiency, ultimately minimizing tracking errors. We validate\nour method through comprehensive experiments both in simulation and on a\nreal-world excavator. Comparative experiments in simulation demonstrate that\nour method outperforms existing learning-based approaches, achieving the\nhighest tracking precision and smoothness with the fewest interactions.\nReal-world experiments further show that our method remains effective under\nload conditions and possesses the ability for continual learning, highlighting\nits practical applicability. For implementation details and source code, please\nrefer to https://github.com/ZiqingZou/EfficientTrack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEfficientTrack\u7684\u65b0\u578b\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6db2\u538b\u6316\u6398\u673a\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u7684\u65f6\u5ef6\u548c\u63a7\u5236\u8026\u5408\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u548c\u95ed\u73af\u52a8\u529b\u5b66\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5b66\u4e60\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6db2\u538b\u6316\u6398\u673a\u7684\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff08\u5982\u65f6\u5ef6\u548c\u63a7\u5236\u8026\u5408\uff09\u4f7f\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u53d8\u5f97\u56f0\u96be\u3002\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u975e\u7ebf\u6027\u7279\u6027\uff0c\u800c\u73b0\u6709\u7684\u5b66\u4e60\u578b\u65b9\u6cd5\u5219\u56e0\u9700\u8981\u5927\u91cf\u73af\u5883\u4ea4\u4e92\u800c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "EfficientTrack\u7ed3\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u548c\u95ed\u73af\u52a8\u529b\u5b66\uff0c\u4ee5\u9ad8\u6548\u7ba1\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u5e76\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0c\u6700\u7ec8\u51cf\u5c11\u8ddf\u8e2a\u8bef\u5dee\u3002", "result": "\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cEfficientTrack\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5e73\u6ed1\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\uff0c\u4e14\u6240\u9700\u4ea4\u4e92\u6b21\u6570\u6700\u5c11\u3002\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u4ecd\u6709\u6548\uff0c\u5e76\u5177\u5907\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "EfficientTrack\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u6db2\u538b\u6316\u6398\u673a\u7684\u9ad8\u6548\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u517c\u5177\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17389", "pdf": "https://arxiv.org/pdf/2509.17389", "abs": "https://arxiv.org/abs/2509.17389", "authors": ["Lois Liow", "Jonty Milford", "Emre Uygun", "Andre Farinha", "Vinoth Viswanathan", "Josh Pinskier", "David Howard"], "title": "3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "Robotics and automation are key enablers to increase throughput in ongoing\nconservation efforts across various threatened ecosystems. Cataloguing,\ndigitisation, husbandry, and similar activities require the ability to interact\nwith delicate, fragile samples without damaging them. Additionally,\nlearning-based solutions to these tasks require the ability to safely acquire\ndata to train manipulation policies through, e.g., reinforcement learning. To\naddress these twin needs, we introduce a novel method to print free-form,\nhighly sensorised soft 'physical twins'. We present an automated design\nworkflow to create complex and customisable 3D soft sensing structures on\ndemand from 3D scans or models. Compared to the state of the art, our soft\nliquid metal sensors faithfully recreate complex natural geometries and display\nexcellent sensing properties suitable for validating performance in delicate\nmanipulation tasks. We demonstrate the application of our physical twins as\n'sensing corals': high-fidelity, 3D printed replicas of scanned corals that\neliminate the need for live coral experimentation, whilst increasing data\nquality, offering an ethical and scalable pathway for advancing autonomous\ncoral handling and soft manipulation broadly. Through extensive bench-top\nmanipulation and underwater grasping experiments, we show that our sensing\ncoral is able to detect grasps under 0.5 N, effectively capturing the delicate\ninteractions and light contact forces required for coral handling. Finally, we\nshowcase the value of our physical twins across two demonstrations: (i)\nautomated coral labelling for lab identification and (ii) robotic coral\naquaculture. Sensing physical twins such as ours can provide richer grasping\nfeedback than conventional sensors providing experimental validation of prior\nto deployment in handling fragile and delicate items.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u7531\u5f62\u6001\u3001\u9ad8\u5ea6\u4f20\u611f\u5668\u5316\u7684\u8f6f\u8d28\u2018\u7269\u7406\u5b6a\u751f\u2019\u6253\u5370\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u62a4\u8106\u5f31\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u654f\u611f\u6837\u672c\u5904\u7406\u548c\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u89e3\u51b3\u5728\u8106\u5f31\u751f\u6001\u7cfb\u7edf\u4e2d\u5904\u7406\u654f\u611f\u6837\u672c\uff08\u5982\u73ca\u745a\uff09\u65f6\u4e0d\u635f\u4f24\u6837\u672c\u4e14\u80fd\u9ad8\u6548\u83b7\u53d6\u6570\u636e\u7684\u95ee\u9898\uff0c\u652f\u6301\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u8bbe\u8ba1\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc73D\u626b\u63cf\u6216\u6a21\u578b\u5feb\u901f\u751f\u6210\u590d\u6742\u3001\u53ef\u5b9a\u5236\u76843D\u8f6f\u4f20\u611f\u7ed3\u6784\uff0c\u6db2\u4f53\u91d1\u5c5e\u4f20\u611f\u5668\u80fd\u7cbe\u786e\u590d\u523b\u590d\u6742\u81ea\u7136\u51e0\u4f55\u5f62\u6001\u3002", "result": "\u7269\u7406\u5b6a\u751f\u4f53\uff08\u5982\u2018\u4f20\u611f\u73ca\u745a\u2019\uff09\u80fd\u68c0\u6d4b\u5c0f\u4e8e0.5N\u7684\u6293\u53d6\u529b\uff0c\u9002\u7528\u4e8e\u73ca\u745a\u5904\u7406\u7b49\u7cbe\u7ec6\u64cd\u4f5c\uff1b\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u52a8\u6807\u8bb0\u548c\u673a\u5668\u4eba\u517b\u6b96\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8106\u5f31\u7269\u54c1\u7684\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u3001\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u4f26\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8f6f\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.17390", "pdf": "https://arxiv.org/pdf/2509.17390", "abs": "https://arxiv.org/abs/2509.17390", "authors": ["Junzhe Wu", "Yufei Jia", "Yiyi Yan", "Zhixing Chen", "Tiao Tan", "Zifan Wang", "Guangyu Wang"], "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR", "categories": ["cs.RO", "68T40, 68U05", "I.6.8"], "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic\nrendering, its vast ecosystem of assets remains incompatible with\nhigh-performance LiDAR simulation, a critical tool for robotics and autonomous\ndriving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with\na truly plug-and-play approach. Our method converts \\textit{any} pretrained\n3DGS model into a high-fidelity, watertight mesh without requiring\nLiDAR-specific supervision or architectural alterations. This conversion is\nachieved through a general pipeline of volumetric discretization and Truncated\nSigned Distance Field (TSDF) extraction. We pair this with a highly optimized,\nGPU-accelerated ray-casting module that simulates LiDAR returns at over 500\nFPS. We validate our approach on indoor and outdoor scenes, demonstrating\nexceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for\ngeometrically accurate depth sensing, our framework extends their utility\nbeyond visualization and unlocks new capabilities for scalable, multimodal\nsimulation. Our open-source implementation is available at\nhttps://github.com/TATP-233/FGGS-LiDAR.", "AI": {"tldr": "FGGS-LiDAR\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4efb\u4f55\u9884\u8bad\u7ec3\u76843D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u3001\u6c34\u5bc6\u7f51\u683c\uff0c\u7528\u4e8e\u9ad8\u6027\u80fdLiDAR\u6a21\u62df\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u903c\u771f\u6e32\u67d3\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5176\u8d44\u4ea7\u4e0eLiDAR\u6a21\u62df\u4e0d\u517c\u5bb9\uff0c\u800c\u540e\u8005\u5bf9\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4f53\u79ef\u79bb\u6563\u5316\u548c\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a\u63d0\u53d6\u7684\u901a\u7528\u6d41\u7a0b\uff0c\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u8f6c\u6362\u4e3a\u7f51\u683c\uff0c\u5e76\u914d\u5408GPU\u52a0\u901f\u7684\u5149\u7ebf\u6295\u5c04\u6a21\u5757\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\u5176\u51e0\u4f55\u4fdd\u771f\u5ea6\u5353\u8d8a\uff0c\u80fd\u4ee5\u8d85\u8fc7500FPS\u7684\u901f\u5ea6\u6a21\u62dfLiDAR\u8fd4\u56de\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\u8d44\u4ea7\u7684\u7528\u9014\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u529f\u80fd\u3002"}}
{"id": "2509.17435", "pdf": "https://arxiv.org/pdf/2509.17435", "abs": "https://arxiv.org/abs/2509.17435", "authors": ["Xiaoyu Wang", "Yan Rui Tan", "William Leong", "Sunan Huang", "Rodney Teo", "Cheng Xiang"], "title": "GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper proposes an image-based visual servoing (IBVS) framework for UAV\nnavigation and collision avoidance using only an RGB camera. While UAV\nnavigation has been extensively studied, it remains challenging to apply IBVS\nin missions involving multiple visual targets and collision avoidance. The\nproposed method achieves navigation without explicit path planning, and\ncollision avoidance is realized through AI-based monocular depth estimation\nfrom RGB images. Unlike approaches that rely on stereo cameras or external\nworkstations, our framework runs fully onboard a Jetson platform, ensuring a\nself-contained and deployable system. Experimental results validate that the\nUAV can navigate across multiple AprilTags and avoid obstacles effectively in\nGPS-denied environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u4f3a\u670d\uff08IBVS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5bfc\u822a\u548c\u907f\u969c\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u8def\u5f84\u89c4\u5212\u7684\u5bfc\u822a\u548c\u57fa\u4e8eAI\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u907f\u969c\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u4eba\u673a\u5bfc\u822a\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u6d89\u53ca\u591a\u4e2a\u89c6\u89c9\u76ee\u6807\u548c\u907f\u969c\u4efb\u52a1\u4e2d\u5e94\u7528IBVS\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u5728Jetson\u5e73\u53f0\u4e0a\u8fd0\u884c\u7684IBVS\u6846\u67b6\uff0c\u5229\u7528AI\u4eceRGB\u56fe\u50cf\u4f30\u8ba1\u6df1\u5ea6\u4ee5\u5b9e\u73b0\u907f\u969c\uff0c\u65e0\u9700\u4f9d\u8d56\u7acb\u4f53\u76f8\u673a\u6216\u5916\u90e8\u5de5\u4f5c\u7ad9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65e0\u4eba\u673a\u80fd\u5728\u65e0GPS\u73af\u5883\u4e2d\u6709\u6548\u5bfc\u822a\u5e76\u901a\u8fc7\u591a\u4e2aAprilTags\uff0c\u540c\u65f6\u907f\u5f00\u969c\u788d\u7269\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u907f\u969c\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17450", "pdf": "https://arxiv.org/pdf/2509.17450", "abs": "https://arxiv.org/abs/2509.17450", "authors": ["Ying Feng", "Hongjie Fang", "Yinong He", "Jingjing Chen", "Chenxi Wang", "Zihao He", "Ruonan Liu", "Cewu Lu"], "title": "Learning Dexterous Manipulation with Quantized Hand State", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous robotic hands enable robots to perform complex manipulations that\nrequire fine-grained control and adaptability. Achieving such manipulation is\nchallenging because the high degrees of freedom tightly couple hand and arm\nmotions, making learning and control difficult. Successful dexterous\nmanipulation relies not only on precise hand motions, but also on accurate\nspatial positioning of the arm and coordinated arm-hand dynamics. However, most\nexisting visuomotor policies represent arm and hand actions in a single\ncombined space, which often causes high-dimensional hand actions to dominate\nthe coupled action space and compromise arm control. To address this, we\npropose DQ-RISE, which quantizes hand states to simplify hand motion prediction\nwhile preserving essential patterns, and applies a continuous relaxation that\nallows arm actions to diffuse jointly with these compact hand states. This\ndesign enables the policy to learn arm-hand coordination from data while\npreventing hand actions from overwhelming the action space. Experiments show\nthat DQ-RISE achieves more balanced and efficient learning, paving the way\ntoward structured and generalizable dexterous manipulation. Project website:\nhttp://rise-policy.github.io/DQ-RISE/", "AI": {"tldr": "DQ-RISE\u901a\u8fc7\u91cf\u5316\u624b\u90e8\u72b6\u6001\u7b80\u5316\u9884\u6d4b\uff0c\u540c\u65f6\u5f15\u5165\u8fde\u7eed\u677e\u5f1b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u624b\u81c2\u4e0e\u624b\u7684\u534f\u8c03\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5c06\u624b\u81c2\u548c\u624b\u7684\u52a8\u4f5c\u8026\u5408\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u5bfc\u81f4\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u5e76\u5f71\u54cd\u624b\u81c2\u63a7\u5236\uff0c\u5f71\u54cd\u4e86\u7cbe\u7ec6\u64cd\u4f5c\u7684\u534f\u8c03\u6027\u3002", "method": "\u63d0\u51faDQ-RISE\u65b9\u6cd5\uff0c\u91cf\u5316\u624b\u90e8\u72b6\u6001\u4ee5\u7b80\u5316\u8fd0\u52a8\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u8fde\u7eed\u677e\u5f1b\u65b9\u6cd5\u5b9e\u73b0\u624b\u81c2\u4e0e\u624b\u72b6\u6001\u7684\u8054\u5408\u6269\u6563\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDQ-RISE\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u548c\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u4e3a\u7ed3\u6784\u5316\u3001\u901a\u7528\u7684\u7cbe\u7ec6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "DQ-RISE\u901a\u8fc7\u4f18\u5316\u624b\u81c2\u4e0e\u624b\u7684\u534f\u8c03\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cbe\u7ec6\u64cd\u4f5c\u7684\u6548\u7387\u3002"}}
{"id": "2509.17572", "pdf": "https://arxiv.org/pdf/2509.17572", "abs": "https://arxiv.org/abs/2509.17572", "authors": ["Vishnu Deo Mishra", "S Ganga Prasath"], "title": "Morphologies of a sagging elastica with intrinsic sensing and actuation", "categories": ["cs.RO", "physics.app-ph"], "comment": null, "summary": "The morphology of a slender soft-robot can be modified by sensing its shape\nvia sensors and exerting moments via actuators embedded along its body. The\nactuating moments required to morph these soft-robots to a desired shape are\noften difficult to compute due to the geometric non-linearity associated with\nthe structure, the errors in modeling the experimental system, and the\nlimitations in sensing and feedback/actuation capabilities. In this article, we\nexplore the effect of a simple feedback strategy (actuation being proportional\nto the sensed curvature) on the shape of a soft-robot, modeled as an elastica.\nThe finite number of sensors and actuators, often seen in experiments, is\ncaptured in the model via filters of specified widths. Using proportional\nfeedback, we study the simple task of straightening the device by compensating\nfor the sagging introduced by its self-weight. The device undergoes a hierarchy\nof morphological instabilities defined in the phase-space given by the\ngravito-bending number, non-dimensional sensing/feedback gain, and the scaled\nwidth of the filter. For complex shape-morphing tasks, given a perfect model of\nthe device with limited sensing and actuating capabilities, we find that a\ntrade-off arises (set by the sensor spacing & actuator size) between capturing\nthe long and short wavelength features. We show that the error in\nshape-morphing is minimal for a fixed filter width when we choose an\nappropriate actuating gain (whose magnitude goes as a square of the filter\nwidth). Our model provides a quantitative lens to study and design slender soft\ndevices with limited sensing and actuating capabilities for complex maneuvering\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u7b80\u5355\u7684\u6bd4\u4f8b\u53cd\u9988\u7b56\u7565\u8c03\u6574\u67d4\u8f6f\u673a\u5668\u4eba\u7684\u5f62\u72b6\uff0c\u5206\u6790\u4e86\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u7684\u9650\u5236\u5bf9\u5f62\u72b6\u8c03\u6574\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u67d4\u8f6f\u673a\u5668\u4eba\u5728\u5f62\u72b6\u8c03\u6574\u4e2d\u7531\u4e8e\u51e0\u4f55\u975e\u7ebf\u6027\u3001\u5efa\u6a21\u8bef\u5dee\u4ee5\u53ca\u4f20\u611f/\u6267\u884c\u80fd\u529b\u9650\u5236\uff0c\u5f80\u5f80\u96be\u4ee5\u8ba1\u7b97\u6240\u9700\u7684\u6267\u884c\u529b\u77e9\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7b80\u5355\u53cd\u9988\u7b56\u7565\u5bf9\u6b64\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u67d4\u8f6f\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u6027\u4f53\uff0c\u4f7f\u7528\u6bd4\u4f8b\u53cd\u9988\u7b56\u7565\uff08\u6267\u884c\u529b\u77e9\u4e0e\u611f\u77e5\u66f2\u7387\u6210\u6b63\u6bd4\uff09\uff0c\u5e76\u901a\u8fc7\u6ee4\u6ce2\u5668\u6a21\u62df\u6709\u9650\u6570\u91cf\u7684\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u3002\u7814\u7a76\u4e86\u8865\u507f\u81ea\u91cd\u5f15\u8d77\u7684\u4e0b\u5782\u7684\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bbe\u5907\u5728\u76f8\u7a7a\u95f4\u4e2d\u7ecf\u5386\u4e86\u4e00\u7cfb\u5217\u5f62\u6001\u4e0d\u7a33\u5b9a\u6027\u3002\u5728\u590d\u6742\u5f62\u72b6\u8c03\u6574\u4efb\u52a1\u4e2d\uff0c\u4f20\u611f\u548c\u6267\u884c\u80fd\u529b\u7684\u9650\u5236\u5bfc\u81f4\u957f\u3001\u77ed\u6ce2\u957f\u7279\u5f81\u7684\u6355\u83b7\u9700\u8981\u6743\u8861\u9519\u8bef\u6700\u5c0f\u5316\u3002", "conclusion": "\u6a21\u578b\u4e3a\u8bbe\u8ba1\u548c\u7814\u7a76\u5177\u6709\u6709\u9650\u4f20\u611f\u548c\u6267\u884c\u80fd\u529b\u7684\u67d4\u8f6f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9a\u91cf\u5206\u6790\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u3002"}}
{"id": "2509.17582", "pdf": "https://arxiv.org/pdf/2509.17582", "abs": "https://arxiv.org/abs/2509.17582", "authors": ["Vassil Atanassov", "Wanming Yu", "Siddhant Gangapurwala", "James Wilson", "Ioannis Havoutis"], "title": "GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots", "categories": ["cs.RO"], "comment": "You can find an associated video here: https://youtu.be/o8Dd44MkG2E", "summary": "Most modern approaches to quadruped locomotion focus on using Deep\nReinforcement Learning (DRL) to learn policies from scratch, in an end-to-end\nmanner. Such methods often fail to scale, as every new problem or application\nrequires time-consuming and iterative reward definition and tuning. We present\nGeneralist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained\nwith Deep Reinforcement Learning that is capable of tracking arbitrary contact\npoints on a quadruped robot. The strength of our approach is that it provides a\ngeneral and modular low-level controller that can be reused for a wider range\nof high-level tasks, without the need to re-train new controllers from scratch.\nWe demonstrate the scalability and robustness of our method by evaluating on a\nwide range of locomotion and manipulation tasks in a common framework and under\na single generalist policy. These include a variety of gaits, traversing\ncomplex terrains (eg. stairs and slopes) as well as previously unseen\nstepping-stones and narrow beams, and interacting with objects (eg. pushing\nbuttons, tracking trajectories). Our framework acquires new behaviors more\nefficiently, simply by combining a task-specific high-level contact planner and\nthe pre-trained generalist policy. A supplementary video can be found at\nhttps://youtu.be/o8Dd44MkG2E.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeCCo\u7684\u901a\u7528\u63a5\u89e6\u6761\u4ef6\u7b56\u7565\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u8ddf\u8e2a\u56db\u8db3\u673a\u5668\u4eba\u7684\u4efb\u610f\u63a5\u89e6\u70b9\uff0c\u907f\u514d\u4e86\u6bcf\u6b21\u65b0\u4efb\u52a1\u90fd\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u548c\u8c03\u6574\u5956\u52b1\u51fd\u6570\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u901a\u7528\u63a5\u89e6\u6761\u4ef6\u7b56\u7565\uff08GeCCo\uff09\uff0c\u5e76\u590d\u7528\u8be5\u4f4e\u5c42\u7b56\u7565\u5b8c\u6210\u591a\u79cd\u9ad8\u5c42\u4efb\u52a1\u3002", "result": "\u5728\u591a\u79cd\u8fd0\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5982\u884c\u8d70\u3001\u7a7f\u8d8a\u590d\u6742\u5730\u5f62\u548c\u4e0e\u7269\u4f53\u4ea4\u4e92\u3002", "conclusion": "GeCCo\u80fd\u591f\u9ad8\u6548\u5730\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u4ec5\u9700\u7ed3\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u9ad8\u5c42\u63a5\u89e6\u89c4\u5212\u5668\u5373\u53ef\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u3002"}}
{"id": "2509.17666", "pdf": "https://arxiv.org/pdf/2509.17666", "abs": "https://arxiv.org/abs/2509.17666", "authors": ["Mimo Shirasaka", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Yoshitaka Ushiku"], "title": "Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery", "categories": ["cs.RO"], "comment": null, "summary": "Object insertion tasks are prone to failures under pose uncertainties and\nenvironmental variations, traditionally requiring manual finetuning or\ncontroller retraining. We present a novel approach for robust and resilient\nobject insertion using a passively compliant soft wrist that enables safe\ncontact absorption through large deformations, without high-frequency control\nor force sensing. Our method structures insertion as compliance-enabled contact\nformations, sequential contact states that progressively constrain degrees of\nfreedom, and integrates automated failure recovery strategies. Our key insight\nis that wrist compliance permits safe, repeated recovery attempts; hence, we\nrefer to it as compliance-enabled failure recovery. We employ a pre-trained\nvision-language model (VLM) that assesses each skill execution from terminal\nposes and images, identifies failure modes, and proposes recovery actions by\nselecting skills and updating goals. In simulation, our method achieved an 83%\nsuccess rate, recovering from failures induced by randomized\nconditions--including grasp misalignments up to 5 degrees, hole-pose errors up\nto 20mm, fivefold increases in friction, and previously unseen\nsquare/rectangular pegs--and we further validate the approach on a real robot.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u88ab\u52a8\u67d4\u987a\u8f6f\u8155\u7684\u7a33\u5065\u7269\u4f53\u63d2\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u53d8\u5f62\u5438\u6536\u63a5\u89e6\u529b\uff0c\u65e0\u9700\u9ad8\u9891\u63a7\u5236\u6216\u529b\u611f\u77e5\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u5931\u8d25\u6062\u590d\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u4f53\u63d2\u5165\u4efb\u52a1\u5728\u59ff\u6001\u4e0d\u786e\u5b9a\u548c\u73af\u5883\u53d8\u5316\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u9700\u8981\u624b\u52a8\u5fae\u8c03\u6216\u63a7\u5236\u5668\u91cd\u65b0\u8bad\u7ec3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u88ab\u52a8\u67d4\u987a\u8f6f\u8155\u5b9e\u73b0\u5927\u53d8\u5f62\u63a5\u89e6\u5438\u6536\uff0c\u5c06\u63d2\u5165\u4efb\u52a1\u5efa\u6a21\u4e3a\u67d4\u987a\u63a5\u89e6\u72b6\u6001\u5e8f\u5217\uff0c\u5e76\u6574\u5408\u81ea\u52a8\u5931\u8d25\u6062\u590d\u7b56\u7565\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc4\u4f30\u6267\u884c\u6548\u679c\u5e76\u63d0\u51fa\u6062\u590d\u52a8\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u572883%\u7684\u60c5\u51b5\u4e0b\u6210\u529f\uff0c\u80fd\u591f\u4ece\u968f\u673a\u6761\u4ef6\uff08\u59825\u5ea6\u7684\u6293\u53d6\u504f\u5dee\u300120mm\u7684\u5b54\u4f4d\u8bef\u5dee\u30015\u500d\u6469\u64e6\u589e\u52a0\u7b49\uff09\u4e2d\u6062\u590d\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u67d4\u987a\u8bbe\u8ba1\u7ed3\u5408\u81ea\u52a8\u5316\u5931\u8d25\u6062\u590d\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u7269\u4f53\u63d2\u5165\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2509.17683", "pdf": "https://arxiv.org/pdf/2509.17683", "abs": "https://arxiv.org/abs/2509.17683", "authors": ["Jonas Gruetter", "Lorenzo Terenzi", "Pascal Egli", "Marco Hutter"], "title": "Towards Learning Boulder Excavation with Hydraulic Excavators", "categories": ["cs.RO"], "comment": null, "summary": "Construction sites frequently require removing large rocks before excavation\nor grading can proceed. Human operators typically extract these boulders using\nonly standard digging buckets, avoiding time-consuming tool changes to\nspecialized grippers. This task demands manipulating irregular objects with\nunknown geometries in harsh outdoor environments where dust, variable lighting,\nand occlusions hinder perception. The excavator must adapt to varying soil\nresistance--dragging along hard-packed surfaces or penetrating soft\nground--while coordinating multiple hydraulic joints to secure rocks using a\nshovel. Current autonomous excavation focuses on continuous media (soil,\ngravel) or uses specialized grippers with detailed geometric planning for\ndiscrete objects. These approaches either cannot handle large irregular rocks\nor require impractical tool changes that interrupt workflow. We train a\nreinforcement learning policy in simulation using rigid-body dynamics and\nanalytical soil models. The policy processes sparse LiDAR points (just 20 per\nrock) from vision-based segmentation and proprioceptive feedback to control\nstandard excavator buckets. The learned agent discovers different strategies\nbased on soil resistance: dragging along the surface in hard soil and\npenetrating directly in soft conditions. Field tests on a 12-ton excavator\nachieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to\n83% for human operators. This demonstrates that standard construction equipment\ncan learn complex manipulation despite sparse perception and challenging\noutdoor conditions.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u6807\u51c6\u6316\u6398\u673a\u5728\u4e0d\u66f4\u6362\u4e13\u7528\u5939\u5177\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u4e0d\u89c4\u5219\u5927\u5ca9\u77f3\uff0c\u6210\u529f\u7387\u8fbe70%\uff0c\u63a5\u8fd1\u4eba\u7c7b\u64cd\u4f5c\u5458\u768483%\u3002", "motivation": "\u89e3\u51b3\u5efa\u7b51\u5de5\u5730\u5728\u6316\u6398\u6216\u5e73\u6574\u4f5c\u4e1a\u4e2d\u9700\u79fb\u9664\u5927\u5ca9\u77f3\u7684\u95ee\u9898\uff0c\u907f\u514d\u56e0\u66f4\u6362\u5939\u5177\u800c\u4e2d\u65ad\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u7ed3\u5408\u7a00\u758fLiDAR\u6570\u636e\u548c\u672c\u4f53\u53cd\u9988\u63a7\u5236\u6807\u51c6\u6316\u6398\u94f2\u3002", "result": "\u73b0\u573a\u6d4b\u8bd5\u6210\u529f\u7387\u4e3a70%\uff0c\u63a5\u8fd1\u4e8e\u4eba\u7c7b\u64cd\u4f5c\u5458\u768483%\u3002", "conclusion": "\u6807\u51c6\u5efa\u7b51\u8bbe\u5907\u80fd\u5728\u611f\u77e5\u7a00\u758f\u548c\u6076\u52a3\u6237\u5916\u6761\u4ef6\u4e0b\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u3002"}}
{"id": "2509.17750", "pdf": "https://arxiv.org/pdf/2509.17750", "abs": "https://arxiv.org/abs/2509.17750", "authors": ["Inkyu Jang", "Jonghae Park", "Chams E. Mballo", "Sihyun Cho", "Claire J. Tomlin", "H. Jin Kim"], "title": "EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": "Workshop on Safe and Robust Robot Learning for Operation in the Real\n  World (SAFE-ROL) at CoRL 2025", "summary": "We present EigenSafe, an operator-theoretic framework for learning-enabled\nsafety-critical control for stochastic systems. In many robotic systems where\ndynamics are best modeled as stochastic systems due to factors such as sensing\nnoise and environmental disturbances, it is challenging for conventional\nmethods such as Hamilton-Jacobi reachability and control barrier functions to\nprovide a holistic measure of safety. We derive a linear operator governing the\ndynamic programming principle for safety probability, and find that its\ndominant eigenpair provides information about safety for both individual states\nand the overall closed-loop system. The proposed learning framework, called\nEigenSafe, jointly learns this dominant eigenpair and a safe backup policy in\nan offline manner. The learned eigenfunction is then used to construct a safety\nfilter that detects potentially unsafe situations and falls back to the backup\npolicy. The framework is validated in three simulated stochastic\nsafety-critical control tasks.", "AI": {"tldr": "EigenSafe\u662f\u4e00\u4e2a\u57fa\u4e8e\u7b97\u5b50\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u968f\u673a\u7cfb\u7edf\u4e2d\u5b66\u4e60\u542f\u53d1\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\uff0c\u901a\u8fc7\u4e3b\u5bfc\u7279\u5f81\u5bf9\u63d0\u4f9b\u5b89\u5168\u5ea6\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982Hamilton-Jacobi\u53ef\u8fbe\u6027\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff09\u96be\u4ee5\u5168\u9762\u8861\u91cf\u968f\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0cEigenSafe\u901a\u8fc7\u4e3b\u5bfc\u7279\u5f81\u5bf9\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u63a8\u5bfc\u4e86\u4e00\u4e2a\u7ebf\u6027\u7b97\u5b50\uff0c\u5176\u4e3b\u5bfc\u7279\u5f81\u5bf9\u5305\u542b\u5355\u4e2a\u72b6\u6001\u548c\u95ed\u73af\u7cfb\u7edf\u7684\u5b89\u5168\u4fe1\u606f\uff0c\u5e76\u79bb\u7ebf\u5b66\u4e60\u8be5\u7279\u5f81\u5bf9\u53ca\u5907\u4efd\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EigenSafe\u4e3a\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u5bfc\u7279\u5f81\u5bf9\u548c\u5907\u4efd\u7b56\u7565\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u3002"}}
{"id": "2509.17759", "pdf": "https://arxiv.org/pdf/2509.17759", "abs": "https://arxiv.org/abs/2509.17759", "authors": ["Chengbo Yuan", "Rui Zhou", "Mengzhen Liu", "Yingdong Hu", "Shengjie Wang", "Li Yi", "Chuan Wen", "Shanghang Zhang", "Yang Gao"], "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies", "categories": ["cs.RO"], "comment": null, "summary": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86MotionTrans\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u6570\u636e\u5171\u540c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u6570\u636e\u4e2d\u76f4\u63a5\u5b66\u4e60\u65b0\u52a8\u4f5c\u4ee5\u5b8c\u6210\u4efb\u52a1\u7684\u76ee\u6807\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u673a\u5668\u4eba\u6570\u636e\u7684\u7a00\u7f3a\u6027\u662f\u74f6\u9888\uff0c\u800c\u4eba\u7c7b\u6570\u636e\u5177\u6709\u4e30\u5bcc\u7684\u591a\u6837\u6027\uff0c\u53ef\u4ee5\u7528\u4e8e\u52a8\u4f5c\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u6570\u636e\u662f\u5426\u80fd\u4f7f\u673a\u5668\u4eba\u7b56\u7565\u76f4\u63a5\u5b66\u4e60\u65b0\u52a8\u4f5c\u5b8c\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86MotionTrans\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u3001\u4eba\u7c7b\u6570\u636e\u8f6c\u6362\u7ba1\u9053\u548c\u52a0\u6743\u5171\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u572830\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5171\u540c\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u5c0613\u4e2a\u4efb\u52a1\u7684\u8fd0\u52a8\u76f4\u63a5\u4ece\u4eba\u7c7b\u6570\u636e\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u7b56\u7565\u4e2d\uff0c\u5176\u4e2d9\u4e2a\u4efb\u52a1\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u975e\u5e73\u51e1\u6210\u529f\u7387\uff0c\u9884\u8bad\u7ec3-\u5fae\u8c03\u6027\u80fd\u63d0\u534740%\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u6210\u529f\u7684\u52a8\u4f5c\u5b66\u4e60\u5173\u952e\u5728\u4e8e\u4e0e\u673a\u5668\u4eba\u6570\u636e\u5171\u540c\u8bad\u7ec3\u548c\u5e7f\u6cdb\u7684\u4efb\u52a1\u76f8\u5173\u52a8\u4f5c\u8986\u76d6\uff0c\u8fd9\u4e3a\u5229\u7528\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.17760", "pdf": "https://arxiv.org/pdf/2509.17760", "abs": "https://arxiv.org/abs/2509.17760", "authors": ["Austin Wilson", "Sahar Kapasi", "Zane Greene", "Alexis E. Block"], "title": "Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research", "categories": ["cs.RO", "cs.HC", "eess.AS"], "comment": null, "summary": "Many research groups face challenges when legacy (unsupported) robotic\nplatforms lose manufacturer support and cannot accommodate modern sensing,\nspeech, and interaction capabilities. We present the Enhanced NAO, a\nrevitalized version of Aldebaran's NAO robot that uses upgraded microphones,\nRGB-D and thermal cameras, and additional compute resources in a fully\nself-contained package. This system combines cloud and local models for\nperception and dialogue, while preserving the NAO's expressive body and\nbehaviors. In a pilot validation study, the Enhanced NAO delivered\nsignificantly higher conversational quality and stronger user preference\ncompared to the NAO AI Edition, without increasing response latency. Key\nupgrades, such as beamforming microphones and low-latency audio processing,\nreduced artifacts like self-hearing and improved multi-party separation.\nExpanded visual and thermal sensing established a foundation for future\ninteraction capabilities. Beyond the NAO, our framework provides a\nplatform-agnostic strategy for extending the lifespan and research utility of\nlegacy robots, ensuring they remain valuable tools for human-robot interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnhanced NAO\u7684\u5347\u7ea7\u7248\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6539\u8fdb\u786c\u4ef6\u548c\u8f6f\u4ef6\u5ef6\u957f\u4e86\u65e7\u6b3eNAO\u673a\u5668\u4eba\u7684\u4f7f\u7528\u5bff\u547d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u56e0\u5236\u9020\u5546\u505c\u6b62\u652f\u6301\u800c\u65e0\u6cd5\u9002\u914d\u73b0\u4ee3\u4f20\u611f\u548c\u4ea4\u4e92\u529f\u80fd\u7684\u65e7\u6b3e\u673a\u5668\u4eba\u5e73\u53f0\u95ee\u9898\u3002", "method": "\u5347\u7ea7\u4e86\u9ea6\u514b\u98ce\u3001RGB-D\u548c\u70ed\u6210\u50cf\u6444\u50cf\u5934\uff0c\u5e76\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u7ed3\u5408\u4e91\u7aef\u548c\u672c\u5730\u6a21\u578b\u8fdb\u884c\u611f\u77e5\u4e0e\u5bf9\u8bdd\u3002", "result": "Enhanced NAO\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bf9\u8bdd\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\uff0c\u4e14\u672a\u589e\u52a0\u54cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5ef6\u957f\u65e7\u6b3e\u673a\u5668\u4eba\u7684\u5bff\u547d\u548c\u7814\u7a76\u4ef7\u503c\u63d0\u4f9b\u4e86\u5e73\u53f0\u65e0\u5173\u7684\u7b56\u7565\u3002"}}
{"id": "2509.17783", "pdf": "https://arxiv.org/pdf/2509.17783", "abs": "https://arxiv.org/abs/2509.17783", "authors": ["Yibo Peng", "Jiahao Yang", "Shenhao Yan", "Ziyu Huang", "Shuang Li", "Shuguang Cui", "Yiming Zhao", "Yatong Han"], "title": "RoboSeek: You Need to Interact with Your Objects", "categories": ["cs.RO"], "comment": null, "summary": "Optimizing and refining action execution through\n  exploration and interaction is a promising way for robotic\n  manipulation. However, practical approaches to interaction driven robotic\nlearning are still underexplored, particularly for\n  long-horizon tasks where sequential decision-making, physical\n  constraints, and perceptual uncertainties pose significant chal lenges.\nMotivated by embodied cognition theory, we propose\n  RoboSeek, a framework for embodied action execution that\n  leverages interactive experience to accomplish manipulation\n  tasks. RoboSeek optimizes prior knowledge from high-level\n  perception models through closed-loop training in simulation\n  and achieves robust real-world execution via a real2sim2real\n  transfer pipeline. Specifically, we first replicate real-world\n  environments in simulation using 3D reconstruction to provide\n  visually and physically consistent environments., then we train\n  policies in simulation using reinforcement learning and the\n  cross-entropy method leveraging visual priors. The learned\n  policies are subsequently deployed on real robotic platforms\n  for execution. RoboSeek is hardware-agnostic and is evaluated\n  on multiple robotic platforms across eight long-horizon ma nipulation tasks\ninvolving sequential interactions, tool use, and\n  object handling. Our approach achieves an average success rate\n  of 79%, significantly outperforming baselines whose success\n  rates remain below 50%, highlighting its generalization and\n  robustness across tasks and platforms. Experimental results\n  validate the effectiveness of our training framework in complex,\n  dynamic real-world settings and demonstrate the stability of the\n  proposed real2sim2real transfer mechanism, paving the way for\n  more generalizable embodied robotic learning. Project Page:\n  https://russderrick.github.io/Roboseek/", "AI": {"tldr": "RoboSeek\u662f\u4e00\u4e2a\u901a\u8fc7\u4e92\u52a8\u7ecf\u9a8c\u4f18\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u5229\u7528\u4eff\u771f\u8bad\u7ec3\u548c\u771f\u5b9e2\u4eff\u771f2\u771f\u5b9e\uff08real2sim2real\uff09\u8fc1\u79fb\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u6267\u884c\uff0c\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53d7\u5177\u8eab\u8ba4\u77e5\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e8f\u5217\u51b3\u7b56\u3001\u7269\u7406\u7ea6\u675f\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc73D\u91cd\u5efa\u4eff\u771f\u771f\u5b9e\u73af\u5883\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4ea4\u53c9\u71b5\u65b9\u6cd5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u901a\u8fc7real2sim2real\u8fc1\u79fb\u5230\u73b0\u5b9e\u5e73\u53f0\u3002", "result": "\u5728\u516b\u4e2a\u957f\u65f6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738779%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u6210\u529f\u7387<50%\uff09\u3002", "conclusion": "RoboSeek\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e86\u66f4\u5177\u901a\u7528\u6027\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.17812", "pdf": "https://arxiv.org/pdf/2509.17812", "abs": "https://arxiv.org/abs/2509.17812", "authors": ["Yitaek Kim", "Casper Hewson Rask", "Christoffer Sloth"], "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation", "categories": ["cs.RO"], "comment": "This paper has submitted to Dexterous Humanoid Manipulation Workshop,\n  Humanoid 2025", "summary": "This paper proposes Tac2Motion, a contact-aware reinforcement learning\nframework to facilitate the learning of contact-rich in-hand manipulation\ntasks, such as removing a lid. To this end, we propose tactile sensing-based\nreward shaping and incorporate the sensing into the observation space through\nembedding. The designed rewards encourage an agent to ensure firm grasping and\nsmooth finger gaiting at the same time, leading to higher data efficiency and\nrobust performance compared to the baseline. We verify the proposed framework\non the opening a lid scenario, showing generalization of the trained policy\ninto a couple of object types and various dynamics such as torsional friction.\nLastly, the learned policy is demonstrated on the multi-fingered robot, Shadow\nRobot, showing that the control policy can be transferred to the real world.\nThe video is available: https://youtu.be/poeJBPR7urQ.", "AI": {"tldr": "Tac2Motion\u662f\u4e00\u4e2a\u57fa\u4e8e\u89e6\u89c9\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u5b66\u4e60\u63a5\u89e6\u4e30\u5bcc\u7684\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\uff0c\u5982\u62e7\u5f00\u76d6\u5b50\u3002\u901a\u8fc7\u89e6\u89c9\u5956\u52b1\u5851\u9020\u548c\u5d4c\u5165\u89c2\u5bdf\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u63a5\u89e6\u4e30\u5bcc\u7684\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u62e7\u5f00\u76d6\u5b50\uff09\u7684\u5b66\u4e60\u96be\u9898\uff0c\u5229\u7528\u89e6\u89c9\u611f\u77e5\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u89e6\u89c9\u611f\u77e5\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u5e76\u5c06\u89e6\u89c9\u4fe1\u606f\u5d4c\u5165\u89c2\u5bdf\u7a7a\u95f4\uff0c\u540c\u65f6\u8bbe\u8ba1\u5956\u52b1\u4ee5\u9f13\u52b1\u7a33\u56fa\u6293\u63e1\u548c\u6d41\u7545\u624b\u6307\u8fd0\u52a8\u3002", "result": "\u5728\u62e7\u5f00\u76d6\u5b50\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5bf9\u591a\u79cd\u7269\u4f53\u7c7b\u578b\u548c\u52a8\u6001\uff08\u5982\u626d\u8f6c\u6469\u64e6\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u7b56\u7565\u53ef\u8fc1\u79fb\u81f3\u771f\u5b9e\u673a\u5668\u4eba\uff08Shadow Robot\uff09\u3002", "conclusion": "Tac2Motion\u6846\u67b6\u901a\u8fc7\u89e6\u89c9\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\u6f5c\u529b\u3002"}}
{"id": "2509.17850", "pdf": "https://arxiv.org/pdf/2509.17850", "abs": "https://arxiv.org/abs/2509.17850", "authors": ["Xiao Zhou", "Zengqi Peng", "Jun Ma"], "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model", "categories": ["cs.RO"], "comment": null, "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time.", "AI": {"tldr": "SocialTraj\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\u539f\u7406\uff08\u793e\u4f1a\u4ef7\u503c\u53d6\u5411\uff0cSVO\uff09\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u5468\u56f4\u8f66\u8f86\u591a\u6a21\u6001\u884c\u4e3a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u6355\u6349\u9a7e\u9a76\u5458\u7684\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u5bfc\u81f4\u9884\u6d4b\u8f68\u8ff9\u4e0e\u5b9e\u9645\u672a\u6765\u8fd0\u52a8\u504f\u79bb\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u4f30\u8ba1\u5468\u56f4\u8f66\u8f86\u7684SVO\uff0c\u5e76\u5c06SVO\u5d4c\u5165\u5230\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u660e\u786e\u7ed3\u5408\u81ea\u8f66\u7684\u672a\u6765\u8f68\u8ff9\u4ee5\u589e\u5f3a\u4ea4\u4e92\u5efa\u6a21\u3002", "result": "\u5728NGSIM\u548cHighD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSocialTraj\u80fd\u591f\u9002\u5e94\u9ad8\u52a8\u6001\u548c\u4ea4\u4e92\u573a\u666f\uff0c\u751f\u6210\u793e\u4f1a\u5408\u89c4\u4e14\u884c\u4e3a\u4e00\u81f4\u7684\u9884\u6d4b\u8f68\u8ff9\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001SVO\u4f30\u8ba1\u548c\u660e\u786e\u7684\u81ea\u8f66\u89c4\u5212\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002"}}
{"id": "2509.17877", "pdf": "https://arxiv.org/pdf/2509.17877", "abs": "https://arxiv.org/abs/2509.17877", "authors": ["Richard Kuhlmann", "Jakob Wolfram", "Boyang Sun", "Jiaxu Xing", "Davide Scaramuzza", "Marc Pollefeys", "Cesar Cadena"], "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous inspection is a central problem in robotics, with applications\nranging from industrial monitoring to search-and-rescue. Traditionally,\ninspection has often been reduced to navigation tasks, where the objective is\nto reach a predefined location while avoiding obstacles. However, this\nformulation captures only part of the real inspection problem. In real-world\nenvironments, the inspection targets may become visible well before their exact\ncoordinates are reached, making further movement both redundant and\ninefficient. What matters more for inspection is not simply arriving at the\ntarget's position, but positioning the robot at a viewpoint from which the\ntarget becomes observable. In this work, we revisit inspection from a\nperception-aware perspective. We propose an end-to-end reinforcement learning\nframework that explicitly incorporates target visibility as the primary\nobjective, enabling the robot to find the shortest trajectory that guarantees\nvisual contact with the target without relying on a map. The learned policy\nleverages both perceptual and proprioceptive sensing and is trained entirely in\nsimulation, before being deployed to a real-world robot. We further develop an\nalgorithm to compute ground-truth shortest inspection paths, which provides a\nreference for evaluation. Through extensive experiments, we show that our\nmethod outperforms existing classical and learning-based navigation approaches,\nyielding more efficient inspection trajectories in both simulated and\nreal-world settings. The project is avialable at\nhttps://sight-over-site.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u7684\u81ea\u4e3b\u68c0\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u786e\u4fdd\u76ee\u6807\u53ef\u89c6\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u67e5\u4efb\u52a1\u4ec5\u5173\u6ce8\u5bfc\u822a\u5230\u76ee\u6807\u4f4d\u7f6e\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u53ef\u89c6\u6027\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u660e\u786e\u5c06\u76ee\u6807\u53ef\u89c6\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u611f\u77e5\u548c\u672c\u4f53\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u9ad8\u6548\u7684\u68c0\u67e5\u8f68\u8ff9\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u611f\u77e5\u5bfc\u5411\u7684\u68c0\u67e5\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u5b8c\u6210\u4efb\u52a1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.17884", "pdf": "https://arxiv.org/pdf/2509.17884", "abs": "https://arxiv.org/abs/2509.17884", "authors": ["Arun L. Bishop", "Juan Alvarez-Padilla", "Sam Schoedel", "Ibrahima Sory Sow", "Juee Chandrachud", "Sheitej Sharma", "Will Kraus", "Beomyeong Park", "Robert J. Griffin", "John M. Dolan", "Zachary Manchester"], "title": "The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control", "categories": ["cs.RO"], "comment": "Accepted to IEEE Humanoids 2025. For videos and code visit\n  https://linearwalking.github.io/", "summary": "When do locomotion controllers require reasoning about nonlinearities? In\nthis work, we show that a whole-body model-predictive controller using a simple\nlinear time-invariant approximation of the whole-body dynamics is able to\nexecute basic locomotion tasks on complex legged robots. The formulation\nrequires no online nonlinear dynamics evaluations or matrix inversions. We\ndemonstrate walking, disturbance rejection, and even navigation to a goal\nposition without a separate footstep planner on a quadrupedal robot. In\naddition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with\nsignificant limb inertia, complex actuator dynamics, and large sim-to-real gap.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u52a8\u529b\u5b66\u7684\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u53ef\u4ee5\u5728\u590d\u6742\u817f\u90e8\u673a\u5668\u4eba\u4e0a\u5b8c\u6210\u57fa\u7840\u8fd0\u52a8\u4efb\u52a1\uff0c\u65e0\u9700\u5728\u7ebf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8ba1\u7b97\u6216\u77e9\u9635\u6c42\u9006\u3002", "motivation": "\u63a2\u7d22\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u8fd0\u52a8\u63a7\u5236\u5668\u9700\u8981\u8003\u8651\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5e76\u9a8c\u8bc1\u7ebf\u6027\u8fd1\u4f3c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u91c7\u7528\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u52a8\u529b\u5b66\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u975e\u7ebf\u6027\u8ba1\u7b97\u3002", "result": "\u8be5\u63a7\u5236\u5668\u6210\u529f\u5728\u56db\u8db3\u673a\u5668\u4eba\u548c\u6db2\u538b\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u884c\u8d70\u3001\u5e72\u6270\u62b5\u6297\u548c\u76ee\u6807\u5bfc\u822a\u7b49\u529f\u80fd\u3002", "conclusion": "\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u52a8\u529b\u5b66\u8db3\u4ee5\u5b8c\u6210\u590d\u6742\u673a\u5668\u4eba\u4e0a\u7684\u57fa\u7840\u8fd0\u52a8\u4efb\u52a1\uff0c\u7b80\u5316\u4e86\u63a7\u5236\u5668\u7684\u5b9e\u73b0\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2509.17940", "pdf": "https://arxiv.org/pdf/2509.17940", "abs": "https://arxiv.org/abs/2509.17940", "authors": ["Shuyao Shang", "Yuntao Chen", "Yuqi Wang", "Yingyan Li", "Zhaoxiang Zhang"], "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "NeurIPS 2025", "summary": "End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.", "AI": {"tldr": "DriveDPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u8bc4\u5206\uff0c\u6539\u8fdb\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u533a\u5206\u770b\u4f3c\u4eba\u7c7b\u884c\u4e3a\u4f46\u4e0d\u5b89\u5168\u7684\u8f68\u8ff9\u3002", "method": "DriveDPO\u7ed3\u5408\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u89c4\u5219\u5b89\u5168\u8bc4\u5206\u4f5c\u4e3a\u7edf\u4e00\u7b56\u7565\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u8f68\u8ff9\u7ea7\u504f\u597d\u5bf9\u9f50\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDriveDPO\u8fbe\u5230\u4e8690.0\u7684PDMS\u6700\u9ad8\u5206\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u3002", "conclusion": "DriveDPO\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\uff0c\u80fd\u591f\u751f\u6210\u66f4\u53ef\u9760\u7684\u884c\u4e3a\u3002"}}
{"id": "2509.17941", "pdf": "https://arxiv.org/pdf/2509.17941", "abs": "https://arxiv.org/abs/2509.17941", "authors": ["Zichao Hu", "Chen Tang", "Michael J. Munje", "Yifeng Zhu", "Alex Liu", "Shuijing Liu", "Garrett Warnell", "Peter Stone", "Joydeep Biswas"], "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://amrl.cs.utexas.edu/ComposableNav/", "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ComposableNav\u65b9\u6cd5\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6839\u636e\u6307\u4ee4\u5bfc\u822a\uff0c\u901a\u8fc7\u5206\u89e3\u6307\u4ee4\u4e3a\u72ec\u7acb\u52a8\u4f5c\u5e76\u7ec4\u5408\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u6307\u4ee4\u5bfc\u822a\u4e2d\u7ec4\u5408\u6027\u89c4\u683c\u7684\u6311\u6218\uff0c\u907f\u514d\u56e0\u52a8\u4f5c\u7ec4\u5408\u6307\u6570\u589e\u957f\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u5206\u9636\u6bb5\u8bad\u7ec3\uff1a\u9884\u8bad\u7ec3\u52a8\u6001\u5bfc\u822a\u7684\u57fa\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e3a\u4e0d\u540c\u52a8\u4f5c\u539f\u8bed\u3002", "result": "ComposableNav\u80fd\u751f\u6210\u6ee1\u8db3\u672a\u89c1\u89c4\u683c\u7ec4\u5408\u7684\u8f68\u8ff9\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u975e\u7ec4\u5408\u65b9\u6cd5\u3002", "conclusion": "\u7ec4\u5408\u5f0f\u65b9\u6cd5\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u6307\u4ee4\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2509.17952", "pdf": "https://arxiv.org/pdf/2509.17952", "abs": "https://arxiv.org/abs/2509.17952", "authors": ["Mahdi Nobar", "J\u00fcrg Keller", "Alessandro Forino", "John Lygeros", "Alisa Rupenyan"], "title": "Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This preprint is intended for submission to IEEE Robotics and\n  Automation Letters (RA-L)", "summary": "We propose a \\textit{guided multi-fidelity Bayesian optimization} framework\nfor data-efficient controller tuning that integrates corrected digital twin\n(DT) simulations with real-world measurements. The method targets closed-loop\nsystems with limited-fidelity simulations or inexpensive approximations. To\naddress model mismatch, we build a multi-fidelity surrogate with a learned\ncorrection model that refines DT estimates from real data. An adaptive\ncost-aware acquisition function balances expected improvement, fidelity, and\nsampling cost. Our method ensures adaptability as new measurements arrive. The\naccuracy of DTs is re-estimated, dynamically adapting both cross-source\ncorrelations and the acquisition function. This ensures that accurate DTs are\nused more frequently, while inaccurate DTs are appropriately downweighted.\nExperiments on robotic drive hardware and supporting numerical studies\ndemonstrate that our method enhances tuning efficiency compared to standard\nBayesian optimization (BO) and multi-fidelity methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f15\u5bfc\u591a\u4fdd\u771f\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\uff0c\u7528\u4e8e\u9ad8\u6548\u63a7\u5236\u5668\u8c03\u53c2\u3002", "motivation": "\u9488\u5bf9\u95ed\u73af\u7cfb\u7edf\u4e2d\u4eff\u771f\u4fdd\u771f\u5ea6\u4e0d\u8db3\u6216\u4f4e\u6210\u672c\u8fd1\u4f3c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6821\u6b63\u6a21\u578b\u51cf\u5c11\u6a21\u578b\u4e0d\u5339\u914d\u3002", "method": "\u6784\u5efa\u591a\u4fdd\u771f\u66ff\u4ee3\u6a21\u578b\uff0c\u7ed3\u5408\u6821\u6b63\u6a21\u578b\u4f18\u5316\u6570\u5b57\u5b6a\u751f\u4f30\u8ba1\u3002\u81ea\u9002\u5e94\u6210\u672c\u611f\u77e5\u91c7\u96c6\u51fd\u6570\u5e73\u8861\u6539\u8fdb\u3001\u4fdd\u771f\u5ea6\u548c\u91c7\u6837\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u9a71\u52a8\u786c\u4ef6\u548c\u6570\u503c\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u591a\u4fdd\u771f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u6570\u5b57\u5b6a\u751f\u51c6\u786e\u6027\uff0c\u4f18\u5316\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u63d0\u5347\u63a7\u5236\u5668\u8c03\u53c2\u6548\u679c\u3002"}}
{"id": "2509.18005", "pdf": "https://arxiv.org/pdf/2509.18005", "abs": "https://arxiv.org/abs/2509.18005", "authors": ["Yanxin Zhang", "Liang He", "Zeyi Kang", "Zuheng Ming", "Kaixing Zhao"], "title": "M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer", "categories": ["cs.RO"], "comment": "8 pages", "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578bM3ET\uff0c\u901a\u8fc7\u7ed3\u5408Mamba\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u548c\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u673a\u5668\u4eba\u73af\u5883\u4e2d\u96be\u4ee5\u5145\u5206\u5229\u7528\u6587\u672c\u6a21\u6001\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578bM3ET\u3002", "method": "M3ET\u7ed3\u5408\u4e86Mamba\u6a21\u5757\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u3001\u5bf9\u9f50\u548c\u6a21\u6001\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cM3ET\u5728\u9884\u8bad\u7ec3\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u5347\u4e862.3\u500d\uff0cVQA\u4efb\u52a1\u51c6\u786e\u7387\u4e3a0.74\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\u4e860.67\u500d\u3002", "conclusion": "M3ET\u7684\u8f7b\u91cf\u8bbe\u8ba1\u4f7f\u5176\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5c3d\u7ba1\u5728EQA\u4efb\u52a1\u4e0a\u6027\u80fd\u6709\u9650\u3002"}}
{"id": "2509.18043", "pdf": "https://arxiv.org/pdf/2509.18043", "abs": "https://arxiv.org/abs/2509.18043", "authors": ["Yinlong Dai", "Andre Keyser", "Dylan P. Losey"], "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Imitation learning (IL) has proven effective across a wide range of\nmanipulation tasks. However, IL policies often struggle when faced with\nout-of-distribution observations; for instance, when the target object is in a\npreviously unseen position or occluded by other objects. In these cases,\nextensive demonstrations are needed for current IL methods to reach robust and\ngeneralizable behaviors. But when humans are faced with these sorts of atypical\ninitial states, we often rearrange the environment for more favorable task\nexecution. For example, a person might rotate a coffee cup so that it is easier\nto grasp the handle, or push a box out of the way so they can directly grasp\ntheir target object. In this work we seek to equip robot learners with the same\ncapability: enabling robots to prepare the environment before executing their\ngiven policy. We propose ReSET, an algorithm that takes initial states -- which\nare outside the policy's distribution -- and autonomously modifies object poses\nso that the restructured scene is similar to training data. Theoretically, we\nshow that this two step process (rearranging the environment before rolling out\nthe given policy) reduces the generalization gap. Practically, our ReSET\nalgorithm combines action-agnostic human videos with task-agnostic\nteleoperation data to i) decide when to modify the scene, ii) predict what\nsimplifying actions a human would take, and iii) map those predictions into\nrobot action primitives. Comparisons with diffusion policies, VLAs, and other\nbaselines show that using ReSET to prepare the environment enables more robust\ntask execution with equal amounts of total training data. See videos at our\nproject website: https://reset2025paper.github.io/", "AI": {"tldr": "ReSET\u7b97\u6cd5\u901a\u8fc7\u5141\u8bb8\u673a\u5668\u4eba\u5728\u6267\u884c\u7b56\u7565\u524d\u8c03\u6574\u73af\u5883\uff0c\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u9762\u5bf9\u975e\u5178\u578b\u521d\u59cb\u72b6\u6001\u65f6\u7684\u6cdb\u5316\u5dee\u8ddd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u9762\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5916\u7684\u89c2\u5bdf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4eba\u7c7b\u901a\u5e38\u4f1a\u8c03\u6574\u73af\u5883\u4ee5\u4f18\u5316\u4efb\u52a1\u6267\u884c\u3002\u7814\u7a76\u76ee\u6807\u662f\u8d4b\u4e88\u673a\u5668\u4eba\u540c\u6837\u7684\u73af\u5883\u8c03\u6574\u80fd\u529b\u3002", "method": "\u63d0\u51faReSET\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u6d41\u7a0b\uff1a\u5148\u8c03\u6574\u5bf9\u8c61\u4f4d\u59ff\u4f7f\u573a\u666f\u66f4\u63a5\u8fd1\u8bad\u7ec3\u6570\u636e\uff0c\u518d\u6267\u884c\u7b56\u7565\u3002\u7ed3\u5408\u4eba\u7c7b\u89c6\u9891\u548c\u9065\u64cd\u4f5c\u6570\u636e\u8fdb\u884c\u573a\u666f\u8c03\u6574\u51b3\u7b56\u548c\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReSET\u5728\u540c\u7b49\u8bad\u7ec3\u6570\u636e\u91cf\u4e0b\uff0c\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6269\u6563\u7b56\u7565\u548cVLA\uff09\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "ReSET\u901a\u8fc7\u9884\u5148\u8c03\u6574\u73af\u5883\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6267\u884c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18053", "pdf": "https://arxiv.org/pdf/2509.18053", "abs": "https://arxiv.org/abs/2509.18053", "authors": ["Hsu-kuang Chiu", "Ryo Hachiuma", "Chien-Yi Wang", "Yu-Chiang Frank Wang", "Min-Hung Chen", "Stephen F. Smith"], "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts", "categories": ["cs.RO"], "comment": null, "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u601d\u7ef4\uff08Graph-of-Thoughts\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\uff0c\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u88ab\u906e\u6321\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u9762\u5bf9\u4f20\u611f\u5668\u88ab\u5927\u578b\u7269\u4f53\u906e\u6321\u65f6\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u800c\u4f20\u7edf\u7684\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u56fe\u601d\u7ef4\u63a8\u7406\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u7ed3\u5408\u906e\u6321\u611f\u77e5\u7684\u611f\u77e5\u80fd\u529b\u548c\u89c4\u5212\u611f\u77e5\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u4f7f\u7528V2V-GoT-QA\u6570\u636e\u96c6\u548cV2V-GoT\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u534f\u540c\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u56fe\u601d\u7ef4\u63a8\u7406\uff0c\u8be5\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.18084", "pdf": "https://arxiv.org/pdf/2509.18084", "abs": "https://arxiv.org/abs/2509.18084", "authors": ["Jiawen Tian", "Liqun Huang", "Zhongren Cui", "Jingchao Qiao", "Jiafeng Xu", "Xiao Ma", "Zeyu Ren"], "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces", "categories": ["cs.RO"], "comment": "Tech Report.13 pages, 9 figures. Project page:\n  https://bytewrist.github.io/", "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.", "AI": {"tldr": "ByteWrist\u662f\u4e00\u79cd\u65b0\u578b\u7684\u9ad8\u5ea6\u7075\u6d3b\u4e14\u62df\u4eba\u5316\u7684\u5e76\u8054\u624b\u8155\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u4e09\u9636\u6bb5\u5e76\u8054\u9a71\u52a8\u673a\u5236\u548c\u5f27\u5f62\u672b\u7aef\u8fde\u6746\u89e3\u51b3\u4e86\u4f20\u7edf\u624b\u8155\u5728\u72ed\u7a84\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u4e32\u884c\u548c\u5e76\u8054\u624b\u8155\u5728\u72ed\u7a84\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u5b58\u5728\u660e\u663e\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7d27\u51d1\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u8bbe\u8ba1\u4ee5\u5e94\u5bf9\u590d\u6742\u4efb\u52a1\uff0c\u5982\u5bb6\u5ead\u670d\u52a1\u3001\u533b\u7597\u8f85\u52a9\u548c\u7cbe\u5bc6\u88c5\u914d\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5d4c\u5957\u7535\u673a\u9a71\u52a8\u8fde\u6746\u3001\u5f27\u5f62\u672b\u7aef\u8fde\u6746\u548c\u4e2d\u592e\u652f\u6491\u7403\u7684\u7403\u5f62\u5173\u8282\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u5b66\u5efa\u6a21\u548c\u6570\u503c\u96c5\u53ef\u6bd4\u77e9\u9635\u89e3\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "ByteWrist\u5728\u72ed\u7a84\u7a7a\u95f4\u64cd\u4f5c\u548c\u53cc\u81c2\u534f\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7d27\u51d1\u6027\u3001\u6548\u7387\u548c\u521a\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edfKinova\u7cfb\u7edf\u3002", "conclusion": "ByteWrist\u662f\u4e0b\u4e00\u4ee3\u5728\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u8bbe\u8ba1\u521b\u65b0\u548c\u6027\u80fd\u8868\u73b0\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16204", "pdf": "https://arxiv.org/pdf/2509.16204", "abs": "https://arxiv.org/abs/2509.16204", "authors": ["Xingang Guo", "Yaxin Li", "Xiangyi Kong", "Yilan Jiang", "Xiayu Zhao", "Zhihua Gong", "Yufan Zhang", "Daixuan Li", "Tianle Sang", "Beixiao Zhu", "Gregory Jun", "Yingbing Huang", "Yiqi Liu", "Yuqi Xue", "Rahul Dev Kundu", "Qi Jian Lim", "Yizhou Zhao", "Luke Alexander Granger", "Mohamed Badr Younis", "Darioush Keivan", "Nippun Sabharwal", "Shreyanka Sinha", "Prakhar Agarwal", "Kojo Vandyck", "Hanlin Mai", "Zichen Wang", "Aditya Venkatesh", "Ayush Barik", "Jiankun Yang", "Chongying Yue", "Jingjie He", "Libin Wang", "Licheng Xu", "Hao Chen", "Jinwen Wang", "Liujun Xu", "Rushabh Shetty", "Ziheng Guo", "Dahui Song", "Manvi Jha", "Weijie Liang", "Weiman Yan", "Bryan Zhang", "Sahil Bhandary Karnoor", "Jialiang Zhang", "Rutva Pandya", "Xinyi Gong", "Mithesh Ballae Ganesh", "Feize Shi", "Ruiling Xu", "Yifan Zhang", "Yanfeng Ouyang", "Lianhui Qin", "Elyse Rosenbaum", "Corey Snyder", "Peter Seiler", "Geir Dullerud", "Xiaojia Shelly Zhang", "Zuofu Cheng", "Pavan Kumar Hanumolu", "Jian Huang", "Mayank Kulkarni", "Mahdi Namazifar", "Huan Zhang", "Bin Hu"], "title": "Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs", "categories": ["cs.CE", "cs.HC", "cs.RO"], "comment": null, "summary": "Today, industry pioneers dream of developing general-purpose AI engineers\ncapable of designing and building humanity's most ambitious projects--from\nstarships that will carry us to distant worlds to Dyson spheres that harness\nstellar energy. Yet engineering design represents a fundamentally different\nchallenge for large language models (LLMs) compared to traditional\ntextbook-style problem solving or factual question answering. Real-world\nengineering design demands the synthesis of domain knowledge, navigation of\ncomplex trade-offs, and management of the tedious processes that consume much\nof practicing engineers' time. Despite these shared challenges across\nengineering disciplines, no benchmark currently captures the unique demands of\nengineering design work. In this work, we introduce ENGDESIGN, an Engineering\nDesign benchmark that evaluates LLMs' abilities to perform practical design\ntasks across nine engineering domains: Operating System Design, Computer\nArchitecture Design, Control System Design, Mechanical Systems, Structural\nDesign, Digital Hardware Design, Analog Integrated Circuit Design, Robotics,\nand Signal Processing. Unlike existing benchmarks that focus on factual recall\nor question answering, ENGDESIGN uniquely emphasizes LLMs' ability to\nsynthesize domain knowledge, reason under constraints, and generate functional,\nobjective-oriented designs. Each task in ENGDESIGN represents a real-world\nengineering design problem, accompanied by a detailed task description\nspecifying design goals, constraints, and performance requirements. We pioneer\na simulation-based evaluation paradigm where LLM-generated designs undergo\nrigorous testing through executable, domain-specific simulations-from circuit\nSPICE simulations to structural finite element analysis, from control system\nvalidation to robotic motion planning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86ENGDESIGN\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5de5\u7a0b\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u80fd\u529b\u7684\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u80fd\u591f\u8bc4\u4f30LLMs\u5728\u5b9e\u9645\u5de5\u7a0b\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u6d89\u53ca\u9886\u57df\u77e5\u8bc6\u7efc\u5408\u548c\u591a\u7ef4\u5ea6\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u56e2\u961f\u8bbe\u8ba1\u4e86\u6db5\u76d6\u4e5d\u4e2a\u5de5\u7a0b\u9886\u57df\u7684\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u6a21\u62df\u771f\u5b9e\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u6d4b\u8bd5\u8bc4\u4f30LLM\u751f\u6210\u7684\u8bbe\u8ba1\u3002", "result": "ENGDESIGN\u9996\u6b21\u4e3a\u8bc4\u4f30LLMs\u5728\u5de5\u7a0b\u8bbe\u8ba1\u7684\u7efc\u5408\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u4eff\u771f\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u586b\u8865\u4e86\u73b0\u6709\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5f3a\u5927\u7684AI\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2509.16267", "pdf": "https://arxiv.org/pdf/2509.16267", "abs": "https://arxiv.org/abs/2509.16267", "authors": ["Victor V. Puche", "Kashish Verma", "Matteo Fumagalli"], "title": "Underground Multi-robot Systems at Work: a revolution in mining", "categories": ["eess.SY", "cs.RO", "cs.SY", "eess.SY (Primary), cs.RO (Secondary)"], "comment": "6 pages, 6 figures, submitted to IEEE SII 2026", "summary": "The growing global demand for critical raw materials (CRMs) has highlighted\nthe need to access difficult and hazardous environments such as abandoned\nunderground mines. These sites pose significant challenges for conventional\nmachinery and human operators due to confined spaces, structural instability,\nand lack of infrastructure. To address this, we propose a modular multi-robot\nsystem designed for autonomous operation in such environments, enabling\nsequential mineral extraction tasks. Unlike existing work that focuses\nprimarily on mapping and inspection through global behavior or central control,\nour approach incorporates physical interaction capabilities using specialized\nrobots coordinated through local high-level behavior control. Our proposed\nsystem utilizes Hierarchical Finite State Machine (HFSM) behaviors to structure\ncomplex task execution across heterogeneous robotic platforms. Each robot has\nits own HFSM behavior to perform sequential autonomy while maintaining overall\nsystem coordination, achieved by triggering behavior execution through\ninter-robot communication. This architecture effectively integrates software\nand hardware components to support collaborative, task-driven multi-robot\noperation in confined underground environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5371\u9669\u7684\u5730\u4e0b\u73af\u5883\u4e2d\u81ea\u4e3b\u6267\u884c\u77ff\u7269\u63d0\u53d6\u4efb\u52a1\uff0c\u901a\u8fc7\u5c40\u90e8\u884c\u4e3a\u63a7\u5236\u548c\u7269\u7406\u4ea4\u4e92\u80fd\u529b\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5168\u7403\u5bf9\u5173\u952e\u539f\u6750\u6599\uff08CRMs\uff09\u7684\u9700\u6c42\u589e\u957f\uff0c\u4fc3\u4f7f\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u5371\u9669\u548c\u53d7\u9650\u73af\u5883\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6709\u9650\u72b6\u6001\u673a\uff08HFSM\uff09\u884c\u4e3a\u63a7\u5236\uff0c\u534f\u8c03\u5f02\u6784\u673a\u5668\u4eba\u5e73\u53f0\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u652f\u6301\u673a\u5668\u4eba\u95f4\u7684\u4ea4\u4e92\u901a\u4fe1\u3002", "result": "\u7cfb\u7edf\u6709\u6548\u6574\u5408\u8f6f\u786c\u4ef6\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u4e3a\u5371\u9669\u73af\u5883\u4e0b\u7684\u77ff\u7269\u63d0\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16365", "pdf": "https://arxiv.org/pdf/2509.16365", "abs": "https://arxiv.org/abs/2509.16365", "authors": ["Dylan James-Kavanaugh", "Patrick McNamee", "Qixu Wang", "Zahra Nili Ahmadabadi"], "title": "Servos for Local Map Exploration Onboard Nonholonomic Vehicles for Extremum Seeking", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "12 pages, 8 figures, IEEE Transactions on Control Systems Technology\n  Submission", "summary": "Extremum seeking control (ESC) often employs perturbation-based estimates of\nderivatives for some sensor field or cost function. These estimates are\ngenerally obtained by simply multiplying the output of a single-unit sensor by\nsome time-varying function. Previous work has focused on sinusoidal\nperturbations to generate derivative estimates with results for arbitrary order\nderivatives of scalar maps or higher up to third-order derivatives of\nmultivariable maps. This work extends the perturbations from sinusoidal to\nbounded periodic or almost periodic functions and considers multivariable maps.\nA necessary and sufficient condition is given for determining if time-varying\nfunctions exist for estimating arbitrary order derivatives of multivariable\nmaps for any given bounded periodic or almost periodic dither signal. These\nresults are then used in a source seeking controller for a nonholonomic vehicle\nwith a sensor actuated by servo. The conducted simulation and real-world\nexperiments demonstrate that by distributing the local map exploration to a\nservo, the nonholonomic vehicle was able to achieve a faster convergence to the\nsource.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6781\u503c\u641c\u7d22\u63a7\u5236\u4e2d\u5982\u4f55\u5229\u7528\u5468\u671f\u6027\u6216\u8fd1\u4f3c\u5468\u671f\u6027\u6270\u52a8\u51fd\u6570\u6765\u4f30\u8ba1\u591a\u53d8\u91cf\u6620\u5c04\u7684\u4efb\u610f\u9636\u5bfc\u6570\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f7f\u7528\u4f3a\u670d\u9a71\u52a8\u7684\u4f20\u611f\u5668\u53ef\u4ee5\u63d0\u9ad8\u975e\u5b8c\u6574\u8f66\u8f86\u5bfb\u627e\u6e90\u70b9\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6781\u503c\u641c\u7d22\u63a7\u5236\u901a\u5e38\u4f7f\u7528\u6b63\u5f26\u6270\u52a8\u6765\u4f30\u8ba1\u5bfc\u6570\uff0c\u4f46\u5176\u9002\u7528\u6027\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u6270\u52a8\u51fd\u6570\u7684\u8303\u56f4\uff0c\u4f7f\u5176\u5305\u62ec\u6709\u754c\u5468\u671f\u6216\u8fd1\u4f3c\u5468\u671f\u51fd\u6570\uff0c\u5e76\u89e3\u51b3\u591a\u53d8\u91cf\u6620\u5c04\u7684\u9ad8\u9636\u5bfc\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5fc5\u8981\u4e14\u5145\u5206\u7684\u6761\u4ef6\uff0c\u7528\u4e8e\u5224\u65ad\u7ed9\u5b9a\u6709\u754c\u5468\u671f\u6216\u8fd1\u4f3c\u5468\u671f\u4fe1\u53f7\u4e0b\u662f\u5426\u5b58\u5728\u65f6\u53d8\u51fd\u6570\u6765\u4f30\u8ba1\u591a\u53d8\u91cf\u6620\u5c04\u7684\u4efb\u610f\u9636\u5bfc\u6570\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f3a\u670d\u9a71\u52a8\u7684\u4f20\u611f\u5668\u5728\u975e\u5b8c\u6574\u8f66\u8f86\u4e0a\u8fdb\u884c\u6e90\u70b9\u641c\u7d22\u63a7\u5236\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u5c40\u90e8\u5730\u56fe\u63a2\u7d22\u4efb\u52a1\u5206\u914d\u7ed9\u4f3a\u670d\u9a71\u52a8\u7684\u4f20\u611f\u5668\uff0c\u975e\u5b8c\u6574\u8f66\u8f86\u80fd\u591f\u66f4\u5feb\u5730\u6536\u655b\u5230\u6e90\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6781\u503c\u641c\u7d22\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6270\u52a8\u51fd\u6570\u9009\u62e9\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u53d8\u91cf\u6620\u5c04\u548c\u975e\u5b8c\u6574\u7cfb\u7edf\u7684\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2509.16370", "pdf": "https://arxiv.org/pdf/2509.16370", "abs": "https://arxiv.org/abs/2509.16370", "authors": ["Jo\u00e3o Sousa-Pinto", "Dominique Orban"], "title": "A Regularized Riccati Recursion for Interior-Point Optimal Control", "categories": ["math.OC", "cs.MS", "cs.RO", "cs.SY", "eess.SY", "49M37, 90C51, 93B45", "G.1.6"], "comment": null, "summary": "We derive a closed-form extension of Riccati's recursion for solving\nregularized LQR problems. We also show how this can be used to solve general\nconstrained, non-convex, discrete-time optimal control problems via a\nregularized interior point method, while guaranteeing that each step is a\ndescent direction of an Augmented Barrier-Lagrangian merit function. We also\nprovide MIT-licensed implementations of our method in C++ and JAX.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316LQR\u95ee\u9898\u7684\u95ed\u5f0f\u6269\u5c55\u89e3\u6cd5\uff0c\u5e76\u5c06\u5176\u7528\u4e8e\u89e3\u51b3\u4e00\u822c\u7ea6\u675f\u3001\u975e\u51f8\u3001\u79bb\u6563\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u6bcf\u4e00\u6b65\u662f\u589e\u5f3a\u969c\u788d-\u62c9\u683c\u6717\u65e5\u7f5a\u51fd\u6570\u7684\u4e0b\u964d\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u6b63\u5219\u5316LQR\u95ee\u9898\u548c\u4e00\u822c\u7ea6\u675f\u3001\u975e\u51f8\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u7b97\u6cd5\u5b9e\u73b0\u3002", "method": "\u57fa\u4e8eRiccati\u9012\u63a8\u7684\u95ed\u5f0f\u6269\u5c55\uff0c\u7ed3\u5408\u6b63\u5219\u5316\u5185\u70b9\u6cd5\uff0c\u4fdd\u8bc1\u6bcf\u4e00\u6b65\u662f\u589e\u5f3a\u969c\u788d-\u62c9\u683c\u6717\u65e5\u7f5a\u51fd\u6570\u7684\u4e0b\u964d\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u5728C++\u548cJAX\u4e2d\u63d0\u4f9b\u4e86MIT\u8bb8\u53ef\u7684\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u6700\u4f18\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.16415", "pdf": "https://arxiv.org/pdf/2509.16415", "abs": "https://arxiv.org/abs/2509.16415", "authors": ["Zhengri Wu", "Yiran Wang", "Yu Wen", "Zeyu Zhang", "Biao Wu", "Hao Tang"], "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.", "AI": {"tldr": "StereoAdapter\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u878d\u5408\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9ad8\u6548\u9002\u5e94\u5927\u578b\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\u548c\u878d\u5408\u5355\u76ee\u4e0e\u7acb\u4f53\u4fe1\u606f\u3002", "method": "\u63d0\u51faStereoAdapter\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u9002\u5e94\u5355\u76ee\u7f16\u7801\u5668\u548c\u5faa\u73af\u7acb\u4f53\u7ec6\u5316\u6a21\u5757\uff0c\u5f15\u5165\u52a8\u6001LoRA\u9002\u5e94\u548c\u5408\u6210\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u3002", "result": "\u5728TartanAir\u548cSQUID\u4e0a\u7684\u6027\u80fd\u5206\u522b\u63d0\u53476.11%\u548c5.12%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "StereoAdapter\u901a\u8fc7\u9ad8\u6548\u9002\u5e94\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16552", "pdf": "https://arxiv.org/pdf/2509.16552", "abs": "https://arxiv.org/abs/2509.16552", "authors": ["Xiaoyang Yan", "Muleilan Pei", "Shaojie Shen"], "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D occupancy prediction is critical for comprehensive scene understanding in\nvision-centric autonomous driving. Recent advances have explored utilizing 3D\nsemantic Gaussians to model occupancy while reducing computational overhead,\nbut they remain constrained by insufficient multi-view spatial interaction and\nlimited multi-frame temporal consistency. To overcome these issues, in this\npaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework\nto enhance both spatial and temporal modeling in existing Gaussian-based\npipelines. Specifically, we develop a guidance-informed spatial aggregation\nstrategy within a dual-mode attention mechanism to strengthen spatial\ninteraction in Gaussian representations. Furthermore, we introduce a\ngeometry-aware temporal fusion scheme that effectively leverages historical\ncontext to improve temporal continuity in scene completion. Extensive\nexperiments on the large-scale nuScenes occupancy prediction benchmark showcase\nthat our proposed approach not only achieves state-of-the-art performance but\nalso delivers markedly better temporal consistency compared to existing\nGaussian-based methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u95f4-\u65f6\u95f4\u9ad8\u65af\u6cfc\u6e85\uff08ST-GS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d3D\u5360\u4f4d\u9884\u6d4b\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6027\u80fd\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76843D\u8bed\u4e49\u9ad8\u65af\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u7a7a\u95f4\u4ea4\u4e92\u548c\u591a\u5e27\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u7684\u5168\u9762\u6027\u3002", "method": "\u901a\u8fc7\u53cc\u6a21\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u7684\u7a7a\u95f4\u805a\u5408\u7b56\u7565\u589e\u5f3a\u9ad8\u65af\u8868\u793a\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u540c\u65f6\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u65f6\u95f4\u878d\u5408\u65b9\u6848\u4ee5\u5229\u7528\u5386\u53f2\u4e0a\u4e0b\u6587\u6539\u5584\u573a\u666f\u5b8c\u6210\u7684\u65f6\u95f4\u8fde\u7eed\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21nuScenes\u5360\u4f4d\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd8\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9ad8\u65af\u65b9\u6cd5\u3002", "conclusion": "ST-GS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16588", "pdf": "https://arxiv.org/pdf/2509.16588", "abs": "https://arxiv.org/abs/2509.16588", "authors": ["Haiming Zhang", "Yiyao Zhu", "Wending Zhou", "Xu Yan", "Yingjie Cai", "Bingbing Liu", "Shuguang Cui", "Zhen Li"], "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025 (Spotlight)", "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection).", "AI": {"tldr": "SQS\u662f\u4e00\u79cd\u65b0\u578b\u7684\u67e5\u8be2\u9a71\u52a8\u7684\u7a00\u758f\u611f\u77e5\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7684splatting\u6280\u672f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u611f\u77e5\u6a21\u578b\uff08SPMs\uff09\u5728\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6837\u5316\u4efb\u52a1\u9700\u6c42\u3002", "method": "SQS\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u9884\u6d4b3D\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u7684\u81ea\u76d1\u7763\u91cd\u5efa\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7279\u5f81\uff1b\u5728\u5fae\u8c03\u9636\u6bb5\u901a\u8fc7\u67e5\u8be2\u4ea4\u4e92\u673a\u5236\u4e0e\u4e0b\u6e38\u4efb\u52a1\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSQS\u57283D\u5360\u6709\u7387\u9884\u6d4b\u548c\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5360\u6709\u7387\u9884\u6d4b\u63d0\u53471.3 mIoU\uff0c3D\u68c0\u6d4b\u63d0\u53471.0 NDS\uff09\u3002", "conclusion": "SQS\u4e3a\u7a00\u758f\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u4efb\u52a13D\u611f\u77e5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16650", "pdf": "https://arxiv.org/pdf/2509.16650", "abs": "https://arxiv.org/abs/2509.16650", "authors": ["Manish Prajapat", "Johannes K\u00f6hler", "Melanie N. Zeilinger", "Andreas Krause"], "title": "Safe Guaranteed Dynamics Exploration with Probabilistic Models", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.DS", "math.OC"], "comment": null, "summary": "Ensuring both optimality and safety is critical for the real-world deployment\nof agents, but becomes particularly challenging when the system dynamics are\nunknown. To address this problem, we introduce a notion of maximum safe\ndynamics learning via sufficient exploration in the space of safe policies. We\npropose a $\\textit{pessimistically}$ safe framework that\n$\\textit{optimistically}$ explores informative states and, despite not reaching\nthem due to model uncertainty, ensures continuous online learning of dynamics.\nThe framework achieves first-of-its-kind results: learning the dynamics model\nsufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a\nfinite time, while ensuring provably safe operation throughout with high\nprobability and without requiring resets. Building on this, we propose an\nalgorithm to maximize rewards while learning the dynamics $\\textit{only to the\nextent needed}$ to achieve close-to-optimal performance. Unlike typical\nreinforcement learning (RL) methods, our approach operates online in a\nnon-episodic setting and ensures safety throughout the learning process. We\ndemonstrate the effectiveness of our approach in challenging domains such as\nautonomous car racing and drone navigation under aerodynamic effects $-$\nscenarios where safety is critical and accurate modeling is difficult.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u60b2\u89c2\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u4e50\u89c2\u63a2\u7d22\u672a\u77e5\u72b6\u6001\u6765\u786e\u4fdd\u52a8\u6001\u6a21\u578b\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u8bc1\u9ad8\u6982\u7387\u7684\u5b89\u5168\u64cd\u4f5c\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u65e0\u4eba\u673a\u5bfc\u822a\u7b49\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7cfb\u7edf\u52a8\u6001\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5728\u786e\u4fdd\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u662f\u5b9e\u9645\u90e8\u7f72\u667a\u80fd\u4f53\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u60b2\u89c2\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u4e50\u89c2\u63a2\u7d22\u4fe1\u606f\u6027\u72b6\u6001\u6765\u5728\u7ebf\u5b66\u4e60\u52a8\u6001\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u65f6\u95f4\u5185\u5b66\u4e60\u52a8\u6001\u6a21\u578b\u5e76\u786e\u4fdd\u9ad8\u6982\u7387\u5b89\u5168\uff0c\u65e0\u9700\u91cd\u7f6e\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u672a\u77e5\u52a8\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u6027\u80fd\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u3002"}}
{"id": "2509.16677", "pdf": "https://arxiv.org/pdf/2509.16677", "abs": "https://arxiv.org/abs/2509.16677", "authors": ["Wenxin Li", "Kunyu Peng", "Di Wen", "Ruiping Liu", "Mengfei Duan", "Kai Luo", "Kailun Yang"], "title": "Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "comment": "The established benchmark and source code will be made publicly\n  available at https://github.com/mylwx/ActiSeg-NL", "summary": "Embodied intelligence relies on accurately segmenting objects actively\ninvolved in interactions. Action-based video object segmentation addresses this\nby linking segmentation with action semantics, but it depends on large-scale\nannotations and prompts that are costly, inconsistent, and prone to multimodal\nnoise such as imprecise masks and referential ambiguity. To date, this\nchallenge remains unexplored. In this work, we take the first step by studying\naction-based video object segmentation under label noise, focusing on two\nsources: textual prompt noise (category flips and within-category noun\nsubstitutions) and mask annotation noise (perturbed object boundaries to mimic\nimprecise supervision). Our contributions are threefold. First, we introduce\ntwo types of label noises for the action-based video object segmentation task.\nSecond, we build up the first action-based video object segmentation under a\nlabel noise benchmark ActiSeg-NL and adapt six label-noise learning strategies\nto this setting, and establish protocols for evaluating them under textual,\nboundary, and mixed noise. Third, we provide a comprehensive analysis linking\nnoise types to failure modes and robustness gains, and we introduce a Parallel\nMask Head Mechanism (PMHM) to address mask annotation noise. Qualitative\nevaluations further reveal characteristic failure modes, including boundary\nleakage and mislocalization under boundary perturbations, as well as occasional\nidentity substitutions under textual flips. Our comparative analysis reveals\nthat different learning strategies exhibit distinct robustness profiles,\ngoverned by a foreground-background trade-off where some achieve balanced\nperformance while others prioritize foreground accuracy at the cost of\nbackground precision. The established benchmark and source code will be made\npublicly available at https://github.com/mylwx/ActiSeg-NL.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff08\u6587\u672c\u63d0\u793a\u566a\u58f0\u548c\u63a9\u7801\u6807\u6ce8\u566a\u58f0\uff09\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u51c6ActiSeg-NL\u3002\u901a\u8fc7\u5206\u6790\u566a\u58f0\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u5e76\u5f15\u5165\u5e76\u884c\u63a9\u7801\u5934\u673a\u5236\uff08PMHM\uff09\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u4e0d\u540c\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "motivation": "\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u548c\u63d0\u793a\uff0c\u4f46\u8fd9\u4e9b\u6807\u6ce8\u5e38\u56e0\u566a\u58f0\uff08\u5982\u4e0d\u7cbe\u786e\u7684\u63a9\u7801\u6216\u6587\u672c\u6b67\u4e49\uff09\u800c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u76ee\u524d\u8fd9\u4e00\u6311\u6218\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\uff0c\u56e0\u6b64\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u9996\u5148\u5b9a\u4e49\u4e86\u6587\u672c\u63d0\u793a\u566a\u58f0\u548c\u63a9\u7801\u6807\u6ce8\u566a\u58f0\u4e24\u79cd\u7c7b\u578b\uff0c\u5e76\u6784\u5efa\u4e86ActiSeg-NL\u57fa\u51c6\u3002\u968f\u540e\uff0c\u8bba\u6587\u5c06\u516d\u79cd\u6807\u7b7e\u566a\u58f0\u5b66\u4e60\u7b56\u7565\u5e94\u7528\u4e8e\u6b64\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u63a9\u7801\u5934\u673a\u5236\uff08PMHM\uff09\u4ee5\u5e94\u5bf9\u63a9\u7801\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u5b66\u4e60\u7b56\u7565\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u5404\u5f02\uff0c\u90e8\u5206\u7b56\u7565\u5728\u5e73\u8861\u6027\u80fd\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u4ee5\u727a\u7272\u80cc\u666f\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u63d0\u5347\u524d\u666f\u51c6\u786e\u6027\u3002PMHM\u6709\u6548\u7f13\u89e3\u4e86\u63a9\u7801\u566a\u58f0\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u4e86PMHM\u7684\u6709\u6548\u6027\u3002\u57fa\u51c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.16721", "pdf": "https://arxiv.org/pdf/2509.16721", "abs": "https://arxiv.org/abs/2509.16721", "authors": ["Haoyuan Li", "Rui Liu", "Hehe Fan", "Yi Yang"], "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "19 pages, 12 figures, 6 tables", "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faText-Scene\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5c063D\u573a\u666f\u89e3\u6790\u4e3a\u6587\u672c\u63cf\u8ff0\u6765\u89e3\u51b33D\u573a\u666f\u7406\u89e3\u4e0e\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u548cMLLMs\u751f\u6210\u51c6\u786e\u3001\u8be6\u7ec6\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u63cf\u8ff0\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u63a8\u51faInPlan3D\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6311\u6218\uff0c\u5982\u4e30\u5bcc\u7684\u7a7a\u95f4\u5173\u7cfb\u548c\u7f3a\u5c11\u5927\u89c4\u6a213D\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u3002", "method": "Text-Scene\u6846\u67b6\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u548cMLLMs\uff0c\u81ea\u52a8\u751f\u62103D\u573a\u666f\u7684\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u751f\u6210\u7684\u63cf\u8ff0\u80fd\u51c6\u786e\u8868\u793a3D\u573a\u666f\u5e76\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "Text-Scene\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bed\u8a00\u63cf\u8ff0\u5de5\u5177\uff0c\u5e76\u63a8\u52a8\u4e86\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16832", "pdf": "https://arxiv.org/pdf/2509.16832", "abs": "https://arxiv.org/abs/2509.16832", "authors": ["Ziyang Xu", "Benedikt Schwab", "Yihui Yang", "Thomas H. Kolbe", "Christoph Holst"], "title": "L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "submit to ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Accurate registration between LiDAR (Light Detection and Ranging) point\nclouds and semantic 3D city models is a fundamental topic in urban digital\ntwinning and a prerequisite for downstream tasks, such as digital construction,\nchange detection and model refinement. However, achieving accurate\nLiDAR-to-Model registration at individual building level remains challenging,\nparticularly due to the generalization uncertainty in semantic 3D city models\nat the Level of Detail 2 (LoD2). This paper addresses this gap by proposing\nL2M-Reg, a plane-based fine registration method that explicitly accounts for\nmodel uncertainty. L2M-Reg consists of three key steps: establishing reliable\nplane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,\nand adaptively estimating vertical translation. Experiments on three real-world\ndatasets demonstrate that L2M-Reg is both more accurate and computationally\nefficient than existing ICP-based and plane-based methods. Overall, L2M-Reg\nprovides a novel building-level solution regarding LiDAR-to-Model registration\nwhen model uncertainty is present.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aL2M-Reg\u7684\u57fa\u4e8e\u5e73\u9762\u7684\u7cbe\u7ec6\u914d\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LiDAR\u70b9\u4e91\u4e0e\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u5728\u5efa\u7b51\u7269\u7ea7\u522b\u914d\u51c6\u7684\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LiDAR\u4e0e\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u5728\u5efa\u7b51\u7269\u7ea7\u522b\u914d\u51c6\u65f6\u7531\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "L2M-Reg\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u5b9e\u73b0\u914d\u51c6\uff1a\u5efa\u7acb\u53ef\u9760\u7684\u5e73\u9762\u5bf9\u5e94\u5173\u7cfb\u3001\u6784\u5efa\u4f2a\u5e73\u9762\u7ea6\u675f\u7684\u9ad8\u65af-\u8d6b\u5c14\u9ed8\u7279\u6a21\u578b\u4ee5\u53ca\u81ea\u9002\u5e94\u4f30\u8ba1\u5782\u76f4\u5e73\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2M-Reg\u6bd4\u73b0\u6709\u7684\u57fa\u4e8eICP\u548c\u5e73\u9762\u7684\u65b9\u6cd5\u66f4\u7cbe\u786e\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "L2M-Reg\u4e3a\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684LiDAR\u5230\u6a21\u578b\u914d\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5efa\u7b51\u7269\u7ea7\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16909", "pdf": "https://arxiv.org/pdf/2509.16909", "abs": "https://arxiv.org/abs/2509.16909", "authors": ["Yijun Yuan", "Zhuoguang Chen", "Kenan Li", "Weibang Wang", "Hang Zhao"], "title": "SLAM-Former: Putting SLAM into One Transformer", "categories": ["cs.CV", "cs.RO"], "comment": "Project Page:https://tsinghua-mars-lab.github.io/SLAM-Former", "summary": "We present SLAM-Former, a novel neural approach that integrates full SLAM\ncapabilities into a single transformer. Similar to traditional SLAM systems,\nSLAM-Former comprises both a frontend and a backend that operate in tandem. The\nfrontend processes sequential monocular images in real-time for incremental\nmapping and tracking, while the backend performs global refinement to ensure a\ngeometrically consistent result. This alternating execution allows the frontend\nand backend to mutually promote one another, enhancing overall system\nperformance. Comprehensive experimental results demonstrate that SLAM-Former\nachieves superior or highly competitive performance compared to\nstate-of-the-art dense SLAM methods.", "AI": {"tldr": "SLAM-Former\u662f\u4e00\u79cd\u65b0\u578b\u7684\u795e\u7ecf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00Transformer\u6574\u5408\u4e86\u5b8c\u6574\u7684SLAM\u529f\u80fd\uff0c\u5305\u62ec\u524d\u7aef\u548c\u540e\u7aef\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5efa\u56fe\u548c\u5168\u5c40\u4f18\u5316\u3002", "motivation": "\u4f20\u7edfSLAM\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u591a\u6a21\u5757\u8bbe\u8ba1\uff0cSLAM-Former\u65e8\u5728\u901a\u8fc7\u5355\u4e00Transformer\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "method": "SLAM-Former\u5206\u4e3a\u524d\u7aef\u548c\u540e\u7aef\uff1a\u524d\u7aef\u5b9e\u65f6\u5904\u7406\u5355\u76ee\u56fe\u50cf\u8fdb\u884c\u589e\u91cf\u5efa\u56fe\u548c\u8ddf\u8e2a\uff0c\u540e\u7aef\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u4ee5\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u4e24\u8005\u4ea4\u66ff\u6267\u884c\u4ee5\u76f8\u4e92\u4fc3\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLAM-Former\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u5bc6\u96c6SLAM\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "SLAM-Former\u901a\u8fc7\u5355\u4e00Transformer\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684SLAM\u529f\u80fd\uff0c\u5c55\u73b0\u4e86\u795e\u7ecf\u65b9\u6cd5\u5728SLAM\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17107", "pdf": "https://arxiv.org/pdf/2509.17107", "abs": "https://arxiv.org/abs/2509.17107", "authors": ["Lingzhao Kong", "Jiacheng Lin", "Siyu Li", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code will be made publicly available at\n  https://github.com/godk0509/CoBEVMoE", "summary": "Collaborative perception aims to extend sensing coverage and improve\nperception accuracy by sharing information among multiple agents. However, due\nto differences in viewpoints and spatial positions, agents often acquire\nheterogeneous observations. Existing intermediate fusion methods primarily\nfocus on aligning similar features, often overlooking the perceptual diversity\namong agents. To address this limitation, we propose CoBEVMoE, a novel\ncollaborative perception framework that operates in the Bird's Eye View (BEV)\nspace and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In\nDMoE, each expert is dynamically generated based on the input features of a\nspecific agent, enabling it to extract distinctive and reliable cues while\nattending to shared semantics. This design allows the fusion process to\nexplicitly model both feature similarity and heterogeneity across agents.\nFurthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance\ninter-expert diversity and improve the discriminability of the fused\nrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets\ndemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,\nit improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the\nAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the\neffectiveness of expert-based heterogeneous feature modeling in multi-agent\ncollaborative perception. The source code will be made publicly available at\nhttps://github.com/godk0509/CoBEVMoE.", "AI": {"tldr": "CoBEVMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u4e13\u5bb6\uff08DMoE\uff09\u67b6\u6784\u5728BEV\u7a7a\u95f4\u4e2d\u5efa\u6a21\u5f02\u6784\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e2d\u95f4\u878d\u5408\u65b9\u6cd5\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u611f\u77e5\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528BEV\u7a7a\u95f4\u548cDMoE\u67b6\u6784\uff0c\u52a8\u6001\u751f\u6210\u4e13\u5bb6\u4ee5\u63d0\u53d6\u5f02\u6784\u7279\u5f81\uff0c\u5e76\u5f15\u5165DEML\u635f\u5931\u589e\u5f3a\u4e13\u5bb6\u591a\u6837\u6027\u3002", "result": "\u5728OPV2V\u548cDAIR-V2X-C\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIoU\u63d0\u53471.5%\uff0cAP@50\u63d0\u53473.0%\u3002", "conclusion": "CoBEVMoE\u9a8c\u8bc1\u4e86\u5f02\u6784\u7279\u5f81\u5efa\u6a21\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17131", "pdf": "https://arxiv.org/pdf/2509.17131", "abs": "https://arxiv.org/abs/2509.17131", "authors": ["Filip Bajraktari", "Luke Bhan", "Miroslav Krstic", "Yuanyuan Shi"], "title": "Delay compensation of multi-input distinct delay nonlinear systems via neural operators", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.DS"], "comment": "8 pages, 1 figure", "summary": "In this work, we present the first stability results for approximate\npredictors in multi-input non-linear systems with distinct actuation delays. We\nshow that if the predictor approximation satisfies a uniform (in time) error\nbound, semi-global practical stability is correspondingly achieved. For such\napproximators, the required uniform error bound depends on the desired region\nof attraction and the number of control inputs in the system. The result is\nachieved through transforming the delay into a transport PDE and conducting\nanalysis on the coupled ODE-PDE cascade. To highlight the viability of such\nerror bounds, we demonstrate our results on a class of approximators - neural\noperators - showcasing sufficiency for satisfying such a universal bound both\ntheoretically and in simulation on a mobile robot experiment.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u591a\u8f93\u5165\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5177\u6709\u4e0d\u540c\u9a71\u52a8\u5ef6\u8fdf\u7684\u8fd1\u4f3c\u9884\u6d4b\u5668\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u6ee1\u8db3\u65f6\u95f4\u4e00\u81f4\u8bef\u5dee\u8fb9\u754c\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u534a\u5168\u5c40\u5b9e\u9645\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u89e3\u51b3\u591a\u8f93\u5165\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u540c\u5ef6\u8fdf\u65f6\u7684\u9884\u6d4b\u5668\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u5ef6\u8fdf\u8f6c\u5316\u4e3a\u4f20\u8f93PDE\uff0c\u5e76\u5728ODE-PDE\u7ea7\u8054\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9884\u6d4b\u5668\u7a33\u5b9a\u6027\u7684\u7406\u8bba\u63a8\u5bfc\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u82e5\u9884\u6d4b\u5668\u8fd1\u4f3c\u6ee1\u8db3\u65f6\u95f4\u4e00\u81f4\u7684\u8bef\u5dee\u8fb9\u754c\uff0c\u5219\u80fd\u5728\u6240\u9700\u5438\u5f15\u533a\u57df\u548c\u7cfb\u7edf\u8f93\u5165\u6570\u91cf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u534a\u5168\u5c40\u5b9e\u9645\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u9a8c\u8bc1\u4e86\u8bef\u5dee\u8fb9\u754c\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17323", "pdf": "https://arxiv.org/pdf/2509.17323", "abs": "https://arxiv.org/abs/2509.17323", "authors": ["Buyin Deng", "Lingxin Huang", "Kai Luo", "Fei Teng", "Kailun Yang"], "title": "DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code will be made publicly available at\n  https://github.com/warriordby/DepTR-MOT", "summary": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic\nperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D\ncues, such as bounding boxes and motion modeling, which struggle under\nocclusions and close-proximity interactions. Trackers relying on these 2D cues\nare particularly unreliable in robotic environments, where dense targets and\nfrequent occlusions are common. While depth information has the potential to\nalleviate these issues, most existing MOT datasets lack depth annotations,\nleading to its underexploited role in the domain. To unveil the potential of\ndepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based\ndetector enhanced with instance-level depth information. Specifically, we\npropose two key innovations: (i) foundation model-based instance-level soft\ndepth label supervision, which refines depth prediction, and (ii) the\ndistillation of dense depth maps to maintain global depth consistency. These\nstrategies enable DepTR-MOT to output instance-level depth during inference,\nwithout requiring foundation models and without additional computational cost.\nBy incorporating depth cues, our method enhances the robustness of the TBD\nparadigm, effectively resolving occlusion and close-proximity challenges.\nExperiments on both the QuadTrack and DanceTrack datasets demonstrate the\neffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,\nrespectively. In particular, results on QuadTrack, a robotic platform MOT\ndataset, highlight the advantages of our method in handling occlusion and\nclose-proximity challenges in robotic tracking. The source code will be made\npublicly available at https://github.com/warriordby/DepTR-MOT.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DepTR-MOT\uff0c\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u89c6\u89c9\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u4fe1\u606f\u6539\u8fdb\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e2D\u7ebf\u7d22\u7684\u8ddf\u8e2a\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7269\u5bc6\u96c6\u548c\u9891\u7e41\u906e\u6321\u7684\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\uff0c\u800c\u6df1\u5ea6\u4fe1\u606f\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u51faDepTR-MOT\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\u6807\u7b7e\u76d1\u7763\u548c\u5bc6\u96c6\u6df1\u5ea6\u56fe\u63d0\u53d6\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u5373\u53ef\u8f93\u51fa\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\u3002", "result": "\u5728QuadTrack\u548cDanceTrack\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f9727.59\u548c44.47\u7684HOTA\u5206\u6570\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6df1\u5ea6\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u9c81\u68d2\u6027\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6e90\u4ee3\u7801\u4ee5\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2509.17341", "pdf": "https://arxiv.org/pdf/2509.17341", "abs": "https://arxiv.org/abs/2509.17341", "authors": ["Lohitvel Gopikannan", "Shashi Ranjan Kumar", "Abhinav Sinha"], "title": "Trajectory Encryption Cooperative Salvo Guidance", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper introduces the concept of trajectory encryption in cooperative\nsimultaneous target interception, wherein heterogeneity in guidance principles\nacross a team of unmanned autonomous systems is leveraged as a strategic design\nfeature. By employing a mix of heterogeneous time-to-go formulations leading to\na cooperative guidance strategy, the swarm of vehicles is able to generate\ndiverse trajectory families. This diversity expands the feasible solution space\nfor simultaneous target interception, enhances robustness under disturbances,\nand enables flexible time-to-go adjustments without predictable detouring. From\nan adversarial perspective, heterogeneity obscures the collective interception\nintent by preventing straightforward prediction of swarm dynamics, effectively\nacting as an encryption layer in the trajectory domain. Simulations demonstrate\nthat the swarm of heterogeneous vehicles is able to intercept a moving target\nsimultaneously from a diverse set of initial engagement configurations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f02\u6784\u5236\u5bfc\u539f\u7406\u7684\u8f68\u8ff9\u52a0\u5bc6\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u8f68\u8ff9\u5bb6\u65cf\u589e\u5f3a\u534f\u540c\u76ee\u6807\u62e6\u622a\u7684\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5728\u65e0\u4eba\u673a\u534f\u540c\u62e6\u622a\u76ee\u6807\u7684\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5e72\u6270\u4e14\u8f68\u8ff9\u53ef\u9884\u6d4b\u6027\u9ad8\uff0c\u5bfc\u81f4\u62e6\u622a\u610f\u56fe\u66b4\u9732\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f02\u6784\u5236\u5bfc\u7b56\u7565\u5b9e\u73b0\u8f68\u8ff9\u52a0\u5bc6\uff0c\u4ece\u800c\u63d0\u9ad8\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u5f02\u6784\u7684\u65f6\u95f4\u5230\u8fbe\uff08time-to-go\uff09\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u534f\u540c\u5236\u5bfc\u65b9\u6cd5\uff0c\u4f7f\u65e0\u4eba\u673a\u7fa4\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\u5bb6\u65cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\u80fd\u591f\u4ece\u591a\u6837\u5316\u7684\u521d\u59cb\u914d\u7f6e\u4e2d\u540c\u65f6\u62e6\u622a\u79fb\u52a8\u76ee\u6807\uff0c\u4e14\u8f68\u8ff9\u4e0d\u53ef\u9884\u6d4b\u6027\u589e\u5f3a\u3002", "conclusion": "\u5f02\u6784\u5236\u5bfc\u7b56\u7565\u901a\u8fc7\u8f68\u8ff9\u52a0\u5bc6\u6709\u6548\u63d0\u9ad8\u4e86\u534f\u540c\u62e6\u622a\u7684\u9690\u853d\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u7fa4\u4efb\u52a1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17430", "pdf": "https://arxiv.org/pdf/2509.17430", "abs": "https://arxiv.org/abs/2509.17430", "authors": ["Gunjan Chhablani", "Xiaomeng Ye", "Muhammad Zubair Irshad", "Zsolt Kira"], "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device", "categories": ["cs.CV", "cs.RO"], "comment": "16 pages, 18 figures, paper accepted at ICCV, 2025", "summary": "The field of Embodied AI predominantly relies on simulation for training and\nevaluation, often using either fully synthetic environments that lack\nphotorealism or high-fidelity real-world reconstructions captured with\nexpensive hardware. As a result, sim-to-real transfer remains a major\nchallenge. In this paper, we introduce EmbodiedSplat, a novel approach that\npersonalizes policy training by efficiently capturing the deployment\nenvironment and fine-tuning policies within the reconstructed scenes. Our\nmethod leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to\nbridge the gap between realistic scene capture and effective training\nenvironments. Using iPhone-captured deployment scenes, we reconstruct meshes\nvia GS, enabling training in settings that closely approximate real-world\nconditions. We conduct a comprehensive analysis of training strategies,\npre-training datasets, and mesh reconstruction techniques, evaluating their\nimpact on sim-to-real predictivity in real-world scenarios. Experimental\nresults demonstrate that agents fine-tuned with EmbodiedSplat outperform both\nzero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and\nsynthetically generated datasets (HSSD), achieving absolute success rate\nimprovements of 20\\% and 40\\% on real-world Image Navigation task. Moreover,\nour approach yields a high sim-vs-real correlation (0.87--0.97) for the\nreconstructed meshes, underscoring its effectiveness in adapting policies to\ndiverse environments with minimal effort. Project page:\nhttps://gchhablani.github.io/embodied-splat", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aEmbodiedSplat\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u6280\u672f\u548cHabitat-Sim\u6a21\u62df\u5668\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u573a\u666f\u91cd\u5efa\u4e2d\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u6548\u679c\u3002", "motivation": "\u5f53\u524dEmbodied AI\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u4eff\u771f\u73af\u5883\uff0c\u4f46\u73b0\u6709\u7684\u5408\u6210\u73af\u5883\u7f3a\u4e4f\u771f\u5b9e\u611f\u6216\u9700\u8981\u6602\u8d35\u8bbe\u5907\u6355\u83b7\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u5bfc\u81f4\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "EmbodiedSplat\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u548cHabitat-Sim\u6a21\u62df\u5668\uff0c\u5229\u7528iPhone\u6355\u83b7\u7684\u573a\u666f\u91cd\u5efa\u7f51\u683c\uff0c\u5728\u8fd1\u4f3c\u771f\u5b9e\u4e16\u754c\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528EmbodiedSplat\u5fae\u8c03\u7684\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u5bfc\u822a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u57fa\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\uff08HM3D\uff09\u548c\u5408\u6210\u6570\u636e\u96c6\uff08HSSD\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u5347\u4e8620%\u548c40%\u3002", "conclusion": "EmbodiedSplat\u901a\u8fc7\u9ad8\u6548\u7684\u573a\u666f\u91cd\u5efa\u548c\u7b56\u7565\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u73af\u5883\u3002"}}
{"id": "2509.17647", "pdf": "https://arxiv.org/pdf/2509.17647", "abs": "https://arxiv.org/abs/2509.17647", "authors": ["Yu Liu", "Baoxiong Jia", "Ruijie Lu", "Chuyue Gan", "Huayu Chen", "Junfeng Ni", "Song-Chun Zhu", "Siyuan Huang"], "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideoArtGS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u7ed3\u5408\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u548c\u6df7\u5408\u4e2d\u5fc3\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u8282\u548c\u7f51\u683c\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u6784\u5efa\u5173\u8282\u7269\u4f53\u7684\u6570\u5b57\u5b6a\u751f\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u6709\u9650\u7684\u89c6\u89d2\u8f93\u5165\u4e2d\u51c6\u786e\u89e3\u8026\u7269\u4f53\u51e0\u4f55\u4e0e\u90e8\u4ef6\u52a8\u6001\u3002", "method": "\u63d0\u51faVideoArtGS\u65b9\u6cd5\uff0c\u5305\u62ec\u8fd0\u52a8\u5148\u9a8c\u6307\u5bfc\u6d41\u7a0b\u548c\u6df7\u5408\u4e2d\u5fc3\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\uff0c\u4f18\u5316\u5173\u8282\u53c2\u6570\u521d\u59cb\u5316\u548c\u90e8\u4ef6\u8fd0\u52a8\u6355\u6349\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVideoArtGS\u5728\u5173\u8282\u548c\u7f51\u683c\u91cd\u5efa\u4e0a\u7684\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "VideoArtGS\u4e3a\u57fa\u4e8e\u89c6\u9891\u7684\u5173\u8282\u7269\u4f53\u91cd\u5efa\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u521b\u5efa\u5b9e\u7528\u6570\u5b57\u5b6a\u751f\u4f53\u7684\u7a81\u7834\u3002"}}
{"id": "2509.17684", "pdf": "https://arxiv.org/pdf/2509.17684", "abs": "https://arxiv.org/abs/2509.17684", "authors": ["ThankGod Egbe", "Peng Wang", "Zhihao Guo", "Zidong Chen"], "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u81ea\u76d1\u7763\u89c6\u89c9\u9aa8\u5e72DINOv3\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u9884\u8bad\u7ec3\u9aa8\u5e72ResNet-18\u3002", "motivation": "\u7814\u7a76\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6709\u6548\u7684\u611f\u77e5\u524d\u7aef\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5728\u56db\u79cd\u57fa\u51c6\u4efb\u52a1\uff08Push-T, Lift, Can, Square\uff09\u4e2d\uff0c\u6bd4\u8f83\u4e86DINOv3\u548cResNet-18\u5728\u4ece\u5934\u8bad\u7ec3\u3001\u51bb\u7ed3\u548c\u5fae\u8c03\u4e09\u79cd\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "result": "(i) \u5fae\u8c03\u540e\u7684DINOv3\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eResNet-18\uff1b(ii) \u51bb\u7ed3\u7684DINOv3\u4ecd\u7136\u5177\u6709\u7ade\u4e89\u529b\uff1b(iii) \u81ea\u76d1\u7763\u7279\u5f81\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u81ea\u76d1\u7763\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9ad8\u6548\u4e14\u901a\u7528\u7684\u611f\u77e5\u524d\u7aef\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u7684\u6f5c\u529b\u3002"}}
