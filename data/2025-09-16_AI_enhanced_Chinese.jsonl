{"id": "2509.10570", "pdf": "https://arxiv.org/pdf/2509.10570", "abs": "https://arxiv.org/abs/2509.10570", "authors": ["Wei Dai", "Shengen Wu", "Wei Wu", "Zhenhao Wang", "Sisuo Lyu", "Haicheng Liao", "Limin Yu", "Weiping Ding", "Runwei Guan", "Yutao Yue"], "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "Trajectory prediction serves as a critical functionality in autonomous\ndriving, enabling the anticipation of future motion paths for traffic\nparticipants such as vehicles and pedestrians, which is essential for driving\nsafety. Although conventional deep learning methods have improved accuracy,\nthey remain hindered by inherent limitations, including lack of\ninterpretability, heavy reliance on large-scale annotated data, and weak\ngeneralization in long-tail scenarios. The rise of Large Foundation Models\n(LFMs) is transforming the research paradigm of trajectory prediction. This\nsurvey offers a systematic review of recent advances in LFMs, particularly\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for\ntrajectory prediction. By integrating linguistic and scene semantics, LFMs\nfacilitate interpretable contextual reasoning, significantly enhancing\nprediction safety and generalization in complex environments. The article\nhighlights three core methodologies: trajectory-language mapping, multimodal\nfusion, and constraint-based reasoning. It covers prediction tasks for both\nvehicles and pedestrians, evaluation metrics, and dataset analyses. Key\nchallenges such as computational latency, data scarcity, and real-world\nrobustness are discussed, along with future research directions including\nlow-latency inference, causality-aware modeling, and motion foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5927\u578b\u57fa\u7840\u6a21\u578b\uff08LFMs\uff09\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u6838\u5fc3\u65b9\u6cd5\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u56e0\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u957f\u5c3e\u573a\u666f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u800c\u53d7\u9650\uff0cLFMs\u7684\u5174\u8d77\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u6838\u5fc3\u65b9\u6cd5\uff1a\u8f68\u8ff9-\u8bed\u8a00\u6620\u5c04\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u57fa\u4e8e\u7ea6\u675f\u7684\u63a8\u7406\uff0c\u7ed3\u5408\u8bed\u8a00\u548c\u573a\u666f\u8bed\u4e49\u63d0\u5347\u9884\u6d4b\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "LFMs\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\uff08\u5982\u8ba1\u7b97\u5ef6\u8fdf\u3001\u6570\u636e\u7a00\u7f3a\u548c\u9c81\u68d2\u6027\uff09\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3001\u56e0\u679c\u611f\u77e5\u5efa\u6a21\u548c\u8fd0\u52a8\u57fa\u7840\u6a21\u578b\uff09\u3002"}}
{"id": "2509.10692", "pdf": "https://arxiv.org/pdf/2509.10692", "abs": "https://arxiv.org/abs/2509.10692", "authors": ["Giuseppe Silano", "Amr Afifi", "Martin Saska", "Antonio Franchi"], "title": "STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle", "categories": ["cs.RO"], "comment": "39 pages, 13 figures", "summary": "This paper presents a novel approach to motion planning and risk analysis for\nenhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).\nThe proposed method uses Signal Temporal Logic (STL) to encode key mission\nobjectives, such as safety, timing, and human preferences, with a strong focus\non ergonomics and comfort. An optimization framework generates dynamically\nfeasible trajectories while considering the MRAV's physical constraints. Given\nthe nonlinear and non-convex nature of the problem, smooth approximations and\ngradient-based techniques assist in handling the problem's computational\ncomplexity. Additionally, an uncertainty-aware risk analysis is incorporated to\nassess potential deviations from the mission specifications, providing insights\ninto the likelihood of mission success under uncertain conditions. Further, an\nevent-triggered replanning strategy is implemented to respond to unforeseen\nevents and external disturbances. The approach is validated through MATLAB and\nGazebo simulations, using an object handover task in a mock-up environment\ninspired by power line maintenance scenarios. The results highlight the\nmethod's effectiveness in achieving safe, efficient, and resilient human-robot\ncollaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u7684\u65e0\u4eba\u673a\u8fd0\u52a8\u89c4\u5212\u4e0e\u98ce\u9669\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u3001\u65f6\u6548\u6027\u548c\u7b26\u5408\u4eba\u673a\u5de5\u7a0b\u5b66\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528STL\u7f16\u7801\u4efb\u52a1\u76ee\u6807\uff0c\u901a\u8fc7\u4f18\u5316\u6846\u67b6\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u5229\u7528\u5e73\u6ed1\u8fd1\u4f3c\u548c\u68af\u5ea6\u6280\u672f\u5904\u7406\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u540c\u65f6\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u5206\u6790\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u89c4\u5212\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.10730", "pdf": "https://arxiv.org/pdf/2509.10730", "abs": "https://arxiv.org/abs/2509.10730", "authors": ["Yunfan Ren", "Yixi Cai", "Haotian Li", "Nan Chen", "Fangcheng Zhu", "Longji Yin", "Fanze Kong", "Rundong Li", "Fu Zhang"], "title": "A Survey on LiDAR-based Autonomous Aerial Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "This survey offers a comprehensive overview of recent advancements in\nLiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,\nperception, planning, and control strategies. Over the past decade, LiDAR\ntechnology has become a crucial enabler for high-speed, agile, and reliable UAV\nnavigation, especially in GPS-denied environments. The paper begins by\nexamining the evolution of LiDAR sensors, emphasizing their unique advantages\nsuch as high accuracy, long-range depth measurements, and robust performance\nunder various lighting conditions, making them particularly well-suited for UAV\napplications. The integration of LiDAR with UAVs has significantly enhanced\ntheir autonomy, enabling complex missions in diverse and challenging\nenvironments. Subsequently, we explore essential software components, including\nperception technologies for state estimation and mapping, as well as trajectory\nplanning and control methodologies, and discuss their adoption in LiDAR-based\nUAVs. Additionally, we analyze various practical applications of the\nLiDAR-based UAVs, ranging from industrial operations to supporting different\naerial platforms and UAV swarm deployments. The survey concludes by discussing\nexisting challenges and proposing future research directions to advance\nLiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent\ndevelopments, this paper aims to provide a valuable resource for researchers\nand practitioners working to push the boundaries of LiDAR-based UAV systems.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u56de\u987e\u4e86\u57fa\u4e8eLiDAR\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u8bbe\u8ba1\u3001\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7b56\u7565\u3002LiDAR\u6280\u672f\u7684\u53d1\u5c55\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u65e0GPS\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22LiDAR\u6280\u672f\u5982\u4f55\u63d0\u5347\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u548c\u6311\u6218\u6027\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u5206\u6790LiDAR\u4f20\u611f\u5668\u7684\u8fdb\u5316\u53ca\u5176\u4f18\u52bf\uff0c\u7ed3\u5408\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6280\u672f\u7684\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u8ba8\u8bba\u5176\u5728\u65e0\u4eba\u673a\u4e2d\u7684\u5e94\u7528\u3002", "result": "LiDAR\u4e0e\u65e0\u4eba\u673a\u7684\u7ed3\u5408\u589e\u5f3a\u4e86\u5176\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5982\u5de5\u4e1a\u64cd\u4f5c\u548c\u65e0\u4eba\u673a\u96c6\u7fa4\u90e8\u7f72\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u591a\u65e0\u4eba\u673a\u534f\u4f5c\u548c\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4ee5\u6269\u5c55LiDAR\u65e0\u4eba\u673a\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.10735", "pdf": "https://arxiv.org/pdf/2509.10735", "abs": "https://arxiv.org/abs/2509.10735", "authors": ["Mohammad Rafiee Javazm", "Yash Kulkarni", "Jiaqi Xue", "Naruhiko Ikoma", "Farshid Alambeigi"], "title": "Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures", "categories": ["cs.RO"], "comment": null, "summary": "Despite the widespread adoption of endoscopic devices for several cancer\nscreening procedures, manual control of these devices still remains challenging\nfor clinicians, leading to several critical issues such as increased workload,\nfatigue, and distractions. To address these issues, in this paper, we introduce\nthe design and development of an intuitive, modular, and easily installable\nmechatronic framework. This framework includes (i) a novel nested collet-chuck\ngripping mechanism that can readily be integrated and assembled with the\nexisting endoscopic devices and control their bending degrees-of-freedom\n(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF\nof a colonoscope, and (iii) a complementary and intuitive user interface that\nenables simultaneous control of all DoFs during the procedure. To analyze the\ndesign of the proposed mechanisms, we also introduce a mathematical modeling\napproach and a design space for optimal selection of the parameters involved in\nthe design of gripping and feeder mechanisms. Our simulation and experimental\nstudies thoroughly demonstrate the performance of the proposed mathematical\nmodeling and robotic framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u6613\u5b89\u88c5\u7684\u673a\u7535\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5185\u7aa5\u955c\u624b\u52a8\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5305\u62ec\u65b0\u578b\u5939\u6301\u673a\u5236\u3001\u8fdb\u7ed9\u673a\u5236\u548c\u76f4\u89c2\u7528\u6237\u754c\u9762\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u624b\u52a8\u63a7\u5236\u5185\u7aa5\u955c\u8bbe\u5907\u5b58\u5728\u64cd\u4f5c\u56f0\u96be\u3001\u5de5\u4f5c\u91cf\u5927\u548c\u75b2\u52b3\u7b49\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u7535\u6846\u67b6\u4f18\u5316\u64cd\u4f5c\u6d41\u7a0b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5d4c\u5957\u5939\u6301\u673a\u5236\u548c\u8fdb\u7ed9\u673a\u5236\uff0c\u7ed3\u5408\u76f4\u89c2\u7528\u6237\u754c\u9762\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u4f18\u5316\u53c2\u6570\u9009\u62e9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u5b66\u6a21\u578b\u548c\u673a\u5668\u4eba\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5185\u7aa5\u955c\u64cd\u4f5c\u7684\u4fbf\u6377\u6027\u548c\u6548\u7387\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10489", "pdf": "https://arxiv.org/pdf/2509.10489", "abs": "https://arxiv.org/abs/2509.10489", "authors": ["Saptarshi Purkayastha", "Hrishikesh Bhagwat", "Keerthika Sunchu", "Orlando Hoilett", "Eddy Odari", "Reuben Thuo", "Martin Wafula", "Celia Kariuki", "Sherri Bucher"], "title": "Development of AI-integrated infrastructure with biomedical device and mobile app for neonatal vital monitoring during and in between kangaroo care sessions", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "Presented at EMBC 2025, July 14-17, 2025", "summary": "Premature infant mortality remains a critical challenge in low- and\nmiddle-income countries (LMICs), with continuous vital sign monitoring being\nessential for early detection of life-threatening conditions. This paper\npresents an integrated system combining NeoWarm, a novel biomedical device,\nwith NeoRoo, a mobile application, and NeoSmartML, a machine learning\ninfrastructure, to enable comprehensive vital sign monitoring during Kangaroo\nMother Care (KMC). Our power-optimized device achieves 6-6.5 days of continuous\noperation on a single charge, while the mobile application implements an\noffline-first architecture with efficient data synchronization. The optical\ncharacter recognition pipeline demonstrates promising accuracy (F1 scores\n0.78-0.875) for automated vital sign extraction from existing NICU monitors.\nExperimental validation shows the system's feasibility for deployment in\nresource-constrained settings, though further optimization of heart rate and\ntemperature detection, along with the risk classification foundation model is\nneeded.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u65b0\u578b\u751f\u7269\u533b\u5b66\u8bbe\u5907NeoWarm\u3001\u79fb\u52a8\u5e94\u7528NeoRoo\u548c\u673a\u5668\u5b66\u4e60\u57fa\u7840\u8bbe\u65bdNeoSmartML\uff0c\u7528\u4e8e\u5728\u888b\u9f20\u5f0f\u6bcd\u4eb2\u62a4\u7406\u4e2d\u5b9e\u73b0\u5168\u9762\u7684\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u3002", "motivation": "\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u4e2d\uff0c\u65e9\u4ea7\u513f\u6b7b\u4ea1\u7387\u4ecd\u662f\u4e00\u4e2a\u4e25\u5cfb\u95ee\u9898\uff0c\u6301\u7eed\u7684\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u5bf9\u65e9\u671f\u53d1\u73b0\u81f4\u547d\u72b6\u51b5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u529f\u8017\u7684\u8bbe\u5907\u3001\u79bb\u7ebf\u4f18\u5148\u7684\u79fb\u52a8\u5e94\u7528\u53ca\u673a\u5668\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u5b9e\u73b0\u5bf9\u751f\u547d\u4f53\u5f81\u7684\u5168\u9762\u76d1\u6d4b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u5fc3\u7387\u548c\u6e29\u5ea6\u68c0\u6d4b\u53ca\u98ce\u9669\u5206\u7c7b\u6a21\u578b\u8fd8\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "\u8be5\u96c6\u6210\u7cfb\u7edf\u4e3a\u89e3\u51b3\u65e9\u4ea7\u513f\u6b7b\u4ea1\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u6280\u672f\u4f18\u5316\u4ee5\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.10757", "pdf": "https://arxiv.org/pdf/2509.10757", "abs": "https://arxiv.org/abs/2509.10757", "authors": ["Kimia Khabiri", "Parsa Hosseininejad", "Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "FastTrack: GPU-Accelerated Tracking for Visual SLAM", "categories": ["cs.RO", "cs.DC"], "comment": "Accepted for presentation at IROS 2025, preprint", "summary": "The tracking module of a visual-inertial SLAM system processes incoming image\nframes and IMU data to estimate the position of the frame in relation to the\nmap. It is important for the tracking to complete in a timely manner for each\nframe to avoid poor localization or tracking loss. We therefore present a new\napproach which leverages GPU computing power to accelerate time-consuming\ncomponents of tracking in order to improve its performance. These components\ninclude stereo feature matching and local map tracking. We implement our design\ninside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates\nan overall improvement in tracking performance of up to 2.8x on a desktop and\nJetson Xavier NX board in stereo-inertial mode, using the well-known SLAM\ndatasets EuRoC and TUM-VI.", "AI": {"tldr": "\u5229\u7528GPU\u52a0\u901f\u89c6\u89c9-\u60ef\u6027SLAM\u7cfb\u7edf\u7684\u8ddf\u8e2a\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u907f\u514d\u5b9a\u4f4d\u4e0d\u51c6\u6216\u8ddf\u8e2a\u4e22\u5931\uff0c\u9700\u8981\u53ca\u65f6\u5b8c\u6210\u6bcf\u5e27\u8ddf\u8e2a\u3002", "method": "\u5229\u7528GPU\u52a0\u901f\u8ddf\u8e2a\u4e2d\u8017\u65f6\u7684\u7ec4\u4ef6\uff08\u5982\u7acb\u4f53\u7279\u5f81\u5339\u914d\u548c\u5c40\u90e8\u5730\u56fe\u8ddf\u8e2a\uff09\uff0c\u5e76\u5728ORB-SLAM3\u4e2d\u7528CUDA\u5b9e\u73b0\u3002", "result": "\u5728\u684c\u9762\u548cJetson Xavier NX\u677f\u4e0a\uff0c\u7acb\u4f53\u60ef\u6027\u6a21\u5f0f\u4e0b\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2.8\u500d\u3002", "conclusion": "GPU\u52a0\u901f\u6709\u6548\u63d0\u5347\u4e86SLAM\u7cfb\u7edf\u7684\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2509.10490", "pdf": "https://arxiv.org/pdf/2509.10490", "abs": "https://arxiv.org/abs/2509.10490", "authors": ["Yuwen Cao", "Guijun Liu", "Tomoaki Ohtsuki", "Howard H. Yang", "Tony Q. S. Quek"], "title": "Distributed Gossip-GAN for Low-overhead CSI Feedback Training in FDD mMIMO-OFDM Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The deep autoencoder (DAE) framework has turned out to be efficient in\nreducing the channel state information (CSI) feedback overhead in massive\nmultiple-input multipleoutput (mMIMO) systems. However, these DAE approaches\npresented in prior works rely heavily on large-scale data collected through the\nbase station (BS) for model training, thus rendering excessive bandwidth usage\nand data privacy issues, particularly for mMIMO systems. When considering\nusers' mobility and encountering new channel environments, the existing CSI\nfeedback models may often need to be retrained. Returning back to previous\nenvironments, however, will make these models perform poorly and face the risk\nof catastrophic forgetting. To solve the above challenging problems, we propose\na novel gossiping generative adversarial network (Gossip-GAN)-aided CSI\nfeedback training framework. Notably, Gossip-GAN enables the CSI feedback\ntraining with low-overhead while preserving users' privacy. Specially, each\nuser collects a small amount of data to train a GAN model. Meanwhile, a fully\ndistributed gossip-learning strategy is exploited to avoid model overfitting,\nand to accelerate the model training as well. Simulation results demonstrate\nthat Gossip-GAN can i) achieve a similar CSI feedback accuracy as centralized\ntraining with real-world datasets, ii) address catastrophic forgetting\nchallenges in mobile scenarios, and iii) greatly reduce the uplink bandwidth\nusage. Besides, our results show that the proposed approach possesses an\ninherent robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGossip-GAN\u7684\u4f4e\u5f00\u9500\u3001\u4fdd\u62a4\u9690\u79c1\u7684CSI\u53cd\u9988\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u4f9d\u8d56\u3001\u9690\u79c1\u95ee\u9898\u548c\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u707e\u96be\u6027\u9057\u5fd8\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684CSI\u53cd\u9988\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5e26\u5bbd\u5360\u7528\u9ad8\u548c\u9690\u79c1\u95ee\u9898\uff0c\u4e14\u5728\u79fb\u52a8\u573a\u666f\u548c\u65b0\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGossip-GAN\u6846\u67b6\uff0c\u7528\u6237\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u8bad\u7ec3GAN\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5206\u5e03\u5f0fgossip\u5b66\u4e60\u7b56\u7565\u907f\u514d\u8fc7\u62df\u5408\u5e76\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cGossip-GAN\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u76f8\u4f3c\u7684\u7cbe\u5ea6\uff0c\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e0a\u884c\u5e26\u5bbd\u5360\u7528\u3002", "conclusion": "Gossip-GAN\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684CSI\u53cd\u9988\u8bad\u7ec3\u6846\u67b6\uff0c\u540c\u65f6\u5728\u79fb\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.10771", "pdf": "https://arxiv.org/pdf/2509.10771", "abs": "https://arxiv.org/abs/2509.10771", "authors": ["Clemens Schwarke", "Mayank Mittal", "Nikita Rudin", "David Hoeller", "Marco Hutter"], "title": "RSL-RL: A Learning Library for Robotics Research", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "RSL-RL is an open-source Reinforcement Learning library tailored to the\nspecific needs of the robotics community. Unlike broad general-purpose\nframeworks, its design philosophy prioritizes a compact and easily modifiable\ncodebase, allowing researchers to adapt and extend algorithms with minimal\noverhead. The library focuses on algorithms most widely adopted in robotics,\ntogether with auxiliary techniques that address robotics-specific challenges.\nOptimized for GPU-only training, RSL-RL achieves high-throughput performance in\nlarge-scale simulation environments. Its effectiveness has been validated in\nboth simulation benchmarks and in real-world robotic experiments, demonstrating\nits utility as a lightweight, extensible, and practical framework to develop\nlearning-based robotic controllers. The library is open-sourced at:\nhttps://github.com/leggedrobotics/rsl_rl.", "AI": {"tldr": "RSL-RL\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u4eba\u793e\u533a\u8bbe\u8ba1\u7684\u5f00\u6e90\u5f3a\u5316\u5b66\u4e60\u5e93\uff0c\u5f3a\u8c03\u7d27\u51d1\u6027\u548c\u6613\u4fee\u6539\u6027\uff0c\u652f\u6301GPU\u9ad8\u6548\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u3002\n", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u548c\u6269\u5c55\u7b97\u6cd5\uff0c\u6ee1\u8db3\u673a\u5668\u4eba\u7279\u5b9a\u7684\u6311\u6218\u9700\u6c42\u3002\n", "method": "\u8bbe\u8ba1\u4e86\u7d27\u51d1\u4e14\u6613\u4fee\u6539\u7684\u4ee3\u7801\u5e93\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u91c7\u7528\u7684\u7b97\u6cd5\u53ca\u8f85\u52a9\u6280\u672f\uff0c\u5e76\u4f18\u5316\u4e86GPU\u8bad\u7ec3\u6027\u80fd\u3002\n", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5b9e\u7528\u6846\u67b6\u7684\u6f5c\u529b\u3002\n", "conclusion": "RSL-RL\u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7814\u7a76\u7684\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5e93\u3002\n"}}
{"id": "2509.10491", "pdf": "https://arxiv.org/pdf/2509.10491", "abs": "https://arxiv.org/abs/2509.10491", "authors": ["Vitalii Bondar", "Serhii Semenov", "Vira Babenko", "Dmytro Holovniak"], "title": "FlowECG: Using Flow Matching to Create a More Efficient ECG Signal Generator", "categories": ["eess.SP", "cs.LG"], "comment": "8 pages, 2 figures, 1 table, reviewed version will be published in\n  \"Sensors, Devices and Systems 2025 Proceedings\" (Springer's Lecture Notes in\n  Electrical Engineering)", "summary": "Synthetic electrocardiogram generation serves medical AI applications\nrequiring privacy-preserving data sharing and training dataset augmentation.\nCurrent diffusion-based methods achieve high generation quality but require\nhundreds of neural network evaluations during sampling, creating computational\nbottlenecks for clinical deployment. We propose FlowECG, a flow matching\napproach that adapts the SSSD-ECG architecture by replacing the iterative\ndiffusion process with continuous flow dynamics. Flow matching learns direct\ntransport paths from noise to data distributions through ordinary differential\nequation solving. We evaluate our method on the PTB-XL dataset using Dynamic\nTime Warping, Wasserstein distance, Maximum Mean Discrepancy, and spectral\nsimilarity metrics. FlowECG matches SSSD-ECG performance at 200 neural function\nevaluations, outperforming the baseline on three metrics. The key finding shows\nthat FlowECG maintains generation quality with substantially fewer sampling\nsteps, achieving comparable results with 10-25 evaluations compared to 200 for\ndiffusion methods. This efficiency improvement reduces computational\nrequirements by an order of magnitude while preserving physiologically\nrealistic 12-lead ECG characteristics. The approach enables practical\ndeployment in resource-limited clinical settings where real-time generation or\nlarge-scale synthetic data creation is needed.", "AI": {"tldr": "FlowECG\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u5408\u6210\u5fc3\u7535\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6269\u6563\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u9002\u5408\u4e34\u5e8a\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6269\u6563\u65b9\u6cd5\u5728\u5408\u6210\u5fc3\u7535\u56fe\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u4ee5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5171\u4eab\u548c\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u65b9\u6cd5\u66ff\u4ee3\u6269\u6563\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5e38\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u76f4\u63a5\u4ece\u566a\u58f0\u5206\u5e03\u5230\u6570\u636e\u5206\u5e03\u7684\u4f20\u8f93\u8def\u5f84\u3002", "result": "FlowECG\u6027\u80fd\u4e0eSSSD-ECG\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0810-25\u6b21\u8bc4\u4f30vs.200\u6b21\uff09\u3002", "conclusion": "FlowECG\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2509.10796", "pdf": "https://arxiv.org/pdf/2509.10796", "abs": "https://arxiv.org/abs/2509.10796", "authors": ["Hanjing Ye", "Weixi Situ", "Jianwei Peng", "Yu Zhan", "Bingyi Xia", "Kuanqi Cai", "Hong Zhang"], "title": "Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following", "categories": ["cs.RO"], "comment": "TBD. All code, data, and deployment scripts are publicly available at\n  https://follow-bench.github.io/", "summary": "Robot person following (RPF) -- mobile robots that follow and assist a\nspecific person -- has emerging applications in personal assistance, security\npatrols, eldercare, and logistics. To be effective, such robots must follow the\ntarget while ensuring safety and comfort for both the target and surrounding\npeople. In this work, we present the first end-to-end study of RPF, which (i)\nsurveys representative scenarios, motion-planning methods, and evaluation\nmetrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a\nunified benchmark simulating diverse scenarios, including various target\ntrajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)\nre-implements six popular RPF planners, ensuring that both safety and comfort\nare systematically considered. Moreover, we evaluate the two highest-performing\nplanners from our benchmark on a differential-drive robot to provide insights\ninto real-world deployment. Extensive simulation and real-world experiments\nprovide quantitative insights into the safety-comfort trade-offs of existing\nplanners, while revealing open challenges and future research directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u8ddf\u968f\uff08RPF\uff09\u7684\u7aef\u5230\u7aef\u7814\u7a76\uff0c\u5305\u62ec\u573a\u666f\u8c03\u67e5\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "motivation": "RPF\u5728\u4e2a\u4eba\u52a9\u7406\u3001\u5b89\u9632\u5de1\u903b\u3001\u8001\u5e74\u62a4\u7406\u548c\u7269\u6d41\u7b49\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u4f46\u9700\u517c\u987e\u76ee\u6807\u8ddf\u968f\u7684\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51faFollow-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u6837\u5316\u573a\u666f\uff0c\u5e76\u91cd\u65b0\u5b9e\u73b0\u4e86\u516d\u79cd\u6d41\u884cRPF\u89c4\u5212\u5668\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u89c4\u5212\u5668\u5728\u5b89\u5168\u6027\u4e0e\u8212\u9002\u6027\u95f4\u7684\u6743\u8861\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\u4e86RPF\u9886\u57df\u7684\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2509.10666", "pdf": "https://arxiv.org/pdf/2509.10666", "abs": "https://arxiv.org/abs/2509.10666", "authors": ["Chongjun Ouyang", "Hao Jiang", "Zhaolin Wang", "Yuanwei Liu", "Zhiguo Ding"], "title": "Uplink and Downlink Communications in Segmented Waveguide-Enabled Pinching-Antenna Systems (SWANs)", "categories": ["eess.SP"], "comment": "Submitted to IEEE journal", "summary": "A segmented waveguide-enabled pinching-antenna system (SWAN) is proposed, in\nwhich a segmented waveguide composed of multiple short dielectric waveguide\nsegments is employed to radiate or receive signals through the pinching\nantennas (PAs) deployed on each segment. Based on this architecture, three\npractical operating protocols are proposed: segment selection (SS), segment\naggregation (SA), and segment multiplexing (SM). For uplink SWAN\ncommunications, where one PA is activated per segment, the segmented structure\neliminates the inter-antenna radiation effect, i.e., signals captured by one PA\nmay re-radiate through other PAs along the same waveguide. This yields a\ntractable and physically consistent uplink signal model for a multi-PA\npinching-antenna system (PASS), which has not been established for conventional\nPASS using a single long waveguide. Building on this model, PA placement\nalgorithms are proposed to maximize the uplink signal-to-noise ratio (SNR).\nClosed-form expressions for the received SNR under the three protocols are\nderived, and the corresponding scaling laws with respect to the number of\nsegments are analyzed. It is proven that the segmented architecture reduces\nboth the average PA-to-user distance and the PA-to-feed distance, thereby\nmitigating both large-scale path loss and in-waveguide propagation loss. These\nresults are extended to downlink SWAN communications, where multiple PAs are\nactivated per segment, and PA placement methods are proposed to maximize the\ndownlink received SNR under the three protocols. Numerical results demonstrate\nthat: \\romannumeral1) among the three protocols, SM achieves the best\nperformance, followed by SA and then SS; and \\romannumeral2) for all protocols,\nthe proposed SWAN achieves a higher SNR than conventional PASS with a single\nlong waveguide in both uplink and downlink scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u6bb5\u6ce2\u5bfc\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\uff08SWAN\uff09\uff0c\u901a\u8fc7\u5206\u6bb5\u6ce2\u5bfc\u548c\u591a\u5929\u7ebf\u534f\u8bae\u63d0\u5347\u4fe1\u53f7\u4f20\u8f93\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u6ce2\u5bfc\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u6ce2\u5bfc\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e2d\u4fe1\u53f7\u91cd\u8f90\u5c04\u548c\u4f20\u64ad\u635f\u8017\u95ee\u9898\uff0c\u63d0\u5347\u4e0a\u884c\u548c\u4e0b\u884c\u7684\u4fe1\u53f7\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u6ce2\u5bfc\u548c\u591a\u5929\u7ebf\u534f\u8bae\uff08SS\u3001SA\u3001SM\uff09\uff0c\u63d0\u51fa\u5929\u7ebf\u5e03\u5c40\u7b97\u6cd5\uff0c\u4f18\u5316\u4fe1\u566a\u6bd4\uff08SNR\uff09\u3002", "result": "\u5206\u6bb5\u7ed3\u6784\u51cf\u5c11\u4e86\u4f20\u64ad\u635f\u8017\u548c\u8def\u5f84\u635f\u8017\uff0cSM\u534f\u8bae\u6027\u80fd\u6700\u4f18\uff0cSWAN\u7cfb\u7edfSNR\u9ad8\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "SWAN\u7cfb\u7edf\u901a\u8fc7\u5206\u6bb5\u8bbe\u8ba1\u548c\u591a\u534f\u8bae\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4e0a\u4e0b\u884c\u573a\u666f\u3002"}}
{"id": "2509.10862", "pdf": "https://arxiv.org/pdf/2509.10862", "abs": "https://arxiv.org/abs/2509.10862", "authors": ["Temma Suzuki", "Kento Kawaharazuka", "Kei Okada"], "title": "A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2025, website -\n  https://tenrobo18.github.io/wiretester-humanoids2025-webpage/", "summary": "Compared with gears and linkages, wires constitute a lightweight,\nlow-friction transmission mechanism. However, because wires are flexible\nmaterials, they tend to introduce large modeling errors, and their adoption in\nindustrial and research robots remains limited.In this study, we built a\nUniversal Wire Testing Machine that enables measurement and adjustment of wire\ncharacteristics to improve the performance of wire-driven mechanisms. Using\nthis testing machine, we carried out removal of initial wire stretch,\nmeasurement of tension transmission efficiency for eight different diameters of\npassive pulleys, and measurement of the dynamic behavior of variable-length\nwires. Finally, we applied the data obtained from this testing machine to the\nforce control of an actual wire-driven robot, reducing the end-effector force\nerror.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u7ebf\u7f06\u6d4b\u8bd5\u673a\uff0c\u7528\u4e8e\u6d4b\u91cf\u548c\u8c03\u6574\u7ebf\u7f06\u7279\u6027\uff0c\u4ee5\u63d0\u9ad8\u7ebf\u7f06\u9a71\u52a8\u673a\u6784\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u7684\u529b\u63a7\u5236\u3002", "motivation": "\u5c3d\u7ba1\u7ebf\u7f06\u662f\u4e00\u79cd\u8f7b\u91cf\u3001\u4f4e\u6469\u64e6\u7684\u4f20\u52a8\u673a\u5236\uff0c\u4f46\u5176\u67d4\u6027\u7279\u6027\u5bfc\u81f4\u5efa\u6a21\u8bef\u5dee\u8f83\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u5de5\u4e1a\u548c\u79d1\u7814\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u53f0\u901a\u7528\u7ebf\u7f06\u6d4b\u8bd5\u673a\uff0c\u7528\u4e8e\u53bb\u9664\u521d\u59cb\u7ebf\u7f06\u62c9\u4f38\u3001\u6d4b\u91cf\u4e0d\u540c\u76f4\u5f84\u88ab\u52a8\u6ed1\u8f6e\u7684\u5f20\u7d27\u4f20\u8f93\u6548\u7387\u4ee5\u53ca\u6d4b\u91cf\u53ef\u53d8\u957f\u5ea6\u7ebf\u7f06\u7684\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u6d4b\u8bd5\u673a\u83b7\u5f97\u7684\u6570\u636e\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u7ebf\u7f06\u9a71\u52a8\u673a\u5668\u4eba\u7684\u529b\u63a7\u5236\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u672b\u7aef\u6267\u884c\u5668\u7684\u529b\u8bef\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u548c\u8c03\u6574\u7ebf\u7f06\u7279\u6027\uff0c\u53ef\u4ee5\u63d0\u5347\u7ebf\u7f06\u9a71\u52a8\u673a\u6784\u7684\u6027\u80fd\uff0c\u4e3a\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.10752", "pdf": "https://arxiv.org/pdf/2509.10752", "abs": "https://arxiv.org/abs/2509.10752", "authors": ["Minseok Kim", "Masato Yomoda", "Minghe Mao", "Nobuaki Kuno", "Koshiro Kitao", "Satoshi Suyama"], "title": "Quasi-Deterministic Modeling of Sub-THz Band Access Channels in Street Canyon Environments", "categories": ["eess.SP"], "comment": null, "summary": "Sub-terahertz (sub-THz) frequencies (100--300 GHz) are expected to play a key\nrole in beyond-5G and 6G mobile networks. However, their quasi-optical\npropagation characteristics require new channel models beyond sub-100 GHz\nextrapolations. This paper presents an extensive double-directional (D-D)\nchannel measurement campaign conducted in an outdoor street-canyon environment\nat 154 GHz and 300 GHz under both line-of-sight (LoS) and non-line-of-sight\n(NLoS) conditions using an in-house-developed channel sounder. Based on these\nmeasurements, clustering with merged datasets across the two frequencies\nenables comparative analyses that identify both common and distinct multipath\nclusters, as well as the frequency dependence of cluster-level characteristics.\nA quasi-deterministic (QD) channel model is then proposed, combining\ndeterministic components, such as LoS and single-bounce reflections from side\nwalls, with random components. Large-scale parameters (path loss, delay spread,\nangular spread, and Rician K-factor) are also evaluated. These results provide\nvaluable insights into sub-THz propagation in urban street canyons and\ncontribute toward the development of accurate, channel models for future 6G\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u53cc\u65b9\u5411\u4fe1\u9053\u6d4b\u91cf\uff0c\u7814\u7a76\u4e86\u57ce\u5e02\u8857\u9053\u5ce1\u8c37\u4e2d154 GHz\u548c300 GHz\u7684\u4fe1\u9053\u4f20\u64ad\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u786e\u5b9a\u6027\u548c\u968f\u673a\u5206\u91cf\u7684\u51c6\u786e\u5b9a\u6027\u4fe1\u9053\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u4e9a\u592a\u8d6b\u5179\u9891\u6bb5\uff08100-300 GHz\uff09\u57286G\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u7279\u6027\uff0c\u586b\u8865\u73b0\u6709\u4fe1\u9053\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "method": "\u5728\u6237\u5916\u8857\u9053\u5ce1\u8c37\u73af\u5883\u4e2d\u8fdb\u884c\u53cc\u65b9\u5411\u4fe1\u9053\u6d4b\u91cf\uff0c\u5206\u6790LOS\u548cNLOS\u6761\u4ef6\u4e0b\u7684\u4fe1\u9053\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u51c6\u786e\u5b9a\u6027\u4fe1\u9053\u6a21\u578b\u3002", "result": "\u63ed\u793a\u4e86\u591a\u5f84\u7c07\u7684\u5171\u540c\u4e0e\u5dee\u5f02\u7279\u6027\u53ca\u5176\u9891\u7387\u4f9d\u8d56\u6027\uff0c\u63d0\u4f9b\u4e86\u8def\u5f84\u635f\u8017\u3001\u5ef6\u8fdf\u6269\u5c55\u7b49\u5927\u5c3a\u5ea6\u53c2\u6570\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u672a\u67656G\u7cfb\u7edf\u7684\u51c6\u786e\u4fe1\u9053\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.10884", "pdf": "https://arxiv.org/pdf/2509.10884", "abs": "https://arxiv.org/abs/2509.10884", "authors": ["Qingxiang Liu", "Ting Huang", "Zeyu Zhang", "Hao Tang"], "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.", "AI": {"tldr": "Nav-R1\u662f\u4e00\u79cd\u7edf\u4e00\u63a8\u7406\u7684\u5d4c\u5165\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7Nav-CoT-110K\u6570\u636e\u96c6\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u4efb\u52a1\u7684\u63a8\u7406\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u67423D\u73af\u5883\u4e2d\u63a8\u7406\u4e0d\u4e00\u81f4\u548c\u5b9e\u65f6\u5bfc\u822a\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21Nav-CoT-110K\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u91c7\u7528Fast-in-Slow\u63a8\u7406\u8303\u5f0f\u3002", "result": "\u5728\u5d4c\u5165\u5f0fAI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53478%\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "Nav-R1\u5728\u63a8\u7406\u548c\u5bfc\u822a\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.10770", "pdf": "https://arxiv.org/pdf/2509.10770", "abs": "https://arxiv.org/abs/2509.10770", "authors": ["Lei Lyu", "Urbashi Mitra"], "title": "Hybrid Atomic Norm Sparse/Diffuse Channel Estimation", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, the hybrid sparse/diffuse (HSD) channel model in frequency\ndomain is proposed. Based on the structural analysis on the resolvable paths\nand diffuse scattering statistics in the channel, the Hybrid\nAtomic-Least-Squares (HALS) algorithm is designed to estimate sparse/diffuse\ncomponents with a combined atomic and l2 regularization. A theoretical analysis\nis conducted on the Lagrangian dual problem and the conditions needed to be\nsatisfied by primal and dual solutions are provided. This analysis, in turn,\nsuggests an algorithm for optimal frequency support estimation. Debiased\nmethods for improved channel estimation are provided. Given differing amounts\nof side information, performance bounds are derived in terms of a genie-aided\nestimator and constrained Cramer-Rao lower bounds (CRLB). Numerical results via\nsimulations on synthetic data as well as real experimental data validate the\nefficacy of the proposed method. There are clear tradeoffs with respect to the\nproperties of the channel with respect to performance: sparsity of specular\npaths and relative energy of diffuse components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u57df\u6df7\u5408\u7a00\u758f/\u6269\u6563(HSD)\u4fe1\u9053\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86HALS\u7b97\u6cd5\u7528\u4e8e\u7a00\u758f/\u6269\u6563\u6210\u5206\u7684\u4f30\u8ba1\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6709\u6548\u4f30\u8ba1\u4fe1\u9053\u4e2d\u7684\u7a00\u758f\u548c\u6269\u6563\u6210\u5206\uff0c\u4ee5\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u6df7\u5408\u539f\u5b50\u6700\u5c0f\u4e8c\u4e58(HALS)\u7b97\u6cd5\uff0c\u7ed3\u5408\u539f\u5b50\u548cl2\u6b63\u5219\u5316\u8fdb\u884c\u7a00\u758f/\u6269\u6563\u6210\u5206\u4f30\u8ba1\uff0c\u5e76\u5206\u6790\u4e86\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u95ee\u9898\u53ca\u9891\u7387\u652f\u6301\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4fe1\u9053\u6027\u80fd\u4e0e\u7a00\u758f\u8def\u5f84\u548c\u6269\u6563\u6210\u5206\u80fd\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fe1\u9053\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u7a00\u758f\u8def\u5f84\u548c\u6269\u6563\u6210\u5206\u7684\u80fd\u91cf\u6709\u660e\u786e\u6743\u8861\u3002"}}
{"id": "2509.10888", "pdf": "https://arxiv.org/pdf/2509.10888", "abs": "https://arxiv.org/abs/2509.10888", "authors": ["Weijie Liu", "Ziyi Qiu", "Shihang Wang", "Deqing Mei", "Yancheng Wang"], "title": "Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics", "categories": ["cs.RO"], "comment": "6 pages, 9 figures(Accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, 2025)", "summary": "Human-like embodied tactile perception is crucial for the next-generation\nintelligent robotics. Achieving large-area, full-body soft coverage with high\nsensitivity and rapid response, akin to human skin, remains a formidable\nchallenge due to critical bottlenecks in encoding efficiency and wiring\ncomplexity in existing flexible tactile sensors, thus significantly hinder the\nscalability and real-time performance required for human skin-level tactile\nperception. Herein, we present a new architecture employing code division\nmultiple access-inspired orthogonal digital encoding to overcome these\nchallenges. Our decentralized encoding strategy transforms conventional serial\nsignal transmission by enabling parallel superposition of energy-orthogonal\nbase codes from distributed sensing nodes, drastically reducing wiring\nrequirements and increasing data throughput. We implemented and validated this\nstrategy with off-the-shelf 16-node sensing array to reconstruct the pressure\ndistribution, achieving a temporal resolution of 12.8 ms using only a single\ntransmission wire. Crucially, the architecture can maintain sub-20ms latency\nacross orders-of-magnitude variations in node number (to thousands of nodes).\nBy fundamentally redefining signal encoding paradigms in soft electronics, this\nwork opens new frontiers in developing scalable embodied intelligent systems\nwith human-like sensory capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u6570\u5b57\u7f16\u7801\u7684\u65b0\u67b6\u6784\uff0c\u89e3\u51b3\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u7f16\u7801\u6548\u7387\u548c\u5e03\u7ebf\u590d\u6742\u6027\u74f6\u9888\uff0c\u5b9e\u73b0\u9ad8\u7075\u654f\u5ea6\u548c\u5feb\u901f\u54cd\u5e94\u7684\u5927\u9762\u79ef\u8986\u76d6\u3002", "motivation": "\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u76ae\u80a4\u7684\u5927\u9762\u79ef\u3001\u9ad8\u7075\u654f\u5ea6\u3001\u5feb\u901f\u54cd\u5e94\u7684\u89e6\u89c9\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u56e0\u7f16\u7801\u6548\u7387\u548c\u5e03\u7ebf\u590d\u6742\u6027\u95ee\u9898\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u7801\u5206\u591a\u5740\u542f\u53d1\u7684\u6b63\u4ea4\u6570\u5b57\u7f16\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f20\u611f\u8282\u70b9\u7684\u80fd\u91cf\u6b63\u4ea4\u57fa\u7801\u5e76\u884c\u53e0\u52a0\uff0c\u663e\u8457\u51cf\u5c11\u5e03\u7ebf\u9700\u6c42\u5e76\u63d0\u9ad8\u6570\u636e\u541e\u5410\u91cf\u3002", "result": "\u572816\u8282\u70b9\u4f20\u611f\u9635\u5217\u4e2d\u9a8c\u8bc1\uff0c\u4ec5\u7528\u5355\u6839\u4f20\u8f93\u7ebf\u5b9e\u73b012.8\u6beb\u79d2\u7684\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u4e14\u8282\u70b9\u6570\u53d8\u5316\u65f6\u4ecd\u80fd\u4fdd\u6301\u4f4e\u4e8e20\u6beb\u79d2\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u5f00\u53d1\u5177\u6709\u4eba\u7c7b\u611f\u5b98\u80fd\u529b\u7684\u53ef\u6269\u5c55\u667a\u80fd\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.10831", "pdf": "https://arxiv.org/pdf/2509.10831", "abs": "https://arxiv.org/abs/2509.10831", "authors": ["Maya Mekel", "Vered Karp", "Satish Mulleti", "Alejandro Cohen"], "title": "Self-Calibrating Integrate-and-Fire Time Encoding Machine", "categories": ["eess.SP"], "comment": "7 pages, 3 figures", "summary": "In this paper, we introduce a novel self-calibrating integrate-and-fire time\nencoding machine (S-IF-TEM) that enables simultaneous parameter estimation and\nsignal reconstruction during sampling, thereby effectively mitigating mismatch\neffects. The proposed framework is developed over a new practical IF-TEM\n(P-IF-TEM) setting, which extends classical models by incorporating device\nmismatches and imperfections that can otherwise lead to significant\nreconstruction errors. Unlike existing IF-TEM settings, P-IF-TEM accounts for\nscenarios where (i) system parameters are inaccurately known and may vary over\ntime, (ii) the integrator discharge time after firings can vary, and (iii) the\nsampler may operate in its nonlinear region under large input dynamic ranges.\nFor this practical model, we derive sampling rate bounds and reconstruction\nconditions that ensure perfect recovery. Analytical results establish the\nconditions for perfect reconstruction under self-calibration, and evaluation\nstudies demonstrate substantial improvements - exceeding 59dB - highlighting\nthe effectiveness of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u6821\u51c6\u79ef\u5206-\u653e\u7535\u65f6\u95f4\u7f16\u7801\u673a\uff08S-IF-TEM\uff09\uff0c\u80fd\u591f\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u548c\u4fe1\u53f7\u91cd\u5efa\uff0c\u6709\u6548\u51cf\u5c11\u5931\u914d\u6548\u5e94\u3002", "motivation": "\u73b0\u6709IF-TEM\u6a21\u578b\u672a\u8003\u8651\u8bbe\u5907\u5931\u914d\u548c\u975e\u7ebf\u6027\u6548\u5e94\uff0c\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\u5927\u3002\u8bba\u6587\u63d0\u51faP-IF-TEM\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86P-IF-TEM\u6a21\u578b\uff0c\u6269\u5c55\u7ecf\u5178\u6a21\u578b\uff0c\u6db5\u76d6\u53c2\u6570\u672a\u77e5/\u65f6\u53d8\u3001\u653e\u7535\u65f6\u95f4\u53ef\u53d8\u53ca\u975e\u7ebf\u6027\u5de5\u4f5c\u533a\u60c5\u51b5\u3002\u63a8\u5bfc\u4e86\u91c7\u6837\u7387\u548c\u91cd\u5efa\u6761\u4ef6\u3002", "result": "\u81ea\u6821\u51c6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5b8c\u7f8e\u91cd\u5efa\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc759dB\u3002", "conclusion": "S-IF-TEM\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u53f7\u91cd\u5efa\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2509.10948", "pdf": "https://arxiv.org/pdf/2509.10948", "abs": "https://arxiv.org/abs/2509.10948", "authors": ["Navid Aftabi", "Philip Samaha", "Jin Ma", "Long Cheng", "Ramy Harik", "Dan Li"], "title": "ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations", "categories": ["cs.RO", "cs.AI", "cs.CR", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Industrial robotic systems are central to automating smart manufacturing\noperations. Connected and automated factories face growing cybersecurity risks\nthat can potentially cause interruptions and damages to physical operations.\nAmong these attacks, data-integrity attacks often involve sophisticated\nexploitation of vulnerabilities that enable an attacker to access and\nmanipulate the operational data and are hence difficult to detect with only\nexisting intrusion detection or model-based detection. This paper addresses the\nchallenges in utilizing existing side-channels to detect data-integrity attacks\nin robotic manufacturing processes by developing an online detection framework,\nViSTR-GP, that cross-checks encoder-reported measurements against a\nvision-based estimate from an overhead camera outside the controller's\nauthority. In this framework, a one-time interactive segmentation initializes\nSAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate\nmaps each mask to measurements, while a matrix-variate Gaussian process models\nnominal residuals, capturing temporal structure and cross-joint correlations. A\nframe-wise test statistic derived from the predictive distribution provides an\nonline detector with interpretable thresholds. We validate the framework on a\nreal-world robotic testbed with synchronized video frame and encoder data,\ncollecting multiple nominal cycles and constructing replay attack scenarios\nwith graded end-effector deviations. Results on the testbed indicate that the\nproposed framework recovers joint angles accurately and detects data-integrity\nattacks earlier with more frequent alarms than all baselines. These\nimprovements are most evident in the most subtle attacks. These results show\nthat plants can detect data-integrity attacks by adding an independent physical\nchannel, bypassing the controller's authority, without needing complex\ninstrumentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aViSTR-GP\u7684\u5728\u7ebf\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5916\u90e8\u6444\u50cf\u5934\u548c\u7f16\u7801\u5668\u6570\u636e\u7684\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u68c0\u6d4b\u673a\u5668\u4eba\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u667a\u80fd\u5236\u9020\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9762\u4e34\u7684\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\u96be\u4ee5\u901a\u8fc7\u4f20\u7edf\u5165\u4fb5\u68c0\u6d4b\u6216\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u53d1\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86ViSTR-GP\u6846\u67b6\uff0c\u7ed3\u5408SAM-Track\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u3001\u4f4e\u79e9\u5f20\u91cf\u56de\u5f52\u548c\u77e9\u9635\u53d8\u5f02\u9ad8\u65af\u8fc7\u7a0b\uff0c\u751f\u6210\u5728\u7ebf\u68c0\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cViSTR-GP\u80fd\u66f4\u65e9\u4e14\u66f4\u9891\u7e41\u5730\u68c0\u6d4b\u5230\u653b\u51fb\uff0c\u5c24\u5176\u5728\u7ec6\u5fae\u653b\u51fb\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u6dfb\u52a0\u72ec\u7acb\u7269\u7406\u901a\u9053\uff0c\u5de5\u5382\u65e0\u9700\u590d\u6742\u4eea\u5668\u5373\u53ef\u6709\u6548\u68c0\u6d4b\u6570\u636e\u5b8c\u6574\u6027\u653b\u51fb\u3002"}}
{"id": "2509.10834", "pdf": "https://arxiv.org/pdf/2509.10834", "abs": "https://arxiv.org/abs/2509.10834", "authors": ["Xiao Liang", "Zhen Qin", "Zhihui Zhu", "Shuang Li"], "title": "Landscape Analysis of Simultaneous Blind Deconvolution and Phase Retrieval via Structured Low-Rank Tensor Recovery", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "17 pages, 18 figures", "summary": "This paper presents a geometric analysis of the simultaneous blind\ndeconvolution and phase retrieval (BDPR) problem via a structured low-rank\ntensor recovery framework. Due to the highly complicated structure of the\nassociated sensing tensor, directly characterizing its optimization landscape\nis intractable. To address this, we introduce a tensor sensing problem as a\ntractable surrogate that preserves the essential structural features of the\ntarget low-rank tensor while enabling rigorous theoretical analysis. As a first\nstep toward understanding this surrogate model, we study the corresponding\npopulation risk, which captures key aspects of the underlying low-rank tensor\nstructure. We characterize the global landscape of the population risk on the\nunit sphere and show that Riemannian gradient descent (RGD) converges linearly\nunder mild conditions. We then extend the analysis to the tensor sensing\nproblem, establishing local geometric properties, proving convergence\nguarantees for RGD, and quantifying robustness under measurement noise. Our\ntheoretical results are further supported by extensive numerical experiments.\nThese findings offer foundational insights into the optimization landscape of\nthe structured low-rank tensor recovery problem, which equivalently\ncharacterizes the original BDPR problem, thereby providing principled guidance\nfor solving the original BDPR problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ed3\u6784\u5316\u4f4e\u79e9\u5f20\u91cf\u6062\u590d\u6846\u67b6\uff0c\u5bf9\u540c\u65f6\u76f2\u53bb\u5377\u79ef\u548c\u76f8\u4f4d\u6062\u590d\uff08BDPR\uff09\u95ee\u9898\u8fdb\u884c\u4e86\u51e0\u4f55\u5206\u6790\u3002\u4e3a\u4e86\u907f\u514d\u76f4\u63a5\u5904\u7406\u590d\u6742\u7684\u9ad8\u7ef4\u5f20\u91cf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u6613\u5206\u6790\u7684\u5f20\u91cf\u611f\u77e5\u95ee\u9898\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u5176\u5168\u5c40\u4f18\u5316\u7279\u6027\uff0c\u5e76\u8bc1\u660e\u4e86Riemannian\u68af\u5ea6\u4e0b\u964d\u7684\u7ebf\u6027\u6536\u655b\u6027\u3002", "motivation": "\u7814\u7a76BDPR\u95ee\u9898\u65f6\uff0c\u76f4\u63a5\u5206\u6790\u9ad8\u7ef4\u5f20\u91cf\u7684\u4f18\u5316\u666f\u89c2\u5341\u5206\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u66f4\u6613\u5206\u6790\u7684\u5f20\u91cf\u611f\u77e5\u95ee\u9898\uff0c\u4fdd\u7559\u539f\u95ee\u9898\u7684\u6838\u5fc3\u7ed3\u6784\u7279\u5f81\uff0c\u4ece\u800c\u4e3aBDPR\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5f20\u91cf\u611f\u77e5\u95ee\u9898\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5168\u5c40\u4f18\u5316\u666f\u89c2\u548cRiemannian\u68af\u5ea6\u4e0b\u964d\uff08RGD\uff09\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5355\u4f4d\u7403\u4e0a\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u5e76\u5c06\u5176\u7ed3\u679c\u6269\u5c55\u5230\u6d4b\u91cf\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cRGD\u5728\u5f20\u91cf\u611f\u77e5\u95ee\u9898\u4e0a\u5177\u6709\u7ebf\u6027\u6536\u655b\u6027\uff0c\u5e76\u4e14\u5728\u566a\u58f0\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u8fd9\u4e3a\u89e3\u51b3\u539f\u59cbBDPR\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u4f4e\u79e9\u5f20\u91cf\u6062\u590d\u6846\u67b6\uff0c\u8bba\u6587\u4e3aBDPR\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u5206\u6790\u5de5\u5177\uff0c\u8bc1\u660e\u4e86RGD\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.10952", "pdf": "https://arxiv.org/pdf/2509.10952", "abs": "https://arxiv.org/abs/2509.10952", "authors": ["Yangcen Liu", "Woo Chul Shin", "Yunhai Han", "Zhenyang Chen", "Harish Ravichandar", "Danfei Xu"], "title": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation", "categories": ["cs.RO"], "comment": "Conference of Robot Learning", "summary": "Learning robot manipulation from abundant human videos offers a scalable\nalternative to costly robot-specific data collection. However, domain gaps\nacross visual, morphological, and physical aspects hinder direct imitation. To\neffectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic\nco-training framework that leverages both human videos and a small amount of\nteleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with\neither action- or visual-based mapping to map retargeted human hand poses to\nrobot joints, followed by MixUp interpolation between paired human and robot\ntrajectories. Our key insights are (1) retargeted human hand trajectories\nprovide informative action labels, and (2) interpolation over the mapped data\ncreates intermediate domains that facilitate smooth domain adaptation during\nco-training. Evaluations on four real-world manipulation tasks (Pick and Place,\nPush, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,\nAbility) show that ImMimic improves task success rates and execution\nsmoothness, highlighting its efficacy to bridge the domain gap for robust robot\nmanipulation. The project website can be found at\nhttps://sites.google.com/view/immimic.", "AI": {"tldr": "ImMimic\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u89c6\u9891\u548c\u5c11\u91cf\u673a\u5668\u4eba\u6f14\u793a\uff0c\u5229\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u548c\u63d2\u503c\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6d41\u7545\u5ea6\u3002", "motivation": "\u901a\u8fc7\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u4ee5\u8282\u7701\u6210\u672c\uff0c\u4f46\u89c6\u89c9\u3001\u5f62\u6001\u548c\u7269\u7406\u9886\u57df\u7684\u5dee\u5f02\u963b\u788d\u4e86\u76f4\u63a5\u6a21\u4eff\u3002", "method": "\u63d0\u51faImMimic\u6846\u67b6\uff0c\u52a8\u6001\u65f6\u95f4\u89c4\u6574+\u63d2\u503c\u65b9\u6cd5\u8fde\u63a5\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u521b\u5efa\u4e2d\u95f4\u57df\u4ee5\u5e73\u6ed1\u9002\u5e94\u3002", "result": "\u5728\u56db\u79cd\u4efb\u52a1\u548c\u56db\u79cd\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u6d41\u7545\u5ea6\u3002", "conclusion": "ImMimic\u80fd\u6709\u6548\u7f29\u5c0f\u9886\u57df\u5dee\u8ddd\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2509.10857", "pdf": "https://arxiv.org/pdf/2509.10857", "abs": "https://arxiv.org/abs/2509.10857", "authors": ["Hugues Kouakou", "Jos\u00e9 Henrique de Morais Goulart", "Raffaele Vitale", "Thomas Oberlin", "David Rousseau", "Cyril Ruckebusch", "Nicolas Dobigeon"], "title": "Online simplex-structured matrix factorization", "categories": ["eess.SP", "physics.chem-ph", "stat.ME"], "comment": null, "summary": "Simplex-structured matrix factorization (SSMF) is a common task encountered\nin signal processing and machine learning. Minimum-volume constrained unmixing\n(MVCU) algorithms are among the most widely used methods to perform this task.\nWhile MVCU algorithms generally perform well in an offline setting, their\ndirect application to online scenarios suffers from scalability limitations due\nto memory and computational demands. To overcome these limitations, this paper\nproposes an approach which can build upon any off-the-shelf MVCU algorithm to\noperate sequentially, i.e., to handle one observation at a time. The key idea\nof the proposed method consists in updating the solution of MVCU only when\nnecessary, guided by an online check of the corresponding optimization problem\nconstraints. It only stores and processes observations identified as\ninformative with respect to the geometrical constraints underlying SSMF. We\ndemonstrate the effectiveness of the approach when analyzing synthetic and real\ndatasets, showing that it achieves estimation accuracy comparable to the\noffline MVCU method upon which it relies, while significantly reducing the\ncomputational cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5728\u7ebf\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6700\u5c0f\u4f53\u79ef\u7ea6\u675f\u89e3\u6df7\uff08MVCU\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5728\u5fc5\u8981\u65f6\u66f4\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1MVCU\u7b97\u6cd5\u5728\u79bb\u7ebf\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ebf\u5e94\u7528\u65f6\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u5176\u6269\u5c55\u6027\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u73b0\u6709MVCU\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u68c0\u67e5\u4f18\u5316\u95ee\u9898\u7ea6\u675f\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u66f4\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4ec5\u5904\u7406\u548c\u5b58\u50a8\u5bf9SSMF\u51e0\u4f55\u7ea6\u675f\u6709\u4fe1\u606f\u91cf\u7684\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u79bb\u7ebfMVCU\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u5728\u7ebf\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MVCU\u7b97\u6cd5\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002"}}
{"id": "2509.10968", "pdf": "https://arxiv.org/pdf/2509.10968", "abs": "https://arxiv.org/abs/2509.10968", "authors": ["Leo Cazenille", "Loona Macabre", "Nicolas Bredeche"], "title": "Pogosim -- a Simulator for Pogobot robots", "categories": ["cs.RO"], "comment": "18 pages, 1 table, 7 figures", "summary": "Pogobots are a new type of open-source/open-hardware robots specifically\ndesigned for swarm robotics research. Their cost-effective and modular design,\ncomplemented by vibration-based and wheel-based locomotion, fast infrared\ncommunication and extensive software architecture facilitate the implementation\nof swarm intelligence algorithms. However, testing even simple distributed\nalgorithms directly on robots is particularly labor-intensive. Scaling to more\ncomplex problems or calibrate user code parameters will have a prohibitively\nhigh strain on available resources. In this article we present Pogosim, a fast\nand scalable simulator for Pogobots, designed to reduce as much as possible\nalgorithm development costs. The exact same code will be used in both\nsimulation and to experimentally drive real robots. This article details the\nsoftware architecture of Pogosim, explain how to write configuration files and\nuser programs and how simulations approximate or differ from experiments. We\ndescribe how a large set of simulations can be launched in parallel, how to\nretrieve and analyze the simulation results, and how to optimize user code\nparameters using optimization algorithms.", "AI": {"tldr": "Pogobots\u662f\u4e00\u79cd\u4e13\u4e3a\u7fa4\u4f53\u673a\u5668\u4eba\u7814\u7a76\u8bbe\u8ba1\u7684\u4f4e\u6210\u672c\u3001\u6a21\u5757\u5316\u673a\u5668\u4eba\uff0cPogosim\u662f\u4e3a\u5176\u5f00\u53d1\u7684\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u6a21\u62df\u5668\uff0c\u65e8\u5728\u964d\u4f4e\u7b97\u6cd5\u5f00\u53d1\u6210\u672c\u3002", "motivation": "\u7814\u7a76\u7fa4\u4f53\u673a\u5668\u4eba\u7b97\u6cd5\u65f6\uff0c\u76f4\u63a5\u5728\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\u6210\u672c\u9ad8\u4e14\u8017\u8d39\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u6a21\u62df\u5668\u6765\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86Pogosim\u6a21\u62df\u5668\uff0c\u652f\u6301\u4e0e\u771f\u5b9e\u673a\u5668\u4eba\u76f8\u540c\u7684\u4ee3\u7801\uff0c\u63d0\u4f9b\u5e76\u884c\u6a21\u62df\u3001\u7ed3\u679c\u5206\u6790\u4e0e\u53c2\u6570\u4f18\u5316\u529f\u80fd\u3002", "result": "Pogosim\u80fd\u591f\u663e\u8457\u964d\u4f4e\u7b97\u6cd5\u5f00\u53d1\u6210\u672c\uff0c\u5e76\u652f\u6301\u590d\u6742\u95ee\u9898\u7684\u6a21\u62df\u548c\u53c2\u6570\u6821\u51c6\u3002", "conclusion": "Pogosim\u4e3a\u7fa4\u4f53\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u540c\u4e00\u4ee3\u7801\u9002\u7528\u4e8e\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\uff0c\u63a8\u52a8\u4e86\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.10874", "pdf": "https://arxiv.org/pdf/2509.10874", "abs": "https://arxiv.org/abs/2509.10874", "authors": ["Baskaran Sripathmanathan", "Xiaowen Dong", "Michael Bronstein"], "title": "On the Impact of Downstream Tasks on Sampling and Reconstructing Noisy Graph Signals", "categories": ["eess.SP", "cs.LG"], "comment": "This work has been accepted for publication at IEEE CAMSAP 2025", "summary": "We investigate graph signal reconstruction and sample selection for\nclassification tasks. We present general theoretical characterisations of\nclassification error applicable to multiple commonly used reconstruction\nmethods, and compare that to the classical reconstruction error. We demonstrate\nthe applicability of our results by using them to derive new optimal sampling\nmethods for linearized graph convolutional networks, and show improvement over\nother graph signal processing based methods.", "AI": {"tldr": "\u7814\u7a76\u4e86\u56fe\u4fe1\u53f7\u91cd\u5efa\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6837\u672c\u9009\u62e9\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u591a\u79cd\u5e38\u89c1\u91cd\u5efa\u65b9\u6cd5\u7684\u5206\u7c7b\u8bef\u5dee\u901a\u7528\u7406\u8bba\u7279\u5f81\uff0c\u5e76\u4e0e\u7ecf\u5178\u91cd\u5efa\u8bef\u5dee\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u63a2\u7d22\u56fe\u4fe1\u53f7\u91cd\u5efa\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6837\u672c\u9009\u62e9\u548c\u91cd\u5efa\u8bef\u5dee\u7684\u7406\u8bba\u5206\u6790\u4e0e\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u591a\u79cd\u5e38\u89c1\u91cd\u5efa\u65b9\u6cd5\u7684\u5206\u7c7b\u8bef\u5dee\u901a\u7528\u7406\u8bba\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7ebf\u6027\u5316\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u4f18\u5316\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u65b0\u65b9\u6cd5\u5728\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4f18\u52bf\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u56fe\u4fe1\u53f7\u5904\u7406\u7684\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u56fe\u4fe1\u53f7\u91cd\u5efa\u548c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9645\u5e94\u7528\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2509.10979", "pdf": "https://arxiv.org/pdf/2509.10979", "abs": "https://arxiv.org/abs/2509.10979", "authors": ["Dimitri Jacquemont", "Carlo Bosio", "Teaya Yang", "Ruiqi Zhang", "Ozgur Orun", "Shuai Li", "Reza Alam", "Thomas M. Schutzius", "Simo A. Makiharju", "Mark W. Mueller"], "title": "Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 10 figures. Submitted to IEEE RA-L", "summary": "Photovoltaic (PV) panels are becoming increasingly widespread in the domain\nof renewable energy, and thus, small efficiency gains can have massive effects.\nAnti-reflective and self-cleaning coatings enhance panel performance but\ndegrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles\n(UAVs) offer a flexible and autonomous way to apply protective coatings more\noften and at lower cost compared to traditional manual coating methods. In this\nletter, we propose a quadcopter-based system, equipped with a liquid dispersion\nmechanism, designed to automate such tasks. The localization stack only uses\nonboard sensors, relying on visual-inertial odometry and the relative position\nof the PV panel detected with respect to the quadcopter. The control relies on\na model-based controller that accounts for the ground effect and the mass\ndecrease of the quadcopter during liquid dispersion. We validate the autonomy\ncapabilities of our system through extensive indoor and outdoor experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5149\u4f0f\u9762\u677f\u4fdd\u62a4\u6d82\u5c42\u81ea\u52a8\u55b7\u6d82\u7cfb\u7edf\uff0c\u5229\u7528\u673a\u8f7d\u4f20\u611f\u5668\u548c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u5b9e\u73b0\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u63a7\u5236\u5668\u4f18\u5316\u55b7\u6d82\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5ba4\u5185\u5916\u81ea\u4e3b\u80fd\u529b\u3002", "motivation": "\u5149\u4f0f\u9762\u677f\u5728\u53ef\u518d\u751f\u80fd\u6e90\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u5c0f\u5e45\u6548\u7387\u63d0\u5347\u53ef\u5e26\u6765\u663e\u8457\u6548\u679c\u3002\u9632\u53cd\u5c04\u548c\u81ea\u6e05\u6d01\u6d82\u5c42\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6613\u964d\u89e3\u9700\u5b9a\u671f\u91cd\u6d82\uff0c\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u6210\u672c\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e86\u642d\u8f7d\u6db2\u4f53\u5206\u6563\u673a\u5236\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u5668\u548c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u5b9a\u4f4d\uff0c\u7ed3\u5408\u6a21\u578b\u63a7\u5236\u5668\u5904\u7406\u5730\u9762\u6548\u5e94\u548c\u8d28\u91cf\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5ba4\u5185\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u81ea\u4e3b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u4f4e\u6210\u672c\u7684\u5149\u4f0f\u9762\u677f\u4fdd\u62a4\u6d82\u5c42\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10917", "pdf": "https://arxiv.org/pdf/2509.10917", "abs": "https://arxiv.org/abs/2509.10917", "authors": ["Yekta Demirci", "Guillaume Mantelet", "St\u00e9phane Martel", "Jean-Fran\u00e7ois Frigon", "Gunes Karabulut Kurt"], "title": "Forecasting Self-Similar User Traffic Demand Using Transformers in LEO Satellite Networks", "categories": ["eess.SP"], "comment": "6 pages", "summary": "In this paper, we propose the use of a transformer-based model to address the\nneed for forecasting user traffic demand in the next generation Low Earth Orbit\n(LEO) satellite networks. Considering a LEO satellite constellation, we present\nthe need to forecast the demand for the satellites in-orbit to utilize dynamic\nbeam-hopping in high granularity. We adopt a traffic dataset with second-order\nself-similar characteristics. Given this traffic dataset, the Fractional\nAuto-regressive Integrated Moving Average (FARIMA) model is considered a\nbenchmark forecasting solution. However, the constrained on-board processing\ncapabilities of LEO satellites, combined with the need to fit a new model for\neach input sequence due to the nature of FARIMA, motivate the investigation of\nalternative solutions. As an alternative, a pretrained probabilistic time\nseries model that utilizes transformers with a Prob-Sparse self-attention\nmechanism is considered. The considered solution is investigated under\ndifferent time granularities with varying sequence and prediction lengths.\nConcluding this paper, we provide extensive simulation results where the\ntransformer-based solution achieved up to six percent better forecasting\naccuracy on certain traffic conditions using mean squared error as the\nperformance indicator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u4e00\u4ee3\u4f4e\u5730\u7403\u8f68\u9053(LEO)\u536b\u661f\u7f51\u7edc\u7684\u7528\u6237\u6d41\u91cf\u9700\u6c42\uff0c\u76f8\u6bd4\u4f20\u7edfFARIMA\u6a21\u578b\uff0c\u65b0\u6a21\u578b\u5728\u7279\u5b9a\u6d41\u91cf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "LEO\u536b\u661f\u661f\u5ea7\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u52a8\u6001\u6ce2\u675f\u8df3\u9891\uff0c\u4f20\u7edfFARIMA\u6a21\u578b\u56e0\u5176\u8ba1\u7b97\u590d\u6742\u6027\u548c\u9700\u4e3a\u6bcf\u4e2a\u8f93\u5165\u5e8f\u5217\u91cd\u65b0\u62df\u5408\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3\u7684\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u5229\u7528Transformer\u548cProb-Sparse\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u548c\u5e8f\u5217\u957f\u5ea6\u4e0b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u7279\u5b9a\u6d41\u91cf\u6761\u4ef6\u4e0b\uff0c\u9884\u6d4b\u7cbe\u5ea6\u6bd4FARIMA\u6a21\u578b\u63d0\u9ad8\u4e866%\uff08\u4ee5\u5747\u65b9\u8bef\u5dee\u4e3a\u6307\u6807\uff09\u3002", "conclusion": "Transformer\u6a21\u578b\u5728LEO\u536b\u661f\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfFARIMA\u6a21\u578b\uff0c\u66f4\u5177\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.11025", "pdf": "https://arxiv.org/pdf/2509.11025", "abs": "https://arxiv.org/abs/2509.11025", "authors": ["Peng Chen", "Jing Liang", "Hui Song", "Kang-Jia Qiao", "Cai-Tong Yue", "Kun-Jie Yu", "Ponnuthurai Nagaratnam Suganthan", "Witold Pedrycz"], "title": "Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "The increasing labor costs in agriculture have accelerated the adoption of\nmulti-robot systems for orchard harvesting. However, efficiently coordinating\nthese systems is challenging due to the complex interplay between makespan and\nenergy consumption, particularly under practical constraints like\nload-dependent speed variations and battery limitations. This paper defines the\nmulti-objective agricultural multi-electrical-robot task allocation (AMERTA)\nproblem, which systematically incorporates these often-overlooked real-world\nconstraints. To address this problem, we propose a hybrid hierarchical route\nreconstruction algorithm (HRRA) that integrates several innovative mechanisms,\nincluding a hierarchical encoding structure, a dual-phase initialization\nmethod, task sequence optimizers, and specialized route reconstruction\noperators. Extensive experiments on 45 test instances demonstrate HRRA's\nsuperior performance against seven state-of-the-art algorithms. Statistical\nanalysis, including the Wilcoxon signed-rank and Friedman tests, empirically\nvalidates HRRA's competitiveness and its unique ability to explore previously\ninaccessible regions of the solution space. In general, this research\ncontributes to the theoretical understanding of multi-robot coordination by\noffering a novel problem formulation and an effective algorithm, thereby also\nproviding practical insights for agricultural automation.", "AI": {"tldr": "\u672c\u6587\u5b9a\u4e49\u4e86\u519c\u4e1a\u591a\u7535\u673a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff08AMERTA\uff09\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u5c42\u6b21\u8def\u7531\u91cd\u5efa\u7b97\u6cd5\uff08HRRA\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u519c\u4e1a\u52b3\u52a8\u529b\u6210\u672c\u4e0a\u5347\u4fc3\u4f7f\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u679c\u56ed\u91c7\u6458\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u9ad8\u6548\u534f\u8c03\u8fd9\u4e9b\u7cfb\u7edf\u9762\u4e34\u590d\u6742\u7684\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u7ea6\u675f\u3002", "method": "\u63d0\u51faHRRA\u7b97\u6cd5\uff0c\u7ed3\u5408\u5c42\u6b21\u7f16\u7801\u7ed3\u6784\u3001\u53cc\u76f8\u521d\u59cb\u5316\u65b9\u6cd5\u3001\u4efb\u52a1\u5e8f\u5217\u4f18\u5316\u5668\u548c\u4e13\u7528\u8def\u7531\u91cd\u5efa\u7b97\u5b50\u3002", "result": "\u572845\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u4e2d\uff0cHRRA\u8868\u73b0\u4f18\u4e8e\u4e03\u79cd\u5148\u8fdb\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u9a8c\u8bc1\u5176\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u673a\u5668\u4eba\u534f\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u95ee\u9898\u5b9a\u4e49\u548c\u6709\u6548\u7b97\u6cd5\uff0c\u63a8\u52a8\u4e86\u519c\u4e1a\u81ea\u52a8\u5316\u5b9e\u8df5\u3002"}}
{"id": "2509.10926", "pdf": "https://arxiv.org/pdf/2509.10926", "abs": "https://arxiv.org/abs/2509.10926", "authors": ["Ashish Patwari", "Ananya Pandey", "Aditya Dabade", "Priyadarshini Raiguru"], "title": "Design and Validation of a MATLAB-based GUI for Coarray Domain Analysis of Sparse Linear Arrays", "categories": ["eess.SP"], "comment": "12 pages, 11 Figures, Currently Under Peer Review", "summary": "This work presents a first-of-its-kind graphical user interface (GUI)-based\nsimulator developed using MATLAB App designer for the comprehensive analysis of\nsparse linear arrays (SLAs) in the difference coarray (DCA) domain. Sparse\nsensor arrays have emerged as a critical solution in enhancing signal\ndetection, direction of arrival (DOA) estimation, and beamforming in fields\nsuch as wireless communication, radar, sonar, and integrated sensing systems.\nThey offer several advantages over traditional uniform arrays, including\nreduced system complexity, lower deployment costs, and improved mitigation of\nmutual coupling effects. The tool enables users to input array configurations,\ncompute DCAs, visualize weight function graphs, and assess the hole-free status\nof arrays, as applicable for coarray processing. Unlike conventional simulators\nthat focus on radiation pattern visualization (array pattern, main lobe and\nsidelobe characteristics, azimuth cut, rectangular view, polar view etc.), this\ntool addresses the behavior of SLAs from a coarray domain perspective.\nNumerical validations demonstrate the tool's correctness, effectiveness, and\nits potential to foster further research in sparse arrays. This simulator could\nalso be used as a teaching aid to drive home complicated topics and attract\nyoung minds towards the fascinating field of sparse array design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u57fa\u4e8eMATLAB App designer\u7684GUI\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u5206\u6790\u7a00\u758f\u7ebf\u6027\u9635\u5217\uff08SLAs\uff09\u5728\u5dee\u5171\u7ebf\u9635\u5217\uff08DCA\uff09\u57df\u7684\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u6a21\u62df\u5668\u5728\u5171\u7ebf\u9635\u5217\u89c6\u89d2\u4e0b\u7684\u7a7a\u767d\u3002", "motivation": "\u7a00\u758f\u4f20\u611f\u5668\u9635\u5217\u5728\u65e0\u7ebf\u901a\u4fe1\u3001\u96f7\u8fbe\u3001\u58f0\u7eb3\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5982\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u6027\u548c\u90e8\u7f72\u6210\u672c\u3002\u7136\u800c\uff0c\u4f20\u7edf\u6a21\u62df\u5668\u4ec5\u5173\u6ce8\u8f90\u5c04\u6a21\u5f0f\u53ef\u89c6\u5316\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5728\u5171\u7ebf\u9635\u5217\u57df\u884c\u4e3a\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u901a\u8fc7MATLAB App designer\u5f00\u53d1GUI\u6a21\u62df\u5668\uff0c\u7528\u6237\u53ef\u8f93\u5165\u9635\u5217\u914d\u7f6e\u3001\u8ba1\u7b97DCA\u3001\u53ef\u89c6\u5316\u6743\u91cd\u51fd\u6570\u56fe\uff0c\u5e76\u8bc4\u4f30\u9635\u5217\u7684\u65e0\u7a7a\u6d1e\u72b6\u6001\u3002", "result": "\u6570\u503c\u9a8c\u8bc1\u8868\u660e\u8be5\u5de5\u5177\u7684\u6b63\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7a00\u758f\u9635\u5217\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u4e0d\u4ec5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u6559\u5b66\u5de5\u5177\uff0c\u5438\u5f15\u5e74\u8f7b\u5b66\u8005\u6295\u8eab\u7a00\u758f\u9635\u5217\u8bbe\u8ba1\u9886\u57df\u3002"}}
{"id": "2509.11109", "pdf": "https://arxiv.org/pdf/2509.11109", "abs": "https://arxiv.org/abs/2509.11109", "authors": ["Jiaxin Huang", "Hanyu Liu", "Yunsheng Ma", "Jian Shen", "Yilin Zheng", "Jiayi Wen", "Baishu Wan", "Pan Li", "Zhigong Song"], "title": "FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers", "categories": ["cs.RO"], "comment": null, "summary": "The embodied intelligence bridges the physical world and information space.\nAs its typical physical embodiment, humanoid robots have shown great promise\nthrough robot learning algorithms in recent years. In this study, a hardware\nplatform, including humanoid robot and exoskeleton-style teleoperation cabin,\nwas developed to realize intuitive remote manipulation and efficient collection\nof anthropomorphic action data. To improve the perception representation of\nhumanoid robot, an imitation learning framework, termed Frequency-Enhanced\nWavelet-based Transformer (FEWT), was proposed, which consists of two primary\nmodules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and\nTime-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale\nwavelet decomposition with the residual network, FE-EMA can dynamically fuse\nfeatures from both time-domain and frequency-domain. This fusion is able to\ncapture feature information across various scales effectively, thereby\nenhancing model robustness. Experimental performance demonstrates that FEWT\nimproves the success rate of the state-of-the-art algorithm (Action Chunking\nwith Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in\nreal-world.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u548c\u5916\u9aa8\u9abc\u5f0f\u8fdc\u7a0b\u64cd\u4f5c\u8231\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFEWT\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u8868\u73b0\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\uff0c\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8fde\u63a5\u7269\u7406\u4e16\u754c\u548c\u4fe1\u606f\u7a7a\u95f4\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u611f\u77e5\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u786c\u4ef6\u5e73\u53f0\u548cFEWT\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5206\u89e3\u4e0e\u6b8b\u5dee\u7f51\u7edc\uff0c\u52a8\u6001\u878d\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u3002", "result": "FEWT\u5728\u4eff\u771f\u4e2d\u5c06\u6700\u5148\u8fdb\u7b97\u6cd5\uff08ACT\u57fa\u7ebf\uff09\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8630%\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u63d0\u9ad8\u4e866-12%\u3002", "conclusion": "FEWT\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u8868\u73b0\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.11081", "pdf": "https://arxiv.org/pdf/2509.11081", "abs": "https://arxiv.org/abs/2509.11081", "authors": ["Yifan Ye", "Bin Chen", "Xiang Li", "Yi Lei", "Zhiwei Liang", "Qingqing Hu", "Can Zhao", "Yanni Ou"], "title": "Experimental Demonstration of Rate-Adaptation via Hybrid Polar-BCH Product Code for Flexible PON", "categories": ["eess.SP"], "comment": "4 Pages,2 figures", "summary": "The flexible-rate Polar-BCH product codes are experimentally demonstrated in\na coherent passive optical network system with 16QAM for the first time. Using\na new hybrid soft- and hard-decision decoder, we achieve a power gain of upto\n1.75 dB over traditional BCH-BCH product codes after 48 km transmission.", "AI": {"tldr": "\u9996\u6b21\u572816QAM\u76f8\u5e72\u65e0\u6e90\u5149\u7f51\u7edc\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u7075\u6d3b\u901f\u7387\u7684Polar-BCH\u4e58\u79ef\u7801\uff0c\u901a\u8fc7\u65b0\u578b\u8f6f\u786c\u5224\u51b3\u6df7\u5408\u89e3\u7801\u5668\uff0c\u572848\u516c\u91cc\u4f20\u8f93\u540e\u6bd4\u4f20\u7edfBCH-BCH\u4e58\u79ef\u7801\u83b7\u5f971.75 dB\u7684\u529f\u7387\u589e\u76ca\u3002", "motivation": "\u7814\u7a76\u7075\u6d3b\u901f\u7387\u7684Polar-BCH\u4e58\u79ef\u7801\u572816QAM\u76f8\u5e72\u65e0\u6e90\u5149\u7f51\u7edc\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u91c7\u7528\u65b0\u578b\u7684\u8f6f\u786c\u5224\u51b3\u6df7\u5408\u89e3\u7801\u5668\uff0c\u4e0e\u4f20\u7edf\u7684BCH-BCH\u4e58\u79ef\u7801\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u572848\u516c\u91cc\u4f20\u8f93\u540e\uff0c\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e861.75 dB\u7684\u529f\u7387\u589e\u76ca\u3002", "conclusion": "\u7075\u6d3b\u901f\u7387\u7684Polar-BCH\u4e58\u79ef\u7801\u4e0e\u65b0\u578b\u89e3\u7801\u5668\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.11125", "pdf": "https://arxiv.org/pdf/2509.11125", "abs": "https://arxiv.org/abs/2509.11125", "authors": ["Zheng Li", "Pei Qu", "Yufei Jia", "Shihui Zhou", "Haizhou Ge", "Jiahang Cao", "Jinni Zhou", "Guyue Zhou", "Jun Ma"], "title": "ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 7 figures", "summary": "Deploying visual reinforcement learning (RL) policies in real-world\nmanipulation is often hindered by camera viewpoint changes. A policy trained\nfrom a fixed front-facing camera may fail when the camera is shifted--an\nunavoidable situation in real-world settings where sensor placement is hard to\nmanage appropriately. Existing methods often rely on precise camera calibration\nor struggle with large perspective changes. To address these limitations, we\npropose ManiVID-3D, a novel 3D RL architecture designed for robotic\nmanipulation, which learns view-invariant representations through\nself-supervised disentangled feature learning. The framework incorporates\nViewNet, a lightweight yet effective module that automatically aligns point\ncloud observations from arbitrary viewpoints into a unified spatial coordinate\nsystem without the need for extrinsic calibration. Additionally, we develop an\nefficient GPU-accelerated batch rendering module capable of processing over\n5000 frames per second, enabling large-scale training for 3D visual RL at\nunprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world\ntasks demonstrates that our approach achieves a 44.7% higher success rate than\nstate-of-the-art methods under viewpoint variations while using 80% fewer\nparameters. The system's robustness to severe perspective changes and strong\nsim-to-real performance highlight the effectiveness of learning geometrically\nconsistent representations for scalable robotic manipulation in unstructured\nenvironments. Our project website can be found in\nhttps://zheng-joe-lee.github.io/manivid3d/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aManiVID-3D\u7684\u65b0\u578b3D\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u89e3\u8026\u7279\u5f81\u5b66\u4e60\u5b9e\u73b0\u89c6\u70b9\u4e0d\u53d8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u7eb5\u4e2d\u56e0\u6444\u50cf\u673a\u89c6\u89d2\u53d8\u5316\u800c\u5931\u6548\u7684\u95ee\u9898\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u5e38\u56e0\u6444\u50cf\u673a\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7b56\u7565\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u6444\u50cf\u673a\u6807\u5b9a\u6216\u96be\u4ee5\u5e94\u5bf9\u5927\u89c6\u89d2\u53d8\u5316\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7ViewNet\u6a21\u5757\u81ea\u52a8\u5bf9\u9f50\u4efb\u610f\u89c6\u89d2\u7684\u70b9\u4e91\u89c2\u6d4b\u5230\u7edf\u4e00\u7a7a\u95f4\u5750\u6807\u7cfb\uff0c\u65e0\u9700\u5916\u90e8\u6807\u5b9a\uff1b\u5f00\u53d1\u9ad8\u6548\u7684GPU\u52a0\u901f\u6279\u91cf\u6e32\u67d3\u6a21\u5757\uff0c\u652f\u6301\u5927\u89c4\u6a213D\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u572810\u4e2a\u6a21\u62df\u548c5\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u70b9\u53d8\u5316\u4e0b\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6280\u672f\u9ad844.7%\uff0c\u4e14\u53c2\u6570\u51cf\u5c1180%\u3002", "conclusion": "\u5b66\u4e60\u51e0\u4f55\u4e00\u81f4\u7684\u8868\u793a\u5bf9\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u64cd\u7eb5\u5177\u6709\u663e\u8457\u6548\u679c\uff0cManiVID-3D\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u7684\u6279\u5904\u7406\u6e32\u67d3\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.11117", "pdf": "https://arxiv.org/pdf/2509.11117", "abs": "https://arxiv.org/abs/2509.11117", "authors": ["Haoyu Wang", "Jiawei Hu", "Jiaqi Xu", "Ying Ju", "A. Lee Swindlehurst"], "title": "Nonreciprocal RIS-Aided Covert Channel Reciprocity Attacks and Countermeasures", "categories": ["eess.SP"], "comment": "submitted to IEEE Trans for review", "summary": "Reconfigurable intelligent surface (RIS) technology enhances wireless\ncommunication performance, but it also introduces new vulnerabilities that can\nbe exploited by adversaries. This paper investigates channel reciprocity attack\n(CRACK) threats in multi-antenna wireless systems operating in time-division\nduplexing mode using a physically consistent non-reciprocal RIS (NR-RIS) model.\nCRACK can degrade communication rate and facilitate passive eavesdropping\nbehavior by distorting the downlink precoding, without requiring any additional\nsignal transmission or channel state information (CSI). Unlike conventional RIS\njamming strategies, the NR-RIS does not need synchronization with the\nlegitimate system and thus can operate with slow or fixed configurations to\nimplement CRACK, obscuring the distinction between the direct and RIS-induced\nchannels and thereby complicating corresponding defensive precoding designs. To\ncounter the CRACK threat posed by NR-RIS, we develop ``SecureCoder,'' a deep\nreinforcement learning-based framework that can mitigate CRACK and determine an\nimproved downlink precoder matrix using the estimated uplink CSI and rate\nfeedback from the users. Simulation results demonstrate the severe performance\ndegradation caused by NR-RIS CRACK and validate the effectiveness of\nSecureCoder in improving both throughput and reducing security threats, thereby\nenhancing system robustness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u591a\u5929\u7ebf\u65e0\u7ebf\u7cfb\u7edf\u4e2d\uff0c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u6280\u672f\u53ef\u80fd\u5bfc\u81f4\u7684\u4fe1\u9053\u4e92\u6613\u6027\u653b\u51fb\uff08CRACK\uff09\u53ca\u5176\u5f71\u54cd\u3002\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6SecureCoder\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u5bf9\u6297CRACK\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "RIS\u6280\u672f\u867d\u80fd\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd\uff0c\u4f46\u5176\u975e\u4e92\u6613\u6027\u7279\u6027\u53ef\u80fd\u88ab\u5229\u7528\u8fdb\u884cCRACK\u653b\u51fb\uff0c\u5bfc\u81f4\u901a\u4fe1\u901f\u7387\u4e0b\u964d\u548c\u88ab\u52a8\u7a83\u542c\u98ce\u9669\u3002\u8bba\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u65b0\u578b\u5a01\u80c1\u5e76\u5f00\u53d1\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6SecureCoder\uff0c\u5229\u7528\u4f30\u8ba1\u7684\u4e0a\u884c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u548c\u7528\u6237\u901f\u7387\u53cd\u9988\uff0c\u4f18\u5316\u4e0b\u884c\u9884\u7f16\u7801\u77e9\u9635\u4ee5\u62b5\u5fa1CRACK\u653b\u51fb\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cNR-RIS CRACK\u4f1a\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\uff0c\u800cSecureCoder\u80fd\u6709\u6548\u63d0\u5347\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u5b89\u5168\u5a01\u80c1\uff0c\u589e\u5f3a\u7cfb\u7edf\u7a33\u5065\u6027\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86RIS\u6280\u672f\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u9632\u5fa1\u6846\u67b6SecureCoder\uff0c\u4e3a\u672a\u6765RIS\u7cfb\u7edf\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.11149", "pdf": "https://arxiv.org/pdf/2509.11149", "abs": "https://arxiv.org/abs/2509.11149", "authors": ["Mintae Kim", "Jiaze Cai", "Koushil Sreenath"], "title": "RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Designing robust controllers for precise, arbitrary trajectory tracking with\nquadrotors is challenging due to nonlinear dynamics and underactuation, and\nbecomes harder with flexible cable-suspended payloads that introduce extra\ndegrees of freedom and hybridness. Classical model-based methods offer\nstability guarantees but require extensive tuning and often do not adapt when\nthe configuration changes, such as when a payload is added or removed, or when\nthe payload mass or cable length varies. We present RoVerFly, a unified\nlearning-based control framework in which a reinforcement learning (RL) policy\nserves as a robust and versatile tracking controller for standard quadrotors\nand for cable-suspended payload systems across a range of configurations.\nTrained with task and domain randomization, the controller is resilient to\ndisturbances and varying dynamics. It achieves strong zero-shot generalization\nacross payload settings, including no payload as well as varying mass and cable\nlength, without controller switching or re-tuning, while retaining the\ninterpretability and structure of a feedback tracking controller. Code and\nsupplementary materials are available at\nhttps://github.com/mintaeshkim/roverfly", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RoVerFly\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u643a\u5e26\u6216\u4e0d\u643a\u5e26\u67d4\u6027\u7f06\u7ef3\u60ac\u6302\u8f7d\u8377\u65f6\u7684\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u56db\u65cb\u7ffc\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u5c24\u5176\u662f\u643a\u5e26\u67d4\u6027\u7f06\u7ef3\u8f7d\u8377\u65f6\u5f15\u5165\u7684\u989d\u5916\u81ea\u7531\u5ea6\uff0c\u4f20\u7edf\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8c03\u53c2\u4e14\u96be\u4ee5\u9002\u5e94\u914d\u7f6e\u53d8\u5316\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u548c\u9886\u57df\u968f\u673a\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\uff0c\u8bad\u7ec3\u4e00\u4e2a\u9c81\u68d2\u7684\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8f7d\u8377\u914d\u7f6e\u3002", "result": "\u63a7\u5236\u5668\u5728\u96f6\u6837\u672c\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5207\u6362\u6216\u91cd\u8c03\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u8f7d\u8377\u8d28\u91cf\u6216\u7f06\u7ef3\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53cd\u9988\u8ddf\u8e2a\u63a7\u5236\u5668\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RoVerFly\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6027\u5f3a\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u8f7d\u8377\u60c5\u51b5\u4e0b\u7684\u56db\u65cb\u7ffc\u7cbe\u786e\u63a7\u5236\u3002"}}
{"id": "2509.11193", "pdf": "https://arxiv.org/pdf/2509.11193", "abs": "https://arxiv.org/abs/2509.11193", "authors": ["Haifan Yin", "Jindiao Huang", "Ruikun Zhang", "Jiwang Wu", "Li Tan"], "title": "Holographic interference surface: A proof of concept based on the principle of interferometry", "categories": ["eess.SP"], "comment": null, "summary": "Revolutionizing communication architectures to achieve a balance between\nenhanced performance and improved efficiency is becoming increasingly critical\nfor wireless communications as the era of ultra-large-scale arrays approaches.\nIn traditional communication architectures, radio frequency (RF) signals are\ntypically converted to baseband for subsequent processing through operations\nsuch as filtering, analog-to-digital conversion and down-conversion, all of\nwhich depend on expensive and power-intensive RF chains. The increased hardware\ncomplexity and escalated power consumption resulting from this dependency\nsignificantly limit the practical deployment of ultra-large-scale arrays. To\naddress these limitations, we propose a holographic communication system based\non the principle of interferometry, designated as holographic interference\nsurfaces (HIS). Utilizing the interference effect of electromagnetic waves, HIS\nestimates the channel state information (CSI) by dealing solely with power\ninformation, which enables the replacement of RF chains with power sensors and\ncompletes the signal processing in radio frequency. As proof-of-concept\ndemonstrations, we implemented a prototype system based on principles of\nholographic interference. Experimental results align well with theoretical\npredictions, confirming the practical viability and effectiveness of the\nproposed HIS. This work provides a new paradigm for building a more\ncost-effective wireless communication architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e72\u6d89\u6d4b\u91cf\u539f\u7406\u7684\u5168\u606f\u901a\u4fe1\u7cfb\u7edf\uff08HIS\uff09\uff0c\u901a\u8fc7\u7535\u78c1\u6ce2\u5e72\u6d89\u6548\u5e94\u5904\u7406\u529f\u7387\u4fe1\u606f\u4ee5\u4f30\u8ba1\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\uff0c\u66ff\u4ee3\u4f20\u7edfRF\u94fe\uff0c\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u548c\u529f\u8017\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u901a\u4fe1\u67b6\u6784\u4f9d\u8d56\u6602\u8d35\u4e14\u9ad8\u529f\u8017\u7684RF\u94fe\uff0c\u9650\u5236\u4e86\u8d85\u5927\u89c4\u6a21\u9635\u5217\u7684\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5168\u606f\u5e72\u6d89\u9762\uff08HIS\uff09\u6280\u672f\uff0c\u5229\u7528\u7535\u78c1\u6ce2\u5e72\u6d89\u6548\u5e94\u5904\u7406\u529f\u7387\u4fe1\u606f\uff0c\u66ff\u4ee3\u4f20\u7edfRF\u94fe\uff0c\u5b8c\u6210\u5c04\u9891\u4fe1\u53f7\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHIS\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u53ca\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u65e0\u7ebf\u901a\u4fe1\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.11185", "pdf": "https://arxiv.org/pdf/2509.11185", "abs": "https://arxiv.org/abs/2509.11185", "authors": ["Kai Chen", "Zhihai Bi", "Guoyang Zhao", "Chunxin Zheng", "Yulin Li", "Hang Zhao", "Jun Ma"], "title": "SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators", "categories": ["cs.RO"], "comment": null, "summary": "Neural-based motion planning methods have achieved remarkable progress for\nrobotic manipulators, yet a fundamental challenge lies in simultaneously\naccounting for both the robot's physical shape and the surrounding environment\nwhen generating safe and feasible motions. Moreover, existing approaches often\nrely on simplified robot models or focus primarily on obstacle representation,\nwhich can lead to incomplete collision detection and degraded performance in\ncluttered scenes. To address these limitations, we propose spatial anchor-based\nmotion policy (SAMP), a unified framework that simultaneously encodes the\nenvironment and the manipulator using signed distance field (SDF) anchored on a\nshared spatial grid. SAMP incorporates a dedicated robot SDF network that\ncaptures the manipulator's precise geometry, enabling collision-aware reasoning\nbeyond coarse link approximations. These representations are fused on spatial\nanchors and used to train a neural motion policy that generates smooth,\ncollision-free trajectories in the proposed efficient feature alignment\nstrategy. Experiments conducted in both simulated and real-world environments\nconsistently show that SAMP outperforms existing methods, delivering an 11%\nincrease in success rate and a 7% reduction in collision rate. These results\nhighlight the benefits of jointly modelling robot and environment geometry,\ndemonstrating its practical value in challenging real-world environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMP\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7a7a\u95f4\u7f51\u683c\u4e0a\u7684\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u540c\u65f6\u7f16\u7801\u73af\u5883\u548c\u673a\u68b0\u81c2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u78b0\u649e\u68c0\u6d4b\u548c\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5e38\u4f9d\u8d56\u7b80\u5316\u7684\u673a\u5668\u4eba\u6a21\u578b\u6216\u4e3b\u8981\u5173\u6ce8\u969c\u788d\u7269\u8868\u793a\uff0c\u5bfc\u81f4\u78b0\u649e\u68c0\u6d4b\u4e0d\u5b8c\u6574\uff0c\u4e14\u5728\u590d\u6742\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faSAMP\u6846\u67b6\uff0c\u4f7f\u7528SDF\u7f16\u7801\u73af\u5883\u548c\u673a\u68b0\u81c2\u51e0\u4f55\uff0c\u7ed3\u5408\u673a\u5668\u4ebaSDF\u7f51\u7edc\u548c\u7a7a\u95f4\u951a\u70b9\uff0c\u8bad\u7ec3\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u7684\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cSAMP\u6210\u529f\u7387\u63d0\u9ad811%\uff0c\u78b0\u649e\u7387\u964d\u4f4e7%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8054\u5408\u5efa\u6a21\u673a\u5668\u4eba\u548c\u73af\u5883\u51e0\u4f55\u5728\u590d\u6742\u5b9e\u9645\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.11243", "pdf": "https://arxiv.org/pdf/2509.11243", "abs": "https://arxiv.org/abs/2509.11243", "authors": ["Haozhen Li", "Ruide Zhang", "Rongqing Zhang", "Xiang Cheng"], "title": "Synesthesia of Machines (SoM)-Empowered Wireless Image Transmission over Complex Dynamic Channel", "categories": ["eess.SP"], "comment": null, "summary": "Wireless image transmission underpins diverse networked intelligent services\nand becomes an increasingly critical issue. Existing works have shown that deep\nlearning-based joint source-channel coding (JSCC) is an effective framework to\nbalance image transmission fidelity and data overhead. However, these studies\noversimplify the communication system as a mere pipeline with noise, failing to\naccount for the complex dynamics of wireless channels and concrete\nphysical-layer transmission process. To address these limitations, we propose a\nSynesthesia of Machines (SoM)-empowered Dynamic Channel Adaptive Transmission\n(DCAT) scheme, designed for practical implementation in real communication\nscenarios. Building upon the Swin Transformer backbone, our DCAT scheme\ndemonstrates robust adaptability to time-selective fading and channel aging\neffects by effectively utilizing the physical-layer transmission\ncharacteristics of wireless channels. Comprehensive experimental results\nconfirm that DCAT consistently achieves superior performance compared with JSCC\nbaseline approaches across all conditions. Furthermore, our neural network\narchitecture demonstrates high scalability due to its interpretable design,\noffering substantial potential for cost-efficient deployment in practical\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u8054\u89c9\u7684\u52a8\u6001\u4fe1\u9053\u81ea\u9002\u5e94\u4f20\u8f93\u65b9\u6848\uff08DCAT\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u8054\u5408\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801\u5728\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u4e2d\u7b80\u5316\u4fe1\u9053\u52a8\u6001\u7684\u95ee\u9898\uff0c\u901a\u8fc7Swin Transformer\u9aa8\u5e72\u7f51\u7edc\u5b9e\u73b0\u5bf9\u4fe1\u9053\u7279\u6027\u7684\u81ea\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u901a\u4fe1\u7cfb\u7edf\u7b80\u5316\u4e3a\u5e26\u566a\u58f0\u7684\u7ba1\u9053\uff0c\u5ffd\u89c6\u4e86\u65e0\u7ebf\u4fe1\u9053\u7684\u590d\u6742\u52a8\u6001\u548c\u7269\u7406\u5c42\u4f20\u8f93\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u57fa\u4e8eSwin Transformer\u9aa8\u5e72\u7f51\u7edc\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u4fe1\u9053\u81ea\u9002\u5e94\u4f20\u8f93\u65b9\u6848\uff08DCAT\uff09\uff0c\u5229\u7528\u7269\u7406\u5c42\u4f20\u8f93\u7279\u6027\u9002\u5e94\u4fe1\u9053\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eDCAT\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "DCAT\u65b9\u6848\u901a\u8fc7\u81ea\u9002\u5e94\u4fe1\u9053\u52a8\u6001\u63d0\u5347\u4e86\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11197", "pdf": "https://arxiv.org/pdf/2509.11197", "abs": "https://arxiv.org/abs/2509.11197", "authors": ["Yunheng Wang", "Yuetong Fang", "Taowen Wang", "Yixiao Feng", "Yawen Tan", "Shuning Zhang", "Peiran Liu", "Yiding Ji", "Renjing Xu"], "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which\nlinks language instructions to perception and control in the real world, is a\ncore capability of embodied robots. Recently, large-scale pretrained foundation\nmodels have been leveraged as shared priors for perception, reasoning, and\naction, enabling zero-shot VLN without task-specific training. However,\nexisting zero-shot VLN methods depend on costly perception and passive scene\nunderstanding, collapsing control to point-level choices. As a result, they are\nexpensive to deploy, misaligned in action semantics, and short-sighted in\nplanning. To address these issues, we present DreamNav that focuses on the\nfollowing three aspects: (1) for reducing sensory cost, our EgoView Corrector\naligns viewpoints and stabilizes egocentric perception; (2) instead of\npoint-level actions, our Trajectory Predictor favors global trajectory-level\nplanning to better align with instruction semantics; and (3) to enable\nanticipatory and long-horizon planning, we propose an Imagination Predictor to\nendow the agent with proactive thinking capability. On VLN-CE and real-world\ntests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the\nstrongest egocentric baseline with extra information by up to 7.49\\% and\n18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first\nzero-shot VLN method to unify trajectory-level planning and active imagination\nwhile using only egocentric inputs.", "AI": {"tldr": "DreamNav\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN-CE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u611f\u77e5\u548c\u88ab\u52a8\u7684\u573a\u666f\u7406\u89e3\uff0c\u5bfc\u81f4\u63a7\u5236\u5c40\u9650\u4e8e\u70b9\u7ea7\u9009\u62e9\uff0c\u5b58\u5728\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u52a8\u4f5c\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u89c4\u5212\u77ed\u89c6\u7684\u95ee\u9898\u3002", "method": "DreamNav\u901a\u8fc7EgoView Corrector\u51cf\u5c11\u611f\u77e5\u6210\u672c\uff0cTrajectory Predictor\u5b9e\u73b0\u8f68\u8ff9\u7ea7\u89c4\u5212\uff0cImagination Predictor\u8d4b\u4e88\u4e3b\u52a8\u60f3\u8c61\u80fd\u529b\u3002", "result": "\u5728VLN-CE\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cDreamNav\u5728SR\u548cSPL\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53477.49%\u548c18.15%\uff0c\u8fbe\u5230\u96f6\u6837\u672cSOTA\u3002", "conclusion": "DreamNav\u9996\u6b21\u5c06\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u7ed3\u5408\uff0c\u4ec5\u4f9d\u8d56\u81ea\u6211\u4e2d\u5fc3\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672cVLN\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.11373", "pdf": "https://arxiv.org/pdf/2509.11373", "abs": "https://arxiv.org/abs/2509.11373", "authors": ["Hadi Zayyani", "Felipe A. P. de Figueiredo", "Mohammad Salman", "Rausley A. A. de Souza"], "title": "Resistor Hopping KLJN Noise Communication Using Small Bias Voltages Supported by ML and Optimum Threshold-Based Detectors", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, a Resistor Hopping (RH) scheme with the addition of biases is\nproposed for secure Kirchhoff Law Johnson-Noise (KLJN) communication. The RH\napproach enables us to increase the bit rate of secure communication between\nAlice and Bob, while also ensuring that the inherent unconditional security of\nKLJN is satisfied. The biases are added to the proposed scheme to better\ndistinguish between Gaussian distributed noises in terms of their means, rather\nthan just using variances. Throughout the paper, we strive to minimize biases\nto achieve a power-efficient scheme. For the detection part of the proposed\nalgorithm, a Maximum-Likelihood (ML) detector is derived. The separability\ncondition of Gaussian distributions is investigated, along with the provision\nof a threshold-based detector that offers both simple and optimal thresholds in\nterms of minimizing the error probability. Some analysis of the proposed\nRH-KLJN communication scheme is provided, including Physical Layer Security\n(PLS) equations. Simulation results demonstrate the advantages of the proposed\nscheme over the classical KLJN scheme, offering a higher data rate and lower\nbit error probability at the expense of increased complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u504f\u7f6e\u7684\u7535\u963b\u8df3\u8dc3\uff08RH\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u5b89\u5168\u7684\u57fa\u5c14\u970d\u592b\u5b9a\u5f8b\u7ea6\u7ff0\u900a\u566a\u58f0\uff08KLJN\uff09\u901a\u4fe1\uff0c\u63d0\u9ad8\u4e86\u6bd4\u7279\u7387\u5e76\u4fdd\u6301\u4e86\u65e0\u6761\u4ef6\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8KLJN\u901a\u4fe1\u7684\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u5176\u56fa\u6709\u7684\u65e0\u6761\u4ef6\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u7535\u963b\u8df3\u8dc3\uff08RH\uff09\u65b9\u6848\u5e76\u52a0\u5165\u504f\u7f6e\uff0c\u901a\u8fc7\u5747\u503c\u533a\u5206\u9ad8\u65af\u5206\u5e03\u566a\u58f0\uff0c\u6700\u5c0f\u5316\u504f\u7f6e\u4ee5\u63d0\u9ad8\u529f\u7387\u6548\u7387\uff0c\u5e76\u4f7f\u7528\u6700\u5927\u4f3c\u7136\uff08ML\uff09\u68c0\u6d4b\u5668\u8fdb\u884c\u4fe1\u53f7\u68c0\u6d4b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u5178KLJN\u65b9\u6848\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u7387\u548c\u66f4\u4f4e\u7684\u8bef\u7801\u7387\uff0c\u4f46\u590d\u6742\u5ea6\u6709\u6240\u589e\u52a0\u3002", "conclusion": "\u63d0\u51fa\u7684RH-KLJN\u65b9\u6848\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\uff0c\u5c3d\u7ba1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4f46\u5176\u9ad8\u6570\u636e\u7387\u548c\u4f4e\u8bef\u7801\u7387\u4f7f\u5176\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.11225", "pdf": "https://arxiv.org/pdf/2509.11225", "abs": "https://arxiv.org/abs/2509.11225", "authors": ["Youzhi Liang", "Eyan Noronha"], "title": "MEMBOT: Memory-Based Robot in Intermittent POMDP", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic systems deployed in real-world environments often operate under\nconditions of partial and often intermittent observability, where sensor inputs\nmay be noisy, occluded, or entirely unavailable due to failures or\nenvironmental constraints. Traditional reinforcement learning (RL) approaches\nthat assume full state observability are ill-equipped for such challenges. In\nthis work, we introduce MEMBOT, a modular memory-based architecture designed to\naddress intermittent partial observability in robotic control tasks. MEMBOT\ndecouples belief inference from policy learning through a two-phase training\nprocess: an offline multi-task learning pretraining stage that learns a robust\ntask-agnostic latent belief encoder using a reconstruction losses, followed by\nfine-tuning of task-specific policies using behavior cloning. The belief\nencoder, implemented as a state-space model (SSM) and a LSTM, integrates\ntemporal sequences of observations and actions to infer latent state\nrepresentations that persist even when observations are dropped. We train and\nevaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and\nRobomimic under varying rates of observation dropout. Results show that MEMBOT\nconsistently outperforms both memoryless and naively recurrent baselines,\nmaintaining up to 80% of peak performance under 50% observation availability.\nThese findings highlight the effectiveness of explicit belief modeling in\nachieving robust, transferable, and data-efficient policies for real-world\npartially observable robotic systems.", "AI": {"tldr": "MEMBOT\u662f\u4e00\u79cd\u6a21\u5757\u5316\u8bb0\u5fc6\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u95f4\u6b47\u6027\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff08\u79bb\u7ebf\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff09\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5168\u72b6\u6001\u53ef\u89c2\u6d4b\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u73af\u5883\u4e2d\u4f20\u611f\u5668\u8f93\u5165\u566a\u58f0\u3001\u906e\u6321\u6216\u7f3a\u5931\u7684\u6311\u6218\u3002", "method": "MEMBOT\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u79bb\u7ebf\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u5b66\u4e60\u9c81\u68d2\u7684\u4efb\u52a1\u65e0\u5173\u6f5c\u5728\u4fe1\u5ff5\u7f16\u7801\u5668\uff0c\u968f\u540e\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u3002", "result": "\u572810\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cMEMBOT\u8868\u73b0\u4f18\u4e8e\u65e0\u8bb0\u5fc6\u548c\u7b80\u5355\u5faa\u73af\u57fa\u7ebf\uff0c\u89c2\u6d4b\u53ef\u7528\u738750%\u65f6\u4ecd\u4fdd\u630180%\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u4fe1\u5ff5\u5efa\u6a21\u5bf9\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u8fc1\u79fb\u4e14\u6570\u636e\u9ad8\u6548\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u6709\u6548\u3002"}}
{"id": "2509.11378", "pdf": "https://arxiv.org/pdf/2509.11378", "abs": "https://arxiv.org/abs/2509.11378", "authors": ["Hadi Zayyani", "Mohammad Salman", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "A Generalized Framework for Quadratic Noise Modulation Using Non-Gaussian Distributions", "categories": ["eess.SP"], "comment": null, "summary": "This letter generalizes noise modulation by introducing two voltage biases\nand employing non-Gaussian noise distributions, such as Mixture of Gaussian\n(MoG) and Laplacian, in addition to traditional Gaussian noise. The proposed\nframework doubles the data rate by enabling discrimination in both the mean and\nvariance of transmitted noise symbols. This novel modulation scheme is referred\nto as Generalized Quadratic Noise Modulation (GQNM). Closed-form expressions\nfor the Bit Error Probability (BEP) are derived for the Generalized Gaussian\n(GG) and Gaussian Mixture of Two Gaussians (GMoTG) cases. Simulation results\ndemonstrate the advantages of the generalized modulation scheme, particularly\nunder non-Gaussian noise assumptions, highlighting its potential for enhanced\nperformance in low-power and secure communication systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u566a\u58f0\u8c03\u5236\u65b9\u6cd5\uff08GQNM\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u7535\u538b\u504f\u5dee\u548c\u975e\u9ad8\u65af\u566a\u58f0\u5206\u5e03\uff0c\u5982\u6df7\u5408\u9ad8\u65af\uff08MoG\uff09\u548c\u62c9\u666e\u62c9\u65af\u5206\u5e03\uff0c\u5b9e\u73b0\u6570\u636e\u901f\u7387\u7ffb\u500d\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u65af\u566a\u58f0\u8c03\u5236\u5728\u4f4e\u529f\u8017\u548c\u5b89\u5168\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u8c03\u5236\u65b9\u6cd5\u4ee5\u9002\u5e94\u975e\u9ad8\u65af\u566a\u58f0\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u5e7f\u4e49\u4e8c\u6b21\u566a\u58f0\u8c03\u5236\uff08GQNM\uff09\uff0c\u5229\u7528\u975e\u9ad8\u65af\u566a\u58f0\u5206\u5e03\uff08\u5982MoG\u548c\u62c9\u666e\u62c9\u65af\uff09\u548c\u4e24\u79cd\u7535\u538b\u504f\u5dee\uff0c\u652f\u6301\u5bf9\u566a\u58f0\u7b26\u53f7\u7684\u5747\u503c\u548c\u65b9\u5dee\u8fdb\u884c\u533a\u5206\u3002", "result": "\u63a8\u5bfc\u4e86\u5e7f\u4e49\u9ad8\u65af\uff08GG\uff09\u548c\u9ad8\u65af\u6df7\u5408\u53cc\u9ad8\u65af\uff08GMoTG\uff09\u60c5\u51b5\u4e0b\u7684\u6bd4\u7279\u9519\u8bef\u6982\u7387\uff08BEP\uff09\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u975e\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u4e0b\u5176\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "GQNM\u5728\u4f4e\u529f\u8017\u548c\u5b89\u5168\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u975e\u9ad8\u65af\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2509.11240", "pdf": "https://arxiv.org/pdf/2509.11240", "abs": "https://arxiv.org/abs/2509.11240", "authors": ["Yechen Zhang", "Bin Gao", "Gang Wang", "Jian Sun", "Zhuo Li"], "title": "CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "11 pages, 8 figures. Submitted to IEEE/ASME T-MECH. Code available at\n  https://github.com/ChenzycBIT/CORB-planner", "summary": "Reinforcement learning (RL) has shown promise in a large number of robotic\ncontrol tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)\nremains challenging, mainly because of reliance on accurate dynamic models and\nplatform-specific sensing, which hinders cross-platform transfer. This paper\npresents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a\nreal-time, RL-based trajectory planning framework for high-speed autonomous UAV\nflight across heterogeneous platforms. The key idea is to combine B-spline\ntrajectory generation with the RL policy producing successive control points\nwith a compact safe flight corridor (SFC) representation obtained via heuristic\nsearch. The SFC abstracts obstacle information in a low-dimensional form,\nmitigating overfitting to platform-specific details and reducing sensitivity to\nmodel inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard\nprogressive training pipeline in simulation. A value-based soft\ndecomposed-critic Q (SDCQ) algorithm is used to learn effective policies within\napproximately ten minutes of training. Benchmarks in simulation and real-world\ntests demonstrate real-time planning on lightweight onboard hardware and\nsupport maximum flight speeds up to 8.2m/s in dense, cluttered environments\nwithout external positioning. Compatibility with various UAV configurations\n(quadrotors, hexarotors) and modest onboard compute underlines the generality\nand robustness of CORB-Planner for practical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6846\u67b6CORB-Planner\uff0c\u901a\u8fc7\u7ed3\u5408B\u6837\u6761\u8f68\u8ff9\u751f\u6210\u548c\u7d27\u51d1\u7684\u5b89\u5168\u98de\u884c\u8d70\u5eca\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u9ad8\u901f\u81ea\u4e3b\u98de\u884c\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u4e0a\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u7cbe\u786e\u52a8\u6001\u6a21\u578b\u548c\u5e73\u53f0\u7279\u5b9a\u4f20\u611f\uff0c\u96be\u4ee5\u5b9e\u73b0\u8de8\u5e73\u53f0\u8fc1\u79fb\u3002", "method": "\u7ed3\u5408B\u6837\u6761\u8f68\u8ff9\u751f\u6210\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5229\u7528\u542f\u53d1\u5f0f\u641c\u7d22\u83b7\u53d6\u7d27\u51d1\u5b89\u5168\u98de\u884c\u8d70\u5eca\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u6a21\u62df\u8bad\u7ec3\u548cSDCQ\u7b97\u6cd5\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0cCORB-Planner\u5b9e\u73b0\u4e86\u5b9e\u65f6\u89c4\u5212\uff0c\u652f\u6301\u6700\u59278.2m/s\u7684\u98de\u884c\u901f\u5ea6\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u65e0\u4eba\u673a\u914d\u7f6e\u3002", "conclusion": "CORB-Planner\u901a\u8fc7\u62bd\u8c61\u969c\u788d\u4fe1\u606f\u548c\u8de8\u5e73\u53f0\u517c\u5bb9\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u9ad8\u901f\u81ea\u4e3b\u98de\u884c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11397", "pdf": "https://arxiv.org/pdf/2509.11397", "abs": "https://arxiv.org/abs/2509.11397", "authors": ["Rafi Beinhorn", "Shay Kreymer", "Amnon Balanov", "Michael Cohen", "Alon Zabatani", "Tamir Bendory"], "title": "Solving ill-conditioned polynomial equations using score-based priors with application to multi-target detection", "categories": ["eess.SP", "stat.ML"], "comment": null, "summary": "Recovering signals from low-order moments is a fundamental yet notoriously\ndifficult task in inverse problems. This recovery process often reduces to\nsolving ill-conditioned systems of polynomial equations. In this work, we\npropose a new framework that integrates score-based diffusion priors with\nmoment-based estimators to regularize and solve these nonlinear inverse\nproblems. This introduces a new role for generative models: stabilizing\npolynomial recovery from noisy statistical features. As a concrete application,\nwe study the multi-target detection (MTD) model in the high-noise regime. We\ndemonstrate two main results: (i) diffusion priors substantially improve\nrecovery from third-order moments, and (ii) they make the super-resolution MTD\nproblem, otherwise ill-posed, feasible. Numerical experiments on MNIST data\nconfirm consistent gains in reconstruction accuracy across SNR levels. Our\nresults suggest a promising new direction for combining generative priors with\nnonlinear polynomial inverse problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u5148\u9a8c\u548c\u77e9\u4f30\u8ba1\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u7ebf\u6027\u9006\u95ee\u9898\u4e2d\u7684\u591a\u9879\u5f0f\u65b9\u7a0b\u6062\u590d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u4f4e\u9636\u77e9\u6062\u590d\u4fe1\u53f7\u8fd9\u4e00\u56f0\u96be\u7684\u9006\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7a33\u5b9a\u591a\u9879\u5f0f\u6062\u590d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u5148\u9a8c\u4e0e\u77e9\u4f30\u8ba1\u76f8\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6b63\u5219\u5316\u548c\u6c42\u89e3\u975e\u7ebf\u6027\u9006\u95ee\u9898\u3002", "result": "\u6269\u6563\u5148\u9a8c\u663e\u8457\u6539\u5584\u4e86\u4ece\u4e09\u9636\u77e9\u7684\u6062\u590d\u6548\u679c\uff0c\u5e76\u4f7f\u539f\u672c\u4e0d\u9002\u5b9a\u7684\u8d85\u5206\u8fa8\u7387\u591a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u53d8\u5f97\u53ef\u884c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ed3\u5408\u751f\u6210\u5148\u9a8c\u4e0e\u975e\u7ebf\u6027\u591a\u9879\u5f0f\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.11270", "pdf": "https://arxiv.org/pdf/2509.11270", "abs": "https://arxiv.org/abs/2509.11270", "authors": ["Ziwen He", "Zhigang Wang", "Yanlong Peng", "Pengxu Chang", "Hong Yang", "Ming Chen"], "title": "Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 3 figures. Accepted at CASE2025. This arXiv version contains\n  minor corrections", "summary": "With the rapid development of the new energy vehicle industry, the efficient\ndisassembly and recycling of power batteries have become a critical challenge\nfor the circular economy. In current unstructured disassembly scenarios, the\ndynamic nature of the environment severely limits the robustness of robotic\nperception, posing a significant barrier to autonomous disassembly in\nindustrial applications. This paper proposes a continual learning framework\nbased on Neuro-Symbolic task and motion planning (TAMP) to enhance the\nadaptability of embodied intelligence systems in dynamic environments. Our\napproach integrates a multimodal perception cross-validation mechanism into a\nbidirectional reasoning flow: the forward working flow dynamically refines and\noptimizes action strategies, while the backward learning flow autonomously\ncollects effective data from historical task executions to facilitate continual\nsystem learning, enabling self-optimization. Experimental results show that the\nproposed framework improves the task success rate in dynamic disassembly\nscenarios from 81.68% to 100%, while reducing the average number of perception\nmisjudgments from 3.389 to 1.128. This research provides a new paradigm for\nenhancing the robustness and adaptability of embodied intelligence in complex\nindustrial environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5728\u52a8\u529b\u7535\u6c60\u62c6\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u65b0\u80fd\u6e90\u6c7d\u8f66\u884c\u4e1a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u52a8\u529b\u7535\u6c60\u7684\u9ad8\u6548\u62c6\u89e3\u4e0e\u56de\u6536\u6210\u4e3a\u5faa\u73af\u7ecf\u6d4e\u7684\u5173\u952e\u6311\u6218\u3002\u52a8\u6001\u73af\u5883\u4e0b\u7684\u975e\u7ed3\u6784\u5316\u62c6\u89e3\u573a\u666f\u9650\u5236\u4e86\u673a\u5668\u4eba\u611f\u77e5\u7684\u9c81\u68d2\u6027\uff0c\u963b\u788d\u4e86\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u81ea\u4e3b\u62c6\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u7684\u53cc\u5411\u63a8\u7406\u6d41\u7a0b\uff1a\u524d\u5411\u5de5\u4f5c\u6d41\u52a8\u6001\u4f18\u5316\u52a8\u4f5c\u7b56\u7565\uff0c\u540e\u5411\u5b66\u4e60\u6d41\u81ea\u4e3b\u6536\u96c6\u5386\u53f2\u4efb\u52a1\u6570\u636e\u4ee5\u652f\u6301\u6301\u7eed\u5b66\u4e60\u4e0e\u81ea\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u52a8\u6001\u62c6\u89e3\u573a\u666f\u4e2d\u7684\u4efb\u52a1\u6210\u529f\u7387\u4ece81.68%\u63d0\u5347\u81f3100%\uff0c\u611f\u77e5\u8bef\u5224\u6b21\u6570\u4ece3.389\u964d\u81f31.128\u3002", "conclusion": "\u7814\u7a76\u4e3a\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u5177\u8eab\u667a\u80fd\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.11419", "pdf": "https://arxiv.org/pdf/2509.11419", "abs": "https://arxiv.org/abs/2509.11419", "authors": ["Mengyuan Ma", "Nhan Thanh Nguyen", "Nir Shlezinger", "Yonina C. Eldar", "A. Lee Swindlehurst", "Markku Juntti"], "title": "Knowledge Distillation for Sensing-Assisted Long-Term Beam Tracking in mmWave Communications", "categories": ["eess.SP"], "comment": "14 pages, 17 figures", "summary": "Infrastructure-mounted sensors can capture rich environmental information to\nenhance communications and facilitate beamforming in millimeter-wave systems.\nThis work presents an efficient sensing-assisted long-term beam tracking\nframework that selects optimal beams from a codebook for current and multiple\nfuture time slots. We first design a large attention-enhanced neural network\n(NN) to fully exploit past visual observations for beam tracking. A\nconvolutional NN extracts compact image features, while gated recurrent units\nwith attention capture the temporal dependencies within sequences. The large NN\nthen acts as the teacher to guide the training of a lightweight student NN via\nknowledge distillation. The student requires shorter input sequences yet\npreserves long-term beam prediction ability. Numerical results demonstrate that\nthe teacher achieves Top-5 accuracies exceeding 93% for current and six future\ntime slots, approaching state-of-the-art performance with a 90% complexity\nreduction. The student closely matches the teacher's performance while cutting\ncomplexity by another 90%, despite operating with 60% shorter input sequences.\nThis improvement significantly enhances data efficiency, reduces latency, and\nlowers power consumption in sensing and processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f20\u611f\u8f85\u52a9\u957f\u671f\u6ce2\u675f\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5b66\u751f\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u3001\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u529f\u8017\u3002", "motivation": "\u5229\u7528\u57fa\u7840\u8bbe\u65bd\u5b89\u88c5\u7684\u4f20\u611f\u5668\u6355\u6349\u73af\u5883\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u901a\u4fe1\u548c\u6ce2\u675f\u6210\u5f62\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5927\u6ce8\u610f\u529b\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\uff08\u6559\u5e08\u7f51\u7edc\uff09\u8fdb\u884c\u6ce2\u675f\u8ddf\u8e2a\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b66\u751f\u7f51\u7edc\u3002\u5b66\u751f\u7f51\u7edc\u5728\u8f93\u5165\u5e8f\u5217\u66f4\u77ed\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u6559\u5e08\u7f51\u7edc\u5728\u5f53\u524d\u548c\u672a\u6765\u516d\u4e2a\u65f6\u95f4\u69fd\u7684Top-5\u51c6\u786e\u7387\u8d85\u8fc793%\uff0c\u590d\u6742\u5ea6\u964d\u4f4e90%\uff1b\u5b66\u751f\u7f51\u7edc\u6027\u80fd\u63a5\u8fd1\u6559\u5e08\u7f51\u7edc\uff0c\u590d\u6742\u5ea6\u8fdb\u4e00\u6b65\u964d\u4f4e90%\uff0c\u8f93\u5165\u5e8f\u5217\u7f29\u77ed60%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u529f\u8017\uff0c\u4e3a\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u6ce2\u675f\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11297", "pdf": "https://arxiv.org/pdf/2509.11297", "abs": "https://arxiv.org/abs/2509.11297", "authors": ["Carl Bettosi", "Lynne Ballie", "Susan Shenkin", "Marta Romeo"], "title": "Policy Learning for Social Robot-Led Physiotherapy", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Social robots offer a promising solution for autonomously guiding patients\nthrough physiotherapy exercise sessions, but effective deployment requires\nadvanced decision-making to adapt to patient needs. A key challenge is the\nscarcity of patient behavior data for developing robust policies. To address\nthis, we engaged 33 expert healthcare practitioners as patient proxies, using\ntheir interactions with our robot to inform a patient behavior model capable of\ngenerating exercise performance metrics and subjective scores on perceived\nexertion. We trained a reinforcement learning-based policy in simulation,\ndemonstrating that it can adapt exercise instructions to individual exertion\ntolerances and fluctuating performance, while also being applicable to patients\nat different recovery stages with varying exercise plans.", "AI": {"tldr": "\u793e\u4f1a\u673a\u5668\u4eba\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u57fa\u4e8e\u4e13\u5bb6\u4ee3\u7406\u6570\u636e\u751f\u6210\u7684\u60a3\u8005\u884c\u4e3a\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u60a3\u8005\u4e2a\u6027\u5316\u8fd0\u52a8\u6307\u5bfc\u548c\u9002\u5e94\u6027\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3\u793e\u4f1a\u673a\u5668\u4eba\u5728\u7269\u7406\u6cbb\u7597\u4e2d\u56e0\u60a3\u8005\u884c\u4e3a\u6570\u636e\u4e0d\u8db3\u800c\u65e0\u6cd5\u6709\u6548\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u5bb6\u4ee3\u7406\u6570\u636e\u5efa\u6a21\u60a3\u8005\u884c\u4e3a\u3002", "method": "\u5229\u752833\u540d\u533b\u7597\u4e13\u5bb6\u4f5c\u4e3a\u60a3\u8005\u4ee3\u7406\uff0c\u6a21\u62df\u60a3\u8005\u884c\u4e3a\u5e76\u751f\u6210\u8fd0\u52a8\u8868\u73b0\u6307\u6807\u548c\u4e3b\u89c2\u75b2\u52b3\u8bc4\u5206\uff0c\u57fa\u4e8e\u6b64\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u7b56\u7565\u80fd\u6839\u636e\u60a3\u8005\u8010\u529b\u548c\u8868\u73b0\u52a8\u6001\u8c03\u6574\u8fd0\u52a8\u6307\u4ee4\uff0c\u5e76\u9002\u7528\u4e8e\u4e0d\u540c\u5eb7\u590d\u9636\u6bb5\u548c\u8fd0\u52a8\u8ba1\u5212\u3002", "conclusion": "\u4e13\u5bb6\u4ee3\u7406\u6570\u636e\u5efa\u6a21\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u793e\u4f1a\u673a\u5668\u4eba\u4e2a\u6027\u5316\u7269\u7406\u6cbb\u7597\u6307\u5bfc\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.11500", "pdf": "https://arxiv.org/pdf/2509.11500", "abs": "https://arxiv.org/abs/2509.11500", "authors": ["Tian Han", "Peter J Smith", "Urbashi Mitra", "Jamie S Evans", "Robin Evans", "Rajitha Senanayake"], "title": "Dynamic Length FSK Waveforms for Joint Communications and Radar", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "15 pages, 7 figures Submitted to IEEE Transactions on Wireless\n  Communications", "summary": "Motivated by the constant modulus property of frequency shift keying (FSK)\nbased waveforms and the stabilisation of its radar performance with an increase\nin the number of subpulses, in this paper an FSK-based dynamic subpulse number\njoint communications and radar waveform design is proposed. From a\ncommunications point of view, the system operates based on traditional FSK\nmodulation. From a sensing point of view, although the subpulses are\ncontinuously generated and transmitted, radar waveforms are dynamically formed\nby monitoring the flatness of the spectrum which in turn guarantees the\naccuracy of the delay estimation. Other constraints on the waveform length are\nused to ensure satisfactory values of the root mean square time duration,\nambiguity function sidelobe levels and prevent overly long waveforms. To\nprovide an estimation of the probability of generating extremely long\nwaveforms, the distribution of the number of subpulses is approximated using a\nBrownian motion process and an existing result on its one-sided exit density.\nNumerical examples are provided to evaluate the accuracy of the approximate\ndistribution, as well as the ambiguity function sidelobe levels and the delay\nand Doppler shift estimation performance of the transmitted waveforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFSK\u7684\u52a8\u6001\u5b50\u8109\u51b2\u6570\u91cf\u8054\u5408\u901a\u4fe1\u4e0e\u96f7\u8fbe\u6ce2\u5f62\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u9891\u8c31\u5e73\u5766\u6027\u52a8\u6001\u5f62\u6210\u96f7\u8fbe\u6ce2\u5f62\uff0c\u786e\u4fdd\u5ef6\u65f6\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u907f\u514d\u4e86\u6ce2\u5f62\u8fc7\u957f\u95ee\u9898\u3002", "motivation": "\u5229\u7528FSK\u6ce2\u5f62\u7684\u6052\u6a21\u7279\u6027\u548c\u589e\u52a0\u5b50\u8109\u51b2\u6570\u91cf\u5bf9\u96f7\u8fbe\u6027\u80fd\u7684\u7a33\u5b9a\u6027\uff0c\u8bbe\u8ba1\u52a8\u6001\u5b50\u8109\u51b2\u6570\u91cf\u7684\u8054\u5408\u901a\u4fe1\u4e0e\u96f7\u8fbe\u6ce2\u5f62\u3002", "method": "\u57fa\u4e8e\u4f20\u7edfFSK\u8c03\u5236\uff0c\u52a8\u6001\u751f\u6210\u96f7\u8fbe\u6ce2\u5f62\uff1b\u901a\u8fc7\u76d1\u63a7\u9891\u8c31\u5e73\u5766\u6027\u548c\u5176\u4ed6\u7ea6\u675f\u6761\u4ef6\uff0c\u786e\u4fdd\u5ef6\u65f6\u4f30\u8ba1\u51c6\u786e\u6027\u53ca\u6ce2\u5f62\u957f\u5ea6\u5408\u7406\u3002", "result": "\u901a\u8fc7\u5e03\u6717\u8fd0\u52a8\u8fd1\u4f3c\u5b50\u8109\u51b2\u6570\u91cf\u5206\u5e03\uff0c\u9a8c\u8bc1\u4e86\u6ce2\u5f62\u8bbe\u8ba1\u7684\u5ef6\u65f6\u548c\u591a\u666e\u52d2\u9891\u79fb\u4f30\u8ba1\u6027\u80fd\u4ee5\u53ca\u6a21\u7cca\u51fd\u6570\u65c1\u74e3\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u901a\u4fe1\u4e0e\u96f7\u8fbe\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6ce2\u5f62\u8fc7\u957f\u95ee\u9898\u3002"}}
{"id": "2509.11306", "pdf": "https://arxiv.org/pdf/2509.11306", "abs": "https://arxiv.org/abs/2509.11306", "authors": ["Carl Bettosi", "Emilyann Nault", "Lynne Baillie", "Markus Garschall", "Marta Romeo", "Beatrix Wais-Zechmann", "Nicole Binderlehner", "Theodoros Georgio"], "title": "Brain-Robot Interface for Exercise Mimicry", "categories": ["cs.RO"], "comment": null, "summary": "For social robots to maintain long-term engagement as exercise instructors,\nrapport-building is essential. Motor mimicry--imitating one's physical\nactions--during social interaction has long been recognized as a powerful tool\nfor fostering rapport, and it is widely used in rehabilitation exercises where\npatients mirror a physiotherapist or video demonstration. We developed a novel\nBrain-Robot Interface (BRI) that allows a social robot instructor to mimic a\npatient's exercise movements in real-time, using mental commands derived from\nthe patient's intention. The system was evaluated in an exploratory study with\n14 participants (3 physiotherapists and 11 hemiparetic patients recovering from\nstroke or other injuries). We found our system successfully demonstrated\nexercise mimicry in 12 sessions; however, accuracy varied. Participants had\npositive perceptions of the robot instructor, with high trust and acceptance\nlevels, which were not affected by the introduction of BRI technology.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u8111\u673a\u5668\u4eba\u63a5\u53e3\uff08BRI\uff09\uff0c\u4f7f\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u60a3\u8005\u7684\u610f\u56fe\u5b9e\u65f6\u6a21\u4eff\u5176\u8fd0\u52a8\uff0c\u7528\u4e8e\u5eb7\u590d\u8bad\u7ec3\uff0c\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u6548\u679c\u826f\u597d\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u8fd0\u52a8\u6307\u5bfc\u7684\u957f\u671f\u53c2\u4e0e\u5ea6\uff0c\u7814\u7a76\u4e86\u901a\u8fc7\u6a21\u4eff\u60a3\u8005\u52a8\u4f5c\u6765\u5efa\u7acb\u60c5\u611f\u8fde\u63a5\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8111\u673a\u5668\u4eba\u63a5\u53e3\uff08BRI\uff09\u6280\u672f\uff0c\u6839\u636e\u60a3\u8005\u610f\u56fe\u751f\u6210\u6307\u4ee4\uff0c\u8ba9\u673a\u5668\u4eba\u5b9e\u65f6\u6a21\u4eff\u60a3\u8005\u8fd0\u52a8\u3002", "result": "\u572814\u540d\u53c2\u4e0e\u8005\uff08\u5305\u62ec3\u540d\u7269\u7406\u6cbb\u7597\u5e08\u548c11\u540d\u4e2d\u98ce\u60a3\u8005\uff09\u7684\u63a2\u7d22\u6027\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u5c55\u793a\u4e86\u8fd0\u52a8\u6a21\u4eff\uff0c\u4f46\u51c6\u786e\u6027\u4e0d\u4e00\uff1b\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u6307\u5bfc\u8005\u6301\u79ef\u6781\u6001\u5ea6\u3002", "conclusion": "BRI\u6280\u672f\u672a\u5f71\u54cd\u60a3\u8005\u5bf9\u673a\u5668\u4eba\u6307\u5bfc\u8005\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\uff0c\u7cfb\u7edf\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u51c6\u786e\u6027\u3002"}}
{"id": "2509.11510", "pdf": "https://arxiv.org/pdf/2509.11510", "abs": "https://arxiv.org/abs/2509.11510", "authors": ["Rishab Parthasarathy", "Michael Popik", "Noah Haefner"], "title": "Radio Frequency Amplitude-Modulation to Frequency-Modulation Signal Converter", "categories": ["eess.SP"], "comment": "23 pages, 27 figures, equal contribution", "summary": "In this project, we wanted to discover an analog topology that could\neffectively convert amplitude-modulated (AM) signals to frequency-modulated\n(FM) signals, while also ensuring that both sets of signals were within their\nrespective radio frequency (RF) bands. To that end, an effective topology for\ndoing so was developed, characterized, and demonstrated, requiring the ability\nto de-modulate incoming signals from the AM radio band--spanning from 530 kHz\nto 1700 kHz--and re-modulate these signals into the FM radio band--spanning\nfrom 88 MHz to 108 MHz. These bands are separated by roughly 86 MHz, presenting\nthe need for the topology to radically alter the incoming frequency before\nre-broadcasting. At its simplest implementation, this required an AM\ndemodulation circuit coupled to a voltage controlled oscillator (VCO).\nTogether, these two circuits translated variations in the incoming envelope\nsignal to variations in the output frequency while still maintaining\nhigh-fidelity audio, similar to how existing radio receiving and broadcasting\nare done. Altogether, the project not only developed a working system but also\nprovided valuable instruction in the design, analysis, and construction of\neffective RF circuits--invaluable to future endeavors within analog\nelectronics.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u62d3\u6251\u7ed3\u6784\uff0c\u5c06AM\u4fe1\u53f7\u8f6c\u6362\u4e3aFM\u4fe1\u53f7\uff0c\u4fdd\u6301\u97f3\u9891\u9ad8\u4fdd\u771f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eRF\u9891\u6bb5\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u6709\u6548\u7684\u6a21\u62df\u62d3\u6251\u7ed3\u6784\uff0c\u5b9e\u73b0AM\u4fe1\u53f7\u5230FM\u4fe1\u53f7\u7684\u8f6c\u6362\uff0c\u540c\u65f6\u786e\u4fdd\u4fe1\u53f7\u5728\u5404\u81eaRF\u9891\u6bb5\u5185\u3002", "method": "\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u4e00\u4e2a\u7ed3\u5408AM\u89e3\u8c03\u7535\u8def\u548c\u7535\u538b\u63a7\u5236\u632f\u8361\u5668(VCO)\u7684\u7cfb\u7edf\uff0c\u4ee5\u8f6c\u6362\u4fe1\u53f7\u9891\u7387\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u529f\u80fd\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86AM\u5230FM\u7684\u8f6c\u6362\uff0c\u5e76\u4fdd\u6301\u4e86\u97f3\u9891\u8d28\u91cf\u3002", "conclusion": "\u8be5\u9879\u76ee\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u76ee\u6807\u7cfb\u7edf\uff0c\u8fd8\u4e3a\u672a\u6765\u6a21\u62df\u7535\u5b50\u9879\u76ee\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u7ecf\u9a8c\u3002"}}
{"id": "2509.11364", "pdf": "https://arxiv.org/pdf/2509.11364", "abs": "https://arxiv.org/abs/2509.11364", "authors": ["Sheng Liu", "Zhe Li", "Weiheng Wang", "Han Sun", "Heng Zhang", "Hongpeng Chen", "Yusen Qin", "Arash Ajoudani", "Yizhao Wang"], "title": "ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation", "categories": ["cs.RO"], "comment": "6D Pose, Diffusion Policy", "summary": "Accurate 6-DoF object pose estimation and tracking are critical for reliable\nrobotic manipulation. However, zero-shot methods often fail under\nviewpoint-induced ambiguities and fixed-camera setups struggle when objects\nmove or become self-occluded. To address these challenges, we propose an active\npose estimation pipeline that combines a Vision-Language Model (VLM) with\n\"robotic imagination\" to dynamically detect and resolve ambiguities in real\ntime. In an offline stage, we render a dense set of views of the CAD model,\ncompute the FoundationPose entropy for each view, and construct a\ngeometric-aware prompt that includes low-entropy (unambiguous) and high-entropy\n(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live\nimage for an ambiguity score; (2) if ambiguity is detected, imagines a discrete\nset of candidate camera poses by rendering virtual views, scores each based on\na weighted combination of VLM ambiguity probability and FoundationPose entropy,\nand then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated\npose estimation. Furthermore, since moving objects may leave the camera's field\nof view, we introduce an active pose tracking module: a diffusion-policy\ntrained via imitation learning, which generates camera trajectories that\npreserve object visibility and minimize pose ambiguity. Experiments in\nsimulation and real-world show that our approach significantly outperforms\nclassical baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u673a\u5668\u4eba\u60f3\u8c61\u529b\u7684\u4e3b\u52a8\u4f4d\u59ff\u4f30\u8ba1\u4e0e\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u548c\u56fa\u5b9a\u6444\u50cf\u5934\u5728\u89c6\u89d2\u53d8\u6362\u548c\u7269\u4f53\u906e\u6321\u65f6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u89c6\u89d2\u53d8\u6362\u4e0b\u7684\u6a21\u7cca\u6027\u548c\u56fa\u5b9a\u6444\u50cf\u5934\u5728\u7269\u4f53\u79fb\u52a8\u6216\u81ea\u906e\u6321\u65f6\u7684\u4e0d\u8db3\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u68c0\u6d4b\u5e76\u89e3\u51b3\u5b9e\u65f6\u6a21\u7cca\u6027\u7684\u4e3b\u52a8\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u79bb\u7ebf\u9636\u6bb5\u548c\u8fd0\u884c\u65f6\u9636\u6bb5\uff1a\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u6e32\u67d3CAD\u6a21\u578b\u7684\u5bc6\u96c6\u89c6\u56fe\u5e76\u8ba1\u7b97\u71b5\u503c\u6784\u5efa\u51e0\u4f55\u611f\u77e5\u63d0\u793a\uff1b\u8fd0\u884c\u65f6\u9636\u6bb5\u901a\u8fc7VLM\u67e5\u8be2\u6a21\u7cca\u6027\u5206\u6570\uff0c\u7ed3\u5408\u865a\u62df\u89c6\u56fe\u6e32\u67d3\u9009\u62e9\u6700\u4f73\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u6269\u6563\u7b56\u7565\u8fdb\u884c\u4e3b\u52a8\u8ffd\u8e2a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408VLM\u548c\u673a\u5668\u4eba\u60f3\u8c61\u529b\uff0c\u52a8\u6001\u89e3\u51b3\u4e86\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u8ffd\u8e2a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.11533", "pdf": "https://arxiv.org/pdf/2509.11533", "abs": "https://arxiv.org/abs/2509.11533", "authors": ["Hongyang Pan", "Yanheng Liu", "Geng Sun", "Qingqing Wu", "Tierui Gong", "Pengfei Wang", "Dusit Niyato", "Chau Yuen"], "title": "Cooperative UAV-mounted RISs-assisted Energy-efficient Communications", "categories": ["eess.SP"], "comment": null, "summary": "Cooperative reconfigurable intelligent surfaces (RISs) are promising\ntechnologies for 6G networks to support a great number of users. Compared with\nthe fixed RISs, the properly deployed RISs may improve the communication\nperformance with less communication energy consumption, thereby improving the\nenergy efficiency. In this paper, we consider a cooperative unmanned aerial\nvehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs\nare carried and enhanced by UAVs to serve multiple ground users (GUs)\nsimultaneously such that achieving the three-dimensional (3D) mobility and\nopportunistic deployment. Specifically, we formulate an energy-efficient\ncommunication problem based on multi-objective optimization framework\n(EEComm-MOF) to jointly consider the beamforming vector of base station (BS),\nthe location deployment and the discrete phase shifts of UAV-RIS system so as\nto simultaneously maximize the minimum available rate over all GUs, maximize\nthe total available rate of all GUs, and minimize the total energy consumption\nof the system, while the transmit power constraint of BS is considered. To\ncomprehensively solve EEComm-MOF which is an NP-hard and non-convex problem\nwith constraints, a non-dominated sorting genetic algorithm-II with a\ncontinuous solution processing mechanism, a discrete solution processing\nmechanism, and a complex solution processing mechanism (INSGA-II-CDC) is\nproposed. Simulations results demonstrate that the proposed INSGA-II-CDC can\nsolve EEComm-MOF effectively and outperforms other benchmarks under different\nparameter settings. Moreover, the stability of INSGA-II-CDC and the\neffectiveness of the improved mechanisms are verified. Finally, the\nimplementability analysis of the algorithm is given.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u65e0\u4eba\u673a\u8f7dRIS\u7684\u534f\u4f5c\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff08EEComm-MOF\uff09\uff0c\u4ee5\u540c\u65f6\u6700\u5927\u5316\u7528\u6237\u53ef\u7528\u901f\u7387\u5e76\u6700\u5c0f\u5316\u7cfb\u7edf\u80fd\u8017\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u9057\u4f20\u7b97\u6cd5\uff08INSGA-II-CDC\uff09\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00NP\u96be\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b36G\u7f51\u7edc\u4e2d\u591a\u7528\u6237\u652f\u6301\u7684\u95ee\u9898\uff0c\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u548c\u80fd\u91cf\u6548\u7387\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u8f7dRIS\u5b9e\u73b0\u4e09\u7ef4\u79fb\u52a8\u548c\u7075\u6d3b\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff08EEComm-MOF\uff09\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u9057\u4f20\u7b97\u6cd5\uff08INSGA-II-CDC\uff09\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u3001RIS\u90e8\u7f72\u548c\u79bb\u6563\u76f8\u79fb\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u6539\u8fdb\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65e0\u4eba\u673a\u8f7dRIS\u534f\u4f5c\u7f51\u7edc\u548c\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u80fd\u663e\u8457\u63d0\u53476G\u7f51\u7edc\u7684\u80fd\u91cf\u6548\u7387\u548c\u901a\u4fe1\u6027\u80fd\uff0cINSGA-II-CDC\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002"}}
{"id": "2509.11388", "pdf": "https://arxiv.org/pdf/2509.11388", "abs": "https://arxiv.org/abs/2509.11388", "authors": ["Romerik Lokossou", "Birhanu Shimelis Girma", "Ozan K. Tonguz", "Ahmed Biyabani"], "title": "Quantum deep reinforcement learning for humanoid robot navigation task", "categories": ["cs.RO"], "comment": null, "summary": "Classical reinforcement learning (RL) methods often struggle in complex,\nhigh-dimensional environments because of their extensive parameter requirements\nand challenges posed by stochastic, non-deterministic settings. This study\nintroduces quantum deep reinforcement learning (QDRL) to train humanoid agents\nefficiently. While previous quantum RL models focused on smaller environments,\nsuch as wheeled robots and robotic arms, our work pioneers the application of\nQDRL to humanoid robotics, specifically in environments with substantial\nobservation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.\nUsing parameterized quantum circuits, we explored a hybrid quantum-classical\nsetup to directly navigate high-dimensional state spaces, bypassing traditional\nmapping and planning. By integrating quantum computing with deep RL, we aim to\ndevelop models that can efficiently learn complex navigation tasks in humanoid\nrobots. We evaluated the performance of the Soft Actor-Critic (SAC) in\nclassical RL against its quantum implementation. The results show that the\nquantum SAC achieves an 8% higher average return (246.40) than the classical\nSAC (228.36) after 92% fewer steps, highlighting the accelerated learning\npotential of quantum computing in RL tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u91cf\u5b50\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08QDRL\uff09\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u52a0\u901f\u5b66\u4e60\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u9ad8\u7ef4\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53c2\u6570\u9700\u6c42\u5927\u4e14\u96be\u4ee5\u5e94\u5bf9\u968f\u673a\u6027\u3002QDRL\u65e8\u5728\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u63d0\u5347\u6548\u7387\u3002", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u8bbe\u7f6e\uff0c\u76f4\u63a5\u5728MuJoCo\u4eba\u5f62\u673a\u5668\u4eba\uff08Humanoid-v4\u548cWalker2d-v4\uff09\u7b49\u9ad8\u7ef4\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u5e76\u4e0e\u7ecf\u5178\u7684Soft Actor-Critic\uff08SAC\uff09\u5bf9\u6bd4\u3002", "result": "\u91cf\u5b50SAC\u7684\u5e73\u5747\u56de\u62a5\uff08246.40\uff09\u6bd4\u7ecf\u5178SAC\uff08228.36\uff09\u9ad88%\uff0c\u4e14\u5b66\u4e60\u6b65\u6570\u51cf\u5c1192%\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u52a0\u901f\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2509.11542", "pdf": "https://arxiv.org/pdf/2509.11542", "abs": "https://arxiv.org/abs/2509.11542", "authors": ["Giorgi Tsintsadze", "Reza Vahdani", "James L. Drewniak", "Richard Zai"], "title": "Simplified Design Approach for Via Transitions up to 67 GHz", "categories": ["eess.SP"], "comment": null, "summary": "A systematic approach for high-speed via transition design is proposed. The\neffects of via barrel radius, anti-pad size, and the distance from adjacent\nstitching (GND) vias on bandwidth are analyzed and characterized. Guidelines\nfor selecting parameter values are provided and validated by correlating 3D\nfull-wave FEM simulation results with actual measurements of the coupon board.\nWhen a sufficient number of stitching vias are used, the via structure can be\napproximated as a coaxial transmission line. The proposed methodology builds on\nthis approximation and also considers high-order modes. With this framework,\nengineers can easily optimize design parameters while intuitively understanding\nhow geometry affects bandwidth. This approach also allows engineers with\nlimited access to expensive and computationally intensive 3D FEM tools to\ndesign high bandwidth vias up to 67 GHz.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u9ad8\u901f\u8fc7\u5b54\u8fc7\u6e21\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u8fc7\u5b54\u53c2\u6570\u5bf9\u5e26\u5bbd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u6307\u5357\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u8fc7\u5b54\u8bbe\u8ba1\u4e2d\u53c2\u6570\u4f18\u5316\u590d\u6742\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u5bf9\u6602\u8d353D FEM\u5de5\u5177\u7684\u4f9d\u8d56\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8fc7\u5b54\u534a\u5f84\u3001\u53cd\u710a\u76d8\u5c3a\u5bf8\u53ca\u76f8\u90bb\u63a5\u5730\u8fc7\u5b54\u8ddd\u79bb\u5bf9\u5e26\u5bbd\u7684\u5f71\u54cd\uff0c\u5229\u75283D FEM\u4eff\u771f\u4e0e\u5b9e\u6d4b\u6570\u636e\u9a8c\u8bc1\u8bbe\u8ba1\u6307\u5357\u3002", "result": "\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u8bbe\u8ba1\u53c2\u6570\uff0c\u652f\u6301\u5e26\u5bbd\u9ad8\u8fbe67 GHz\u7684\u8fc7\u5b54\u8bbe\u8ba1\uff0c\u4e14\u7b80\u5316\u4e86\u5de5\u7a0b\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u8bbe\u8ba1\u4f18\u5316\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u901f\u8fc7\u5b54\u8bbe\u8ba1\u7684\u6548\u7387\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2509.11402", "pdf": "https://arxiv.org/pdf/2509.11402", "abs": "https://arxiv.org/abs/2509.11402", "authors": ["Alessandra Rossi", "Patrick Holthaus", "Gabriella Lakatos", "S\u00edlvia Moros", "Ali Fallahi", "Murat Kirtay", "Marie Postma", "Erhan Oztop"], "title": "TRUST 2025: SCRITA and RTSS @ RO-MAN 2025", "categories": ["cs.RO"], "comment": "TRUST 2025 workshop proceedings containing 7 papers", "summary": "The TRUST workshop is the result of a collaboration between two established\nworkshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance\nand Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic\nSocieties). This joint initiative brings together the complementary goals of\nthese workshops to advance research on trust from both the human and robot\nperspectives.\n  Website: https://scrita.herts.ac.uk/2025/", "AI": {"tldr": "TRUST\u5de5\u4f5c\u574a\u662fSCRITA\u548cRTSS\u4e24\u4e2a\u5de5\u4f5c\u574a\u7684\u8054\u5408\u6210\u679c\uff0c\u65e8\u5728\u4ece\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u4e24\u4e2a\u89d2\u5ea6\u63a8\u8fdb\u4fe1\u4efb\u7814\u7a76\u3002", "motivation": "\u901a\u8fc7\u6574\u5408SCRITA\u548cRTSS\u7684\u4e92\u8865\u76ee\u6807\uff0c\u4fc3\u8fdb\u4ece\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u53cc\u91cd\u89c6\u89d2\u7684\u4fe1\u4efb\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u8054\u5408\u4e24\u4e2a\u5df2\u5efa\u7acb\u7684\u5de5\u4f5c\u574a\uff0c\u7ed3\u5408\u5404\u81ea\u7684\u7814\u7a76\u65b9\u5411\u4e0e\u6210\u679c\u3002", "result": "TRUST\u5de5\u4f5c\u574a\u6210\u529f\u6574\u5408\u4e86\u4e24\u4e2a\u5de5\u4f5c\u574a\u7684\u8d44\u6e90\u4e0e\u76ee\u6807\u3002", "conclusion": "TRUST\u5de5\u4f5c\u574a\u4e3a\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5408\u4f5c\u5e73\u53f0\u3002"}}
{"id": "2509.11551", "pdf": "https://arxiv.org/pdf/2509.11551", "abs": "https://arxiv.org/abs/2509.11551", "authors": ["Yida Zhang", "Qiuyan Liu", "Hongtao Luo", "Yuqi Xia", "Qiang Wang"], "title": "Stacked Intelligent Metasurface for End-to-End OFDM System", "categories": ["eess.SP"], "comment": null, "summary": "Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabled\nwave-domain signal processing have emerged as promising research directions for\noffloading baseband digital processing tasks and efficiently simplifying\ntransceiver design. However, existing architectures are limited to employing\nSIM (DPSIM) for a single communication function, such as precoding or\ncombining. To further enhance the overall performance of SIM (DPSIM)-assisted\nsystems and achieve end-to-end (E2E) joint optimization from the transmitted\nbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2E\northogonal frequency-division multiplexing (OFDM) system, where traditional\ncommunication tasks such as modulation, precoding, combining, and demodulation\nare performed simultaneously during electromagnetic (EM) forward propagation.\nFurthermore, inspired by the idea of abstracting real metasurfaces as hidden\nlayers of a neural network, we propose the electromagnetic neural network\n(EMNN) to enable the control of the E2E OFDM communication system. In addition,\ntransfer learning is introduced into the model training, and a training and\ndeployment framework for the EMNN is designed. Simulation results demonstrate\nthat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems can\nachieve robust bitstream transmission under complex channel conditions. Our\nstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2E\nOFDM systems in the design of next-generation transceivers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5806\u53e0\u667a\u80fd\u8d85\u8868\u9762\uff08SIM/DPSIM\uff09\u7684\u7aef\u5230\u7aefOFDM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7535\u78c1\u795e\u7ecf\u7f51\u7edc\uff08EMNN\uff09\u5b9e\u73b0\u901a\u4fe1\u4efb\u52a1\u7684\u5168\u94fe\u8def\u4f18\u5316\uff0c\u5c55\u793a\u5176\u5728\u590d\u6742\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SIM/DPSIM\u67b6\u6784\u4ec5\u652f\u6301\u5355\u4e00\u901a\u4fe1\u529f\u80fd\uff08\u5982\u9884\u7f16\u7801\u6216\u7ec4\u5408\uff09\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u3002", "method": "\u5229\u7528SIM/DPSIM\u5728\u6267\u884cOFDM\u4efb\u52a1\uff08\u5982\u8c03\u5236\u3001\u9884\u7f16\u7801\u3001\u89e3\u8c03\uff09\u7684\u540c\u65f6\u8fdb\u884c\u7535\u78c1\u524d\u5411\u4f20\u64ad\uff0c\u5e76\u901a\u8fc7\u62bd\u8c61\u8d85\u8868\u9762\u4e3a\u795e\u7ecf\u7f51\u7edc\u9690\u85cf\u5c42\u8bbe\u8ba1EMNN\u6846\u67b6\uff0c\u5f15\u5165\u8fc1\u79fb\u5b66\u4e60\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0cSIM/DPSIM\u8f85\u52a9\u7684\u7aef\u5230\u7aefOFDM\u7cfb\u7edf\u5728\u590d\u6742\u4fe1\u9053\u4e0b\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u6bd4\u7279\u6d41\u4f20\u8f93\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86EMNN\u548cSIM/DPSIM\u8f85\u52a9OFDM\u7cfb\u7edf\u5728\u672a\u6765\u6536\u53d1\u5668\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.11417", "pdf": "https://arxiv.org/pdf/2509.11417", "abs": "https://arxiv.org/abs/2509.11417", "authors": ["Shresth Grover", "Akshay Gopalkrishnan", "Bo Ai", "Henrik I. Christensen", "Hao Su", "Xuanlin Li"], "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project Page: https://gen-vla.github.io/", "summary": "Vision-language-action (VLA) models finetuned from vision-language models\n(VLMs) hold the promise of leveraging rich pretrained representations to build\ngeneralist robots across diverse tasks and environments. However, direct\nfine-tuning on robot data often disrupts these representations and limits\ngeneralization. We present a framework that better preserves pretrained\nfeatures while adapting them for robot manipulation. Our approach introduces\nthree components: (i) a dual-encoder design with one frozen vision encoder to\nretain pretrained features and another trainable for task adaptation, (ii) a\nstring-based action tokenizer that casts continuous actions into character\nsequences aligned with the model's pretraining domain, and (iii) a co-training\nstrategy that combines robot demonstrations with vision-language datasets\nemphasizing spatial reasoning and affordances. Evaluations in simulation and on\nreal robots show that our method improves robustness to visual perturbations,\ngeneralization to novel instructions and environments, and overall task success\ncompared to baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53cc\u91cd\u7f16\u7801\u5668\u8bbe\u8ba1\u3001\u5b57\u7b26\u4e32\u52a8\u4f5c\u5206\u8bcd\u5668\u548c\u5171\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u4fdd\u7559\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u5e76\u9002\u5e94\u673a\u5668\u4eba\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76f4\u63a5\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5fae\u8c03\u5f97\u5230\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u5904\u7406\u673a\u5668\u4eba\u4efb\u52a1\u65f6\u53ef\u80fd\u4f1a\u7834\u574f\u9884\u8bad\u7ec3\u7684\u8868\u5f81\u5e76\u9650\u5236\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u4fdd\u7559\u7279\u5f81\u5e76\u9002\u5e94\u673a\u5668\u4eba\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e86\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\uff08i\uff09\u53cc\u91cd\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u4e00\u4e2a\u51bb\u7ed3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4fdd\u7559\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u53e6\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u9002\u5e94\u4efb\u52a1\uff1b\uff08ii\uff09\u5c06\u8fde\u7eed\u52a8\u4f5c\u8f6c\u6362\u4e3a\u5b57\u7b26\u5e8f\u5217\u7684\u5b57\u7b26\u4e32\u52a8\u4f5c\u5206\u8bcd\u5668\uff1b\uff08iii\uff09\u7ed3\u5408\u673a\u5668\u4eba\u6f14\u793a\u548c\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u7684\u5171\u540c\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3001\u5bf9\u65b0\u6307\u4ee4\u548c\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4fdd\u7559\u9884\u8bad\u7ec3\u7279\u5f81\u7684\u540c\u65f6\u6210\u529f\u9002\u5e94\u4e86\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.11571", "pdf": "https://arxiv.org/pdf/2509.11571", "abs": "https://arxiv.org/abs/2509.11571", "authors": ["Zhiyuan Liu", "Qingyu Liu", "Shuhang Zhang", "Hongliang Zhang", "Lingyang Song"], "title": "RadioLAM: A Large AI Model for Fine-Grained 3D Radio Map Estimation", "categories": ["eess.SP"], "comment": "Submitted to IEEE JSAC", "summary": "A radio map captures the spatial distribution of wireless channel parameters,\nsuch as the strength of the signal received, across a geographic area. The\nproblem of fine-grained three-dimensional (3D) radio map estimation involves\ninferring a high-resolution radio map for the two-dimensional (2D) area at an\narbitrary target height within a 3D region of interest, using radio samples\ncollected by sensors sparsely distributed in that 3D region. Solutions to the\nproblem are crucial for efficient spectrum management in 3D spaces,\nparticularly for drones in the rapidly developing low-altitude economy.\nHowever, this problem is challenging due to ultra-sparse sampling, where the\nnumber of collected radio samples is far fewer than the desired resolution of\nthe radio map to be estimated. In this paper, we design a Large Artificial\nIntelligence Model (LAM) called RadioLAM for the problem. RadioLAM employs the\ncreative power and the strong generalization capability of LAM to address the\nultra-sparse sampling challenge. It consists of three key blocks: 1) an\naugmentation block, using the radio propagation model to project the radio\nsamples collected at different heights to the 2D area at the target height; 2)\na generation block, leveraging an LAM under an Mixture of Experts (MoE)\narchitecture to generate a candidate set of fine-grained radio maps for the\ntarget 2D area; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best map from the candidate set. Extensive simulations\nshow that RadioLAM is able to solve the fine-grained 3D radio map estimation\nproblem efficiently from an ultra-low sampling rate of 0.1%, and significantly\noutperforms the state-of-the-art.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadioLAM\u7684\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u4e09\u7ef4\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d85\u7a00\u758f\u91c7\u6837\u6761\u4ef6\u4e0b\u7684\u9ad8\u5206\u8fa8\u7387\u5730\u56fe\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u7b49\u8bbe\u5907\u5bf9\u4e09\u7ef4\u7a7a\u95f4\u9891\u8c31\u7ba1\u7406\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u8d85\u7a00\u758f\u91c7\u6837\u95ee\u9898\u4f7f\u5f97\u9ad8\u5206\u8fa8\u7387\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u4f30\u8ba1\u6781\u5177\u6311\u6218\u6027\u3002", "method": "RadioLAM\u7531\u4e09\u4e2a\u5173\u952e\u6a21\u5757\u7ec4\u6210\uff1a1\uff09\u589e\u5f3a\u6a21\u5757\uff0c\u5229\u7528\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u578b\u5c06\u4e0d\u540c\u9ad8\u5ea6\u91c7\u96c6\u7684\u6837\u672c\u6295\u5f71\u5230\u76ee\u6807\u9ad8\u5ea6\u7684\u4e8c\u7ef4\u533a\u57df\uff1b2\uff09\u751f\u6210\u6a21\u5757\uff0c\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08MoE\uff09\u7684\u5927\u578bAI\u6a21\u578b\u751f\u6210\u5019\u9009\u65e0\u7ebf\u7535\u5730\u56fe\uff1b3\uff09\u9009\u4e3e\u6a21\u5757\uff0c\u901a\u8fc7\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u578b\u9009\u51fa\u6700\u4f73\u5730\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRadioLAM\u80fd\u5728\u91c7\u6837\u7387\u4f4e\u81f30.1%\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "RadioLAM\u4e3a\u89e3\u51b3\u8d85\u7a00\u758f\u91c7\u6837\u6761\u4ef6\u4e0b\u7684\u4e09\u7ef4\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.11433", "pdf": "https://arxiv.org/pdf/2509.11433", "abs": "https://arxiv.org/abs/2509.11433", "authors": ["Pedro Portugal", "Damian D. Venghaus", "Diego Lopez"], "title": "A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs", "categories": ["cs.RO"], "comment": "21 pages, 10 figures, Python post-processor source code and web\n  interface included", "summary": "Affordable desktop CNC routers are common in education, prototyping, and\nmakerspaces, but most lack a rotary axis, limiting fabrication of rotationally\nsymmetric or multi-sided parts. Existing solutions often require hardware\nretrofits, alternative controllers, or commercial CAM software, raising cost\nand complexity. This work presents a software-only framework for indexed rotary\nmachining on GRBL-based CNCs. A custom post-processor converts planar toolpaths\ninto discrete rotary steps, executed through a browser-based interface. While\nnot equivalent to continuous 4-axis machining, the method enables practical\nrotary-axis fabrication using only standard, off-the-shelf mechanics, without\nfirmware modification. By reducing technical and financial barriers, the\nframework expands access to multi-axis machining in classrooms, makerspaces,\nand small workshops, supporting hands-on learning and rapid prototyping.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f6f\u4ef6\u6846\u67b6\uff0c\u4f7fGRBL\u6570\u63a7\u673a\u5e8a\u5b9e\u73b0\u4f4e\u6210\u672c\u65cb\u8f6c\u52a0\u5de5\uff0c\u65e0\u9700\u786c\u4ef6\u6539\u9020\u6216\u5546\u4e1aCAM\u8f6f\u4ef6\u3002", "motivation": "\u89e3\u51b3\u6559\u80b2\u3001\u539f\u578b\u8bbe\u8ba1\u548c\u521b\u5ba2\u7a7a\u95f4\u4e2d\u684c\u9762CNC\u673a\u5e8a\u7f3a\u4e4f\u65cb\u8f6c\u8f74\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u6280\u672f\u548c\u6210\u672c\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u5b9a\u5236\u540e\u5904\u7406\u5668\u5c06\u5e73\u9762\u5200\u5177\u8def\u5f84\u8f6c\u6362\u4e3a\u79bb\u6563\u65cb\u8f6c\u6b65\u9aa4\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u754c\u9762\u6267\u884c\u3002", "result": "\u5b9e\u73b0\u4e86\u7528\u6807\u51c6\u786c\u4ef6\u8fdb\u884c\u65cb\u8f6c\u52a0\u5de5\uff0c\u65e0\u9700\u56fa\u4ef6\u4fee\u6539\uff0c\u9002\u7528\u4e8e\u6559\u5b66\u548c\u5feb\u901f\u539f\u578b\u5236\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u591a\u8f74\u52a0\u5de5\u7684\u53ef\u53ca\u6027\uff0c\u652f\u6301\u6559\u80b2\u548c\u521b\u5ba2\u7684\u5b9e\u8df5\u9700\u6c42\u3002"}}
{"id": "2509.11607", "pdf": "https://arxiv.org/pdf/2509.11607", "abs": "https://arxiv.org/abs/2509.11607", "authors": ["Jun Wu", "Yaoqi Yang", "Weijie Yuan", "Wenchao Liu", "Jiacheng Wang", "Tianqi Mao", "Lin Zhou", "Yuanhao Cui", "Fan Liu", "Geng Sun", "Nan Wu", "Dezhi Zheng", "Jindan Xu", "Nan Ma", "Zhiyong Feng", "Wei Xu", "Dusit Niyato", "Chau Yuen", "Xiaojun Jing", "Zhiguo Shi", "Yingchang Liang", "Shi Jin", "Dong In Kim", "Jiangzhou Wang", "Ping Zhang", "Hao Yin", "Jun Zhang"], "title": "Low-Altitude Wireless Networks: A Survey", "categories": ["eess.SP"], "comment": null, "summary": "The rapid development of the low-altitude economy has imposed unprecedented\ndemands on wireless infrastructure to accommodate large-scale drone deployments\nand facilitate intelligent services in dynamic airspace environments. However,\nunlocking its full potential in practical applications presents significant\nchallenges. Traditional aerial systems predominantly focus on air-ground\ncommunication services, often neglecting the integration of sensing,\ncomputation, control, and energy-delivering functions, which hinders the\nability to meet diverse mission-critical demands. Besides, the absence of\nsystematic low-altitude airspace planning and management exacerbates issues\nregarding dynamic interference in three-dimensional space, coverage\ninstability, and scalability. To overcome these challenges, a comprehensive\nframework, termed low-altitude wireless network (LAWN), has emerged to\nseamlessly integrate communication, sensing, computation, control, and air\ntraffic management into a unified design. This article provides a comprehensive\noverview of LAWN systems, introducing LAWN system fundamentals and the\nevolution of functional designs. Subsequently, we delve into performance\nevaluation metrics and review critical concerns surrounding privacy and\nsecurity in the open-air network environment. Finally, we present the\ncutting-edge developments in airspace structuring and air traffic management,\nproviding insights to facilitate the practical deployment of LAWNs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u7684\u7efc\u5408\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u4f4e\u7a7a\u7ecf\u6d4e\u9700\u6c42\u4e0e\u4f20\u7edf\u7a7a\u4e2d\u7cfb\u7edf\u5c40\u9650\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u901a\u4fe1\u3001\u611f\u77e5\u3001\u8ba1\u7b97\u3001\u63a7\u5236\u548c\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u4e2d\u7684\u4e00\u4f53\u5316\u8bbe\u8ba1\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u65e0\u7ebf\u57fa\u7840\u8bbe\u65bd\u63d0\u51fa\u4e86\u524d\u6240\u672a\u6709\u7684\u9700\u6c42\uff0c\u4f46\u76ee\u524d\u4f20\u7edf\u7a7a\u4e2d\u7cfb\u7edf\u672a\u80fd\u6574\u5408\u591a\u529f\u80fd\u670d\u52a1\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u4f4e\u7a7a\u7a7a\u57df\u89c4\u5212\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u3001\u611f\u77e5\u3001\u8ba1\u7b97\u3001\u63a7\u5236\u548c\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u7edf\u4e00\u6574\u5408\uff0c\u5e76\u5bf9\u5176\u6027\u80fd\u8bc4\u4f30\u3001\u9690\u79c1\u5b89\u5168\u53ca\u5b9e\u9645\u90e8\u7f72\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002", "result": "LAWN\u6846\u67b6\u4e3a\u52a8\u6001\u7a7a\u57df\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u5927\u89c4\u6a21\u90e8\u7f72\u548c\u667a\u80fd\u670d\u52a1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u4f4e\u7a7a\u7a7a\u57df\u7ed3\u6784\u5316\u4e0e\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u7684\u524d\u6cbf\u53d1\u5c55\u3002", "conclusion": "LAWN\u6846\u67b6\u901a\u8fc7\u4e00\u4f53\u5316\u8bbe\u8ba1\u89e3\u51b3\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u591a\u7ef4\u5ea6\u9700\u6c42\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u4f46\u9690\u79c1\u4e0e\u5b89\u5168\u95ee\u9898\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.11481", "pdf": "https://arxiv.org/pdf/2509.11481", "abs": "https://arxiv.org/abs/2509.11481", "authors": ["Jonas Eschmann", "Dario Albani", "Giuseppe Loianno"], "title": "RAPTOR: A Foundation Policy for Quadrotor Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans are remarkably data-efficient when adapting to new unseen conditions,\nlike driving a new car. In contrast, modern robotic control systems, like\nneural network policies trained using Reinforcement Learning (RL), are highly\nspecialized for single environments. Because of this overfitting, they are\nknown to break down even under small differences like the Simulation-to-Reality\n(Sim2Real) gap and require system identification and retraining for even\nminimal changes to the system. In this work, we present RAPTOR, a method for\ntraining a highly adaptive foundation policy for quadrotor control. Our method\nenables training a single, end-to-end neural-network policy to control a wide\nvariety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg\nthat also differ in motor type (brushed vs. brushless), frame type (soft vs.\nrigid), propeller type (2/3/4-blade), and flight controller\n(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy\nwith only 2084 parameters is sufficient for zero-shot adaptation to a wide\nvariety of platforms. The adaptation through In-Context Learning is made\npossible by using a recurrence in the hidden layer. The policy is trained\nthrough a novel Meta-Imitation Learning algorithm, where we sample 1000\nquadrotors and train a teacher policy for each of them using Reinforcement\nLearning. Subsequently, the 1000 teachers are distilled into a single, adaptive\nstudent policy. We find that within milliseconds, the resulting foundation\npolicy adapts zero-shot to unseen quadrotors. We extensively test the\ncapabilities of the foundation policy under numerous conditions (trajectory\ntracking, indoor/outdoor, wind disturbance, poking, different propellers).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAPTOR\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u9ad8\u5ea6\u81ea\u9002\u5e94\u7684\u57fa\u7840\u7b56\u7565\uff0c\u4ee5\u63a7\u5236\u591a\u79cd\u4e0d\u540c\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u9002\u5e94\u6027\u5f3a\u4e14\u96f6\u6837\u672c\u9ad8\u6548\u3002", "motivation": "\u4eba\u7c7b\u5728\u9762\u5bf9\u65b0\u73af\u5883\u65f6\u80fd\u9ad8\u6548\u9002\u5e94\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7f51\u7edc\u7b56\u7565\uff09\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u9002\u5e94\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528Meta-Imitation Learning\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec31000\u4e2a\u6559\u5e08\u7b56\u7565\u5e76\u5c06\u5176\u77e5\u8bc6\u84b8\u998f\u4e3a\u4e00\u4e2a\u81ea\u9002\u5e94\u5b66\u751f\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec52084\u53c2\u6570\u7684\u6781\u5c0f\u7b56\u7565\u80fd\u5728\u6beb\u79d2\u7ea7\u5185\u9002\u5e9410\u79cd\u4e0d\u540c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u5e76\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RAPTOR\u65b9\u6cd5\u5c55\u793a\u4e86\u9ad8\u5ea6\u81ea\u9002\u5e94\u57fa\u7840\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u7684\u6cdb\u5316\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.11725", "pdf": "https://arxiv.org/pdf/2509.11725", "abs": "https://arxiv.org/abs/2509.11725", "authors": ["Mengyuan Ma", "Nhan Thanh Nguyen", "Nir Shlezinger", "Yonina C. Eldar", "Markku Juntti"], "title": "Attention-Enhanced Learning for Sensing-Assisted Long-Term Beam Tracking in mmWave Communications", "categories": ["eess.SP"], "comment": "5 pages, 6 figures, submitted to ICASSP2026", "summary": "Beam training and prediction in millimeter-wave communications are highly\nchallenging due to fast time-varying channels and sensitivity to blockages and\nmobility. In this context, infrastructure-mounted cameras can capture rich\nenvironmental information that can facilitate beam tracking design. In this\nwork, we develop an efficient attention-enhanced machine learning model for\nlong-term beam tracking built upon convolutional neural networks and gated\nrecurrent units to predict both current and future beams from past observed\nimages. The integrated temporal attention mechanism substantially improves its\npredictive performance. Numerical results demonstrate that the proposed design\nachieves Top-5 beam prediction accuracies exceeding 90% across both current and\nsix future time slots, significantly reducing overhead arising from sensing and\nprocessing for beam training. It further attains 97% of state-of-the-art\nperformance with only 3% of the computational complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6ce2\u675f\u8ffd\u8e2a\uff0c\u901a\u8fc7\u7ed3\u5408CNN\u548cGRU\u4ee5\u53ca\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u6ce2\u675f\u8bad\u7ec3\u548c\u9884\u6d4b\u56e0\u5feb\u901f\u53d8\u5316\u7684\u4fe1\u9053\u548c\u5bf9\u963b\u585e\u53ca\u79fb\u52a8\u7684\u654f\u611f\u6027\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u5229\u7528\u57fa\u7840\u8bbe\u65bd\u6444\u50cf\u5934\u6355\u83b7\u7684\u73af\u5883\u4fe1\u606f\u53ef\u8f85\u52a9\u6ce2\u675f\u8ddf\u8e2a\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u8fc7\u53bb\u7684\u89c2\u6d4b\u56fe\u50cf\u4e2d\u9884\u6d4b\u5f53\u524d\u548c\u672a\u6765\u7684\u6ce2\u675f\u3002", "result": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u5728Top-5\u6ce2\u675f\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u8d85\u8fc790%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6ce2\u675f\u8bad\u7ec3\u7684\u611f\u77e5\u548c\u5904\u7406\u5f00\u9500\uff0c\u4e14\u4ec5\u75283%\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8fbe\u523097%\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6ce2\u675f\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11504", "pdf": "https://arxiv.org/pdf/2509.11504", "abs": "https://arxiv.org/abs/2509.11504", "authors": ["Yidan Lu", "Yinzhao Dong", "Jiahui Zhang", "Ji Ma", "Peng Lu"], "title": "FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction", "categories": ["cs.RO"], "comment": "Published in IEEE Robotics and Automation Letters, Vol. 10, No. 7,\n  pp. 6632-6639, 2025", "summary": "Fall recovery for legged robots remains challenging, particularly on complex\nterrains where traditional controllers fail due to incomplete terrain\nperception and uncertain interactions. We present \\textbf{FR-Net}, a\nlearning-based framework that enables quadrupedal robots to recover from\narbitrary fall poses across diverse environments. Central to our approach is a\nMass-Contact Predictor network that estimates the robot's mass distribution and\ncontact states from limited sensory inputs, facilitating effective recovery\nstrategies. Our carefully designed reward functions ensure safe recovery even\non steep stairs without dangerous rolling motions common to existing methods.\nTrained entirely in simulation using privileged learning, our framework guides\npolicy learning without requiring explicit terrain data during deployment. We\ndemonstrate the generalization capabilities of \\textbf{FR-Net} across different\nquadrupedal platforms in simulation and validate its performance through\nextensive real-world experiments on the Go2 robot in 10 challenging scenarios.\nOur results indicate that explicit mass-contact prediction is key to robust\nfall recovery, offering a promising direction for generalizable quadrupedal\nskills.", "AI": {"tldr": "FR-Net\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u8d28\u91cf\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u8dcc\u5012\u6062\u590d\u3002", "motivation": "\u5728\u590d\u6742\u5730\u5f62\u4e0a\uff0c\u4f20\u7edf\u63a7\u5236\u5668\u56e0\u611f\u77e5\u4e0d\u5b8c\u6574\u548c\u4ea4\u4e92\u4e0d\u786e\u5b9a\u800c\u5931\u8d25\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8dcc\u5012\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Mass-Contact Predictor\u7f51\u7edc\u4ece\u6709\u9650\u4f20\u611f\u5668\u8f93\u5165\u9884\u6d4b\u8d28\u91cf\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u7ed3\u5408\u5956\u52b1\u51fd\u6570\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86FR-Net\u5728\u591a\u79cd\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u6062\u590d\u4e14\u907f\u514d\u5371\u9669\u6eda\u52a8\u52a8\u4f5c\u3002", "conclusion": "\u660e\u786e\u7684\u8d28\u91cf-\u63a5\u89e6\u9884\u6d4b\u662f\u9c81\u68d2\u8dcc\u5012\u6062\u590d\u7684\u5173\u952e\uff0c\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u6280\u80fd\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.11923", "pdf": "https://arxiv.org/pdf/2509.11923", "abs": "https://arxiv.org/abs/2509.11923", "authors": ["Mingjun Ying", "Peijie Ma", "Dipankar Shakya", "Theodore S. Rappaport"], "title": "Multi-Stage Location Optimization Through Power Delay Profile Alignment Using Site-Specific Wireless Ray Tracing", "categories": ["eess.SP"], "comment": "6 pages, 3 figures, 2 tables", "summary": "Ray tracing (RT) simulations require accurate transmitter (TX) and receiver\n(RX) location information from real-world measurements to accurately\ncharacterize wireless propagation behavior in an environment. Such wireless\npropagation measurements typically employ GPS-based logging for TX/RX\nlocations, which can produce meter-level errors that lead to unreliable RT\ncalibration and validation. These location misalignments cause inaccurate\ninteractions between RT-generated multipath components (MPCs) and the modeled\n3D environment, which lead to erroneous channel predictions, and severe\ndiscrepancies between simulated and measured power delay profiles (PDPs) and\nchannel characteristics. Moreover, the same RT-generated PDPs using inaccurate\nlocations result in calibration errors when adjusting material properties such\nas conductivity and permittivity.\n  This paper presents a systematic multi-stage TX/RX location calibration\nframework to correct location errors and consequently align measured and\nsimulated omnidirectional PDPs.\n  Optimization is performed using a computationally efficient multi-stage grid\nsearch and the Powell method. Applying the location calibration framework to\nNYU WIRELESS urban-microcell (UMi) measurements at 6.75 GHz and 16.95 GHz\ncorrected TX/RX location errors of up to 7 m. The framework reduced the\ncomposite loss function by 42.3\\% for line-of-sight (LOS) and 13.5\\% for\nnon-line-of-sight (NLOS) scenarios. Furthermore, peak power prediction accuracy\nimproved by approximately 1 dB on average. Such improved geometric alignment\nenables accurate channel prediction, vital for beam management and\ninfrastructure deployment for next-generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5TX/RX\u4f4d\u7f6e\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u7ea0\u6b63GPS\u5b9a\u4f4d\u8bef\u5dee\uff0c\u63d0\u9ad8\u5c04\u7ebf\u8ffd\u8e2a\u4eff\u771f\u4e0e\u5b9e\u6d4b\u6570\u636e\u7684\u5bf9\u9f50\u5ea6\uff0c\u4f18\u5316\u65e0\u7ebf\u4f20\u64ad\u884c\u4e3a\u5206\u6790\u3002", "motivation": "GPS\u5b9a\u4f4d\u8bef\u5dee\u5bfc\u81f4\u5c04\u7ebf\u8ffd\u8e2a\u4eff\u771f\u4e0e\u5b9e\u6d4b\u6570\u636e\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u65e0\u7ebf\u4fe1\u9053\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e9f\u9700\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u7f51\u683c\u641c\u7d22\u548cPowell\u65b9\u6cd5\u4f18\u5316TX/RX\u4f4d\u7f6e\uff0c\u6821\u51c6\u5b9e\u6d4b\u4e0e\u4eff\u771f\u5168\u5411\u529f\u7387\u5ef6\u8fdf\u5256\u9762\u3002", "result": "\u5728\u6821\u6b63\u4f4d\u7f6e\u8bef\u5dee\u540e\uff0cLOS\u548cNLOS\u573a\u666f\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\u5206\u522b\u51cf\u5c1142.3%\u548c13.5%\uff0c\u5cf0\u503c\u529f\u7387\u9884\u6d4b\u7cbe\u5ea6\u5e73\u5747\u63d0\u9ad81 dB\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u9053\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5bf9\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u6ce2\u675f\u7ba1\u7406\u548c\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.11506", "pdf": "https://arxiv.org/pdf/2509.11506", "abs": "https://arxiv.org/abs/2509.11506", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Kei Okada"], "title": "Design and Development of a Remotely Wire-Driven Walking Robot", "categories": ["cs.RO"], "comment": "Accepted Humanoids2025, website -\n  https://hatofly.github.io/remote-wire-driven-quadruped/", "summary": "Operating in environments too harsh or inaccessible for humans is one of the\ncritical roles expected of robots. However, such environments often pose risks\nto electronic components as well. To overcome this, various approaches have\nbeen developed, including autonomous mobile robots without electronics,\nhydraulic remotely actuated mobile robots, and long-reach robot arms driven by\nwires. Among these, electronics-free autonomous robots cannot make complex\ndecisions, while hydraulically actuated mobile robots and wire-driven robot\narms are used in harsh environments such as nuclear power plants. Mobile robots\noffer greater reach and obstacle avoidance than robot arms, and wire mechanisms\noffer broader environmental applicability than hydraulics. However, wire-driven\nsystems have not been used for remote actuation of mobile robots. In this\nstudy, we propose a novel mechanism called Remote Wire Drive that enables\nremote actuation of mobile robots via wires. This mechanism is a series\nconnection of decoupled joints, a mechanism used in wire-driven robot arms,\nadapted for power transmission. We experimentally validated its feasibility by\nactuating a wire-driven quadruped robot, which we also developed in this study,\nthrough Remote Wire Drive.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRemote Wire Drive\u7684\u65b0\u578b\u673a\u5236\uff0c\u901a\u8fc7\u7535\u7ebf\u8fdc\u7a0b\u9a71\u52a8\u79fb\u52a8\u673a\u5668\u4eba\u3002", "motivation": "\u89e3\u51b3\u5728\u6076\u52a3\u73af\u5883\u4e2d\u7535\u5b50\u8bbe\u5907\u6613\u53d7\u635f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u65e0\u7535\u5b50\u81ea\u4e3b\u673a\u5668\u4eba\u51b3\u7b56\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89e3\u8026\u5173\u8282\u7684\u4e32\u8054\u8fde\u63a5\u673a\u5236\uff0c\u9002\u914d\u7535\u7ebf\u9a71\u52a8\u7684\u673a\u5668\u4eba\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7535\u7ebf\u9a71\u52a8\u7684\u56db\u8db3\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Remote Wire Drive\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u9a71\u52a8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u3002", "conclusion": "Remote Wire Drive\u4e3a\u6076\u52a3\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u8fdc\u7a0b\u9a71\u52a8\u65b9\u6848\u3002"}}
{"id": "2509.11994", "pdf": "https://arxiv.org/pdf/2509.11994", "abs": "https://arxiv.org/abs/2509.11994", "authors": ["Souvik Paul", "Iv\u00e1n Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu"], "title": "Optimized Sparse Network Coverage via L1-norm Minimization", "categories": ["eess.SP"], "comment": "Submitted for IEEE CAMSAP 2025 conference", "summary": "The selection of nodes that can serve as cluster heads, local sinks and\ngateways is a critical challenge in distributed sensor and communication\nnetworks. This paper presents a novel framework for identifying a minimal set\nof nexus nodes to ensure full network coverage while minimizing cost. By\nformulating the problem as a convex relaxation of the NP-hard set cover\nproblem, we integrate the graph theoretic centrality measures of node degree\nand betweenness centrality into a cost function optimized via a relaxed L1-norm\nminimization. The proposed approach is applicable to static and dynamic network\nscenarios and does not require location or distance estimation. Through\nsimulations across various graph models and dynamic conditions, it is shown\nthat the method achieves faster execution times (lower complexity) and\ncompetitive sparsity compared to classical greedy and genetic algorithms (GA),\noffering a robust, distributed, and cost-efficient node selection solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u677e\u5f1b\u4f18\u5316\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u9ad8\u6548\u9009\u62e9\u5173\u952e\u8282\u70b9\u4ee5\u5b9e\u73b0\u5168\u8986\u76d6\u5e76\u6700\u5c0f\u5316\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u548c\u901a\u4fe1\u7f51\u7edc\u4e2d\u5173\u952e\u8282\u70b9\u9009\u62e9\u7684\u590d\u6742\u6027\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3aNP\u96be\u96c6\u5408\u8986\u76d6\u95ee\u9898\u7684\u51f8\u677e\u5f1b\uff0c\u7ed3\u5408\u8282\u70b9\u5ea6\u548c\u4ecb\u6570\u4e2d\u5fc3\u6027\uff0c\u901a\u8fc7L1\u8303\u6570\u6700\u5c0f\u5316\u4f18\u5316\u3002", "result": "\u5728\u5404\u79cd\u56fe\u548c\u52a8\u6001\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u6bd4\u8d2a\u5fc3\u548c\u9057\u4f20\u7b97\u6cd5\u66f4\u5feb\u3001\u66f4\u7a00\u758f\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5206\u5e03\u5f0f\u3001\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u8282\u70b9\u9009\u62e9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11516", "pdf": "https://arxiv.org/pdf/2509.11516", "abs": "https://arxiv.org/abs/2509.11516", "authors": ["Chengjin Wang", "Zheng Yan", "Yanmin Zhou", "Runjie Shen", "Zhipeng Wang", "Bin Cheng", "Bin He"], "title": "PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Box/cabinet scenarios with stacked objects pose significant challenges for\nrobotic motion due to visual occlusions and constrained free space. Traditional\ncollision-free trajectory planning methods often fail when no collision-free\npaths exist, and may even lead to catastrophic collisions caused by invisible\nobjects. To overcome these challenges, we propose an operational aware\ninteractive motion planner (PaiP) a real-time closed-loop planning framework\nutilizing multimodal tactile perception. This framework autonomously infers\nobject interaction features by perceiving motion effects at interaction\ninterfaces. These interaction features are incorporated into grid maps to\ngenerate operational cost maps. Building upon this representation, we extend\nsampling-based planning methods to interactive planning by optimizing both path\ncost and operational cost. Experimental results demonstrate that PaiP achieves\nrobust motion in narrow spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u7684\u5b9e\u65f6\u95ed\u73af\u89c4\u5212\u6846\u67b6\uff08PaiP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u72ed\u7a84\u7a7a\u95f4\u4e2d\u5806\u53e0\u7269\u4f53\u7684\u89c6\u89c9\u906e\u6321\u548c\u81ea\u7531\u7a7a\u95f4\u53d7\u9650\u7ed9\u673a\u5668\u4eba\u8fd0\u52a8\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u5728\u65e0\u53ef\u884c\u8def\u5f84\u65f6\u53ef\u80fd\u5931\u8d25\u6216\u5f15\u53d1\u78b0\u649e\u3002", "method": "PaiP\u901a\u8fc7\u611f\u77e5\u4ea4\u4e92\u754c\u9762\u7684\u8fd0\u52a8\u6548\u5e94\u63a8\u65ad\u7269\u4f53\u4ea4\u4e92\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u878d\u5165\u6805\u683c\u5730\u56fe\u751f\u6210\u64cd\u4f5c\u6210\u672c\u56fe\uff0c\u6269\u5c55\u91c7\u6837\u89c4\u5212\u65b9\u6cd5\u4ee5\u4f18\u5316\u8def\u5f84\u548c\u64cd\u4f5c\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPaiP\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "PaiP\u901a\u8fc7\u7ed3\u5408\u4ea4\u4e92\u7279\u5f81\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2509.12032", "pdf": "https://arxiv.org/pdf/2509.12032", "abs": "https://arxiv.org/abs/2509.12032", "authors": ["Baiyang Liu", "Jiewei Huang", "Tuo Wu", "Huan Meng", "Fengcheng Mei", "Lei Ning", "Kai-Kit Wong", "Hang Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "Meta Fluid Antenna: Architecture Design, Performance Analysis, Experimental Examination", "categories": ["eess.SP"], "comment": "13 pages", "summary": "Fluid antenna systems (FAS) have recently emerged as a promising solution for\nsixth-generation (6G) ultra-dense connectivity. These systems utilize dynamic\nradiating and/or shaping techniques to mitigate interference and improve\nspectral efficiency without relying on channel state information (CSI). The\nreported improvements achieved by employing a single dynamically activated\nradiating position in fluid antenna multiple access (FAMA) are significant. To\nfully realize the potential of FAMA in multi-user multiplexing, we propose\nleveraging the unique fast-switching capabilities of a single radio-frequency\n(RF)-chain meta-fluid antenna structure to achieve multi-activation. This\nallows for a significantly larger set of independent radiating states without\nrequiring additional signal processing. Simulations demonstrate that\nmulti-activation FAMA enables robust multi-user multiplexing with a higher\nsignal-to-interference ratio (SIR) under various Rayleigh-fading environments\ncompared to other single RF-chain technologies. We further show that the SIR\ncan be optimized within a 15~$\\mu s$ timeframe under a multi-user\nRayleigh-fading channel, making the proposed scheme highly suitable for\nfast-changing wireless environments. Verified through the theoretical Jakes'\nmodel, full three-dimensional (3D) electromagnetic (EM) simulations and\nexperimental validation, multi-activation FAMA enables effective CSI-free,\nmulti-user communication, offering a scalable solution for high-capacity\nwireless networks.", "AI": {"tldr": "\u591a\u6fc0\u6d3b\u6d41\u4f53\u5929\u7ebf\u591a\u5740\uff08FAMA\uff09\u5229\u7528\u5355\u5c04\u9891\u94fe\u7ed3\u6784\u5b9e\u73b0\u591a\u7528\u6237\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u5e72\u6270\u6bd4\uff08SIR\uff09\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u53d8\u5316\u7684\u65e0\u7ebf\u73af\u5883\u3002", "motivation": "\u4f20\u7edf6G\u8d85\u5bc6\u96c6\u8fde\u63a5\u9700\u8981\u89e3\u51b3\u5e72\u6270\u548c\u9891\u8c31\u6548\u7387\u95ee\u9898\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u3002", "method": "\u901a\u8fc7\u5355\u5c04\u9891\u94fe\u7ed3\u6784\u5b9e\u73b0\u591a\u6fc0\u6d3b\uff0c\u6269\u5c55\u72ec\u7acb\u8f90\u5c04\u72b6\u6001\u96c6\uff0c\u65e0\u9700\u989d\u5916\u4fe1\u53f7\u5904\u7406\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u591a\u6fc0\u6d3bFAMA\u5728\u591a\u7528\u6237Rayleigh\u8870\u843d\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347SIR\uff0c\u5e76\u572815\u03bcs\u5185\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "\u591a\u6fc0\u6d3bFAMA\u4e3a\u9ad8\u5bb9\u91cf\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684CSI\u514d\u591a\u7528\u6237\u901a\u4fe1\u65b9\u6848\u3002"}}
{"id": "2509.11567", "pdf": "https://arxiv.org/pdf/2509.11567", "abs": "https://arxiv.org/abs/2509.11567", "authors": ["Eron Ristich", "Jiahe Wang", "Lei Zhang", "Sultan Haidar Ali", "Wanxin Jin", "Yi Ren", "Jiefeng Sun"], "title": "Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection", "categories": ["cs.RO"], "comment": "7 pages (+2 pages of references), 8 figures", "summary": "Soft continuum robots can allow for biocompatible yet compliant motions, such\nas the ability of octopus arms to swim, crawl, and manipulate objects. However,\ncurrent state-of-the-art continuum robots can only achieve real-time task-space\ncontrol (i.e., tip control) but not whole-shape control, mainly due to the high\ncomputational cost from its infinite degrees of freedom. In this paper, we\npresent a data-driven Koopman operator-based approach for the shape control of\nsimulated multi-segment tendon-driven soft continuum robots with the Kirchhoff\nrod model. Using data collected from these simulated soft robots, we conduct a\nper-segment projection scheme on the state of the robots allowing for the\nidentification of control-affine Koopman models that are an order of magnitude\nmore accurate than without the projection scheme. Using these learned Koopman\nmodels, we use a linear model predictive control (MPC) to control the robots to\na collection of target shapes of varying complexity. Our method realizes\ncomputationally efficient closed-loop control, and demonstrates the feasibility\nof real-time shape control for soft robots. We envision this work can pave the\nway for practical shape control of soft continuum robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u591a\u6bb5\u808c\u8171\u9a71\u52a8\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u95ed\u73af\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u8fde\u7eed\u673a\u5668\u4eba\u7684\u7814\u7a76\u4ec5\u9650\u4e8e\u5b9e\u65f6\u4efb\u52a1\u7a7a\u95f4\u63a7\u5236\uff08\u5982\u5c16\u7aef\u63a7\u5236\uff09\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5168\u5f62\u72b6\u63a7\u5236\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5176\u65e0\u9650\u81ea\u7531\u5ea6\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528Kirchhoff\u6746\u6a21\u578b\u6a21\u62df\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6bcf\u6bb5\u6295\u5f71\u65b9\u6848\u8bc6\u522b\u63a7\u5236\u4eff\u5c04Koopman\u6a21\u578b\uff0c\u5e76\u5229\u7528\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u5b9e\u73b0\u5f62\u72b6\u63a7\u5236\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u95ed\u73af\u63a7\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u5b9e\u65f6\u5f62\u72b6\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5b9e\u9645\u5f62\u72b6\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.12089", "pdf": "https://arxiv.org/pdf/2509.12089", "abs": "https://arxiv.org/abs/2509.12089", "authors": ["Qiying Hu"], "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss", "categories": ["eess.SP", "cs.CL"], "comment": null, "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadarLLM\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6d77\u6d0b\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5bf9\u96f7\u8fbe\u4fe1\u53f7\u7279\u5f81\u7684\u5206\u6790\u80fd\u529b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u635f\u5931\u51fd\u6570\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6355\u83b7\u901a\u7528\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u6d77\u6d0b\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u65f6\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SCR\uff09\u573a\u666f\u4e2d\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faRadarLLM\u6846\u67b6\uff0c\u91c7\u7528\u504f\u597d\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u9009\u62e9\u6027\u4f18\u5316\u4e0d\u540c\u7279\u5f81\u5757\uff08feature patches\uff09\u7684\u5b66\u4e60\u503c\uff0c\u5f15\u5bfc\u6a21\u578b\u4e13\u6ce8\u4e8e\u6700\u5177\u6cdb\u5316\u6027\u7684\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRadarLLM\u5728\u4f4eSCR\u573a\u666f\u4e0b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u65b9\u6cd5\uff0c\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RadarLLM\u901a\u8fc7\u9009\u62e9\u6027\u4f18\u5316\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u9884\u8bad\u7ec3LLM\u5728\u6d77\u6d0b\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4eSCR\u548c\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.11594", "pdf": "https://arxiv.org/pdf/2509.11594", "abs": "https://arxiv.org/abs/2509.11594", "authors": ["Jizhuo Chen", "Diwen Liu", "Jiaming Wang", "Harold Soh"], "title": "GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Jizhuo Chen and Diwen Liu contributed equally", "summary": "GBPP is a fast learning based scorer that selects a robot base pose for\ngrasping from a single RGB-D snapshot. The method uses a two stage curriculum:\n(1) a simple distance-visibility rule auto-labels a large dataset at low cost;\nand (2) a smaller set of high fidelity simulation trials refines the model to\nmatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLP\nscores dense grids of candidate poses, enabling rapid online selection without\nfull task-and-motion optimization. In simulation and on a real mobile\nmanipulator, GBPP outperforms proximity and geometry only baselines, choosing\nsafer and more reachable stances and degrading gracefully when wrong. The\nresults offer a practical recipe for data efficient, geometry aware base\nplacement: use inexpensive heuristics for coverage, then calibrate with\ntargeted simulation.", "AI": {"tldr": "GBPP\u662f\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5b66\u4e60\u7684\u8bc4\u5206\u5668\uff0c\u7528\u4e8e\u4ece\u5355\u4e2aRGB-D\u5feb\u7167\u4e2d\u9009\u62e9\u673a\u5668\u4eba\u6293\u53d6\u7684\u57fa\u672c\u59ff\u6001\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u4f18\u5316\u3002", "motivation": "\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u57fa\u4f4d\u59ff\u6001\u9009\u62e9\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u51e0\u4f55\u611f\u77e5\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u7528\u7b80\u5355\u8ddd\u79bb-\u53ef\u89c1\u6027\u89c4\u5219\u81ea\u52a8\u6807\u6ce8\u5927\u91cf\u6570\u636e\uff0c\u518d\u7528\u9ad8\u4fdd\u771f\u6a21\u62df\u8bd5\u9a8c\u4f18\u5316\u6a21\u578b\uff1b\u4f7f\u7528PointNet++\u98ce\u683c\u70b9\u4e91\u7f16\u7801\u5668\u548cMLP\u8bc4\u5206\u5019\u9009\u59ff\u6001\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\uff0cGBPP\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u63a5\u8fd1\u5ea6\u548c\u51e0\u4f55\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9009\u62e9\u66f4\u5b89\u5168\u3001\u53ef\u8fbe\u7684\u57fa\u4f4d\uff0c\u9519\u8bef\u65f6\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "GBPP\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff1a\u5148\u4f7f\u7528\u4f4e\u6210\u672c\u542f\u53d1\u5f0f\u8986\u76d6\uff0c\u518d\u901a\u8fc7\u76ee\u6807\u6a21\u62df\u6821\u51c6\uff0c\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u548c\u51e0\u4f55\u611f\u77e5\u7684\u57fa\u4f4d\u653e\u7f6e\u3002"}}
{"id": "2509.12110", "pdf": "https://arxiv.org/pdf/2509.12110", "abs": "https://arxiv.org/abs/2509.12110", "authors": ["Qiying Hu", "Linping Zhang", "Xueqian Wang", "Gang Li", "Yu Liu", "Xiao-Ping Zhang"], "title": "When marine radar target detection meets pretrained large language models", "categories": ["eess.SP", "cs.CL", "cs.LG"], "comment": null, "summary": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u5f81\u9884\u5904\u7406\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u96f7\u8fbe\u56de\u6ce2\u4fe1\u53f7\u7684\u9ad8\u7ef4\u6a21\u5f0f\u63d0\u53d6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7279\u5f81\u5197\u4f59\u548c\u6a21\u578b\u5927\u5c0f\u9650\u5236\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u9884\u5904\u7406\u6a21\u5757\u5bf9\u96f7\u8fbe\u5e8f\u5217\u7279\u5f81\u8fdb\u884c\u6807\u8bb0\u5316\uff0c\u5e76\u5e94\u7528\u8865\u4e01\u9009\u62e9\u7b97\u6cd5\u8fc7\u6ee4\u65e0\u4fe1\u606f\u6bb5\uff0c\u968f\u540e\u5c06\u6240\u9009\u8865\u4e01\u6295\u5f71\u5230\u9884\u8bad\u7ec3LLM\u7684\u7279\u5f81\u7a7a\u95f4\u5d4c\u5165\u4e2d\uff0c\u4ec5\u5fae\u8c03\u5f52\u4e00\u5316\u5c42\u4ee5\u51cf\u8f7b\u8bad\u7ec3\u8d1f\u62c5\u3002", "result": "\u5728\u5b9e\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3LLM\u7684\u7279\u5f81\u9884\u5904\u7406\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u96f7\u8fbe\u4fe1\u53f7\u5206\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2509.11617", "pdf": "https://arxiv.org/pdf/2509.11617", "abs": "https://arxiv.org/abs/2509.11617", "authors": ["Qi Zheng", "Chaoran Zhang", "Zijian Liang", "EnTe Lin", "Shubo Cui", "Qinghongbing Xie", "Zhaobo Xu", "Long Zeng"], "title": "AssemMate: Graph-Based LLM for Robotic Assembly Assistance", "categories": ["cs.RO"], "comment": null, "summary": "Large Language Model (LLM)-based robotic assembly assistance has gained\nsignificant research attention. It requires the injection of domain-specific\nknowledge to guide the assembly process through natural language interaction\nwith humans. Despite some progress, existing methods represent knowledge in the\nform of natural language text. Due to the long context and redundant content,\nthey struggle to meet the robots' requirements for real-time and precise\nreasoning. In order to bridge this gap, we present AssemMate, which utilizes\nthe graph\\textemdash a concise and accurate form of knowledge\nrepresentation\\textemdash as input. This graph-based LLM enables knowledge\ngraph question answering (KGQA), supporting human-robot interaction and\nassembly task planning for specific products. Beyond interactive QA, AssemMate\nalso supports sensing stacked scenes and executing grasping to assist with\nassembly. Specifically, a self-supervised Graph Convolutional Network (GCN)\nencodes knowledge graph entities and relations into a latent space and aligns\nthem with LLM's representation, enabling the LLM to understand graph\ninformation. In addition, a vision-enhanced strategy is employed to address\nstacked scenes in grasping. Through training and evaluation, AssemMate\noutperforms existing methods, achieving 6.4\\% higher accuracy, 3 times faster\ninference, and 28 times shorter context length, while demonstrating strong\ngeneralization ability on random graphs. And our approach further demonstrates\nsuperiority through robotic grasping experiments in both simulated and\nreal-world settings. More details can be found on the project page:\nhttps://github.com/cristina304/AssemMate.git", "AI": {"tldr": "AssemMate\u5229\u7528\u56fe\u7ed3\u6784\u4f5c\u4e3a\u77e5\u8bc6\u8868\u793a\u5f62\u5f0f\uff0c\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u548cLLM\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u7cbe\u786e\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5728\u51c6\u786e\u7387\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\u5b58\u5728\u5b9e\u65f6\u6027\u548c\u7cbe\u786e\u63a8\u7406\u7684\u4e0d\u8db3\uff0cAssemMate\u65e8\u5728\u901a\u8fc7\u56fe\u7ed3\u6784\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AssemMate\u4f7f\u7528GCN\u5c06\u77e5\u8bc6\u56fe\u7f16\u7801\u4e3a\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u4e0eLLM\u5bf9\u9f50\uff0c\u540c\u65f6\u91c7\u7528\u89c6\u89c9\u589e\u5f3a\u7b56\u7565\u5904\u7406\u5806\u53e0\u573a\u666f\u3002", "result": "AssemMate\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u9ad8\u4e866.4%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u4e863\u500d\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7f29\u77ed\u4e8628\u500d\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AssemMate\u901a\u8fc7\u56fe\u7ed3\u6784\u548cLLM\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u5907\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.10481", "pdf": "https://arxiv.org/pdf/2509.10481", "abs": "https://arxiv.org/abs/2509.10481", "authors": ["Hongtao Liang", "Yihe Diao", "YuHang Wu", "Fuhui Zhou", "Qihui Wu"], "title": "Synergetic Empowerment: Wireless Communications Meets Embodied Intelligence", "categories": ["cs.NI", "cs.RO", "cs.SY", "eess.SP", "eess.SY"], "comment": "8 pages, 5 figures", "summary": "Wireless communication is evolving into an agent era, where large-scale\nagents with inherent embodied intelligence are not just users but active\nparticipants. The perfect combination of wireless communication and embodied\nintelligence can achieve a synergetic empowerment and greatly facilitate the\ndevelopment of agent communication. An overview of this synergetic empowerment\nis presented, framing it as a co-evolutionary process that transforms wireless\ncommunication from a simple utility into the digital nervous system of a\ncollective intelligence, while simultaneously elevating isolated agents into a\nunified superorganism with emergent capabilities far exceeding individual\ncontributions. Moreover, we elaborate how embodied intelligence and wireless\ncommunication mutually benefit each other through the lens of the\nperception-cognition-execution (PCE) loop, revealing a fundamental duality\nwhere each PCE stage both challenges network capacity and creates unprecedented\nopportunities for system-wide optimization. Furthermore, critical open issues\nand future research directions are identified.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u65e0\u7ebf\u901a\u4fe1\u4e0e\u5177\u8eab\u667a\u80fd\u7684\u7ed3\u5408\uff0c\u5c06\u5176\u89c6\u4e3a\u4e00\u79cd\u534f\u540c\u8fdb\u5316\u7684\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u4e86\u8fd9\u4e00\u7ed3\u5408\u5982\u4f55\u63d0\u5347\u96c6\u4f53\u667a\u80fd\u548c\u7cfb\u7edf\u7684\u6574\u4f53\u4f18\u5316\u3002", "motivation": "\u901a\u8fc7\u5c06\u65e0\u7ebf\u901a\u4fe1\u4e0e\u5177\u8eab\u667a\u80fd\u7ed3\u5408\uff0c\u53ef\u4ee5\u5c06\u5176\u4ece\u7b80\u5355\u7684\u5de5\u5177\u8f6c\u53d8\u4e3a\u96c6\u4f53\u667a\u80fd\u7684\u6570\u5b57\u795e\u7ecf\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u5347\u5355\u4e2a\u4ee3\u7406\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u4ece\u611f\u77e5-\u8ba4\u77e5-\u6267\u884c\uff08PCE\uff09\u5faa\u73af\u7684\u89d2\u5ea6\uff0c\u5206\u6790\u4e86\u5177\u8eab\u667a\u80fd\u4e0e\u65e0\u7ebf\u901a\u4fe1\u5982\u4f55\u76f8\u4e92\u4fc3\u8fdb\u548c\u6311\u6218\u7f51\u7edc\u5bb9\u91cf\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e24\u8005\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5e76\u4e3a\u7cfb\u7edf\u7ea7\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86\u672a\u6765\u7684\u5f00\u653e\u95ee\u9898\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.11621", "pdf": "https://arxiv.org/pdf/2509.11621", "abs": "https://arxiv.org/abs/2509.11621", "authors": ["Xiangtong Yao", "Yirui Zhou", "Yuan Meng", "Yanwen Liu", "Liangyu Dong", "Zitao Zhang", "Zhenshan Bing", "Kai Huang", "Fuchun Sun", "Alois Knoll"], "title": "Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios", "categories": ["cs.RO"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Diffusion policies are powerful visuomotor models for robotic manipulation,\nyet they often fail to generalize to manipulators or end-effectors unseen\nduring training and struggle to accommodate new task requirements at inference\ntime. Addressing this typically requires costly data recollection and policy\nretraining for each new hardware or task configuration. To overcome this, we\nintroduce an adaptation-projection strategy that enables a diffusion policy to\nperform zero-shot adaptation to novel manipulators and dynamic task settings,\nentirely at inference time and without any retraining. Our method first trains\na diffusion policy in SE(3) space using demonstrations from a base manipulator.\nDuring online deployment, it projects the policy's generated trajectories to\nsatisfy the kinematic and task-specific constraints imposed by the new hardware\nand objectives. Moreover, this projection dynamically adapts to physical\ndifferences (e.g., tool-center-point offsets, jaw widths) and task requirements\n(e.g., obstacle heights), ensuring robust and successful execution. We validate\nour approach on real-world pick-and-place, pushing, and pouring tasks across\nmultiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped\nwith a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F\ngrippers, and various 3D-printed designs. Our results demonstrate consistently\nhigh success rates in these cross-manipulator scenarios, proving the\neffectiveness and practicality of our adaptation-projection strategy. The code\nwill be released after peer review.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7SE(3)\u7a7a\u95f4\u8bad\u7ec3\u7684\u6269\u6563\u7b56\u7565\u548c\u5728\u7ebf\u8f68\u8ff9\u6295\u5f71\uff0c\u89e3\u51b3\u673a\u68b0\u81c2\u548c\u4efb\u52a1\u9700\u6c42\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u672a\u89c1\u8fc7\u7684\u673a\u68b0\u81c2\u6216\u4efb\u52a1\u9700\u6c42\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u91cd\u65b0\u6536\u96c6\u6570\u636e\u548c\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528SE(3)\u7a7a\u95f4\u8bad\u7ec3\u57fa\u673a\u68b0\u81c2\u7b56\u7565\uff0c\u5728\u7ebf\u6295\u5f71\u751f\u6210\u8f68\u8ff9\u4ee5\u6ee1\u8db3\u65b0\u786c\u4ef6\u548c\u4efb\u52a1\u7ea6\u675f\u3002", "result": "\u5728\u591a\u79cd\u673a\u68b0\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u6210\u529f\u7387\uff0c\u5982Franka Panda\u548cKuka iiwa 14\u3002", "conclusion": "\u8be5\u7b56\u7565\u5728\u96f6\u6837\u672c\u9002\u5e94\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2509.10487", "pdf": "https://arxiv.org/pdf/2509.10487", "abs": "https://arxiv.org/abs/2509.10487", "authors": ["Ruizhi Zhang", "Yuchen Zhang", "Lipeng Zhu", "Ying Zhang", "Rui Zhang"], "title": "A Deep Learning Framework for Joint Channel Acquisition and Communication Optimization in Movable Antenna Systems", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "This paper presents an end-to-end deep learning framework in a movable\nantenna (MA)-enabled multiuser communication system. In contrast to the\nconventional works assuming perfect channel state information (CSI), we address\nthe practical CSI acquisition issue through the design of pilot signals and\nquantized CSI feedback, and further incorporate the joint optimization of\nchannel estimation, MA placement, and precoding design. The proposed mechanism\nenables the system to learn an optimized transmission strategy from imperfect\nchannel data, overcoming the limitations of conventional methods that conduct\nchannel estimation and antenna position optimization separately. To balance the\nperformance and overhead, we further extend the proposed framework to optimize\nthe antenna placement based on the statistical CSI. Simulation results\ndemonstrate that the proposed approach consistently outperforms traditional\nbenchmarks in terms of achievable sum-rate of users, especially under limited\nfeedback and sparse channel environments. Notably, it achieves a performance\ncomparable to the widely-adopted gradient-based methods with perfect CSI, while\nmaintaining significantly lower CSI feedback overhead. These results highlight\nthe effectiveness and adaptability of learning-based MA system design for\nfuture wireless systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u591a\u7528\u6237\u901a\u4fe1\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5b8c\u7f8e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u5047\u8bbe\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5b8c\u7f8eCSI\uff0c\u4f46\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2dCSI\u83b7\u53d6\u5b58\u5728\u8bef\u5dee\uff0c\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4fe1\u9053\u4f30\u8ba1\u3001\u5929\u7ebf\u5e03\u7f6e\u548c\u9884\u7f16\u7801\u8bbe\u8ba1\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5bfc\u9891\u4fe1\u53f7\u548c\u91cf\u5316CSI\u53cd\u9988\u673a\u5236\uff0c\u5e76\u80fd\u57fa\u4e8e\u7edf\u8ba1CSI\u4f18\u5316\u5929\u7ebf\u5e03\u7f6e\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7528\u6237\u53ef\u8fbe\u5230\u603b\u901f\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\uff0c\u5c24\u5176\u5728\u6709\u9650\u53cd\u9988\u548c\u7a00\u758f\u4fe1\u9053\u73af\u5883\u4e0b\u8868\u73b0\u7a81\u51fa\uff0c\u6027\u80fd\u63a5\u8fd1\u5b8c\u7f8eCSI\u7684\u68af\u5ea6\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u53cd\u9988\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684MA\u7cfb\u7edf\u8bbe\u8ba1\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.11663", "pdf": "https://arxiv.org/pdf/2509.11663", "abs": "https://arxiv.org/abs/2509.11663", "authors": ["Haisheng Wang", "Weiming Zhi"], "title": "ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8 pages, 6 figures, 2026 IEEE Conference on Robotics and Automation\n  (ICRA 2026)", "summary": "This paper formulates the Embodied Questions Answering (EQsA) problem,\nintroduces a corresponding benchmark, and proposes a system to tackle the\nproblem. Classical Embodied Question Answering (EQA) is typically formulated as\nanswering one single question by actively exploring a 3D environment. Real\ndeployments, however, often demand handling multiple questions that may arrive\nasynchronously and carry different urgencies. We formalize this setting as\nEmbodied Questions Answering (EQsA) and present ParaEQsA, a framework for\nparallel, urgency-aware scheduling and answering. ParaEQsA leverages a group\nmemory module shared among questions to reduce redundant exploration, and a\npriority-planning module to dynamically schedule questions. To evaluate this\nsetting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)\nbenchmark containing 40 indoor scenes and five questions per scene (200 in\ntotal), featuring asynchronous follow-up questions and urgency labels. We\nfurther propose metrics for EQsA performance: Direct Answer Rate (DAR), and\nNormalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency\nand responsiveness of this system. ParaEQsA consistently outperforms strong\nsequential baselines adapted from recent EQA systems, while reducing\nexploration and delay. Empirical evaluations investigate the relative\ncontributions of priority, urgency modeling, spatial scope, reward estimation,\nand dependency reasoning within our framework. Together, these results\ndemonstrate that urgency-aware, parallel scheduling is key to making embodied\nagents responsive and efficient under realistic, multi-question workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e76\u884c\u3001\u7d27\u8feb\u6027\u611f\u77e5\u7684Embodied Questions Answering (EQsA)\u95ee\u9898\uff0c\u5e76\u4ecb\u7ecd\u4e86ParaEQsA\u6846\u67b6\u548cPAEQs\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u95ee\u9898\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfEmbodied Question Answering (EQA)\u4ec5\u5904\u7406\u5355\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u5904\u7406\u5f02\u6b65\u5230\u8fbe\u4e14\u7d27\u8feb\u6027\u4e0d\u540c\u7684\u591a\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86ParaEQsA\u6846\u67b6\uff0c\u5305\u542b\u5171\u4eab\u7ec4\u5185\u5b58\u6a21\u5757\u4ee5\u51cf\u5c11\u5197\u4f59\u63a2\u7d22\uff0c\u548c\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\u7684\u4f18\u5148\u7ea7\u89c4\u5212\u6a21\u5757\u3002", "result": "ParaEQsA\u6027\u80fd\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u51cf\u5c11\u4e86\u63a2\u7d22\u548c\u5ef6\u8fdf\uff0c\u5e76\u901a\u8fc7PAEQs\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7d27\u8feb\u6027\u611f\u77e5\u7684\u5e76\u884c\u8c03\u5ea6\u662f\u63d0\u5347\u591a\u95ee\u9898\u573a\u666f\u4e0b\u667a\u80fd\u4f53\u54cd\u5e94\u6027\u548c\u6548\u7387\u7684\u5173\u952e\u3002"}}
{"id": "2509.10508", "pdf": "https://arxiv.org/pdf/2509.10508", "abs": "https://arxiv.org/abs/2509.10508", "authors": ["Aathira G Menon", "Prabu Krishnan", "Shyam Lal"], "title": "CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi Head Attention for Heterogeneous Vehicular Networks", "categories": ["cs.NI", "cs.AI", "eess.SP"], "comment": "10 pages, 10 figures, 6 tables, (to be published)", "summary": "Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking\ndifferent communication technologies such as sub-6GHz, mm-wave and DSRC to meet\ndiverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address\nthe humongous user demands-but maintaining a steady connection in a highly\nmobile, real-world conditions remain a challenge. Though there has been ample\nof studies on beam prediction models a dedicated solution for HetVNets is\nsparsely explored. Hence, it is the need of the hour to develop a reliable beam\nprediction solution, specifically for HetVNets. This paper introduces a\nlightweight deep learning-based solution termed-\"CAR-BRAINet\" which consists of\nconvolutional neural networks with a powerful multi-head attention (MHA)\nmechanism. Existing literature on beam prediction is largely studied under a\nlimited, idealised vehicular scenario, often overlooking the real-time\ncomplexities and intricacies of vehicular networks. Therefore, this study aims\nto mimic the complexities of a real-time driving scenario by incorporating key\nfactors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the\neffect of Doppler shifts under high velocity and varying distance and SNR\nlevels into three high-quality dynamic datasets pertaining to urban, rural and\nhighway vehicular networks. CAR-BRAINet performs effectively across all the\nvehicular scenarios, demonstrating precise beam prediction with minimal beam\noverhead and a steady improvement of 17.9422% on the spectral efficiency over\nthe existing methods. Thus, this study justifies the effectiveness of\nCAR-BRAINet in complex HetVNets, offering promising performance without relying\non the location angle and antenna dimensions of the mobile users, and thereby\nreducing the redundant sensor-latency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CAR-BRAINet\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u5f02\u6784\u8f66\u8f7d\u7f51\u7edc\u7684\u6ce2\u675f\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5b9e\u65f6\u573a\u666f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f02\u6784\u8f66\u8f7d\u7f51\u7edc\uff08HetVNets\uff09\u57285G/B5G\u7f51\u7edc\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u9ad8\u5ea6\u79fb\u52a8\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u8fde\u63a5\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e13\u7528\u4e8eHetVNets\u7684\u6ce2\u675f\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u53ef\u9760\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCAR-BRAINet\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6a21\u62df\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\uff0c\u8003\u8651MAC\u534f\u8bae\u3001\u591a\u666e\u52d2\u6548\u5e94\u548c\u4fe1\u566a\u6bd4\u7b49\u56e0\u7d20\uff0c\u751f\u6210\u52a8\u6001\u6570\u636e\u96c6\u3002", "result": "CAR-BRAINet\u5728\u6240\u6709\u8f66\u8f7d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6ce2\u675f\u9884\u6d4b\u51c6\u786e\u4e14\u5f00\u9500\u4f4e\uff0c\u9891\u8c31\u6548\u7387\u63d0\u534717.9422%\uff0c\u4e14\u4e0d\u4f9d\u8d56\u7528\u6237\u4f4d\u7f6e\u548c\u5929\u7ebf\u7ef4\u5ea6\u3002", "conclusion": "CAR-BRAINet\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742HetVNets\u4e2d\u7684\u6ce2\u675f\u9884\u6d4b\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u4f20\u611f\u5668\u5ef6\u8fdf\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2509.11688", "pdf": "https://arxiv.org/pdf/2509.11688", "abs": "https://arxiv.org/abs/2509.11688", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "The control of robotic systems in complex, shared collaborative workspaces\npresents significant challenges in achieving robust performance and safety when\nlearning from experienced or simulated data is employed in the pipeline. A\nprimary bottleneck is the reliance on coordinate-dependent models, which leads\nto profound data inefficiency by failing to generalize physical interactions\nacross different frames of reference. This forces learning algorithms to\nrediscover fundamental physical principles in every new orientation,\nartificially inflating the complexity of the learning task. This paper\nintroduces a novel framework that synergizes a coordinate-free, unreduced\nmultibody dynamics and kinematics model based on tensor mechanics with a\nData-Assisted Control (DAC) architecture. A non-recursive, closed-form\nNewton-Euler model in an augmented matrix form is derived that is optimized for\ntensor-based control design. This structure enables a principled decomposition\nof the system into a structurally certain, physically grounded part and an\nuncertain, empirical, and interaction-focused part, mediated by a virtual port\nvariable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,\ncontrol, and learning is proposed. The coordinate-free control laws for the\nstructurally certain part provide a stable and abstract command interface,\nproven via Lyapunov analysis. Eventually, the model and closed-loop system are\nvalidated through simulations. This work provides a naturally ideal input for\ndata-efficient, frame-invariant learning algorithms, such as equivariant\nlearning, designed to learn the uncertain interaction. The synergy directly\naddresses the data-inefficiency problem, increases explainability and\ninterpretability, and paves the way for more robust and generalizable robotic\ncontrol in interactive environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u529b\u5b66\u548c\u65e0\u5750\u6807\u591a\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u8f85\u52a9\u63a7\u5236\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5171\u4eab\u534f\u4f5c\u7a7a\u95f4\u4e2d\u56e0\u4f9d\u8d56\u5750\u6807\u7cfb\u6a21\u578b\u5bfc\u81f4\u7684\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u5750\u6807\u7cfb\u6a21\u578b\uff0c\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u53c2\u8003\u6846\u67b6\u4e2d\u901a\u7528\u5316\u7269\u7406\u4ea4\u4e92\uff0c\u9700\u8981\u91cd\u590d\u5b66\u4e60\u7269\u7406\u539f\u7406\uff0c\u589e\u52a0\u4e86\u4efb\u52a1\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u5750\u6807\u3001\u975e\u9012\u5f52\u7684\u725b\u987f-\u6b27\u62c9\u6a21\u578b\uff0c\u57fa\u4e8e\u5f20\u91cf\u529b\u5b66\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6570\u636e\u8f85\u52a9\u63a7\u5236\u67b6\u6784\uff0c\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u7269\u7406\u786e\u5b9a\u90e8\u5206\u548c\u4ea4\u4e92\u4e0d\u786e\u5b9a\u90e8\u5206\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u7aef\u53e3\u53d8\u91cf\u8fde\u63a5\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u5206\u6790\u8bc1\u660e\u4e86\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u548c\u95ed\u73af\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u3001\u6846\u67b6\u4e0d\u53d8\u7684\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u7b49\u53d8\u5b66\u4e60\uff09\u63d0\u4f9b\u4e86\u7406\u60f3\u8f93\u5165\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10878", "pdf": "https://arxiv.org/pdf/2509.10878", "abs": "https://arxiv.org/abs/2509.10878", "authors": ["Homa Nikbakht", "Husheng Li", "Zhu Han", "H. Vincent Poor"], "title": "A Broadcast Channel Framework for MIMO-OFDM Integrated Sensing and Communication", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Integrated sensing and communication (ISAC) is expected to be one of the\nmajor features of 6G wireless networks. In an ISAC system, communications and\nsensing functionalities are jointly performed using the same waveform,\nfrequency band and hardware, thereby enabling various use cases such as in\ncyber physical systems, digital twin and smart cities. A major challenge to the\ndesign and analysis of ISAC is a unified framework that incorporates the two\ndistinct functions. By viewing ISAC as a type of broadcast channel, in this\npaper, we propose a unified ISAC framework in which communication and sensing\nsignals are broadcast to the actual communication users and virtual sensing\nusers. This framework allows the application of existing multiplexing schemes,\nsuch as dirty paper coding (DPC) and frequency division multiplexing (FDM) that\nhave been intensively studied in data communications and information theory.\nWithin this framework, we propose different superposition coding schemes, for\ncases when the sensing waveform is known or unknown to the communication\nreceiver. We propose the waveform optimization algorithms in a multiple-input\nmultiple-output (MIMO) setting accounting for the effects of clutter and\nDoppler shift. The proposed framework is numerically evaluated for different\nschemes under various sensing and communications performance metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6846\u67b6\uff0c\u5c06\u5176\u89c6\u4e3a\u5e7f\u64ad\u4fe1\u9053\uff0c\u5e76\u5e94\u7528\u73b0\u6709\u7684\u590d\u7528\u65b9\u6848\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "6G\u65e0\u7ebf\u7f51\u7edc\u4e2d\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e24\u79cd\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u5c06ISAC\u89c6\u4e3a\u5e7f\u64ad\u4fe1\u9053\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u53e0\u52a0\u7f16\u7801\u65b9\u6848\u548c\u6ce2\u5f62\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728MIMO\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u6570\u503c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u4e0d\u540c\u6027\u80fd\u6307\u6807\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a6G\u7f51\u7edc\u4e2dISAC\u7684\u8bbe\u8ba1\u4e0e\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11740", "pdf": "https://arxiv.org/pdf/2509.11740", "abs": "https://arxiv.org/abs/2509.11740", "authors": ["Davide Peron", "Victor Nan Fernandez-Ayala", "Lukas Segelmark"], "title": "From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting", "categories": ["cs.RO"], "comment": "Submitted for publication at IEEE ICRA 2026", "summary": "Autonomous stocking in retail environments, particularly supermarkets,\npresents challenges due to dynamic human interactions, constrained spaces, and\ndiverse product geometries. This paper introduces an efficient end-to-end\nrobotic system for autonomous shelf stocking and fronting, integrating\ncommercially available hardware with a scalable algorithmic architecture. A\nmajor contribution of this work is the system integration of off-the-shelf\nhardware and ROS2-based perception, planning, and control into a single\ndeployable platform for retail environments. Our solution leverages Behavior\nTrees (BTs) for task planning, fine-tuned vision models for object detection,\nand a two-step Model Predictive Control (MPC) framework for precise shelf\nnavigation using ArUco markers. Laboratory experiments replicating realistic\nsupermarket conditions demonstrate reliable performance, achieving over 98%\nsuccess in pick-and-place operations across a total of more than 700 stocking\nevents. However, our comparative benchmarks indicate that the performance and\ncost-effectiveness of current autonomous systems remain inferior to that of\nhuman workers, which we use to highlight key improvement areas and quantify the\nprogress still required before widespread commercial deployment can\nrealistically be achieved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d85\u5e02\u8d27\u67b6\u81ea\u4e3b\u8865\u8d27\u7684\u7aef\u5230\u7aef\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u5546\u7528\u786c\u4ef6\u548c\u53ef\u6269\u5c55\u7b97\u6cd5\u67b6\u6784\uff0c\u5b9e\u9a8c\u8868\u73b0\u53ef\u9760\u4f46\u6210\u672c\u6548\u76ca\u4ecd\u4e0d\u53ca\u4eba\u5de5\u3002", "motivation": "\u8d85\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8865\u8d27\u56e0\u52a8\u6001\u4eba\u6d41\u3001\u72ed\u7a84\u7a7a\u95f4\u548c\u591a\u6837\u5546\u54c1\u5f62\u6001\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u6027\u80fd\u4e0e\u6210\u672c\u6548\u76ca\u4e0d\u53ca\u4eba\u5de5\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u5546\u7528\u786c\u4ef6\u3001ROS2\u6846\u67b6\u3001\u884c\u4e3a\u6811\u4efb\u52a1\u89c4\u5212\u3001\u4f18\u5316\u89c6\u89c9\u6a21\u578b\u548c\u4e24\u6b65\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5229\u7528ArUco\u6807\u8bb0\u8fdb\u884c\u7cbe\u786e\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u5ba4\u6a21\u62df\u5b9e\u9a8c\u4e2d\u5b8c\u6210700\u591a\u6b21\u8865\u8d27\u64cd\u4f5c\uff0c\u6210\u529f\u7387\u8d85\u8fc798%\uff0c\u4f46\u5bf9\u6bd4\u663e\u793a\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u4ecd\u4f4e\u4e8e\u4eba\u5de5\u3002", "conclusion": "\u7cfb\u7edf\u8868\u73b0\u53ef\u9760\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u548c\u6210\u672c\uff0c\u624d\u80fd\u5b9e\u73b0\u5546\u4e1a\u5316\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.10999", "pdf": "https://arxiv.org/pdf/2509.10999", "abs": "https://arxiv.org/abs/2509.10999", "authors": ["Saman Mazaheri Khamaneh", "Tong Wu", "Wei Sun", "Cong Chen"], "title": "Real-Time Defense Against Coordinated Cyber-Physical Attacks: A Robust Constrained Reinforcement Learning Approach", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Modern power systems face increasing vulnerability to sophisticated\ncyber-physical attacks beyond traditional N-1 contingency frameworks. Existing\nsecurity paradigms face a critical bottleneck: efficiently identifying\nworst-case scenarios and rapidly coordinating defensive responses are hindered\nby intensive computation and time delays, during which cascading failures can\npropagate. This paper presents a novel tri-level robust constrained\nreinforcement learning (RCRL) framework for robust power system security. The\nframework generates diverse system states through AC-OPF formulations,\nidentifies worst-case N-K attack scenarios for each state, and trains policies\nto mitigate these scenarios across all operating conditions without requiring\npredefined attack patterns. The framework addresses constraint satisfaction\nthrough Beta-blending projection-based feasible action mapping techniques\nduring training and primal-dual augmented Lagrangian optimization for\ndeployment. Once trained, the RCRL policy learns how to control observed\ncyber-physical attacks in real time. Validation on IEEE benchmark systems\ndemonstrates effectiveness against coordinated N-K attacks, causing widespread\ncascading failures throughout the network. The learned policy can successfully\nrespond rapidly to recover system-wide constraints back to normal within 0.21\nms inference times, establishing superior resilience for critical\ninfrastructure protection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u5c42\u6846\u67b6\uff08RCRL\uff09\uff0c\u7528\u4e8e\u5feb\u901f\u8bc6\u522b\u548c\u5e94\u5bf9\u7535\u7f51\u4e2d\u7684N-K\u653b\u51fb\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u5b9e\u65f6\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u9762\u4e34\u4f20\u7edfN-1\u6846\u67b6\u65e0\u6cd5\u5e94\u5bf9\u7684\u590d\u6742\u7f51\u7edc-\u7269\u7406\u653b\u51fb\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u6700\u574f\u60c5\u51b5\u548c\u534f\u8c03\u9632\u5fa1\u54cd\u5e94\u65f6\u5b58\u5728\u8ba1\u7b97\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7AC-OPF\u751f\u6210\u591a\u6837\u7cfb\u7edf\u72b6\u6001\uff0c\u8bc6\u522bN-K\u653b\u51fb\u573a\u666f\uff0c\u5229\u7528Beta\u6df7\u5408\u6295\u5f71\u548c\u62c9\u683c\u6717\u65e5\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u5b9e\u65f6\u653b\u51fb\u63a7\u5236\u3002", "result": "\u5728IEEE\u7cfb\u7edf\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cRCRL\u80fd\u5feb\u901f\u6062\u590d\u7cfb\u7edf\u7ea6\u675f\uff0c\u63a8\u7406\u65f6\u95f4\u4ec50.21\u6beb\u79d2\uff0c\u6709\u6548\u62b5\u5fa1\u7f51\u7edc\u7ea7\u8054\u6545\u969c\u3002", "conclusion": "RCRL\u6846\u67b6\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u5b9e\u65f6\u6062\u590d\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\u3002"}}
{"id": "2509.11742", "pdf": "https://arxiv.org/pdf/2509.11742", "abs": "https://arxiv.org/abs/2509.11742", "authors": ["Jianping Li", "Kaisong Zhu", "Zhongyuan Liu", "Rui Jin", "Xinhang Xu", "Pengfei Wan", "Lihua Xie"], "title": "Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap", "categories": ["cs.RO"], "comment": null, "summary": "LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as\nOSM provides lightweight global priors such as building footprints. These\npriors enhance global consistency for robot navigation, but OSM is often\nincomplete or outdated, limiting its reliability in real-world deployment.\nMeanwhile, LiDAR itself suffers from a limited field of view (FoV), where\nmotorized rotation is commonly used to achieve panoramic coverage. Existing\nmotorized LiDAR systems, however, typically employ constant-speed scanning that\ndisregards both scene structure and map priors, leading to wasted effort in\nfeature-sparse regions and degraded localization accuracy. To address these\nchallenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework\nthat integrates global priors with local observability prediction to improve\nlocalization robustness. Specifically, we augment uncertainty-aware model\npredictive control with an OSM-aware term that adaptively allocates scanning\neffort according to both scene-dependent observability and the spatial\ndistribution of OSM features. The method is implemented in ROS with a motorized\nLiDAR odometry backend and evaluated in both simulation and real-world\nexperiments. Results on campus roads, indoor corridors, and urban environments\ndemonstrate significant reductions in trajectory error compared to\nconstant-speed baselines, while maintaining scan completeness. These findings\nhighlight the potential of coupling open-source maps with adaptive LiDAR\nscanning to achieve robust and efficient localization in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOSM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94LiDAR\u626b\u63cf\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u5148\u9a8c\u548c\u5c40\u90e8\u53ef\u89c2\u6d4b\u6027\u9884\u6d4b\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002", "motivation": "OSM\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u5168\u5c40\u5148\u9a8c\uff08\u5982\u5efa\u7b51\u8f6e\u5ed3\uff09\uff0c\u4f46\u5b58\u5728\u4e0d\u5b8c\u6574\u6216\u8fc7\u65f6\u95ee\u9898\uff1bLiDAR\u56e0\u56fa\u5b9a\u89c6\u573a\u548c\u626b\u63cf\u65b9\u5f0f\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u7ed3\u5408OSM\u611f\u77e5\u9879\uff0c\u6839\u636e\u573a\u666f\u53ef\u89c2\u6d4b\u6027\u548cOSM\u7279\u5f81\u5206\u5e03\u81ea\u9002\u5e94\u5206\u914d\u626b\u63cf\u8d44\u6e90\u3002", "result": "\u5728\u6821\u56ed\u3001\u5ba4\u5185\u8d70\u5eca\u548c\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u8f68\u8ff9\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u626b\u63cf\u5b8c\u6574\u6027\u3002", "conclusion": "\u7ed3\u5408\u5f00\u6e90\u5730\u56fe\u4e0e\u81ea\u9002\u5e94LiDAR\u626b\u63cf\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5b9a\u4f4d\u3002"}}
{"id": "2509.11099", "pdf": "https://arxiv.org/pdf/2509.11099", "abs": "https://arxiv.org/abs/2509.11099", "authors": ["Huizhang Yang"], "title": "The Microwave Rainbow: How Geometry Paints Colours in Microwave Vision", "categories": ["eess.IV", "eess.SP"], "comment": null, "summary": "Microwave vision from spaceborne synthetic aperture radar (SAR) provides an\nall-weather, day-and-night capability to observe Earth, yet much of the\ninformation encoded in its signals remains undeciphered. Recent high-resolution\nimagery has revealed a striking phenomenon: man-made structures systematically\nappear in a spectrum of colours, the physical origin of which has been an open\nquestion. Here we show that this effect, which we term the microwave rainbow,\nis a form of geometric dispersion arising from structures acting as intrinsic\ndiffraction gratings. We introduce a geometric-physical model that provides a\ndirect analytical link between a target's geometry and its observed colour\nsignature. This model quantitatively explains the full range of signatures,\nfrom continuous colour gradients on curved surfaces (zero-order diffraction) to\nrepeating spectral patterns from periodic structures (high-order diffraction).\nThis work transforms colour from a visual artefact into a precise measure of\nphysical form, enabling the geometry of both critical infrastructure and\nnatural phenomena to be mapped directly from space. Our findings establish the\nphysical basis for a new remote sensing modality: microwave colour vision, and\nopen a new frontier in how we perceive our world.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u51e0\u4f55\u7269\u7406\u6a21\u578b\u89e3\u91ca\u4e86SAR\u56fe\u50cf\u4e2d\u4eba\u9020\u7ed3\u6784\u5448\u73b0\u591a\u5f69\u73b0\u8c61\uff08\u5fae\u6ce2\u5f69\u8679\uff09\u7684\u7269\u7406\u8d77\u6e90\uff0c\u5e76\u5c06\u5176\u4ece\u89c6\u89c9\u4f2a\u5f71\u8f6c\u5316\u4e3a\u7269\u7406\u5f62\u6001\u7684\u7cbe\u786e\u6d4b\u91cf\u5de5\u5177\u3002", "motivation": "\u65e8\u5728\u63ed\u793aSAR\u56fe\u50cf\u4e2d\u4eba\u9020\u7ed3\u6784\u591a\u5f69\u73b0\u8c61\u7684\u7269\u7406\u8d77\u6e90\uff0c\u5e76\u5c06\u5176\u53d1\u5c55\u4e3a\u4e00\u79cd\u65b0\u7684\u9065\u611f\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u7269\u7406\u6a21\u578b\uff0c\u5c06\u76ee\u6807\u7684\u51e0\u4f55\u5f62\u72b6\u4e0e\u5176\u89c2\u6d4b\u5230\u7684\u5f69\u8272\u7279\u5f81\u76f4\u63a5\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u6a21\u578b\u5b9a\u91cf\u89e3\u91ca\u4e86\u4ece\u66f2\u9762\u4e0a\u7684\u8fde\u7eed\u989c\u8272\u68af\u5ea6\u5230\u5468\u671f\u6027\u7ed3\u6784\u7684\u9ad8\u9636\u884d\u5c04\u6a21\u5f0f\u7684\u5b8c\u6574\u73b0\u8c61\u3002", "conclusion": "\u5fae\u6ce2\u8272\u5f69\u89c6\u89c9\u4e3aSAR\u9065\u611f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u7a7a\u95f4\u6620\u5c04\u7269\u7406\u5f62\u6001\u3002"}}
{"id": "2509.11766", "pdf": "https://arxiv.org/pdf/2509.11766", "abs": "https://arxiv.org/abs/2509.11766", "authors": ["Andy Zhai", "Brae Liu", "Bruno Fang", "Chalse Cai", "Ellie Ma", "Ethan Yin", "Hao Wang", "Hugo Zhou", "James Wang", "Lights Shi", "Lucy Liang", "Make Wang", "Qian Wang", "Roy Gan", "Ryan Yu", "Shalfun Li", "Starrick Liu", "Sylas Chen", "Vincent Chen", "Zach Xu"], "title": "Igniting VLMs toward the Embodied Space", "categories": ["cs.RO"], "comment": null, "summary": "While foundation models show remarkable progress in language and vision,\nexisting vision-language models (VLMs) still have limited spatial and\nembodiment understanding. Transferring VLMs to embodied domains reveals\nfundamental mismatches between modalities, pretraining distributions, and\ntraining objectives, leaving action comprehension and generation as a central\nbottleneck on the path to AGI.\n  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages\nlarge-scale multimodal pretraining to achieve (1) embodiment-aware\nvision-language understanding, (2) strong language-action association, and (3)\nrobust manipulation capability.\n  Our approach employs a tightly coupled architecture and multi-strategies\ntraining curriculum that enables Unified Cross-Level CoT-seamlessly unifying\ninstruction reasoning, subgoal decomposition, and fine-grained action synthesis\nwithin a single differentiable framework.\n  Our results show that WALL-OSS attains high success on complex long-horizon\nmanipulations, demonstrates strong instruction-following capabilities, complex\nunderstanding and reasoning, and outperforms strong baselines, thereby\nproviding a reliable and scalable path from VLMs to embodied foundation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WALL-OSS\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u5177\u8eab\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5177\u8eab\u667a\u80fd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u5177\u8eab\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7d27\u5bc6\u8026\u5408\u7684\u67b6\u6784\u548c\u591a\u7b56\u7565\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5b9e\u73b0\u4e86\u6307\u4ee4\u63a8\u7406\u3001\u5b50\u76ee\u6807\u5206\u89e3\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5408\u6210\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "WALL-OSS\u5728\u590d\u6742\u957f\u7a0b\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "WALL-OSS\u4e3a\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5230\u5177\u8eab\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2509.11146", "pdf": "https://arxiv.org/pdf/2509.11146", "abs": "https://arxiv.org/abs/2509.11146", "authors": ["Byungchang So"], "title": "Maximum diversity, weighting and invariants of time series", "categories": ["stat.ML", "cs.LG", "eess.SP", "math.MG", "46N40, 51F99, 68T10"], "comment": null, "summary": "Magnitude, obtained as a special case of Euler characteristic of enriched\ncategory, represents a sense of the size of metric spaces and is related to\nclassical notions such as cardinality, dimension, and volume. While the studies\nhave explained the meaning of magnitude from various perspectives, continuity\nalso gives a valuable view of magnitude. Based on established results about\ncontinuity of magnitude and maximum diversity, this article focuses on\ncontinuity of weighting, a distribution whose totality is magnitude, and its\nvariation corresponding to maximum diversity. Meanwhile, recent studies also\nilluminated the connection between magnitude and data analysis by applying\nmagnitude theory to point clouds representing the data or the set of model\nparameters. This article will also provide an application for time series\nanalysis by introducing a new kind of invariants of periodic time series, where\nthe invariance follows directly from the continuity results. As a use-case, a\nsimple machine learning experiment is conducted with real-world data, in which\nthe suggested invariants improved the performance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5ea6\u91cf\u7a7a\u95f4\u5927\u5c0f\u7684\u6982\u5ff5\u2014\u2014magnitude\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u8fde\u7eed\u6027\u53ca\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u901a\u8fc7\u5206\u6790magnitude\u7684\u8fde\u7eed\u6027\u548c\u6743\u91cd\u5206\u5e03\uff0c\u63a2\u7d22\u5176\u5728\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u57fa\u4e8e\u5df2\u6709\u7684\u8fde\u7eed\u6027\u548c\u6700\u5927\u591a\u6837\u6027\u7ed3\u679c\uff0c\u7814\u7a76\u4e86\u6743\u91cd\u7684\u8fde\u7eed\u6027\u53ca\u5176\u53d8\u5316\uff1b\u5e76\u5c06magnitude\u7406\u8bba\u5e94\u7528\u4e8e\u5468\u671f\u6027\u65f6\u95f4\u5e8f\u5217\uff0c\u63d0\u51fa\u65b0\u7684\u4e0d\u53d8\u91cf\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u7684\u4e0d\u53d8\u91cf\u80fd\u591f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "magnitude\u7684\u8fde\u7eed\u6027\u7814\u7a76\u53ca\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u5c55\u73b0\u4e86\u5176\u7406\u8bba\u548c\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2509.11783", "pdf": "https://arxiv.org/pdf/2509.11783", "abs": "https://arxiv.org/abs/2509.11783", "authors": ["Shiqi Gong", "Sebastian Zudaire", "Chi Zhang", "Zhen Li"], "title": "Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations", "categories": ["cs.RO"], "comment": "Accepted by 2025 8th International Conference on Robotics, Control\n  and Automation Engineering (RCAE 2025)", "summary": "Traditional industrial robot programming is often complex and time-consuming,\ntypically requiring weeks or even months of effort from expert programmers.\nAlthough Programming by Demonstration (PbD) offers a more accessible\nalternative, intuitive interfaces for robot control and demonstration\ncollection remain challenging. To address this, we propose an Augmented Reality\n(AR)-enhanced robot teleoperation system that integrates AR-based control with\nspatial point cloud rendering, enabling intuitive, contact-free demonstrations.\nThis approach allows operators to control robots remotely without entering the\nworkspace or using conventional tools like the teach pendant. The proposed\nsystem is generally applicable and has been demonstrated on ABB robot\nplatforms, specifically validated with the IRB 1200 industrial robot and the\nGoFa 5 collaborative robot. A user study evaluates the impact of real-time\nenvironmental perception, specifically with and without point cloud rendering,\non task completion accuracy, efficiency, and user confidence. Results indicate\nthat enhanced perception significantly improves task performance by 28% and\nenhances user experience, as reflected by a 12% increase in the System\nUsability Scale (SUS) score. This work contributes to the advancement of\nintuitive robot teleoperation, AR interface design, environmental perception,\nand teleoperation safety mechanisms in industrial settings for demonstration\ncollection. The collected demonstrations may serve as valuable training data\nfor machine learning applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u8f85\u52a9\u7684\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7AR\u63a7\u5236\u548c\u7a7a\u95f4\u70b9\u4e91\u6e32\u67d3\u5b9e\u73b0\u76f4\u89c2\u3001\u65e0\u63a5\u89e6\u7684\u6f14\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u7f16\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0c\u7f16\u7a0b\u6f14\u793a\uff08PbD\uff09\u867d\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u76f4\u89c2\u7684\u673a\u5668\u4eba\u63a7\u5236\u548c\u6f14\u793a\u6536\u96c6\u754c\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86AR\u589e\u5f3a\u7684\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408AR\u63a7\u5236\u548c\u70b9\u4e91\u6e32\u67d3\uff0c\u5b9e\u73b0\u5bf9\u673a\u5668\u4eba\u7684\u8fdc\u7a0b\u65e0\u63a5\u89e6\u63a7\u5236\uff0c\u5e76\u5728ABB\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5b9e\u65f6\u73af\u5883\u611f\u77e5\uff08\u70b9\u4e91\u6e32\u67d3\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u6027\uff0828%\u63d0\u5347\uff09\u548c\u7528\u6237\u4f53\u9a8c\uff08SUS\u8bc4\u5206\u63d0\u9ad812%\uff09\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86\u76f4\u89c2\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u3001AR\u754c\u9762\u8bbe\u8ba1\u3001\u73af\u5883\u611f\u77e5\u53ca\u5de5\u4e1a\u6f14\u793a\u6536\u96c6\u7684\u5b89\u5168\u6027\uff0c\u6536\u96c6\u7684\u6f14\u793a\u6570\u636e\u53ef\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u3002"}}
{"id": "2509.11606", "pdf": "https://arxiv.org/pdf/2509.11606", "abs": "https://arxiv.org/abs/2509.11606", "authors": ["Milan Marocchi", "Matthew Fynn", "Kayapanda Mandana", "Yue Rong"], "title": "Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals", "categories": ["cs.SD", "cs.LG", "eess.SP"], "comment": "35 pages, 37 figures, 19 tables", "summary": "Cardiovascular diseases (CVDs) are the leading cause of death worldwide,\naccounting for approximately 17.9 million deaths each year. Early detection is\ncritical, creating a demand for accurate and inexpensive pre-screening methods.\nDeep learning has recently been applied to classify abnormal heart sounds\nindicative of CVDs using synchronised phonocardiogram (PCG) and\nelectrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,\nstate-of-the-art architectures remain underutilised due to the limited\navailability of synchronised and multichannel datasets. Augmented datasets and\npre-trained models provide a pathway to overcome these limitations, enabling\ntransformer-based architectures to be trained effectively. This work combines\ntraditional signal processing with denoising diffusion models, WaveGrad and\nDiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based\nclassifier on multimodal and multichannel heart sound datasets. The approach\nachieves state-of-the-art performance. On the Computing in Cardiology (CinC)\n2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),\nsensitivity, specificity and Matthew's correlation coefficient (MCC) reach\n92.48\\%, 93.05\\%, 93.63\\%, 92.48\\%, 94.93\\% and 0.8283, respectively. Using the\nsynchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\\%,\n92.21\\%, 94.35\\%, 90.10\\%, 95.12\\% and 0.8380 are achieved for accuracy, UAR,\nsensitivity, specificity and MCC, respectively. Using a wearable vest dataset\nconsisting of mPCG data, the model achieves 77.13\\% accuracy, 74.25\\% UAR,\n86.47\\% sensitivity, 62.04\\% specificity, and 0.5082 MCC. These results\ndemonstrate the effectiveness of transformer-based models for CVD detection\nwhen supported by augmented datasets, highlighting their potential to advance\nmultimodal and multichannel heart sound classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u4e0e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8bad\u7ec3Wav2Vec 2.0\u5206\u7c7b\u5668\uff0c\u5728\u591a\u6a21\u6001\u5fc3\u97f3\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u5fc3\u8840\u7ba1\u75be\u75c5\u68c0\u6d4b", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u9700\u6c42\u8feb\u5207\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u540c\u6b65\u548c\u591a\u901a\u9053\u6570\u636e\u96c6\u6709\u9650\u800c\u672a\u80fd\u5145\u5206\u53d1\u6325\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u4e0eWaveGrad\u548cDiffWave\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u57fa\u4e8eWav2Vec 2.0\u7684\u5206\u7c7b\u5668\u3002", "result": "\u5728\u5355\u901a\u9053PCG\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8fbe92.48%\uff0c\u540c\u6b65PCG\u548cECG\u4fe1\u53f7\u8fbe93.14%\uff0c\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a0d\u900a\u4f46\u4ecd\u6709\u6f5c\u529b\u3002", "conclusion": "\u589e\u5f3a\u6570\u636e\u96c6\u652f\u6301\u4e0b\u7684Transformer\u6a21\u578b\u5728\u591a\u6a21\u6001\u5fc3\u97f3\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u63a8\u52a8\u5fc3\u8840\u7ba1\u75be\u75c5\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.11791", "pdf": "https://arxiv.org/pdf/2509.11791", "abs": "https://arxiv.org/abs/2509.11791", "authors": ["Lauri Suomela", "Sasanka Kuruppu Arachchige", "German F. Torres", "Harry Edelman", "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"], "title": "Synthetic vs. Real Training Data for Visual Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "Presented at CoRL 2025 workshop on \"Making Sense of Data in Robotics\"", "summary": "This paper investigates how the performance of visual navigation policies\ntrained in simulation compares to policies trained with real-world data.\nPerformance degradation of simulator-trained policies is often significant when\nthey are evaluated in the real world. However, despite this well-known\nsim-to-real gap, we demonstrate that simulator-trained policies can match the\nperformance of their real-world-trained counterparts.\n  Central to our approach is a navigation policy architecture that bridges the\nsim-to-real appearance gap by leveraging pretrained visual representations and\nruns real-time on robot hardware. Evaluations on a wheeled mobile robot show\nthat the proposed policy, when trained in simulation, outperforms its\nreal-world-trained version by 31% and the prior state-of-the-art methods by 50%\nin navigation success rate. Policy generalization is verified by deploying the\nsame model onboard a drone.\n  Our results highlight the importance of diverse image encoder pretraining for\nsim-to-real generalization, and identify on-policy learning as a key advantage\nof simulated training over training with real data.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5408\u7406\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u4eff\u771f\u8bad\u7ec3\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u53ef\u4ee5\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u6210\u529f\u586b\u8865\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u8bad\u7ec3\u7b56\u7565\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u67b6\u6784\u6539\u8fdb\u4f7f\u5176\u6027\u80fd\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u5e76\u80fd\u5728\u673a\u5668\u4eba\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u7684\u5bfc\u822a\u7b56\u7565\u67b6\u6784\u3002", "result": "\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u5bfc\u822a\u6210\u529f\u7387\u4e0a\u6bd4\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7248\u672c\u9ad831%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad850%\u3002", "conclusion": "\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u7684\u591a\u6837\u6027\u5bf9\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4eff\u771f\u8bad\u7ec3\u5177\u6709\u5b9e\u65f6\u7b56\u7565\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.11829", "pdf": "https://arxiv.org/pdf/2509.11829", "abs": "https://arxiv.org/abs/2509.11829", "authors": ["Veronika Oehl", "Alexander Damm"], "title": "WAFER: A new method to retrieve sun-induced fluorescence based on spectral wavelet decompositions", "categories": ["physics.geo-ph", "eess.SP"], "comment": "20 pages, 13 figures. Published in Remote Sensing of Environment\n  (2023)", "summary": "Sun-induced fluorescence (SIF) as a close remote sensing based proxy for\nphotosynthesis is accepted as a useful measure to remotely monitor vegetation\nhealth and gross primary productivity. In this work we present the new\nretrieval method WAFER (WAvelet decomposition FluorEscence Retrieval) based on\nwavelet decompositions of the measured spectra of reflected radiance as well as\na reference radiance not containing fluorescence. By comparing absolute\nabsorption line depths by means of the corresponding wavelet coefficients, a\nrelative reflectance is retrieved independently of the fluorescence, i.e.\nwithout introducing a coupling between reflectance and fluorescence. The\nfluorescence can then be derived as the remaining offset. This method can be\napplied to arbitrary chosen wavelength windows in the whole spectral range,\nsuch that all the spectral data available is exploited, including the\nseparation into several frequency (i.e. width of absorption lines) levels and\nwithout the need of extensive training datasets. At the same time, the\nassumptions about the reflectance shape are minimal and no spectral shape\nassumptions are imposed on the fluorescence, which not only avoids biases\narising from wrong or differing fluorescence models across different spatial\nscales and retrieval methods but also allows for the exploration of this\nspectral shape for different measurement setups. WAFER is tested on a synthetic\ndataset as well as several diurnal datasets acquired with a field spectrometer\n(FloX) over an agricultural site. We compare the WAFER method to two\nestablished retrieval methods, namely the improved Fraunhofer line\ndiscrimination (iFLD) method and spectral fitting method (SFM) and find a good\nagreement with the added possibility of exploring the true spectral shape of\nthe offset signal and free choice of the retrieval window. (abbreviated)", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8367\u5149\u53cd\u6f14\u65b9\u6cd5WAFER\uff0c\u57fa\u4e8e\u5c0f\u6ce2\u5206\u89e3\u6280\u672f\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u590d\u6742\u7684\u53cd\u5c04\u7387\u6a21\u578b\u5047\u8bbe\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u6ce2\u6bb5\u7684\u5bbd\u6cdb\u6ce2\u957f\u7a97\u53e3\u3002", "motivation": "\u592a\u9633\u8bf1\u5bfc\u8367\u5149\uff08SIF\uff09\u662f\u76d1\u6d4b\u690d\u88ab\u5149\u5408\u4f5c\u7528\u7684\u91cd\u8981\u6307\u6807\uff0c\u73b0\u6709\u53cd\u6f14\u65b9\u6cd5\u5b58\u5728\u5149\u8c31\u5f62\u72b6\u5047\u8bbe\u6216\u8bad\u7ec3\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u6280\u672f\u3002", "method": "WAFER\u65b9\u6cd5\u901a\u8fc7\u5c0f\u6ce2\u5206\u89e3\u53cd\u5c04\u5149\u8c31\u548c\u53c2\u8003\u5149\u8c31\u7684\u7edd\u5bf9\u503c\u5438\u6536\u7ebf\u6df1\u5ea6\uff0c\u72ec\u7acb\u4e8e\u8367\u5149\u63d0\u53d6\u76f8\u5bf9\u53cd\u5c04\u7387\uff0c\u8367\u5149\u4fe1\u53f7\u901a\u8fc7\u5269\u4f59\u504f\u79fb\u91cf\u83b7\u53d6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u5b9e\u5730\u6d4b\u91cf\u6570\u636e\u4e2d\uff0cWAFER\u4e0eiFLD\u548cSFM\u7b49\u65b9\u6cd5\u8868\u73b0\u4e00\u81f4\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8367\u5149\u4fe1\u53f7\u771f\u5b9e\u5149\u8c31\u5f62\u72b6\u7684\u63a2\u7d22\u53ef\u80fd\u6027\u3002", "conclusion": "WAFER\u662f\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u5047\u8bbe\u9650\u5236\u5c11\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6d4b\u91cf\u573a\u666f\uff0c\u5e76\u80fd\u907f\u514d\u73b0\u6709\u6a21\u578b\u7684\u504f\u5dee\u3002"}}
{"id": "2509.11793", "pdf": "https://arxiv.org/pdf/2509.11793", "abs": "https://arxiv.org/abs/2509.11793", "authors": ["Mihir Kulkarni", "Mihir Dharmadhikari", "Nikhil Khedekar", "Morten Nissov", "Mohit Singh", "Philipp Weiss", "Kostas Alexis"], "title": "UniPilot: Enabling GPS-Denied Autonomy Across Embodiments", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents UniPilot, a compact hardware-software autonomy payload\nthat can be integrated across diverse robot embodiments to enable autonomous\noperation in GPS-denied environments. The system integrates a multi-modal\nsensing suite including LiDAR, radar, vision, and inertial sensing for robust\noperation in conditions where uni-modal approaches may fail. UniPilot runs a\ncomplete autonomy software comprising multi-modal perception, exploration and\ninspection path planning, and learning-based navigation policies. The payload\nprovides robust localization, mapping, planning, and safety and control\ncapabilities in a single unit that can be deployed across a wide range of\nplatforms. A large number of experiments are conducted across diverse\nenvironments and on a variety of robot platforms to validate the mapping,\nplanning, and safe navigation capabilities enabled by the payload.", "AI": {"tldr": "UniPilot\u662f\u4e00\u79cd\u786c\u4ef6-\u8f6f\u4ef6\u7ed3\u5408\u7684\u81ea\u4e3b\u8d1f\u8f7d\uff0c\u53ef\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\u4e3a\u591a\u6837\u673a\u5668\u4eba\u63d0\u4f9b\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u5355\u4e00\u6a21\u6001\u4f20\u611f\u5668\u65e0\u6cd5\u4fdd\u8bc1\u673a\u5668\u4eba\u7a33\u5065\u8fd0\u884c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u611f\u77e5\u548c\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u6574\u5408LiDAR\u3001\u96f7\u8fbe\u3001\u89c6\u89c9\u548c\u60ef\u6027\u4f20\u611f\u7684\u591a\u6a21\u6001\u611f\u77e5\u5957\u4ef6\uff0c\u7ed3\u5408\u81ea\u4e3b\u8f6f\u4ef6\uff08\u611f\u77e5\u3001\u8def\u5f84\u89c4\u5212\u3001\u5bfc\u822a\u7b56\u7565\uff09\uff0c\u5b9e\u73b0\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u89c4\u5212\u548c\u5b89\u5168\u63a7\u5236\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cUniPilot\u80fd\u5728\u591a\u6837\u73af\u5883\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7a33\u5065\u7684\u5efa\u56fe\u3001\u89c4\u5212\u548c\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "UniPilot\u662f\u4e00\u79cd\u4fbf\u643a\u4e14\u901a\u7528\u7684\u81ea\u4e3b\u8d1f\u8f7d\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.11981", "pdf": "https://arxiv.org/pdf/2509.11981", "abs": "https://arxiv.org/abs/2509.11981", "authors": ["Haoze He", "Artemis Pados", "Daniel Kressner"], "title": "RJD-BASE: Multi-Modal Spectral Clustering via Randomized Joint Diagonalization", "categories": ["math.NA", "cs.NA", "eess.SP"], "comment": null, "summary": "We revisit the problem of spectral clustering in multimodal settings, where\neach data modality is encoded as a graph Laplacian. While classical\napproaches--including joint diagonalization, spectral co-regularization, and\nmultiview clustering--attempt to align embeddings across modalities, they often\nrely on costly iterative refinement and may fail to directly target the\nspectral subspace relevant for clustering. In this work, we introduce two key\ninnovations. First, we bring the power of randomization to this setting by\nsampling random convex combinations of Laplacians as a simple and scalable\nalternative to explicit eigenspace alignment. Second, we propose a principled\nselection rule based on Bottom-$k$ Aggregated Spectral Energy (BASE)--a\n$k$-dimensional extension of the directional smoothness objective from recent\nminimax formulations--which we uniquely apply as a selection mechanism rather\nthan an optimization target. The result is Randomized Joint Diagonalization\nwith BASE Selection (RJD-BASE), a method that is easily implementable,\ncomputationally efficient, aligned with the clustering objective, and grounded\nin decades of progress in standard eigensolvers. Through experiments on\nsynthetic and real-world datasets, we show that RJD-BASE reliably selects\nhigh-quality embeddings, outperforming classical multimodal clustering methods\nat low computational cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRJD-BASE\u7684\u968f\u673a\u5316\u8054\u5408\u5bf9\u89d2\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408BASE\u9009\u62e9\u89c4\u5219\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u8c31\u805a\u7c7b\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u805a\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u7ec6\u5316\u4e14\u672a\u76f4\u63a5\u9488\u5bf9\u805a\u7c7b\u76ee\u6807\u7684\u8c31\u5b50\u7a7a\u95f4\uff0c\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6027\u80fd\u5c40\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u968f\u673a\u51f8\u7ec4\u5408Laplacian\u77e9\u9635\u548c\u57fa\u4e8eBottom-k\u805a\u5408\u8c31\u80fd\u91cf\uff08BASE\uff09\u7684\u9009\u62e9\u89c4\u5219\uff0c\u63d0\u51faRJD-BASE\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRJD-BASE\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80fd\u53ef\u9760\u9009\u62e9\u9ad8\u8d28\u91cf\u5d4c\u5165\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "RJD-BASE\u4e3a\u591a\u6a21\u6001\u8c31\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u4e0e\u805a\u7c7b\u76ee\u6807\u4e00\u81f4\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.11839", "pdf": "https://arxiv.org/pdf/2509.11839", "abs": "https://arxiv.org/abs/2509.11839", "authors": ["Jiacheng Liu", "Pengxiang Ding", "Qihang Zhou", "Yuxuan Wu", "Da Huang", "Zimian Peng", "Wei Xiao", "Weinan Zhang", "Lixin Yang", "Cewu Lu", "Donglin Wang"], "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory. For more details, please\nrefer to https://jiachengliu3.github.io/TrajBooster.", "AI": {"tldr": "KORR\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u5168\u5c40\u52a8\u6001\u5efa\u6a21\u63d0\u5347\u957f\u671f\u4efb\u52a1\u548c\u9ad8\u7cbe\u5ea6\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u671f\u4efb\u52a1\u548c\u9ad8\u7cbe\u5ea6\u63a7\u5236\u4e2d\u56e0\u8bef\u5dee\u7d2f\u79ef\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5f15\u5165\u5168\u5c40\u52a8\u6001\u5efa\u6a21\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51faKORR\u6846\u67b6\uff0c\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5728\u9690\u542b\u7a7a\u95f4\u4e2d\u5efa\u7acb\u7ebf\u6027\u4e0d\u53d8\u7ed3\u6784\uff0c\u6307\u5bfc\u6b8b\u5dee\u7b56\u7565\u7684\u5168\u5c40\u4f18\u5316\u3002", "result": "\u5728\u957f\u671f\u3001\u7cbe\u7ec6\u7684\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u4e2d\uff0cKORR\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Koopman\u57fa\u7840\u7684\u5efa\u6a21\u65b9\u6cd5\u4e3a\u73b0\u4ee3\u5b66\u4e60\u6280\u672f\u4e0e\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u67b6\u8bbe\u6865\u6881\uff0c\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\u3002"}}
{"id": "2509.12074", "pdf": "https://arxiv.org/pdf/2509.12074", "abs": "https://arxiv.org/abs/2509.12074", "authors": ["Mohammadreza Narimani", "Alireza Pourreza", "Ali Moghimi", "Parastoo Farajpoor", "Hamid Jafarbiglu", "Mohsen B. Mesgaran"], "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP", "68T07, 68T45, 68U10", "I.5.4; I.4.6; I.2.6"], "comment": "Author-accepted version. Accepted and presented at AGRICONTROL 2025\n  (8th IFAC Conference on Sensing, Control and Automation Technologies for\n  Agriculture), UC Davis, USA. To appear in IFAC-PapersOnLine (Elsevier)", "summary": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic\nweed that threatens tomato production by extracting nutrients from the host. We\ninvestigate early detection using leaf-level spectral reflectance (400-2500 nm)\nand ensemble machine learning. In a field experiment in Woodland, California,\nwe tracked 300 tomato plants across growth stages defined by growing degree\ndays (GDD). Leaf reflectance was acquired with a portable spectrometer and\npreprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,\ncorrelation-based band reduction). Clear class differences were observed near\n1500 nm and 2000 nm water absorption features, consistent with reduced leaf\nwater content in infected plants at early stages. An ensemble combining Random\nForest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at\n585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy\ndeclined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and\nweed interference. Despite the small number of infected plants and\nenvironmental confounders, results show that proximal sensing with ensemble\nlearning enables timely detection of broomrape before canopy symptoms are\nvisible, supporting targeted interventions and reduced yield losses.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u53f6\u7247\u5149\u8c31\u53cd\u5c04\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5206\u652f\u9ed1\u7a57\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe89%\u3002", "motivation": "\u5206\u652f\u9ed1\u7a57\u75c5\u662f\u4e00\u79cd\u5bc4\u751f\u6027\u6742\u8349\uff0c\u5a01\u80c1\u756a\u8304\u751f\u4ea7\uff0c\u65e9\u671f\u68c0\u6d4b\u53ef\u51cf\u5c11\u4ea7\u91cf\u635f\u5931\u3002", "method": "\u4f7f\u7528\u4fbf\u643a\u5f0f\u5149\u8c31\u4eea\u91c7\u96c6\u53f6\u7247\u53cd\u5c04\u6570\u636e\uff0c\u7ed3\u5408\u9884\u5904\u7406\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001XGBoost\u7b49\uff09\u3002", "result": "\u5728585 GDD\u65f6\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe89%\uff0c\u4f46\u968f\u7740\u751f\u957f\u9636\u6bb5\u63a8\u8fdb\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u81f369%\u3002", "conclusion": "\u8fd1\u7aef\u4f20\u611f\u7ed3\u5408\u96c6\u6210\u5b66\u4e60\u80fd\u5728\u8089\u773c\u53ef\u89c1\u75c7\u72b6\u524d\u53ca\u65f6\u68c0\u6d4b\u5206\u652f\u9ed1\u7a57\u75c5\uff0c\u6709\u52a9\u4e8e\u9488\u5bf9\u6027\u5e72\u9884\u3002"}}
{"id": "2509.11865", "pdf": "https://arxiv.org/pdf/2509.11865", "abs": "https://arxiv.org/abs/2509.11865", "authors": ["Travis Davies", "Yiqi Huang", "Yunxin Liu", "Xiang Chen", "Huxian Liu", "Luhui Hu"], "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies.", "AI": {"tldr": "Tenma\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6269\u6563-Transformer\u7b56\u7565\uff0c\u7528\u4e8e\u53cc\u81c2\u63a7\u5236\uff0c\u901a\u8fc7\u8de8\u5b9e\u4f8b\u5b66\u4e60\u5728\u5f02\u6784\u3001\u591a\u6a21\u6001\u673a\u5668\u4eba\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u6210\u529f\u738788.95%\uff0c\u8fdc\u8d85\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u8f7b\u91cf\u7ea7\u8de8\u5b9e\u4f8b\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7ed3\u5408Transformer\u548c\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "method": "Tenma\u91c7\u7528\u4e86\u8de8\u5b9e\u4f8b\u5f52\u4e00\u5316\u5668\u3001\u8054\u5408\u72b6\u6001-\u65f6\u95f4\u7f16\u7801\u5668\u548c\u4f18\u5316\u7684\u6269\u6563\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u6574\u5408\u591a\u89c6\u56feRGB\u3001\u672c\u4f53\u611f\u77e5\u548c\u8bed\u8a00\u4fe1\u606f\u3002", "result": "Tenma\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6210\u529f\u738788.95%\uff0c\u4e14\u5728\u5bf9\u8c61\u548c\u573a\u666f\u53d8\u5316\u4e0b\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u8fdc\u8d85\u57fa\u7ebf\uff0818.12%\uff09\u3002", "conclusion": "Tenma\u5c55\u793a\u4e86\u591a\u6a21\u6001\u548c\u8de8\u5b9e\u4f8b\u5b66\u4e60\u7b56\u7565\u5728\u63d0\u5347\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u80fd\u529b\u4e0a\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.11930", "pdf": "https://arxiv.org/pdf/2509.11930", "abs": "https://arxiv.org/abs/2509.11930", "authors": ["Ruijia Liu", "Ancheng Hou", "Shaoyuan Li", "Xiang Yin"], "title": "VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Diffusion-based planners have gained significant recent attention for their\nrobustness and performance in long-horizon tasks. However, most existing\nplanners rely on a fixed, pre-specified horizon during both training and\ninference. This rigidity often produces length-mismatch (trajectories that are\ntoo short or too long) and brittle performance across instances with varying\ngeometric or dynamical difficulty. In this paper, we introduce the Variable\nHorizon Diffuser (VHD) framework, which treats the horizon as a learned\nvariable rather than a fixed hyperparameter. Given a start-goal pair, we first\npredict an instance-specific horizon using a learned Length Predictor model,\nwhich guides a Diffusion Planner to generate a trajectory of the desired\nlength. Our design maintains compatibility with existing diffusion planners by\ncontrolling trajectory length through initial noise shaping and training on\nrandomly cropped sub-trajectories, without requiring architectural changes.\nEmpirically, VHD improves success rates and path efficiency in maze-navigation\nand robot-arm control benchmarks, showing greater robustness to horizon\nmismatch and unseen lengths, while keeping training simple and offline-only.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVHD\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u53ef\u53d8\u65f6\u95f4\u8303\u56f4\u6765\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u89c4\u5212\u5668\u4e2d\u56fa\u5b9a\u65f6\u95f4\u8303\u56f4\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u548c\u673a\u5668\u4eba\u63a7\u5236\u7684\u6210\u529f\u7387\u4e0e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u89c4\u5212\u5668\u5728\u8bad\u7ec3\u548c\u63a8\u65ad\u4e2d\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u65f6\u95f4\u8303\u56f4\uff0c\u5bfc\u81f4\u8f68\u8ff9\u957f\u5ea6\u4e0d\u5339\u914d\u548c\u6027\u80fd\u8106\u5f31\u6027\u3002VHD\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u53ef\u53d8\u65f6\u95f4\u8303\u56f4\u6765\u63d0\u5347\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "VHD\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u7684\u65f6\u95f4\u8303\u56f4\u9884\u6d4b\u6a21\u578b\uff08Length Predictor\uff09\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5b9e\u4f8b\u9884\u6d4b\u5408\u9002\u7684\u65f6\u95f4\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u521d\u59cb\u566a\u58f0\u8c03\u6574\u548c\u968f\u673a\u88c1\u526a\u5b50\u8f68\u8ff9\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6269\u6563\u89c4\u5212\u5668\u7684\u67b6\u6784\u3002", "result": "VHD\u5728\u8ff7\u5bab\u5bfc\u822a\u548c\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\uff0c\u5bf9\u65f6\u95f4\u8303\u56f4\u4e0d\u5339\u914d\u548c\u672a\u89c1\u957f\u5ea6\u7684\u4efb\u52a1\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VHD\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u89c4\u5212\u5668\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bad\u7ec3\u7684\u7b80\u5355\u6027\u548c\u79bb\u7ebf\u7279\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u957f\u65f6\u4efb\u52a1\u3002"}}
{"id": "2509.11964", "pdf": "https://arxiv.org/pdf/2509.11964", "abs": "https://arxiv.org/abs/2509.11964", "authors": ["Junyoung Kim", "Minsik Jeon", "Jihong Min", "Kiho Kwak", "Junwon Seo"], "title": "E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping", "categories": ["cs.RO"], "comment": "Our project website can be found at\n  https://kjyoung.github.io/Homepage/#/Projects/E2-BKI", "summary": "Semantic mapping aims to construct a 3D semantic representation of the\nenvironment, providing essential knowledge for robots operating in complex\noutdoor settings. While Bayesian Kernel Inference (BKI) addresses\ndiscontinuities of map inference from sparse sensor data, existing semantic\nmapping methods suffer from various sources of uncertainties in challenging\noutdoor environments. To address these issues, we propose an uncertainty-aware\nsemantic mapping framework that handles multiple sources of uncertainties,\nwhich significantly degrade mapping performance. Our method estimates\nuncertainties in semantic predictions using Evidential Deep Learning and\nincorporates them into BKI for robust semantic inference. It further aggregates\nnoisy observations into coherent Gaussian representations to mitigate the\nimpact of unreliable points, while employing geometry-aligned kernels that\nadapt to complex scene structures. These Gaussian primitives effectively fuse\nlocal geometric and semantic information, enabling robust, uncertainty-aware\nmapping in complex outdoor scenarios. Comprehensive evaluation across diverse\noff-road and urban outdoor environments demonstrates consistent improvements in\nmapping quality, uncertainty calibration, representational flexibility, and\nrobustness, while maintaining real-time efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8bed\u4e49\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u6838\u63a8\u65ad\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8bed\u4e49\u6620\u5c04\u7684\u591a\u6e90\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6620\u5c04\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49\u6620\u5c04\u65b9\u6cd5\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u56e0\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u6e90\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u7a00\u758f\u4f20\u611f\u5668\u6570\u636e\u7684\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7684\u6846\u67b6\u4f7f\u7528\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u4f30\u8ba1\u8bed\u4e49\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c06\u5176\u878d\u5165\u8d1d\u53f6\u65af\u6838\u63a8\u65ad\uff1b\u540c\u65f6\u901a\u8fc7\u9ad8\u65af\u8868\u793a\u805a\u5408\u566a\u58f0\u89c2\u6d4b\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55\u5bf9\u9f50\u6838\u9002\u5e94\u590d\u6742\u573a\u666f\u7ed3\u6784\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6237\u5916\u73af\u5883\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6620\u5c04\u8d28\u91cf\u3001\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3001\u8868\u793a\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4e00\u81f4\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6e90\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u548c\u9ad8\u65af\u57fa\u5143\u878d\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u590d\u6742\u6237\u5916\u73af\u5883\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u8bed\u4e49\u6620\u5c04\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11971", "pdf": "https://arxiv.org/pdf/2509.11971", "abs": "https://arxiv.org/abs/2509.11971", "authors": ["James C. Ward", "Alex Bott", "Connor York", "Edmund R. Hunt"], "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study", "categories": ["cs.RO", "cs.AI", "cs.CR"], "comment": null, "summary": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5bf9\u624b\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u5bf9\u591a\u673a\u5668\u4eba\u5de1\u903b\u7cfb\u7edf\u7684\u653b\u51fb\uff0c\u4ee5\u8bc4\u4f30\u7cfb\u7edf\u5728\u6709\u9650\u65f6\u95f4\u5185\u963b\u6b62\u5165\u4fb5\u7684\u80fd\u529b\u3002\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u65b0\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u672a\u6765\u5de1\u903b\u7b56\u7565\u8bbe\u8ba1\u3002", "motivation": "\u901a\u8fc7\u6a21\u62df\u5bf9\u7269\u7406\u81ea\u4e3b\u7cfb\u7edf\u7684\u654c\u5bf9\u653b\u51fb\uff0c\u53ef\u4ee5\u8bc4\u4f30\u5176\u6297\u653b\u51fb\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u6f0f\u6d1e\u611f\u77e5\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002\u672c\u6587\u4ee5\u591a\u673a\u5668\u4eba\u5de1\u903b\u4e3a\u4f8b\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89c2\u5bdf\u5de1\u903b\u884c\u4e3a\u5e76\u5c1d\u8bd5\u5728\u9650\u5b9a\u65f6\u95f4\u5185\u672a\u88ab\u68c0\u6d4b\u5730\u5165\u4fb5\u5b89\u5168\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5bf9\u624b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u89c2\u5bdf\u673a\u5668\u4eba\u5de1\u903b\u884c\u4e3a\u6765\u9884\u6d4b\u6700\u4f73\u5165\u4fb5\u8def\u5f84\uff0c\u5e76\u5728\u6709\u9650\u65f6\u95f4\u5185\u5c1d\u8bd5\u672a\u88ab\u68c0\u6d4b\u5730\u8fdb\u5165\u5b89\u5168\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e3a\u5de1\u903b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u3002\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u88ab\u7528\u4e8e\u8bc4\u4f30\u591a\u79cd\u9886\u5148\u7684\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u5de1\u903b\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u673a\u5668\u4eba\u5de1\u903b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5de5\u5177\uff0c\u901a\u8fc7\u66f4\u4e25\u683c\u7684\u5bf9\u624b\u6a21\u578b\u63ed\u793a\u4e86\u6f5c\u5728\u6f0f\u6d1e\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5de1\u903b\u7b56\u7565\u7684\u4f18\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2509.12008", "pdf": "https://arxiv.org/pdf/2509.12008", "abs": "https://arxiv.org/abs/2509.12008", "authors": ["Yuqing Song", "Cesare Tonola", "Stefano Savazzi", "Sanaz Kianoush", "Nicola Pedrocchi", "Stephan Sigg"], "title": "Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees", "categories": ["cs.RO"], "comment": null, "summary": "As robots become increasingly prevalent in both homes and industrial\nsettings, the demand for intuitive and efficient human-machine interaction\ncontinues to rise. Gesture recognition offers an intuitive control method that\ndoes not require physical contact with devices and can be implemented using\nvarious sensing technologies. Wireless solutions are particularly flexible and\nminimally invasive. While camera-based vision systems are commonly used, they\noften raise privacy concerns and can struggle in complex or poorly lit\nenvironments. In contrast, radar sensing preserves privacy, is robust to\nocclusions and lighting, and provides rich spatial data such as distance,\nrelative velocity, and angle. We present a gesture-controlled robotic arm using\nmm-wave radar for reliable, contactless motion recognition. Nine gestures are\nrecognized and mapped to real-time commands with precision. Case studies are\nconducted to demonstrate the system practicality, performance and reliability\nfor gesture-based robotic manipulation. Unlike prior work that treats gesture\nrecognition and robotic control separately, our system unifies both into a\nreal-time pipeline for seamless, contactless human-robot interaction.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u901a\u8fc7\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5b9e\u73b0\u624b\u52bf\u63a7\u5236\u7684\u673a\u5668\u4eba\u624b\u81c2\uff0c\u89e3\u51b3\u4e86\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u9690\u79c1\u548c\u590d\u6742\u73af\u5883\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u548c\u5de5\u4e1a\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u76f4\u89c2\u4e14\u975e\u63a5\u89e6\u5f0f\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u6beb\u7c73\u6ce2\u96f7\u8fbe\u8bc6\u522b\u4e5d\u79cd\u624b\u52bf\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u4e3a\u5b9e\u65f6\u63a7\u5236\u547d\u4ee4\uff0c\u7ed3\u5408\u624b\u52bf\u8bc6\u522b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u65e0\u7f1d\u7684\u975e\u63a5\u89e6\u5f0f\u4ea4\u4e92\u3002", "conclusion": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u624b\u52bf\u63a7\u5236\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u53cb\u597d\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12129", "pdf": "https://arxiv.org/pdf/2509.12129", "abs": "https://arxiv.org/abs/2509.12129", "authors": ["Jiazhao Zhang", "Anqi Li", "Yunpeng Qi", "Minghan Li", "Jiahang Liu", "Shaoan Wang", "Haoran Liu", "Gengze Zhou", "Yuze Wu", "Xingxing Li", "Yuxin Fan", "Wenjun Li", "Zhibo Chen", "Fei Gao", "Qi Wu", "Zhizheng Zhang", "He Wang"], "title": "Embodied Navigation Foundation Model", "categories": ["cs.RO"], "comment": "Project Page: https://pku-epic.github.io/NavFoM-Web/", "summary": "Navigation is a fundamental capability in embodied AI, representing the\nintelligence required to perceive and interact within physical environments\nfollowing language instructions. Despite significant progress in large\nVision-Language Models (VLMs), which exhibit remarkable zero-shot performance\non general vision-language tasks, their generalization ability in embodied\nnavigation remains largely confined to narrow task settings and\nembodiment-specific architectures. In this work, we introduce a\ncross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained\non eight million navigation samples that encompass quadrupeds, drones, wheeled\nrobots, and vehicles, and spanning diverse tasks such as vision-and-language\nnavigation, object searching, target tracking, and autonomous driving. NavFoM\nemploys a unified architecture that processes multimodal navigation inputs from\nvarying camera configurations and navigation horizons. To accommodate diverse\ncamera setups and temporal horizons, NavFoM incorporates identifier tokens that\nembed camera view information of embodiments and the temporal context of tasks.\nFurthermore, to meet the demands of real-world deployment, NavFoM controls all\nobservation tokens using a dynamically adjusted sampling strategy under a\nlimited token length budget. Extensive evaluations on public benchmarks\ndemonstrate that our model achieves state-of-the-art or highly competitive\nperformance across multiple navigation tasks and embodiments without requiring\ntask-specific fine-tuning. Additional real-world experiments further confirm\nthe strong generalization capability and practical applicability of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u4efb\u52a1\u548c\u8de8\u5b9e\u4f53\u7684\u5bfc\u822a\u57fa\u7840\u6a21\u578bNavFoM\uff0c\u7edf\u4e00\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u5728\u591a\u79cd\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u9002\u5e94\u4e0d\u540c\u5b9e\u4f53\u914d\u7f6e\u548c\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u67b6\u6784\uff0c\u5f15\u5165\u6807\u8bc6\u7b26\u5d4c\u5165\u76f8\u673a\u89c6\u56fe\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u52a8\u6001\u8c03\u6574\u89c2\u6d4b\u4ee4\u724c\u4ee5\u4f18\u5316\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "conclusion": "NavFoM\u5c55\u73b0\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5bfc\u822a\u9886\u57df\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12151", "pdf": "https://arxiv.org/pdf/2509.12151", "abs": "https://arxiv.org/abs/2509.12151", "authors": ["Zongyao Yi", "Joachim Hertzberg", "Martin Atzmueller"], "title": "Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a learnable physics simulator that provides accurate motion and\nforce-torque prediction of robot end effectors in contact-rich manipulation.\nThe proposed model extends the state-of-the-art GNN-based simulator (FIGNet)\nwith novel node and edge types, enabling action-conditional predictions for\ncontrol and state estimation tasks. In simulation, the MPC agent using our\nmodel matches the performance of the same controller with the ground truth\ndynamics model in a challenging peg-in-hole task, while in the real-world\nexperiment, our model achieves a 50% improvement in motion prediction accuracy\nand 3$\\times$ increase in force-torque prediction precision over the baseline\nphysics simulator. Source code and data are publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u7269\u7406\u6a21\u62df\u5668\uff0c\u6539\u8fdb\u73b0\u6709\u7684GNN\u6a21\u62df\u5668\uff08FIGNet\uff09\uff0c\u5728\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8fd0\u52a8\u548c\u529b-\u529b\u77e9\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u4e2d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8fd0\u52a8\u548c\u529b-\u529b\u77e9\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u6269\u5c55FIGNet\u6a21\u578b\uff0c\u5f15\u5165\u65b0\u7684\u8282\u70b9\u548c\u8fb9\u7c7b\u578b\uff0c\u652f\u6301\u52a8\u4f5c\u6761\u4ef6\u9884\u6d4b\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cMPC\u63a7\u5236\u5668\u8868\u73b0\u4e0e\u771f\u5b9e\u52a8\u6001\u6a21\u578b\u76f8\u5f53\uff1b\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u8fd0\u52a8\u548c\u529b-\u529b\u77e9\u9884\u6d4b\u51c6\u786e\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2509.10711", "pdf": "https://arxiv.org/pdf/2509.10711", "abs": "https://arxiv.org/abs/2509.10711", "authors": ["Subhajit Pramanick", "Saswata Jana", "Partha Sarathi Mandal", "Gokarna Sharma"], "title": "Asynchronous Gathering of Opaque Robots with Mobility Faults", "categories": ["cs.DC", "cs.MA", "cs.RO"], "comment": "38 pages, 26 figures, and 1 table", "summary": "We consider the fundamental benchmarking problem of gathering in an\n$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail\nat any execution, under asynchrony. Two seminal results established\nimpossibility of a solution in the oblivious robot (OBLOT) model in a\n$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault\nsystem under asynchrony. Recently, a breakthrough result circumvented the first\nimpossibility result by giving a deterministic algorithm in a $(2,0)$-fault\nsystem under asynchrony in the luminous robot (LUMI) model using 2-colored\nlights. However, a breakthrough result established impossibility of gathering\nin a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this\npaper, we consider a {\\em mobility fault} model in which a robot crash only\nimpacts it mobility but not the operation of the light.\n  We establish four results under asynchrony in LUMI with the mobility fault\nmodel. We show that it is impossible to solve gathering in a $(2,1)$-mobility\nfault system using 2-colored lights, and then give a solution using 3-colored\nlights, which is optimal w.r.t. the number of colors. We then consider an\n$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two\ndeterministic algorithms that exhibit a nice time-color trade-off: The first\nwith time $O(N)$ using 7-colored lights and the second with time\n$O(\\max\\{\\ell,f\\})$ using 26-colored lights, where $\\ell< N$ is the number of\ndistinct convex layers of robot positions in the initial configuration.\nInterestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an\n$(N,f)$-mobility fault system are the first to be analysed time complexity, can\nwithstand obstructed visibility (opaque robot model) and asynchronous\nscheduling.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u5177\u6709(N,f)-\u6545\u969c\u7cfb\u7edf\u7684\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79fb\u52a8\u6545\u969c\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u56db\u79cd\u7ed3\u679c\uff0c\u5305\u62ec\u4e0d\u53ef\u80fd\u6027\u548c\u786e\u5b9a\u6027\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u5f02\u6b65\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b58\u5728\u79fb\u52a8\u6545\u969c\u65f6\u7684\u805a\u96c6\u95ee\u9898\uff0c\u63a2\u7d22\u4e0d\u540c\u989c\u8272\u706f\u5149\u548c\u7b97\u6cd5\u5bf9\u65f6\u95f4\u548c\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528LUMI\u6a21\u578b\uff08\u5e26\u706f\u5149\u673a\u5668\u4eba\uff09\u548c\u79fb\u52a8\u6545\u969c\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6545\u969c\u7cfb\u7edf\u548c\u4e0d\u540c\u989c\u8272\u706f\u5149\u4e0b\u7684\u805a\u96c6\u53ef\u80fd\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u786e\u5b9a\u6027\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5728(2,1)-\u79fb\u52a8\u6545\u969c\u7cfb\u7edf\u4e2d\u4f7f\u75282\u8272\u706f\u5149\u65e0\u6cd5\u5b9e\u73b0\u805a\u96c6\uff0c\u4f46\u4f7f\u75283\u8272\u706f\u5149\u53ef\u884c\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u5206\u522b\u5177\u67097\u8272\u548c26\u8272\u706f\u5149\uff0c\u5c55\u793a\u4e86\u65f6\u95f4\u4e0e\u989c\u8272\u7684\u6298\u8877\u3002", "conclusion": "\u5728\u79fb\u52a8\u6545\u969c\u6a21\u578b\u4e0b\uff0c\u805a\u96c6\u95ee\u9898\u7684\u6700\u4f18\u89e3\u4f9d\u8d56\u4e8e\u706f\u5149\u7684\u989c\u8272\u6570\u91cf\u548c\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6545\u969c\u6570\u91cf\u6709\u9650\u65f6\uff0c\u7b97\u6cd5\u7684\u6548\u7387\u6700\u4f18\u3002"}}
{"id": "2509.10813", "pdf": "https://arxiv.org/pdf/2509.10813", "abs": "https://arxiv.org/abs/2509.10813", "authors": ["Weipeng Zhong", "Peizhou Cao", "Yichen Jin", "Li Luo", "Wenzhe Cai", "Jingli Lin", "Hanqing Wang", "Zhaoyang Lyu", "Tai Wang", "Bo Dai", "Xudong Xu", "Jiangmiao Pang"], "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aInternScenes\u7684\u5927\u89c4\u6a21\u53ef\u6a21\u62df\u5ba4\u5185\u573a\u666f\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5982\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u3001\u5e03\u5c40\u7b80\u5355\u3001\u5bf9\u8c61\u78b0\u649e\u4e25\u91cd\u7b49\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u6570\u636e\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u5e03\u5c40\u771f\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u63a8\u52a8Embodied AI\u7684\u53d1\u5c55\u3002", "method": "\u6574\u5408\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u573a\u666f\u6765\u6e90\uff08\u771f\u5b9e\u626b\u63cf\u3001\u7a0b\u5e8f\u751f\u6210\u3001\u8bbe\u8ba1\u5e08\u521b\u5efa\uff09\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u7ba1\u9053\u5904\u7406\u786e\u4fdd\u53ef\u6a21\u62df\u6027\u548c\u4ea4\u4e92\u6027\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u7ea640,000\u4e2a\u573a\u666f\u30011.96M\u4e2a3D\u5bf9\u8c61\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u590d\u6742\u573a\u666f\u5e03\u5c40\u751f\u6210\u548c\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "InternScenes\u4e3a\u590d\u6742\u573a\u666f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u89c4\u6a21\u5316\uff0c\u5e76\u627f\u8bfa\u5f00\u6e90\u6570\u636e\u548c\u5de5\u5177\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2509.10841", "pdf": "https://arxiv.org/pdf/2509.10841", "abs": "https://arxiv.org/abs/2509.10841", "authors": ["Simone Mosco", "Daniel Fusaro", "Wanmeng Li", "Emanuele Menegatti", "Alberto Pretto"], "title": "Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to Computer Vision and Image Understanding", "summary": "LiDAR point cloud semantic segmentation is essential for interpreting 3D\nenvironments in applications such as autonomous driving and robotics. Recent\nmethods achieve strong performance by exploiting different point cloud\nrepresentations or incorporating data from other sensors, such as cameras or\nexternal datasets. However, these approaches often suffer from high\ncomputational complexity and require large amounts of training data, limiting\ntheir generalization in data-scarce scenarios. In this paper, we improve the\nperformance of point-based methods by effectively learning features from 2D\nrepresentations through point-plane projections, enabling the extraction of\ncomplementary information while relying solely on LiDAR data. Additionally, we\nintroduce a geometry-aware technique for data augmentation that aligns with\nLiDAR sensor properties and mitigates class imbalance. We implemented and\nevaluated our method that applies point-plane projections onto multiple\ninformative 2D representations of the point cloud. Experiments demonstrate that\nthis approach leads to significant improvements in limited-data scenarios,\nwhile also achieving competitive results on two publicly available standard\ndatasets, as SemanticKITTI and PandaSet. The code of our method is available at\nhttps://github.com/SiMoM0/3PNet", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u70b9\u5e73\u9762\u6295\u5f71\u4ece2D\u8868\u793a\u4e2d\u5b66\u4e60\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86LiDAR\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u51e0\u4f55\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5927\u6570\u636e\u9700\u6c42\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u70b9\u5e73\u9762\u6295\u5f71\u4ece2D\u8868\u793a\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u6570\u636e\u6216\u4f20\u611f\u5668\uff0c\u9002\u7528\u4e8e\u591a\u79cdLiDAR\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2509.10867", "pdf": "https://arxiv.org/pdf/2509.10867", "abs": "https://arxiv.org/abs/2509.10867", "authors": ["Leonardo Grando", "Jos\u00e9 Roberto Emiliano Leite", "Edson Luiz Ursini"], "title": "Agent-based Simulation for Drone Charging in an Internet of Things Environment System", "categories": ["cs.MA", "cs.NI", "cs.RO"], "comment": "4 pages", "summary": "This paper presents an agent-based simulation model for coordinating battery\nrecharging in drone swarms, focusing on applications in Internet of Things\n(IoT) and Industry 4.0 environments. The proposed model includes a detailed\ndescription of the simulation methodology, system architecture, and\nimplementation. One practical use case is explored: Smart Farming, highlighting\nhow autonomous coordination strategies can optimize battery usage and mission\nefficiency in large-scale drone deployments. This work uses a machine learning\ntechnique to analyze the agent-based simulation sensitivity analysis output\nresults.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u6a21\u578b\uff0c\u7528\u4e8e\u534f\u8c03\u65e0\u4eba\u673a\u7fa4\u7684\u7535\u6c60\u5145\u7535\uff0c\u91cd\u70b9\u5e94\u7528\u4e8e\u7269\u8054\u7f51\u548c\u5de5\u4e1a4.0\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u90e8\u7f72\u4e2d\u7535\u6c60\u4f7f\u7528\u548c\u4efb\u52a1\u6548\u7387\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6a21\u62df\u65b9\u6cd5\u3001\u7cfb\u7edf\u67b6\u6784\u548c\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u4e86\u6a21\u62df\u654f\u611f\u6027\u5206\u6790\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5728\u667a\u80fd\u519c\u4e1a\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u6a21\u578b\u8bc1\u660e\u4e86\u81ea\u4e3b\u534f\u8c03\u7b56\u7565\u5728\u4f18\u5316\u65e0\u4eba\u673a\u7535\u6c60\u4f7f\u7528\u548c\u4efb\u52a1\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.11082", "pdf": "https://arxiv.org/pdf/2509.11082", "abs": "https://arxiv.org/abs/2509.11082", "authors": ["Zongwu Xie", "Kaijie Yun", "Yang Liu", "Yiming Ji", "Han Li"], "title": "Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present a robust multi-modal framework for predicting traversability\ncostmaps for planetary rovers. Our model fuses camera and LiDAR data to produce\na bird's-eye-view (BEV) terrain costmap, trained self-supervised using\nIMU-derived labels. Key updates include a DINOv3-based image encoder,\nFiLM-based sensor fusion, and an optimization loss combining Huber and\nsmoothness terms. Experimental ablations (removing image color, occluding\ninputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases\nfrom ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry\ndominates the learned cost and the model is highly robust. We attribute the\nsmall performance differences to the IMU labeling primarily reflecting terrain\ngeometry rather than semantics and to limited data diversity. Unlike prior work\nclaiming large gains, we emphasize our contributions: (1) a high-fidelity,\nreproducible simulation environment; (2) a self-supervised IMU-based labeling\npipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss\nlimitations and future work such as domain generalization and dataset\nexpansion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u884c\u661f\u63a2\u6d4b\u8f66\u7684\u53ef\u901a\u884c\u6027\u6210\u672c\u5730\u56fe\uff0c\u878d\u5408\u76f8\u673a\u548cLiDAR\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u884c\u661f\u63a2\u6d4b\u8f66\u5728\u5730\u5f62\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u901a\u884c\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u7ed3\u5408DINOv3\u56fe\u50cf\u7f16\u7801\u5668\u548cFiLM\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0c\u81ea\u76d1\u7763\u8bad\u7ec3\u4f7f\u7528IMU\u751f\u6210\u6807\u7b7e\uff0c\u4f18\u5316\u635f\u5931\u51fd\u6570\u7ed3\u5408Huber\u548c\u5e73\u6ed1\u9879\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5bf9\u8f93\u5165\u5e72\u6270\uff08\u5982LiDAR\u7a00\u758f\u5316\uff09\u5177\u6709\u9ad8\u5ea6\u9c81\u68d2\u6027\uff0cMAE\u4ec5\u5c0f\u5e45\u589e\u52a0\uff080.0775\u52300.0915\uff09\u3002", "conclusion": "\u6a21\u578b\u4ee5\u51e0\u4f55\u4fe1\u606f\u4e3a\u4e3b\u5bfc\uff0c\u8d21\u732e\u5305\u62ec\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u3001\u81ea\u76d1\u7763\u6807\u7b7e\u751f\u6210\u548c\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u672a\u6765\u9700\u6539\u8fdb\u9886\u57df\u6cdb\u5316\u548c\u6570\u636e\u96c6\u6269\u5c55\u3002"}}
{"id": "2509.11453", "pdf": "https://arxiv.org/pdf/2509.11453", "abs": "https://arxiv.org/abs/2509.11453", "authors": ["BaiChen Fan", "Sifan Zhou", "Jian Li", "Shibo Zhao", "Muqing Cao", "Qin Wang"], "title": "Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "9 pages, 7 figures", "summary": "LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics\nand autonomous systems. Existing methods typically follow frame-wise motion\nestimation or a sequence-based paradigm. However, the two-frame methods are\nefficient but lack long-term temporal context, making them vulnerable in sparse\nor occluded scenes, while sequence-based methods that process multiple point\nclouds gain robustness at a significant computational cost. To resolve this\ndilemma, we propose a novel trajectory-based paradigm and its instantiation,\nTrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame\ntracker by implicitly learning motion continuity from historical bounding box\ntrajectories alone-without requiring additional, costly point cloud inputs. It\nfirst generates a fast, explicit motion proposal and then uses an implicit\nmotion modeling module to predict the future trajectory, which in turn refines\nand corrects the initial proposal. Extensive experiments on the large-scale\nNuScenes benchmark show that TrajTrack achieves new state-of-the-art\nperformance, dramatically improving tracking precision by 4.48% over a strong\nbaseline while running at 56 FPS. Besides, we also demonstrate the strong\ngeneralizability of TrajTrack across different base trackers. Video is\navailable at https://www.bilibili.com/video/BV1ahYgzmEWP.", "AI": {"tldr": "TrajTrack \u662f\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u7684 3D \u5355\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u9690\u5f0f\u5b66\u4e60\u8fd0\u52a8\u8fde\u7eed\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u5e76\u4fdd\u6301\u4e86\u9ad8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u6216\u906e\u6321\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u7ed3\u5408\u663e\u5f0f\u8fd0\u52a8\u63d0\u8bae\u548c\u9690\u5f0f\u8fd0\u52a8\u5efa\u6a21\u6a21\u5757\uff0c\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u5728 NuScenes \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7cbe\u5ea6\u63d0\u5347 4.48%\uff0c\u8fd0\u884c\u901f\u5ea6\u4e3a 56 FPS\u3002", "conclusion": "TrajTrack \u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.11480", "pdf": "https://arxiv.org/pdf/2509.11480", "abs": "https://arxiv.org/abs/2509.11480", "authors": ["Amir Taherin", "Juyi Lin", "Arash Akbari", "Arman Akbari", "Pu Zhao", "Weiwei Chen", "David Kaeli", "Yanzhi Wang"], "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "cs.RO"], "comment": "To appear in the Asilomar Conference on Signals, Systems, and\n  Computers 2025", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist\npolicies for robotic control, yet their performance scaling across model\narchitectures and hardware platforms, as well as their associated power\nbudgets, remain poorly understood. This work presents an evaluation of five\nrepresentative VLA models -- spanning state-of-the-art baselines and two newly\nproposed architectures -- targeting edge and datacenter GPU platforms. Using\nthe LIBERO benchmark, we measure accuracy alongside system-level metrics,\nincluding latency, throughput, and peak memory usage, under varying edge power\nconstraints and high-performance datacenter GPU configurations. Our results\nidentify distinct scaling trends: (1) architectural choices, such as action\ntokenization and model backbone size, strongly influence throughput and memory\nfootprint; (2) power-constrained edge devices exhibit non-linear performance\ndegradation, with some configurations matching or exceeding older datacenter\nGPUs; and (3) high-throughput variants can be achieved without significant\naccuracy loss. These findings provide actionable insights when selecting and\noptimizing VLAs across a range of deployment constraints. Our work challenges\ncurrent assumptions about the superiority of datacenter hardware for robotic\ninference.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cdVision-Language-Action\uff08VLA\uff09\u6a21\u578b\u5728\u8fb9\u7f18\u548c\u6570\u636e\u4e2d\u5fc3GPU\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u9009\u62e9\u548c\u786c\u4ef6\u914d\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u67b6\u6784\u548c\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u6269\u5c55\u53ca\u529f\u8017\u9884\u7b97\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f7f\u7528LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e94\u79cdVLA\u6a21\u578b\uff08\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u4e24\u79cd\u67b6\u6784\uff09\uff0c\u5728\u8fb9\u7f18\u548c\u6570\u636e\u4e2d\u5fc3GPU\u5e73\u53f0\u4e0a\u6d4b\u91cf\u4e86\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u7b49\u6307\u6807\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a(1)\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u541e\u5410\u91cf\u548c\u5185\u5b58\u5360\u7528\uff1b(2)\u8fb9\u7f18\u8bbe\u5907\u5728\u529f\u8017\u9650\u5236\u4e0b\u8868\u73b0\u51fa\u975e\u7ebf\u6027\u6027\u80fd\u4e0b\u964d\uff1b(3)\u9ad8\u541e\u5410\u91cf\u53d8\u4f53\u53ef\u5b9e\u73b0\u51e0\u4e4e\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u6570\u636e\u4e2d\u5fc3\u786c\u4ef6\u5728\u673a\u5668\u4eba\u63a8\u65ad\u4e2d\u7684\u4f18\u8d8a\u6027\u5047\u8bbe\uff0c\u4e3a\u5728\u4e0d\u540c\u90e8\u7f72\u7ea6\u675f\u4e0b\u9009\u62e9\u548c\u4f18\u5316VLA\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2509.11508", "pdf": "https://arxiv.org/pdf/2509.11508", "abs": "https://arxiv.org/abs/2509.11508", "authors": ["Tinglong Deng", "Hang Tao", "Xinxiang Wang", "Yinyan Wang", "Hanjiang Luo"], "title": "SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach", "categories": ["cs.MA", "cs.LG", "cs.RO"], "comment": null, "summary": "As underwater human activities are increasing, the demand for underwater\ncommunication service presents a significant challenge. Existing underwater\ndiver communication methods face hurdles due to inherent disadvantages and\ncomplex underwater environments. To address this issue, we propose a scheme\nthat utilizes maritime unmanned systems to assist divers with reliable and\nhigh-speed communication. Multiple AUVs are equipped with optical and acoustic\nmultimodal communication devices as relay nodes, providing adaptive\ncommunication services based on changes in the diver's activity area. By using\na multi-agent reinforcement learning (MARL) approach to control the cooperative\nmovement of AUVs, high-speed and reliable data transmission between divers can\nbe achieved. At the same time, utilizing the advantages of on-demand deployment\nand wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to\ncoordinate and forward information from AUVs, and controlling AUVs to\nadaptively select relay USV nodes for data transmission, high-quality\ncommunication between divers and surface platform can be achieved. Through\nsimulation verification, the proposed scheme can effectively achieve reliable\nand high-speed communication for divers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6d77\u4e0a\u65e0\u4eba\u7cfb\u7edf\u8f85\u52a9\u6f5c\u6c34\u5458\u5b9e\u73b0\u9ad8\u901f\u53ef\u9760\u901a\u4fe1\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u591aAUV\u548c\u591aUSV\u534f\u540c\u5de5\u4f5c\u3002", "motivation": "\u968f\u7740\u6c34\u4e0b\u4eba\u7c7b\u6d3b\u52a8\u589e\u52a0\uff0c\u4f20\u7edf\u6c34\u4e0b\u901a\u4fe1\u65b9\u6cd5\u9762\u4e34\u73af\u5883\u548c\u8bbe\u5907\u56fa\u6709\u7684\u52a3\u52bf\uff0c\u4e9f\u9700\u65b0\u578b\u901a\u4fe1\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591aAUV\u88c5\u5907\u5149\u58f0\u591a\u6a21\u6001\u901a\u4fe1\u8bbe\u5907\u4f5c\u4e3a\u4e2d\u7ee7\u8282\u70b9\uff0c\u7ed3\u5408MARL\u63a7\u5236AUV\u534f\u540c\u79fb\u52a8\uff1b\u5229\u7528USV\u534f\u8c03AUV\u4fe1\u606f\u8f6c\u53d1\uff0c\u5b9e\u73b0\u6f5c\u6c34\u5458\u4e0e\u6c34\u9762\u5e73\u53f0\u901a\u4fe1\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u6709\u6548\u5b9e\u73b0\u6f5c\u6c34\u5458\u95f4\u53ca\u6f5c\u6c34\u5458\u4e0e\u6c34\u9762\u5e73\u53f0\u95f4\u7684\u9ad8\u901f\u53ef\u9760\u901a\u4fe1\u3002", "conclusion": "\u8be5\u65b9\u6848\u901a\u8fc7\u591a\u65e0\u4eba\u7cfb\u7edf\u534f\u540c\u548c\u81ea\u9002\u5e94\u4e2d\u7ee7\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u901a\u4fe1\u7684\u6311\u6218\u3002"}}
{"id": "2509.11755", "pdf": "https://arxiv.org/pdf/2509.11755", "abs": "https://arxiv.org/abs/2509.11755", "authors": ["Paul Templier", "Hannah Janmohamed", "David Labonte", "Antoine Cully"], "title": "Time to Play: Simulating Early-Life Animal Dynamics Enhances Robotics Locomotion Discovery", "categories": ["cs.NE", "cs.RO"], "comment": null, "summary": "Developmental changes in body morphology profoundly shape locomotion in\nanimals, yet artificial agents and robots are typically trained under static\nphysical parameters. Inspired by ontogenetic scaling of muscle power in\nbiology, we propose Scaling Mechanical Output over Lifetime (SMOL), a novel\ncurriculum that dynamically modulates robot actuator strength to mimic natural\nvariations in power-to-weight ratio during growth and ageing. Integrating SMOL\ninto the MAP-Elites quality-diversity framework, we vary the torque in standard\nrobotics tasks to mimic the evolution of strength in animals as they grow up\nand as their body changes. Through comprehensive empirical evaluation, we show\nthat the SMOL schedule consistently elevates both performance and diversity of\nlocomotion behaviours across varied control scenarios, by allowing agents to\nleverage advantageous physics early on to discover skills that act as stepping\nstones when they reach their final standard body properties. Based on studies\nof the total power output in humans, we also implement the SMOL-Human schedule\nthat models isometric body variations due to non-linear changes like puberty,\nand study its impact on robotics locomotion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMOL\u7684\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u6267\u884c\u5668\u5f3a\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u6a21\u62df\u751f\u7269\u751f\u957f\u8fc7\u7a0b\u4e2d\u7684\u529f\u7387-\u91cd\u91cf\u6bd4\u53d8\u5316\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u8868\u73b0\u548c\u591a\u6837\u6027\u3002", "motivation": "\u53d7\u751f\u7269\u751f\u957f\u8fc7\u7a0b\u4e2d\u808c\u8089\u529f\u7387\u53d8\u5316\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u901a\u5e38\u5728\u9759\u6001\u7269\u7406\u53c2\u6570\u4e0b\u8bad\u7ec3\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5c06SMOL\u6574\u5408\u5230MAP-Elites\u8d28\u91cf-\u591a\u6837\u6027\u6846\u67b6\u4e2d\uff0c\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u626d\u77e9\u4ee5\u6a21\u62df\u52a8\u7269\u751f\u957f\u8fc7\u7a0b\u4e2d\u529b\u91cf\u7684\u53d8\u5316\u3002", "result": "SMOL\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u540c\u63a7\u5236\u573a\u666f\u4e0b\u7684\u8fd0\u52a8\u8868\u73b0\u548c\u884c\u4e3a\u591a\u6837\u6027\u3002", "conclusion": "\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u6267\u884c\u5668\u5f3a\u5ea6\u7684SMOL\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7279\u522b\u662f\u5728\u6a21\u62df\u751f\u7269\u751f\u957f\u8fc7\u7a0b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2509.11868", "pdf": "https://arxiv.org/pdf/2509.11868", "abs": "https://arxiv.org/abs/2509.11868", "authors": ["Sabrina Patania", "Luca Annese", "Anna Lambiase", "Anita Pellegrini", "Tom Foulsham", "Azzurra Ruggeri", "Silvia Rossi", "Silvia Serino", "Dimitri Ognibene"], "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO", "I.2; I.2.7; I.2.10; J.4"], "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/", "summary": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.", "AI": {"tldr": "PerspAct\u7cfb\u7edf\u7ed3\u5408ReAct\u8303\u5f0f\u548cLLMs\uff0c\u6a21\u62df\u89c6\u89d2\u91c7\u6837\u7684\u53d1\u5c55\u9636\u6bb5\uff0c\u8bc4\u4f30GPT\u751f\u6210\u7b26\u5408\u53d1\u5c55\u9636\u6bb5\u5185\u90e8\u53d9\u4e8b\u7684\u80fd\u529b\u53ca\u5176\u5bf9\u534f\u4f5c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "motivation": "\u8bed\u8a00\u548c\u5177\u8eab\u89c6\u89d2\u91c7\u6837\u5bf9\u4eba\u7c7b\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5f88\u5c11\u540c\u65f6\u8003\u8651\u4e24\u8005\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6269\u5c55\u5bfc\u6f14\u4efb\u52a1\uff0c\u8bc4\u4f30GPT\u751f\u6210\u53d1\u5c55\u4e00\u81f4\u53d9\u4e8b\u540e\u4efb\u52a1\u6267\u884c\u7684\u6548\u679c\uff0c\u4ece\u52a8\u4f5c\u9009\u62e9\u548c\u4efb\u52a1\u6548\u7387\u4e24\u65b9\u9762\u5206\u6790\u3002", "result": "GPT\u80fd\u751f\u6210\u53d1\u5c55\u4e00\u81f4\u7684\u53d9\u4e8b\uff0c\u4f46\u4e92\u52a8\u4e2d\u4f1a\u5411\u9ad8\u7ea7\u9636\u6bb5\u8f6c\u53d8\uff0c\u9ad8\u7ea7\u9636\u6bb5\u534f\u4f5c\u66f4\u6709\u6548\uff0c\u4f4e\u7ea7\u9636\u6bb5\u5728\u590d\u6742\u60c5\u5883\u4e2d\u6548\u679c\u4e0d\u4e00\u3002", "conclusion": "\u7ed3\u5408\u5177\u8eab\u89c6\u89d2\u91c7\u6837\u548c\u8bed\u8a00\u80fd\u66f4\u597d\u6a21\u62df\u53d1\u5c55\u52a8\u6001\uff0c\u5185\u90e8\u8a00\u8bed\u8bc4\u4f30\u5728\u8bed\u8a00\u4e0e\u5177\u8eab\u4efb\u52a1\u7ed3\u5408\u4e2d\u5f88\u91cd\u8981\u3002"}}
{"id": "2509.11959", "pdf": "https://arxiv.org/pdf/2509.11959", "abs": "https://arxiv.org/abs/2509.11959", "authors": ["Ao Liang", "Youquan Liu", "Yu Yang", "Dongyue Lu", "Linfeng Li", "Lingdong Kong", "Huaici Zhao", "Wei Tsang Ooi"], "title": "Learning to Generate 4D LiDAR Sequences", "categories": ["cs.CV", "cs.RO"], "comment": "Abstract Paper (Non-Archival) @ ICCV 2025 Wild3D Workshop; GitHub\n  Repo at https://lidarcrafter.github.io/", "summary": "While generative world models have advanced video and occupancy-based data\nsynthesis, LiDAR generation remains underexplored despite its importance for\naccurate 3D perception. Extending generation to 4D LiDAR data introduces\nchallenges in controllability, temporal stability, and evaluation. We present\nLiDARCrafter, a unified framework that converts free-form language into\neditable LiDAR sequences. Instructions are parsed into ego-centric scene\ngraphs, which a tri-branch diffusion model transforms into object layouts,\ntrajectories, and shapes. A range-image diffusion model generates the initial\nscan, and an autoregressive module extends it into a temporally coherent\nsequence. The explicit layout design further supports object-level editing,\nsuch as insertion or relocation. To enable fair assessment, we provide\nEvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On\nnuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and\ntemporal consistency, offering a foundation for LiDAR-based simulation and data\naugmentation.", "AI": {"tldr": "LiDARCrafter\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u5c06\u81ea\u7531\u683c\u5f0f\u8bed\u8a00\u8f6c\u5316\u4e3a\u53ef\u7f16\u8f91\u7684LiDAR\u5e8f\u5217\uff0c\u89e3\u51b3\u4e864D LiDAR\u6570\u636e\u751f\u6210\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1LiDAR\u5728\u7cbe\u786e3D\u611f\u77e5\u4e2d\u7684\u91cd\u8981\u6027\u5f88\u9ad8\uff0c\u4f46LiDAR\u751f\u6210\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u57284D\u6570\u636e\u4e2d\u7684\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86LiDARCrafter\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u89e3\u6790\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u56fe\uff0c\u901a\u8fc7\u4e09\u652f\u6269\u6563\u6a21\u578b\u751f\u6210\u5bf9\u8c61\u5e03\u5c40\u3001\u8f68\u8ff9\u548c\u5f62\u72b6\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u8303\u56f4\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u5757\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u5e8f\u5217\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cLiDARCrafter\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u63a7\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LiDARCrafter\u4e3a\u57fa\u4e8eLiDAR\u7684\u6a21\u62df\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86EvalSuite\u4f5c\u4e3a\u516c\u5e73\u8bc4\u4f30\u7684\u57fa\u51c6\u3002"}}
{"id": "2509.12081", "pdf": "https://arxiv.org/pdf/2509.12081", "abs": "https://arxiv.org/abs/2509.12081", "authors": ["Anirudha Majumdar"], "title": "Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "This paper proposes deception as a mechanism for out-of-distribution (OOD)\ngeneralization: by learning data representations that make training data appear\nindependent and identically distributed (iid) to an observer, we can identify\nstable features that eliminate spurious correlations and generalize to unseen\ndomains. We refer to this principle as deceptive risk minimization (DRM) and\ninstantiate it with a practical differentiable objective that simultaneously\nlearns features that eliminate distribution shifts from the perspective of a\ndetector based on conformal martingales while minimizing a task-specific loss.\nIn contrast to domain adaptation or prior invariant representation learning\nmethods, DRM does not require access to test data or a partitioning of training\ndata into a finite number of data-generating domains. We demonstrate the\nefficacy of DRM on numerical experiments with concept shift and a simulated\nimitation learning setting with covariate shift in environments that a robot is\ndeployed in.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6b3a\u9a97\u673a\u5236\u5b9e\u73b0\u5206\u5e03\u5916\u6cdb\u5316\u7684\u65b9\u6cd5\uff08DRM\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u4f7f\u8bad\u7ec3\u6570\u636e\u5bf9\u89c2\u5bdf\u8005\u5448\u73b0\u72ec\u7acb\u540c\u5206\u5e03\u7684\u6570\u636e\u8868\u793a\uff0c\u4ee5\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u6d4b\u8bd5\u6570\u636e\u6216\u5c06\u8bad\u7ec3\u6570\u636e\u5212\u5206\u4e3a\u6709\u9650\u7684\u6570\u636e\u751f\u6210\u57df\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002DRM\u65e8\u5728\u65e0\u9700\u8fd9\u4e9b\u6761\u4ef6\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7279\u5f81\u5b66\u4e60\u548c\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u6b3a\u9a97\u6027\u98ce\u9669\u6700\u5c0f\u5316\uff08DRM\uff09\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u76ee\u6807\u540c\u65f6\u5b66\u4e60\u6d88\u9664\u5206\u5e03\u53d8\u5316\u7684\u7279\u5f81\u5e76\u6700\u5c0f\u5316\u4efb\u52a1\u7279\u5b9a\u635f\u5931\uff0c\u5229\u7528\u57fa\u4e8e\u5171\u5f62\u9785\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u6982\u5ff5\u504f\u79fb\u548c\u534f\u53d8\u91cf\u504f\u79fb\u7684\u6a21\u62df\u73af\u5883\u4e2d\uff0cDRM\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u90e8\u7f72\u73af\u5883\u4e2d\u3002", "conclusion": "DRM\u662f\u4e00\u79cd\u65e0\u9700\u6d4b\u8bd5\u6570\u636e\u6216\u57df\u5212\u5206\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5206\u5e03\u5916\u6cdb\u5316\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
