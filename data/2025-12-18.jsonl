{"id": "2512.14893", "pdf": "https://arxiv.org/pdf/2512.14893", "abs": "https://arxiv.org/abs/2512.14893", "authors": ["Reza Mohammadkhani", "Azad Azizzadeh", "Seyed Vahab Al-Din Makki", "John Thompson", "Maziar Nekovee"], "title": "Compensation of Coarse Quantization Effects on Channel Estimation and BER in Massive MIMO", "categories": ["eess.SP", "eess.SY"], "comment": "12 pages, submitted to IEEE Transactions", "summary": "Low-resolution quantization is essential to reduce implementation cost and power consumption in massive multiple-input multiple-output (MIMO) systems for 5G and 6G. While most existing studies assume perfect channel state information (CSI), we model the impact of coarse quantization noise on both channel estimation and data transmission, yielding a more realistic assessment of system performance under imperfect CSI conditions in the uplink. We develop a tight approximation for the bit-error ratio (BER) of uncoded M-QAM with zero-forcing detection, based on the linear minimum mean-square error (LMMSE) channel estimate. These analytical results enable compensation strategies that jointly optimize quantization resolution, transmit power, and pilot length across different numbers of users and base station antennas. We further demonstrate the applicability of the proposed framework through several design scenarios that highlight its effectiveness in optimizing system parameters and improving energy efficiency under quantization constraints. For example, in a 16-QAM system, extending the pilot sequence by 2.5 times and lowering transmit power by 0.5 dB enables a 3-bit quantized system to match the BER of the full-resolution case. The proposed framework offers a fast and accurate alternative to Monte Carlo simulations, enabling practical system optimization under realistic quantization constraints."}
{"id": "2512.15045", "pdf": "https://arxiv.org/pdf/2512.15045", "abs": "https://arxiv.org/abs/2512.15045", "authors": ["Aparna Parameswaran", "Hoyoung Kim", "Sangkil Kim"], "title": "Janus Metasurface Breaking Polarization Symmetry: Surface-Modulated Electromagnetic Wave Radiation with Coexistent Linear and Circular Polarization", "categories": ["eess.SP"], "comment": null, "summary": "In this work, a Janus metasurface based tensor impedance holographic antenna (JHA) is proposed that simultaneously radiates linearly polarized (LP) and circularly polarized (CP) beams from a single aperture excited by a single feed. The proposed design introduces modified tensor impedance equations to significantly reduce cross-polarization at higher radiation angles. It demonstrates broadband operation bandwidth of 0.5 GHz while maintaining high circular polarization purity. The design methodology is verified using aperture field integration theory, ensuring that the impedance distribution produces the desired far-field radiation patterns. Prototypes of three variations of the holographic antenna are fabricated, validating its performance. The radiation characteristics of the proposed antenna make it an attractive choice for advanced broadband communication applications."}
{"id": "2512.15062", "pdf": "https://arxiv.org/pdf/2512.15062", "abs": "https://arxiv.org/abs/2512.15062", "authors": ["Nadia Abdolkhani", "Nada Abdel Khalek", "Walaa Hamouda", "Iyad Dayoub"], "title": "Deep Reinforcement Learning for Joint Time and Power Management in SWIPT-EH CIoT", "categories": ["eess.SP", "cs.NI"], "comment": "Published in IEEE Communications Letters, 2025. This arXiv version is the authors' accepted manuscript", "summary": "This letter presents a novel deep reinforcement learning (DRL) approach for joint time allocation and power control in a cognitive Internet of Things (CIoT) system with simultaneous wireless information and power transfer (SWIPT). The CIoT transmitter autonomously manages energy harvesting (EH) and transmissions using a learnable time switching factor while optimizing power to enhance throughput and lifetime. The joint optimization is modeled as a Markov decision process under small-scale fading, realistic EH, and interference constraints. We develop a double deep Q-network (DDQN) enhanced with an upper confidence bound. Simulations benchmark our approach, showing superior performance over existing DRL methods."}
{"id": "2512.15105", "pdf": "https://arxiv.org/pdf/2512.15105", "abs": "https://arxiv.org/abs/2512.15105", "authors": ["Jundong Qi", "Weize Sun", "Shaowu Chen", "Lei Huang", "Qiuchen Liu"], "title": "CF-Net: A Cross-Feature Reconstruction Network for High-Accuracy 1-Bit Target Classification", "categories": ["eess.SP"], "comment": "14 pages, 10 figures. Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Target classification is a fundamental task in radar systems, and its performance critically depends on the quantization precision of the signal. While high-precision quantization (e.g. 16-bit) is well established, 1-bit quantization offers distinct advantages by enabling direct sampling at high frequencies and eliminating complex intermediate stages. However, its extreme quantization leads to significant information loss. Although higher sampling rates can compensate for this loss, such oversampling is impractical at the high frequencies targeted for direct sampling. To achieve high-accuracy classification directly from 1-bit radar data under the same sampling rate, this paper proposes a novel two-stage deep learning framework, CF-Net. First, we introduce a self-supervised pre-training strategy based on a dual-branch U-Net architecture. This network learns to restore high-fidelity 16-bit images from their 1-bit counterparts via a cross-feature reconstruction task, forcing the 1-bit encoder to learn robust features despite extreme quantization. Subsequently, this pre-trained encoder is repurposed and fine-tuned for the downstream multi-class target classification task. Experiments on two radar target datasets demonstrate that CF-Net can effectively extract discriminative features from 1-bit imagery, achieving comparable and even superior accuracy to some 16-bit methods without oversampling."}
{"id": "2512.14952", "pdf": "https://arxiv.org/pdf/2512.14952", "abs": "https://arxiv.org/abs/2512.14952", "authors": ["Iddo Yehoshua Wald", "Amber Maimon", "Shiyao Zhang", "Dennis Küster", "Robert Porzel", "Tanja Schultz", "Rainer Malaka"], "title": "Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to appear in the ACM/IEEE International Conference on Human-Robot Interaction (HRI '26), Edinburgh, United Kingdom. Iddo Yehoshua Wald and Amber Maimon contributed equally", "summary": "Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy."}
{"id": "2512.15109", "pdf": "https://arxiv.org/pdf/2512.15109", "abs": "https://arxiv.org/abs/2512.15109", "authors": ["Zhuoran Li", "Zhen Gao", "Xinhua Liu", "Zheng Wang", "Xiaotian Zhou", "Lei Liu", "Yongpeng Wu", "Wei Feng", "Yongming Huang"], "title": "Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network", "categories": ["eess.SP"], "comment": null, "summary": "The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems."}
{"id": "2512.15020", "pdf": "https://arxiv.org/pdf/2512.15020", "abs": "https://arxiv.org/abs/2512.15020", "authors": ["Wenlong Xia", "Jinhao Zhang", "Ce Zhang", "Yaojia Wang", "Youmin Gong", "Jie Mei"], "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision", "categories": ["cs.RO"], "comment": null, "summary": "Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \\emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released."}
{"id": "2512.15119", "pdf": "https://arxiv.org/pdf/2512.15119", "abs": "https://arxiv.org/abs/2512.15119", "authors": ["Jiayang Wan", "Ke He", "Yafei Wang", "Fan Liu", "Wenjin Wang", "Shi Jin"], "title": "QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management", "categories": ["eess.SP", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction."}
{"id": "2512.15047", "pdf": "https://arxiv.org/pdf/2512.15047", "abs": "https://arxiv.org/abs/2512.15047", "authors": ["Yunheng Wang", "Yixiao Feng", "Yuetong Fang", "Shuning Zhang", "Tan Jing", "Jian Li", "Xiangrui Jiang", "Renjing Xu"], "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability."}
{"id": "2512.15246", "pdf": "https://arxiv.org/pdf/2512.15246", "abs": "https://arxiv.org/abs/2512.15246", "authors": ["Nguyen Thanh Vinh", "Manoj Vishwanath", "Thinh Nguyen-Quang", "Nguyen Viet Ha", "Bui Thanh Tung", "Huy-Dung Han", "Nguyen Quang Linh", "Nguyen Hai Linh", "Hung Cao"], "title": "Enhancing Alzheimer's Detection through Late Fusion of Multi-Modal EEG Features", "categories": ["eess.SP"], "comment": null, "summary": "Alzheimer s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline, where early detection is essential for timely intervention and improved patient outcomes. Traditional diagnostic methods are time-consuming and require expert interpretation, thus, automated approaches are highly desirable. This study presents a novel deep learning framework for AD diagnosis using Electroencephalograph (EEG) signals, integrating multiple feature extraction techniques including alpha-wave analysis, Discrete Wavelet Transform (DWT), and Markov Transition Fields (MTF). A late-fusion strategy is employed to combine predictions from separate neural networks trained on these diverse representations, capturing both temporal and frequency-domain patterns in the EEG data. The proposed model attains a classification accuracy of 87.23%, with a precision of 87.95%, a recall of 86.91%, and an F1 score of 87.42% when evaluated on a publicly available dataset, demonstrating its potential for reliable, scalable, and early AD screening. Rigorous preprocessing and targeted frequency band selection, particularly in the alpha range due to its cognitive relevance, further enhance performance. This work highlights the promise of deep learning in supporting physicians with efficient and accessible tools for early AD diagnosis."}
{"id": "2512.15080", "pdf": "https://arxiv.org/pdf/2512.15080", "abs": "https://arxiv.org/abs/2512.15080", "authors": ["Gaurav Bansal"], "title": "NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles", "categories": ["cs.RO"], "comment": "10 pages, 5 figures, 2 tables", "summary": "Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.\n  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.\n  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable."}
{"id": "2512.15268", "pdf": "https://arxiv.org/pdf/2512.15268", "abs": "https://arxiv.org/abs/2512.15268", "authors": ["Joachim Tapparel", "Andreas Burg"], "title": "Dataset and UAV Propagation Channel Modeling for LoRa in the 860 MHz ISM Band", "categories": ["eess.SP"], "comment": "Accepted for publication in ACSSC", "summary": "LoRa is one of the most widely used low-power wide-area network technology for the Internet of Things. To achieve long-range communication with low power consumption at a low cost, LoRa uses a chirp spread spectrum modulation and transmits in the sub-GHz unlicensed industrial, scientific, and medical (ISM) frequency bands. Due to the rapid densification of IoT networks, it is crucial to obtain tailored channel models to evaluate the performance of LoRa networks. While channel models for cellular technologies have been investigated extensively, specific characteristics of LoRa transmissions operating at long range with a rather small (~ 250kHz) bandwidth require dedicated measurement campaigns and modeling efforts. In this work, we leverage an SDR-based testbed to gather and publish a dataset of LoRa frames transmitted in a campus environment. The dataset includes IQ samples of the received frames at multiple locations and allows for the evaluation of channel variations with high time resolution. Using the gathered data, we derive empirical propagation channel models for LoRa that include receiver correlation over distance for three scenarios: unmanned aerial vehicle (UAV) line-of-sight (LoS), UAV non-LoS, and pedestrian non-LoS. Furthermore, the dataset is annotated with synchronization information, enabling the evaluation of receiver algorithms using experimental data."}
{"id": "2512.15111", "pdf": "https://arxiv.org/pdf/2512.15111", "abs": "https://arxiv.org/abs/2512.15111", "authors": ["Dongmyeong Lee", "Jesse Quattrociocchi", "Christian Ellis", "Rwik Rana", "Amanda Adkins", "Adam Uccello", "Garrett Warnell", "Joydeep Biswas"], "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment."}
{"id": "2512.15279", "pdf": "https://arxiv.org/pdf/2512.15279", "abs": "https://arxiv.org/abs/2512.15279", "authors": ["Le Hao", "Robin Neuder", "Mohamadreza Delbari", "Alejandro Jiménez-Sáez", "Vahid Jamali", "Arash Asadi", "Andrea Ortiz"], "title": "Learning-Based Phase Shift Optimization of Liquid Crystal RIS in Dynamic mmWave Networks", "categories": ["eess.SP"], "comment": null, "summary": "To enhance coverage and signal quality in millimeter-wave (mmWave) frequencies, reconfigurable intelligent surfaces (RISs) have emerged as a game-changing solution to manipulate the wireless environment. Traditional semiconductor-based RISs face scalability issues due to high power consumption. Meanwhile, liquid crystal-based RISs (LC-RISs) offer energy-efficient and cost-effective operation even for large arrays. However, this promise has a caveat. LC-RISs suffer from long reconfiguration times, on the order of tens of milliseconds, which limits their applicability in dynamic scenarios. To date, prior works have focused on hardware design aspects or static scenarios to address this limitation, but little attention has been paid to optimization solutions for dynamic settings. Our paper fills this gap by proposing a reinforcement learning-based optimization framework to dynamically control the phase shifts of LC-RISs and maximize the data rate of a moving user. Specifically, we propose a Deep Deterministic Policy Gradient (DDPG) algorithm that adapts the LC-RIS phase shifts without requiring perfect channel state information and balances the tradeoff between signal-to-noise ratio (SNR) and configuration time. We validate our approach through high-fidelity ray tracing simulations, leveraging measurement data from an LC-RIS prototype. Our results demonstrate the potential of our solution to bring adaptive control to dynamic LC-RIS-assisted mmWave systems."}
{"id": "2512.15195", "pdf": "https://arxiv.org/pdf/2512.15195", "abs": "https://arxiv.org/abs/2512.15195", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Stephan Amann", "Lukas Marc Listl", "Oliver Bringmann"], "title": "EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "Submitted at IEEE IV 2026", "summary": "Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving."}
{"id": "2512.15283", "pdf": "https://arxiv.org/pdf/2512.15283", "abs": "https://arxiv.org/abs/2512.15283", "authors": ["Colin Cros", "Laurent Ferro-Famil"], "title": "Moment-Matching Array Processing Technique for diffuse source estimation", "categories": ["eess.SP"], "comment": null, "summary": "Direction of Arrival (DOA) estimation is a fundamental problem in signal processing. Diffuse sources, whose power density cannot be represented with a single angular coordinate, are usually characterized based on prior assumptions, which associate the source angular density with a specific set of functions. However, these assumptions can lead to significant estimation biases when they are incorrect. This paper introduces the Moment-Matching Estimation Technique (MoMET), a low-complexity method for estimating the mean DOA, spread, and power of a narrow diffuse source without requiring prior knowledge on the source distribution. The unknown source density is characterized by its mean DOA and its first central moments, which are estimated through covariance matching techniques which fit the empirical covariance of the measurements to that modeled from the moments. The MoMET parameterization is robust to incorrect model assumptions, and numerically efficient. The asymptotic bias and covariance of the new estimator are derived and its performance is demonstrated through simulations."}
{"id": "2512.15215", "pdf": "https://arxiv.org/pdf/2512.15215", "abs": "https://arxiv.org/abs/2512.15215", "authors": ["Erik Brorsson", "Kristian Ceder", "Ze Zhang", "Sabino Francesco Roselli", "Endre Erős", "Martin Dahl", "Beatrice Alenljung", "Jessica Lindblom", "Thanh Bui", "Emmanuel Dean", "Lennart Svensson", "Kristofer Bengtsson", "Per-Lage Götvall", "Knut Åkesson"], "title": "Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives", "categories": ["cs.RO"], "comment": null, "summary": "The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments."}
{"id": "2512.15290", "pdf": "https://arxiv.org/pdf/2512.15290", "abs": "https://arxiv.org/abs/2512.15290", "authors": ["Jie Zhou", "Junhao Xie"], "title": "On the Asymptotic Performance of Diagonally Loaded Detectors for Large Arrays: To Achieve CFAR and Optimality", "categories": ["eess.SP"], "comment": null, "summary": "This paper addresses two critical limitations in diagonally loaded (DL) adaptive matched filter (AMF) detector: (1) the lack of CFAR property with respect to arbitrary covariance matrices, and (2) the absence of selection criteria for optimal loading factor from the perspective of maximizing the detection probability (Pd). We provide solutions to both challenges through a comprehensive analysis for the asymptotic performance of DL-AMF under large dimensional regime (LDR) where the dimension N and sample size K tend to infinity whereas their ratio N/K converges to a constant c\\in(0,1). The analytical results show that any DL detectors constructed by normalizing the random variable |a|2=|sH(R+λIN)-1y0|2 with a deterministic quantity or a random variable that converges almost surely to a deterministic value will exhibit equivalent performance under LDR. Following this idea, we derive two CFAR DL detectors: CFAR DL semi-clairvoyant matched filter (CFAR-DL-SCMF) detector and CFAR DL adaptive matched filter (CFAR-DL-AMF) detector, by normalizing |a|2 with an appropriate deterministic quantity and its consistent estimate, respectively. The theoretical analysis and simulations show that both CFAR-DL-SCMF and CFAR-DL-AMF achieve CFAR with respect to covariance matrix, target steering vector and loading factor. Furthermore, we derive the asymptotically optimal loading factor λ_opt by maximizing the explicit expression of asymptotic Pd. For practical implementation, we provide a consistent estimator for λ_opt under LDR. Based on λ_opt and its consistent estimate, we establish the optimal CFAR-DL-SCMF (opt-CFAR-DL-SCMF) and the optimal CFAR-DL-AMF (opt-CFAR-DL-AMF). Numerical examples demonstrate that the proposed opt-CFAR-DL-SCMF and opt-CFAR-DL-AMF consistently outperform EL-AMF and persymmetric AMF in both full-rank and low-rank clutter plus noise environments."}
{"id": "2512.15258", "pdf": "https://arxiv.org/pdf/2512.15258", "abs": "https://arxiv.org/abs/2512.15258", "authors": ["Yuze Wu", "Mo Zhu", "Xingxing Li", "Yuheng Du", "Yuxin Fan", "Wenjun Li", "Xin Zhou", "Fei Gao"], "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots."}
{"id": "2512.15441", "pdf": "https://arxiv.org/pdf/2512.15441", "abs": "https://arxiv.org/abs/2512.15441", "authors": ["Gilderlan Tavares de Araújo", "André L. F. de Almeida Buno Sokal", "Gabor Fodor", "Paulo R. B. Gomes"], "title": "Semi-Blind Joint Channel and Symbol Estimation for Beyond Diagonal Reconfigurable Surfaces", "categories": ["eess.SP"], "comment": null, "summary": "The beyond-diagonal reconfigurable intelligent surface (BD-RIS) is a recent architecture in which scattering elements are interconnected to enhance the degrees of freedom for wave control, yielding performance gains over traditional single-connected RISs. For BD-RIS, channel estimation - well-studied for conventional RIS - becomes more challenging due to the complex connections and a larger number of coefficients. Prior works rely on pilot-assisted estimation followed by data decoding. This paper introduces a semi-blind tensor-based approach for joint channel and symbol estimation that eliminates the need for training sequences by leveraging data symbols directly. A practical scenario with time-varying user terminal-RIS channels under mobility is considered. By reformulating the received signal from a tensor decomposition perspective, we develop two semi-blind receivers: a two-stage method transforming the fourth-order PARATUCK model into a third-order PARAFAC model, and a single-stage iterative process based on fourth-order TUCKER decomposition. Identifiability conditions for reliable joint recovery are derived, and numerical results demonstrate the performance advantages and trade-offs of the proposed schemes over existing solutions."}
{"id": "2512.15282", "pdf": "https://arxiv.org/pdf/2512.15282", "abs": "https://arxiv.org/abs/2512.15282", "authors": ["Martijn IJtsma", "Salvatore Hargis"], "title": "A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies", "categories": ["cs.RO", "cs.HC"], "comment": "Under review at IEEE Transactions on Human-Machine Systems. 12 pages, 5 figures", "summary": "Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation."}
{"id": "2512.15546", "pdf": "https://arxiv.org/pdf/2512.15546", "abs": "https://arxiv.org/abs/2512.15546", "authors": ["Heedong Do", "Angel Lozano"], "title": "Optimum Discrete Beamforming via Minkowski Sum of Polygons", "categories": ["eess.SP"], "comment": null, "summary": "This letter casts the problem of optimum discrete beamforming as the computation of the Minkowski sum of convex polygons, which is itself a convex polygon. The number of vertices of the latter is at most the sum of the number of vertices of the original polygons, enabling its efficient computation. This original and intuitive formulation confirms that the optimum beamforming solution can be found efficiently."}
{"id": "2512.15309", "pdf": "https://arxiv.org/pdf/2512.15309", "abs": "https://arxiv.org/abs/2512.15309", "authors": ["Kai Zhang", "Shoubin Chen", "Dong Li", "Baiyang Zhang", "Tao Huang", "Zehao Wu", "Jiasheng Chen", "Bo Zhang"], "title": "GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments", "categories": ["cs.RO"], "comment": "6 pages, published in ICUS2025", "summary": "Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments."}
{"id": "2512.15558", "pdf": "https://arxiv.org/pdf/2512.15558", "abs": "https://arxiv.org/abs/2512.15558", "authors": ["Nadia Abdolkhani", "Nada Abdel Khalek", "Walaa Hamouda"], "title": "Deep Reinforcement Learning for EH-Enabled Cognitive-IoT Under Jamming Attacks", "categories": ["eess.SP", "cs.NI"], "comment": "Published in IEEE Internet of Things Journal. This arXiv version is the authors' accepted manuscript", "summary": "In the evolving landscape of the Internet of Things (IoT), integrating cognitive radio (CR) has become a practical solution to address the challenge of spectrum scarcity, leading to the development of cognitive IoT (CIoT). However, the vulnerability of radio communications makes radio jamming attacks a key concern in CIoT networks. In this paper, we introduce a novel deep reinforcement learning (DRL) approach designed to optimize throughput and extend network lifetime of an energy-constrained CIoT system under jamming attacks. This DRL framework equips a CIoT device with the autonomy to manage energy harvesting (EH) and data transmission, while also regulating its transmit power to respect spectrum-sharing constraints. We formulate the optimization problem under various constraints, and we model the CIoT device's interactions within the channel as a model-free Markov decision process (MDP). The MDP serves as a foundation to develop a double deep Q-network (DDQN), designed to help the CIoT agent learn the optimal communication policy to navigate challenges such as dynamic channel occupancy, jamming attacks, and channel fading while achieving its goal. Additionally, we introduce a variant of the upper confidence bound (UCB) algorithm, named UCB-IA, which enhances the CIoT network's ability to efficiently navigate jamming attacks within the channel. The proposed DRL algorithm does not rely on prior knowledge and uses locally observable information such as channel occupancy, jamming activity, channel gain, and energy arrival to make decisions. Extensive simulations prove that our proposed DRL algorithm that utilizes the UCB-IA strategy surpasses existing benchmarks, allowing for a more adaptive, energy-efficient, and secure spectrum sharing in CIoT networks."}
{"id": "2512.15379", "pdf": "https://arxiv.org/pdf/2512.15379", "abs": "https://arxiv.org/abs/2512.15379", "authors": ["Michael Amir", "Manon Flageat", "Amanda Prorok"], "title": "Remotely Detectable Robot Policy Watermarking", "categories": ["cs.RO", "cs.CR", "cs.LG", "eess.SY"], "comment": null, "summary": "The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \\textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations."}
{"id": "2512.15229", "pdf": "https://arxiv.org/pdf/2512.15229", "abs": "https://arxiv.org/abs/2512.15229", "authors": ["Elio Gruttadauria", "Mathieu Fontaine", "Jonathan Le Roux", "Slim Essid"], "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization", "categories": ["cs.LG", "cs.SD", "eess.SP"], "comment": null, "summary": "We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient."}
{"id": "2512.15411", "pdf": "https://arxiv.org/pdf/2512.15411", "abs": "https://arxiv.org/abs/2512.15411", "authors": ["Zhenhan Yin", "Xuanhan Wang", "Jiahao Jiang", "Kaiyuan Deng", "Pengqi Chen", "Shuangle Li", "Chong Liu", "Xing Xu", "ingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks."}
{"id": "2512.15232", "pdf": "https://arxiv.org/pdf/2512.15232", "abs": "https://arxiv.org/abs/2512.15232", "authors": ["Guillaume Koechlin", "Filippo Bovera", "Elena Degli Innocenti", "Barbara Santini", "Alessandro Venturi", "Simona Vazio", "Piercesare Secchi"], "title": "A Blind Source Separation Framework to Monitor Sectoral Power Demand from Grid-Scale Load Measurements", "categories": ["stat.AP", "eess.SP"], "comment": null, "summary": "As we are moving towards decentralized power systems dominated by intermittent electricity generation from renewable energy sources, demand-side flexibility is becoming a critical issue. In this context, it is essential to understand the composition of electricity demand at various scales of the power grid. At the regional or national scale, there is however little visibility on the relative contributions of different consumer categories, due to the complexity and costs of collecting end-users consumption data. To address this issue, we propose a blind source separation framework based on a constrained variant of non-negative matrix factorization to monitor the consumption of residential, services and industrial sectors at high frequency from aggregate high-voltage grid load measurements. Applying the method to Italy's national load curve between 2021 and 2023, we reconstruct accurate hourly consumption profiles for each sector. Results reveal that both households and services daily consumption behaviors are driven by two distinct regimes related to the season and day type whereas industrial demand follows a single, stable daily profile. Besides, the monthly consumption estimates of each sector derived from the disaggregated load are found to closely align with sample-based indices and be more precise than forecasting approaches based on these indices for real-time monitoring."}
{"id": "2512.15448", "pdf": "https://arxiv.org/pdf/2512.15448", "abs": "https://arxiv.org/abs/2512.15448", "authors": ["Sinan Emre", "Victor Barasuol", "Matteo Villa", "Claudio Semini"], "title": "Load-Based Variable Transmission Mechanism for Robotic Applications", "categories": ["cs.RO"], "comment": "22nd International Conference on Advanced Robotics (ICAR 2025)", "summary": "This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical."}
{"id": "2512.15344", "pdf": "https://arxiv.org/pdf/2512.15344", "abs": "https://arxiv.org/abs/2512.15344", "authors": ["Hiroyoshi Nagahama", "Katsufumi Inoue", "Masayoshi Todorokihara", "Michifumi Yoshioka"], "title": "Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\\% accuracy (+5.4\\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems."}
{"id": "2512.15557", "pdf": "https://arxiv.org/pdf/2512.15557", "abs": "https://arxiv.org/abs/2512.15557", "authors": ["Evgenii Kruzhkov", "Raphael Memmesheimer", "Sven Behnke"], "title": "OMCL: Open-vocabulary Monte Carlo Localization", "categories": ["cs.RO"], "comment": "Accepted to IEEE RA-L", "summary": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes."}
{"id": "2512.15385", "pdf": "https://arxiv.org/pdf/2512.15385", "abs": "https://arxiv.org/abs/2512.15385", "authors": ["Julian Oelhaf", "Mehran Pashaei", "Georg Kordowich", "Christian Bergler", "Andreas Maier", "Johann Jäger", "Siming Bayer"], "title": "Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection", "categories": ["cs.LG", "eess.SP"], "comment": "This paper is a postprint of a paper submitted to and accepted for publication in the 20th IET International Conference on Developments in Power System Protection (DPSP Global 2026) and is subject to Institution of Engineering and Technology Copyright. The copy of record is available at the IET Digital Library", "summary": "The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.\n  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems."}
{"id": "2512.15597", "pdf": "https://arxiv.org/pdf/2512.15597", "abs": "https://arxiv.org/abs/2512.15597", "authors": ["Giacomo Picardi", "Saverio Iacoponi", "Matias Carandell", "Jorge Aguirregomezcorta", "Mrudul Chellapurath", "Joaquin del Rio", "Marcello Calisti", "Iacopo Aguzzi"], "title": "An Open Toolkit for Underwater Field Robotics", "categories": ["cs.RO"], "comment": "10 pages, 8 figures", "summary": "Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.\n  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.\n  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.\n  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics."}
{"id": "2512.15399", "pdf": "https://arxiv.org/pdf/2512.15399", "abs": "https://arxiv.org/abs/2512.15399", "authors": ["Phillip Stephan", "Florian Euchner", "Stephan ten Brink"], "title": "Three-Dimensional Radio Localization: A Channel Charting-Based Approach", "categories": ["cs.IT", "eess.SP"], "comment": null, "summary": "Channel charting creates a low-dimensional representation of the radio environment in a self-supervised manner using manifold learning. Preserving relative spatial distances in the latent space, channel charting is well suited to support user localization. While prior work on channel charting has mainly focused on two-dimensional scenarios, real-world environments are inherently three-dimensional. In this work, we investigate two distinct three-dimensional indoor localization scenarios using simulated, but realistic ray tracing-based datasets: a factory hall with a three-dimensional spatial distribution of datapoints, and a multistory building where each floor exhibits a two-dimensional datapoint distribution. For the first scenario, we apply the concept of augmented channel charting, which combines classical localization and channel charting, to a three-dimensional setting. For the second scenario, we introduce multistory channel charting, a two-stage approach consisting of floor classification via clustering followed by the training of a dedicated expert neural network for channel charting on each individual floor, thereby enhancing the channel charting performance. In addition, we propose a novel feature engineering method designed to extract sparse features from the beamspace channel state information that are suitable for localization."}
{"id": "2512.15692", "pdf": "https://arxiv.org/pdf/2512.15692", "abs": "https://arxiv.org/abs/2512.15692", "authors": ["Jonas Pai", "Liam Achenbach", "Victoriano Montesinos", "Benedek Forrai", "Oier Mees", "Elvis Nava"], "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures."}
{"id": "2512.15562", "pdf": "https://arxiv.org/pdf/2512.15562", "abs": "https://arxiv.org/abs/2512.15562", "authors": ["Xingyu Zhou", "Le Liang", "Hao Ye", "Jing Zhang", "Chao-Kai Wen", "Shi Jin"], "title": "Reducing Pilots in Channel Estimation With Predictive Foundation Models", "categories": ["cs.IT", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Accurate channel state information (CSI) acquisition is essential for modern wireless systems, which becomes increasingly difficult under large antenna arrays, strict pilot overhead constraints, and diverse deployment environments. Existing artificial intelligence-based solutions often lack robustness and fail to generalize across scenarios. To address this limitation, this paper introduces a predictive-foundation-model-based channel estimation framework that enables accurate, low-overhead, and generalizable CSI acquisition. The proposed framework employs a predictive foundation model trained on large-scale cross-domain CSI data to extract universal channel representations and provide predictive priors with strong cross-scenario transferability. A pilot processing network based on a vision transformer architecture is further designed to capture spatial, temporal, and frequency correlations from pilot observations. An efficient fusion mechanism integrates predictive priors with real-time measurements, enabling reliable CSI reconstruction even under sparse or noisy conditions. Extensive evaluations across diverse configurations demonstrate that the proposed estimator significantly outperforms both classical and data-driven baselines in accuracy, robustness, and generalization capability."}
{"id": "2512.14757", "pdf": "https://arxiv.org/pdf/2512.14757", "abs": "https://arxiv.org/abs/2512.14757", "authors": ["Tomohito Kawabata", "Xinyu Zhang", "Ling Xiao"], "title": "SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance."}
{"id": "2512.15618", "pdf": "https://arxiv.org/pdf/2512.15618", "abs": "https://arxiv.org/abs/2512.15618", "authors": ["Morgan Coe", "Gruffudd Jones", "Leah-Nani Alconcel", "Marina Gashinova"], "title": "Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented."}
{"id": "2512.15117", "pdf": "https://arxiv.org/pdf/2512.15117", "abs": "https://arxiv.org/abs/2512.15117", "authors": ["Pilyoung Kim", "Yun Xie", "Sujin Yang"], "title": "I am here for you\": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable", "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": null, "summary": "General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI."}
{"id": "2512.15691", "pdf": "https://arxiv.org/pdf/2512.15691", "abs": "https://arxiv.org/abs/2512.15691", "authors": ["Matin Mortaheb", "Erciyes Karakaya", "Sennur Ulukus"], "title": "Multi-Modal Semantic Communication", "categories": ["cs.LG", "cs.IT", "eess.SP", "eess.SY"], "comment": null, "summary": "Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments."}
{"id": "2512.15181", "pdf": "https://arxiv.org/pdf/2512.15181", "abs": "https://arxiv.org/abs/2512.15181", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Stephan Amann", "Oliver Bringmann"], "title": "Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at IEEE ICVES 2025", "summary": "Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles."}
{"id": "2512.15423", "pdf": "https://arxiv.org/pdf/2512.15423", "abs": "https://arxiv.org/abs/2512.15423", "authors": ["Hoang Nguyen", "Xiaohao Xu", "Xiaonan Huang"], "title": "Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction."}
{"id": "2512.15476", "pdf": "https://arxiv.org/pdf/2512.15476", "abs": "https://arxiv.org/abs/2512.15476", "authors": ["Pranav Vaidhyanathan", "Aristotelis Papatheodorou", "David R. M. Arvidsson-Shukur", "Mark T. Mitchison", "Natalia Ares", "Ioannis Havoutis"], "title": "QuantGraph: A Receding-Horizon Quantum Graph Solver", "categories": ["quant-ph", "cs.RO", "eess.SY", "physics.comp-ph"], "comment": "P.Vaidhyanathan and A. Papatheodorou contributed equally to this work. 11 pages, 4 figures, 1 table, 2 algorithms", "summary": "Dynamic programming is a cornerstone of graph-based optimization. While effective, it scales unfavorably with problem size. In this work, we present QuantGraph, a two-stage quantum-enhanced framework that casts local and global graph-optimization problems as quantum searches over discrete trajectory spaces. The solver is designed to operate efficiently by first finding a sequence of locally optimal transitions in the graph (local stage), without considering full trajectories. The accumulated cost of these transitions acts as a threshold that prunes the search space (up to 60% reduction for certain examples). The subsequent global stage, based on this threshold, refines the solution. Both stages utilize variants of the Grover-adaptive-search algorithm. To achieve scalability and robustness, we draw on principles from control theory and embed QuantGraph's global stage within a receding-horizon model-predictive-control scheme. This classical layer stabilizes and guides the quantum search, improving precision and reducing computational burden. In practice, the resulting closed-loop system exhibits robust behavior and lower overall complexity. Notably, for a fixed query budget, QuantGraph attains a 2x increase in control-discretization precision while still benefiting from Grover-search's inherent quadratic speedup compared to classical methods."}
