{"id": "2509.12348", "pdf": "https://arxiv.org/pdf/2509.12348", "abs": "https://arxiv.org/abs/2509.12348", "authors": ["Hua Chen", "Tao Gong", "Tuo Wu", "Maged Elkashlan", "Baiyang Liu", "Chan-Byoung Chae", "Kin-Fai Tong", "Kai-Kit Wong"], "title": "FAS-ARIS: Turning Multipath Challenges Into Localization Opportunities", "categories": ["eess.SP"], "comment": "13 pages", "summary": "Traditional single-input single-output (SISO) systems face fundamental\nlimitations in achieving accurate three-dimensional (3D) localization due to\nlimited spatial degrees of freedom (DoF) and the adverse impact of multipath\npropagation. This paper proposes a novel fluid antenna system (FAS)-active\nreconfigurable intelligent surface (ARIS) framework that transforms multipath\neffects from a hindrance into a resource for enhanced localization. By\nsynergistically combining the signal amplification capabilities of ARIS with\nthe spatial diversity enabled by FAS, the proposed system achieves robust 3D\nuser equipment (UE) positioning -- without relying on auxiliary information\nsuch as time-of-arrival (ToA) or frequency diversity. The system exploits both\nline-of-sight (LoS) and non-line-of-sight (NLoS) components through a tailored\nsignal decoupling strategy. We design novel UE pilot sequences and ARIS phase\nconfigurations to effectively separate LoS and NLoS channels, enabling\nindependent parameter estimation. A multi-stage estimation algorithm is then\napplied: the multiple signal classification (MUSIC) algorithm estimates\nangle-of-arrival (AoA) from the direct path, while maximum likelihood\nestimation with interior-point refinement recovers cascaded channel parameters\nfrom the reflected path. Finally, geometric triangulation using least-squares\nestimation determines the UE's 3D position based on the extracted AoA\ninformation. Comprehensive performance analysis, including the derivation of\nCram\\'{e}r-Rao bounds for both channel and position estimation, establishes\ntheoretical benchmarks. Simulation results confirm that the proposed FAS-ARIS\nframework achieves near-optimal localization accuracy while maintaining\nrobustness in rich multipath environments -- effectively turning conventional\nlocalization challenges into advantages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684FAS-ARIS\u6846\u67b6\uff0c\u5c06\u591a\u5f84\u6548\u5e94\u8f6c\u5316\u4e3a\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76843D\u5b9a\u4f4d\u3002", "motivation": "\u4f20\u7edfSISO\u7cfb\u7edf\u5728\u591a\u5f84\u73af\u5883\u4e0b\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u76843D\u5b9a\u4f4d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u5c06\u591a\u5f84\u6548\u5e94\u4ece\u963b\u788d\u8f6c\u53d8\u4e3a\u4f18\u52bf\u3002", "method": "\u7ed3\u5408ARIS\u7684\u4fe1\u53f7\u653e\u5927\u80fd\u529b\u548cFAS\u7684\u7a7a\u95f4\u5206\u96c6\uff0c\u8bbe\u8ba1\u4e13\u7528\u4fe1\u53f7\u89e3\u8026\u7b56\u7565\u548c\u591a\u9636\u6bb5\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5305\u62ecMUSIC\u7b97\u6cd5\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFAS-ARIS\u6846\u67b6\u5728\u591a\u5f84\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u5f84\u6548\u5e94\uff0cFAS-ARIS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf3D\u5b9a\u4f4d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.12359", "pdf": "https://arxiv.org/pdf/2509.12359", "abs": "https://arxiv.org/abs/2509.12359", "authors": ["Henry Carvajal Mora", "Nathaly Orozco", "Fernando Almeida Garc\u00eda", "Jos\u00e9 Vega-S\u00e1nchez", "Felipe Grijalva", "Edgar Benitez Olivo"], "title": "Partial Secrecy Analysis in Wireless Systems: Diversity-Enhanced PLS over Generalized Fading Channels", "categories": ["eess.SP"], "comment": null, "summary": "Securing information in future mobile networks is challenging, especially for\ndevices with limited computational resources. Physical layer security (PLS)\noffers a viable solution by leveraging wireless channel randomness. When full\nsecrecy is unattainable, the partial secrecy regime provides a realistic\nalternative. This work analyzes partial secrecy performance under the\ngeneralized multicluster fluctuating two-ray (MFTR) fading model, which\nsubsumes many classical fading cases. We study a system with a transmitter (A),\nlegitimate receiver (B), and eavesdropper (E), both B and E using antenna\narrays with maximal ratio combining (MRC), under i.n.i.d. fading. Exact and\nclosed-form approximations are derived for key secrecy metrics: generalized\nsecrecy outage probability (GSOP), average fractional equivocation (AFE), and\naverage information leakage rate (AILR). The results, validated by Monte Carlo\nsimulations, retain constant complexity regardless of diversity order. The MFTR\nmodel's flexibility enables comprehensive assessment across fading conditions,\nshowing that more MRC branches at B enhance secrecy performance depending on\nthe A-E link characteristics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u672a\u6765\u79fb\u52a8\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u7269\u7406\u5c42\u5b89\u5168\uff08PLS\uff09\u548c\u90e8\u5206\u4fdd\u5bc6\u673a\u5236\u89e3\u51b3\u8d44\u6e90\u6709\u9650\u8bbe\u5907\u7684\u4fe1\u606f\u5b89\u5168\u95ee\u9898\u3002\u8bba\u6587\u5206\u6790\u4e86\u5e7f\u4e49\u591a\u7c07\u6ce2\u52a8\u53cc\u5c04\u7ebf\uff08MFTR\uff09\u8870\u843d\u6a21\u578b\u4e0b\u7684\u90e8\u5206\u4fdd\u5bc6\u6027\u80fd\uff0c\u5e76\u63a8\u5bfc\u4e86\u5173\u952e\u4fdd\u5bc6\u6307\u6807\u7684\u89e3\u6790\u89e3\u3002", "motivation": "\u672a\u6765\u7684\u79fb\u52a8\u7f51\u7edc\u4e2d\uff0c\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u5728\u4fdd\u969c\u4fe1\u606f\u5b89\u5168\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u7269\u7406\u5c42\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u5229\u7528\u65e0\u7ebf\u4fe1\u9053\u968f\u673a\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u4fdd\u5bc6\u65f6\uff0c\u90e8\u5206\u4fdd\u5bc6\u6210\u4e3a\u4e00\u79cd\u73b0\u5b9e\u9009\u62e9\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86\u53d1\u5c04\u5668\u3001\u5408\u6cd5\u63a5\u6536\u8005\u548c\u7a83\u542c\u8005\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528MFTR\u8870\u843d\u6a21\u578b\u548c\u6700\u5927\u6bd4\u5408\u5e76\uff08MRC\uff09\u6280\u672f\u3002\u901a\u8fc7\u63a8\u5bfc\u5e7f\u4e49\u4fdd\u5bc6\u4e2d\u65ad\u6982\u7387\uff08GSOP\uff09\u3001\u5e73\u5747\u90e8\u5206\u6df7\u6dc6\uff08AFE\uff09\u548c\u5e73\u5747\u4fe1\u606f\u6cc4\u6f0f\u7387\uff08AILR\uff09\u7684\u7cbe\u786e\u548c\u8fd1\u4f3c\u5c01\u95ed\u5f0f\u89e3\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cMFTR\u6a21\u578b\u7684\u7075\u6d3b\u6027\u4f7f\u5176\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u4e0d\u540c\u8870\u843d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5408\u6cd5\u63a5\u6536\u8005\u7684\u66f4\u591aMRC\u5206\u652f\u589e\u5f3a\u4e86\u4fdd\u5bc6\u6027\u80fd\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u53d1\u5c04\u5668\u4e0e\u7a83\u542c\u8005\u94fe\u8def\u7684\u7279\u6027\u3002", "conclusion": "\u8bba\u6587\u9a8c\u8bc1\u4e86\u90e8\u5206\u4fdd\u5bc6\u673a\u5236\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u4e86MFTR\u6a21\u578b\u53ca\u5176\u63a8\u5bfc\u7684\u4fdd\u5bc6\u6307\u6807\u5728\u8bc4\u4f30\u590d\u6742\u8870\u843d\u6761\u4ef6\u4e0b\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.12510", "pdf": "https://arxiv.org/pdf/2509.12510", "abs": "https://arxiv.org/abs/2509.12510", "authors": ["Wei Shao", "Ruoyu Zhang", "Zequan Liang", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device", "categories": ["eess.SP"], "comment": "In the proceedings of IEEE-EMBS BSN 2025", "summary": "Wearable photoplethysmography (PPG) is embedded in billions of devices, yet\nits optical waveform is easily corrupted by motion, perfusion loss, and ambient\nlight, jeopardizing downstream cardiometric analytics. Existing signal-quality\nassessment (SQA) methods rely either on brittle heuristics or on data-hungry\nsupervised models. We introduce the first fully unsupervised SQA pipeline for\nwrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,\nunlabeled data from heterogeneous sources (varying in device and sampling\nfrequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,\nthe learned representation is stable across differences in LED wavelength,\ndrive intensity, and device optics, as well as wrist motion). Stage 2 converts\neach 512-D encoder embedding into a 4-D topological signature via persistent\nhomology (PH) and clusters these signatures with HDBSCAN. To produce a binary\nsignal-quality index (SQI), the acceptable PPG signals are represented by the\ndensest cluster while the remaining clusters are assumed to mainly contain\npoor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,\nDavies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,\nrespectively, on a stratified sample of 10,000 windows. In this study, we\npropose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)\nframework that offers a drop-in, scalable, cross-device quality gate for PPG\nsignals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u65e0\u76d1\u7763\u4fe1\u53f7\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u624b\u8155PPG\u4fe1\u53f7\u7684\u8de8\u8bbe\u5907\u8d28\u91cf\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709PPG\u4fe1\u53f7\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u89c4\u5219\u6216\u6570\u636e\u5bc6\u96c6\u578b\u76d1\u7763\u6a21\u578b\uff0c\u6548\u679c\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u76841-D ResNet-18\u8bad\u7ec3\u65e0\u76d1\u7763\u5d4c\u5165\uff1b2\uff09\u901a\u8fc7\u6301\u7eed\u540c\u8c03\u8f6c\u6362\u5d4c\u5165\u4e3a\u62d3\u6251\u7b7e\u540d\u5e76\u7528HDBSCAN\u805a\u7c7b\uff0c\u751f\u6210\u4e8c\u8fdb\u5236\u4fe1\u53f7\u8d28\u91cf\u6307\u6570\u3002", "result": "\u5728\u4e0d\u91cd\u65b0\u8c03\u53c2\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u572810,000\u4e2a\u7a97\u53e3\u6837\u672c\u4e0a\u7684Silhouette\u3001Davies-Bouldin\u548cCalinski-Harabasz\u5f97\u5206\u5206\u522b\u4e3a0.72\u30010.34\u548c6173\u3002", "conclusion": "\u63d0\u51fa\u7684SSL-TDA\u6846\u67b6\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684PPG\u4fe1\u53f7\u8d28\u91cf\u8bc4\u4f30\uff0c\u5177\u6709\u8de8\u8bbe\u5907\u548c\u53ef\u6269\u5c55\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.12515", "pdf": "https://arxiv.org/pdf/2509.12515", "abs": "https://arxiv.org/abs/2509.12515", "authors": ["Zequan Liang", "Ruoyu Zhang", "Wei Shao", "krishna Karthik", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG", "categories": ["eess.SP"], "comment": "In the proceedings of IEEE-EMBS International Conference on Body\n  Sensor Networks 2025", "summary": "Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.\nTraditional SpO2 estimation methods often rely on complex clinical calibration,\nmaking them unsuitable for low-power, wearable applications. In this paper, we\npropose a transfer learning-based framework for the rapid adaptation of SpO2\nestimation to energy-efficient wearable devices using low-sampling-rate (25Hz)\ndual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long\nShort-Term Memory (BiLSTM) model with self-attention on a public clinical\ndataset, then fine-tune it using data collected from our wearable We-Be band\nand an FDA-approved reference pulse oximeter. Experimental results show that\nour approach achieves a mean absolute error (MAE) of 2.967% on the public\ndataset and 2.624% on the private dataset, significantly outperforming\ntraditional calibration and non-transferred machine learning baselines.\nMoreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,\nexcluding baseline draw. Our method also attains an MAE of 3.284% in\ninstantaneous SpO2 prediction, effectively capturing rapid fluctuations. These\nresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring\non wearable devices without the need for clinical calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u4f4e\u529f\u8017\u53ef\u7a7f\u6234\u8bbe\u5907SpO2\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6821\u51c6\u548c\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edfSpO2\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u4e34\u5e8a\u6821\u51c6\uff0c\u4e0d\u9002\u5408\u4f4e\u529f\u8017\u53ef\u7a7f\u6234\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u53cc\u901a\u905325Hz PPG\u6570\u636e\uff0c\u5148\u9884\u8bad\u7ec3BiLSTM+\u81ea\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u518d\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u5fae\u8c03\u3002", "result": "MAE\u4e3a2.967%\uff08\u516c\u5171\u6570\u636e\u96c6\uff09\u548c2.624%\uff08\u79c1\u6709\u6570\u636e\u96c6\uff09\uff0c\u529f\u8017\u964d\u4f4e40%\u3002", "conclusion": "\u65e0\u9700\u4e34\u5e8a\u6821\u51c6\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u4f4e\u529f\u8017SpO2\u76d1\u6d4b\u3002"}}
{"id": "2509.12367", "pdf": "https://arxiv.org/pdf/2509.12367", "abs": "https://arxiv.org/abs/2509.12367", "authors": ["Daniel Lindmark", "Jonas Andersson", "Kenneth Bodin", "Tora Bodin", "Hugo B\u00f6rjesson", "Fredrik Nordfeldth", "Martin Servin"], "title": "An integrated process for design and control of lunar robotics using AI and simulation", "categories": ["cs.RO", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "We envision an integrated process for developing lunar construction\nequipment, where physical design and control are explored in parallel. In this\npaper, we describe a technical framework that supports this process. It relies\non OpenPLX, a readable/writable declarative language that links CAD-models and\nautonomous systems to high-fidelity, real-time 3D simulations of contacting\nmultibody dynamics, machine regolith interaction forces, and non-ideal sensors.\nTo demonstrate its capabilities, we present two case studies, including an\nautonomous lunar rover that combines a vision-language model for navigation\nwith a reinforcement learning-based control policy for locomotion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5316\u5f00\u53d1\u6708\u7403\u5efa\u8bbe\u8bbe\u5907\u7684\u6280\u672f\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u8bbe\u8ba1\u4e0e\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u652f\u6301\u5e76\u884c\u63a2\u7d22\u7269\u7406\u8bbe\u8ba1\u548c\u63a7\u5236\u7684\u96c6\u6210\u5316\u6d41\u7a0b\uff0c\u4ee5\u652f\u6301\u6708\u7403\u5efa\u8bbe\u8bbe\u5907\u7684\u5f00\u53d1\u3002", "method": "\u4f7f\u7528OpenPLX\u8bed\u8a00\u94fe\u63a5CAD\u6a21\u578b\u4e0e\u81ea\u4e3b\u7cfb\u7edf\uff0c\u7ed3\u5408\u9ad8\u7cbe\u5ea6\u5b9e\u65f63D\u6a21\u62df\uff0c\u5305\u62ec\u63a5\u89e6\u591a\u4f53\u52a8\u529b\u5b66\u3001\u673a\u5668\u4e0e\u6708\u58e4\u76f8\u4e92\u4f5c\u7528\u529b\u4ee5\u53ca\u975e\u7406\u60f3\u4f20\u611f\u5668\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u5982\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bfc\u822a\u548c\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u7684\u81ea\u4e3b\u6708\u7403\u8f66\uff09\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u6280\u672f\u6846\u67b6\u4e3a\u6708\u7403\u5efa\u8bbe\u8bbe\u5907\u7684\u5f00\u53d1\u548c\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12518", "pdf": "https://arxiv.org/pdf/2509.12518", "abs": "https://arxiv.org/abs/2509.12518", "authors": ["Zequan Liang", "Ruoyu Zhang", "Wei Shao", "Mahdi Pirayesh Shirazi Nejad", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning", "categories": ["eess.SP"], "comment": "In the proceedings of IEEE-EMBS International Conference on Body\n  Sensor Networks 2025", "summary": "Accurate and generalizable blood pressure (BP) estimation is vital for the\nearly detection and management of cardiovascular diseases. In this study, we\nenforce subject-level data splitting on a public multi-wavelength\nphotoplethysmography (PPG) dataset and propose a generalizable BP estimation\nframework based on curriculum-adversarial learning. Our approach combines\ncurriculum learning, which transitions from hypertension classification to BP\nregression, with domain-adversarial training that confuses subject identity to\nencourage the learning of subject-invariant features. Experiments show that\nmulti-channel fusion consistently outperforms single-channel models. On the\nfour-wavelength PPG dataset, our method achieves strong performance under\nstrict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for\nsystolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP).\nAdditionally, ablation studies validate the effectiveness of both the\ncurriculum and adversarial components. These results highlight the potential of\nleveraging complementary information in multi-wavelength PPG and\ncurriculum-adversarial strategies for accurate and robust BP estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5bf9\u6297\u5b66\u4e60\u7684\u901a\u7528\u8840\u538b\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6ce2\u957fPPG\u6570\u636e\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u53d7\u8bd5\u8005\u7ea7\u6570\u636e\u5206\u5272\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u8840\u538b\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u7cbe\u786e\u4e14\u6cdb\u5316\u7684\u8840\u538b\u4f30\u8ba1\u5bf9\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u591a\u6ce2\u957fPPG\u6570\u636e\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\uff08\u4ece\u9ad8\u8840\u538b\u5206\u7c7b\u8fc7\u6e21\u5230\u8840\u538b\u56de\u5f52\uff09\u548c\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\uff08\u6df7\u6dc6\u53d7\u8bd5\u8005\u8eab\u4efd\u4ee5\u5b66\u4e60\u53d7\u8bd5\u8005\u4e0d\u53d8\u7279\u5f81\uff09\u7684\u7ed3\u5408\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002", "result": "\u5728\u591a\u6ce2\u957fPPG\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u4e25\u683c\u7684\u53d7\u8bd5\u8005\u7ea7\u5206\u5272\u4e0b\u8868\u73b0\u4f18\u5f02\uff0cSBP\u548cDBP\u7684MAE\u5206\u522b\u4e3a14.2mmHg\u548c6.4mmHg\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6ce2\u957fPPG\u6570\u636e\u548c\u8bfe\u7a0b\u5bf9\u6297\u7b56\u7565\u5728\u8840\u538b\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u8840\u538b\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.12379", "pdf": "https://arxiv.org/pdf/2509.12379", "abs": "https://arxiv.org/abs/2509.12379", "authors": ["Divyam Goel", "Yufei Wang", "Tiancheng Wu", "Guixiu Qiao", "Pavel Piliptchak", "David Held", "Zackory Erickson"], "title": "Geometric Red-Teaming for Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted at the 9th Annual Conference on Robot Learning (CoRL 2025,\n  Oral)", "summary": "Standard evaluation protocols in robotic manipulation typically assess policy\nperformance over curated, in-distribution test sets, offering limited insight\ninto how systems fail under plausible variation. We introduce Geometric\nRed-Teaming (GRT), a red-teaming framework that probes robustness through\nobject-centric geometric perturbations, automatically generating CrashShapes --\nstructurally valid, user-constrained mesh deformations that trigger\ncatastrophic failures in pre-trained manipulation policies. The method\nintegrates a Jacobian field-based deformation model with a gradient-free,\nsimulator-in-the-loop optimization strategy. Across insertion, articulation,\nand grasping tasks, GRT consistently discovers deformations that collapse\npolicy performance, revealing brittle failure modes missed by static\nbenchmarks. By combining task-level policy rollouts with constraint-aware shape\nexploration, we aim to build a general purpose framework for structured,\nobject-centric robustness evaluation in robotic manipulation. We additionally\nshow that fine-tuning on individual CrashShapes, a process we refer to as\nblue-teaming, improves task success by up to 60 percentage points on those\nshapes, while preserving performance on the original object, demonstrating the\nutility of red-teamed geometries for targeted policy refinement. Finally, we\nvalidate both red-teaming and blue-teaming results with a real robotic arm,\nobserving that simulated CrashShapes reduce task success from 90% to as low as\n22.5%, and that blue-teaming recovers performance to up to 90% on the\ncorresponding real-world geometry -- closely matching simulation outcomes.\nVideos and code can be found on our project website:\nhttps://georedteam.github.io/ .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u51e0\u4f55\u7ea2\u961f\u6d4b\u8bd5\uff08GRT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u51e0\u4f55\u6270\u52a8\u81ea\u52a8\u89e6\u53d1\u9884\u8bad\u7ec3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5931\u8d25\uff0c\u63ed\u793a\u4e86\u9759\u6001\u57fa\u51c6\u672a\u68c0\u6d4b\u5230\u7684\u8106\u5f31\u6027\u3002\u901a\u8fc7\u84dd\u961f\u8c03\u4f18\uff0c\u7b56\u7565\u6027\u80fd\u53ef\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u8bc4\u4f30\u534f\u8bae\u4ec5\u9650\u4e8e\u9759\u6001\u6d4b\u8bd5\u96c6\uff0c\u65e0\u6cd5\u63ed\u793a\u7b56\u7565\u5728\u5b9e\u9645\u51e0\u4f55\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "method": "GRT\u7ed3\u5408\u57fa\u4e8e\u96c5\u53ef\u6bd4\u573a\u7684\u53d8\u5f62\u6a21\u578b\u548c\u68af\u5ea6\u65e0\u4f18\u5316\u7b56\u7565\uff0c\u81ea\u52a8\u751f\u6210\u5408\u6cd5\u53d8\u5f62\uff08CrashShapes\uff09\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5faa\u73af\u9a8c\u8bc1\u7b56\u7565\u6027\u80fd\u5d29\u6e83\u3002", "result": "GRT\u6210\u529f\u53d1\u73b0\u5bfc\u81f4\u7b56\u7565\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u53d8\u5f62\uff0c\u84dd\u961f\u8c03\u4f18\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe60%\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u4eff\u771f\u7ed3\u679c\u3002", "conclusion": "GRT\u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u53ef\u901a\u8fc7\u51e0\u4f55\u7ea2\u961f\u6d4b\u8bd5\u8bc6\u522b\u7b56\u7565\u8106\u5f31\u6027\uff0c\u5e76\u7ed3\u5408\u84dd\u961f\u8c03\u4f18\u9488\u5bf9\u6027\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u4eff\u771f\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002"}}
{"id": "2509.12605", "pdf": "https://arxiv.org/pdf/2509.12605", "abs": "https://arxiv.org/abs/2509.12605", "authors": ["Yang Chen", "Yeonju Lee", "Yao Shi", "Qiyu Sun"], "title": "Kalman Filtering of Stationary Graph Signals", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we propose a novel definition of stationary graph signals,\nformulated with respect to a symmetric graph shift, such as the graph\nLaplacian. We show that stationary graph signals can be generated by\ntransmitting white noise through polynomial graph channels, and that their\nstationarity is preserved under polynomial channel transmission.\n  In this paper, we also investigate Kalman filtering to dynamical systems\ncharacterized by polynomial state and observation matrices. We demonstrate that\nKalman filtering maintains the stationarity of graph signals, while effectively\nincorporating both system dynamics and noise structure. In comparison to the\nstatic inverse filtering method and naive zero-signal strategy, the Kalman\nfiltering procedure yields more accurate and adaptive signal estimates,\nhighlighting its robustness and versatility in graph signal processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u56fe\u79fb\uff08\u5982\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff09\u7684\u9759\u6001\u56fe\u4fe1\u53f7\u65b0\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u901a\u8fc7\u591a\u9879\u5f0f\u56fe\u901a\u9053\u751f\u6210\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728\u591a\u9879\u5f0f\u72b6\u6001\u548c\u89c2\u6d4b\u77e9\u9635\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u793a\u5176\u4f18\u4e8e\u9759\u6001\u9006\u6ee4\u6ce2\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u9759\u6001\u56fe\u4fe1\u53f7\u7684\u5b9a\u4e49\u53ca\u5176\u5728\u591a\u9879\u5f0f\u901a\u9053\u4e2d\u7684\u6027\u8d28\uff0c\u5e76\u7814\u7a76\u5361\u5c14\u66fc\u6ee4\u6ce2\u5982\u4f55\u6709\u6548\u7ed3\u5408\u52a8\u6001\u7cfb\u7edf\u548c\u566a\u58f0\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u591a\u9879\u5f0f\u56fe\u901a\u9053\u751f\u6210\u9759\u6001\u56fe\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5904\u7406\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u591a\u9879\u5f0f\u77e9\u9635\u3002", "result": "\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728\u4fdd\u6301\u56fe\u4fe1\u53f7\u9759\u6001\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u81ea\u9002\u5e94\u7684\u4fe1\u53f7\u4f30\u8ba1\u3002", "conclusion": "\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728\u9759\u6001\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.12390", "pdf": "https://arxiv.org/pdf/2509.12390", "abs": "https://arxiv.org/abs/2509.12390", "authors": ["Evangelos Psomiadis", "Panagiotis Tsiotras"], "title": "Distributed Event-Triggered Distance-Based Formation Control for Multi-Agent Systems", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "This paper addresses the problem of collaborative formation control for\nmulti-agent systems with limited resources. We consider a team of robots tasked\nwith achieving a desired formation from arbitrary initial configurations. To\nreduce unnecessary control updates and conserve resources, we propose a\ndistributed event-triggered formation controller that relies on inter-agent\ndistance measurements. Control updates are triggered only when the measurement\nerror exceeds a predefined threshold, ensuring system stability. The proposed\ncontroller is validated through extensive simulations and real-world\nexperiments involving different formations, communication topologies,\nscalability tests, and variations in design parameters, while also being\ncompared against periodic triggering strategies. Results demonstrate that the\nevent-triggered approach significantly reduces control efforts while preserving\nformation performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u4e8b\u4ef6\u89e6\u53d1\u7f16\u961f\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4ec5\u5728\u6d4b\u91cf\u8bef\u5dee\u8d85\u8fc7\u9884\u8bbe\u9608\u503c\u65f6\u66f4\u65b0\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u7684\u534f\u540c\u7f16\u961f\u63a7\u5236\u95ee\u9898\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u63a7\u5236\u66f4\u65b0\u4ee5\u8282\u7701\u8d44\u6e90\u3002", "method": "\u57fa\u4e8e\u667a\u80fd\u4f53\u95f4\u8ddd\u79bb\u6d4b\u91cf\u7684\u5206\u5e03\u5f0f\u4e8b\u4ef6\u89e6\u53d1\u63a7\u5236\u5668\uff0c\u4ec5\u5728\u8bef\u5dee\u8d85\u9608\u503c\u65f6\u89e6\u53d1\u63a7\u5236\u66f4\u65b0\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u63a7\u5236\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7f16\u961f\u6027\u80fd\uff0c\u4e14\u4f18\u4e8e\u5468\u671f\u6027\u89e6\u53d1\u7b56\u7565\u3002", "conclusion": "\u4e8b\u4ef6\u89e6\u53d1\u5f0f\u63a7\u5236\u5668\u5728\u8d44\u6e90\u53d7\u9650\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.12646", "pdf": "https://arxiv.org/pdf/2509.12646", "abs": "https://arxiv.org/abs/2509.12646", "authors": ["Yixin Ding", "Haoyu Jiang", "Xiaoli Xu", "Yanan Liang", "Yong Zeng"], "title": "Data Fusion for BS-UE Cooperative MIMO-OFDM ISAC", "categories": ["eess.SP"], "comment": "6 pages, 4 figures", "summary": "Integrated sensing and communication (ISAC) is a promising technique for\nexpanding the functionalities of wireless networks with enhanced spectral\nefficiency. The 3rd Generation Partnership Project (3GPP) has defined six basic\nsensing operation modes in wireless networks. To further enhance the sensing\ncapability of wireless networks, this paper proposes a new sensing operation\nmode, i.e., the base station (BS) and user equipment (UE) cooperative sensing.\nSpecifically, after decoding the communication data, the UE further processes\nthe received signal to extract the target sensing information. We propose an\nefficient algorithm for fusing the sensing results obtained by the BS and UE,\nby exploiting the geometric relationship among BS, UE and targets as well as\nthe expected sensing quality in the BS monostatic and BS-UE bistatic sensing.\nThe results show that the proposed data fusion method for cooperative sensing\ncan effectively improve the position and velocity estimation accuracy of\nmultiple targets, and provide a new approach on the expansion of the sensing\npattern.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u7ad9\uff08BS\uff09\u4e0e\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u534f\u540c\u611f\u77e5\u6a21\u5f0f\uff0c\u901a\u8fc7\u878d\u5408BS\u548cUE\u7684\u611f\u77e5\u7ed3\u679c\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u76ee\u6807\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u65e0\u7ebf\u7f51\u7edc\u7684\u611f\u77e5\u80fd\u529b\uff0c\u672c\u6587\u7814\u7a76\u4e86BS\u4e0eUE\u534f\u540c\u611f\u77e5\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u6269\u5c55\u65e0\u7ebf\u7f51\u7edc\u7684\u611f\u77e5\u529f\u80fd\u5e76\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5229\u7528BS\u3001UE\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u4ee5\u53caBS\u5355\u9759\u6001\u548cBS-UE\u53cc\u9759\u6001\u611f\u77e5\u7684\u9884\u671f\u8d28\u91cf\uff0c\u878d\u5408BS\u548cUE\u7684\u611f\u77e5\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u534f\u540c\u611f\u77e5\u7684\u6570\u636e\u878d\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u76ee\u6807\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "BS\u4e0eUE\u534f\u540c\u611f\u77e5\u6a21\u5f0f\u4e3a\u6269\u5c55\u65e0\u7ebf\u7f51\u7edc\u7684\u611f\u77e5\u6a21\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2509.12398", "pdf": "https://arxiv.org/pdf/2509.12398", "abs": "https://arxiv.org/abs/2509.12398", "authors": ["Michael Lorenz", "Bertram Taetz", "Gabriele Bleser-Taetz", "Didier Stricker"], "title": "MinJointTracker: Real-time inertial kinematic chain tracking with joint position estimation and minimal state size", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "10 pages, 2 figures", "summary": "Inertial motion capture is a promising approach for capturing motion outside\nthe laboratory. However, as one major drawback, most of the current methods\nrequire different quantities to be calibrated or computed offline as part of\nthe setup process, such as segment lengths, relative orientations between\ninertial measurement units (IMUs) and segment coordinate frames (IMU-to-segment\ncalibrations) or the joint positions in the IMU frames. This renders the setup\nprocess inconvenient. This work contributes to real-time capable\ncalibration-free inertial tracking of a kinematic chain, i.e. simultaneous\nrecursive Bayesian estimation of global IMU angular kinematics and joint\npositions in the IMU frames, with a minimal state size. Experimental results on\nsimulated IMU data from a three-link kinematic chain (manipulator study) as\nwell as re-simulated IMU data from healthy humans walking (lower body study)\nshow that the calibration-free and lightweight algorithm provides not only\ndrift-free relative but also drift-free absolute orientation estimates with a\nglobal heading reference for only one IMU as well as robust and fast\nconvergence of joint position estimates in the different movement scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u7684\u5b9e\u65f6\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b97\u6cd5\u5b9e\u73b0\u65e0\u6f02\u79fb\u7684\u76f8\u5bf9\u548c\u7edd\u5bf9\u65b9\u5411\u4f30\u8ba1\uff0c\u540c\u65f6\u5feb\u901f\u6536\u655b\u5230\u5173\u8282\u4f4d\u7f6e\u4f30\u8ba1\u3002", "motivation": "\u5f53\u524d\u60ef\u6027\u8fd0\u52a8\u6355\u6349\u65b9\u6cd5\u9700\u8981\u79bb\u7ebf\u6821\u51c6\uff0c\u5bfc\u81f4\u8bbe\u7f6e\u8fc7\u7a0b\u4e0d\u4fbf\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5b9e\u65f6\u3001\u65e0\u9700\u6821\u51c6\u7684\u60ef\u6027\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9012\u5f52\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u72b6\u6001\u7a7a\u95f4\uff0c\u540c\u65f6\u4f30\u8ba1\u5168\u5c40IMU\u89d2\u8fd0\u52a8\u548c\u5173\u8282\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u94fe\u63a5\u8fd0\u52a8\u94fe\u548c\u4eba\u884c\u8d70\u6570\u636e\u4e2d\u5747\u80fd\u63d0\u4f9b\u65e0\u6f02\u79fb\u7684\u65b9\u5411\u4f30\u8ba1\u548c\u5feb\u901f\u6536\u655b\u7684\u5173\u8282\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6821\u51c6\uff0c\u8f7b\u91cf\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8fd0\u52a8\u6355\u6349\u3002"}}
{"id": "2509.12658", "pdf": "https://arxiv.org/pdf/2509.12658", "abs": "https://arxiv.org/abs/2509.12658", "authors": ["Po-Heng Chou", "Jiun-Jia Wu", "Wan-Jen Huang", "Ronald Y. Chang"], "title": "Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": "6 pages, 5 figures, 2 tables, and accepted by 2025 IEEE Globecom\n  Workshops", "summary": "In this paper, we propose a sustainable long short-term memory (LSTM)-based\nprecoding framework for reconfigurable intelligent surface (RIS)-assisted\nmillimeter-wave (mmWave) MIMO systems. Instead of explicit channel state\ninformation (CSI) estimation, the framework exploits uplink pilot sequences to\nimplicitly learn channel characteristics, reducing both pilot overhead and\ninference complexity. Practical hardware constraints are addressed by\nincorporating the phase-dependent amplitude model of RIS elements, while a\nmulti-label training strategy improves robustness when multiple near-optimal\ncodewords yield comparable performance. Simulations show that the proposed\ndesign achieves over 90% of the spectral efficiency of exhaustive search (ES)\nwith only 2.2% of its computation time, cutting energy consumption by nearly\ntwo orders of magnitude. The method also demonstrates resilience under\ndistribution mismatch and scalability to larger RIS arrays, making it a\npractical and energy-efficient solution for sustainable 6G wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7684\u53ef\u6301\u7eed\u9884\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8eRIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\uff0c\u51cf\u5c11\u5f00\u9500\u548c\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCSI\u4f30\u8ba1\u7684\u9ad8\u5f00\u9500\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u540c\u65f6\u8003\u8651RIS\u786c\u4ef6\u7ea6\u675f\u548c\u591a\u6807\u7b7e\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u5229\u7528\u4e0a\u884c\u5bfc\u9891\u5e8f\u5217\u9690\u5f0f\u5b66\u4e60\u4fe1\u9053\u7279\u6027\uff0c\u7ed3\u5408RIS\u76f8\u4f4d\u4f9d\u8d56\u7684\u5e45\u5ea6\u6a21\u578b\u548c\u591a\u6807\u7b7e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u5176\u9891\u8c31\u6548\u7387\u8fbe\u5230\u7a77\u4e3e\u641c\u7d22\u768490%\u4ee5\u4e0a\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a\u51762.2%\uff0c\u80fd\u8017\u964d\u4f4e\u8fd1\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u4e0d\u5339\u914d\u548c\u5927\u89c4\u6a21RIS\u9635\u5217\u4e0b\u4ecd\u8868\u73b0\u7a33\u5065\uff0c\u662f\u4e00\u79cd\u53ef\u6301\u7eed\u4e14\u9ad8\u6548\u76846G\u65e0\u7ebf\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12444", "pdf": "https://arxiv.org/pdf/2509.12444", "abs": "https://arxiv.org/abs/2509.12444", "authors": ["Weiting Feng", "Kyle L. Walker", "Yunjie Yang", "Francesco Giorgio-Serchi"], "title": "Computing forward statics from tendon-length in flexible-joint hyper-redundant manipulators", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "To be presented at IROS 2025, Hangzhou, China", "summary": "Hyper-redundant tendon-driven manipulators offer greater flexibility and\ncompliance over traditional manipulators. A common way of controlling such\nmanipulators relies on adjusting tendon lengths, which is an accessible control\nparameter. This approach works well when the kinematic configuration is\nrepresentative of the real operational conditions. However, when dealing with\nmanipulators of larger size subject to gravity, it becomes necessary to solve a\nstatic force problem, using tendon force as the input and employing a mapping\nfrom the configuration space to retrieve tendon length. Alternatively,\nmeasurements of the manipulator posture can be used to iteratively adjust\ntendon lengths to achieve a desired posture. Hence, either tension measurement\nor state estimation of the manipulator are required, both of which are not\nalways accurately available. Here, we propose a solution by reconciling cables\ntension and length as the input for the solution of the system forward statics.\nWe develop a screw-based formulation for a tendon-driven, multi-segment,\nhyper-redundant manipulator with elastic joints and introduce a forward statics\niterative solution method that equivalently makes use of either tendon length\nor tension as the input. This strategy is experimentally validated using a\ntraditional tension input first, subsequently showing the efficacy of the\nmethod when exclusively tendon lengths are used. The results confirm the\npossibility to perform open-loop control in static conditions using a kinematic\ninput only, thus bypassing some of the practical problems with tension\nmeasurement and state estimation of hyper-redundant systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u8d85\u5197\u4f59\u8171\u9a71\u52a8\u673a\u68b0\u81c2\u63a7\u5236\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8171\u7684\u957f\u5ea6\u548c\u5f20\u529b\u4f5c\u4e3a\u8f93\u5165\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5bf9\u5f20\u529b\u6d4b\u91cf\u6216\u72b6\u6001\u4f30\u8ba1\u7684\u9700\u6c42\u3002", "motivation": "\u8d85\u5197\u4f59\u8171\u9a71\u52a8\u673a\u68b0\u81c2\u5728\u63a7\u5236\u4e2d\u9762\u4e34\u91cd\u529b\u5f71\u54cd\u548c\u8171\u529b\u6d4b\u91cf\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4ec5\u8c03\u6574\u8171\u957f\u6216\u8171\u529b\u8f93\u5165\uff09\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u7684\u9759\u6001\u529b\u95ee\u9898\u89e3\u6cd5\uff0c\u5f00\u53d1\u4e86\u5f39\u6027\u5173\u8282\u8171\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u6b63\u5411\u9759\u6001\u8fed\u4ee3\u6c42\u89e3\u65b9\u6cd5\uff0c\u53ef\u540c\u65f6\u5229\u7528\u8171\u957f\u6216\u8171\u529b\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u8171\u957f\u8f93\u5165\u65f6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u9759\u6001\u6761\u4ef6\u4e0b\u7684\u5f00\u73af\u63a7\u5236\uff0c\u7ed5\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6d4b\u91cf\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4ec5\u7528\u8fd0\u52a8\u5b66\u8f93\u5165\uff08\u8171\u957f\uff09\u5373\u53ef\u5b9e\u73b0\u9759\u6001\u6761\u4ef6\u4e0b\u7684\u5f00\u73af\u63a7\u5236\uff0c\u7b80\u5316\u4e86\u8d85\u5197\u4f59\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u95ee\u9898\u3002"}}
{"id": "2509.12698", "pdf": "https://arxiv.org/pdf/2509.12698", "abs": "https://arxiv.org/abs/2509.12698", "authors": ["Yifan Jiang", "Qingqing Wu", "Hongxun Hui", "Wen Chen", "Derrick Wing Kwan Ng"], "title": "Low-Altitude UAV Tracking via Sensing-Assisted Predictive Beamforming", "categories": ["eess.SP", "cs.ET", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "comment": "13 pages, submitted to IEEE Transaction journals", "summary": "Sensing-assisted predictive beamforming, as one of the enabling technologies\nfor emerging integrated sensing and communication (ISAC) paradigm, shows\nsignificant promise for enhancing various future unmanned aerial vehicle (UAV)\napplications. However, current works predominately emphasized on spectral\nefficiency enhancement, while the impact of such beamforming techniques on the\ncommunication reliability was largely unexplored and challenging to\ncharacterize. To fill this research gap and tackle this issue, this paper\ninvestigates outage capacity maximization for UAV tracking under the\nsensing-assisted predictive beamforming scheme. Specifically, a\ncellular-connected UAV tracking scheme is proposed leveraging extended Kalman\nfiltering (EKF), where the predicted UAV trajectory, sensing duration ratio,\nand target constant received signal-to-noise ratio (SNR) are jointly optimized\nto maximize the outage capacity at each time slot. To address the implicit\nnature of the objective function, closed-form approximations of the outage\nprobabilities (OPs) at both prediction and measurement stages of each time slot\nare proposed based on second-order Taylor expansions, providing an efficient\nand full characterization of outage capacity. Subsequently, an efficient\nalgorithm is proposed based on a combination of bisection search and successive\nconvex approximation (SCA) to address the non-convex optimization problem with\nguaranteed convergence. To further reduce computational complexity, a second\nefficient algorithm is developed based on alternating optimization (AO).\nSimulation results validate the accuracy of the derived OP approximations, the\neffectiveness of the proposed algorithms, and the significant outage capacity\nenhancement over various benchmarks, while also indicating a trade-off between\ndecreasing path loss and enjoying wide beam coverage for outage capacity\nmaximization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4f20\u611f\u8f85\u52a9\u9884\u6d4b\u6ce2\u675f\u6210\u5f62\u4e0b\u7684\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u65ad\u5bb9\u91cf\u6700\u5927\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9891\u8c31\u6548\u7387\u63d0\u5347\uff0c\u800c\u4f20\u611f\u8f85\u52a9\u6ce2\u675f\u6210\u5f62\u5bf9\u901a\u4fe1\u53ef\u9760\u6027\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65e0\u4eba\u673a\u8ddf\u8e2a\u65b9\u6848\uff0c\u5e76\u8054\u5408\u4f18\u5316\u9884\u6d4b\u8f68\u8ff9\u3001\u4f20\u611f\u6301\u7eed\u65f6\u95f4\u6bd4\u548c\u76ee\u6807\u6052\u5b9a\u4fe1\u566a\u6bd4\u3002\u901a\u8fc7\u4e8c\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3c\u4e2d\u65ad\u6982\u7387\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff08SCA\u548cAO\uff09\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4e2d\u65ad\u5bb9\u91cf\uff0c\u5e76\u5728\u8def\u5f84\u635f\u8017\u548c\u6ce2\u675f\u8986\u76d6\u4e4b\u95f4\u63ed\u793a\u4e86\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u4e3aISAC\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2509.12458", "pdf": "https://arxiv.org/pdf/2509.12458", "abs": "https://arxiv.org/abs/2509.12458", "authors": ["\u00c0lmos Veres-Vit\u00e0lyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-P\u00e9rez"], "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "comment": "13 pages, 16 figures, 3 tables, 45 references", "summary": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for\nnavigating indoor and hard-to-reach areas, yet their significant constraints in\npayload and autonomy have largely prevented their use for complex tasks like\nhigh-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we\nintroduce a novel system architecture that enables fully autonomous,\nhigh-fidelity 3D scanning of static objects using UAVs weighing under 100\ngrams. Our core innovation lies in a dual-reconstruction pipeline that creates\na real-time feedback loop between data capture and flight control. A\nnear-real-time (near-RT) process uses Structure from Motion (SfM) to generate\nan instantaneous pointcloud of the object. The system analyzes the model\nquality on the fly and dynamically adapts the UAV's trajectory to intelligently\ncapture new images of poorly covered areas. This ensures comprehensive data\nacquisition. For the final, detailed output, a non-real-time (non-RT) pipeline\nemploys a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)\napproach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)\nlocation data to achieve superior accuracy. We implemented and validated this\narchitecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both\nsingle- and multi-UAV configurations, conclusively show that dynamic trajectory\nadaptation consistently improves reconstruction quality over static flight\npaths. This work demonstrates a scalable and autonomous solution that unlocks\nthe potential of miniaturized UAVs for fine-grained 3D reconstruction in\nconstrained environments, a capability previously limited to much larger\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u5316\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u91cd\u5efa\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u52a8\u6001\u8c03\u6574\u98de\u884c\u8def\u5f84\u4ee5\u4f18\u5316\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u5f53\u524d\u5c0f\u578b\u65e0\u4eba\u673a\u56e0\u8f7d\u8377\u548c\u81ea\u4e3b\u6027\u53d7\u9650\uff0c\u96be\u4ee5\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u5982\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u5efa\u6d41\u7a0b\uff1a\u5b9e\u65f6\u70b9\u4e91\u751f\u6210\u4e0e\u52a8\u6001\u8f68\u8ff9\u8c03\u6574\uff0c\u7ed3\u5408SfM\u548cN3DR\u65b9\u6cd5\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u52a8\u6001\u8f68\u8ff9\u8c03\u6574\u663e\u8457\u4f18\u4e8e\u9759\u6001\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u5355\u673a\u6216\u591a\u673a\u914d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6848\u5c55\u793a\u4e86\u8f7b\u91cf\u5316\u65e0\u4eba\u673a\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u7ec63D\u91cd\u5efa\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.12748", "pdf": "https://arxiv.org/pdf/2509.12748", "abs": "https://arxiv.org/abs/2509.12748", "authors": ["Haiyang Li", "Tianqi Mao", "Pengyu Wang", "Ruiqi Liu", "Shunyu Li", "Zhaocheng Wang"], "title": "NEFT: A Unified Transformer Framework for Efficient Near-Field CSI Feedback in XL-MIMO Systems", "categories": ["eess.SP"], "comment": null, "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems,\noperating in the near-field region due to their massive antenna arrays, are a\nkey enabler of next-generation wireless communications but face significant\nchallenges in channel state information (CSI) feedback. Deep learning has\nemerged as a powerful tool by learning compact CSI representations for\nfeedback. However, existing methods struggle to capture the intricate structure\nof near-field CSI while incurring prohibitive computational overhead on\npractical mobile devices. To overcome these limitations, we propose the\nNear-Field Efficient Feedback Transformer (NEFT) family for accurate and\nefficient near-field CSI feedback across diverse hardware platforms. Built on a\nhierarchical Vision Transformer backbone, NEFT is extended with lightweight\nvariants to meet various deployment constraints: NEFT-Compact applies\nmulti-level knowledge distillation (KD) to reduce complexity while maintaining\naccuracy, and NEFT-Hybrid and NEFT-Edge address encoder- and edge-constrained\nscenarios via attention-free encoding and KD. Extensive simulations show that\nNEFT achieves a 15--21 dB improvement in normalized mean-squared error (NMSE)\nover state-of-the-art methods, while NEFT-Compact and NEFT-Edge reduce total\nFLOPs by 25--36% with negligible accuracy loss. Moreover, NEFT-Hybrid lowers\nencoder-side complexity by up to 64%, enabling deployment in highly asymmetric\ndevice scenarios. These results establish NEFT as a practical and scalable\nsolution for near-field CSI feedback in XL-MIMO systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6781\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08XL-MIMO\uff09\u7cfb\u7edf\u4e2d\u8fd1\u573a\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u53cd\u9988\u7684\u9ad8\u6548\u53d8\u538b\u5668\u5bb6\u65cfNEFT\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u53d8\u4f53\u9002\u5e94\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd1\u573aCSI\u53cd\u9988\u4e2d\u96be\u4ee5\u6355\u83b7\u590d\u6742\u7ed3\u6784\u4e14\u8ba1\u7b97\u5f00\u9500\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u5206\u5c42\u89c6\u89c9\u53d8\u538b\u5668\uff08Vision Transformer\uff09\u6784\u5efaNEFT\uff0c\u63a8\u51fa\u8f7b\u91cf\u5316\u53d8\u4f53\uff08NEFT-Compact\u3001NEFT-Hybrid\u3001NEFT-Edge\uff09\uff0c\u7ed3\u5408\u591a\u7ea7\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u548c\u65e0\u6ce8\u610f\u529b\u7f16\u7801\u6280\u672f\u3002", "result": "NEFT\u5728\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534715-21 dB\uff0c\u8f7b\u91cf\u5316\u53d8\u4f53\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c1125-36%\uff0c\u540c\u65f6\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "NEFT\u4e3aXL-MIMO\u7cfb\u7edf\u4e2d\u7684\u8fd1\u573aCSI\u53cd\u9988\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12468", "pdf": "https://arxiv.org/pdf/2509.12468", "abs": "https://arxiv.org/abs/2509.12468", "authors": ["Shipeng Liu", "Meghana Sagare", "Shubham Patil", "Feifei Qian"], "title": "Bio-inspired tail oscillation enables robot fast crawling on deformable granular terrains", "categories": ["cs.RO"], "comment": null, "summary": "Deformable substrates such as sand and mud present significant challenges for\nterrestrial robots due to complex robot-terrain interactions. Inspired by\nmudskippers, amphibious animals that naturally adjust their tail morphology and\nmovement jointly to navigate such environments, we investigate how tail design\nand control can jointly enhance flipper-driven locomotion on granular media.\nUsing a bio-inspired robot modeled after the mudskipper, we experimentally\ncompared locomotion performance between idle and actively oscillating tail\nconfigurations. Tail oscillation increased robot speed by 67% and reduced body\ndrag by 46%. Shear force measurements revealed that this improvement was\nenabled by tail oscillation fluidizing the substrate, thereby reducing\nresistance. Additionally, tail morphology strongly influenced the oscillation\nstrategy: designs with larger horizontal surface areas leveraged the\noscillation-reduced shear resistance more effectively by limiting insertion\ndepth. Based on these findings, we present a design principle to inform tail\naction selection based on substrate strength and tail morphology. Our results\noffer new insights into tail design and control for improving robot locomotion\non deformable substrates, with implications for agricultural robotics, search\nand rescue, and environmental exploration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5c3e\u5df4\u8bbe\u8ba1\u548c\u63a7\u5236\u7684\u8054\u5408\u4f18\u5316\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u53ef\u53d8\u5f62\u57fa\u5e95\u4e0a\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u53d7\u5f39\u6d82\u9c7c\u542f\u53d1\uff0c\u5b9e\u9a8c\u8868\u660e\u5c3e\u5df4\u632f\u8361\u80fd\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u51cf\u5c11\u963b\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u6c99\u571f\u7b49\u53ef\u53d8\u5f62\u57fa\u5e95\u4e0a\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u53d7\u5f39\u6d82\u9c7c\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5c3e\u5df4\u8bbe\u8ba1\u548c\u63a7\u5236\u5bf9\u8fd0\u52a8\u7684\u4f18\u5316\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u5f39\u6d82\u9c7c\u542f\u53d1\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u9759\u6001\u5c3e\u5df4\u548c\u4e3b\u52a8\u632f\u8361\u5c3e\u5df4\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u526a\u5207\u529b\u6d4b\u91cf\u5206\u6790\u539f\u56e0\u3002", "result": "\u5c3e\u5df4\u632f\u8361\u4f7f\u673a\u5668\u4eba\u901f\u5ea6\u63d0\u534767%\uff0c\u963b\u529b\u51cf\u5c1146%\uff1b\u5c3e\u5df4\u5f62\u6001\u663e\u8457\u5f71\u54cd\u632f\u8361\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u57fa\u5e95\u5f3a\u5ea6\u548c\u5c3e\u5df4\u5f62\u6001\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4e3a\u519c\u4e1a\u3001\u641c\u6551\u548c\u73af\u5883\u63a2\u7d22\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2509.12770", "pdf": "https://arxiv.org/pdf/2509.12770", "abs": "https://arxiv.org/abs/2509.12770", "authors": ["Giorgi Tsintsadze", "Haran Manoharan", "Aaron Harmon", "Daniel Commerou", "Connor Buneta", "Brian Booth", "Daryl Beetner"], "title": "EMC Limit Level Guidelines for In-System Interference with GPS Receivers", "categories": ["eess.SP"], "comment": null, "summary": "Because GPS signals are weak, electronic systems and components that are\nplaced near GPS receivers can easily cause disruptive electromagnetic\ninterference through their unintended radiated emissions. In this paper, EMC\nlimit level guidelines are presented for electronics that are intended to be\nplaced near to GPS receivers, as often happens in automotive and other\napplications. One of the challenges of defining limit-levels for systems\nintended to be integrated with GPS receivers is that the impact of noise at the\ninput of the receiver may vary substantially depending on the form of the noise\ndue to the correlator function implemented by GPS receiver. The quality of the\ncorrelated signal is typically represented using the carrier-to-noise ratio ($C\n/ N_0$). A theoretical model predicting the degredation of the carrier-to-noise\nratio with radio frequency interference is presented in this paper and is\nvalidated with realistic noise sources. The model is then used to develop\nguidelines to assess the impact of unintended emissions from electronic devices\non nearby GPS receivers based on the frequency, bandwidth, and magnitude of the\nnoise. These guidelines provide a more nuanced method of evaluating emissions\nthan simple limit lines that are used by many emissions standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9GPS\u63a5\u6536\u5668\u9644\u8fd1\u7535\u5b50\u8bbe\u5907\u7684EMC\u9650\u5236\u6c34\u5e73\u6307\u5357\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u6a21\u578b\u548c\u5b9e\u9645\u566a\u58f0\u6e90\u9a8c\u8bc1\u4e86\u5176\u5bf9\u8f7d\u566a\u6bd4\u7684\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8eGPS\u4fe1\u53f7\u8f83\u5f31\uff0c\u9644\u8fd1\u7684\u7535\u5b50\u8bbe\u5907\u53ef\u80fd\u4ea7\u751f\u7535\u78c1\u5e72\u6270\uff0c\u5f71\u54cdGPS\u63a5\u6536\u5668\u7684\u6027\u80fd\u3002\u9700\u8981\u5236\u5b9a\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6a21\u578b\u9884\u6d4b\u5c04\u9891\u5e72\u6270\u5bf9\u8f7d\u566a\u6bd4\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u566a\u58f0\u6e90\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u8f7d\u566a\u6bd4\u7684\u4e0b\u964d\uff0c\u5e76\u7528\u4e8e\u5236\u5b9a\u57fa\u4e8e\u566a\u58f0\u9891\u7387\u3001\u5e26\u5bbd\u548c\u5e45\u5ea6\u7684\u8bc4\u4f30\u6307\u5357\u3002", "conclusion": "\u65b0\u6307\u5357\u63d0\u4f9b\u4e86\u6bd4\u7b80\u5355\u9650\u5236\u7ebf\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eGPS\u63a5\u6536\u5668\u9644\u8fd1\u7684\u7535\u5b50\u8bbe\u5907\u8bbe\u8ba1\u3002"}}
{"id": "2509.12507", "pdf": "https://arxiv.org/pdf/2509.12507", "abs": "https://arxiv.org/abs/2509.12507", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T07, 68T40", "I.2.9; I.2.6"], "comment": "DOI: 10.3389/frobt.2023.1110534. This is the author's LaTeX version", "summary": "One of the main goals of robotics and intelligent agent research is to enable\nnatural communication with humans in physically situated settings. While recent\nwork has focused on verbal modes such as language and speech, non-verbal\ncommunication is crucial for flexible interaction. We present a framework for\ngenerating pointing gestures in embodied agents by combining imitation and\nreinforcement learning. Using a small motion capture dataset, our method learns\na motor control policy that produces physically valid, naturalistic gestures\nwith high referential accuracy. We evaluate the approach against supervised\nlearning and retrieval baselines in both objective metrics and a virtual\nreality referential game with human users. Results show that our system\nachieves higher naturalness and accuracy than state-of-the-art supervised\nmodels, highlighting the promise of imitation-RL for communicative gesture\ngeneration and its potential application to robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u9ad8\u81ea\u7136\u6027\u548c\u51c6\u786e\u6027\u7684\u6307\u5411\u624b\u52bf\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u5b9e\u73b0\u667a\u80fd\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u81ea\u7136\u6c9f\u901a\uff0c\u5c24\u5176\u662f\u975e\u8bed\u8a00\u6c9f\u901a\uff08\u5982\u6307\u5411\u624b\u52bf\uff09\u5bf9\u4e8e\u7075\u6d3b\u4ea4\u4e92\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u52a8\u4f5c\u6355\u6349\u6570\u636e\u8bad\u7ec3\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u751f\u6210\u7269\u7406\u6709\u6548\u4e14\u81ea\u7136\u7684\u6307\u5411\u624b\u52bf\u3002", "result": "\u5728\u5ba2\u89c2\u6307\u6807\u548c\u865a\u62df\u73b0\u5b9e\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u81ea\u7136\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u5728\u751f\u6210\u901a\u4fe1\u624b\u52bf\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u3002"}}
{"id": "2509.12821", "pdf": "https://arxiv.org/pdf/2509.12821", "abs": "https://arxiv.org/abs/2509.12821", "authors": ["Martin Zach", "Youssef Haouchat", "Michael Unser"], "title": "A Statistical Benchmark for Diffusion Posterior Sampling Algorithms", "categories": ["eess.SP"], "comment": null, "summary": "We propose a statistical benchmark for diffusion posterior sampling (DPS)\nalgorithms for Bayesian linear inverse problems. The benchmark synthesizes\nsignals from sparse L\\'evy-process priors whose posteriors admit efficient\nGibbs methods. These Gibbs methods can be used to obtain gold-standard\nposterior samples that can be compared to the samples obtained by the DPS\nalgorithms. By using the Gibbs methods for the resolution of the denoising\nproblems in the reverse diffusion, the framework also isolates the error that\narises from the approximations to the likelihood score. We instantiate the\nbenchmark with the minimum-mean-squared-error optimality gap and posterior\ncoverage tests and provide numerical experiments for popular DPS algorithms on\nthe inverse problems of denoising, deconvolution, imputation, and\nreconstruction from partial Fourier measurements. We release the benchmark code\nat https://github.com/zacmar/dps-benchmark. The repository exposes simple\nplug-in interfaces, reference scripts, and config-driven runs so that new\nalgorithms can be added and evaluated with minimal effort. We invite\nresearchers to contribute and report results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6269\u6563\u540e\u9a8c\u91c7\u6837\uff08DPS\uff09\u7b97\u6cd5\u7684\u7edf\u8ba1\u57fa\u51c6\uff0c\u901a\u8fc7\u7a00\u758fL\\'evy\u8fc7\u7a0b\u5148\u9a8c\u751f\u6210\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u4e0eDPS\u7b97\u6cd5\u6837\u672c\u7684\u5bf9\u6bd4\u3002", "motivation": "\u4e3aBayesian\u7ebf\u6027\u9006\u95ee\u9898\u4e2d\u7684DPS\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9760\u7684\u7edf\u8ba1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u7a00\u758fL\\'evy\u8fc7\u7a0b\u5148\u9a8c\u548c\u9ad8\u6548\u7684Gibbs\u65b9\u6cd5\u751f\u6210\u9ec4\u91d1\u6807\u51c6\u540e\u9a8c\u6837\u672c\uff0c\u5e76\u4e0eDPS\u7b97\u6cd5\u7ed3\u679c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9DPS\u7b97\u6cd5\u7684\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u53bb\u566a\u3001\u89e3\u5377\u79ef\u3001\u586b\u8865\u548c\u90e8\u5206\u5085\u91cc\u53f6\u6d4b\u91cf\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3aDPS\u7b97\u6cd5\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u5e76\u9f13\u52b1\u7814\u7a76\u8005\u8d21\u732e\u7ed3\u679c\u4ee5\u4fc3\u8fdb\u7b97\u6cd5\u6539\u8fdb\u3002"}}
{"id": "2509.12516", "pdf": "https://arxiv.org/pdf/2509.12516", "abs": "https://arxiv.org/abs/2509.12516", "authors": ["William Ward", "Sarah Etter", "Jesse Quattrociocchi", "Christian Ellis", "Adam J. Thorpe", "Ufuk Topcu"], "title": "Zero to Autonomy in Real-Time: Online Adaptation of Dynamics in Unstructured Environments", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Autonomous robots must go from zero prior knowledge to safe control within\nseconds to operate in unstructured environments. Abrupt terrain changes, such\nas a sudden transition to ice, create dynamics shifts that can destabilize\nplanners unless the model adapts in real-time. We present a method for online\nadaptation that combines function encoders with recursive least squares,\ntreating the function encoder coefficients as latent states updated from\nstreaming odometry. This yields constant-time coefficient estimation without\ngradient-based inner-loop updates, enabling adaptation from only a few seconds\nof data. We evaluate our approach on a Van der Pol system to highlight\nalgorithmic behavior, in a Unity simulator for high-fidelity off-road\nnavigation, and on a Clearpath Jackal robot, including on a challenging terrain\nat a local ice rink. Across these settings, our method improves model accuracy\nand downstream planning, reducing collisions compared to static and\nmeta-learning baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5728\u7ebf\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u7684\u5730\u5f62\u7a81\u53d8\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u51fd\u6570\u7f16\u7801\u5668\u548c\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u9002\u5e94\u548c\u66f4\u5b89\u5168\u7684\u89c4\u5212\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u65e0\u7ed3\u6784\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u9700\u8981\u5feb\u901f\u9002\u5e94\u5730\u5f62\u7a81\u53d8\uff08\u5982\u7a81\u7136\u8f6c\u4e3a\u51b0\u9762\uff09\uff0c\u4ee5\u907f\u514d\u52a8\u529b\u5b66\u53d8\u5316\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u51fd\u6570\u7f16\u7801\u5668\u548c\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff0c\u5c06\u51fd\u6570\u7f16\u7801\u5668\u7cfb\u6570\u4f5c\u4e3a\u6f5c\u5728\u72b6\u6001\u5e76\u901a\u8fc7\u91cc\u7a0b\u8ba1\u6570\u636e\u6d41\u5b9e\u65f6\u66f4\u65b0\uff0c\u65e0\u9700\u57fa\u4e8e\u68af\u5ea6\u7684\u5185\u5faa\u73af\u66f4\u65b0\u3002", "result": "\u5728Van der Pol\u7cfb\u7edf\u3001Unity\u6a21\u62df\u5668\u548cClearpath Jackal\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u548c\u89c4\u5212\u6548\u679c\uff0c\u51cf\u5c11\u4e86\u78b0\u649e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u5730\u5f62\u53d8\u5316\uff0c\u4f18\u4e8e\u9759\u6001\u548c\u5143\u5b66\u4e60\u57fa\u7ebf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002"}}
{"id": "2509.12857", "pdf": "https://arxiv.org/pdf/2509.12857", "abs": "https://arxiv.org/abs/2509.12857", "authors": ["Yi Zhang", "Rui Guo", "Yonina C. Eldar"], "title": "Bayesian Signal Separation via Plug-and-Play Diffusion-Within-Gibbs Sampling", "categories": ["eess.SP", "G.3"], "comment": "5 pages, 1 figure, submitted to conference", "summary": "We propose a posterior sampling algorithm for the problem of estimating\nmultiple independent source signals from their noisy superposition. The\nproposed algorithm is a combination of Gibbs sampling method and plug-and-play\n(PnP) diffusion priors. Unlike most existing diffusion-model-based approaches\nfor signal separation, our method allows source priors to be learned separately\nand flexibly combined without retraining. Moreover, under the assumption of\nperfect diffusion model training, the proposed method provably produces samples\nfrom the posterior distribution. Experiments on the task of heartbeat\nextraction from mixtures with synthetic motion artifacts demonstrate the\nsuperior performance of our method over existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5409\u5e03\u65af\u91c7\u6837\u548cPnP\u6269\u6563\u5148\u9a8c\u7684\u540e\u9a8c\u91c7\u6837\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u53e0\u52a0\u4e2d\u4f30\u8ba1\u591a\u4e2a\u72ec\u7acb\u6e90\u4fe1\u53f7\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7075\u6d3b\u7ec4\u5408\u5148\u9a8c\u3002", "motivation": "\u89e3\u51b3\u4ece\u566a\u58f0\u53e0\u52a0\u4e2d\u4f30\u8ba1\u591a\u4e2a\u72ec\u7acb\u6e90\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u5e76\u5141\u8bb8\u7075\u6d3b\u7ec4\u5408\u5148\u9a8c\u3002", "method": "\u7ed3\u5408\u5409\u5e03\u65af\u91c7\u6837\u548cPnP\u6269\u6563\u5148\u9a8c\u7684\u540e\u9a8c\u91c7\u6837\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5fc3\u8df3\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.12531", "pdf": "https://arxiv.org/pdf/2509.12531", "abs": "https://arxiv.org/abs/2509.12531", "authors": ["Scott Jones", "Liyou Zhou", "Sebastian W. Pattinson"], "title": "Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40 (Primary) 93C85, 62L20 (Secondary)", "I.2.6; I.2.9; I.4.8; F.2.2"], "comment": null, "summary": "In visuomotor policy learning, the control policy for the robotic agent is\nderived directly from visual inputs. The typical approach, where a policy and\nvision encoder are trained jointly from scratch, generalizes poorly to novel\nvisual scene changes. Using pre-trained vision models (PVMs) to inform a policy\nnetwork improves robustness in model-free reinforcement learning (MFRL). Recent\ndevelopments in Model-based reinforcement learning (MBRL) suggest that MBRL is\nmore sample-efficient than MFRL. However, counterintuitively, existing work has\nfound PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness\nin MBRL, specifically on generalization under visual domain shifts. We show\nthat, in scenarios with severe shifts, PVMs perform much better than a baseline\nmodel trained from scratch. We further investigate the effects of varying\nlevels of fine-tuning of PVMs. Our results show that partial fine-tuning can\nmaintain the highest average task performance under the most extreme\ndistribution shifts. Our results demonstrate that PVMs are highly successful in\npromoting robustness in visual policy learning, providing compelling evidence\nfor their wider adoption in model-based robotic learning applications.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff08PVMs\uff09\u5728\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff08MBRL\uff09\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u7b56\u7565\u5728\u89c6\u89c9\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u90e8\u5206\u5fae\u8c03\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22PVMs\u5728MBRL\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83PVMs\u4e0e\u57fa\u7ebf\u6a21\u578b\u5728\u4e25\u91cd\u89c6\u89c9\u504f\u79fb\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u5fae\u8c03\u7a0b\u5ea6\u7684\u5f71\u54cd\u3002", "result": "PVMs\u5728\u6781\u7aef\u504f\u79fb\u4e0b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u90e8\u5206\u5fae\u8c03\u80fd\u4fdd\u6301\u6700\u9ad8\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "PVMs\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u7b56\u7565\u5b66\u4e60\u7684\u9c81\u68d2\u6027\uff0c\u503c\u5f97\u5728MBRL\u4e2d\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.12870", "pdf": "https://arxiv.org/pdf/2509.12870", "abs": "https://arxiv.org/abs/2509.12870", "authors": ["Ruichen Wang", "Zhikang Ni", "Pengzhou Wang", "Xiya Cao", "Zhi Li", "Bao Zhang"], "title": "Towards personalized, precise and survey-free environment recognition: AI-enhanced sensor fusion without pre-deployment", "categories": ["eess.SP"], "comment": "5 pages, 7 figures, conference", "summary": "Accurate and personalized environment recognition is essential for seamless\nindoor positioning and optimized connectivity, yet traditional fingerprinting\nrequires costly site surveys and lacks user-level adaptation. We present a\nsurvey-free, on-device sensor-fusion framework that builds a personalized,\nlightweight multi-source fingerprint (FP) database from pedestrian dead\nreckoning (PDR), WiFi/cellular, GNSS, and interaction time tags. Matching is\nperformed by an AI-enhanced dynamic time warping module (AIDTW) that aligns\nnoisy, asynchronous sequences. To turn perception into continually improving\nactions, a cloud-edge online Reinforcement Learning from Human Feedback (RLHF)\nloop aggregates desensitized summaries and human feedback in the cloud to\noptimize a policy via proximal policy optimization (PPO), and periodically\ndistills updates to devices. Across indoor/outdoor scenarios, our system\nreduces network-transition latency (measured by time-to-switch, TTS) by 32-65%\nin daily environments compared with conventional baselines, without\nsite-specific pre-deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u73b0\u573a\u8c03\u67e5\u7684\u4e2a\u6027\u5316\u4f20\u611f\u5668\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7AI\u589e\u5f3a\u7684\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7f51\u7edc\u5207\u6362\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u7cbe\u51c6\u73af\u5883\u8bc6\u522b\u548c\u7f51\u7edc\u4f18\u5316\u3002", "method": "\u5229\u7528PDR\u3001WiFi/\u8702\u7a9d\u3001GNSS\u548c\u4ea4\u4e92\u65f6\u95f4\u6807\u7b7e\u6784\u5efa\u8f7b\u91cf\u7ea7\u591a\u6e90\u6307\u7eb9\u6570\u636e\u5e93\uff0c\u91c7\u7528AIDTW\u6a21\u5757\u8fdb\u884c\u5e8f\u5217\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u4e91\u8fb9RLHF\u5faa\u73af\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\uff0c\u7f51\u7edc\u5207\u6362\u5ef6\u8fdf\u964d\u4f4e\u4e8632-65%\uff0c\u4e14\u65e0\u9700\u7279\u5b9a\u573a\u5730\u9884\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u73af\u5883\u8bc6\u522b\u548c\u7f51\u7edc\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.12562", "pdf": "https://arxiv.org/pdf/2509.12562", "abs": "https://arxiv.org/abs/2509.12562", "authors": ["Zhefei Gong", "Shangke Lyu", "Pengxiang Ding", "Wei Xiao", "Donglin Wang"], "title": "Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory.", "AI": {"tldr": "KORR\u662f\u4e00\u4e2a\u57fa\u4e8eKoopman\u7b97\u5b50\u7684\u6b8b\u5dee\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u52a8\u529b\u5b66\u5efa\u6a21\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4efb\u52a1\u548c\u9ad8\u7cbe\u5ea6\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u671f\u4efb\u52a1\u548c\u9ad8\u7cbe\u5ea6\u63a7\u5236\u4e2d\u5bb9\u6613\u56e0\u8bef\u5dee\u7d2f\u79ef\u800c\u53d7\u9650\uff0c\u73b0\u6709\u6b8b\u5dee\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5168\u5c40\u72b6\u6001\u6f14\u5316\u7684\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ebf\u6027\u65f6\u4e0d\u53d8\u7ed3\u6784\uff0c\u7ed3\u5408\u6b8b\u5dee\u7b56\u7565\u4fee\u6b63\uff0c\u5f62\u6210KORR\u6846\u67b6\uff0c\u5b9e\u73b0\u5168\u5c40\u5f15\u5bfc\u7684\u52a8\u4f5c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKORR\u5728\u957f\u671f\u7cbe\u7ec6\u5316\u673a\u5668\u4eba\u7ec4\u88c5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "KORR\u6846\u67b6\u5c55\u793a\u4e86Koopman\u7406\u8bba\u5728\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u4e0e\u5b66\u4e60\u65b9\u6cd5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.12954", "pdf": "https://arxiv.org/pdf/2509.12954", "abs": "https://arxiv.org/abs/2509.12954", "authors": ["Traian E. Abrudan", "Kartik Patel", "John Kimionis", "Tara Esmaeilbeig", "Eleftherios Kampianakis", "Sahan Damith Liyanaarachchi", "Michael Eggleston"], "title": "Next-Generation Backscatter Networks for Integrated Communications and RF Sensing", "categories": ["eess.SP", "H.1.1"], "comment": "15 pages + appendix", "summary": "This paper provides a comprehensive analysis and theoretical foundation for\nnext-generation backscatter networks that move beyond communication and\nintegrate RF location sensing capabilities. An end-to-end system model for\nwideband OFDM backscatter systems is derived, including detailed\ncharacterization of propagation channels, receiver chain impairments, RF tag\noperation, and unsynchronized network nodes. The theoretical system model is\nvalidated through experimental evaluation using actual hardware, demonstrating\nthe detailed model's accuracy. A practical bistatic ranging method that can\noperate with unsynchronized nodes is presented, along with the Cram\\'er-Rao\nLower Bound (CRLB) derived to show the achievable performance limits. Our\nexperimental results demonstrate the system performance for communication, RF\nsensing, and ranging, while also benchmarking against the derived theoretical\nlimits. This analytical framework and experimental validation establish\nfundamental understanding of distributed, unsynchronized backscatter systems\nfor future machine-type communication networks that are deployed in massive\nscale, while remaining energy-efficient.", "AI": {"tldr": "\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3\u53cd\u5411\u6563\u5c04\u7f51\u7edc\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u6790\u548c\u7406\u8bba\u57fa\u7840\uff0c\u8d85\u8d8a\u901a\u4fe1\u529f\u80fd\u5e76\u96c6\u6210\u4e86\u5c04\u9891\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u4e0b\u4e00\u4ee3\u53cd\u5411\u6563\u5c04\u7f51\u7edc\uff0c\u7ed3\u5408\u901a\u4fe1\u4e0e\u5c04\u9891\u5b9a\u4f4d\u529f\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u9ad8\u6548\u80fd\u673a\u5668\u7c7b\u901a\u4fe1\u7f51\u7edc\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u7aef\u5230\u7aef\u7684\u5e7f\u57dfOFDM\u53cd\u5411\u6563\u5c04\u7cfb\u7edf\u6a21\u578b\uff0c\u5305\u62ec\u4f20\u64ad\u4fe1\u9053\u3001\u63a5\u6536\u94fe\u7f3a\u9677\u3001RF\u6807\u7b7e\u64cd\u4f5c\u548c\u975e\u540c\u6b65\u7f51\u7edc\u8282\u70b9\u7684\u8be6\u7ec6\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u7cfb\u7edf\u5728\u901a\u4fe1\u3001\u5c04\u9891\u611f\u77e5\u548c\u6d4b\u8ddd\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u5206\u6790\u6846\u67b6\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u9ad8\u6548\u80fd\u673a\u5668\u7c7b\u901a\u4fe1\u7f51\u7edc\u63d0\u4f9b\u4e86\u5bf9\u5206\u5e03\u5f0f\u3001\u975e\u540c\u6b65\u53cd\u5411\u6563\u5c04\u7cfb\u7edf\u7684\u57fa\u672c\u7406\u89e3\u3002"}}
{"id": "2509.12594", "pdf": "https://arxiv.org/pdf/2509.12594", "abs": "https://arxiv.org/abs/2509.12594", "authors": ["Titong Jiang", "Xuefeng Jiang", "Yuan Ma", "Xin Wen", "Bailin Li", "Kun Zhan", "Peng Jia", "Yahui Liu", "Sheng Sun", "Xianpeng Lang"], "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Under review. Project site:\n  https://liauto-research.github.io/LightVLA", "summary": "We present LightVLA, a simple yet effective differentiable token pruning\nframework for vision-language-action (VLA) models. While VLA models have shown\nimpressive capability in executing real-world robotic tasks, their deployment\non resource-constrained platforms is often bottlenecked by the heavy\nattention-based computation over large sets of visual tokens. LightVLA\naddresses this challenge through adaptive, performance-driven pruning of visual\ntokens: It generates dynamic queries to evaluate visual token importance, and\nadopts Gumbel softmax to enable differentiable token selection. Through\nfine-tuning, LightVLA learns to preserve the most informative visual tokens\nwhile pruning tokens which do not contribute to task execution, thereby\nimproving efficiency and performance simultaneously. Notably, LightVLA requires\nno heuristic magic numbers and introduces no additional trainable parameters,\nmaking it compatible with modern inference frameworks. Experimental results\ndemonstrate that LightVLA outperforms different VLA models and existing token\npruning methods across diverse tasks on the LIBERO benchmark, achieving higher\nsuccess rates with substantially reduced computational overhead. Specifically,\nLightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%\nimprovement in task success rate. Meanwhile, we also investigate the learnable\nquery-based token pruning method LightVLA* with additional trainable\nparameters, which also achieves satisfactory performance. Our work reveals that\nas VLA pursues optimal performance, LightVLA spontaneously learns to prune\ntokens from a performance-driven perspective. To the best of our knowledge,\nLightVLA is the first work to apply adaptive visual token pruning to VLA tasks\nwith the collateral goals of efficiency and performance, marking a significant\nstep toward more efficient, powerful and practical real-time robotic systems.", "AI": {"tldr": "LightVLA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u5fae\u5206\u89c6\u89c9token\u88c1\u526a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30token\u91cd\u8981\u6027\u5e76\u5229\u7528Gumbel softmax\u5b9e\u73b0\u88c1\u526a\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u65f6\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u53c2\u6570\u7684\u81ea\u9002\u5e94token\u88c1\u526a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u52a8\u6001\u67e5\u8be2\u8bc4\u4f30\u89c6\u89c9token\u91cd\u8981\u6027\uff0c\u7ed3\u5408Gumbel softmax\u5b9e\u73b0\u53ef\u5fae\u5206\u88c1\u526a\uff0c\u901a\u8fc7\u5fae\u8c03\u4fdd\u7559\u5173\u952etoken\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightVLA\u964d\u4f4eFLOPs\u548c\u5ef6\u8fdf59.1%\u548c38.2%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53472.9%\u3002", "conclusion": "LightVLA\u9996\u6b21\u5728VLA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.12971", "pdf": "https://arxiv.org/pdf/2509.12971", "abs": "https://arxiv.org/abs/2509.12971", "authors": ["Wenyi Yan", "Zeyuan Li", "Lu Gan", "Honqing Liu", "Guoquan Li"], "title": "Difference-Based Recovery for Modulo Sampling: Tightened Bounds and Robustness Guarantees", "categories": ["eess.SP"], "comment": null, "summary": "Conventional analog-to-digital converters (ADCs) clip when signals exceed\ntheir input range. Modulo (unlimited) sampling overcomes this limitation by\nfolding the signal before digitization, but existing recovery methods are\neither computationally intensive or constrained by loose oversampling bounds\nthat demand high sampling rates. In addition, none account for sampling jitter,\nwhich is unavoidable in practice. This paper revisits difference-based recovery\nand establishes new theoretical and practical guarantees. In the noiseless\nsetting, we prove that arbitrarily high difference order reduces the sufficient\noversampling factor from $2\\pi e$ to $\\pi$, substantially tightening classical\nbounds. For fixed order $N$, we derive a noise-aware sampling condition that\nguarantees stable recovery. For second-order difference-based recovery ($N=2$),\nwe further extend the analysis to non-uniform sampling, proving robustness\nunder bounded jitter. An FPGA-based hardware prototype demonstrates reliable\nreconstruction with amplitude expansion up to $\\rho = 108$, confirming the\nfeasibility of high-performance unlimited sensing with a simple and robust\nrecovery pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u5dee\u5206\u7684\u6062\u590d\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edfADC\u5728\u4fe1\u53f7\u8d85\u8fc7\u8f93\u5165\u8303\u56f4\u65f6\u7684\u9650\u5236\uff0c\u7406\u8bba\u8bc1\u660e\u548c\u786c\u4ef6\u5b9e\u9a8c\u5747\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u4f20\u7edfADC\u5728\u4fe1\u53f7\u8d85\u8fc7\u8f93\u5165\u8303\u56f4\u65f6\u4f1a\u5931\u771f\uff0c\u73b0\u6709\u7684\u6a21\u6570\u91c7\u6837\u65b9\u6cd5\u867d\u80fd\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u6062\u590d\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u6216\u91c7\u6837\u7387\u8981\u6c42\u9ad8\uff0c\u4e14\u5ffd\u89c6\u4e86\u91c7\u6837\u6296\u52a8\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5dee\u5206\u7684\u6062\u590d\u65b9\u6cd5\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u66f4\u9ad8\u9636\u5dee\u5206\u53ef\u4ee5\u964d\u4f4e\u6240\u9700\u7684\u8fc7\u91c7\u6837\u56e0\u5b50\uff0c\u5e76\u5bf9\u566a\u58f0\u548c\u975e\u5747\u5300\u91c7\u6837\uff08\u6296\u52a8\uff09\u60c5\u51b5\u8fdb\u884c\u4e86\u6269\u5c55\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u9ad8\u9636\u5dee\u5206\u53ef\u5c06\u8fc7\u91c7\u6837\u56e0\u5b50\u4ece$2\\pi e$\u964d\u81f3$\\pi$\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u5e45\u5ea6\u6269\u5c55\u9ad8\u8fbe$\\rho = 108$\u65f6\u7684\u53ef\u9760\u91cd\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u4e14\u7a33\u5065\u7684\u6062\u590d\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u65e0\u9650\u5236\u4f20\u611f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12618", "pdf": "https://arxiv.org/pdf/2509.12618", "abs": "https://arxiv.org/abs/2509.12618", "authors": ["Zekai Zhang", "Weiye Zhu", "Hewei Pan", "Xiangchen Wang", "Rongtao Xu", "Xing Sun", "Feng Zheng"], "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The Vision-and-Language Navigation (VLN) task requires an agent to follow\nnatural language instructions and navigate through complex environments.\nExisting MLLM-based VLN methods primarily rely on imitation learning (IL) and\noften use DAgger for post-training to mitigate covariate shift. While\neffective, these approaches incur substantial data collection and training\ncosts. Reinforcement learning (RL) offers a promising alternative. However,\nprior VLN RL methods lack dynamic interaction with the environment and depend\non expert trajectories for reward shaping, rather than engaging in open-ended\nactive exploration. This restricts the agent's ability to discover diverse and\nplausible navigation routes. To address these limitations, we propose\nActiveVLN, a VLN framework that explicitly enables active exploration through\nmulti-turn RL. In the first stage, a small fraction of expert trajectories is\nused for IL to bootstrap the agent. In the second stage, the agent iteratively\npredicts and executes actions, automatically collects diverse trajectories, and\noptimizes multiple rollouts via the GRPO objective. To further improve RL\nefficiency, we introduce a dynamic early-stopping strategy to prune long-tail\nor likely failed trajectories, along with additional engineering optimizations.\nExperiments show that ActiveVLN achieves the largest performance gains over IL\nbaselines compared to both DAgger-based and prior RL-based post-training\nmethods, while reaching competitive performance with state-of-the-art\napproaches despite using a smaller model. Code and data will be released soon.", "AI": {"tldr": "ActiveVLN\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5b9e\u73b0\u4e3b\u52a8\u63a2\u7d22\u7684VLN\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u548c\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u7684VLN\u65b9\u6cd5\u6570\u636e\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709RL\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u4ea4\u4e92\u548c\u4e3b\u52a8\u63a2\u7d22\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u8def\u5f84\u7684\u591a\u6837\u6027\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u5148\u7528\u5c11\u91cf\u4e13\u5bb6\u8f68\u8ff9\u8fdb\u884cIL\uff0c\u518d\u901a\u8fc7\u591a\u8f6eRL\u4e3b\u52a8\u6536\u96c6\u591a\u6837\u8f68\u8ff9\u5e76\u4f18\u5316\u3002\u91c7\u7528GRPO\u76ee\u6807\u548c\u52a8\u6001\u63d0\u524d\u505c\u6b62\u7b56\u7565\u3002", "result": "ActiveVLN\u5728\u6027\u80fd\u63d0\u5347\u4e0a\u4f18\u4e8eIL\u548c\u73b0\u6709RL\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u66f4\u5c0f\u3002", "conclusion": "ActiveVLN\u4e3aVLN\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e3b\u52a8\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13004", "pdf": "https://arxiv.org/pdf/2509.13004", "abs": "https://arxiv.org/abs/2509.13004", "authors": ["Jona Cappelle", "Jarne Van Mulders", "Sarah Goossens", "Thomas Reher", "Liesbet Van der Perre", "Lieven De Strycker", "Bram Van de Poel", "Gilles Callebaut"], "title": "RF-Powered Batteryless Plant Movement Sensor for Precision Agriculture", "categories": ["eess.SP"], "comment": null, "summary": "Precision agriculture demands non-invasive, energy-efficient, and sustainable\nplant monitoring solutions. In this work, we present the design and\nimplementation of a lightweight, batteryless plant movement sensor powered\nsolely by RF energy. This sensor targets Controlled Environment Agriculture\n(CEA) and utilizes inertial measurements units (IMUs) to monitor leaf motion,\nwhich correlates with plant physiological responses to environmental stress. By\neliminating the battery, we reduce the ecological footprint, weight, and\nmaintenance requirements, transitioning from lifetime-based to operation-based\nenergy storage. Our design minimizes circuit complexity while enabling\nflexible, adaptive readout scheduling based on energy availability and sensor\ndata. We detail the energy requirements, RF power transfer considerations,\nintegration constraints, and outline future directions, including multi-antenna\npower delivery and networked sensor synchronization.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u7535\u6c60\u7684\u8f7b\u91cf\u7ea7\u690d\u7269\u8fd0\u52a8\u4f20\u611f\u5668\uff0c\u901a\u8fc7RF\u80fd\u91cf\u4f9b\u7535\uff0c\u7528\u4e8e\u76d1\u6d4b\u690d\u7269\u751f\u7406\u53cd\u5e94\u3002", "motivation": "\u6ee1\u8db3\u7cbe\u51c6\u519c\u4e1a\u5bf9\u975e\u4fb5\u5165\u3001\u8282\u80fd\u548c\u53ef\u6301\u7eed\u690d\u7269\u76d1\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528IMU\u76d1\u6d4b\u53f6\u7247\u8fd0\u52a8\uff0cRF\u80fd\u91cf\u4f9b\u7535\uff0c\u7b80\u5316\u7535\u8def\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8c03\u5ea6\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u51fa\u65e0\u7535\u6c60\u4f20\u611f\u5668\uff0c\u51cf\u5c11\u751f\u6001\u8db3\u8ff9\u548c\u7ef4\u62a4\u9700\u6c42\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u591a\u5929\u7ebf\u4f9b\u7535\u548c\u7f51\u7edc\u5316\u4f20\u611f\u5668\u540c\u6b65\u3002"}}
{"id": "2509.12620", "pdf": "https://arxiv.org/pdf/2509.12620", "abs": "https://arxiv.org/abs/2509.12620", "authors": ["Yikai Chen", "Zhi Zheng", "Jin Wang", "Bingye He", "Xiangyu Xu", "Jialu Zhang", "Huan Yu", "Guodong Lu"], "title": "PerchMobi^3: A Multi-Modal Robot with Power-Reuse Quad-Fan Mechanism for Air-Ground-Wall Locomotion", "categories": ["cs.RO"], "comment": "7 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Achieving seamless integration of aerial flight, ground driving, and wall\nclimbing within a single robotic platform remains a major challenge, as\nexisting designs often rely on additional adhesion actuators that increase\ncomplexity, reduce efficiency, and compromise reliability. To address these\nlimitations, we present PerchMobi^3, a quad-fan, negative-pressure,\nair-ground-wall robot that implements a propulsion-adhesion power-reuse\nmechanism. By repurposing four ducted fans to simultaneously provide aerial\nthrust and negative-pressure adhesion, and integrating them with four actively\ndriven wheels, PerchMobi^3 eliminates dedicated pumps while maintaining a\nlightweight and compact design. To the best of our knowledge, this is the first\nquad-fan prototype to demonstrate functional power reuse for multi-modal\nlocomotion. A modeling and control framework enables coordinated operation\nacross ground, wall, and aerial domains with fan-assisted transitions. The\nfeasibility of the design is validated through a comprehensive set of\nexperiments covering ground driving, payload-assisted wall climbing, aerial\nflight, and cross-mode transitions, demonstrating robust adaptability across\nlocomotion scenarios. These results highlight the potential of PerchMobi^3 as a\nnovel design paradigm for multi-modal robotic mobility, paving the way for\nfuture extensions toward autonomous and application-oriented deployment.", "AI": {"tldr": "PerchMobi\u00b3 \u662f\u4e00\u79cd\u96c6\u98de\u884c\u3001\u5730\u9762\u884c\u9a76\u548c\u5899\u9762\u6500\u722c\u4e8e\u4e00\u4f53\u7684\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u63a8\u8fdb-\u5438\u9644\u52a8\u529b\u590d\u7528\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u8bbe\u8ba1\uff0c\u65e0\u9700\u989d\u5916\u5438\u9644\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bbe\u8ba1\u4e2d\u56e0\u989d\u5916\u5438\u9644\u8bbe\u5907\u5bfc\u81f4\u7684\u590d\u6742\u6027\u589e\u52a0\u3001\u6548\u7387\u964d\u4f4e\u548c\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56db\u98ce\u6247\u8bbe\u8ba1\uff0c\u540c\u65f6\u63d0\u4f9b\u7a7a\u4e2d\u63a8\u529b\u548c\u8d1f\u538b\u5438\u9644\uff0c\u5e76\u914d\u5907\u56db\u4e2a\u9a71\u52a8\u8f6e\uff0c\u5f00\u53d1\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6\u4ee5\u534f\u8c03\u591a\u6a21\u5f0f\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff08\u5730\u9762\u884c\u9a76\u3001\u5899\u9762\u6500\u722c\u3001\u7a7a\u4e2d\u98de\u884c\u53ca\u6a21\u5f0f\u8f6c\u6362\uff09\u4e2d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "PerchMobi\u00b3 \u4e3a\u591a\u6a21\u6001\u673a\u5668\u4eba\u79fb\u52a8\u6027\u63d0\u4f9b\u4e86\u65b0\u8303\u672c\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u548c\u5e94\u7528\u5bfc\u5411\u7684\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.13030", "pdf": "https://arxiv.org/pdf/2509.13030", "abs": "https://arxiv.org/abs/2509.13030", "authors": ["Ge Chen", "Panqi Chen", "Lei Cheng"], "title": "Deep Tensor Learning for Reliable Channel Charting from Incomplete and Noisy Measurements", "categories": ["eess.SP"], "comment": null, "summary": "Channel charting has emerged as a powerful tool for user equipment\nlocalization and wireless environment sensing. Its efficacy lies in mapping\nhigh-dimensional channel data into low-dimensional features that preserve the\nrelative similarities of the original data. However, existing channel charting\nmethods are largely developed using simulated or indoor measurements, often\nassuming clean and complete channel data across all frequency bands. In\ncontrast, real-world channels collected from base stations are typically\nincomplete due to frequency hopping and are significantly noisy, particularly\nat cell edges. These challenging conditions greatly degrade the performance of\ncurrent methods. To address this, we propose a deep tensor learning method that\nleverages the inherent tensor structure of wireless channels to effectively\nextract informative while low-dimensional features (i.e., channel charts) from\nnoisy and incomplete measurements. Experimental results demonstrate the\nreliability and effectiveness of the proposed approach in these challenging\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f20\u91cf\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u7684\u65e0\u7ebf\u4fe1\u9053\u6570\u636e\u4e2d\u63d0\u53d6\u4f4e\u7ef4\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4fe1\u9053\u56fe\u6280\u672f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4fe1\u9053\u56fe\u6280\u672f\u591a\u57fa\u4e8e\u6a21\u62df\u6216\u5ba4\u5185\u6d4b\u91cf\uff0c\u5047\u8bbe\u6570\u636e\u5e72\u51c0\u4e14\u5b8c\u6574\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u7684\u4fe1\u9053\u6570\u636e\u5f80\u5f80\u4e0d\u5b8c\u6574\u4e14\u566a\u58f0\u8f83\u5927\uff0c\u7279\u522b\u662f\u5728\u5c0f\u533a\u8fb9\u7f18\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f20\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u7ebf\u4fe1\u9053\u7684\u5f20\u91cf\u7ed3\u6784\uff0c\u4ece\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u7684\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u4f4e\u7ef4\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5f20\u91cf\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u4fe1\u9053\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u63d0\u5347\u4e86\u4fe1\u9053\u56fe\u7684\u6027\u80fd\u3002"}}
{"id": "2509.12674", "pdf": "https://arxiv.org/pdf/2509.12674", "abs": "https://arxiv.org/abs/2509.12674", "authors": ["Anna Johansson", "Daniel Lindmark", "Viktor Wiberg", "Martin Servin"], "title": "Safety filtering of robotic manipulation under environment uncertainty: a computational approach", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "Robotic manipulation in dynamic and unstructured environments requires safety\nmechanisms that exploit what is known and what is uncertain about the world.\nExisting safety filters often assume full observability, limiting their\napplicability in real-world tasks. We propose a physics-based safety filtering\nscheme that leverages high-fidelity simulation to assess control policies under\nuncertainty in world parameters. The method combines dense rollout with nominal\nparameters and parallelizable sparse re-evaluation at critical\nstate-transitions, quantified through generalized factors of safety for stable\ngrasping and actuator limits, and targeted uncertainty reduction through\nprobing actions. We demonstrate the approach in a simulated bimanual\nmanipulation task with uncertain object mass and friction, showing that unsafe\ntrajectories can be identified and filtered efficiently. Our results highlight\nphysics-based sparse safety evaluation as a scalable strategy for safe robotic\nmanipulation under uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u5b89\u5168\u8fc7\u6ee4\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8bc4\u4f30\u63a7\u5236\u7b56\u7565\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u7a00\u758f\u91cd\u8bc4\u4f30\u6765\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u8fc7\u6ee4\u5668\u901a\u5e38\u5047\u8bbe\u5b8c\u5168\u53ef\u89c2\u6d4b\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5bc6\u96c6\u6eda\u52a8\u548c\u7a00\u758f\u91cd\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u8bc4\u4f30\u63a7\u5236\u7b56\u7565\uff0c\u91cf\u5316\u7a33\u5b9a\u6027\u6293\u53d6\u548c\u6267\u884c\u5668\u9650\u5236\u7684\u5b89\u5168\u7cfb\u6570\uff0c\u5e76\u4f7f\u7528\u63a2\u6d4b\u884c\u52a8\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u6a21\u62df\u7684\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u8bc6\u522b\u5e76\u8fc7\u6ee4\u4e86\u4e0d\u5b89\u5168\u8f68\u8ff9\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7269\u7406\u7684\u7a00\u758f\u5b89\u5168\u8bc4\u4f30\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u9002\u5408\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u5b89\u5168\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2509.13071", "pdf": "https://arxiv.org/pdf/2509.13071", "abs": "https://arxiv.org/abs/2509.13071", "authors": ["Yuan Liu", "Linlong Wu", "Xuesong Cai", "M. R. Bhavani Shankar"], "title": "Scatterer Localization Using Multi-Bounce Paths", "categories": ["eess.SP"], "comment": "Presented in ISCS25", "summary": "Indoor sensing is challenging because of the multi-bounce effect, spherical\nwavefront, and spatial nonstationarity (SNS) of the near-field effect. This\npaper addresses radio-based environment sensing considering these issues.\nSpecifically, graph theory (GT) is used to model the multi-bounce propagation\nof the near field. In this manner, indoor reflectors/scatterers are modeled as\nvertices in a propagation graph, the multi-bounce paths are modeled by the\nedges linking the vertices. Besides, the coupled multipath parameters in the\nnear field, i.e., range and angles, are denoted directly by the coordinates of\nvertices. Then, the space-alternating generalized expectation-maximization\n(SAGE) algorithm is adapted to the proposed Graph theory-based dictionary-aided\nMulti-bounce SAGE (GM-SAGE), where the searching parameters including range and\nangle of departure/arrival (AoD/AoA) are transformed to the coordinates of\nscatterers in the graph. The proposed algorithm is validated through\nmeasurement-calibrated ray tracing (RT) in a complex indoor office. The results\ndemonstrate that the proposed GM-SAGE can deal with multi-bounce channels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7406\u8bba\u7684\u591a\u53cd\u5c04\u5ba4\u5185\u73af\u5883\u611f\u77e5\u65b9\u6cd5\uff0c\u5229\u7528GM-SAGE\u7b97\u6cd5\u5904\u7406\u591a\u53cd\u5c04\u4fe1\u9053\u7684\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u5ba4\u5185\u4f20\u611f\u56e0\u591a\u53cd\u5c04\u6548\u5e94\u3001\u7403\u9762\u6ce2\u524d\u548c\u8fd1\u573a\u6548\u5e94\u4e2d\u7684\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u800c\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u5efa\u6a21\u548c\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56fe\u7406\u8bba\u5efa\u6a21\u591a\u53cd\u5c04\u4f20\u64ad\uff0c\u5c06\u53cd\u5c04\u4f53/\u6563\u5c04\u4f53\u5efa\u6a21\u4e3a\u56fe\u4e2d\u7684\u9876\u70b9\uff0c\u591a\u53cd\u5c04\u8def\u5f84\u4e3a\u8fb9\u3002\u7ed3\u5408SAGE\u7b97\u6cd5\u63d0\u51faGM-SAGE\uff0c\u901a\u8fc7\u9876\u70b9\u5750\u6807\u8868\u793a\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u590d\u6742\u5ba4\u5185\u529e\u516c\u5ba4\u7684\u6d4b\u91cf\u6821\u51c6\u5c04\u7ebf\u8ffd\u8e2a\u9a8c\u8bc1\uff0cGM-SAGE\u80fd\u6709\u6548\u5904\u7406\u591a\u53cd\u5c04\u4fe1\u9053\u3002", "conclusion": "GM-SAGE\u65b9\u6cd5\u4e3a\u591a\u53cd\u5c04\u4fe1\u9053\u5efa\u6a21\u548c\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12702", "pdf": "https://arxiv.org/pdf/2509.12702", "abs": "https://arxiv.org/abs/2509.12702", "authors": ["Hongrui Zhao", "Xunlan Zhou", "Boris Ivanovic", "Negar Mehr"], "title": "UDON: Uncertainty-weighted Distributed Optimization for Multi-Robot Neural Implicit Mapping under Extreme Communication Constraints", "categories": ["cs.RO"], "comment": null, "summary": "Multi-robot mapping with neural implicit representations enables the compact\nreconstruction of complex environments. However, it demands robustness against\ncommunication challenges like packet loss and limited bandwidth. While prior\nworks have introduced various mechanisms to mitigate communication disruptions,\nperformance degradation still occurs under extremely low communication success\nrates. This paper presents UDON, a real-time multi-agent neural implicit\nmapping framework that introduces a novel uncertainty-weighted distributed\noptimization to achieve high-quality mapping under severe communication\ndeterioration. The uncertainty weighting prioritizes more reliable portions of\nthe map, while the distributed optimization isolates and penalizes mapping\ndisagreement between individual pairs of communicating agents. We conduct\nextensive experiments on standard benchmark datasets and real-world robot\nhardware. We demonstrate that UDON significantly outperforms existing\nbaselines, maintaining high-fidelity reconstructions and consistent scene\nrepresentations even under extreme communication degradation (as low as 1%\nsuccess rate).", "AI": {"tldr": "UDON\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5206\u5e03\u5f0f\u4f18\u5316\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u795e\u7ecf\u9690\u5f0f\u5efa\u56fe\uff0c\u5728\u6781\u4f4e\u901a\u4fe1\u6210\u529f\u7387\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u8d28\u91cf\u5efa\u56fe\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5efa\u56fe\u4e2d\u901a\u4fe1\u6311\u6218\uff08\u5982\u4e22\u5305\u548c\u5e26\u5bbd\u9650\u5236\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faUDON\u6846\u67b6\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5206\u5e03\u5f0f\u4f18\u5316\uff0c\u4f18\u5148\u5904\u7406\u53ef\u9760\u5730\u56fe\u90e8\u5206\u5e76\u60e9\u7f5a\u901a\u4fe1\u4ee3\u7406\u95f4\u7684\u5efa\u56fe\u5206\u6b67\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u5b9e\u9645\u786c\u4ef6\u4e0a\u9a8c\u8bc1\uff0cUDON\u5728\u6781\u4f4e\u901a\u4fe1\u6210\u529f\u7387\uff08\u59821%\uff09\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UDON\u5728\u4e25\u91cd\u901a\u4fe1\u9000\u5316\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5efa\u56fe\u548c\u4e00\u81f4\u7684\u573a\u666f\u8868\u793a\u3002"}}
{"id": "2509.13287", "pdf": "https://arxiv.org/pdf/2509.13287", "abs": "https://arxiv.org/abs/2509.13287", "authors": ["Nandan Sriranga", "Haodong Yang", "Pramod K. Varshney"], "title": "Transmitter Subspace-Aware Target Detection in Two-Channel Passive Radars with Inter-Receiver Collaboration", "categories": ["eess.SP"], "comment": null, "summary": "We address target detection in a single Delay-Doppler cell using spatially\ndistributed two-channel passive radars. An unknown illuminator of opportunity\n(IO) is assumed to emit a waveform lying in a known low-dimensional subspace\n(e.g., OFDM). Each receiver transforms its reference and surveillance signals\nonto the IO subspace after noise-whitening, to obtain cross-correlation (CC)\nmeasurements. To save bandwidth, receivers collaboratively exchange and\nlinearly combine the CC output, and only a subset transmits them to a fusion\ncenter (FC) over a multiple-access channel (MAC). Collaboration weights are\ndesigned using the moments of the FC measurement to enhance detection\nperformance.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5206\u5e03\u5f0f\u53cc\u901a\u9053\u88ab\u52a8\u96f7\u8fbe\u5728\u5355\u4e2a\u5ef6\u8fdf\u591a\u666e\u52d2\u5355\u5143\u4e2d\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5728\u672a\u77e5\u673a\u4f1a\u53d1\u5c04\u6e90\uff08IO\uff09\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u6ce2\u5f62\uff08\u5982OFDM\uff09\u5b9e\u73b0\u9ad8\u6548\u76ee\u6807\u68c0\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u63a5\u6536\u673a\u5c06\u53c2\u8003\u548c\u76d1\u89c6\u4fe1\u53f7\u8f6c\u6362\u5230IO\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u566a\u58f0\u767d\u5316\uff0c\u5f97\u5230\u4e92\u76f8\u5173\u6d4b\u91cf\uff1b\u901a\u8fc7\u534f\u4f5c\u4ea4\u6362\u548c\u7ebf\u6027\u7ec4\u5408\u8282\u7701\u5e26\u5bbd\uff0c\u4ec5\u90e8\u5206\u6570\u636e\u4f20\u8f93\u5230\u878d\u5408\u4e2d\u5fc3\u3002", "result": "\u901a\u8fc7\u8bbe\u8ba1\u534f\u4f5c\u6743\u91cd\u4f18\u5316\u878d\u5408\u4e2d\u5fc3\u6d4b\u91cf\uff0c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5206\u5e03\u5f0f\u88ab\u52a8\u96f7\u8fbe\u534f\u4f5c\u65b9\u6cd5\u5728\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.12714", "pdf": "https://arxiv.org/pdf/2509.12714", "abs": "https://arxiv.org/abs/2509.12714", "authors": ["Kit-Wa Sou", "Junhao Gong", "Shoujie Li", "Chuqiao Lyu", "Ziwu Song", "Shilong Mu", "Wenbo Ding"], "title": "Moir\u00e9Tac: A Dual-Mode Visuotactile Sensor for Multidimensional Perception Using Moir\u00e9 Pattern Amplification", "categories": ["cs.RO", "eess.SP"], "comment": null, "summary": "Visuotactile sensors typically employ sparse marker arrays that limit spatial\nresolution and lack clear analytical force-to-image relationships. To solve\nthis problem, we present \\textbf{Moir\\'eTac}, a dual-mode sensor that generates\ndense interference patterns via overlapping micro-gratings within a transparent\narchitecture. When two gratings overlap with misalignment, they create moir\\'e\npatterns that amplify microscopic deformations. The design preserves optical\nclarity for vision tasks while producing continuous moir\\'e fields for tactile\nsensing, enabling simultaneous 6-axis force/torque measurement, contact\nlocalization, and visual perception. We combine physics-based features\n(brightness, phase gradient, orientation, and period) from moir\\'e patterns\nwith deep spatial features. These are mapped to 6-axis force/torque\nmeasurements, enabling interpretable regression through end-to-end learning.\nExperimental results demonstrate three capabilities: force/torque measurement\nwith R^2 > 0.98 across tested axes; sensitivity tuning through geometric\nparameters (threefold gain adjustment); and vision functionality for object\nclassification despite moir\\'e overlay. Finally, we integrate the sensor into a\nrobotic arm for cap removal with coordinated force and torque control,\nvalidating its potential for dexterous manipulation.", "AI": {"tldr": "Moir\u00e9Tac\u662f\u4e00\u79cd\u53cc\u6a21\u5f0f\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u5fae\u5149\u6805\u91cd\u53e0\u4ea7\u751f\u5bc6\u96c6\u5e72\u6d89\u56fe\u6848\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u76846\u8f74\u529b/\u626d\u77e9\u6d4b\u91cf\u3001\u63a5\u89e6\u5b9a\u4f4d\u548c\u89c6\u89c9\u611f\u77e5\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u4e14\u7f3a\u4e4f\u6e05\u6670\u7684\u529b-\u56fe\u50cf\u5173\u7cfb\u3002", "method": "\u5229\u7528\u91cd\u53e0\u5fae\u5149\u6805\u4ea7\u751f\u83ab\u5c14\u6761\u7eb9\uff0c\u653e\u5927\u5fae\u89c2\u5f62\u53d8\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u529b/\u626d\u77e9\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u9ad8\u7cbe\u5ea6\u529b/\u626d\u77e9\u6d4b\u91cf\uff08R\u00b2 > 0.98\uff09\u3001\u7075\u654f\u5ea6\u53ef\u8c03\u53ca\u652f\u6301\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u3002", "conclusion": "Moir\u00e9Tac\u5728\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.12176", "pdf": "https://arxiv.org/pdf/2508.12176", "abs": "https://arxiv.org/abs/2508.12176", "authors": ["Zhiwei Zheng", "Dongyin Hu", "Mingmin Zhao"], "title": "Scalable RF Simulation in Generative 4D Worlds", "categories": ["cs.CV", "cs.AI", "eess.SP"], "comment": null, "summary": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving\nalternative to vision-based methods for indoor perception tasks. However,\ncollecting high-quality RF data in dynamic and diverse indoor environments\nremains a major challenge. To address this, we introduce WaveVerse, a\nprompt-based, scalable framework that simulates realistic RF signals from\ngenerated indoor scenes with human motions. WaveVerse introduces a\nlanguage-guided 4D world generator, which includes a state-aware causal\ntransformer for human motion generation conditioned on spatial constraints and\ntexts, and a phase-coherent ray tracing simulator that enables the simulation\nof accurate and coherent RF signals. Experiments demonstrate the effectiveness\nof our approach in conditioned human motion generation and highlight how phase\ncoherence is applied to beamforming and respiration monitoring. We further\npresent two case studies in ML-based high-resolution imaging and human activity\nrecognition, demonstrating that WaveVerse not only enables data generation for\nRF imaging for the first time, but also consistently achieves performance gain\nin both data-limited and data-adequate scenarios.", "AI": {"tldr": "WaveVerse\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5ba4\u5185\u573a\u666f\u548c\u4eba\u4f53\u52a8\u4f5c\u6765\u6a21\u62df\u771f\u5b9e\u5c04\u9891\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u591a\u6837\u73af\u5883\u4e2d\u9ad8\u8d28\u91cf\u5c04\u9891\u6570\u636e\u6536\u96c6\u7684\u96be\u9898\u3002", "motivation": "\u5c04\u9891\uff08RF\uff09\u4f20\u611f\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5ba4\u5185\u611f\u77e5\u65b9\u6cd5\uff0c\u4f46\u5728\u52a8\u6001\u591a\u6837\u7684\u73af\u5883\u4e2d\u6536\u96c6\u9ad8\u8d28\u91cf\u6570\u636e\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "WaveVerse\u7ed3\u5408\u4e86\u8bed\u8a00\u5f15\u5bfc\u76844D\u4e16\u754c\u751f\u6210\u5668\uff08\u542b\u72b6\u6001\u611f\u77e5\u56e0\u679c\u53d8\u6362\u5668\uff09\u548c\u76f8\u4f4d\u76f8\u5e72\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\uff0c\u751f\u6210\u7a7a\u95f4\u7ea6\u675f\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684\u4eba\u4f53\u52a8\u4f5c\u53ca\u5c04\u9891\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u663e\u793aWaveVerse\u5728\u6761\u4ef6\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u548c\u76f8\u4f4d\u76f8\u5e72\u6027\uff08\u5e94\u7528\u4e8e\u6ce2\u675f\u6210\u5f62\u548c\u547c\u5438\u76d1\u6d4b\uff09\u65b9\u9762\u6709\u6548\uff0c\u5e76\u5728ML\u6210\u50cf\u548c\u6d3b\u52a8\u8bc6\u522b\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "WaveVerse\u9996\u6b21\u5b9e\u73b0\u4e86\u5c04\u9891\u6210\u50cf\u6570\u636e\u751f\u6210\uff0c\u4e14\u5728\u6570\u636e\u6709\u9650\u6216\u5145\u8db3\u573a\u666f\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.12723", "pdf": "https://arxiv.org/pdf/2509.12723", "abs": "https://arxiv.org/abs/2509.12723", "authors": ["Kai Zhang", "Eric Lucet", "Julien Alexandre Dit Sandretto", "Shoubin Chen", "David Filait"], "title": "NAMOUnc: Navigation Among Movable Obstacles with Decision Making on Uncertainty Interval", "categories": ["cs.RO"], "comment": "11 pages, ICINCO2025", "summary": "Navigation among movable obstacles (NAMO) is a critical task in robotics,\noften challenged by real-world uncertainties such as observation noise, model\napproximations, action failures, and partial observability. Existing solutions\nfrequently assume ideal conditions, leading to suboptimal or risky decisions.\nThis paper introduces NAMOUnc, a novel framework designed to address these\nuncertainties by integrating them into the decision-making process. We first\nestimate them and compare the corresponding time cost intervals for removing\nand bypassing obstacles, optimizing both the success rate and time efficiency,\nensuring safer and more efficient navigation. We validate our method through\nextensive simulations and real-world experiments, demonstrating significant\nimprovements over existing NAMO frameworks. More details can be found in our\nwebsite: https://kai-zhang-er.github.io/namo-uncertainty/", "AI": {"tldr": "NAMOUnc\u6846\u67b6\u9488\u5bf9\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u89c2\u6d4b\u566a\u58f0\u3001\u52a8\u4f5c\u5931\u8d25\u7b49\uff09\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u63d0\u9ad8\u6210\u529f\u7387\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u73b0\u6709NAMO\u89e3\u51b3\u65b9\u6848\u5047\u8bbe\u7406\u60f3\u6761\u4ef6\uff0c\u5bfc\u81f4\u51b3\u7b56\u6b20\u4f73\u6216\u98ce\u9669\u9ad8\uff0c\u9700\u5904\u7406\u73b0\u5b9e\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u5e76\u6bd4\u8f83\u79fb\u9664\u548c\u7ed5\u8fc7\u969c\u788d\u7684\u65f6\u95f4\u6210\u672c\u533a\u95f4\uff0c\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86NAMOUnc\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "NAMOUnc\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u548c\u9ad8\u6548\u7684\u5bfc\u822a\u3002"}}
{"id": "2509.12691", "pdf": "https://arxiv.org/pdf/2509.12691", "abs": "https://arxiv.org/abs/2509.12691", "authors": ["Sri Satish Krishna Chaitanya Bulusu", "Mikko Sillanp\u00e4\u00e4"], "title": "Power-Dominance in Estimation Theory: A Third Pathological Axis", "categories": ["stat.ME", "eess.SP", "math.ST", "stat.ML", "stat.TH"], "comment": "5 pages, 1 figure", "summary": "This paper introduces a novel framework for estimation theory by introducing\na second-order diagnostic for estimator design. While classical analysis\nfocuses on the bias-variance trade-off, we present a more foundational\nconstraint. This result is model-agnostic, domain-agnostic, and is valid for\nboth parametric and non-parametric problems, Bayesian and frequentist\nframeworks. We propose to classify the estimators into three primary power\nregimes. We theoretically establish that any estimator operating in the\n`power-dominant regime' incurs an unavoidable mean-squared error penalty,\nmaking it structurally prone to sub-optimal performance. We propose a\n`safe-zone law' and make this diagnostic intuitive through two safe-zone maps.\nOne map is a geometric visualization analogous to a receiver operating\ncharacteristic curve for estimators, and the other map shows that the safe-zone\ncorresponds to a bounded optimization problem, while the forbidden\n`power-dominant zone' represents an unbounded optimization landscape. This\nframework reframes estimator design as a path optimization problem, providing\nnew theoretical underpinnings for regularization and inspiring novel design\nphilosophies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f30\u8ba1\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e8c\u9636\u8bca\u65ad\u5de5\u5177\u5bf9\u4f30\u8ba1\u5668\u8bbe\u8ba1\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u9ad8\u529f\u7387\u533a\u57df\u5de5\u4f5c\u7684\u4f30\u8ba1\u5668\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002", "motivation": "\u4f20\u7edf\u4f30\u8ba1\u7406\u8bba\u4e3b\u8981\u5173\u6ce8\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u800c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u66f4\u57fa\u7840\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u578b\u3001\u9886\u57df\u53ca\u7edf\u8ba1\u6846\u67b6\uff08\u8d1d\u53f6\u65af\u548c\u9891\u7387\u4e3b\u4e49\uff09\u3002", "method": "\u901a\u8fc7\u5c06\u4f30\u8ba1\u5668\u5206\u4e3a\u4e09\u79cd\u4e3b\u8981\u529f\u7387\u533a\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5b89\u5168\u533a\u6cd5\u5219\u201d\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u76f4\u89c2\u7684\u5b89\u5168\u533a\u56fe\u5c55\u793a\u8fd9\u4e00\u5206\u7c7b\u3002\u5176\u4e2d\u4e00\u4e2a\u7c7b\u4f3cROC\u66f2\u7ebf\uff0c\u53e6\u4e00\u4e2a\u663e\u793a\u4f18\u5316\u95ee\u9898\u7684\u6709\u754c\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u9ad8\u529f\u7387\u533a\u57df\u7684\u4f30\u8ba1\u5668\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u800c\u5b89\u5168\u533a\u5bf9\u5e94\u6709\u754c\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u6846\u67b6\u5c06\u4f30\u8ba1\u5668\u8bbe\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8def\u5f84\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u542f\u53d1\u4e86\u65b0\u7684\u8bbe\u8ba1\u7406\u5ff5\u3002"}}
{"id": "2509.12739", "pdf": "https://arxiv.org/pdf/2509.12739", "abs": "https://arxiv.org/abs/2509.12739", "authors": ["Trung Kien La", "Eric Guiffo Kaigom"], "title": "Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the\n  10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on Robotics\n  July 15-18, 2025 || Paris, France for publication under a Creative Commons\n  Licence CC-BY-NC-ND", "summary": "In this work, deep neural networks made up of multiple hidden Long Short-Term\nMemory (LSTM) and Feedforward layers are trained to predict the thermal\nbehavior of the joint motors of robot manipulators. A model-free and scalable\napproach is adopted. It accommodates complexity and uncertainty challenges\nstemming from the derivation, identification, and validation of a large number\nof parameters of an approximation model that is hardly available. To this end,\nsensed joint torques are collected and processed to foresee the thermal\nbehavior of joint motors. Promising prediction results of the machine learning\nbased capture of the temperature dynamics of joint motors of a redundant robot\nwith seven joints are presented.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4f7f\u7528\u591a\u5c42LSTM\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u65e0\u6a21\u578b\u3001\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u9884\u6d4b\u673a\u5668\u4eba\u5173\u8282\u7535\u673a\u7684\u70ed\u884c\u4e3a\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u53c2\u6570\u63a8\u5bfc\u3001\u8bc6\u522b\u548c\u9a8c\u8bc1\uff0c\u590d\u6742\u4e14\u4e0d\u786e\u5b9a\u6027\u9ad8\u3002\u56e0\u6b64\uff0c\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u5c42LSTM\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u611f\u6d4b\u7684\u5173\u8282\u529b\u77e9\u6570\u636e\u9884\u6d4b\u7535\u673a\u6e29\u5ea6\u53d8\u5316\u3002", "result": "\u5728\u4e03\u81ea\u7531\u5ea6\u5197\u4f59\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6e29\u5ea6\u52a8\u6001\u9884\u6d4b\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u6709\u6548\u907f\u514d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5173\u8282\u70ed\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.12740", "pdf": "https://arxiv.org/pdf/2509.12740", "abs": "https://arxiv.org/abs/2509.12740", "authors": ["Eric Guiffo Kaigom"], "title": "Deep Generative and Discriminative Digital Twin endowed with Variational Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of Physical Robots in Industry 6.0 and Society 6.0", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the to\n  the 10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on\n  Robotics July 15-18, 2025 || Paris, France for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "Robots are unrelentingly used to achieve operational efficiency in Industry\n4.0 along with symbiotic and sustainable assistance for the work-force in\nIndustry 5.0. As resilience, robustness, and well-being are required in\nanti-fragile manufacturing and human-centric societal tasks, an autonomous\nanticipation and adaption to thermal saturation and burns due to motors\noverheating become instrumental for human safety and robot availability. Robots\nare thereby expected to self-sustain their performance and deliver user\nexperience, in addition to communicating their capability to other agents in\nadvance to ensure fully automated thermally feasible tasks, and prolong their\nlifetime without human intervention. However, the traditional robot shutdown,\nwhen facing an imminent thermal saturation, inhibits productivity in factories\nand comfort in the society, while cooling strategies are hard to implement\nafter the robot acquisition. In this work, smart digital twins endowed with\ngenerative AI, i.e., variational autoencoders, are leveraged to manage\nthermally anomalous and generate uncritical robot states. The notion of thermal\ndifficulty is derived from the reconstruction error of variational\nautoencoders. A robot can use this score to predict, anticipate, and share the\nthermal feasibility of desired motion profiles to meet requirements from\nemerging applications in Industry 6.0 and Society 6.0.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u751f\u6210\u5f0fAI\u7684\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff08\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\u6765\u9884\u6d4b\u548c\u7ba1\u7406\u673a\u5668\u4eba\u8fc7\u70ed\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u5de5\u4e1a\u548c\u793e\u4f1a\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8fc7\u70ed\u65f6\u7684\u505c\u673a\u7b56\u7565\u4f1a\u964d\u4f4e\u751f\u4ea7\u6548\u7387\u548c\u8212\u9002\u5ea6\uff0c\u800c\u51b7\u5374\u7b56\u7565\u96be\u4ee5\u5b9e\u65bd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u4e3b\u9884\u6d4b\u548c\u9002\u5e94\u70ed\u9971\u548c\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u4eba\u673a\u5b89\u5168\u548c\u673a\u5668\u4eba\u53ef\u7528\u6027\u3002", "method": "\u91c7\u7528\u667a\u80fd\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u751f\u6210\u65e0\u98ce\u9669\u7684\u673a\u5668\u4eba\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u91cd\u5efa\u8bef\u5dee\u63a8\u5bfc\u201c\u70ed\u96be\u5ea6\u201d\u8bc4\u5206\uff0c\u9884\u6d4b\u548c\u5171\u4eab\u8fd0\u52a8\u5256\u9762\u7684\u70ed\u53ef\u884c\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u4e3b\u9884\u6d4b\u548c\u7ba1\u7406\u70ed\u5f02\u5e38\uff0c\u907f\u514d\u505c\u673a\uff0c\u540c\u65f6\u6ee1\u8db3\u5de5\u4e1a6.0\u548c\u793e\u4f1a6.0\u7684\u9700\u6c42\u3002", "conclusion": "\u667a\u80fd\u6570\u5b57\u5b6a\u751f\u548c\u751f\u6210\u5f0fAI\u7684\u7ed3\u5408\u4e3a\u673a\u5668\u4eba\u70ed\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2509.12920", "pdf": "https://arxiv.org/pdf/2509.12920", "abs": "https://arxiv.org/abs/2509.12920", "authors": ["Huseyin Karaca", "Suleyman Serdar Kozat"], "title": "Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "We propose a soft gradient boosting framework for sequential regression that\nembeds a learnable linear feature transform within the boosting procedure. At\neach boosting iteration, we train a soft decision tree and learn a linear input\nfeature transform Q together. This approach is particularly advantageous in\nhigh-dimensional, data-scarce scenarios, as it discovers the most relevant\ninput representations while boosting. We demonstrate, using both synthetic and\nreal-world datasets, that our method effectively and efficiently increases the\nperformance by an end-to-end optimization of feature selection/transform and\nboosting while avoiding overfitting. We also extend our algorithm to\ndifferentiable non-linear transforms if overfitting is not a problem. To\nsupport reproducibility and future work, we share our code publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u7528\u4e8e\u5e8f\u5217\u56de\u5f52\uff0c\u901a\u8fc7\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u53ef\u5b66\u4e60\u7684\u7ebf\u6027\u7279\u5f81\u53d8\u6362\uff0c\u63d0\u5347\u4e86\u5728\u9ad8\u7ef4\u548c\u5c11\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u548c\u5c11\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u6700\u76f8\u5173\u7684\u8f93\u5165\u8868\u793a\u6765\u63d0\u5347\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u5728\u6bcf\u4e2a\u63d0\u5347\u8fed\u4ee3\u4e2d\uff0c\u8bad\u7ec3\u4e00\u4e2a\u8f6f\u51b3\u7b56\u6811\u5e76\u5b66\u4e60\u7ebf\u6027\u7279\u5f81\u53d8\u6362 Q\uff0c\u540c\u65f6\u4f18\u5316\u7279\u5f81\u9009\u62e9\u548c\u63d0\u5347\u8fc7\u7a0b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u907f\u514d\u4e86\u8fc7\u62df\u5408\u3002", "conclusion": "\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u7279\u5f81\u53d8\u6362\u4e0e\u63d0\u5347\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.12741", "pdf": "https://arxiv.org/pdf/2509.12741", "abs": "https://arxiv.org/abs/2509.12741", "authors": ["Alexis Yihong Hao", "Yufei Wang", "Navin Sriram Ravie", "Bharath Hegde", "David Held", "Zackory Erickson"], "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "CoRL 2025", "summary": "Robot-assisted dressing has the potential to significantly improve the lives\nof individuals with mobility impairments. To ensure an effective and\ncomfortable dressing experience, the robot must be able to handle challenging\ndeformable garments, apply appropriate forces, and adapt to limb movements\nthroughout the dressing process. Prior work often makes simplifying assumptions\n-- such as static human limbs during dressing -- which limits real-world\napplicability. In this work, we develop a robot-assisted dressing system\ncapable of handling partial observations with visual occlusions, as well as\nrobustly adapting to arm motions during the dressing process. Given a policy\ntrained in simulation with partial observations, we propose a method to\nfine-tune it in the real world using a small amount of data and multi-modal\nfeedback from vision and force sensing, to further improve the policy's\nadaptability to arm motions and enhance safety. We evaluate our method in\nsimulation with simplified articulated human meshes and in a real world human\nstudy with 12 participants across 264 dressing trials. Our policy successfully\ndresses two long-sleeve everyday garments onto the participants while being\nadaptive to various kinds of arm motions, and greatly outperforms prior\nbaselines in terms of task completion and user feedback. Video are available at\nhttps://dressing-motion.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u8863\u7cfb\u7edf\uff0c\u80fd\u591f\u9002\u5e94\u624b\u81c2\u8fd0\u52a8\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u53cd\u9988\u63d0\u5347\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u63d0\u5347\u884c\u52a8\u4e0d\u4fbf\u8005\u7684\u751f\u6d3b\u8d28\u91cf\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u9759\u6001\u5047\u8bbe\u5bf9\u5b9e\u9645\u5e94\u7528\u7684\u5c40\u9650\u3002", "method": "\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\u5e76\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u548c\u591a\u6a21\u6001\u53cd\u9988\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6210\u529f\u5b8c\u6210264\u6b21\u7a7f\u8863\u8bd5\u9a8c\uff0c\u9002\u5e94\u591a\u79cd\u624b\u81c2\u8fd0\u52a8\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.13166", "pdf": "https://arxiv.org/pdf/2509.13166", "abs": "https://arxiv.org/abs/2509.13166", "authors": ["Filippo Fabiani", "Andrea Simonetto"], "title": "Concentration inequalities for semidefinite least squares based on data", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "math.OC"], "comment": null, "summary": "We study data-driven least squares (LS) problems with semidefinite (SD)\nconstraints and derive finite-sample guarantees on the spectrum of their\noptimal solutions when these constraints are relaxed. In particular, we provide\na high confidence bound allowing one to solve a simpler program in place of the\nfull SDLS problem, while ensuring that the eigenvalues of the resulting\nsolution are $\\varepsilon$-close of those enforced by the SD constraints. The\ndeveloped certificate, which consistently shrinks as the number of data\nincreases, turns out to be easy-to-compute, distribution-free, and only\nrequires independent and identically distributed samples. Moreover, when the\nSDLS is used to learn an unknown quadratic function, we establish bounds on the\nerror between a gradient descent iterate minimizing the surrogate cost obtained\nwith no SD constraints and the true minimizer.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5177\u6709\u534a\u5b9a\uff08SD\uff09\u7ea6\u675f\u7684\u6570\u636e\u9a71\u52a8\u6700\u5c0f\u4e8c\u4e58\uff08LS\uff09\u95ee\u9898\uff0c\u5e76\u5728\u653e\u677e\u7ea6\u675f\u65f6\u63d0\u4f9b\u4e86\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5728\u534a\u5b9a\u7ea6\u675f\u4e0b\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u8bc1\u89e3\u7684\u8c31\u6027\u8d28\u63a5\u8fd1\u7ea6\u675f\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7f6e\u4fe1\u5ea6\u754c\u9650\uff0c\u5141\u8bb8\u7528\u66f4\u7b80\u5355\u7684\u7a0b\u5e8f\u66ff\u4ee3\u5b8c\u6574\u7684\u534a\u5b9a\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6613\u4e8e\u8ba1\u7b97\u3001\u5206\u5e03\u65e0\u5173\u4e14\u4ec5\u9700\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u786e\u4fdd\u89e3\u7684\u7279\u5f81\u503c\u63a5\u8fd1\u7ea6\u675f\u8981\u6c42\u3002", "conclusion": "\u4e3a\u5b66\u4e60\u672a\u77e5\u4e8c\u6b21\u51fd\u6570\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\u9650\uff0c\u5c55\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3\u4e0e\u771f\u5b9e\u6781\u5c0f\u503c\u4e4b\u95f4\u7684\u8bef\u5dee\u3002"}}
{"id": "2509.12747", "pdf": "https://arxiv.org/pdf/2509.12747", "abs": "https://arxiv.org/abs/2509.12747", "authors": ["Botao He", "Amir Hossein Shahidzadeh", "Yu Chen", "Jiayi Wu", "Tianrui Guan", "Guofei Chen", "Howie Choset", "Dinesh Manocha", "Glen Chou", "Cornelia Fermuller", "Yiannis Aloimonos"], "title": "NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts", "categories": ["cs.RO"], "comment": null, "summary": "This paper explores traversability estimation for robot navigation. A key\nbottleneck in traversability estimation lies in efficiently achieving reliable\nand robust predictions while accurately encoding both geometric and semantic\ninformation across diverse environments. We introduce Navigation via Mixture of\nExperts (NAVMOE), a hierarchical and modular approach for traversability\nestimation and local navigation. NAVMOE combines multiple specialized models\nfor specific terrain types, each of which can be either a classical model-based\nor a learning-based approach that predicts traversability for specific terrain\ntypes. NAVMOE dynamically weights the contributions of different models based\non the input environment through a gating network. Overall, our approach offers\nthree advantages: First, NAVMOE enables traversability estimation to adaptively\nleverage specialized approaches for different terrains, which enhances\ngeneralization across diverse and unseen environments. Second, our approach\nsignificantly improves efficiency with negligible cost of solution quality by\nintroducing a training-free lazy gating mechanism, which is designed to\nminimize the number of activated experts during inference. Third, our approach\nuses a two-stage training strategy that enables the training for the gating\nnetworks within the hybrid MoE method that contains nondifferentiable modules.\nExtensive experiments show that NAVMOE delivers a better efficiency and\nperformance balance than any individual expert or full ensemble across\ndifferent domains, improving cross- domain generalization and reducing average\ncomputational cost by 81.2% via lazy gating, with less than a 2% loss in path\nquality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNAVMOE\u7684\u5206\u5c42\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u53ef\u7a7f\u884c\u6027\u4f30\u8ba1\u3002\u5b83\u7ed3\u5408\u4e86\u591a\u79cd\u9488\u5bf9\u6027\u5730\u5f62\u7684\u4e13\u4e1a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u63d0\u5347\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u53ef\u7a7f\u884c\u6027\u4f30\u8ba1\u4e2d\u9ad8\u6548\u53ef\u9760\u9884\u6d4b\u7684\u74f6\u9888\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u51c6\u786e\u7f16\u7801\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08NAVMOE\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ecf\u5178\u6a21\u578b\u548c\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u52a0\u6743\u3002\u5f15\u5165\u514d\u8bad\u7ec3\u7684\u61d2\u60f0\u95e8\u63a7\u673a\u5236\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "NAVMOE\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u4e00\u4e13\u5bb6\u6216\u5b8c\u6574\u96c6\u6210\u65b9\u6cd5\uff0c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e81.2%\uff0c\u8def\u5f84\u8d28\u91cf\u635f\u5931\u5c0f\u4e8e2%\u3002", "conclusion": "NAVMOE\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u548c\u4e13\u4e1a\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u53ef\u7a7f\u884c\u6027\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u9ad8\u8d28\u91cf\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13178", "pdf": "https://arxiv.org/pdf/2509.13178", "abs": "https://arxiv.org/abs/2509.13178", "authors": ["Claudio Battiloro", "Andrea Cavallo", "Elvin Isufi"], "title": "CoVariance Filters and Neural Networks over Hilbert Spaces", "categories": ["cs.LG", "eess.SP"], "comment": "6 pages, 3 figures", "summary": "CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical\ncovariance matrix of signals defined over finite-dimensional Hilbert spaces,\nmotivated by robustness and transferability properties. Yet, little is known\nabout how these arguments extend to infinite-dimensional Hilbert spaces. In\nthis work, we take a first step by introducing a novel convolutional learning\nframework for signals defined over infinite-dimensional Hilbert spaces,\ncentered on the (empirical) covariance operator. We constructively define\nHilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)\nas stacks of HVF filterbanks with nonlinear activations. We propose a\nprincipled discretization procedure, and we prove that empirical HVFs can\nrecover the Functional PCA (FPCA) of the filtered signals. We then describe the\nversatility of our framework with examples ranging from multivariate\nreal-valued functions to reproducing kernel Hilbert spaces. Finally, we\nvalidate HVNs on both synthetic and real-world time-series classification\ntasks, showing robust performance compared to MLP and FPCA-based classifiers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Hilbert\u534f\u65b9\u5dee\u7f51\u7edc(HVN)\uff0c\u7528\u4e8e\u5904\u7406\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684CoVariance Neural Networks (VNNs)\u5728\u6709\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5728\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86Hilbert coVariance Filters (HVFs)\u548cHVN\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u534f\u65b9\u5dee\u7b97\u5b50\u7684\u64cd\u4f5c\u8fdb\u884c\u5377\u79ef\u5b66\u4e60\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u79bb\u6563\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHVN\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u591a\u5c42\u611f\u77e5\u673a(MLP)\u548cFunctional PCA(FPCA)\u7684\u6027\u80fd\u3002", "conclusion": "HVN\u4e3a\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.12754", "pdf": "https://arxiv.org/pdf/2509.12754", "abs": "https://arxiv.org/abs/2509.12754", "authors": ["Saki Hashimoto", "Shoichi Hasegawa", "Tomochika Ishikawa", "Akira Taniguchi", "Yoshinobu Hagiwara", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "Robots operating in domestic and office environments must understand object\nownership to correctly execute instructions such as ``Bring me my cup.''\nHowever, ownership cannot be reliably inferred from visual features alone. To\naddress this gap, we propose Active Ownership Learning (ActOwL), a framework\nthat enables robots to actively generate and ask ownership-related questions to\nusers. ActOwL employs a probabilistic generative model to select questions that\nmaximize information gain, thereby acquiring ownership knowledge efficiently to\nimprove learning efficiency. Additionally, by leveraging commonsense knowledge\nfrom Large Language Models (LLM), objects are pre-classified as either shared\nor owned, and only owned objects are targeted for questioning. Through\nexperiments in a simulated home environment and a real-world laboratory\nsetting, ActOwL achieved significantly higher ownership clustering accuracy\nwith fewer questions than baseline methods. These findings demonstrate the\neffectiveness of combining active inference with LLM-guided commonsense\nreasoning, advancing the capability of robots to acquire ownership knowledge\nfor practical and socially appropriate task execution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aActOwL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u63d0\u95ee\u548cLLM\u8f85\u52a9\u7684\u5e38\u8bc6\u63a8\u7406\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u9ad8\u6548\u5b66\u4e60\u7269\u54c1\u6240\u6709\u6743\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u548c\u529e\u516c\u73af\u5883\u4e2d\u9700\u8981\u7406\u89e3\u7269\u54c1\u6240\u6709\u6743\u4ee5\u6b63\u786e\u6267\u884c\u6307\u4ee4\uff0c\u4f46\u4ec5\u9760\u89c6\u89c9\u7279\u5f81\u65e0\u6cd5\u53ef\u9760\u63a8\u65ad\u6240\u6709\u6743\u3002", "method": "\u63d0\u51fa\u4e86Active Ownership Learning (ActOwL)\u6846\u67b6\uff0c\u5229\u7528\u6982\u7387\u751f\u6210\u6a21\u578b\u4e3b\u52a8\u751f\u6210\u95ee\u9898\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\uff0c\u5e76\u7ed3\u5408LLM\u7684\u5e38\u8bc6\u77e5\u8bc6\u9884\u5206\u7c7b\u7269\u54c1\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cActOwL\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u6743\u805a\u7c7b\u51c6\u786e\u6027\uff0c\u4e14\u63d0\u95ee\u6b21\u6570\u66f4\u5c11\u3002", "conclusion": "\u7ed3\u5408\u4e3b\u52a8\u63a8\u65ad\u548cLLM\u5f15\u5bfc\u7684\u5e38\u8bc6\u63a8\u7406\uff0c\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u83b7\u53d6\u6240\u6709\u6743\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u5b9e\u7528\u4e14\u7b26\u5408\u793e\u4ea4\u89c4\u8303\u3002"}}
{"id": "2509.12776", "pdf": "https://arxiv.org/pdf/2509.12776", "abs": "https://arxiv.org/abs/2509.12776", "authors": ["Renjie Wang", "Shangke Lyu", "Xin Lang", "Wei Xiao", "Donglin Wang"], "title": "Integrating Trajectory Optimization and Reinforcement Learning for Quadrupedal Jumping with Terrain-Adaptive Landing", "categories": ["cs.RO"], "comment": "Accepted by IROS 2025", "summary": "Jumping constitutes an essential component of quadruped robots' locomotion\ncapabilities, which includes dynamic take-off and adaptive landing. Existing\nquadrupedal jumping studies mainly focused on the stance and flight phase by\nassuming a flat landing ground, which is impractical in many real world cases.\nThis work proposes a safe landing framework that achieves adaptive landing on\nrough terrains by combining Trajectory Optimization (TO) and Reinforcement\nLearning (RL) together. The RL agent learns to track the reference motion\ngenerated by TO in the environments with rough terrains. To enable the learning\nof compliant landing skills on challenging terrains, a reward relaxation\nstrategy is synthesized to encourage exploration during landing recovery\nperiod. Extensive experiments validate the accurate tracking and safe landing\nskills benefiting from our proposed method in various scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5728\u7c97\u7cd9\u5730\u5f62\u4e0a\u5b9e\u73b0\u81ea\u9002\u5e94\u5b89\u5168\u7740\u9646\u3002\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u5e73\u5766\u7740\u9646\u5730\u9762\uff0c\u800c\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u98de\u884c\u548c\u7ad9\u7acb\u9636\u6bb5\uff0c\u4e14\u5047\u8bbe\u5e73\u5766\u7740\u9646\u5730\u9762\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5b9e\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u89e3\u51b3\u7c97\u7cd9\u5730\u5f62\u4e0b\u7684\u5b89\u5168\u7740\u9646\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\uff08TO\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0cRL\u667a\u80fd\u4f53\u5b66\u4e60\u5728\u7c97\u7cd9\u5730\u5f62\u4e2d\u8ffd\u8e2aTO\u751f\u6210\u7684\u53c2\u8003\u8fd0\u52a8\u3002\u91c7\u7528\u5956\u52b1\u677e\u5f1b\u7b56\u7565\u9f13\u52b1\u7740\u9646\u6062\u590d\u671f\u7684\u63a2\u7d22\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u51c6\u786e\u8ffd\u8e2a\u548c\u5b89\u5168\u7740\u9646\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u7c97\u7cd9\u5730\u5f62\u4e0a\u7684\u81ea\u9002\u5e94\u5b89\u5168\u7740\u9646\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.12813", "pdf": "https://arxiv.org/pdf/2509.12813", "abs": "https://arxiv.org/abs/2509.12813", "authors": ["Bowen Ye", "Junyue Huang", "Yang Liu", "Xiaozhen Qiao", "Xiang Yin"], "title": "Bridging Perception and Planning: Towards End-to-End Planning for Signal Temporal Logic Tasks", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We investigate the task and motion planning problem for Signal Temporal Logic\n(STL) specifications in robotics. Existing STL methods rely on pre-defined maps\nor mobility representations, which are ineffective in unstructured real-world\nenvironments. We propose the \\emph{Structured-MoE STL Planner}\n(\\textbf{S-MSP}), a differentiable framework that maps synchronized multi-view\ncamera observations and an STL specification directly to a feasible trajectory.\nS-MSP integrates STL constraints within a unified pipeline, trained with a\ncomposite loss that combines trajectory reconstruction and STL robustness. A\n\\emph{structure-aware} Mixture-of-Experts (MoE) model enables horizon-aware\nspecialization by projecting sub-tasks into temporally anchored embeddings. We\nevaluate S-MSP using a high-fidelity simulation of factory-logistics scenarios\nwith temporally constrained tasks. Experiments show that S-MSP outperforms\nsingle-expert baselines in STL satisfaction and trajectory feasibility. A\nrule-based \\emph{safety filter} at inference improves physical executability\nwithout compromising logical correctness, showcasing the practicality of the\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aS-MSP\u7684\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u76f8\u673a\u89c2\u6d4b\u548cSTL\u89c4\u8303\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709STL\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u5730\u56fe\u6216\u79fb\u52a8\u8868\u793a\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73b0\u5b9e\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "S-MSP\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684Mixture-of-Experts (MoE)\u6a21\u578b\u548c\u7ec4\u5408\u635f\u5931\u51fd\u6570\uff08\u8f68\u8ff9\u91cd\u5efa\u548cSTL\u9c81\u68d2\u6027\uff09\u5b9e\u73b0STL\u7ea6\u675f\u7684\u7edf\u4e00\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS-MSP\u5728STL\u6ee1\u8db3\u5ea6\u548c\u8f68\u8ff9\u53ef\u884c\u6027\u4e0a\u4f18\u4e8e\u5355\u4e00\u4e13\u5bb6\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u5b89\u5168\u8fc7\u6ee4\u5668\u63d0\u5347\u7269\u7406\u53ef\u6267\u884c\u6027\u3002", "conclusion": "S-MSP\u5728\u5de5\u5382\u7269\u6d41\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\uff0c\u517c\u987e\u903b\u8f91\u6b63\u786e\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\u3002"}}
{"id": "2509.12838", "pdf": "https://arxiv.org/pdf/2509.12838", "abs": "https://arxiv.org/abs/2509.12838", "authors": ["Kento Murata", "Shoichi Hasegawa", "Tomochika Ishikawa", "Yoshinobu Hagiwara", "Akira Taniguchi", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "It is crucial to efficiently execute instructions such as \"Find an apple and\na banana\" or \"Get ready for a field trip,\" which require searching for multiple\nobjects or understanding context-dependent commands. This study addresses the\nchallenging problem of determining which robot should be assigned to which part\nof a task when each robot possesses different situational on-site\nknowledge-specifically, spatial concepts learned from the area designated to it\nby the user. We propose a task planning framework that leverages large language\nmodels (LLMs) and spatial concepts to decompose natural language instructions\ninto subtasks and allocate them to multiple robots. We designed a novel\nfew-shot prompting strategy that enables LLMs to infer required objects from\nambiguous commands and decompose them into appropriate subtasks. In our\nexperiments, the proposed method achieved 47/50 successful assignments,\noutperforming random (28/50) and commonsense-based assignment (26/50).\nFurthermore, we conducted qualitative evaluations using two actual mobile\nmanipulators. The results demonstrated that our framework could handle\ninstructions, including those involving ad hoc categories such as \"Get ready\nfor a field trip,\" by successfully performing task decomposition, assignment,\nsequential planning, and execution.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u7a7a\u95f4\u6982\u5ff5\u7684\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u5206\u914d\u7ed9\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5982\u4f55\u6839\u636e\u6bcf\u4e2a\u673a\u5668\u4eba\u7684\u60c5\u5883\u77e5\u8bc6\u5408\u7406\u5206\u914d\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u6a21\u7cca\u6216\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6307\u4ee4\u65f6\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u7684\u65b0\u7b56\u7565\uff0c\u4f7fLLMs\u80fd\u591f\u4ece\u6a21\u7cca\u6307\u4ee4\u4e2d\u63a8\u65ad\u6240\u9700\u5bf9\u8c61\u5e76\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5206\u914d\u4e0a\u7684\u6210\u529f\u7387\u4e3a47/50\uff0c\u4f18\u4e8e\u968f\u673a\u5206\u914d\uff0828/50\uff09\u548c\u57fa\u4e8e\u5e38\u8bc6\u7684\u5206\u914d\uff0826/50\uff09\u3002\u5b9e\u9645\u673a\u5668\u4eba\u6d4b\u8bd5\u4e5f\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u6307\u4ee4\uff0c\u6210\u529f\u5b8c\u6210\u4efb\u52a1\u5206\u89e3\u3001\u5206\u914d\u3001\u89c4\u5212\u548c\u6267\u884c\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.12846", "pdf": "https://arxiv.org/pdf/2509.12846", "abs": "https://arxiv.org/abs/2509.12846", "authors": ["Junlin Song", "Antoine Richard", "Miguel Olivares-Mendez"], "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual-inertial fusion is crucial for a large amount of intelligent and\nautonomous applications, such as robot navigation and augmented reality. To\nbootstrap and achieve optimal state estimation, the spatial-temporal\ndisplacements between IMU and cameras must be calibrated in advance. Most\nexisting calibration methods adopt continuous-time state representation, more\nspecifically the B-spline. Despite these methods achieve precise\nspatial-temporal calibration, they suffer from high computational cost caused\nby continuous-time state representation. To this end, we propose a novel and\nextremely efficient calibration method that unleashes the power of\ndiscrete-time state representation. Moreover, the weakness of discrete-time\nstate representation in temporal calibration is tackled in this paper. With the\nincreasing production of drones, cellphones and other visual-inertial\nplatforms, if one million devices need calibration around the world, saving one\nminute for the calibration of each device means saving 2083 work days in total.\nTo benefit both the research and industry communities, our code will be\nopen-source.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u79bb\u6563\u65f6\u95f4\u72b6\u6001\u8868\u793a\u7684\u89c6\u89c9-\u60ef\u6027\u6821\u51c6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002", "motivation": "\u89c6\u89c9-\u60ef\u6027\u6821\u51c6\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u79bb\u6563\u65f6\u95f4\u72b6\u6001\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u4e86\u79bb\u6563\u65b9\u6cd5\u5728\u65f6\u95f4\u6821\u51c6\u4e0a\u7684\u4e0d\u8db3\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6821\u51c6\u6548\u7387\uff0c\u8282\u7701\u4e86\u5927\u91cf\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bbe\u5907\u7684\u6821\u51c6\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u4e86\u89c6\u89c9-\u60ef\u6027\u6821\u51c6\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5bf9\u7814\u7a76\u548c\u5de5\u4e1a\u793e\u533a\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.12851", "pdf": "https://arxiv.org/pdf/2509.12851", "abs": "https://arxiv.org/abs/2509.12851", "authors": ["Antoine L\u00e9nat", "Olivier Cheminat", "Damien Chablat", "Camilo Charron"], "title": "A Novel Skill Modeling Approach: Integrating Vergnaud's Scheme with Cognitive Architectures", "categories": ["cs.RO"], "comment": null, "summary": "Human-machine interaction is increasingly important in industry, and this\ntrend will only intensify with the rise of Industry 5.0. Human operators have\nskills that need to be adapted when using machines to achieve the best results.\nIt is crucial to highlight the operator's skills and understand how they use\nand adapt them [18]. A rigorous description of these skills is necessary to\ncompare performance with and without robot assistance. Predicate logic, used by\nVergnaud within Piaget's scheme concept, offers a promising approach. However,\nthis theory doesn't account for cognitive system constraints, such as the\ntiming of actions, the limitation of cognitive resources, the parallelization\nof tasks, or the activation of automatic gestures contrary to optimal\nknowledge. Integrating these constraints is essential for representing agent\nskills understanding skill transfer between biological and mechanical\nstructures. Cognitive architectures models [2] address these needs by\ndescribing cognitive structure and can be combined with the scheme for mutual\nbenefit. Welding provides a relevant case study, as it highlights the\nchallenges faced by operators, even highly skilled ones. Welding's complexity\nstems from the need for constant skill adaptation to variable parameters like\npart position and process. This adaptation is crucial, as weld quality, a key\nfactor, is only assessed afterward via destructive testing. Thus, the welder is\nconfronted with a complex perception-decision-action cycle, where the\nevaluation of the impact of his actions is delayed and where errors are\ndefinitive. This dynamic underscores the importance of understanding and\nmodeling the skills of operators.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u673a\u4ea4\u4e92\u5728\u5de5\u4e1a\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728Industry 5.0\u65f6\u4ee3\u4e0b\uff0c\u5f3a\u8c03\u4e86\u64cd\u4f5c\u5458\u6280\u80fd\u7684\u9002\u5e94\u6027\u4e0e\u5efa\u6a21\u9700\u6c42\u3002", "motivation": "\u968f\u7740Industry 5.0\u7684\u5174\u8d77\uff0c\u4eba\u673a\u4ea4\u4e92\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u64cd\u4f5c\u5458\u5728\u4f7f\u7528\u673a\u5668\u65f6\u9700\u8981\u8c03\u6574\u5176\u6280\u80fd\u4ee5\u8fbe\u5230\u6700\u4f73\u6548\u679c\uff0c\u56e0\u6b64\u4e9f\u9700\u5bf9\u8fd9\u4e9b\u6280\u80fd\u8fdb\u884c\u5efa\u6a21\u548c\u7406\u89e3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408Vergnaud\u7684\u8c13\u8bcd\u903b\u8f91\uff08\u57fa\u4e8ePiaget\u7684\u65b9\u6848\u6982\u5ff5\uff09\u4e0e\u8ba4\u77e5\u67b6\u6784\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u8ba4\u77e5\u7cfb\u7edf\u7ea6\u675f\uff08\u5982\u65f6\u95f4\u9650\u5236\u3001\u8ba4\u77e5\u8d44\u6e90\u5206\u914d\u7b49\uff09\u672a\u88ab\u539f\u7406\u8bba\u6db5\u76d6\u7684\u95ee\u9898\u3002\u5e76\u4ee5\u710a\u63a5\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u64cd\u4f5c\u5458\u6280\u80fd\u9002\u5e94\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u710a\u63a5\u64cd\u4f5c\u5458\u9700\u8981\u52a8\u6001\u8c03\u6574\u6280\u80fd\u4ee5\u5e94\u5bf9\u53ef\u53d8\u53c2\u6570\uff0c\u4f46\u7531\u4e8e\u710a\u63a5\u8d28\u91cf\u53ea\u80fd\u4e8b\u540e\u8bc4\u4f30\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u53cd\u9988\u548c\u4e0d\u53ef\u9006\u9519\u8bef\u7684\u7279\u70b9\u7a81\u51fa\u4e86\u6280\u80fd\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\uff0c\u7ed3\u5408\u8c13\u8bcd\u903b\u8f91\u4e0e\u8ba4\u77e5\u67b6\u6784\u6a21\u578b\u53ef\u4ee5\u66f4\u5168\u9762\u5730\u63cf\u8ff0\u64cd\u4f5c\u5458\u6280\u80fd\uff0c\u5e76\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6280\u80fd\u8f6c\u79fb\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u710a\u63a5\u7b49\u590d\u6742\u5de5\u4e1a\u573a\u666f\u4e2d\u3002"}}
{"id": "2509.12858", "pdf": "https://arxiv.org/pdf/2509.12858", "abs": "https://arxiv.org/abs/2509.12858", "authors": ["Yidan Lu", "Rurui Yang", "Qiran Kou", "Mengting Chen", "Tao Fan", "Peter Cui", "Yinzhao Dong", "Peng Lu"], "title": "Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning has produced remarkable advances in humanoid\nlocomotion, yet a fundamental dilemma persists for real-world deployment:\npolicies must choose between the robustness of reactive proprioceptive control\nor the proactivity of complex, fragile perception-driven systems. This paper\nresolves this dilemma by introducing a paradigm that imbues a purely\nproprioceptive policy with proactive capabilities, achieving the foresight of\nperception without its deployment-time costs. Our core contribution is a\ncontrastive learning framework that compels the actor's latent state to encode\nprivileged environmental information from simulation. Crucially, this\n``distilled awareness\" empowers an adaptive gait clock, allowing the policy to\nproactively adjust its rhythm based on an inferred understanding of the\nterrain. This synergy resolves the classic trade-off between rigid, clocked\ngaits and unstable clock-free policies. We validate our approach with zero-shot\nsim-to-real transfer to a full-sized humanoid, demonstrating highly robust\nlocomotion over challenging terrains, including 30 cm high steps and 26.5{\\deg}\nslopes, proving the effectiveness of our method. Website:\nhttps://lu-yidan.github.io/cra-loco.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8d4b\u4e88\u7eaf\u672c\u4f53\u611f\u77e5\u7b56\u7565\u524d\u77bb\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u4e0e\u524d\u77bb\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u6b65\u6001\u63a7\u5236\u4e2d\u53cd\u5e94\u6027\u9c81\u68d2\u63a7\u5236\u4e0e\u524d\u77bb\u6027\u611f\u77e5\u7cfb\u7edf\u4e4b\u95f4\u7684\u4e24\u96be\u9009\u62e9\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u6a21\u62df\u73af\u5883\u4e2d\u7684\u7279\u6743\u4fe1\u606f\u7f16\u7801\u5230\u7b56\u7565\u7684\u6f5c\u5728\u72b6\u6001\u4e2d\uff0c\u5b9e\u73b0\u524d\u77bb\u6027\u80fd\u529b\u3002", "result": "\u5728\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u9ad8\u9c81\u68d2\u6027\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u6b65\u6001\u63a7\u5236\u4e2d\u7684\u521a\u6027\u6b65\u6001\u4e0e\u4e0d\u7a33\u5b9a\u6b65\u6001\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2509.12863", "pdf": "https://arxiv.org/pdf/2509.12863", "abs": "https://arxiv.org/abs/2509.12863", "authors": ["Haozhan Ni", "Jingsong Liang", "Chenyu He", "Yuhong Cao", "Guillaume Sartoretti"], "title": "GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous robot exploration (ARE) is the process of a robot autonomously\nnavigating and mapping an unknown environment. Recent Reinforcement Learning\n(RL)-based approaches typically formulate ARE as a sequential decision-making\nproblem defined on a collision-free informative graph. However, these methods\noften demonstrate limited reasoning ability over graph-structured data.\nMoreover, due to the insufficient consideration of robot motion, the resulting\nRL policies are generally optimized to minimize travel distance, while\nneglecting time efficiency. To overcome these limitations, we propose GRATE, a\nDeep Reinforcement Learning (DRL)-based approach that leverages a Graph\nTransformer to effectively capture both local structure patterns and global\ncontextual dependencies of the informative graph, thereby enhancing the model's\nreasoning capability across the entire environment. In addition, we deploy a\nKalman filter to smooth the waypoint outputs, ensuring that the resulting path\nis kinodynamically feasible for the robot to follow. Experimental results\ndemonstrate that our method exhibits better exploration efficiency (up to 21.5%\nin distance and 21.3% in time to complete exploration) than state-of-the-art\nconventional and learning-based baselines in various simulation benchmarks. We\nalso validate our planner in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08GRATE\uff09\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u63a2\u7d22\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u53d8\u538b\u5668\u589e\u5f3a\u5bf9\u73af\u5883\u4fe1\u606f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u63a2\u7d22\u65b9\u6cd5\u5728\u56fe\u5f62\u6570\u636e\u63a8\u7406\u548c\u8fd0\u52a8\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u56fe\u53d8\u538b\u5668\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u4f18\u5316\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u63d0\u5347\u5168\u5c40\u63a8\u7406\u80fd\u529b\u548c\u8fd0\u52a8\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGRATE\u5728\u8ddd\u79bb\u548c\u65f6\u95f4\u6548\u7387\u4e0a\u5206\u522b\u63d0\u534721.5%\u548c21.3%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GRATE\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a2\u7d22\u6548\u7387\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.12880", "pdf": "https://arxiv.org/pdf/2509.12880", "abs": "https://arxiv.org/abs/2509.12880", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T05, 68T40", "I.2.9; I.2.6; H.5.2"], "comment": "Presented at the Context-Awareness in HRI (CONAWA) Workshop, ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2022), March 7, 2022", "summary": "Pointing is a key mode of interaction with robots, yet most prior work has\nfocused on recognition rather than generation. We present a motion capture\ndataset of human pointing gestures covering diverse styles, handedness, and\nspatial targets. Using reinforcement learning with motion imitation, we train\npolicies that reproduce human-like pointing while maximizing precision. Results\nshow our approach enables context-aware pointing behaviors in simulation,\nbalancing task performance with natural dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u52a8\u4f5c\u6355\u6349\u7684\u4eba\u7c7b\u6307\u5411\u624b\u52bf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u4f5c\u6a21\u4eff\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u81ea\u7136\u7684\u6307\u5411\u884c\u4e3a\u3002", "motivation": "\u6307\u5411\u662f\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u5173\u952e\u6a21\u5f0f\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u8bc6\u522b\u800c\u975e\u751f\u6210\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u4f5c\u6a21\u4eff\u8bad\u7ec3\u7b56\u7565\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6\u4e14\u81ea\u7136\u7684\u6307\u5411\u8fd0\u52a8\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6307\u5411\u884c\u4e3a\uff0c\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u548c\u81ea\u7136\u52a8\u6001\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u751f\u6210\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u81ea\u7136\u7684\u6307\u5411\u884c\u4e3a\uff0c\u4e3a\u673a\u5668\u4eba\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.12890", "pdf": "https://arxiv.org/pdf/2509.12890", "abs": "https://arxiv.org/abs/2509.12890", "authors": ["Malte Probst", "Raphael Wenzel", "Monica Dasi"], "title": "Responsibility and Engagement -- Evaluating Interactions in Social Robot Navigation", "categories": ["cs.RO"], "comment": "under review for 2026 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "In Social Robot Navigation (SRN), the availability of meaningful metrics is\ncrucial for evaluating trajectories from human-robot interactions. In the SRN\ncontext, such interactions often relate to resolving conflicts between two or\nmore agents. Correspondingly, the shares to which agents contribute to the\nresolution of such conflicts are important. This paper builds on recent work,\nwhich proposed a Responsibility metric capturing such shares. We extend this\nframework in two directions: First, we model the conflict buildup phase by\nintroducing a time normalization. Second, we propose the related Engagement\nmetric, which captures how the agents' actions intensify a conflict. In a\ncomprehensive series of simulated scenarios with dyadic, group and crowd\ninteractions, we show that the metrics carry meaningful information about the\ncooperative resolution of conflicts in interactions. They can be used to assess\nbehavior quality and foresightedness. We extensively discuss applicability,\ndesign choices and limitations of the proposed metrics.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86\u8d23\u4efb\u5ea6\u91cf\u6846\u67b6\uff0c\u63d0\u51fa\u65f6\u95f4\u5f52\u4e00\u5316\u548c\u53c2\u4e0e\u5ea6\u91cf\uff0c\u7528\u4e8e\u8bc4\u4f30\u793e\u4f1a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u51b2\u7a81\u89e3\u51b3\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u793e\u4f1a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u51b2\u7a81\u89e3\u51b3\u7684\u8d21\u732e\u5ea6\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u4ea4\u4e92\u884c\u4e3a\u7684\u8bc4\u4f30\u8d28\u91cf\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u5f52\u4e00\u5316\u5efa\u6a21\u51b2\u7a81\u79ef\u7d2f\u9636\u6bb5\uff0c\u63d0\u51fa\u53c2\u4e0e\u5ea6\u91cf\u6355\u6349\u884c\u4e3a\u5bf9\u51b2\u7a81\u7684\u52a0\u5267\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u5728\u6a21\u62df\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5ea6\u91cf\u5bf9\u51b2\u7a81\u89e3\u51b3\u4fe1\u606f\u7684\u6709\u6548\u6027\uff0c\u53ef\u7528\u4e8e\u884c\u4e3a\u8d28\u91cf\u548c\u524d\u77bb\u6027\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ea6\u91cf\u65b9\u6cd5\u6709\u6548\uff0c\u9002\u7528\u4e8e\u8bc4\u4f30\u793e\u4f1a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u51b2\u7a81\u89e3\u51b3\u884c\u4e3a\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5e94\u7528\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.12912", "pdf": "https://arxiv.org/pdf/2509.12912", "abs": "https://arxiv.org/abs/2509.12912", "authors": ["Raphael Wenzel", "Malte Probst"], "title": "Spotting the Unfriendly Robot -- Towards better Metrics for Interactions", "categories": ["cs.RO"], "comment": "Presented at 2025 IEEE Conference on Robotics and Automation (ICRA)\n  Workshop: Advances in Social Navigation: Planning, HRI and Beyond", "summary": "Establishing standardized metrics for Social Robot Navigation (SRN)\nalgorithms for assessing the quality and social compliance of robot behavior\naround humans is essential for SRN research. Currently, commonly used\nevaluation metrics lack the ability to quantify how cooperative an agent\nbehaves in interaction with humans. Concretely, in a simple frontal approach\nscenario, no metric specifically captures if both agents cooperate or if one\nagent stays on collision course and the other agent is forced to evade. To\naddress this limitation, we propose two new metrics, a conflict intensity\nmetric and the responsibility metric. Together, these metrics are capable of\nevaluating the quality of human-robot interactions by showing how much a given\nalgorithm has contributed to reducing a conflict and which agent actually took\nresponsibility of the resolution. This work aims to contribute to the\ndevelopment of a comprehensive and standardized evaluation methodology for SRN,\nultimately enhancing the safety, efficiency, and social acceptance of robots in\nhuman-centric environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6307\u6807\uff08\u51b2\u7a81\u5f3a\u5ea6\u6307\u6807\u548c\u8d23\u4efb\u6307\u6807\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\uff08SRN\uff09\u7b97\u6cd5\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u6307\u6807\u65e0\u6cd5\u91cf\u5316\u5408\u4f5c\u884c\u4e3a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dSRN\u7b97\u6cd5\u7684\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u91cf\u5316\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5408\u4f5c\u6027\uff0c\u7279\u522b\u662f\u5728\u51b2\u7a81\u89e3\u51b3\u4e2d\u8c01\u627f\u62c5\u4e86\u8d23\u4efb\u7684\u95ee\u9898\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6307\u6807\u6765\u5b8c\u5584\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u51b2\u7a81\u5f3a\u5ea6\u6307\u6807\u548c\u8d23\u4efb\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u7b97\u6cd5\u5728\u51b2\u7a81\u89e3\u51b3\u4e2d\u7684\u8d21\u732e\u548c\u884c\u4e3a\u8d23\u4efb\u3002", "result": "\u65b0\u6307\u6807\u80fd\u591f\u91cf\u5316\u51b2\u7a81\u51cf\u5c11\u7684\u7a0b\u5ea6\u53ca\u8d23\u4efb\u5f52\u5c5e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSRN\u7b97\u6cd5\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u671b\u589e\u5f3a\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2509.12928", "pdf": "https://arxiv.org/pdf/2509.12928", "abs": "https://arxiv.org/abs/2509.12928", "authors": ["Peiwen Yang", "Mingquan Jiang", "Xinyue Shen", "Heping Zhang"], "title": "Spatiotemporal Calibration for Laser Vision Sensor in Hand-eye System Based on Straight-line Constraint", "categories": ["cs.RO"], "comment": "Submitted to IEEE RAL", "summary": "Laser vision sensors (LVS) are critical perception modules for industrial\nrobots, facilitating real-time acquisition of workpiece geometric data in\nwelding applications. However, the camera communication delay will lead to a\ntemporal desynchronization between captured images and the robot motions.\nAdditionally, hand-eye extrinsic parameters may vary during prolonged\nmeasurement. To address these issues, we introduce a measurement model of LVS\nconsidering the effect of the camera's time-offset and propose a teaching-free\nspatiotemporal calibration method utilizing line constraints. This method\ninvolves a robot equipped with an LVS repeatedly scanning straight-line fillet\nwelds using S-shaped trajectories. Regardless of the robot's orientation\nchanges, all measured welding positions are constrained to a straight-line,\nrepresented by Plucker coordinates. Moreover, a nonlinear optimization model\nbased on straight-line constraints is established. Subsequently, the\nLevenberg-Marquardt algorithm (LMA) is employed to optimize parameters,\nincluding time-offset, hand-eye extrinsic parameters, and straight-line\nparameters. The feasibility and accuracy of the proposed approach are\nquantitatively validated through experiments on curved weld scanning. We\nopen-sourced the code, dataset, and simulation report at\nhttps://anonymous.4open.science/r/LVS_ST_CALIB-015F/README.md.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u7ebf\u7ea6\u675f\u7684\u65e0\u793a\u6559\u65f6\u7a7a\u6821\u51c6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6fc0\u5149\u89c6\u89c9\u4f20\u611f\u5668\uff08LVS\uff09\u4e2d\u7684\u65f6\u95f4\u504f\u79fb\u548c\u624b\u773c\u5916\u53c2\u53d8\u5316\u95ee\u9898\u3002", "motivation": "\u76f8\u673a\u901a\u4fe1\u5ef6\u8fdf\u548c\u624b\u773c\u5916\u53c2\u53d8\u5316\u4f1a\u5bfc\u81f4\u5de5\u4e1a\u673a\u5668\u4eba\u5728\u710a\u63a5\u5e94\u7528\u4e2d\u91c7\u96c6\u7684\u5de5\u4ef6\u51e0\u4f55\u6570\u636e\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u4e0d\u540c\u6b65\u3002", "method": "\u901a\u8fc7\u76f4\u7ebf\u710a\u7f1d\u626b\u63cf\u548cS\u5f62\u8f68\u8ff9\uff0c\u5229\u7528Pl\u00fccker\u5750\u6807\u8868\u793a\u76f4\u7ebf\u7ea6\u675f\uff0c\u5efa\u7acb\u975e\u7ebf\u6027\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u91c7\u7528LMA\u7b97\u6cd5\u4f18\u5316\u65f6\u95f4\u504f\u79fb\u3001\u624b\u773c\u5916\u53c2\u548c\u76f4\u7ebf\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u66f2\u7ebf\u710a\u7f1d\u626b\u63cf\u4e2d\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u540c\u6b65\u95ee\u9898\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002"}}
{"id": "2509.12969", "pdf": "https://arxiv.org/pdf/2509.12969", "abs": "https://arxiv.org/abs/2509.12969", "authors": ["Jae-Hyun Lee", "Jonghoo Park", "Kyu-Jin Cho"], "title": "Tendon-Based Proprioception in an Anthropomorphic Underactuated Robotic Hand with Series Elastic Actuators", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, Supplementary video, Submitted to IEEE Robotics\n  and Automation Letters (RA-L)", "summary": "Anthropomorphic underactuated hands are widely employed for their versatility\nand structural simplicity. In such systems, compact sensing integration and\nproper interpretation aligned with underactuation are crucial for realizing\npractical grasp functionalities. This study proposes an anthropomorphic\nunderactuated hand that achieves comprehensive situational awareness of\nhand-object interaction, utilizing tendon-based proprioception provided by\nseries elastic actuators (SEAs). We developed a compact SEA with high accuracy\nand reliability that can be seamlessly integrated into sensorless fingers. By\ncoupling proprioceptive sensing with potential energy-based modeling, the\nsystem estimates key grasp-related variables, including contact timing, joint\nangles, relative object stiffness, and finger configuration changes indicating\nexternal disturbances. These estimated variables enable grasp posture\nreconstruction, safe handling of deformable objects, and blind grasping with\nproprioceptive-only recognition of objects with varying geometry and stiffness.\nFinger-level experiments and hand-level demonstrations confirmed the\neffectiveness of the proposed approach. The results demonstrate that\ntendon-based proprioception serves as a compact and robust sensing modality for\npractical manipulation without reliance on vision or tactile feedback.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u808c\u8171\u672c\u4f53\u611f\u77e5\u7684\u4eff\u751f\u6b20\u9a71\u52a8\u624b\uff0c\u901a\u8fc7\u5f39\u6027\u6267\u884c\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u611f\u77e5\uff0c\u65e0\u9700\u89c6\u89c9\u6216\u89e6\u89c9\u53cd\u9988\u5373\u53ef\u5b8c\u6210\u6293\u53d6\u4efb\u52a1\u3002", "motivation": "\u6b20\u9a71\u52a8\u624b\u7684\u591a\u529f\u80fd\u6027\u548c\u7ed3\u6784\u7b80\u5355\u6027\u4f7f\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u7d27\u51d1\u4f20\u611f\u5668\u96c6\u6210\u548c\u611f\u77e5\u5bf9\u9f50\u662f\u5b9e\u73b0\u5b9e\u7528\u6293\u53d6\u529f\u80fd\u7684\u5173\u952e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u9ad8\u7cbe\u5ea6\u7684\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\uff0c\u7ed3\u5408\u808c\u8171\u672c\u4f53\u611f\u77e5\u548c\u52bf\u80fd\u5efa\u6a21\uff0c\u4f30\u8ba1\u6293\u53d6\u76f8\u5173\u53d8\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u7cfb\u7edf\u80fd\u591f\u91cd\u5efa\u6293\u53d6\u59ff\u52bf\u3001\u5b89\u5168\u5904\u7406\u53ef\u53d8\u6027\u7269\u4f53\uff0c\u5e76\u5b9e\u73b0\u65e0\u89c6\u89c9\u53cd\u9988\u7684\u76f2\u6293\u53d6\u3002", "conclusion": "\u808c\u8171\u672c\u4f53\u611f\u77e5\u662f\u4e00\u79cd\u7d27\u51d1\u4e14\u9c81\u68d2\u7684\u611f\u77e5\u65b9\u5f0f\uff0c\u9002\u7528\u4e8e\u65e0\u89c6\u89c9\u6216\u89e6\u89c9\u53cd\u9988\u7684\u5b9e\u7528\u64cd\u4f5c\u3002"}}
{"id": "2509.12982", "pdf": "https://arxiv.org/pdf/2509.12982", "abs": "https://arxiv.org/abs/2509.12982", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali", "Beatriz Sanguino", "Tongtong Wang", "Guoyuan Li", "Houxiang Zhang", "Thomas Peyrucain"], "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.SE"], "comment": "15 pages, 4 figures, 3 tables", "summary": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff08ODiSAR\uff09\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u91cd\u6784\u8bef\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5c55\u73b0\u4e86\u9ad8\u6027\u80fd\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u9002\u5e94\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u4e3b\u52a8\u68c0\u6d4b\u548c\u5e94\u5bf9\u5f02\u5e38\u884c\u4e3a\uff08\u5305\u62ecOOD\u6848\u4f8b\uff09\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u8fdb\u884cOOD\u68c0\u6d4b\u3002", "method": "ODiSAR\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u673a\u5668\u4eba\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u548c\u8499\u7279\u5361\u6d1bDropout\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u9884\u6d4b\u65b9\u5dee\u68c0\u6d4bOOD\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cODiSAR\u5728\u4e24\u79cd\u5de5\u4e1a\u673a\u5668\u4eba\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u68c0\u6d4b\u6027\u80fd\u9ad8\u8fbe98% AUROC\u300196% TNR@TPR95\u548c95% F1-score\u3002", "conclusion": "ODiSAR\u4e0d\u4ec5\u80fd\u9ad8\u6548\u68c0\u6d4bOOD\u884c\u4e3a\uff0c\u8fd8\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u4ee5\u652f\u6301\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13024", "pdf": "https://arxiv.org/pdf/2509.13024", "abs": "https://arxiv.org/abs/2509.13024", "authors": ["Haohan Min", "Zhoujian Li", "Yu Yang", "Jinyu Chen", "Shenghai Yuan"], "title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D Perception", "categories": ["cs.RO"], "comment": null, "summary": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u540d\u4e3aDVDP\u7684\u65b0\u578b\u7aef\u5230\u7aef\u89c6\u89c9\u5bf9\u63a5\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cc\u76eeRGB-D\u6444\u50cf\u5934\u76f4\u63a5\u8f93\u51fa\u5bf9\u63a5\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5bf9\u63a5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u89c9\u5bf9\u63a5\u65b9\u6cd5\u5bf9\u673a\u5668\u4eba\u521d\u59cb\u4f4d\u7f6e\u8981\u6c42\u4e25\u683c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\u548c\u66f4\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5bf9\u63a5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u89c6\u89c9\u5bf9\u63a5\u7b56\u7565DVDP\uff0c\u4ec5\u9700\u53cc\u76eeRGB-D\u6444\u50cf\u5934\uff0c\u7ed3\u5408\u865a\u62df\u4e0e\u73b0\u5b9e\u73af\u5883\u91c7\u96c6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDVDP\u6027\u80fd\u4f18\u8d8a\uff0c\u80fd\u591f\u751f\u6210\u5e73\u6ed1\u4e14\u7b26\u5408\u7269\u7406\u7ea6\u675f\u7684\u5bf9\u63a5\u8f68\u8ff9\uff0c\u5b9e\u9645\u90e8\u7f72\u5728SCOUT Mini\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DVDP\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u81ea\u52a8\u5bf9\u63a5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u884c\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.13069", "pdf": "https://arxiv.org/pdf/2509.13069", "abs": "https://arxiv.org/abs/2509.13069", "authors": ["James C. Ward", "Arthur Richards", "Edmund R. Hunt"], "title": "Practical Handling of Dynamic Environments in Decentralised Multi-Robot Patrol", "categories": ["cs.RO"], "comment": null, "summary": "Persistent monitoring using robot teams is of interest in fields such as\nsecurity, environmental monitoring, and disaster recovery. Performing such\nmonitoring in a fully on-line decentralised fashion has significant potential\nadvantages for robustness, adaptability, and scalability of monitoring\nsolutions, including, in principle, the capacity to effectively adapt in\nreal-time to a changing environment. We examine this through the lens of\nmulti-robot patrol, in which teams of patrol robots must persistently minimise\ntime between visits to points of interest, within environments where\ntraversability of routes is highly dynamic. These dynamics must be observed by\npatrol agents and accounted for in a fully decentralised on-line manner. In\nthis work, we present a new method of monitoring and adjusting for environment\ndynamics in a decentralised multi-robot patrol team. We demonstrate that our\nmethod significantly outperforms realistic baselines in highly dynamic\nscenarios, and also investigate dynamic scenarios in which explicitly\naccounting for environment dynamics may be unnecessary or impractical.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u6301\u4e45\u5de1\u903b\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6563\u5f0f\u5728\u7ebf\u8c03\u6574\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u6301\u4e45\u76d1\u63a7\u5728\u5b89\u5168\u3001\u73af\u5883\u76d1\u6d4b\u548c\u707e\u96be\u6062\u590d\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u5206\u6563\u5f0f\u5728\u7ebf\u65b9\u6cd5\u80fd\u63d0\u5347\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6563\u5f0f\u7684\u591a\u673a\u5668\u4eba\u5de1\u903b\u65b9\u6cd5\uff0c\u5b9e\u65f6\u76d1\u6d4b\u5e76\u8c03\u6574\u73af\u5883\u52a8\u6001\u5bf9\u5de1\u903b\u8def\u7ebf\u7684\u5f71\u54cd\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9ad8\u5ea6\u52a8\u6001\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u67d0\u4e9b\u60c5\u51b5\u4e0b\u52a8\u6001\u8c03\u6574\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u5206\u6563\u5f0f\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\u5728\u591a\u673a\u5668\u4eba\u6301\u4e45\u5de1\u903b\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u6839\u636e\u73af\u5883\u52a8\u6001\u7075\u6d3b\u5e94\u7528\u3002"}}
{"id": "2509.13074", "pdf": "https://arxiv.org/pdf/2509.13074", "abs": "https://arxiv.org/abs/2509.13074", "authors": ["Simon Fritsch", "Liam Achenbach", "Riccardo Bianco", "Nicola Irmiger", "Gawain Marti", "Samuel Visca", "Chenyu Yang", "Davide Liconti", "Barnabas Gavin Cangan", "Robert Jomar Malate", "Ronan J. Hinchet", "Robert K. Katzschmann"], "title": "Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of Freedom by Fusing the Abduction of Digits Four and Five", "categories": ["cs.RO"], "comment": "First five listed authors have equal contribution", "summary": "This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand\nthat departs from purely anthropomorphic designs to achieve an expanded grasp\nenvelope, enable manipulation poses beyond human capability, and reduce the\nrequired number of actuators. This is achieved by combining the\nadduction/abduction (Add/Abd) joint of digits four and five into a single joint\nwith a large range of motion. The combined joint increases the workspace of the\ndigits by 400\\% and reduces the required DoFs while retaining dexterity.\nExperimental results demonstrate that the combined Add/Abd joint enables the\nhand to grasp objects with a side distance of up to 200 mm. Reinforcement\nlearning-based investigations show that the design enables grasping policies\nthat are effective not only for handling larger objects but also for achieving\nenhanced grasp stability. In teleoperated trials, the hand successfully\nperformed 86\\% of attempted grasps on suitable YCB objects, including\nchallenging non-anthropomorphic configurations. These findings validate the\ndesign's ability to enhance grasp stability, flexibility, and dexterous\nmanipulation without added complexity, making it well-suited for a wide range\nof applications.", "AI": {"tldr": "SABD\u624b\u662f\u4e00\u79cd16\u81ea\u7531\u5ea6\u7684\u673a\u5668\u4eba\u624b\uff0c\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u5b9e\u73b0\u66f4\u5927\u7684\u6293\u53d6\u8303\u56f4\u3001\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u66f4\u5c11\u7684\u9a71\u52a8\u5668\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf\u4eff\u4eba\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u624b\u5728\u6293\u53d6\u8303\u56f4\u548c\u7075\u6d3b\u6027\u4e0a\u53d7\u9650\uff0cSABD\u624b\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u5173\u8282\u8bbe\u8ba1\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06\u7b2c\u56db\u548c\u7b2c\u4e94\u624b\u6307\u7684\u5185\u6536/\u5916\u5c55\u5173\u8282\u5408\u5e76\u4e3a\u4e00\u4e2a\u5177\u6709\u5927\u8fd0\u52a8\u8303\u56f4\u7684\u5173\u8282\uff0c\u4ee5\u51cf\u5c11\u81ea\u7531\u5ea6\u5e76\u6269\u5927\u5de5\u4f5c\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5408\u5e76\u540e\u7684\u5173\u8282\u4f7f\u6293\u53d6\u8303\u56f4\u6269\u5927400%\uff0c\u5e76\u80fd\u7a33\u5b9a\u6293\u53d6\u4fa7\u8ddd\u8fbe200\u6beb\u7c73\u7684\u7269\u4f53\u3002\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u4e86\u5176\u5728\u6293\u53d6\u5927\u7269\u4f53\u548c\u589e\u5f3a\u7a33\u5b9a\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "SABD\u624b\u7684\u8bbe\u8ba1\u5728\u4fdd\u6301\u7075\u5de7\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.13077", "pdf": "https://arxiv.org/pdf/2509.13077", "abs": "https://arxiv.org/abs/2509.13077", "authors": ["Jonathan K\u00fclz", "Sehoon Ha", "Matthias Althoff"], "title": "A Design Co-Pilot for Task-Tailored Manipulators", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Although robotic manipulators are used in an ever-growing range of\napplications, robot manufacturers typically follow a ``one-fits-all''\nphilosophy, employing identical manipulators in various settings. This often\nleads to suboptimal performance, as general-purpose designs fail to exploit\nparticularities of tasks. The development of custom, task-tailored robots is\nhindered by long, cost-intensive development cycles and the high cost of\ncustomized hardware. Recently, various computational design methods have been\ndevised to overcome the bottleneck of human engineering. In addition, a surge\nof modular robots allows quick and economical adaptation to changing industrial\nsettings. This work proposes an approach to automatically designing and\noptimizing robot morphologies tailored to a specific environment. To this end,\nwe learn the inverse kinematics for a wide range of different manipulators. A\nfully differentiable framework realizes gradient-based fine-tuning of designed\nrobots and inverse kinematics solutions. Our generative approach accelerates\nthe generation of specialized designs from hours with optimization-based\nmethods to seconds, serving as a design co-pilot that enables instant\nadaptation and effective human-AI collaboration. Numerical experiments show\nthat our approach finds robots that can navigate cluttered environments,\nmanipulators that perform well across a specified workspace, and can be adapted\nto different hardware constraints. Finally, we demonstrate the real-world\napplicability of our method by setting up a modular robot designed in\nsimulation that successfully moves through an obstacle course.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u673a\u5668\u4eba\u5f62\u6001\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u5206\u6846\u67b6\u5b9e\u73b0\u68af\u5ea6\u5fae\u8c03\u548c\u751f\u6210\u4e13\u4e1a\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5236\u9020\u591a\u4e3a\u901a\u7528\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4efb\u52a1\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u5b9a\u5236\u5316\u8bbe\u8ba1\u6210\u672c\u9ad8\u3001\u5468\u671f\u957f\u3002", "method": "\u5b66\u4e60\u591a\u79cd\u673a\u68b0\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\uff0c\u91c7\u7528\u5fae\u5206\u6846\u67b6\u5b9e\u73b0\u673a\u5668\u4eba\u5f62\u6001\u548c\u9006\u8fd0\u52a8\u5b66\u7684\u68af\u5ea6\u5fae\u8c03\uff0c\u751f\u6210\u4e13\u4e3a\u7279\u5b9a\u73af\u5883\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u627e\u5230\u9002\u5e94\u4e0d\u540c\u73af\u5883\u548c\u786c\u4ef6\u7ea6\u675f\u7684\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u5e76\u5728\u73b0\u5b9e\u6a21\u5757\u5316\u673a\u5668\u4eba\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u673a\u5668\u4eba\u5b9a\u5236\u5316\u8bbe\u8ba1\uff0c\u4fc3\u8fdb\u4e86\u5373\u65f6\u9002\u5e94\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.13095", "pdf": "https://arxiv.org/pdf/2509.13095", "abs": "https://arxiv.org/abs/2509.13095", "authors": ["Zijie Zhao", "Honglei Guo", "Shengqian Chen", "Kaixuan Xu", "Bo Jiang", "Yuanheng Zhu", "Dongbin Zhao"], "title": "Empowering Multi-Robot Cooperation via Sequential World Models", "categories": ["cs.RO"], "comment": null, "summary": "Model-based reinforcement learning (MBRL) has shown significant potential in\nrobotics due to its high sample efficiency and planning capability. However,\nextending MBRL to multi-robot cooperation remains challenging due to the\ncomplexity of joint dynamics. To address this, we propose the Sequential World\nModel (SeqWM), a novel framework that integrates the sequential paradigm into\nmodel-based multi-agent reinforcement learning. SeqWM employs independent,\nsequentially structured agent-wise world models to decompose complex joint\ndynamics. Latent rollouts and decision-making are performed through sequential\ncommunication, where each agent generates its future trajectory and plans its\nactions based on the predictions of its predecessors. This design enables\nexplicit intention sharing, enhancing cooperative performance, and reduces\ncommunication overhead to linear complexity. Results in challenging simulated\nenvironments (Bi-DexHands and Multi-Quad) show that SeqWM outperforms existing\nstate-of-the-art model-free and model-based baselines in both overall\nperformance and sample efficiency, while exhibiting advanced cooperative\nbehaviors such as predictive adaptation and role division. Furthermore, SeqWM\nhas been success fully deployed on physical quadruped robots, demonstrating its\neffectiveness in real-world multi-robot systems. Demos and code are available\nat: https://github.com/zhaozijie2022/seqwm-marl", "AI": {"tldr": "SeqWM\u662f\u4e00\u79cd\u65b0\u578b\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u901a\u4fe1\u548c\u72ec\u7acb\u4e16\u754c\u6a21\u578b\u7b80\u5316\u8054\u5408\u52a8\u529b\u5b66\uff0c\u63d0\u9ad8\u534f\u4f5c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u8054\u5408\u52a8\u529b\u5b66\u590d\u6742\u6027\u662f\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "SeqWM\u91c7\u7528\u72ec\u7acb\u3001\u987a\u5e8f\u7ed3\u6784\u7684\u4ee3\u7406\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u987a\u5e8f\u901a\u4fe1\u8fdb\u884c\u6f5c\u5728\u5c55\u5f00\u548c\u51b3\u7b56\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cSeqWM\u5728\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u9884\u6d4b\u9002\u5e94\u548c\u89d2\u8272\u5206\u5de5\u7b49\u534f\u4f5c\u884c\u4e3a\u3002", "conclusion": "SeqWM\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7684\u590d\u6742\u6027\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.13109", "pdf": "https://arxiv.org/pdf/2509.13109", "abs": "https://arxiv.org/abs/2509.13109", "authors": ["Fabian Fl\u00fcrenbrock", "Yanick B\u00fcchel", "Johannes K\u00f6hler", "Marianne Schmid Daners", "Melanie N. Zeilinger"], "title": "Model Predictive Control with Reference Learning for Soft Robotic Intracranial Pressure Waveform Modulation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper introduces a learning-based control framework for a soft robotic\nactuator system designed to modulate intracranial pressure (ICP) waveforms,\nwhich is essential for studying cerebrospinal fluid dynamics and pathological\nprocesses underlying neurological disorders. A two-layer framework is proposed\nto safely achieve a desired ICP waveform modulation. First, a model predictive\ncontroller (MPC) with a disturbance observer is used for offset-free tracking\nof the system's motor position reference trajectory under safety constraints.\nSecond, to address the unknown nonlinear dependence of ICP on the motor\nposition, we employ a Bayesian optimization (BO) algorithm used for online\nlearning of a motor position reference trajectory that yields the desired ICP\nmodulation. The framework is experimentally validated using a test bench with a\nbrain phantom that replicates realistic ICP dynamics in vitro. Compared to a\npreviously employed proportional-integral-derivative controller, the MPC\nreduces mean and maximum motor position reference tracking errors by 83 % and\n73 %, respectively. In less than 20 iterations, the BO algorithm learns a motor\nposition reference trajectory that yields an ICP waveform with the desired mean\nand amplitude.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff0c\u7528\u4e8e\u8c03\u5236\u9885\u5185\u538b\u6ce2\u5f62\uff0c\u4ee5\u7814\u7a76\u8111\u810a\u6db2\u52a8\u529b\u5b66\u548c\u795e\u7ecf\u75be\u75c5\u7684\u75c5\u7406\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u8111\u810a\u6db2\u52a8\u529b\u5b66\u548c\u795e\u7ecf\u75be\u75c5\u7684\u75c5\u7406\u8fc7\u7a0b\u9700\u8981\u7cbe\u786e\u8c03\u5236\u9885\u5185\u538b\u6ce2\u5f62\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u975e\u7ebf\u6027\u4f9d\u8d56\u6027\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u6846\u67b6\uff1a1) \u4f7f\u7528\u5e26\u6270\u52a8\u89c2\u6d4b\u5668\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668(MPC)\u5b9e\u73b0\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u7535\u673a\u4f4d\u7f6e\u8f68\u8ff9\u8ddf\u8e2a\uff1b2) \u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316(BO)\u7b97\u6cd5\u5728\u7ebf\u5b66\u4e60\u80fd\u751f\u6210\u76ee\u6807ICP\u6ce2\u5f62\u7684\u7535\u673a\u4f4d\u7f6e\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cMPC\u5c06\u7535\u673a\u4f4d\u7f6e\u8ddf\u8e2a\u8bef\u5dee\u7684\u5e73\u5747\u503c\u548c\u6700\u5927\u503c\u5206\u522b\u964d\u4f4e83%\u548c73%\uff1bBO\u7b97\u6cd5\u572820\u6b21\u8fed\u4ee3\u5185\u6210\u529f\u5b66\u4e60\u5230\u76ee\u6807ICP\u6ce2\u5f62\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5b89\u5168\u6027\u548c\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2509.13126", "pdf": "https://arxiv.org/pdf/2509.13126", "abs": "https://arxiv.org/abs/2509.13126", "authors": ["Miquel Oller", "An Dang", "Nima Fazeli"], "title": "Hydrosoft: Non-Holonomic Hydroelastic Models for Compliant Tactile Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Tactile sensors have long been valued for their perceptual capabilities,\noffering rich insights into the otherwise hidden interface between the robot\nand grasped objects. Yet their inherent compliance -- a key driver of\nforce-rich interactions -- remains underexplored. The central challenge is to\ncapture the complex, nonlinear dynamics introduced by these passive-compliant\nelements. Here, we present a computationally efficient non-holonomic\nhydroelastic model that accurately models path-dependent contact force\ndistributions and dynamic surface area variations. Our insight is to extend the\nobject's state space, explicitly incorporating the distributed forces generated\nby the compliant sensor. Our differentiable formulation not only accounts for\npath-dependent behavior but also enables gradient-based trajectory\noptimization, seamlessly integrating with high-resolution tactile feedback. We\ndemonstrate the effectiveness of our approach across a range of simulated and\nreal-world experiments and highlight the importance of modeling the path\ndependence of sensor dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u7684\u975e\u5b8c\u6574\u6c34\u5f39\u6027\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u5efa\u6a21\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8def\u5f84\u4f9d\u8d56\u63a5\u89e6\u529b\u5206\u5e03\u548c\u52a8\u6001\u8868\u9762\u79ef\u53d8\u5316\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u5668\u7684\u56fa\u6709\u67d4\u987a\u6027\u662f\u5176\u529b\u53cd\u9988\u7684\u5173\u952e\uff0c\u4f46\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u7269\u4f53\u72b6\u6001\u7a7a\u95f4\uff0c\u663e\u5f0f\u7eb3\u5165\u67d4\u987a\u4f20\u611f\u5668\u4ea7\u751f\u7684\u5206\u5e03\u5f0f\u529b\uff0c\u63d0\u51fa\u4e86\u53ef\u5fae\u5206\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f3a\u8c03\u4e86\u8def\u5f84\u4f9d\u8d56\u4f20\u611f\u5668\u52a8\u6001\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\uff0c\u5e76\u4e3a\u57fa\u4e8e\u68af\u5ea6\u7684\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.13132", "pdf": "https://arxiv.org/pdf/2509.13132", "abs": "https://arxiv.org/abs/2509.13132", "authors": ["Zhihao Zhang", "Chengyang Peng", "Minghao Zhu", "Ekim Yurtsever", "Keith A. Redmill"], "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous driving in dense, dynamic environments requires decision-making\nsystems that can exploit both spatial structure and long-horizon temporal\ndependencies while remaining robust to uncertainty. This work presents a novel\nframework that integrates multi-channel bird's-eye-view occupancy grids with\ntransformer-based sequence modeling for tactical driving in complex roundabout\nscenarios. To address the imbalance between frequent low-risk states and rare\nsafety-critical decisions, we propose the Uncertainty-Weighted Decision\nTransformer (UWDT). UWDT employs a frozen teacher transformer to estimate\nper-token predictive entropy, which is then used as a weight in the student\nmodel's loss function. This mechanism amplifies learning from uncertain,\nhigh-impact states while maintaining stability across common low-risk\ntransitions. Experiments in a roundabout simulator, across varying traffic\ndensities, show that UWDT consistently outperforms other baselines in terms of\nreward, collision rate, and behavioral stability. The results demonstrate that\nuncertainty-aware, spatial-temporal transformers can deliver safer and more\nefficient decision-making for autonomous driving in complex traffic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u51b3\u7b56\u53d8\u6362\u5668\uff08UWDT\uff09\uff0c\u901a\u8fc7\u591a\u901a\u9053\u9e1f\u77b0\u56fe\u5360\u7528\u7f51\u683c\u4e0e\u53d8\u6362\u5668\u5e8f\u5217\u5efa\u6a21\u7ed3\u5408\uff0c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73af\u5c9b\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u9700\u540c\u65f6\u5229\u7528\u7a7a\u95f4\u7ed3\u6784\u548c\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5904\u7406\u4f4e\u98ce\u9669\u72b6\u6001\u4e0e\u9ad8\u98ce\u9669\u51b3\u7b56\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51faUWDT\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u6559\u5e08\u53d8\u6362\u5668\u4f30\u8ba1\u9884\u6d4b\u71b5\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6743\u91cd\u7528\u4e8e\u5b66\u751f\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u589e\u5f3a\u5bf9\u4e0d\u786e\u5b9a\u9ad8\u98ce\u9669\u72b6\u6001\u7684\u5b66\u4e60\u3002", "result": "\u5728\u73af\u5c9b\u6a21\u62df\u5668\u4e2d\uff0cUWDT\u5728\u5956\u52b1\u3001\u78b0\u649e\u7387\u548c\u884c\u4e3a\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7a7a\u95f4-\u65f6\u95f4\u53d8\u6362\u5668\u80fd\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.13164", "pdf": "https://arxiv.org/pdf/2509.13164", "abs": "https://arxiv.org/abs/2509.13164", "authors": ["Jiawei Wang", "Haowei Sun", "Xintao Yan", "Shuo Feng", "Jun Gao", "Henry X. Liu"], "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 6 figures. Codes and videos are available at\n  https://wjiawei.com/terasim-world-web/", "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.", "AI": {"tldr": "TeraSim-World \u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u80fd\u5408\u6210\u771f\u5b9e\u4e14\u5730\u7406\u591a\u6837\u7684\u5b89\u5168\u5173\u952e\u6570\u636e\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u4e3b\u8981\u6765\u81ea\u6a21\u62df\u5668\uff08\u5b58\u5728\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff09\u6216\u771f\u5b9e\u9053\u8def\u6d4b\u8bd5\uff08\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff09\uff0cTeraSim-World\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4ece\u5730\u7406\u6570\u636e\u6e90\u83b7\u53d6\u771f\u5b9e\u5730\u56fe\u548c\u4ea4\u901a\u9700\u6c42\uff0c\u6a21\u62df\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u5236\u9020\u6781\u7aef\u60c5\u51b5\uff0c\u7ed3\u5408\u8857\u666f\u5b9e\u73b0\u903c\u771f\u4f20\u611f\u5668\u6e32\u67d3\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u5173\u952e\u7684\u5408\u6210\u6570\u636e\u6846\u67b6\u3002", "conclusion": "TeraSim-World\u586b\u8865\u4e86\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u5408\u6210\u9886\u57df\u7684\u7a7a\u767d\uff0c\u652f\u6301\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2509.13177", "pdf": "https://arxiv.org/pdf/2509.13177", "abs": "https://arxiv.org/abs/2509.13177", "authors": ["Salvatore Esposito", "Mat\u00edas Mattamala", "Daniel Rebain", "Francis Xiatian Zhang", "Kevin Dhaliwal", "Mohsen Khadem", "Subramanian Ramamoorthy"], "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation", "categories": ["cs.RO"], "comment": null, "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.", "AI": {"tldr": "ROOM\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u903c\u771f\u652f\u6c14\u7ba1\u955c\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u62df\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u7684\u751f\u6210\uff0c\u7528\u4e8e\u533b\u7597\u673a\u5668\u4eba\u4efb\u52a1\u7684\u9a8c\u8bc1\u548c\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5728\u652f\u6c14\u7ba1\u955c\u624b\u672f\u4e2d\u9762\u4e34\u5b9e\u9645\u8bad\u7ec3\u548c\u6d4b\u8bd5\u73af\u5883\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6570\u636e\u6536\u96c6\u7684\u4f26\u7406\u548c\u5b89\u5168\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u60a3\u8005CT\u626b\u63cf\uff0cROOM\u6846\u67b6\u751f\u6210\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff08\u5982RGB\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u3001\u5149\u6d41\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "ROOM\u751f\u6210\u7684\u6570\u636e\u53ef\u7528\u4e8e\u59ff\u6001\u4f30\u8ba1\u548c\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\uff0c\u5e76\u80fd\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u4ee5\u5e94\u5bf9\u533b\u7597\u573a\u666f\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "ROOM\u6709\u671b\u4e3a\u4e34\u5e8a\u96be\u4ee5\u6355\u6349\u7684\u591a\u6837\u60a3\u8005\u89e3\u5256\u7ed3\u6784\u548c\u624b\u672f\u573a\u666f\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2509.13200", "pdf": "https://arxiv.org/pdf/2509.13200", "abs": "https://arxiv.org/abs/2509.13200", "authors": ["Moonyoung Lee", "Dong Ki Kim", "Jai Krishna Bandi", "Max Smith", "Aileen Liao", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening", "categories": ["cs.RO"], "comment": "7 pages", "summary": "Humanoid robots promise to operate in everyday human environments without\nrequiring modifications to the surroundings. Among the many skills needed,\nopening doors is essential, as doors are the most common gateways in built\nspaces and often limit where a robot can go. Door opening, however, poses\nunique challenges as it is a long-horizon task under partial observability,\nsuch as reasoning about the door's unobservable latch state that dictates\nwhether the robot should rotate the handle or push the door. This ambiguity\nmakes standard behavior cloning prone to mode collapse, yielding blended or\nout-of-sequence actions. We introduce StageACT, a stage-conditioned imitation\nlearning framework that augments low-level policies with task-stage inputs.\nThis effective addition increases robustness to partial observability, leading\nto higher success rates and shorter completion times. On a humanoid operating\nin a real-world office environment, StageACT achieves a 55% success rate on\npreviously unseen doors, more than doubling the best baseline. Moreover, our\nmethod supports intentional behavior guidance through stage prompting, enabling\nrecovery behaviors. These results highlight stage conditioning as a lightweight\nyet powerful mechanism for long-horizon humanoid loco-manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9636\u6bb5\u6761\u4ef6\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6StageACT\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5f00\u95e8\u4efb\u52a1\u4e2d\u7684\u957f\u89c6\u91ce\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u5f00\u95e8\u662f\u4eba\u5f62\u673a\u5668\u4eba\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u5fc5\u9700\u7684\u57fa\u672c\u6280\u80fd\uff0c\u4f46\u7531\u4e8e\u95e8\u7684\u672a\u89c2\u6d4b\u72b6\u6001\uff08\u5982\u95e8\u95e9\u72b6\u6001\uff09\u5bfc\u81f4\u4e86\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u6a21\u5f0f\u5d29\u6e83\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86StageACT\u6846\u67b6\uff0c\u901a\u8fc7\u9636\u6bb5\u6761\u4ef6\u589e\u5f3a\u4f4e\u7ea7\u7b56\u7565\uff0c\u589e\u52a0\u5bf9\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u529e\u516c\u5ba4\u73af\u5883\u4e2d\uff0cStageACT\u5bf9\u672a\u89c1\u8fc7\u95e8\u7684\u6210\u529f\u7387\u8fbe\u5230\u4e8655%\uff0c\u662f\u57fa\u7ebf\u65b9\u6cd5\u7684\u4e24\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u652f\u6301\u884c\u4e3a\u5f15\u5bfc\u548c\u6062\u590d\u3002", "conclusion": "\u9636\u6bb5\u6761\u4ef6\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u673a\u5236\uff0c\u9002\u7528\u4e8e\u957f\u89c6\u91ce\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5b9a\u4f4d-\u64cd\u63a7\u4efb\u52a1\u3002"}}
{"id": "2509.13239", "pdf": "https://arxiv.org/pdf/2509.13239", "abs": "https://arxiv.org/abs/2509.13239", "authors": ["Tianxu An", "Flavio De Vincenti", "Yuntao Ma", "Marco Hutter", "Stelian Coros"], "title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic Reward Curriculum", "categories": ["cs.RO"], "comment": null, "summary": "We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5355\u81c2\u817f\u5f0f\u673a\u5668\u4eba\u5b8c\u6210\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\uff0c\u5e76\u5728\u5355\u673a\u5668\u4eba\u53ca\u53cc\u673a\u5668\u4eba\u534f\u4f5c\u573a\u666f\u4e2d\u5c55\u793a\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u91ce\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6267\u884c\u65f6\u95f4\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u4e0e\u534f\u8c03\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5956\u52b1\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u9010\u6b65\u5f15\u5bfc\u673a\u5668\u4eba\u5b8c\u6210\u5b50\u76ee\u6807\u6765\u8bad\u7ec3\u5355\u4e00\u7b56\u7565\uff0c\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534755%\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c1118.6%\u3002\u53cc\u673a\u5668\u4eba\u573a\u666f\u4e0b\uff0c\u5b9e\u73b0\u81ea\u4e3b\u6ce8\u610f\u529b\u8f6c\u79fb\u4ee5\u4fc3\u8fdb\u534f\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u53cc\u817f\u5f0f\u673a\u68b0\u81c2\u7684\u534f\u4f5c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.13249", "pdf": "https://arxiv.org/pdf/2509.13249", "abs": "https://arxiv.org/abs/2509.13249", "authors": ["Ye Li", "Daming Liu", "Yanhe Zhu", "Junming Zhang", "Yongsheng Luo", "Ziqi Wang", "Chenyu Liu", "Jie Zhao"], "title": "Design and Control of a Perching Drone Inspired by the Prey-Capturing Mechanism of Venus Flytrap", "categories": ["cs.RO"], "comment": null, "summary": "The endurance and energy efficiency of drones remain critical challenges in\ntheir design and operation. To extend mission duration, numerous studies\nexplored perching mechanisms that enable drones to conserve energy by\ntemporarily suspending flight. This paper presents a new perching drone that\nutilizes an active flexible perching mechanism inspired by the rapid predation\nmechanism of the Venus flytrap, achieving perching in less than 100 ms. The\nproposed system is designed for high-speed adaptability to the perching\ntargets. The overall drone design is outlined, followed by the development and\nvalidation of the biomimetic perching structure. To enhance the system\nstability, a cascade extended high-gain observer (EHGO) based control method is\ndeveloped, which can estimate and compensate for the external disturbance in\nreal time. The experimental results demonstrate the adaptability of the\nperching structure and the superiority of the cascaded EHGO in resisting wind\nand perching disturbances.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u4eba\u673a\u6816\u505c\u673a\u5236\uff0c\u7075\u611f\u6765\u81ea\u6355\u8747\u8349\u7684\u5feb\u901f\u6355\u98df\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728100\u6beb\u79d2\u5185\u7684\u9ad8\u901f\u6816\u505c\u3002\u8bbe\u8ba1\u8fd8\u5305\u62ec\u57fa\u4e8e\u7ea7\u8054EHGO\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u673a\u5236\u5728\u6297\u98ce\u548c\u73af\u5883\u5e72\u6270\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u7eed\u822a\u548c\u80fd\u6548\u662f\u5176\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u6816\u505c\u673a\u5236\u53ef\u5e2e\u52a9\u65e0\u4eba\u673a\u8282\u7701\u80fd\u6e90\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4eff\u751f\u5b66\u65b9\u6cd5\u5f00\u53d1\u4e00\u79cd\u5feb\u901f\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6816\u505c\u65e0\u4eba\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6355\u8747\u8349\u673a\u5236\u7684\u4e3b\u52a8\u67d4\u6027\u6816\u505c\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u7ea7\u8054\u9ad8\u589e\u76ca\u89c2\u6d4b\u5668\uff08EHGO\uff09\u63a7\u5236\u65b9\u6cd5\uff0c\u5b9e\u65f6\u4f30\u8ba1\u548c\u8865\u507f\u5916\u90e8\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6816\u505c\u7ed3\u6784\u5177\u6709\u9ad8\u9002\u5e94\u6027\uff0c\u7ea7\u8054EHGO\u5728\u62b5\u5fa1\u98ce\u548c\u6816\u505c\u5e72\u6270\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u4eff\u751f\u6816\u505c\u673a\u5236\u548c\u63a7\u5236\u65b9\u6cd5\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7684\u6816\u505c\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.13279", "pdf": "https://arxiv.org/pdf/2509.13279", "abs": "https://arxiv.org/abs/2509.13279", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt", "Carlos Gonzalez", "Mingyo Seo", "Luis Sentis"], "title": "HARMONIC: A Content-Centric Cognitive Robotic Architecture", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper introduces HARMONIC, a cognitive-robotic architecture designed for\nrobots in human-robotic teams. HARMONIC supports semantic perception\ninterpretation, human-like decision-making, and intentional language\ncommunication. It addresses the issues of safety and quality of results; aims\nto solve problems of data scarcity, explainability, and safety; and promotes\ntransparency and trust. Two proof-of-concept HARMONIC-based robotic systems are\ndemonstrated, each implemented in both a high-fidelity simulation environment\nand on physical robotic platforms.", "AI": {"tldr": "HARMONIC\u662f\u4e00\u79cd\u8ba4\u77e5\u673a\u5668\u4eba\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u4eba\u7c7b-\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u7684\u8bed\u4e49\u611f\u77e5\u3001\u51b3\u7b56\u548c\u6c9f\u901a\u95ee\u9898\uff0c\u5e76\u5173\u6ce8\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b-\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u3002", "method": "HARMONIC\u67b6\u6784\u7ed3\u5408\u4e86\u8bed\u4e49\u611f\u77e5\u89e3\u91ca\u3001\u7c7b\u4eba\u51b3\u7b56\u548c\u610f\u56fe\u8bed\u8a00\u901a\u4fe1\uff0c\u5e76\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86\u4e24\u4e2a\u57fa\u4e8eHARMONIC\u7684\u6982\u5ff5\u9a8c\u8bc1\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "HARMONIC\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u56e2\u961f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u900f\u660e\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12207", "pdf": "https://arxiv.org/pdf/2509.12207", "abs": "https://arxiv.org/abs/2509.12207", "authors": ["Hanqi Zhu", "Wuyang Zhang", "Xinran Zhang", "Ziyang Tao", "Xinrui Lin", "Yu Zhang", "Jianmin Ji", "Yanyong Zhang"], "title": "UrgenGo: Urgency-Aware Transparent GPU Kernel Launching for Autonomous Driving", "categories": ["cs.OS", "cs.RO"], "comment": null, "summary": "The rapid advancements in autonomous driving have introduced increasingly\ncomplex, real-time GPU-bound tasks critical for reliable vehicle operation.\nHowever, the proprietary nature of these autonomous systems and closed-source\nGPU drivers hinder fine-grained control over GPU executions, often resulting in\nmissed deadlines that compromise vehicle performance. To address this, we\npresent UrgenGo, a non-intrusive, urgency-aware GPU scheduling system that\noperates without access to application source code. UrgenGo implicitly\nprioritizes GPU executions through transparent kernel launch manipulation,\nemploying task-level stream binding, delayed kernel launching, and batched\nkernel launch synchronization. We conducted extensive real-world evaluations in\ncollaboration with a self-driving startup, developing 11 GPU-bound task chains\nfor a realistic autonomous navigation application and implementing our system\non a self-driving bus. Our results show a significant 61% reduction in the\noverall deadline miss ratio, compared to the state-of-the-art GPU scheduler\nthat requires source code modifications.", "AI": {"tldr": "UrgenGo\u662f\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u3001\u7d27\u6025\u611f\u77e5\u7684GPU\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u900f\u660e\u5185\u6838\u542f\u52a8\u64cd\u4f5c\u663e\u8457\u51cf\u5c11\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u622a\u6b62\u65f6\u95f4\u9057\u6f0f\u3002", "motivation": "\u7531\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5c01\u95ed\u6027\uff0cGPU\u8c03\u5ea6\u65e0\u6cd5\u7cbe\u7ec6\u63a7\u5236\uff0c\u5bfc\u81f4\u4efb\u52a1\u622a\u6b62\u65f6\u95f4\u9057\u6f0f\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "UrgenGo\u91c7\u7528\u4efb\u52a1\u7ea7\u6d41\u7ed1\u5b9a\u3001\u5ef6\u8fdf\u5185\u6838\u542f\u52a8\u548c\u6279\u91cf\u5185\u6838\u542f\u52a8\u540c\u6b65\uff0c\u65e0\u9700\u4fee\u6539\u6e90\u4ee3\u7801\u3002", "result": "\u5728\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\uff0cUrgenGo\u6bd4\u73b0\u6709\u6280\u672f\u51cf\u5c1161%\u7684\u622a\u6b62\u65f6\u95f4\u9057\u6f0f\u3002", "conclusion": "UrgenGo\u4e3a\u975e\u4fb5\u5165\u5f0fGPU\u8c03\u5ea6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2509.12250", "pdf": "https://arxiv.org/pdf/2509.12250", "abs": "https://arxiv.org/abs/2509.12250", "authors": ["Yihong Ji", "Yunze Liu", "Yiyao Zhuo", "Weijiang Yu", "Fei Ma", "Joshua Huang", "Fei Yu"], "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted at ACM MM 2025", "summary": "The perception and generation of Human-Object Interaction (HOI) are crucial\nfor fields such as robotics, AR/VR, and human behavior understanding. However,\ncurrent approaches model this task in an offline setting, where information at\neach time step can be drawn from the entire interaction sequence. In contrast,\nin real-world scenarios, the information available at each time step comes only\nfrom the current moment and historical data, i.e., an online setting. We find\nthat offline methods perform poorly in an online context. Based on this\nobservation, we propose two new tasks: Online HOI Generation and Perception. To\naddress this task, we introduce the OnlineHOI framework, a network architecture\nbased on the Mamba framework that employs a memory mechanism. By leveraging\nMamba's powerful modeling capabilities for streaming data and the Memory\nmechanism's efficient integration of historical information, we achieve\nstate-of-the-art results on the Core4D and OAKINK2 online generation tasks, as\nwell as the online HOI4D perception task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u751f\u6210\u4e0e\u611f\u77e5\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8eMamba\u6846\u67b6\u7684OnlineHOI\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u8bb0\u5fc6\u673a\u5236\uff0c\u5728\u591a\u4e2a\u5728\u7ebf\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HOI\u65b9\u6cd5\u5728\u79bb\u7ebf\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5728\u7ebf\u573a\u666f\uff08\u4ec5\u4f9d\u8d56\u5f53\u524d\u548c\u5386\u53f2\u6570\u636e\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5728\u7ebf\u8bbe\u7f6e\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86OnlineHOI\u6846\u67b6\uff0c\u57fa\u4e8eMamba\u67b6\u6784\u5e76\u7ed3\u5408\u8bb0\u5fc6\u673a\u5236\uff0c\u9ad8\u6548\u6574\u5408\u5386\u53f2\u4fe1\u606f\u4ee5\u5904\u7406\u6d41\u5f0f\u6570\u636e\u3002", "result": "\u5728Core4D\u3001OAKINK2\u5728\u7ebf\u751f\u6210\u4efb\u52a1\u53caHOI4D\u5728\u7ebf\u611f\u77e5\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "OnlineHOI\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebfHOI\u4efb\u52a1\u7684\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.12715", "pdf": "https://arxiv.org/pdf/2509.12715", "abs": "https://arxiv.org/abs/2509.12715", "authors": ["Heng Zhang", "Haichuan Hu", "Yaomin Shen", "Weihao Yu", "Yilei Yuan", "Haochen You", "Guo Cheng", "Zijian Zhang", "Lubin Gan", "Huihui Wei", "Hao Zhang", "Jin Huang"], "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non multimodal tasks through scaled architectures and extensive training.\nHowever, existing Mixture of Experts (MoE) approaches face challenges due to\nthe asymmetry between visual and linguistic processing. Visual information is\nspatially complete, while language requires maintaining sequential context. As\na result, MoE models struggle to balance modality-specific features and\ncross-modal interactions. Through systematic analysis, we observe that language\nexperts in deeper layers progressively lose contextual grounding and rely more\non parametric knowledge rather than utilizing the provided visual and\nlinguistic information. To address this, we propose AsyMoE, a novel\narchitecture that models this asymmetry using three specialized expert groups.\nWe design intra-modality experts for modality-specific processing, hyperbolic\ninter-modality experts for hierarchical cross-modal interactions, and\nevidence-priority language experts to suppress parametric biases and maintain\ncontextual grounding. Extensive experiments demonstrate that AsyMoE achieves\n26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific\nMoE respectively, with 25.45% fewer activated parameters than dense models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AsyMoE\u67b6\u6784\uff0c\u9488\u5bf9\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u7684\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u79cd\u4e13\u5bb6\u6a21\u5757\u63d0\u5347\u8de8\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u4e0d\u5bf9\u79f0\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u8bed\u8a00\u4e13\u5bb6\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u9010\u6e10\u5931\u53bb\u4e0a\u4e0b\u6587\u5173\u8054\u6027\u3002", "method": "\u63d0\u51faAsyMoE\uff0c\u5305\u542b\u4e09\u79cd\u4e13\u5bb6\uff1a\u6a21\u6001\u5185\u4e13\u5bb6\u3001\u53cc\u66f2\u8de8\u6a21\u6001\u4e13\u5bb6\u548c\u8bc1\u636e\u4f18\u5148\u8bed\u8a00\u4e13\u5bb6\uff0c\u5206\u522b\u5904\u7406\u6a21\u6001\u7279\u5f02\u6027\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6291\u5236\u53c2\u6570\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAsyMoE\u5728\u51c6\u786e\u7387\u4e0a\u6bd4\u4f20\u7edf\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\u63d0\u534726.58%\u548c15.45%\uff0c\u4e14\u53c2\u6570\u6fc0\u6d3b\u91cf\u51cf\u5c1125.45%\u3002", "conclusion": "AsyMoE\u901a\u8fc7\u9488\u5bf9\u6027\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u4e0d\u5bf9\u79f0\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.13089", "pdf": "https://arxiv.org/pdf/2509.13089", "abs": "https://arxiv.org/abs/2509.13089", "authors": ["Jonas Werheid", "Shengjie He", "Aymen Gannouni", "Anas Abdelrazeq", "Robert H. Schmitt"], "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Quality control of assembly processes is essential in manufacturing to ensure\nnot only the quality of individual components but also their proper integration\ninto the final product. To assist in this matter, automated assembly control\nusing computer vision methods has been widely implemented. However, the costs\nassociated with image acquisition, annotation, and training of computer vision\nalgorithms pose challenges for integration, especially for small- and\nmedium-sized enterprises (SMEs), which often lack the resources for extensive\ntraining, data collection, and manual image annotation. Synthetic data offers\nthe potential to reduce manual data collection and labeling. Nevertheless, its\npractical application in the context of assembly quality remains limited. In\nthis work, we present a novel approach for easily integrable and data-efficient\nvisual assembly control. Our approach leverages simulated scene generation\nbased on computer-aided design (CAD) data and object detection algorithms. The\nresults demonstrate a time-saving pipeline for generating image data in\nmanufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)\nup to 99,5% for correctly identifying instances of synthetic planetary gear\nsystem components within our simulated training data, and up to 93% when\ntransferred to real-world camera-captured testing data. This research\nhighlights the effectiveness of synthetic data generation within an adaptable\npipeline and underscores its potential to support SMEs in implementing\nresource-efficient visual assembly control solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCAD\u6570\u636e\u548c\u5bf9\u8c61\u68c0\u6d4b\u7b97\u6cd5\u7684\u89c6\u89c9\u88c5\u914d\u8d28\u91cf\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528\u5408\u6210\u6570\u636e\u51cf\u5c11\u624b\u52a8\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u53d6\u5f97\u4e86\u9ad8\u6027\u80fd\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u4e2d\u5c0f\u4f01\u4e1a\uff08SMEs\uff09\u5728\u88c5\u914d\u8d28\u91cf\u63a7\u5236\u4e2d\u56e0\u8d44\u6e90\u6709\u9650\u800c\u96be\u4ee5\u5b9e\u65bd\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u573a\u666f\u751f\u6210\uff08\u57fa\u4e8eCAD\u6570\u636e\uff09\u548c\u5bf9\u8c61\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u89c6\u89c9\u88c5\u914d\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u523099.5%\u7684mAP@0.5:0.95\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u523093%\u7684\u6027\u80fd\u3002", "conclusion": "\u5408\u6210\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u9ad8\u6548\u4e14\u53ef\u9002\u5e94\uff0c\u6709\u6f5c\u529b\u652f\u6301\u4e2d\u5c0f\u4f01\u4e1a\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684\u89c6\u89c9\u88c5\u914d\u63a7\u5236\u3002"}}
{"id": "2509.13257", "pdf": "https://arxiv.org/pdf/2509.13257", "abs": "https://arxiv.org/abs/2509.13257", "authors": ["Sriram S. K. S. Narayanan", "Sajad Ahmadi", "Javad Mohammadpour Velni", "Umesh Vaidya"], "title": "Safety Critical Model Predictive Control Using Discrete-Time Control Density Functions", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "This paper presents MPC-CDF, a new approach integrating control density\nfunctions (CDFs) within a model predictive control (MPC) framework to ensure\nsafety-critical control in nonlinear dynamical systems. By using the dual\nformulation of the navigation problem, we incorporate CDFs into the MPC\nframework, ensuring both convergence and safety in a discrete-time setting.\nThese density functions are endowed with a physical interpretation, where the\nassociated measure signifies the occupancy of system trajectories. Leveraging\nthis occupancy-based perspective, we synthesize safety-critical controllers\nusing the proposed MPC-CDF framework. We illustrate the safety properties of\nthis framework using a unicycle model and compare it with a control barrier\nfunction-based method. The efficacy of this approach is demonstrated in the\nautonomous safe navigation of an underwater vehicle, which avoids complex and\narbitrary obstacles while achieving the desired level of safety.", "AI": {"tldr": "MPC-CDF\u662f\u4e00\u79cd\u5c06\u63a7\u5236\u5bc6\u5ea6\u51fd\u6570\uff08CDFs\uff09\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u4fdd\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u3002", "motivation": "\u901a\u8fc7CDFs\u5728MPC\u6846\u67b6\u4e2d\u7684\u96c6\u6210\uff0c\u786e\u4fdd\u975e\u7ebf\u6027\u7cfb\u7edf\u5728\u79bb\u6563\u65f6\u95f4\u4e0b\u7684\u6536\u655b\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u5bfc\u822a\u95ee\u9898\u7684\u5bf9\u5076\u516c\u5f0f\u5c06CDFs\u7eb3\u5165MPC\uff0c\u7ed3\u5408\u5360\u7528\u7387\u89c6\u89d2\u5408\u6210\u5b89\u5168\u5173\u952e\u63a7\u5236\u5668\u3002", "result": "\u5728\u4f7f\u7528\u5355\u8f6e\u6a21\u578b\u548c\u4e0e\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u65b9\u6cd5\u7684\u6bd4\u8f83\u4e2d\uff0c\u8bc1\u660e\u4e86MPC-CDF\u7684\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "MPC-CDF\u5728\u81ea\u52a8\u9a7e\u9a76\u6c34\u4e0b\u8f66\u8f86\u7684\u590d\u6742\u969c\u788d\u89c4\u907f\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u80fd\u529b\u3002"}}
{"id": "2509.13288", "pdf": "https://arxiv.org/pdf/2509.13288", "abs": "https://arxiv.org/abs/2509.13288", "authors": ["Marjorie McShane", "Sergei Nirenburg", "Sanjay Oruganti", "Jesse English"], "title": "Shapes of Cognition for Computational Cognitive Modeling", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Shapes of cognition is a new conceptual paradigm for the computational\ncognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are\nremembered constellations of sensory, linguistic, conceptual, episodic, and\nprocedural knowledge that allow agents to cut through the complexity of real\nlife the same way as people do: by expecting things to be typical, recognizing\npatterns, acting by habit, reasoning by analogy, satisficing, and generally\nminimizing cognitive load to the degree situations permit. Atypical outcomes\nare treated using shapes-based recovery methods, such as learning on the fly,\nasking a human partner for help, or seeking an actionable, even if imperfect,\nsituational understanding. Although shapes is an umbrella term, it is not\nvague: shapes-based modeling involves particular objectives, hypotheses,\nmodeling strategies, knowledge bases, and actual models of wide-ranging\nphenomena, all implemented within a particular cognitive architecture. Such\nspecificity is needed both to vet our hypotheses and to achieve our practical\naims of building useful agent systems that are explainable, extensible, and\nworthy of our trust, even in critical domains. However, although the LEIA\nexample of shapes-based modeling is specific, the principles can be applied\nmore broadly, giving new life to knowledge-based and hybrid AI.", "AI": {"tldr": "\u201cShapes of cognition\u201d\u662f\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u8ba4\u77e5\u5efa\u6a21\u8303\u5f0f\uff0c\u7528\u4e8e\u8bed\u8a00\u8d4b\u80fd\u667a\u80fd\u4f53\uff08LEIAs\uff09\uff0c\u901a\u8fc7\u8bb0\u5fc6\u7684\u77e5\u8bc6\u661f\u5ea7\u7b80\u5316\u73b0\u5b9e\u590d\u6742\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5178\u578b\u5316\u3001\u6a21\u5f0f\u8bc6\u522b\u7b49\u65b9\u5f0f\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u5904\u7406\u975e\u5178\u578b\u60c5\u51b5\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u4e14\u503c\u5f97\u4fe1\u4efb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u201c\u5f62\u72b6\u201d\u7684\u5177\u4f53\u76ee\u6807\u3001\u5047\u8bbe\u3001\u5efa\u6a21\u7b56\u7565\u548c\u77e5\u8bc6\u5e93\uff0c\u5728\u7279\u5b9a\u8ba4\u77e5\u67b6\u6784\u4e2d\u5b9e\u73b0\u5e7f\u6cdb\u73b0\u8c61\u7684\u5efa\u6a21\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8eLEIAs\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u77e5\u8bc6\u578b\u548c\u6df7\u5408\u578bAI\uff0c\u4e3a\u5176\u6ce8\u5165\u65b0\u6d3b\u529b\u3002", "conclusion": "\u201cShapes of cognition\u201d\u4e3a\u667a\u80fd\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u517c\u5177\u7406\u8bba\u9a8c\u8bc1\u4e0e\u5b9e\u8df5\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
