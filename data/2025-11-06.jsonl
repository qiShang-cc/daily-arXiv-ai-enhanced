{"id": "2511.02845", "pdf": "https://arxiv.org/pdf/2511.02845", "abs": "https://arxiv.org/abs/2511.02845", "authors": ["Yuxuan Liu", "Chiya Zhang", "Yifeng Yuan", "Chunlong He", "Weizheng Zhang", "Gaojie Chen"], "title": "AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair", "categories": ["eess.SP", "cs.AI", "physics.ins-det"], "comment": "12 pages, 11 figures", "summary": "The advancement of next-generation Wi-Fi technology heavily relies on sensing\ncapabilities, which play a pivotal role in enabling sophisticated applications.\nIn response to the growing demand for large-scale deployments, contemporary\nWi-Fi sensing systems strive to achieve high-precision perception while\nmaintaining minimal bandwidth consumption and antenna count requirements.\nRemarkably, various AI-driven perception technologies have demonstrated the\nability to surpass the traditional resolution limitations imposed by radar\ntheory. However, the theoretical underpinnings of this phenomenon have not been\nthoroughly investigated in existing research. In this study, we found that\nunder hardware-constrained conditions, the performance gains brought by AI to\nWi-Fi sensing systems primarily originate from two aspects: prior information\nand temporal correlation. Prior information enables the AI to generate\nplausible details based on vague input, while temporal correlation helps reduce\nthe upper bound of sensing error. We developed an AI-based Wi-Fi sensing system\nusing a single transceiver pair and designed experiments focusing on human pose\nestimation and indoor localization to validate the theoretical claims. The\nresults confirm the performance gains contributed by temporal correlation and\nprior information."}
{"id": "2511.02846", "pdf": "https://arxiv.org/pdf/2511.02846", "abs": "https://arxiv.org/abs/2511.02846", "authors": ["Zan Li", "Kyongmin Yeo", "Wesley Gifford", "Lara Marcuse", "Madeline Fields", "Bülent Yener"], "title": "Spatio-Temporal Attention Network for Epileptic Seizure Prediction", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "In this study, we present a deep learning framework that learns complex\nspatio-temporal correlation structures of EEG signals through a Spatio-Temporal\nAttention Network (STAN) for accurate predictions of onset of seizures for\nEpilepsy patients. Unlike existing methods, which rely on feature engineering\nand/or assume fixed preictal durations, our approach simultaneously models\nspatio-temporal correlations through STAN and employs an adversarial\ndiscriminator to distinguish preictal from interictal attention patterns,\nenabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets\ndemonstrates 96.6\\% sensitivity with 0.011/h false detection rate on CHB-MIT,\nand 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming\nstate-of-the-art methods. The framework reliably detects preictal states at\nleast 15 minutes before an onset, with patient-specific windows extending to 45\nminutes, providing sufficient intervention time for clinical applications."}
{"id": "2511.02848", "pdf": "https://arxiv.org/pdf/2511.02848", "abs": "https://arxiv.org/abs/2511.02848", "authors": ["Shantanu Sarkar", "Piotr Nabrzyski", "Saurabh Prasad", "Jose Luis Contreras-Vidal"], "title": "EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "Accepted for presentation at the NeurIPS 2025 Workshop on Foundation\n  Models for the Brain and Body", "summary": "Electroencephalography (EEG) is a widely used non-invasive technique for\nmonitoring brain activity, but low signal-to-noise ratios (SNR) due to various\nartifacts often compromise its utility. Conventional artifact removal methods\nrequire manual intervention or risk suppressing critical neural features during\nfiltering/reconstruction. Recent advances in generative models, including\nVariational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),\nhave shown promise for EEG reconstruction; however, these approaches often lack\nintegrated temporal-spectral-spatial sensitivity and are computationally\nintensive, limiting their suitability for real-time applications like\nbrain-computer interfaces (BCIs). To overcome these challenges, we introduce\nEEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction\nvia cross-subject transfer learning - developed using Keras TensorFlow\n(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume\nconduction across neighboring channels, band-specific convolution encoding, and\ndynamic latent feature extraction through sliding windows. By integrating\nreference-based scaling, the framework ensures continuity across successive\nwindows and generalizes effectively across subjects. This design improves\nspatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean\nspectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate\noverfitting, and maintains computational efficiency for robust, real-time EEG\npreprocessing in neurophysiological and BCI applications."}
{"id": "2511.02849", "pdf": "https://arxiv.org/pdf/2511.02849", "abs": "https://arxiv.org/abs/2511.02849", "authors": ["Beyza Cinar", "Maria Maleshkova"], "title": "Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData", "categories": ["eess.SP", "cs.CV", "eess.IV"], "comment": "11 pages, 5 Tables, 4 Figures, BHI 2025 conference (JBHI special\n  issue)", "summary": "Individualized therapy is driven forward by medical data analysis, which\nprovides insight into the patient's context. In particular, for Type 1 Diabetes\n(T1D), which is an autoimmune disease, relationships between demographics,\nsensor data, and context can be analyzed. However, outliers, noisy data, and\nsmall data volumes cannot provide a reliable analysis. Hence, the research\ndomain requires large volumes of high-quality data. Moreover, missing values\ncan lead to information loss. To address this limitation, this study improves\nthe data quality of DiaData, an integration of 15 separate datasets containing\nglucose values from 2510 subjects with T1D. Notably, we make the following\ncontributions: 1) Outliers are identified with the interquartile range (IQR)\napproach and treated by replacing them with missing values. 2) Small gaps\n($\\le$ 25 min) are imputed with linear interpolation and larger gaps ($\\ge$ 30\nand $<$ 120 min) with Stineman interpolation. Based on a visual comparison,\nStineman interpolation provides more realistic glucose estimates than linear\ninterpolation for larger gaps. 3) After data cleaning, the correlation between\nglucose and heart rate is analyzed, yielding a moderate relation between 15 and\n60 minutes before hypoglycemia ($\\le$ 70 mg/dL). 4) Finally, a benchmark for\nhypoglycemia classification is provided with a state-of-the-art ResNet model.\nThe model is trained with the Maindatabase and Subdatabase II of DiaData to\nclassify hypoglycemia onset up to 2 hours in advance. Training with more data\nimproves performance by 7% while using quality-refined data yields a 2-3% gain\ncompared to raw data."}
{"id": "2511.02937", "pdf": "https://arxiv.org/pdf/2511.02937", "abs": "https://arxiv.org/abs/2511.02937", "authors": ["Mirco Felske", "Jannik Redenius", "Georg Happich", "Julius Schöning"], "title": "Toward an Agricultural Operational Design Domain: A Framework", "categories": ["cs.RO", "cs.SE", "cs.SY", "eess.SY", "I.2.9; I.1.4; J.7"], "comment": "18 pages, 7 figures, 2 tables", "summary": "The agricultural sector increasingly relies on autonomous systems that\noperate in complex and variable environments. Unlike on-road applications,\nagricultural automation integrates driving and working processes, each of which\nimposes distinct operational constraints. Handling this complexity and ensuring\nconsistency throughout the development and validation processes requires a\nstructured, transparent, and verified description of the environment. However,\nexisting Operational Design Domain (ODD) concepts do not yet address the unique\nchallenges of agricultural applications.\n  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,\nwhich can be used to describe and verify the operational boundaries of\nautonomous agricultural systems. The Ag-ODD Framework consists of three core\nelements. First, the Ag-ODD description concept, which provides a structured\nmethod for unambiguously defining environmental and operational parameters\nusing concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model\nderived from the PEGASUS 6-Layer Model, has been extended to include a process\nlayer to capture dynamic agricultural operations. Third, the iterative\nverification process verifies the Ag-ODD against its corresponding logical\nscenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness\nand consistency.\n  Together, these elements provide a consistent approach for creating\nunambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD\nFramework can support the standardization and scalability of environmental\ndescriptions for autonomous agricultural systems."}
{"id": "2511.02850", "pdf": "https://arxiv.org/pdf/2511.02850", "abs": "https://arxiv.org/abs/2511.02850", "authors": ["Youssif Abuzied", "Hassan AbdEltawab", "Abdelrhman Gaber", "Tamer ElBatt"], "title": "ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "This paper presents ECGXtract, a deep learning-based approach for\ninterpretable ECG feature extraction, addressing the limitations of traditional\nsignal processing and black-box machine learning methods. In particular, we\ndevelop convolutional neural network models capable of extracting both temporal\nand morphological features with strong correlations to a clinically validated\nground truth. Initially, each model is trained to extract a single feature,\nensuring precise and interpretable outputs. A series of experiments is then\ncarried out to evaluate the proposed method across multiple setups, including\nglobal versus lead-specific features, different sampling frequencies, and\ncomparisons with other approaches such as ECGdeli. Our findings show that\nECGXtract achieves robust performance across most features with a mean\ncorrelation score of 0.80 with the ground truth for global features, with lead\nII consistently providing the best results. For lead-specific features,\nECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract\nachieves superior results to the state-of-the-art open source ECGdeli as it got\na higher correlation score with the ground truth in 90% of the features.\nFurthermore, we explore the feasibility of extracting multiple features\nsimultaneously utilizing a single model. Semantic grouping is proved to be\neffective for global features, while large-scale grouping and lead-specific\nmulti-output models show notable performance drops. These results highlight the\npotential of structured grouping strategies to balance the computational\nefficiency vs. model accuracy, paving the way for more scalable and clinically\ninterpretable ECG feature extraction systems in limited resource settings."}
{"id": "2511.02994", "pdf": "https://arxiv.org/pdf/2511.02994", "abs": "https://arxiv.org/abs/2511.02994", "authors": ["Syed Mostaquim Ali", "Taufiq Rahman", "Ghazal Farhani", "Mohamed H. Zaki", "Benoit Anctil", "Dominique Charlebois"], "title": "Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "For developing safe Autonomous Driving Systems (ADS), rigorous testing is\nrequired before they are deemed safe for road deployments. Since comprehensive\nconventional physical testing is impractical due to cost and safety concerns,\nVirtual Testing Environments (VTE) can be adopted as an alternative. Comparing\nVTE-generated sensor outputs against their real-world analogues can be a strong\nindication that the VTE accurately represents reality. Correspondingly, this\nwork explores a comprehensive experimental approach to finding evaluation\nmetrics suitable for comparing real-world and simulated LiDAR scans. The\nmetrics were tested in terms of sensitivity and accuracy with different noise,\ndensity, distortion, sensor orientation, and channel settings. From comparing\nthe metrics, we found that Density Aware Chamfer Distance (DCD) works best\nacross all cases. In the second step of the research, a Virtual Testing\nEnvironment was generated using real LiDAR scan data. The data was collected in\na controlled environment with only static objects using an instrumented vehicle\nequipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from\nthe VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans\nwere compared in terms of model perception and geometric similarity. Actual and\nsimulated LiDAR scans have a similar semantic segmentation output with a mIoU\nof 21\\% with corrected intensity and an average density aware chamfer distance\n(DCD) of 0.63. This indicates a slight difference in the geometric properties\nof simulated and real LiDAR scans and a significant difference between model\noutputs. During the comparison, density-aware chamfer distance was found to be\nthe most correlated among the metrics with perception methods."}
{"id": "2511.02851", "pdf": "https://arxiv.org/pdf/2511.02851", "abs": "https://arxiv.org/abs/2511.02851", "authors": ["Rushuang Zhou", "Yuan-Ting Zhang", "M. Jamal Deen", "Yining Dong"], "title": "Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Deploying advanced cardiac artificial intelligence for daily cardiac\nmonitoring is hindered by its reliance on extensive medical data and high\ncomputational resources. Low-cost cardiac intelligence (LCCI) offers a\npromising alternative by using wearable device data, such as 1-lead\nelectrocardiogram (ECG), but it suffers from a significant diagnostic\nperformance gap compared to high-cost cardiac intelligence (HCCI). To bridge\nthis gap, we propose LiteHeart, a semi-supervised knowledge distillation\nframework. LiteHeart introduces a region-aware distillation module to mimic how\ncardiologists focus on diagnostically relevant ECG regions and a cross-layer\nmutual information module to align the decision processes of LCCI and HCCI\nsystems. Using a semi-supervised training strategy, LiteHeart further improves\nmodel robustness under limited supervision. Evaluated on five datasets covering\nover 38 cardiovascular diseases, LiteHeart substantially reduces the\nperformance gap between LCCI and HCCI, outperforming existing methods by 4.27%\nto 7.10% in macro F1 score. These results demonstrate that LiteHeart\nsignificantly enhances the diagnostic capabilities of low-cost cardiac\nintelligence systems, paving the way for scalable, affordable, and accurate\ndaily cardiac healthcare using wearable technologies."}
{"id": "2511.03075", "pdf": "https://arxiv.org/pdf/2511.03075", "abs": "https://arxiv.org/abs/2511.03075", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Yvan R. Petillot"], "title": "A Collaborative Reasoning Framework for Anomaly Diagnostics in Underwater Robotics", "categories": ["cs.RO"], "comment": "Paper was submitted for ICRA 2026", "summary": "The safe deployment of autonomous systems in safety-critical settings\nrequires a paradigm that combines human expertise with AI-driven analysis,\nespecially when anomalies are unforeseen. We introduce AURA (Autonomous\nResilience Agent), a collaborative framework for anomaly and fault diagnostics\nin robotics. AURA integrates large language models (LLMs), a high-fidelity\ndigital twin (DT), and human-in-the-loop interaction to detect and respond to\nanomalous behavior in real time. The architecture uses two agents with clear\nroles: (i) a low-level State Anomaly Characterization Agent that monitors\ntelemetry and converts signals into a structured natural-language problem\ndescription, and (ii) a high-level Diagnostic Reasoning Agent that conducts a\nknowledge-grounded dialogue with an operator to identify root causes, drawing\non external sources. Human-validated diagnoses are then converted into new\ntraining examples that refine the low-level perceptual model. This feedback\nloop progressively distills expert knowledge into the AI, transforming it from\na static tool into an adaptive partner. We describe the framework's operating\nprinciples and provide a concrete implementation, establishing a pattern for\ntrustworthy, continually improving human-robot teams."}
{"id": "2511.02852", "pdf": "https://arxiv.org/pdf/2511.02852", "abs": "https://arxiv.org/abs/2511.02852", "authors": ["Shengze Xue", "Yu Ren", "Jiacheng Hong", "Run Ni", "Shuangjiu Xiao", "Deli Dong"], "title": "Real-Time Interactive Hybrid Ocean: Spectrum-Consistent Wave Particle-FFT Coupling", "categories": ["eess.SP", "cs.GR", "cs.MM"], "comment": null, "summary": "Fast Fourier Transform-based (FFT) spectral oceans are widely adopted for\ntheir efficiency and large-scale realism, but they assume global stationarity\nand spatial homogeneity, making it difficult to represent non-uniform seas and\nnear-field interactions (e.g., ships and floaters). In contrast, wave particles\ncapture local wakes and ripples, yet are costly to maintain at scale and hard\nto match global spectral statistics.We present a real-time interactive hybrid\nocean: a global FFT background coupled with local wave-particle (WP) patch\nregions around interactive objects, jointly driven under a unified set of\nspectral parameters and dispersion. At patch boundaries, particles are injected\naccording to the same directional spectrum as the FFT, aligning the local\nfrequency-direction distribution with the background and matching energy\ndensity, without disturbing the far field.Our approach introduces two main\ninnovations: (1) Hybrid ocean representation. We couple a global FFT background\nwith local WP patches under a unified spectrum, achieving large-scale spectral\nconsistency while supporting localized wakes and ripples.(2) Frequency-bucketed\nimplementation. We design a particle sampling and GPU-parallel synthesis scheme\nbased on frequency buckets, which preserves spectral energy consistency and\nsustains real-time interactive performance.Together, these innovations enable a\nunified framework that delivers both large-scale spectral realism and\nfine-grained interactivity in real time."}
{"id": "2511.03077", "pdf": "https://arxiv.org/pdf/2511.03077", "abs": "https://arxiv.org/abs/2511.03077", "authors": ["R. Khorrambakht", "Joaquim Ortiz-Haro", "Joseph Amigo", "Omar Mostafa", "Daniel Dugas", "Franziska Meier", "Ludovic Righetti"], "title": "WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models", "categories": ["cs.RO"], "comment": null, "summary": "Robots must understand their environment from raw sensory inputs and reason\nabout the consequences of their actions in it to solve complex tasks. Behavior\nCloning (BC) leverages task-specific human demonstrations to learn this\nknowledge as end-to-end policies. However, these policies are difficult to\ntransfer to new tasks, and generating training data is challenging because it\nrequires careful demonstrations and frequent environment resets. In contrast to\nsuch policy-based view, in this paper we take a model-based approach where we\ncollect a few hours of unstructured easy-to-collect play data to learn an\naction-conditioned visual world model, a diffusion-based action sampler, and\noptionally a reward model. The world model -- in combination with the action\nsampler and a reward model -- is then used to optimize long sequences of\nactions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are\nexecuted on the robot via a zeroth-order Model Predictive Controller (MPC). We\nshow that the action sampler mitigates hallucinations of the world model during\nplanning and validate our approach on 3 real-world robotic tasks with varying\nlevels of planning and modeling complexity. Our experiments support the\nhypothesis that planning leads to a significant improvement over BC baselines\non a standard manipulation test environment."}
{"id": "2511.02853", "pdf": "https://arxiv.org/pdf/2511.02853", "abs": "https://arxiv.org/abs/2511.02853", "authors": ["Young-Seok Kweon", "Gi-Hwan Shin", "Ji-Yong Kim", "Bokyeong Ryu", "Seong-Whan Lee"], "title": "Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "30 pages, 8 figures", "summary": "Conscious state estimation is important in various medical settings,\nincluding sleep staging and anesthesia management, to ensure patient safety and\noptimize health outcomes. Traditional methods predominantly utilize\nelectroencephalography (EEG), which faces challenges such as high sensitivity\nto noise and the requirement for controlled environments. In this study, we\npropose the consciousness-ECG transformer that leverages electrocardiography\n(ECG) signals for non-invasive and reliable conscious state estimation. Our\napproach employs a transformer with decoupled query attention to effectively\ncapture heart rate variability features that distinguish between conscious and\nunconscious states. We implemented the conscious state estimation system with\nreal-time monitoring and validated our system on datasets involving sleep\nstaging and anesthesia level monitoring during surgeries. Experimental results\ndemonstrate that our model outperforms baseline models, achieving accuracies of\n0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our\nmodel achieves the highest area under curve values of 0.786 and 0.895 on sleep\nstaging and anesthesia level monitoring, respectively. The proposed system\noffers a practical and robust alternative to EEG-based methods, particularly\nsuited for dynamic clinical environments. Our results highlight the potential\nof ECG-based consciousness monitoring to enhance patient safety and advance our\nunderstanding of conscious states."}
{"id": "2511.03078", "pdf": "https://arxiv.org/pdf/2511.03078", "abs": "https://arxiv.org/abs/2511.03078", "authors": ["Rohan Kota", "Kaival Shah", "J. Edward Colgate", "Gregory Reardon"], "title": "3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors", "categories": ["cs.RO"], "comment": null, "summary": "Tactile sensing plays a key role in enabling dexterous and reliable robotic\nmanipulation, but realizing this capability requires substantial calibration to\nconvert raw sensor readings into physically meaningful quantities. Despite its\nnear-universal necessity, the calibration process remains ad hoc and\nlabor-intensive. Here, we introduce \\libname{}, an open-source library that\ntransforms a low-cost 3D printer into an automated probing device capable of\ngenerating large volumes of labeled training data for tactile sensor\ncalibration. We demonstrate the utility of \\libname{} by calibrating two\ncommercially available vision-based tactile sensors, DIGIT and GelSight Mini,\nto reconstruct high-quality depth maps using the collected data and a custom\nconvolutional neural network. In addition, we perform a data ablation study to\ndetermine how much data is needed for accurate calibration, providing practical\nguidelines for researchers working with these specific sensors, and we\nbenchmark the trained models on previously unseen objects to evaluate\ncalibration accuracy and generalization performance. By automating tactile\nsensor calibration, \\libname{} can accelerate tactile sensing research,\nsimplify sensor deployment, and promote the practical integration of tactile\nsensing in robotic platforms."}
{"id": "2511.02880", "pdf": "https://arxiv.org/pdf/2511.02880", "abs": "https://arxiv.org/abs/2511.02880", "authors": ["Zehui Zhan", "Yaojun Hu", "Jiajing Zhan", "Wanchen Lian", "Wanqing Wu", "Jintai Chen"], "title": "NEF-NET+: Adapting Electrocardio panorama in the wild", "categories": ["eess.SP", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Conventional multi-lead electrocardiogram (ECG) systems capture cardiac\nsignals from a fixed set of anatomical viewpoints defined by lead placement.\nHowever, certain cardiac conditions (e.g., Brugada syndrome) require\nadditional, non-standard viewpoints to reveal diagnostically critical patterns\nthat may be absent in standard leads. To systematically overcome this\nlimitation, Nef-Net was recently introduced to reconstruct a continuous\nelectrocardiac field, enabling virtual observation of ECG signals from\narbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net\noperates under idealized assumptions and faces in-the-wild challenges, such as\nlong-duration ECG modeling, robustness to device-specific signal artifacts, and\nsuboptimal lead placement calibration. This paper presents NEF-NET+, an\nenhanced framework for realistic panoramic ECG synthesis that supports\narbitrary-length signal synthesis from any desired view, generalizes across ECG\ndevices, and compensates for operator-induced deviations in electrode\nplacement. These capabilities are enabled by a newly designed model\narchitecture that performs direct view transformation, incorporating a workflow\ncomprising offline pretraining, device calibration tuning steps as well as an\non-the-fly calibration step for patient-specific adaptation. To rigorously\nevaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama\nbenchmark, called Panobench, comprising 5367 recordings with 48-view per\nsubject, capturing the full spatial variability of cardiac electrical activity.\nExperimental results show that NEF-NET+ delivers substantial improvements over\nNef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The\ncode and Panobench will be released in a subsequent publication."}
{"id": "2511.03165", "pdf": "https://arxiv.org/pdf/2511.03165", "abs": "https://arxiv.org/abs/2511.03165", "authors": ["Raj Surya Rajendran Kathirvel", "Zach A Chavis", "Stephen J. Guy", "Karthik Desingh"], "title": "SENT Map -- Semantically Enhanced Topological Maps with Foundation Models", "categories": ["cs.RO"], "comment": "Accepted at ICRA 2025 Workshop on Foundation Models and\n  Neuro-Symbolic AI for Robotics", "summary": "We introduce SENT-Map, a semantically enhanced topological map for\nrepresenting indoor environments, designed to support autonomous navigation and\nmanipulation by leveraging advancements in foundational models (FMs). Through\nrepresenting the environment in a JSON text format, we enable semantic\ninformation to be added and edited in a format that both humans and FMs\nunderstand, while grounding the robot to existing nodes during planning to\navoid infeasible states during deployment. Our proposed framework employs a two\nstage approach, first mapping the environment alongside an operator with a\nVision-FM, then using the SENT-Map representation alongside a natural-language\nquery within an FM for planning. Our experimental results show that\nsemantic-enhancement enables even small locally-deployable FMs to successfully\nplan over indoor environments."}
{"id": "2511.02884", "pdf": "https://arxiv.org/pdf/2511.02884", "abs": "https://arxiv.org/abs/2511.02884", "authors": ["Dariush Salami", "Nima Bahmani", "Hüseyin Yiğitler", "Stephan Sigg"], "title": "Adaptive Internal Calibration for Temperature-Robust mmWave FMCW Radars", "categories": ["eess.SP", "cs.HC"], "comment": "Accepted to be published in ACM international joint conference on\n  Pervasive and Ubiquitous Computing (UbiComp)", "summary": "We present a novel internal calibration framework for Millimeter- Wave\n(mmWave) Frequency-Modulated Continuous-Wave (FMCW) radars to ensure robust\nperformance under internal temperature variations, tailored for deployment in\ndense wireless networks. Our approach mitigates the impact of\ntemperature-induced drifts in radar hardware, enhancing reliability. We propose\na temperature compensation model that leverages internal sensor data and signal\nprocessing techniques to maintain measurement accuracy. Experimental results\ndemonstrate improved robustness across a range of internal temperature\nconditions, with minimal computational overhead, ensuring scalability in dense\nnetwork environments. The framework also incorporates ethical design\nprinciples, avoiding reliance on sensitive external data. The proposed scheme\nreduces the Pearson correlation between the amplitude of the Intermediate\nFrequency (IF) signal and internal temperature drift up to 84%, significantly\nmitigating the temperature drift."}
{"id": "2511.03167", "pdf": "https://arxiv.org/pdf/2511.03167", "abs": "https://arxiv.org/abs/2511.03167", "authors": ["Xin Liu", "Jinze Wu", "Yinghui Li", "Chenkun Qi", "Yufei Xue", "Feng Gao"], "title": "Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Multi-legged robots offer enhanced stability to navigate complex terrains\nwith their multiple legs interacting with the environment. However, how to\neffectively coordinate the multiple legs in a larger action exploration space\nto generate natural and robust movements is a key issue. In this paper, we\nintroduce a motion prior-based approach, successfully applying deep\nreinforcement learning algorithms to a real hexapod robot. We generate a\ndataset of optimized motion priors, and train an adversarial discriminator\nbased on the priors to guide the hexapod robot to learn natural gaits. The\nlearned policy is then successfully transferred to a real hexapod robot, and\ndemonstrate natural gait patterns and remarkable robustness without visual\ninformation in complex terrains. This is the first time that a reinforcement\nlearning controller has been used to achieve complex terrain walking on a real\nhexapod robot."}
{"id": "2511.02938", "pdf": "https://arxiv.org/pdf/2511.02938", "abs": "https://arxiv.org/abs/2511.02938", "authors": ["Sepideh KhakzadGharamaleki", "Hassan Rivaz", "Brandon Helfield"], "title": "From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Conventional pulse-echo ultrasound suffers when low-cost probes deliver only\nnarrow fractional bandwidths, elongating pulses and erasing high-frequency\ndetail. We address this limitation by learning a data-driven mapping from\nband-limited to broadband spectrogram of radio-frequency (RF) lines. To this\nend, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on\nsimulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst\nphantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by\n6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also\nsharpens point-target rows in a completely unseen resolution phantom,\ndemonstrating strong out-of-distribution generalisation without sacrificing\nframe rate or phase information. These results indicate that a purely software\nupgrade can endow installed narrow-band probes with broadband-like performance,\npotentially widening access to high-resolution ultrasound in\nresource-constrained settings."}
{"id": "2511.03181", "pdf": "https://arxiv.org/pdf/2511.03181", "abs": "https://arxiv.org/abs/2511.03181", "authors": ["Rewida Ali", "Cristian C. Beltran-Hernandez", "Weiwei Wan", "Kensuke Harada"], "title": "Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Human-robot cooperation is essential in environments such as warehouses and\nretail stores, where workers frequently handle deformable objects like paper,\nbags, and fabrics. Coordinating robotic actions with human assistance remains\ndifficult due to the unpredictable dynamics of deformable materials and the\nneed for adaptive force control. To explore this challenge, we focus on the\ntask of gift wrapping, which exemplifies a long-horizon manipulation problem\ninvolving precise folding, controlled creasing, and secure fixation of paper.\nSuccess is achieved when the robot completes the sequence to produce a neatly\nwrapped package with clean folds and no tears.\n  We propose a learning-based framework that integrates a high-level task\nplanner powered by a large language model (LLM) with a low-level hybrid\nimitation learning (IL) and reinforcement learning (RL) policy. At its core is\na Sub-task Aware Robotic Transformer (START) that learns a unified policy from\nhuman demonstrations. The key novelty lies in capturing long-range temporal\ndependencies across the full wrapping sequence within a single model. Unlike\nvanilla Action Chunking with Transformer (ACT), typically applied to short\ntasks, our method introduces sub-task IDs that provide explicit temporal\ngrounding. This enables robust performance across the entire wrapping process\nand supports flexible execution, as the policy learns sub-goals rather than\nmerely replicating motion sequences.\n  Our framework achieves a 97% success rate on real-world wrapping tasks. We\nshow that the unified transformer-based policy reduces the need for specialized\nmodels, allows controlled human supervision, and effectively bridges high-level\nintent with the fine-grained force control required for deformable object\nmanipulation."}
{"id": "2511.03130", "pdf": "https://arxiv.org/pdf/2511.03130", "abs": "https://arxiv.org/abs/2511.03130", "authors": ["Ved Prakash Dubey", "Shovan Bhaumik"], "title": "Consensus Tracking of an Underwater Vehicle Using Weighted Harmonic Mean Density", "categories": ["eess.SP"], "comment": null, "summary": "This paper addresses an underwater target tracking problem in which a large\nnumber of sonobuoy sensors are deployed on a surveillance region. The region is\ndivided into several sub-regions, where a single tracker, capable of generating\ntrack is installed. Each sonobuoy can measure the direction of arrival of\nacoustic signals (known as bearing angles) and communicate the measurements\nwith the local tracker. Further, each local tracker can communicate with all\nother trackers, where each of them can exchange their estimate and finally a\nconsensus is reached. We propose a weighted harmonic mean density (HMD) based\ntracking to reach a consensus and provide a solution for the fusion of Gaussian\ndensities. In this approach, optimal weights are assigned by minimizing the\nKullback-Leibler divergence measure. Performance of the proposed method is\nmeasured using root mean square error, percentage of track divergence, and\nnormalized estimation error squared. Simulation results demonstrate that the\noptimized HMD-based fusion outperforms existing fusion methods during a\ndistributed tracking."}
{"id": "2511.03189", "pdf": "https://arxiv.org/pdf/2511.03189", "abs": "https://arxiv.org/abs/2511.03189", "authors": ["Zeqing Zhang", "Weifeng Lu", "Lei Yang", "Wei Jing", "Bowei Tang", "Jia Pan"], "title": "Collaborative Assembly Policy Learning of a Sightless Robot", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "Accepted by IEEE ROBIO 2025", "summary": "This paper explores a physical human-robot collaboration (pHRC) task\ninvolving the joint insertion of a board into a frame by a sightless robot and\na human operator. While admittance control is commonly used in pHRC tasks, it\ncan be challenging to measure the force/torque applied by the human for\naccurate human intent estimation, limiting the robot's ability to assist in the\ncollaborative task. Other methods that attempt to solve pHRC tasks using\nreinforcement learning (RL) are also unsuitable for the board-insertion task\ndue to its safety constraints and sparse rewards. Therefore, we propose a novel\nRL approach that utilizes a human-designed admittance controller to facilitate\nmore active robot behavior and reduce human effort. Through simulation and\nreal-world experiments, we demonstrate that our approach outperforms admittance\ncontrol in terms of success rate and task completion time. Additionally, we\nobserved a significant reduction in measured force/torque when using our\nproposed approach compared to admittance control. The video of the experiments\nis available at https://youtu.be/va07Gw6YIog."}
{"id": "2511.03133", "pdf": "https://arxiv.org/pdf/2511.03133", "abs": "https://arxiv.org/abs/2511.03133", "authors": ["Ziheng Zhang", "Wen Chen", "Qingqing Wu", "Haoran Qin", "Zhendong Li", "Qiong Wu"], "title": "Analysis and Algorithm for Multi IRS Collaborative Localization via Hybrid Time Angle Estimation", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a novel multiple intelligent reflecting surfaces (IRSs)\ncollaborative hybrid localization system, which involves deploying multiple\nIRSs near the target area and achieving target localization through joint time\ndelay and angle estimation. Specifically, echo signals from all reflective\nelements are received by each sensor and jointly processed to estimate the time\ndelay and angle parameters. Based on the above model, we derive the Fisher\nInformation Matrix (FIM) for cascaded delay, Angle of Arrival (AOA), and Angle\nof Departure (AOD) estimation in semi passive passive models, along with the\ncorresponding Cramer Rao Bound (CRB). To achieve precise estimation close to\nthe CRB, we design efficient algorithms for angle and location estimation. For\nangle estimation, reflective signals are categorized into three cases based on\ntheir rank, with different signal preprocessing. By constructing an atomic norm\nset and minimizing the atomic norm, the joint angle estimation problem is\ntransformed into a convex optimization problem, and low-complexity estimation\nof multiple AOA and AOD pairs is achieved using the Alternating Direction\nMethod of Multipliers (ADMM). For location estimation, we propose a three-stage\nlocalization algorithm that combines weighted least squares, total least\nsquares, and quadratic correction to handle errors in the coefficient matrix\nand observation vector, thus improving accuracy. Numerical simulations validate\nthe superiority of the proposed system, demonstrating that the system's\ncollaboration, hybrid localization, and distributed deployment provide\nsubstantial benefits, as well as the accuracy of the proposed estimation\nalgorithms, particularly in low signal to noise ratio (SNR) condition."}
{"id": "2511.03400", "pdf": "https://arxiv.org/pdf/2511.03400", "abs": "https://arxiv.org/abs/2511.03400", "authors": ["Minquan Gao", "Xinyi Li", "Qing Yan", "Xiaojian Sun", "Xiaopan Zhang", "Chien-Ming Huang", "Jiachen Li"], "title": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement", "categories": ["cs.RO"], "comment": "8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M", "summary": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies."}
{"id": "2511.03220", "pdf": "https://arxiv.org/pdf/2511.03220", "abs": "https://arxiv.org/abs/2511.03220", "authors": ["Tianhao Mao", "Le Liang", "Jie Yang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "Multimodal-Wireless: A Large-Scale Dataset for Sensing and Communication", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents Multimodal-Wireless, an open-source multimodal sensing\ndataset designed for wireless communication research. The dataset is generated\nthrough an integrated and customizable data pipeline built upon the CARLA\nsimulator and Sionna framework. It contains approximately 160,000 frames\ncollected across four virtual towns, sixteen communication scenarios, and three\nweather conditions, encompassing multiple sensing modalities--communication\nchannel, light detection and ranging, RGB and depth cameras, inertial\nmeasurement unit, and radar. This paper provides a comprehensive overview of\nthe dataset, outlining its key features, overall framework, and technical\nimplementation details. In addition, it explores potential research\napplications concerning communication and collaborative perception, exemplified\nby beam prediction using a multimodal large language model. The dataset is open\nin https://le-liang.github.io/mmw/."}
{"id": "2511.03444", "pdf": "https://arxiv.org/pdf/2511.03444", "abs": "https://arxiv.org/abs/2511.03444", "authors": ["Vesna Poprcova", "Iulia Lefter", "Martijn Warnier", "Frances Brazier"], "title": "Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach", "categories": ["cs.RO"], "comment": "Accepted at Value Engineering in AI (VALE) Workshop (ECAI 2025)", "summary": "Social anxiety is a prevalent mental health condition that can significantly\nimpact overall well-being and quality of life. Despite its widespread effects,\nadequate support or treatment for social anxiety is often insufficient.\nAdvances in technology, particularly in social robotics, offer promising\nopportunities to complement traditional mental health. As an initial step\ntoward developing effective solutions, it is essential to understand the values\nthat shape what is considered meaningful, acceptable, and helpful. In this\nstudy, a participatory design workshop was conducted with mental health\nacademic researchers to elicit the underlying values that should inform the\ndesign of socially assistive robots for social anxiety support. Through\ncreative, reflective, and envisioning activities, participants explored\nscenarios and design possibilities, allowing for systematic elicitation of\nvalues, expectations, needs, and preferences related to robot-supported\ninterventions. The findings reveal rich insights into design-relevant\nvalues-including adaptivity, acceptance, and efficacy-that are core to support\nfor individuals with social anxiety. This study highlights the significance of\na research-led approach to value elicitation, emphasising user-centred and\ncontext-aware design considerations in the development of socially assistive\nrobots."}
{"id": "2511.03283", "pdf": "https://arxiv.org/pdf/2511.03283", "abs": "https://arxiv.org/abs/2511.03283", "authors": ["Zhiyuan Zhai", "Wei Ni", "Xin Wang", "Dusit Niyato", "Ekram Hossain"], "title": "Integrated Sensing and Communication with UAV Swarms via Decentralized Consensus ADMM", "categories": ["eess.SP"], "comment": null, "summary": "UAV swarms can form virtual antenna arrays to exploit additional spatial\ndegrees of freedom and enhance integrated sensing and communication (ISAC). The\noptimization of UAV positions is challenging due to the distributed nature of\nswarms and the lack of a global view at individual UAVs.\n  This paper presents a new decentralized optimization framework that allows\nUAVs to decide their locations in parallel and reach consensus on a globally\noptimal swarm geometry for ISAC.\n  Specifically, we derive the achievable uplink rate and Cram\\'er-Rao Bound\n(CRB) as tractable metrics for communication and sensing, respectively.\n  The UAV positions are optimized to balance maximizing the communication rate\nand minimizing the CRB.\n  To solve this non-convex problem with coupled variables, we develop a\ndecentralized consensus alternating direction method of multipliers (ADMM)\nalgorithm, which enables the UAVs to iteratively align their local updates and\nreach consensus.\n  The algorithm decomposes the global objective into local projection updates,\nproxy-assisted consensus coordination, and lightweight dual updates, ensuring\nscalability and consistency throughout the swarm.\n  Simulations demonstrate that the proposed consensus ADMM algorithm converges\nrapidly with strong scalability, and that the UAV swarm significantly\noutperforms fixed-array baselines in both communication and sensing\nperformance."}
{"id": "2511.03481", "pdf": "https://arxiv.org/pdf/2511.03481", "abs": "https://arxiv.org/abs/2511.03481", "authors": ["Jianbo Yuan", "Haohua Zhu", "Jing Dai", "Sheng Yi"], "title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages 18 fogures, IEEE RAL accept", "summary": "The human hand plays a vital role in daily life and industrial applications,\nyet replicating its multifunctional capabilities-including motion, sensing, and\ncoordinated manipulation-with robotic systems remains a formidable challenge.\nDeveloping a dexterous robotic hand requires balancing human-like agility with\nengineering constraints such as complexity, size-to-weight ratio, durability,\nand force-sensing performance. This letter presents Dex-Hand 021, a\nhigh-performance, cable-driven five-finger robotic hand with 12 active and 7\npassive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight\n1 kg design. We propose a proprioceptive force-sensing-based admittance control\nmethod to enhance manipulation. Experimental results demonstrate its superior\nperformance: a single-finger load capacity exceeding 10 N, fingertip\nrepeatability under 0.001 m, and force estimation errors below 0.2 N. Compared\nto PID control, joint torques in multi-object grasping are reduced by 31.19%,\nsignificantly improves force-sensing capability while preventing overload\nduring collisions. The hand excels in both power and precision grasps,\nsuccessfully executing 33 GRASP taxonomy motions and complex manipulation\ntasks. This work advances the design of lightweight, industrial-grade dexterous\nhands and enhances proprioceptive control, contributing to robotic manipulation\nand intelligent manufacturing."}
{"id": "2511.03284", "pdf": "https://arxiv.org/pdf/2511.03284", "abs": "https://arxiv.org/abs/2511.03284", "authors": ["Zhiyuan Zhai", "Xiaojun Yuan", "Xin Wang", "Geoffrey Ye Li"], "title": "Decentralized Federated Learning with Distributed Aggregation Weight Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Decentralized federated learning (DFL) is an emerging paradigm to enable edge\ndevices collaboratively training a learning model using a device-to-device\n(D2D) communication manner without the coordination of a parameter server (PS).\nAggregation weights, also known as mixing weights, are crucial in DFL process,\nand impact the learning efficiency and accuracy. Conventional design relies on\na so-called central entity to collect all local information and conduct system\noptimization to obtain appropriate weights. In this paper, we develop a\ndistributed aggregation weight optimization algorithm to align with the\ndecentralized nature of DFL. We analyze convergence by quantitatively capturing\nthe impact of the aggregation weights over decentralized communication\nnetworks. Based on the analysis, we then formulate a learning performance\noptimization problem by designing the aggregation weights to minimize the\nderived convergence bound. The optimization problem is further transformed as\nan eigenvalue optimization problem and solved by our proposed subgradient-based\nalgorithm in a distributed fashion. In our algorithm, edge devices only need\nlocal information to obtain the optimal aggregation weights through local (D2D)\ncommunications, just like the learning itself. Therefore, the optimization,\ncommunication, and learning process can be all conducted in a distributed\nfashion, which leads to a genuinely distributed DFL system. Numerical results\ndemonstrate the superiority of the proposed algorithm in practical DFL\ndeployment."}
{"id": "2511.03497", "pdf": "https://arxiv.org/pdf/2511.03497", "abs": "https://arxiv.org/abs/2511.03497", "authors": ["Lei Fu", "Sahar Salimpour", "Leonardo Militano", "Harry Edelman", "Jorge Peña Queralta", "Giovanni Toffetti"], "title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications", "categories": ["cs.RO", "cs.AI", "cs.SE"], "comment": null, "summary": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags."}
{"id": "2511.03290", "pdf": "https://arxiv.org/pdf/2511.03290", "abs": "https://arxiv.org/abs/2511.03290", "authors": ["Jinhao Yi", "Weijun Gao", "Chong Han"], "title": "Diffusion-Driven Terahertz Air-Ground Communications under Dynamic Atmospheric Turbulence", "categories": ["eess.SP"], "comment": null, "summary": "The ever-increasing demand for ultra-high data rates in space-air-ground\nintegrated networks (SAGINs) has rendered terahertz THz communications a\npromising technology owing to its exceptionally broad and continuous spectrum\nresources. Nevertheless, in air-ground (AG) scenarios, the high mobility of\naircraft induces intense and rapidly fluctuating turbulence, leading to\nadditional propagation loss that is often overlooked in existing studies. To\nbridge this gap, this paper presents an AI-empowered THz AG communication\nframework that explicitly models turbulence-induced attenuation through fluid\ndynamics and integrates it into an adaptive optimization paradigm for\ncommunication performance enhancement. Specifically, a fluid-dynamics-informed\nattenuation model is established to characterize aircraft-generated turbulence\nand quantify its impact on THz signal propagation. Building upon this model, a\njoint power-attitude optimization problem is formulated to adaptively allocate\ntransmit power and adjust aircraft attitude for maximizing link capacity. The\noptimization problem is efficiently solved using a diffusion-based algorithm\nthat learns the nonlinear relationship between flight configuration and\nturbulence-induced attenuation. Comprehensive numerical evaluations demonstrate\nthat the turbulence-induced attenuation ranges from 18 to 28 dB under attacking\nangles between -10 degree and 10 degree at 0.7 Mach, verifying the pronounced\nimpact of aircraft-induced turbulence on THz propagation. Furthermore, the\nproposed framework attains an average capacity of 11.241 bps/Hz, substantially\noutperforming existing strategies by 22.8% and 66.5%, and approaching\napproximately 98% of the theoretical capacity limit."}
{"id": "2511.03550", "pdf": "https://arxiv.org/pdf/2511.03550", "abs": "https://arxiv.org/abs/2511.03550", "authors": ["Hong Wang", "Ridhima Phatak", "James Ocampo", "Zhao Han"], "title": "Indicating Robot Vision Capabilities with Augmented Reality", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Research indicates that humans can mistakenly assume that robots and humans\nhave the same field of view (FoV), possessing an inaccurate mental model of\nrobots. This misperception may lead to failures during human-robot\ncollaboration tasks where robots might be asked to complete impossible tasks\nabout out-of-view objects. The issue is more severe when robots do not have a\nchance to scan the scene to update their world model while focusing on assigned\ntasks. To help align humans' mental models of robots' vision capabilities, we\npropose four FoV indicators in augmented reality (AR) and conducted a user\nhuman-subjects experiment (N=41) to evaluate them in terms of accuracy,\nconfidence, task efficiency, and workload. These indicators span a spectrum\nfrom egocentric (robot's eye and head space) to allocentric (task space).\nResults showed that the allocentric blocks at the task space had the highest\naccuracy with a delay in interpreting the robot's FoV. The egocentric indicator\nof deeper eye sockets, possible for physical alteration, also increased\naccuracy. In all indicators, participants' confidence was high while cognitive\nload remained low. Finally, we contribute six guidelines for practitioners to\napply our AR indicators or physical alterations to align humans' mental models\nwith robots' vision capabilities."}
{"id": "2511.03291", "pdf": "https://arxiv.org/pdf/2511.03291", "abs": "https://arxiv.org/abs/2511.03291", "authors": ["Zhiyuan Zhai", "Shuyan Hu", "Wei Ni", "Xiaojun Yuan", "Xin Wang"], "title": "Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks", "categories": ["eess.SP"], "comment": null, "summary": "Decentralized machine learning (DML) supports collaborative training in\nlarge-scale networks with no central server. It is sensitive to the quality and\nreliability of inter-device communications that result in time-varying and\nstochastic topologies. This paper studies the impact of unreliable\ncommunication on the convergence of DML and establishes a direct connection\nbetween the spectral properties of the mixing process and the global\nperformance. We provide rigorous convergence guarantees under random topologies\nand derive bounds that characterize the impact of the expected mixing matrix's\nspectral properties on learning. We formulate a spectral optimization problem\nthat minimizes the spectral radius of the expected second-order mixing matrix\nto enhance the convergence rate under probabilistic link failures. To solve\nthis non-smooth spectral problem in a fully decentralized manner, we design an\nefficient subgradient-based algorithm that integrates Chebyshev-accelerated\neigenvector estimation with local update and aggregation weight adjustment,\nwhile ensuring symmetry and stochasticity constraints without central\ncoordination. Experiments on a realistic low Earth orbit (LEO) satellite\nconstellation with time-varying inter-satellite link models and real-world\nremote sensing data demonstrate the feasibility and effectiveness of the\nproposed method. The method significantly improves classification accuracy and\nconvergence efficiency compared to existing baselines, validating its\napplicability in satellite and other decentralized systems."}
{"id": "2511.03571", "pdf": "https://arxiv.org/pdf/2511.03571", "abs": "https://arxiv.org/abs/2511.03571", "authors": ["Hao Shi", "Ze Wang", "Shangwei Guo", "Mengfei Duan", "Song Wang", "Teng Chen", "Kailun Yang", "Lin Wang", "Kaiwei Wang"], "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Datasets and code will be publicly available at\n  https://github.com/MasterHow/OneOcc", "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc."}
{"id": "2511.03292", "pdf": "https://arxiv.org/pdf/2511.03292", "abs": "https://arxiv.org/abs/2511.03292", "authors": ["Qiuyuan Yang", "Cunhua Pan", "Ruidong Li", "Zhenkun Zhang", "Hong Ren", "Changhong Wang", "Jiangzhou Wang"], "title": "UAV SAR Imaging with 5G NR OFDM Signals in NLOS Environments", "categories": ["eess.SP"], "comment": null, "summary": "The integration of sensing and communication (ISAC) has significant potential\nfor future wireless systems, enabling efficient spectrum utilization and novel\napplication scenarios. In this paper, we propose a cooperative ISAC framework\nfor synthetic aperture radar (SAR) imaging by leveraging orthogonal frequency\ndivision multiplexing (OFDM) communication signals. We address the challenge of\nsevere imaging degradation in non-line-of-sight (NLOS) environments under the\nQUAsi Deterministic RadIo channel GenerAtor (QuaDRiGa). To detect weak signals\nand eliminate false points, we develop a two-stage compressed sensing-space\nalternating generalized expectation maximization (CS-SAGE) scheme for\nhigh-precision scatterer localization. In stage I, orthogonal matching pursuit\n(OMP) is employed for coarse estimation to identify the approximate locations\nof dominant scatterers. Then, the SAGE algorithm in stage II performs fine\nestimation to accurately extract scatterer parameters. Simulation results\nvalidate the effectiveness of the proposed cooperative ISAC framework, and\nprovide valuable insights for practical system design."}
{"id": "2511.03576", "pdf": "https://arxiv.org/pdf/2511.03576", "abs": "https://arxiv.org/abs/2511.03576", "authors": ["Aniol Civit", "Antonio Andriella", "Carles Sierra", "Guillem Alenyà"], "title": "Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9; I.2.4"], "comment": "Preprint submitted to a journal", "summary": "While personalisation in Human-Robot Interaction (HRI) has advanced\nsignificantly, most existing approaches focus on single-user adaptation,\noverlooking scenarios involving multiple stakeholders with potentially\nconflicting preferences. To address this, we propose the Multi-User Preferences\nQuantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user\npersonalisation framework based on Quantitative Bipolar Argumentation\nFrameworks (QBAFs) that explicitly models and resolves multi-user preference\nconflicts. Unlike prior work in Argumentation Frameworks, which typically\nassumes static inputs, our approach is tailored to robotics: it incorporates\nboth users' arguments and the robot's dynamic observations of the environment,\nallowing the system to adapt over time and respond to changing contexts.\nPreferences, both positive and negative, are represented as arguments whose\nstrength is recalculated iteratively based on new information. The framework's\nproperties and capabilities are presented and validated through a realistic\ncase study, where an assistive robot mediates between the conflicting\npreferences of a caregiver and a care recipient during a frailty assessment\ntask. This evaluation further includes a sensitivity analysis of argument base\nscores, demonstrating how preference outcomes can be shaped by user input and\ncontextual observations. By offering a transparent, structured, and\ncontext-sensitive approach to resolving competing user preferences, this work\nadvances the field of multi-user HRI. It provides a principled alternative to\ndata-driven methods, enabling robots to navigate conflicts in real-world\nenvironments."}
{"id": "2511.03302", "pdf": "https://arxiv.org/pdf/2511.03302", "abs": "https://arxiv.org/abs/2511.03302", "authors": ["Xiaoyun Wang", "Yutong Zhang", "Sen Wang", "Sun Qi", "Hanning Wang", "Qixing Wang", "Jing Jin", "Jiwei He", "Nan Li"], "title": "C-RAN Advanced: From a Network Cooperation Perspective", "categories": ["eess.SP"], "comment": null, "summary": "Future mobile networks in the sixth generation (6G) are poised for a paradigm\nshift from conventional communication services toward comprehensive information\nservices, driving the evolution of radio access network (RAN) architectures\ntoward enhanced cooperation, intelligence, and service orientation. Building\nupon the concept of centralized, collaborative, cloud, and clean RAN (C-RAN),\nthis article proposes a novel cooperative, intelligent, and service-based RAN\n(CIS-RAN) architecture. Focusing on cooperation, CIS-RAN extends the\ntraditional cooperative communication paradigm by further integrating\ncooperative sensing and cooperative artificial intelligence (AI). To improve\nboth performance and effectiveness across diverse application scenarios,\nCIS-RAN enhances network cooperation throughout the entire process of\nacquisition, transmission, and processing, thereby enabling efficient\ninformation acquisition, diverse cooperative interactions, and intelligent\nfusion decision-making. Key technologies are discussed, with network\ncooperative multiple-input multiple-output (MIMO) examined as a case study,\ndemonstrating superior performance over traditional architectures, as\ndemonstrated by numerical results. Future research directions are outlined,\nemphasizing the continued exploration and advancement of the CIS-RAN\narchitecture, particularly in enhancing network cooperation."}
{"id": "2511.03591", "pdf": "https://arxiv.org/pdf/2511.03591", "abs": "https://arxiv.org/abs/2511.03591", "authors": ["Qingyi Chen", "Ruiqi Ni", "Jun Kim", "Ahmed H. Qureshi"], "title": "Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Safe multi-agent motion planning (MAMP) under task-induced constraints is a\ncritical challenge in robotics. Many real-world scenarios require robots to\nnavigate dynamic environments while adhering to manifold constraints imposed by\ntasks. For example, service robots must carry cups upright while avoiding\ncollisions with humans or other robots. Despite recent advances in\ndecentralized MAMP for high-dimensional systems, incorporating manifold\nconstraints remains difficult. To address this, we propose a\nmanifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for\ndecentralized MAMP. Our method solves HJR problems under manifold constraints\nto capture task-aware safety conditions, which are then integrated into a\ndecentralized trajectory optimization planner. This enables robots to generate\nmotion plans that are both safe and task-feasible without requiring assumptions\nabout other agents' policies. Our approach generalizes across diverse\nmanifold-constrained tasks and scales effectively to high-dimensional\nmulti-agent manipulation problems. Experiments show that our method outperforms\nexisting constrained motion planners and operates at speeds suitable for\nreal-world applications. Video demonstrations are available at\nhttps://youtu.be/RYcEHMnPTH8 ."}
{"id": "2511.03401", "pdf": "https://arxiv.org/pdf/2511.03401", "abs": "https://arxiv.org/abs/2511.03401", "authors": ["Kunrui Cao", "Jingyu Chen", "Panagiotis D. Diamantoulakis", "Lei Zhou", "Xingwang Li", "Yuanwei Liu", "George K. Karagiannidis"], "title": "Performance Analysis of Wireless-Powered Pinching Antenna Systems", "categories": ["eess.SP", "H.1"], "comment": "13 pages, 8 figures", "summary": "Pinching antenna system (PAS) serves as a groundbreaking paradigm that\nenhances wireless communications by flexibly adjusting the position of pinching\nantenna (PA) and establishing a strong line-of-sight (LoS) link, thereby\nreducing the free-space path loss. This paper introduces the concept of\nwireless-powered PAS, and investigates the reliability of wireless-powered PAS\nto explore the advantages of PA in improving the performance of\nwireless-powered communication (WPC) system. In addition, we derive the\nclosed-form expressions of outage probability and ergodic rate for the\npractical lossy waveguide case and ideal lossless waveguide case, respectively,\nand analyze the optimal deployment of waveguides and user to provide valuable\ninsights for guiding their deployments. The results show that an increase in\nthe absorption coefficient and in the dimensions of the user area leads to\nhigher in-waveguide and free-space propagation losses, respectively, which in\nturn increase the outage probability and reduce the ergodic rate of the\nwireless-powered PAS. However, the performance of wireless-powered PAS is\nseverely affected by the absorption coefficient and the waveguide length, e.g.,\nunder conditions of high absorption coefficient and long waveguide, the outage\nprobability of wireless-powered PAS is even worse than that of traditional WPC\nsystem. While the ergodic rate of wireless-powered PAS is better than that of\ntraditional WPC system under conditions of high absorption coefficient and long\nwaveguide. Interestingly, the wireless-powered PAS has the optimal time\nallocation factor and optimal distance between power station (PS) and access\npoint (AP) to minimize the outage probability or maximize the ergodic rate.\nMoreover, the system performance of PS and AP separated at the optimal distance\nbetween PS and AP is superior to that of PS and AP integrated into a hybrid\naccess point."}
{"id": "2511.03622", "pdf": "https://arxiv.org/pdf/2511.03622", "abs": "https://arxiv.org/abs/2511.03622", "authors": ["Swadhin Agrawal", "Sujoy Bhore", "Joseph S. B. Mitchell", "P. B. Sujit", "Aayush Gohil"], "title": "Multi-robot searching with limited sensing range for static and mobile intruders", "categories": ["cs.RO", "cs.CG", "cs.CR", "cs.MA"], "comment": null, "summary": "We consider the problem of searching for an intruder in a geometric domain by\nutilizing multiple search robots. The domain is a simply connected orthogonal\npolygon with edges parallel to the cartesian coordinate axes. Each robot has a\nlimited sensing capability. We study the problem for both static and mobile\nintruders. It turns out that the problem of finding an intruder is NP-hard,\neven for a stationary intruder. Given this intractability, we turn our\nattention towards developing efficient and robust algorithms, namely methods\nbased on space-filling curves, random search, and cooperative random search.\nMoreover, for each proposed algorithm, we evaluate the trade-off between the\nnumber of search robots and the time required for the robots to complete the\nsearch process while considering the geometric properties of the connected\northogonal search area."}
{"id": "2511.03465", "pdf": "https://arxiv.org/pdf/2511.03465", "abs": "https://arxiv.org/abs/2511.03465", "authors": ["Javier Giménez", "José A. Cortés", "Francisco Javier Cañete", "Eduardo Martos-Naya", "Luis Díez"], "title": "A Modified Pulse and Design Framework to Halve the Complexity of OFDM Spectral Shaping Techniques", "categories": ["eess.SP"], "comment": "5 pages, 1 figure, journal paper", "summary": "Orthogonal frequency division multiplexing (OFDM) is a widespread modulation\nbut suffers from high out-of-band emissions (OOBE). Spectral shaping strategies\nsuch as precoding, active interference cancellation (AIC) and time-domain\nmethods are effective at reducing the OOBE but entail optimization procedures\nand real-time implementation costs which might be considerable. This letter\nproposes a modification of the conventional OFDM waveform aimed at reducing the\ncost associated to many of the state-of-theart spectral shaping techniques and\nsets a framework for future works that want to benefit from the same reduction.\nThis approach may reduce both the number of coefficients involved in the\noptimization and the number of products of its implementation by up to 50%."}
{"id": "2511.03651", "pdf": "https://arxiv.org/pdf/2511.03651", "abs": "https://arxiv.org/abs/2511.03651", "authors": ["Andrei A. Korigodskii", "Oleg D. Kalachev", "Artem E. Vasiunik", "Matvei V. Urvantsev", "Georgii E. Bondar"], "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "I.2.9; J.5"], "comment": null, "summary": "This paper presents the innovative design and successful deployment of a\npioneering autonomous unmanned aerial system developed for executing the\nworld's largest mural painted by a drone. Addressing the dual challenges of\nmaintaining artistic precision and operational reliability under adverse\noutdoor conditions such as wind and direct sunlight, our work introduces a\nrobust system capable of navigating and painting outdoors with unprecedented\naccuracy. Key to our approach is a novel navigation system that combines an\ninfrared (IR) motion capture camera and LiDAR technology, enabling precise\nlocation tracking tailored specifically for largescale artistic applications.\nWe employ a unique control architecture that uses different regulation in\ntangential and normal directions relative to the planned path, enabling precise\ntrajectory tracking and stable line rendering. We also present algorithms for\ntrajectory planning and path optimization, allowing for complex curve drawing\nand area filling. The system includes a custom-designed paint spraying\nmechanism, specifically engineered to function effectively amidst the turbulent\nairflow generated by the drone's propellers, which also protects the drone's\ncritical components from paint-related damage, ensuring longevity and\nconsistent performance. Experimental results demonstrate the system's\nrobustness and precision in varied conditions, showcasing its potential for\nautonomous large-scale art creation and expanding the functional applications\nof robotics in creative fields."}
{"id": "2511.03487", "pdf": "https://arxiv.org/pdf/2511.03487", "abs": "https://arxiv.org/abs/2511.03487", "authors": ["Yameng Liu", "Jianhua Zhang", "Yuxiang Zhang", "Zhiqiang Yuan", "Chuangxin Jiang", "Junchen Liu", "Wei Hong", "Yingyang Li", "Yan Li", "Guangyi Liu"], "title": "A Novel Multi-Reference-Point Modeling Framework for Monostatic Background Channel: Toward 3GPP ISAC Standardization", "categories": ["eess.SP"], "comment": null, "summary": "Integrated Sensing and Communication (ISAC) has been identified as a key 6G\napplication by ITU and 3GPP. A realistic, standard-compatible channel model is\nessential for ISAC system design. To characterize the impact of Sensing Targets\n(STs), 3GPP defines ISAC channel as a combination of target and background\nchannels, comprising multipath components related to STs and those originating\nsolely from the environment, respectively. Although the background channel does\nnot carry direct ST information, its accurate modeling is critical for\nevaluating sensing performance, especially in complex environments. Existing\ncommunication standards characterize propagation between separated transmitter\n(Tx) and receiver (Rx). However, modeling background channels in the ISAC\nmonostatic mode, where the Tx and Rx are co-located, remains a pressing\nchallenge. In this paper, we firstly conduct ISAC monostatic background channel\nmeasurements for an indoor scenario at 28 GHz. Realistic channel parameters are\nextracted, revealing pronounced single-hop propagation and discrete multipath\ndistribution. Inspired by these properties, a novel stochastic model is\nproposed to characterizing the ISAC monostatic background channel as the\nsuperposition of sub-channels between the monostatic Tx&Rx and multiple\ncommunication Rx-like Reference Points (RPs). This model is compatible with\nstandardizations, and a 3GPP-extended implementation framework is introduced.\nFinally, a genetic algorithm-based method is proposed to extract the optimal\nnumber and placement of multi-RPs. The optimization approach and modeling\nframework are validated by comparing measured and simulated channel parameters.\nResults demonstrate that the proposed model effectively captures monostatic\nbackground channel characteristics, addresses a critical gap in ISAC channel\nmodeling, and supports 6G standardization."}
{"id": "2511.03652", "pdf": "https://arxiv.org/pdf/2511.03652", "abs": "https://arxiv.org/abs/2511.03652", "authors": ["Azizollah Taheri", "Derya Aksaray"], "title": "Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "This paper addresses a motion planning problem to achieve\nspatio-temporal-logical tasks, expressed by syntactically co-safe linear\ntemporal logic specifications (scLTL\\next), in uncertain environments. Here,\nthe uncertainty is modeled as some probabilistic knowledge on the semantic\nlabels of the environment. For example, the task is \"first go to region 1, then\ngo to region 2\"; however, the exact locations of regions 1 and 2 are not known\na priori, instead a probabilistic belief is available. We propose a novel\nautomata-theoretic approach, where a special product automaton is constructed\nto capture the uncertainty related to semantic labels, and a reward function is\ndesigned for each edge of this product automaton. The proposed algorithm\nutilizes value iteration for online replanning. We show some theoretical\nresults and present some simulations/experiments to demonstrate the efficacy of\nthe proposed approach."}
{"id": "2511.03612", "pdf": "https://arxiv.org/pdf/2511.03612", "abs": "https://arxiv.org/abs/2511.03612", "authors": ["Yingjie Xu", "Xuesong Cai", "Michiel Sandra", "Sara Willhammar", "Fredrik Tufvesson"], "title": "3D Cooperative User Tracking for Distributed Integrated Sensing and Communication", "categories": ["eess.SP"], "comment": null, "summary": "As integrated sensing and communication (ISAC) becomes an integral part of 6G\nnetworks, distributed ISAC (DISAC) is expected to enhance both sensing and\ncommunication performance through its decentralized architecture. This paper\npresents a complete framework to address the challenge of cooperative user\ntracking in DISAC systems. By incorporating a global probability hypothesis\ndensity (PHD) filter and a field-of-view-aware access point (AP) management\nstrategy, the framework enables accurate user tracking using radio signals\nwhile optimizing AP scheduling. In addition, a real-world distributed MIMO\nchannel measurement campaign is performed to evaluate the effectiveness of the\nframework. The results demonstrate that a centimeter-level root mean-square\ntrajectory error can be achieved. Furthermore, the results show that it is not\nnecessary to keep APs active at all times to maintain high tracking accuracy,\nindicating the need for robust and efficient AP management. These findings\nprovide valuable insight into practical deployments and further development of\ncooperative user tracking techniques in DISAC systems."}
{"id": "2511.03676", "pdf": "https://arxiv.org/pdf/2511.03676", "abs": "https://arxiv.org/abs/2511.03676", "authors": ["Taito Tashiro", "Tomoko Yonezawa", "Hirotake Yamazoe"], "title": "Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design", "categories": ["cs.RO", "cs.HC", "H.5.2; I.2.9"], "comment": "5 pages, 5 figures, HAI2025 Workshop on Socially Aware and\n  Cooperative Intelligent Systems", "summary": "This study investigates how human motion cues can be used to design\nexpressive robot-arm movements. Using the imperfect-information game Geister,\nwe analyzed two types of human piece-moving motions: natural gameplay\n(unconscious tendencies) and instructed expressions (intentional cues). Based\non these findings, we created phase-specific robot motions by varying movement\nspeed and stop duration, and evaluated observer impressions under two\npresentation modalities: a physical robot and a recorded video. Results\nindicate that late-phase motion timing, particularly during withdrawal, plays\nan important role in impression formation and that physical embodiment enhances\nthe interpretability of motion cues. These findings provide insights for\ndesigning expressive robot motions based on human timing behavior."}
{"id": "2511.02993", "pdf": "https://arxiv.org/pdf/2511.02993", "abs": "https://arxiv.org/abs/2511.02993", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Zekun Chang", "Thijs Roumen", "Rajalakshmi Nandakumar"], "title": "PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat", "categories": ["cs.CR", "cs.HC", "eess.SP"], "comment": "20 pages, 5 figures", "summary": "Wireless sensing technologies can now detect heartbeats using radio frequency\nand acoustic signals, raising significant privacy concerns. Existing privacy\nsolutions either protect from all sensing systems indiscriminately preventing\nany utility or operate post-data collection, failing to enable selective access\nwhere authorized devices can monitor while unauthorized ones cannot. We present\na key-based physical obfuscation system, PrivyWave, that addresses this\nchallenge by generating controlled decoy heartbeat signals at\ncryptographically-determined frequencies. Unauthorized sensors receive a\nmixture of real and decoy signals that are indistinguishable without the secret\nkey, while authorized sensors use the key to filter out decoys and recover\naccurate measurements. Our evaluation with 13 participants demonstrates\neffective protection across both sensing modalities: for mmWave radar,\nunauthorized sensors show 21.3 BPM mean absolute error while authorized sensors\nmaintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error\nincreases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system\noperates across multiple sensing modalities without per-modality customization\nand provides cryptographic obfuscation guarantees. Performance benchmarks show\nrobust protection across different distances (30-150 cm), orientations\n(120{\\deg} field of view), and diverse indoor environments, establishing\nphysical-layer obfuscation as a viable approach for selective privacy in\npervasive health monitoring."}
{"id": "2511.03691", "pdf": "https://arxiv.org/pdf/2511.03691", "abs": "https://arxiv.org/abs/2511.03691", "authors": ["Zhihang Qin", "Yueheng Zhang", "Wan Su", "Linxin Hou", "Shenghao Zhou", "Zhijun Chen", "Yu Jun Tan", "Cecilia Laschi"], "title": "Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping", "categories": ["cs.RO"], "comment": null, "summary": "Conventional fluid-driven soft grippers typically depend on external sources,\nwhich limit portability and long-term autonomy. This work introduces a\nself-contained soft gripper with fixed size that operates solely through\ninternal liquid redistribution among three interconnected bistable snap-through\nchambers. When the top sensing chamber deforms upon contact, the displaced\nliquid triggers snap-through expansion of the grasping chambers, enabling\nstable and size-selective grasping without continuous energy input. The\ninternal hydraulic feedback further allows passive adaptation of gripping\npressure to object stiffness. This source-free and compact design opens new\npossibilities for lightweight, stiffness-adaptive fluid-driven manipulation in\nsoft robotics, providing a feasible approach for targeted size-specific\nsampling and operation in underwater and field environments."}
{"id": "2511.03084", "pdf": "https://arxiv.org/pdf/2511.03084", "abs": "https://arxiv.org/abs/2511.03084", "authors": ["Gowtham Premananth", "Carol Espy-Wilson"], "title": "Quantifying Articulatory Coordination as a Biomarker for Schizophrenia", "categories": ["eess.AS", "cs.LG", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Advances in artificial intelligence (AI) and deep learning have improved\ndiagnostic capabilities in healthcare, yet limited interpretability continues\nto hinder clinical adoption. Schizophrenia, a complex disorder with diverse\nsymptoms including disorganized speech and social withdrawal, demands tools\nthat capture symptom severity and provide clinically meaningful insights beyond\nbinary diagnosis. Here, we present an interpretable framework that leverages\narticulatory speech features through eigenspectra difference plots and a\nweighted sum with exponential decay (WSED) to quantify vocal tract\ncoordination. Eigenspectra plots effectively distinguished complex from simpler\ncoordination patterns, and WSED scores reliably separated these groups, with\nambiguity confined to a narrow range near zero. Importantly, WSED scores\ncorrelated not only with overall BPRS severity but also with the balance\nbetween positive and negative symptoms, reflecting more complex coordination in\nsubjects with pronounced positive symptoms and the opposite trend for stronger\nnegative symptoms. This approach offers a transparent, severity-sensitive\nbiomarker for schizophrenia, advancing the potential for clinically\ninterpretable speech-based assessment tools."}
{"id": "2511.01210", "pdf": "https://arxiv.org/pdf/2511.01210", "abs": "https://arxiv.org/abs/2511.01210", "authors": ["Heyu Guo", "Shanmu Wang", "Ruichun Ma", "Shiqi Jiang", "Yasaman Ghasempour", "Omid Abari", "Baining Guo", "Lili Qi"], "title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization for\naction prediction through large-scale vision-language pretraining. However,\nmost existing models rely solely on RGB cameras, limiting their perception and,\nconsequently, manipulation capabilities. We present OmniVLA, an omni-modality\nVLA model that integrates novel sensing modalities for physically-grounded\nspatial intelligence beyond RGB perception. The core of our approach is the\nsensor-masked image, a unified representation that overlays spatially grounded\nand physically meaningful masks onto the RGB images, derived from sensors\nincluding an infrared camera, a mmWave radar, and a microphone array. This\nimage-native unification keeps sensor input close to RGB statistics to\nfacilitate training, provides a uniform interface across sensor hardware, and\nenables data-efficient learning with lightweight per-sensor projectors. Built\non this, we present a multisensory vision-language-action model architecture\nand train the model based on an RGB-pretrained VLA backbone. We evaluate\nOmniVLA on challenging real-world tasks where sensor-modality perception is\nneeded to guide the manipulation. OmniVLA achieves an average task success rate\nof 84%, significantly outperforms both RGB-only and raw-sensor-input baseline\nmodels by 59% and 28% respectively, meanwhile showing higher learning\nefficiency and stronger generalization capability."}
{"id": "2511.03086", "pdf": "https://arxiv.org/pdf/2511.03086", "abs": "https://arxiv.org/abs/2511.03086", "authors": ["Gowtham Premananth", "Philip Resnik", "Sonia Bansal", "Deanna L. Kelly", "Carol Espy-Wilson"], "title": "Speech-Based Prioritization for Schizophrenia Intervention", "categories": ["eess.AS", "eess.SP"], "comment": "Submitted for ICASSP 2026", "summary": "Millions of people suffer from mental health conditions, yet many remain\nundiagnosed or receive delayed care due to limited clinical resources and\nlabor-intensive assessment methods. While most machine-assisted approaches\nfocus on diagnostic classification, estimating symptom severity is essential\nfor prioritizing care, particularly in resource-constrained settings.\nSpeech-based AI provides a scalable alternative by enabling automated,\ncontinuous, and remote monitoring, reducing reliance on subjective self-reports\nand time-consuming evaluations. In this paper, we introduce a speech-based\nmodel for pairwise comparison of schizophrenia symptom severity, leveraging\narticulatory and acoustic features. These comparisons are used to generate\nseverity rankings via the Bradley-Terry model. Our approach outperforms\nprevious regression-based models on ranking-based metrics, offering a more\neffective solution for clinical triage and prioritization."}
{"id": "2511.02953", "pdf": "https://arxiv.org/pdf/2511.02953", "abs": "https://arxiv.org/abs/2511.02953", "authors": ["Sadiq Layi Macaulay", "Nimet Kaygusuz", "Simon Hadfield"], "title": "EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Event cameras, with their high dynamic range (HDR) and low latency, offer a\npromising alternative for robust depth estimation in challenging environments.\nHowever, many event-based depth estimation approaches are constrained by\nsmall-scale annotated datasets, limiting their generalizability to real-world\nscenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event\ncamera dataset curated from publicly available YouTube footage, which contains\nmore than 13B events across various environmental conditions and motions,\nincluding seasonal hiking, flying, scenic driving, and underwater exploration.\nEvtSlowTV is an order of magnitude larger than existing event datasets,\nproviding an unconstrained, naturalistic setting for event-based depth\nlearning. This work shows the suitability of EvtSlowTV for a self-supervised\nlearning framework to capitalise on the HDR potential of raw event streams. We\nfurther demonstrate that training with EvtSlowTV enhances the model's ability\nto generalise to complex scenes and motions. Our approach removes the need for\nframe-based annotations and preserves the asynchronous nature of event data."}
{"id": "2511.03089", "pdf": "https://arxiv.org/pdf/2511.03089", "abs": "https://arxiv.org/abs/2511.03089", "authors": ["Gowtham Premananth", "Carol Espy-Wilson"], "title": "A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures", "categories": ["cs.CL", "eess.AS", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "Language disruptions are one of the well-known effects of schizophrenia\nsymptoms. They are often manifested as disorganized speech and impaired\ndiscourse coherence. These abnormalities in spontaneous language production\nreflect underlying cognitive disturbances and have the potential to serve as\nobjective markers for symptom severity and diagnosis of schizophrenia. This\nstudy focuses on how these language disruptions can be characterized in terms\nof two computational linguistic measures: surprisal and semantic coherence. By\ncomputing surprisal and semantic coherence of language using computational\nmodels, this study investigates how they differ between subjects with\nschizophrenia and healthy controls. Furthermore, this study provides further\ninsight into how language disruptions in terms of these linguistic measures\nchange with varying degrees of schizophrenia symptom severity."}
{"id": "2511.03173", "pdf": "https://arxiv.org/pdf/2511.03173", "abs": "https://arxiv.org/abs/2511.03173", "authors": ["Arsalan Muhammad", "Wasiu Akande Ahmed", "Omada Friday Ojonugwa", "Paul Puspendu Biswas"], "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies", "categories": ["astro-ph.EP", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "The rapid growth of cislunar activities, including lunar landings, the Lunar\nGateway, and in-space refueling stations, requires advances in cost-efficient\ntrajectory design and reliable integration of navigation and remote sensing.\nTraditional Earth-Moon transfers suffer from rigid launch windows and high\npropellant demands, while Earth-based GNSS systems provide little to no\ncoverage beyond geostationary orbit. This limits autonomy and environmental\nawareness in cislunar space. This review compares four major transfer\nstrategies by evaluating velocity requirements, flight durations, and fuel\nefficiency, and by identifying their suitability for both crewed and robotic\nmissions. The emerging role of artificial intelligence and machine learning is\nhighlighted: convolutional neural networks support automated crater recognition\nand digital terrain model generation, while deep reinforcement learning enables\nadaptive trajectory refinement during descent and landing to reduce risk and\ndecision latency. The study also examines how GNSS-Reflectometry and advanced\nPositioning, Navigation, and Timing architectures can extend navigation\ncapabilities beyond current limits. GNSS-R can act as a bistatic radar for\nmapping lunar ice, soil properties, and surface topography, while PNT systems\nsupport autonomous rendezvous, Lagrange point station-keeping, and coordinated\nsatellite swarm operations. Combining these developments establishes a scalable\nframework for sustainable cislunar exploration and long-term human and robotic\npresence."}
{"id": "2511.03162", "pdf": "https://arxiv.org/pdf/2511.03162", "abs": "https://arxiv.org/abs/2511.03162", "authors": ["Yijing Chu", "Qinxuan Xiang", "Sipei Zhao", "Ming Wu", "Y. Zhao", "Guangzheng Yu"], "title": "Active Noise Control Method Using Time Domain Neural Networks for Path Decoupling", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": null, "summary": "In decentralized active noise control (ANC) systems, crosstalk between\nmultichannel secondary sources and error microphones significantly degrades\ncontrol accuracy. Moreover, prefiltering reference signals in filtered-x (Fx)\ntype algorithms may further introduce modeling errors. A theoretical analysis\nof the Fx-based decentralized control algorithm was performed, which reveals\nhow prefiltering and crosstalk affect the control performance. Then, a hybrid\nmethod combining fixed-value neural networks and adaptive strategies was\nproposed for efficient decentralized ANC. The adaptive filter models the\nprimary path of its own channel online using the least mean square (LMS)\nalgorithm while the neural network (named DecNet) is used for secondary paths\ninverting and decoupling. The hybrid DecNet-LMS algorithm was implemented in\nthe time domain to guarantee causality and avoid latency. Simulation results\nwith measured acoustic paths show that the proposed method outperforms the\nexisting ANC algorithms using either traditional adaptive filters or neural\nnetwork-based fixed-coefficient methods under different acoustic conditions."}
{"id": "2511.03187", "pdf": "https://arxiv.org/pdf/2511.03187", "abs": "https://arxiv.org/abs/2511.03187", "authors": ["Jonghae Park", "Daesol Cho", "Jusuk Lee", "Dongseok Shim", "Inkyu Jang", "H. Jin Kim"], "title": "Periodic Skill Discovery", "categories": ["cs.LG", "cs.RO"], "comment": "NeurIPS 2025", "summary": "Unsupervised skill discovery in reinforcement learning (RL) aims to learn\ndiverse behaviors without relying on external rewards. However, current methods\noften overlook the periodic nature of learned skills, focusing instead on\nincreasing the mutual dependence between states and skills or maximizing the\ndistance traveled in latent space. Considering that many robotic tasks --\nparticularly those involving locomotion -- require periodic behaviors across\nvarying timescales, the ability to discover diverse periodic skills is\nessential. Motivated by this, we propose Periodic Skill Discovery (PSD), a\nframework that discovers periodic behaviors in an unsupervised manner. The key\nidea of PSD is to train an encoder that maps states to a circular latent space,\nthereby naturally encoding periodicity in the latent representation. By\ncapturing temporal distance, PSD can effectively learn skills with diverse\nperiods in complex robotic tasks, even with pixel-based observations. We\nfurther show that these learned skills achieve high performance on downstream\ntasks such as hurdling. Moreover, integrating PSD with an existing skill\ndiscovery method offers more diverse behaviors, thus broadening the agent's\nrepertoire. Our code and demos are available at\nhttps://jonghaepark.github.io/psd/"}
{"id": "2511.03263", "pdf": "https://arxiv.org/pdf/2511.03263", "abs": "https://arxiv.org/abs/2511.03263", "authors": ["Ruizhe Zheng", "Lingyan Mao", "Dingding Han", "Tian Luo", "Yi Wang", "Jing Ding", "Yuguo Yu"], "title": "FAPEX: Fractional Amplitude-Phase Expressor for Robust Cross-Subject Seizure Prediction", "categories": ["q-bio.NC", "eess.SP"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Spotlight Poster", "summary": "Precise, generalizable subject-agnostic seizure prediction (SASP) remains a\nfundamental challenge due to the intrinsic complexity and significant spectral\nvariability of electrophysiological signals across individuals and recording\nmodalities. We propose FAPEX, a novel architecture that introduces a learnable\nfractional neural frame operator (FrNFO) for adaptive time-frequency\ndecomposition. Unlike conventional models that exhibit spectral bias toward low\nfrequencies, our FrNFO employs fractional-order convolutions to capture both\nhigh and low-frequency dynamics, achieving approximately 10% improvement in\nF1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the\nextraction of instantaneous phase and amplitude representations that are\nparticularly informative for preictal biomarker discovery and enhance\nout-of-distribution generalization. FAPEX further integrates structural\nstate-space modeling and channelwise attention, allowing it to handle\nheterogeneous electrode montages. Evaluated across 12 benchmarks spanning\nspecies (human, rat, dog, macaque) and modalities (Scalp-EEG, SEEG, ECoG, LFP),\nFAPEX consistently outperforms 23 supervised and 10 self-supervised baselines\nunder nested cross-validation, with gains of up to 15% in sensitivity on\ncomplex cross-domain scenarios. It further demonstrates superior performance in\nseveral external validation cohorts. To our knowledge, these establish FAPEX as\nthe first epilepsy model to show consistent superiority in SASP, offering a\npromising solution for discovering epileptic biomarker evidence supporting the\nexistence of a distinct and identifiable preictal state and clinical\ntranslation."}
{"id": "2511.03366", "pdf": "https://arxiv.org/pdf/2511.03366", "abs": "https://arxiv.org/abs/2511.03366", "authors": ["Kapila W. S. Palitharathna", "Constantinos Psomas", "Ioannis Krikidis"], "title": "Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under Ship Attitude Variation", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "This paper has been submitted to the IEEE International Conference on\n  Communications (ICC 2026) conference", "summary": "In this paper, we propose a lightwave power transfer-enabled underwater\noptical integrated sensing and communication (O-ISAC) system, where an access\npoint (AP) mounted on a seasurface ship transmits lightwave signals to two\nnodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink\ninformation to the AP, and ($ii$) a sensing target whose position is estimated\nby the AP using an array of pinhole cameras. To capture practical deployment\nconditions, the ship attitude variation is modeled through its roll, pitch, and\nyaw angles, each following a Gaussian distribution under low-to-moderate sea\nstates. Closed-form approximations are derived for the mean squared error (MSE)\nof target localization and the achievable uplink data rate. Analytical and\nsimulation results demonstrate excellent agreement, validating the proposed\nmodels and derived expressions, while revealing the fundamental\ncommunication-sensing tradeoff in the O-ISAC system. The results further\nprovide valuable design insights, including the optimal camera placement on the\nship to minimize localization error, achieving a minimum MSE of $10^{-2}$\n$\\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation\nof $10^{\\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered\nsetup."}
{"id": "2511.03632", "pdf": "https://arxiv.org/pdf/2511.03632", "abs": "https://arxiv.org/abs/2511.03632", "authors": ["Cemil Vahapoglu", "Timothy J. O'Shea", "Wan Liu", "Sennur Ulukus"], "title": "Neural Beamforming with Doppler-Aware Sparse Attention for High Mobility Environments", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "Beamforming has significance for enhancing spectral efficiency and mitigating\ninterference in multi-antenna wireless systems, facilitating spatial\nmultiplexing and diversity in dense and high mobility scenarios. Traditional\nbeamforming techniques such as zero-forcing beamforming (ZFBF) and minimum mean\nsquare error (MMSE) beamforming experience performance deterioration under\nadverse channel conditions. Deep learning-based beamforming offers an\nalternative with nonlinear mappings from channel state information (CSI) to\nbeamforming weights by improving robustness against dynamic channel\nenvironments. Transformer-based models are particularly effective due to their\nability to model long-range dependencies across time and frequency. However,\ntheir quadratic attention complexity limits scalability in large OFDM grids.\nRecent studies address this issue through sparse attention mechanisms that\nreduce complexity while maintaining expressiveness, yet often employ patterns\nthat disregard channel dynamics, as they are not specifically designed for\nwireless communication scenarios. In this work, we propose a Doppler-aware\nSparse Neural Network Beamforming (Doppler-aware Sparse NNBF) model that\nincorporates a channel-adaptive sparse attention mechanism in a multi-user\nsingle-input multiple-output (MU-SIMO) setting. The proposed sparsity structure\nis configurable along 2D time-frequency axes based on channel dynamics and is\ntheoretically proven to ensure full connectivity within p hops, where p is the\nnumber of attention heads. Simulation results under urban macro (UMa) channel\nconditions show that Doppler-aware Sparse NNBF significantly outperforms both a\nfixed-pattern baseline, referred to as Standard Sparse NNBF, and conventional\nbeamforming techniques ZFBF and MMSE beamforming in high mobility scenarios,\nwhile maintaining structured sparsity with a controlled number of attended keys\nper query."}
