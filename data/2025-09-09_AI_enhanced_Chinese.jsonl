{"id": "2509.05464", "pdf": "https://arxiv.org/pdf/2509.05464", "abs": "https://arxiv.org/abs/2509.05464", "authors": ["Qiang Fu", "Changhui Li"], "title": "Developing a Framework to Simulate Quantitative Ultrasound Flow and Tissue Motion for Ultrafast Doppler Ultrasound", "categories": ["eess.SP"], "comment": null, "summary": "Ultrafast power Doppler imaging (uPDI) has made significant progress and\nbecome an important imaging method for both research and clinical\nimplementations. While, it lacks simulation tools that can perform\nthree-dimensional (3D) quantitative flow with tissue motion close to realistic\nconditions. In this study, we explore to construct an open-source framework,\nnamed 3D-Fully Quantitative Flow (3D-FQFlow), to provide quantitative modeling\nof 3D vascular flow with tissue motion and uPDI imaging. The framework\nintegrates a L-system-based vascular generator with SimVascular CFD for\nhemodynamics, a tissue motion simulator supporting user-defined or\nclinical-data-driven condition, an optimized PFILED ultrasound simulator, a\nprecomputed-matrix-based reconstructor, and a quantitative analyzer\n(MSE/PSNR/SSIM). Results demonstrate distinct influences of four motion\npatterns on SVD decomposition; successful 3D imaging of rabbit kidney (SSIM =\n0.951), generated vasculature (SSIM = 0.902), and clinical pulmonary arteries\n(SSIM = 0.850); and GPU acceleration permitting 1-million-scatterer simulation\nin 4,117 seconds with 18.8* speedup for 100-frame 3D-uPDI generation. 3D-FQFlow\nestablishes the first open-source framework for quantitative validation of uPDI\nunder realistic vascular and motion conditions, creating a reproducible\nstandard for microvascular imaging research\n(https://github.com/FortuneOU/3D-FQFlow).", "AI": {"tldr": "3D-FQFlow\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u7684\u8840\u7ba1\u548c\u8fd0\u52a8\u6761\u4ef6\u4e0b\u5b9a\u91cf\u9a8c\u8bc1\u8d85\u5feb\u529f\u7387\u591a\u666e\u52d2\u6210\u50cf\uff08uPDI\uff09\uff0c\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u6807\u51c6\u7528\u4e8e\u5fae\u8840\u7ba1\u6210\u50cf\u7814\u7a76\u3002", "motivation": "\u73b0\u6709uPDI\u7f3a\u4e4f\u80fd\u591f\u6a21\u62df\u4e09\u7ef4\u5b9a\u91cf\u8840\u6d41\u548c\u7ec4\u7ec7\u8fd0\u52a8\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u63a5\u8fd1\u771f\u5b9e\u6761\u4ef6\u4e0b\u3002", "method": "\u96c6\u6210\u4e86\u57fa\u4e8eL\u7cfb\u7edf\u7684\u8840\u7ba1\u751f\u6210\u5668\u3001SimVascular CFD\u8840\u6d41\u52a8\u529b\u5b66\u6a21\u62df\u3001\u652f\u6301\u7528\u6237\u5b9a\u4e49\u6216\u4e34\u5e8a\u6570\u636e\u9a71\u52a8\u7684\u7ec4\u7ec7\u8fd0\u52a8\u6a21\u62df\u5668\u3001\u4f18\u5316\u7684PFILED\u8d85\u58f0\u6a21\u62df\u5668\u3001\u9884\u8ba1\u7b97\u77e9\u9635\u91cd\u5efa\u5668\u548c\u5b9a\u91cf\u5206\u6790\u5668\u3002", "result": "\u5c55\u793a\u4e86\u56db\u79cd\u8fd0\u52a8\u6a21\u5f0f\u5bf9SVD\u5206\u89e3\u7684\u663e\u8457\u5f71\u54cd\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5154\u80be\u810f\uff08SSIM=0.951\uff09\u3001\u751f\u6210\u8840\u7ba1\uff08SSIM=0.902\uff09\u548c\u4e34\u5e8a\u80ba\u52a8\u8109\uff08SSIM=0.850\uff09\u7684\u4e09\u7ef4\u6210\u50cf\uff0cGPU\u52a0\u901f\u4f7f\u5f97100\u5e273D-uPDI\u751f\u6210\u901f\u5ea6\u63d0\u534718.8\u500d\u3002", "conclusion": "3D-FQFlow\u4e3a\u9996\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u4e3auPDI\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u5b9a\u91cf\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u6807\u51c6\u3002"}}
{"id": "2509.05565", "pdf": "https://arxiv.org/pdf/2509.05565", "abs": "https://arxiv.org/abs/2509.05565", "authors": ["Zhihao Tao", "Athina Petropulu", "H. Vincent Poor"], "title": "Time-Modulated Intelligent Reflecting Surfaces for Integrated Sensing, Communication and Security: A Generative AI Design Framework", "categories": ["eess.SP"], "comment": "submitted to Nature Portfolio Journal, npj Wireless Technology, the\n  special issue on ISAC", "summary": "We propose a novel approach to achieve physical layer security for integrated\nsensing and communication (ISAC) systems operating in the presence of targets\nthat may be eavesdroppers. The system is aided by a time-modulated intelligent\nreflecting surface (TM-IRS), which is configured to preserve the integrity of\nthe transmitted data at one or more legitimate communication users (CUs) while\nmaking them appear scrambled in all other directions. The TM-IRS design\nleverages a generative flow network (GFlowNet) framework to learn a stochastic\npolicy that samples high-performing TM-IRS configurations from a vast discrete\nparameter space. Specifically, we begin by formulating the achievable sum rate\nfor the legitimate CUs and the beampattern gain toward the target direction,\nbased on which we construct reward functions for GFlowNets that jointly capture\nboth communication and sensing performance. The TM-IRS design is modeled as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a complete configuration of TM-IRS parameters. GFlowNets,\nparametrized by deep neural networks are employed to learn a stochastic policy\nthat samples TM-IRS parameter sets with probability proportional to their\nassociated reward. Experimental results demonstrate the effectiveness of the\nproposed GFlowNet-based method in integrating sensing, communication and\nsecurity simultaneously, and also exhibit significant sampling efficiency as\ncompared to the exhaustive combinatorial search and enhanced robustness against\nthe rule-based TM-IRS design method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGFlowNet\u7684TM-IRS\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8eISAC\u7cfb\u7edf\u7684\u7269\u7406\u5c42\u5b89\u5168\u4fdd\u62a4\uff0c\u901a\u8fc7\u5728\u5408\u6cd5\u7528\u6237\u65b9\u5411\u4fdd\u6301\u6570\u636e\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u5728\u5176\u5b83\u65b9\u5411\u6270\u4e71\u6570\u636e\uff0c\u540c\u65f6\u517c\u987e\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3ISAC\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u7a83\u542c\u76ee\u6807\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u5408\u6cd5\u7528\u6237\u7684\u901a\u4fe1\u5b89\u5168\uff0c\u540c\u65f6\u517c\u987e\u611f\u77e5\u6027\u80fd\u7684\u4f18\u5316\u3002", "method": "\u5229\u7528GFlowNet\u6846\u67b6\u5b66\u4e60TM-IRS\u7684\u914d\u7f6e\u7b56\u7565\uff0c\u5efa\u6a21\u4e3a\u786e\u5b9a\u6027MDP\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53c2\u91cf\u5316\uff0c\u91c7\u6837\u9ad8\u5956\u52b1\u7684TM-IRS\u53c2\u6570\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u540c\u65f6\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u5b89\u5168\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u76f8\u8f83\u4e8e\u7a77\u4e3e\u641c\u7d22\u66f4\u5177\u91c7\u6837\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GFlowNet\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86TM-IRS\u8bbe\u8ba1\u4e2d\u7684\u9ad8\u7ef4\u53c2\u6570\u4f18\u5316\u95ee\u9898\uff0c\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7269\u7406\u5c42\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05639", "pdf": "https://arxiv.org/pdf/2509.05639", "abs": "https://arxiv.org/abs/2509.05639", "authors": ["Yijie Liu", "Weidong Mei", "He Sun", "Dong Wang", "Peilan Wang"], "title": "Power-Measurement-Based Channel Estimation for Beyond Diagonal RIS", "categories": ["eess.SP"], "comment": null, "summary": "Beyond diagonal reconfigurable intelligent surface (BD-RIS), with its\nenhanced degrees of freedom compared to conventional RIS, has demonstrated\nnotable potential for enhancing wireless communication performance. However, a\nkey challenge in employing BD-RIS lies in accurately acquiring its channel\nstate information (CSI) with both the base station (BS) and users. Existing\nBD-RIS channel estimation methods rely mainly on dedicated pilot signals, which\nincrease system overhead and may be incompatible with current communication\nprotocols. To overcome these limitations, this letter proposes a new\nsingle-layer neural network (NN)-enabled channel estimation method utilizing\nonly the easily accessible received power measurements at user terminals. In\nparticular, we show that the received signal power can be expressed in a form\nsimilar to a single-layer NN, where the weights represent the BD-RIS's CSI.\nThis structure enables the recovery of CSI using the backward propagation,\nbased on power measurements collected under varying training reflection\ncoefficients. Numerical results show that our proposed method can achieve a\nsmall normalized mean square error (NMSE), particularly when the number of\ntraining reflections is large.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u7684BD-RIS\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u5229\u7528\u7528\u6237\u7ec8\u7aef\u63a5\u6536\u529f\u7387\u6d4b\u91cf\uff0c\u51cf\u5c11\u7cfb\u7edf\u5f00\u9500\u5e76\u517c\u5bb9\u73b0\u6709\u534f\u8bae\u3002", "motivation": "\u89e3\u51b3BD-RIS\u4fe1\u9053\u4f30\u8ba1\u4e2d\u4f9d\u8d56\u4e13\u7528\u5bfc\u9891\u4fe1\u53f7\u5bfc\u81f4\u7cfb\u7edf\u5f00\u9500\u5927\u4e14\u4e0e\u73b0\u6709\u534f\u8bae\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u5c06\u63a5\u6536\u4fe1\u53f7\u529f\u7387\u8868\u8fbe\u4e3a\u7c7b\u4f3c\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\u5f62\u5f0f\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4ece\u529f\u7387\u6d4b\u91cf\u4e2d\u6062\u590d\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u5728\u8bad\u7ec3\u53cd\u5c04\u6b21\u6570\u591a\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aBD-RIS\u4fe1\u9053\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u534f\u8bae\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2509.05677", "pdf": "https://arxiv.org/pdf/2509.05677", "abs": "https://arxiv.org/abs/2509.05677", "authors": ["Xuancheng Zhu", "Zhiwen Zhou", "Yong Zeng"], "title": "Full-Angle Ray Antenna Array and Omnicell Wireless Communication System", "categories": ["eess.SP"], "comment": null, "summary": "Ray antenna array (RAA) was recently proposed as a novel multi-antenna\narchitecture that arranges multiple massive cheap antenna elements into simple\nuniform linear arrays (sULAs) with different orientations. Compared with\ntraditional architectures like hybrid analog/digital beamforming with uniform\nlinear array (ULA) and uniform circular array (UCA), RAA has several promising\nadvantages such as significantly reduced hardware cost, higher beamforming\ngains and the ability of providing uniform angular resolution for all\ndirections. In this paper, we propose a full-angle RAA architecture and an\ninnovative omnicell wireless communication paradigm enabled by full-angle RAA.\nThe proposed full-angle RAA expands RAA's orientation angle to the full angle\ndomain, such that the RAA's advantages can be exploited to all directions. This\nfurther enables the new concept of omnicell wireless communication system, with\nthe base station equipped by full-angle RAA and deployed at the center of each\ncell. Compared to the conventional cell sectoring wireless communication\nsystem, the proposed omnicell system is expected to not only significantly\nreduce the inter-user interference, but also improve the cost efficiency.\nExtensive analytical and numerical results are provided to compare those key\nperformance indicators such as the spatial resolution and the communication\nrate of the proposed full-angle RAA based omnicell wireless communication\nsystem against the conventional ULA/UCA-based cell sectoring systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u89d2\u5ea6\u5c04\u7ebf\u5929\u7ebf\u9635\u5217\uff08RAA\uff09\u67b6\u6784\u53ca\u57fa\u4e8e\u8be5\u67b6\u6784\u7684omnicell\u65e0\u7ebf\u901a\u4fe1\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3001\u63d0\u9ad8\u6ce2\u675f\u6210\u5f62\u589e\u76ca\u5e76\u5b9e\u73b0\u5168\u65b9\u5411\u5747\u5300\u89d2\u5206\u8fa8\u7387\uff0c\u4ece\u800c\u51cf\u5c11\u7528\u6237\u95f4\u5e72\u6270\u5e76\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u5929\u7ebf\u67b6\u6784\uff08\u5982ULA\u548cUCA\uff09\u5728\u786c\u4ef6\u6210\u672c\u548c\u6ce2\u675f\u6210\u5f62\u589e\u76ca\u4e0a\u5b58\u5728\u5c40\u9650\uff0cRAA\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7b80\u5355\u7ebf\u6027\u9635\u5217\u6392\u5217\u548c\u5168\u89d2\u5ea6\u6269\u5c55\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63a8\u52a8\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u9769\u65b0\u3002", "method": "\u63d0\u51fa\u5168\u89d2\u5ea6RAA\u67b6\u6784\uff0c\u6269\u5c55\u5176\u65b9\u5411\u89d2\u5ea6\u81f3\u5168\u89d2\u5ea6\u57df\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u8be5\u67b6\u6784\u7684omnicell\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u3002\u901a\u8fc7\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\uff0c\u5bf9\u6bd4\u5176\u4e0e\u4f20\u7edf\u7684ULA/UCA\u6247\u533a\u5316\u7cfb\u7edf\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u5168\u89d2\u5ea6RAA\u7684omnicell\u7cfb\u7edf\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u901a\u4fe1\u901f\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6247\u533a\u5316\u7cfb\u7edf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7528\u6237\u95f4\u5e72\u6270\u5e76\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "conclusion": "\u5168\u89d2\u5ea6RAA\u67b6\u6784\u53ca\u5176omnicell\u7cfb\u7edf\u4e3a\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u66ff\u4ee3\u4f20\u7edf\u6247\u533a\u5316\u7cfb\u7edf\u3002"}}
{"id": "2509.05314", "pdf": "https://arxiv.org/pdf/2509.05314", "abs": "https://arxiv.org/abs/2509.05314", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8pages; 7figures; 4 tables", "summary": "Data scarcity continues to be a major challenge in the field of robotic\nmanipulation. Although diffusion models provide a promising solution for\ngenerating robotic manipulation videos, existing methods largely depend on 2D\ntrajectories, which inherently face issues with 3D spatial ambiguity. In this\nwork, we present a novel framework named ManipDreamer3D for generating\nplausible 3D-aware robotic manipulation videos from the input image and the\ntext instruction. Our method combines 3D trajectory planning with a\nreconstructed 3D occupancy map created from a third-person perspective, along\nwith a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D\nfirst reconstructs the 3D occupancy representation from the input image and\nthen computes an optimized 3D end-effector trajectory, minimizing path length\nwhile avoiding collisions. Next, we employ a latent editing technique to create\nvideo sequences from the initial image latent and the optimized 3D trajectory.\nThis process conditions our specially trained trajectory-to-video diffusion\nmodel to produce robotic pick-and-place videos. Our method generates robotic\nvideos with autonomously planned plausible 3D trajectories, significantly\nreducing human intervention requirements. Experimental results demonstrate\nsuperior visual quality compared to existing methods.", "AI": {"tldr": "ManipDreamer3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8f93\u5165\u56fe\u50cf\u548c\u6587\u672c\u6307\u4ee4\u751f\u62103D\u611f\u77e5\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u7ed3\u54083D\u8f68\u8ff9\u89c4\u5212\u548c\u8f68\u8ff9\u5230\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u8f68\u8ff9\u5bfc\u81f43D\u7a7a\u95f4\u6a21\u7cca\u6027\u3002", "method": "\u7ed3\u54083D\u8f68\u8ff9\u89c4\u5212\u548c\u91cd\u5efa\u76843D\u5360\u7528\u56fe\uff0c\u4f7f\u7528\u8f68\u8ff9\u5230\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u81ea\u4e3b\u89c4\u52123D\u8f68\u8ff9\u7684\u89c6\u9891\u3002", "conclusion": "ManipDreamer3D\u6709\u6548\u89e3\u51b3\u4e863D\u7a7a\u95f4\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2509.05683", "pdf": "https://arxiv.org/pdf/2509.05683", "abs": "https://arxiv.org/abs/2509.05683", "authors": ["Kuranage Roche Rayan Ranasinghe", "Henrique L. Senger", "Gustavo P. Gon\u00e7alves", "Hyeon Seok Rou", "Bruno S. Chang", "Giuseppe Thadeu Freitas de Abreu", "Didier Le Ruyet"], "title": "Affine Filter Bank Modulation (AFBM): A Novel 6G ISAC Waveform with Low PAPR and OOBE", "categories": ["eess.SP"], "comment": "Submitted to an IEEE journal", "summary": "We propose the affine filter bank modulation (AFBM) waveform for enhanced\nintegrated sensing and communications (ISAC) in sixth generation (6G), designed\nby drawing on concepts from classical filter bank multicarrier modulation\n(FBMC) theory and recent advances in chirp-domain waveforms, particularly\naffine frequency division multiplexing (AFDM). Specifically, AFBM exhibits\nseveral desirable properties, with emphasis on its remarkably low\npeak-to-average power ratio (PAPR) and reduced out-of-band emission (OOBE) when\nbenchmarked against the conventional AFDM waveform under doubly-dispersive (DD)\nchannel conditions. In the communications setting, reliable symbol detection is\nachieved using a tailored low-complexity Gaussian belief propagation\n(GaBP)-based algorithm, while in the sensing setting, a range and velocity\nestimation approach is developed that integrates an expectation maximization\n(EM)-assisted probabilistic data association (PDA) framework to accurately\nidentify surrounding targets. The highlighted performance and benefits of AFBM\nare validated through analytical and numerical evaluations, including\nconventional metrics such as ambiguity function (AF), bit error rate (BER), and\nroot mean square error (RMSE), consolidating its position as a promising\nwaveform for next-generation wireless systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAFBM\u7684\u6ce2\u5f62\uff0c\u7ed3\u5408FBMC\u7406\u8bba\u548cAFDM\uff0c\u5177\u5907\u4f4ePAPR\u548cOOBE\u7279\u6027\uff0c\u9002\u7528\u4e8e6G ISAC\u3002\u901a\u8fc7GaBP\u7b97\u6cd5\u5b9e\u73b0\u53ef\u9760\u901a\u4fe1\uff0cEM-PDA\u6846\u67b6\u5b9e\u73b0\u7cbe\u51c6\u611f\u77e5\uff0c\u7efc\u5408\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfAFDM\u3002", "motivation": "\u4e3a6G\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u6ce2\u5f62\uff0c\u514b\u670d\u4f20\u7edf\u6ce2\u5f62\u7684\u529f\u7387\u548c\u5e26\u5bbd\u9650\u5236\u3002", "method": "\u57fa\u4e8eFBMC\u548cAFDM\u7406\u8bba\u8bbe\u8ba1AFBM\u6ce2\u5f62\uff0c\u91c7\u7528GaBP\u7b97\u6cd5\u548cEM-PDA\u6846\u67b6\u5206\u522b\u4f18\u5316\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002", "result": "AFBM\u5177\u6709\u4f4ePAPR\u548cOOBE\uff0c\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u5747\u4f18\u4e8e\u4f20\u7edfAFDM\u3002", "conclusion": "AFBM\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u6ce2\u5f62\uff0c\u9002\u54086G ISAC\u5e94\u7528\u3002"}}
{"id": "2509.05315", "pdf": "https://arxiv.org/pdf/2509.05315", "abs": "https://arxiv.org/abs/2509.05315", "authors": ["Petros Loukas", "David Bassir", "Savvas Chatzichristofis", "Angelos Amanatiadis"], "title": "Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has pushed their\nboundaries to many applications in various domains. Recently, the research\ncommunity has started to evaluate their potential adoption in autonomous\nvehicles and especially as complementary modules in the perception and planning\nsoftware stacks. However, their evaluation is limited in synthetic datasets or\nmanually driving datasets without the ground truth knowledge and more\nprecisely, how the current perception and planning algorithms would perform in\nthe cases under evaluation. For this reason, this work evaluates LLMs on\nreal-world edge cases where current autonomous vehicles have been proven to\nfail. The proposed architecture consists of an open vocabulary object detector\ncoupled with prompt engineering and large language model contextual reasoning.\nWe evaluate several state-of-the-art models against real edge cases and provide\nqualitative comparison results along with a discussion on the findings for the\npotential application of LLMs as anomaly detectors in autonomous vehicles.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u4f5c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5931\u8d25\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u591a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u9010\u6e10\u663e\u73b0\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u5408\u6210\u6570\u636e\u96c6\u6216\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u5176\u5728\u73b0\u6709\u611f\u77e5\u4e0e\u89c4\u5212\u7b97\u6cd5\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u67b6\u6784\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u5bf9\u51e0\u79cd\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9a\u6027\u6bd4\u8f83\u7ed3\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86LLMs\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86LLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u8fb9\u7f18\u6848\u4f8b\u65f6\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u5176\u6027\u80fd\u3002"}}
{"id": "2509.05692", "pdf": "https://arxiv.org/pdf/2509.05692", "abs": "https://arxiv.org/abs/2509.05692", "authors": ["Armin Farhadi", "Maryam Cheraghy", "Qingqing Wu", "Eduard Jorswieck"], "title": "Resource Allocation and Beamforming in FIM-Assisted BS and STAR-BD-RIS-Aided NOMA: A Meta-Learning Approach", "categories": ["eess.SP"], "comment": null, "summary": "This study explores a flexible intelligent metasurface (FIM)-based wireless\ncommunication system that integrates simultaneously transmitting and reflecting\nbeyond diagonal reconfigurable intelligent surfaces (STAR-BD-RIS) with\nnon-orthogonal multiple access (NOMA). The system features a multi-antenna\nFIM-assisted base station (BS) aided by dual-sector BD-RIS. The FIM consists of\ncost-effective radiating elements that can independently emit signals and\ndynamically adjust their vertical positions (\"morphing\"). The goal is to\nmaximize energy efficiency by jointly optimizing BS beamforming, the\nSTAR-BD-RIS matrix, NOMA constraints, and the FIM surface shape under power\nlimits. Due to the problem's non-convexity, a meta-soft actor-critic (Meta-SAC)\nalgorithm is proposed for adaptive optimization. Simulation results show that\nMeta-SAC outperforms the Meta-DDPG algorithm, and FIM-assisted designs yield\nsubstantial energy efficiency gains over benchmark schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67d4\u6027\u667a\u80fd\u8d85\u8868\u9762\uff08FIM\uff09\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86STAR-BD-RIS\u548cNOMA\u6280\u672f\uff0c\u5e76\u901a\u8fc7Meta-SAC\u7b97\u6cd5\u5b9e\u73b0\u80fd\u6548\u6700\u5927\u5316\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7FIM\u548cSTAR-BD-RIS\u6280\u672f\u63d0\u5347\u901a\u4fe1\u7cfb\u7edf\u7684\u80fd\u91cf\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u591a\u5929\u7ebfFIM\u8f85\u52a9\u57fa\u7ad9\uff0c\u7ed3\u5408\u53cc\u6247\u533aBD-RIS\uff0c\u5e76\u63d0\u51fa\u4e86Meta-SAC\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cMeta-SAC\u7b97\u6cd5\u4f18\u4e8eMeta-DDPG\uff0cFIM\u8f85\u52a9\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u3002", "conclusion": "FIM\u4e0eSTAR-BD-RIS\u7684\u7ed3\u5408\uff0c\u52a0\u4e0aMeta-SAC\u7b97\u6cd5\u7684\u4f18\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u80fd\u6548\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05338", "pdf": "https://arxiv.org/pdf/2509.05338", "abs": "https://arxiv.org/abs/2509.05338", "authors": ["Atsushi Masumori", "Norihiro Maruyama", "Itsuki Doi", "johnsmith", "Hiroki Sato", "Takashi Ikegami"], "title": "Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We introduce Plantbot, a hybrid lifeform that connects a living plant with a\nmobile robot through a network of large language model (LLM) modules. Each\nmodule - responsible for sensing, vision, dialogue, or action - operates\nasynchronously and communicates via natural language, enabling seamless\ninteraction across biological and artificial domains. This architecture\nleverages the capacity of LLMs to serve as hybrid interfaces, where natural\nlanguage functions as a universal protocol, translating multimodal data (soil\nmoisture, temperature, visual context) into linguistic messages that coordinate\nsystem behaviors. The integrated network transforms plant states into robotic\nactions, installing normativity essential for agency within the sensor-motor\nloop. By combining biological and robotic elements through LLM-mediated\ncommunication, Plantbot behaves as an embodied, adaptive agent capable of\nresponding autonomously to environmental conditions. This approach suggests\npossibilities for a new model of artificial life, where decentralized, LLM\nmodules coordination enable novel interactions between biological and\nartificial systems.", "AI": {"tldr": "Plantbot\u662f\u4e00\u79cd\u6df7\u5408\u751f\u547d\u5f62\u5f0f\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u5757\u7f51\u7edc\u5c06\u690d\u7269\u4e0e\u79fb\u52a8\u673a\u5668\u4eba\u8fde\u63a5\u8d77\u6765\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u751f\u7269\u4e0e\u4eba\u5de5\u9886\u57df\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u751f\u7269\u548c\u673a\u5668\u4eba\u5143\u7d20\uff0c\u63a2\u7d22LLM\u4f5c\u4e3a\u6df7\u5408\u754c\u9762\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u81ea\u4e3b\u54cd\u5e94\u73af\u5883\u7684\u65b0\u578b\u4eba\u5de5\u751f\u547d\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u8fd0\u884c\u7684LLM\u6a21\u5757\uff08\u611f\u77e5\u3001\u89c6\u89c9\u3001\u5bf9\u8bdd\u3001\u884c\u52a8\uff09\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\uff0c\u5c06\u591a\u6a21\u6001\u6570\u636e\u8f6c\u5316\u4e3a\u8bed\u8a00\u6d88\u606f\u4ee5\u534f\u8c03\u7cfb\u7edf\u884c\u4e3a\u3002", "result": "Plantbot\u80fd\u591f\u5c06\u690d\u7269\u72b6\u6001\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u8868\u73b0\u51fa\u81ea\u4e3b\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u7269\u548c\u4eba\u5de5\u7cfb\u7edf\u7684\u65b0\u578b\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86LLM\u5728\u4eba\u5de5\u751f\u547d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.05903", "pdf": "https://arxiv.org/pdf/2509.05903", "abs": "https://arxiv.org/abs/2509.05903", "authors": ["Wei Huang", "Junpeng Lu", "Tianhe Xu", "Jianxu Shu", "Hao Zhang", "Kaitao Meng", "Yanan Wu"], "title": "Optimal Anchor Deployment and Topology Design for Large-Scale AUV Navigation", "categories": ["eess.SP"], "comment": null, "summary": "Seafloor acoustic anchors are an important component of AUV navigation,\nproviding absolute updates that correct inertial dead-reckoning. Unlike\nterrestrial positioning systems, the deployment of underwater anchor nodes is\nusually sparse due to the uneven distribution of underwater users, as well as\nthe high economic cost and difficult maintenance of underwater equipment. These\nanchor nodes lack satellite coverage and cannot form ubiquitous backhaul as\nterrestrial nodes do. In this paper, we investigate the optimal anchor\ndeployment topology to provide high-quality AUV navigation and positioning\nservices. We first analyze the possible deployment mode in large-scale\nunderwater navigation system, and formulate a topology optimization for\nunderwater anchor node deployment. Then, we derive a scaling law about the\ninfluence of anchors in each cluster on the navigation performance within a\ngiven area and demonstrate a service area coverage condition with a high\nprobability of reaching the destination. Finally, the optimization performance\nis evaluated through experimental results.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6d77\u5e95\u58f0\u5b66\u951a\u70b9\u7684\u6700\u4f18\u90e8\u7f72\u62d3\u6251\uff0c\u4ee5\u63d0\u5347AUV\u5bfc\u822a\u6027\u80fd\uff0c\u5206\u6790\u4e86\u5927\u89c4\u6a21\u6c34\u4e0b\u5bfc\u822a\u7cfb\u7edf\u7684\u90e8\u7f72\u6a21\u5f0f\uff0c\u5e76\u63a8\u5bfc\u4e86\u951a\u70b9\u5bf9\u5bfc\u822a\u6027\u80fd\u7684\u5f71\u54cd\u89c4\u5f8b\u3002", "motivation": "\u7531\u4e8e\u6c34\u4e0b\u8bbe\u5907\u90e8\u7f72\u7a00\u758f\u4e14\u6210\u672c\u9ad8\uff0c\u65e0\u6cd5\u50cf\u5730\u9762\u8282\u70b9\u4e00\u6837\u5b9e\u73b0\u5168\u8986\u76d6\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6700\u4f18\u90e8\u7f72\u62d3\u6251\u4ee5\u63d0\u9ad8AUV\u5bfc\u822a\u8d28\u91cf\u3002", "method": "\u5206\u6790\u4e86\u53ef\u80fd\u7684\u90e8\u7f72\u6a21\u5f0f\uff0c\u63d0\u51fa\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u4e86\u951a\u70b9\u5bf9\u5bfc\u822a\u6027\u80fd\u7684\u91cf\u5316\u89c4\u5f8b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f18\u5316\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u9ad8\u6982\u7387\u5230\u8fbe\u76ee\u7684\u5730\u7684\u670d\u52a1\u533a\u57df\u8986\u76d6\u6761\u4ef6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6c34\u4e0b\u951a\u70b9\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86AUV\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2509.05345", "pdf": "https://arxiv.org/pdf/2509.05345", "abs": "https://arxiv.org/abs/2509.05345", "authors": ["Jiasheng Qu", "Zhuo Huang", "Dezhao Guo", "Hailin Sun", "Aoran Lyu", "Chengkai Dai", "Yeung Yam", "Guoxin Fang"], "title": "INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing", "categories": ["cs.RO", "cs.CG"], "comment": null, "summary": "We introduce a general, scalable computational framework for multi-axis 3D\nprinting based on implicit neural fields (INFs) that unifies all stages of\ntoolpath generation and global collision-free motion planning. In our pipeline,\ninput models are represented as signed distance fields, with fabrication\nobjectives such as support-free printing, surface finish quality, and extrusion\ncontrol being directly encoded in the optimization of an implicit guidance\nfield. This unified approach enables toolpath optimization across both surface\nand interior domains, allowing shell and infill paths to be generated via\nimplicit field interpolation. The printing sequence and multi-axis motion are\nthen jointly optimized over a continuous quaternion field. Our continuous\nformulation constructs the evolving printing object as a time-varying SDF,\nsupporting differentiable global collision handling throughout INF-based motion\nplanning. Compared to explicit-representation-based methods, INF-3DP achieves\nup to two orders of magnitude speedup and significantly reduces\nwaypoint-to-surface error. We validate our framework on diverse, complex models\nand demonstrate its efficiency with physical fabrication experiments using a\nrobot-assisted multi-axis system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u573a\uff08INFs\uff09\u7684\u591a\u8f743D\u6253\u5370\u901a\u7528\u8ba1\u7b97\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u5de5\u5177\u8def\u5f84\u751f\u6210\u548c\u5168\u5c40\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u7684\u5404\u4e2a\u9636\u6bb5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf3D\u6253\u5370\u65b9\u6cd5\u5728\u5de5\u5177\u8def\u5f84\u4f18\u5316\u548c\u78b0\u649e\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u9690\u5f0f\u8ddd\u79bb\u573a\u8868\u793a\u6a21\u578b\uff0c\u5c06\u6253\u5370\u76ee\u6807\uff08\u5982\u65e0\u652f\u6491\u6253\u5370\u548c\u8868\u9762\u8d28\u91cf\uff09\u7f16\u7801\u5230\u9690\u5f0f\u5f15\u5bfc\u573a\u7684\u4f18\u5316\u4e2d\uff0c\u5e76\u901a\u8fc7\u8fde\u7eed\u7684\u56db\u5143\u6570\u573a\u8054\u5408\u4f18\u5316\u6253\u5370\u5e8f\u5217\u548c\u591a\u8f74\u8fd0\u52a8\u3002", "result": "\u4e0e\u57fa\u4e8e\u663e\u5f0f\u8868\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cINF-3DP\u5b9e\u73b0\u4e86\u901f\u5ea6\u4e0a\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u63d0\u5347\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8def\u5f84\u70b9\u5230\u8868\u9762\u7684\u8bef\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u8f743D\u6253\u5370\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6a21\u578b\u7684\u7269\u7406\u5236\u9020\u3002"}}
{"id": "2509.05955", "pdf": "https://arxiv.org/pdf/2509.05955", "abs": "https://arxiv.org/abs/2509.05955", "authors": ["Jiali He", "Sheng Shen", "Jiamin Wu", "Xiaohan Kong", "Yamei Dai", "Liang Tan", "Zheng Xu"], "title": "Active noise cancellation in ultra-low field MRI: distinct strategies for different channels", "categories": ["eess.SP"], "comment": null, "summary": "Ultra-low field magnetic resonance imaging(ULF-MRI) systems operating in open\nenvironments are highly susceptible to composite electromagnetic\ninterference(EMI). Different imaging channels respond non-uniformly to EMI\nowing to their distinct coupling characteristics. Here, we investigate\nchannel-specific interference pathways in a permanent-magnet-based low-field\nMRI system and show that saddle coils are intrinsically more vulnerable to\ntransverse EMI components than solenoidal coils. To mitigate these\nheterogeneous coupling effects, we propose a dual-stage suppression strategy\nthat combines front-end spatial-domain inverse field reconstruction with\nback-end channel-adaptive active noise cancellation. Experiments demonstrate\nthat this approach suppresses EMI by more than 80%, substantially improves\ninter-channel signal-to-noise ratio(SNR) consistency, and enhances the\nfused-image SNR by 24%. These findings elucidate the channel-dependent nature\nof EMI coupling and establish targeted mitigation strategies, providing both a\ntheoretical basis and practical guidance for noise suppression in future\narray-coil ULF-MRI systems.", "AI": {"tldr": "\u5206\u6790\u4e86\u6c38\u78c1\u4f4e\u573aMRI\u7cfb\u7edf\u4e2d\u901a\u9053\u7279\u5f02\u6027\u5e72\u6270\u8def\u5f84\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u6291\u5236\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7535\u78c1\u5e72\u6270\u5e76\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u8d85\u4f4e\u573aMRI\u7cfb\u7edf\u4e2d\u4e0d\u540c\u901a\u9053\u5bf9\u7535\u78c1\u5e72\u6270\u7684\u975e\u5747\u5300\u54cd\u5e94\uff0c\u4ee5\u89e3\u51b3\u5e72\u6270\u95ee\u9898\u5e76\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u524d\u7aef\u7a7a\u95f4\u57df\u9006\u573a\u91cd\u5efa\u548c\u540e\u7aef\u901a\u9053\u81ea\u9002\u5e94\u4e3b\u52a8\u566a\u58f0\u6d88\u9664\u76f8\u7ed3\u5408\u7684\u53cc\u91cd\u6291\u5236\u7b56\u7565\u6765\u964d\u4f4e\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c1180%\u4ee5\u4e0a\u7684\u5e72\u6270\uff0c\u63d0\u9ad8\u4fe1\u566a\u6bd4\u4e00\u81f4\u602724%\u3002", "conclusion": "\u63ed\u793a\u4e86\u7535\u78c1\u5e72\u6270\u7684\u901a\u9053\u4f9d\u8d56\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u9635\u5217\u7ebf\u5708\u7cfb\u7edf\u7684\u566a\u58f0\u6291\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.05355", "pdf": "https://arxiv.org/pdf/2509.05355", "abs": "https://arxiv.org/abs/2509.05355", "authors": ["Ahmed R. Sadik", "Muhammad Ashfaq", "Niko M\u00e4kitalo", "Tommi Mikkonen"], "title": "Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "The deployment of autonomous drone swarms in disaster response missions\nnecessitates the development of flexible, scalable, and robust coordination\nsystems. Traditional fixed architectures struggle to cope with dynamic and\nunpredictable environments, leading to inefficiencies in energy consumption and\nconnectivity. This paper addresses this gap by proposing an adaptive\narchitecture for drone swarms, leveraging a Large Language Model to dynamically\nselect the optimal architecture as centralized, hierarchical, or holonic based\non real time mission parameters such as task complexity, swarm size, and\ncommunication stability. Our system addresses the challenges of scalability,\nadaptability, and robustness,ensuring efficient energy consumption and\nmaintaining connectivity under varying conditions. Extensive simulations\ndemonstrate that our adaptive architecture outperforms traditional static\nmodels in terms of scalability, energy efficiency, and connectivity. These\nresults highlight the potential of our approach to provide a scalable,\nadaptable, and resilient solution for real world disaster response scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u7fa4\u67b6\u6784\uff0c\u7528\u4e8e\u52a8\u6001\u9009\u62e9\u6700\u4f18\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u3001\u5206\u5c42\u5f0f\u6216\u6574\u4f53\u5f0f\uff09\uff0c\u4ee5\u5e94\u5bf9\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u73af\u5883\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u67b6\u6784\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u4e0d\u53ef\u9884\u6d4b\u7684\u73af\u5883\uff0c\u5bfc\u81f4\u80fd\u6e90\u6d88\u8017\u548c\u8fde\u63a5\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u3001\u7fa4\u89c4\u6a21\u548c\u901a\u4fe1\u7a33\u5b9a\u6027\u7b49\u5b9e\u65f6\u53c2\u6570\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u67b6\u6784\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u67b6\u6784\u5728\u53ef\u6269\u5c55\u6027\u3001\u80fd\u6e90\u6548\u7387\u548c\u8fde\u63a5\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u707e\u96be\u54cd\u5e94\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05959", "pdf": "https://arxiv.org/pdf/2509.05959", "abs": "https://arxiv.org/abs/2509.05959", "authors": ["Pierluigi Poggiolini", "Francesco Poletti"], "title": "The Case for a DNANF 1Pb/s Trans-Atlantic Submarine Cable", "categories": ["eess.SP"], "comment": "preprint of a European Conference on Optical Communications (ECOC)\n  2025 paper", "summary": "The recent progress in low-loss hollow-core fibers allows to speculate on the\npossibility of building a transatlantic submarine cable that can achieve the\ngoal of 1 Pb/s per direction, leveraging bidirectional transmission, and at the\nsame time drastically increase span length, theoretically to 200km.", "AI": {"tldr": "\u4f4e\u635f\u8017\u7a7a\u5fc3\u5149\u7ea4\u7684\u8fdb\u5c55\u53ef\u80fd\u5b9e\u73b0\u8de8\u5927\u897f\u6d0b\u6d77\u5e95\u7535\u7f06\uff0c\u652f\u6301\u5355\u65b9\u54111 Pb/s\u901f\u7387\u548c200km\u8de8\u8ddd\u3002", "motivation": "\u63a2\u7d22\u7a7a\u5fc3\u5149\u7ea4\u6280\u672f\u5728\u9ad8\u901f\u957f\u8ddd\u79bb\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5229\u7528\u53cc\u5411\u4f20\u8f93\u548c\u7a7a\u5fc3\u5149\u7ea4\u7684\u4f4e\u635f\u8017\u7279\u6027\u3002", "result": "\u7406\u8bba\u4e0a\u53ef\u5b9e\u73b01 Pb/s\u6bcf\u65b9\u5411\u548c200km\u8de8\u8ddd\u7684\u901a\u4fe1\u3002", "conclusion": "\u7a7a\u5fc3\u5149\u7ea4\u6709\u671b\u5927\u5e45\u63d0\u5347\u8de8\u6d0b\u901a\u4fe1\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.05356", "pdf": "https://arxiv.org/pdf/2509.05356", "abs": "https://arxiv.org/abs/2509.05356", "authors": ["Justus Huebotter", "Pablo Lanillos", "Marcel van Gerven", "Serge Thill"], "title": "Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u8bad\u7ec3\u5168\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u624b\u81c2\u7684\u591a\u81ea\u7531\u5ea6\u63a7\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u7ef4\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bad\u7ec3SNNs\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u8fde\u7eed\u8fd0\u52a8\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u6f0f\u7535\u79ef\u5206\u70b9\u706b\uff08LIF\uff09\u52a8\u529b\u5b66\u548c\u66ff\u4ee3\u68af\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u52a8\u6001\u9884\u6d4b\u7684\u524d\u5411\u6a21\u578b\u548c\u76ee\u6807\u5bfc\u5411\u52a8\u4f5c\u7684\u7b56\u7565\u7f51\u7edc\u3002", "result": "\u57282D\u5e73\u9762\u5230\u8fbe\u4efb\u52a1\u548c6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4eff\u771f\u4e2d\uff0cSNNs\u8868\u73b0\u7a33\u5b9a\u5e76\u80fd\u5b9e\u73b0\u7cbe\u786e\u7684\u626d\u77e9\u63a7\u5236\u3002\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u4e86\u521d\u59cb\u5316\u3001\u53ef\u5b66\u4e60\u65f6\u95f4\u5e38\u6570\u548c\u6b63\u5219\u5316\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u867d\u7136SNNs\u53ef\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u6709\u6548\u7684\u63a7\u5236\uff0c\u4f46\u5176\u5bf9\u8d85\u53c2\u6570\u8bbe\u7f6e\u9ad8\u5ea6\u654f\u611f\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.05971", "pdf": "https://arxiv.org/pdf/2509.05971", "abs": "https://arxiv.org/abs/2509.05971", "authors": ["Kaiyi Chi", "Yinghui He", "Qianqian Yang", "Zhiping Jiang", "Yuanchao Shu", "Zhiqin Wang", "Jun Luo", "Jiming Chen"], "title": "DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions", "categories": ["eess.SP", "cs.MM"], "comment": "13 pages, 43 figures", "summary": "Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a\npromising technique in 6G for enhancing the efficiency and reliability of data\ntransmission across diverse modalities, particularly in low signal-to-noise\nratio (SNR) environments. This advantage is realized by leveraging powerful\nneural networks to learn an optimal end-to-end mapping from the source data\ndirectly to the transmit symbol sequence, eliminating the need for separate\nsource coding, channel coding, and modulation. Although numerous efforts have\nbeen made towards efficient DeepJSCC, they have largely stayed at numerical\nsimulations that can be far from practice, leaving the real-world viability of\nDeepJSCC largely unverified. To this end, we prototype DeepStream upon\northogonal frequency division multiplexing (OFDM) technology to offer efficient\nand robust DeepJSCC for multimedia transmission. In conforming to OFDM, we\ndevelop both a feature-to-symbol mapping method and a cross-subcarrier\nprecoding method to improve the subcarrier independence and reduce\npeak-to-average power ratio. To reduce system complexity and enable flexibility\nin accommodating varying quality of service requirements, we further propose a\nprogressive coding strategy that adjusts the compression ratio based on latency\nwith minimal performance loss. We implement DeepStream for real-time image\ntransmission and video streaming using software-defined radio. Extensive\nevaluations verify that DeepStream outperforms both the standard scheme and the\ndirect deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves\na PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video\nstreaming, whereas the standard scheme fails to recover meaningful information.", "AI": {"tldr": "DeepStream\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u6280\u672f\uff0c\u901a\u8fc7OFDM\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u5a92\u4f53\u4f20\u8f93\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709DeepJSCC\u7814\u7a76\u591a\u505c\u7559\u5728\u6570\u503c\u6a21\u62df\uff0c\u7f3a\u4e4f\u5b9e\u9645\u9a8c\u8bc1\uff0cDeepStream\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u7279\u5f81\u5230\u7b26\u53f7\u6620\u5c04\u548c\u4ea4\u53c9\u5b50\u8f7d\u6ce2\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6e10\u8fdb\u7f16\u7801\u7b56\u7565\u4ee5\u9002\u5e94\u4e0d\u540c\u670d\u52a1\u8d28\u91cf\u9700\u6c42\u3002", "result": "DeepStream\u572810 dB SNR\u4e0b\uff0c\u56fe\u50cf\u4f20\u8f93PSNR\u8fbe35 dB\uff0c\u89c6\u9891\u6d41MS-SSIM\u8fbe20 dB\uff0c\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u65b9\u6848\u3002", "conclusion": "DeepStream\u9a8c\u8bc1\u4e86DeepJSCC\u5728\u73b0\u5b9e\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.05368", "pdf": "https://arxiv.org/pdf/2509.05368", "abs": "https://arxiv.org/abs/2509.05368", "authors": ["Quan Chen", "Chenrui Shi", "Qi Chen", "Yuwei Wu", "Zhi Gao", "Xintong Zhang", "Rui Gao", "Kun Wu", "Yunde Jia"], "title": "Long-Horizon Visual Imitation Learning via Plan and Code Reflection", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.9; I.2.10"], "comment": "9 pages, 4 figures. Submitted to AAAI 2026", "summary": "Learning from long-horizon demonstrations with complex action sequences\npresents significant challenges for visual imitation learning, particularly in\nunderstanding temporal relationships of actions and spatial relationships\nbetween objects. In this paper, we propose a new agent framework that\nincorporates two dedicated reflection modules to enhance both plan and code\ngeneration. The plan generation module produces an initial action sequence,\nwhich is then verified by the plan reflection module to ensure temporal\ncoherence and spatial alignment with the demonstration video. The code\ngeneration module translates the plan into executable code, while the code\nreflection module verifies and refines the generated code to ensure correctness\nand consistency with the generated plan. These two reflection modules jointly\nenable the agent to detect and correct errors in both the plan generation and\ncode generation, improving performance in tasks with intricate temporal and\nspatial dependencies. To support systematic evaluation, we introduce\nLongVILBench, a benchmark comprising 300 human demonstrations with action\nsequences of up to 18 steps. LongVILBench emphasizes temporal and spatial\ncomplexity across multiple task types. Experimental results demonstrate that\nexisting methods perform poorly on this benchmark, whereas our new framework\nestablishes a strong baseline for long-horizon visual imitation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u53cd\u601d\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u5347\u8ba1\u5212\u548c\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u4e2d\u957f\u65f6\u7a0b\u548c\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u7684\u6311\u6218\u3002", "motivation": "\u957f\u65f6\u7a0b\u6f14\u793a\u4e2d\u7684\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u5bf9\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u662f\u52a8\u4f5c\u7684\u65f6\u95f4\u5173\u7cfb\u548c\u5bf9\u8c61\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u6846\u67b6\u5305\u542b\u8ba1\u5212\u751f\u6210\u6a21\u5757\u548c\u4ee3\u7801\u751f\u6210\u6a21\u5757\uff0c\u5206\u522b\u901a\u8fc7\u53cd\u601d\u6a21\u5757\u9a8c\u8bc1\u548c\u6539\u8fdb\uff0c\u786e\u4fdd\u8ba1\u5212\u548c\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6846\u67b6\u5728LongVILBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cd\u601d\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.05977", "pdf": "https://arxiv.org/pdf/2509.05977", "abs": "https://arxiv.org/abs/2509.05977", "authors": ["Ayush Jha", "Dhanireddy Chandrika", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "title": "3D-Image Reconstruction using MIMO-SAR FMCW Radar", "categories": ["eess.SP"], "comment": "Presented In ISCS25", "summary": "With the advancement of millimeter-wave radar technology, Synthetic Aperture\nRadar (SAR) imaging at millimeter-wave frequencies has gained significant\nattention in both academic research and industrial applications. However,\ntraditional SAR imaging algorithms primarily focus on extracting\ntwo-dimensional information from detected targets, which limits their potential\nfor 3D scene reconstruction. In this work, we demonstrated a fast time-domain\nreconstruction algorithm for achieving high-resolution 3D radar imaging at\nmillimeter-wave (mmWave) frequencies. This approach leverages a combination of\nvirtual Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous\nWave (FMCW) radar with the precision of Synthetic Aperture Radar (SAR)\ntechnique, setting the stage for a new era of advanced radar imaging\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62dfMIMO FMCW\u96f7\u8fbe\u548cSAR\u6280\u672f\u7684\u9ad8\u5206\u8fa8\u73873D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6210\u50cf\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edfSAR\u7b97\u6cd5\u4ec5\u63d0\u53d6\u4e8c\u7ef4\u4fe1\u606f\uff0c\u9650\u5236\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u9ad8\u5206\u8fa8\u73873D\u6210\u50cf\u3002", "method": "\u7ed3\u5408\u865a\u62dfMIMO FMCW\u96f7\u8fbe\u548cSAR\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u65f6\u57df\u91cd\u5efa\u7b97\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u6beb\u7c73\u6ce2\u9891\u7387\u4e0b\u7684\u9ad8\u5206\u8fa8\u73873D\u96f7\u8fbe\u6210\u50cf\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u5148\u8fdb\u7684\u96f7\u8fbe\u6210\u50cf\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.05391", "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Sch\u00fcppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "categories": ["cs.RO", "cs.HC", "cs.MM"], "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30Magic Leap 2\u63a7\u5236\u5668\u5728AR\u786c\u4ef6\u4e2d\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u586b\u8865\u4e86\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7ebf\u548c\u65b9\u6cd5\u8bba\u3002", "motivation": "\u5546\u4e1aAR\u786c\u4ef6\u7684\u4e25\u683c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u73b0\u4ee3\u5934\u6234\u663e\u793a\u5668\uff08HMD\uff09\u7684\u5de5\u5177\u8ddf\u8e2a\u516c\u5171\u57fa\u51c6\u6709\u9650\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u624b\u81c2\u548c\u5149\u5b66\u8ddf\u8e2a\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u51c6\uff0c\u8bc4\u4f30\u9759\u6001\u548c\u52a8\u6001\u6027\u80fd\uff0c\u5305\u62ec\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u8def\u5f84\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u4e86ML2\u63a7\u5236\u5668\u7684\u51c6\u786e\u6027\u548c\u91cd\u590d\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bc4\u4f30\u63a7\u5236\u5668\u5728\u5de5\u4e1aAR\u5f15\u5bfc\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.06070", "pdf": "https://arxiv.org/pdf/2509.06070", "abs": "https://arxiv.org/abs/2509.06070", "authors": ["Abdulmohsen Alsaui", "Neel Kanth Kundu", "Hyundong Shin", "Octavia A. Dobre"], "title": "Quantum Radar for ISAC: Sum-Rate Optimization", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) is emerging as a key enabler for\nspectrum-efficient and hardware-converged wireless networks. However, classical\nradar systems within ISAC architectures face fundamental limitations under low\nsignal power and high-noise conditions. This paper proposes a novel framework\nthat embeds quantum illumination radar into a base station to simultaneously\nsupport full-duplex classical communication and quantum-enhanced target\ndetection. The resulting integrated quantum sensing and classical communication\n(IQSCC) system is optimized via a sum-rate maximization formulation subject to\nradar sensing constraints. The non-convex joint optimization of transmit power\nand beamforming vectors is tackled using the successive convex approximation\ntechnique. Furthermore, we derive performance bounds for classical and quantum\nradar protocols under the statistical detection theory, highlighting the\nquantum advantage in low signal-to-interference-plus-noise ratio regimes.\nSimulation results demonstrate that the proposed IQSCC system achieves a higher\ncommunication throughput than the conventional ISAC baseline while satisfying\nthe sensing requirement.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u91cf\u5b50\u4f20\u611f\u4e0e\u7ecf\u5178\u901a\u4fe1\uff08IQSCC\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u57fa\u7ad9\u4e2d\u5d4c\u5165\u91cf\u5b50\u7167\u660e\u96f7\u8fbe\uff0c\u540c\u65f6\u652f\u6301\u5168\u53cc\u5de5\u7ecf\u5178\u901a\u4fe1\u548c\u91cf\u5b50\u589e\u5f3a\u76ee\u6807\u68c0\u6d4b\uff0c\u4f18\u5316\u4e86\u4f20\u8f93\u529f\u7387\u548c\u6ce2\u675f\u5f62\u6210\u5411\u91cf\u7684\u8054\u5408\u95ee\u9898\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u5c55\u793a\u4e86\u91cf\u5b50\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfISAC\u67b6\u6784\u4e2d\u96f7\u8fbe\u7cfb\u7edf\u5728\u4f4e\u4fe1\u53f7\u529f\u7387\u548c\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u91cf\u5b50\u96f7\u8fbe\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u9891\u8c31\u6548\u7387\u548c\u786c\u4ef6\u878d\u5408\u65e0\u7ebf\u7f51\u7edc\u7684\u6027\u80fd\u3002", "method": "\u8bba\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u548c\u901f\u7387\u6700\u5927\u5316\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u96f7\u8fbe\u611f\u77e5\u7ea6\u675f\uff0c\u4f7f\u7528\u9010\u6b21\u51f8\u903c\u8fd1\u6280\u672f\u89e3\u51b3\u4e86\u975e\u51f8\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63a8\u5bfc\u4e86\u7ecf\u5178\u548c\u91cf\u5b50\u96f7\u8fbe\u534f\u8bae\u7684\u6027\u80fd\u754c\u9650\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684IQSCC\u7cfb\u7edf\u5728\u6ee1\u8db3\u611f\u77e5\u9700\u6c42\u7684\u540c\u65f6\uff0c\u6bd4\u4f20\u7edfISAC\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u901a\u4fe1\u541e\u5410\u91cf\u3002", "conclusion": "IQSCC\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u91cf\u5b50\u4f20\u611f\u548c\u7ecf\u5178\u901a\u4fe1\uff0c\u5c55\u793a\u4e86\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u91cf\u5b50\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2509.05397", "pdf": "https://arxiv.org/pdf/2509.05397", "abs": "https://arxiv.org/abs/2509.05397", "authors": ["Matthew Lai", "Keegan Go", "Zhibin Li", "Torsten Kroger", "Stefan Schaal", "Kelsey Allen", "Jonathan Scholz"], "title": "RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "Published in Science Robotics", "summary": "Modern robotic manufacturing requires collision-free coordination of multiple\nrobots to complete numerous tasks in shared, obstacle-rich workspaces. Although\nindividual tasks may be simple in isolation, automated joint task allocation,\nscheduling, and motion planning under spatio-temporal constraints remain\ncomputationally intractable for classical methods at real-world scales.\nExisting multi-arm systems deployed in the industry rely on human intuition and\nexperience to design feasible trajectories manually in a labor-intensive\nprocess. To address this challenge, we propose a reinforcement learning (RL)\nframework to achieve automated task and motion planning, tested in an\nobstacle-rich environment with eight robots performing 40 reaching tasks in a\nshared workspace, where any robot can perform any task in any order. Our\napproach builds on a graph neural network (GNN) policy trained via RL on\nprocedurally-generated environments with diverse obstacle layouts, robot\nconfigurations, and task distributions. It employs a graph representation of\nscenes and a graph policy neural network trained through reinforcement learning\nto generate trajectories of multiple robots, jointly solving the sub-problems\nof task allocation, scheduling, and motion planning. Trained on large randomly\ngenerated task sets in simulation, our policy generalizes zero-shot to unseen\nsettings with varying robot placements, obstacle geometries, and task poses. We\nfurther demonstrate that the high-speed capability of our solution enables its\nuse in workcell layout optimization, improving solution times. The speed and\nscalability of our planner also open the door to new capabilities such as\nfault-tolerant planning and online perception-based re-planning, where rapid\nadaptation to dynamic task sets is required.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u96be\u4ee5\u9ad8\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7684\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u591a\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u89e3\u51b3\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u6846\u67b6\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u573a\u666f\uff0c\u63d0\u9ad8\u4e86\u89c4\u5212\u901f\u5ea6\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2509.06170", "pdf": "https://arxiv.org/pdf/2509.06170", "abs": "https://arxiv.org/abs/2509.06170", "authors": ["Hao Jiang", "Zhaolin Wang", "Yuanwei Liu", "Arumugam Nallanathan", "Zhiguo Ding"], "title": "Pinching Antenna System (PASS) Enhanced Covert Communications: Against Warden via Sensing", "categories": ["eess.SP"], "comment": "Submit to possible IEEE journal", "summary": "A sensing-aided covert communication network empowered by pinching antenna\nsystems (PASS) is proposed in this work. Unlike conventional fixed-position\nMIMO arrays, PASS dynamically reconfigures its pinching antennas (PAs) closer\nto the legitimate user, substantially enhancing covertness. To further secure\nthe adversary's channel state information (CSI), a sensing function is\nleveraged to track the malicious warden's movements. In particular, this paper\nfirst proposes an extended Kalman filter (EKF) based approach to fulfilling the\ntracking function. Building on this, a covert communication problem is\nformulated with a joint design of beamforming, artificial noise (AN) signals,\nand the position of PAs. Then, the beamforming and AN design subproblems are\nresolved jointly with a subspace approach, while the PA position optimization\nsubproblem is handled by a deep reinforcement learning (DRL) approach by\ntreating the evolution of the warden's mobility status as a temporally\ncorrected process. Numerical results are presented and demonstrate that: i) the\nEKF approach can accurately track the warden's CSI with low complexity, ii) the\neffectiveness of the proposed solution is verified by its outperformance over\nthe greedy and searching-based benchmarks, and iii) with new design degrees of\nfreedom (DoFs), the performance of PASS is superior to the conventional\nfully-digital MIMO systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u538b\u63a5\u5929\u7ebf\u7cfb\u7edf\u7684\u611f\u77e5\u8f85\u52a9\u9690\u853d\u901a\u4fe1\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5929\u7ebf\u4f4d\u7f6e\u589e\u5f3a\u9690\u853d\u6027\uff0c\u7ed3\u5408\u611f\u77e5\u529f\u80fd\u8ffd\u8e2a\u6076\u610f\u76d1\u542c\u8005\uff0c\u5e76\u901a\u8fc7EKF\u548cDRL\u4f18\u5316\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfMIMO\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfMIMO\u7cfb\u7edf\u9690\u853d\u6027\u6709\u9650\uff0c\u9700\u8981\u52a8\u6001\u8c03\u6574\u5929\u7ebf\u4f4d\u7f6e\u5e76\u8ffd\u8e2a\u76d1\u542c\u8005\u4ee5\u63d0\u5347\u9690\u853d\u901a\u4fe1\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faEKF\u8ffd\u8e2a\u76d1\u542c\u8005\u8fd0\u52a8\uff0c\u8054\u5408\u8bbe\u8ba1\u6ce2\u675f\u6210\u5f62\u3001\u4eba\u5de5\u566a\u58f0\u548c\u538b\u63a5\u5929\u7ebf\u4f4d\u7f6e\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u65b9\u6cd5\u548cDRL\u5206\u522b\u4f18\u5316\u3002", "result": "EKF\u80fd\u4f4e\u590d\u6742\u5ea6\u7cbe\u786e\u8ffd\u8e2a\u76d1\u542c\u8005\uff1b\u63d0\u51fa\u7684\u65b9\u6848\u4f18\u4e8e\u8d2a\u5a6a\u548c\u641c\u7d22\u57fa\u51c6\uff1b\u538b\u63a5\u5929\u7ebf\u7cfb\u7edf\u6bd4\u4f20\u7edfMIMO\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u52a8\u6001\u5929\u7ebf\u548c\u611f\u77e5\u8f85\u52a9\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u9690\u853d\u901a\u4fe1\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u538b\u63a5\u5929\u7ebf\u7cfb\u7edf\u5728\u65b0\u81ea\u7531\u5ea6\u4e0b\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.05433", "pdf": "https://arxiv.org/pdf/2509.05433", "abs": "https://arxiv.org/abs/2509.05433", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering", "categories": ["cs.RO"], "comment": "20 pages, 5 figures", "summary": "Haptic interfaces that can simultaneously modulate multiple physical\nproperties remain a fundamental challenge in human-robot interaction. Existing\nsystems typically allow the rendering of either geometric features or\nmechanical properties, but rarely both, within wearable form factors. Here, we\nintroduce HapMorph, a pneumatic framework that enables continuous, simultaneous\nmodulation of object size and stiffness through antagonistic fabric-based\npneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for\nhands interaction achieving size variation from 50 to 104 mm, stiffness\nmodulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through\nsystematic characterization, we demonstrate decoupled control of size and\nstiffness properties via dual-chamber pressure regulation. Human perception\nstudies with 10 participants reveal that users can distinguish nine discrete\nstates across three size categories and three stiffness levels with 89.4%\naccuracy and 6.7 s average response time. We further demonstrate extended\narchitectures that combine AFPAs with complementary pneumatic structures to\nenable shape or geometry morphing with concurrent stiffness control. Our\nresults establish antagonistic pneumatic principle as a pathway toward\nnext-generation haptic interfaces, capable of multi-dimensiona rendering\nproperties within practical wearable constraints.", "AI": {"tldr": "HapMorph\u662f\u4e00\u79cd\u6c14\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u7ec7\u7269\u6c14\u52a8\u6267\u884c\u5668\uff08AFPAs\uff09\u5b9e\u73b0\u7269\u4f53\u5c3a\u5bf8\u548c\u521a\u5ea6\u7684\u8fde\u7eed\u540c\u6b65\u8c03\u5236\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u5c55\u793a\u4e86\u591a\u7ef4\u5ea6\u89e6\u89c9\u6e32\u67d3\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u754c\u9762\u901a\u5e38\u53ea\u80fd\u6e32\u67d3\u51e0\u4f55\u7279\u5f81\u6216\u673a\u68b0\u5c5e\u6027\uff0c\u96be\u4ee5\u540c\u65f6\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u5b9e\u73b0\u591a\u79cd\u7269\u7406\u5c5e\u6027\u7684\u8c03\u5236\u3002", "method": "\u91c7\u7528AFPAs\u6280\u672f\uff0c\u901a\u8fc7\u53cc\u8154\u538b\u529b\u8c03\u8282\u5b9e\u73b0\u5c3a\u5bf8\u548c\u521a\u5ea6\u7684\u89e3\u8026\u63a7\u5236\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u8868\u5f81\u548c\u7528\u6237\u611f\u77e5\u7814\u7a76\u3002", "result": "\u539f\u578b\u5b9e\u73b0\u4e8650\u81f3104 mm\u7684\u5c3a\u5bf8\u53d8\u5316\u30014.7 N/mm\u7684\u521a\u5ea6\u8c03\u5236\uff0c\u7528\u6237\u80fd\u4ee589.4%\u51c6\u786e\u7387\u533a\u52069\u79cd\u72b6\u6001\u3002", "conclusion": "HapMorph\u5c55\u793a\u4e86\u5bf9\u6297\u6027\u6c14\u52a8\u539f\u7406\u5728\u591a\u7ef4\u5ea6\u89e6\u89c9\u754c\u9762\u4e2d\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u53ef\u7a7f\u6234\u5e94\u7528\u3002"}}
{"id": "2509.06257", "pdf": "https://arxiv.org/pdf/2509.06257", "abs": "https://arxiv.org/abs/2509.06257", "authors": ["Yuyan Wu", "Jiale Zhang", "Moon Lee", "Cherrelle Smith", "Xinyi Li", "Ankur Senapati", "Pei Zhang", "Hae Young Noh"], "title": "Human Body Weight Estimation Through Music-Induced Bed Vibrations", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "Submitted to Mobicom 2026", "summary": "Rapid and accurate body weight estimation is critical in emergency medical\ncare, as it directly influences treatment decisions, such as drug dosing,\ndefibrillation energy selection, and fluid resuscitation. Traditional methods\nsuch as stand-on scales, length-based tapes, or transfer-based weighing scales\nare often impractical for immobilized patients, inaccurate, or labor-intensive\nand time-consuming. This paper introduces MelodyBedScale, a non-intrusive and\nrapid on-bed weight estimation system that leverages bed vibration induced by\nmusic. The core insight is that body weight affects the vibration transfer\nfunction of the bed-body system, which is captured using vibration sensors\nplaced on opposite sides of the bed. First, we identify weight-sensitive\nfrequency bands and compose clinically acceptable soft, natural music with high\nsignal energy in these frequency bands. This music is then played through a\nspeaker mounted on the bed to induce bed vibrations. Additionally, to\nefficiently capture the complex weight-vibration relationship with limited data\nand enhance generalizability to unseen individuals and weights, we\ntheoretically analyze the weight-vibration relationship and integrate the\nresults into the activation functions of the neural network for\nphysics-informed weight regression. We evaluated MelodyBedScale on both wooden\nand steel beds across 11 participants, achieving a mean absolute error of up to\n1.55 kg.", "AI": {"tldr": "MelodyBedScale\u662f\u4e00\u79cd\u5229\u7528\u97f3\u4e50\u5f15\u8d77\u7684\u5e8a\u632f\u52a8\u6765\u5feb\u901f\u3001\u975e\u4fb5\u5165\u6027\u5730\u4f30\u7b97\u60a3\u8005\u4f53\u91cd\u7684\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u6025\u6551\u533b\u7597\u573a\u666f\u3002", "motivation": "\u5728\u6025\u6551\u533b\u7597\u4e2d\uff0c\u5feb\u901f\u51c6\u786e\u7684\u4f53\u91cd\u4f30\u7b97\u5bf9\u6cbb\u7597\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4f53\u91cd\u79e4\u6216\u957f\u5ea6\u6d4b\u91cf\uff09\u5f80\u5f80\u4e0d\u5b9e\u7528\u6216\u4e0d\u51c6\u786e\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u632f\u52a8\u4f20\u611f\u5668\u6355\u6349\u5e8a\u632f\u52a8\u4fe1\u53f7\uff0c\u7ed3\u5408\u7279\u5b9a\u9891\u7387\u7684\u97f3\u4e50\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u8fdb\u884c\u4f53\u91cd\u56de\u5f52\u5206\u6790\u3002", "result": "\u572811\u540d\u53c2\u4e0e\u8005\u4e2d\uff0cMelodyBedScale\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.55\u516c\u65a4\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u5e8a\u94fa\u3002", "conclusion": "MelodyBedScale\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u6025\u6551\u4f53\u91cd\u4f30\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05475", "pdf": "https://arxiv.org/pdf/2509.05475", "abs": "https://arxiv.org/abs/2509.05475", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Autonomous regolith excavation is a cornerstone of in-situ resource\nutilization for a sustained human presence beyond Earth. However, this task is\nfundamentally hindered by the complex interaction dynamics of granular media\nand the operational need for robots to use diverse tools. To address these\nchallenges, this work introduces a framework where a model-based reinforcement\nlearning agent learns within a parallelized simulation. This environment\nleverages high-fidelity particle physics and procedural generation to create a\nvast distribution of both lunar terrains and excavation tool geometries. To\nmaster this diversity, the agent learns an adaptive interaction strategy by\ndynamically modulating its own stiffness and damping at each control step\nthrough operational space control. Our experiments demonstrate that training\nwith a procedural distribution of tools is critical for generalization and\nenables the development of sophisticated tool-aware behavior. Furthermore, we\nshow that augmenting the agent with visual feedback significantly improves task\nsuccess. These results represent a validated methodology for developing the\nrobust and versatile autonomous systems required for the foundational tasks of\nfuture space missions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6708\u7403\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6316\u6398\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u521a\u5ea6\u548c\u963b\u5c3c\u4ee5\u53ca\u89c6\u89c9\u53cd\u9988\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u6708\u7403\u73af\u5883\u4e2d\u56e0\u590d\u6742\u9897\u7c92\u4ecb\u8d28\u4ea4\u4e92\u548c\u591a\u6837\u5316\u5de5\u5177\u9700\u6c42\u5bfc\u81f4\u7684\u81ea\u4e3b\u6316\u6398\u96be\u9898\u3002", "method": "\u4f7f\u7528\u5e76\u884c\u5316\u6a21\u62df\u548c\u9ad8\u4fdd\u771f\u7c92\u5b50\u7269\u7406\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u521a\u5ea6\u548c\u963b\u5c3c\u5b9e\u73b0\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5de5\u5177\u591a\u6837\u6027\u8bad\u7ec3\u548c\u89c6\u89c9\u53cd\u9988\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u592a\u7a7a\u4efb\u52a1\u6240\u9700\u7684\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2509.06491", "pdf": "https://arxiv.org/pdf/2509.06491", "abs": "https://arxiv.org/abs/2509.06491", "authors": ["Siddarth Marwaha", "Pawel Kryszkiewicz", "Eduard Jorswieck"], "title": "Optimal Distortion-Aware Multi-User Power Allocation for Massive MIMO Networks", "categories": ["eess.SP"], "comment": null, "summary": "Real-world wireless transmitter front-ends exhibit certain nonlinear\nbehavior, e.g., signal clipping by a Power Amplifier (PA). Although many\nresource allocation solutions do not consider this for simplicity, it leads to\ninaccurate results or a reduced number of degrees of freedom, not achieving the\nglobal performance. In this work, we propose an optimal PA distortion-aware\npower allocation strategy in a downlink orthogonal frequency division multiplex\n(OFDM) based massive multiple-input multiple-output (M-MIMO) system. Assuming a\nsoft-limiter PA model, where the transmission occurs under small-scale\nindependent and identically distributed (i.i.d) Rayleigh fading channel, we\nderive the wideband signal-to-noise-and-distortion ratio (SNDR) and formulate\nthe power allocation problem. Most interestingly, the distortion introduced by\nthe PA leads to an SNDR-efficient operating point without explicit transmit\npower constraints. While the optimization problem is non-convex, we decouple it\ninto a non-convex total power allocation problem and a convex power\ndistribution problem among the users (UEs). We propose an alternating\noptimization algorithm to find the optimum solution. Our simulation results\nshow significant sum-rate gains over existing distortion-neglecting solutions,\ne.g., a median 4 times increase and a median 50\\% increase for a 64-antenna and\n512-antenna base station serving 60 users, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u529f\u7387\u653e\u5927\u5668\uff08PA\uff09\u975e\u7ebf\u6027\u5931\u771f\u7684\u6700\u4f18\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65e0\u7ebf\u53d1\u5c04\u524d\u7aef\u5b58\u5728\u975e\u7ebf\u6027\u884c\u4e3a\uff08\u5982PA\u4fe1\u53f7\u524a\u6ce2\uff09\uff0c\u4f46\u8bb8\u591a\u8d44\u6e90\u5206\u914d\u65b9\u6848\u7b80\u5316\u4e86\u8fd9\u4e00\u70b9\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u51c6\u786e\u6216\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u57fa\u4e8e\u8f6f\u9650\u5236PA\u6a21\u578b\u548c\u5c0f\u5c3a\u5ea6\u745e\u5229\u8870\u843d\u4fe1\u9053\uff0c\u63a8\u5bfc\u4e86\u5bbd\u5e26\u4fe1\u566a\u5931\u771f\u6bd4\uff08SNDR\uff09\uff0c\u5e76\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u975e\u51f8\u603b\u529f\u7387\u5206\u914d\u548c\u51f8\u529f\u7387\u5206\u5e03\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5ffd\u7565\u5931\u771f\u7684\u65b9\u6848\uff0c\u5176\u548c\u901f\u7387\u663e\u8457\u63d0\u5347\uff0864\u5929\u7ebf\u57fa\u7ad9\u63d0\u53474\u500d\uff0c512\u5929\u7ebf\u57fa\u7ad9\u63d0\u534750%\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86PA\u975e\u7ebf\u6027\u5931\u771f\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6027\u80fd\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2509.05500", "pdf": "https://arxiv.org/pdf/2509.05500", "abs": "https://arxiv.org/abs/2509.05500", "authors": ["Yanda Yang", "Max Sokolich", "Fatma Ceren Kirmizitas", "Sambeeta Das", "Andreas A. Malikopoulos"], "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance", "categories": ["cs.RO", "cs.AI"], "comment": "56 pages, 19 figures including Supplementary Materials. Supplementary\n  videos available at\n  https://robotyyd.github.io/yanda-yang.github.io/vascular-parkour.html.\n  Preprint. This version has not been peer reviewed", "summary": "Autonomous microrobots in blood vessels could enable minimally invasive\ntherapies, but navigation is challenged by dense, moving obstacles. We propose\na real-time path planning framework that couples an analytic geometry global\nplanner (AGP) with two reactive local escape controllers, one based on rules\nand one based on reinforcement learning, to handle sudden moving obstacles.\nUsing real-time imaging, the system estimates the positions of the microrobot,\nobstacles, and targets and computes collision-free motions. In simulation, AGP\nyields shorter paths and faster planning than weighted A* (WA*), particle swarm\noptimization (PSO), and rapidly exploring random trees (RRT), while maintaining\nfeasibility and determinism. We extend AGP from 2D to 3D without loss of speed.\nIn both simulations and experiments, the combined global planner and local\ncontrollers reliably avoid moving obstacles and reach targets. The average\nplanning time is 40 ms per frame, compatible with 25 fps image acquisition and\nreal-time closed-loop control. These results advance autonomous microrobot\nnavigation and targeted drug delivery in vascular environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u89c4\u5212\u5668\u548c\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5728\u5bc6\u96c6\u52a8\u6001\u969c\u788d\u7269\u4e2d\u5bfc\u822a\u5fae\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u907f\u969c\u548c\u76ee\u6807\u5230\u8fbe\u3002", "motivation": "\u5728\u8840\u7ba1\u73af\u5883\u4e2d\uff0c\u5fae\u673a\u5668\u4eba\u7684\u5bfc\u822a\u9762\u4e34\u5bc6\u96c6\u52a8\u6001\u969c\u788d\u7269\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u5fae\u521b\u6cbb\u7597\u548c\u9776\u5411\u836f\u7269\u9012\u9001\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5168\u5c40\u51e0\u4f55\u89c4\u5212\u5668\uff08AGP\uff09\u548c\u4e24\u79cd\u53cd\u5e94\u5f0f\u5c40\u90e8\u63a7\u5236\u5668\uff08\u57fa\u4e8e\u89c4\u5219\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u5b9e\u65f6\u6210\u50cf\u4f30\u8ba1\u4f4d\u7f6e\u5e76\u8ba1\u7b97\u65e0\u78b0\u649e\u8def\u5f84\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0cAGP\u5728\u8def\u5f84\u957f\u5ea6\u548c\u89c4\u5212\u901f\u5ea6\u4e0a\u4f18\u4e8eWA*, PSO\u548cRRT\uff0c\u4e14\u5e73\u5747\u6bcf\u5e27\u89c4\u5212\u65f6\u95f4\u4e3a40\u6beb\u79d2\uff0c\u6ee1\u8db3\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5fae\u673a\u5668\u4eba\u5728\u8840\u7ba1\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u9776\u5411\u6cbb\u7597\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.06506", "pdf": "https://arxiv.org/pdf/2509.06506", "abs": "https://arxiv.org/abs/2509.06506", "authors": ["Ensong Liu", "Rongqing Zhang", "Xiang Cheng", "Jian Tang"], "title": "Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception", "categories": ["eess.SP"], "comment": null, "summary": "Collaborative perception enables more accurate and comprehensive scene\nunderstanding by learning how to share information between agents, with LiDAR\npoint clouds providing essential precise spatial data. Due to the substantial\ndata volume generated by LiDAR sensors, efficient point cloud transmission is\nessential for low-latency multi-agent collaboration. In this work, we propose\nan efficient, robust and applicable LiDAR point cloud transmission system via\nthe Synesthesia of Machines (SoM), termed LiDAR Point Cloud Feature\nTransmission (LPC-FT), to support collaborative perception among multiple\nagents. Specifically, we employ a density-preserving deep point cloud\ncompression method that encodes the complete point cloud into a downsampled\nefficient representation. To mitigate the effects of the wireless channel, we\ndesign a channel encoder module based on self-attention to enhance LiDAR point\ncloud features and a feature fusion module based on cross-attention to\nintegrate features from transceivers. Furthermore, we utilize the nonlinear\nactivation layer and transfer learning to improve the training of deep neural\nnetworks in the presence the digital channel noise. Experimental results\ndemonstrate that the proposed LPC-FT is more robust and effective than\ntraditional octree-based compression followed by channel coding, and\noutperforms state-of-the-art deep learning-based compression techniques and\nexisting semantic communication methods, reducing the Chamfer Distance by 30%\nand improving the PSNR by 1.9 dB on average. Owing to its superior\nreconstruction performance and robustness against channel variations, LPC-FT is\nexpected to support collaborative perception tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684LiDAR\u70b9\u4e91\u4f20\u8f93\u7cfb\u7edfLPC-FT\uff0c\u901a\u8fc7\u5bc6\u5ea6\u4fdd\u6301\u7684\u6df1\u5ea6\u538b\u7f29\u65b9\u6cd5\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u4f20\u8f93\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u611f\u77e5\u3002", "motivation": "\u89e3\u51b3LiDAR\u70b9\u4e91\u6570\u636e\u4f20\u8f93\u6548\u7387\u4f4e\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u611f\u77e5\u3002", "method": "\u91c7\u7528\u5bc6\u5ea6\u4fdd\u6301\u7684\u6df1\u5ea6\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\u7f16\u7801\u70b9\u4e91\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u4fe1\u9053\u7f16\u7801\u6a21\u5757\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5229\u7528\u975e\u7ebf\u6027\u6fc0\u6d3b\u5c42\u548c\u8fc1\u79fb\u5b66\u4e60\u4f18\u5316\u8bad\u7ec3\u3002", "result": "LPC-FT\u5728Chamfer Distance\u4e0a\u964d\u4f4e30%\uff0cPSNR\u63d0\u53471.9 dB\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u538b\u7f29\u6280\u672f\u3002", "conclusion": "LPC-FT\u5728\u91cd\u5efa\u6027\u80fd\u548c\u4fe1\u9053\u53d8\u5316\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u534f\u4f5c\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2509.05547", "pdf": "https://arxiv.org/pdf/2509.05547", "abs": "https://arxiv.org/abs/2509.05547", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning.", "AI": {"tldr": "TeleopLab\u662f\u4e00\u4e2a\u79fb\u52a8\u8bbe\u5907\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u81c2\u548c\u667a\u80fd\u624b\u673a\u754c\u9762\u4e3a\u5b66\u751f\u63d0\u4f9b\u8fdc\u7a0b\u5b9e\u9a8c\u5ba4\u4f53\u9a8c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u8fdc\u7a0b\u6559\u80b2\u4e2d\u5b9e\u9645\u8bbe\u5907\u4e92\u52a8\u7684\u9ad8\u6210\u672c\u548c\u975e\u76f4\u89c2\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684STEM\u5b66\u4e60\u5e73\u53f0\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u673a\u5668\u4eba\u81c2\u3001\u81ea\u9002\u5e94\u5939\u5177\u3001\u6444\u50cf\u5934\u3001\u667a\u80fd\u624b\u673a\u754c\u9762\u548c\u89c6\u9891\u901a\u8bdd\u8f6f\u4ef6\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u6027\u80fd\u3001\u89c2\u70b9\u3001\u53ef\u7528\u6027\u548c\u5de5\u4f5c\u91cf\u3002", "result": "\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1146.1%\uff0c\u5b66\u751f\u89c2\u70b9\u6539\u5584\uff0c\u5de5\u4f5c\u91cf\u53ef\u7ba1\u7406\uff0838.2\uff09\uff0c\u53ef\u7528\u6027\u9ad8\uff0873.8\uff09\u3002", "conclusion": "TeleopLab\u6210\u529f\u8fde\u63a5\u4e86\u5b9e\u4f53\u5b9e\u9a8c\u5ba4\u4e0e\u8fdc\u7a0b\u6559\u80b2\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8fdc\u7a0b\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06569", "pdf": "https://arxiv.org/pdf/2509.06569", "abs": "https://arxiv.org/abs/2509.06569", "authors": ["Chenyu Zhang", "Yuanhang Wu", "Xiaoxi Ma", "Wei Yi"], "title": "Integrated Detection and Tracking Based on Radar Range-Doppler Feature", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Detection and tracking are the basic tasks of radar systems. Current joint\ndetection tracking methods, which focus on dynamically adjusting detection\nthresholds from tracking results, still present challenges in fully utilizing\nthe potential of radar signals. These are mainly reflected in the limited\ncapacity of the constant false-alarm rate model to accurately represent\ninformation, the insufficient depiction of complex scenes, and the limited\ninformation acquired by the tracker. We introduce the Integrated Detection and\nTracking based on radar feature (InDT) method, which comprises a network\narchitecture for radar signal detection and a tracker that leverages detection\nassistance. The InDT detector extracts feature information from each\nRange-Doppler (RD) matrix and then returns the target position through the\nfeature enhancement module and the detection head. The InDT tracker adaptively\nupdates the measurement noise covariance of the Kalman filter based on\ndetection confidence. The similarity of target RD features is measured by\ncosine distance, which enhances the data association process by combining\nlocation and feature information. Finally, the efficacy of the proposed method\nwas validated through testing on both simulated data and publicly available\ndatasets.", "AI": {"tldr": "\u96f7\u8fbe\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u8054\u5408\u65b9\u6cd5InDT\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u8ddf\u8e2a\u5668\u4f18\u5316\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u96f7\u8fbe\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\u5b58\u5728\u4fe1\u53f7\u6f5c\u529b\u5229\u7528\u4e0d\u8db3\u3001\u590d\u6742\u573a\u666f\u523b\u753b\u4e0d\u8db3\u548c\u8ddf\u8e2a\u4fe1\u606f\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faInDT\u65b9\u6cd5\uff0c\u7ed3\u5408\u96f7\u8fbe\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u548c\u8ddf\u8e2a\u5668\u4f18\u5316\uff0c\u5229\u7528RD\u77e9\u9635\u7279\u5f81\u589e\u5f3a\u68c0\u6d4b\u4e0e\u6570\u636e\u5173\u8054\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "InDT\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05581", "pdf": "https://arxiv.org/pdf/2509.05581", "abs": "https://arxiv.org/abs/2509.05581", "authors": ["Arturo Flores Alvarez", "Fatemeh Zargarbashi", "Havel Liu", "Shiqi Wang", "Liam Edwards", "Jessica Anz", "Alex Xu", "Fan Shi", "Stelian Coros", "Dennis W. Hong"], "title": "Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "68T40", "I.2.9; I.2.6"], "comment": "8 pages, 11 figures, accepted at IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2025", "summary": "We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a\ncustom-built humanoid robot designed for entertainment applications. Unlike\ntraditional humanoids, entertainment robots present unique challenges due to\naesthetic-driven design choices. Cosmo embodies these with a disproportionately\nlarge head (16% of total mass), limited sensing, and protective shells that\nconsiderably restrict movement. To address these challenges, we apply\nAdversarial Motion Priors (AMP) to enable the robot to learn natural-looking\nmovements while maintaining physical stability. We develop tailored domain\nrandomization techniques and specialized reward structures to ensure safe\nsim-to-real, protecting valuable hardware components during deployment. Our\nexperiments demonstrate that AMP generates stable standing and walking\nbehaviors despite Cosmo's extreme mass distribution and movement constraints.\nThese results establish a promising direction for robots that balance aesthetic\nappeal with functional performance, suggesting that learning-based methods can\neffectively adapt to aesthetic-driven design constraints.", "AI": {"tldr": "\u7814\u7a76\u8005\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u6280\u672f\uff0c\u4e3a\u5a31\u4e50\u4eba\u5f62\u673a\u5668\u4ebaCosmo\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u81ea\u7136\u52a8\u4f5c\u53c8\u80fd\u786e\u4fdd\u7a33\u5b9a\u6027\u7684\u8fd0\u52a8\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u9886\u57df\u968f\u673a\u5316\u548c\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u6210\u529f\u5e94\u5bf9\u4e86Cosmo\u56e0\u8bbe\u8ba1\u7f8e\u5b66\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u5a31\u4e50\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u901a\u5e38\u6ce8\u91cd\u7f8e\u5b66\uff0c\u4f8b\u5982Cosmo\u7684\u5927\u5934\u90e8\u548c\u9632\u62a4\u5916\u58f3\uff0c\u8fd9\u4e9b\u8bbe\u8ba1\u9650\u5236\u4e86\u8fd0\u52a8\u80fd\u529b\u5e76\u589e\u52a0\u4e86\u63a7\u5236\u96be\u5ea6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b66\u4e60\u65b9\u6cd5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u529f\u80fd\u4e0e\u7f8e\u5b66\u7684\u5e73\u8861\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u6280\u672f\uff0c\u7ed3\u5408\u5b9a\u5236\u5316\u7684\u9886\u57df\u968f\u673a\u5316\u548c\u5956\u52b1\u673a\u5236\uff0c\u786e\u4fdd\u52a8\u4f5c\u81ea\u7136\u4e14\u7a33\u5b9a\uff0c\u540c\u65f6\u4fdd\u62a4\u786c\u4ef6\u5b89\u5168\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1Cosmo\u7684\u8d28\u91cf\u5206\u5e03\u6781\u7aef\u4e14\u8fd0\u52a8\u53d7\u9650\uff0cAMP\u4ecd\u80fd\u751f\u6210\u7a33\u5b9a\u7684\u7ad9\u7acb\u548c\u884c\u8d70\u884c\u4e3a\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u7f8e\u5b66\u9a71\u52a8\u7684\u8bbe\u8ba1\u7ea6\u675f\uff0c\u4e3a\u529f\u80fd\u6027\u4e0e\u7f8e\u5b66\u517c\u5177\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2509.06615", "pdf": "https://arxiv.org/pdf/2509.06615", "abs": "https://arxiv.org/abs/2509.06615", "authors": ["Wouter Jansen", "Jan Steckel"], "title": "Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations", "categories": ["eess.SP", "cs.CV"], "comment": "Accepted for publication at IEEE IUS 2025", "summary": "In environments where visual sensors falter, in-air sonar provides a reliable\nalternative for autonomous systems. While previous research has successfully\nclassified individual acoustic landmarks, this paper takes a step towards\nincreasing information capacity by introducing reflector constellations as\nencoded tags. Our primary contribution is a multi-label Convolutional Neural\nNetwork (CNN) designed to simultaneously identify multiple, closely spaced\nreflectors from a single in-air 3D sonar measurement. Our initial findings on a\nsmall dataset confirm the feasibility of this approach, validating the ability\nto decode these complex acoustic patterns. Secondly, we investigated using\nadaptive beamforming with null-steering to isolate individual reflectors for\nsingle-label classification. Finally, we discuss the experimental results and\nlimitations, offering key insights and future directions for developing\nacoustic landmark systems with significantly increased information entropy and\ntheir accurate and robust detection and classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6807\u7b7e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u7528\u4e8e\u4ece\u5355\u6b213D\u58f0\u7eb3\u6d4b\u91cf\u4e2d\u540c\u65f6\u8bc6\u522b\u591a\u4e2a\u7d27\u5bc6\u6392\u5217\u7684\u53cd\u5c04\u5668\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6ce2\u675f\u6210\u5f62\u6280\u672f\u9694\u79bb\u5355\u4e2a\u53cd\u5c04\u5668\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u89c6\u89c9\u4f20\u611f\u5668\u5931\u6548\u7684\u73af\u5883\u4e2d\uff0c\u58f0\u7eb3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4ec5\u80fd\u5206\u7c7b\u5355\u4e2a\u58f0\u5b66\u5730\u6807\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53cd\u5c04\u5668\u661f\u5ea7\u4f5c\u4e3a\u7f16\u7801\u6807\u7b7e\uff0c\u63d0\u9ad8\u4fe1\u606f\u5bb9\u91cf\u3002", "method": "\u4f7f\u7528\u591a\u6807\u7b7eCNN\u540c\u65f6\u8bc6\u522b\u591a\u4e2a\u53cd\u5c04\u5668\uff0c\u5e76\u63a2\u7d22\u81ea\u9002\u5e94\u6ce2\u675f\u6210\u5f62\u4e0e\u7a7a\u8f6c\u5411\u6280\u672f\u4ee5\u9694\u79bb\u5355\u4e2a\u53cd\u5c04\u5668\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u89e3\u7801\u590d\u6742\u7684\u58f0\u5b66\u6a21\u5f0f\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u63d0\u9ad8\u58f0\u5b66\u5730\u6807\u7cfb\u7edf\u4fe1\u606f\u71b5\u7684\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9a8c\u7ed3\u679c\u7684\u5c40\u9650\u6027\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.05599", "pdf": "https://arxiv.org/pdf/2509.05599", "abs": "https://arxiv.org/abs/2509.05599", "authors": ["Kai Zhang", "Guoyang Zhao", "Jianxing Shi", "Bonan Liu", "Weiqing Qi", "Jun Ma"], "title": "MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion", "categories": ["cs.RO"], "comment": null, "summary": "Detecting and localizing glass in 3D environments poses significant\nchallenges for visual perception systems, as the optical properties of glass\noften hinder conventional sensors from accurately distinguishing glass\nsurfaces. The lack of real-world datasets focused on glass objects further\nimpedes progress in this field. To address this issue, we introduce a new\ndataset featuring a wide range of glass configurations with precise 3D\nannotations, collected from distinct real-world scenarios. On the basis of this\ndataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D\nglass detection across diverse environments. To overcome the challenges posed\nby the ambiguous appearance and context diversity of glass, we propose an\nadaptive feature fusion module that empowers the network to effectively capture\ncontextual information in varying conditions. Additionally, to exploit the\ndistinct planar geometry of glass surfaces, we present a plane regression\npipeline, which enables seamless integration of geometric properties within our\nframework. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches in both glass segmentation and monocular glass\ndepth estimation. Our results highlight the advantages of combining geometric\nand contextual cues for transparent surface understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMonoGlass3D\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u76ee3D\u73bb\u7483\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5316\u73bb\u7483\u914d\u7f6e\u7684\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u73bb\u7483\u7684\u5149\u5b66\u7279\u6027\u4f7f\u4f20\u7edf\u4f20\u611f\u5668\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u5176\u8868\u9762\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e5f\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u6a21\u5757\u4ee5\u6355\u6349\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5e73\u9762\u56de\u5f52\u6d41\u6c34\u7ebf\u4ee5\u5229\u7528\u73bb\u7483\u7684\u5e73\u9762\u51e0\u4f55\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73bb\u7483\u5206\u5272\u548c\u5355\u76ee\u73bb\u7483\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u7ed3\u5408\u51e0\u4f55\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u5bf9\u900f\u660e\u8868\u9762\u7406\u89e3\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.06651", "pdf": "https://arxiv.org/pdf/2509.06651", "abs": "https://arxiv.org/abs/2509.06651", "authors": ["Mikael Rinkinen", "Mehdi Safarpour", "Shahriar Shahabuddin", "Olli Silven", "Lauri Koskinen"], "title": "Near-Threshold Voltage Massive MIMO Computing", "categories": ["eess.SP"], "comment": null, "summary": "Massive MIMO systems have the potential to significantly enhance spectral\nefficiency, yet their widespread integration is hindered by the high power\nconsumption of the underlying computations. This paper explores the\napplicability and effectiveness of Algorithm-Based Fault Tolerance (ABFT) for\nmassive MIMO signal processing to tackle the reliability challenge of Near\nThreshold Computing (NTC). We propose modifying matrix arithmetic Newton\niteration MIMO algorithm to seamlessly integrate ABFT to detect any\ncomputational errors by inspecting the final result. The overhead from ABFT\ndepends largely on the matrix dimensions, which in this context are dictated by\nthe number of user equipments involved in the computation. NTC is a promising\nstrategy for reducing the energy consumption in digital circuits by operating\ntransistors at extremely reduced voltages. However, NTC is highly susceptible\nto variations in Process, Voltage, and Temperature (PVT) which can lead to\nincreased error rates in computations. Traditional techniques for enabling NTC,\nsuch as dynamic voltage and frequency scaling guided by circuit level timing\nerror detection methods, introduce considerable hardware complexity and are\ndifficult to implement at high clock frequencies. In this context ABFT has\nemerged as a lightweight error detection method tailored for matrix operations\nwithout requiring any modifications on circuit-level and can be implemented\npurely in software.A MIMO accelerator was implemented on a reconfigurable\nhardware platform. Experimental results demonstrate that for sufficiently large\nproblem sizes, the proposed method achieves a 36% power saving compared to\nbaseline, with only an average of 3% computational overhead, at default clock\nfrequency. These results indicate that combining ABFT with near-threshold\noperation provides a viable path toward energy-efficient and robust massive\nMIMO processors.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u5bb9\u9519\uff08ABFT\uff09\u63d0\u5347\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7ed3\u5408\u8fd1\u9608\u503c\u8ba1\u7b97\uff08NTC\uff09\u964d\u4f4e\u529f\u8017\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u8282\u7ea636%\u7684\u529f\u8017\u4e14\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a3%\u3002", "motivation": "\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u867d\u80fd\u63d0\u5347\u9891\u8c31\u6548\u7387\uff0c\u4f46\u9ad8\u529f\u8017\u95ee\u9898\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u8fd1\u9608\u503c\u8ba1\u7b97\uff08NTC\uff09\u867d\u80fd\u964d\u4f4e\u80fd\u8017\uff0c\u4f46\u6613\u53d7\u5de5\u827a\u3001\u7535\u538b\u548c\u6e29\u5ea6\uff08PVT\uff09\u5f71\u54cd\u5bfc\u81f4\u8ba1\u7b97\u9519\u8bef\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7ABFT\u89e3\u51b3NTC\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u77e9\u9635\u8fd0\u7b97\u725b\u987f\u8fed\u4ee3MIMO\u7b97\u6cd5\uff0c\u65e0\u7f1d\u96c6\u6210ABFT\u4ee5\u68c0\u6d4b\u8ba1\u7b97\u9519\u8bef\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\uff0c\u7eaf\u8f6f\u4ef6\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u53ef\u8282\u7ea636%\u7684\u529f\u8017\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a3%\u3002", "conclusion": "\u7ed3\u5408ABFT\u4e0eNTC\u662f\u5b9e\u73b0\u9ad8\u6548\u8282\u80fd\u4e14\u9c81\u68d2\u7684\u5927\u89c4\u6a21MIMO\u5904\u7406\u5668\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.05672", "pdf": "https://arxiv.org/pdf/2509.05672", "abs": "https://arxiv.org/abs/2509.05672", "authors": ["Juho Kalliokoski", "Evan G. Center", "Steven M. LaValle", "Timo Ojala", "Basak Sakcak"], "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation", "categories": ["cs.RO"], "comment": "Immersive telepresence, shared control", "summary": "Telepresence robots enable users to interact with remote environments, but\nefficient and intuitive navigation remains a challenge. In this work, we\ndeveloped and evaluated a shared control method, in which the robot navigates\nautonomously while allowing users to affect the path generation to better suit\ntheir needs. We compared this with control switching, where users toggle\nbetween direct and automated control. We hypothesized that shared control would\nmaintain efficiency comparable to control switching while potentially reducing\nuser workload. The results of two consecutive user studies (each with final\nsample of n=20) showed that shared control does not degrade navigation\nefficiency, but did not show a significant reduction in task load compared to\ncontrol switching. Further research is needed to explore the underlying factors\nthat influence user preference and performance in these control systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u5728\u8fdc\u7a0b\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6548\u7387\u4e0e\u63a7\u5236\u5207\u6362\u76f8\u5f53\uff0c\u4f46\u672a\u80fd\u663e\u8457\u964d\u4f4e\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u8fdc\u7a0b\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6548\u7387\u4e0e\u76f4\u89c2\u6027\u4ecd\u7136\u662f\u6311\u6218\uff0c\u7814\u7a76\u8005\u65e8\u5728\u63a2\u7d22\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u662f\u5426\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u6237\u53ef\u5f71\u54cd\u8def\u5f84\u751f\u6210\uff0c\u540c\u65f6\u4e0e\u63a7\u5236\u5207\u6362\u65b9\u6cd5\uff08\u7528\u6237\u5207\u6362\u76f4\u63a5\u548c\u81ea\u52a8\u63a7\u5236\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5171\u4eab\u63a7\u5236\u672a\u964d\u4f4e\u5bfc\u822a\u6548\u7387\uff0c\u4f46\u672a\u663e\u8457\u51cf\u8f7b\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f71\u54cd\u7528\u6237\u504f\u597d\u548c\u6027\u80fd\u7684\u56e0\u7d20\u3002"}}
{"id": "2509.06662", "pdf": "https://arxiv.org/pdf/2509.06662", "abs": "https://arxiv.org/abs/2509.06662", "authors": ["Ao Huang", "Xidong Mu", "Li Guo", "Guangyu Zhu"], "title": "SE and EE Tradeoff in Active STAR-RIS Assisted Systems With Hardware Impairments", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates the problem of resource efficiency maximization in an\nactive simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) assisted communication system under practical transceiver\nhardware impairments (HWIs). We aim to obtain an optimal tradeoff between\nsystem spectral efficiency (SE) and energy efficiency (EE), by jointly\noptimizing the base station (BS) transmit beamforming and the active STAR-RIS\nbeamforming. To tackle the challenges in the fractional objective function, we\nbegin by applying the quadratic transformation method to simplify it into a\nmanageable form. An alternating optimization-based algorithm is then developed\nto iteratively update the BS and STAR-RIS beamforming coefficients. Simulation\nresults demonstrate that the proposed scheme performs better than other\nbaseline schemes in the presence of HWIs. Moreover, the variation of the\nachievable SE-EE region with different transmit power budgets is analyzed.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u786c\u4ef6\u635f\u4f24\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u548cSTAR-RIS\u7684\u6ce2\u675f\u6210\u5f62\uff0c\u5b9e\u73b0\u9891\u8c31\u6548\u7387\uff08SE\uff09\u548c\u80fd\u91cf\u6548\u7387\uff08EE\uff09\u7684\u6700\u4f18\u6743\u8861\u3002", "motivation": "\u5728\u786c\u4ef6\u635f\u4f24\uff08HWIs\uff09\u7684\u73b0\u5b9e\u6761\u4ef6\u4e0b\uff0c\u4f18\u5316STAR-RIS\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u7684\u8d44\u6e90\u6548\u7387\uff0c\u4ee5\u5b9e\u73b0\u9891\u8c31\u6548\u7387\u548c\u80fd\u91cf\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "\u91c7\u7528\u4e8c\u6b21\u53d8\u6362\u6cd5\u5904\u7406\u5206\u6570\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u5f00\u53d1\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u8fed\u4ee3\u66f4\u65b0\u57fa\u7ad9\u548cSTAR-RIS\u7684\u6ce2\u675f\u6210\u5f62\u7cfb\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6848\u5728\u786c\u4ef6\u635f\u4f24\u4e0b\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6848\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u53d1\u5c04\u529f\u7387\u9884\u7b97\u4e0bSE-EE\u533a\u57df\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u786c\u4ef6\u635f\u4f24\u6761\u4ef6\u4e0b\u80fd\u591f\u6709\u6548\u4f18\u5316\u8d44\u6e90\u6548\u7387\uff0c\u4e3aSTAR-RIS\u8f85\u52a9\u7cfb\u7edf\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2509.05701", "pdf": "https://arxiv.org/pdf/2509.05701", "abs": "https://arxiv.org/abs/2509.05701", "authors": ["Siyuan Wang", "Shuyi Zhang", "Zhen Tian", "Yuheng Yao", "Gongsen Wang", "Yu Zhao"], "title": "A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm", "categories": ["cs.RO"], "comment": null, "summary": "Robot path planning is a fundamental challenge in enhancing the environmental\nadaptability of autonomous navigation systems. This paper presents a hybrid\npath planning algorithm, A-star PRM, which incorporates dynamic weights. By\nembedding the Manhattan distance heuristic of the A-star algorithm into the\nrandom sampling process of PRM, the algorithm achieves a balanced optimization\nof path quality and computational efficiency. The approach uses a hierarchical\nsampling strategy and a dynamic connection mechanism, greatly improving\nadaptability to complex obstacle distributions. Experiments show that under a\nbaseline configuration with one thousand sampled vertices, the path length of\nA-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter\nthan that of PRM with p value less than 0.01. With high-density sampling using\nthree thousand vertices, the path length is reduced by 0.94 percent, 1036.61\nmeters compared with 1046.42 meters, while the increase in computational time\nis cut to about one tenth of the PRM increase, 71 percent compared with 785\npercent. These results confirm the comprehensive advantages of A-star PRM in\npath quality, stability, and computational efficiency. Compared with existing\nhybrid algorithms, the proposed method shows clear benefits, especially in\nnarrow channels and scenarios with dynamic obstacles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u6743\u91cd\u7684\u6df7\u5408\u8def\u5f84\u89c4\u5212\u7b97\u6cd5A-star PRM\uff0c\u901a\u8fc7\u5d4c\u5165A-Star\u7b97\u6cd5\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u542f\u53d1\u5f0f\u5230PRM\u7684\u968f\u673a\u91c7\u6837\u4e2d\uff0c\u5b9e\u73b0\u4e86\u8def\u5f84\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u63d0\u5347\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u7684\u73af\u5883\u9002\u5e94\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u969c\u788d\u5206\u5e03\u573a\u666f\u4e2d\u3002", "method": "\u7ed3\u5408A-Star\u7b97\u6cd5\u7684\u542f\u53d1\u5f0f\u4e0ePRM\u7684\u968f\u673a\u91c7\u6837\uff0c\u91c7\u7528\u5206\u5c42\u91c7\u6837\u7b56\u7565\u548c\u52a8\u6001\u8fde\u63a5\u673a\u5236\u3002", "result": "\u57281000\u4e2a\u91c7\u6837\u9876\u70b9\u4e0b\uff0c\u8def\u5f84\u957f\u5ea6\u6bd4PRM\u7f29\u77ed42.3%\uff1b\u57283000\u4e2a\u9876\u70b9\u4e0b\uff0c\u8def\u5f84\u957f\u5ea6\u51cf\u5c110.94%\uff0c\u8ba1\u7b97\u65f6\u95f4\u589e\u957f\u4ec5\u4e3aPRM\u768410%\u3002", "conclusion": "A-star PRM\u5728\u8def\u5f84\u8d28\u91cf\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u7b97\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u72ed\u7a84\u901a\u9053\u548c\u52a8\u6001\u969c\u788d\u573a\u666f\u3002"}}
{"id": "2509.06672", "pdf": "https://arxiv.org/pdf/2509.06672", "abs": "https://arxiv.org/abs/2509.06672", "authors": ["Ahmad Bazzi", "Mingjun Ying", "Ojas Kanhere", "Theodore S. Rappaport", "Marwa Chafii"], "title": "ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communications (ISAC) is emerging as a cornerstone\ntechnology for sixth generation (6G) wireless systems, unifying connectivity\nand environmental mapping through shared hardware, spectrum, and waveforms. The\nfollowing paper presents an ISAC imaging framework utilizing channel state\ninformation (CSI) per-path components, transmitter (TX) positions, and receiver\n(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz in\nthe upper mid-band. Our work shows how each resolvable multipath component can\nbe extracted from CSI estimation and cast into an equivalent three-dimensional\nreflection point by fusing its angle and delay information, which is useful and\nchallenging for multi-bounce reflections. The primary contribution of the paper\nis the two-segment reflection point optimization algorithm, which independently\nestimates the path lengths from the TX position and RX position to an\nequivalent reflection point (ERP) on the object surface, thus enabling precise\ngeometric reconstruction. Subsequently, we aggregate the ERPs derived from\nmultiple pairs of TX and RX positions, generating dense three dimensional point\nclouds representing the objects in the channel. Experimental results validate\nthat the proposed ISAC imaging framework accurately reconstructs object\nsurfaces, edges, and curved features. To the best of our knowledge, this paper\nprovides the first demonstration of multi bounce ISAC imaging using wireless\nray tracing at 6.75 GHz.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCSI\u591a\u5f84\u5206\u91cf\u7684ISAC\u6210\u50cf\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6bb5\u53cd\u5c04\u70b9\u4f18\u5316\u7b97\u6cd5\u7cbe\u786e\u91cd\u5efa\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u57286.75 GHz\u9891\u6bb5\u4e0a\u9996\u6b21\u5b9e\u73b0\u591a\u8df3ISAC\u6210\u50cf\u3002", "motivation": "6G\u65e0\u7ebf\u7cfb\u7edf\u4e2d\uff0cISAC\u6280\u672f\u901a\u8fc7\u5171\u4eab\u786c\u4ef6\u548c\u9891\u8c31\u5b9e\u73b0\u8fde\u63a5\u548c\u73af\u5883\u6620\u5c04\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u591a\u8df3\u53cd\u5c04\u573a\u666f\u4e0b\uff0c\u7cbe\u786e\u7684\u51e0\u4f55\u91cd\u6784\u662f\u4e00\u9879\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u5229\u7528\u6821\u51c6\u7684NYURay\u5c04\u7ebf\u8ffd\u8e2a\u5668\u83b7\u53d6CSI\u3001TX\u548cRX\u4f4d\u7f6e\u4fe1\u606f\uff0c\u901a\u8fc7\u63d0\u53d6\u591a\u5f84\u5206\u91cf\u5e76\u5c06\u5176\u89d2\u5ea6\u548c\u65f6\u5ef6\u4fe1\u606f\u878d\u5408\u4e3a\u7b49\u6548\u53cd\u5c04\u70b9\uff0c\u8fdb\u4e00\u6b65\u91c7\u7528\u4e24\u6bb5\u53cd\u5c04\u70b9\u4f18\u5316\u7b97\u6cd5\u72ec\u7acb\u4f30\u8ba1\u8def\u5f84\u957f\u5ea6\u4ee5\u5b9e\u73b0\u7cbe\u786e\u91cd\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u51c6\u786e\u91cd\u5efa\u7269\u4f53\u8868\u9762\u3001\u8fb9\u7f18\u548c\u66f2\u7ebf\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u57286.75 GHz\u9891\u6bb5\u5c55\u793a\u4e86\u591a\u8df3ISAC\u6210\u50cf\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.05723", "pdf": "https://arxiv.org/pdf/2509.05723", "abs": "https://arxiv.org/abs/2509.05723", "authors": ["Liansheng Wang", "Xinke Zhang", "Chenhui Li", "Dongjiao He", "Yihan Pan", "Jianjun Yi"], "title": "Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous\nsystems, yet its deployment on resource-constrained platforms remains\nchallenging due to computational and memory limitations. We propose Super-LIO,\na robust LIO system that demands both high performance and accuracy, ideal for\napplications such as aerial robots and mobile autonomous systems. At the core\nof Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that\nlimits each voxel to eight fused subvoxels, enabling strict point density\ncontrol and incremental denoising during map updates. This design enables a\nsimple yet efficient and accurate map structure, which can be easily integrated\ninto existing LIO frameworks. Additionally, Super-LIO designs a\nheuristic-guided KNN strategy (HKNN) that accelerates the correspondence search\nby leveraging spatial locality, further reducing runtime overhead. We evaluated\nthe proposed system using four publicly available datasets and several\nself-collected datasets, totaling more than 30 sequences. Extensive testing on\nboth X86 and ARM platforms confirms that Super-LIO offers superior efficiency\nand robustness, while maintaining competitive accuracy. Super-LIO processes\neach frame approximately 73% faster than SOTA, while consuming less CPU\nresources. The system is fully open-source and plug-and-play compatible with a\nwide range of LiDAR sensors and platforms. The implementation is available at:\nhttps://github.com/Liansheng-Wang/Super-LIO.git", "AI": {"tldr": "Super-LIO\u662f\u4e00\u79cd\u9ad8\u6548\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u516b\u53c9\u4f53\u7d20\u5730\u56fe\u7ed3\u6784\u548c\u542f\u53d1\u5f0fKNN\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u548c\u5185\u5b58\u9650\u5236\uff0cLiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u5b58\u5728\u6311\u6218\u3002Super-LIO\u65e8\u5728\u63d0\u4f9b\u9ad8\u6027\u80fd\u548c\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u81ea\u4e3b\u7cfb\u7edf\u3002", "method": "Super-LIO\u91c7\u7528\u79f0\u4e3aOctVox\u7684\u7d27\u51d1\u516b\u53c9\u4f53\u7d20\u5730\u56fe\u7ed3\u6784\uff0c\u9650\u5236\u6bcf\u4e2a\u4f53\u7d20\u4e3a\u516b\u4e2a\u878d\u5408\u5b50\u4f53\u7d20\uff0c\u5b9e\u73b0\u4e25\u683c\u7684\u70b9\u5bc6\u5ea6\u63a7\u5236\u548c\u589e\u91cf\u53bb\u566a\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u542f\u53d1\u5f0fKNN\u7b56\u7565\uff08HKNN\uff09\uff0c\u5229\u7528\u7a7a\u95f4\u5c40\u90e8\u6027\u52a0\u901f\u5bf9\u5e94\u641c\u7d22\u3002", "result": "\u5728X86\u548cARM\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cSuper-LIO\u6bd4\u73b0\u6709\u6280\u672f\u5feb73%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u4f4eCPU\u8d44\u6e90\u6d88\u8017\u3002\u6d4b\u8bd5\u8986\u76d6\u4e8630\u591a\u4e2a\u516c\u5f00\u548c\u81ea\u91c7\u6570\u636e\u96c6\u3002", "conclusion": "Super-LIO\u901a\u8fc7\u9ad8\u6548\u7684\u5730\u56fe\u8bbe\u8ba1\u548c\u641c\u7d22\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u7cbe\u5ea6\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u548c\u5e73\u53f0\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2509.06751", "pdf": "https://arxiv.org/pdf/2509.06751", "abs": "https://arxiv.org/abs/2509.06751", "authors": ["Weicheng Gao"], "title": "RadHARSimulator V1: Model-Based FMCW Radar Human Activity Recognition Simulator", "categories": ["eess.SP", "68T45", "I.5.4"], "comment": "17 pages, 12 figures, 5 tables", "summary": "Radar-based human activity recognition (HAR) is a pivotal research area for\napplications requiring non-invasive monitoring. However, the acquisition of\ndiverse and high-fidelity radar datasets for robust algorithm development\nremains a significant challenge. To overcome this bottleneck, a model-based\nfrequency-modulated continuous wave (FMCW) radar HAR simulator is developed.\nThe simulator integrates an anthropometrically scaled $13$-scatterer kinematic\nmodel to simulate $12$ distinct activities. The FMCW radar echo model is\nemployed, which incorporates dynamic radar cross-section (RCS), free-space or\nthrough-the-wall propagation, and a calibrated noise floor to ensure signal\nfidelity. The simulated raw data is then processed through a complete pipeline,\nincluding moving target indication (MTI), bulk Doppler compensation, and\nSavitzky-Golay denoising, culminating in the generation of high-resolution\nrange-time map (RTM) and Doppler-time maps (DTMs) via both short-time Fourier\ntransform (STFT) and Fourier synchrosqueezed transform (FSST). Finally, a novel\nneural network method is proposed to validate the effectiveness of the radar\nHAR. Numerical experiments demonstrate that the simulator successfully\ngenerates high-fidelity and distinct micro-Doppler signature, which provides a\nvaluable tool for radar HAR algorithm design and validation. The installer of\nthis simulator is released at:\n\\href{https://github.com/JoeyBGOfficial/RadHARSimulatorV1-Model-Based-FMCW-Radar-Human-Activity-Recognition-Simulator}{Github/JoeyBGOfficial/RadHARSimulatorV1}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684FMCW\u96f7\u8fbeHAR\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u4ee5\u652f\u6301\u7b97\u6cd5\u5f00\u53d1\u3002", "motivation": "\u96f7\u8fbeHAR\u9700\u8981\u591a\u6837\u5316\u548c\u9ad8\u4fdd\u771f\u7684\u6570\u636e\u96c6\uff0c\u4f46\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b13\u6563\u5c04\u4f53\u8fd0\u52a8\u5b66\u6a21\u578b\u7684\u6a21\u62df\u5668\uff0c\u6a21\u62df12\u79cd\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7STFT\u548cFSST\u751f\u6210RTM\u548cDTM\u3002", "result": "\u6a21\u62df\u5668\u6210\u529f\u751f\u6210\u4e86\u9ad8\u4fdd\u771f\u4e14\u72ec\u7279\u7684\u5fae\u591a\u666e\u52d2\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u7b97\u6cd5\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6a21\u62df\u5668\u4e3a\u96f7\u8fbeHAR\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.05777", "pdf": "https://arxiv.org/pdf/2509.05777", "abs": "https://arxiv.org/abs/2509.05777", "authors": ["Zhihao Lin", "Zhen Tian"], "title": "Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey", "categories": ["cs.RO"], "comment": "This paper provides a comprehensive review for scenario-based\n  game-theoretic methods", "summary": "Game-based interactive driving simulations have emerged as versatile\nplatforms for advancing decision-making algorithms in road transport mobility.\nWhile these environments offer safe, scalable, and engaging settings for\ntesting driving strategies, ensuring both realism and robust performance amid\ndynamic and diverse scenarios remains a significant challenge. Recently, the\nintegration of game-based techniques with advanced learning frameworks has\nenabled the development of adaptive decision-making models that effectively\nmanage the complexities inherent in varied driving conditions. These models\noutperform traditional simulation methods, especially when addressing\nscenario-specific challenges, ranging from obstacle avoidance on highways and\nprecise maneuvering during on-ramp merging to navigation in roundabouts,\nunsignalized intersections, and even the high-speed demands of autonomous\nracing. Despite numerous innovations in game-based interactive driving, a\nsystematic review comparing these approaches across different scenarios is\nstill missing. This survey provides a comprehensive evaluation of game-based\ninteractive driving methods by summarizing recent advancements and inherent\nroadway features in each scenario. Furthermore, the reviewed algorithms are\ncritically assessed based on their adaptation of the standard game model and an\nanalysis of their specific mechanisms to understand their impact on\ndecision-making performance. Finally, the survey discusses the limitations of\ncurrent approaches and outlines promising directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6e38\u620f\u7684\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u6a21\u62df\u5728\u9053\u8def\u8fd0\u8f93\u51b3\u7b56\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6700\u65b0\u8fdb\u5c55\u548c\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u901a\u8fc7\u6e38\u620f\u6280\u672f\u4e0e\u5148\u8fdb\u5b66\u4e60\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u63d0\u5347\u9a7e\u9a76\u51b3\u7b56\u6a21\u578b\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u6bd4\u8f83\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6e38\u620f\u5316\u9a7e\u9a76\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u7b97\u6cd5\u673a\u5236\u548c\u5bf9\u51b3\u7b56\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u9a7e\u9a76\u51b3\u7b56\u3002"}}
{"id": "2509.06820", "pdf": "https://arxiv.org/pdf/2509.06820", "abs": "https://arxiv.org/abs/2509.06820", "authors": ["Yu-Hsiang Huang", "Po-Heng Chou", "Wan-Jen Huang", "Walid Saad", "C. -C. Jay Kuo"], "title": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI", "categories": ["eess.SP", "cs.LG"], "comment": "6 pages, 4 figures, 2 tables, accepted by 2025 IEEE Globecom", "summary": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eff\u8272\u5b66\u4e60\uff08GL\uff09\u7684\u9884\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8eSTAR-RIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2MIMO\u5e7f\u64ad\u7cfb\u7edf\uff0c\u65e0\u9700\u663e\u5f0fCSI\u4f30\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u672a\u67656G\u7f51\u7edc\u5f3a\u8c03\u73af\u5883\u53ef\u6301\u7eed\u6027\uff0c\u901a\u8fc7\u5e7f\u64ad\u4f20\u8f93\u67b6\u6784\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u5e76\u51cf\u5c11\u5197\u4f59\u4f20\u8f93\u548c\u529f\u8017\u3002", "method": "\u91c7\u7528GL\u6846\u67b6\uff0c\u7ed3\u5408Saab\u3001RFT\u548cXGBoost\u6280\u672f\uff0c\u76f4\u63a5\u5229\u7528\u4e0a\u884c\u5bfc\u9891\u4fe1\u53f7\u9884\u6d4bSTAR-RIS\u7cfb\u6570\u548c\u53d1\u5c04\u9884\u7f16\u7801\u5668\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0cGL\u5728\u9891\u8c31\u6548\u7387\u4e0a\u63a5\u8fd1\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u8ba1\u7b97\u91cf\u51cf\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "GL\u6846\u67b6\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e7f\u64ad\u573a\u666f\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.05923", "pdf": "https://arxiv.org/pdf/2509.05923", "abs": "https://arxiv.org/abs/2509.05923", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "title": "eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the visual-inertial setup is commonly\nadopted due to complementary characteristics between sensors (e.g., scale\nperception and low drift). For optimal event-based visual-inertial fusion,\naccurate spatiotemporal (extrinsic and temporal) calibration is required. In\nthis work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator\nfor event-based visual-inertial systems, utilizing the widely used circle grid\nboard. Building upon the grid pattern recognition and tracking methods in\neKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and\nefficient initialization, where all parameters in the estimator would be\naccurately recovered. Subsequently, a continuous-time-based batch optimization\nis conducted to refine the initialized parameters toward better states. The\nresults of extensive real-world experiments show that eKalibr-Inertial can\nachieve accurate event-based visual-inertial spatiotemporal calibration. The\nimplementation of eKalibr-Inertial is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u65f6\u7a7a\u6821\u51c6\u65b9\u6cd5eKalibr-Inertial\uff0c\u5229\u7528\u5706\u7f51\u683c\u677f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6821\u51c6\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u7814\u7a76\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8fd0\u52a8\u4f30\u8ba1\u7b49\u9886\u57df\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u7cbe\u786e\u7684\u65f6\u7a7a\u6821\u51c6\u4ee5\u5b9e\u73b0\u6700\u4f18\u7684\u89c6\u89c9-\u60ef\u6027\u878d\u5408\u3002", "method": "\u57fa\u4e8eeKalibr\u7cfb\u5217\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u521d\u59cb\u5316\u548c\u8fde\u7eed\u65f6\u95f4\u6279\u91cf\u4f18\u5316\uff0c\u6821\u51c6\u4e8b\u4ef6\u76f8\u673a\u4e0e\u60ef\u6027\u4f20\u611f\u5668\u7684\u65f6\u7a7a\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ceKalibr-Inertial\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4e8b\u4ef6\u76f8\u673a-\u60ef\u6027\u4f20\u611f\u5668\u65f6\u7a7a\u6821\u51c6\u3002", "conclusion": "eKalibr-Inertial\u4e3a\u4e8b\u4ef6\u76f8\u673a-\u60ef\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6821\u51c6\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2509.05447", "pdf": "https://arxiv.org/pdf/2509.05447", "abs": "https://arxiv.org/abs/2509.05447", "authors": ["Zhongyuan Zhao", "Gunjan Verma", "Ananthram Swami", "Santiago Segarra"], "title": "Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)", "categories": ["cs.NI", "cs.DM", "cs.LG", "eess.SP", "05-08", "C.2.1; I.2.8; G.2.2"], "comment": "15 pages, 18 figures, accepted to IEEE Transactions on Wireless\n  Communications. This is the extended journal version of the conference paper\n  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, \"Distributed Link\n  Sparsification for Scalable Scheduling using Graph Neural Networks,\" IEEE\n  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )", "summary": "In wireless networks characterized by dense connectivity, the significant\nsignaling overhead generated by distributed link scheduling algorithms can\nexacerbate issues like congestion, energy consumption, and radio footprint\nexpansion. To mitigate these challenges, we propose a distributed link\nsparsification scheme employing graph neural networks (GNNs) to reduce\nscheduling overhead for delay-tolerant traffic while maintaining network\ncapacity. A GNN module is trained to adjust contention thresholds for\nindividual links based on traffic statistics and network topology, enabling\nlinks to withdraw from scheduling contention when they are unlikely to succeed.\nOur approach is facilitated by a novel offline constrained {unsupervised}\nlearning algorithm capable of balancing two competing objectives: minimizing\nscheduling overhead while ensuring that total utility meets the required level.\nIn simulated wireless multi-hop networks with up to 500 links, our link\nsparsification technique effectively alleviates network congestion and reduces\nradio footprints across four distinct distributed link scheduling protocols.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5206\u5e03\u5f0f\u94fe\u8def\u7a00\u758f\u5316\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u9ad8\u5bc6\u5ea6\u65e0\u7ebf\u7f51\u7edc\u7684\u4fe1\u4ee4\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u7f51\u7edc\u5bb9\u91cf\u3002", "motivation": "\u5728\u9ad8\u5bc6\u5ea6\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u5206\u5e03\u5f0f\u94fe\u8def\u8c03\u5ea6\u7b97\u6cd5\u7684\u4fe1\u4ee4\u5f00\u9500\u4f1a\u5bfc\u81f4\u62e5\u585e\u3001\u80fd\u8017\u589e\u52a0\u548c\u5c04\u9891\u8db3\u8ff9\u6269\u5c55\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u65e0\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bad\u7ec3GNN\u6a21\u5757\u4ee5\u6839\u636e\u6d41\u91cf\u7edf\u8ba1\u548c\u7f51\u7edc\u62d3\u6251\u8c03\u6574\u94fe\u8def\u7ade\u4e89\u9608\u503c\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8c03\u5ea6\u7ade\u4e89\u3002", "result": "\u5728\u6a21\u62df\u7684500\u94fe\u8def\u65e0\u7ebf\u591a\u8df3\u7f51\u7edc\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u62e5\u585e\u5e76\u51cf\u5c11\u4e86\u5c04\u9891\u8db3\u8ff9\uff0c\u9002\u7528\u4e8e\u56db\u79cd\u4e0d\u540c\u7684\u5206\u5e03\u5f0f\u94fe\u8def\u8c03\u5ea6\u534f\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u9ad8\u5bc6\u5ea6\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u94fe\u8def\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u5f00\u9500\u51cf\u5c11\u4e0e\u7f51\u7edc\u6027\u80fd\u7684\u5173\u7cfb\u3002"}}
{"id": "2509.06031", "pdf": "https://arxiv.org/pdf/2509.06031", "abs": "https://arxiv.org/abs/2509.06031", "authors": ["Junhui Huang", "Yuhe Gong", "Changsheng Li", "Xingguang Duan", "Luis Figueredo"], "title": "ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction", "categories": ["cs.RO"], "comment": null, "summary": "We present ZLATTE, a geometry-aware, learning-free framework for\nlanguage-driven trajectory reshaping in human-robot interaction. Unlike prior\nlearning-based methods, ZLATTE leverages Vision-Language Models to register\nobjects as geometric primitives and employs a Large Language Model to translate\nnatural language instructions into explicit geometric and kinematic\nconstraints. These constraints are integrated into a potential field\noptimization to adapt initial trajectories while preserving feasibility and\nsafety. A multi-agent strategy further enhances robustness under complex or\nconflicting commands. Simulation and real-world experiments demonstrate that\nZLATTE achieves smoother, safer, and more interpretable trajectory\nmodifications compared to state-of-the-art baselines.", "AI": {"tldr": "ZLATTE\u662f\u4e00\u4e2a\u65e0\u9700\u5b66\u4e60\u7684\u3001\u51e0\u4f55\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u91cd\u5851\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8f68\u8ff9\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u51e0\u4f55\u4e0e\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u5b89\u5168\u3001\u5e73\u6ed1\u7684\u8f68\u8ff9\u4fee\u6539\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u8f68\u8ff9\u4fee\u6539\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u5e73\u6ed1\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u51e0\u4f55\u4e0e\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u573a\u4f18\u5316\u548c\u591a\u667a\u80fd\u4f53\u7b56\u7565\u5b9e\u73b0\u8f68\u8ff9\u4fee\u6539\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cZLATTE\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u80fd\u5b9e\u73b0\u5e73\u6ed1\u3001\u5b89\u5168\u4e14\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\u4fee\u6539\u3002", "conclusion": "ZLATTE\u65e0\u9700\u5b66\u4e60\uff0c\u6846\u67b6\u7b80\u5355\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u8f68\u8ff9\u4f18\u5316\u4efb\u52a1\u3002"}}
{"id": "2509.05720", "pdf": "https://arxiv.org/pdf/2509.05720", "abs": "https://arxiv.org/abs/2509.05720", "authors": ["Jesper Brunnstr\u00f6m", "Martin Bo M\u00f8ller", "Jan \u00d8stergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Time-domain sound field estimation using kernel ridge regression", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": null, "summary": "Sound field estimation methods based on kernel ridge regression have proven\neffective, allowing for strict enforcement of physical properties, in addition\nto the inclusion of prior knowledge such as directionality of the sound field.\nThese methods have been formulated for single-frequency sound fields,\nrestricting the types of data and prior knowledge that can be used. In this\npaper, the kernel ridge regression approach is generalized to consider\ndiscrete-time sound fields. The proposed method provides time-domain sound\nfield estimates that can be computed in closed form, are guaranteed to be\nphysically realizable, and for which time-domain properties of the sound fields\ncan be exploited to improve estimation performance. Exploiting prior\ninformation on the time-domain behaviour of room impulse responses, the\nestimation performance of the proposed method is shown to be improved using a\ntime-domain data weighting, demonstrating the usefulness of the proposed\napproach. It is further shown using both simulated and real data that the\ntime-domain data weighting can be combined with a directional weighting,\nexploiting prior knowledge of both spatial and temporal properties of the room\nimpulse responses. The theoretical framework of the proposed method enables\nsolving a broader class of sound field estimation problems using kernel ridge\nregression where it would be required to consider the time-domain response\nrather than the frequency-domain response of each frequency separately.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u57fa\u4e8e\u6838\u5cad\u56de\u5f52\u7684\u58f0\u573a\u4f30\u8ba1\u65b9\u6cd5\u6269\u5c55\u5230\u79bb\u6563\u65f6\u95f4\u58f0\u573a\uff0c\u63d0\u4f9b\u4e86\u65f6\u57df\u58f0\u573a\u4f30\u8ba1\uff0c\u5e76\u5229\u7528\u65f6\u57df\u6570\u636e\u52a0\u6743\u548c\u65b9\u5411\u6027\u52a0\u6743\u6765\u63d0\u5347\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u9891\u58f0\u573a\uff0c\u9650\u5236\u4e86\u6570\u636e\u548c\u5148\u9a8c\u77e5\u8bc6\u7684\u5229\u7528\u3002\u672c\u6587\u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u65f6\u57df\u58f0\u573a\uff0c\u4ee5\u89e3\u51b3\u66f4\u5e7f\u6cdb\u7684\u58f0\u573a\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5cad\u56de\u5f52\u7684\u65f6\u57df\u58f0\u573a\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u57df\u6570\u636e\u52a0\u6743\u548c\u65b9\u5411\u6027\u52a0\u6743\u7ed3\u5408\u7269\u7406\u53ef\u5b9e\u73b0\u6027\u7ea6\u675f\uff0c\u63d0\u5347\u4f30\u8ba1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65f6\u57df\u6570\u636e\u52a0\u6743\u548c\u65b9\u5411\u6027\u52a0\u6743\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u58f0\u573a\u4f30\u8ba1\u6027\u80fd\uff0c\u5e76\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u58f0\u573a\u4f30\u8ba1\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6269\u5c55\u4e86\u6838\u5cad\u56de\u5f52\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u8003\u8651\u65f6\u57df\u54cd\u5e94\u7684\u58f0\u573a\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06048", "pdf": "https://arxiv.org/pdf/2509.06048", "abs": "https://arxiv.org/abs/2509.06048", "authors": ["Yi Dong", "Yangjun Liu", "Jinjun Duan", "Yang Li", "Zhendong Dai"], "title": "Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness", "categories": ["cs.RO"], "comment": "Yi Dong and Yangjun Liu contributed equally to the work. Accepted by\n  Robotics and Autonomous Systems.\n  https://authors.elsevier.com/c/1lgjX3HdG3supQ", "summary": "With the rapid development of the warehousing and logistics industries, the\npacking of goods has gradually attracted the attention of academia and\nindustry. The packing of footwear products is a typical representative\npaired-item packing task involving irregular shapes and deformable objects.\nAlthough studies on shoe packing have been conducted, different initial states\ndue to the irregular shapes of shoes and standard packing placement poses have\nnot been considered. This study proposes a robotic manipulation framework,\nincluding a perception module, reorientation planners, and a packing planner,\nthat can complete the packing of pairs of shoes in any initial state. First, to\nadapt to the large intraclass variations due to the state, shape, and\ndeformation of the shoe, we propose a vision module based on semantic\nkeypoints, which can also infer more information such as size, state, pose, and\nmanipulation points by combining geometric features. Subsequently, we not only\nproposed primitive-based reorientation methods for different states of a single\ndeformable shoe but also proposed a fast reorientation method for the top state\nusing box edge contact and gravity, which further improved the efficiency of\nreorientation. Finally, based on the perception module and reorientation\nmethods, we propose a task planner for shoe pair packing in any initial state\nto provide an optimal packing strategy. Real-world experiments were conducted\nto verify the robustness of the reorientation methods and the effectiveness of\nthe packing strategy for various types of shoes. In this study, we highlight\nthe potential of semantic keypoint representation methods, introduce new\nperspectives on the reorientation of 3D deformable objects and multi-object\nmanipulation, and provide a reference for paired object packing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u978b\u7c7b\u4ea7\u54c1\u8fd9\u79cd\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u53ef\u53d8\u5f62\u7269\u54c1\u7684\u6210\u5bf9\u5305\u88c5\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u89c6\u89c9\u6a21\u5757\u548c\u591a\u79cd\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5305\u88c5\u3002", "motivation": "\u968f\u7740\u4ed3\u50a8\u548c\u7269\u6d41\u884c\u4e1a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5546\u54c1\u5305\u88c5\u95ee\u9898\u53d7\u5230\u5b66\u754c\u548c\u5de5\u4e1a\u754c\u5173\u6ce8\u3002\u978b\u7c7b\u4ea7\u54c1\u56e0\u5176\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u53ef\u53d8\u5f62\u7279\u6027\u6210\u4e3a\u5178\u578b\u4ee3\u8868\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u8003\u8651\u4e0d\u540c\u521d\u59cb\u72b6\u6001\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5305\u542b\u611f\u77e5\u6a21\u5757\u3001\u91cd\u5b9a\u5411\u89c4\u5212\u548c\u5305\u88c5\u89c4\u5212\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u7684\u89c6\u89c9\u6a21\u5757\u63d0\u53d6\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u91cd\u5b9a\u5411\u65b9\u6cd5\u548c\u4efb\u52a1\u89c4\u5212\u5668\u5b8c\u6210\u5305\u88c5\u3002", "result": "\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u5305\u88c5\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u978b\u7c7b\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u8bed\u4e49\u5173\u952e\u70b9\u8868\u793a\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u91cd\u5b9a\u5411\u548c\u591a\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.06378", "pdf": "https://arxiv.org/pdf/2509.06378", "abs": "https://arxiv.org/abs/2509.06378", "authors": ["Ye Yuan", "Shuowen Zhang"], "title": "Beyond Diagonal IRS Aided OFDM: Rate Maximization under Frequency-Dependent Reflection", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "To appear in Proc. IEEE Global Communications Conference (Globecom),\n  2025", "summary": "This paper studies a broadband orthogonal frequency division multiplexing\n(OFDM) system aided by a beyond diagonal intelligent reflecting surface\n(BD-IRS), where inter-connections exist among different elements such that the\nreflection matrix can exhibit a beyond diagonal structure. Under practical\ncircuit structures, the reflection matrix of the BD-IRS is generally dependent\non the circuit parameters (e.g., capacitance matrix for all tunable capacitors)\nas well as the operating frequency, which leads to couplings among the BD-IRS\nreflection matrices over different sub-carriers and consequently new challenges\nin the BD-IRS design. Motivated by this, we first model the relationship\nbetween the BD-IRS reflection matrices over different sub-carriers and the\ntunable capacitance matrix, and then formulate the joint optimization problem\nof the tunable capacitance matrix and power allocation over OFDM sub-carriers\nto maximize the achievable rate of the OFDM system. Despite the non-convexity\nof the problem, we propose an effective algorithm for finding a high-quality\nfeasible solution via leveraging alternating optimization and successive convex\napproximation. Numerical results show the superiority of our proposed design\nover benchmark designs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8d85\u5bf9\u89d2\u667a\u80fd\u53cd\u5c04\u9762(BD-IRS)\u7684\u5bbd\u5e26\u6b63\u4ea4\u9891\u5206\u590d\u7528(OFDM)\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86BD-IRS\u53cd\u5c04\u77e9\u9635\u5728\u4e0d\u540c\u5b50\u8f7d\u6ce2\u95f4\u7684\u8026\u5408\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "BD-IRS\u7684\u53cd\u5c04\u77e9\u9635\u5728\u5b9e\u9645\u7535\u8def\u7ed3\u6784\u4e2d\u4f9d\u8d56\u4e8e\u7535\u8def\u53c2\u6570\u548c\u5de5\u4f5c\u9891\u7387\uff0c\u5bfc\u81f4\u4e0d\u540c\u5b50\u8f7d\u6ce2\u95f4\u7684\u53cd\u5c04\u77e9\u9635\u8026\u5408\uff0c\u589e\u52a0\u4e86\u8bbe\u8ba1\u96be\u5ea6\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5efa\u6a21BD-IRS\u53cd\u5c04\u77e9\u9635\u4e0e\u53ef\u8c03\u7535\u5bb9\u77e9\u9635\u7684\u5173\u7cfb\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u9010\u6b21\u51f8\u903c\u8fd1\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u53ef\u8c03\u7535\u5bb9\u77e9\u9635\u548cOFDM\u5b50\u8f7d\u6ce2\u7684\u529f\u7387\u5206\u914d\u3002", "result": "\u6570\u503c\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u63d0\u5347\u7cfb\u7edf\u53ef\u8fbe\u901f\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u51c6\u8bbe\u8ba1\u3002", "conclusion": "BD-IRS\u5728\u591a\u5b50\u8f7d\u6ce2\u8026\u5408\u73af\u5883\u4e0b\u80fd\u901a\u8fc7\u8054\u5408\u4f18\u5316\u663e\u8457\u63d0\u5347OFDM\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.06061", "pdf": "https://arxiv.org/pdf/2509.06061", "abs": "https://arxiv.org/abs/2509.06061", "authors": ["Faiza Babakano", "Ahmed Fahmin", "Bojie Shen", "Muhammad Aamir Cheema", "Isma Farah Siddiqui"], "title": "Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain", "categories": ["cs.RO", "cs.DB"], "comment": null, "summary": "Autonomous Mobile Robots (AMRs) operate on battery power, making energy\nefficiency a critical consideration, particularly in outdoor environments where\nterrain variations affect energy consumption. While prior research has\nprimarily focused on computing energy-efficient paths from a source to a\ndestination, these approaches often overlook practical scenarios where a robot\nneeds to pick up an object en route - an action that can significantly impact\nenergy consumption due to changes in payload. This paper introduces the\nObject-Pickup Minimum Energy Path Problem (OMEPP), which addresses\nenergy-efficient route planning for AMRs required to pick up an object from one\nof many possible locations and deliver it to a destination. To address OMEPP,\nwe first introduce a baseline algorithm that employs the Z star algorithm, a\nvariant of A star tailored for energy-efficient routing, to iteratively visit\neach pickup point. While this approach guarantees optimality, it suffers from\nhigh computational cost due to repeated searches at each pickup location. To\nmitigate this inefficiency, we propose a concurrent PCPD search that manages\nmultiple Z star searches simultaneously across all pickup points. Central to\nour solution is the Payload-Constrained Path Database (PCPD), an extension of\nthe Compressed Path Database (CPD) that incorporates payload constraints. We\ndemonstrate that PCPD significantly reduces branching factors during search,\nimproving overall performance. Although the concurrent PCPD search may produce\nslightly suboptimal solutions, extensive experiments on real-world datasets\nshow it achieves near-optimal performance while being one to two orders of\nmagnitude faster than the baseline algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OMEPP\u95ee\u9898\uff0c\u65e8\u5728\u89e3\u51b3AMR\u5728\u9014\u4e2d\u62fe\u53d6\u7269\u4f53\u65f6\u7684\u80fd\u91cf\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePCPD\u7684\u5e76\u53d1\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "AMR\u5728\u6237\u5916\u590d\u6742\u5730\u5f62\u4e2d\u8fd0\u884c\u65f6\uff0c\u62fe\u53d6\u7269\u4f53\u7684\u884c\u4e3a\u4f1a\u56e0\u8f7d\u91cd\u53d8\u5316\u663e\u8457\u5f71\u54cd\u80fd\u91cf\u6d88\u8017\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u8fd9\u4e00\u5b9e\u9645\u573a\u666f\u3002", "method": "\u8bba\u6587\u9996\u5148\u63d0\u51fa\u57fa\u4e8eZ star\u7b97\u6cd5\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u968f\u540e\u5f15\u5165\u4e86\u5e76\u53d1PCPD\u641c\u7d22\u7b97\u6cd5\uff0c\u5229\u7528Payload-Constrained Path Database\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e76\u53d1PCPD\u641c\u7d22\u7b97\u6cd5\u5728\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "PCPD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86AMR\u5728\u62fe\u53d6\u7269\u4f53\u65f6\u7684\u80fd\u91cf\u9ad8\u6548\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2509.06395", "pdf": "https://arxiv.org/pdf/2509.06395", "abs": "https://arxiv.org/abs/2509.06395", "authors": ["Lili Chen", "Changyang She", "Jingge Zhu", "Jamie Evans"], "title": "Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Meeting minimum data rate constraints is a significant challenge in wireless\ncommunication systems, particularly as network complexity grows. Traditional\ndeep learning approaches often address these constraints by incorporating\npenalty terms into the loss function and tuning hyperparameters empirically.\nHowever, this heuristic treatment offers no theoretical convergence guarantees\nand frequently fails to satisfy QoS requirements in practical scenarios.\nBuilding upon the structure of the WMMSE algorithm, we first extend it to a\nmulti-channel setting with QoS constraints, resulting in the enhanced WMMSE\n(eWMMSE) algorithm, which is provably convergent to a locally optimal solution\nwhen the problem is feasible. To further reduce computational complexity and\nimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of\nsupporting simultaneous multi-channel allocation per user. To overcome the\nlimitations of traditional deep learning methods, we propose a principled\nframework that integrates GNN with a Lagrangian-based primal-dual optimization\nmethod. By training the GNN within the Lagrangian framework, we ensure\nsatisfaction of QoS constraints and convergence to a stationary point.\nExtensive simulations demonstrate that JCPGNN-M matches the performance of\neWMMSE while offering significant gains in inference speed, generalization to\nlarger networks, and robustness under imperfect channel state information. This\nwork presents a scalable and theoretically grounded solution for constrained\nresource allocation in future wireless networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GNN\u548c\u62c9\u683c\u6717\u65e5\u4f18\u5316\u7684\u6846\u67b6JCPGNN-M\uff0c\u7528\u4e8e\u6ee1\u8db3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684QoS\u7ea6\u675f\uff0c\u5176\u6027\u80fd\u4e0eeWMMSE\u76f8\u5f53\u4f46\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1QoS\u7ea6\u675f\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u6269\u5c55WMMSE\u7b97\u6cd5\u5230\u591a\u901a\u9053\u8bbe\u7f6e\uff08eWMMSE\uff09\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eGNN\u548c\u62c9\u683c\u6717\u65e5\u4f18\u5316\u7684JCPGNN-M\u7b97\u6cd5\u3002", "result": "JCPGNN-M\u5728\u6027\u80fd\u4e0a\u4e0eeWMMSE\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7ea6\u675f\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.06115", "pdf": "https://arxiv.org/pdf/2509.06115", "abs": "https://arxiv.org/abs/2509.06115", "authors": ["Runjiao Bao", "Lin Zhang", "Tianwei Niu", "Haoyu Yuan", "Shoukun Wang"], "title": "Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Four-wheel independent steering (4WIS) systems provide mobile robots with a\nrich set of motion modes, such as Ackermann steering, lateral steering, and\nparallel movement, offering superior maneuverability in constrained\nenvironments. However, existing path planning methods generally assume a single\nkinematic model and thus fail to fully exploit the multi-modal capabilities of\n4WIS platforms. To address this limitation, we propose an extended Hybrid A*\nframework that operates in a four-dimensional state space incorporating both\nspatial states and motion modes. Within this framework, we design multi-modal\nReeds-Shepp curves tailored to the distinct kinematic constraints of each\nmotion mode, develop an enhanced heuristic function that accounts for\nmode-switching costs, and introduce a terminal connection strategy with\nintelligent mode selection to ensure smooth transitions between different\nsteering patterns. The proposed planner enables seamless integration of\nmultiple motion modalities within a single path, significantly improving\nflexibility and adaptability in complex environments. Results demonstrate\nsignificantly improved planning performance for 4WIS robots in complex\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684Hybrid A*\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\uff084WIS\uff09\u673a\u5668\u4eba\u7684\u591a\u6a21\u6001\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u75284WIS\u591a\u6a21\u6001\u80fd\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u8fd0\u52a8\u6a21\u5f0f\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u75284WIS\u7cfb\u7edf\u591a\u6a21\u6001\u7684\u673a\u52a8\u6027\u4f18\u52bf\u3002", "method": "\u6269\u5c55Hybrid A*\u6846\u67b6\u81f3\u56db\u7ef4\u72b6\u6001\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001Reeds-Shepp\u66f2\u7ebf\u3001\u6539\u8fdb\u542f\u53d1\u5f0f\u51fd\u6570\u4ee5\u8003\u8651\u6a21\u5f0f\u5207\u6362\u6210\u672c\uff0c\u5e76\u5f15\u5165\u667a\u80fd\u6a21\u5f0f\u9009\u62e9\u7684\u7ec8\u7aef\u8fde\u63a5\u7b56\u7565\u3002", "result": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e864WIS\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u8fd0\u52a8\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.06588", "pdf": "https://arxiv.org/pdf/2509.06588", "abs": "https://arxiv.org/abs/2509.06588", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity", "categories": ["eess.SY", "cs.DC", "cs.SY", "eess.SP", "math.OC"], "comment": "Digital Signal Processing journal", "summary": "This paper considers automatic generation control over an information-sharing\nnetwork of communicating generators as a multi-agent system. The optimization\nsolution is distributed among the agents based on information consensus\nalgorithms, while addressing the generators' ramp-rate-limits (RRL). This is\ntypically ignored in the existing linear/nonlinear optimization solutions but\nthey exist in real-time power generation scenarios. Without addressing the RRL,\nthe generators cannot follow the assigned rate of generating power by the\noptimization algorithm; therefore, the existing solutions may not necessarily\nconverge to the exact optimal cost or may lose feasibility in practice. The\nproposed solution in this work addresses the ramp-rate-limit constraint along\nwith the box constraint (limits on the generated powers) and the\ncoupling-constraint (generation-demand balance) at all iteration times of the\nalgorithm. The latter is referred to as the anytime feasibility and implies\nthat at every termination point of the algorithm, the balance between the\ndemand and generated power holds. To improve the convergence rate of the\nalgorithm we further consider internal signum-based nonlinearity. We also show\nthat our solution can tolerate communication link removal. This follows from\nthe uniform-connectivity assumption on the communication network.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u4fe1\u606f\u5171\u4eab\u7f51\u7edc\u7684\u53d1\u7535\u673a\u7ec4\u81ea\u52a8\u751f\u6210\u63a7\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ebf\u6027/\u975e\u7ebf\u6027\u4f18\u5316\u65b9\u6848\u4e2d\u5ffd\u7565\u7684\u722c\u5761\u7387\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6848\u5e38\u5ffd\u7565\u53d1\u7535\u673a\u7ec4\u7684\u722c\u5761\u7387\u9650\u5236\uff0c\u5bfc\u81f4\u5b9e\u9645\u4e2d\u65e0\u6cd5\u5b8c\u5168\u9075\u5faa\u4f18\u5316\u7b97\u6cd5\u7684\u529f\u7387\u5206\u914d\uff0c\u5f71\u54cd\u6536\u655b\u6027\u548c\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u5f0f\u4fe1\u606f\u5171\u8bc6\u7b97\u6cd5\uff0c\u7ed3\u5408\u722c\u5761\u7387\u9650\u5236\u3001\u529f\u7387\u9650\u5236\u548c\u4f9b\u9700\u5e73\u8861\u7ea6\u675f\uff0c\u786e\u4fdd\u7b97\u6cd5\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u90fd\u6ee1\u8db3\u53ef\u884c\u6027\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u5728\u4fdd\u8bc1\u5b9e\u65f6\u4f9b\u9700\u5e73\u8861\u7684\u540c\u65f6\uff0c\u652f\u6301\u901a\u4fe1\u94fe\u8def\u5931\u6548\u7684\u5bb9\u5fcd\u6027\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5185\u90e8\u7b26\u53f7\u975e\u7ebf\u6027\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u722c\u5761\u7387\u9650\u5236\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4f18\u5316\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6536\u655b\u6027\u3002"}}
{"id": "2509.06119", "pdf": "https://arxiv.org/pdf/2509.06119", "abs": "https://arxiv.org/abs/2509.06119", "authors": ["Shiqi Xu", "Lihao Zhang", "Yuyang Du", "Qun Yang", "Soung Chang Liew"], "title": "A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Recent progress in robotics has underscored the demand for real-time control\nin applications such as manufacturing, healthcare, and autonomous systems,\nwhere the timely delivery of mission-critical commands under heterogeneous\nrobotic traffic is paramount for operational efficacy and safety. In these\nscenarios, mission-critical traffic follows a strict deadline-constrained\ncommunication pattern: commands must arrive within defined QoS deadlines,\notherwise late arrivals can degrade performance or destabilize control loops.In\nthis work, we demonstrate on a real-time SDR platform that CSMA, widely adopted\nin robotic communications,suffers severe degradation under high robot traffic\nloads, with contention-induced collisions and delays disrupting the on-time\narrival of mission-critical packets. To address this problem, we propose an\nIEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's\ndeterministic slot scheduling with CSMA's adaptability for heterogeneous robot\ntraffic.The protocol achieves collision-free, low-latency mission-critical\ncommand delivery and IEEE 802.11 compatibility through the synergistic\nintegration of sub-microsecond PTP-based slot synchronization-essential for\nestablishing precise timing for TDMA, a three-session superframe with dynamic\nTDMA allocation for structured and adaptable traffic management,and beacon-NAV\nprotection to preemptively secure these critical communication sessions from\ninterference. Emulation experiments on real-time SDR testbed and Robot\nOperating System (ROS) simulation show that the proposed protocol reduces\nmissed-deadline errors by 93% compared to the CSMA baseline. In high-speed\nrobot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)\ntrajectory error by up to 90% compared with a CSMA baseline, all while\nmaintaining throughput for non-critical traffic within +-2%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408TDMA/CSMA\u534f\u8bae\uff0c\u89e3\u51b3\u4e86\u5728\u9ad8\u8d1f\u8f7d\u673a\u5668\u4eba\u901a\u4fe1\u4e2dCSMA\u5bfc\u81f4\u7684\u5ef6\u8fdf\u548c\u78b0\u649e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5173\u952e\u6d41\u91cf\u7684\u5b9e\u65f6\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u4eba\u5e94\u7528\uff08\u5982\u5236\u9020\u3001\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\uff09\u5bf9\u5b9e\u65f6\u63a7\u5236\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0cCSMA\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4efb\u52a1\u5173\u952e\u547d\u4ee4\u65e0\u6cd5\u6309\u65f6\u5230\u8fbe\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u517c\u5bb9IEEE 802.11\u7684\u6df7\u5408TDMA/CSMA\u534f\u8bae\uff0c\u7ed3\u5408\u4e86TDMA\u7684\u786e\u5b9a\u6027\u65f6\u9699\u8c03\u5ea6\u548cCSMA\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u65f6\u95f4\u540c\u6b65\u3001\u52a8\u6001TDMA\u5206\u914d\u548c\u4fe1\u6807-NAV\u4fdd\u62a4\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u65e0\u78b0\u649e\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4CSMA\u57fa\u7ebf\uff0c\u8be5\u534f\u8bae\u5c06\u4efb\u52a1\u5173\u952e\u6d41\u91cf\u7684\u9519\u8fc7\u622a\u6b62\u671f\u9519\u8bef\u51cf\u5c11\u4e8693%\uff0c\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\u4e8690%\uff0c\u540c\u65f6\u975e\u5173\u952e\u6d41\u91cf\u7684\u541e\u5410\u91cf\u53d8\u5316\u4fdd\u6301\u5728\u00b12%\u4ee5\u5185\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u534f\u8bae\u5728\u9ad8\u8d1f\u8f7d\u673a\u5668\u4eba\u901a\u4fe1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5173\u952e\u6d41\u91cf\u7684\u5b9e\u65f6\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u5173\u952e\u6d41\u91cf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.06598", "pdf": "https://arxiv.org/pdf/2509.06598", "abs": "https://arxiv.org/abs/2509.06598", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.04845", "summary": "In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u4fe1\u606f\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u7acb\u4f53\u58f0\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\u4efb\u52a1\uff083D SELD\uff09\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e8c\u540d\u3002", "motivation": "\u4f20\u7edf\u7684SELD\u65b9\u6cd5\u4f9d\u8d56\u591a\u901a\u9053\u8f93\u5165\uff0c\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u4fe1\u606f\uff08\u97f3\u9891\u548c\u89c6\u89c9\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684CLAP\uff08\u97f3\u9891\uff09\u548cOWL-ViT\uff08\u89c6\u89c9\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u6539\u8fdb\u7684Conformer\u6a21\u5757\uff08Cross-Modal Conformer\uff09\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u96c6\u6210\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5728DCASE2025 Task3 Stereo SELD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7b2c\u4e8c\u540d\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D SELD\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u6a21\u5757\u8d21\u732e\u548c\u67b6\u6784\u4f18\u5316\u3002"}}
{"id": "2509.06191", "pdf": "https://arxiv.org/pdf/2509.06191", "abs": "https://arxiv.org/abs/2509.06191", "authors": ["Yifei Ren", "Edward Johns"], "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project webpage with robot videos:\n  https://www.robot-learning.uk/op-gen", "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.", "AI": {"tldr": "3D\u751f\u6210\u6a21\u578b\u901a\u8fc7\u5355\u6b21\u771f\u5b9e\u6f14\u793a\u7684\u6570\u636e\u96c6\u589e\u5f3a\uff0c\u5b9e\u73b0\u591a\u65b9\u5411\u7b56\u7565\u5b66\u4e60\uff0c\u51cf\u5c11\u6f14\u793a\u9700\u6c42\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d223D\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u96c6\u51cf\u5c11\u771f\u5b9e\u6f14\u793a\u7684\u9700\u6c42\u3002", "method": "\u5229\u75283D\u751f\u6210\u6a21\u578b\u4ece\u5355\u6b21\u771f\u5b9e\u6f14\u793a\u4e2d\u751f\u6210\u60f3\u8c61\u6570\u636e\u96c6\uff0c\u5e76\u5b66\u4e60\u591a\u65b9\u5411\u7b56\u7565\u3002", "result": "\u673a\u5668\u4eba\u80fd\u5728\u8fdc\u79bb\u6f14\u793a\u521d\u59cb\u72b6\u6001\u4e0b\u5b8c\u6210\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u7b56\u7565\u5b66\u4e60\u6240\u9700\u7684\u6f14\u793a\u6b21\u6570\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "3D\u751f\u6210\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.06599", "pdf": "https://arxiv.org/pdf/2509.06599", "abs": "https://arxiv.org/abs/2509.06599", "authors": ["Sri Satish Krishna Chaitanya Bulusu", "Mikko Sillanp\u00e4\u00e4"], "title": "Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems", "categories": ["cs.LG", "cs.CC", "cs.SY", "eess.SP", "eess.SY", "math.ST", "stat.TH"], "comment": "15 pages, 1 figure, 2 photographs", "summary": "Dynamic nonlinear systems exhibit distortions arising from coupled static and\ndynamic effects. Their intertwined nature poses major challenges for\ndata-driven modeling. This paper presents a theoretical framework grounded in\nstructured decomposition, variance analysis, and task-centric complexity\nbounds.\n  The framework employs a directional lower bound on interactions between\nmeasurable system components, extending orthogonality in inner product spaces\nto structurally asymmetric settings. This bound supports variance inequalities\nfor decomposed systems. Key behavioral indicators are introduced along with a\nmemory finiteness index. A rigorous power-based condition establishes a\nmeasurable link between finite memory in realizable systems and the First Law\nof Thermodynamics. This offers a more foundational perspective than classical\nbounds based on the Second Law.\n  Building on this foundation, we formulate a `Behavioral Uncertainty\nPrinciple,' demonstrating that static and dynamic distortions cannot be\nminimized simultaneously. We identify that real-world systems seem to resist\ncomplete deterministic decomposition due to entangled static and dynamic\neffects. We also present two general-purpose theorems linking function variance\nto mean-squared Lipschitz continuity and learning complexity. This yields a\nmodel-agnostic, task-aware complexity metric, showing that lower-variance\ncomponents are inherently easier to learn.\n  These insights explain the empirical benefits of structured residual\nlearning, including improved generalization, reduced parameter count, and lower\ntraining cost, as previously observed in power amplifier linearization\nexperiments. The framework is broadly applicable and offers a scalable,\ntheoretically grounded approach to modeling complex dynamic nonlinear systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u5206\u89e3\u3001\u65b9\u5dee\u5206\u6790\u548c\u4efb\u52a1\u4e2d\u5fc3\u590d\u6742\u5ea6\u754c\u9650\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u9759\u52a8\u6001\u8026\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u539f\u5219\u3002", "motivation": "\u52a8\u6001\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u9759\u52a8\u6001\u6548\u5e94\u7684\u8026\u5408\u5e26\u6765\u5931\u771f\uff0c\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u5206\u89e3\u3001\u65b9\u5dee\u5206\u6790\u3001\u4efb\u52a1\u4e2d\u5fc3\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u5f15\u5165\u65b9\u5411\u6027\u4e0b\u754c\u548c\u5185\u5b58\u6709\u9650\u6027\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u539f\u5219\uff0c\u9a8c\u8bc1\u4e86\u9759\u52a8\u6001\u5931\u771f\u65e0\u6cd5\u540c\u65f6\u6700\u5c0f\u5316\uff0c\u5e76\u5efa\u7acb\u4e86\u51fd\u6570\u65b9\u5dee\u4e0e\u5b66\u4e60\u590d\u6742\u5ea6\u7684\u8054\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u52a8\u6001\u975e\u7ebf\u6027\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u7ed3\u6784\u5316\u6b8b\u5dee\u5b66\u4e60\u7684\u7ecf\u9a8c\u4f18\u52bf\u3002"}}
{"id": "2509.06201", "pdf": "https://arxiv.org/pdf/2509.06201", "abs": "https://arxiv.org/abs/2509.06201", "authors": ["Jun Yamada", "Adithyavairavan Murali", "Ajay Mandlekar", "Clemens Eppner", "Ingmar Posner", "Balakumar Sundaralingam"], "title": "Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "14 pages, 17 figures", "summary": "Grasping of diverse objects in unstructured environments remains a\nsignificant challenge. Open-loop grasping methods, effective in controlled\nsettings, struggle in cluttered environments. Grasp prediction errors and\nobject pose changes during grasping are the main causes of failure. In\ncontrast, closed-loop methods address these challenges in simplified settings\n(e.g., single object on a table) on a limited set of objects, with no path to\ngeneralization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping\npolicy designed for robust and reactive grasping of novel objects in cluttered\nenvironments. Grasp-MPC incorporates a value function, trained on visual\nobservations from a large-scale synthetic dataset of 2 million grasp\ntrajectories that include successful and failed attempts. We deploy this\nlearned value function in an MPC framework in combination with other cost terms\nthat encourage collision avoidance and smooth execution. We evaluate Grasp-MPC\non FetchBench and real-world settings across diverse environments. Grasp-MPC\nimproves grasp success rates by up to 32.6% in simulation and 33.3% in\nreal-world noisy conditions, outperforming open-loop, diffusion policy,\ntransformer policy, and IQL approaches. Videos and more at\nhttp://grasp-mpc.github.io.", "AI": {"tldr": "Grasp-MPC\u662f\u4e00\u79cd\u95ed\u73af6\u81ea\u7531\u5ea6\u89c6\u89c9\u6293\u53d6\u7b56\u7565\uff0c\u65e8\u5728\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bf9\u65b0\u7269\u4f53\u8fdb\u884c\u7a33\u5065\u548c\u53cd\u5e94\u8fc5\u901f\u7684\u6293\u53d6\uff0c\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u591a\u6837\u5316\u7269\u4f53\u6293\u53d6\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f00\u73af\u65b9\u6cd5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u95ed\u73af\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u5e76\u5728MPC\u6846\u67b6\u4e2d\u90e8\u7f72\uff0c\u878d\u5408\u907f\u78b0\u548c\u5e73\u6ed1\u6267\u884c\u7684\u989d\u5916\u6210\u672c\u9879\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5608\u6742\u6761\u4ef6\u4e0b\uff0c\u6293\u53d6\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e8632.6%\u548c33.3%\u3002", "conclusion": "Grasp-MPC\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5bf9\u65b0\u7269\u4f53\u7684\u6293\u53d6\u8868\u73b0\u51fa\u663e\u8457\u7684\u7a33\u5065\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.06898", "pdf": "https://arxiv.org/pdf/2509.06898", "abs": "https://arxiv.org/abs/2509.06898", "authors": ["Zhihui Gao", "Zhecun Liu", "Tingjun Chen"], "title": "BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation", "categories": ["cs.NI", "eess.SP"], "comment": "14 pages, 17 figures", "summary": "The coexistence between incumbent radar signals and commercial 5G signals\nnecessitates a versatile and ubiquitous radar sensing for efficient and\nadaptive spectrum sharing. In this context, leveraging the densely deployed 5G\nbase stations (BS) for radar sensing is particularly promising, offering both\nwide coverage and immediate feedback to 5G scheduling. However, the targeting\nradar signals are superimposed with concurrent 5G uplink transmissions received\nby the BS, and practical deployment also demands a lightweight, portable radar\nsensing model. This paper presents BatStation, a lightweight, in-situ radar\nsensing framework seamlessly integrated into 5G BSs. BatStation leverages\nuplink resource grids to extract radar signals through three key components:\n(i) radar signal separation to cancel concurrent 5G transmissions and reveal\nthe radar signals, (ii) resource grid reshaping to align time-frequency\nresolution with radar pulse characteristics, and (iii) zero-shot template\ncorrelation based on a portable model trained purely on synthetic data that\nsupports detection, classification, and localization of radar pulses without\nfine-tuning using experimental data. We implement BatStation on a\nsoftware-defined radio (SDR) testbed and evaluate its performance with real 5G\ntraffic in the CBRS band. Results show robust performance across diverse radar\ntypes, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),\nclassification accuracy up to 97.00%, and median localization errors of\n2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,\nBatStation achieves this performance with a runtime latency of only 0.11/0.94\nms on GPU/CPU, meeting the real-time requirement of 5G networks.", "AI": {"tldr": "BatStation\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u96f7\u8fbe\u611f\u77e5\u6846\u67b6\uff0c\u96c6\u6210\u4e8e5G\u57fa\u7ad9\u4e2d\uff0c\u5229\u7528\u4e0a\u884c\u8d44\u6e90\u7f51\u683c\u63d0\u53d6\u96f7\u8fbe\u4fe1\u53f7\uff0c\u652f\u6301\u5b9e\u65f6\u96f7\u8fbe\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe\u4fe1\u53f7\u4e0e5G\u4fe1\u53f7\u5171\u5b58\u9700\u8981\u9ad8\u6548\u7684\u9891\u8c31\u5171\u4eab\uff0c\u5229\u7528\u5bc6\u96c6\u90e8\u7f72\u76845G\u57fa\u7ad9\u8fdb\u884c\u96f7\u8fbe\u611f\u77e5\u53ef\u5b9e\u73b0\u5e7f\u57df\u8986\u76d6\u548c\u5b9e\u65f6\u53cd\u9988\u3002", "method": "\u901a\u8fc7\u96f7\u8fbe\u4fe1\u53f7\u5206\u79bb\u3001\u8d44\u6e90\u7f51\u683c\u91cd\u5851\u548c\u96f6\u6837\u672c\u6a21\u677f\u5173\u8054\u4e09\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\u96f7\u8fbe\u4fe1\u53f7\u63d0\u53d6\u548c\u611f\u77e5\u3002", "result": "\u5728\u771f\u5b9e5G\u6d41\u91cf\u4e0b\uff0cBatStation\u8868\u73b0\u51fa\u8272\uff0c\u68c0\u6d4b\u6982\u7387\u8fbe97.02%\uff0c\u5206\u7c7b\u51c6\u786e\u738797%\uff0c\u8fd0\u884c\u65f6\u5ef6\u4ec5\u4e3a0.11/0.94 ms\u3002", "conclusion": "BatStation\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u96f7\u8fbe\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e5G\u7f51\u7edc\u7684\u5b9e\u65f6\u9700\u6c42\u3002"}}
{"id": "2509.06233", "pdf": "https://arxiv.org/pdf/2509.06233", "abs": "https://arxiv.org/abs/2509.06233", "authors": ["Tongxuan Tian", "Xuhui Kang", "Yen-Ling Kuo"], "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Conference on Robot Learning (CoRL) 2025. Project website:\n  https://o3afford.github.io/", "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5355\u6837\u672c\u5b66\u4e60\u76843D\u7269\u4f53\u95f4\u4ea4\u4e92\u80fd\u529b\uff08affordance\uff09\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u70b9\u4e91\u6570\u636e\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5bf9\u65b0\u9896\u7269\u4f53\u548c\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7ed3\u5408\u589e\u5f3a\u4e86\u4efb\u52a1\u7ea6\u675f\u529f\u80fd\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\u7684\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u4e2d\u591a\u6570\u4ea4\u4e92\u6d89\u53ca\u7269\u4f53\u5bf9\u7684\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6570\u636e\u6709\u9650\u6761\u4ef6\u4e0b\u7269\u4f53\u95f4\u4ea4\u4e92\u80fd\u529b\u7684\u6620\u5c04\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u4e0e\u70b9\u4e91\u6570\u636e\u7684\u51e0\u4f55\u7406\u89e3\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5355\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u8fdb\u4e00\u6b65\u6574\u54083D\u4ea4\u4e92\u80fd\u529b\u8868\u793a\u4e0eLLMs\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\uff08O$^3$Afford\uff09\u57283D\u7269\u4f53\u95f4\u4ea4\u4e92\u80fd\u529b\u6620\u5c04\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u4ea4\u4e92\u7684\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u4e3aLLMs\u7684\u4efb\u52a1\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.06285", "pdf": "https://arxiv.org/pdf/2509.06285", "abs": "https://arxiv.org/abs/2509.06285", "authors": ["Xiangcheng Hu", "Xieyuanli Chen", "Mingkai Jia", "Jin Wu", "Ping Tan", "Steven L. Waslander"], "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration", "categories": ["cs.RO"], "comment": "24 pages, 19 figures, 9 tables", "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.", "AI": {"tldr": "DCReg \u662f\u4e00\u4e2a\u89e3\u51b3 LiDAR \u70b9\u4e91\u914d\u51c6\u4e2d\u51e0\u4f55\u9000\u5316\u6216\u72ed\u7a84\u73af\u5883\u4e0b\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u3001\u5206\u6790\u548c\u7f13\u89e3\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5728\u51e0\u4f55\u9000\u5316\u6216\u72ed\u7a84\u73af\u5883\u4e2d\uff0cLiDAR \u70b9\u4e91\u914d\u51c6\u95ee\u9898\u53d8\u5f97\u75c5\u6001\uff0c\u5bfc\u81f4\u89e3\u4e0d\u7a33\u5b9a\u548c\u7cbe\u5ea6\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DCReg \u901a\u8fc7 Schur \u8865\u5206\u89e3\u89e3\u8026\u65cb\u8f6c\u548c\u5e73\u79fb\u5b50\u7a7a\u95f4\uff0c\u5b9a\u91cf\u8868\u5f81\u9000\u5316\u65b9\u5411\uff0c\u5e76\u8bbe\u8ba1\u9009\u62e9\u6027\u7a33\u5b9a\u7b56\u7565\uff0c\u4f7f\u7528\u9884\u6761\u4ef6\u5171\u8f6d\u68af\u5ea6\u6cd5\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCReg \u5728\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0a\u63d0\u5347\u4e86 20%-50%\uff0c\u8fd0\u7b97\u901f\u5ea6\u63d0\u9ad8\u4e86 5-100 \u500d\u3002", "conclusion": "DCReg \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u75c5\u6001\u73af\u5883\u4e0b\u7684\u70b9\u4e91\u914d\u51c6\u6027\u80fd\u3002"}}
{"id": "2509.06296", "pdf": "https://arxiv.org/pdf/2509.06296", "abs": "https://arxiv.org/abs/2509.06296", "authors": ["Francisco Affonso", "Felipe Andrade G. Tommaselli", "Juliano Negri", "Vivian S. Medeiros", "Mateus V. Gasparino", "Girish Chowdhary", "Marcelo Becker"], "title": "Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": "Under review at IEEE Robotics and Automation Letters. 8 pages", "summary": "Traditional RL-based locomotion controllers often suffer from low data\nefficiency, requiring extensive interaction to achieve robust performance. We\npresent a model-based reinforcement learning (MBRL) framework that improves\nsample efficiency for quadrupedal locomotion by appending synthetic data to the\nend of standard rollouts in PPO-based controllers, following the Dyna-Style\nparadigm. A predictive model, trained alongside the policy, generates\nshort-horizon synthetic transitions that are gradually integrated using a\nscheduling strategy based on the policy update iterations. Through an ablation\nstudy, we identified a strong correlation between sample efficiency and rollout\nlength, which guided the design of our experiments. We validated our approach\nin simulation on the Unitree Go1 robot and showed that replacing part of the\nsimulated steps with synthetic ones not only mimics extended rollouts but also\nimproves policy return and reduces variance. Finally, we demonstrate that this\nimprovement transfers to the ability to track a wide range of locomotion\ncommands using fewer simulated steps.", "AI": {"tldr": "\u4f20\u7edfRL\u63a7\u5236\u5668\u6570\u636e\u6548\u7387\u4f4e\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff08MBRL\uff09\u6846\u67b6\uff0c\u7ed3\u5408Dyna-Style\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u7b56\u7565\u66f4\u65b0\u8fed\u4ee3\u9010\u6b65\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u5408\u6210\u6570\u636e\u4e0d\u4ec5\u80fd\u6a21\u62df\u66f4\u957f\u7684rollout\uff0c\u8fd8\u80fd\u63d0\u5347\u7b56\u7565\u56de\u62a5\u5e76\u964d\u4f4e\u65b9\u5dee\uff0c\u540c\u65f6\u51cf\u5c11\u5b9e\u9645\u6a21\u62df\u6b65\u9aa4\u7684\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u5e76\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u547d\u4ee4\u7684\u8ddf\u8e2a\u3002"}}
{"id": "2509.06342", "pdf": "https://arxiv.org/pdf/2509.06342", "abs": "https://arxiv.org/abs/2509.06342", "authors": ["Filip Bjelonic", "Fabian Tischhauser", "Marco Hutter"], "title": "Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots", "categories": ["cs.RO"], "comment": "Submitted to The International Journal of Robotics Research (IJRR),\n  25 Figures, 7 Tables, Open Source Data available at ETH Research Collection.\n  Open Source software available soon", "summary": "Legged robots must achieve both robust locomotion and energy efficiency to be\npractical in real-world environments. Yet controllers trained in simulation\noften fail to transfer reliably, and most existing approaches neglect\nactuator-specific energy losses or depend on complex, hand-tuned reward\nformulations. We propose a framework that integrates sim-to-real reinforcement\nlearning with a physics-grounded energy model for permanent magnet synchronous\nmotors. The framework requires a minimal parameter set to capture the\nsimulation-to-reality gap and employs a compact four-term reward with a\nfirst-principle-based energetic loss formulation that balances electrical and\nmechanical dissipation. We evaluate and validate the approach through a\nbottom-up dynamic parameter identification study, spanning actuators,\nfull-robot in-air trajectories and on-ground locomotion. The framework is\ntested on three primary platforms and deployed on ten additional robots,\ndemonstrating reliable policy transfer without randomization of dynamic\nparameters. Our method improves energetic efficiency over state-of-the-art\nmethods, achieving a 32 percent reduction in the full Cost of Transport of\nANYmal (value 1.27). All code, models, and datasets will be released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u4e0e\u7269\u7406\u57fa\u7840\u80fd\u91cf\u6a21\u578b\u7684\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u7684\u80fd\u6548\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u817f\u5f0f\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u80fd\u6548\uff0c\u89e3\u51b3\u4eff\u771f\u63a7\u5236\u5668\u5728\u73b0\u5b9e\u4e2d\u5931\u6548\u7684\u95ee\u9898\uff0c\u5e76\u7b80\u5316\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\u3002", "method": "\u7ed3\u5408\u4eff\u771f\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u4e0e\u57fa\u4e8e\u6c38\u78c1\u540c\u6b65\u7535\u673a\u7684\u7269\u7406\u80fd\u91cf\u6a21\u578b\uff0c\u91c7\u7528\u7b80\u5316\u7684\u56db\u9879\u5956\u52b1\u51fd\u6570\u548c\u80fd\u91cf\u635f\u5931\u516c\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86ANYmal\u6574\u4f53\u8fd0\u8f93\u6210\u672c\u964d\u4f4e32%\uff08\u503c\u4e3a1.27\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u7684\u80fd\u6548\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u590d\u6742\u7684\u53c2\u6570\u968f\u673a\u5316\u3002"}}
{"id": "2509.06375", "pdf": "https://arxiv.org/pdf/2509.06375", "abs": "https://arxiv.org/abs/2509.06375", "authors": ["Fujiang Yuan", "Zhen Tian", "Yangfan He", "Guojian Zou", "Chunhong Yuan", "Yanhong Peng", "Zhihao Lin"], "title": "Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "In recent years, ensuring safety, efficiency, and comfort in interactive\nautonomous driving has become a critical challenge. Traditional model-based\ntechniques, such as game-theoretic methods and robust control, are often overly\nconservative or computationally intensive. Conversely, learning-based\napproaches typically require extensive training data and frequently exhibit\nlimited interpretability and generalizability. Simpler strategies, such as Risk\nPotential Fields (RPF), provide lightweight alternatives with minimal data\ndemands but are inherently static and struggle to adapt effectively to dynamic\ntraffic conditions. To overcome these limitations, we propose the Evolutionary\nRisk Potential Field (ERPF), a novel approach that dynamically updates risk\nassessments in dynamical scenarios based on historical obstacle proximity data.\nWe introduce a Risk-Ellipse construct that combines longitudinal reach and\nlateral uncertainty into a unified spatial temporal collision envelope.\nAdditionally, we define an adaptive Evolution Factor metric, computed through\nsigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard\n(TWH), which dynamically adjusts the dimensions of the ellipse axes in real\ntime. This adaptive risk metric is integrated seamlessly into a Model\nPredictive Control (MPC) framework, enabling autonomous vehicles to proactively\naddress complex interactive driving scenarios in terms of uncertain driving of\nsurrounding vehicles. Comprehensive comparative experiments demonstrate that\nour ERPF-MPC approach consistently achieves smoother trajectories, higher\naverage speeds, and collision-free navigation, offering a robust and adaptive\nsolution suitable for complex interactive driving environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aERPF\u7684\u52a8\u6001\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408Risk-Ellipse\u548c\u81ea\u9002\u5e94Evolution Factor\uff0c\u901a\u8fc7MPC\u6846\u67b6\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7684\u5e73\u6ed1\u3001\u9ad8\u6548\u548c\u65e0\u78b0\u649e\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u535a\u5f08\u8bba\u6216\u9c81\u68d2\u63a7\u5236\uff09\u4fdd\u5b88\u6216\u8ba1\u7b97\u91cf\u5927\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u89e3\u91ca\u6027\u5dee\u3002RPF\u65b9\u6cd5\u8f7b\u91cf\u4f46\u9759\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u4ea4\u901a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86ERPF\u3002", "method": "\u63d0\u51faEvolutionary Risk Potential Field (ERPF)\uff0c\u7ed3\u5408Risk-Ellipse\u548c\u81ea\u9002\u5e94Evolution Factor\uff08\u57fa\u4e8eTTC\u548cTWH\uff09\uff0c\u5e76\u4e0eMPC\u6846\u67b6\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cERPF-MPC\u5728\u590d\u6742\u4ea4\u4e92\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u3001\u901f\u5ea6\u66f4\u9ad8\u4e14\u65e0\u78b0\u649e\u3002", "conclusion": "ERPF-MPC\u4e3a\u590d\u6742\u4ea4\u4e92\u9a7e\u9a76\u73af\u5883\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06404", "pdf": "https://arxiv.org/pdf/2509.06404", "abs": "https://arxiv.org/abs/2509.06404", "authors": ["Kaikai Wang", "Tianxun Li", "Liang Xu", "Qinglei Hu", "Keyou You"], "title": "Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining", "categories": ["cs.RO"], "comment": "12 pages, 9 figures, accepted to RA-L", "summary": "While Model Predictive Control (MPC) enforces safety via constraints, its\nreal-time execution can exceed embedded compute budgets. We propose a\nBarrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework\nthat synergizes neural networks' fast computation with MPC's\nconstraint-handling capability. To ensure strict safety, we replace traditional\nEuclidean distance with Control Barrier Functions (CBFs) for collision\navoidance. We integrate an offline-learned neural value function into the\noptimization objective of a Short-horizon MPC, substantially reducing online\ncomputational complexity. Additionally, we use a second neural network to learn\nthe sensitivity of the value function to system parameters, and adaptively\nadjust the neural value function based on this neural sensitivity when model\nparameters change, eliminating the need for retraining and reducing offline\ncomputation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano\nshow that BAN-MPC solves 200 times faster than traditional MPC, enabling\ncollision-free navigation with control error below 5\\% under model parameter\nvariations within 15\\%, making it an effective embedded MPC alternative.", "AI": {"tldr": "BAN-MPC\u6846\u67b6\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u8ba1\u7b97\u4e0eMPC\u7684\u7ea6\u675f\u5904\u7406\u80fd\u529b\uff0c\u901a\u8fc7CBF\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u6bd4\u4f20\u7edfMPC\u5feb200\u500d\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfMPC\u5728\u5b9e\u65f6\u6267\u884c\u65f6\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u4e25\u683c\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faBAN-MPC\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0eMPC\uff0c\u4f7f\u7528CBF\u66ff\u4ee3\u6b27\u6c0f\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u4ef7\u503c\u51fd\u6570\u51cf\u5c11\u5728\u7ebf\u8ba1\u7b97\u3002", "result": "\u5728Jetson Nano\u4e0a\uff0cBAN-MPC\u6bd4\u4f20\u7edfMPC\u5feb200\u500d\uff0c\u63a7\u5236\u8bef\u5dee\u4f4e\u4e8e5%\uff0c\u9002\u5e94\u53c2\u6570\u53d8\u5316\u3002", "conclusion": "BAN-MPC\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5d4c\u5165\u5f0fMPC\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u5408\u5b9e\u65f6\u5b89\u5168\u63a7\u5236\u3002"}}
{"id": "2509.06433", "pdf": "https://arxiv.org/pdf/2509.06433", "abs": "https://arxiv.org/abs/2509.06433", "authors": ["Ian Page", "Pierre Susbielle", "Olivier Aycard", "Pierre-Brice Wieber"], "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation", "categories": ["cs.RO"], "comment": null, "summary": "Achieving efficient remote teleoperation is particularly challenging in\nunknown environments, as the teleoperator must rapidly build an understanding\nof the site's layout. Online 3D mapping is a proven strategy to tackle this\nchallenge, as it enables the teleoperator to progressively explore the site\nfrom multiple perspectives. However, traditional online map-based teleoperation\nsystems struggle to generate visually accurate 3D maps in real-time due to the\nhigh computational cost involved, leading to poor teleoperation performances.\nIn this work, we propose a solution to improve teleoperation efficiency in\nunknown environments. Our approach proposes a novel, modular and efficient\nGPU-based integration between recent advancement in gaussian splatting SLAM and\nexisting online map-based teleoperation systems. We compare the proposed\nsolution against state-of-the-art teleoperation systems and validate its\nperformances through real-world experiments using an aerial vehicle. The\nresults show significant improvements in decision-making speed and more\naccurate interaction with the environment, leading to greater teleoperation\nefficiency. In doing so, our system enhances remote teleoperation by seamlessly\nintegrating photorealistic mapping generation with real-time performances,\nenabling effective teleoperation in unfamiliar environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5206\u5272SLAM\u7684\u9ad8\u6548GPU\u96c6\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8fdc\u7a0b\u9065\u63a7\u6548\u7387\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u9ad8\u6548\u8fdc\u7a0b\u9065\u63a7\u9700\u8981\u5feb\u901f\u7406\u89e3\u73af\u5883\u5e03\u5c40\uff0c\u4f20\u7edf\u5728\u7ebf\u5730\u56fe\u65b9\u6cd5\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u9ad8\u65af\u5206\u5272SLAM\u4e0e\u73b0\u6709\u5728\u7ebf\u5730\u56fe\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316GPU\u96c6\u6210\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u903c\u771f\u76843D\u5730\u56fe\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u51b3\u7b56\u901f\u5ea6\u548c\u73af\u5883\u4ea4\u4e92\u51c6\u786e\u6027\uff0c\u63d0\u5347\u4e86\u9065\u63a7\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u5b9e\u65f6\u903c\u771f\u5730\u56fe\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9065\u63a7\u6027\u80fd\u3002"}}
{"id": "2509.06469", "pdf": "https://arxiv.org/pdf/2509.06469", "abs": "https://arxiv.org/abs/2509.06469", "authors": ["Benedikt Kreis", "Malte Mosbach", "Anny Ripke", "Muhammad Ehsan Ullah", "Sven Behnke", "Maren Bennewitz"], "title": "Interactive Shaping of Granular Media Using Reinforcement Learning", "categories": ["cs.RO"], "comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025", "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, outperforming two baseline\napproaches.", "AI": {"tldr": "\u6458\u8981\u63cf\u8ff0\u4e86\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u673a\u5668\u4eba\u624b\u81c2\u64cd\u7eb5\u9897\u7c92\u4ecb\u8d28\uff08\u5982\u6c99\u5b50\uff09\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u548c\u590d\u6742\u52a8\u6001\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9897\u7c92\u4ecb\u8d28\u7684\u81ea\u4e3b\u64cd\u7eb5\u5728\u5efa\u7b51\u3001\u6316\u6398\u548c\u589e\u6750\u5236\u9020\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u548c\u590d\u6742\u52a8\u6001\u4f7f\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u624b\u81c2\uff08\u914d\u5907\u7acb\u65b9\u4f53\u672b\u7aef\u6267\u884c\u5668\u548c\u7acb\u4f53\u76f8\u673a\uff09\u5b66\u4e60\u64cd\u7eb5\u9897\u7c92\u4ecb\u8d28\u7684\u76ee\u6807\u7ed3\u6784\uff0c\u5f3a\u8c03\u7d27\u51d1\u89c2\u6d4b\u548c\u7b80\u6d01\u5956\u52b1\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u89c6\u89c9\u7b56\u7565\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e3a\u9897\u7c92\u4ecb\u8d28\u7684\u64cd\u7eb5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u7b56\u7565\u8bad\u7ec3\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.06481", "pdf": "https://arxiv.org/pdf/2509.06481", "abs": "https://arxiv.org/abs/2509.06481", "authors": ["Vinita Sao", "Tu Dac Ho", "Sujoy Bhore", "P. B. Sujit"], "title": "Event Driven CBBA with Reduced Communication", "categories": ["cs.RO"], "comment": null, "summary": "In various scenarios such as multi-drone surveillance and search-and-rescue\noperations, deploying multiple robots is essential to accomplish multiple tasks\nat once. Due to the limited communication range of these vehicles, a\ndecentralised task allocation algorithm is crucial for effective task\ndistribution among robots. The consensus-based bundle algorithm (CBBA) has been\npromising for multi-robot operation, offering theoretical guarantees. However,\nCBBA demands continuous communication, leading to potential congestion and\npacket loss that can hinder performance. In this study, we introduce an\nevent-driven communication mechanism designed to address these communication\nchallenges while maintaining the convergence and performance bounds of CBBA. We\ndemonstrate theoretically that the solution quality matches that of CBBA and\nvalidate the approach with Monte-Carlo simulations across varying targets,\nagents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can\nreduce message transmissions by up to 52%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u673a\u5236\uff08ED-CBBA\uff09\uff0c\u4ee5\u51cf\u5c11\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\u7684\u901a\u4fe1\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301CBBA\u7684\u6027\u80fd\u548c\u6536\u655b\u6027\u3002", "motivation": "\u5728\u591a\u65e0\u4eba\u673a\u76d1\u89c6\u548c\u641c\u6551\u7b49\u573a\u666f\u4e2d\uff0c\u9ad8\u6548\u7684\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46CBBA\u7684\u8fde\u7eed\u901a\u4fe1\u9700\u6c42\u53ef\u80fd\u5bfc\u81f4\u7f51\u7edc\u62e5\u585e\u548c\u6570\u636e\u5305\u4e22\u5931\u3002", "method": "\u5f15\u5165\u4e86\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u673a\u5236\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u901a\u4fe1\uff0c\u540c\u65f6\u786e\u4fdd\u4efb\u52a1\u5206\u914d\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u3002", "result": "\u7406\u8bba\u8bc1\u660eED-CBBA\u4e0eCBBA\u5177\u6709\u76f8\u540c\u7684\u89e3\u8d28\u91cf\uff0c\u5b9e\u9a8c\u663e\u793a\u6d88\u606f\u4f20\u8f93\u51cf\u5c11\u4e8652%\u3002", "conclusion": "ED-CBBA\u5728\u51cf\u5c11\u901a\u4fe1\u8d1f\u62c5\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86CBBA\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2509.06582", "pdf": "https://arxiv.org/pdf/2509.06582", "abs": "https://arxiv.org/abs/2509.06582", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gr\u00f6ne", "Mathias G\u00fcnther", "Oliver Deussen"], "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7528\u6237VR\u5171\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u8fd0\u52a8\u6355\u6349\u4e0eSLAM\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u5e27\u7387\u7684\u540c\u6b65\u865a\u62df\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8ddf\u8e2a\uff08\u5bfc\u81f4\u5ef6\u8fdf\uff09\u6216\u4e00\u6b21\u6027\u6807\u5b9a\uff08\u65e0\u6cd5\u7ea0\u6b63\u6f02\u79fb\uff09\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408HMD SLAM\u8ddf\u8e2a\u4e0e\u5916\u90e8\u6e90\u5b9e\u65f6\u5bf9\u9f50\uff0c\u652f\u6301\u8bbe\u5907\u95f4\u5b9e\u65f6\u59ff\u6001\u5171\u4eab\u3002", "result": "\u6846\u67b6\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u3001\u8212\u9002\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u5171\u4f4dVR\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u7528\u6237\u81ea\u7136\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06593", "pdf": "https://arxiv.org/pdf/2509.06593", "abs": "https://arxiv.org/abs/2509.06593", "authors": ["Meher V. R. Malladi", "Tiziano Guadagnino", "Luca Lobefaro", "Cyrill Stachniss"], "title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling", "categories": ["cs.RO"], "comment": null, "summary": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\u7684\u9c81\u68d2\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u7b80\u5316\u4e86IMU\u96c6\u6210\u5e76\u7ed3\u5408\u626b\u63cf\u5230\u5730\u56fe\u7684LiDAR\u6ce8\u518c\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u91cc\u7a0b\u8ba1\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7cbe\u786e\u7684\u91cc\u7a0b\u8ba1\u662f\u5173\u952e\u7ec4\u4ef6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8e\u4e0d\u540c\u4f20\u611f\u5668\u548c\u573a\u666f\u7684\u9c81\u68d2\u91cc\u7a0b\u8ba1\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7b80\u5316\u7684IMU\u96c6\u6210\u8fd0\u52a8\u6a21\u578b\uff0c\u76f4\u63a5\u901a\u8fc7\u626b\u63cf\u5230\u5730\u56fe\u7684\u65b9\u5f0f\u6ce8\u518cLiDAR\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u65b0\u7684LiDAR\u6ce8\u518c\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u4ee5\u76f8\u540c\u914d\u7f6e\u9002\u5e94\u4e0d\u540c\u4f20\u611f\u5668\u548c\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u91cc\u7a0b\u8ba1\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u65e0\u9700\u4f20\u611f\u5668\u7279\u5b9a\u5efa\u6a21\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2509.06597", "pdf": "https://arxiv.org/pdf/2509.06597", "abs": "https://arxiv.org/abs/2509.06597", "authors": ["Frederik Plahl", "Georgios Katranis", "Ilshat Mamaev", "Andrey Morozov"], "title": "LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods", "categories": ["cs.RO"], "comment": "Preprint of final paper that will appear in the Proceedings of the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\n  2025)", "summary": "We present LiHRA, a novel dataset designed to facilitate the development of\nautomated, learning-based, or classical risk monitoring (RM) methods for\nHuman-Robot Interaction (HRI) scenarios. The growing prevalence of\ncollaborative robots in industrial environments has increased the need for\nreliable safety systems. However, the lack of high-quality datasets that\ncapture realistic human-robot interactions, including potentially dangerous\nevents, slows development. LiHRA addresses this challenge by providing a\ncomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body\nkeypoints, and robot joint states, capturing the complete spatial and dynamic\ncontext of human-robot collaboration. This combination of modalities allows for\nprecise tracking of human movement, robot actions, and environmental\nconditions, enabling accurate RM during collaborative tasks. The LiHRA dataset\ncovers six representative HRI scenarios involving collaborative and coexistent\ntasks, object handovers, and surface polishing, with safe and hazardous\nversions of each scenario. In total, the data set includes 4,431 labeled point\nclouds recorded at 10 Hz, providing a rich resource for training and\nbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrate\nLiHRA's utility, we introduce an RM method that quantifies the risk level in\neach scenario over time. This method leverages contextual information,\nincluding robot states and the dynamic model of the robot. With its combination\nof high-resolution LiDAR data, precise human tracking, robot state data, and\nrealistic collision events, LiHRA offers an essential foundation for future\nresearch into real-time RM and adaptive safety strategies in human-robot\nworkspaces.", "AI": {"tldr": "LiHRA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u573a\u666f\u4e2d\u81ea\u52a8\u5316\u3001\u5b66\u4e60\u578b\u6216\u7ecf\u5178\u98ce\u9669\u76d1\u6d4b\uff08RM\uff09\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u7a7a\u767d\u3002", "motivation": "\u968f\u7740\u534f\u4f5c\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u5bf9\u53ef\u9760\u5b89\u5168\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u6355\u6349\u771f\u5b9e\u4eba\u673a\u4e92\u52a8\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u963b\u788d\u4e86\u53d1\u5c55\u3002", "method": "LiHRA\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u54083D LiDAR\u70b9\u4e91\u3001\u4eba\u4f53\u5173\u952e\u70b9\u548c\u673a\u5668\u4eba\u5173\u8282\u72b6\u6001\uff0c\u8bb0\u5f55\u4e86\u534f\u4f5c\u4efb\u52a1\u7684\u5b8c\u6574\u7a7a\u95f4\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b4,431\u4e2a\u6807\u8bb0\u70b9\u4e91\uff0c\u8986\u76d6\u516d\u79cdHRI\u573a\u666f\uff0c\u652f\u6301\u8bad\u7ec3\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684RM\u65b9\u6cd5\u3002", "conclusion": "LiHRA\u4e3a\u5b9e\u65f6\u98ce\u9669\u76d1\u6d4b\u548c\u81ea\u9002\u5e94\u5b89\u5168\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4eba\u673a\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u7814\u7a76\u3002"}}
{"id": "2509.06644", "pdf": "https://arxiv.org/pdf/2509.06644", "abs": "https://arxiv.org/abs/2509.06644", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating\nthe state-of-the-art performance in the agricultural domain. Code:\nhttps://github.com/AlexTraveling/T-araVLN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-araVLN\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6a21\u5757\u63d0\u5347\u519c\u4e1a\u673a\u5668\u4eba\u5bf9\u590d\u6742\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u519c\u4e1a\u673a\u5668\u4eba\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\u6216\u56fa\u5b9a\u8f68\u9053\u79fb\u52a8\uff0cAgriVLN\u65b9\u6cd5\u867d\u80fd\u5904\u7406\u7b80\u5355\u6307\u4ee4\uff0c\u4f46\u5bf9\u590d\u6742\u6307\u4ee4\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faT-araVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u7ffb\u8bd1\u6a21\u5757\u5c06\u539f\u59cb\u6307\u4ee4\u4f18\u5316\u4e3a\u66f4\u7cbe\u786e\u7684\u7248\u672c\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSR\u4ece0.47\u63d0\u5347\u52300.63\uff0cNE\u4ece2.91m\u964d\u81f32.28m\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "T-araVLN\u5728\u519c\u4e1a\u673a\u5668\u4eba\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2509.06682", "pdf": "https://arxiv.org/pdf/2509.06682", "abs": "https://arxiv.org/abs/2509.06682", "authors": ["Sajad Ahmadi", "Mohammadreza Davoodi", "Javad Mohammadpour Velni"], "title": "An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents an adaptive coverage control method for a fleet of\noff-road and Unmanned Ground Vehicles (UGVs) operating in dynamic\n(time-varying) agricultural environments. Traditional coverage control\napproaches often assume static conditions, making them unsuitable for\nreal-world farming scenarios where obstacles, such as moving machinery and\nuneven terrains, create continuous challenges. To address this, we propose a\nreal-time path planning framework that integrates Unmanned Aerial Vehicles\n(UAVs) for obstacle detection and terrain assessment, allowing UGVs to\ndynamically adjust their coverage paths. The environment is modeled as a\nweighted directed graph, where the edge weights are continuously updated based\non the UAV observations to reflect obstacle motion and terrain variations. The\nproposed approach incorporates Voronoi-based partitioning, adaptive edge weight\nassignment, and cost-based path optimization to enhance navigation efficiency.\nSimulation results demonstrate the effectiveness of the proposed method in\nimproving path planning, reducing traversal costs, and maintaining robust\ncoverage in the presence of dynamic obstacles and muddy terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u519c\u4e1a\u73af\u5883\u7684\u81ea\u9002\u5e94\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u5b9e\u65f6\u68c0\u6d4b\u969c\u788d\u7269\u548c\u5730\u5f62\u53d8\u5316\uff0c\u6307\u5bfc\u5730\u9762\u8f66\u8f86\u52a8\u6001\u8c03\u6574\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u73af\u5883\uff0c\u65e0\u6cd5\u5e94\u5bf9\u519c\u4e1a\u573a\u666f\u4e2d\u79fb\u52a8\u969c\u788d\u7269\u548c\u590d\u6742\u5730\u5f62\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u65e0\u4eba\u673a\u89c2\u6d4b\u5b9e\u65f6\u66f4\u65b0\u52a0\u6743\u6709\u5411\u56fe\u6a21\u578b\uff0c\u91c7\u7528Voronoi\u5206\u533a\u3001\u81ea\u9002\u5e94\u8fb9\u6743\u5206\u914d\u548c\u57fa\u4e8e\u6210\u672c\u7684\u8def\u5f84\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u964d\u4f4e\u79fb\u52a8\u6210\u672c\uff0c\u5e76\u5728\u52a8\u6001\u969c\u788d\u548c\u6ce5\u6cde\u5730\u5f62\u4e2d\u4fdd\u6301\u8986\u76d6\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u8f66\u8f86\u8986\u76d6\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06687", "pdf": "https://arxiv.org/pdf/2509.06687", "abs": "https://arxiv.org/abs/2509.06687", "authors": ["Sajad Ahmadi", "Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Deploying self-navigating surface vessels in inland waterways offers a\nsustainable alternative to reduce road traffic congestion and emissions.\nHowever, navigating confined waterways presents unique challenges, including\nnarrow channels, higher traffic density, and hydrodynamic disturbances.\nExisting methods for autonomous vessel navigation often lack the robustness or\nprecision required for such environments. This paper presents a new motion\nplanning approach for Automated Surface Vessels (ASVs) using Robust Model\nPredictive Control (RMPC) combined with Control Barrier Functions (CBFs). By\nincorporating channel borders and obstacles as safety constraints within the\ncontrol design framework, the proposed method ensures both collision avoidance\nand robust navigation on complex waterways. Simulation results demonstrate the\nefficacy of the proposed method in safely guiding ASVs under realistic\nconditions, highlighting its improved safety and adaptability compared to the\nstate-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9c81\u68d2\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RMPC\uff09\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u65b0\u578b\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u6c34\u9762\u8239\u8236\uff08ASVs\uff09\u5728\u5185\u9646\u6c34\u9053\u7684\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u5185\u9646\u6c34\u9053\u4e2d\u7684\u81ea\u52a8\u5bfc\u822a\u53ef\u4ee5\u7f13\u89e3\u4ea4\u901a\u62e5\u5835\u548c\u51cf\u5c11\u6392\u653e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u72ed\u7a84\u6c34\u9053\u548c\u9ad8\u5bc6\u5ea6\u4ea4\u901a\u73af\u5883\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u91c7\u7528RMPC\u548cCBFs\u7684\u7ed3\u5408\u65b9\u6cd5\uff0c\u5c06\u6c34\u9053\u8fb9\u754c\u548c\u969c\u788d\u7269\u4f5c\u4e3a\u5b89\u5168\u7ea6\u675f\u7eb3\u5165\u63a7\u5236\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6c34\u9053\u73af\u5883\u4e0b\u80fd\u6709\u6548\u907f\u514d\u78b0\u649e\u5e76\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\uff0c\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u6c34\u9762\u8239\u8236\u7684\u5b89\u5168\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u5185\u9646\u6c34\u9053\u73af\u5883\u3002"}}
{"id": "2509.06768", "pdf": "https://arxiv.org/pdf/2509.06768", "abs": "https://arxiv.org/abs/2509.06768", "authors": ["Oluwadamilola Sotomi", "Devika Kodi", "Kiruthiga Chandra Shekar", "Aliasghar Arab"], "title": "Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous robots operating in dynamic environments should identify and\nreport anomalies. Embodying proactive mitigation improves safety and\noperational continuity. This paper presents a multimodal anomaly detection and\nmitigation system that integrates vision-language models and large language\nmodels to identify and report hazardous situations and conflicts in real-time.\nThe proposed system enables robots to perceive, interpret, report, and if\npossible respond to urban and environmental anomalies through proactive\ndetection mechanisms and automated mitigation actions. A key contribution in\nthis paper is the integration of Hazardous and Conflict states into the robot's\ndecision-making framework, where each anomaly type can trigger specific\nmitigation strategies. User studies (n = 30) demonstrated the effectiveness of\nthe system in anomaly detection with 91.2% prediction accuracy and relatively\nlow latency response times using edge-ai architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u65f6\u8bc6\u522b\u548c\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u60c5\u51b5\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8bc6\u522b\u5e76\u62a5\u544a\u5f02\u5e38\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u8fde\u7eed\u6027\u3002", "method": "\u7cfb\u7edf\u878d\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5371\u9669\u548c\u51b2\u7a81\u72b6\u6001\u7eb3\u5165\u673a\u5668\u4eba\u51b3\u7b56\u6846\u67b6\uff0c\u89e6\u53d1\u7279\u5b9a\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08n=30\uff09\u663e\u793a\uff0c\u7cfb\u7edf\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe91.2%\uff0c\u54cd\u5e94\u5ef6\u8fdf\u4f4e\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u548c\u81ea\u52a8\u7f13\u89e3\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2509.06819", "pdf": "https://arxiv.org/pdf/2509.06819", "abs": "https://arxiv.org/abs/2509.06819", "authors": ["Daniel San Jos\u00e9 Pro", "Oliver Hausd\u00f6rfer", "Ralf R\u00f6mer", "Maximilian D\u00f6sch", "Martin Schuck", "Angela P. Sch\u00f6llig"], "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation", "categories": ["cs.RO"], "comment": "5 pages, 5 figures", "summary": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.", "AI": {"tldr": "CRISP \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684 C++ \u5b9e\u73b0\uff0c\u652f\u6301 ROS2 \u63a7\u5236\u6807\u51c6\uff0c\u7528\u4e8e\u5b9e\u73b0\u5e73\u6ed1\u7684 Cartesian \u548c\u5173\u8282\u7a7a\u95f4\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5b66\u4e60\u578b\u63a7\u5236\u5668\u548c\u8fdc\u7a0b\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5b66\u4e60\u578b\u63a7\u5236\u5668\u751f\u6210\u7684\u4f4e\u9891\u6216\u4e0d\u8fde\u7eed\u72b6\u6001\u53d8\u5316\u95ee\u9898\uff0c\u9700\u8981\u4f4e\u7ea7\u63a7\u5236\u5668\u5b9e\u73b0\u5e73\u6ed1\u53c2\u8003\u8ddf\u8e2a\u548c\u63a5\u89e6\u4ea4\u4e92\u7684\u517c\u5bb9\u6027\u3002", "method": "\u5f00\u53d1\u4e86 CRISP\uff0c\u517c\u5bb9 ROS2 \u63a7\u5236\u6807\u51c6\uff0c\u652f\u6301 Python \u548c Gymnasium \u63a5\u53e3\uff0c\u7edf\u4e00\u4e86\u6570\u636e\u8bb0\u5f55\u548c\u7b56\u7565\u90e8\u7f72\u6d41\u7a0b\u3002", "result": "\u5728 Franka Robotics FR3 \u786c\u4ef6\u548c Kuka IIWA14 \u4e0e Kinova Gen3 \u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u96c6\u6210\u548c\u9ad8\u6027\u80fd\u3002", "conclusion": "CRISP \u63d0\u4f9b\u4e86\u5b66\u4e60\u578b\u65b9\u6cd5\u5728 ROS2 \u517c\u5bb9\u673a\u68b0\u81c2\u4e0a\u7684\u5feb\u901f\u5e94\u7528\uff0c\u964d\u4f4e\u4e86\u5b9e\u9a8c\u95e8\u69db\u3002"}}
{"id": "2509.06882", "pdf": "https://arxiv.org/pdf/2509.06882", "abs": "https://arxiv.org/abs/2509.06882", "authors": ["Zhiheng Chen", "Wei Wang"], "title": "Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles", "categories": ["cs.RO"], "comment": "This work has been accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u6700\u4f18\u63a7\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5fae\u578b\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\uff08MicroASV\uff09\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u5fae\u578b\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\u5728\u72ed\u7a84\u6216\u6d45\u6c34\u533a\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u975e\u7ebf\u6027\u6c34\u52a8\u529b\u5efa\u6a21\u56f0\u96be\u548c\u73af\u5883\u5e72\u6270\u654f\u611f\u6027\u95ee\u9898\uff0c\u5176\u7cbe\u786e\u63a7\u5236\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u9a71\u52a8\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u5f31\u516c\u5f0f\u5316\u7684\u5728\u7ebf\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5b9e\u65f6\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u9762\u5bf9\u672a\u77e5\u8d1f\u8f7d\u548c\u5916\u90e8\u5e72\u6270\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u5728\u7ebf\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u4e3aMicroASV\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u53ef\u9760\u548c\u7cbe\u786e\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002"}}
{"id": "2509.06932", "pdf": "https://arxiv.org/pdf/2509.06932", "abs": "https://arxiv.org/abs/2509.06932", "authors": ["Yuqing Wen", "Hebei Li", "Kefan Gu", "Yucheng Zhao", "Tiancai Wang", "Xiaoyan Sun"], "title": "LLaDA-VLA: Vision Language Diffusion Action Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.", "AI": {"tldr": "LLaDA-VLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9996\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u6269\u6563-\u52a8\u4f5c\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u5229\u7528\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u548c\u591a\u6a21\u6001\u5e94\u7528\u4e2d\u7684\u7ade\u4e89\u6027\u80fd\uff0c\u63a2\u7d22\u5176\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e86\u5c40\u90e8\u7279\u6b8a\u4ee4\u724c\u5206\u7c7b\u7b56\u7565\u548c\u5206\u5c42\u52a8\u4f5c\u7ed3\u6784\u5316\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u9002\u914d\u673a\u5668\u4eba\u9886\u57df\u3002", "result": "LLaDA-VLA\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2509.06951", "pdf": "https://arxiv.org/pdf/2509.06951", "abs": "https://arxiv.org/abs/2509.06951", "authors": ["Qi Lv", "Weijie Kong", "Hao Li", "Jia Zeng", "Zherui Qiu", "Delin Qu", "Haoming Song", "Qizhi Chen", "Xiang Deng", "Jiangmiao Pang"], "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.", "AI": {"tldr": "F1\u662f\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u524d\u77bb\u751f\u6210\u5230\u51b3\u7b56\u6d41\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u8bed\u8a00\u6761\u4ef6\u4efb\u52a1\u7684\u6267\u884c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u591a\u4e3a\u53cd\u5e94\u5f0f\u72b6\u6001\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u5bfc\u81f4\u77ed\u89c6\u884c\u4e3a\u548c\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u5dee\uff0cF1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "F1\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5305\u542b\u611f\u77e5\u3001\u524d\u77bb\u751f\u6210\u548c\u63a7\u5236\u6a21\u5757\uff0c\u901a\u8fc7\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u524d\u77bb\u9884\u6d4b\u673a\u5236\u5b9e\u73b0\u89c4\u5212\u3002", "result": "F1\u5728\u73b0\u5b9e\u4efb\u52a1\u548c\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "F1\u901a\u8fc7\u89c6\u89c9\u524d\u77bb\u9884\u6d4b\u548c\u6a21\u5757\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.06953", "pdf": "https://arxiv.org/pdf/2509.06953", "abs": "https://arxiv.org/abs/2509.06953", "authors": ["Jiahui Yang", "Jason Jingzhou Liu", "Yulong Li", "Youssef Khaky", "Kenneth Shaw", "Deepak Pathak"], "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Website at \\url{deep-reactive-policy.com}", "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com", "AI": {"tldr": "DRP\u662f\u4e00\u79cd\u89c6\u89c9\u8fd0\u52a8\u795e\u7ecf\u8fd0\u52a8\u7b56\u7565\uff0c\u4e13\u4e3a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u53cd\u5e94\u6027\u8fd0\u52a8\u800c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u751f\u6210\u65e0\u78b0\u649e\u8fd0\u52a8\u7684\u96be\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u73b0\u6709\u795e\u7ecf\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDRP\uff0c\u57fa\u4e8e\u70b9\u4e91\u8f93\u5165\uff0c\u6838\u5fc3\u662f\u9884\u8bad\u7ec3\u7684IMPACT\u6a21\u578b\uff0c\u7ed3\u5408\u8fed\u4ee3\u5e08\u751f\u5fae\u8c03\u548c\u52a8\u6001\u969c\u788d\u7269\u907f\u514d\u6a21\u5757DCP-RMP\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\uff0cDRP\u5728\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u7ecf\u5178\u548c\u795e\u7ecf\u65b9\u6cd5\u3002", "conclusion": "DRP\u5c55\u793a\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u751f\u6210\u53cd\u5e94\u6027\u8fd0\u52a8\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.05337", "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "categories": ["cs.CV", "cs.RO"], "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u4f53\u8dcc\u5012\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u9aa8\u9abc\u7279\u5f81\u8f93\u5165\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u4eba\u7c7b\u8dcc\u5012\u7684\u68c0\u6d4b\u4e0e\u9884\u9632\u662f\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5173\u952e\u90e8\u5206\uff0c\u4f46\u76ee\u524d\u5bf9\u8dcc\u5012\u524d\u7684\u9884\u6d4b\u53ca\u7a33\u5b9a\u6027\u4e0e\u5373\u5c06\u8dcc\u5012\u4e4b\u95f4\u7684\u8fc7\u6e21\u72b6\u6001\u5206\u6790\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528DGNN\u4f5c\u4e3a\u5206\u7c7b\u5668\u533a\u5206\u7a33\u5b9a\u3001\u8fc7\u6e21\u548c\u8dcc\u5012\u4e09\u79cd\u6b65\u6001\u72b6\u6001\uff0cLSTM\u7f51\u7edc\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u65e9\u671f\u8dcc\u5012\u68c0\u6d4b\u3002\u6a21\u578b\u57fa\u4e8eOUMVLP-Pose\u548cURFD\u6570\u636e\u96c6\u8bad\u7ec3\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u8bef\u5dee\u548c\u8bc6\u522b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4ec5\u4f9d\u8d56DGNN\u7684\u6a21\u578b\u548c\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u6a21\u578b\uff0c\u8868\u660e\u89e3\u8026\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8dcc\u5012\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u76d1\u6d4b\u8fc7\u6e21\u72b6\u6001\uff0c\u4e3a\u9ad8\u7ea7\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2509.05380", "pdf": "https://arxiv.org/pdf/2509.05380", "abs": "https://arxiv.org/abs/2509.05380", "authors": ["Yoana Pita Lorenzo"], "title": "Cumplimiento del Reglamento (UE) 2024/1689 en rob\u00f3tica y sistemas aut\u00f3nomos: una revisi\u00f3n sistem\u00e1tica de la literatura", "categories": ["cs.CY", "cs.AI", "cs.CR", "cs.RO"], "comment": "in Spanish language", "summary": "This systematic literature review analyzes the current state of compliance\nwith Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on\ncybersecurity frameworks and methodologies. Using the PRISMA protocol, 22\nstudies were selected from 243 initial records across IEEE Xplore, ACM DL,\nScopus, and Web of Science. Findings reveal partial regulatory alignment: while\nprogress has been made in risk management and encrypted communications,\nsignificant gaps persist in explainability modules, real-time human oversight,\nand knowledge base traceability. Only 40% of reviewed solutions explicitly\naddress transparency requirements, and 30% implement failure intervention\nmechanisms. The study concludes that modular approaches integrating risk,\nsupervision, and continuous auditing are essential to meet the AI Act mandates\nin autonomous robotics.", "AI": {"tldr": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7b26\u5408\u6b27\u76df2024/1689\u6cd5\u89c4\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u5173\u6ce8\u7f51\u7edc\u5b89\u5168\u6846\u67b6\u548c\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f53\u524d\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7f51\u7edc\u5b89\u5168\u65b9\u9762\u7684\u5408\u89c4\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6b27\u76df2024/1689\u6cd5\u89c4\u7684\u8981\u6c42\u3002", "method": "\u91c7\u7528PRISMA\u534f\u8bae\uff0c\u4eceIEEE Xplore\u3001ACM DL\u3001Scopus\u548cWeb of Science\u7684243\u6761\u521d\u59cb\u8bb0\u5f55\u4e2d\u7b5b\u9009\u4e8622\u9879\u7814\u7a76\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u90e8\u5206\u5408\u89c4\u6027\u8fdb\u5c55\u4e3b\u8981\u4f53\u73b0\u5728\u98ce\u9669\u7ba1\u7406\u548c\u52a0\u5bc6\u901a\u4fe1\u4e0a\uff0c\u4f46\u5728\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u3001\u5b9e\u65f6\u4eba\u5de5\u76d1\u7763\u548c\u77e5\u8bc6\u5e93\u53ef\u8ffd\u6eaf\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u300240%\u7684\u89e3\u51b3\u65b9\u6848\u660e\u786e\u89e3\u51b3\u900f\u660e\u5ea6\u8981\u6c42\uff0c30%\u5b9e\u65bd\u6545\u969c\u5e72\u9884\u673a\u5236\u3002", "conclusion": "\u7814\u7a76\u603b\u7ed3\u8ba4\u4e3a\uff0c\u6574\u5408\u98ce\u9669\u7ba1\u7406\u3001\u76d1\u7763\u548c\u6301\u7eed\u5ba1\u8ba1\u7684\u6a21\u5757\u5316\u65b9\u6cd5\u5bf9\u4e8e\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684AI\u6cd5\u6848\u8981\u6c42\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.05512", "pdf": "https://arxiv.org/pdf/2509.05512", "abs": "https://arxiv.org/abs/2509.05512", "authors": ["Bryce Grant", "Peng Wang"], "title": "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IROS 2025", "summary": "This paper introduces Quaternion Approximate Networks (QUAN), a novel deep\nlearning framework that leverages quaternion algebra for rotation equivariant\nimage classification and object detection. Unlike conventional quaternion\nneural networks attempting to operate entirely in the quaternion domain, QUAN\napproximates quaternion convolution through Hamilton product decomposition\nusing real-valued operations. This approach preserves geometric properties\nwhile enabling efficient implementation with custom CUDA kernels. We introduce\nIndependent Quaternion Batch Normalization (IQBN) for training stability and\nextend quaternion operations to spatial attention mechanisms. QUAN is evaluated\non image classification (CIFAR-10/100, ImageNet), object detection (COCO,\nDOTA), and robotic perception tasks. In classification tasks, QUAN achieves\nhigher accuracy with fewer parameters and faster convergence compared to\nexisting convolution and quaternion-based models. For objection detection, QUAN\ndemonstrates improved parameter efficiency and rotation handling over standard\nConvolutional Neural Networks (CNNs) while establishing the SOTA for quaternion\nCNNs in this downstream task. These results highlight its potential for\ndeployment in resource-constrained robotic systems requiring rotation-aware\nperception and application in other domains.", "AI": {"tldr": "QUAN\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u56db\u5143\u6570\u4ee3\u6570\u5b9e\u73b0\u65cb\u8f6c\u7b49\u53d8\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5b9e\u6570\u8fd0\u7b97\u8fd1\u4f3c\u56db\u5143\u6570\u5377\u79ef\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65cb\u8f6c\u7b49\u53d8\u4efb\u52a1\u4e2d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0cQUAN\u901a\u8fc7\u56db\u5143\u6570\u4ee3\u6570\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u51e0\u4f55\u7279\u5f81\u5904\u7406\u65b9\u6848\u3002", "method": "\u91c7\u7528Hamilton\u4e58\u79ef\u5206\u89e3\u5b9e\u73b0\u56db\u5143\u6570\u5377\u79ef\u7684\u5b9e\u6570\u8fd1\u4f3c\uff0c\u5f15\u5165IQBN\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u6269\u5c55\u4e86\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u5206\u7c7b\u548c\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cQUAN\u5728\u7cbe\u5ea6\u3001\u53c2\u6570\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u5143\u6570CNN\u4e2d\u8fbe\u5230SOTA\u3002", "conclusion": "QUAN\u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u90e8\u7f72\u6f5c\u529b\uff0c\u9002\u5408\u9700\u8981\u65cb\u8f6c\u611f\u77e5\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.05513", "pdf": "https://arxiv.org/pdf/2509.05513", "abs": "https://arxiv.org/abs/2509.05513", "authors": ["Ahad Jawaid", "Yu Xiang"], "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "4 pages, 1 figure", "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.", "AI": {"tldr": "OpenEgo\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7684\u81ea\u6211\u4e2d\u5fc3\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u624b\u90e8\u59ff\u52bf\u6ce8\u91ca\u548c\u610f\u56fe\u5bf9\u9f50\u7684\u52a8\u4f5c\u57fa\u5143\uff0c\u4ee5\u652f\u6301\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5b66\u4e60\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u6570\u636e\u96c6\u901a\u5e38\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u5b9a\u4f4d\u7684\u52a8\u4f5c\u63cf\u8ff0\u6216\u7075\u5de7\u7684\u624b\u90e8\u6ce8\u91ca\uff0cOpenEgo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "OpenEgo\u6574\u5408\u4e86\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff0c\u7edf\u4e00\u624b\u90e8\u59ff\u52bf\u5e03\u5c40\u5e76\u63d0\u4f9b\u63cf\u8ff0\u6027\u3001\u5e26\u65f6\u95f4\u6233\u7684\u52a8\u4f5c\u57fa\u5143\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6761\u4ef6\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b1107\u5c0f\u65f6\u7684\u89c6\u9891\uff0c\u8986\u76d6290\u4e2a\u64cd\u4f5c\u4efb\u52a1\u548c600\u591a\u4e2a\u73af\u5883\uff0c\u9a8c\u8bc1\u4e86\u5176\u6548\u7528\u3002", "conclusion": "OpenEgo\u964d\u4f4e\u4e86\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\u7684\u969c\u788d\uff0c\u5e76\u652f\u6301\u613f\u666f-\u8bed\u8a00-\u52a8\u4f5c\u5b66\u4e60\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2509.05578", "pdf": "https://arxiv.org/pdf/2509.05578", "abs": "https://arxiv.org/abs/2509.05578", "authors": ["Ruixun Liu", "Lingyu Kong", "Derun Li", "Hang Zhao"], "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown strong vision-language\nreasoning abilities but still lack robust 3D spatial understanding, which is\ncritical for autonomous driving. This limitation stems from two key challenges:\n(1) the difficulty of constructing accessible yet effective 3D representations\nwithout expensive manual annotations, and (2) the loss of fine-grained spatial\ndetails in VLMs due to the absence of large-scale 3D vision-language\npretraining. To address these challenges, we propose OccVLA, a novel framework\nthat integrates 3D occupancy representations into a unified multimodal\nreasoning process. Unlike prior approaches that rely on explicit 3D inputs,\nOccVLA treats dense 3D occupancy as both a predictive output and a supervisory\nsignal, enabling the model to learn fine-grained spatial structures directly\nfrom 2D visual inputs. The occupancy predictions are regarded as implicit\nreasoning processes and can be skipped during inference without performance\ndegradation, thereby adding no extra computational overhead. OccVLA achieves\nstate-of-the-art results on the nuScenes benchmark for trajectory planning and\ndemonstrates superior performance on 3D visual question-answering tasks,\noffering a scalable, interpretable, and fully vision-based solution for\nautonomous driving.", "AI": {"tldr": "OccVLA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc73D\u5360\u636e\u8868\u793a\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\uff0c\u65e0\u9700\u663e\u5f0f3D\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57283D\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u4e3b\u8981\u9762\u4e343D\u8868\u793a\u6784\u5efa\u548c\u7a7a\u95f4\u7ec6\u8282\u4e22\u5931\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faOccVLA\u6846\u67b6\uff0c\u5c063D\u5360\u636e\u8868\u793a\u4f5c\u4e3a\u9884\u6d4b\u8f93\u51fa\u548c\u76d1\u7763\u4fe1\u53f7\uff0c\u76f4\u63a5\u4ece2D\u89c6\u89c9\u8f93\u5165\u5b66\u4e60\u7cbe\u7ec6\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8f68\u8ff9\u89c4\u5212\u548c3D\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u5747\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "OccVLA\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05614", "pdf": "https://arxiv.org/pdf/2509.05614", "abs": "https://arxiv.org/abs/2509.05614", "authors": ["Hanzhen Wang", "Jiaming Xu", "Jiayi Pan", "Yongkang Zhou", "Guohao Dai"], "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "8pages, 10 figures,", "summary": "Pruning accelerates compute-bound models by reducing computation. Recently\napplied to Vision-Language-Action (VLA) models, existing methods prune tokens\nusing only local info from current action, ignoring global context from prior\nactions, causing >20% success rate drop and limited speedup. We observe high\nsimilarity across consecutive actions and propose leveraging both local\n(current) and global (past) info for smarter token selection. We introduce\nSpecPrune-VLA, a training-free method with two-level pruning and heuristic\ncontrol: (1) Static pruning at action level: uses global history and local\ncontext to reduce visual tokens per action; (2) Dynamic pruning at layer level:\nprunes tokens per layer based on layer-specific importance; (3) Lightweight\naction-aware controller: classifies actions as coarse/fine-grained (by speed),\nadjusting pruning aggressiveness since fine-grained actions are\npruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times\nspeedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.\nOpenVLA-OFT, with negligible success rate loss.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecPrune-VLA\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u8fdb\u884c\u4e24\u7ea7\u526a\u679d\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4e2d\u4ec5\u5229\u7528\u5c40\u90e8\u4fe1\u606f\u526a\u679d\u4ee4\u724c\uff0c\u5bfc\u81f4\u6210\u529f\u7387\u4e0b\u964d\u548c\u901f\u5ea6\u63d0\u5347\u6709\u9650\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u8fde\u7eed\u52a8\u4f5c\u95f4\u7684\u9ad8\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4ee5\u4f18\u5316\u526a\u679d\u3002", "method": "SpecPrune-VLA\u91c7\u7528\u4e24\u7ea7\u526a\u679d\uff1a\u9759\u6001\uff08\u52a8\u4f5c\u7ea7\u522b\uff0c\u57fa\u4e8e\u5168\u5c40\u5386\u53f2\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\uff09\u548c\u52a8\u6001\uff08\u5c42\u7ea7\u522b\uff0c\u57fa\u4e8e\u5c42\u91cd\u8981\u6027\uff09\u3002\u6b64\u5916\uff0c\u8f7b\u91cf\u7ea7\u52a8\u4f5c\u611f\u77e5\u63a7\u5236\u5668\u6839\u636e\u52a8\u4f5c\u7c7b\u578b\uff08\u7c97/\u7ec6\u7c92\u5ea6\uff09\u8c03\u6574\u526a\u679d\u5f3a\u5ea6\u3002", "result": "\u5728LIBERO\u6570\u636e\u96c6\u4e0a\uff0cSpecPrune-VLA\u5728NVIDIA A800\u548cRTX 3090\u4e0a\u7684\u901f\u5ea6\u5206\u522b\u63d0\u5347\u81f3OpenVLA-OFT\u76841.46\u500d\u548c1.57\u500d\uff0c\u6210\u529f\u7387\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u7684\u526a\u679d\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.05645", "pdf": "https://arxiv.org/pdf/2509.05645", "abs": "https://arxiv.org/abs/2509.05645", "authors": ["Yan-Shan Lu", "Miguel Arana-Catania", "Saurabh Upadhyay", "Leonard Felicetti"], "title": "Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.CV", "cs.RO"], "comment": "8 pages, 6 figures, 2 tables. ESA ASTRA 2025", "summary": "Mars exploration requires precise and reliable terrain models to ensure safe\nrover navigation across its unpredictable and often hazardous landscapes.\nStereoscopic vision serves a critical role in the rover's perception, allowing\nscene reconstruction by generating precise depth maps through stereo matching.\nState-of-the-art Martian planetary exploration uses traditional local\nblock-matching, aggregates cost over square windows, and refines disparities\nvia smoothness constraints. However, this method often struggles with\nlow-texture images, occlusion, and repetitive patterns because it considers\nonly limited neighbouring pixels and lacks a wider understanding of scene\ncontext. This paper uses Semi-Global Matching (SGM) with superpixel-based\nrefinement to mitigate the inherent block artefacts and recover lost details.\nThe approach balances the efficiency and accuracy of SGM and adds context-aware\nsegmentation to support more coherent depth inference. The proposed method has\nbeen evaluated in three datasets with successful results: In a Mars analogue,\nthe terrain maps obtained show improved structural consistency, particularly in\nsloped or occlusion-prone regions. Large gaps behind rocks, which are common in\nraw disparity outputs, are reduced, and surface details like small rocks and\nedges are captured more accurately. Another two datasets, evaluated to test the\nmethod's general robustness and adaptability, show more precise disparity maps\nand more consistent terrain models, better suited for the demands of autonomous\nnavigation on Mars, and competitive accuracy across both non-occluded and\nfull-image error metrics. This paper outlines the entire terrain modelling\nprocess, from finding corresponding features to generating the final 2D\nnavigation maps, offering a complete pipeline suitable for integration in\nfuture planetary exploration missions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u534a\u5168\u5c40\u5339\u914d\uff08SGM\uff09\u548c\u8d85\u50cf\u7d20\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u706b\u661f\u5730\u5f62\u5efa\u6a21\u4e2d\u7684\u6df1\u5ea6\u56fe\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5757\u5339\u914d\u5728\u4f4e\u7eb9\u7406\u548c\u906e\u6321\u533a\u57df\u7684\u4e0d\u8db3\u3002", "motivation": "\u706b\u661f\u63a2\u7d22\u9700\u8981\u9ad8\u7cbe\u5ea6\u5730\u5f62\u6a21\u578b\u4ee5\u4fdd\u969c\u63a2\u6d4b\u8f66\u5b89\u5168\u5bfc\u822a\uff0c\u4f20\u7edf\u5757\u5339\u914d\u65b9\u6cd5\u5728\u4f4e\u7eb9\u7406\u548c\u906e\u6321\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u534a\u5168\u5c40\u5339\u914d\uff08SGM\uff09\u7ed3\u5408\u8d85\u50cf\u7d20\u4f18\u5316\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u5272\u63d0\u5347\u6df1\u5ea6\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u5730\u5f62\u7ed3\u6784\u4e00\u81f4\u6027\u63d0\u5347\uff0c\u5ca9\u77f3\u540e\u7684\u95f4\u9699\u51cf\u5c11\uff0c\u7ec6\u8282\u6355\u6349\u66f4\u51c6\u786e\uff0c\u9002\u7528\u4e8e\u706b\u661f\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u6761\u5b8c\u6574\u7684\u5efa\u6a21\u6d41\u7a0b\uff0c\u663e\u8457\u6539\u5584\u4e86\u6df1\u5ea6\u56fe\u7684\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.05728", "pdf": "https://arxiv.org/pdf/2509.05728", "abs": "https://arxiv.org/abs/2509.05728", "authors": ["Niels Balemans", "Ali Anwar", "Jan Steckel", "Siegfried Mercelis"], "title": "LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper extends LiDAR-BIND, a modular multi-modal fusion framework that\nbinds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,\nwith mechanisms that explicitly enforce temporal consistency. We introduce\nthree contributions: (i) temporal embedding similarity that aligns consecutive\nlatents, (ii) a motion-aligned transformation loss that matches displacement\nbetween predictions and ground truth LiDAR, and (iii) windows temporal fusion\nusing a specialised temporal module. We further update the model architecture\nto better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR\ntranslation demonstrate improved temporal and spatial coherence, yielding lower\nabsolute trajectory error and better occupancy map accuracy in\nCartographer-based SLAM (Simultaneous Localisation and Mapping). We propose\ndifferent metrics based on the Fr\\'echet Video Motion Distance (FVMD) and a\ncorrelation-peak distance metric providing practical temporal quality\nindicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or\nLiDAR-BIND-T, maintains plug-and-play modality fusion while substantially\nenhancing temporal stability, resulting in improved robustness and performance\nfor downstream SLAM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86LiDAR-BIND\u6846\u67b6\uff0c\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u673a\u5236\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u4f18\u5316\u4e0b\u6e38SLAM\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u4f20\u611f\u5668\uff08\u96f7\u8fbe\u3001\u58f0\u7eb3\uff09\u4e0eLiDAR\u878d\u5408\u65f6\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347SLAM\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u673a\u5236\uff1a(i)\u65f6\u95f4\u5d4c\u5165\u76f8\u4f3c\u6027\u5bf9\u9f50\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c(ii)\u8fd0\u52a8\u5bf9\u9f50\u53d8\u6362\u635f\u5931\u5339\u914d\u9884\u6d4b\u4e0eLiDAR\u771f\u503c\u4f4d\u79fb\uff0c(iii)\u4e13\u7528\u65f6\u95f4\u6a21\u5757\u5b9e\u73b0\u7a97\u53e3\u65f6\u95f4\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u96f7\u8fbe/\u58f0\u7eb3\u5230LiDAR\u7684\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u65f6\u7a7a\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\uff0c\u964d\u4f4e\u4e86\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86SLAM\u7684\u5360\u7528\u56fe\u7cbe\u5ea6\u3002", "conclusion": "LiDAR-BIND-T\u5728\u4fdd\u6301\u5373\u63d2\u5373\u7528\u591a\u6a21\u6001\u878d\u5408\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u63d0\u5347\u4e86SLAM\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.05747", "pdf": "https://arxiv.org/pdf/2509.05747", "abs": "https://arxiv.org/abs/2509.05747", "authors": ["Leo Ho", "Yinghao Huang", "Dafei Qin", "Mingyi Shi", "Wangpok Tse", "Wei Liu", "Junichi Yamagishi", "Taku Komura"], "title": "InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO", "I.5.4"], "comment": "The first two authors contributed equally to this work", "summary": "We address the problem of accurate capture of interactive behaviors between\ntwo people in daily scenarios. Most previous works either only consider one\nperson or solely focus on conversational gestures of two people, assuming the\nbody orientation and/or position of each actor are constant or barely change\nover each interaction. In contrast, we propose to simultaneously model two\npeople's activities, and target objective-driven, dynamic, and semantically\nconsistent interactions which often span longer duration and cover bigger\nspace. To this end, we capture a new multi-modal dataset dubbed InterAct, which\nis composed of 241 motion sequences where two people perform a realistic and\ncoherent scenario for one minute or longer over a complete interaction. For\neach sequence, two actors are assigned different roles and emotion labels, and\ncollaborate to finish one task or conduct a common interaction activity. The\naudios, body motions, and facial expressions of both persons are captured.\nInterAct contains diverse and complex motions of individuals and interesting\nand relatively long-term interaction patterns barely seen before. We also\ndemonstrate a simple yet effective diffusion-based method that estimates\ninteractive face expressions and body motions of two people from speech inputs.\nOur method regresses the body motions in a hierarchical manner, and we also\npropose a novel fine-tuning mechanism to improve the lip accuracy of facial\nexpressions. To facilitate further research, the data and code is made\navailable at https://hku-cg.github.io/interact/ .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u51c6\u786e\u6355\u6349\u65e5\u5e38\u573a\u666f\u4e2d\u4e24\u4e2a\u4eba\u4e4b\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u884c\u4e3a\uff0c\u5e76\u53d1\u5e03\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6InterAct\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u4ec5\u5173\u6ce8\u5355\u4eba\u884c\u4e3a\u6216\u9759\u6001\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5efa\u6a21\u52a8\u6001\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u957f\u65f6\u95f4\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u5c42\u6b21\u56de\u5f52\u65b9\u6cd5\uff0c\u4ece\u8bed\u97f3\u8f93\u5165\u4f30\u8ba1\u4ea4\u4e92\u5f0f\u9762\u90e8\u8868\u60c5\u548c\u8eab\u4f53\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5fae\u8c03\u673a\u5236\u4ee5\u63d0\u9ad8\u5507\u90e8\u51c6\u786e\u6027\u3002", "result": "\u53d1\u5e03\u4e86\u5305\u542b241\u4e2a\u52a8\u6001\u5e8f\u5217\u7684InterAct\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u591a\u6837\u5316\u7684\u590d\u6742\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u4ea4\u4e92\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.05855", "pdf": "https://arxiv.org/pdf/2509.05855", "abs": "https://arxiv.org/abs/2509.05855", "authors": ["Thijs Masmeijer", "Caleb Swain", "Jeff Hill", "Ed Habtour"], "title": "Programming tension in 3D printed networks inspired by spiderwebs", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Each element in tensioned structural networks -- such as tensegrity,\narchitectural fabrics, or medical braces/meshes -- requires a specific tension\nlevel to achieve and maintain the desired shape, stability, and compliance.\nThese structures are challenging to manufacture, 3D print, or assemble because\nflattening the network during fabrication introduces multiplicative\ninaccuracies in the network's final tension gradients. This study overcomes\nthis challenge by offering a fabrication algorithm for direct 3D printing of\nsuch networks with programmed tension gradients, an approach analogous to the\nspinning of spiderwebs. The algorithm: (i) defines the desired network and\nprescribes its tension gradients using the force density method; (ii) converts\nthe network into an unstretched counterpart by numerically optimizing vertex\nlocations toward target element lengths and converting straight elements into\narcs to resolve any remaining error; and (iii) decomposes the network into\nprintable toolpaths; Optional additional steps are: (iv) flattening curved 2D\nnetworks or 3D networks to ensure 3D printing compatibility; and (v)\nautomatically resolving any unwanted crossings introduced by the flattening\nprocess. The proposed method is experimentally validated using 2D unit cells of\nviscoelastic filaments, where accurate tension gradients are achieved with an\naverage element strain error of less than 1.0\\%. The method remains effective\nfor networks with element minimum length and maximum stress of 5.8 mm and 7.3\nMPa, respectively. The method is used to demonstrate the fabrication of three\ncomplex cases: a flat spiderweb, a curved mesh, and a tensegrity system. The\nprogrammable tension gradient algorithm can be utilized to produce compact,\nintegrated cable networks, enabling novel applications such as moment-exerting\nstructures in medical braces and splints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a53D\u6253\u5370\u5e26\u7f16\u7a0b\u5f20\u529b\u68af\u5ea6\u7ed3\u6784\u7684\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5236\u9020\u4e2d\u7684\u5f20\u529b\u5206\u5e03\u4e0d\u51c6\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5236\u9020\u65b9\u6cd5\u4e2d\uff0c\u5f20\u529b\u7f51\u7edc\u7684\u5f20\u529b\u68af\u5ea6\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\uff0c\u5bfc\u81f4\u5f62\u72b6\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u529b\u5bc6\u5ea6\u6cd5\u5b9a\u4e49\u7f51\u7edc\u548c\u5f20\u529b\u68af\u5ea6\uff0c\u4f18\u5316\u9876\u70b9\u4f4d\u7f6e\u5e76\u5c06\u76f4\u7ebf\u5143\u7d20\u8f6c\u6362\u4e3a\u5f27\u7ebf\uff0c\u5206\u89e3\u4e3a\u53ef\u6253\u5370\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u5f20\u529b\u68af\u5ea6\u51c6\u786e\uff0c\u5e94\u53d8\u8bef\u5dee\u5c0f\u4e8e1%\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u53ef\u5b9e\u73b0\u7d27\u51d1\u7684\u96c6\u6210\u7535\u7f06\u7f51\u7edc\uff0c\u4e3a\u533b\u7597\u5668\u68b0\u7b49\u9886\u57df\u63d0\u4f9b\u65b0\u5e94\u7528\u3002"}}
{"id": "2509.06103", "pdf": "https://arxiv.org/pdf/2509.06103", "abs": "https://arxiv.org/abs/2509.06103", "authors": ["Divij Gupta", "Arkajit Aich"], "title": "Advancing Resource Extraction Systems in Martian Volcanic Terrain: Rover Design, Power Consumption and Hazard Analysis", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.RO", "physics.space-ph"], "comment": "23 pages, 5 figures", "summary": "This study proposes a schematic plan for in-situ resource utilization (ISRU)\nin Martian volcanic terrains. The work investigated the complexity of volcanic\nterrains and Martian environmental hazards and suggested comprehensive\nengineering strategies to overcome the odds and establish a successful mining\nprogram in Martian volcanic regions. Slope stabilization methods - such as\nterracing and anchored drilling rigs - with terrain-adaptive rovers capable of\nautonomous operations on steep unstable slopes has been suggested as feasible\nsolutions to navigate the complex geological terrains of Martian volcanoes. The\nmid range rover design with a mass of approximately 2.1 t, proposed here for\nmining operations, incorporates a six-wheel rocker-bogie suspension,\nanchoring-enabled drilling arm, dust-mitigation solar arrays, and advanced\nsensing systems for hazard detection and navigation. A comparative analysis\nregarding choice of roads and rails for building transport infrastructure has\nalso been performed. We have also looked into the energy requirement of the\nrover to work under extreme environmental conditions of Mars and suggested a\ncombination of solar and nuclear power to account for the huge energy\nrequirements of sustained operations on Mars. The results demonstrate that\nmission success in these environments depends on integrating mechanical\nresilience, environmental adaptability, and operational autonomy, enabling\nsustainable access to resources in one of Mars' most geologically challenging\nsettings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u706b\u661f\u706b\u5c71\u5730\u5f62\u4e2d\u5229\u7528\u539f\u4f4d\u8d44\u6e90\uff08ISRU\uff09\u7684\u65b9\u6848\uff0c\u7814\u7a76\u4e86\u706b\u5c71\u5730\u5f62\u7684\u590d\u6742\u6027\u548c\u73af\u5883\u5371\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u5de5\u7a0b\u7b56\u7565\uff0c\u5305\u62ec\u659c\u5761\u7a33\u5b9a\u65b9\u6cd5\u548c\u5730\u5f62\u81ea\u9002\u5e94\u6f2b\u6e38\u8f66\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u706b\u661f\u706b\u5c71\u5730\u533a\u8fdb\u884c\u91c7\u77ff\u4efb\u52a1\u7684\u590d\u6742\u5730\u8d28\u548c\u73af\u5883\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u6781\u7aef\u73af\u5883\u4e14\u5177\u5907\u9ad8\u5ea6\u81ea\u4e3b\u6027\u7684\u91c7\u77ff\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u659c\u5761\u7a33\u5b9a\u65b9\u6cd5\uff08\u5982\u9636\u68af\u5316\u548c\u951a\u5b9a\u94bb\u673a\uff09\u548c\u5730\u5f62\u81ea\u9002\u5e94\u6f2b\u6e38\u8f66\u8bbe\u8ba1\uff0c\u7ed3\u5408\u592a\u9633\u80fd\u548c\u6838\u80fd\u4f9b\u7535\uff0c\u5e76\u8fdb\u884c\u9053\u8def\u4e0e\u8f68\u9053\u8fd0\u8f93\u57fa\u7840\u8bbe\u65bd\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4efb\u52a1\u6210\u529f\u4f9d\u8d56\u4e8e\u673a\u68b0\u97e7\u6027\u3001\u73af\u5883\u9002\u5e94\u80fd\u529b\u548c\u64cd\u4f5c\u81ea\u4e3b\u6027\u7684\u7ed3\u5408\uff0c\u786e\u4fdd\u4e86\u5728\u706b\u661f\u6700\u5177\u6311\u6218\u6027\u7684\u5730\u8d28\u73af\u5883\u4e2d\u53ef\u6301\u7eed\u83b7\u53d6\u8d44\u6e90\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u63d0\u51fa\u7684\u91c7\u77ff\u65b9\u6848\u5728\u706b\u661f\u706b\u5c71\u5730\u5f62\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5f3a\u8c03\u4e86\u591a\u6280\u672f\u96c6\u6210\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.06333", "pdf": "https://arxiv.org/pdf/2509.06333", "abs": "https://arxiv.org/abs/2509.06333", "authors": ["Penelope Brown", "Julie Stephany Berrio Perez", "Mao Shan", "Stewart Worrall"], "title": "Multi-Modal Camera-Based Detection of Vulnerable Road Users", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists\nrepresent more than half of global traffic deaths, yet their detection remains\nchallenging in poor lighting, adverse weather, and unbalanced data sets. This\npaper presents a multimodal detection framework that integrates RGB and thermal\ninfrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,\nBDD100K, and Teledyne FLIR datasets, with class re-weighting and light\naugmentations to improve minority-class performance and robustness, experiments\nshow that 640-pixel resolution and partial backbone freezing optimise accuracy\nand efficiency, while class-weighted losses enhance recall for rare VRUs.\nResults highlight that thermal models achieve the highest precision, and\nRGB-to-thermal augmentation boosts recall, demonstrating the potential of\nmultimodal detection to improve VRU safety at intersections.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408RGB\u548c\u70ed\u7ea2\u5916\u6210\u50cf\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08\u5982\u884c\u4eba\u3001\u9a91\u884c\u8005\u548c\u6469\u6258\u8f66\u624b\uff09\u5728\u5168\u7403\u4ea4\u901a\u6b7b\u4ea1\u4e2d\u5360\u6bd4\u8d85\u8fc7\u4e00\u534a\uff0c\u4f46\u5728\u5149\u7ebf\u4e0d\u8db3\u3001\u6076\u52a3\u5929\u6c14\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0b\u68c0\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684YOLOv8\u6a21\u578b\u96c6\u6210RGB\u548c\u70ed\u7ea2\u5916\u6210\u50cf\uff0c\u8bad\u7ec3\u6570\u636e\u5305\u62ecKITTI\u3001BDD100K\u548cTeledyne FLIR\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7c7b\u522b\u91cd\u65b0\u52a0\u6743\u548c\u8f7b\u5ea6\u589e\u5f3a\u63d0\u5347\u5c11\u6570\u7c7b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c640\u50cf\u7d20\u5206\u8fa8\u7387\u548c\u90e8\u5206\u4e3b\u5e72\u51bb\u7ed3\u4f18\u5316\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7c7b\u522b\u52a0\u6743\u635f\u5931\u63d0\u9ad8\u4e86\u7a00\u6709VRUs\u7684\u53ec\u56de\u7387\u3002", "result": "\u70ed\u7ea2\u5916\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7cbe\u5ea6\uff0cRGB\u5230\u70ed\u7ea2\u5916\u7684\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4e86\u53ec\u56de\u7387\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u68c0\u6d4b\u5728\u63d0\u5347\u4ea4\u53c9\u8def\u53e3VRU\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\u901a\u8fc7\u7ed3\u5408RGB\u548c\u70ed\u7ea2\u5916\u6210\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86VRUs\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\uff0c\u6709\u671b\u6539\u5584\u4ea4\u901a\u5b89\u5168\u3002"}}
{"id": "2509.06374", "pdf": "https://arxiv.org/pdf/2509.06374", "abs": "https://arxiv.org/abs/2509.06374", "authors": ["Hiroya Makino", "Seigo Ito"], "title": "MAPF-HD: Multi-Agent Path Finding in High-Density Environments", "categories": ["cs.MA", "cs.RO"], "comment": "9 pages, 12 figures", "summary": "Multi-agent path finding (MAPF) involves planning efficient paths for\nmultiple agents to move simultaneously while avoiding collisions. In typical\nwarehouse environments, agents are often sparsely distributed along aisles.\nHowever, increasing the agent density can improve space efficiency. When the\nagent density is high, we must optimize the paths not only for goal-assigned\nagents but also for those obstructing them. This study proposes a novel MAPF\nframework for high-density environments (MAPF-HD). Several studies have\nexplored MAPF in similar settings using integer linear programming (ILP).\nHowever, ILP-based methods require substantial computation time to optimize all\nagent paths simultaneously. Even in small grid-based environments with fewer\nthan $100$ cells, these computations can incur tens to hundreds of seconds.\nThese high computational costs render these methods impractical for large-scale\napplications such as automated warehouses and valet parking. To address these\nlimitations, we introduce the phased null-agent swapping (PHANS) method. PHANS\nemploys a heuristic approach to incrementally swap positions between agents and\nempty vertices. This method solves the MAPF-HD problem within seconds to tens\nof seconds, even in large environments containing more than $700$ cells. The\nproposed method can potentially improve efficiency in various real-world\napplications such as warehouse logistics, traffic management, or crowd control.\nCode is available at https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff08MAPF-HD\uff09\uff0c\u901a\u8fc7PHANS\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u65b9\u6cd5\u8ba1\u7b97\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u5728\u9ad8\u5bc6\u5ea6\u73af\u5883\u4e2d\uff08\u5982\u81ea\u52a8\u5316\u4ed3\u5e93\uff09\uff0c\u4f20\u7edfILP\u65b9\u6cd5\u56e0\u8ba1\u7b97\u65f6\u95f4\u957f\u800c\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86PHANS\u65b9\u6cd5\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u4ea4\u6362\u667a\u80fd\u4f53\u4e0e\u7a7a\u4f4d\u4f4d\u7f6e\uff0c\u9010\u6b65\u4f18\u5316\u8def\u5f84\u3002", "result": "PHANS\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\uff08\u8d85\u8fc7700\u4e2a\u5355\u5143\uff09\u80fd\u5728\u51e0\u79d2\u81f3\u51e0\u5341\u79d2\u5185\u89e3\u51b3\u95ee\u9898\uff0c\u663e\u8457\u5feb\u4e8eILP\u65b9\u6cd5\u3002", "conclusion": "PHANS\u65b9\u6cd5\u9ad8\u6548\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u4ed3\u5e93\u7269\u6d41\u3001\u4ea4\u901a\u7ba1\u7406\u7b49\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2509.06426", "pdf": "https://arxiv.org/pdf/2509.06426", "abs": "https://arxiv.org/abs/2509.06426", "authors": ["Pembe Gizem \u00d6zdil", "Chuanfang Ning", "Jasper S. Phelps", "Sibo Wang-Chen", "Guy Elisha", "Alexander Blanke", "Auke Ijspeert", "Pavan Ramdya"], "title": "Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "comment": "23 pages, 11 figures", "summary": "Computational models are critical to advance our understanding of how neural,\nbiomechanical, and physical systems interact to orchestrate animal behaviors.\nDespite the availability of near-complete reconstructions of the Drosophila\nmelanogaster central nervous system, musculature, and exoskeleton, anatomically\nand physically grounded models of fly leg muscles are still missing. These\nmodels provide an indispensable bridge between motor neuron activity and joint\nmovements. Here, we introduce the first 3D, data-driven musculoskeletal model\nof Drosophila legs, implemented in both OpenSim and MuJoCo simulation\nenvironments. Our model incorporates a Hill-type muscle representation based on\nhigh-resolution X-ray scans from multiple fixed specimens. We present a\npipeline for constructing muscle models using morphological imaging data and\nfor optimizing unknown muscle parameters specific to the fly. We then combine\nour musculoskeletal models with detailed 3D pose estimation data from behaving\nflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of\nmuscle activity across diverse walking and grooming behaviors predict\ncoordinated muscle synergies that can be tested experimentally. Furthermore, by\ntraining imitation learning policies in MuJoCo, we test the effect of different\npassive joint properties on learning speed and find that damping and stiffness\nfacilitate learning. Overall, our model enables the investigation of motor\ncontrol in an experimentally tractable model organism, providing insights into\nhow biomechanics contribute to generation of complex limb movements. Moreover,\nour model can be used to control embodied artificial agents to generate\nnaturalistic and compliant locomotion in simulated environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u679c\u8747\u817f\u90e8\u76843D\u6570\u636e\u9a71\u52a8\u808c\u8089\u9aa8\u9abc\u6a21\u578b\uff0c\u7ed3\u5408OpenSim\u548cMuJoCo\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u7814\u7a76\u7535\u673a\u63a7\u5236\u5e76\u9884\u6d4b\u808c\u8089\u534f\u540c\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u679c\u8747\u7684\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u3001\u808c\u8089\u548c\u5916\u9aa8\u9abc\u5df2\u8fd1\u5b8c\u5168\u91cd\u5efa\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u89e3\u5256\u548c\u7269\u7406\u7684\u817f\u90e8\u808c\u8089\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u662f\u8fde\u63a5\u8fd0\u52a8\u795e\u7ecf\u5143\u6d3b\u52a8\u548c\u5173\u8282\u8fd0\u52a8\u7684\u6865\u6881\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387X\u5c04\u7ebf\u626b\u63cf\u548c\u591a\u6807\u672c\u6570\u636e\uff0c\u6784\u5efaHill\u578b\u808c\u8089\u6a21\u578b\uff0c\u5e76\u7ed3\u54083D\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u5b9e\u73b0\u808c\u8089\u9a71\u52a8\u7684\u884c\u4e3a\u91cd\u653e\u3002\u5728MuJoCo\u4e2d\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u591a\u79cd\u884c\u4e3a\u4e2d\u7684\u808c\u8089\u534f\u540c\u4f5c\u7528\uff0c\u5e76\u9a8c\u8bc1\u4e86\u88ab\u52a8\u5173\u8282\u7279\u6027\u5bf9\u5b66\u4e60\u901f\u5ea6\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7814\u7a76\u679c\u8747\u7684\u7535\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u751f\u7269\u529b\u5b66\u5728\u590d\u6742\u80a2\u4f53\u8fd0\u52a8\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u4eba\u5de5\u4ee3\u7406\u7684\u81ea\u7136\u8fd0\u52a8\u751f\u6210\u3002"}}
{"id": "2509.06660", "pdf": "https://arxiv.org/pdf/2509.06660", "abs": "https://arxiv.org/abs/2509.06660", "authors": ["Cailei Liang", "Adrian Bodenmann", "Emma J Curtis", "Samuel Simmons", "Kazunori Nagano", "Stan Brown", "Adam Riese", "Blair Thornton"], "title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f4d\u7f6e\u5143\u6570\u636e\u5728\u4e0d\u540c\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7b56\u7565\u4e2d\u5bf9\u6d77\u5e8a\u56fe\u50cf\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u4f4d\u7f6e\u6b63\u5219\u5316\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\u4e2d\u3002", "motivation": "\u63d0\u9ad8\u6d77\u5e8a\u89c6\u89c9\u56fe\u50cf\u7684\u9ad8\u901a\u91cf\u89e3\u91ca\u6548\u7387\uff0c\u63a2\u7d22\u4f4d\u7f6e\u5143\u6570\u636e\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8bc4\u4f30\u4e86\u516d\u79cdSSL\u6846\u67b6\uff08\u5305\u62ecCNN\u548cViT\u6a21\u578b\uff09\u5728\u4e09\u79cd\u6d77\u5e8a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u4f4d\u7f6e\u6b63\u5219\u5316\u7684\u6548\u679c\u3002", "result": "\u4f4d\u7f6e\u6b63\u5219\u5316\u5e73\u5747\u63d0\u5347\u4e86CNN\u548cViT\u7684F1\u5206\u6570\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\u4e2d\uff1b\u9ad8\u7ef4ViT\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4f4d\u7f6e\u5143\u6570\u636e\u5bf9SSL\u6b63\u5219\u5316\u6709\u4ef7\u503c\uff0c\u7279\u522b\u662f\u4f4e\u7ef4\u8868\u793a\uff1b\u9ad8\u7ef4ViT\u5728\u6d77\u5e8a\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.06678", "pdf": "https://arxiv.org/pdf/2509.06678", "abs": "https://arxiv.org/abs/2509.06678", "authors": ["Cailei Liang", "Adrian Bodenmann", "Sam Fenton", "Blair Thornton"], "title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u805a\u7c7b\u6846\u67b6\uff08OCF\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u65e0\u76d1\u7763\u89e3\u6790\u6d77\u5e95\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u81ea\u9002\u5e94\u4efb\u52a1\u548c\u9ad8\u6548\u901a\u4fe1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOCF\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe0.68\uff0c\u4e14\u8ba1\u7b97\u65f6\u95f4\u7a33\u5b9a\u3002", "motivation": "\u968f\u7740\u957f\u7eed\u822a\u548c\u6d77\u5e95\u9a7b\u7559AUV\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5b9e\u65f6\u89e3\u6790\u6d77\u5e95\u56fe\u50cf\u4ee5\u652f\u6301\u81ea\u9002\u5e94\u4efb\u52a1\u548c\u4f18\u5316\u901a\u4fe1\u6548\u7387\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u6570\u636e\u96c6\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u63d0\u51faOCF\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u8868\u6027\u6837\u672c\u52a8\u6001\u6355\u6349\u7279\u5f81\u5206\u5e03\uff0c\u652f\u6301\u5b9e\u65f6\u805a\u7c7b\uff0c\u65e0\u9700\u91cd\u65b0\u5904\u7406\u5386\u53f2\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u4e86\u52a8\u6001\u7c07\u5408\u5e76\u4e0e\u5206\u88c2\u3002", "result": "OCF\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5206\u6570\u4e3a0.68\uff0c\u8ba1\u7b97\u65f6\u95f4\u968f\u6570\u636e\u91cf\u589e\u52a0\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f18\u4e8e\u5176\u4ed6\u5728\u7ebf\u805a\u7c7b\u65b9\u6cd5\u3002", "conclusion": "OCF\u9002\u7528\u4e8e\u957f\u671f\u81ea\u4e3b\u6d77\u6d0b\u63a2\u7d22\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u6570\u636e\u6458\u8981\u5e76\u652f\u6301\u8def\u5f84\u89c4\u5212\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.06736", "pdf": "https://arxiv.org/pdf/2509.06736", "abs": "https://arxiv.org/abs/2509.06736", "authors": ["Jie Yang", "Jiajun Chen", "Zhangyue Yin", "Shuo Chen", "Yuxin Wang", "Yiran Guo", "Yuan Li", "Yining Zheng", "Xuanjing Huang", "Xipeng Qiu"], "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VehicleWorld\u73af\u5883\u53caState-based Function Call\uff08SFC\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u51fd\u6570\u8c03\u7528\u5728\u667a\u80fd\u6c7d\u8f66\u5ea7\u8231\u4e2d\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u3002", "motivation": "\u667a\u80fd\u6c7d\u8f66\u5ea7\u8231\u7684\u590d\u6742\u6027\u8981\u6c42\u9ad8\u6548\u7684API\u4ee3\u7406\u534f\u8c03\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u65e0\u72b6\u6001\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\u5dee\u3002", "method": "\u901a\u8fc7VehicleWorld\uff08\u5305\u542b30\u6a21\u5757\u3001250 API\u548c680\u5c5e\u6027\uff09\u63d0\u51faSFC\u65b9\u6cd5\uff0c\u660e\u786e\u7cfb\u7edf\u72b6\u6001\u5e76\u76f4\u63a5\u5b9e\u73b0\u72b6\u6001\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFC\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6267\u884c\u51c6\u786e\u6027\u66f4\u9ad8\u4e14\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "SFC\u4e3a\u667a\u80fd\u6c7d\u8f66\u5ea7\u8231\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u73af\u5883\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.06741", "pdf": "https://arxiv.org/pdf/2509.06741", "abs": "https://arxiv.org/abs/2509.06741", "authors": ["Christian Geckeler", "Niklas Neugebauer", "Manasi Muglikar", "Davide Scaramuzza", "Stefano Mintchev"], "title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light", "categories": ["cs.CV", "cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e8b\u4ef6\u5149\u8c31\u7cfb\u7edf\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u68ee\u6797\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u4f4e\u5ef6\u8fdf\u7684\u6df1\u5ea6\u91cd\u5efa\u548c\u591a\u5149\u8c31\u6210\u50cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4f20\u611f\u65b9\u6cd5\u5728\u68ee\u6797\u73af\u5883\u4e2d\u5b58\u5728\u7684\u5ef6\u8fdf\u3001\u6df1\u5ea6\u5206\u8fa8\u7387\u5dee\u548c\u5bf9\u73af\u5883\u5149\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8c03\u5236\u7ed3\u6784\u5316\u5149\u7684\u6ce2\u957f\uff0c\u540c\u65f6\u8fdb\u884c\u6df1\u5ea6\u91cd\u5efa\u548c\u591a\u5149\u8c31\u6210\u50cf\uff0c\u8986\u76d6650 nm\u81f3850 nm\u6ce2\u6bb5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6df1\u5ea6\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e60%\uff0c\u5149\u8c31\u7cbe\u5ea6\u4e0e\u5546\u7528\u8bbe\u5907\u76f8\u5f53\uff0c\u6df1\u5ea6\u6570\u636e\u8f85\u52a9\u6750\u6599\u5206\u7c7b\u7cbe\u5ea6\u63d0\u534730%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u4e2d\u7684\u8f7b\u91cf\u3001\u96c6\u6210\u548c\u9c81\u68d2\u611f\u77e5\u4e0e\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2509.06893", "pdf": "https://arxiv.org/pdf/2509.06893", "abs": "https://arxiv.org/abs/2509.06893", "authors": ["Noble Harasha", "Nancy Lynch"], "title": "Nanobot Algorithms for Treatment of Diffuse Cancer", "categories": ["cs.MA", "cs.RO", "q-bio.QM"], "comment": "Abridged abstract shown here; 34 pages, 9 figures", "summary": "Motile nanosized particles, or \"nanobots\", promise more effective and less\ntoxic targeted drug delivery because of their unique scale and precision. We\nconsider the case in which the cancer is \"diffuse\", dispersed such that there\nare multiple distinct cancer sites. We investigate the problem of a swarm of\nnanobots locating these sites and treating them by dropping drug payloads at\nthe sites. To improve the success of the treatment, the drug payloads must be\nallocated between sites according to their \"demands\"; this requires extra\nnanobot coordination. We present a mathematical model of the behavior of the\nnanobot agents and of their colloidal environment. This includes a movement\nmodel for agents based upon experimental findings from actual nanoparticles in\nwhich bots noisily ascend and descend chemical gradients. We present three\nalgorithms: The first algorithm, called KM, is the most representative of\nreality, with agents simply following naturally existing chemical signals that\nsurround each cancer site. The second algorithm, KMA, includes an additional\nchemical payload which amplifies the existing natural signals. The third\nalgorithm, KMAR, includes another additional chemical payload which counteracts\nthe other signals, instead inducing negative chemotaxis in agents such that\nthey are repelled from sites that are already sufficiently treated. We present\nsimulation results for all algorithms across different types of cancer\narrangements. For KM, we show that the treatment is generally successful unless\nthe natural chemical signals are weak, in which case the treatment progresses\ntoo slowly. For KMA, we demonstrate a significant improvement in treatment\nspeed but a drop in eventual success, except for concentrated cancer patterns.\nFor KMAR, our results show great performance across all types of cancer\npatterns, demonstrating robustness and adaptability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u2018\u7eb3\u7c73\u673a\u5668\u4eba\u2019\u5728\u591a\u5904\u764c\u75c7\u6269\u6563\u60c5\u51b5\u4e0b\u901a\u8fc7\u534f\u8c03\u6295\u653e\u836f\u7269\u6765\u6cbb\u7597\u7684\u6548\u679c\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u7b97\u6cd5\uff08KM\u3001KMA\u3001KMAR\uff09\uff0c\u5176\u4e2dKMAR\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u5f3a\u9002\u5e94\u6027\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u7eb3\u7c73\u673a\u5668\u4eba\u56e0\u5176\u5c3a\u5ea6\u5c0f\u548c\u7cbe\u51c6\u6027\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u6709\u6548\u4e14\u6bd2\u6027\u66f4\u4f4e\u7684\u9776\u5411\u836f\u7269\u9012\u9001\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u5904\u6269\u6563\u764c\u75c7\u7684\u6cbb\u7597\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u6a21\u578b\u63cf\u8ff0\u7eb3\u7c73\u673a\u5668\u4eba\u548c\u80f6\u4f53\u73af\u5883\u884c\u4e3a\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7b97\u6cd5\uff1aKM\uff08\u81ea\u7136\u4fe1\u53f7\u8ddf\u968f\uff09\u3001KMA\uff08\u589e\u5f3a\u4fe1\u53f7\uff09\u3001KMAR\uff08\u6291\u5236\u4fe1\u53f7\uff09\u3002", "result": "KM\u5728\u4fe1\u53f7\u5f31\u65f6\u6cbb\u7597\u901f\u5ea6\u6162\uff1bKMA\u901f\u5ea6\u63d0\u5347\u4f46\u6210\u529f\u7387\u4e0b\u964d\uff1bKMAR\u5728\u6240\u6709\u764c\u75c7\u6a21\u5f0f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "KMAR\u7b97\u6cd5\u5728\u9002\u5e94\u6027\u548c\u6cbb\u7597\u6210\u529f\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6269\u6563\u764c\u75c7\u6cbb\u7597\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2509.06934", "pdf": "https://arxiv.org/pdf/2509.06934", "abs": "https://arxiv.org/abs/2509.06934", "authors": ["Agam Oberlender", "Hadas Erel"], "title": "\"It was Tragic\": Exploring the Impact of a Robot's Shutdown", "categories": ["cs.HC", "cs.RO"], "comment": "8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025", "summary": "It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u4eec\u503e\u5411\u4e8e\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u793e\u4ea4\u5b9e\u4f53\uff0c\u5373\u4f7f\u662f\u672a\u8bbe\u8ba1\u7528\u4e8e\u793e\u4ea4\u4e92\u52a8\u7684\u673a\u5668\u4eba\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u673a\u5668\u4eba\u7684\u5173\u673a\u52a8\u4f5c\u4f1a\u5f71\u54cd\u5176\u611f\u77e5\u548c\u8bc4\u4ef7\u3002", "motivation": "\u63a2\u8ba8\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u5173\u673a\u52a8\u4f5c\u7684\u793e\u4ea4\u89e3\u8bfb\u53ca\u5176\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u53c2\u4e0e\u8005\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u540e\uff0c\u5206\u522b\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u5173\u95ed\u673a\u5668\u4eba\uff1a\u4e00\u79cd\u662f\u76f4\u63a5\u5173\u673a\uff0c\u53e6\u4e00\u79cd\u662f\u6a21\u4eff'\u7761\u89c9'\u7684\u52a8\u4f5c\u5173\u673a\u3002", "result": "\u76f4\u63a5\u5173\u673a\u7684\u52a8\u4f5c\u88ab\u8d1f\u9762\u89e3\u8bfb\u4e3a'\u6b7b\u4ea1'\uff0c\u800c\u6a21\u4eff\u7761\u89c9\u7684\u52a8\u4f5c\u5219\u88ab\u4e2d\u6027\u89e3\u8bfb\u4e3a'\u7761\u89c9'\uff0c\u540e\u8005\u63d0\u9ad8\u4e86\u597d\u611f\u5ea6\u3001\u667a\u80fd\u611f\u548c\u751f\u547d\u529b\u611f\u77e5\u3002", "conclusion": "\u8bbe\u8ba1\u673a\u5668\u4eba\u5173\u673a\u52a8\u4f5c\u65f6\u5e94\u8003\u8651\u4eba\u4eec\u7684\u793e\u4ea4\u611f\u77e5\u503e\u5411\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002"}}
