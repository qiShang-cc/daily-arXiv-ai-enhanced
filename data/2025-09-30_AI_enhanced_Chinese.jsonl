{"id": "2509.22693", "pdf": "https://arxiv.org/pdf/2509.22693", "abs": "https://arxiv.org/abs/2509.22693", "authors": ["Muhammad Hafil Nugraha", "Fauzi Abdul", "Lastiko Bramantyo", "Estiko Rijanto", "Roni Permana Saputra", "Oka Mahendra"], "title": "Mobile Robot Localization via Indoor Positioning System and Odometry Fusion", "categories": ["cs.RO"], "comment": "6 pages, 7 figures,", "summary": "Accurate localization is crucial for effectively operating mobile robots in\nindoor environments. This paper presents a comprehensive approach to mobile\nrobot localization by integrating an ultrasound-based indoor positioning system\n(IPS) with wheel odometry data via sensor fusion techniques. The fusion\nmethodology leverages the strengths of both IPS and wheel odometry,\ncompensating for the individual limitations of each method. The Extended Kalman\nFilter (EKF) fusion method combines the data from the IPS sensors and the\nrobot's wheel odometry, providing a robust and reliable localization solution.\nExtensive experiments in a controlled indoor environment reveal that the\nfusion-based localization system significantly enhances accuracy and precision\ncompared to standalone systems. The results demonstrate significant\nimprovements in trajectory tracking, with the EKF-based approach reducing\nerrors associated with wheel slippage and sensor noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d85\u58f0\u6ce2\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff08IPS\uff09\u548c\u8f66\u8f6e\u91cc\u7a0b\u8ba1\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u589e\u5f3a\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u72ec\u7acb\u7cfb\u7edf\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u51c6\u786e\u5b9a\u4f4d\u662f\u5176\u6709\u6548\u64cd\u4f5c\u7684\u5173\u952e\u3002\u5355\u72ec\u4f9d\u8d56IPS\u6216\u8f66\u8f6e\u91cc\u7a0b\u8ba1\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u878d\u5408\u65b9\u6cd5\u6765\u514b\u670d\u5404\u81ea\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528EKF\u878d\u5408IPS\u548c\u8f66\u8f6e\u91cc\u7a0b\u8ba1\u7684\u6570\u636e\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u5e76\u5f25\u8865\u5404\u81ea\u7684\u4e0d\u8db3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5408\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8f66\u8f6e\u6253\u6ed1\u548c\u4f20\u611f\u5668\u566a\u58f0\u5e26\u6765\u7684\u8bef\u5dee\u3002", "conclusion": "EKF\u878d\u5408\u65b9\u6cd5\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u9760\u7684\u5ba4\u5185\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22694", "pdf": "https://arxiv.org/pdf/2509.22694", "abs": "https://arxiv.org/abs/2509.22694", "authors": ["Rakha Rahmadani Pratama", "Catur Hilman A. H. B. Baskoro", "Joga Dharma Setiawan", "Dyah Kusuma Dewi", "P Paryanto", "Mochammad Ariyanto", "Roni Permana Saputra"], "title": "Nonlinear Model Predictive Control with Single-Shooting Method for Autonomous Personal Mobility Vehicle", "categories": ["cs.RO"], "comment": "15 pages, 3 figures, 4 tables", "summary": "This paper introduces a proposed control method for autonomous personal\nmobility vehicles, specifically the Single-passenger Electric Autonomous\nTransporter (SEATER), using Nonlinear Model Predictive Control (NMPC). The\nproposed method leverages a single-shooting approach to solve the optimal\ncontrol problem (OCP) via non-linear programming (NLP). The proposed NMPC is\nimplemented to a non-holonomic vehicle with a differential drive system, using\nodometry data as localization feedback to guide the vehicle towards its target\npose while achieving objectives and adhering to constraints, such as obstacle\navoidance. To evaluate the performance of the proposed method, a number of\nsimulations have been conducted in both obstacle-free and static obstacle\nenvironments. The SEATER model and testing environment have been developed in\nthe Gazebo Simulation and the NMPC are implemented within the Robot Operating\nSystem (ROS) framework. The simulation results demonstrate that the NMPC-based\napproach successfully controls the vehicle to reach the desired target location\nwhile satisfying the imposed constraints. Furthermore, this study highlights\nthe robustness and real-time effectiveness of NMPC with a single-shooting\napproach for autonomous vehicle control in the evaluated scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7684\u81ea\u4e3b\u4e2a\u4eba\u79fb\u52a8\u8f66\u8f86\uff08SEATER\uff09\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u5c04\u51fb\u65b9\u6cd5\u89e3\u51b3\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u5728Gazebo\u548cROS\u6846\u67b6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u4e2a\u4eba\u79fb\u52a8\u8f66\u8f86\uff08\u5982SEATER\uff09\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u6ee1\u8db3\u76ee\u6807\u5b9a\u4f4d\u548c\u907f\u969c\u7b49\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u548c\u5355\u5c04\u51fb\u65b9\u6cd5\uff0c\u7ed3\u5408\u5dee\u901f\u9a71\u52a8\u7cfb\u7edf\u548c\u91cc\u7a0b\u8ba1\u6570\u636e\u8fdb\u884c\u5b9a\u4f4d\u53cd\u9988\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236\u8f66\u8f86\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\uff0c\u5e76\u5728\u969c\u788d\u7269\u73af\u5883\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "NMPC\u7ed3\u5408\u5355\u5c04\u51fb\u65b9\u6cd5\u5728\u81ea\u4e3b\u8f66\u8f86\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.22695", "pdf": "https://arxiv.org/pdf/2509.22695", "abs": "https://arxiv.org/abs/2509.22695", "authors": ["Zhitao Wang", "Yanke Wang", "Jiangtao Wen", "Roberto Horowitz", "Yuxing Han"], "title": "ReSeFlow: Rectifying SE(3)-Equivariant Policy Learning Flows", "categories": ["cs.RO", "cs.CV"], "comment": "This work was submitted to 2026 IEEE International Conference on\n  Robotics & Automation", "summary": "Robotic manipulation in unstructured environments requires the generation of\nrobust and long-horizon trajectory-level policy with conditions of perceptual\nobservations and benefits from the advantages of SE(3)-equivariant diffusion\nmodels that are data-efficient. However, these models suffer from the inference\ntime costs. Inspired by the inference efficiency of rectified flows, we\nintroduce the rectification to the SE(3)-diffusion models and propose the\nReSeFlow, i.e., Rectifying SE(3)-Equivariant Policy Learning Flows, providing\nfast, geodesic-consistent, least-computational policy generation. Crucially,\nboth components employ SE(3)-equivariant networks to preserve rotational and\ntranslational symmetry, enabling robust generalization under rigid-body\nmotions. With the verification on the simulated benchmarks, we find that the\nproposed ReSeFlow with only one inference step can achieve better performance\nwith lower geodesic distance than the baseline methods, achieving up to a 48.5%\nerror reduction on the painting task and a 21.9% reduction on the rotating\ntriangle task compared to the baseline's 100-step inference. This method takes\nadvantages of both SE(3) equivariance and rectified flow and puts it forward\nfor the real-world application of generative policy learning models with the\ndata and inference efficiency.", "AI": {"tldr": "ReSeFlow\u7ed3\u5408SE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\u4e0e\u6574\u6d41\u6d41\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u6548\u7684\u8f68\u8ff9\u7b56\u7565\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\u548c\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3SE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63a8\u7406\u65f6\u95f4\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6574\u6d41\u6d41\u5230SE(3)-\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51faReSeFlow\uff0c\u5b9e\u73b0\u4e00\u6b65\u63a8\u7406\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u6b65\u63a8\u7406\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "ReSeFlow\u7ed3\u5408SE(3)-\u7b49\u53d8\u6027\u548c\u6574\u6d41\u6d41\uff0c\u4e3a\u751f\u6210\u7b56\u7565\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22698", "pdf": "https://arxiv.org/pdf/2509.22698", "abs": "https://arxiv.org/abs/2509.22698", "authors": ["Hailong Zhang", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments", "categories": ["cs.RO", "cs.AI"], "comment": "Main paper (15 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "Intelligent agents often require collaborative strategies to achieve complex\ntasks beyond individual capabilities in real-world scenarios. While existing\naudio-visual navigation (AVN) research mainly focuses on single-agent systems,\ntheir limitations emerge in dynamic 3D environments where rapid multi-agent\ncoordination is critical, especially for time-sensitive applications like\nemergency response. This paper introduces MASTAVN (Multi-Agent Scalable\nTransformer Audio-Visual Navigation), a scalable framework enabling two agents\nto collaboratively localize and navigate toward an audio target in shared 3D\nenvironments. By integrating cross-agent communication protocols and joint\naudio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal\nsynchronization. Through rigorous evaluation in photorealistic 3D simulators\n(Replica and Matterport3D), MASTAVN achieves significant reductions in task\ncompletion time and notable improvements in navigation success rates compared\nto single-agent and non-collaborative baselines. This highlights the essential\nrole of spatiotemporal coordination in multi-agent systems. Our findings\nvalidate MASTAVN's effectiveness in time-sensitive emergency scenarios and\nestablish a paradigm for advancing scalable multi-agent embodied intelligence\nin complex 3D environments.", "AI": {"tldr": "MASTAVN\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u548c\u8054\u5408\u97f3\u9891-\u89c6\u89c9\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u4e86\u57283D\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u5bfc\u822a\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f46\u5728\u52a8\u60013D\u73af\u5883\u4e2d\u591a\u4ee3\u7406\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5e94\u6025\u54cd\u5e94\u7b49\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faMASTAVN\u6846\u67b6\uff0c\u96c6\u6210\u4ea4\u53c9\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\u548c\u8054\u5408\u97f3\u9891-\u89c6\u89c9\u878d\u5408\u673a\u5236\uff0c\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u95f4\u540c\u6b65\u3002", "result": "\u5728Replica\u548cMatterport3D\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1\uff0cMASTAVN\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u5e76\u63d0\u9ad8\u5bfc\u822a\u6210\u529f\u7387\u3002", "conclusion": "MASTAVN\u5728\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u67423D\u73af\u5883\u4e2d\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u63d0\u4f9b\u4e86\u8303\u5f0f\u3002"}}
{"id": "2509.22716", "pdf": "https://arxiv.org/pdf/2509.22716", "abs": "https://arxiv.org/abs/2509.22716", "authors": ["Hung-Ying Chu", "Guan-Wei Chen", "Shao-Yu Wei", "Yu-Cheng Lin"], "title": "Large Language Models for 3D IC Space Planning", "categories": ["cs.RO"], "comment": "Accepted at AICCC 2025", "summary": "Three-dimensional integrated circuits (3D ICs) have emerged as a promising\nsolution to the scaling limits of two-dimensional designs, offering higher\nintegration density, shorter interconnects, and improved performance. As design\ncomplexity increases, effective space planning becomes essential to reduce dead\nspace and ensure layout quality. This study investigates the use of large\nlanguage models (LLMs) for 3D IC space planning through a post-order slicing\ntree representation, which guarantees legal space plans while aiming to\nminimize dead space. Open-source LLMs were fine-tuned on large-scale synthetic\ndatasets and further evaluated on MCNC-derived 3D benchmarks. Experimental\nresults indicate that the proposed framework achieves a favorable balance\nbetween runtime efficiency, legality, and dead-space reduction, with\nzero-dead-space layouts obtained in a significant portion of test cases under\npractical runtime budgets. Beyond synthetic benchmarks, the method generalizes\nto MCNC cases such as ami33 and ami49, though larger and irregular instances\nremain challenging. The approach also shows potential for cross-domain\napplications, including logistics and 3D object placement, where spatial\nefficiency is critical. Overall, the results suggest that LLM-based space\nplanning can serve as a data-driven complement to traditional electronic design\nautomation (EDA) methods, providing new insights for scalable 3D layout\ngeneration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e09\u7ef4\u96c6\u6210\u7535\u8def\uff083D ICs\uff09\u7a7a\u95f4\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u5e8f\u5207\u7247\u6811\u8868\u793a\u751f\u6210\u5408\u6cd5\u5e03\u5c40\u5e76\u51cf\u5c11\u6b7b\u533a\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u3001\u5408\u6cd5\u6027\u548c\u6b7b\u533a\u51cf\u5c11\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u968f\u7740\u4e09\u7ef4\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u6709\u6548\u7684\u7a7a\u95f4\u89c4\u5212\u5bf9\u4e8e\u51cf\u5c11\u6b7b\u533a\u548c\u786e\u4fdd\u5e03\u5c40\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5f00\u6e90LLMs\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u540e\u5e8f\u5207\u7247\u6811\u8868\u793a\u751f\u6210\u5408\u6cd5\u7a7a\u95f4\u89c4\u5212\u3002", "result": "\u5728MCNC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6b7b\u533a\u51cf\u5c11\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u90e8\u5206\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6b7b\u533a\u5e03\u5c40\u3002", "conclusion": "LLM-based\u7684\u7a7a\u95f4\u89c4\u5212\u53ef\u4f5c\u4e3a\u4f20\u7edfEDA\u65b9\u6cd5\u7684\u6570\u636e\u9a71\u52a8\u8865\u5145\uff0c\u4e3a\u53ef\u6269\u5c55\u76843D\u5e03\u5c40\u751f\u6210\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2509.22754", "pdf": "https://arxiv.org/pdf/2509.22754", "abs": "https://arxiv.org/abs/2509.22754", "authors": ["Merve Atasever", "Zhuochen Liu", "Qingpei Li", "Akshay Hitendra Shah", "Hans Walker", "Jyotirmoy V. Deshmukh", "Rahul Jain"], "title": "Self-driving cars: Are we there yet?", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Autonomous driving remains a highly active research domain that seeks to\nenable vehicles to perceive dynamic environments, predict the future\ntrajectories of traffic agents such as vehicles, pedestrians, and cyclists and\nplan safe and efficient future motions. To advance the field, several\ncompetitive platforms and benchmarks have been established to provide\nstandardized datasets and evaluation protocols. Among these, leaderboards by\nthe CARLA organization and nuPlan and the Waymo Open Dataset have become\nleading benchmarks for assessing motion planning algorithms. Each offers a\nunique dataset and challenging planning problems spanning a wide range of\ndriving scenarios and conditions. In this study, we present a comprehensive\ncomparative analysis of the motion planning methods featured on these three\nleaderboards. To ensure a fair and unified evaluation, we adopt CARLA\nleaderboard v2.0 as our common evaluation platform and modify the selected\nmodels for compatibility. By highlighting the strengths and weaknesses of\ncurrent approaches, we identify prevailing trends, common challenges, and\nsuggest potential directions for advancing motion planning research.", "AI": {"tldr": "\u8bba\u6587\u5bf9CARLA\u3001nuPlan\u548cWaymo Open Dataset\u4e09\u5927\u5e73\u53f0\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u4f7f\u7528CARLA leaderboard v2.0\u4f5c\u4e3a\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u73b0\u6709\u5e73\u53f0\u5982CARLA\u3001nuPlan\u548cWaymo Open Dataset\u5df2\u6210\u4e3a\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u4e3b\u8981\u57fa\u51c6\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6bd4\u8f83\u5206\u6790\u3002", "method": "\u91c7\u7528CARLA leaderboard v2.0\u4f5c\u4e3a\u7edf\u4e00\u8bc4\u4f30\u5e73\u53f0\uff0c\u5bf9\u4e09\u5927\u5e73\u53f0\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u8fdb\u884c\u8c03\u6574\u4ee5\u786e\u4fdd\u517c\u5bb9\u6027\uff0c\u5e76\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u603b\u7ed3\u4e86\u4e3b\u8981\u8d8b\u52bf\u548c\u5171\u540c\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22756", "pdf": "https://arxiv.org/pdf/2509.22756", "abs": "https://arxiv.org/abs/2509.22756", "authors": ["Shiyi Liang", "Xinyuan Chang", "Changjie Wu", "Huiyuan Yan", "Yifan Bai", "Xinran Liu", "Hang Zhang", "Yujian Yuan", "Shuang Zeng", "Mu Xu", "Xing Wei"], "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.", "AI": {"tldr": "PAMR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u8f66\u9053\u51e0\u4f55\u548c\u4ea4\u901a\u89c4\u5219\u7684\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u9a7e\u9a76\u4e2d\u89c4\u5219\u6301\u7eed\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u65f6\u95f4\u9a7e\u9a76\u4e2d\u7684\u4ea4\u901a\u89c4\u5219\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u89c4\u5219\u7684\u6301\u7eed\u6027\uff0c\u5bfc\u81f4\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u4e0d\u8db3\u3002", "method": "PAMR\u901a\u8fc7Map-Rule Co-Construction\u548cMap-Rule Cache\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8f66\u9053\u51e0\u4f55\u4e0e\u4ea4\u901a\u89c4\u5219\u7684\u81ea\u56de\u5f52\u534f\u540c\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPAMR\u5728\u8054\u5408\u5411\u91cf-\u89c4\u5219\u6620\u5c04\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u957f\u5e8f\u5217\u9a7e\u9a76\u4e2d\u4fdd\u6301\u4e86\u89c4\u5219\u7684\u6301\u7eed\u6027\u3002", "conclusion": "PAMR\u4e3a\u9ad8\u7cbe\u5ea6\u5730\u56fe\u6784\u5efa\u548c\u4ea4\u901a\u89c4\u5219\u6301\u7eed\u6027\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22801", "pdf": "https://arxiv.org/pdf/2509.22801", "abs": "https://arxiv.org/abs/2509.22801", "authors": ["Huajing Zhao", "Brian Flynn", "Adam Norton", "Holly Yanco"], "title": "Towards Developing Standards and Guidelines for Robot Grasping and Manipulation Pipelines in the COMPARE Ecosystem", "categories": ["cs.RO"], "comment": "The 3rd AAAI Fall Symposium on Unifying Representations for Robot\n  Application Development (UR-RAD), Arlington, VA, USA, November 2025", "summary": "The COMPARE Ecosystem aims to improve the compatibility and benchmarking of\nopen-source products for robot manipulation through a series of activities. One\nsuch activity is the development of standards and guidelines to specify\nmodularization practices at the component-level for individual modules (e.g.,\nperception, grasp planning, motion planning) and integrations of components\nthat form robot manipulation capabilities at the pipeline-level. This paper\nbriefly reviews our work-in-progress to date to (1) build repositories of\nopen-source products to identify common characteristics of each component in\nthe pipeline, (2) investigate existing modular pipelines to glean best\npractices, and (3) develop new modular pipelines that advance prior work while\nabiding by the proposed standards and guidelines.", "AI": {"tldr": "COMPARE\u751f\u6001\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u4e00\u7cfb\u5217\u6d3b\u52a8\u63d0\u5347\u5f00\u6e90\u673a\u5668\u4eba\u64cd\u4f5c\u4ea7\u54c1\u7684\u517c\u5bb9\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\uff0c\u5305\u62ec\u7ec4\u4ef6\u7ea7\u522b\u548c\u7ba1\u9053\u7ea7\u522b\u7684\u6a21\u5757\u5316\u6807\u51c6\u548c\u6307\u5357\u5f00\u53d1\u3002", "motivation": "\u63d0\u9ad8\u5f00\u6e90\u673a\u5668\u4eba\u64cd\u4f5c\u4ea7\u54c1\u7684\u517c\u5bb9\u6027\u548c\u6027\u80fd\u8bc4\u4f30\u6807\u51c6\u5316\u3002", "method": "\u5f00\u53d1\u6807\u51c6\u548c\u6307\u5357\uff0c\u6784\u5efa\u5f00\u6e90\u4ea7\u54c1\u5e93\uff0c\u7814\u7a76\u73b0\u6709\u6a21\u5757\u5316\u7ba1\u9053\u4ee5\u603b\u7ed3\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u5f00\u53d1\u65b0\u7684\u6a21\u5757\u5316\u7ba1\u9053\u3002", "result": "\u5de5\u4f5c\u8fdb\u5c55\u5305\u62ec\u8bc6\u522b\u7ba1\u9053\u4e2d\u5404\u7ec4\u4ef6\u7684\u5171\u540c\u7279\u5f81\u3001\u603b\u7ed3\u6700\u4f73\u5b9e\u8df5\u4ee5\u53ca\u5f00\u53d1\u7b26\u5408\u6807\u51c6\u7684\u65b0\u7ba1\u9053\u3002", "conclusion": "COMPARE\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u6a21\u5757\u5316\u6807\u51c6\u548c\u6307\u5357\u7684\u5f00\u53d1\u548c\u5b9e\u8df5\u5e94\u7528\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u5f00\u6e90\u4ea7\u54c1\u7684\u517c\u5bb9\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.22795", "pdf": "https://arxiv.org/pdf/2509.22795", "abs": "https://arxiv.org/abs/2509.22795", "authors": ["Yi Hu", "Zheyuan Cheng"], "title": "Generative Modeling and Decision Fusion for Unknown Event Detection and Classification Using Synchrophasor Data", "categories": ["eess.SP", "cs.AI", "cs.SY", "eess.SY"], "comment": "10 pages", "summary": "Reliable detection and classification of power system events are critical for\nmaintaining grid stability and situational awareness. Existing approaches often\ndepend on limited labeled datasets, which restricts their ability to generalize\nto rare or unseen disturbances. This paper proposes a novel framework that\nintegrates generative modeling, sliding-window temporal processing, and\ndecision fusion to achieve robust event detection and classification using\nsynchrophasor data. A variational autoencoder-generative adversarial network is\nemployed to model normal operating conditions, where both reconstruction error\nand discriminator error are extracted as anomaly indicators. Two complementary\ndecision strategies are developed: a threshold-based rule for computational\nefficiency and a convex hull-based method for robustness under complex error\ndistributions. These features are organized into spatiotemporal detection and\nclassification matrices through a sliding-window mechanism, and an\nidentification and decision fusion stage integrates the outputs across PMUs.\nThis design enables the framework to identify known events while systematically\nclassifying previously unseen disturbances into a new category, addressing a\nkey limitation of supervised classifiers. Experimental results demonstrate\nstate-of-the-art accuracy, surpassing machine learning, deep learning, and\nenvelope-based baselines. The ability to recognize unknown events further\nhighlights the adaptability and practical value of the proposed approach for\nwide-area event analysis in modern power systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5efa\u6a21\u3001\u6ed1\u52a8\u7a97\u53e3\u5904\u7406\u548c\u51b3\u7b56\u878d\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u529b\u7cfb\u7edf\u4e2d\u9c81\u68d2\u7684\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u80fd\u591f\u5728\u8bc6\u522b\u5df2\u77e5\u4e8b\u4ef6\u7684\u540c\u65f6\u5206\u7c7b\u672a\u77e5\u6270\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u7f55\u89c1\u6216\u672a\u89c1\u8fc7\u7684\u6270\u52a8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668-\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5efa\u6a21\u6b63\u5e38\u64cd\u4f5c\u6761\u4ef6\uff0c\u63d0\u53d6\u91cd\u6784\u8bef\u5dee\u548c\u5224\u522b\u8bef\u5dee\u4f5c\u4e3a\u5f02\u5e38\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u9608\u503c\u89c4\u5219\u548c\u51f8\u5305\u65b9\u6cd5\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u80fd\u8bc6\u522b\u672a\u77e5\u4e8b\u4ef6\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u76d1\u7763\u5206\u7c7b\u5668\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u7684\u5e7f\u57df\u4e8b\u4ef6\u5206\u6790\u3002"}}
{"id": "2509.22815", "pdf": "https://arxiv.org/pdf/2509.22815", "abs": "https://arxiv.org/abs/2509.22815", "authors": ["Ruturaj Sambhus", "Muneeb Ahmad", "Basit Muhammad Imran", "Sujith Vijayan", "Dylan P. Losey", "Kaveh Akbari Hamed"], "title": "Teleoperator-Aware and Safety-Critical Adaptive Nonlinear MPC for Shared Autonomy in Obstacle Avoidance of Legged Robots", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "Ensuring safe and effective collaboration between humans and autonomous\nlegged robots is a fundamental challenge in shared autonomy, particularly for\nteleoperated systems navigating cluttered environments. Conventional\nshared-control approaches often rely on fixed blending strategies that fail to\ncapture the dynamics of legged locomotion and may compromise safety. This paper\npresents a teleoperator-aware, safety-critical, adaptive nonlinear model\npredictive control (ANMPC) framework for shared autonomy of quadrupedal robots\nin obstacle-avoidance tasks. The framework employs a fixed arbitration weight\nbetween human and robot actions but enhances this scheme by modeling the human\ninput with a noisily rational Boltzmann model, whose parameters are adapted\nonline using a projected gradient descent (PGD) law from observed joystick\ncommands. Safety is enforced through control barrier function (CBF) constraints\nintegrated into a computationally efficient NMPC, ensuring forward invariance\nof safe sets despite uncertainty in human behavior. The control architecture is\nhierarchical: a high-level CBF-based ANMPC (10 Hz) generates blended\nhuman-robot velocity references, a mid-level dynamics-aware NMPC (60 Hz)\nenforces reduced-order single rigid body (SRB) dynamics to track these\nreferences, and a low-level nonlinear whole-body controller (500 Hz) imposes\nthe full-order dynamics via quadratic programming to track the mid-level\ntrajectories. Extensive numerical and hardware experiments, together with a\nuser study, on a Unitree Go2 quadrupedal robot validate the framework,\ndemonstrating real-time obstacle avoidance, online learning of human intent\nparameters, and safe teleoperator collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08ANMPC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u907f\u969c\u4efb\u52a1\u4e2d\u7684\u5171\u4eab\u81ea\u4e3b\u6743\uff0c\u7ed3\u5408\u4eba\u7c7b\u610f\u56fe\u5efa\u6a21\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u4e0e\u81ea\u4e3b\u56db\u8db3\u673a\u5668\u4eba\u5728\u5171\u4eab\u81ea\u4e3b\u6743\u4e2d\u7684\u5b89\u5168\u548c\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6742\u4e71\u73af\u5883\u4e2d\u56fa\u5b9a\u6df7\u5408\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u6b65\u6001\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408Boltzmann\u6a21\u578b\u5728\u7ebf\u5b66\u4e60\u4eba\u7c7b\u610f\u56fe\uff0c\u5e76\u901a\u8fc7CBF\u7ea6\u675f\u96c6\u6210\u5230NMPC\u4e2d\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u548c\u9ad8\u6548\u6027\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u3001\u786c\u4ef6\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u907f\u969c\u548c\u5b89\u5168\u7684\u8fdc\u7a0b\u534f\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684ANMPC\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.22810", "pdf": "https://arxiv.org/pdf/2509.22810", "abs": "https://arxiv.org/abs/2509.22810", "authors": ["Jianheng Zhou", "Chenyu Liu", "Jinan Zhou", "Yi Ding", "Yang Liu", "Haoran Luo", "Ziyu Jia", "Xinliang Zhou"], "title": "Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Sleep staging is essential for diagnosing sleep disorders and assessing\nneurological health. Existing automatic methods typically extract features from\ncomplex polysomnography (PSG) signals and train domain-specific models, which\noften lack intuitiveness and require large, specialized datasets. To overcome\nthese limitations, we introduce a new paradigm for sleep staging that leverages\nlarge multimodal general-purpose models to emulate clinical diagnostic\npractices. Specifically, we convert raw one-dimensional PSG time-series into\nintuitive two-dimensional waveform images and then fine-tune a multimodal large\nmodel to learn from these representations. Experiments on three public datasets\n(ISRUC, MASS, SHHS) demonstrate that our approach enables general-purpose\nmodels, without prior exposure to sleep data, to acquire robust staging\ncapabilities. Moreover, explanation analysis reveals our model learned to mimic\nthe visual diagnostic workflow of human experts for sleep staging by PSG\nimages. The proposed method consistently outperforms state-of-the-art baselines\nin accuracy and robustness, highlighting its efficiency and practical value for\nmedical applications. The code for the signal-to-image pipeline and the PSG\nimage dataset will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u901a\u7528\u6a21\u578b\u7684\u65b0\u7761\u7720\u5206\u671f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06PSG\u4fe1\u53f7\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u6ce2\u5f62\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7761\u7720\u5206\u671f\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742PSG\u4fe1\u53f7\u548c\u9886\u57df\u4e13\u7528\u6a21\u578b\uff0c\u7f3a\u4e4f\u76f4\u89c2\u6027\u4e14\u9700\u8981\u5927\u91cf\u6570\u636e\u3002\u65b0\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u901a\u7528\u6a21\u578b\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u6d41\u7a0b\uff0c\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06\u4e00\u7ef4PSG\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u76f4\u89c2\u7684\u4e8c\u7ef4\u6ce2\u5f62\u56fe\u50cf\uff0c\u5e76\u5fae\u8c03\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u672a\u9884\u5148\u63a5\u89e6\u7761\u7720\u6570\u636e\u3002\u6a21\u578b\u80fd\u6a21\u4eff\u4e13\u5bb6\u7684\u89c6\u89c9\u8bca\u65ad\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u5c06\u4e3a\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4fbf\u5229\u3002"}}
{"id": "2509.22825", "pdf": "https://arxiv.org/pdf/2509.22825", "abs": "https://arxiv.org/abs/2509.22825", "authors": ["Philip Sanderink", "Yingfan Zhou", "Shuzhen Luo", "Cheng Fang"], "title": "Parameter Identification of a Differentiable Human Arm Musculoskeletal Model without Deep Muscle EMG Reconstruction", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Accurate parameter identification of a subject-specific human musculoskeletal\nmodel is crucial to the development of safe and reliable physically\ncollaborative robotic systems, for instance, assistive exoskeletons.\nElectromyography (EMG)-based parameter identification methods have demonstrated\npromising performance for personalized musculoskeletal modeling, whereas their\napplicability is limited by the difficulty of measuring deep muscle EMGs\ninvasively. Although several strategies have been proposed to reconstruct deep\nmuscle EMGs or activations for parameter identification, their reliability and\nrobustness are limited by assumptions about the deep muscle behavior. In this\nwork, we proposed an approach to simultaneously identify the bone and\nsuperficial muscle parameters of a human arm musculoskeletal model without\nreconstructing the deep muscle EMGs. This is achieved by only using the\nleast-squares solution of the deep muscle forces to calculate a loss gradient\nwith respect to the model parameters for identifying them in a framework of\ndifferentiable optimization. The results of extensive comparative simulations\nmanifested that our proposed method can achieve comparable estimation accuracy\ncompared to a similar method, but with all the muscle EMGs available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u5efa\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u7684\u4eba\u7c7b\u624b\u81c2\u9aa8\u9abc\u808c\u8089\u6a21\u578b\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u89e3\u8ba1\u7b97\u635f\u5931\u68af\u5ea6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4e2a\u6027\u5316\u9aa8\u9abc\u808c\u8089\u6a21\u578b\u7684\u53c2\u6570\u8bc6\u522b\u5bf9\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u5982\u8f85\u52a9\u5916\u9aa8\u9abc\uff09\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709EMG-based\u65b9\u6cd5\u56e0\u96be\u4ee5\u6d4b\u91cf\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u800c\u53d7\u9650\u3002", "method": "\u4ec5\u4f7f\u7528\u8868\u5c42\u808c\u8089EMG\u4fe1\u53f7\u548c\u6df1\u5c42\u808c\u8089\u529b\u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\uff0c\u901a\u8fc7\u53ef\u5fae\u4f18\u5316\u6846\u67b6\u8bc6\u522b\u6a21\u578b\u53c2\u6570\uff0c\u907f\u514d\u4e86\u6df1\u5c42\u808c\u8089\u884c\u4e3a\u7684\u5047\u8bbe\u3002", "result": "\u5e7f\u6cdb\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u6df1\u5c42\u808c\u8089EMG\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0e\u5168\u808c\u8089EMG\u4fe1\u53f7\u65b9\u6cd5\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u65e0\u521b\u53c2\u6570\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u9760\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.22869", "pdf": "https://arxiv.org/pdf/2509.22869", "abs": "https://arxiv.org/abs/2509.22869", "authors": ["Abdulkadir Bilge", "Erdem Ergen", "Burak Soner", "Sinem Coleri"], "title": "Scalable Wi-Fi RSS-Based Indoor Localization via Automatic Vision-Assisted Calibration", "categories": ["eess.SP", "cs.AI"], "comment": "Presented at the ICAT 2025 conference, Sarajevo, September 2025. See\n  https://icat.etf.unsa.ba/2025/", "summary": "Wi-Fi-based positioning promises a scalable and privacy-preserving solution\nfor location-based services in indoor environments such as malls, airports, and\ncampuses. RSS-based methods are widely deployable as RSS data is available on\nall Wi-Fi-capable devices, but RSS is highly sensitive to multipath, channel\nvariations, and receiver characteristics. While supervised learning methods\noffer improved robustness, they require large amounts of labeled data, which is\noften costly to obtain. We introduce a lightweight framework that solves this\nby automating high-resolution synchronized RSS-location data collection using a\nshort, camera-assisted calibration phase. An overhead camera is calibrated only\nonce with ArUco markers and then tracks a device collecting RSS data from\nbroadcast packets of nearby access points across Wi-Fi channels. The resulting\n(x, y, RSS) dataset is used to automatically train mobile-deployable\nlocalization algorithms, avoiding the privacy concerns of continuous video\nmonitoring. We quantify the accuracy limits of such vision-assisted RSS data\ncollection under key factors such as tracking precision and label\nsynchronization. Using the collected experimental data, we benchmark\ntraditional and supervised learning approaches under varying signal conditions\nand device types, demonstrating improved accuracy and generalization,\nvalidating the utility of the proposed framework for practical use. All code,\ntools, and datasets are released as open source.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u77ed\u65f6\u95f4\u6444\u50cf\u5934\u8f85\u52a9\u6821\u51c6\u81ea\u52a8\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u540c\u6b65RSS-\u4f4d\u7f6e\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5ba4\u5185Wi-Fi\u5b9a\u4f4d\u4e2d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "Wi-Fi\u57fa\u4e8eRSS\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5e7f\u6cdb\u53ef\u7528\uff0c\u4f46RSS\u5bf9\u591a\u5f84\u3001\u4fe1\u9053\u53d8\u5316\u548c\u63a5\u6536\u5668\u7279\u6027\u654f\u611f\u3002\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u6444\u50cf\u5934\u8f85\u52a9\u6821\u51c6\u9636\u6bb5\u81ea\u52a8\u6536\u96c6RSS-\u4f4d\u7f6e\u6570\u636e\u3002\u901a\u8fc7ArUco\u6807\u8bb0\u6821\u51c6\u6444\u50cf\u5934\u5e76\u8ddf\u8e2a\u8bbe\u5907\uff0c\u751f\u6210\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u5b9a\u4f4d\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u4fe1\u53f7\u6761\u4ef6\u548c\u8bbe\u5907\u7c7b\u578b\u4e0b\u7684\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5ba4\u5185Wi-Fi\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9690\u79c1\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22828", "pdf": "https://arxiv.org/pdf/2509.22828", "abs": "https://arxiv.org/abs/2509.22828", "authors": ["Arman Barghi", "Hamed Hosseini", "Seraj Ghasemi", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Dynamic Buffers: Cost-Efficient Planning for Tabletop Rearrangement with Stacking", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "comment": null, "summary": "Rearranging objects in cluttered tabletop environments remains a\nlong-standing challenge in robotics. Classical planners often generate\ninefficient, high-cost plans by shuffling objects individually and using fixed\nbuffers--temporary spaces such as empty table regions or static stacks--to\nresolve conflicts. When only free table locations are used as buffers, dense\nscenes become inefficient, since placing an object can restrict others from\nreaching their goals and complicate planning. Allowing stacking provides extra\nbuffer capacity, but conventional stacking is static: once an object supports\nanother, the base cannot be moved, which limits efficiency. To overcome these\nissues, a novel planning primitive called the Dynamic Buffer is introduced.\nInspired by human grouping strategies, it enables robots to form temporary,\nmovable stacks that can be transported as a unit. This improves both\nfeasibility and efficiency in dense layouts, and it also reduces travel in\nlarge-scale settings where space is abundant. Compared with a state-of-the-art\nrearrangement planner, the approach reduces manipulator travel cost by 11.89%\nin dense scenarios with a stationary robot and by 5.69% in large, low-density\nsettings with a mobile manipulator. Practicality is validated through\nexperiments on a Delta parallel robot with a two-finger gripper. These findings\nestablish dynamic buffering as a key primitive for cost-efficient and robust\nrearrangement planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u52a8\u6001\u7f13\u51b2\u533a\u201d\u7684\u65b0\u89c4\u5212\u539f\u8bed\uff0c\u7528\u4e8e\u89e3\u51b3\u5bc6\u96c6\u684c\u9762\u73af\u5883\u4e2d\u7269\u4f53\u91cd\u65b0\u6392\u5217\u7684\u4f4e\u6548\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5141\u8bb8\u4e34\u65f6\u3001\u53ef\u79fb\u52a8\u7684\u5806\u53e0\u6765\u63d0\u5347\u89c4\u5212\u6548\u7387\u548c\u53ef\u884c\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u4f53\u91cd\u65b0\u6392\u5217\u89c4\u5212\u5668\u5728\u5904\u7406\u5bc6\u96c6\u73af\u5883\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4f7f\u7528\u56fa\u5b9a\u7f13\u51b2\u533a\uff08\u5982\u9759\u6001\u5806\u53e0\u6216\u7a7a\u95f2\u684c\u9762\u533a\u57df\uff09\u800c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u52a8\u6001\u7f13\u51b2\u533a\u65b9\u6cd5\u3002", "method": "\u52a8\u6001\u7f13\u51b2\u533a\u53d7\u4eba\u7c7b\u5206\u7ec4\u7b56\u7565\u542f\u53d1\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u5f62\u6210\u4e34\u65f6\u3001\u53ef\u79fb\u52a8\u7684\u5806\u53e0\u5355\u5143\uff0c\u4ece\u800c\u63d0\u5347\u89c4\u5212\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u79fb\u52a8\u673a\u5668\u4eba\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u89c4\u5212\u5668\uff0c\u52a8\u6001\u7f13\u51b2\u533a\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u51cf\u5c11\u4e8611.89%\u7684\u673a\u68b0\u81c2\u79fb\u52a8\u6210\u672c\uff0c\u5728\u4f4e\u5bc6\u5ea6\u73af\u5883\u4e2d\u51cf\u5c11\u4e865.69%\u3002\u5b9e\u9a8c\u901a\u8fc7\u5728Delta\u5e76\u884c\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u52a8\u6001\u7f13\u51b2\u533a\u88ab\u786e\u7acb\u4e3a\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u91cd\u65b0\u6392\u5217\u89c4\u5212\u7684\u5173\u952e\u539f\u8bed\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2509.22891", "pdf": "https://arxiv.org/pdf/2509.22891", "abs": "https://arxiv.org/abs/2509.22891", "authors": ["Ashwini Kulkarni", "Santosh Nannuru"], "title": "Time-Frequency Analysis of Non-Uniformly Sampled Signals via Sample Density Adaptation", "categories": ["eess.SP", "astro-ph.IM"], "comment": null, "summary": "The analysis of non-stationary signals in non-uniformly sampled data is a\nchallenging task. Time-integrated methods, such as the generalised Lomb-Scargle\n(GLS) periodogram, provide a robust statistical assessment of persistent\nperiodicities but are insensitive to transient events. Conversely, existing\ntime-frequency methods often rely on fixed-duration windows or interpolation,\nwhich can be suboptimal for non-uniform data. We introduce the non-uniform\nStockwell-transform (NUST), a time-frequency framework that applies a localized\ndensity adaptive spectral analysis directly to non-uniformly sampled data. NUST\nemploys a doubly adaptive window that adjusts its width based on both frequency\nand local data density, providing detailed time-frequency information for both\ntransient and persistent signals. We validate the NUST on numerous\nnon-uniformly sampled synthetic signals, demonstrating its superior\ntime-localization performance compared to GLS. Furthermore, we apply NUST to\nHARPS radial velocity data of the multi-planetary system HD 10180, successfully\ndistinguishing coherent planetary signals from stellar activity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u975e\u5747\u5300\u91c7\u6837\u6570\u636e\u7684\u65f6\u9891\u5206\u6790\u65b9\u6cd5NUST\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u975e\u5747\u5300\u91c7\u6837\u6570\u636e\u4e2d\u975e\u5e73\u7a33\u4fe1\u53f7\u5206\u6790\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u77ac\u6001\u4e8b\u4ef6\u7684\u68c0\u6d4b\u3002", "method": "\u5f15\u5165\u975e\u5747\u5300Stockwell\u53d8\u6362(NUST)\uff0c\u91c7\u7528\u53cc\u91cd\u81ea\u9002\u5e94\u7a97\u53e3\uff0c\u76f4\u63a5\u5206\u6790\u975e\u5747\u5300\u6570\u636e\u3002", "result": "\u5728\u5408\u6210\u4fe1\u53f7\u548cHD 10180\u884c\u661f\u7cfb\u7edf\u6570\u636e\u4e2d\uff0cNUST\u8868\u73b0\u51fa\u4f18\u4e8eGLS\u7684\u6027\u80fd\u3002", "conclusion": "NUST\u4e3a\u975e\u5747\u5300\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u65f6\u9891\u5206\u6790\u5de5\u5177\uff0c\u80fd\u533a\u5206\u884c\u661f\u4fe1\u53f7\u4e0e\u6052\u661f\u6d3b\u52a8\u3002"}}
{"id": "2509.22847", "pdf": "https://arxiv.org/pdf/2509.22847", "abs": "https://arxiv.org/abs/2509.22847", "authors": ["Brandon Vu", "Shameek Ganguly", "Pushkar Joshi"], "title": "Empart: Interactive Convex Decomposition for Converting Meshes to Parts", "categories": ["cs.RO"], "comment": null, "summary": "Simplifying complex 3D meshes is a crucial step in robotics applications to\nenable efficient motion planning and physics simulation. Common methods, such\nas approximate convex decomposition, represent a mesh as a collection of simple\nparts, which are computationally inexpensive to simulate. However, existing\napproaches apply a uniform error tolerance across the entire mesh, which can\nresult in a sub-optimal trade-off between accuracy and performance. For\ninstance, a robot grasping an object needs high-fidelity geometry in the\nvicinity of the contact surfaces but can tolerate a coarser simplification\nelsewhere. A uniform tolerance can lead to excessive detail in non-critical\nareas or insufficient detail where it's needed most.\n  To address this limitation, we introduce Empart, an interactive tool that\nallows users to specify different simplification tolerances for selected\nregions of a mesh. Our method leverages existing convex decomposition\nalgorithms as a sub-routine but uses a novel, parallelized framework to handle\nregion-specific constraints efficiently. Empart provides a user-friendly\ninterface with visual feedback on approximation error and simulation\nperformance, enabling designers to iteratively refine their decomposition. We\ndemonstrate that our approach significantly reduces the number of convex parts\ncompared to a state-of-the-art method (V-HACD) at a fixed error threshold,\nleading to substantial speedups in simulation performance. For a robotic\npick-and-place task, Empart-generated collision meshes reduced the overall\nsimulation time by 69% compared to a uniform decomposition, highlighting the\nvalue of interactive, region-specific simplification for performant robotics\napplications.", "AI": {"tldr": "Empart \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u4e3a\u7f51\u683c\u7684\u4e0d\u540c\u533a\u57df\u6307\u5b9a\u4e0d\u540c\u7684\u7b80\u5316\u5bb9\u5fcd\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u51f8\u90e8\u5206\u7684\u6570\u91cf\u5e76\u63d0\u5347\u4e86\u4eff\u771f\u6027\u80fd\u3002", "motivation": "\u7b80\u5316\u590d\u67423D\u7f51\u683c\u5bf9\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7684\u8bef\u5dee\u5bb9\u5fcd\u5ea6\uff0c\u5bfc\u81f4\u5728\u975e\u5173\u952e\u533a\u57df\u8fc7\u5ea6\u7ec6\u8282\u5316\u6216\u5173\u952e\u533a\u57df\u7ec6\u8282\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u73b0\u6709\u51f8\u5206\u89e3\u7b97\u6cd5\u4f5c\u4e3a\u5b50\u7a0b\u5e8f\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u6846\u67b6\u5904\u7406\u533a\u57df\u7279\u5b9a\u7ea6\u675f\uff0c\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u754c\u9762\u548c\u53ef\u89c6\u5316\u53cd\u9988\u3002", "result": "\u5728\u56fa\u5b9a\u8bef\u5dee\u9608\u503c\u4e0b\uff0c\u76f8\u6bd4V-HACD\u65b9\u6cd5\uff0cEmpart\u663e\u8457\u51cf\u5c11\u4e86\u51f8\u90e8\u5206\u6570\u91cf\uff0c\u5e76\u5c06\u4eff\u771f\u65f6\u95f4\u51cf\u5c11\u4e8669%\u3002", "conclusion": "Empart\u5c55\u793a\u4e86\u4ea4\u4e92\u5f0f\u3001\u533a\u57df\u7279\u5b9a\u7684\u7f51\u683c\u7b80\u5316\u5728\u9ad8\u6548\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.23065", "pdf": "https://arxiv.org/pdf/2509.23065", "abs": "https://arxiv.org/abs/2509.23065", "authors": ["Mohammad Amin Saeidi", "Hina Tabassum"], "title": "Resource Allocation in Cooperative Mid-band/THz Networks in the Presence of Mobility", "categories": ["eess.SP", "math.CV"], "comment": "This paper has been accepted for publication in IEEE journals", "summary": "This paper develops a comprehensive framework to investigate and optimize the\ndownlink performance of cooperative multi-band networks (MBNs) operating on\nupper mid-band (UMB) and terahertz (THz) frequencies, where base stations (BSs)\nin each band cooperatively serve users. The framework captures sophisticated\nfeatures such as near-field channel modeling, fully and partially connected\nantenna architectures, and users' mobility. First, we consider joint user\nassociation and hybrid beamforming optimization to maximize the system\nsum-rate, subject to power constraints, maximum cluster size of cooperating\nBSs, and users' quality-of-service (QoS) constraints. By leveraging fractional\nprogramming FP and majorization-minimization techniques, an iterative algorithm\nis proposed to solve the non-convex optimization problem. We then consider\nhandover (HO)-aware resource allocation for moving users in a cooperative\nUMB/THz MBN. Two HO-aware resource allocation methods are proposed. The first\nmethod focuses on maximizing the HO-aware system sum-rate subject to HO-aware\nQoS constraints. Using Jensen's inequality and properties of logarithmic\nfunctions, the non-convex optimization problem is tightly approximated with a\nconvex one and solved. The second method addresses a multi-objective\noptimization problem to maximize the system sum-rate, while minimizing the\ntotal number of HOs. Numerical results demonstrate the efficacy of the proposed\nalgorithms, cooperative UMB/THz MBN over stand-alone THz networks, as well as\nthe critical importance of accurate near-field modeling in extremely large\nantenna arrays. Moreover, the proposed HO-aware resource allocation methods\neffectively mitigate the impact of HOs, enhancing performance in the considered\nsystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u4f18\u5316\u534f\u4f5c\u591a\u9891\u6bb5\u7f51\u7edc\uff08MBN\uff09\u7684\u4e0b\u884c\u6027\u80fd\uff0c\u6db5\u76d6\u8054\u5408\u7528\u6237\u5173\u8054\u3001\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u548c\u5207\u6362\u611f\u77e5\u8d44\u6e90\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u534f\u4f5c\u591a\u9891\u6bb5\u7f51\u7edc\u5728UMB\u548cTHz\u9891\u6bb5\u4e2d\u7684\u6027\u80fd\u4f18\u5316\uff0c\u5c24\u5176\u662f\u7528\u6237\u79fb\u52a8\u6027\u548c\u8fd1\u573a\u4fe1\u9053\u6a21\u578b\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u5206\u6570\u89c4\u5212\u548c\u4e3b\u4f18\u5316\u6280\u672f\u63d0\u51fa\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u79fb\u52a8\u7528\u6237\u8bbe\u8ba1\u5207\u6362\u611f\u77e5\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u603b\u901f\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u5207\u6362\u6b21\u6570\uff0c\u534f\u4f5cUMB/THz\u7f51\u7edc\u4f18\u4e8e\u72ec\u7acbTHz\u7f51\u7edc\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u7b97\u6cd5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5f3a\u8c03\u4e86\u8fd1\u573a\u5efa\u6a21\u548c\u5207\u6362\u7ba1\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22883", "pdf": "https://arxiv.org/pdf/2509.22883", "abs": "https://arxiv.org/abs/2509.22883", "authors": ["Kaleb Ben Naveed", "Haejoon Lee", "Dimitra Panagou"], "title": "Multi-Robot Allocation for Information Gathering in Non-Uniform Spatiotemporal Environments", "categories": ["cs.RO"], "comment": "Submitted to American Control Conference (ACC) 2026", "summary": "Autonomous robots are increasingly deployed to estimate spatiotemporal fields\n(e.g., wind, temperature, gas concentration) that vary across space and time.\nWe consider environments divided into non-overlapping regions with distinct\nspatial and temporal dynamics, termed non-uniform spatiotemporal environments.\nGaussian Processes (GPs) can be used to estimate these fields. The GP model\ndepends on a kernel that encodes how the field co-varies in space and time,\nwith its spatial and temporal lengthscales defining the correlation. Hence,\nwhen these lengthscales are incorrect or do not correspond to the actual field,\nthe estimates of uncertainty can be highly inaccurate. Existing GP methods\noften assume one global lengthscale or update only periodically; some allow\nspatial variation but ignore temporal changes. To address these limitations, we\npropose a two-phase framework for multi-robot field estimation. Phase 1 uses a\nvariogram-driven planner to learn region-specific spatial lengthscales. Phase 2\nemploys an allocation strategy that reassigns robots based on the current\nuncertainty, and updates sampling as temporal lengthscales are refined. For\nencoding uncertainty, we utilize clarity, an information metric from our\nearlier work. We evaluate the proposed method across diverse environments and\nprovide convergence analysis for spatial lengthscale estimation, along with\ndynamic regret bounds quantifying the gap to the oracle's allocation sequence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u591a\u673a\u5668\u4eba\u573a\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u5747\u5300\u65f6\u7a7a\u73af\u5883\u4e2d\u7684\u573a\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709GP\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u88ab\u5e7f\u6cdb\u7528\u4e8e\u4f30\u8ba1\u65f6\u7a7a\u53d8\u5316\u7684\u573a\uff08\u5982\u98ce\u901f\u3001\u6e29\u5ea6\u3001\u6c14\u4f53\u6d53\u5ea6\uff09\uff0c\u4f46\u73b0\u6709\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5168\u5c40\u5c3a\u5ea6\u6216\u5ffd\u7565\u65f6\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u53d8\u5dee\u51fd\u6570\u9a71\u52a8\u7684\u89c4\u5212\u5668\u5b66\u4e60\u533a\u57df\u7279\u5b9a\u7684\u7a7a\u95f4\u5c3a\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u91cd\u65b0\u5206\u914d\u673a\u5668\u4eba\uff0c\u5e76\u968f\u65f6\u95f4\u5c3a\u5ea6\u7ec6\u5316\u66f4\u65b0\u91c7\u6837\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u7a7a\u95f4\u5c3a\u5ea6\u4f30\u8ba1\u7684\u6536\u655b\u5206\u6790\uff0c\u5e76\u91cf\u5316\u4e86\u4e0e\u6700\u4f73\u5206\u914d\u5e8f\u5217\u7684\u52a8\u6001\u9057\u61be\u754c\u9650\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5747\u5300\u65f6\u7a7a\u573a\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.23302", "pdf": "https://arxiv.org/pdf/2509.23302", "abs": "https://arxiv.org/abs/2509.23302", "authors": ["Wilson de Souza Junior", "Taufik Abrao", "Amine Mezghani", "Ekram Hossain"], "title": "Dual-Function Beam Pattern Design for Multi-Target ISAC Systems: A Decoupled Approach", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "13 pages; 7 figures; tables; 3 tables; manuscript submitted to IEEE\n  journal", "summary": "We investigate the beampattern design problem for mono-static multi-user (MU)\nmulti-point-target integrated sensing and communication (ISAC) systems, where a\ndual-function multiple-input multiple-output (DF-MIMO) base station (BS)\nperforms downlink communication and radar sensing simultaneously. In ISAC\nsystems, sensing and communication inherently compete for resources. As\ncommunication demand increases, the beam pattern is reshaped, which might\ndegrade the direction of arrival (DoA) sensing accuracy, measured in terms of\nmean-squared error (MSE) and lower-bounded by the Cramer-Rao lower bound\n(CRLB). Since conventional joint formulations of the sensing-based problem\noften overlook this trade-off, our work addresses it by decomposing the\nsensing-based problem into two subproblems (SPs). This decomposition enables a\nmore effective exploitation of the beam pattern's physical properties, which we\nrefer to as the Sensing-Guided Communication Dual-Function (SGCDF) beam pattern\ndesign. We further develop a low-complexity extension using the Riemannian\nManifold Optimization (RMO) and convex closed-set projection. Simulation\nresults confirm that the proposed method improves multi-target estimation\naccuracy, compared to traditional joint optimization strategies, by preserving\nthe beam pattern, while the low-complexity version offers an excellent\nperformance-complexity tradeoff, maintaining high accuracy with significantly\nreduced computational cost.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u7528\u6237\u591a\u70b9\u76ee\u6807\u7684\u5355\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u65b9\u5411\u56fe\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u5f15\u5bfc\u7684\u53cc\u529f\u80fd\u6ce2\u675f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u548c\u4f7f\u7528\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u590d\u6742\u5ea6\u6743\u8861\u3002", "motivation": "\u5728ISAC\u7cfb\u7edf\u4e2d\uff0c\u611f\u77e5\u548c\u901a\u4fe1\u5bf9\u8d44\u6e90\u7684\u7ade\u4e89\u53ef\u80fd\u5bfc\u81f4\u6ce2\u675f\u65b9\u5411\u56fe\u7684\u91cd\u5851\uff0c\u964d\u4f4e\u611f\u77e5\u7cbe\u5ea6\u3002\u4f20\u7edf\u65b9\u6cd5\u5e38\u5ffd\u89c6\u8fd9\u79cd\u6743\u8861\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u611f\u77e5\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u63d0\u51faSGCDF\u6ce2\u675f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5229\u7528Riemannian\u6d41\u5f62\u4f18\u5316\u548c\u51f8\u95ed\u96c6\u6295\u5f71\u5f00\u53d1\u4f4e\u590d\u6742\u5ea6\u6269\u5c55\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u76ee\u6807\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u4f4e\u590d\u6742\u5ea6\u7248\u672c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u672c\u6587\u65b9\u6cd5\u5728ISAC\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u611f\u77e5\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5e73\u8861\u3002"}}
{"id": "2509.22910", "pdf": "https://arxiv.org/pdf/2509.22910", "abs": "https://arxiv.org/abs/2509.22910", "authors": ["Yanwei Du", "Jing-Chen Peng", "Patricio A. Vela"], "title": "Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM", "categories": ["cs.RO"], "comment": "8 pages, 9 figures, 1 table. Submitted to IEEE Conference", "summary": "Given that Visual SLAM relies on appearance cues for localization and scene\nunderstanding, texture-less or visually degraded environments (e.g., plain\nwalls or low lighting) lead to poor pose estimation and track loss. However,\nrobots are typically equipped with sensors that provide some form of dead\nreckoning odometry with reasonable short-time performance but unreliable\nlong-time performance. The Good Weights (GW) algorithm described here provides\na framework to adaptively integrate dead reckoning (DR) with passive visual\nSLAM for continuous and accurate frame-level pose estimation. Importantly, it\ndescribes how all modules in a comprehensive SLAM system must be modified to\nincorporate DR into its design. Adaptive weighting increases DR influence when\nvisual tracking is unreliable and reduces when visual feature information is\nstrong, maintaining pose track without overreliance on DR. Good Weights yields\na practical solution for mobile navigation that improves visual SLAM\nperformance and robustness. Experiments on collected datasets and in real-world\ndeployment demonstrate the benefits of Good Weights.", "AI": {"tldr": "Good Weights\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u822a\u8ff9\u63a8\u7b97\u548c\u89c6\u89c9SLAM\uff0c\u63d0\u5347\u4e86\u5728\u7eb9\u7406\u7f3a\u5931\u6216\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u7f3a\u4e4f\u7eb9\u7406\u6216\u89c6\u89c9\u9000\u5316\u7684\u73af\u5883\u4e2d\uff0c\u89c6\u89c9SLAM\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u822a\u8ff9\u63a8\u7b97\u867d\u77ed\u671f\u53ef\u7528\u4f46\u957f\u671f\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "Good Weights\u7b97\u6cd5\u81ea\u9002\u5e94\u8c03\u6574\u822a\u8ff9\u63a8\u7b97\u548c\u89c6\u89c9SLAM\u7684\u6743\u91cd\uff0c\u89c6\u89c9\u4fe1\u606f\u5f3a\u5927\u65f6\u51cf\u5c11\u822a\u8ff9\u63a8\u7b97\u7684\u5f71\u54cd\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGood Weights\u5728\u6570\u636e\u96c6\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u63d0\u5347\u4e86\u89c6\u89c9SLAM\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Good Weights\u4e3a\u79fb\u52a8\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u6539\u5584\u4e86\u89c6\u89c9SLAM\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.23444", "pdf": "https://arxiv.org/pdf/2509.23444", "abs": "https://arxiv.org/abs/2509.23444", "authors": ["Lorenzo Italiano", "Alireza Pourafzal", "Hui Chen", "Mattia Brambilla", "Gonzalo Seco-Granados", "Monica Nicoli", "Henk Wymeersch"], "title": "HoloTrace: a Location Privacy Preservation Solution for mmWave MIMO-OFDM Systems", "categories": ["eess.SP"], "comment": "submitted to IEEE Journal on Selected Areas in Communications", "summary": "The technological innovation towards 6G cellular networks introduces\nunprecedented capabilities for user equipment (UE) localization, but it also\nraises serious concerns about physical layer location privacy. This paper\nintroduces HoloTrace, a signal-level privacy preservation framework that relies\non user-side spoofing of localization-relevant features to prevent the\nextraction of precise location information from the signals received by a base\nstation (BS) in a mmWave MIMO-OFDM system. Spoofing is performed by the user on\nlocation parameters such as angle of arrival (AoA), angle of departure (AoD),\nand time difference of arrival (TDoA). Without requiring any protocol\nmodification nor network-side support, our method strategically perturbs pilot\ntransmissions to prevent a BS from performing non-consensual UE localization.\nThe methodology allows the UE to spoof its position, keeping the precoder\nunchanged. We formulate spoofing as a unified rank-constrained projection\nproblem, and provide closed-form solutions under varying levels of channel\nstate information (CSI) at the UE, including scenarios with and without CSI\nknowledge. Simulation results confirm that the proposed approach enables the UE\nto deceive the BS, inducing significant localization errors, while the impact\non link capacity varies depending on the spoofed position. Our findings\nestablish HoloTrace as a practical and robust privacy-preserving solution for\nfuture 6G networks.", "AI": {"tldr": "HoloTrace\u662f\u4e00\u79cd\u9488\u5bf96G\u7f51\u7edc\u4e2d\u7528\u6237\u5b9a\u4f4d\u9690\u79c1\u7684\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u7aef\u5bf9\u4fe1\u53f7\u53c2\u6570\u7684\u6b3a\u9a97\u6765\u9632\u6b62\u57fa\u7ad9\u63d0\u53d6\u7cbe\u786e\u4f4d\u7f6e\u4fe1\u606f\uff0c\u65e0\u9700\u4fee\u6539\u534f\u8bae\u6216\u7f51\u7edc\u652f\u6301\u3002", "motivation": "6G\u7f51\u7edc\u6280\u672f\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u7528\u6237\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u7269\u7406\u5c42\u4f4d\u7f6e\u9690\u79c1\u7684\u4e25\u91cd\u62c5\u5fe7\u3002HoloTrace\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u514d\u53d7\u975e\u6388\u6743\u5b9a\u4f4d\u3002", "method": "HoloTrace\u901a\u8fc7\u5bf9\u5230\u8fbe\u89d2\uff08AoA\uff09\u3001\u51fa\u53d1\u89d2\uff08AoD\uff09\u548c\u5230\u8fbe\u65f6\u95f4\u5dee\uff08TDoA\uff09\u7b49\u5b9a\u4f4d\u76f8\u5173\u53c2\u6570\u7684\u6b3a\u9a97\uff0c\u5e72\u6270\u57fa\u7ad9\u7684\u5b9a\u4f4d\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5c06\u6b3a\u9a97\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u6709\u79e9\u7ea6\u675f\u6295\u5f71\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u95ed\u5f0f\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHoloTrace\u80fd\u591f\u6709\u6548\u6b3a\u9a97\u57fa\u7ad9\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u5b9a\u4f4d\u8bef\u5dee\uff0c\u540c\u65f6\u5bf9\u94fe\u8def\u5bb9\u91cf\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u6b3a\u9a97\u7684\u4f4d\u7f6e\u3002", "conclusion": "HoloTrace\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u672a\u67656G\u7f51\u7edc\u3002"}}
{"id": "2509.22914", "pdf": "https://arxiv.org/pdf/2509.22914", "abs": "https://arxiv.org/abs/2509.22914", "authors": ["Rohan Walia", "Yusheng Wang", "Ralf R\u00f6mer", "Masahiro Nishio", "Angela P. Schoellig", "Jun Ota"], "title": "ARMimic: Learning Robotic Manipulation from Passive Human Demonstrations in Augmented Reality", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning is a powerful paradigm for robot skill acquisition, yet\nconventional demonstration methods--such as kinesthetic teaching and\nteleoperation--are cumbersome, hardware-heavy, and disruptive to workflows.\nRecently, passive observation using extended reality (XR) headsets has shown\npromise for egocentric demonstration collection, yet current approaches require\nadditional hardware, complex calibration, or constrained recording conditions\nthat limit scalability and usability. We present ARMimic, a novel framework\nthat overcomes these limitations with a lightweight and hardware-minimal setup\nfor scalable, robot-free data collection using only a consumer XR headset and a\nstationary workplace camera. ARMimic integrates egocentric hand tracking,\naugmented reality (AR) robot overlays, and real-time depth sensing to ensure\ncollision-aware, kinematically feasible demonstrations. A unified imitation\nlearning pipeline is at the core of our method, treating both human and virtual\nrobot trajectories as interchangeable, which enables policies that generalize\nacross different embodiments and environments. We validate ARMimic on two\nmanipulation tasks, including challenging long-horizon bowl stacking. In our\nexperiments, ARMimic reduces demonstration time by 50% compared to\nteleoperation and improves task success by 11% over ACT, a state-of-the-art\nbaseline trained on teleoperated data. Our results demonstrate that ARMimic\nenables safe, seamless, and in-the-wild data collection, offering great\npotential for scalable robot learning in diverse real-world settings.", "AI": {"tldr": "ARMimic\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u6d88\u8d39\u7ea7XR\u5934\u663e\u548c\u56fa\u5b9a\u6444\u50cf\u5934\u5b9e\u73b0\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u91c7\u96c6\uff0c\u51cf\u5c11\u6f14\u793a\u65f6\u95f450%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534711%\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u6280\u80fd\u6f14\u793a\u65b9\u6cd5\uff08\u5982\u8fd0\u52a8\u6559\u5b66\u548c\u8fdc\u7a0b\u64cd\u4f5c\uff09\u7e41\u7410\u4e14\u786c\u4ef6\u4f9d\u8d56\u9ad8\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53ef\u7528\u6027\u3002", "method": "ARMimic\u7ed3\u5408\u624b\u90e8\u8ddf\u8e2a\u3001AR\u673a\u5668\u4eba\u53e0\u52a0\u548c\u5b9e\u65f6\u6df1\u5ea6\u611f\u77e5\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u4e14\u8fd0\u52a8\u5b66\u53ef\u884c\u7684\u6f14\u793a\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u6a21\u4eff\u5b66\u4e60\u7ba1\u9053\u5904\u7406\u4eba\u7c7b\u548c\u865a\u62df\u673a\u5668\u4eba\u8f68\u8ff9\u3002", "result": "\u5728\u4e24\u4e2a\u64cd\u7eb5\u4efb\u52a1\uff08\u5305\u62ec\u5806\u53e0\u7897\uff09\u4e2d\uff0cARMimic\u51cf\u5c11\u4e8650%\u7684\u6f14\u793a\u65f6\u95f4\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6bd4\u57fa\u7ebfACT\u63d0\u9ad8\u4e8611%\u3002", "conclusion": "ARMimic\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u65e0\u7f1d\u4e14\u7075\u6d3b\u7684\u91ce\u5916\u6570\u636e\u91c7\u96c6\uff0c\u4e3a\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2509.23520", "pdf": "https://arxiv.org/pdf/2509.23520", "abs": "https://arxiv.org/abs/2509.23520", "authors": ["Kalpesh Jaykar", "Prasanth Velvaluri", "Nian X. Sun", "Richard D. James"], "title": "Theoretical framework of passive ME antenna arrays enabling in-vivo monitoring: A pathway to smart implants", "categories": ["eess.SP"], "comment": null, "summary": "A new brain-computer interface (BCI) technology, deployed through minimally\ninvasive surgery, is changing the way we think about treating severe\nneurological conditions. The central idea is to place a device called Stentrode\nin the brain's vasculature, which enables neuromodulation and helps patients\nregain the ability to communicate. However, in such devices, the battery and\nelectronics are wired and could introduce damage or implant malfunction. In\nthese cases, a Stentrode integrated with magnetoelectric (ME) antennas could be\nof great interest. ME antennas offer significant advantages over traditional\nantennas, leveraging acoustic resonance rather than electromagnetic resonance\nto achieve a size reduction of up to five orders of magnitude. In addition to\ntheir compactness and immunity to ground-plane interference, ME antennas could\nbe adopted for use in vascular implants, such as coronary stents, potentially\nenabling minimally invasive monitoring and communication. Despite these\nadvantages, a single antenna embedded in the implant may be constrained by the\nlimited volume of magnetostrictive material, which could result in low output\ngain. To address this gain limitation, we propose using antenna arrays designed\nto produce constructive interference at a designated far-field point, ideally\nlocated outside the patient, to enhance signal transmission and receiving\ncapabilities. We develop a mathematical model to represent the antennas and\noptimize their spatial arrangement and phase synchronization. Simulations based\non this model demonstrate promising high-gain performance at the prescribed\nfar-field location through phase manipulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u65b0\u578b\u8111\u673a\u63a5\u53e3\u6280\u672fStentrode\uff0c\u901a\u8fc7\u5fae\u521b\u624b\u672f\u5728\u5927\u8111\u8840\u7ba1\u4e2d\u90e8\u7f72\uff0c\u7ed3\u5408\u78c1\u7535\u5929\u7ebf\u9635\u5217\u4ee5\u63d0\u5347\u4fe1\u53f7\u4f20\u8f93\u548c\u63a5\u6536\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5929\u7ebf\u8f93\u51fa\u589e\u76ca\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8111\u673a\u63a5\u53e3\u8bbe\u5907\u4e2d\u7684\u7535\u6c60\u548c\u7535\u5b50\u5143\u4ef6\u901a\u8fc7\u6709\u7ebf\u8fde\u63a5\uff0c\u53ef\u80fd\u5bfc\u81f4\u635f\u574f\u6216\u690d\u5165\u6545\u969c\u3002\u78c1\u7535\u5929\u7ebf\u56e0\u5176\u5c3a\u5bf8\u5c0f\u548c\u5bf9\u5730\u5e73\u9762\u5e72\u6270\u514d\u75ab\u7684\u4f18\u52bf\uff0c\u6709\u671b\u7528\u4e8e\u8840\u7ba1\u690d\u5165\u7269\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u5229\u7528\u5929\u7ebf\u9635\u5217\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5728\u6307\u5b9a\u8fdc\u573a\u70b9\u4ea7\u751f\u76f8\u957f\u5e72\u6d89\u6765\u589e\u5f3a\u4fe1\u53f7\u4f20\u8f93\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u5b66\u6a21\u578b\u4f18\u5316\u5929\u7ebf\u7684\u7a7a\u95f4\u6392\u5217\u548c\u76f8\u4f4d\u540c\u6b65\u3002", "result": "\u57fa\u4e8e\u6a21\u578b\u7684\u4eff\u771f\u663e\u793a\uff0c\u901a\u8fc7\u76f8\u4f4d\u64cd\u63a7\uff0c\u5929\u7ebf\u9635\u5217\u5728\u6307\u5b9a\u8fdc\u573a\u4f4d\u7f6e\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u9ad8\u589e\u76ca\u6027\u80fd\u3002", "conclusion": "\u78c1\u7535\u5929\u7ebf\u9635\u5217\u7684\u5e94\u7528\u4e3a\u8111\u673a\u63a5\u53e3\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4fe1\u53f7\u4f20\u8f93\u6548\u7387\uff0c\u5c55\u73b0\u4e86\u5728\u5fae\u521b\u76d1\u6d4b\u548c\u901a\u8baf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.22937", "pdf": "https://arxiv.org/pdf/2509.22937", "abs": "https://arxiv.org/abs/2509.22937", "authors": ["Trent Weiss", "Amar Kulkarni", "Madhur Behl"], "title": "DBF-MA: A Differential Bayesian Filtering Planner for Multi-Agent Autonomous Racing Overtakes", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A significant challenge in autonomous racing is to generate overtaking\nmaneuvers. Racing agents must execute these maneuvers on complex racetracks\nwith little room for error. Optimization techniques and graph-based methods\nhave been proposed, but these methods often rely on oversimplified assumptions\nfor collision-avoidance and dynamic constraints. In this work, we present an\napproach to trajectory synthesis based on an extension of the Differential\nBayesian Filtering framework. Our approach for collision-free trajectory\nsynthesis frames the problem as one of Bayesian Inference over the space of\nComposite Bezier Curves. Our method is derivative-free, does not require a\nspherical approximation of the vehicle footprint, linearization of constraints,\nor simplifying upper bounds on collision avoidance. We conduct a closed-loop\nanalysis of DBF-MA and find it successfully overtakes an opponent in 87% of\ntested scenarios, outperforming existing methods in autonomous overtaking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u7684\u8f68\u8ff9\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u7684\u8d85\u8f66\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7b80\u5316\u5047\u8bbe\uff0c\u5e76\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u80cc\u666f\u662f\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u590d\u6742\u7684\u8d85\u8f66\u52a8\u4f5c\u751f\u6210\uff0c\u5c24\u5176\u662f\u907f\u514d\u78b0\u649e\u548c\u52a8\u6001\u7ea6\u675f\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5dee\u5206\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u5408\u6210\u95ee\u9898\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u4f20\u7edf\u65b9\u6cd5\u7684\u7b80\u5316\u5047\u8bbe\u3002", "result": "\u5728\u95ed\u73af\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u572887%\u7684\u573a\u666f\u4e2d\u6210\u529f\u8d85\u8f66\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u590d\u6742\u8d5b\u9053\u4e0a\u7684\u8d85\u8f66\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u5408\u6210\u65b9\u6848\u3002"}}
{"id": "2509.23644", "pdf": "https://arxiv.org/pdf/2509.23644", "abs": "https://arxiv.org/abs/2509.23644", "authors": ["Omkar Nitsure", "Sampath Kumar Dondapati", "Satish Mulleti"], "title": "Learnable Kernels for FRI -- Joint Kernel Encoder Optimization and Hardware Validation", "categories": ["eess.SP"], "comment": "10 pages", "summary": "Finite Rate of Innovation (FRI) sampling techniques provide efficient\nframeworks for reconstructing signals with inherent sparsity at rates below\nNyquist. However, traditional FRI reconstruction methods rely heavily on\npre-defined kernels, often limiting hardware implementation and reconstruction\naccuracy under noisy conditions. In this paper, we propose a robust, flexible,\nand practically implementable framework for FRI reconstruction by introducing\nnovel learnable kernel strategies. First, we demonstrate effective\nreconstruction using known, fixed kernels such as truncated Gaussian and\nGaussian pair kernels, which mitigate the requirement that the samples should\nhave a sum-of-exponentials (SoE) form. Next, we extend this concept by jointly\noptimizing both the sampling kernel and reconstruction encoder through a\nunified learning approach, yielding adaptive kernels that significantly\noutperform traditional methods in resolution and noise robustness, with reduced\nsampling rates. Furthermore, we propose a practical hardware realization by\nrepresenting kernels as sums of two exponential decay signals with jointly\noptimized poles, facilitating compact, efficient analog implementations. Our\napproach is validated experimentally through hardware implementations using a\nunity-gain Sallen-Key analog filter, achieving accurate real-world signal\nrecovery. The developed convolutional neural network-based encoder\nsubstantially reduces computational complexity, demonstrating competitive\nperformance with fewer parameters, making our method particularly suitable for\nresource-constrained, edge-based deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u6838\u7b56\u7565\u7684FRI\u4fe1\u53f7\u91cd\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u91c7\u6837\u6838\u548c\u91cd\u6784\u7f16\u7801\u5668\uff0c\u63d0\u9ad8\u4e86\u5206\u8fa8\u7387\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u786c\u4ef6\u5b9e\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684FRI\u91cd\u6784\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u6838\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u5b9e\u73b0\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u91cd\u6784\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6838\u7b56\u7565\uff0c\u5305\u62ec\u56fa\u5b9a\u6838\u548c\u8054\u5408\u4f18\u5316\u91c7\u6837\u6838\u4e0e\u91cd\u6784\u7f16\u7801\u5668\u7684\u81ea\u9002\u5e94\u6838\uff0c\u5e76\u8bbe\u8ba1\u4e86\u786c\u4ef6\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c\u566a\u58f0\u9c81\u68d2\u6027\u4e0a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u6027\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.22955", "pdf": "https://arxiv.org/pdf/2509.22955", "abs": "https://arxiv.org/abs/2509.22955", "authors": ["Pietro Bruschi"], "title": "Hierarchical Control Design for Space Robots with Application to In-Orbit Servicing Missions", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In-Orbit Servicing and Active Debris Removal require advanced robotic\ncapabilities for capturing and detumbling uncooperative targets. This work\npresents a hierarchical control framework for autonomous robotic capture of\ntumbling objects in space. A simulation environment is developed, incorporating\nsloshing dynamics of the chaser, a rarely studied effect in space robotics. The\nproposed controller combines an inner Lyapunov-based robust control loop for\nmulti-body dynamics with an outer loop addressing an extended inverse\nkinematics problem. Simulation results show improved robustness and\nadaptability compared to existing control schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u6355\u83b7\u592a\u7a7a\u7ffb\u6eda\u7269\u4f53\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5185\u73afLyapunov\u9c81\u68d2\u63a7\u5236\u548c\u5916\u73af\u6269\u5c55\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u592a\u7a7a\u4e2d\u7684\u5728\u8f68\u670d\u52a1\u548c\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u9700\u8981\u5148\u8fdb\u7684\u673a\u5668\u4eba\u6280\u672f\u6765\u6355\u83b7\u548c\u4e0d\u7a33\u5b9a\u975e\u5408\u4f5c\u76ee\u6807\uff0c\u4e3a\u6b64\u9700\u8981\u89e3\u51b3\u591a\u4f53\u52a8\u529b\u5b66\u548c\u6db2\u4f53\u6643\u52a8\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u5305\u542b\u8ffd\u6355\u5668\u7684\u6db2\u4f53\u6643\u52a8\u52a8\u529b\u5b66\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Lyapunov\u9c81\u68d2\u63a7\u5236\u548c\u6269\u5c55\u9006\u8fd0\u52a8\u5b66\u7684\u5206\u5c42\u63a7\u5236\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u63a7\u5236\u6846\u67b6\u5728\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u63a7\u5236\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u592a\u7a7a\u673a\u5668\u4eba\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u5408\u4f5c\u76ee\u6807\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.23687", "pdf": "https://arxiv.org/pdf/2509.23687", "abs": "https://arxiv.org/abs/2509.23687", "authors": ["Runze Dong", "Buhong Wang", "Cunqian Feng", "Jiang Weng", "Chen Han", "Jiwei Tian"], "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u4fe1\u611f\u4e00\u4f53\u5316\uff08ISAC\uff09\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\u8bbe\u8ba1\u6df7\u5408\u6ce2\u675f\u6210\u5f62\uff08HBF\uff09\u3001\u4eba\u5de5\u566a\u58f0\uff08AN\uff09\u6ce8\u5165\u548c\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c06\u65e0\u4eba\u673a\u89c6\u4e3a\u7a7a\u4e2d\u57fa\u7ad9\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u4f5c\u4e3aISAC\u7528\u6237\u7684\u89d2\u8272\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u5730\u9762\u57fa\u7ad9\u7684\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u63d0\u5347\u5b89\u5168\u6027\u548c\u9891\u8c31\u6548\u7387\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u4f18\u5316\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u5668\u548c\u65e0\u4eba\u673a\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4f4e\u590d\u6742\u5ea6\u77e9\u9635\u5206\u89e3\u5c06\u6570\u5b57\u89e3\u5206\u89e3\u4e3a\u6a21\u62df\u548c\u6570\u5b57\u7ec4\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u5bc6\u7387\u548c\u9891\u8c31\u6548\u7387\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684ISAC\u6846\u67b6\u548c\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u65e0\u4eba\u673a\u7f51\u7edc\u4e2d\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\u548c\u9891\u8c31\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7a7a\u4e2d\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22970", "pdf": "https://arxiv.org/pdf/2509.22970", "abs": "https://arxiv.org/abs/2509.22970", "authors": ["Siheng Zhao", "Jiageng Mao", "Wei Chow", "Zeyu Shangguan", "Tianheng Shi", "Rong Xue", "Yuxi Zheng", "Yijia Weng", "Yang You", "Daniel Seita", "Leonidas Guibas", "Sergey Zakharov", "Vitor Guizilini", "Yue Wang"], "title": "Robot Learning from Any Images", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "CoRL 2025 camera ready", "summary": "We introduce RoLA, a framework that transforms any in-the-wild image into an\ninteractive, physics-enabled robotic environment. Unlike previous methods, RoLA\noperates directly on a single image without requiring additional hardware or\ndigital assets. Our framework democratizes robotic data generation by producing\nmassive visuomotor robotic demonstrations within minutes from a wide range of\nimage sources, including camera captures, robotic datasets, and Internet\nimages. At its core, our approach combines a novel method for single-view\nphysical scene recovery with an efficient visual blending strategy for\nphotorealistic data collection. We demonstrate RoLA's versatility across\napplications like scalable robotic data generation and augmentation, robot\nlearning from Internet images, and single-image real-to-sim-to-real systems for\nmanipulators and humanoids. Video results are available at\nhttps://sihengz02.github.io/RoLA .", "AI": {"tldr": "RoLA\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4efb\u610f\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u8f6c\u5316\u4e3a\u5177\u6709\u7269\u7406\u4ea4\u4e92\u80fd\u529b\u7684\u673a\u5668\u4eba\u73af\u5883\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u6570\u5b57\u8d44\u4ea7\u3002\u5b83\u901a\u8fc7\u5355\u89c6\u56fe\u7269\u7406\u573a\u666f\u6062\u590d\u548c\u9ad8\u6548\u7684\u89c6\u89c9\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u5927\u89c4\u6a21\u7684\u53ef\u89c6\u5316\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\u65b9\u6cd5\u9700\u8981\u989d\u5916\u786c\u4ef6\u6216\u6570\u5b57\u8d44\u4ea7\u7684\u95ee\u9898\uff0cRoLA\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u4ece\u800c\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7684\u666e\u53ca\u548c\u53d1\u5c55\u3002", "method": "RoLA\u7ed3\u5408\u4e86\u5355\u89c6\u56fe\u7269\u7406\u573a\u666f\u6062\u590d\u65b9\u6cd5\u548c\u89c6\u89c9\u878d\u5408\u7b56\u7565\uff0c\u76f4\u63a5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u7269\u7406\u4ea4\u4e92\u73af\u5883\u3002", "result": "RoLA\u80fd\u591f\u5feb\u901f\u751f\u6210\u5927\u91cf\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\uff0c\u5982\u673a\u5668\u4eba\u6570\u636e\u6269\u5c55\u3001\u4ece\u4e92\u8054\u7f51\u56fe\u50cf\u5b66\u4e60\u4ee5\u53ca\u771f\u5b9e\u5230\u4eff\u771f\u518d\u5230\u771f\u5b9e\u7684\u7cfb\u7edf\u3002", "conclusion": "RoLA\u4e3a\u673a\u5668\u4eba\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.23792", "pdf": "https://arxiv.org/pdf/2509.23792", "abs": "https://arxiv.org/abs/2509.23792", "authors": ["Kabuto Arai", "Takumi Yoshida", "Takumi Takahashi", "Koji Ishibashi"], "title": "Expectation Propagation-Based Signal Detection for Highly Correlated MIMO Systems", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Large-scale multiple-input-multiple-output (MIMO) systems typically operate\nin dense array deployments with limited scattering environments, leading to\nhighly correlated and ill-conditioned channel matrices that severely degrade\nthe performance of message-passing-based detectors. To tackle this issue, this\npaper proposes an expectation propagation (EP)-based detector, termed\noverlapping block partitioning EP (OvEP). In OvEP, the large-scale measurement\nvector is partitioned into partially overlapping blocks. For each block and its\noverlapping part, a low-complexity linear minimum mean square error\n(LMMSE)-based filter is designed according to the partitioned structure. The\nresulting LMMSE outputs are then combined to generate the input to the\ndenoiser. In this combining process, subtracting the overlapping-part outputs\nfrom the block outputs effectively mitigates the adverse effects of inter-block\ncorrelation induced by high spatial correlation. The proposed algorithm is\nconsistently derived within the EP framework, and its fixed point is\ntheoretically proven to coincide with the stationary point of a relaxed\nKullback- Leibler (KL) minimization problem. The mechanisms underlying the\ntheoretically predicted performance improvement are further clarified through\nnumerical simulations. The proposed algorithm achieves performance close to\nconventional LMMSE-EP with lower computational complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u671f\u671b\u4f20\u64ad\uff08EP\uff09\u7684\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\u5668OvEP\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u9ad8\u5ea6\u76f8\u5173\u548c\u75c5\u6001\u4fe1\u9053\u77e9\u9635\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u901a\u8fc7\u91cd\u53e0\u5757\u5206\u533a\u548cLMMSE\u6ee4\u6ce2\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u56e0\u4fe1\u9053\u77e9\u9635\u9ad8\u5ea6\u76f8\u5173\u548c\u75c5\u6001\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u91cd\u53e0\u5757\u5206\u533aEP\uff08OvEP\uff09\u68c0\u6d4b\u5668\uff0c\u5c06\u6d4b\u91cf\u5411\u91cf\u5212\u5206\u4e3a\u91cd\u53e0\u5757\uff0c\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6LMMSE\u6ee4\u6ce2\u5668\uff0c\u5e76\u5408\u5e76\u8f93\u51fa\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6539\u5584\u6027\u80fd\u3002", "result": "OvEP\u5728\u8f83\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u4f20\u7edfLMMSE-EP\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u7684\u6027\u80fd\u63d0\u5347\u673a\u5236\u3002", "conclusion": "OvEP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u68c0\u6d4b\u5668\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u9ad8\u76f8\u5173\u6027\u95ee\u9898\u3002"}}
{"id": "2509.22976", "pdf": "https://arxiv.org/pdf/2509.22976", "abs": "https://arxiv.org/abs/2509.22976", "authors": ["Rounak Bhattacharya", "Vrithik R. Guthikonda", "Ashwin P. Dani"], "title": "Safe Task Space Synchronization with Time-Delayed Information", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "In this paper, an adaptive controller is designed for the synchronization of\nthe trajectory of a robot with unknown kinematics and dynamics to that of the\ncurrent human trajectory in the task space using the delayed human trajectory\ninformation. The communication time delay may be a result of various factors\nthat arise in human-robot collaboration tasks, such as sensor processing or\nfusion to estimate trajectory/intent, network delays, or computational\nlimitations. The developed adaptive controller uses Barrier Lyapunov Function\n(BLF) to constrain the Cartesian coordinates of the robot to ensure safety, an\nICL-based adaptive law to account for the unknown kinematics, and a\ngradient-based adaptive law to estimate unknown dynamics. Barrier\nLyapunov-Krasovskii (LK) functionals are used for the stability analysis to\nshow that the synchronization and parameter estimation errors remain\nsemi-globally uniformly ultimately bounded (SGUUB). The simulation results\nbased on a human-robot synchronization scenario with time delay are provided to\ndemonstrate the effectiveness of the designed synchronization controller with\nsafety constraints.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f68\u8ff9\u4e0e\u4eba\u7c7b\u8f68\u8ff9\u5728\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u540c\u6b65\uff0c\u8003\u8651\u4e86\u901a\u4fe1\u5ef6\u8fdf\u548c\u672a\u77e5\u8fd0\u52a8\u5b66/\u52a8\u529b\u5b66\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u4e2d\u56e0\u4f20\u611f\u5668\u5904\u7406\u3001\u7f51\u7edc\u5ef6\u8fdf\u6216\u8ba1\u7b97\u9650\u5236\u5bfc\u81f4\u7684\u8f68\u8ff9\u540c\u6b65\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528Barrier Lyapunov Function\uff08BLF\uff09\u9650\u5236\u673a\u5668\u4eba\u5750\u6807\uff0c\u7ed3\u5408ICL\u81ea\u9002\u5e94\u5f8b\u4f30\u8ba1\u672a\u77e5\u8fd0\u52a8\u5b66\u548c\u68af\u5ea6\u81ea\u9002\u5e94\u5f8b\u4f30\u8ba1\u672a\u77e5\u52a8\u529b\u5b66\u3002", "result": "\u901a\u8fc7Barrier Lyapunov-Krasovskii\u51fd\u6570\u5206\u6790\uff0c\u8bc1\u660e\u540c\u6b65\u548c\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u662f\u534a\u5168\u5c40\u4e00\u81f4\u6700\u7ec8\u6709\u754c\u7684\uff08SGUUB\uff09\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u63a7\u5236\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bbe\u8ba1\u7684\u81ea\u9002\u5e94\u63a7\u5236\u5668\u5728\u5b58\u5728\u65f6\u95f4\u5ef6\u8fdf\u548c\u5b89\u5168\u6027\u7ea6\u675f\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4eba\u673a\u8f68\u8ff9\u540c\u6b65\u3002"}}
{"id": "2509.23807", "pdf": "https://arxiv.org/pdf/2509.23807", "abs": "https://arxiv.org/abs/2509.23807", "authors": ["Hongyu Wang", "Wenjia Xu", "Guangzuo Li", "Siyuan Wan", "Yaohua Sun", "Jiuniu Wang", "Mugen Peng"], "title": "Online Specific Emitter Identification via Collision-Alleviated Signal Hash", "categories": ["eess.SP"], "comment": "This paper has been accepted by IEEE Transactions on Vehicular\n  Technology", "summary": "Specific Emitter Identification (SEI) has been widely studied, aiming to\ndistinguish signals from different emitters given training samples from those\nemitters. However, real-world scenarios often require identifying signals from\nnovel emitters previously unseen. Since these novel emitters only have a few or\nno prior samples, existing models struggle to identify signals from novel\nemitters online and tend to bias toward the distribution of seen emitters. To\naddress these challenges, we propose the Online Specific Emitter Identification\n(OSEI) task, comprising both online \\revise{few-shot and generalized zero-shot}\nlearning tasks. It requires constructing models using signal samples from seen\nemitters and then identifying new samples from seen and novel emitters online\nduring inference. We propose a novel hash-based model, Collision-Alleviated\nSignal Hash (CASH), providing a unified approach for addressing the OSEI task.\nThe CASH operates in two steps: in the seen emitters identifying step, a signal\nencoder and a seen emitters identifier determine whether the signal sample is\nfrom seen emitters, mitigating the model from biasing toward seen emitters\ndistribution. In the signal hash coding step, an online signal hasher assigns a\nhash code to each signal sample, identifying its specific emitter. Experimental\nresults on real-world signal datasets (i.e., ADSB and ORACLE) demonstrate that\nour method accurately identifies signals from both seen and novel emitters\nonline. This model outperforms existing methods by a minimum of 6.08\\% and\n8.55\\% in accuracy for the few-shot and \\revise{generalized zero-shot learning\n}tasks, respectively. The code will be open-sourced at\n\\href{https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u7279\u5b9a\u53d1\u5c04\u5668\u8bc6\u522b\uff08OSEI\uff09\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u54c8\u5e0c\u6a21\u578bCASH\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8bc6\u522b\u672a\u89c1\u53d1\u5c04\u5668\u4fe1\u53f7\u65f6\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u9700\u8981\u8bc6\u522b\u672a\u89c1\u53d1\u5c04\u5668\u7684\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5bf9\u6b64\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u54c8\u5e0c\u6a21\u578bCASH\uff0c\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u8bc6\u522b\u5df2\u77e5\u53d1\u5c04\u5668\u548c\u4e3a\u4fe1\u53f7\u6837\u672c\u5206\u914d\u54c8\u5e0c\u7801\u3002", "result": "\u5728ADSB\u548cORACLE\u6570\u636e\u96c6\u4e0a\uff0cCASH\u5728few-shot\u548c\u5e7f\u4e49\u96f6\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e86\u81f3\u5c116.08%\u548c8.55%\u3002", "conclusion": "CASH\u6a21\u578b\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5df2\u77e5\u548c\u672a\u89c1\u53d1\u5c04\u5668\u7684\u4fe1\u53f7\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.23021", "pdf": "https://arxiv.org/pdf/2509.23021", "abs": "https://arxiv.org/abs/2509.23021", "authors": ["Xiao Hu", "Qi Yin", "Yangming Shi", "Yang Ye"], "title": "UniPrototype: Humn-Robot Skill Learning with Uniform Prototypes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Data scarcity remains a fundamental challenge in robot learning. While human\ndemonstrations benefit from abundant motion capture data and vast internet\nresources, robotic manipulation suffers from limited training examples. To\nbridge this gap between human and robot manipulation capabilities, we propose\nUniPrototype, a novel framework that enables effective knowledge transfer from\nhuman to robot domains via shared motion primitives. ur approach makes three\nkey contributions: (1) We introduce a compositional prototype discovery\nmechanism with soft assignments, enabling multiple primitives to co-activate\nand thus capture blended and hierarchical skills; (2) We propose an adaptive\nprototype selection strategy that automatically adjusts the number of\nprototypes to match task complexity, ensuring scalable and efficient\nrepresentation; (3) We demonstrate the effectiveness of our method through\nextensive experiments in both simulation environments and real-world robotic\nsystems. Our results show that UniPrototype successfully transfers human\nmanipulation knowledge to robots, significantly improving learning efficiency\nand task performance compared to existing approaches.The code and dataset will\nbe released upon acceptance at an anonymous repository.", "AI": {"tldr": "UniPrototype\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u8fd0\u52a8\u539f\u8bed\uff0c\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u7f3a\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u800c\u4eba\u7c7b\u6f14\u793a\u5f97\u76ca\u4e8e\u4e30\u5bcc\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u4e92\u8054\u7f51\u8d44\u6e90\u3002UniPrototype\u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faUniPrototype\u6846\u67b6\uff0c\u5305\u62ec\u8f6f\u5206\u914d\u7684\u7ec4\u5408\u539f\u578b\u53d1\u73b0\u673a\u5236\u3001\u81ea\u9002\u5e94\u539f\u578b\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniPrototype\u6210\u529f\u5c06\u4eba\u7c7b\u64cd\u4f5c\u77e5\u8bc6\u8f6c\u79fb\u5230\u673a\u5668\u4eba\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "UniPrototype\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.23920", "pdf": "https://arxiv.org/pdf/2509.23920", "abs": "https://arxiv.org/abs/2509.23920", "authors": ["Masahiro Kurisaki"], "title": "Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime", "categories": ["eess.SP", "math.PR", "stat.ME", "stat.ML"], "comment": "This paper is a self-contained exposition of the methodological part\n  of Section 4 in arXiv:2501.16333", "summary": "We propose a new asymptotic expansion method for nonlinear filtering, based\non a small parameter in the system noise. The conditional expectation is\nexpanded as a power series in the noise level, with each coefficient computed\nby solving a system of ordinary differential equations. This approach mitigates\nthe trade-off between computational efficiency and accuracy inherent in\nexisting methods such as Gaussian approximations and particle filters.\nMoreover, by incorporating an Edgeworth-type expansion, our method captures\ncomplex features of the conditional distribution, such as multimodality, with\nsignificantly lower computational cost than conventional filtering algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7cfb\u7edf\u566a\u58f0\u5c0f\u53c2\u6570\u7684\u975e\u7ebf\u6027\u6ee4\u6ce2\u6e10\u8fd1\u5c55\u5f00\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3ODE\u7cfb\u6570\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6761\u4ef6\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u9ad8\u65af\u8fd1\u4f3c\u548c\u7c92\u5b50\u6ee4\u6ce2\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u6355\u6349\u6761\u4ef6\u5206\u5e03\u7684\u590d\u6742\u7279\u5f81\u3002", "method": "\u5229\u7528\u566a\u58f0\u6c34\u5e73\u4f5c\u4e3a\u5c0f\u53c2\u6570\uff0c\u5c06\u6761\u4ef6\u671f\u671b\u5c55\u5f00\u4e3a\u5e42\u7ea7\u6570\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3ODE\u8ba1\u7b97\u7cfb\u6570\uff0c\u7ed3\u5408Edgeworth\u578b\u5c55\u5f00\u6355\u6349\u591a\u6a21\u6001\u7b49\u7279\u5f81\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u65b0\u65b9\u6cd5\u5728\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6761\u4ef6\u5206\u5e03\u7684\u51c6\u786e\u6355\u6349\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ebf\u6027\u6ee4\u6ce2\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5206\u5e03\u573a\u666f\u3002"}}
{"id": "2509.23048", "pdf": "https://arxiv.org/pdf/2509.23048", "abs": "https://arxiv.org/abs/2509.23048", "authors": ["Chang Liu", "Badrinath Balasubramaniam", "Neal Yancey", "Michael Severson", "Adam Shine", "Philip Bove", "Beiwen Li", "Xiao Liang", "Minghui Zheng"], "title": "RAISE: A Robot-Assisted Selective Disassembly and Sorting System for End-of-Life Phones", "categories": ["cs.RO"], "comment": null, "summary": "End-of-Life (EoL) phones significantly exacerbate global e-waste challenges\ndue to their high production volumes and short lifecycles. Disassembly is among\nthe most critical processes in EoL phone recycling. However, it relies heavily\non human labor due to product variability. Consequently, the manual process is\nboth labor-intensive and time-consuming. In this paper, we propose a low-cost,\neasily deployable automated and selective disassembly and sorting system for\nEoL phones, consisting of three subsystems: an adaptive cutting system, a\nvision-based robotic sorting system, and a battery removal system. The system\ncan process over 120 phones per hour with an average disassembly success rate\nof 98.9%, efficiently delivering selected high-value components to downstream\nprocessing. It provides a reliable and scalable automated solution to the\npressing challenge of EoL phone disassembly. Additionally, the automated system\ncan enhance disassembly economics, converting a previously unprofitable process\ninto one that yields a net profit per unit weight of EoL phones.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u62c6\u89e3\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u5e9f\u5f03\u624b\u673a\uff0c\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u62c6\u89e3\u7684\u4f4e\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5e9f\u5f03\u624b\u673a\u56e0\u5176\u9ad8\u4ea7\u91cf\u548c\u77ed\u751f\u547d\u5468\u671f\u52a0\u5267\u4e86\u5168\u7403\u7535\u5b50\u5e9f\u7269\u95ee\u9898\uff0c\u4f20\u7edf\u4eba\u5de5\u62c6\u89e3\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8bbe\u8ba1\u4e86\u7531\u81ea\u9002\u5e94\u5207\u5272\u7cfb\u7edf\u3001\u89c6\u89c9\u5f15\u5bfc\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u548c\u7535\u6c60\u79fb\u9664\u7cfb\u7edf\u7ec4\u6210\u7684\u81ea\u52a8\u5316\u62c6\u89e3\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u6bcf\u5c0f\u65f6\u53ef\u5904\u7406120\u591a\u90e8\u624b\u673a\uff0c\u5e73\u5747\u62c6\u89e3\u6210\u529f\u7387\u8fbe98.9%\uff0c\u5e76\u5c06\u9ad8\u4ef7\u503c\u7ec4\u4ef6\u9ad8\u6548\u8f93\u9001\u81f3\u4e0b\u6e38\u5904\u7406\u73af\u8282\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5e9f\u5f03\u624b\u673a\u62c6\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2509.24097", "pdf": "https://arxiv.org/pdf/2509.24097", "abs": "https://arxiv.org/abs/2509.24097", "authors": ["Henglin Pu", "Zhu Han", "Athina P. Petropulu", "Husheng Li"], "title": "Wideband Integrated Sensing and Communications: Spectral Efficiency and Signaling Design", "categories": ["eess.SP"], "comment": null, "summary": "In integrated sensing and communications (ISAC), a distinguishing feature of\n6G wireless networks, the main challenge lies in integrating the two distinct\nfunctions of sensing and communication within the same waveform. In this paper,\nthe ISAC waveform synthesis is studied in the wideband regime, since a large\nbandwidth can simplify the analysis and is justified by the employment of\nmillimeter wave or higher frequency band. Standard orthogonal frequency\ndivision multiplexing (OFDM) signaling is assumed, and the wideband analysis of\nsensing is a counterpart of the existing studies on wideband communications. It\nis proposed that the phase over such OFDM subcarriers is for modulating\ncommunication messages while the power spectral density (PSD) is shaped for the\nsensing performance. Beyond OFDM, we further reveal a duality between the\nproposed PSD-shaping rule and the orthogonal time frequency space (OTFS)\nwaveform. Flattening the OTFS delay-axis PSD produces the same integrated\nsidelobe level (ISL) reduction effect in the delay-Doppler domain as PSD\ncontrol achieves for OFDM in the frequency domain. To balance communication and\nsensing performance over frequency-selective channels, we propose a\nlow-complexity, water-filling-like allocator with an explicit PSD-flatness\n(variance) constraint. The performance of the proposed wideband ISAC scheme is\ndemonstrated using both numerical simulations and hardware experiments.", "AI": {"tldr": "\u7814\u7a766G\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u6ce2\u5f62\u5408\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u5728\u5bbd\u5e26\u6761\u4ef6\u4e0b\u901a\u8fc7OFDM\u4fe1\u53f7\u7684\u76f8\u4f4d\u8c03\u5236\u901a\u4fe1\u6d88\u606f\uff0c\u5e76\u901a\u8fc7PSD\u63a7\u5236\u4f18\u5316\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793aOTFS\u6ce2\u5f62\u7684\u7c7b\u4f3c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u57286G\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u529f\u80fd\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5373\u5728\u540c\u4e00\u6ce2\u5f62\u4e2d\u6574\u5408\u4e8c\u8005\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5bbd\u5e26\u5206\u6790\u7b80\u5316\u8bbe\u8ba1\u3002", "method": "\u5047\u8bbe\u6807\u51c6OFDM\u4fe1\u53f7\uff0c\u5728\u5bbd\u5e26\u6761\u4ef6\u4e0b\u901a\u8fc7PSD\u5f62\u72b6\u63a7\u5236\u4f18\u5316\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u4f4e\u590d\u6742\u5ea6\u7684\u6c34\u586b\u5145\u5206\u914d\u5668\u5e73\u8861\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u5bbd\u5e26ISAC\u65b9\u6848\u901a\u8fc7\u6570\u503c\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0cOTFS\u6ce2\u5f62\u7684\u5ef6\u8fdf\u8f74PSD\u5e73\u5766\u5316\u4e5f\u80fd\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "conclusion": "\u5bbd\u5e26\u6ce2\u5f62\u8bbe\u8ba1\u80fd\u6709\u6548\u5e73\u8861\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\uff0c\u4e3a6G ISAC\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23075", "pdf": "https://arxiv.org/pdf/2509.23075", "abs": "https://arxiv.org/abs/2509.23075", "authors": ["Soofiyan Atar", "Daniel Huang", "Florian Richter", "Michael Yip"], "title": "In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands with Sim-to-Real Transfer", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) and sim-to-real transfer have advanced robotic\nmanipulation of rigid objects. Yet, policies remain brittle when applied to\narticulated mechanisms due to contact-rich dynamics and under-modeled joint\nphenomena such as friction, stiction, backlash, and clearances. We address this\nchallenge through dexterous in-hand manipulation of articulated tools using a\nrobotic hand with reduced articulation and kinematic redundancy relative to the\nhuman hand. Our controller augments a simulation-trained base policy with a\nsensor-driven refinement learned from hardware demonstrations, conditioning on\nproprioception and target articulation states while fusing whole-hand tactile\nand force feedback with the policy's internal action intent via\ncross-attention-based integration. This design enables online adaptation to\ninstance-specific articulation properties, stabilizes contact interactions,\nregulates internal forces, and coordinates coupled-link motion under\nperturbations. We validate our approach across a diversity of real-world\nexamples, including scissors, pliers, minimally invasive surgical tools, and\nstaplers. We achieve robust transfer from simulation to hardware, improved\ndisturbance resilience, and generalization to previously unseen articulated\ntools, thereby reducing reliance on precise physical modeling in contact-rich\nsettings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u548c\u786c\u4ef6\u6f14\u793a\u5b66\u4e60\u7684\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u624b\u5bf9\u94f0\u94fe\u5de5\u5177\u7684\u64cd\u63a7\u80fd\u529b\uff0c\u901a\u8fc7\u89e6\u89c9\u548c\u529b\u53cd\u9988\u589e\u5f3a\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u65b9\u6cd5\u5728\u5904\u7406\u94f0\u94fe\u673a\u5236\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u4e3b\u8981\u7531\u4e8e\u590d\u6742\u7684\u52a8\u529b\u5b66\u73b0\u8c61\uff08\u5982\u6469\u64e6\u3001\u95f4\u9699\uff09\u672a\u88ab\u5145\u5206\u5efa\u6a21\u3002", "method": "\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u7684\u57fa\u7840\u7b56\u7565\u548c\u786c\u4ef6\u6f14\u793a\u5b66\u4e60\u7684\u4f20\u611f\u5668\u9a71\u52a8\u7ec6\u5316\uff0c\u5229\u7528\u89e6\u89c9\u3001\u529b\u53cd\u9988\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6574\u5408\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u591a\u79cd\u5de5\u5177\uff08\u5982\u526a\u5200\u3001\u624b\u672f\u5de5\u5177\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u7684\u7a33\u5065\u8f6c\u79fb\u548c\u5bf9\u65b0\u5de5\u5177\u7684\u6cdb\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u7cbe\u786e\u7269\u7406\u5efa\u6a21\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u94f0\u94fe\u5de5\u5177\u64cd\u63a7\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.24178", "pdf": "https://arxiv.org/pdf/2509.24178", "abs": "https://arxiv.org/abs/2509.24178", "authors": ["Chengwei Zhou", "Steve Majerus", "Gourav Datta"], "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State Monitoring", "categories": ["eess.SP"], "comment": "Under review", "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5c42\u6d41\u5f0fTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u5206\u7c7b\u8180\u80f1\u538b\u529b\u72b6\u6001\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8180\u80f1\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u548c\u6d45\u5c42\u5206\u7c7b\u5668\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u4fe1\u53f7\u52a8\u6001\u3002", "method": "\u91c7\u7528\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5355\u5c42\u6d41\u5f0fTransformer\u6a21\u578b\uff0c\u5305\u542b\u65f6\u95f4\u591a\u5934\u81ea\u6ce8\u610f\u529b\u548c\u72b6\u6001\u7f13\u5b58\u3002", "result": "\u572891\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u80fd\u91cf\u6548\u7387\u548c\u5ef6\u8fdf\u6548\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u8fb9\u7f18\u786c\u4ef6\u90e8\u7f72\uff0c\u5982\u8fb9\u7f18GPU\u548c\u5fae\u63a7\u5236\u5668\u3002"}}
{"id": "2509.23107", "pdf": "https://arxiv.org/pdf/2509.23107", "abs": "https://arxiv.org/abs/2509.23107", "authors": ["Yi Wang", "Zeyu Xue", "Mujie Liu", "Tongqin Zhang", "Yan Hu", "Zhou Zhao", "Chenguang Yang", "Zhenyu Lu"], "title": "Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Teleoperation via natural-language reduces operator workload and enhances\nsafety in high-risk or remote settings. However, in dynamic remote scenes,\ntransmission latency during bidirectional communication creates gaps between\nremote perceived states and operator intent, leading to command\nmisunderstanding and incorrect execution. To mitigate this, we introduce the\nSpatio-Temporal Open-Vocabulary Scene Graph (ST-OVSG), a representation that\nenriches open-vocabulary perception with temporal dynamics and lightweight\nlatency annotations. ST-OVSG leverages LVLMs to construct open-vocabulary 3D\nobject representations, and extends them into the temporal domain via Hungarian\nassignment with our temporal matching cost, yielding a unified spatio-temporal\nscene graph. A latency tag is embedded to enable LVLM planners to\nretrospectively query past scene states, thereby resolving local-remote state\nmismatches caused by transmission delays. To further reduce redundancy and\nhighlight task-relevant cues, we propose a task-oriented subgraph filtering\nstrategy that produces compact inputs for the planner. ST-OVSG generalizes to\nnovel categories and enhances planning robustness against transmission latency\nwithout requiring fine-tuning. Experiments show that our method achieves 74\npercent node accuracy on the Replica benchmark, outperforming ConceptGraph.\nNotably, in the latency-robustness experiment, the LVLM planner assisted by\nST-OVSG achieved a planning success rate of 70.5 percent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aST-OVSG\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u7a7a\u52a8\u6001\u548c\u5ef6\u8fdf\u6807\u6ce8\u7684\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u56fe\uff0c\u89e3\u51b3\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u5ef6\u8fdf\u5bfc\u81f4\u7684\u6307\u4ee4\u8bef\u89e3\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Replica\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u4f20\u8f93\u5ef6\u8fdf\u4f1a\u5bfc\u81f4\u611f\u77e5\u72b6\u6001\u4e0e\u64cd\u4f5c\u610f\u56fe\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u5f15\u53d1\u6307\u4ee4\u8bef\u89e3\u548c\u6267\u884c\u9519\u8bef\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86ST-OVSG\u65b9\u6cd5\u3002", "method": "ST-OVSG\u7ed3\u5408\u4e86\u5f00\u653e\u8bcd\u6c473D\u5bf9\u8c61\u8868\u793a\u548c\u65f6\u7a7a\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u5308\u7259\u5229\u7b97\u6cd5\u5b9e\u73b0\u65f6\u95f4\u57df\u6269\u5c55\u3002\u540c\u65f6\uff0c\u5d4c\u5165\u5ef6\u8fdf\u6807\u7b7e\u548c\u4efb\u52a1\u5bfc\u5411\u5b50\u56fe\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u89c4\u5212\u8f93\u5165\u3002", "result": "\u5728Replica\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cST-OVSG\u7684\u8282\u70b9\u51c6\u786e\u7387\u8fbe\u523074%\uff0c\u4e14\u5728\u5ef6\u8fdf\u9c81\u68d2\u6027\u5b9e\u9a8c\u4e2d\uff0cLVLM\u89c4\u5212\u5668\u7684\u6210\u529f\u7387\u63d0\u5347\u81f370.5%\u3002", "conclusion": "ST-OVSG\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u589e\u5f3a\u89c4\u5212\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6cdb\u5316\u5230\u65b0\u7c7b\u522b\u3002"}}
{"id": "2509.24222", "pdf": "https://arxiv.org/pdf/2509.24222", "abs": "https://arxiv.org/abs/2509.24222", "authors": ["Zhisheng Chen", "Yingwei Zhang", "Qizhen Lan", "Tianyu Liu", "Huacan Wang", "Yi Ding", "Ziyu Jia", "Ronghao Chen", "Kun Wang", "Xinliang Zhou"], "title": "Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Foundation models pretrained on various and unlabeled data have demonstrated\nsignificant success in natural language and vision, but their application to\nelectroencephalography (EEG) remains challenged due to the signal's unique\nproperties. Existing brain foundation models that inherit architectures\ndesigned for text or images lead to three limitations in pre-training: 1)\nconflating time-domain waveform patterns with frequency-domain rhythmic\nfeatures in a single processing stream, 2) ignoring the critical spatial\ntopology of electrodes with different standards, and 3) reliance on the\ninflexible, dense network to process functionally distinct EEG patterns. To\naddress these challenges, we introduce the Unified Neural Topological\nFoundation Model (Uni-NTFM), which is designed based on neuroscience principles\nto produce universal and interpretable representations. Uni-NTFM integrates\nthree core innovations: 1) a decoupled architecture parallelly encodes time,\nfrequency, and raw signal representations before performing cross-domain\nfeature integration; 2) a topological embedding mechanism to unify electrodes\nfrom different international standards and generate structured input sequences\nfor brain regions; and 3) a Mixture-of-Experts neural Transformer that\nefficiently scales model capacity by routing signal patterns to specialized\nsubnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B\nparameters and was pretrained on over 28,000 hours of diverse EEG data via a\ndual-domain masked reconstruction objective. Uni-NTFM significantly outperforms\nexisting task-specific methods and foundation models across nine distinct\ndownstream tasks under both linear probing and fine-tuning settings,\ndemonstrating a superior ability to learn universal representations of brain\nactivity.", "AI": {"tldr": "Uni-NTFM\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u539f\u7406\u7684\u7edf\u4e00\u795e\u7ecf\u62d3\u6251\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86EEG\u4fe1\u53f7\u5904\u7406\u7684\u4e09\u5927\u6311\u6218\uff0c\u5e76\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u8111\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406EEG\u4fe1\u53f7\u65f6\u5b58\u5728\u65f6\u95f4\u57df\u4e0e\u9891\u7387\u57df\u7279\u5f81\u6df7\u6dc6\u3001\u5ffd\u7565\u7535\u6781\u7a7a\u95f4\u62d3\u6251\u4ee5\u53ca\u4f9d\u8d56\u5bc6\u96c6\u7f51\u7edc\u7684\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86Uni-NTFM\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u67b6\u6784\u5e76\u884c\u5904\u7406\u65f6\u95f4\u3001\u9891\u7387\u548c\u539f\u59cb\u4fe1\u53f7\uff0c\u7ed3\u5408\u62d3\u6251\u5d4c\u5165\u673a\u5236\u548cMixture-of-Experts Transformer\u3002", "result": "Uni-NTFM\u5728\u4e5d\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5b66\u4e60\u5927\u8111\u6d3b\u52a8\u901a\u7528\u8868\u793a\u7684\u80fd\u529b\u3002", "conclusion": "Uni-NTFM\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u89c4\u6a21\u6269\u5c55\uff0c\u4e3aEEG\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23111", "pdf": "https://arxiv.org/pdf/2509.23111", "abs": "https://arxiv.org/abs/2509.23111", "authors": ["Chen Yizhe", "Wang Qi", "Hu Dongxiao", "Jingzhe Fang", "Liu Sichao", "Zixin An", "Hongliang Niu", "Haoran Liu", "Li Dong", "Chuanfen Feng", "Lan Dapeng", "Liu Yu", "Zhibo Pang"], "title": "Liaohe-CobotMagic-PnP: an Imitation Learning Dataset of Intelligent Robot for Industrial Applications", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to IAI 2025 (International Conference on Industrial\n  Artificial Intelligence), Shenyang, China, Aug 21 - 24, 2025. Preprint\n  (before IEEE copyright transfer)", "summary": "In Industry 4.0 applications, dynamic environmental interference induces\nhighly nonlinear and strongly coupled interactions between the environmental\nstate and robotic behavior. Effectively representing dynamic environmental\nstates through multimodal sensor data fusion remains a critical challenge in\ncurrent robotic datasets. To address this, an industrial-grade multimodal\ninterference dataset is presented, designed for robotic perception and control\nunder complex conditions. The dataset integrates multi-dimensional interference\nfeatures including size, color, and lighting variations, and employs\nhigh-precision sensors to synchronously collect visual, torque, and joint-state\nmeasurements. Scenarios with geometric similarity exceeding 85\\% and\nstandardized lighting gradients are included to ensure real-world\nrepresentativeness. Microsecond-level time-synchronization and\nvibration-resistant data acquisition protocols, implemented via the Robot\nOperating System (ROS), guarantee temporal and operational fidelity.\nExperimental results demonstrate that the dataset enhances model validation\nrobustness and improves robotic operational stability in dynamic,\ninterference-rich environments. The dataset is publicly available\nat:https://modelscope.cn/datasets/Liaoh_LAB/Liaohe-CobotMagic-PnP.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u591a\u6a21\u6001\u5e72\u6270\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u7684\u6311\u6218\u3002", "motivation": "\u5de5\u4e1a4.0\u5e94\u7528\u4e2d\uff0c\u52a8\u6001\u73af\u5883\u5e72\u6270\u5bfc\u81f4\u73af\u5883\u72b6\u6001\u4e0e\u673a\u5668\u4eba\u884c\u4e3a\u9ad8\u5ea6\u975e\u7ebf\u6027\u8026\u5408\uff0c\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u96be\u4ee5\u6709\u6548\u8868\u5f81\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u7684\u52a8\u6001\u73af\u5883\u72b6\u6001\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u5927\u5c0f\u3001\u989c\u8272\u548c\u5149\u7167\u53d8\u5316\u7684\u591a\u7ef4\u5e72\u6270\u7279\u5f81\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u9ad8\u7cbe\u5ea6\u4f20\u611f\u5668\u540c\u6b65\u91c7\u96c6\u89c6\u89c9\u3001\u626d\u77e9\u548c\u5173\u8282\u72b6\u6001\u6570\u636e\uff0c\u5e76\u901a\u8fc7ROS\u5b9e\u73b0\u5fae\u79d2\u7ea7\u65f6\u95f4\u540c\u6b65\u548c\u6297\u632f\u52a8\u6570\u636e\u91c7\u96c6\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6a21\u578b\u9a8c\u8bc1\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u52a8\u6001\u5e72\u6270\u73af\u5883\u4e2d\u6539\u5584\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u771f\u5b9e\u4e14\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u652f\u6301\uff0c\u5e76\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2509.24355", "pdf": "https://arxiv.org/pdf/2509.24355", "abs": "https://arxiv.org/abs/2509.24355", "authors": ["Sefa Kayrakl\u0131k", "Recep Ba\u015f", "Hasan O\u011fuzhan \u00c7al\u0131\u015fkan", "Samed \u015eahino\u011flu", "Sercan Erdo\u011fan", "\u0130lhami \u00dcnal", "\u0130brahim H\u00f6kelek", "K\u0131van\u00e7 Nurdan", "Ali G\u00f6r\u00e7in"], "title": "N78 Frequency Band Modular RIS Design and Implementation", "categories": ["eess.SP"], "comment": "Presented at EuMC2025, Copyright EuMA", "summary": "Reconfigurable intelligent surface (RIS), capable of dynamically controlling\nwireless propagation characteristics using reflecting antenna elements, is a\npromising technology for enhancing signal coverage and improving end-user\nconnectivity in next-generation wireless networks. This paper presents a\ncomplete design flow of a modular RIS prototype operating at the n78 frequency\nband, starting from the simulations to the prototype development and testing.\nAn RIS prototype includes one master and up to sixteen slave blocks, each of\nwhich has an identical hardware structure with $8\\times 8$ reflecting surface\nelements and a controller board. The phase shift response of each unit element\nis controlled with a PIN diode to form a $180^\\circ$ phase difference between\nthe ON and OFF states. The measurement experiment using two RIS blocks, horn\nantennas, and a vector network analyzer showed that the improvement of the\nreceived signal power is more than $15$ dB across the n78 frequency band for a\ngiven placement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u539f\u578b\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5c55\u793a\u4e86\u5176\u5728n78\u9891\u6bb5\u63d0\u5347\u4fe1\u53f7\u529f\u7387\u7684\u6548\u679c\u3002", "motivation": "\u901a\u8fc7RIS\u6280\u672f\u52a8\u6001\u63a7\u5236\u65e0\u7ebf\u4f20\u64ad\u7279\u6027\uff0c\u589e\u5f3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u4fe1\u53f7\u8986\u76d6\u548c\u7ec8\u7aef\u8fde\u63a5\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u4e3b\u4ece\u6a21\u5757\u7684RIS\u539f\u578b\uff0c\u6bcf\u4e2a\u6a21\u5757\u5177\u67098\u00d78\u53cd\u5c04\u8868\u9762\u5355\u5143\u548c\u63a7\u5236\u5668\u677f\uff0c\u901a\u8fc7PIN\u4e8c\u6781\u7ba1\u63a7\u5236\u76f8\u4f4d\u5dee\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRIS\u5728n78\u9891\u6bb5\u53ef\u5c06\u63a5\u6536\u4fe1\u53f7\u529f\u7387\u63d0\u5347\u8d85\u8fc715dB\u3002", "conclusion": "\u8be5RIS\u539f\u578b\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u4fe1\u53f7\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.23112", "pdf": "https://arxiv.org/pdf/2509.23112", "abs": "https://arxiv.org/abs/2509.23112", "authors": ["Ryo Watanabe", "Maxime Alvarez", "Pablo Ferreiro", "Pavel Savkin", "Genki Sano"], "title": "FTACT: Force Torque aware Action Chunking Transformer for Pick-and-Reorient Bottle Task", "categories": ["cs.RO"], "comment": null, "summary": "Manipulator robots are increasingly being deployed in retail environments,\nyet contact rich edge cases still trigger costly human teleoperation. A\nprominent example is upright lying beverage bottles, where purely visual cues\nare often insufficient to resolve subtle contact events required for precise\nmanipulation. We present a multimodal Imitation Learning policy that augments\nthe Action Chunking Transformer with force and torque sensing, enabling\nend-to-end learning over images, joint states, and forces and torques. Deployed\non Ghost, single-arm platform by Telexistence Inc, our approach improves\nPick-and-Reorient bottle task by detecting and exploiting contact transitions\nduring pressing and placement. Hardware experiments demonstrate greater task\nsuccess compared to baseline matching the observation space of ACT as an\nablation and experiments indicate that force and torque signals are beneficial\nin the press and place phases where visual observability is limited, supporting\nthe use of interaction forces as a complementary modality for contact rich\nskills. The results suggest a practical path to scaling retail manipulation by\ncombining modern imitation learning architectures with lightweight force and\ntorque sensing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u529b/\u626d\u77e9\u4f20\u611f\uff0c\u63d0\u5347\u673a\u68b0\u81c2\u5728\u96f6\u552e\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u96f6\u552e\u73af\u5883\u4e2d\u673a\u68b0\u81c2\u5728\u63a5\u89e6\u5bc6\u96c6\u7684\u8fb9\u7f18\u60c5\u51b5\u4e0b\u4ecd\u9700\u6602\u8d35\u7684\u4eba\u5de5\u9065\u63a7\u64cd\u4f5c\uff0c\u7279\u522b\u662f\u76f4\u7acb\u996e\u6599\u74f6\u7684\u7cbe\u786e\u64cd\u4f5c\uff0c\u4ec5\u9760\u89c6\u89c9\u63d0\u793a\u5f80\u5f80\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u6269\u5c55\u4e86Action Chunking Transformer\uff08ACT\uff09\uff0c\u7ed3\u5408\u56fe\u50cf\u3001\u5173\u8282\u72b6\u6001\u548c\u529b/\u626d\u77e9\u4f20\u611f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Pick-and-Reorient\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u53d7\u9650\u7684\u538b\u5236\u548c\u653e\u7f6e\u9636\u6bb5\uff0c\u529b/\u626d\u77e9\u4fe1\u53f7\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u7ed3\u5408\u73b0\u4ee3\u6a21\u4eff\u5b66\u4e60\u67b6\u6784\u4e0e\u8f7b\u91cf\u7ea7\u529b/\u626d\u77e9\u4f20\u611f\uff0c\u4e3a\u96f6\u552e\u73af\u5883\u4e2d\u6269\u5c55\u673a\u68b0\u81c2\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.24428", "pdf": "https://arxiv.org/pdf/2509.24428", "abs": "https://arxiv.org/abs/2509.24428", "authors": ["Santos Michelena", "Maxime Ferreira Da Costa", "Jos\u00e9 Picheral"], "title": "Strong Basin of Attraction for Unmixing Kernels With the Variable Projection Method", "categories": ["eess.SP", "cs.NA", "math.NA"], "comment": "5 pages, 4 figures. Submitted to the 2026 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "The problem of recovering a mixture of spike signals convolved with distinct\npoint spread functions (PSFs) lying on a parametric manifold, under the\nassumption that the spike locations are known, is studied. The PSF unmixing\nproblem is formulated as a projected non-linear least squares estimator. A\nlower bound on the radius of the region of strong convexity is established in\nthe presence of noise as a function of the manifold coherence and Lipschitz\nproperties, guaranteeing convergence and stability of the optimization program.\nNumerical experiments highlight the speed of decay of the PSF class in the\nproblem's conditioning and confirm theoretical findings. Finally, the proposed\nestimator is deployed on real-world spectroscopic data from laser-induced\nbreakdown spectroscopy (LIBS), removing the need for manual calibration and\nvalidating the method's practical relevance.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5df2\u77e5\u5c16\u5cf0\u4fe1\u53f7\u4f4d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u53c2\u6570\u6d41\u5f62\u4e0a\u4e0d\u540c\u70b9\u6269\u6563\u51fd\u6570\uff08PSF\uff09\u7684\u6df7\u5408\u4e2d\u6062\u590d\u4fe1\u53f7\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6295\u5f71\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2dPSF\u6df7\u53e0\u4fe1\u53f7\u7684\u6062\u590d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u672a\u77e5PSF\u7684\u60c5\u51b5\u4e0b\uff0c\u907f\u514d\u624b\u52a8\u6821\u51c6\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6295\u5f71\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5206\u6790\u6d41\u5f62\u76f8\u5e72\u6027\u548cLipschitz\u6027\u8d28\uff0c\u5efa\u7acb\u5f3a\u51f8\u533a\u57df\u7684\u534a\u5f84\u4e0b\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u4f30\u8ba1\u5668\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e14\u5728\u771f\u5b9e\u5149\u8c31\u6570\u636e\uff08LIBS\uff09\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u53ef\u9760\uff0c\u8fd8\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u51cf\u5c11\u624b\u52a8\u6821\u51c6\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.23118", "pdf": "https://arxiv.org/pdf/2509.23118", "abs": "https://arxiv.org/abs/2509.23118", "authors": ["Zeyi Li", "Zhe Tang", "Kyeong Soo Kim", "Sihao Li", "Jeremy S. Smith"], "title": "EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation", "categories": ["cs.RO", "cs.LG", "cs.NI"], "comment": "8 pages, 7 figures, 3 tables, and submitted for presentation at a\n  conference", "summary": "Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting\ncannot meet the growing demand for accurate indoor localization and navigation\ndue to its lower accuracy, while solutions based on light detection and ranging\n(LiDAR) can provide better localization performance but is limited by their\nhigher deployment cost and complexity. To address these issues, we propose a\nnovel indoor localization and navigation framework integrating Wi-Fi RSSI\nfingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and\ninertial measurement unit (IMU) navigation based on an extended Kalman filter\n(EKF). Specifically, coarse localization by deep neural network (DNN)-based\nWi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a\nGmapping-based SLAM to generate an occupancy grid map and output high-frequency\nattitude estimates, which is followed by EKF prediction-update integrating\nsensor information while effectively suppressing Wi-Fi-induced noise and IMU\ndrift errors. Multi-group real-world experiments conducted on the IR building\nat Xi'an Jiaotong-Liverpool University demonstrates that the proposed\nmulti-sensor fusion framework suppresses the instability caused by individual\napproaches and thereby provides stable accuracy across all path configurations\nwith mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In\ncontrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m\nin areas with severe signal interference, and those of LiDAR/IMU localization\nare between 0.6233 m and 2.8803 m due to cumulative drift.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Wi-Fi RSSI\u6307\u7eb9\u3001LiDAR SLAM\u548cIMU\u5bfc\u822a\u7684\u65b0\u578b\u5ba4\u5185\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7EKF\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u514b\u670d\u4e86\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfWi-Fi\u6307\u7eb9\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800cLiDAR\u65b9\u6848\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u53c8\u6613\u4e8e\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528DNN\u8fdb\u884cWi-Fi\u7c97\u5b9a\u4f4d\uff0c\u7ed3\u5408IMU\u52a8\u6001\u5b9a\u4f4d\u548cGmapping SLAM\u751f\u6210\u5730\u56fe\uff0c\u6700\u540e\u901a\u8fc7EKF\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u6291\u5236Wi-Fi\u566a\u58f0\u548cIMU\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u7684\u5e73\u57472D\u8bef\u5dee\u4e3a0.2449\u81f30.3781\u7c73\uff0c\u4f18\u4e8eWi-Fi\u6307\u7eb9\u76841.3404\u7c73\u548cLiDAR/IMU\u76840.6233\u81f32.8803\u7c73\u3002", "conclusion": "\u591a\u4f20\u611f\u5668\u878d\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5ba4\u5185\u73af\u5883\u3002"}}
{"id": "2509.24537", "pdf": "https://arxiv.org/pdf/2509.24537", "abs": "https://arxiv.org/abs/2509.24537", "authors": ["Philipp del Hougne"], "title": "Low-Complexity Wireless Multi-Port Sensing by Multiplexed De-Embedding of an Over-the-Air Fixture", "categories": ["eess.SP", "physics.app-ph"], "comment": "9 pages including 5 figures", "summary": "Wireless multi-port sensing remotely retrieves the scattering matrix of a\nmulti-port device under test (DUT) connected to a set of\nnot-directly-accessible (NDA) antennas that couple over-the-air (OTA) to a set\nof accessible antennas. If (i) the OTA fixture characteristics are known, and\n(ii) the number of independent measurements at the accessible antennas is\nsufficient, the OTA fixture can be de-embedded to recover the DUT\ncharacteristics. In recent prior work, we solved (i) by connecting the NDA\nantennas to a specific known tunable load network (TLN). Here, we tackle (ii)\nby additionally using the TLN to provide measurement diversity. The connection\nbetween OTA fixture and TLN constitutes a programmable fixture (PF). When the\nDUT characteristics cannot be identified based on a single PF realization, we\nadd measurement diversity with multiple PF realizations. The underlying\n\"multiplexed de-embedding\" achieves the joint de-embedding of an ensemble of PF\nrealizations when a single PF realization cannot be de-embedded. We\nexperimentally demonstrate our concept by remotely estimating the scattering\nmatrix of a reciprocal, non-unitary 4-port DUT (10 complex-valued unknowns) via\na rich-scattering OTA fixture purely based on measurements of a single\ntransmission coefficient between two accessible antennas across 30 different PF\nrealizations. We systematically study the trade-off between the number of\nindependent measurements at the accessible antennas and the number of PF\nrealizations. Multiplexed de-embedding of the OTA fixture paves the path to\nimplementing wireless multi-port sensing with low hardware complexity in areas\nlike RFID and wireless bioelectronics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7ebf\u591a\u7aef\u53e3\u4f20\u611f\u65b9\u6cd5\uff0c\u901a\u8fc7OTA\u5939\u5177\u548c\u53ef\u8c03\u8d1f\u8f7d\u7f51\u7edc\u5b9e\u73b0DUT\u7684\u6563\u5c04\u77e9\u9635\u8fdc\u7a0b\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u6d4b\u91cf\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u65e0\u7ebf\u591a\u7aef\u53e3\u4f20\u611f\u4e2d\uff0c\u7531\u4e8eOTA\u5939\u5177\u7279\u6027\u5df2\u77e5\u4f46\u72ec\u7acb\u6d4b\u91cf\u4e0d\u8db3\u65f6\uff0cDUT\u7279\u6027\u65e0\u6cd5\u51c6\u786e\u6062\u590d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u8c03\u8d1f\u8f7d\u7f51\u7edc\uff08TLN\uff09\u589e\u52a0\u6d4b\u91cf\u591a\u6837\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u4e2aPF\u5b9e\u73b0\u8054\u5408\u89e3\u5d4c\u5165\uff08multiplexed de-embedding\uff09\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u8fdc\u7a0b\u4f30\u8ba1\u4e86\u4e00\u4e2a4\u7aef\u53e3DUT\u7684\u6563\u5c04\u77e9\u9635\uff0c\u5e76\u901a\u8fc730\u79cdPF\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aRFID\u548c\u65e0\u7ebf\u751f\u7269\u7535\u5b50\u7b49\u9886\u57df\u7684\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u65e0\u7ebf\u591a\u7aef\u53e3\u4f20\u611f\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.23155", "pdf": "https://arxiv.org/pdf/2509.23155", "abs": "https://arxiv.org/abs/2509.23155", "authors": ["Abdul Monaf Chowdhury", "Akm Moshiur Rahman Mazumder", "Rabeya Akter", "Safaeid Hossain Arib"], "title": "LAGEA: Language Guided Embodied Agents for Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation benefits from foundation models that describe goals, but\ntoday's agents still lack a principled way to learn from their own mistakes. We\nask whether natural language can serve as feedback, an error reasoning signal\nthat helps embodied agents diagnose what went wrong and correct course. We\nintroduce LAGEA (Language Guided Embodied Agents), a framework that turns\nepisodic, schema-constrained reflections from a vision language model (VLM)\ninto temporally grounded guidance for reinforcement learning. LAGEA summarizes\neach attempt in concise language, localizes the decisive moments in the\ntrajectory, aligns feedback with visual state in a shared representation, and\nconverts goal progress and feedback agreement into bounded, step-wise shaping\nrewardswhose influence is modulated by an adaptive, failure-aware coefficient.\nThis design yields dense signals early when exploration needs direction and\ngracefully recedes as competence grows. On the Meta-World MT10 embodied\nmanipulation benchmark, LAGEA improves average success over the\nstate-of-the-art (SOTA) methods by 9.0% on random goals and 5.3% on fixed\ngoals, while converging faster. These results support our hypothesis: language,\nwhen structured and grounded in time, is an effective mechanism for teaching\nrobots to self-reflect on mistakes and make better choices. Code will be\nreleased soon.", "AI": {"tldr": "LAGEA\u6846\u67b6\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u65f6\u95f4\u951a\u5b9a\u7684\u6307\u5bfc\uff0c\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u6548\u679c\uff0c\u5728Meta-World MT10\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u64cd\u63a7\u6a21\u578b\u7f3a\u4e4f\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u5b66\u4e60\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7814\u7a76\u81ea\u7136\u8bed\u8a00\u662f\u5426\u80fd\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\u5e2e\u52a9\u673a\u5668\u4eba\u8bca\u65ad\u9519\u8bef\u5e76\u4fee\u6b63\u884c\u4e3a\u3002", "method": "\u63d0\u51faLAGEA\u6846\u67b6\uff0c\u5229\u7528VLM\u751f\u6210\u8bed\u8a00\u603b\u7ed3\u548c\u65f6\u95f4\u951a\u5b9a\u7684\u53cd\u9988\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u754c\u7684\u9010\u6b65\u5956\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7cfb\u6570\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728Meta-World MT10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAGEA\u76f8\u6bd4SOTA\u65b9\u6cd5\u5728\u968f\u673a\u76ee\u6807\u548c\u56fa\u5b9a\u76ee\u6807\u4e0a\u5e73\u5747\u6210\u529f\u7387\u5206\u522b\u63d0\u53479.0%\u548c5.3%\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7ed3\u6784\u5316\u4e14\u65f6\u95f4\u951a\u5b9a\u7684\u8bed\u8a00\u53cd\u9988\u80fd\u6709\u6548\u5e2e\u52a9\u673a\u5668\u4eba\u81ea\u6211\u53cd\u601d\u5e76\u6539\u8fdb\u51b3\u7b56\u3002"}}
{"id": "2509.24588", "pdf": "https://arxiv.org/pdf/2509.24588", "abs": "https://arxiv.org/abs/2509.24588", "authors": ["Luis F. Abanto-Leon", "Muhammad Salman", "Lismer Andres Caceres-Najarro"], "title": "BARProp: Fast-Converging and Memory-Efficient RSS-Based Localization Algorithm for IoT", "categories": ["eess.SP"], "comment": "9 pages, 8 figures, and 4 tables", "summary": "Leveraging received signal strength (RSS) measurements for indoor\nlocalization is highly attractive due to their inherent availability in\nubiquitous wireless protocols. However, prevailing RSS-based methods often\ndepend on complex computational algorithms or specialized hardware, rendering\nthem impractical for low-cost access points. To address these challenges, this\npaper introduces buffer-aided RMSProp (BARProp), a fast and memory-efficient\nlocalization algorithm specifically designed for RSS-based tasks. The key\ninnovation of BARProp lies in a novel mechanism that dynamically adapts the\ndecay factor by monitoring the energy variations of recent gradients stored in\na buffer, thereby achieving both accelerated convergence and enhanced\nstability. Furthermore, BARProp requires less than 15% of the memory used by\nstate-of-the-art methods. Extensive evaluations with real-world data\ndemonstrate that BARProp not only achieves higher localization accuracy but\nalso delivers at least a fourfold improvement in convergence speed compared to\nexisting benchmarks.", "AI": {"tldr": "BARProp\u662f\u4e00\u79cd\u57fa\u4e8eRSS\u7684\u5feb\u901f\u3001\u5185\u5b58\u9ad8\u6548\u7684\u5ba4\u5185\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8870\u51cf\u56e0\u5b50\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528RSS\u8fdb\u884c\u5ba4\u5185\u5b9a\u4f4d\u867d\u7136\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7b97\u6cd5\u6216\u4e13\u7528\u786c\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f4e\u6210\u672c\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faBARProp\u7b97\u6cd5\uff0c\u901a\u8fc7\u76d1\u6d4b\u68af\u5ea6\u80fd\u91cf\u53d8\u5316\u52a8\u6001\u8c03\u6574\u8870\u51cf\u56e0\u5b50\uff0c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBARProp\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u5185\u5b58\u5360\u7528\u4ec5\u4e3a\u517615%\u3002", "conclusion": "BARProp\u4e3a\u4f4e\u6210\u672c\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684RSS\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23185", "pdf": "https://arxiv.org/pdf/2509.23185", "abs": "https://arxiv.org/abs/2509.23185", "authors": ["Ziyi Zhou", "Qian Meng", "Hadas Kress-Gazit", "Ye Zhao"], "title": "Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "We present an integrated planning framework for quadrupedal locomotion over\ndynamically changing, unforeseen terrains. Existing methods often depend on\nheuristics for real-time foothold selection-limiting robustness and\nadaptability-or rely on computationally intensive trajectory optimization\nacross complex terrains and long horizons. In contrast, our approach combines\nreactive synthesis for generating correct-by-construction symbolic-level\ncontrollers with mixed-integer convex programming (MICP) for dynamic and\nphysically feasible footstep planning during each symbolic transition. To\nreduce the reliance on costly MICP solves and accommodate specifications that\nmay be violated due to physical infeasibility, we adopt a symbolic repair\nmechanism that selectively generates only the required symbolic transitions.\nDuring execution, real-time MICP replanning based on actual terrain data,\ncombined with runtime symbolic repair and delay-aware coordination, enables\nseamless bridging between offline synthesis and online operation. Through\nextensive simulation and hardware experiments, we validate the framework's\nability to identify missing locomotion skills and respond effectively in\nsafety-critical environments, including scattered stepping stones and rebar\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u53d8\u5316\u3001\u672a\u77e5\u5730\u5f62\u4e0a\u8fd0\u52a8\u7684\u96c6\u6210\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u7ea7\u63a7\u5236\u5668\u548c\u6df7\u5408\u6574\u6570\u51f8\u89c4\u5212\uff08MICP\uff09\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6b65\u6001\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u53cd\u5e94\u5f0f\u5408\u6210\u548cMICP\u6b65\u6001\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u4fee\u590d\u673a\u5236\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u7f3a\u5931\u7684\u8fd0\u52a8\u6280\u80fd\u5e76\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u6709\u6548\u54cd\u5e94\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.24683", "pdf": "https://arxiv.org/pdf/2509.24683", "abs": "https://arxiv.org/abs/2509.24683", "authors": ["Johan Arbustini", "Eric Elzenheimer", "Elizaveta Spetzler", "Pablo Mendoza", "Daniel Fern\u00e1ndez", "Jordi Madrenas", "Jeffrey McCord", "Michael H\u00f6ft", "Robert Rieger", "Andreas Bahr"], "title": "Impedance Modeling of Magnetometers: A Path Toward Low-Noise Readout Circuits", "categories": ["eess.SP"], "comment": "4 pages, 3 figures, BMT2025 conference paper", "summary": "Optimizing sensor readout schemes and integrated circuit designs for both\nopen-loop and closed-loop implementations requires precise modeling and\nsimulation strategies. This study introduces a novel two-port impedance model\nto estimate the behavior of a converse Magnetoelectric (cME) sensor. This model\nprovides a possible framework for calculating transfer functions and simulating\nmagnetometer behavior in both continuous- and discrete-time simulation\nenvironments, and it is also possibly transferable to other magnetometer types.\nCommon S-parameters were measured experimentally using an impedance analyzer\nand converted to Z-parameters to create a transfer function for system-level\nsimulations. The model was validated through an analysis of output-related\nnoise using MATLAB and LTSpice simulations to optimize the noise of the analog\ncircuit parts of the system. The simulation results were compared with\nexperimental measurements using a Zurich Instruments lock-in amplifier and the\ncustom-designed low-noise printed circuit board (PCB) under model\nconsiderations. The proposed methodology derives noise considerations and the\ntransfer function of a magnetometer. These are essential for readout schemes\nfor mixed-signal circuit design. This allows low-noise electronics to be\ndesigned and extended to other sensor interface electronics, broadening their\napplicability in high-performance magnetic sensing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u7aef\u53e3\u963b\u6297\u6a21\u578b\uff0c\u7528\u4e8e\u4f30\u8ba1\u4e00\u79cd\u78c1\u7535\u4f20\u611f\u5668\u7684\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f18\u5316\u4f20\u611f\u5668\u8bfb\u51fa\u65b9\u6848\u548c\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u9700\u8981\u7cbe\u786e\u7684\u5efa\u6a21\u548c\u4eff\u771f\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7cbe\u5ea6\u78c1\u611f\u5e94\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u963b\u6297\u5206\u6790\u4eea\u6d4b\u91cfS\u53c2\u6570\u5e76\u8f6c\u6362\u4e3aZ\u53c2\u6570\uff0c\u6784\u5efa\u4f20\u9012\u51fd\u6570\uff0c\u4f7f\u7528MATLAB\u548cLTSpice\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6210\u529f\u4f18\u5316\u4e86\u7cfb\u7edf\u6a21\u62df\u7535\u8def\u90e8\u5206\u7684\u566a\u58f0\uff0c\u4e3a\u9ad8\u6027\u80fd\u78c1\u4f20\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u4f9d\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df7\u5408\u4fe1\u53f7\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u566a\u58f0\u5206\u6790\u548c\u4f20\u9012\u51fd\u6570\u7684\u63a8\u5bfc\uff0c\u62d3\u5bbd\u4e86\u5176\u5728\u9ad8\u6027\u80fd\u4f20\u611f\u5668\u63a5\u53e3\u7535\u5b50\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.23203", "pdf": "https://arxiv.org/pdf/2509.23203", "abs": "https://arxiv.org/abs/2509.23203", "authors": ["Kai Yang", "Tianlin Zhang", "Zhengbo Wang", "Zedong Chu", "Xiaolong Wu", "Yang Cai", "Mu Xu"], "title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Generalizing local navigation policies across diverse robot morphologies is a\ncritical challenge. Progress is often hindered by the need for costly and\nembodiment-specific data, the tight coupling of planning and control, and the\n\"disastrous averaging\" problem where deterministic models fail to capture\nmulti-modal decisions (e.g., turning left or right). We introduce CE-Nav, a\nnovel two-stage (IL-then-RL) framework that systematically decouples universal\ngeometric reasoning from embodiment-specific dynamic adaptation. First, we\ntrain an embodiment-agnostic General Expert offline using imitation learning.\nThis expert, a conditional normalizing flow model named VelFlow, learns the\nfull distribution of kinematically-sound actions from a large-scale dataset\ngenerated by a classical planner, completely avoiding real robot data and\nresolving the multi-modality issue. Second, for a new robot, we freeze the\nexpert and use it as a guiding prior to train a lightweight, Dynamics-Aware\nRefiner via online reinforcement learning. This refiner rapidly learns to\ncompensate for the target robot's specific dynamics and controller\nimperfections with minimal environmental interaction. Extensive experiments on\nquadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art\nperformance while drastically reducing adaptation cost. Successful real-world\ndeployments further validate our approach as an efficient and scalable solution\nfor building generalizable navigation systems.", "AI": {"tldr": "CE-Nav\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\uff08\u6a21\u4eff\u5b66\u4e60\u540e\u5f3a\u5316\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u901a\u7528\u51e0\u4f55\u63a8\u7406\u4e0e\u7279\u5b9a\u5f62\u6001\u7684\u52a8\u6001\u9002\u5e94\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u5bfc\u822a\u7b56\u7565\u6cdb\u5316\uff0c\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u8de8\u673a\u5668\u4eba\u5f62\u6001\u7684\u901a\u7528\u5bfc\u822a\u7b56\u7565\u95ee\u9898\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u9700\u6c42\u9ad8\u3001\u89c4\u5212\u4e0e\u63a7\u5236\u7d27\u5bc6\u8026\u5408\u53ca\u591a\u6a21\u6001\u51b3\u7b56\u95ee\u9898\u3002", "method": "CE-Nav\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2a\u5f62\u6001\u65e0\u5173\u7684\u901a\u7528\u4e13\u5bb6\uff08VelFlow\u6a21\u578b\uff09\uff0c\u518d\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u52a8\u6001\u9002\u5e94\u5668\uff0c\u5feb\u901f\u9002\u5e94\u65b0\u673a\u5668\u4eba\u7684\u52a8\u6001\u7279\u6027\u3002", "result": "\u5728\u56db\u8db3\u3001\u53cc\u8db3\u548c\u56db\u65cb\u7ffc\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCE-Nav\u6027\u80fd\u4f18\u5f02\u4e14\u663e\u8457\u964d\u4f4e\u4e86\u9002\u5e94\u6210\u672c\uff0c\u771f\u5b9e\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CE-Nav\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u901a\u7528\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u5f62\u6001\u7684\u5bfc\u822a\u7b56\u7565\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.24805", "pdf": "https://arxiv.org/pdf/2509.24805", "abs": "https://arxiv.org/abs/2509.24805", "authors": ["Andriy Enttsel", "Alex Marchioni", "Andrea Zanellini", "Mauro Mangia", "Gianluca Setti", "Riccardo Rovatti"], "title": "RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "12 pages, 11 figures", "summary": "Extensive monitoring systems generate data that is usually compressed for\nnetwork transmission. This compressed data might then be processed in the cloud\nfor tasks such as anomaly detection. However, compression can potentially\nimpair the detector's ability to distinguish between regular and irregular\npatterns due to information loss. Here we extend the information-theoretic\nframework introduced in [1] to simultaneously address the trade-off between the\nthree features on which the effectiveness of the system depends: the\neffectiveness of compression, the amount of distortion it introduces, and the\ndistinguishability between compressed normal signals and compressed anomalous\nsignals. We leverage a Gaussian assumption to draw curves showing how moving on\na Pareto surface helps administer such a trade-off better than simply relying\non optimal rate-distortion compression and hoping that compressed signals can\nbe distinguished from each other.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u538b\u7f29\u6570\u636e\u5bf9\u5f02\u5e38\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u8861\u538b\u7f29\u6548\u7387\u3001\u5931\u771f\u5ea6\u548c\u4fe1\u53f7\u53ef\u533a\u5206\u6027\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u4f20\u8f93\u7684\u6570\u636e\u53ef\u80fd\u56e0\u4fe1\u606f\u635f\u5931\u5f71\u54cd\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u540c\u65f6\u4f18\u5316\u538b\u7f29\u3001\u5931\u771f\u548c\u4fe1\u53f7\u533a\u5206\u5ea6\u3002", "method": "\u6269\u5c55\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af\u5047\u8bbe\u7ed8\u5236Pareto\u66f2\u7ebf\uff0c\u6743\u8861\u538b\u7f29\u6548\u7387\u3001\u5931\u771f\u548c\u4fe1\u53f7\u53ef\u533a\u5206\u6027\u3002", "result": "\u901a\u8fc7\u6743\u8861\u4e09\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u7ba1\u7406\u538b\u7f29\u4e0e\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u6700\u4f18\u7387\u5931\u771f\u538b\u7f29\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u4fe1\u53f7\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2509.23214", "pdf": "https://arxiv.org/pdf/2509.23214", "abs": "https://arxiv.org/abs/2509.23214", "authors": ["Benjamin Wong", "Aaron Weber", "Mohamed M. Safwat", "Santosh Devasia", "Ashis G. Banerjee"], "title": "Simulated Annealing for Multi-Robot Ergodic Information Acquisition Using Graph-Based Discretization", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "One of the goals of active information acquisition using multi-robot teams is\nto keep the relative uncertainty in each region at the same level to maintain\nidentical acquisition quality (e.g., consistent target detection) in all the\nregions. To achieve this goal, ergodic coverage can be used to assign the\nnumber of samples according to the quality of observation, i.e., sampling noise\nlevels. However, the noise levels are unknown to the robots. Although this\nnoise can be estimated from samples, the estimates are unreliable at first and\ncan generate fluctuating values. The main contribution of this paper is to use\nsimulated annealing to generate the target sampling distribution, starting from\nuniform and gradually shifting to an estimated optimal distribution, by varying\nthe coldness parameter of a Boltzmann distribution with the estimated sampling\nentropy as energy. Simulation results show a substantial improvement of both\ntransient and asymptotic entropy compared to both uniform and direct-ergodic\nsearches. Finally, a demonstration is performed with a TurtleBot swarm system\nto validate the physical applicability of the algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6a21\u62df\u9000\u706b\u7b97\u6cd5\u751f\u6210\u76ee\u6807\u91c7\u6837\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u4ee5\u5728\u591a\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u4fdd\u6301\u5404\u533a\u57df\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\uff0c\u4ece\u800c\u63d0\u9ad8\u4fe1\u606f\u83b7\u53d6\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u672a\u77e5\u566a\u58f0\u6c34\u5e73\u4e0b\u5982\u4f55\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03\uff0c\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u89c2\u6d4b\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff0c\u4ece\u5747\u5300\u5206\u5e03\u9010\u6b65\u8fc7\u6e21\u5230\u4f30\u8ba1\u7684\u6700\u4f18\u5206\u5e03\uff0c\u5229\u7528Boltzmann\u5206\u5e03\u548c\u91c7\u6837\u71b5\u4f5c\u4e3a\u80fd\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u77ac\u65f6\u548c\u6e10\u8fdb\u71b5\u4e0a\u5747\u4f18\u4e8e\u5747\u5300\u91c7\u6837\u548c\u76f4\u63a5\u904d\u5386\u641c\u7d22\uff0c\u5e76\u5728TurtleBot\u7fa4\u4f53\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u7269\u7406\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4fe1\u606f\u83b7\u53d6\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.24819", "pdf": "https://arxiv.org/pdf/2509.24819", "abs": "https://arxiv.org/abs/2509.24819", "authors": ["Kunyu Wu", "Qiushi Zhao", "Zihan Feng", "Yunxi Mu", "Hao Qin", "Xinyu Zhang", "Xingqi Zhang"], "title": "Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Urban railway systems increasingly rely on communication based train control\n(CBTC) systems, where optimal deployment of access points (APs) in tunnels is\ncritical for robust wireless coverage. Traditional methods, such as empirical\nmodel-based optimization algorithms, are hindered by excessive measurement\nrequirements and suboptimal solutions, while machine learning (ML) approaches\noften struggle with complex tunnel environments. This paper proposes a deep\nreinforcement learning (DRL) driven framework that integrates parabolic wave\nequation (PWE) channel modeling, conditional generative adversarial network\n(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for\nAP placement optimization. The PWE method generates high-fidelity path loss\ndistributions for a subset of AP positions, which are then expanded by the cGAN\nto create high resolution path loss maps for all candidate positions,\nsignificantly reducing simulation costs while maintaining physical accuracy. In\nthe DRL framework, the state space captures AP positions and coverage, the\naction space defines AP adjustments, and the reward function encourages signal\nimprovement while penalizing deployment costs. The dueling DQN enhances\nconvergence speed and exploration exploitation balance, increasing the\nlikelihood of reaching optimal configurations. Comparative experiments show\nthat the proposed method outperforms a conventional Hooke Jeeves optimizer and\ntraditional DQN, delivering AP configurations with higher average received\npower, better worst-case coverage, and improved computational efficiency. This\nwork integrates high-fidelity electromagnetic simulation, generative modeling,\nand AI-driven optimization, offering a scalable and data-efficient solution for\nnext-generation CBTC systems in complex tunnel environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u3001\u629b\u7269\u6ce2\u65b9\u7a0b\uff08PWE\uff09\u4fe1\u9053\u5efa\u6a21\u548c\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u96a7\u9053\u4e2d\u901a\u4fe1\u57fa\u5217\u8f66\u63a7\u5236\u7cfb\u7edf\uff08CBTC\uff09\u7684\u63a5\u5165\u70b9\uff08AP\uff09\u90e8\u7f72\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4eff\u771f\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u7ecf\u9a8c\u6a21\u578b\u4f18\u5316\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u6d4b\u91cf\u4e14\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u96a7\u9053\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u4f18\u5316AP\u90e8\u7f72\u3002", "method": "\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86PWE\u751f\u6210\u9ad8\u4fdd\u771f\u8def\u5f84\u635f\u8017\u5206\u5e03\uff0ccGAN\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u8def\u5f84\u635f\u8017\u56fe\uff0c\u5e76\u4f7f\u7528Dueling DQN\u8fdb\u884cAP\u4f4d\u7f6e\u4f18\u5316\u3002\u72b6\u6001\u7a7a\u95f4\u6355\u83b7AP\u4f4d\u7f6e\u548c\u8986\u76d6\u8303\u56f4\uff0c\u52a8\u4f5c\u7a7a\u95f4\u5b9a\u4e49AP\u8c03\u6574\uff0c\u5956\u52b1\u51fd\u6570\u5e73\u8861\u4fe1\u53f7\u6539\u5584\u4e0e\u90e8\u7f72\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684Hooke Jeeves\u4f18\u5316\u5668\u548c\u6807\u51c6DQN\u8868\u73b0\u66f4\u4f18\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u63a5\u6536\u529f\u7387\u3001\u66f4\u597d\u7684\u6700\u574f\u60c5\u51b5\u8986\u76d6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9ad8\u4fdd\u771f\u7535\u78c1\u6a21\u62df\u3001\u751f\u6210\u6a21\u578b\u548cAI\u9a71\u52a8\u7684\u4f18\u5316\uff0c\u4e3a\u590d\u6742\u96a7\u9053\u73af\u5883\u4e2d\u7684CBTC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23220", "pdf": "https://arxiv.org/pdf/2509.23220", "abs": "https://arxiv.org/abs/2509.23220", "authors": ["Ye Chen", "Zichen Zhou", "Jianyu Dou", "Te Cui", "Yi Yang", "Yufeng Yue"], "title": "GLUE: Global-Local Unified Encoding for Imitation Learning via Key-Patch Tracking", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "In recent years, visual representation learning has gained widespread\nattention in robotic imitation learning. However, in complex\nOut-of-Distribution(OOD) settings characterized by clutter and occlusion, the\nattention of global visual representations can be diluted or interfered,\nleading to degraded policy performance. The invariance of local representations\nfor task-relevant objects offers a solution. By efficiently utilizing these\nlocal representations, training and testing data can be mapped to a more\nsimilar feature space, thereby mitigating the covariate shift problem.\nAccordingly, we propose GLUE, a global-local unified encoding framework for\nimitation learning based on key-patch tracking. GLUE selects and tracks\nkey-patches as critical local representations by employing a text-guided\nmechanism. It features a novel fusion framework where global patch features\nquery local patches to distill essential information, yielding fine-grained\nlocal features with low heterogeneity relative to the global context. This\nfused representation steers the robot's visual attention toward task-relevant\nobjects and preserves precise global context, which together align the training\nand testing distributions into a similar and task-informative feature space,\nultimately enhancing the robustness of the imitation learning policy.\nExperiments demonstrate that GLUE achieves strong performance across diverse\ntasks in both simulation and real-world settings, outperforming the strongest\nbaseline by 17.6% in simulation, 36.3% in real-world environments, and 58.3% on\nreal-world generalization settings. The project website of GLUE is available at\nhttps://GLUE666.github.io/.", "AI": {"tldr": "GLUE\u662f\u4e00\u79cd\u5168\u5c40-\u5c40\u90e8\u7edf\u4e00\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u8865\u4e01\u8ddf\u8e2a\u89e3\u51b3\u590d\u6742\u5206\u5e03\u5916\u73af\u5883\u4e0b\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u590d\u6742\u5206\u5e03\u5916\uff08OOD\uff09\u73af\u5883\u4e2d\uff0c\u5168\u5c40\u89c6\u89c9\u8868\u793a\u7684\u6ce8\u610f\u529b\u53ef\u80fd\u88ab\u5e72\u6270\uff0c\u5bfc\u81f4\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u3002\u5c40\u90e8\u8868\u793a\u7684\u4efb\u52a1\u76f8\u5173\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGLUE\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u673a\u5236\u9009\u62e9\u5e76\u8ddf\u8e2a\u5173\u952e\u8865\u4e01\u4f5c\u4e3a\u5c40\u90e8\u8868\u793a\uff0c\u878d\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u4ee5\u751f\u6210\u4f4e\u5f02\u8d28\u6027\u5c40\u90e8\u7279\u5f81\u3002", "result": "GLUE\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5206\u522b\u8d85\u8d8a\u6700\u5f3a\u57fa\u7ebf17.6%\u300136.3%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u4efb\u52a1\u4e2d\u63d0\u534758.3%\u3002", "conclusion": "GLUE\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u878d\u5408\u6709\u6548\u5bf9\u9f50\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.24941", "pdf": "https://arxiv.org/pdf/2509.24941", "abs": "https://arxiv.org/abs/2509.24941", "authors": ["Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Emil Bj\u00f6rnson"], "title": "Low-Complexity Receiver Design for Multicarrier CAPA-based Systems in Doubly-Dispersive Channels", "categories": ["eess.SP"], "comment": "Submitted to an IEEE conference", "summary": "We propose a novel low-complexity receiver design for multicarrier continuous\naperture array (CAPA) systems operating over doubly-dispersive (DD) channels.\nThe receiver leverages a Gaussian Belief Propagation (GaBP)-based framework\nthat hinges only on element-wise scalar operations for the detection of the\ntransmitted symbols. Simulation results for the orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) waveforms demonstrate significant\nperformance improvements in terms of uncoded bit error rate (BER) compared to\nconventional discrete antenna array systems, while maintaining very low\ncomputational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u63a5\u6536\u5668\u8bbe\u8ba1\uff0c\u7528\u4e8e\u591a\u8f7d\u6ce2\u8fde\u7eed\u5b54\u5f84\u9635\u5217\uff08CAPA\uff09\u7cfb\u7edf\u5728\u53cc\u5206\u6563\u4fe1\u9053\u4e2d\u7684\u64cd\u4f5c\uff0c\u901a\u8fc7\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\uff08GaBP\uff09\u6846\u67b6\u5b9e\u73b0\u9ad8\u6027\u80fd\u68c0\u6d4b\u3002", "motivation": "\u9488\u5bf9\u53cc\u5206\u6563\u4fe1\u9053\u4e2d\u591a\u8f7d\u6ce2\u8fde\u7eed\u5b54\u5f84\u9635\u5217\u7cfb\u7edf\u7684\u4fe1\u53f7\u68c0\u6d4b\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u63a5\u6536\u5668\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u7f6e\u4fe1\u4f20\u64ad\uff08GaBP\uff09\u6846\u67b6\uff0c\u4ec5\u9700\u6807\u91cf\u8fd0\u7b97\u5b9e\u73b0\u7b26\u53f7\u68c0\u6d4b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u79bb\u6563\u5929\u7ebf\u9635\u5217\u7cfb\u7edf\u76f8\u6bd4\uff0cOFDM\u3001OTFS\u548cAFDM\u6ce2\u5f62\u5728\u672a\u7f16\u7801\u6bd4\u7279\u8bef\u7801\u7387\uff08BER\uff09\u65b9\u9762\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6781\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2509.23223", "pdf": "https://arxiv.org/pdf/2509.23223", "abs": "https://arxiv.org/abs/2509.23223", "authors": ["Aoqian Zhang", "Zixuan Zhuang", "Chunzheng Wang", "Shuzhi Sam Ge", "Fan Shi", "Cheng Xiang"], "title": "SAC-Loco: Safe and Adjustable Compliant Quadrupedal Locomotion", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Quadruped robots are designed to achieve agile locomotion by mimicking legged\nanimals. However, existing control methods for quadrupeds often lack one of the\nkey capabilities observed in animals: adaptive and adjustable compliance in\nresponse to external disturbances. Most locomotion controllers do not provide\ntunable compliance and tend to fail under large perturbations. In this work, we\npropose a switched policy framework for compliant and safe quadruped\nlocomotion. First, we train a force compliant policy with adjustable compliance\nlevels using a teacher student reinforcement learning framework, eliminating\nthe need for explicit force sensing. Next, we develop a safe policy based on\nthe capture point concept to stabilize the robot when the compliant policy\nfails. Finally, we introduce a recoverability network that predicts the\nlikelihood of failure and switches between the compliant and safe policies.\nTogether, this framework enables quadruped robots to achieve both force\ncompliance and robust safety when subjected to severe external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u5207\u6362\u7b56\u7565\u6846\u67b6\uff0c\u7ed3\u5408\u529b\u987a\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u56db\u8db3\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u7269\u822c\u7684\u81ea\u9002\u5e94\u548c\u53ef\u8c03\u987a\u5e94\u6027\uff0c\u6613\u53d7\u6270\u52a8\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5e08\u751f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u5177\u6709\u53ef\u8c03\u987a\u5e94\u6027\u7684\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6355\u83b7\u70b9\u7684\u5b89\u5168\u7b56\u7565\u4e0e\u53ef\u6062\u590d\u6027\u9884\u6d4b\u7f51\u7edc\u3002", "result": "\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e25\u91cd\u5916\u90e8\u6270\u52a8\u4e0b\u7684\u529b\u987a\u5e94\u6027\u548c\u7a33\u5065\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u987a\u5e94\u6027\u548c\u5b89\u5168\u6027\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.25095", "pdf": "https://arxiv.org/pdf/2509.25095", "abs": "https://arxiv.org/abs/2509.25095", "authors": ["M A Al-Masud", "Juan Miguel Lopez Alcaraz", "Nils Strodthoff"], "title": "Benchmarking ECG Foundational Models: A Reality Check Across Clinical Tasks", "categories": ["eess.SP", "cs.LG"], "comment": "26 pages, 3 figures source code under\n  https://github.com/AI4HealthUOL/ecg-fm-benchmarking", "summary": "The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet\nmachine learning for ECG interpretation remains fragmented, often limited to\nnarrow tasks or datasets. Foundation models promise broader adaptability, but\ntheir generalization across diverse ECG tasks is not well understood. We\nbenchmarked eight ECG foundation models on 26 clinically relevant tasks using\n12 public datasets comprising 1,650 regression and classification targets.\nModels were evaluated under fine-tuning and frozen settings, with scaling\nanalyses across dataset sizes. Results show heterogeneous performance across\ndomains: in the most widely studied domain, adult ECG interpretation, three\nfoundation models consistently outperformed strong supervised baselines. In\ncontrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,\ndominated other categories where most foundation models failed to surpass\nsupervised learning. Foundation models also displayed distinct scaling\nbehaviors with dataset size, which are critical for small-scale clinical\napplications. Overall, while foundation models show promise for adult ECG\nanalysis, substantial gaps remain in cardiac structure, outcome prediction, and\npatient characterization. Notably, ECG-CPC's strong performance despite being\norders of magnitude smaller and consuming minimal computational resources\nhighlights untapped opportunities for advancing ECG foundation models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e8612\u5bfc\u8054\u5fc3\u7535\u56fe\uff08ECG\uff09\u57fa\u7840\u6a21\u578b\u572826\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u6210\u4ebaECG\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u4ecd\u6709\u4e0d\u8db3\uff0c\u5c24\u5176\u662fECG-CPC\u6a21\u578b\u5728\u8d44\u6e90\u6d88\u8017\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u63a2\u7d22ECG\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u591f\u5728\u591a\u6837\u5316\u7684\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u586b\u8865\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728ECG\u89e3\u91ca\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u572812\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5bf98\u79cdECG\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d61,650\u4e2a\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u5fae\u8c03\u548c\u51bb\u7ed3\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6210\u4ebaECG\u89e3\u91ca\u4efb\u52a1\u4e2d\uff0c\u4e09\u79cd\u57fa\u7840\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u800cECG-CPC\u5728\u8d44\u6e90\u6709\u9650\u7684\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff1b\u4f46\u5728\u5fc3\u810f\u7ed3\u6784\u3001\u7ed3\u679c\u9884\u6d4b\u548c\u60a3\u8005\u7279\u5f81\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u6210\u4ebaECG\u5206\u6790\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5747\uff1bECG-CPC\u7684\u9ad8\u6548\u6027\u4e3a\u672a\u6765ECG\u57fa\u7840\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.23224", "pdf": "https://arxiv.org/pdf/2509.23224", "abs": "https://arxiv.org/abs/2509.23224", "authors": ["Kohei Sendai", "Maxime Alvarez", "Tatsuya Matsushima", "Yutaka Matsuo", "Yusuke Iwasawa"], "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA)\nmodels often predict action chunks; however, this action chunking harms\nreactivity under inference delay and long horizons. We introduce Asynchronous\nAction Chunk Correction (A2C2), which is a lightweight real-time chunk\ncorrection head that runs every control step and adds a time-aware correction\nto any off-the-shelf VLA's action chunk. The module combines the latest\nobservation, the predicted action from VLA (base action), a positional feature\nthat encodes the index of the base action within the chunk, and some features\nfrom the base policy, then outputs a per-step correction. This preserves the\nbase model's competence while restoring closed-loop responsiveness. The\napproach requires no retraining of the base policy and is orthogonal to\nasynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic\nKinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent\nsuccess rate improvements across increasing delays and execution horizons (+23%\npoint and +7% point respectively, compared to RTC), and also improves\nrobustness for long horizons even with zero injected delay. Since the\ncorrection head is small and fast, there is minimal overhead compared to the\ninference of large VLA models. These results indicate that A2C2 is an\neffective, plug-in mechanism for deploying high-capacity chunking policies in\nreal-time control.", "AI": {"tldr": "A2C2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b9e\u65f6\u52a8\u4f5c\u5757\u6821\u6b63\u6a21\u5757\uff0c\u53ef\u63d0\u9ad8VLA\u6a21\u578b\u5728\u5b9e\u65f6\u63a7\u5236\u4e2d\u7684\u53cd\u5e94\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u56e0\u52a8\u4f5c\u5757\u9884\u6d4b\u5bfc\u81f4\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u957f\u65f6\u4efb\u52a1\u53cd\u5e94\u6027\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6700\u65b0\u89c2\u6d4b\u3001\u57fa\u7840\u52a8\u4f5c\u3001\u4f4d\u7f6e\u7279\u5f81\u548c\u57fa\u7840\u7b56\u7565\u7279\u5f81\uff0c\u5b9e\u65f6\u8f93\u51fa\u6bcf\u6b65\u6821\u6b63\u52a8\u4f5c\u3002", "result": "\u5728\u52a8\u6001Kinetix\u4efb\u52a1\u548cLIBERO Spatial\u4e0a\uff0cA2C2\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "A2C2\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u673a\u5236\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u4e2d\u7684\u9ad8\u5bb9\u91cf\u52a8\u4f5c\u5757\u7b56\u7565\u3002"}}
{"id": "2509.23244", "pdf": "https://arxiv.org/pdf/2509.23244", "abs": "https://arxiv.org/abs/2509.23244", "authors": ["Shamir Matan", "Elhadad Osher", "Nageris Ben", "Mirsky Reuth"], "title": "Online Dynamic Goal Recognition in Gym Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Goal Recognition (GR) is the task of inferring an agent's intended goal from\npartial observations of its behavior, typically in an online and one-shot\nsetting. Despite recent advances in model-free GR, particularly in applications\nsuch as human-robot interaction, surveillance, and assistive systems, the field\nremains fragmented due to inconsistencies in benchmarks, domains, and\nevaluation protocols.\n  To address this, we introduce gr-libs\n(https://github.com/MatanShamir1/gr_libs) and gr-envs\n(https://github.com/MatanShamir1/gr_envs), two complementary open-source\nframeworks that support the development, evaluation, and comparison of GR\nalgorithms in Gym-compatible environments. gr-libs includes modular\nimplementations of MDP-based GR baselines, diagnostic tools, and evaluation\nutilities. gr-envs provides a curated suite of environments adapted for dynamic\nand goal-directed behavior, along with wrappers that ensure compatibility with\nstandard reinforcement learning toolkits. Together, these libraries offer a\nstandardized, extensible, and reproducible platform for advancing GR research.\nBoth packages are open-source and available on GitHub and PyPI.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u5f00\u6e90\u6846\u67b6\uff08gr-libs\u548cgr-envs\uff09\uff0c\u65e8\u5728\u6807\u51c6\u5316\u548c\u76ee\u6807\u8bc6\u522b\uff08GR\uff09\u7b97\u6cd5\u7684\u5f00\u53d1\u3001\u8bc4\u4f30\u4e0e\u6bd4\u8f83\uff0c\u89e3\u51b3\u9886\u57df\u5185\u56e0\u57fa\u51c6\u3001\u9886\u57df\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u5206\u88c2\u95ee\u9898\u3002", "motivation": "\u5f53\u524dGR\u9886\u57df\u5b58\u5728\u57fa\u51c6\u3001\u8bc4\u4f30\u6807\u51c6\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u7814\u7a76\u8fdb\u5c55\u548c\u7b97\u6cd5\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u4e92\u8865\u7684\u5f00\u6e90\u6846\u67b6\uff1agr-libs\u63d0\u4f9bGR\u7b97\u6cd5\u7684\u6a21\u5757\u5316\u5b9e\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\uff0cgr-envs\u63d0\u4f9b\u9002\u5408\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684Gym\u517c\u5bb9\u73af\u5883\u3002", "result": "\u8fd9\u4e24\u4e2a\u6846\u67b6\u4e3a\u6807\u51c6\u5316\u7684GR\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u590d\u7684\u5e73\u53f0\uff0c\u652f\u6301\u52a8\u6001\u548c\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684\u5206\u6790\u3002", "conclusion": "\u5f00\u6e90\u6846\u67b6gr-libs\u548cgr-envs\u4e3aGR\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2509.23230", "pdf": "https://arxiv.org/pdf/2509.23230", "abs": "https://arxiv.org/abs/2509.23230", "authors": ["Haoyu Wang", "Renyuan Ma", "Gonzalo Mateos", "Luana Ruiz"], "title": "A Generative Model for Controllable Feature Heterophily in Graphs", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "We introduce a principled generative framework for graph signals that enables\nexplicit control of feature heterophily, a key property underlying the\neffectiveness of graph learning methods. Our model combines a Lipschitz\ngraphon-based random graph generator with Gaussian node features filtered\nthrough a smooth spectral function of the rescaled Laplacian. We establish new\ntheoretical guarantees: (i) a concentration result for the empirical\nheterophily score; and (ii) almost-sure convergence of the feature heterophily\nmeasure to a deterministic functional of the graphon degree profile, based on a\ngraphon-limit law for polynomial averages of Laplacian eigenvalues. These\nresults elucidate how the interplay between the graphon and the filter governs\nthe limiting level of feature heterophily, providing a tunable mechanism for\ndata modeling and generation. We validate the theory through experiments\ndemonstrating precise control of homophily across graph families and spectral\nfilters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u56fe\u4fe1\u53f7\u7684\u6846\u67b6\uff0c\u53ef\u663e\u5f0f\u63a7\u5236\u7279\u5f81\u5f02\u8d28\u6027\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7279\u5f81\u5f02\u8d28\u6027\u662f\u5173\u952e\u5c5e\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u663e\u5f0f\u63a7\u5236\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "\u7ed3\u5408Lipschitz\u56fe\u5143\u968f\u673a\u56fe\u751f\u6210\u5668\u548c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u77e9\u9635\u5e73\u6ed1\u8c31\u51fd\u6570\u8fc7\u6ee4\u7684\u9ad8\u65af\u8282\u70b9\u7279\u5f81\u3002", "result": "\u5efa\u7acb\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u7ecf\u9a8c\u5f02\u8d28\u6027\u8bc4\u5206\u7684\u6536\u655b\u6027\u548c\u5f02\u8d28\u6027\u6d4b\u91cf\u7684\u786e\u5b9a\u6027\u51fd\u6570\u5316\u3002", "conclusion": "\u6846\u67b6\u901a\u8fc7\u56fe\u5143\u548c\u6ee4\u6ce2\u5668\u7684\u4ea4\u4e92\u63d0\u4f9b\u53ef\u8c03\u7684\u5f02\u8d28\u6027\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7cbe\u786e\u6027\u3002"}}
{"id": "2509.23281", "pdf": "https://arxiv.org/pdf/2509.23281", "abs": "https://arxiv.org/abs/2509.23281", "authors": ["Francesco Marchiori", "Rohan Sinha", "Christopher Agia", "Alexander Robey", "George J. Pappas", "Mauro Conti", "Marco Pavone"], "title": "Preventing Robotic Jailbreaking via Multimodal Domain Adaptation", "categories": ["cs.RO", "I.2.6; I.2.9"], "comment": "Project page: https://j-dapt.github.io/. 9 pages, 6 figures", "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) are\nincreasingly deployed in robotic environments but remain vulnerable to\njailbreaking attacks that bypass safety mechanisms and drive unsafe or\nphysically harmful behaviors in the real world. Data-driven defenses such as\njailbreak classifiers show promise, yet they struggle to generalize in domains\nwhere specialized datasets are scarce, limiting their effectiveness in robotics\nand other safety-critical contexts. To address this gap, we introduce J-DAPT, a\nlightweight framework for multimodal jailbreak detection through\nattention-based fusion and domain adaptation. J-DAPT integrates textual and\nvisual embeddings to capture both semantic intent and environmental grounding,\nwhile aligning general-purpose jailbreak datasets with domain-specific\nreference data. Evaluations across autonomous driving, maritime robotics, and\nquadruped navigation show that J-DAPT boosts detection accuracy to nearly 100%\nwith minimal overhead. These results demonstrate that J-DAPT provides a\npractical defense for securing VLMs in robotic applications. Additional\nmaterials are made available at: https://j-dapt.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86J-DAPT\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u548c\u57df\u9002\u5e94\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524dLLMs\u548cVLMs\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u6570\u636e\u9a71\u52a8\u9632\u5fa1\u5728\u673a\u5668\u4eba\u9886\u57df\u56e0\u6570\u636e\u96c6\u7a00\u7f3a\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "J-DAPT\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u548c\u591a\u6a21\u6001\u5d4c\u5165\uff08\u6587\u672c\u548c\u89c6\u89c9\uff09\uff0c\u7ed3\u5408\u9886\u57df\u9002\u914d\u6765\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u8868\u660eJ-DAPT\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u81f3\u8fd1100\uff05\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "J-DAPT\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684VLMs\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2509.23274", "pdf": "https://arxiv.org/pdf/2509.23274", "abs": "https://arxiv.org/abs/2509.23274", "authors": ["Yirun Wang", "Yongqing Wang", "Yuyao Shen", "Gongpu Wang", "Chintha Tellambura"], "title": "RIS- and Multi-Snapshot-Enabled SISO 3D Position and Velocity Estimation With Single Base Station", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "15 pages, 7 figures", "summary": "Reconfigurable intelligent surface (RIS) panels can act as cost-effective\nanchors for radio localization, complementing conventional base station (BS)\nanchors. This paper investigates joint three-dimensional position and velocity\nestimation (3D-JPVE) in single-input single-output (SISO) systems with only one\nBS available. We first theoretically show that 3D-JPVE is infeasible when\nrelying solely on a single RIS or on multiple snapshots alone. To address this,\nwe propose combining RIS deployment with multi-snapshot utilization to enable\nrealizable 3D-JPVE. A two-stage method is developed for multi-snapshot channel\nparameter estimation, comprising a tensor-based coarse estimation step followed\nby a maximum likelihood refinement step. In particular, we introduce a\nthird-order tensor formulation to decompose the challenging 3D joint\nangle-of-departure and Doppler shift estimation (3D-JADE) into two tractable\nsubproblems, which are jointly solved via a low-complexity alternating\noptimization approach. Building on the channel parameter estimates, we further\ndesign a two-stage low-complexity method for optimal 3D-JPVE: coarse estimation\nis obtained from differential measurements through linear equations, and the\npreliminary results are refined iteratively using the original measurements.\nMoreover, we derive the closed-form Cramer-Rao lower bound (CRLB) and show that\nthe proposed 3D-JPVE method approaches CRLB-level accuracy. Simulation results\nconfirm the statistical efficiency of the proposed estimators and demonstrate\nsubstantial 3D-JPVE performance gains when deploying active RIS compared to\npassive RIS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RIS\u548c\u591a\u5feb\u7167\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u5355\u57fa\u7ad9SISO\u7cfb\u7edf\u4e2d\u76843D\u4f4d\u7f6e\u548c\u901f\u5ea6\u8054\u5408\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5355RIS\u6216\u591a\u5feb\u7167\u5355\u72ec\u4f7f\u7528\u65f6\u65e0\u6cd5\u5b9e\u73b03D-JPVE\u7684\u95ee\u9898\uff0c\u8bba\u6587\u63a2\u7d22\u4e86RIS\u4e0e\u591a\u5feb\u7167\u7684\u7ed3\u5408\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5c063D-JADE\u95ee\u9898\u62c6\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u7ebf\u6027\u65b9\u7a0b\u548c\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b03D-JPVE\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7edf\u8ba1\u4e0a\u63a5\u8fd1CRLB\u7cbe\u5ea6\uff0c\u4e14\u4e3b\u52a8RIS\u7684\u6027\u80fd\u4f18\u4e8e\u88ab\u52a8RIS\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u7ed3\u5408RIS\u548c\u591a\u5feb\u7167\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4f4e\u590d\u6742\u5ea6\u5b9e\u73b0\u3002"}}
{"id": "2509.23288", "pdf": "https://arxiv.org/pdf/2509.23288", "abs": "https://arxiv.org/abs/2509.23288", "authors": ["Yafes Enes \u015eahiner", "Esat Yusuf G\u00fcndo\u011fdu", "Volkan Sezer"], "title": "A Novel Narrow Region Detector for Sampling-Based Planners' Efficiency: Match Based Passage Identifier", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous technology, which has become widespread today, appears in many\ndifferent configurations such as mobile robots, manipulators, and drones. One\nof the most important tasks of these vehicles during autonomous operations is\npath planning. In the literature, path planners are generally divided into two\ncategories: probabilistic and deterministic methods. In the analysis of\nprobabilistic methods, the common problem of almost all methods is observed in\nnarrow passage environments. In this paper, a novel sampler is proposed that\ndeterministically identifies narrow passage environments using occupancy grid\nmaps and accordingly increases the amount of sampling in these regions. The\ncodes of the algorithm is provided as open source. To evaluate the performance\nof the algorithm, benchmark studies are conducted in three distinct categories:\nspecific and random simulation environments, and a real-world environment. As a\nresult, it is observed that our algorithm provides higher performance in\nplanning time and number of milestones compared to the baseline samplers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u91c7\u6837\u5668\uff0c\u5229\u7528\u5360\u7528\u6805\u683c\u5730\u56fe\u786e\u5b9a\u6027\u5730\u8bc6\u522b\u72ed\u7a84\u901a\u9053\u73af\u5883\uff0c\u5e76\u589e\u52a0\u8fd9\u4e9b\u533a\u57df\u7684\u91c7\u6837\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u5f84\u89c4\u5212\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u6280\u672f\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u5728\u72ed\u7a84\u901a\u9053\u73af\u5883\u4e2d\u5b58\u5728\u666e\u904d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u5360\u7528\u6805\u683c\u5730\u56fe\u786e\u5b9a\u6027\u5730\u8bc6\u522b\u72ed\u7a84\u901a\u9053\uff0c\u5e76\u9488\u5bf9\u6027\u589e\u52a0\u91c7\u6837\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u89c4\u5212\u65f6\u95f4\u548c\u91cc\u7a0b\u7891\u6570\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u91c7\u6837\u5668\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u72ed\u7a84\u901a\u9053\u73af\u5883\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23364", "pdf": "https://arxiv.org/pdf/2509.23364", "abs": "https://arxiv.org/abs/2509.23364", "authors": ["Francesca Ronchini", "Luca Comanducci", "Simone Marcucci", "Fabio Antonacci"], "title": "AI-Assisted Music Production: A User Study on Text-to-Music Models", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": "Accepted at 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 25)", "summary": "Text-to-music models have revolutionized the creative landscape, offering new\npossibilities for music creation. Yet their integration into musicians\nworkflows remains underexplored. This paper presents a case study on how TTM\nmodels impact music production, based on a user study of their effect on\nproducers creative workflows. Participants produce tracks using a custom tool\ncombining TTM and source separation models. Semi-structured interviews and\nthematic analysis reveal key challenges, opportunities, and ethical\nconsiderations. The findings offer insights into the transformative potential\nof TTMs in music production, as well as challenges in their real-world\nintegration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u97f3\u4e50\uff08TTM\uff09\u6a21\u578b\u5bf9\u97f3\u4e50\u5236\u4f5c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u6848\u4f8b\u5206\u6790\u63ed\u793a\u4e86\u5176\u5728\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8TTM\u6a21\u578b\u5982\u4f55\u5b9e\u9645\u878d\u5165\u97f3\u4e50\u5236\u4f5c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7528\u6237\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u4f7f\u7528\u7ed3\u5408TTM\u548c\u6e90\u5206\u79bb\u6a21\u578b\u7684\u5de5\u5177\u5236\u4f5c\u97f3\u4e50\uff0c\u5e76\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86TTM\u6a21\u578b\u5728\u97f3\u4e50\u5236\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u5b9e\u9645\u96c6\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "TTM\u6a21\u578b\u5177\u6709\u53d8\u9769\u97f3\u4e50\u5236\u4f5c\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u548c\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2509.23308", "pdf": "https://arxiv.org/pdf/2509.23308", "abs": "https://arxiv.org/abs/2509.23308", "authors": ["Jun Chen", "Jiaqing Ma", "Philip Dames"], "title": "Distributed Multi-Robot Multi-Target Simultaneous Search and Tracking in an Unknown Non-convex Environment", "categories": ["cs.RO"], "comment": null, "summary": "In unknown non-convex environments, such as indoor and underground spaces,\ndeploying a fleet of robots to explore the surroundings while simultaneously\nsearching for and tracking targets of interest to maintain high-precision data\ncollection represents a fundamental challenge that urgently requires resolution\nin applications such as environmental monitoring and rescue operations. Current\nresearch has made significant progress in addressing environmental exploration,\ninformation search, and target tracking problems, but has yet to establish a\nframework for simultaneously optimizing these tasks in complex environments. In\nthis paper, we propose a novel motion planning algorithm framework that\nintegrates three control strategies: a frontier-based exploration strategy, a\nguaranteed coverage strategy based on Lloyd's algorithm, and a sensor-based\nmulti-target tracking strategy. By incorporating these three strategies, the\nproposed algorithm balances coverage search and high-precision active tracking\nduring exploration. Our approach is validated through a series of MATLAB\nsimulations, demonstrating validity and superiority over standard approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u6846\u67b6\uff0c\u6574\u5408\u524d\u6cbf\u63a2\u7d22\u3001\u8986\u76d6\u4fdd\u8bc1\u548c\u591a\u76ee\u6807\u8ffd\u8e2a\u7b56\u7565\uff0c\u4f18\u5316\u975e\u51f8\u73af\u5883\u4e2d\u673a\u5668\u4eba\u63a2\u7d22\u4e0e\u8ffd\u8e2a\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u975e\u51f8\u73af\u5883\u4e2d\u673a\u5668\u4eba\u540c\u65f6\u63a2\u7d22\u3001\u641c\u7d22\u548c\u8ffd\u8e2a\u76ee\u6807\u7684\u6311\u6218\uff0c\u6ee1\u8db3\u73af\u5883\u76d1\u6d4b\u548c\u6551\u63f4\u7b49\u5e94\u7528\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u524d\u6cbf\u63a2\u7d22\u7b56\u7565\u3001Lloyd\u7b97\u6cd5\u8986\u76d6\u4fdd\u8bc1\u7b56\u7565\u548c\u4f20\u611f\u5668\u591a\u76ee\u6807\u8ffd\u8e2a\u7b56\u7565\u3002", "result": "MATLAB\u4eff\u771f\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u5e73\u8861\u63a2\u7d22\u4e2d\u7684\u8986\u76d6\u641c\u7d22\u4e0e\u9ad8\u7cbe\u5ea6\u4e3b\u52a8\u8ffd\u8e2a\uff0c\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2509.23442", "pdf": "https://arxiv.org/pdf/2509.23442", "abs": "https://arxiv.org/abs/2509.23442", "authors": ["Md. Saiful Bari Siddiqui", "Mohammed Imamul Hassan Bhuiyan"], "title": "S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via Spatial-Spectral Summarizer Fusion Network", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "comment": "Submitted to IEEE Journal of Biomedical and Health Informatics\n  (JBHI). This preprint includes few additional details not present in the\n  journal submission", "summary": "Convolutional Neural Networks have become a cornerstone of medical image\nanalysis due to their proficiency in learning hierarchical spatial features.\nHowever, this focus on a single domain is inefficient at capturing global,\nholistic patterns and fails to explicitly model an image's frequency-domain\ncharacteristics. To address these challenges, we propose the Spatial-Spectral\nSummarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns\nfrom both spatial and spectral representations simultaneously. The S$^3$F-Net\nperforms a fusion of a deep spatial CNN with our proposed shallow spectral\nencoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer,\nwhich leverages the Convolution Theorem by applying a bank of learnable filters\ndirectly to an image's full Fourier spectrum via a computation-efficient\nelement-wise multiplication. This allows the SpectralFilter layer to attain a\nglobal receptive field instantaneously, with its output being distilled by a\nlightweight summarizer network. We evaluate S$^3$F-Net across four medical\nimaging datasets spanning different modalities to validate its efficacy and\ngeneralizability. Our framework consistently and significantly outperforms its\nstrong spatial-only baseline in all cases, with accuracy improvements of up to\n5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive\naccuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs\nbetter on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11%\naccuracy, surpassing many top-performing, much deeper models. Our\nexplainability analysis also reveals that the S$^3$F-Net learns to dynamically\nadjust its reliance on each branch based on the input pathology. These results\nverify that our dual-domain approach is a powerful and generalizable paradigm\nfor medical image analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS$^3$F-Net\u7684\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u548c\u9891\u57df\u8868\u793a\u6765\u89e3\u51b3CNN\u5728\u533b\u7597\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u5c55\u793a\u4e86\u901a\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u533b\u7597\u56fe\u50cf\u5206\u6790\u4e2d\u4e13\u6ce8\u4e8e\u7a7a\u95f4\u7279\u5f81\uff0c\u4f46\u5ffd\u7565\u4e86\u9891\u57df\u4fe1\u606f\uff0c\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u6a21\u5f0f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u548c\u9891\u57df\u8868\u793a\u7684\u53cc\u5206\u652f\u6846\u67b6\u3002", "method": "S$^3$F-Net\u7ed3\u5408\u4e86\u6df1\u5ea6\u7a7a\u95f4CNN\u548c\u6d45\u5c42\u9891\u57df\u7f16\u7801\u5668SpectraNet\uff0c\u540e\u8005\u901a\u8fc7SpectralFilter\u5c42\u76f4\u63a5\u5bf9\u56fe\u50cf\u7684\u5085\u91cc\u53f6\u8c31\u8fdb\u884c\u9ad8\u6548\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5168\u5c40\u611f\u53d7\u91ce\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u6a21\u6001\u7684\u533b\u7597\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cS$^3$F-Net\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u7a7a\u95f4\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u63d0\u53475.13%\uff0c\u5e76\u5728BRISC2025\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.76%\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u53cc\u57df\u5b66\u4e60\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u901a\u7528\u7684\u533b\u7597\u56fe\u50cf\u5206\u6790\u8303\u5f0f\uff0c\u80fd\u591f\u6839\u636e\u8f93\u5165\u75c5\u7406\u52a8\u6001\u8c03\u6574\u5bf9\u7a7a\u95f4\u548c\u9891\u57df\u4fe1\u606f\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.23312", "pdf": "https://arxiv.org/pdf/2509.23312", "abs": "https://arxiv.org/abs/2509.23312", "authors": ["Johannes A. Gaus", "Junheon Yoon", "Woo-Jeong Baek", "Seungwon Choi", "Suhan Park", "Jaeheung Park"], "title": "GUARD: Toward a Compromise between Traditional Control and Learning for Safe Robot Systems", "categories": ["cs.RO"], "comment": "Submitted as workshop paper to IEEE IROS 2025", "summary": "This paper presents the framework \\textbf{GUARD} (\\textbf{G}uided robot\ncontrol via \\textbf{U}ncertainty attribution and prob\\textbf{A}bilistic kernel\noptimization for \\textbf{R}isk-aware \\textbf{D}ecision making) that combines\ntraditional control with an uncertainty-aware perception technique using active\nlearning with real-time capability for safe robot collision avoidance. By doing\nso, this manuscript addresses the central challenge in robotics of finding a\nreasonable compromise between traditional methods and learning algorithms to\nfoster the development of safe, yet efficient and flexible applications. By\nunifying a reactive model predictive countouring control (RMPCC) with an\nIterative Closest Point (ICP) algorithm that enables the attribution of\nuncertainty sources online using active learning with real-time capability via\na probabilistic kernel optimization technique, \\emph{GUARD} inherently handles\nthe existing ambiguity of the term \\textit{safety} that exists in robotics\nliterature. Experimental studies indicate the high performance of \\emph{GUARD},\nthereby highlighting the relevance and need to broaden its applicability in\nfuture.", "AI": {"tldr": "GUARD\u6846\u67b6\u7ed3\u5408\u4f20\u7edf\u63a7\u5236\u4e0e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u5b9e\u65f6\u80fd\u529b\u5b9e\u73b0\u673a\u5668\u4eba\u5b89\u5168\u907f\u969c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e0e\u5b66\u4e60\u7b97\u6cd5\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63a8\u52a8\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u673a\u5668\u4eba\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u53cd\u5e94\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236(RMPCC)\u548c\u8fed\u4ee3\u6700\u8fd1\u70b9(ICP)\u7b97\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u6838\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGUARD\u6027\u80fd\u4f18\u8d8a\uff0c\u51f8\u663e\u5176\u5e7f\u6cdb\u5e94\u7528\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "GUARD\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5b89\u5168\u6027\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7684\u5e94\u7528\u62d3\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.23454", "pdf": "https://arxiv.org/pdf/2509.23454", "abs": "https://arxiv.org/abs/2509.23454", "authors": ["Md. Saiful Bari Siddiqui", "Utsab Saha"], "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "comment": "Submitted to ICASSP 2026. This preprint includes some additional\n  details beyond the conference submission", "summary": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training.", "AI": {"tldr": "AudioFuse\u7ed3\u54082D\u9891\u8c31\u56fe\u548c1D\u6ce2\u5f62\u4e24\u79cd\u4e92\u8865\u8868\u793a\uff0c\u901a\u8fc7\u5bbd\u6d45ViT\u548c\u6d45\u5c42CNN\u51cf\u8f7b\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u5728PCG\u5206\u7c7b\u4e2d\u53d6\u5f97\u4f18\u79c0\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u97f3\u9891\u4fe1\u53f7\uff08\u5982PCG\uff09\u7684\u9891\u8c31\u548c\u65f6\u57df\u4fe1\u606f\u5747\u5177\u8bca\u65ad\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf2D\u9891\u8c31\u56fe\u4f1a\u4e22\u5931\u76f8\u4f4d\u4fe1\u606f\u548c\u65f6\u95f4\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faAudioFuse\uff0c\u96c6\u6210\u5bbd\u6d45ViT\u5904\u7406\u9891\u8c31\u56fe\u548c\u6d45\u5c421D CNN\u5904\u7406\u6ce2\u5f62\uff0c\u901a\u8fc7\u4e92\u8865\u8868\u793a\u51cf\u8f7b\u8fc7\u62df\u5408\u98ce\u9669\u3002", "result": "\u5728PhysioNet 2016\u6570\u636e\u96c6\u4e0a\uff0cROC-AUC\u8fbe0.8608\uff0c\u8d85\u8d8a\u5355\u6a21\u6001\u57fa\u7ebf\uff1b\u5728PASCAL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\uff08ROC-AUC 0.7181\uff09\u3002", "conclusion": "\u878d\u5408\u4e92\u8865\u8868\u793a\u63d0\u4f9b\u5f3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5373\u53ef\u6784\u5efa\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u7684\u5206\u7c7b\u5668\u3002"}}
{"id": "2509.23328", "pdf": "https://arxiv.org/pdf/2509.23328", "abs": "https://arxiv.org/abs/2509.23328", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Space Robotics Bench: Robot Learning Beyond Earth", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "The growing ambition for space exploration demands robust autonomous systems\nthat can operate in unstructured environments under extreme extraterrestrial\nconditions. The adoption of robot learning in this domain is severely hindered\nby the prohibitive cost of technology demonstrations and the limited\navailability of data. To bridge this gap, we introduce the Space Robotics\nBench, an open-source simulation framework for robot learning in space. It\noffers a modular architecture that integrates on-demand procedural generation\nwith massively parallel simulation environments to support the creation of vast\nand diverse training distributions for learning-based agents. To ground\nresearch and enable direct comparison, the framework includes a comprehensive\nsuite of benchmark tasks that span a wide range of mission-relevant scenarios.\nWe establish performance baselines using standard reinforcement learning\nalgorithms and present a series of experimental case studies that investigate\nkey challenges in generalization, end-to-end learning, adaptive control, and\nsim-to-real transfer. Our results reveal insights into the limitations of\ncurrent methods and demonstrate the utility of the framework in producing\npolicies capable of real-world operation. These contributions establish the\nSpace Robotics Bench as a valuable resource for developing, benchmarking, and\ndeploying the robust autonomous systems required for the final frontier.", "AI": {"tldr": "Space Robotics Bench\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u62df\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u548c\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u73af\u5883\u652f\u6301\u673a\u5668\u4eba\u5b66\u4e60\u5728\u592a\u7a7a\u9886\u57df\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u6280\u672f\u6f14\u793a\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u592a\u7a7a\u63a2\u7d22\u9700\u8981\u80fd\u5728\u6781\u7aef\u5916\u661f\u73af\u5883\u4e0b\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4f46\u673a\u5668\u4eba\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u56e0\u9ad8\u6210\u672c\u548c\u6570\u636e\u7a00\u7f3a\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51faSpace Robotics Bench\u6846\u67b6\uff0c\u7ed3\u5408\u6309\u9700\u7a0b\u5e8f\u751f\u6210\u548c\u5927\u89c4\u6a21\u5e76\u884c\u4eff\u771f\u73af\u5883\uff0c\u521b\u5efa\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u57fa\u51c6\u4efb\u52a1\u5957\u4ef6\u3002", "result": "\u901a\u8fc7\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5efa\u7acb\u6027\u80fd\u57fa\u7ebf\uff0c\u63ed\u793a\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u6846\u67b6\u80fd\u751f\u6210\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u7b56\u7565\u3002", "conclusion": "Space Robotics Bench\u4e3a\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u592a\u7a7a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2509.23518", "pdf": "https://arxiv.org/pdf/2509.23518", "abs": "https://arxiv.org/abs/2509.23518", "authors": ["Ana Patr\u00edcia Pinto", "Rute Bettencourt", "Urbano J. Nunes", "Gabriel Pires"], "title": "Eye-Tracking and BCI Integration for Assistive Communication in Locked-In Syndrome: Pilot Study with Healthy Participants", "categories": ["cs.HC", "eess.SP"], "comment": "4 pages, 4 figures, accepted for the 8th IEEE ENBENG 2025 Conference", "summary": "Patients with Amyotrophic Lateral Sclerosis (ALS) progressively lose\nvoluntary motor control, often leading to a Locked-In State (LIS), or in severe\ncases, a Completely Locked-in State (CLIS). Eye-tracking (ET) systems are\ncommon communication tools in early LIS but become ineffective as oculomotor\nfunction declines. EEG-based Brain-Computer Interfaces (BCIs) offer a\nnon-muscular communication alternative, but delayed adoption may reduce\nperformance due to diminished goal-directed thinking. This study presents a\npreliminary hybrid BCI framework combining ET and BCI to support a gradual\ntransition between modalities. A group of five healthy participants tested a\nmodified P300-based BCI. Gaze and EEG data were processed in real time, and an\nET-BCI fusion algorithm was proposed to enhance detection of user intention.\nResults indicate that combining both modalities may maintain high accuracy and\noffers insights on how to potentially improve communication continuity for\npatients transitioning from LIS to CLIS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\uff08ET\uff09\u548c\u8111\u7535\u6ce2\uff08BCI\uff09\u7684\u6df7\u5408BCI\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301ALS\u60a3\u8005\u4ece\u90e8\u5206\u9501\u5b9a\u72b6\u6001\uff08LIS\uff09\u8fc7\u6e21\u5230\u5b8c\u5168\u9501\u5b9a\u72b6\u6001\uff08CLIS\uff09\u65f6\u7684\u6c9f\u901a\u8fde\u7eed\u6027\u3002", "motivation": "ALS\u60a3\u8005\u968f\u7740\u75c5\u60c5\u53d1\u5c55\u4f1a\u4e27\u5931\u773c\u52a8\u529f\u80fd\uff0c\u5bfc\u81f4ET\u7cfb\u7edf\u5931\u6548\u3002\u4f20\u7edfBCI\u53ef\u80fd\u56e0\u5ef6\u8fdf\u4f7f\u7528\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408ET\u548cBCI\u7684\u8fc7\u6e21\u65b9\u6848\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408BCI\u6846\u67b6\uff0c\u7ed3\u5408ET\u548cP300 BCI\uff0c\u5e76\u5f00\u53d1\u4e86ET-BCI\u878d\u5408\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u5904\u7406\u6570\u636e\u589e\u5f3a\u7528\u6237\u610f\u56fe\u68c0\u6d4b\u3002", "result": "\u4e94\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u521d\u6b65\u6d4b\u8bd5\u8868\u660e\uff0c\u7ec4\u5408\u4e24\u79cd\u6a21\u6001\u53ef\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u4e3aLIS\u5230CLIS\u7684\u8fc7\u6e21\u63d0\u4f9b\u6f5c\u5728\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u6df7\u5408BCI\u6846\u67b6\u6709\u671b\u7ef4\u6301ALS\u60a3\u8005\u7684\u6c9f\u901a\u8fde\u7eed\u6027\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u4e34\u5e8a\u7814\u7a76\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.23456", "pdf": "https://arxiv.org/pdf/2509.23456", "abs": "https://arxiv.org/abs/2509.23456", "authors": ["Arjun Sadananda", "Ravi Banavar", "Kavi Arya"], "title": "Robust Orientation Estimation with TRIAD-aided Manifold EKF", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "The manifold extended Kalman filter (Manifold EKF) has found extensive\napplication for attitude determination. Magnetometers employed as sensors for\nsuch attitude determination are easily prone to disturbances by their\nsensitivity to calibration and external magnetic fields. The TRIAD (Tri-Axial\nAttitude Determination) algorithm is well known as a sub-optimal attitude\nestimator. In this article, we incorporate this sub-optimal feature of the\nTRIAD in mitigating the influence of the magnetometer reading in the pitch and\nroll axis determination in the Manifold EKF algorithm. We substantiate our\nresults with experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TRIAD\u7b97\u6cd5\u548cManifold EKF\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u78c1\u529b\u8ba1\u5bf9\u4fef\u4ef0\u548c\u6a2a\u6eda\u8f74\u59ff\u6001\u786e\u5b9a\u7684\u5e72\u6270\u3002", "motivation": "\u78c1\u529b\u8ba1\u4f5c\u4e3a\u59ff\u6001\u786e\u5b9a\u7684\u4f20\u611f\u5668\u5bb9\u6613\u53d7\u5230\u6821\u51c6\u548c\u5916\u90e8\u78c1\u573a\u7684\u5e72\u6270\uff0c\u5f71\u54cdManifold EKF\u7684\u7cbe\u5ea6\u3002TRIAD\u7b97\u6cd5\u867d\u7136\u6b21\u4f18\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u8fd9\u79cd\u5e72\u6270\u3002", "method": "\u901a\u8fc7\u5c06TRIAD\u7b97\u6cd5\u7684\u6b21\u4f18\u7279\u6027\u878d\u5165Manifold EKF\u4e2d\uff0c\u51cf\u5c11\u78c1\u529b\u8ba1\u8bfb\u6570\u5728\u4fef\u4ef0\u548c\u6a2a\u6eda\u8f74\u786e\u5b9a\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u51cf\u8f7b\u78c1\u529b\u8ba1\u5e72\u6270\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408TRIAD\u7684Manifold EKF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u59ff\u6001\u786e\u5b9a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23524", "pdf": "https://arxiv.org/pdf/2509.23524", "abs": "https://arxiv.org/abs/2509.23524", "authors": ["Nicoli Leal", "Rute Bettencourt", "Urbano J. Nunes", "Gabriel Pires"], "title": "EEG-Based Framework for Reflexive and Perceptual Assessment in CLIS: Preliminary Study in Healthy Volunteers", "categories": ["q-bio.NC", "eess.SP"], "comment": "4 pages, 5 figures, accepted for 8th IEEE ENBENG Conference", "summary": "Despite the general assumption that completely locked-in state (CLIS)\npatients remain conscious and aware of their environment, the effectiveness of\nbrain-computer interfaces (BCIs) in facilitating communication has been\nlimited, as reported both in the literature and in our own findings. This\nlimitation is likely attributable to impairments in executive functions,\nworking memory, and vigilance, which appear to hinder the establishment of\nreliable BCI-based communication. The main goal of this research is to develop\na neurophysiological report designed to support the evaluation of the cognitive\nstate of these individuals and determine their ability to interact with BCIs.\nTo achieve this, we designed a set of paradigms to assess CLIS patients at the\nreflexive and perceptual levels, based on neural responses associated with\nsensory and perceptual processing, including Mismatch Negativity (MMN), Steady\nState Auditory Evoked Potential (SSAEP), and Steady State Visual Evoked\nPotential (SSVEP). Pilot testing with five healthy participants demonstrates\nthe feasibility of generating a neurophysiological report for cognitive\nassessment at both levels.", "AI": {"tldr": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u795e\u7ecf\u751f\u7406\u62a5\u544a\uff0c\u8bc4\u4f30\u5b8c\u5168\u9501\u95ed\u72b6\u6001\uff08CLIS\uff09\u60a3\u8005\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u4ee5\u652f\u6301BCI\u4ea4\u4e92\u3002", "motivation": "\u5c3d\u7ba1CLIS\u60a3\u8005\u88ab\u8ba4\u4e3a\u6709\u610f\u8bc6\uff0c\u4f46BCI\u901a\u4fe1\u6548\u679c\u6709\u9650\uff0c\u53ef\u80fd\u4e0e\u6267\u884c\u529f\u80fd\u3001\u5de5\u4f5c\u8bb0\u5fc6\u548c\u8b66\u89c9\u6027\u53d7\u635f\u6709\u5173\u3002", "method": "\u8bbe\u8ba1\u53cd\u5c04\u548c\u611f\u77e5\u5c42\u9762\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u57fa\u4e8eMMN\u3001SSAEP\u548cSSVEP\u7b49\u795e\u7ecf\u53cd\u5e94\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u8868\u660e\uff0c\u795e\u7ecf\u751f\u7406\u62a5\u544a\u5728\u5065\u5eb7\u53c2\u4e0e\u8005\u4e2d\u53ef\u884c\u3002", "conclusion": "\u7814\u7a76\u4e3aCLIS\u60a3\u8005\u7684\u8ba4\u77e5\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6f5c\u5728\u5de5\u5177\uff0c\u652f\u6301BCI\u4ea4\u4e92\u80fd\u529b\u5224\u65ad\u3002"}}
{"id": "2509.23468", "pdf": "https://arxiv.org/pdf/2509.23468", "abs": "https://arxiv.org/abs/2509.23468", "authors": ["Haonan Chen", "Jiaming Xu", "Hongyu Chen", "Kaiwen Hong", "Binghao Huang", "Chaoqi Liu", "Jiayuan Mao", "Yunzhu Li", "Yilun Du", "Katherine Driggs-Campbell"], "title": "Multi-Modal Manipulation via Multi-Modal Policy Consensus", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages, 7 figures", "summary": "Effectively integrating diverse sensory modalities is crucial for robotic\nmanipulation. However, the typical approach of feature concatenation is often\nsuboptimal: dominant modalities such as vision can overwhelm sparse but\ncritical signals like touch in contact-rich tasks, and monolithic architectures\ncannot flexibly incorporate new or missing modalities without retraining. Our\nmethod factorizes the policy into a set of diffusion models, each specialized\nfor a single representation (e.g., vision or touch), and employs a router\nnetwork that learns consensus weights to adaptively combine their\ncontributions, enabling incremental of new representations. We evaluate our\napproach on simulated manipulation tasks in {RLBench}, as well as real-world\ntasks such as occluded object picking, in-hand spoon reorientation, and puzzle\ninsertion, where it significantly outperforms feature-concatenation baselines\non scenarios requiring multimodal reasoning. Our policy further demonstrates\nrobustness to physical perturbations and sensor corruption. We further conduct\nperturbation-based importance analysis, which reveals adaptive shifts between\nmodalities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u89e3\u7b56\u7565\u4e3a\u591a\u4e2a\u6269\u6563\u6a21\u578b\u5e76\u5229\u7528\u8def\u7531\u7f51\u7edc\u52a8\u6001\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u7684\u4e0d\u8db3\uff0c\u4f20\u7edf\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\u6613\u53d7\u4e3b\u5bfc\u6a21\u6001\uff08\u5982\u89c6\u89c9\uff09\u5f71\u54cd\uff0c\u4e14\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u65b0\u6a21\u6001\u6216\u7f3a\u5931\u6a21\u6001\u3002", "method": "\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u7ec4\u4e13\u95e8\u5904\u7406\u5355\u4e2a\u6a21\u6001\uff08\u5982\u89c6\u89c9\u6216\u89e6\u89c9\uff09\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u6743\u91cd\u6765\u52a8\u6001\u7ec4\u5408\u5404\u6a21\u6001\u7684\u8d21\u732e\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u5bf9\u7269\u7406\u6270\u52a8\u548c\u4f20\u611f\u5668\u635f\u574f\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6574\u5408\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u80fd\u81ea\u9002\u5e94\u4e0d\u540c\u6a21\u6001\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.23590", "pdf": "https://arxiv.org/pdf/2509.23590", "abs": "https://arxiv.org/abs/2509.23590", "authors": ["Fangyu Liu", "Peiwen Jiang", "Wenjin Wang", "Chao-Kai Wen", "Shi Jin", "Jun Zhang"], "title": "Foundation Model-Based Adaptive Semantic Image Transmission for Dynamic Wireless Environments", "categories": ["eess.IV", "eess.SP"], "comment": null, "summary": "Foundation model-based semantic transmission has recently shown great\npotential in wireless image communication. However, existing methods exhibit\ntwo major limitations: (i) they overlook the varying importance of semantic\ncomponents for specific downstream tasks, and (ii) they insufficiently exploit\nwireless domain knowledge, resulting in limited robustness under dynamic\nchannel conditions. To overcome these challenges, this paper proposes a\nfoundation model-based adaptive semantic image transmission system for dynamic\nwireless environments, such as autonomous driving. The proposed system\ndecomposes each image into a semantic segmentation map and a compressed\nrepresentation, enabling task-aware prioritization of critical objects and\nfine-grained textures. A task-adaptive precoding mechanism then allocates radio\nresources according to the semantic importance of extracted features. To ensure\naccurate channel information for precoding, a channel estimation knowledge map\n(CEKM) is constructed using a conditional diffusion model that integrates user\nposition, velocity, and sparse channel samples to train scenario-specific\nlightweight estimators. At the receiver, a conditional diffusion model\nreconstructs high-quality images from the received semantic features, ensuring\nrobustness against channel impairments and partial data loss. Simulation\nresults on the BDD100K dataset with multi-scenario channels generated by\nQuaDRiGa demonstrate that the proposed method outperforms existing approaches\nin terms of perceptual quality (SSIM, LPIPS, FID), task-specific accuracy\n(IoU), and transmission efficiency. These results highlight the effectiveness\nof integrating task-aware semantic decomposition, scenario-adaptive channel\nestimation, and diffusion-based reconstruction for robust semantic transmission\nin dynamic wireless environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u81ea\u9002\u5e94\u8bed\u4e49\u56fe\u50cf\u4f20\u8f93\u7cfb\u7edf\uff0c\u7528\u4e8e\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u5206\u89e3\u56fe\u50cf\u4e3a\u8bed\u4e49\u5206\u5272\u56fe\u548c\u538b\u7f29\u8868\u793a\uff0c\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u7684\u8d44\u6e90\u5206\u914d\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u4f20\u8f93\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bed\u4e49\u7ec4\u4ef6\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u65e0\u7ebf\u9886\u57df\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u4fe1\u9053\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8bed\u4e49\u5206\u5272\u56fe\u548c\u538b\u7f29\u8868\u793a\uff0c\u91c7\u7528\u4efb\u52a1\u81ea\u9002\u5e94\u9884\u7f16\u7801\u673a\u5236\u5206\u914d\u8d44\u6e90\uff0c\u5e76\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6784\u5efa\u4fe1\u9053\u4f30\u8ba1\u77e5\u8bc6\u56fe\u548c\u63a5\u6536\u7aef\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u5728BDD100K\u6570\u636e\u96c6\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u3001\u4efb\u52a1\u51c6\u786e\u6027\u548c\u4f20\u8f93\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u8bed\u4e49\u5206\u89e3\u3001\u573a\u666f\u81ea\u9002\u5e94\u4fe1\u9053\u4f30\u8ba1\u548c\u6269\u6563\u91cd\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u4f20\u8f93\u6548\u679c\u3002"}}
{"id": "2509.23506", "pdf": "https://arxiv.org/pdf/2509.23506", "abs": "https://arxiv.org/abs/2509.23506", "authors": ["Dan BW Choe", "Sundhar Vinodh Sangeetha", "Steven Emanuel", "Chih-Yuan Chiu", "Samuel Coogan", "Shreyas Kousik"], "title": "Ask, Reason, Assist: Decentralized Robot Collaboration via Language and Logic", "categories": ["cs.RO"], "comment": null, "summary": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel decentralized\nframework that enables robots to request and provide help. The process begins\nwhen a robot detects a conflict and uses a Large Language Model (LLM) to decide\nwhether external assistance is required. If so, it crafts and broadcasts a\nnatural language (NL) help request. Potential helper robots reason over the\nrequest and respond with offers of assistance, including information about the\neffect on their ongoing tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar,\nensuring syntactically valid NL-to-STL translations, which are then solved as a\nMixed Integer Linear Program (MILP). Finally, the requester robot selects a\nhelper by reasoning over the expected increase in system-level total task\ncompletion time. We evaluated our framework through experiments comparing\ndifferent helper-selection strategies and found that considering multiple\noffers allows the requester to minimize added makespan. Our approach\nsignificantly outperforms heuristics such as selecting the nearest available\ncandidate helper robot, and achieves performance comparable to a centralized\n\"Oracle\" baseline but without heavy information demands.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u903b\u8f91\u7ffb\u8bd1\u5b9e\u73b0\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u65e0\u7f1d\u534f\u4f5c\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u4ed3\u50a8\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u4e4b\u95f4\u7684\u534f\u4f5c\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u89e3\u51b3\u7a81\u53d1\u51b2\u7a81\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u548c\u4fe1\u53f7\u65f6\u6001\u903b\u8f91\u7ffb\u8bd1\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u9009\u62e9\u6700\u8fd1\u53ef\u7528\u673a\u5668\u4eba\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u9ad8\u6548\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23785", "pdf": "https://arxiv.org/pdf/2509.23785", "abs": "https://arxiv.org/abs/2509.23785", "authors": ["Fabian Fl\u00fcrenbrock", "Christian T. Stoeck", "Markus F. Oertel", "Miriam Weisskopf", "Melanie N. Zeilinger", "Marianne Schmid Daners", "Leonie Korn"], "title": "A quantitative analysis of intraventricular bioimpedance in an in vivo pilot study with contextual pressure measurements", "categories": ["physics.med-ph", "eess.SP"], "comment": null, "summary": "Hydrocephalus is a neurological condition characterized by disturbed\ncerebrospinal fluid (CSF) dynamics and is typically treated with shunt systems\nthat drain excessive CSF out of the ventricular system. Continuous monitoring\nof ventricular CSF volume, however, remains a major unmet need in the clinical\nmanagement of this condition. While intraventricular bioimpedance (BI) has been\nproposed as a potential marker of CSF volume, prior investigations have been\nlimited to simulations, in vitro phantoms, and small animal models. This work\npresents the development of a measurement system for intraventricular BI and\nits evaluation in a large animal model. The measurement system was first\nvalidated in vitro using a mechatronic test bench replicating physiological CSF\ndynamics and subsequently applied in an in vivo pilot study with concurrent CSF\nand blood pressure monitoring. Time series analysis of the recorded signals\nrevealed physiological BI waveform components linked to the cardiac and\nrespiratory cycles. In addition, changes in BI following CSF volume alterations\ninduced through intrathecal bolus infusions of artificial CSF were observed and\nfound to be correlated to changes in CSF and blood pressures. These results\nprovide the first in vivo evidence in a large animal model that BI reflects CSF\ndynamics as well as cerebral hemodynamics. Complementing intracranial pressure\nand CSF drainage measurements in smart shunt systems with BI could enable more\ncomprehensive patient monitoring and physiologically informed control of\nhydrocephalus therapy.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u8111\u5ba4\u5185\u751f\u7269\u963b\u6297\uff08BI\uff09\u76d1\u6d4b\u7684\u7cfb\u7edf\uff0c\u5e76\u5728\u5927\u578b\u52a8\u7269\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u53cd\u6620\u8111\u810a\u6db2\uff08CSF\uff09\u52a8\u529b\u5b66\u548c\u8111\u8840\u6d41\u52a8\u529b\u5b66\uff0c\u6709\u671b\u4e3a\u8111\u79ef\u6c34\u7684\u4e34\u5e8a\u7ba1\u7406\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u76d1\u6d4b\u624b\u6bb5\u3002", "motivation": "\u8111\u79ef\u6c34\u7684\u4e34\u5e8a\u7ba1\u7406\u4e2d\uff0c\u6301\u7eed\u76d1\u6d4b\u8111\u810a\u6db2\uff08CSF\uff09\u4f53\u79ef\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u9700\u6c42\u3002\u5c3d\u7ba1\u751f\u7269\u963b\u6297\uff08BI\uff09\u5df2\u88ab\u63d0\u51fa\u4f5c\u4e3a\u6f5c\u5728\u7684CSF\u4f53\u79ef\u6807\u5fd7\u7269\uff0c\u4f46\u5148\u524d\u7814\u7a76\u4ec5\u9650\u4e8e\u6a21\u62df\u3001\u4f53\u5916\u6a21\u578b\u548c\u5c0f\u52a8\u7269\u5b9e\u9a8c\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u8111\u5ba4\u5185BI\u6d4b\u91cf\u7cfb\u7edf\uff0c\u5148\u5728\u4f53\u5916\u6a21\u62df\u751f\u7406CSF\u52a8\u529b\u5b66\u7684\u673a\u7535\u6d4b\u8bd5\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u968f\u540e\u5728\u5927\u578b\u52a8\u7269\u6a21\u578b\u4e2d\u8fdb\u884c\u4f53\u5185\u5b9e\u9a8c\uff0c\u5e76\u4e0eCSF\u548c\u8840\u538b\u76d1\u6d4b\u540c\u6b65\u8fdb\u884c\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u5fc3\u7387\u548c\u547c\u5438\u5468\u671f\u76f8\u5173\u7684\u751f\u7406BI\u6ce2\u5f62\u6210\u5206\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u9798\u5185\u6ce8\u5c04\u4eba\u5de5CSF\u8bf1\u5bfcCSF\u4f53\u79ef\u53d8\u5316\u65f6\uff0cBI\u53d8\u5316\u4e0eCSF\u548c\u8840\u538b\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u5927\u578b\u52a8\u7269\u6a21\u578b\u4e2d\u8bc1\u660eBI\u80fd\u591f\u53cd\u6620CSF\u52a8\u529b\u5b66\u548c\u8111\u8840\u6d41\u52a8\u529b\u5b66\uff0c\u672a\u6765\u7ed3\u5408\u9885\u5185\u538b\u548cCSF\u5f15\u6d41\u76d1\u6d4b\uff0c\u6709\u671b\u4e3a\u8111\u79ef\u6c34\u6cbb\u7597\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u76d1\u6d4b\u548c\u63a7\u5236\u624b\u6bb5\u3002"}}
{"id": "2509.23556", "pdf": "https://arxiv.org/pdf/2509.23556", "abs": "https://arxiv.org/abs/2509.23556", "authors": ["Curtis C. Johnson", "Carlo Alessi", "Egidio Falotico", "Marc D. Killpack"], "title": "Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso via Guided Reinforcement Learning", "categories": ["cs.RO"], "comment": "Submitted to IEEE Transactions on Robotics for review", "summary": "Whole-body manipulation is a powerful yet underexplored approach that enables\nrobots to interact with large, heavy, or awkward objects using more than just\ntheir end-effectors. Soft robots, with their inherent passive compliance, are\nparticularly well-suited for such contact-rich manipulation tasks, but their\nuncertainties in kinematics and dynamics pose significant challenges for\nsimulation and control. In this work, we address this challenge with a\nsimulation that can run up to 350x real time on a single thread in MuJoCo and\nprovide a detailed analysis of the critical tradeoffs between speed and\naccuracy for this simulation. Using this framework, we demonstrate a successful\nzero-shot sim-to-real transfer of a learned whole-body manipulation policy,\nachieving an 88% success rate on the Baloo hardware platform. We show that\nguiding RL with a simple motion primitive is critical to this success where\nstandard reward shaping methods struggled to produce a stable and successful\npolicy for whole-body manipulation. Furthermore, our analysis reveals that the\nlearned policy does not simply mimic the motion primitive. It exhibits\nbeneficial reactive behavior, such as re-grasping and perturbation recovery. We\nanalyze and contrast this learned policy against an open-loop baseline to show\nthat the policy can also exhibit aggressive over-corrections under\nperturbation. To our knowledge, this is the first demonstration of forceful,\nsix-DoF whole-body manipulation using two continuum soft arms on a large-scale\nplatform (10 kg payloads), with zero-shot policy transfer.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4eff\u771f\u65b9\u6cd5\uff0c\u652f\u6301\u8f6f\u673a\u5668\u4eba\u5168\u8eab\u64cd\u63a7\u4efb\u52a1\u7684\u5feb\u901f\u4eff\u771f\u4e0e\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u96f6\u6b21\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8f6c\u79fb\uff0c\u5728\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e8688%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8f6f\u673a\u5668\u4eba\u5728\u5168\u8eab\u64cd\u63a7\u4efb\u52a1\u4e2d\u5b58\u5728\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u5b66\u4e0d\u786e\u5b9a\u6027\uff0c\u4eff\u771f\u548c\u63a7\u5236\u9762\u4e34\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5c55\u793a\u9ad8\u6548\u4eff\u771f\u548c\u5b66\u4e60\u7b56\u7565\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528MuJoCo\u5b9e\u73b0\u9ad8\u901f\u4eff\u771f\uff08\u6700\u9ad8350\u500d\u5b9e\u65f6\u901f\u5ea6\uff09\uff0c\u7ed3\u5408\u7b80\u5355\u8fd0\u52a8\u539f\u8bed\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u5168\u8eab\u64cd\u63a7\u7b56\u7565\u3002", "result": "\u5728Baloo\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e8688%\u7684\u6210\u529f\u7387\uff0c\u7b56\u7565\u8868\u73b0\u51fa\u53cd\u5e94\u884c\u4e3a\uff08\u5982\u91cd\u65b0\u6293\u53d6\u548c\u6270\u52a8\u6062\u590d\uff09\uff0c\u4e14\u5728\u6270\u52a8\u4e0b\u663e\u793a\u51fa\u8fc7\u4fee\u6b63\u884c\u4e3a\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5c55\u793a\u4e86\u8f6f\u673a\u5668\u4eba\u516d\u81ea\u7531\u5ea6\u5168\u8eab\u64cd\u63a7\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u9ad8\u6548\u4eff\u771f\u548c\u5b66\u4e60\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e3a\u96f6\u6b21\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u63d0\u4f9b\u4e86\u6210\u529f\u6848\u4f8b\u3002"}}
{"id": "2509.23905", "pdf": "https://arxiv.org/pdf/2509.23905", "abs": "https://arxiv.org/abs/2509.23905", "authors": ["Tianjiao Sun", "Ningyan Guo", "Haozhe Gu", "Yanyan Peng", "Zhiyong Feng"], "title": "Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication\nnetworks has become an increasingly vital approach for remediating coverage\nlimitations in infrastructure-deficient environments, with especially pressing\napplications in temporary scenarios, such as emergency rescue, military and\nsecurity operations, and remote area coverage. However, complex geographic\nenvironments lead to unpredictable and highly dynamic wireless channel\nconditions, resulting in frequent interruptions of air-to-ground (A2G) links\nthat severely constrain the reliability and quality of service in UAV\nswarm-assisted mobile communications. To improve the quality of UAV\nswarm-assisted communications in complex geographic environments, we propose an\nintegrated communication and control co-design mechanism. Given the stringent\nenergy constraints inherent in UAV swarms, our proposed mechanism is designed\nto optimize energy efficiency while maintaining an equilibrium between\nequitable communication rates for mobile ground users (GUs) and UAV energy\nexpenditure. We formulate the joint resource allocation and 3D trajectory\ncontrol problem as a Markov decision process (MDP), and develop a multi-agent\nreinforcement learning (MARL) framework to enable real-time coordinated actions\nacross the UAV swarm. To optimize the action policy of UAV swarms, we propose a\nnovel multi-agent hybrid proximal policy optimization with action masking\n(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action\nspaces. The algorithm incorporates action masking to enforce hard constraints\nin high-dimensional action spaces. Experimental results demonstrate that our\napproach achieves a fairness index of 0.99 while reducing energy consumption by\nup to 25% compared to baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u901a\u4fe1\u548c\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u7684\u673a\u5236\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u65e0\u4eba\u673a\u7fa4\u7684\u80fd\u91cf\u6548\u7387\u548c\u901a\u4fe1\u8d28\u91cf\uff0c\u5728\u590d\u6742\u5730\u7406\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u7684\u73af\u5883\u4e2d\u63d0\u4f9b\u4e34\u65f6\u901a\u4fe1\u8986\u76d6\uff0c\u4f46\u590d\u6742\u5730\u7406\u73af\u5883\u5bfc\u81f4\u65e0\u7ebf\u4fe1\u9053\u52a8\u6001\u53d8\u5316\uff0c\u5f71\u54cd\u901a\u4fe1\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u901a\u4fe1\u548c\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u673a\u5236\uff0c\u5229\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u65b0\u578b\u7b97\u6cd5MAHPPO-AM\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c3D\u8f68\u8ff9\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u80fd\u6e90\u6d88\u8017\u51cf\u5c1125%\u7684\u540c\u65f6\uff0c\u516c\u5e73\u6307\u6570\u8fbe\u52300.99\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7fa4\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u8d28\u91cf\u548c\u80fd\u6e90\u6548\u7387\uff0c\u4e3a\u4e34\u65f6\u901a\u4fe1\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23561", "pdf": "https://arxiv.org/pdf/2509.23561", "abs": "https://arxiv.org/abs/2509.23561", "authors": ["Jianren Wang", "Jie Han", "Abhinav Gupta", "Deepak Pathak", "Yang Zhang"], "title": "High Torque Density PCB Axial Flux Permanent Magnet Motor for Micro Robots", "categories": ["cs.RO"], "comment": null, "summary": "Quasi-direct-drive (QDD) actuation is transforming legged and manipulator\nrobots by eliminating high-ratio gearboxes, yet it demands motors that deliver\nvery high torque at low speed within a thin, disc-shaped joint envelope.\nAxial-flux permanent-magnet (AFPM) machines meet these geometric and torque\nrequirements, but scaling them below a 20mm outer diameter is hampered by poor\ncopper fill in conventional wound stators, inflating resistance and throttling\ncontinuous torque. This paper introduces a micro-scale AFPM motor that\novercomes these limitations through printed-circuit-board (PCB) windings\nfabricated with advanced IC-substrate high-density interconnect (HDI)\ntechnology. The resulting 48-layer stator-formed by stacking four 12-layer HDI\nmodules-achieves a record 45\\% copper fill in a package only 5mm thick and 19mm\nin diameter. We perform comprehensive electromagnetic and thermal analyses to\ninform the motor design, then fabricate a prototype whose performance\ncharacteristics are experimentally verified.", "AI": {"tldr": "\u5fae\u5c3a\u5ea6\u8f74\u5411\u78c1\u901a\u6c38\u78c1\u7535\u673a\uff08AFPM\uff09\u91c7\u7528PCB\u7ed5\u7ec4\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u94dc\u586b\u5145\u7387\uff0c\u63d0\u5347\u626d\u77e9\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfQDD\u9a71\u52a8\u673a\u5668\u4eba\u9700\u8981\u9ad8\u626d\u77e9\u3001\u4f4e\u901f\u5ea6\u7684\u7535\u673a\uff0c\u4f46\u5fae\u5c3a\u5ea6\u4e0b\u94dc\u586b\u5145\u7387\u4e0d\u8db3\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u5229\u7528HDI\u6280\u672f\u5236\u9020\u768448\u5c42PCB\u7ed5\u7ec4\uff0c\u5b9e\u73b045%\u94dc\u586b\u5145\u7387\uff0c\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u539f\u578b\u3002", "result": "\u7535\u673a\u539a\u5ea6\u4ec55mm\uff0c\u76f4\u5f8419mm\uff0c\u6027\u80fd\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "PCB\u7ed5\u7ec4\u6280\u672f\u89e3\u51b3\u4e86\u5fae\u5c3a\u5ea6AFPM\u7535\u673a\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u5c0f\u578b\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u9a71\u52a8\u65b9\u6848\u3002"}}
{"id": "2509.23921", "pdf": "https://arxiv.org/pdf/2509.23921", "abs": "https://arxiv.org/abs/2509.23921", "authors": ["Jo\u00e3o Paulo P. G. Marques", "Catherine Rosenberg"], "title": "Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "We conduct a comprehensive evaluation of the performance of the uplink of\nOFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing\n(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where\nonly the strongest data stream is enabled per scheduled user; Block\nDiagonalization (BD), where all possible streams are enabled per scheduled\nuser; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible\nstream allocation per user. The Radio Resource Management (RRM) of the uplink\nof all OFDMA-based systems must be done over an entire Time-Slot (TS) due to\npower management, making it challenging. To enable this study, we propose an\nefficient heuristic based on greedy-up searches for stream-sets that provides\nfeasible solutions. It operates over the TS and considers fairness, practical\nModulation and Coding Schemes and all RRM processes. The results show that, for\nRural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if\nthe number of users is small (resp. large), while for Urban Macro scenarios,\nCTR1 emerges as an alternative to CTRF due to its similar performance. We also\nshow that the system parameters can substantially impact the performance of the\nZF strategies and that BD performance is more impaired with a simpler power\nmanagement scheme than CTR1 and CTRF.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86OFDMA-based MU-MIMO\u7cfb\u7edf\u4e2d\u4e09\u79cdZF\u6ce2\u675f\u6210\u5f62\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d2a\u5a6a\u641c\u7d22\u7684\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\uff0cBD\u548cCTR1\u53ef\u4ee5\u66ff\u4ee3\u66f4\u590d\u6742\u7684CTRF\u3002", "motivation": "\u7814\u7a76OFDMA-based MU-MIMO\u7cfb\u7edf\u4e2dZF\u6ce2\u675f\u6210\u5f62\u7b56\u7565\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e0a\u884c\u94fe\u8def\u8d44\u6e90\u7ba1\u7406\u4e2d\u7531\u4e8e\u529f\u7387\u7ba1\u7406\u7684\u6311\u6218\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d2a\u5a6a\u641c\u7d22\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u65f6\u95f4\u69fd\u5185\u8fdb\u884c\u6d41\u96c6\u9009\u62e9\uff0c\u540c\u65f6\u8003\u8651\u4e86\u516c\u5e73\u6027\u3001\u8c03\u5236\u7f16\u7801\u65b9\u6848\u548c\u8d44\u6e90\u7ba1\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u519c\u6751\u5b8f\u573a\u666f\u4e2d\uff0cBD\u6216CTR1\u53ef\u4ee5\u5728\u7528\u6237\u6570\u91cf\u8f83\u5c11\u6216\u8f83\u591a\u65f6\u66ff\u4ee3CTRF\uff1b\u5728\u57ce\u5e02\u5b8f\u573a\u666f\u4e2d\uff0cCTR1\u6027\u80fd\u63a5\u8fd1CTRF\u3002\u7cfb\u7edf\u53c2\u6570\u5bf9ZF\u7b56\u7565\u5f71\u54cd\u663e\u8457\uff0c\u4e14BD\u5728\u7b80\u5355\u529f\u7387\u7ba1\u7406\u65b9\u6848\u4e0b\u6027\u80fd\u4e0b\u964d\u66f4\u591a\u3002", "conclusion": "ZF\u7b56\u7565\u7684\u6027\u80fd\u56e0\u573a\u666f\u548c\u7528\u6237\u6570\u91cf\u800c\u5f02\uff0cBD\u548cCTR1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u66ff\u4ee3CTRF\uff0c\u4f46\u7cfb\u7edf\u53c2\u6570\u548c\u529f\u7387\u7ba1\u7406\u65b9\u6848\u7684\u5f71\u54cd\u4e0d\u5bb9\u5ffd\u89c6\u3002"}}
{"id": "2509.23563", "pdf": "https://arxiv.org/pdf/2509.23563", "abs": "https://arxiv.org/abs/2509.23563", "authors": ["Seungchan Kim", "Omar Alama", "Dmytro Kurdydyk", "John Keller", "Nikhil Keetha", "Wenshan Wang", "Yonatan Bisk", "Sebastian Scherer"], "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Aerial outdoor semantic navigation requires robots to explore large,\nunstructured environments to locate target objects. Recent advances in semantic\nnavigation have demonstrated open-set object-goal navigation in indoor\nsettings, but these methods remain limited by constrained spatial ranges and\nstructured layouts, making them unsuitable for long-range outdoor search. While\noutdoor semantic navigation approaches exist, they either rely on reactive\npolicies based on current observations, which tend to produce short-sighted\nbehaviors, or precompute scene graphs offline for navigation, limiting\nadaptability to online deployment. We present RAVEN, a 3D memory-based,\nbehavior tree framework for aerial semantic navigation in unstructured outdoor\nenvironments. It (1) uses a spatially consistent semantic voxel-ray map as\npersistent memory, enabling long-horizon planning and avoiding purely reactive\nbehaviors, (2) combines short-range voxel search and long-range ray search to\nscale to large environments, (3) leverages a large vision-language model to\nsuggest auxiliary cues, mitigating sparsity of outdoor targets. These\ncomponents are coordinated by a behavior tree, which adaptively switches\nbehaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor\nsimulation environments over 100 semantic tasks, encompassing single-object\nsearch, multi-class, multi-instance navigation and sequential task changes.\nResults show RAVEN outperforms baselines by 85.25% in simulation and\ndemonstrate its real-world applicability through deployment on an aerial robot\nin outdoor field tests.", "AI": {"tldr": "RAVEN\u662f\u4e00\u4e2a\u57fa\u4e8e\u884c\u4e3a\u6811\u76843D\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u8bed\u4e49\u5bfc\u822a\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u671f\u89c4\u5212\u548c\u77ed\u671f\u53cd\u5e94\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49\u5bfc\u822a\u65b9\u6cd5\u5728\u6237\u5916\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u77ed\u89c6\u7684\u54cd\u5e94\u7b56\u7565\uff0c\u8981\u4e48\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u90e8\u7f72\u9700\u6c42\u3002RAVEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u5728\u5927\u578b\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u9ad8\u6548\u5bfc\u822a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RAVEN\u91c7\u7528\u7a7a\u95f4\u4e00\u81f4\u7684\u8bed\u4e49\u4f53\u7d20\u5c04\u7ebf\u5730\u56fe\u4f5c\u4e3a\u6301\u4e45\u8bb0\u5fc6\uff0c\u7ed3\u5408\u77ed\u671f\u4f53\u7d20\u641c\u7d22\u548c\u957f\u671f\u5c04\u7ebf\u641c\u7d22\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8f85\u52a9\u63d0\u793a\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u8fc7\u884c\u4e3a\u6811\u534f\u8c03\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u884c\u4e3a\u5207\u6362\u3002", "result": "RAVEN\u572810\u4e2a\u903c\u771f\u7684\u6237\u5916\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u4e86100\u4e2a\u8bed\u4e49\u4efb\u52a1\u7684\u6d4b\u8bd5\uff0c\u6027\u80fd\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd585.25%\uff0c\u5e76\u5728\u5b9e\u9645\u65e0\u4eba\u673a\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "RAVEN\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u6846\u67b6\u548c\u884c\u4e3a\u6811\u534f\u8c03\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u6237\u5916\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5bfc\u822a\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.24085", "pdf": "https://arxiv.org/pdf/2509.24085", "abs": "https://arxiv.org/abs/2509.24085", "authors": ["Ju-Hyung Lee", "Yanqing Lu", "Klaus Doppler"], "title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "comment": null, "summary": "We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a\nframework for cooperative cross-layer optimization in device-to-device (D2D)\ncommunication. Building on our previous work on single-device on-device LLMs,\nPEARL extends the paradigm by leveraging both publisher and subscriber states\nto guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which\nnormalizes latency by application tolerances and modulates energy by device\nbattery states, provides richer supervision for KL-based finetuning. We study\ntwo lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves\nthe best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms\ninference at near-identical objective scores. Across synthetic scenarios\ngrounded in real measurements, PEARL improves objective scores over heuristic\nand compact model baselines and reduces energy by up to 16% in cooperative\nlow-battery cases. These results demonstrate that peer-aware context,\nreward-aligned training, and head-based efficiency make LLMs practical for\nalways-on, on-device cross-layer control.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u7528\u4e8eD2D\u901a\u4fe1\u7684\u5408\u4f5c\u8de8\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u8bbe\u5907\u72b6\u6001\u6307\u5bfcWA\u53c2\u6570\u9009\u62e9\uff0c\u5e76\u901a\u8fc7KL\u5fae\u8c03\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5355\u8bbe\u5907LLM\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5408\u4f5c\u4f18\u5316\u63d0\u5347D2D\u901a\u4fe1\u7684\u6548\u7387\u4e0e\u80fd\u8017\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u5956\u52b1\u548cKL\u5fae\u8c03\uff0c\u63d0\u51faPEARL\u548cPEARL-Lite\u4e24\u79cd\u8f7b\u91cf\u7ea7\u53d8\u4f53\u3002", "result": "PEARL\u5728\u5408\u6210\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e16%\uff0cPEARL-Lite\u5b9e\u73b020ms\u5185\u7684\u63a8\u7406\u3002", "conclusion": "LLMs\u5728\u8bbe\u5907\u95f4\u5408\u4f5c\u63a7\u5236\u548c\u80fd\u6548\u4f18\u5316\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.23567", "pdf": "https://arxiv.org/pdf/2509.23567", "abs": "https://arxiv.org/abs/2509.23567", "authors": ["Fangting Xu", "Jilin Zhu", "Xiaoming Gu", "Jianzhong Tang"], "title": "GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With Geometry-Based Expert Selection", "categories": ["cs.RO"], "comment": null, "summary": "Robust and human-like dexterous grasping of general objects is a critical\ncapability for advancing intelligent robotic manipulation in real-world\nscenarios. However, existing reinforcement learning methods guided by grasp\npriors often result in unnatural behaviors. In this work, we present\n\\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that\nexplicitly accounts for task-relevant wrist orientation and thumb-index\npinching coordination. The dataset covers 773 objects in 82 categories,\nproviding a rich foundation for training human-like grasp strategies. Building\nupon this dataset, we perform geometry-based clustering to group objects by\nshape, enabling a two-stage Geometry-based Expert Selection (GES) framework\nthat selects among specialized experts for grasping diverse object geometries,\nthereby enhancing adaptability to diverse shapes and generalization across\ncategories. Our approach demonstrates natural grasp postures and achieves high\nsuccess rates of 99.4\\% and 96.3\\% on the train and test sets, respectively,\nshowcasing strong generalization and high-quality grasp execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u805a\u7c7b\u7684\u4e13\u5bb6\u9009\u62e9\u6846\u67b6ContactGrasp\uff0c\u7528\u4e8e\u5b9e\u73b0\u7c7b\u4eba\u5316\u6293\u53d6\uff0c\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6293\u53d6\u59ff\u52bf\u4e0d\u81ea\u7136\u7684\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u7684\u81ea\u7136\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b773\u4e2a\u5bf9\u8c61\u7684\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u805a\u7c7b\u548c\u4e24\u9636\u6bb5\u4e13\u5bb6\u9009\u62e9\u6846\u67b6\uff08GES\uff09\u5b9e\u73b0\u591a\u6837\u5316\u6293\u53d6\u3002", "result": "\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.4%\u548c96.3%\u7684\u6210\u529f\u7387\uff0c\u5c55\u73b0\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u6293\u53d6\u3002", "conclusion": "ContactGrasp\u6570\u636e\u96c6\u7ed3\u5408GES\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6293\u53d6\u7684\u81ea\u7136\u6027\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5bf9\u8c61\u5f62\u72b6\u3002"}}
{"id": "2509.24373", "pdf": "https://arxiv.org/pdf/2509.24373", "abs": "https://arxiv.org/abs/2509.24373", "authors": ["Matteo Zecchin", "Unnikrishnan Kunnath Ganesan", "Giuseppe Durisi", "Petar Popovski", "Osvaldo Simeone"], "title": "Prediction-Powered Communication with Distortion Guarantees", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "The development of 6G wireless systems is taking place alongside the\ndevelopment of increasingly intelligent wireless devices and network nodes. The\nchanging technological landscape is motivating a rethinking of classical\nShannon information theory that emphasizes semantic and task-oriented\nparadigms. In this paper, we study a prediction-powered communication setting,\nin which devices, equipped with artificial intelligence (AI)-based predictors,\ncommunicate under zero-delay constraints with strict distortion guarantees. Two\nclasses of distortion measures are considered: (i) outage-based metrics,\nsuitable for tasks tolerating occasional packet losses, such as real-time\ncontrol or monitoring; and (ii) bounded distortion metrics, relevant to\nsemantic-rich tasks like text or video transmission. We propose two zero-delay\ncompression algorithms leveraging online conformal prediction to provide\nper-sequence guarantees on the distortion of reconstructed sequences over\nerror-free and packet-erasure channels with feedback. For erasure channels, we\nintroduce a doubly-adaptive conformal update to compensate for channel-induced\nerrors and derive sufficient conditions on erasure statistics to ensure\ndistortion constraints. Experiments on semantic text compression validate the\napproach, showing significant bit rate reductions while strictly meeting\ndistortion guarantees compared to state-of-the-art prediction-powered\ncompression methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e866G\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u9884\u6d4b\u9a71\u52a8\u901a\u4fe1\uff0c\u63d0\u51fa\u4e24\u79cd\u96f6\u5ef6\u8fdf\u538b\u7f29\u7b97\u6cd5\uff0c\u5229\u7528\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u4fdd\u8bc1\u5931\u771f\u7ea6\u675f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bed\u4e49\u6587\u672c\u538b\u7f29\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u968f\u77406G\u548c\u667a\u80fd\u8bbe\u5907\u7684\u53d1\u5c55\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u4f20\u7edf\u4fe1\u606f\u7406\u8bba\uff0c\u8f6c\u5411\u8bed\u4e49\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u901a\u4fe1\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u96f6\u5ef6\u8fdf\u538b\u7f29\u7b97\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u65e0\u5dee\u9519\u548c\u4e22\u5305\u53cd\u9988\u4fe1\u9053\uff0c\u4fdd\u8bc1\u5931\u771f\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u65b0\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4e25\u683c\u6ee1\u8db3\u5931\u771f\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u4e49\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a6G\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.23575", "pdf": "https://arxiv.org/pdf/2509.23575", "abs": "https://arxiv.org/abs/2509.23575", "authors": ["Jianshu Hu", "Lidi Wang", "Shujia Li", "Yunpeng Jiang", "Xiao Li", "Paul Weng", "Yutong Ban"], "title": "Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D Keypoints", "categories": ["cs.RO"], "comment": null, "summary": "Hierarchical coarse-to-fine policy, where a coarse branch predicts a region\nof interest to guide a fine-grained action predictor, has demonstrated\nsignificant potential in robotic 3D manipulation tasks by especially enhancing\nsample efficiency and enabling more precise manipulation. However, even\naugmented with pre-trained models, these hierarchical policies still suffer\nfrom generalization issues. To enhance generalization to novel instructions and\nenvironment variations, we propose Coarse-to-fine Language-Aligned manipulation\nPolicy (CLAP), a framework that integrates three key components: 1) task\ndecomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware\nrepresentation. Through comprehensive experiments in simulation and on a real\nrobot, we demonstrate its superior generalization capability. Specifically, on\nGemBench, a benchmark designed for evaluating generalization, our approach\nachieves a 12\\% higher average success rate than the SOTA method while using\nonly 1/5 of the training trajectories. In real-world experiments, our policy,\ntrained on only 10 demonstrations, successfully generalizes to novel\ninstructions and environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLAP\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001VLM\u5fae\u8c033D\u5173\u952e\u70b9\u9884\u6d4b\u548c3D\u611f\u77e5\u8868\u793a\uff0c\u63d0\u5347\u4e863D\u64cd\u7eb5\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5206\u5c42\u7b56\u7565\u5728\u6cdb\u5316\u65b0\u6307\u4ee4\u548c\u73af\u5883\u53d8\u5316\u65f6\u5b58\u5728\u4e0d\u8db3\uff0cCLAP\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u5bf9\u9f50\u548c\u4efb\u52a1\u5206\u89e3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "CLAP\u6846\u67b6\u6574\u5408\u4efb\u52a1\u5206\u89e3\u3001VLM\u5fae\u8c033D\u5173\u952e\u70b9\u9884\u6d4b\u548c3D\u611f\u77e5\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728GemBench\u4e0a\uff0cCLAP\u5e73\u5747\u6210\u529f\u7387\u6bd4SOTA\u9ad812%\uff0c\u4e14\u4ec5\u97001/5\u7684\u8bad\u7ec3\u6570\u636e\uff1b\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u4ec510\u6b21\u6f14\u793a\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u65b0\u6307\u4ee4\u548c\u73af\u5883\u3002", "conclusion": "CLAP\u57283D\u64cd\u7eb5\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u6548\u7387\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.24433", "pdf": "https://arxiv.org/pdf/2509.24433", "abs": "https://arxiv.org/abs/2509.24433", "authors": ["Xin Wei", "Weidong Mei", "Xuan Huang", "Zhi Chen", "Boyu Ning"], "title": "Energy-Efficient Movable Antennas: Mechanical Power Modeling and Performance Optimization", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Movable antennas (MAs) offer additional spatial degrees of freedom (DoFs) to\nenhance communication performance through local antenna movement. However, to\nachieve accurate and fast antenna movement, MA drivers entail non-negligible\nmechanical power consumption, rendering energy efficiency (EE) optimization\nmore critical compared to conventional fixed-position antenna (FPA) systems. To\naddress this issue, we develop a fundamental power consumption model for\nstepper motor-driven multi-MA systems based on electric motor theory. Based on\nthis model, we formulate an EE maximization problem from a multi-MA base\nstation (BS) to multiple single-FPA users. We aim to jointly optimize the MAs'\npositions, moving speeds, and the BS's transmit precoding matrix subject to\ncollision-avoidance constraints during the multi-MA movements. However, this\nproblem is difficult to solve. To tackle this challenge, we first reveal that\nthe collision-avoidance constraints can always be relaxed without loss of\noptimality by properly renumbering the MA indices. For the resulting relaxed\nproblem, we first consider a simplified single-user setup and uncover a hidden\nmonotonicity of the EE performance with respect to the MAs' moving speeds. To\nsolve the remaining optimization problem, we develop a two-layer optimization\nframework. In the inner layer, the Dinkelbach algorithm is employed to derive\nthe optimal beamforming solution for any given MA positions. In the outer\nlayer, a sequential update algorithm is proposed to iteratively refine the MA\npositions based on the optimal values obtained from the inner layer. Next, we\nproceed to the general multi-user case and propose an alternating optimization\n(AO) algorithm. Numerical results demonstrate that despite the additional\nmechanical power consumption, the proposed algorithms can outperform both\nconventional FPA systems and other existing EE maximization benchmarks", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u79fb\u52a8\u5929\u7ebf(MAs)\u5728\u901a\u4fe1\u6027\u80fd\u4f18\u5316\u4e2d\u7684\u80fd\u6548\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u8fdb\u7535\u673a\u7684\u529f\u8017\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u4e86\u591a\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u7684\u80fd\u6548\u6700\u5927\u5316\u95ee\u9898\u3002", "motivation": "\u79fb\u52a8\u5929\u7ebf(MAs)\u901a\u8fc7\u5c40\u90e8\u5929\u7ebf\u79fb\u52a8\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\uff0c\u4f46\u673a\u68b0\u529f\u8017\u95ee\u9898\u4f7f\u5176\u80fd\u6548\u4f18\u5316\u6bd4\u56fa\u5b9a\u5929\u7ebf(FPA)\u7cfb\u7edf\u66f4\u4e3a\u5173\u952e\u3002", "method": "\u5efa\u7acb\u4e86\u6b65\u8fdb\u7535\u673a\u9a71\u52a8\u7684\u591a\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u7684\u529f\u8017\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e24\u5c42\u4f18\u5316\u6846\u67b6\uff08Dinkelbach\u7b97\u6cd5\u548c\u5185\u5c42\u4f18\u5316\uff09\uff0c\u5e76\u5728\u591a\u7528\u6237\u573a\u666f\u4e0b\u4f7f\u7528\u4e86\u4ea4\u66ff\u4f18\u5316(AO)\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u673a\u68b0\u529f\u8017\u589e\u52a0\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u80fd\u6548\u4e0a\u4f18\u4e8e\u4f20\u7edfFPA\u7cfb\u7edf\u548c\u5176\u4ed6\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5408\u7406\u4f18\u5316\u79fb\u52a8\u5929\u7ebf\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u80fd\u6548\uff0c\u8bc1\u660e\u4e86\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.23623", "pdf": "https://arxiv.org/pdf/2509.23623", "abs": "https://arxiv.org/abs/2509.23623", "authors": ["Nicholas Pagliocca", "Behrad Koohbor", "Mitja Trkov"], "title": "Encoding Material Safety using Control Barrier Functions for Soft Actuator Control", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Until recently, the concept of soft robot safety was an informal notion,\noften attributed solely to the fact that soft robots are less likely to damage\ntheir operating environment than rigid robots. As the field moves toward\nfeedback control for practical applications, it becomes increasingly important\nto define what safety means and to characterize how soft robots can become\nunsafe. The unifying theme of soft robotics is to achieve useful functionality\nthrough deformation. Consequently, limitations in constitutive model accuracy\nand risks of material failure are inherent to all soft robots and pose a key\nchallenge in designing provably safe controllers. This work introduces a formal\ndefinition of material safety based on strain energy functions and provides a\ncontroller that enforces it. We characterize safe and unsafe sets of an\nincompressible hyperelastic material and demonstrate that safety can be\nenforced using a high-order control barrier function (HOCBF) with quadratic\nprogram-based feedback control. As a case study, we consider a pressurized\nhyperelastic tube with inertial effects, first-order viscous effects, and\nfull-state feedback. Simulation results verify that the proposed methodology\ncan enforce the material safety specification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e94\u53d8\u80fd\u51fd\u6570\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u6750\u6599\u5b89\u5168\u6b63\u5f0f\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08HOCBF\uff09\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u968f\u7740\u8f6f\u4f53\u673a\u5668\u4eba\u53cd\u9988\u63a7\u5236\u7684\u5e94\u7528\u589e\u591a\uff0c\u4e9f\u9700\u660e\u786e\u5b89\u5168\u5b9a\u4e49\u53ca\u5176\u5931\u6548\u673a\u5236\uff0c\u4ee5\u8bbe\u8ba1\u53ef\u8bc1\u660e\u5b89\u5168\u7684\u63a7\u5236\u5668\u3002", "method": "\u5229\u7528\u5e94\u53d8\u80fd\u51fd\u6570\u5b9a\u4e49\u6750\u6599\u5b89\u5168\uff0c\u901a\u8fc7HOCBF\u548c\u4e8c\u6b21\u89c4\u5212\u53cd\u9988\u63a7\u5236\u5b9e\u73b0\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6267\u884c\u6750\u6599\u5b89\u5168\u89c4\u8303\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u5b89\u5168\u5b9a\u4e49\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.24601", "pdf": "https://arxiv.org/pdf/2509.24601", "abs": "https://arxiv.org/abs/2509.24601", "authors": ["Jae-Bum Seo", "Muhammad Salman", "Lismer Andres Caceres-Najarro"], "title": "CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence", "categories": ["cs.LG", "eess.SP"], "comment": "14 pages, 3 figures, 8 tables", "summary": "Existing on-device AI architectures for resource-constrained environments\nface two critical limitations: they lack compactness, with parameter\nrequirements scaling proportionally to task complexity, and they exhibit poor\ngeneralizability, performing effectively only on specific application domains\n(e.g., models designed for regression tasks cannot adapt to natural language\nprocessing (NLP) applications). In this paper, we propose CURA, an architecture\ninspired by analog audio signal processing circuits that provides a compact and\nlightweight solution for diverse machine learning tasks across multiple\ndomains. Our architecture offers three key advantages over existing approaches:\n(1) Compactness: it requires significantly fewer parameters regardless of task\ncomplexity; (2) Generalizability: it adapts seamlessly across regression,\nclassification, complex NLP, and computer vision tasks; and (3) Complex pattern\nrecognition: it can capture intricate data patterns while maintaining extremely\nlow model complexity. We evaluated CURA across diverse datasets and domains.\nFor compactness, it achieved equivalent accuracy using up to 2,500 times fewer\nparameters compared to baseline models. For generalizability, it demonstrated\nconsistent performance across four NLP benchmarks and one computer vision\ndataset, nearly matching specialized existing models (achieving F1-scores up to\n90%). Lastly, it delivers superior forecasting accuracy for complex patterns,\nachieving 1.6 times lower mean absolute error and 2.1 times lower mean squared\nerror than competing models.", "AI": {"tldr": "CURA\u662f\u4e00\u79cd\u53d7\u6a21\u62df\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u7535\u8def\u542f\u53d1\u7684\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bbe\u5907AI\u5728\u7d27\u51d1\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u8bbe\u5907AI\u67b6\u6784\u5728\u7d27\u51d1\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u63d0\u51faCURA\u67b6\u6784\uff0c\u57fa\u4e8e\u6a21\u62df\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u7535\u8def\uff0c\u5b9e\u73b0\u7d27\u51d1\u3001\u8f7b\u91cf\u5316\u7684\u591a\u9886\u57df\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002", "result": "CURA\u5728\u7d27\u51d1\u6027\uff08\u53c2\u6570\u51cf\u5c112500\u500d\uff09\u3001\u6cdb\u5316\u6027\uff08\u8de8\u9886\u57df\u6027\u80fd\u4e00\u81f4\uff09\u548c\u590d\u6742\u6a21\u5f0f\u8bc6\u522b\uff08\u8bef\u5dee\u66f4\u4f4e\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CURA\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684AI\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23650", "pdf": "https://arxiv.org/pdf/2509.23650", "abs": "https://arxiv.org/abs/2509.23650", "authors": ["Peizhuo Li", "Hongyi Li", "Yuxuan Ma", "Linnan Chang", "Xinrong Yang", "Ruiqi Yu", "Yifeng Zhang", "Yuhong Cao", "Qiuguo Zhu", "Guillaume Sartoretti"], "title": "KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe Egocentric Legged Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Vision-based locomotion has shown great promise in enabling legged robots to\nperceive and adapt to complex environments. However, visual information is\ninherently fragile, being vulnerable to occlusions, reflections, and lighting\nchanges, which often cause instability in locomotion. Inspired by animal\nsensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial\nintegration framework, where kinesthetics encodes proprioceptive sensing of\nbody motion and visuospatial reasoning captures visual perception of\nsurrounding terrain. Specifically, KiVi separates these pathways, leveraging\nproprioception as a stable backbone while selectively incorporating vision for\nterrain awareness and obstacle avoidance. This modality-balanced, yet\nintegrative design, combined with memory-enhanced attention, allows the robot\nto robustly interpret visual cues while maintaining fallback stability through\nproprioception. Extensive experiments show that our method enables quadruped\nrobots to stably traverse diverse terrains and operate reliably in unstructured\noutdoor environments, remaining robust to out-of-distribution (OOD) visual\nnoise and occlusion unseen during training, thereby highlighting its\neffectiveness and applicability to real-world legged locomotion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKiVi\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u611f\u77e5\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7a33\u5b9a\u79fb\u52a8\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u4fe1\u606f\u6613\u53d7\u906e\u6321\u3001\u53cd\u5149\u548c\u5149\u7167\u53d8\u5316\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u7269\u611f\u6d4b\u8fd0\u52a8\u6574\u5408\u7684\u7075\u611f\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u3002", "method": "KiVi\u6846\u67b6\u5206\u79bb\u672c\u4f53\u611f\u89c9\u548c\u89c6\u89c9\u63a8\u7406\u8def\u5f84\uff0c\u5229\u7528\u672c\u4f53\u611f\u89c9\u4f5c\u4e3a\u7a33\u5b9a\u57fa\u7840\uff0c\u9009\u62e9\u6027\u7ed3\u5408\u89c6\u89c9\u7528\u4e8e\u5730\u5f62\u611f\u77e5\u548c\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u591a\u6837\u5730\u5f62\u4e2d\u7a33\u5b9a\u79fb\u52a8\uff0c\u5e76\u5bf9\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u89c6\u89c9\u566a\u58f0\u548c\u906e\u6321\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "KiVi\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.24944", "pdf": "https://arxiv.org/pdf/2509.24944", "abs": "https://arxiv.org/abs/2509.24944", "authors": ["Hongchuan Jia", "Fayu Wan", "Vladimir Mordachev", "J\u00e9r\u00f4me Rossignol", "Glauco Fontagalland", "Nour Murad", "Blaise Ravelo"], "title": "Experimental Study of Magnetic Near-Field Microstrip Electronic Probe for PCB EMC Emission Measurement", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "An experimental study on magnetic near-field (NF) scanning of printed circuit\nboard (PCB) emission radiation is developed in this paper. The design and\ninstallation of the electromagnetic (EM) NF scanner is introduced. The test bed\nof magnetic NF emission in the microwave frequency range is described. The\nmethodology of the microstrip magnetic NF probe is discussed. The probe\ncalibration process was performed following the IEC 61967-1 NF scanning\nstandard. The NF scanner functioning is tested with passive microstrip circuit\nsquare loop probe and device under test (DUT) PCB radiation in the test plan\npositioned at 1-mm above the ground plane. Based on the standard test with\nI-shape 50-$\\Omega$ transmission line (TL), the calibration process of radiated\nmagnetic field was validated by comparison between HFSS__ simulation and\nexperimentation in very wideband frequency from 0.1-GHz to 3-GHz. Then, a\nnonstandard TL based DUT was experimented. Accordingly, the cartographies of\nscanned magnetic NF at two different test frequencies, 2 GHz and 3 GHz, are\ndiscussed. The NF scanner is under development for targeting the EMC radiated\nemission of PCB dedicated to operate in 6G wireless communication.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u5370\u5237\u7535\u8def\u677f(PCB)\u8f90\u5c04\u7684\u78c1\u8fd1\u573a(NF)\u626b\u63cf\u5b9e\u9a8c\u7cfb\u7edf\uff0c\u4ecb\u7ecd\u4e86\u7535\u78c1NF\u626b\u63cf\u4eea\u7684\u8bbe\u8ba1\u3001\u5b89\u88c5\u548c\u6821\u51c6\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u529f\u80fd\u3002", "motivation": "\u5f00\u53d1\u548c\u9a8c\u8bc1\u4e00\u79cd\u7528\u4e8e6G\u65e0\u7ebf\u901a\u4fe1PCB\u7535\u78c1\u517c\u5bb9\u6027(EMC)\u8f90\u5c04\u53d1\u5c04\u7684\u78c1NF\u626b\u63cf\u4eea\u3002", "method": "\u8bbe\u8ba1\u548c\u5b89\u88c5\u7535\u78c1NF\u626b\u63cf\u4eea\uff0c\u4f7f\u7528\u5fae\u5e26\u78c1NF\u63a2\u5934\uff0c\u9075\u5faaIEC 61967-1\u6807\u51c6\u8fdb\u884c\u6821\u51c6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u529f\u80fd\u3002", "result": "\u626b\u63cf\u4eea\u57280.1GHz\u81f33GHz\u8303\u56f4\u5185\u9a8c\u8bc1\u4e86\u6821\u51c6\u8fc7\u7a0b\uff0c\u5e76\u5bf9\u975e\u6807\u51c6\u4f20\u8f93\u7ebfDUT\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8ba8\u8bba\u4e862GHz\u548c3GHz\u4e0b\u7684\u78c1NF\u626b\u63cf\u56fe\u3002", "conclusion": "\u8be5NF\u626b\u63cf\u4eea\u9002\u7528\u4e8e6G\u65e0\u7ebf\u901a\u4fe1PCB\u7684EMC\u8f90\u5c04\u53d1\u5c04\u6d4b\u8bd5\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002"}}
{"id": "2509.23651", "pdf": "https://arxiv.org/pdf/2509.23651", "abs": "https://arxiv.org/abs/2509.23651", "authors": ["Xinrong Yang", "Peizhuo Li", "Hongyi Li", "Junkai Lu", "Linnan Chang", "Yuhong Cao", "Yifeng Zhang", "Ge Sun", "Guillaume Sartoretti"], "title": "HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod Robot", "categories": ["cs.RO"], "comment": null, "summary": "Robots in real-world environments are often required to move/manipulate\nobjects comparable in weight to their own bodies. Compared to grasping and\ncarrying, pushing provides a more straightforward and efficient non-prehensile\nmanipulation strategy, avoiding complex grasp design while leveraging direct\ncontact to regulate an object's pose. Achieving effective pushing, however,\ndemands both sufficient manipulation forces and the ability to maintain\nstability, which is particularly challenging when dealing with heavy or\nirregular objects. To address these challenges, we propose HeLoM, a\nlearning-based hierarchical whole-body manipulation framework for a hexapod\nrobot that exploits coordinated multi-limb control. Inspired by the cooperative\nstrategies of multi-legged insects, our framework leverages redundant contact\npoints and high degrees of freedom to enable dynamic redistribution of contact\nforces. HeLoM's high-level planner plans pushing behaviors and target object\nposes, while its low-level controller maintains locomotion stability and\ngenerates dynamically consistent joint actions. Our policies trained in\nsimulation are directly deployed on real robots without additional fine-tuning.\nThis design allows the robot to maintain balance while exerting continuous and\ncontrollable pushing forces through coordinated foreleg interaction and\nsupportive hind-leg propulsion. We validate the effectiveness of HeLoM through\nboth simulation and real-world experiments. Results show that our framework can\nstably push boxes of varying sizes and unknown physical properties to\ndesignated goal poses in the real world.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeLoM\u7684\u5c42\u6b21\u5316\u5168\u8eab\u64cd\u7eb5\u6846\u67b6\uff0c\u7528\u4e8e\u516d\u8db3\u673a\u5668\u4eba\u5728\u5904\u7406\u91cd\u91cf\u6216\u5f62\u72b6\u4e0d\u89c4\u5219\u7269\u4f53\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u975e\u6293\u53d6\u5f0f\u63a8\u52a8\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u79fb\u52a8\u6216\u64cd\u7eb5\u4e0e\u81ea\u8eab\u91cd\u91cf\u76f8\u5f53\u7684\u7269\u4f53\uff0c\u63a8\u52a8\u4f5c\u4e3a\u4e00\u79cd\u76f4\u63a5\u7684\u4e14\u9ad8\u6548\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u7eb5\u7b56\u7565\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u6293\u53d6\u8bbe\u8ba1\uff0c\u4f46\u9700\u8981\u8db3\u591f\u7684\u64cd\u7eb5\u529b\u548c\u7a33\u5b9a\u6027\u3002", "method": "HeLoM\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u5c42\u6b21\u5316\u8bbe\u8ba1\uff0c\u9ad8\u5c42\u89c4\u5212\u63a8\u52a8\u884c\u4e3a\u548c\u76ee\u6807\u4f4d\u59ff\uff0c\u4f4e\u5c42\u63a7\u5236\u5668\u7ef4\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\u5e76\u751f\u6210\u52a8\u6001\u4e00\u81f4\u7684\u5173\u8282\u52a8\u4f5c\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u591a\u8db3\u6606\u866b\u7684\u534f\u4f5c\u7b56\u7565\u3002", "result": "\u6846\u67b6\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u540e\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u80fd\u591f\u7a33\u5b9a\u63a8\u52a8\u4e0d\u540c\u5c3a\u5bf8\u548c\u672a\u77e5\u7269\u7406\u7279\u6027\u7684\u7bb1\u5b50\u5230\u76ee\u6807\u4f4d\u59ff\u3002", "conclusion": "HeLoM\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u591a\u80a2\u63a7\u5236\u548c\u52a8\u6001\u63a5\u89e6\u529b\u5206\u914d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u63a8\u52a8\u64cd\u4f5c\u3002"}}
{"id": "2509.25067", "pdf": "https://arxiv.org/pdf/2509.25067", "abs": "https://arxiv.org/abs/2509.25067", "authors": ["Rohollah Vahdani", "S. Mohammad Razavizadeh"], "title": "Capacity Achieving Design for Hybrid Beamforming in Millimeter Wave Massive MIMO Systems", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "Published in IEEE Transactions on Vehicular Technology, Volume: 74,\n  Issue: 3, March 2025", "summary": "Hybrid digital and analog beamforming is a highly effective technique for\nimplementing beamforming methods in millimeter wave (mmWave) systems. It\nprovides a viable solution to replace the complex fully digital beamforming\ntechniques. However, the current design of precoding and combining matrices in\nhybrid beamforming solely relies on the channel information, neglecting the\ncrucial consideration of the structure of covariance matrices of the transmit\nsignals. In this paper, we present a novel approach for the joint design of\nhybrid beamforming matrices at the transmitter and receiver. This approach is\ncentered around the optimization of the covariance matrix of the transmitted\nsignals. Our goal is to maximize the downlink sum rate capacity of the system\nby achieving an optimal design of the transmit covariance matrix. We tackle the\nnon-convex nature of this problem by leveraging the dual relationship between\nthe broadcast channel (BC) and the multiple access channel (MAC). Through\nextensive simulations in various scenarios, including point-to-point\nmulti-input multi-output (MIMO), multi-user (MU) multi-input single-output\n(MISO), and MU-MIMO, we demonstrate the superiority of our proposed method over\ntraditional designs. These results highlight the effectiveness and versatility\nof our approach in optimizing beamforming for mmWave systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u8054\u5408\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u53d1\u5c04\u4fe1\u53f7\u7684\u534f\u65b9\u5dee\u77e9\u9635\u6765\u6700\u5927\u5316\u4e0b\u884c\u94fe\u8def\u7684\u603b\u901f\u7387\u5bb9\u91cf\u3002", "motivation": "\u5f53\u524d\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u4e2d\uff0c\u9884\u7f16\u7801\u548c\u7ec4\u5408\u77e9\u9635\u4ec5\u4f9d\u8d56\u4e8e\u4fe1\u9053\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u53d1\u5c04\u4fe1\u53f7\u534f\u65b9\u5dee\u77e9\u9635\u7684\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u5229\u7528\u5e7f\u64ad\u4fe1\u9053\uff08BC\uff09\u548c\u591a\u5740\u4fe1\u9053\uff08MAC\uff09\u4e4b\u95f4\u7684\u5bf9\u5076\u5173\u7cfb\uff0c\u4f18\u5316\u53d1\u5c04\u4fe1\u53f7\u7684\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u4ee5\u89e3\u51b3\u975e\u51f8\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u573a\u666f\uff08\u5982\u70b9\u5bf9\u70b9MIMO\u3001\u591a\u7528\u6237MISO\u548cMU-MIMO\uff09\u7684\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2509.23655", "pdf": "https://arxiv.org/pdf/2509.23655", "abs": "https://arxiv.org/abs/2509.23655", "authors": ["Rokas Bendikas", "Daniel Dijkman", "Markus Peschl", "Sanjay Haresh", "Pietro Mazzaglia"], "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at 9th Conference on Robot Learning (CoRL 2025), Seoul,\n  Korea", "summary": "Vision-Language-Action (VLA) models offer a pivotal approach to learning\nrobotic manipulation at scale by repurposing large pre-trained\nVision-Language-Models (VLM) to output robotic actions. However, adapting VLMs\nfor robotic domains comes with an unnecessarily high computational cost, which\nwe attribute to the tokenization scheme of visual inputs. In this work, we aim\nto enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric\nTokenization for VLAs. Building on the insights of object-centric\nrepresentation learning, our method introduces an inductive bias towards scene\nobjects and the agent's own visual information. As a result, we find that\nOat-VLA can drastically reduce the number of visual tokens to just a few tokens\nwithout sacrificing performance. We reveal that Oat-VLA converges at least\ntwice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in\ndiverse real-world pick and place tasks.", "AI": {"tldr": "Oat-VLA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u9002\u5e94\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5230\u673a\u5668\u4eba\u9886\u57df\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u6807\u8bb0\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Oat-VLA\uff0c\u4e00\u79cd\u9762\u5411\u5bf9\u8c61\u548c\u673a\u5668\u4eba\u81ea\u8eab\u7684\u89c6\u89c9\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u5316\u8868\u793a\u5b66\u4e60\u7684\u6d1e\u5bdf\u3002", "result": "Oat-VLA\u80fd\u5c06\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u51cf\u5c11\u81f3\u6781\u5c11\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u81f3\u5c11\u662fOpenVLA\u7684\u4e24\u500d\uff0c\u5728LIBERO\u5957\u4ef6\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Oat-VLA\u901a\u8fc7\u9ad8\u6548\u7684\u6807\u8bb0\u5316\u65b9\u6848\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.23656", "pdf": "https://arxiv.org/pdf/2509.23656", "abs": "https://arxiv.org/abs/2509.23656", "authors": ["Liangting Wu", "Roberto Tron"], "title": "Certifiably Optimal State Estimation and Robot Calibration Using Trace-Constrained SDP", "categories": ["cs.RO"], "comment": null, "summary": "Many nonconvex problems in robotics can be relaxed into convex formulations\nvia semidefinite programming (SDP), which offers the advantage of global\noptimality. The practical quality of these solutions, however, critically\ndepends on achieving rank-1 matrices, a condition that typically requires\nadditional tightening. In this work, we focus on trace-constrained SDPs, where\nthe decision variables are positive semidefinite (PSD) matrices with fixed\ntrace values. These additional constraints not only capture important\nstructural properties but also facilitate first-order methods for recovering\nrank-1 solutions. We introduce customized fixed-trace variables and constraints\nto represent common robotic quantities such as rotations and translations,\nwhich can be exactly recovered when the corresponding variables are rank-1. To\nfurther improve practical performance, we develop a gradient-based refinement\nprocedure that projects relaxed SDP solutions toward rank-1, low-cost\ncandidates, which can then be certified for global optimality via the dual\nproblem. We demonstrate that many robotics tasks can be expressed within this\ntrace-constrained SDP framework, and showcase its effectiveness through\nsimulations in perspective-n-point (PnP) estimation, hand-eye calibration, and\ndual-robot system calibration. To support broader use, we also introduce a\nmodular ``virtual robot'' abstraction that simplifies modeling across different\nproblem settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u5b66\u4e2d\u975e\u51f8\u95ee\u9898\u7684\u51f8\u677e\u5f1b\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u548c\u56fa\u5b9a\u8ff9\u7ea6\u675f\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u7ec6\u5316\u7a0b\u5e8f\u8fdb\u4e00\u6b65\u63d0\u5347\u89e3\u7684\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u975e\u51f8\u95ee\u9898\u7684\u5168\u5c40\u6700\u4f18\u89e3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u8bc1\u89e3\u7684\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u8ff9\u7ea6\u675f\u7684\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u68af\u5ea6\u7ec6\u5316\u7a0b\u5e8f\uff0c\u5c06\u677e\u5f1b\u7684\u89e3\u4f18\u5316\u4e3a\u4f4e\u79e9\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PnP\u4f30\u8ba1\u3001\u624b\u773c\u6807\u5b9a\u548c\u53cc\u673a\u5668\u4eba\u7cfb\u7edf\u6807\u5b9a\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u56fa\u5b9a\u8ff9\u7ea6\u675f\u7684SDP\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.23705", "pdf": "https://arxiv.org/pdf/2509.23705", "abs": "https://arxiv.org/abs/2509.23705", "authors": ["Jun Chen", "Mingjia Chen", "Shinkyu Park"], "title": "MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload Adaptation", "categories": ["cs.RO"], "comment": null, "summary": "Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing\npaths for multiple robots to effectively cover a large area of interest.\nConventional approaches to MCPP typically assume that robots move at fixed\nvelocities, which is often unrealistic in real-world applications where robots\nmust adapt their speeds based on the specific coverage tasks assigned to\nthem.Consequently, conventional approaches often lead to imbalanced workload\ndistribution among robots and increased completion time for coverage tasks. To\naddress this, we introduce a novel Multi-robot Dynamic Coverage Path Planning\n(MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP\ndynamically estimates each robot's remaining workload by approximating the\ntarget distribution with Gaussian mixture models, and assigns coverage regions\nusing a capacity-constrained Voronoi diagram. We further develop a distributed\nimplementation of MDCPP for range-constrained robotic networks. Simulation\nresults validate the efficacy of MDCPP, showing qualitative improvements and\nsuperior performance compared to an existing sweeping algorithm, and a\nquantifiable impact of communication range on coverage efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u673a\u5668\u4eba\u52a8\u6001\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff08MDCPP\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\uff0c\u63d0\u9ad8\u4e86\u8986\u76d6\u4efb\u52a1\u7684\u6548\u7387\u548c\u5747\u8861\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u8986\u76d6\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5047\u8bbe\u673a\u5668\u4eba\u901f\u5ea6\u56fa\u5b9a\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5747\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u3002", "method": "MDCPP\u5229\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4f30\u8ba1\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u901a\u8fc7\u5bb9\u91cf\u53d7\u9650\u7684Voronoi\u56fe\u5206\u914d\u8986\u76d6\u533a\u57df\uff0c\u8fd8\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5b9e\u73b0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cMDCPP\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u4e14\u901a\u4fe1\u8303\u56f4\u5bf9\u8986\u76d6\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "MDCPP\u5728\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5747\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2509.23721", "pdf": "https://arxiv.org/pdf/2509.23721", "abs": "https://arxiv.org/abs/2509.23721", "authors": ["Chi Chu", "Huazhe Xu"], "title": "DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware Motion Manifold Primitives", "categories": ["cs.RO"], "comment": null, "summary": "Dynamic manipulation is a key capability for advancing robot performance,\nenabling skills such as tossing. While recent learning-based approaches have\npushed the field forward, most methods still rely on manually designed action\nparameterizations, limiting their ability to produce the highly coordinated\nmotions required in complex tasks. Motion planning can generate feasible\ntrajectories, but the dynamics gap-stemming from control inaccuracies, contact\nuncertainties, and aerodynamic effects-often causes large deviations between\nplanned and executed trajectories. In this work, we propose Dynamics-Aware\nMotion Manifold Primitives (DA-MMP), a motion generation framework for\ngoal-conditioned dynamic manipulation, and instantiate it on a challenging\nreal-world ring-tossing task. Our approach extends motion manifold primitives\nto variable-length trajectories through a compact parametrization and learns a\nhigh-quality manifold from a large-scale dataset of planned motions. Building\non this manifold, a conditional flow matching model is trained in the latent\nspace with a small set of real-world trials, enabling the generation of\nthrowing trajectories that account for execution dynamics. Experiments show\nthat our method can generate coordinated and smooth motion trajectories for the\nring-tossing task. In real-world evaluations, it achieves high success rates\nand even surpasses the performance of trained human experts. Moreover, it\ngeneralizes to novel targets beyond the training range, indicating that it\nsuccessfully learns the underlying trajectory-dynamics mapping.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u611f\u77e5\u7684\u8fd0\u52a8\u751f\u6210\u6846\u67b6DA-MMP\uff0c\u7528\u4e8e\u76ee\u6807\u6761\u4ef6\u52a8\u6001\u64cd\u7eb5\uff0c\u5e76\u5728\u771f\u5b9e\u7684\u73af\u629b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\uff0c\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u52a8\u4f5c\u53c2\u6570\u6216\u5b58\u5728\u52a8\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u9650\u5236\u4e86\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6269\u5c55\u8fd0\u52a8\u6d41\u5f62\u57fa\u5143\uff0c\u5b66\u4e60\u9ad8\u8d28\u91cf\u6d41\u5f62\uff0c\u5e76\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u4ee5\u751f\u6210\u8003\u8651\u6267\u884c\u52a8\u6001\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\uff0c\u751a\u81f3\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u76ee\u6807\u3002", "conclusion": "DA-MMP\u6210\u529f\u5b66\u4e60\u4e86\u8f68\u8ff9-\u52a8\u6001\u6620\u5c04\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u64cd\u7eb5\u4efb\u52a1\u3002"}}
{"id": "2509.23745", "pdf": "https://arxiv.org/pdf/2509.23745", "abs": "https://arxiv.org/abs/2509.23745", "authors": ["Min Liu", "Deepak Pathak", "Ananye Agarwal"], "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to CoRL 2025", "summary": "Modern locomotion controllers are manually tuned for specific embodiments. We\npresent LocoFormer, a generalist omni-bodied locomotion model that can control\npreviously unseen legged and wheeled robots, even without precise knowledge of\ntheir kinematics. LocoFormer is able to adapt to changes in morphology and\ndynamics at test time. We find that two key choices enable adaptation. First,\nwe train massive scale RL on procedurally generated robots with aggressive\ndomain randomization. Second, in contrast to previous policies that are myopic\nwith short context lengths, we extend context by orders of magnitude to span\nepisode boundaries. We deploy the same LocoFormer to varied robots and show\nrobust control even with large disturbances such as weight change and motor\nfailures. In extreme scenarios, we see emergent adaptation across episodes,\nLocoFormer learns from falls in early episodes to improve control strategies in\nlater ones. We believe that this simple, yet general recipe can be used to\ntrain foundation models for other robotic skills in the future. Videos at\ngeneralist-locomotion.github.io.", "AI": {"tldr": "LocoFormer\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5168\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u6a21\u578b\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5f62\u6001\u548c\u52a8\u6001\u53d8\u5316\u7684\u673a\u5668\u4eba\uff0c\u65e0\u9700\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u77e5\u8bc6\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u63a7\u5236\u5668\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u624b\u52a8\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002LocoFormer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u9c81\u68d2\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ed3\u5408\u751f\u6210\u7684\u591a\u6837\u5316\u673a\u5668\u4eba\u548c\u5f3a\u9886\u57df\u968f\u673a\u5316\uff0c\u540c\u65f6\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ee5\u8de8\u8d8a\u4efb\u52a1\u8fb9\u754c\u3002", "result": "LocoFormer\u5728\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u9c81\u68d2\uff0c\u80fd\u591f\u5e94\u5bf9\u91cd\u91cf\u53d8\u5316\u548c\u7535\u673a\u6545\u969c\u7b49\u5e72\u6270\uff0c\u5e76\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8de8\u4efb\u52a1\u7684\u9002\u5e94\u6027\u5b66\u4e60\u3002", "conclusion": "LocoFormer\u7684\u7b80\u5355\u901a\u7528\u65b9\u6cd5\u4e3a\u672a\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u6280\u80fd\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.23778", "pdf": "https://arxiv.org/pdf/2509.23778", "abs": "https://arxiv.org/abs/2509.23778", "authors": ["Zeyuan Zhang", "Chaoran Li", "Shao Zhang", "Ying Wen"], "title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": "Preprint Under Review", "summary": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of\nMulti-Agent Path Finding (MAPF), where agents are required to sequentially\ncomplete tasks with fixed-location pickup and delivery demands. Although\nlearning-based methods have made progress in MAPD, they often perform poorly in\nwarehouse-like environments with narrow pathways and long corridors when\nrelying only on local observations for distributed decision-making.\nCommunication learning can alleviate the lack of global information but\nintroduce high computational complexity due to point-to-point communication. To\naddress this challenge, we formulate MAPF as a sequence modeling problem and\nprove that path-finding policies under sequence modeling possess\norder-invariant optimality, ensuring its effectiveness in MAPD. Building on\nthis, we propose the Sequential Pathfinder (SePar), which leverages the\nTransformer paradigm to achieve implicit information exchange, reducing\ndecision-making complexity from exponential to linear while maintaining\nefficiency and global awareness. Experiments demonstrate that SePar\nconsistently outperforms existing learning-based methods across various MAPF\ntasks and their variants, and generalizes well to unseen environments.\nFurthermore, we highlight the necessity of integrating imitation learning in\ncomplex maps like warehouses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSePar\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u5229\u7528Transformer\u8303\u5f0f\u5b9e\u73b0\u9690\u5f0f\u4fe1\u606f\u4ea4\u6362\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u51b3\u7b56\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u62fe\u53d6\u4ea4\u4ed8\u4efb\u52a1\uff08MAPD\uff09\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u72ed\u7a84\u901a\u9053\u548c\u957f\u8d70\u5eca\u7684\u4ed3\u5e93\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06MAPF\u95ee\u9898\u5efa\u6a21\u4e3a\u5e8f\u5217\u95ee\u9898\uff0c\u63d0\u51faSequential Pathfinder (SePar)\u65b9\u6cd5\uff0c\u5229\u7528Transformer\u5b9e\u73b0\u9690\u5f0f\u4fe1\u606f\u4ea4\u6362\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "SePar\u5728\u591a\u79cdMAPF\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u5728MAPD\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u5730\u56fe\uff08\u5982\u4ed3\u5e93\uff09\u4e2d\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.23801", "pdf": "https://arxiv.org/pdf/2509.23801", "abs": "https://arxiv.org/abs/2509.23801", "authors": ["Shuning Zhang", "Renjing Xu", "Zhanchen Zhu", "Xiangyu Chen", "Yunheng Wang", "Xu Jiang", "Peibo Duan"], "title": "High-Precision Climbing Robot Localization Using Planar Array UWB/GPS/IMU/Barometer Integration", "categories": ["cs.RO"], "comment": null, "summary": "To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edfAMFA\uff0c\u7528\u4e8e\u590d\u6742\u9ad8\u6d77\u62d4\u73af\u5883\u4e2d\u6500\u722c\u673a\u5668\u4eba\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u901a\u8fc7UWB\u3001GPS\u3001IMU\u548c\u6c14\u538b\u8ba1\u7684\u6570\u636e\u878d\u5408\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86GPS\u906e\u6321\u548cUWB\u975e\u89c6\u8ddd\u95ee\u9898\uff0c\u6700\u7ec8\u5b9e\u9a8c\u663e\u793a\u5b9a\u4f4d\u7cbe\u5ea6\u4e3a0.48\u7c73\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u9ad8\u6d77\u62d4\u73af\u5883\u4e0b\u5355\u4e00\u4f20\u611f\u5668\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6500\u722c\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eAMFA\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u67b6\u6784\uff0c\u5f00\u53d1UWB\u4e0e\u6c14\u538b\u8ba1\u7684\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4f7f\u7528UKF\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5b9a\u4f4d\u7cbe\u5ea6\u8fbe0.48\u7c73\uff0c\u6700\u5927\u8bef\u5dee\u4e3a1.50\u7c73\uff0c\u4f18\u4e8eGPS/INS-EKF\u7b49\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "AMFA\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u6500\u722c\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.23821", "pdf": "https://arxiv.org/pdf/2509.23821", "abs": "https://arxiv.org/abs/2509.23821", "authors": ["Federico Pablo-Marti", "Carlos Mir Fernandez"], "title": "Fostering Robots: A Governance-First Conceptual Framework for Domestic, Curriculum-Based Trajectory Collection", "categories": ["cs.RO"], "comment": "7 pages, 2 figures", "summary": "We propose a conceptual, empirically testable framework for Robot Fostering,\n-a curriculum-driven, governance-first approach to domestic robot deployments,\nemphasizing long-term, curated interaction trajectories. We formalize\ntrajectory quality with quantifiable metrics and evaluation protocols aligned\nwith EU-grade governance standards, delineating a low-resource empirical\nroadmap to enable rigorous validation through future pilot studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u3001\u53ef\u5b9e\u8bc1\u6d4b\u8bd5\u7684\u673a\u5668\u4eba\u57f9\u517b\u6846\u67b6\uff0c\u5f3a\u8c03\u957f\u671f\u3001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4ea4\u4e92\u8f68\u8ff9\uff0c\u5e76\u91cf\u5316\u8bc4\u4f30\u6807\u51c6\u4ee5\u7b26\u5408\u6b27\u76df\u6cbb\u7406\u6807\u51c6\u3002", "motivation": "\u65e8\u5728\u4e3a\u5bb6\u7528\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u9a71\u52a8\u548c\u6cbb\u7406\u4f18\u5148\u7684\u957f\u671f\u4e92\u52a8\u6a21\u5f0f\uff0c\u786e\u4fdd\u5176\u9ad8\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u8f68\u8ff9\u8d28\u91cf\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4f4e\u8d44\u6e90\u5b9e\u8bc1\u8def\u7ebf\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u7b26\u5408\u6b27\u76df\u6807\u51c6\u7684\u6cbb\u7406\u6807\u51c6\uff0c\u4e3a\u672a\u6765\u8bd5\u70b9\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5bb6\u7528\u673a\u5668\u4eba\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u6709\u671b\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6548\u679c\u3002"}}
{"id": "2509.23823", "pdf": "https://arxiv.org/pdf/2509.23823", "abs": "https://arxiv.org/abs/2509.23823", "authors": ["Tian Nian", "Weijie Ke", "Yao Mu", "Tianxing Chen", "Shaolong Zhu", "Bingshan Hu"], "title": "Control Your Robot: A Unified System for Robot Control and Policy Deployment", "categories": ["cs.RO"], "comment": "Code: https://github.com/Tian-Nian/control_your_robot", "summary": "Cross-platform robot control remains difficult because hardware interfaces,\ndata formats, and control paradigms vary widely, which fragments toolchains and\nslows deployment. To address this, we present Control Your Robot, a modular,\ngeneral-purpose framework that unifies data collection and policy deployment\nacross diverse platforms. The system reduces fragmentation through a\nstandardized workflow with modular design, unified APIs, and a closed-loop\narchitecture. It supports flexible robot registration, dual-mode control with\nteleoperation and trajectory playback, and seamless integration from multimodal\ndata acquisition to inference. Experiments on single-arm and dual-arm systems\nshow efficient, low-latency data collection and effective support for policy\nlearning with imitation learning and vision-language-action models. Policies\ntrained on data gathered by Control Your Robot match expert demonstrations\nclosely, indicating that the framework enables scalable and reproducible robot\nlearning across platforms.", "AI": {"tldr": "Control Your Robot \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u901a\u7528\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8de8\u5e73\u53f0\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u786c\u4ef6\u63a5\u53e3\u548c\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u7edf\u4e00API\uff0c\u652f\u6301\u9ad8\u6548\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u7531\u4e8e\u786c\u4ef6\u63a5\u53e3\u3001\u6570\u636e\u683c\u5f0f\u548c\u63a7\u5236\u8303\u5f0f\u7684\u591a\u6837\u6027\uff0c\u8de8\u5e73\u53f0\u673a\u5668\u4eba\u63a7\u5236\u5b58\u5728\u5de5\u5177\u94fe\u788e\u7247\u5316\u548c\u90e8\u7f72\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faControl Your Robot\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u7edf\u4e00API\u548c\u95ed\u73af\u67b6\u6784\uff0c\u652f\u6301\u7075\u6d3b\u673a\u5668\u4eba\u6ce8\u518c\u3001\u53cc\u6a21\u5f0f\u63a7\u5236\uff08\u9065\u64cd\u4f5c\u548c\u8f68\u8ff9\u56de\u653e\uff09\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\u5230\u63a8\u7406\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u5355\u81c2\u548c\u53cc\u81c2\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u6570\u636e\u6536\u96c6\uff0c\u5e76\u6709\u6548\u652f\u6301\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "Control Your Robot\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8de8\u5e73\u53f0\u7684\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u63a5\u8fd1\u4e13\u5bb6\u6f14\u793a\u6c34\u5e73\u3002"}}
{"id": "2509.23829", "pdf": "https://arxiv.org/pdf/2509.23829", "abs": "https://arxiv.org/abs/2509.23829", "authors": ["Kefei Zhu", "Fengshuo Bai", "YuanHao Xiang", "Yishuai Cai", "Xinglin Chen", "Ruochong Li", "Xingtao Wang", "Hao Dong", "Yaodong Yang", "Xiaopeng Fan", "Yuanpei Chen"], "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation", "categories": ["cs.RO"], "comment": "NeurIPS 2025, Spotlight", "summary": "Dexterous manipulation is critical for advancing robot capabilities in\nreal-world applications, yet diverse and high-quality datasets remain scarce.\nExisting data collection methods either rely on human teleoperation or require\nsignificant human engineering, or generate data with limited diversity, which\nrestricts their scalability and generalization. In this paper, we introduce\nDexFlyWheel, a scalable data generation framework that employs a self-improving\ncycle to continuously enrich data diversity. Starting from efficient seed\ndemonstrations warmup, DexFlyWheel expands the dataset through iterative\ncycles. Each cycle follows a closed-loop pipeline that integrates Imitation\nLearning (IL), residual Reinforcement Learning (RL), rollout trajectory\ncollection, and data augmentation. Specifically, IL extracts human-like\nbehaviors from demonstrations, and residual RL enhances policy generalization.\nThe learned policy is then used to generate trajectories in simulation, which\nare further augmented across diverse environments and spatial configurations\nbefore being fed back into the next cycle. Over successive iterations, a\nself-improving data flywheel effect emerges, producing datasets that cover\ndiverse scenarios and thereby scaling policy performance. Experimental results\ndemonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across\nfour challenging tasks. Policies trained on our dataset achieve an average\nsuccess rate of 81.9\\% on the challenge test sets and successfully transfer to\nthe real world through digital twin, achieving a 78.3\\% success rate on\ndual-arm lift tasks.", "AI": {"tldr": "DexFlyWheel\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u4e0d\u65ad\u4e30\u5bcc\u6570\u636e\u591a\u6837\u6027\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7075\u6d3b\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u6216\u4f9d\u8d56\u4eba\u5de5\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7075\u6d3b\u6027\u64cd\u4f5c\u7684\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "DexFlyWheel\u91c7\u7528\u95ed\u73af\u6d41\u7a0b\uff0c\u96c6\u6210\u6a21\u4eff\u5b66\u4e60\u3001\u5269\u4f59\u5f3a\u5316\u5b66\u4e60\u3001\u8f68\u8ff9\u6536\u96c6\u548c\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u6269\u5c55\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDexFlyWheel\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u751f\u6210\u4e862000\u591a\u6837\u672c\uff0c\u7b56\u7565\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5e73\u5747\u6210\u529f\u7387\u4e3a81.9%\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8fbe\u523078.3%\u3002", "conclusion": "DexFlyWheel\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u7684\u98de\u8f6e\u6548\u5e94\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u591a\u6837\u6027\u548c\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.23960", "pdf": "https://arxiv.org/pdf/2509.23960", "abs": "https://arxiv.org/abs/2509.23960", "authors": ["Manan Tayal", "Aditya Singh", "Shishir Kolathaya", "Somil Bansal"], "title": "MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control", "categories": ["cs.RO", "cs.AI"], "comment": "9 Pages, 4 Figures, 3 Tables. First two authors have contributed\n  equally", "summary": "Co-optimizing safety and performance in large-scale multi-agent systems\nremains a fundamental challenge. Existing approaches based on multi-agent\nreinforcement learning (MARL), safety filtering, or Model Predictive Control\n(MPC) either lack strict safety guarantees, suffer from conservatism, or fail\nto scale effectively. We propose MAD-PINN, a decentralized physics-informed\nmachine learning framework for solving the multi-agent state-constrained\noptimal control problem (MASC-OCP). Our method leverages an epigraph-based\nreformulation of SC-OCP to simultaneously capture performance and safety, and\napproximates its solution via a physics-informed neural network. Scalability is\nachieved by training the SC-OCP value function on reduced-agent systems and\ndeploying them in a decentralized fashion, where each agent relies only on\nlocal observations of its neighbours for decision-making. To further enhance\nsafety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based\nneighbour selection strategy to prioritize safety-critical interactions, and a\nreceding-horizon policy execution scheme that adapts to dynamic interactions\nwhile reducing computational burden. Experiments on multi-agent navigation\ntasks demonstrate that MAD-PINN achieves superior safety-performance\ntrade-offs, maintains scalability as the number of agents grows, and\nconsistently outperforms state-of-the-art baselines.", "AI": {"tldr": "MAD-PINN\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u3001\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u5b89\u5168\u6027\u548c\u6027\u80fd\u4f18\u5316\uff0c\u5728\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5982\u4f55\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3001\u5b89\u5168\u8fc7\u6ee4\u6216\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMAD-PINN\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8eepigraph\u7684\u91cd\u6784\u65b9\u6cd5\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u548c\u90e8\u7f72\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAD-PINN\u5728\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u6743\u8861\uff0c\u4e14\u968f\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "MAD-PINN\u901a\u8fc7\u521b\u65b0\u7684\u53bb\u4e2d\u5fc3\u5316\u548c\u5b89\u5168\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u3002"}}
{"id": "2509.24094", "pdf": "https://arxiv.org/pdf/2509.24094", "abs": "https://arxiv.org/abs/2509.24094", "authors": ["Vignesh Ramanathan", "Michael Milford", "Tobias Fischer"], "title": "Prepare for Warp Speed: Sub-millisecond Visual Place Recognition Using Event Cameras", "categories": ["cs.RO"], "comment": null, "summary": "Visual Place Recognition (VPR) enables systems to identify previously visited\nlocations within a map, a fundamental task for autonomous navigation. Prior\nworks have developed VPR solutions using event cameras, which asynchronously\nmeasure per-pixel brightness changes with microsecond temporal resolution.\nHowever, these approaches rely on dense representations of the inherently\nsparse camera output and require tens to hundreds of milliseconds of event data\nto predict a place. Here, we break this paradigm with Flash, a lightweight VPR\nsystem that predicts places using sub-millisecond slices of event data. Our\nmethod is based on the observation that active pixel locations provide strong\ndiscriminative features for VPR. Flash encodes these active pixel locations\nusing efficient binary frames and computes similarities via fast bitwise\noperations, which are then normalized based on the relative event activity in\nthe query and reference frames. Flash improves Recall@1 for sub-millisecond VPR\nover existing baselines by 11.33x on the indoor QCR-Event-Dataset and 5.92x on\nthe 8 km Brisbane-Event-VPR dataset. Moreover, our approach reduces the\nduration for which the robot must operate without awareness of its position, as\nevidenced by a localization latency metric we term Time to Correct Match (TCM).\nTo the best of our knowledge, this is the first work to demonstrate\nsub-millisecond VPR using event cameras.", "AI": {"tldr": "Flash\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e9a\u6beb\u79d2\u7ea7\u522b\u7684\u4e8b\u4ef6\u6570\u636e\u9884\u6d4b\u5730\u70b9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684VPR\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u8868\u793a\u4e14\u9700\u8981\u5927\u91cf\u4e8b\u4ef6\u6570\u636e\uff0cFlash\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "Flash\u5229\u7528\u6d3b\u8dc3\u50cf\u7d20\u4f4d\u7f6e\u7684\u7279\u5f81\uff0c\u91c7\u7528\u9ad8\u6548\u4e8c\u8fdb\u5236\u5e27\u548c\u5feb\u901f\u4f4d\u8fd0\u7b97\u8fdb\u884c\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002", "result": "\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\uff0cFlash\u7684Recall@1\u5206\u522b\u63d0\u5347\u4e8611.33\u500d\u548c5.92\u500d\uff0c\u5e76\u51cf\u5c11\u4e86\u5b9a\u4f4d\u5ef6\u8fdf\u3002", "conclusion": "Flash\u9996\u6b21\u5b9e\u73b0\u4e86\u4e9a\u6beb\u79d2\u7ea7\u522b\u7684VPR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2509.24124", "pdf": "https://arxiv.org/pdf/2509.24124", "abs": "https://arxiv.org/abs/2509.24124", "authors": ["Ilari Vallivaara", "Bingnan Duan", "Yinhuan Dong", "Tughrul Arslan"], "title": "Ancestry Tree Clustering for Particle Filter Diversity Maintenance", "categories": ["cs.RO", "cs.AI", "cs.LG", "F.2.2; G.3; I.5.3; F.2.2; I.2.9; G.3; I.5.3"], "comment": "15th International Conference on Indoor Positioning and Indoor\n  Navigation, 15-18 September 2025, Tampere, Finland Originally 8 pages. The\n  online version with appendices is 14 pages", "summary": "We propose a method for linear-time diversity maintenance in particle\nfiltering. It clusters particles based on ancestry tree topology: closely\nrelated particles in sufficiently large subtrees are grouped together. The main\nidea is that the tree structure implicitly encodes similarity without the need\nfor spatial or other domain-specific metrics. This approach, when combined with\nintra-cluster fitness sharing and the protection of particles not included in a\ncluster, effectively prevents premature convergence in multimodal environments\nwhile maintaining estimate compactness. We validate our approach in a\nmultimodal robotics simulation and a real-world multimodal indoor environment.\nWe compare the performance to several diversity maintenance algorithms from the\nliterature, including Deterministic Resampling and Particle Gaussian Mixtures.\nOur algorithm achieves high success rates with little to no negative effect on\ncompactness, showing particular robustness to different domains and challenging\ninitial conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u65f6\u95f4\u591a\u6837\u6027\u7ef4\u62a4\u7684\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u7956\u5148\u6811\u62d3\u6251\u7684\u805a\u7c7b\u9632\u6b62\u65e9\u719f\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7c92\u5b50\u6ee4\u6ce2\u7684\u65e9\u719f\u6536\u655b\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4f30\u8ba1\u7684\u7d27\u51d1\u6027\u3002", "method": "\u57fa\u4e8e\u7956\u5148\u6811\u62d3\u6251\u805a\u7c7b\u7c92\u5b50\uff0c\u7ed3\u5408\u7c07\u5185\u9002\u5e94\u5ea6\u5171\u4eab\u548c\u4fdd\u62a4\u672a\u805a\u7c7b\u7c92\u5b50\u3002", "result": "\u5728\u591a\u6a21\u6001\u673a\u5668\u4eba\u4eff\u771f\u548c\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u591a\u6837\u6027\u7ef4\u62a4\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u548c\u6311\u6218\u6027\u521d\u59cb\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u4e14\u5bf9\u7d27\u51d1\u6027\u5f71\u54cd\u5c0f\u3002"}}
{"id": "2509.24126", "pdf": "https://arxiv.org/pdf/2509.24126", "abs": "https://arxiv.org/abs/2509.24126", "authors": ["Athanasios Bacharis", "Konstantinos D. Polyzos", "Georgios B. Giannakis", "Nikolaos Papanikolopoulos"], "title": "BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Active vision (AV) has been in the spotlight of robotics research due to its\nemergence in numerous applications including agricultural tasks such as\nprecision crop monitoring and autonomous harvesting to list a few. A major AV\nproblem that gained popularity is the 3D reconstruction of targeted\nenvironments using 2D images from diverse viewpoints. While collecting and\nprocessing a large number of arbitrarily captured 2D images can be arduous in\nmany practical scenarios, a more efficient solution involves optimizing the\nplacement of available cameras in 3D space to capture fewer, yet more\ninformative, images that provide sufficient visual information for effective\nreconstruction of the environment of interest. This process termed as view\nplanning (VP), can be markedly challenged (i) by noise emerging in the location\nof the cameras and/or in the extracted images, and (ii) by the need to\ngeneralize well in other unknown similar agricultural environments without need\nfor re-optimizing or re-training. To cope with these challenges, the present\nwork presents a novel VP framework that considers a reconstruction\nquality-based optimization formulation that relies on the notion of\n`structure-from-motion' to reconstruct the 3D structure of the sought\nenvironment from the selected 2D images. With no analytic expression of the\noptimization function and with costly function evaluations, a Bayesian\noptimization approach is proposed to efficiently carry out the VP process using\nonly a few function evaluations, while accounting for different noise cases.\nNumerical tests on both simulated and real agricultural settings signify the\nbenefits of the advocated VP approach in efficiently estimating the optimal\ncamera placement to accurately reconstruct 3D environments of interest, and\ngeneralize well on similar unknown environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u89c6\u89d2\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u548c\u6cdb\u5316\u9700\u6c42\u4e0b\u9ad8\u6548\u786e\u5b9a\u6700\u4f18\u76f8\u673a\u4f4d\u7f6e\uff0c\u4ee5\u5b9e\u73b03D\u73af\u5883\u91cd\u5efa\u3002", "motivation": "\u4e3b\u52a8\u89c6\u89c9\u5728\u519c\u4e1a\u4efb\u52a1\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6536\u96c6\u548c\u5904\u7406\u5927\u91cf\u56fe\u50cf\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u91cd\u5efa\u8d28\u91cf\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408'structure-from-motion'\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u9ad8\u6548\u6c42\u89e3\u76f8\u673a\u6700\u4f18\u4f4d\u7f6e\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u4f30\u8ba1\u6700\u4f18\u76f8\u673a\u4f4d\u7f6e\uff0c\u5e76\u51c6\u786e\u91cd\u5efa3D\u73af\u5883\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89d2\u89c4\u5212\u6846\u67b6\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u9002\u5e94\u672a\u77e5\u7684\u7c7b\u4f3c\u519c\u4e1a\u73af\u5883\uff0c\u65e0\u9700\u91cd\u65b0\u4f18\u5316\u6216\u8bad\u7ec3\u3002"}}
{"id": "2509.24129", "pdf": "https://arxiv.org/pdf/2509.24129", "abs": "https://arxiv.org/abs/2509.24129", "authors": ["Priyanka Mandikal", "Jiaheng Hu", "Shivin Dass", "Sagnik Majumder", "Roberto Mart\u00edn-Mart\u00edn", "Kristen Grauman"], "title": "Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Most robot manipulation focuses on changing the kinematic state of objects:\npicking, placing, opening, or rotating them. However, a wide range of\nreal-world manipulation tasks involve a different class of object state\nchange--such as mashing, spreading, or slicing--where the object's physical and\nvisual state evolve progressively without necessarily changing its position. We\npresent SPARTA, the first unified framework for the family of object state\nchange manipulation tasks. Our key insight is that these tasks share a common\nstructural pattern: they involve spatially-progressing, object-centric changes\nthat can be represented as regions transitioning from an actionable to a\ntransformed state. Building on this insight, SPARTA integrates spatially\nprogressing object change segmentation maps, a visual skill to perceive\nactionable vs. transformed regions for specific object state change tasks, to\ngenerate a) structured policy observations that strip away appearance\nvariability, and b) dense rewards that capture incremental progress over time.\nThese are leveraged in two SPARTA policy variants: reinforcement learning for\nfine-grained control without demonstrations or simulation; and greedy control\nfor fast, lightweight deployment. We validate SPARTA on a real robot for three\nchallenging tasks across 10 diverse real-world objects, achieving significant\nimprovements in training time and accuracy over sparse rewards and visual\ngoal-conditioned baselines. Our results highlight progress-aware visual\nrepresentations as a versatile foundation for the broader family of object\nstate manipulation tasks. Project website:\nhttps://vision.cs.utexas.edu/projects/sparta-robot", "AI": {"tldr": "SPARTA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5bf9\u8c61\u72b6\u6001\u53d8\u5316\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\uff0c\u5982\u6405\u62cc\u3001\u6d82\u62b9\u6216\u5207\u7247\u3002\u5b83\u901a\u8fc7\u7a7a\u95f4\u63a8\u8fdb\u7684\u89c6\u89c9\u6280\u80fd\u548c\u5bc6\u96c6\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8bb8\u591a\u64cd\u7eb5\u4efb\u52a1\u6d89\u53ca\u5bf9\u8c61\u7269\u7406\u548c\u89c6\u89c9\u72b6\u6001\u7684\u6e10\u8fdb\u53d8\u5316\uff0c\u800c\u975e\u7b80\u5355\u7684\u8fd0\u52a8\u72b6\u6001\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u7edf\u4e00\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86SPARTA\u6846\u67b6\u3002", "method": "SPARTA\u5229\u7528\u4e86\u5bf9\u8c61\u72b6\u6001\u53d8\u5316\u7684\u7a7a\u95f4\u63a8\u8fdb\u6a21\u5f0f\uff0c\u901a\u8fc7\u611f\u77e5\u53ef\u64cd\u4f5c\u4e0e\u5df2\u53d8\u5316\u533a\u57df\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7b56\u7565\u89c2\u5bdf\u548c\u5bc6\u96c6\u5956\u52b1\u3002\u63d0\u4f9b\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u8d2a\u5a6a\u63a7\u5236\u7684\u4e24\u79cd\u7b56\u7565\u53d8\u4f53\u3002", "result": "\u572810\u79cd\u4e0d\u540c\u771f\u5b9e\u5bf9\u8c61\u4e0a\u76843\u4e2a\u4efb\u52a1\u4e2d\uff0cSPARTA\u663e\u8457\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u89c6\u89c9\u76ee\u6807\u6761\u4ef6\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u51c6\u786e\u6027\u7684\u63d0\u5347\u3002", "conclusion": "SPARTA\u7684\u6210\u529f\u8868\u660e\uff0c\u57fa\u4e8e\u89c6\u89c9\u8868\u5f81\u7684\u6e10\u8fdb\u611f\u77e5\u65b9\u6cd5\u662f\u5904\u7406\u5e7f\u6cdb\u5bf9\u8c61\u72b6\u6001\u64cd\u7eb5\u4efb\u52a1\u7684\u6709\u6548\u57fa\u7840\u3002"}}
{"id": "2509.24143", "pdf": "https://arxiv.org/pdf/2509.24143", "abs": "https://arxiv.org/abs/2509.24143", "authors": ["Deepak Prakash Kumar", "Swaroop Darbha", "Satyanarayana Gupta Manyam", "David Casbeer"], "title": "A Novel Model for 3D Motion Planning for a Generalized Dubins Vehicle with Pitch and Yaw Rate Constraints", "categories": ["cs.RO", "math.OC"], "comment": "The code for this paper is available at\n  https://github.com/DeepakPrakashKumar/3D-Motion-Planning-for-Generalized-Dubins-with-Pitch-Yaw-constraints", "summary": "In this paper, we propose a new modeling approach and a fast algorithm for 3D\nmotion planning, applicable for fixed-wing unmanned aerial vehicles. The goal\nis to construct the shortest path connecting given initial and final\nconfigurations subject to motion constraints. Our work differs from existing\nliterature in two ways. First, we consider full vehicle orientation using a\nbody-attached frame, which includes roll, pitch, and yaw angles. However,\nexisting work uses only pitch and/or heading angle, which is insufficient to\nuniquely determine orientation. Second, we use two control inputs to represent\nbounded pitch and yaw rates, reflecting control by two separate actuators. In\ncontrast, most previous methods rely on a single input, such as path curvature,\nwhich is insufficient for accurately modeling the vehicle's kinematics in 3D.\nWe use a rotation minimizing frame to describe the vehicle's configuration and\nits evolution, and construct paths by concatenating optimal Dubins paths on\nspherical, cylindrical, or planar surfaces. Numerical simulations show our\napproach generates feasible paths within 10 seconds on average and yields\nshorter paths than existing methods in most cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u548c\u5feb\u901f\u7b97\u6cd5\uff0c\u7528\u4e8e\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u76843D\u8fd0\u52a8\u89c4\u5212\uff0c\u76ee\u6807\u662f\u6784\u5efa\u6ee1\u8db3\u8fd0\u52a8\u7ea6\u675f\u7684\u6700\u77ed\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u901a\u5e38\u4ec5\u8003\u8651\u90e8\u5206\u8f66\u8f86\u65b9\u5411\uff08\u5982\u4fef\u4ef0\u89d2\u548c\u504f\u822a\u89d2\uff09\uff0c\u5e76\u4e14\u4f7f\u7528\u5355\u4e00\u63a7\u5236\u8f93\u5165\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff03D\u8fd0\u52a8\u5b66\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f66\u8eab\u9644\u7740\u7684\u6846\u67b6\u8003\u8651\u5b8c\u6574\u7684\u8f66\u8f86\u65b9\u5411\uff08\u6eda\u8f6c\u3001\u4fef\u4ef0\u548c\u504f\u822a\u89d2\uff09\uff0c\u91c7\u7528\u4e24\u4e2a\u63a7\u5236\u8f93\u5165\u8868\u793a\u6709\u754c\u7684\u4fef\u4ef0\u548c\u504f\u822a\u901f\u7387\uff0c\u5e76\u901a\u8fc7\u65cb\u8f6c\u6700\u5c0f\u5316\u6846\u67b6\u548c\u62fc\u63a5\u6700\u4f18Dubins\u8def\u5f84\u6784\u5efa\u8def\u5f84\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u572810\u79d2\u5185\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8def\u5f84\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u77ed\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b8c\u6574\u7684\u65b9\u5411\u63cf\u8ff0\u548c\u63a7\u5236\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8fd0\u52a8\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.24160", "pdf": "https://arxiv.org/pdf/2509.24160", "abs": "https://arxiv.org/abs/2509.24160", "authors": ["Tomoyuki Kagaya", "Subramanian Lakshmi", "Yuxuan Lou", "Thong Jing Yuan", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Yang You"], "title": "Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly explored in robot manipulation,\nbut many existing methods struggle to adapt to new environments. Many systems\nrequire either environment-specific policy training or depend on fixed prompts\nand single-shot code generation, leading to limited transferability and manual\nre-tuning. We introduce Memory Transfer Planning (MTP), a framework that\nleverages successful control-code examples from different environments as\nprocedural knowledge, using them as in-context guidance for LLM-driven\nplanning. Specifically, MTP (i) generates an initial plan and code using LLMs,\n(ii) retrieves relevant successful examples from a code memory, and (iii)\ncontextually adapts the retrieved code to the target setting for re-planning\nwithout updating model parameters. We evaluate MTP on RLBench, CALVIN, and a\nphysical robot, demonstrating effectiveness beyond simulation. Across these\nsettings, MTP consistently improved success rate and adaptability compared with\nfixed-prompt code generation, naive retrieval, and memory-free re-planning.\nFurthermore, in hardware experiments, leveraging a memory constructed in\nsimulation proved effective. MTP provides a practical approach that exploits\nprocedural knowledge to realize robust LLM-based planning across diverse\nrobotic manipulation scenarios, enhancing adaptability to novel environments\nand bridging simulation and real-world deployment.", "AI": {"tldr": "MTP\u6846\u67b6\u5229\u7528\u6210\u529f\u7684\u63a7\u5236\u4ee3\u7801\u793a\u4f8b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u63d0\u5347\u4e86LLM\u5728\u673a\u5668\u4eba\u64cd\u7eb5\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u4f9d\u8d56\u7279\u5b9a\u73af\u5883\u8bad\u7ec3\u6216\u56fa\u5b9a\u63d0\u793a\uff0c\u7f3a\u4e4f\u53ef\u8fc1\u79fb\u6027\u548c\u624b\u52a8\u8c03\u6574\u7684\u9700\u6c42\u3002", "method": "MTP\u901a\u8fc7\u751f\u6210\u521d\u59cb\u8ba1\u5212\u3001\u4ece\u4ee3\u7801\u8bb0\u5fc6\u5e93\u68c0\u7d22\u76f8\u5173\u793a\u4f8b\u5e76\u4e0a\u4e0b\u6587\u9002\u5e94\u76ee\u6807\u73af\u5883\uff0c\u5b9e\u73b0\u65e0\u9700\u8c03\u6574\u6a21\u578b\u53c2\u6570\u7684\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728RLBench\u3001CALVIN\u548c\u7269\u7406\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cMTP\u76f8\u6bd4\u56fa\u5b9a\u63d0\u793a\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\u3002", "conclusion": "MTP\u5229\u7528\u7a0b\u5e8f\u5316\u77e5\u8bc6\u589e\u5f3aLLM\u89c4\u5212\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u9002\u5e94\u65b0\u73af\u5883\u5e76\u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.24163", "pdf": "https://arxiv.org/pdf/2509.24163", "abs": "https://arxiv.org/abs/2509.24163", "authors": ["Wanming Yu", "Adrian R\u00f6fer", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models", "categories": ["cs.RO"], "comment": null, "summary": "Pretrained large language models (LLMs) can work as high-level robotic\nplanners by reasoning over abstract task descriptions and natural language\ninstructions, etc. However, they have shown a lack of knowledge and\neffectiveness in planning long-horizon robotic manipulation tasks where the\nphysical properties of the objects are essential. An example is the stacking of\ncontainers with hidden objects inside, which involves reasoning over hidden\nphysics properties such as weight and stability. To this end, this paper\nproposes to use multimodal LLMs as high-level planners for such long-horizon\nrobotic stacking tasks. The LLM takes multimodal inputs for each object to\nstack and infers the current best stacking sequence by reasoning over stacking\npreferences. Furthermore, in order to enable the LLM to reason over multiple\npreferences at the same time without giving explicit instructions, we propose\nto create a custom dataset considering stacking preferences including weight,\nstability, size, and footprint, to fine-tune the LLM. Compared to the\npretrained LLM with prompt tuning, we demonstrate the improved stacking\ncompletion of the LLM fine-tuned with our custom dataset via large-scale\nsimulation evaluation. Furthermore, we showcase the effectiveness of the\nproposed framework for the long-horizon stacking task on a real humanoid robot\nin an online manner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u957f\u671f\u673a\u5668\u4eba\u5806\u53e0\u4efb\u52a1\u7684\u9ad8\u5c42\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u63a8\u7406\u5806\u53e0\u504f\u597d\uff08\u5982\u91cd\u91cf\u3001\u7a33\u5b9a\u6027\u7b49\uff09\u6765\u6539\u8fdb\u4efb\u52a1\u5b8c\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u7684LLM\u5728\u9700\u8981\u7269\u4f53\u7269\u7406\u5c5e\u6027\u7684\u957f\u671f\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5982\u5806\u53e0\u9690\u85cf\u7269\u4f53\u7684\u5bb9\u5668\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4f5c\u4e3a\u9ad8\u5c42\u89c4\u5212\u5668\uff0c\u8f93\u5165\u591a\u6a21\u6001\u4fe1\u606f\uff1b\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff08\u5305\u542b\u5806\u53e0\u504f\u597d\uff09\u5fae\u8c03LLM\uff0c\u65e0\u9700\u663e\u5f0f\u6307\u4ee4\u5373\u53ef\u63a8\u7406\u591a\u504f\u597d\u3002", "result": "\u5fae\u8c03\u540e\u7684LLM\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5806\u53e0\u4efb\u52a1\u8868\u73b0\u4f18\u4e8e\u4ec5\u63d0\u793a\u8c03\u4f18\u7684\u9884\u8bad\u7ec3LLM\u3002", "conclusion": "\u591a\u6a21\u6001LLM\u7ed3\u5408\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u5806\u53e0\u4efb\u52a1\u7684\u89c4\u5212\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.24175", "pdf": "https://arxiv.org/pdf/2509.24175", "abs": "https://arxiv.org/abs/2509.24175", "authors": ["Rafael Kourdis", "Maciej St\u0119pie\u0144", "J\u00e9r\u00f4me Manhes", "Nicolas Mansard", "Steve Tonneau", "Philippe Sou\u00e8res", "Thomas Flayols"], "title": "Very High Frequency Interpolation for Direct Torque Control", "categories": ["cs.RO"], "comment": null, "summary": "Torque control enables agile and robust robot motion, but deployment is often\nhindered by instability and hardware limits. Here, we present a novel solution\nto execute whole-body linear feedback at up to 40 kHz on open-source hardware.\nWe use this to interpolate non-linear schemes during real-world execution, such\nas inverse dynamics and learned torque policies. Our results show that by\nstabilizing torque controllers, high-frequency linear feedback could be an\neffective route towards unlocking the potential of torque-controlled robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f00\u6e90\u786c\u4ef6\u4e0a\u4ee5\u9ad8\u8fbe40 kHz\u9891\u7387\u6267\u884c\u5168\u8eab\u7ebf\u6027\u53cd\u9988\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u626d\u77e9\u63a7\u5236\u5668\u5e76\u652f\u6301\u975e\u7ebf\u6027\u65b9\u6848\u5b9e\u65f6\u6267\u884c\u3002", "motivation": "\u626d\u77e9\u63a7\u5236\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u5177\u6709\u654f\u6377\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u5e38\u56e0\u4e0d\u7a33\u5b9a\u6027\u548c\u786c\u4ef6\u9650\u5236\u800c\u53d7\u963b\u3002", "method": "\u901a\u8fc7\u5728\u5f00\u6e90\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe40 kHz\u7684\u5168\u8eab\u7ebf\u6027\u53cd\u9988\uff0c\u5e76\u5b9e\u65f6\u63d2\u503c\u975e\u7ebf\u6027\u65b9\u6848\uff08\u5982\u9006\u52a8\u529b\u5b66\u548c\u5b66\u4e60\u626d\u77e9\u7b56\u7565\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7a33\u5b9a\u626d\u77e9\u63a7\u5236\u5668\u5e76\u901a\u8fc7\u9ad8\u9891\u7ebf\u6027\u53cd\u9988\u53ef\u4ee5\u6709\u6548\u91ca\u653e\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u7684\u9ad8\u9891\u7ebf\u6027\u53cd\u9988\u662f\u89e3\u9501\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u6f5c\u529b\u7684\u4e00\u79cd\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.24219", "pdf": "https://arxiv.org/pdf/2509.24219", "abs": "https://arxiv.org/abs/2509.24219", "authors": ["Tomoyuki Kagaya", "Subramanian Lakshmi", "Anbang Ye", "Thong Jing Yuan", "Jayashree Karlekar", "Sugiri Pranata", "Natsuki Murakami", "Akira Kinose", "Yang You"], "title": "ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL)\noften adapt slowly to new tasks, whereas recent Large Language Models (LLMs)\nand Vision-Language Models (VLMs) promise knowledge-rich planning from minimal\ndata. Deploying LLMs/VLMs for motion planning, however, faces two key\nobstacles: (i) symbolic plans are rarely grounded in scene geometry and object\nphysics, and (ii) model outputs can vary for identical prompts, undermining\nexecution reliability. We propose ViReSkill, a framework that pairs\nvision-grounded replanning with a skill memory for accumulation and reuse. When\na failure occurs, the replanner generates a new action sequence conditioned on\nthe current scene, tailored to the observed state. On success, the executed\nplan is stored as a reusable skill and replayed in future encounters without\nadditional calls to LLMs/VLMs. This feedback loop enables autonomous continual\nlearning: each attempt immediately expands the skill set and stabilizes\nsubsequent executions. We evaluate ViReSkill on simulators such as LIBERO and\nRLBench as well as on a physical robot. Across all settings, it consistently\noutperforms conventional baselines in task success rate, demonstrating robust\nsim-to-real generalization.", "AI": {"tldr": "ViReSkill\u7ed3\u5408\u89c6\u89c9\u91cd\u89c4\u5212\u548c\u6280\u80fd\u8bb0\u5fc6\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs/VLMs\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7f3a\u4e4f\u51e0\u4f55\u548c\u7269\u7406\u57fa\u7840\u53ca\u6267\u884c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u91cd\u89c4\u5212\u548c\u6280\u80fd\u8bb0\u5fc6\uff0c\u5931\u8d25\u65f6\u751f\u6210\u65b0\u52a8\u4f5c\u5e8f\u5217\uff0c\u6210\u529f\u65f6\u5b58\u50a8\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\u3002", "result": "\u5728LIBERO\u3001RLBench\u548c\u5b9e\u4f53\u673a\u5668\u4eba\u4e0a\u4efb\u52a1\u6210\u529f\u7387\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u3002", "conclusion": "ViReSkill\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u5b9e\u73b0\u81ea\u4e3b\u8fde\u7eed\u5b66\u4e60\uff0c\u63d0\u5347\u6267\u884c\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.24235", "pdf": "https://arxiv.org/pdf/2509.24235", "abs": "https://arxiv.org/abs/2509.24235", "authors": ["Xuan Lin", "Jiming Ren", "Yandong Luo", "Weijun Xie", "Ye Zhao"], "title": "Towards Tighter Convex Relaxation of Mixed-integer Programs: Leveraging Logic Network Flow for Task and Motion Planning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "35 pages, 17 figures, 7 tables", "summary": "This paper proposes an optimization-based task and motion planning framework,\nnamed \"Logic Network Flow\", that integrates temporal logic specifications into\nmixed-integer programs for efficient robot planning. Inspired by the\nGraph-of-Convex-Sets formulation, temporal predicates are encoded as polyhedron\nconstraints on each edge of a network flow model, instead of as constraints\nbetween nodes in traditional Logic Tree formulations. We further propose a\nnetwork-flow-based Fourier-Motzkin elimination procedure that removes\ncontinuous flow variables while preserving convex relaxation tightness, leading\nto provably tighter convex relaxations and fewer constraints than Logic Tree\nformulations. For temporal logic motion planning with piecewise-affine dynamic\nsystems, comprehensive experiments across vehicle routing, multi-robot\ncoordination, and temporal logic control on dynamical systems using point mass\nand linear inverted pendulum models demonstrate computational speedups of up to\nseveral orders of magnitude. Hardware demonstrations with quadrupedal robots\nvalidate real-time replanning capabilities under dynamically changing\nenvironmental conditions. The project website is at\nhttps://logicnetworkflow.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cLogic Network Flow\u201d\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u878d\u5165\u6df7\u5408\u6574\u6570\u7a0b\u5e8f\u4e2d\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u89c4\u5212\u7684\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u5c06\u65f6\u5e8f\u8c13\u8bcd\u7f16\u7801\u4e3a\u7f51\u7edc\u6d41\u6a21\u578b\u8fb9\u7f18\u7684\u591a\u9762\u4f53\u7ea6\u675f\uff0c\u800c\u975e\u4f20\u7edfLogic Tree\u4e2d\u7684\u8282\u70b9\u95f4\u7ea6\u675f\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u6d41\u6a21\u578b\u7684Fourier-Motzkin\u6d88\u9664\u6cd5\uff0c\u53bb\u9664\u8fde\u7eed\u6d41\u53d8\u91cf\u540c\u65f6\u4fdd\u6301\u51f8\u677e\u5f1b\u7d27\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u65f6\u5e8f\u903b\u8f91\u63a7\u5236\u4e2d\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u6761\u4ef6\u4e0b\u5c55\u793a\u4e86\u5b9e\u65f6\u91cd\u65b0\u89c4\u5212\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2509.24236", "pdf": "https://arxiv.org/pdf/2509.24236", "abs": "https://arxiv.org/abs/2509.24236", "authors": ["Siyan Dong", "Zijun Wang", "Lulu Cai", "Yi Ma", "Yanchao Yang"], "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u521d\u59cb\u5316\u548c\u4f18\u5316\u7ec6\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u76f8\u673a\u4e0d\u7a33\u5b9a\u8fd0\u52a8\u65f6\u5b9e\u73b0\u5b9e\u65f6\u5bc6\u96c6\u573a\u666f\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RGB-D SLAM\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u76f8\u673a\u8fd0\u52a8\u4e0d\u7a33\u5b9a\uff08\u5982\u5927\u89c6\u89d2\u53d8\u5316\u3001\u5feb\u901f\u8fd0\u52a8\u6216\u7a81\u7136\u6296\u52a8\uff09\u65f6RGB-D SLAM\u7cfb\u7edf\u7684\u5931\u6548\u95ee\u9898\uff0c\u5e73\u8861\u5b66\u4e60\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u76f8\u673a\u4f4d\u59ff\u56de\u5f52\u7f51\u7edc\u9884\u6d4b\u76f8\u5bf9\u4f4d\u59ff\u4f5c\u4e3a\u4f18\u5316\u7b97\u6cd5\u7684\u521d\u59cb\u503c\uff0c\u518d\u4f7f\u7528\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u8fdb\u4e00\u6b65\u5bf9\u9f50\u6df1\u5ea6\u56fe\u50cf\u4e0e\u573a\u666f\u51e0\u4f55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4e0d\u7a33\u5b9a\u8fd0\u52a8\u4e2d\u548c\u7a33\u5b9a\u5e8f\u5217\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u4e14\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u7ed3\u5408\u5b66\u4e60\u548c\u4f18\u5316\u7684\u7b80\u5355\u65b9\u6cd5\u80fd\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u4e3a\u5bc6\u96c6\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24243", "pdf": "https://arxiv.org/pdf/2509.24243", "abs": "https://arxiv.org/abs/2509.24243", "authors": ["Jeongyong Yang", "Seunghwan Jang", "Soojean Han"], "title": "SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions", "categories": ["cs.RO", "cs.AI"], "comment": "10 pages, 7 figures, 4 tables", "summary": "Generative planners based on flow matching (FM) can produce high-quality\npaths in one or a few ODE steps, but their sampling dynamics offer no formal\nsafety guarantees and can yield incomplete paths near constraints. We present\nSafeFlowMatcher, a planning framework that couples FM with control barrier\nfunctions (CBFs) to achieve both real-time efficiency and certified safety.\nSafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a\nprediction phase integrates the learned FM once (or a few steps) to obtain a\ncandidate path without intervention; (ii) a correction phase refines this path\nwith a vanishing time-scaled vector field and a CBF-based quadratic program\nthat minimally perturbs the vector field. We prove a barrier certificate for\nthe resulting flow system, establishing forward invariance of a robust safe set\nand finite-time convergence to the safe set. By enforcing safety only on the\nexecuted path (rather than on all intermediate latent paths), SafeFlowMatcher\navoids distributional drift and mitigates local trap problems. Across maze\nnavigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,\nand safer paths than diffusion- and FM-based baselines. Extensive ablations\ncorroborate the contributions of the PC integrator and the barrier certificate.", "AI": {"tldr": "SafeFlowMatcher\u662f\u4e00\u79cd\u7ed3\u5408\u6d41\u5339\u914d\uff08FM\uff09\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9884\u6d4b-\u6821\u6b63\u79ef\u5206\u5668\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u548c\u5b89\u5168\u6027\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u89c4\u5212\u5668\u7f3a\u5c11\u6b63\u5f0f\u5b89\u5168\u6027\u4fdd\u8bc1\u4e14\u53ef\u80fd\u5728\u7ea6\u675f\u9644\u8fd1\u751f\u6210\u4e0d\u5b8c\u6574\u8def\u5f84\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9884\u6d4b-\u6821\u6b63\uff08PC\uff09\u79ef\u5206\u5668\uff0c\u9884\u6d4b\u9636\u6bb5\u901a\u8fc7FM\u751f\u6210\u5019\u9009\u8def\u5f84\uff0c\u6821\u6b63\u9636\u6bb5\u901a\u8fc7CBFs\u548c\u4e8c\u6b21\u89c4\u5212\u6700\u5c0f\u6270\u52a8\u4f18\u5316\u8def\u5f84\u3002", "result": "\u8bc1\u660e\u4e86\u5c4f\u969c\u8bc1\u4e66\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e86\u8def\u5f84\u7684\u5b89\u5168\u6027\u548c\u6536\u655b\u6027\uff0c\u5728\u8ff7\u5bab\u5bfc\u822a\u548c\u8fd0\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SafeFlowMatcher\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5e73\u6ed1\u4e14\u5b89\u5168\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PC\u79ef\u5206\u5668\u548c\u5c4f\u969c\u8bc1\u4e66\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.24281", "pdf": "https://arxiv.org/pdf/2509.24281", "abs": "https://arxiv.org/abs/2509.24281", "authors": ["Kasra Torshizi", "Chak Lam Shek", "Khuzema Habib", "Guangyao Shi", "Pratap Tokekar", "Troi Williams"], "title": "Contextual Neural Moving Horizon Estimation for Robust Quadrotor Control in Varying Conditions", "categories": ["cs.RO"], "comment": "9 pages, 7 Figures, Kasra Torshizi and Chak Lam Shek contributed\n  equally", "summary": "Adaptive controllers on quadrotors typically rely on estimation of\ndisturbances to ensure robust trajectory tracking. Estimating disturbances\nacross diverse environmental contexts is challenging due to the inherent\nvariability and uncertainty in the real world. Such estimators require\nextensive fine-tuning for a specific scenario, which makes them inflexible and\nbrittle to changing conditions. Machine-learning approaches, such as training a\nneural network to tune the estimator's parameters, are promising. However,\ncollecting data across all possible environmental contexts is impossible. It is\nalso inefficient as the same estimator parameters could work for \"nearby\"\ncontexts. In this paper, we present a sequential decision making strategy that\ndecides which environmental contexts, using Bayesian Optimization with a\nGaussian Process, to collect data from in order to ensure robust performance\nacross a wide range of contexts. Our method, Contextual NeuroMHE, eliminates\nthe need for exhaustive training across all environments while maintaining\nrobust performance under different conditions. By enabling the neural network\nto adapt its parameters dynamically, our method improves both efficiency and\ngeneralization. Experimental results in various real-world settings demonstrate\nthat our approach outperforms the prior work by 20.3\\% in terms of maximum\nabsolute position error and can capture the variations in the environment with\na few carefully chosen contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u9002\u5e94\u63a7\u5236\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7279\u5b9a\u73af\u5883\u4e0a\u4e0b\u6587\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u63a7\u5236\u5668\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u73af\u5883\u8fdb\u884c\u5927\u91cf\u53c2\u6570\u8c03\u6574\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u6536\u96c6\u6240\u6709\u73af\u5883\u6570\u636e\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u9009\u62e9\u5173\u952e\u73af\u5883\u4e0a\u4e0b\u6587\u8fdb\u884c\u6570\u636e\u6536\u96c6\uff0c\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\uff0c\u63d0\u51faContextual NeuroMHE\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u5927\u7edd\u5bf9\u4f4d\u7f6e\u8bef\u5dee\u4e0a\u4f18\u4e8e\u524d\u65b9\u6cd520.3%\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u7cbe\u9009\u4e0a\u4e0b\u6587\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "conclusion": "Contextual NeuroMHE\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u53d8\u73af\u5883\u3002"}}
{"id": "2509.24313", "pdf": "https://arxiv.org/pdf/2509.24313", "abs": "https://arxiv.org/abs/2509.24313", "authors": ["Korbinian Moller", "Roland Stroop", "Mattia Piccinini", "Alexander Langmann", "Johannes Betz"], "title": "Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning", "categories": ["cs.RO"], "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based motion planning is a well-established approach in autonomous\ndriving, valued for its modularity and analytical tractability. In complex\nurban scenarios, however, uniform or heuristic sampling often produces many\ninfeasible or irrelevant trajectories. We address this limitation with a hybrid\nframework that learns where to sample while keeping trajectory generation and\nevaluation fully analytical and verifiable. A reinforcement learning (RL) agent\nguides the sampling process toward regions of the action space likely to yield\nfeasible trajectories, while evaluation and final selection remains governed by\ndeterministic feasibility checks and cost functions. We couple the RL sampler\nwith a world model (WM) based on a decodable deep set encoder, enabling both\nvariable numbers of traffic participants and reconstructable latent\nrepresentations. The approach is evaluated in the CommonRoad simulation\nenvironment, showing up to 99% fewer required samples and a runtime reduction\nof up to 84% while maintaining planning quality in terms of success and\ncollision-free rates. These improvements lead to faster, more reliable\ndecision-making for autonomous vehicles in urban environments, achieving safer\nand more responsive navigation under real-world constraints. Code and trained\nartifacts are publicly available at:\nhttps://github.com/TUM-AVS/Learning-to-Sample", "AI": {"tldr": "\u4e00\u79cd\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u91c7\u6837\u4f4d\u7f6e\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6548\u7387\uff0c\u51cf\u5c11\u65e0\u6548\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u5747\u5300\u6216\u542f\u53d1\u5f0f\u91c7\u6837\u5e38\u4ea7\u751f\u5927\u91cf\u4e0d\u53ef\u884c\u6216\u4e0d\u76f8\u5173\u7684\u8f68\u8ff9\uff0c\u9650\u5236\u4e86\u89c4\u5212\u6548\u7387\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u53ef\u89e3\u7801\u7684\u6df1\u5ea6\u96c6\u5408\u7f16\u7801\u5668\u4e16\u754c\u6a21\u578b\uff08WM\uff09\uff0c\u6307\u5bfc\u91c7\u6837\u5e76\u4fdd\u6301\u8f68\u8ff9\u751f\u6210\u7684\u89e3\u6790\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "result": "\u5728CommonRoad\u73af\u5883\u4e2d\uff0c\u6240\u9700\u6837\u672c\u51cf\u5c1199%\uff0c\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed84%\uff0c\u540c\u65f6\u4fdd\u6301\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u7684\u57ce\u5e02\u5bfc\u822a\u3002"}}
{"id": "2509.24321", "pdf": "https://arxiv.org/pdf/2509.24321", "abs": "https://arxiv.org/abs/2509.24321", "authors": ["Yao Wang", "Zhirui Sun", "Wenzheng Chi", "Baozhi Jia", "Wenjun Xu", "Jiankun Wang"], "title": "SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm", "categories": ["cs.RO"], "comment": null, "summary": "Understanding human instructions and accomplishing Vision-Language Navigation\ntasks in unknown environments is essential for robots. However, existing\nmodular approaches heavily rely on the quality of training data and often\nexhibit poor generalization. Vision-Language Model based methods, while\ndemonstrating strong generalization capabilities, tend to perform\nunsatisfactorily when semantic cues are weak. To address these issues, this\npaper proposes SONAR, an aggregated reasoning approach through a cross modal\nparadigm. The proposed method integrates a semantic map based target prediction\nmodule with a Vision-Language Model based value map module, enabling more\nrobust navigation in unknown environments with varying levels of semantic cues,\nand effectively balancing generalization ability with scene adaptability. In\nterms of target localization, we propose a strategy that integrates multi-scale\nsemantic maps with confidence maps, aiming to mitigate false detections of\ntarget objects. We conducted an evaluation of the SONAR within the Gazebo\nsimulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the\nexperimental benchmark. Experimental results demonstrate that SONAR achieves a\nsuccess rate of 38.4% and an SPL of 17.7%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSONAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8303\u5f0f\u6574\u5408\u8bed\u4e49\u5730\u56fe\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u6216\u8bed\u4e49\u7ebf\u7d22\u5f31\u65f6\u8868\u73b0\u4e0d\u4f73\u3002SONAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u5730\u56fe\u7684\u76ee\u6807\u9884\u6d4b\u6a21\u5757\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u503c\u6620\u5c04\u6a21\u5757\uff0c\u5e76\u96c6\u6210\u591a\u5c3a\u5ea6\u8bed\u4e49\u5730\u56fe\u4e0e\u7f6e\u4fe1\u5ea6\u5730\u56fe\u4ee5\u51cf\u5c11\u76ee\u6807\u8bef\u68c0\u3002", "result": "\u5728MP3D\u6570\u636e\u96c6\u4e0a\uff0cSONAR\u7684\u6210\u529f\u7387\u4e3a38.4%\uff0cSPL\u4e3a17.7%\u3002", "conclusion": "SONAR\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u9c81\u68d2\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u573a\u666f\u9002\u5e94\u6027\u3002"}}
{"id": "2509.24387", "pdf": "https://arxiv.org/pdf/2509.24387", "abs": "https://arxiv.org/abs/2509.24387", "authors": ["Xin Ding", "Jianyu Wei", "Yifan Yang", "Shiqi Jiang", "Qianxi Zhang", "Hao Wu", "Fucheng Jia", "Liang Mi", "Yuxuan Yan", "Weijun Wang", "Yunxin Liu", "Zhibo Chen", "Ting Cao"], "title": "AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Vision Language Navigation (VLN) requires agents to follow natural language\ninstructions by grounding them in sequential visual observations over long\nhorizons. Explicit reasoning could enhance temporal consistency and perception\naction alignment, but reasoning at fixed steps often leads to suboptimal\nperformance and unnecessary computation. To address this, we propose AdaNav, an\nuncertainty-based adaptive reasoning framework for VLN. At its core is the\nUncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that\ndynamically triggers reasoning. We introduce Action Entropy as a policy prior\nfor UAR and progressively refine it through a Heuristics to RL training method,\nenabling agents to learn difficulty aware reasoning policies under the strict\ndata limitations of embodied tasks. Results show that with only 6K training\nsamples, AdaNav achieves substantial gains over closed source models trained on\nmillion scale data, improving success rate by 20% on R2R val-unseen, 11.7% on\nRxR-CE, and 11.4% in real world scenes. The code is available at\nhttps://github.com/xinding-sys/AdaNav.", "AI": {"tldr": "AdaNav\u662f\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u89e6\u53d1\u63a8\u7406\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u6b65\u957f\u7684\u63a8\u7406\u65b9\u6cd5\u5728VLN\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u8ba1\u7b97\u5197\u4f59\uff0c\u9700\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u6765\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faUncertainty Adaptive Reasoning Block\uff08UAR\uff09\uff0c\u5229\u7528\u52a8\u4f5c\u71b5\u4f5c\u4e3a\u7b56\u7565\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u542f\u53d1\u5f0f\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u52a8\u6001\u89e6\u53d1\u63a8\u7406\u3002", "result": "\u4ec5\u75286K\u8bad\u7ec3\u6837\u672c\uff0cAdaNav\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u95ed\u6e90\u6a21\u578b\uff08R2R val-unseen\u63d0\u534720%\uff0cRxR-CE\u63d0\u534711.7%\uff0c\u771f\u5b9e\u573a\u666f\u63d0\u534711.4%\uff09\u3002", "conclusion": "AdaNav\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u548c\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u6709\u9650\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u5bfc\u822a\u3002"}}
{"id": "2509.24413", "pdf": "https://arxiv.org/pdf/2509.24413", "abs": "https://arxiv.org/abs/2509.24413", "authors": ["Tianqiang Yan", "Ziqiao Lin", "Sicheng Wang", "Tianwei Zhang", "Zhenglong Sun"], "title": "DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The emergence of large pre-trained models based on natural language has\nbreathed new life into robotics development. Extensive research has integrated\nlarge models with robots, utilizing the powerful semantic understanding and\ngeneration capabilities of large models to facilitate robot control through\nnatural language instructions gradually. However, we found that robots that\nstrictly adhere to human instructions, especially those containing misleading\ninformation, may encounter errors during task execution, potentially leading to\nsafety hazards. This resembles the concept of counterfactuals in natural\nlanguage processing (NLP), which has not yet attracted much attention in\nrobotic research. In an effort to highlight this issue for future studies, this\npaper introduced directive counterfactuals (DCFs) arising from misleading human\ndirectives. We present DynaMIC, a framework for generating robot task flows to\nidentify DCFs and relay feedback to humans proactively. This capability can\nhelp robots be sensitive to potential DCFs within a task, thus enhancing the\nreliability of the execution process. We conducted semantic-level experiments\nand ablation studies, showcasing the effectiveness of this framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DynaMIC\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u53cd\u9988\u8bef\u5bfc\u6027\u6307\u4ee4\uff08DCFs\uff09\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\uff0c\u673a\u5668\u4eba\u4e25\u683c\u9075\u5faa\u5e26\u6709\u8bef\u5bfc\u4fe1\u606f\u7684\u6307\u4ee4\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u9519\u8bef\u548c\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u76ee\u524d\u673a\u5668\u4eba\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u5f15\u5165DCFs\u6982\u5ff5\uff0c\u5e76\u63d0\u51faDynaMIC\u6846\u67b6\uff0c\u751f\u6210\u4efb\u52a1\u6d41\u7a0b\u4ee5\u8bc6\u522bDCFs\u5e76\u4e3b\u52a8\u53cd\u9988\u7ed9\u4eba\u7c7b\u3002", "result": "\u901a\u8fc7\u8bed\u4e49\u7ea7\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "DynaMIC\u6846\u67b6\u80fd\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u6f5c\u5728DCFs\u7684\u654f\u611f\u6027\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.24524", "pdf": "https://arxiv.org/pdf/2509.24524", "abs": "https://arxiv.org/abs/2509.24524", "authors": ["Zhihao Wang", "Jianxiong Li", "Jinliang Zheng", "Wencong Zhang", "Dongxiu Liu", "Yinan Zheng", "Haoyi Niu", "Junzhi Yu", "Xianyuan Zhan"], "title": "PhysiAgent: An Embodied Agent Framework in Physical World", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved notable success but often\nstruggle with limited generalizations. To address this, integrating generalized\nVision-Language Models (VLMs) as assistants to VLAs has emerged as a popular\nsolution. However, current approaches often combine these models in rigid,\nsequential structures: using VLMs primarily for high-level scene understanding\nand task planning, and VLAs merely as executors of lower-level actions, leading\nto ineffective collaboration and poor grounding challenges. In this paper, we\npropose an embodied agent framework, PhysiAgent, tailored to operate\neffectively in physical environments. By incorporating monitor, memory,\nself-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent\noffers an autonomous scaffolding framework to prompt VLMs to organize different\ncomponents based on real-time proficiency feedback from VLAs to maximally\nexploit VLAs' capabilities. Experimental results demonstrate significant\nimprovements in task-solving performance on complex real-world robotic tasks,\nshowcasing effective self-regulation of VLMs, coherent tool collaboration, and\nadaptive evolution of the framework during execution. PhysiAgent makes\npractical and pioneering efforts to integrate VLMs and VLAs, effectively\ngrounding embodied agent frameworks in real-world settings.", "AI": {"tldr": "PhysiAgent\u662f\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u76d1\u63a7\u3001\u8bb0\u5fc6\u548c\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u6cdb\u5316\u548c\u534f\u4f5c\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u534f\u4f5c\u65b9\u5f0f\u50f5\u786c\uff0c\u5bfc\u81f4\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u3002PhysiAgent\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u6846\u67b6\u4f18\u5316\u4e24\u8005\u7684\u534f\u4f5c\u3002", "method": "\u63d0\u51faPhysiAgent\u6846\u67b6\uff0c\u6574\u5408\u76d1\u63a7\u3001\u8bb0\u5fc6\u3001\u81ea\u6211\u53cd\u601d\u673a\u5236\u548c\u8f7b\u91cf\u7ea7\u5de5\u5177\u5305\uff0c\u57fa\u4e8eVLA\u7684\u5b9e\u65f6\u53cd\u9988\u52a8\u6001\u8c03\u6574VLM\u7684\u7ec4\u4ef6\u7ec4\u7ec7\u3002", "result": "\u5728\u590d\u6742\u73b0\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cPhysiAgent\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u89e3\u51b3\u6027\u80fd\uff0c\u5c55\u793a\u4e86VLM\u7684\u6709\u6548\u81ea\u6211\u8c03\u8282\u548c\u5de5\u5177\u534f\u4f5c\u80fd\u529b\u3002", "conclusion": "PhysiAgent\u4e3aVLM\u548cVLA\u7684\u6709\u6548\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2509.24530", "pdf": "https://arxiv.org/pdf/2509.24530", "abs": "https://arxiv.org/abs/2509.24530", "authors": ["Giulia Pusceddu", "Sara Mongile", "Francesco Rea", "Alessandra Sciutti"], "title": "Game Theory to Study Cooperation in Human-Robot Mixed Groups: Exploring the Potential of the Public Good Game", "categories": ["cs.RO"], "comment": "Work presented at the workshop BAILAR in conjunction with IEEE RO-MAN\n  2023. Peer reviewed", "summary": "In this study, we explore the potential of Game Theory as a means to\ninvestigate cooperation and trust in human-robot mixed groups. Particularly, we\nintroduce the Public Good Game (PGG), a model highlighting the tension between\nindividual self-interest and collective well-being. In this work, we present a\nmodified version of the PGG, where three human participants engage in the game\nwith the humanoid robot iCub to assess whether various robot game strategies\n(e.g., always cooperate, always free ride, and tit-for-tat) can influence the\nparticipants' inclination to cooperate. We test our setup during a pilot study\nwith nineteen participants. A preliminary analysis indicates that participants\nprefer not to invest their money in the common pool, despite they perceive the\nrobot as generous. By conducting this research, we seek to gain valuable\ninsights into the role that robots can play in promoting trust and cohesion\nduring human-robot interactions within group contexts. The results of this\nstudy may hold considerable potential for developing social robots capable of\nfostering trust and cooperation within mixed human-robot groups.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u535a\u5f08\u8bba\u7814\u7a76\u4eba\u673a\u6df7\u5408\u7fa4\u4f53\u4e2d\u7684\u5408\u4f5c\u4e0e\u4fe1\u4efb\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u516c\u5171\u7269\u54c1\u6e38\u620f\uff08PGG\uff09\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u4eba\u7b56\u7565\u5bf9\u4eba\u7c7b\u5408\u4f5c\u503e\u5411\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u5728\u4fc3\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u4fe1\u4efb\u4e0e\u51dd\u805a\u529b\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4ee5\u671f\u4e3a\u5f00\u53d1\u80fd\u591f\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5408\u4f5c\u7684\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u89c1\u89e3\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684PGG\u5b9e\u9a8c\uff0c\u4e09\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u4e0e\u4eba\u5f62\u673a\u5668\u4ebaiCub\u4e92\u52a8\uff0c\u6d4b\u8bd5\u4e0d\u540c\u673a\u5668\u4eba\u7b56\u7565\uff08\u5982\u603b\u662f\u5408\u4f5c\u3001\u603b\u662f\u642d\u4fbf\u8f66\u3001\u4ee5\u7259\u8fd8\u7259\uff09\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u521d\u6b65\u5206\u6790\u663e\u793a\uff0c\u5c3d\u7ba1\u53c2\u4e0e\u8005\u8ba4\u4e3a\u673a\u5668\u4eba\u6177\u6168\uff0c\u4f46\u4ed6\u4eec\u4ecd\u503e\u5411\u4e8e\u4e0d\u5411\u516c\u5171\u6c60\u6295\u8d44\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u80fd\u591f\u4fc3\u8fdb\u4eba\u673a\u7fa4\u4f53\u4fe1\u4efb\u4e0e\u5408\u4f5c\u7684\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2509.24539", "pdf": "https://arxiv.org/pdf/2509.24539", "abs": "https://arxiv.org/abs/2509.24539", "authors": ["Nayari Marie Lessa", "Melya Boukheddimi", "Frank Kirchner"], "title": "Unlocking the Potential of Soft Actor-Critic for Imitation Learning", "categories": ["cs.RO"], "comment": null, "summary": "Learning-based methods have enabled robots to acquire bio-inspired movements\nwith increasing levels of naturalness and adaptability. Among these, Imitation\nLearning (IL) has proven effective in transferring complex motion patterns from\nanimals to robotic systems. However, current state-of-the-art frameworks\npredominantly rely on Proximal Policy Optimization (PPO), an on-policy\nalgorithm that prioritizes stability over sample efficiency and policy\ngeneralization. This paper proposes a novel IL framework that combines\nAdversarial Motion Priors (AMP) with the off-policy Soft Actor-Critic (SAC)\nalgorithm to overcome these limitations. This integration leverages\nreplay-driven learning and entropy-regularized exploration, enabling\nnaturalistic behavior and task execution, improving data efficiency and\nrobustness. We evaluate the proposed approach (AMP+SAC) on quadruped gaits\ninvolving multiple reference motions and diverse terrains. Experimental results\ndemonstrate that the proposed framework not only maintains stable task\nexecution but also achieves higher imitation rewards compared to the widely\nused AMP+PPO method. These findings highlight the potential of an off-policy IL\nformulation for advancing motion generation in robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08AMP\uff09\u548c\u79bb\u7b56\u7565\u8f6f\u884c\u52a8\u8005-\u6279\u8bc4\u8005\uff08SAC\uff09\u7b97\u6cd5\u7684\u65b0\u578b\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4eff\u751f\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u81ea\u7136\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06AMP\u4e0eSAC\u7ed3\u5408\uff0c\u5229\u7528\u56de\u653e\u9a71\u52a8\u7684\u5b66\u4e60\u548c\u71b5\u6b63\u5219\u5316\u63a2\u7d22\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAMP+SAC\u5728\u591a\u5730\u5f62\u56db\u8db3\u8fd0\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6a21\u4eff\u5956\u52b1\u548c\u7a33\u5b9a\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u79bb\u7b56\u7565\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2509.24575", "pdf": "https://arxiv.org/pdf/2509.24575", "abs": "https://arxiv.org/abs/2509.24575", "authors": ["Nicolas Pfitzer", "Eduardo Sebasti\u00e1n", "Ajay Shankar", "Amanda Prorok"], "title": "Prompting Robot Teams with Natural Language", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": null, "summary": "This paper presents a framework towards prompting multi-robot teams with\nhigh-level tasks using natural language expressions. Our objective is to use\nthe reasoning capabilities demonstrated by recent language models in\nunderstanding and decomposing human expressions of intent, and repurpose these\nfor multi-robot collaboration and decision-making. The key challenge is that an\nindividual's behavior in a collective can be hard to specify and interpret, and\nmust continuously adapt to actions from others. This necessitates a framework\nthat possesses the representational capacity required by the logic and\nsemantics of a task, and yet supports decentralized and interactive real-time\noperation. We solve this dilemma by recognizing that a task can be represented\nas a deterministic finite automaton (DFA), and that recurrent neural networks\n(RNNs) can encode numerous automata. This allows us to distill the logic and\nsequential decompositions of sub-tasks obtained from a language model into an\nRNN, and align its internal states with the semantics of a given task. By\ntraining a graph neural network (GNN) control policy that is conditioned on the\nhidden states of the RNN and the language embeddings, our method enables robots\nto execute task-relevant actions in a decentralized manner. We present\nevaluations of this single light-weight interpretable model on various\nsimulated and real-world multi-robot tasks that require sequential and\ncollaborative behavior by the team -- sites.google.com/view/prompting-teams.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u591a\u673a\u5668\u4eba\u56e2\u961f\u6267\u884c\u9ad8\u7ea7\u4efb\u52a1\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548cRNN\u3001GNN\u5b9e\u73b0\u5206\u6563\u534f\u4f5c\u3002", "motivation": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u7406\u89e3\u548c\u5206\u89e3\u4eba\u7c7b\u610f\u56fe\u8868\u8fbe\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e0e\u51b3\u7b56\u4e2d\u3002", "method": "\u5c06\u4efb\u52a1\u8868\u793a\u4e3aDFA\uff0c\u901a\u8fc7RNN\u7f16\u7801\u4efb\u52a1\u903b\u8f91\uff0c\u5e76\u7528GNN\u63a7\u5236\u7b56\u7565\u5206\u6563\u6267\u884c\u4efb\u52a1\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u591a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6846\u67b6\u6210\u529f\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u591a\u673a\u5668\u4eba\u5206\u6563\u534f\u4f5c\u7ed3\u5408\uff0c\u9002\u7528\u4e8e\u987a\u5e8f\u548c\u534f\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.24579", "pdf": "https://arxiv.org/pdf/2509.24579", "abs": "https://arxiv.org/abs/2509.24579", "authors": ["Linzhi Wu", "Aoran Mei", "Xiyue Wang", "Guo-Niu Zhu", "Zhongxue Gan"], "title": "U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Diffusion-based methods have been acknowledged as a powerful paradigm for\nend-to-end visuomotor control in robotics. Most existing approaches adopt a\nDiffusion Policy in U-Net architecture (DP-U), which, while effective, suffers\nfrom limited global context modeling and over-smoothing artifacts. To address\nthese issues, we propose U-DiT Policy, a novel U-shaped Diffusion Transformer\nframework. U-DiT preserves the multi-scale feature fusion advantages of U-Net\nwhile integrating the global context modeling capability of Transformers,\nthereby enhancing representational power and policy expressiveness. We evaluate\nU-DiT extensively across both simulation and real-world robotic manipulation\ntasks. In simulation, U-DiT achieves an average performance gain of 10\\% over\nbaseline methods and surpasses Transformer-based diffusion policies (DP-T) that\nuse AdaLN blocks by 6\\% under comparable parameter budgets. On real-world\nrobotic tasks, U-DiT demonstrates superior generalization and robustness,\nachieving an average improvement of 22.5\\% over DP-U. In addition, robustness\nand generalization experiments under distractor and lighting variations further\nhighlight the advantages of U-DiT. These results highlight the effectiveness\nand practical potential of U-DiT Policy as a new foundation for diffusion-based\nrobotic manipulation.", "AI": {"tldr": "U-DiT Policy\u662f\u4e00\u79cd\u65b0\u578b\u7684U\u5f62\u6269\u6563\u53d8\u6362\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u4e86U-Net\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4f18\u52bf\u548cTransformers\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684DP-U\u65b9\u6cd5\u5728\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8fc7\u5ea6\u5e73\u6ed1\u4f2a\u5f71\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u8868\u793a\u80fd\u529b\u548c\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faU-DiT Policy\u6846\u67b6\uff0c\u4fdd\u7559U-Net\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u540c\u65f6\u5f15\u5165Transformers\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cU-DiT\u5206\u522b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u534710%\u548c22.5%\uff0c\u4e14\u5728\u591a\u53d8\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "U-DiT Policy\u4f5c\u4e3a\u4e00\u79cd\u65b0\u57fa\u7840\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24591", "pdf": "https://arxiv.org/pdf/2509.24591", "abs": "https://arxiv.org/abs/2509.24591", "authors": ["Haozhuo Zhang", "Michele Caprio", "Jing Shao", "Qiang Zhang", "Jian Tang", "Shanghang Zhang", "Wei Pan"], "title": "PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present PoseDiff, a conditional diffusion model that unifies robot state\nestimation and control within a single framework. At its core, PoseDiff maps\nraw visual observations into structured robot states-such as 3D keypoints or\njoint angles-from a single RGB image, eliminating the need for multi-stage\npipelines or auxiliary modalities. Building upon this foundation, PoseDiff\nextends naturally to video-to-action inverse dynamics: by conditioning on\nsparse video keyframes generated by world models, it produces smooth and\ncontinuous long-horizon action sequences through an overlap-averaging strategy.\nThis unified design enables scalable and efficient integration of perception\nand control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy\nand real-time performance for pose estimation. On Libero-Object manipulation\ntasks, it substantially improves success rates over existing inverse dynamics\nmodules, even under strict offline settings. Together, these results show that\nPoseDiff provides a scalable, accurate, and efficient bridge between\nperception, planning, and control in embodied AI. The video visualization\nresults can be found on the project page:\nhttps://haozhuo-zhang.github.io/PoseDiff-project-page/.", "AI": {"tldr": "PoseDiff\u662f\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4e0e\u63a7\u5236\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u5f20RGB\u56fe\u50cf\u76f4\u63a5\u751f\u6210\u7ed3\u6784\u5316\u72b6\u6001\uff08\u59823D\u5173\u952e\u70b9\u6216\u5173\u8282\u89d2\uff09\uff0c\u65e0\u9700\u591a\u9636\u6bb5\u6d41\u7a0b\u6216\u8f85\u52a9\u6a21\u6001\u3002\u5b83\u8fd8\u652f\u6301\u89c6\u9891\u5230\u52a8\u4f5c\u7684\u9006\u5411\u52a8\u529b\u5b66\uff0c\u751f\u6210\u6d41\u7545\u7684\u957f\u65f6\u7a0b\u52a8\u4f5c\u5e8f\u5217\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u63a7\u5236\u4e4b\u95f4\u7684\u96c6\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8de8\u6a21\u5757\u534f\u540c\u3002", "method": "\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u89c6\u89c9\u89c2\u6d4b\u6620\u5c04\u5230\u673a\u5668\u4eba\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u5173\u952e\u5e27\u751f\u6210\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728DREAM\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u9886\u5148\u7684\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff1b\u5728Libero-Object\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "conclusion": "PoseDiff\u4e3a\u5177\u8eabAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u611f\u77e5\u4e0e\u63a7\u5236\u6865\u6881\u3002"}}
{"id": "2509.24661", "pdf": "https://arxiv.org/pdf/2509.24661", "abs": "https://arxiv.org/abs/2509.24661", "authors": ["Zhiyuan Wu", "Rolandos Alexandros Potamias", "Xuyang Zhang", "Zhongqun Zhang", "Jiankang Deng", "Shan Luo"], "title": "CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Cross-embodiment dexterous grasp synthesis refers to adaptively generating\nand optimizing grasps for various robotic hands with different morphologies.\nThis capability is crucial for achieving versatile robotic manipulation in\ndiverse environments and requires substantial amounts of reliable and diverse\ngrasp data for effective model training and robust generalization. However,\nexisting approaches either rely on physics-based optimization that lacks\nhuman-like kinematic understanding or require extensive manual data collection\nprocesses that are limited to anthropomorphic structures. In this paper, we\npropose CEDex, a novel cross-embodiment dexterous grasp synthesis method at\nscale that bridges human grasping kinematics and robot kinematics by aligning\nrobot kinematic models with generated human-like contact representations. Given\nan object's point cloud and an arbitrary robotic hand model, CEDex first\ngenerates human-like contact representations using a Conditional Variational\nAuto-encoder pretrained on human contact data. It then performs kinematic human\ncontact alignment through topological merging to consolidate multiple human\nhand parts into unified robot components, followed by a signed distance\nfield-based grasp optimization with physics-aware constraints. Using CEDex, we\nconstruct the largest cross-embodiment grasp dataset to date, comprising 500K\nobjects across four gripper types with 20M total grasps. Extensive experiments\nshow that CEDex outperforms state-of-the-art approaches and our dataset\nbenefits cross-embodiment grasp learning with high-quality diverse grasps.", "AI": {"tldr": "CEDex\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8de8\u5f62\u6001\u7075\u5de7\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6293\u53d6\u8fd0\u52a8\u5b66\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6293\u53d6\u751f\u6210\u548c\u4f18\u5316\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u4e4f\u4eba\u7c7b\u8fd0\u52a8\u5b66\u7406\u89e3\u6216\u9700\u5927\u91cf\u4eba\u5de5\u6570\u636e\u6536\u96c6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "method": "CEDex\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u4eba\u7c7b\u63a5\u89e6\u8868\u793a\uff0c\u901a\u8fc7\u62d3\u6251\u5408\u5e76\u548c\u57fa\u4e8e\u8ddd\u79bb\u573a\u7684\u4f18\u5316\u5b9e\u73b0\u6293\u53d6\u5408\u6210\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b500K\u5bf9\u8c61\u548c20M\u6293\u53d6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793aCEDex\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CEDex\u4e3a\u8de8\u5f62\u6001\u6293\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6570\u636e\u96c6\u4fc3\u8fdb\u4e86\u6293\u53d6\u5b66\u4e60\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2509.24697", "pdf": "https://arxiv.org/pdf/2509.24697", "abs": "https://arxiv.org/abs/2509.24697", "authors": ["Evelyn D'Elia", "Paolo Maria Viceconte", "Lorenzo Rapetti", "Diego Ferigo", "Giulio Romualdi", "Giuseppe L'Erario", "Raffaello Camoriano", "Daniele Pucci"], "title": "Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "This paper has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS), Hangzhou,\n  China, 2025", "summary": "Recent trends in humanoid robot control have successfully employed imitation\nlearning to enable the learned generation of smooth, human-like trajectories\nfrom human data. While these approaches make more realistic motions possible,\nthey are limited by the amount of available motion data, and do not incorporate\nprior knowledge about the physical laws governing the system and its\ninteractions with the environment. Thus they may violate such laws, leading to\ndivergent trajectories and sliding contacts which limit real-world stability.\nWe address such limitations via a two-pronged learning strategy which leverages\nthe known physics of the system and fundamental control principles. First, we\nencode physics priors during supervised imitation learning to promote\ntrajectory feasibility. Second, we minimize drift at inference time by applying\na proportional-integral controller directly to the generated output state. We\nvalidate our method on various locomotion behaviors for the ergoCub humanoid\nrobot, where a physics-informed loss encourages zero contact foot velocity. Our\nexperiments demonstrate that the proposed approach is compatible with multiple\ncontrollers on a real robot and significantly improves the accuracy and\nphysical constraint conformity of generated trajectories.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u63a7\u5236\u539f\u7406\uff0c\u6539\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u63d0\u9ad8\u771f\u5b9e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u867d\u80fd\u751f\u6210\u5e73\u6ed1\u8f68\u8ff9\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u91cf\u4e14\u5ffd\u7565\u7269\u7406\u89c4\u5f8b\uff0c\u5bfc\u81f4\u8f68\u8ff9\u53d1\u6563\u548c\u6ed1\u52a8\u63a5\u89e6\uff0c\u5f71\u54cd\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u53cc\u7ba1\u9f50\u4e0b\u7684\u5b66\u4e60\u7b56\u7565\uff1a1\uff09\u5728\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u4e2d\u5f15\u5165\u7269\u7406\u5148\u9a8c\uff1b2\uff09\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u6bd4\u4f8b\u79ef\u5206\u63a7\u5236\u5668\u51cf\u5c11\u6f02\u79fb\u3002", "result": "\u5728ergoCub\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u51c6\u786e\u6027\u548c\u7269\u7406\u7ea6\u675f\u7b26\u5408\u6027\uff0c\u517c\u5bb9\u591a\u79cd\u63a7\u5236\u5668\u3002", "conclusion": "\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u63a7\u5236\u539f\u7406\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u771f\u5b9e\u6027\u3002"}}
{"id": "2509.24706", "pdf": "https://arxiv.org/pdf/2509.24706", "abs": "https://arxiv.org/abs/2509.24706", "authors": ["Andreea Tulbure", "Rene Zurbruegg", "Timm Grigat", "Marco Hutter"], "title": "LLM-Handover:Exploiting LLMs for Task-Oriented Robot-Human Handovers", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Effective human-robot collaboration depends on task-oriented handovers, where\nrobots present objects in ways that support the partners intended use. However,\nmany existing approaches neglect the humans post-handover action, relying on\nassumptions that limit generalizability. To address this gap, we propose\nLLM-Handover, a novel framework that integrates large language model\n(LLM)-based reasoning with part segmentation to enable context-aware grasp\nselection and execution. Given an RGB-D image and a task description, our\nsystem infers relevant object parts and selects grasps that optimize\npost-handover usability. To support evaluation, we introduce a new dataset of\n60 household objects spanning 12 categories, each annotated with detailed part\nlabels. We first demonstrate that our approach improves the performance of the\nused state-of-the-art part segmentation method, in the context of robot-human\nhandovers. Next, we show that LLM-Handover achieves higher grasp success rates\nand adapts better to post-handover task constraints. During hardware\nexperiments, we achieve a success rate of 83% in a zero-shot setting over\nconventional and unconventional post-handover tasks. Finally, our user study\nunderlines that our method enables more intuitive, context-aware handovers,\nwith participants preferring it in 86% of cases.", "AI": {"tldr": "LLM-Handover\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u90e8\u5206\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba-\u4eba\u7c7b\u4ea4\u63a5\u4efb\u52a1\u4e2d\u7684\u6293\u53d6\u9009\u62e9\u548c\u6267\u884c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u4ea4\u63a5\u65b9\u6cd5\u5e38\u5ffd\u89c6\u4eba\u7c7b\u4ea4\u63a5\u540e\u7684\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6839\u636e\u4efb\u52a1\u4e0a\u4e0b\u6587\u9009\u62e9\u6293\u53d6\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7RGB-D\u56fe\u50cf\u548c\u4efb\u52a1\u63cf\u8ff0\uff0c\u7cfb\u7edf\u63a8\u65ad\u76f8\u5173\u7269\u4f53\u90e8\u5206\u5e76\u9009\u62e9\u4f18\u5316\u4ea4\u63a5\u540e\u4f7f\u7528\u6027\u7684\u6293\u53d6\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u4efb\u52a1\u6210\u529f\u738783%\uff0c\u7528\u6237\u7814\u7a76\u4e2d86%\u7684\u53c2\u4e0e\u8005\u504f\u597d\u8be5\u65b9\u6cd5\u3002", "conclusion": "LLM-Handover\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u63a5\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2509.24733", "pdf": "https://arxiv.org/pdf/2509.24733", "abs": "https://arxiv.org/abs/2509.24733", "authors": ["Zihao Xu", "Kuankuan Sima", "Junhao Deng", "Zixuan Zhuang", "Chunzheng Wang", "Ce Hao", "Jin Song Dong"], "title": "APREBot: Active Perception System for Reflexive Evasion Robot", "categories": ["cs.RO"], "comment": null, "summary": "Reliable onboard perception is critical for quadruped robots navigating\ndynamic environments, where obstacles can emerge from any direction under\nstrict reaction-time constraints. Single-sensor systems face inherent\nlimitations: LiDAR provides omnidirectional coverage but lacks rich texture\ninformation, while cameras capture high-resolution detail but suffer from\nrestricted field of view. We introduce APREBot (Active Perception System for\nReflexive Evasion Robot), a novel framework that integrates reflexive evasion\nwith active hierarchical perception. APREBot strategically combines LiDAR-based\nomnidirectional scanning with camera-based active focusing, achieving\ncomprehensive environmental awareness essential for agile obstacle avoidance in\nquadruped robots. We validate APREBot through extensive sim-to-real experiments\non a quadruped platform, evaluating diverse obstacle types, trajectories, and\napproach directions. Our results demonstrate substantial improvements over\nstate-of-the-art baselines in both safety metrics and operational efficiency,\nhighlighting APREBot's potential for dependable autonomy in safety-critical\nscenarios. Videos are available at https://sites.google.com/view/aprebot/", "AI": {"tldr": "APREBot\u662f\u4e00\u79cd\u96c6\u6210\u4e86LiDAR\u548c\u6444\u50cf\u5934\u7684\u4e3b\u52a8\u611f\u77e5\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u969c\u788d\u7269\u907f\u969c\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u611f\u77e5\u7684\u6311\u6218\uff0c\u5f25\u8865\u5355\u4e00\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408LiDAR\u7684\u5168\u5411\u626b\u63cf\u548c\u6444\u50cf\u5934\u7684\u4e3b\u52a8\u805a\u7126\uff0c\u5b9e\u73b0\u5206\u5c42\u611f\u77e5\u548c\u53cd\u5c04\u6027\u907f\u969c\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0cAPREBot\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "APREBot\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u81ea\u4e3b\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24763", "pdf": "https://arxiv.org/pdf/2509.24763", "abs": "https://arxiv.org/abs/2509.24763", "authors": ["Xiangyi Meng", "Delun Li", "Zihao Mao", "Yi Yang", "Wenjie Song"], "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework", "categories": ["cs.RO"], "comment": null, "summary": "Zero-shot object navigation in unknown environments presents significant\nchallenges, mainly due to two key limitations: insufficient semantic guidance\nleads to inefficient exploration, while limited spatial memory resulting from\nenvironmental structure causes entrapment in local regions. To address these\nissues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object\nnavigation method based on the TARE hierarchical exploration framework,\nintegrating a viewpoint generation strategy balancing spatial coverage and\nsemantic density with an LLM-based global guidance mechanism. The performance\nimprovement of the proposed method is due to two key innovations. First, the\nviewpoint generation strategy prioritizes areas of high semantic density within\ntraversable sub-regions to maximize spatial coverage and minimize invalid\nexploration. Second, coupled with an LLM-based global guidance mechanism, it\nassesses semantic associations to direct navigation toward high-value spaces,\npreventing local entrapment and ensuring efficient exploration. Deployed on\nhybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves\nreal-time operation and superior performance. On Matterport3D and\nHabitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and\n11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140,\nrespectively, over state-of-the-art methods.", "AI": {"tldr": "SSR-ZSON\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u8bed\u4e49\u76f8\u5bf9\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u7a7a\u95f4\u8986\u76d6\u548c\u8bed\u4e49\u5bc6\u5ea6\uff0c\u7ed3\u5408LLM\u5168\u5c40\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u4e2d\u56e0\u8bed\u4e49\u5f15\u5bfc\u4e0d\u8db3\u548c\u7a7a\u95f4\u8bb0\u5fc6\u6709\u9650\u5bfc\u81f4\u7684\u4f4e\u6548\u63a2\u7d22\u548c\u5c40\u90e8\u9677\u9631\u95ee\u9898\u3002", "method": "\u57fa\u4e8eTARE\u5206\u5c42\u63a2\u7d22\u6846\u67b6\uff0c\u96c6\u6210\u89c6\u70b9\u751f\u6210\u7b56\u7565\u548cLLM\u5168\u5c40\u5f15\u5bfc\u673a\u5236\uff0c\u4f18\u5148\u9ad8\u8bed\u4e49\u5bc6\u5ea6\u533a\u57df\u5e76\u8bc4\u4f30\u8bed\u4e49\u5173\u8054\u3002", "result": "\u5728Matterport3D\u548cHabitat-Matterport3D\u6570\u636e\u96c6\u4e0a\uff0cSR\u63d0\u9ad8\u4e8618.5%\u548c11.2%\uff0cSPL\u63d0\u5347\u4e860.181\u548c0.140\u3002", "conclusion": "SSR-ZSON\u901a\u8fc7\u521b\u65b0\u7684\u89c6\u70b9\u751f\u6210\u548c\u8bed\u4e49\u5173\u8054\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u6548\u5bfc\u822a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.24768", "pdf": "https://arxiv.org/pdf/2509.24768", "abs": "https://arxiv.org/abs/2509.24768", "authors": ["Eric Hannus", "Miika Malin", "Tran Nguyen Le", "Ville Kyrki"], "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks", "categories": ["cs.RO"], "comment": "Under review for ICRA 2026", "summary": "Vision-language-action models (VLAs) have become an increasingly popular\napproach for addressing robot manipulation problems in recent years. However,\nsuch models need to output actions at a rate suitable for robot control, which\nlimits the size of the language model they can be based on, and consequently,\ntheir language understanding capabilities. Manipulation tasks may require\ncomplex language instructions, such as identifying target objects by their\nrelative positions, to specify human intention. Therefore, we introduce IA-VLA,\na framework that utilizes the extensive language understanding of a large\nvision language model as a pre-processing stage to generate improved context to\naugment the input of a VLA. We evaluate the framework on a set of semantically\ncomplex tasks which have been underexplored in VLA literature, namely tasks\ninvolving visual duplicates, i.e., visually indistinguishable objects. A\ndataset of three types of scenes with duplicate objects is used to compare a\nbaseline VLA against two augmented variants. The experiments show that the VLA\nbenefits from the augmentation scheme, especially when faced with language\ninstructions that require the VLA to extrapolate from concepts it has seen in\nthe demonstrations. For the code, dataset, and videos, see\nhttps://sites.google.com/view/ia-vla.", "AI": {"tldr": "IA-VLA\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u589e\u5f3aVLAs\u5728\u5904\u7406\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u65f6\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u91cd\u590d\u5bf9\u8c61\u4efb\u52a1\u4e2d\u3002", "motivation": "\u73b0\u6709VLAs\u53d7\u9650\u4e8e\u5b9e\u65f6\u52a8\u4f5c\u8f93\u51fa\u7684\u9700\u6c42\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u5982\u76f8\u5bf9\u4f4d\u7f6e\u63cf\u8ff0\u7684\u6307\u4ee4\u3002", "method": "\u5f15\u5165IA-VLA\u6846\u67b6\uff0c\u501f\u52a9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\uff0c\u589e\u5f3aVLA\u7684\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIA-VLA\u5728\u5904\u7406\u89c6\u89c9\u91cd\u590d\u5bf9\u8c61\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4ece\u6f14\u793a\u4e2d\u63a8\u5bfc\u6982\u5ff5\u7684\u590d\u6742\u6307\u4ee4\u4e0a\u3002", "conclusion": "IA-VLA\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86VLAs\u7684\u590d\u6742\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.24797", "pdf": "https://arxiv.org/pdf/2509.24797", "abs": "https://arxiv.org/abs/2509.24797", "authors": ["Zizhao Tong", "Di Chen", "Sicheng Hu", "Hongwei Fan", "Liliang Chen", "Guanghui Ren", "Hao Tang", "Hao Dong", "Ling Shao"], "title": "Fidelity-Aware Data Composition for Robust Robot Generalization", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "33 pages", "summary": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CIFT\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u7ec4\u5408\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u4fe1\u606f\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u540c\u8d28\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5bb9\u6613\u9677\u5165\u6377\u5f84\u5b66\u4e60\uff0c\u5bfc\u81f4\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u867d\u5f15\u5165\u591a\u6837\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faCIFT\u6846\u67b6\uff0c\u57fa\u4e8e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u4ee3\u7406\u6307\u6807\u4f18\u5316\u6570\u636e\u7ec4\u5408\uff0c\u5f15\u5165Decoherence Point\u6982\u5ff5\u8bc6\u522b\u8bad\u7ec3\u7a33\u5b9a\u6027\u53d8\u5316\uff0c\u5e76\u63d0\u51faMVAug\u751f\u6210\u56e0\u679c\u89e3\u8026\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5e94\u7528CIFT\u540e\uff0c\u7b56\u7565\u67b6\u6784\uff08\u5982$\\pi_0$\u548cDiffusion Policy\uff09\u7684OOD\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc754%\u3002", "conclusion": "\u4fe1\u606f\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u6570\u636e\u7ec4\u5408\u662f\u5f00\u53d1\u9c81\u68d2\u901a\u7528\u673a\u5668\u4eba\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.24864", "pdf": "https://arxiv.org/pdf/2509.24864", "abs": "https://arxiv.org/abs/2509.24864", "authors": ["Mingxi Zhou", "Farhang Naderi", "Yuewei Fu", "Tony Jacob", "Lin Zhao", "Manavi Panjnani", "Chengzhi Yuan", "William McConnell", "Emir Cem Gezer"], "title": "Towards Modular and Accessible AUV Systems", "categories": ["cs.RO"], "comment": "6 pages, accepted by 2024 IEEE/OES Autonomous Underwater Vehicles\n  Symposium (AUV)", "summary": "This paper reports the development of a new open-access modular framework,\ncalled Marine Vehicle Packages (MVP), for Autonomous Underwater Vehicles. The\nframework consists of both software and hardware designs allowing easy\nconstruction of AUV for research with increased customizability and sufficient\npayload capacity. This paper will present the scalable hardware system design\nand the modular software design architecture. New features, such as articulated\nthruster integration and high-level Graphic User Interface will be discussed.\nBoth simulation and field experiments results are shown to highlight the\nperformance and compatibility of the MVP.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMarine Vehicle Packages (MVP)\u7684\u65b0\u578b\u5f00\u653e\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7814\u7a76\uff0c\u517c\u5177\u8f6f\u786c\u4ef6\u8bbe\u8ba1\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7814\u7a76\u63d0\u4f9b\u9ad8\u5ea6\u5b9a\u5236\u5316\u4e14\u6613\u4e8e\u6784\u5efa\u7684\u6a21\u5757\u5316\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u7cfb\u7edf\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u65b0\u529f\u80fd\u5982\u53ef\u8c03\u8282\u63a8\u8fdb\u5668\u548c\u9ad8\u5c42\u56fe\u5f62\u7528\u6237\u754c\u9762\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MVP\u7684\u6027\u80fd\u4e0e\u517c\u5bb9\u6027\u3002", "conclusion": "MVP\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7684\u5b9a\u5236\u5316\u80fd\u529b\u548c\u7814\u7a76\u6548\u7387\u3002"}}
{"id": "2509.24867", "pdf": "https://arxiv.org/pdf/2509.24867", "abs": "https://arxiv.org/abs/2509.24867", "authors": ["Mariadas Capsran Roshan", "Edgar M Hidalgo", "Mats Isaksson", "Michelle Dunn", "Jagannatha Charjee Pyaraka"], "title": "Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Echocardiography is a key imaging modality for cardiac assessment but remains\nhighly operator-dependent, and access to trained sonographers is limited in\nunderserved settings. Teleoperated robotic echocardiography has been proposed\nas a solution; however, clinical studies report longer examination times than\nmanual procedures, increasing diagnostic delays and operator workload.\nAutomating non-expert tasks, such as automatically moving the probe to an ideal\nstarting pose, offers a pathway to reduce this burden. Prior vision- and\ndepth-based approaches to estimate an initial probe pose are sensitive to\nlighting, texture, and anatomical variability. We propose a robot-mounted 2D\nLiDAR-based approach that reconstructs the chest surface in 3D and estimates\nthe initial probe pose automatically. To the best of our knowledge, this is the\nfirst demonstration of robot-mounted 2D LiDAR used for 3D reconstruction of a\nhuman body surface. Through plane-based extrinsic calibration, the\ntransformation between the LiDAR and robot base frames was estimated with an\noverall root mean square (RMS) residual of 1.8 mm and rotational uncertainty\nbelow 0.2{\\deg}. The chest front surface, reconstructed from two linear LiDAR\nsweeps, was aligned with non-rigid templates to identify an initial probe pose.\nA mannequin-based study assessing reconstruction accuracy showed mean surface\nerrors of 2.78 +/- 0.21 mm. Human trials (N=5) evaluating the proposed approach\nfound probe initial points typically 20-30 mm from the clinically defined\ninitial point, while the variation across repeated trials on the same subject\nwas less than 4 mm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u642d\u8f7d\u76842D LiDAR\u6280\u672f\uff0c\u7528\u4e8e\u81ea\u52a8\u4f30\u8ba1\u5fc3\u810f\u8d85\u58f0\u63a2\u5934\u521d\u59cb\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8d85\u58f0\u68c0\u67e5\u4f9d\u8d56\u64cd\u4f5c\u8005\u7684\u95ee\u9898\u3002", "motivation": "\u5fc3\u810f\u8d85\u58f0\u68c0\u67e5\u64cd\u4f5c\u4f9d\u8d56\u6027\u5f3a\u4e14\u8bad\u7ec3\u6709\u7d20\u7684\u6280\u5e08\u8d44\u6e90\u6709\u9650\uff0c\u5c24\u5176\u5728\u504f\u8fdc\u5730\u533a\u3002\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u65b9\u6cd5\u8017\u65f6\u589e\u52a0\uff0c\u81ea\u52a8\u5316\u975e\u4e13\u5bb6\u4efb\u52a1\u6709\u671b\u51cf\u8f7b\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u673a\u5668\u4eba\u642d\u8f7d\u76842D LiDAR\u91cd\u5efa\u80f8\u90e83D\u8868\u9762\uff0c\u901a\u8fc7\u6821\u51c6\u548c\u6a21\u677f\u5bf9\u9f50\u81ea\u52a8\u4f30\u8ba1\u63a2\u5934\u521d\u59cb\u4f4d\u7f6e\u3002", "result": "\u6821\u51c6\u7cbe\u5ea6RMS\u4e3a1.8 mm\uff0c\u65cb\u8f6c\u8bef\u5dee\u4f4e\u4e8e0.2\u00b0\u3002\u4eba\u4f53\u8bd5\u9a8c\u663e\u793a\u63a2\u5934\u521d\u59cb\u70b9\u8bef\u5dee20-30 mm\uff0c\u91cd\u590d\u8bd5\u9a8c\u53d8\u5f02\u5c0f\u4e8e4 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u5fc3\u810f\u8d85\u58f0\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u7cbe\u5ea6\u3002"}}
{"id": "2509.24892", "pdf": "https://arxiv.org/pdf/2509.24892", "abs": "https://arxiv.org/abs/2509.24892", "authors": ["Shilong Ji", "Yinuo Chen", "Chuqi Wang", "Jiayu Chen", "Ruize Zhang", "Feng Gao", "Wenhao Tang", "Shu'ang Yu", "Sirui Xiang", "Xinlei Chen", "Chao Yu", "Yu Wang"], "title": "JuggleRL: Mastering Ball Juggling with a Quadrotor via Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Aerial robots interacting with objects must perform precise, contact-rich\nmaneuvers under uncertainty. In this paper, we study the problem of aerial ball\njuggling using a quadrotor equipped with a racket, a task that demands accurate\ntiming, stable control, and continuous adaptation. We propose JuggleRL, the\nfirst reinforcement learning-based system for aerial juggling. It learns\nclosed-loop policies in large-scale simulation using systematic calibration of\nquadrotor and ball dynamics to reduce the sim-to-real gap. The training\nincorporates reward shaping to encourage racket-centered hits and sustained\njuggling, as well as domain randomization over ball position and coefficient of\nrestitution to enhance robustness and transferability. The learned policy\noutputs mid-level commands executed by a low-level controller and is deployed\nzero-shot on real hardware, where an enhanced perception module with a\nlightweight communication protocol reduces delays in high-frequency state\nestimation and ensures real-time control. Experiments show that JuggleRL\nachieves an average of $311$ hits over $10$ consecutive trials in the real\nworld, with a maximum of $462$ hits observed, far exceeding a model-based\nbaseline that reaches at most $14$ hits with an average of $3.1$. Moreover, the\npolicy generalizes to unseen conditions, successfully juggling a lighter $5$ g\nball with an average of $145.9$ hits. This work demonstrates that reinforcement\nlearning can empower aerial robots with robust and stable control in dynamic\ninteraction tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJuggleRL\uff0c\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7a7a\u4e2d\u6742\u6280\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u95ed\u73af\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u7a7a\u4e2d\u7403\u6742\u6280\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e0\u4eba\u673a\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u6267\u884c\u7cbe\u786e\u4e14\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5982\u7a7a\u4e2d\u7403\u6742\u6280\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u548c\u9886\u57df\u968f\u673a\u5316\u4ee5\u51cf\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "result": "JuggleRL\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5e73\u5747\u5b8c\u6210311\u6b21\u51fb\u7403\uff0c\u6700\u9ad8\u8fbe462\u6b21\uff0c\u8fdc\u8d85\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u52a9\u529b\u65e0\u4eba\u673a\u5728\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u63a7\u5236\u3002"}}
{"id": "2509.24903", "pdf": "https://arxiv.org/pdf/2509.24903", "abs": "https://arxiv.org/abs/2509.24903", "authors": ["Lantao Li", "Kang Yang", "Rui Song", "Chen Sun"], "title": "DRCP: Diffusion on Reinforced Cooperative Perception for Perceiving Beyond Limits", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": null, "summary": "Cooperative perception enabled by Vehicle-to-Everything communication has\nshown great promise in enhancing situational awareness for autonomous vehicles\nand other mobile robotic platforms. Despite recent advances in perception\nbackbones and multi-agent fusion, real-world deployments remain challenged by\nhard detection cases, exemplified by partial detections and noise accumulation\nwhich limit downstream detection accuracy. This work presents Diffusion on\nReinforced Cooperative Perception (DRCP), a real-time deployable framework\ndesigned to address aforementioned issues in dynamic driving environments. DRCP\nintegrates two key components: (1) Precise-Pyramid-Cross-Modality-Cross-Agent,\na cross-modal cooperative perception module that leverages\ncamera-intrinsic-aware angular partitioning for attention-based fusion and\nadaptive convolution to better exploit external features; and (2)\nMask-Diffusion-Mask-Aggregation, a novel lightweight diffusion-based refinement\nmodule that encourages robustness against feature perturbations and aligns\nbird's-eye-view features closer to the task-optimal manifold. The proposed\nsystem achieves real-time performance on mobile platforms while significantly\nimproving robustness under challenging conditions. Code will be released in\nlate 2025.", "AI": {"tldr": "DRCP\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u534f\u4f5c\u611f\u77e5\u548c\u8f7b\u91cf\u7ea7\u6269\u6563\u4f18\u5316\u6a21\u5757\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u611f\u77e5\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56e0\u90e8\u5206\u68c0\u6d4b\u548c\u566a\u58f0\u7d2f\u79ef\u5bfc\u81f4\u7684\u4e0b\u6e38\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u878d\u5408Precise-Pyramid-Cross-Modality-Cross-Agent\u6a21\u5757\u548cMask-Diffusion-Mask-Aggregation\u6a21\u5757\uff0c\u4f18\u5316\u7279\u5f81\u878d\u5408\u4e0e\u6269\u6563\u3002", "result": "\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u5e76\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "DRCP\u4e3a\u52a8\u6001\u9a7e\u9a76\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u534f\u4f5c\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24907", "pdf": "https://arxiv.org/pdf/2509.24907", "abs": "https://arxiv.org/abs/2509.24907", "authors": ["Thanh Long Nguyen", "Duc Phu Nguyen", "Thanh Thao Ton Nu", "Quan Le", "Thuan Hoang Tran", "Manh Duong Phung"], "title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "{Recognizing human interactions is essential for social robots as it enables\nthem to navigate safely and naturally in shared environments. Conventional\nrobotic systems however often focus on obstacle avoidance, neglecting social\ncues necessary for seamless human-robot interaction. To address this gap, we\npropose a framework to recognize human group interactions for socially aware\nnavigation. Our method utilizes color and depth frames from a monocular RGB-D\ncamera to estimate 3D human keypoints and positions. Principal component\nanalysis (PCA) is then used to determine dominant interaction directions. The\nshoelace formula is finally applied to compute interest points and engagement\nareas. Extensive experiments have been conducted to evaluate the validity of\nthe proposed method. The results show that our method is capable of recognizing\ngroup interactions across different scenarios with varying numbers of\nindividuals. It also achieves high-speed performance, processing each frame in\napproximately 4 ms on a single-board computer used in robotic systems. The\nmethod is implemented as a ROS 2 package making it simple to integrate into\nexisting navigation systems. Source code is available at\nhttps://github.com/thanhlong103/social-interaction-detector", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRGB-D\u76f8\u673a\u548cPCA\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u4eba\u7fa4\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u793e\u4ea4\u673a\u5668\u4eba\u7684\u81ea\u7136\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5e38\u5ffd\u89c6\u793e\u4ea4\u7ebf\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u6d41\u7545\u6027\u3002", "method": "\u5229\u7528RGB-D\u76f8\u673a\u83b7\u53d63D\u4eba\u4f53\u5173\u952e\u70b9\u548c\u4f4d\u7f6e\uff0c\u901a\u8fc7PCA\u786e\u5b9a\u4ea4\u4e92\u65b9\u5411\uff0c\u5e76\u7ed3\u5408shoelace\u516c\u5f0f\u8ba1\u7b97\u5174\u8da3\u70b9\u548c\u4e92\u52a8\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7fa4\u4f53\u4ea4\u4e92\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u5feb\uff084 ms/\u5e27\uff09\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7fa4\u4f53\u4ea4\u4e92\u8bc6\u522b\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.24917", "pdf": "https://arxiv.org/pdf/2509.24917", "abs": "https://arxiv.org/abs/2509.24917", "authors": ["Markus Peschl", "Pietro Mazzaglia", "Daniel Dijkman"], "title": "From Code to Action: Hierarchical Learning of Diffusion-VLM Policies", "categories": ["cs.RO", "cs.LG"], "comment": "19 pages including references, 6 figures. Accepted to CoRL LEAP 2025", "summary": "Imitation learning for robotic manipulation often suffers from limited\ngeneralization and data scarcity, especially in complex, long-horizon tasks. In\nthis work, we introduce a hierarchical framework that leverages code-generating\nvision-language models (VLMs) in combination with low-level diffusion policies\nto effectively imitate and generalize robotic behavior. Our key insight is to\ntreat open-source robotic APIs not only as execution interfaces but also as\nsources of structured supervision: the associated subtask functions - when\nexposed - can serve as modular, semantically meaningful labels. We train a VLM\nto decompose task descriptions into executable subroutines, which are then\ngrounded through a diffusion policy trained to imitate the corresponding robot\nbehavior. To handle the non-Markovian nature of both code execution and certain\nreal-world tasks, such as object swapping, our architecture incorporates a\nmemory mechanism that maintains subtask context across time. We find that this\ndesign enables interpretable policy decomposition, improves generalization when\ncompared to flat policies and enables separate evaluation of high-level\nplanning and low-level control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7801\u751f\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u4f4e\u7ea7\u522b\u6269\u6563\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u5728\u590d\u6742\u3001\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528VLM\u5206\u89e3\u4efb\u52a1\u63cf\u8ff0\u4e3a\u53ef\u6267\u884c\u5b50\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u7b56\u7565\u6a21\u4eff\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u540c\u65f6\u5f15\u5165\u8bb0\u5fc6\u673a\u5236\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u5206\u89e3\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u652f\u6301\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u7684\u5355\u72ec\u8bc4\u4f30\u3002", "conclusion": "\u5206\u5c42\u6846\u67b6\u548c\u7ed3\u6784\u5316\u76d1\u7763\u662f\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.24921", "pdf": "https://arxiv.org/pdf/2509.24921", "abs": "https://arxiv.org/abs/2509.24921", "authors": ["Pablo Pueyo", "Fernando Caballero", "Ana Cristina Murillo", "Eduardo Montijano"], "title": "CineWild: Balancing Art and Robotics for Ethical Wildlife Documentary Filmmaking", "categories": ["cs.RO", "cs.MM"], "comment": null, "summary": "Drones, or unmanned aerial vehicles (UAVs), have become powerful tools across\ndomains-from industry to the arts. In documentary filmmaking, they offer\ndynamic, otherwise unreachable perspectives, transforming how stories are told.\nWildlife documentaries especially benefit, yet drones also raise ethical\nconcerns: the risk of disturbing the animals they aim to capture. This paper\nintroduces CineWild, an autonomous UAV framework that combines robotics,\ncinematography, and ethics. Built on model predictive control, CineWild\ndynamically adjusts flight paths and camera settings to balance cinematic\nquality with animal welfare. Key features include adaptive zoom for filming\nfrom acoustic and visual safe distances, path-planning that avoids an animal's\nfield of view, and smooth, low-noise maneuvers. CineWild exemplifies\ninterdisciplinary innovation-bridging engineering, visual storytelling, and\nenvironmental ethics. We validate the system through simulation studies and\nwill release the code upon acceptance.", "AI": {"tldr": "CineWild\u662f\u4e00\u4e2a\u7ed3\u5408\u673a\u5668\u4eba\u3001\u6444\u5f71\u548c\u4f26\u7406\u5b66\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u98de\u884c\u8def\u5f84\u548c\u76f8\u673a\u8bbe\u7f6e\u6765\u5e73\u8861\u7535\u5f71\u8d28\u91cf\u4e0e\u52a8\u7269\u798f\u5229\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u7eaa\u5f55\u7247\u5236\u4f5c\u4e2d\u63d0\u4f9b\u4e86\u72ec\u7279\u89c6\u89d2\uff0c\u4f46\u4e5f\u53ef\u80fd\u5e72\u6270\u91ce\u751f\u52a8\u7269\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4f26\u7406\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0cCineWild\u91c7\u7528\u81ea\u9002\u5e94\u53d8\u7126\u3001\u8def\u5f84\u89c4\u5212\u548c\u4f4e\u566a\u97f3\u673a\u52a8\u6765\u51cf\u5c11\u5bf9\u52a8\u7269\u7684\u5e72\u6270\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002", "conclusion": "CineWild\u5c55\u793a\u4e86\u5de5\u7a0b\u6280\u672f\u3001\u89c6\u89c9\u53d9\u4e8b\u548c\u73af\u5883\u4f26\u7406\u7684\u8de8\u5b66\u79d1\u521b\u65b0\u3002"}}
{"id": "2509.24928", "pdf": "https://arxiv.org/pdf/2509.24928", "abs": "https://arxiv.org/abs/2509.24928", "authors": ["Shunan Yin", "Zehui Lu", "Shaoshuai Mou"], "title": "Trajectory Prediction via Bayesian Intention Inference under Unknown Goals and Kinematics", "categories": ["cs.RO"], "comment": null, "summary": "This work introduces an adaptive Bayesian algorithm for real-time trajectory\nprediction via intention inference, where a target's intentions and motion\ncharacteristics are unknown and subject to change. The method concurrently\nestimates two critical variables: the target's current intention, modeled as a\nMarkovian latent state, and an intention parameter that describes the target's\nadherence to a shortest-path policy. By integrating this joint update\ntechnique, the algorithm maintains robustness against abrupt intention shifts\nand unknown motion dynamics. A sampling-based trajectory prediction mechanism\nthen exploits these adaptive estimates to generate probabilistic forecasts with\nquantified uncertainty. We validate the framework through numerical\nexperiments: Ablation studies of two cases, and a 500-trial Monte Carlo\nanalysis; Hardware demonstrations on quadrotor and quadrupedal platforms.\nExperimental results demonstrate that the proposed approach significantly\noutperforms non-adaptive and partially adaptive methods. The method operates in\nreal time around 270 Hz without requiring training or detailed prior knowledge\nof target behavior, showcasing its applicability in various robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8d1d\u53f6\u65af\u7b97\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u610f\u56fe\u63a8\u65ad\u5b9e\u65f6\u9884\u6d4b\u76ee\u6807\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u610f\u56fe\u548c\u8fd0\u52a8\u7279\u6027\u672a\u77e5\u4e14\u53ef\u80fd\u53d8\u5316\u7684\u60c5\u51b5\u3002", "motivation": "\u89e3\u51b3\u76ee\u6807\u610f\u56fe\u548c\u8fd0\u52a8\u7279\u6027\u672a\u77e5\u4e14\u53ef\u80fd\u53d8\u5316\u65f6\uff0c\u5b9e\u65f6\u9884\u6d4b\u8f68\u8ff9\u7684\u6311\u6218\u3002", "method": "\u7b97\u6cd5\u540c\u65f6\u4f30\u8ba1\u76ee\u6807\u5f53\u524d\u610f\u56fe\uff08\u9a6c\u5c14\u53ef\u592b\u6f5c\u5728\u72b6\u6001\uff09\u548c\u610f\u56fe\u53c2\u6570\uff08\u63cf\u8ff0\u76ee\u6807\u9075\u5faa\u6700\u77ed\u8def\u5f84\u7b56\u7565\u7684\u7a0b\u5ea6\uff09\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u673a\u5236\u751f\u6210\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u975e\u81ea\u9002\u5e94\u548c\u90e8\u5206\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5b9e\u65f6\u8fd0\u884c\u9891\u7387\u8fbe270 Hz\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u65e0\u9700\u8bad\u7ec3\u6216\u8be6\u7ec6\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2509.24948", "pdf": "https://arxiv.org/pdf/2509.24948", "abs": "https://arxiv.org/abs/2509.24948", "authors": ["Junjin Xiao", "Yandan Yang", "Xinyuan Chang", "Ronghan Chen", "Feng Xiong", "Mu Xu", "Wei-Shi Zheng", "Qing Zhang"], "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models trained via imitation learning suffer\nfrom significant performance degradation in data-scarce scenarios due to their\nreliance on large-scale demonstration datasets. Although reinforcement learning\n(RL)-based post-training has proven effective in addressing data scarcity, its\napplication to VLA models is hindered by the non-resettable nature of\nreal-world environments. This limitation is particularly critical in high-risk\ndomains such as industrial automation, where interactions often induce state\nchanges that are costly or infeasible to revert. Furthermore, existing VLA\napproaches lack a reliable mechanism for detecting task completion, leading to\nredundant actions that reduce overall task success rates. To address these\nchallenges, we propose World-Env, an RL-based post-training framework that\nreplaces physical interaction with a low-cost, world model-based virtual\nsimulator. World-Env consists of two key components: (1) a video-based world\nsimulator that generates temporally consistent future visual observations, and\n(2) a vision-language model (VLM)-guided instant reflector that provides\ncontinuous reward signals and predicts action termination. This simulated\nenvironment enables VLA models to safely explore and generalize beyond their\ninitial imitation learning distribution. Our method achieves notable\nperformance gains with as few as five expert demonstrations per task.\nExperiments on complex robotic manipulation tasks demonstrate that World-Env\neffectively overcomes the data inefficiency, safety constraints, and\ninefficient execution of conventional VLA models that rely on real-world\ninteraction, offering a practical and scalable solution for post-training in\nresource-constrained settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWorld-Env\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u4eff\u771f\u6539\u8fdbVLA\u6a21\u578b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "VLA\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u4e14\u96be\u4ee5\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u91cd\u7f6e\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u5de5\u4e1a\u81ea\u52a8\u5316\uff09\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "World-Env\u6846\u67b6\u5305\u542b\u89c6\u9891\u4eff\u771f\u5668\u548cVLM\u5f15\u5bfc\u7684\u53cd\u5c04\u5668\uff0c\u901a\u8fc7\u865a\u62df\u4eff\u771f\u66ff\u4ee3\u771f\u5b9e\u4ea4\u4e92\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u540e\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e94\u4e2a\u4e13\u5bb6\u6f14\u793a\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6709\u6548\u514b\u670d\u6570\u636e\u4f4e\u6548\u548c\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "World-Env\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684VLA\u6a21\u578b\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24956", "pdf": "https://arxiv.org/pdf/2509.24956", "abs": "https://arxiv.org/abs/2509.24956", "authors": ["Jan Ole von Hartz", "Lukas Schweizer", "Joschka Boedecker", "Abhinav Valada"], "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative robot policies such as Flow Matching offer flexible, multi-modal\npolicy learning but are sample-inefficient. Although object-centric policies\nimprove sample efficiency, it does not resolve this limitation. In this work,\nwe propose Multi-Stream Generative Policy (MSG), an inference-time composition\nframework that trains multiple object-centric policies and combines them at\ninference to improve generalization and sample efficiency. MSG is\nmodel-agnostic and inference-only, hence widely applicable to various\ngenerative policies and training paradigms. We perform extensive experiments\nboth in simulation and on a real robot, demonstrating that our approach learns\nhigh-quality generative policies from as few as five demonstrations, resulting\nin a 95% reduction in demonstrations, and improves policy performance by 89\npercent compared to single-stream approaches. Furthermore, we present\ncomprehensive ablation studies on various composition strategies and provide\npractical recommendations for deployment. Finally, MSG enables zero-shot object\ninstance transfer. We make our code publicly available at\nhttps://msg.cs.uni-freiburg.de.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6d41\u751f\u6210\u7b56\u7565\uff08MSG\uff09\uff0c\u901a\u8fc7\u63a8\u65ad\u65f6\u7ec4\u5408\u591a\u4e2a\u5bf9\u8c61\u4e2d\u5fc3\u7b56\u7565\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u6f14\u793a\u9700\u6c42\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\uff08\u5982Flow Matching\uff09\u6837\u672c\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u7b56\u7565\u672a\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMSG\u6846\u67b6\uff0c\u8bad\u7ec3\u591a\u4e2a\u5bf9\u8c61\u4e2d\u5fc3\u7b56\u7565\u5e76\u5728\u63a8\u65ad\u65f6\u7ec4\u5408\uff0c\u6a21\u578b\u65e0\u5173\u4e14\u4ec5\u9700\u63a8\u65ad\u3002", "result": "\u4ec5\u97005\u6b21\u6f14\u793a\u5373\u53ef\u5b66\u4e60\u9ad8\u8d28\u91cf\u7b56\u7565\uff0c\u51cf\u5c1195%\u6f14\u793a\u9700\u6c42\uff0c\u6027\u80fd\u63d0\u534789%\u3002", "conclusion": "MSG\u6709\u6548\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u96f6\u6837\u672c\u5bf9\u8c61\u5b9e\u4f8b\u8fc1\u79fb\u3002"}}
{"id": "2509.24972", "pdf": "https://arxiv.org/pdf/2509.24972", "abs": "https://arxiv.org/abs/2509.24972", "authors": ["Vijja Wichitwechkarn", "Emlyn Williams", "Charles Fox", "Ruchi Choudhary"], "title": "Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in one-shot imitation learning have enabled robots to acquire\nnew manipulation skills from a single human demonstration. While existing\nmethods achieve strong performance on single-step tasks, they remain limited in\ntheir ability to handle long-horizon, multi-step tasks without additional model\ntraining or manual annotation. We propose a method that can be applied to this\nsetting provided a single demonstration without additional model training or\nmanual annotation. We evaluated our method on multi-step and single-step\nmanipulation tasks where our method achieves an average success rate of 82.5%\nand 90%, respectively. Our method matches and exceeds the performance of the\nbaselines in both these cases. We also compare the performance and\ncomputational efficiency of alternative pre-trained feature extractors within\nour framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6807\u6ce8\u7684\u5355\u6b21\u793a\u8303\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6b65\u548c\u5355\u6b65\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u6b65\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u4ecd\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002", "method": "\u57fa\u4e8e\u5355\u6b21\u793a\u8303\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u6216\u624b\u52a8\u6807\u6ce8\u3002", "result": "\u591a\u6b65\u4efb\u52a1\u5e73\u5747\u6210\u529f\u738782.5%\uff0c\u5355\u6b65\u4efb\u52a190%\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6b65\u548c\u5355\u6b65\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2509.24995", "pdf": "https://arxiv.org/pdf/2509.24995", "abs": "https://arxiv.org/abs/2509.24995", "authors": ["Da Saem Lee", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "Path Diffuser: Diffusion Model for Data-Driven Traffic Simulator", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Simulating diverse and realistic traffic scenarios is critical for developing\nand testing autonomous planning. Traditional rule-based planners lack diversity\nand realism, while learning-based simulators often replay, forecast, or edit\nscenarios using historical agent trajectories. However, they struggle to\ngenerate new scenarios, limiting scalability and diversity due to their\nreliance on fully annotated logs and historical data. Thus, a key challenge for\na learning-based simulator's performance is that it requires agents' past\ntrajectories and pose information in addition to map data, which might not be\navailable for all agents on the road.Without which, generated scenarios often\nproduce unrealistic trajectories that deviate from drivable areas, particularly\nunder out-of-distribution (OOD) map scenes (e.g., curved roads). To address\nthis, we propose Path Diffuser (PD): a two-stage, diffusion model for\ngenerating agent pose initializations and their corresponding trajectories\nconditioned on the map, free of any historical context of agents' trajectories.\nFurthermore, PD incorporates a motion primitive-based prior, leveraging Frenet\nframe candidate trajectories to enhance diversity while ensuring road-compliant\ntrajectory generation. We also explore various design choices for modeling\ncomplex multi-agent interactions. We demonstrate the effectiveness of our\nmethod through extensive experiments on the Argoverse2 Dataset and additionally\nevaluate the generalizability of the approach on OOD map variants. Notably,\nPath Diffuser outperforms the baseline methods by 1.92x on distribution\nmetrics, 1.14x on common-sense metrics, and 1.62x on road compliance from\nadversarial benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Path Diffuser (PD) \u7684\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u57fa\u4e8e\u5730\u56fe\u7684\u8f66\u8f86\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u89c4\u5212\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u62df\u5668\u5728\u751f\u6210\u591a\u6837\u5316\u4e14\u771f\u5b9e\u7684\u4ea4\u901a\u573a\u666f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5386\u53f2\u8f68\u8ff9\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "PD \u91c7\u7528\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u52a8\u57fa\u5143\u5148\u9a8c\u548c Frenet \u5e27\u5019\u9009\u8f68\u8ff9\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f68\u8ff9\u7b26\u5408\u9053\u8def\u89c4\u8303\u3002", "result": "\u5728 Argoverse2 \u6570\u636e\u96c6\u4e0a\uff0cPD \u5728\u5206\u5e03\u6307\u6807\u3001\u5e38\u8bc6\u6307\u6807\u548c\u9053\u8def\u5408\u89c4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Path Diffuser \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u4ea4\u901a\u573a\u666f\u3002"}}
{"id": "2509.25032", "pdf": "https://arxiv.org/pdf/2509.25032", "abs": "https://arxiv.org/abs/2509.25032", "authors": ["Ryosuke Takanami", "Petr Khrapchenkov", "Shu Morikuni", "Jumpei Arima", "Yuta Takaba", "Shunsuke Maeda", "Takuya Okubo", "Genki Sano", "Satoshi Sekioka", "Aoi Kadoya", "Motonari Kambara", "Naoya Nishiura", "Haruto Suzuki", "Takanori Yoshimoto", "Koya Sakamoto", "Shinnosuke Ono", "Hu Yang", "Daichi Yashima", "Aoi Horo", "Tomohiro Motoda", "Kensuke Chiyoma", "Hiroshi Ito", "Koki Fukuda", "Akihito Goto", "Kazumi Morinaga", "Yuya Ikeda", "Riko Kawada", "Masaki Yoshikawa", "Norio Kosuge", "Yuki Noguchi", "Kei Ota", "Tatsuya Matsushima", "Yusuke Iwasawa", "Yutaka Matsuo", "Tetsuya Ogata"], "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "As robots transition from controlled settings to unstructured human\nenvironments, building generalist agents that can reliably follow natural\nlanguage instructions remains a central challenge. Progress in robust mobile\nmanipulation requires large-scale multimodal datasets that capture contact-rich\nand long-horizon tasks, yet existing resources lack synchronized force-torque\nsensing, hierarchical annotations, and explicit failure cases. We address this\ngap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset\nfor mobile manipulation. It includes synchronized RGB images, joint states,\nsix-axis wrist force-torque signals, and internal robot states, together with a\nnovel two-layer annotation schema of sub-goals and primitive actions for\nhierarchical learning and error analysis. The initial dataset comprises 25,469\nepisodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is\nfully standardized in the LeRobot v2.1 format. By uniquely integrating mobile\nmanipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa\nprovides a critical benchmark for advancing the next generation of\nVision-Language-Action models. The first version of our dataset is now\navailable at https://huggingface.co/datasets/airoa-org/airoa-moma .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AIRoA MoMa\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u79fb\u52a8\u64cd\u4f5c\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u548c\u5206\u5c42\u6ce8\u91ca\uff0c\u7528\u4e8e\u652f\u6301\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8fdb\u9636\u7814\u7a76\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u6765\u652f\u6301\u9c81\u68d2\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u540c\u6b65\u7684RGB\u56fe\u50cf\u3001\u5173\u8282\u72b6\u6001\u3001\u516d\u8f74\u624b\u8155\u529b\u626d\u77e9\u4fe1\u53f7\u548c\u673a\u5668\u4eba\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u5206\u5c42\u6ce8\u91ca\uff08\u5b50\u76ee\u6807\u548c\u539f\u59cb\u52a8\u4f5c\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b25,469\u4e2a\u6837\u672c\uff08\u7ea694\u5c0f\u65f6\u6570\u636e\uff09\uff0c\u6807\u51c6\u5316\u4e3aLeRobot v2.1\u683c\u5f0f\uff0c\u652f\u6301\u5206\u5c42\u5b66\u4e60\u548c\u9519\u8bef\u5206\u6790\u3002", "conclusion": "AIRoA MoMa\u6570\u636e\u96c6\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u79fb\u52a8\u64cd\u4f5c\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.25056", "pdf": "https://arxiv.org/pdf/2509.25056", "abs": "https://arxiv.org/abs/2509.25056", "authors": ["Kenny Truong", "Yongkyu Lee", "Jason Irie", "Shivam Kumar Panda", "Shahab Ahmad", "Md. Mukhlesur Rahman", "M. Khalid Jawed"], "title": "AgriCruiser: An Open Source Agriculture Robot for Over-the-row Navigation", "categories": ["cs.RO"], "comment": "GitHub: https://github.com/structuresComp/agri-cruiser", "summary": "We present the AgriCruiser, an open-source over-the-row agricultural robot\ndeveloped for low-cost deployment and rapid adaptation across diverse crops and\nrow layouts. The chassis provides an adjustable track width of 1.42 m to 1.57\nm, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact\npivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland\nmaneuvers. The platform is designed for the integration of the other\nsubsystems, and in this study, a precision spraying system was implemented to\nassess its effectiveness in weed management. In twelve flax plots, a single\nrobotic spray pass reduced total weed populations (pigweed and Venice mallow)\nby 24- to 42-fold compared to manual weeding in four flax plots, while also\ncausing less crop damage. Mobility experiments conducted on concrete, asphalt,\ngravel, grass, and both wet and dry soil confirmed reliable traversal\nconsistent with torque sizing. The complete chassis can be constructed from\ncommodity T-slot extrusion with minimal machining, resulting in a bill of\nmaterials costing approximately $5,000 - $6,000, which enables replication and\ncustomization. The mentioned results demonstrate that low-cost, reconfigurable\nover-the-row robots can achieve effective weed management with reduced crop\ndamage and labor requirements, while providing a versatile foundation for\nphenotyping, sensing, and other agriculture applications. Design files and\nimplementation details are released to accelerate research and adoption of\nmodular agricultural robotics.", "AI": {"tldr": "AgriCruiser\u662f\u4e00\u6b3e\u4f4e\u6210\u672c\u3001\u53ef\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u4f5c\u7269\u548c\u884c\u5e03\u5c40\u7684\u5f00\u6e90\u519c\u4e1a\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7cbe\u51c6\u55b7\u6d12\u5b9e\u73b0\u9ad8\u6548\u9664\u8349\uff0c\u51cf\u5c11\u4f5c\u7269\u635f\u4f24\uff0c\u5e76\u63d0\u4f9b\u591a\u529f\u80fd\u519c\u4e1a\u5e94\u7528\u57fa\u7840\u3002", "motivation": "\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u53ef\u91cd\u6784\u7684\u519c\u4e1a\u673a\u5668\u4eba\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6742\u8349\u7ba1\u7406\u5e76\u51cf\u5c11\u4f5c\u7269\u635f\u4f24\uff0c\u540c\u65f6\u4e3a\u5176\u4ed6\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u591a\u529f\u80fd\u5e73\u53f0\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u8c03\u8282\u8f68\u9053\u5bbd\u5ea6\u548c\u5730\u9762\u9ad8\u5ea6\u7684\u673a\u5668\u4eba\u5e95\u76d8\uff0c\u914d\u5907\u7cbe\u51c6\u55b7\u6d12\u7cfb\u7edf\uff0c\u5e76\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u6d4b\u8bd5\u5176\u79fb\u52a8\u6027\u80fd\u3002", "result": "\u5355\u6b21\u673a\u5668\u4eba\u55b7\u6d12\u572812\u5757\u4e9a\u9ebb\u7530\u4e2d\u6742\u8349\u6570\u91cf\u51cf\u5c1124\u81f342\u500d\uff0c\u4f5c\u7269\u635f\u4f24\u66f4\u5c11\uff0c\u5e95\u76d8\u6210\u672c\u7ea6\u4e3a5000\u81f36000\u7f8e\u5143\u3002", "conclusion": "AgriCruiser\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u53ef\u91cd\u6784\u673a\u5668\u4eba\u5728\u9ad8\u6548\u9664\u8349\u548c\u591a\u529f\u80fd\u519c\u4e1a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u8bbe\u8ba1\u4ee5\u4fc3\u8fdb\u7814\u7a76\u548c\u91c7\u7528\u3002"}}
{"id": "2509.25091", "pdf": "https://arxiv.org/pdf/2509.25091", "abs": "https://arxiv.org/abs/2509.25091", "authors": ["Lakshan Lavan", "Lanojithan Thiyagarasa", "Udara Muthugala", "Rajitha de Silva"], "title": "Crop Spirals: Re-thinking the field layout for future robotic agriculture", "categories": ["cs.RO"], "comment": "Submitted to Computers and Electronics in Agriculture", "summary": "Conventional linear crop layouts, optimised for tractors, hinder robotic\nnavigation with tight turns, long travel distances, and perceptual aliasing. We\npropose a robot-centric square spiral layout with a central tramline, enabling\nsimpler motion and more efficient coverage. To exploit this geometry, we\ndevelop a navigation stack combining DH-ResNet18 waypoint regression,\npixel-to-odometry mapping, A* planning, and model predictive control (MPC). In\nsimulations, the spiral layout yields up to 28% shorter paths and about 25%\nfaster execution for waypoint-based tasks across 500 waypoints than linear\nlayouts, while full-field coverage performance is comparable to an optimised\nlinear U-turn strategy. Multi-robot studies demonstrate efficient coordination\non the spirals rule-constrained graph, with a greedy allocator achieving 33-37%\nlower batch completion times than a Hungarian assignment under our setup. These\nresults highlight the potential of redesigning field geometry to better suit\nautonomous agriculture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u673a\u5668\u4eba\u7684\u65b9\u5f62\u87ba\u65cb\u4f5c\u7269\u5e03\u5c40\uff08\u66ff\u4ee3\u4f20\u7edf\u7684\u7ebf\u6027\u5e03\u5c40\uff09\uff0c\u7ed3\u5408\u65b0\u7684\u5bfc\u822a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u4f5c\u7269\u5e03\u5c40\u4e13\u4e3a\u62d6\u62c9\u673a\u8bbe\u8ba1\uff0c\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u4e0d\u5229\uff08\u5982\u8f6c\u5f2f\u56f0\u96be\u3001\u8ddd\u79bb\u957f\u3001\u611f\u77e5\u6df7\u6dc6\uff09\u3002", "method": "\u8bbe\u8ba1\u4e86\u673a\u5668\u4eba\u4e2d\u5fc3\u7684\u65b9\u5f62\u87ba\u65cb\u5e03\u5c40\u53ca\u914d\u5957\u5bfc\u822a\u6280\u672f\uff08DH-ResNet18\u3001A*\u89c4\u5212\u3001MPC\u7b49\uff09\u3002", "result": "\u6a21\u62df\u663e\u793a\uff0c\u87ba\u65cb\u5e03\u5c40\u8def\u5f84\u7f29\u77ed28%\u3001\u6267\u884c\u63d0\u901f25%\uff0c\u591a\u673a\u5668\u4eba\u534f\u8c03\u6548\u7387\u63d0\u534733-37%\u3002", "conclusion": "\u91cd\u65b0\u8bbe\u8ba1\u7530\u95f4\u51e0\u4f55\u5e03\u5c40\u4ee5\u9002\u5e94\u81ea\u4e3b\u519c\u4e1a\uff0c\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2509.25097", "pdf": "https://arxiv.org/pdf/2509.25097", "abs": "https://arxiv.org/abs/2509.25097", "authors": ["Jes\u00fas Roche", "Eduardo Sebasti\u00e1n", "Eduardo Montijano"], "title": "Curriculum Imitation Learning of Distributed Multi-Robot Policies", "categories": ["cs.RO", "cs.LG", "cs.MA"], "comment": "Accepted and presented at the Eight Iberian Robotics Conference, 2025", "summary": "Learning control policies for multi-robot systems (MRS) remains a major\nchallenge due to long-term coordination and the difficulty of obtaining\nrealistic training data. In this work, we address both limitations within an\nimitation learning framework. First, we shift the typical role of Curriculum\nLearning in MRS, from scalability with the number of robots, to focus on\nimproving long-term coordination. We propose a curriculum strategy that\ngradually increases the length of expert trajectories during training,\nstabilizing learning and enhancing the accuracy of long-term behaviors. Second,\nwe introduce a method to approximate the egocentric perception of each robot\nusing only third-person global state demonstrations. Our approach transforms\nidealized trajectories into locally available observations by filtering\nneighbors, converting reference frames, and simulating onboard sensor\nvariability. Both contributions are integrated into a physics-informed\ntechnique to produce scalable, distributed policies from observations. We\nconduct experiments across two tasks with varying team sizes and noise levels.\nResults show that our curriculum improves long-term accuracy, while our\nperceptual estimation method yields policies that are robust to realistic\nuncertainty. Together, these strategies enable the learning of robust,\ndistributed controllers from global demonstrations, even in the absence of\nexpert actions or onboard measurements.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08MRS\uff09\u957f\u671f\u534f\u8c03\u56f0\u96be\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u548c\u611f\u77e5\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u9ad8\u957f\u671f\u884c\u4e3a\u7684\u51c6\u786e\u6027\u5e76\u589e\u5f3a\u7b56\u7565\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u63a7\u5236\u7b56\u7565\u5b66\u4e60\u56e0\u957f\u671f\u534f\u8c03\u548c\u8bad\u7ec3\u6570\u636e\u96be\u83b7\u53d6\u800c\u9762\u4e34\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u9010\u6b65\u589e\u52a0\u4e13\u5bb6\u8f68\u8ff9\u957f\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u5c06\u5168\u5c40\u6f14\u793a\u8f6c\u5316\u4e3a\u5c40\u90e8\u611f\u77e5\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u751f\u6210\u5206\u5e03\u5f0f\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u4e86\u957f\u671f\u884c\u4e3a\u51c6\u786e\u6027\uff0c\u611f\u77e5\u4f30\u8ba1\u65b9\u6cd5\u589e\u5f3a\u4e86\u7b56\u7565\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u5408\u4e24\u79cd\u7b56\u7565\uff0c\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u5168\u7403\u6f14\u793a\u751f\u6210\u9c81\u68d2\u3001\u5206\u5e03\u5f0f\u7684\u63a7\u5236\u5668\u3002"}}
{"id": "2509.25124", "pdf": "https://arxiv.org/pdf/2509.25124", "abs": "https://arxiv.org/abs/2509.25124", "authors": ["David Smith Sundarsingh", "Yifei Li", "Tianji Tang", "George J. Pappas", "Nikolay Atanasov", "Yiannis Kantaros"], "title": "Safe Planning in Unknown Environments using Conformalized Semantic Maps", "categories": ["cs.RO"], "comment": "8 pages, 5 figures, 2 algorithms, 1 table", "summary": "This paper addresses semantic planning problems in unknown environments under\nperceptual uncertainty. The environment contains multiple unknown semantically\nlabeled regions or objects, and the robot must reach desired locations while\nmaintaining class-dependent distances from them. We aim to compute robot paths\nthat complete such semantic reach-avoid tasks with user-defined probability\ndespite uncertain perception. Existing planning algorithms either ignore\nperceptual uncertainty - thus lacking correctness guarantees - or assume known\nsensor models and noise characteristics. In contrast, we present the first\nplanner for semantic reach-avoid tasks that achieves user-specified mission\ncompletion rates without requiring any knowledge of sensor models or noise.\nThis is enabled by quantifying uncertainty in semantic maps - constructed\non-the-fly from perceptual measurements - using conformal prediction in a\nmodel- and distribution-free manner. We validate our approach and the\ntheoretical mission completion rates through extensive experiments, showing\nthat it consistently outperforms baselines in mission success rates.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5982\u4f55\u89e3\u51b3\u8bed\u4e49\u89c4\u5212\u95ee\u9898\uff0c\u514b\u670d\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u5df2\u77e5\u4f20\u611f\u5668\u6a21\u578b\u7684\u8bed\u4e49\u5230\u8fbe-\u907f\u969c\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5b8c\u6210\u8bed\u4e49\u5230\u8fbe-\u907f\u969c\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u8981\u4e48\u5ffd\u7565\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u8981\u4e48\u9700\u8981\u5df2\u77e5\u4f20\u611f\u5668\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u52a8\u6001\u91cf\u5316\u8bed\u4e49\u5730\u56fe\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u5047\u8bbe\u4f20\u611f\u5668\u6a21\u578b\u6216\u566a\u58f0\u5206\u5e03\uff0c\u4ece\u800c\u8ba1\u7b97\u51fa\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u6982\u7387\u7684\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u7b26\u5408\u7406\u8bba\u4e0a\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u6a21\u578b\u652f\u6301\u7684\u8bed\u4e49\u89c4\u5212\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5230\u8fbe-\u907f\u969c\u4efb\u52a1\u3002"}}
{"id": "2312.01239", "pdf": "https://arxiv.org/pdf/2312.01239", "abs": "https://arxiv.org/abs/2312.01239", "authors": ["Raghavv Goel", "Cecilia Morales", "Manpreet Singh", "Artur Dubrawski", "John Galeotti", "Howie Choset"], "title": "Motion Informed Needle Segmentation in Ultrasound Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.RO"], "comment": "7 pages, 4 figures, accepted at ISBI 2024", "summary": "Segmenting a moving needle in ultrasound images is challenging due to the\npresence of artifacts, noise, and needle occlusion. This task becomes even more\ndemanding in scenarios where data availability is limited. In this paper, we\npresent a novel approach for needle segmentation for 2D ultrasound that\ncombines classical Kalman Filter (KF) techniques with data-driven learning,\nincorporating both needle features and needle motion. Our method offers three\nkey contributions. First, we propose a compatible framework that seamlessly\nintegrates into commonly used encoder-decoder style architectures. Second, we\ndemonstrate superior performance compared to recent state-of-the-art needle\nsegmentation models using our novel convolutional neural network (CNN) based\nKF-inspired block, achieving a 15\\% reduction in pixel-wise needle tip error\nand an 8\\% reduction in length error. Third, to our knowledge we are the first\nto implement a learnable filter to incorporate non-linear needle motion for\nimproving needle segmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ecf\u5178\u5361\u5c14\u66fc\u6ee4\u6ce2\u6280\u672f\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u7684\u9488\u5206\u5272\u65b0\u65b9\u6cd5\uff0c\u57282D\u8d85\u58f0\u56fe\u50cf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u8d85\u58f0\u56fe\u50cf\u4e2d\u7531\u4e8e\u4f2a\u5f71\u3001\u566a\u58f0\u548c\u9488\u906e\u6321\u5bfc\u81f4\u7684\u9488\u5206\u5272\u96be\u9898\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u4e0b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u6280\u672f\u548cCNN\uff0c\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u6ee4\u6ce2\u5668\u548c\u517c\u5bb9\u6846\u67b6\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9488\u5c16\u4f4d\u7f6e\u8bef\u5dee\u4e0a\u51cf\u5c1115%\uff0c\u957f\u5ea6\u8bef\u5dee\u51cf\u5c118%\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u53ef\u5b66\u4e60\u6ee4\u6ce2\u5668\u7684\u9488\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.23641", "pdf": "https://arxiv.org/pdf/2509.23641", "abs": "https://arxiv.org/abs/2509.23641", "authors": ["Yixiao Chen", "Ruining Yang", "Xin Chen", "Jia He", "Dongliang Xu", "Yue Yao"], "title": "From Static to Dynamic: a Survey of Topology-Aware Perception in Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 3 figures", "summary": "The key to achieving autonomous driving lies in topology-aware perception,\nthe structured understanding of the driving environment with an emphasis on\nlane topology and road semantics. This survey systematically reviews four core\nresearch directions under this theme: vectorized map construction, topological\nstructure modeling, prior knowledge fusion, and language model-based\nperception. Across these directions, we observe a unifying trend: a paradigm\nshift from static, pre-built maps to dynamic, sensor-driven perception.\nSpecifically, traditional static maps have provided semantic context for\nautonomous systems. However, they are costly to construct, difficult to update\nin real time, and lack generalization across regions, limiting their\nscalability. In contrast, dynamic representations leverage on-board sensor data\nfor real-time map construction and topology reasoning. Each of the four\nresearch directions contributes to this shift through compact spatial modeling,\nsemantic relational reasoning, robust domain knowledge integration, and\nmultimodal scene understanding powered by pre-trained language models.\nTogether, they pave the way for more adaptive, scalable, and explainable\nautonomous driving systems.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63a2\u8ba8\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u62d3\u6251\u611f\u77e5\u611f\u77e5\u7684\u56db\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u4ece\u9759\u6001\u5730\u56fe\u5230\u52a8\u6001\u611f\u77e5\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u5730\u56fe\u6210\u672c\u9ad8\u3001\u66f4\u65b0\u96be\u4e14\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u8f6c\u5411\u5b9e\u65f6\u3001\u52a8\u6001\u7684\u4f20\u611f\u5668\u9a71\u52a8\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u56db\u4e2a\u6838\u5fc3\u65b9\u5411\uff1a\u77e2\u91cf\u5316\u5730\u56fe\u6784\u5efa\u3001\u62d3\u6251\u7ed3\u6784\u5efa\u6a21\u3001\u5148\u9a8c\u77e5\u8bc6\u878d\u5408\u548c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u5171\u540c\u63a8\u52a8\u4e86\u66f4\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u6269\u5c55\u6027\u9ad8\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "conclusion": "\u52a8\u6001\u611f\u77e5\u8303\u5f0f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u66f4\u5177\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23647", "pdf": "https://arxiv.org/pdf/2509.23647", "abs": "https://arxiv.org/abs/2509.23647", "authors": ["Xingjian Yang", "Ashis G. Banerjee"], "title": "Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Robust 6D pose estimation of novel objects under challenging illumination\nremains a significant challenge, often requiring a trade-off between accurate\ninitial pose estimation and efficient real-time tracking. We present a unified\nframework explicitly designed for efficient execution on edge devices, which\nsynergizes a robust initial estimation module with a fast motion-based tracker.\nThe key to our approach is a shared, lighting-invariant color-pair feature\nrepresentation that forms a consistent foundation for both stages. For initial\nestimation, this feature facilitates robust registration between the live RGB-D\nview and the object's 3D mesh. For tracking, the same feature logic validates\ntemporal correspondences, enabling a lightweight model to reliably regress the\nobject's motion. Extensive experiments on benchmark datasets demonstrate that\nour integrated approach is both effective and robust, providing competitive\npose estimation accuracy while maintaining high-fidelity tracking even through\nabrupt pose changes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c6D\u59ff\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408\u4e86\u7a33\u5065\u7684\u521d\u59cb\u4f30\u8ba1\u6a21\u5757\u548c\u5feb\u901f\u7684\u8fd0\u52a8\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u7684\u7167\u660e\u4e0d\u53d8\u7279\u5f81\u8868\u793a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u9488\u5bf9\u5728\u6311\u6218\u6027\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u65b0\u9896\u7269\u4f53\u76846D\u59ff\u6001\u4f30\u8ba1\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u521d\u59cb\u59ff\u6001\u4f30\u8ba1\u4e0e\u5b9e\u65f6\u8ddf\u8e2a\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7684\u7167\u660e\u4e0d\u53d8\u989c\u8272\u5bf9\u7279\u5f81\u8868\u793a\uff0c\u5206\u522b\u7528\u4e8e\u521d\u59cb\u4f30\u8ba1\u548c\u8ddf\u8e2a\u9636\u6bb5\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u521d\u59cb\u59ff\u6001\u4f30\u8ba1\u548c\u9ad8\u6548\u7684\u8fd0\u52a8\u8ddf\u8e2a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9ad8\u4fdd\u771f\u8ddf\u8e2a\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u7a81\u53d1\u59ff\u6001\u53d8\u5316\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u7279\u5f81\u8868\u793a\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5149\u7167\u6311\u6218\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u4e3a6D\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23733", "pdf": "https://arxiv.org/pdf/2509.23733", "abs": "https://arxiv.org/abs/2509.23733", "authors": ["Hangtian Zhao", "Xiang Chen", "Yizhe Li", "Qianhao Wang", "Haibo Lu", "Fei Gao"], "title": "FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper we propose FastViDAR, a novel framework that takes four fisheye\ncamera inputs and produces a full $360^\\circ$ depth map along with per-camera\ndepth, fusion depth, and confidence estimates. Our main contributions are: (1)\nWe introduce Alternative Hierarchical Attention (AHA) mechanism that\nefficiently fuses features across views through separate intra-frame and\ninter-frame windowed self-attention, achieving cross-view feature mixing with\nreduced overhead. (2) We propose a novel ERP fusion approach that projects\nmulti-view depth estimates to a shared equirectangular coordinate system to\nobtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D\nand 2D3D-S datasets for comprehensive evaluation, demonstrating competitive\nzero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA\nOrin NX embedded hardware. Project page:\n\\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}", "AI": {"tldr": "FastViDAR\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u9c7c\u773c\u6444\u50cf\u5934\u751f\u6210360\u5ea6\u6df1\u5ea6\u56fe\uff0c\u5e76\u91c7\u7528AHA\u673a\u5236\u548cERP\u878d\u5408\u65b9\u6cd5\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u63d0\u51faFastViDAR\u65e8\u5728\u9ad8\u6548\u878d\u5408\u591a\u89c6\u89d2\u6df1\u5ea6\u4fe1\u606f\uff0c\u5b9e\u73b0\u5b9e\u65f6360\u5ea6\u6df1\u5ea6\u4f30\u8ba1\u3002", "method": "\u5f15\u5165AHA\u673a\u5236\u8fdb\u884c\u8de8\u89c6\u89d2\u7279\u5f81\u878d\u5408\uff0c\u5e76\u901a\u8fc7ERP\u6295\u5f71\u65b9\u6cd5\u6574\u5408\u591a\u89c6\u89d2\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728HM3D\u548c2D3D-S\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u65f6\u6027\u8fbe20 FPS\u3002", "conclusion": "FastViDAR\u5728\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u786c\u4ef6\u5e94\u7528\u3002"}}
{"id": "2509.23737", "pdf": "https://arxiv.org/pdf/2509.23737", "abs": "https://arxiv.org/abs/2509.23737", "authors": ["Guole Shen", "Tianchen Deng", "Yanbo Wang", "Yongtao Chen", "Yilin Shen", "Jiuming Liu", "Jingchuan Wang"], "title": "GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "DUSt3R-based end-to-end scene reconstruction has recently shown promising\nresults in dense visual SLAM. However, most existing methods only use image\npairs to estimate pointmaps, overlooking spatial memory and global\nconsistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework\nfor dense scene reconstruction and pose estimation from RGB images without any\nprior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based\nframeworks, which operate on all image pairs and predict per-pair point maps in\nlocal coordinate frames, our method supports sequentialized input and\nincrementally estimates metric-scale point clouds in the global coordinate. In\norder to improve consistent spatial correlation, we use a latent state for\nspatial memory and design a transformer-based gated update module to reset and\nupdate the spatial memory that continuously aggregates and tracks relevant 3D\ninformation across frames. Furthermore, we partition the scene into submaps,\napply local alignment within each submap, and register all submaps into a\ncommon world frame using relative constraints, producing a globally consistent\nmap. Experiments on various datasets show that our framework achieves superior\nreconstruction accuracy while maintaining real-time performance.", "AI": {"tldr": "GRS-SLAM3R\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684SLAM\u6846\u67b6\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u5b9e\u73b0\u5bc6\u96c6\u573a\u666f\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u65e0\u9700\u5148\u9a8c\u573a\u666f\u6216\u76f8\u673a\u53c2\u6570\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709DUSt3R\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u56fe\u50cf\u5bf9\u4f30\u8ba1\u70b9\u56fe\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u8bb0\u5fc6\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u652f\u6301\u5e8f\u5217\u8f93\u5165\u548c\u5168\u5c40\u5750\u6807\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u72b6\u6001\u5b58\u50a8\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u8bbe\u8ba1\u57fa\u4e8eTransformer\u7684\u95e8\u63a7\u66f4\u65b0\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u5b50\u56fe\u5206\u5272\u548c\u5c40\u90e8\u5bf9\u9f50\u6280\u672f\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "GRS-SLAM3R\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6SLAM\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.23852", "pdf": "https://arxiv.org/pdf/2509.23852", "abs": "https://arxiv.org/abs/2509.23852", "authors": ["Yiheng Huang", "Junran Peng", "Silei Shen", "Jingwei Yang", "ZeJi Wei", "ChenCheng Bai", "Yonghao He", "Wei Sui", "Muyi Sun", "Yan Liu", "Xu-Cheng Yin", "Man Zhang", "Zhaoxiang Zhang", "Chuanchen Luo"], "title": "SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where", "categories": ["cs.GR", "cs.MM", "cs.RO"], "comment": null, "summary": "The accompanying actions and gestures in dialogue are often closely linked to\ninteractions with the environment, such as looking toward the interlocutor or\nusing gestures to point to the described target at appropriate moments. Speech\nand semantics guide the production of gestures by determining their timing\n(WHEN) and style (HOW), while the spatial locations of interactive objects\ndictate their directional execution (WHERE). Existing approaches either rely\nsolely on descriptive language to generate motions or utilize audio to produce\nnon-interactive gestures, thereby lacking the characterization of interactive\ntiming and spatial intent. This significantly limits the applicability of\nconversational gesture generation, whether in robotics or in the fields of game\nand animation production. To address this gap, we present a full-stack\nsolution. We first established a unique data collection method to\nsimultaneously capture high-precision human motion and spatial intent. We then\ndeveloped a generation model driven by audio, language, and spatial data,\nalongside dedicated metrics for evaluating interaction timing and spatial\naccuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,\ncontext-aware physical interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u6808\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u97f3\u9891\u3001\u8bed\u8a00\u548c\u7a7a\u95f4\u6570\u636e\u751f\u6210\u5bf9\u8bdd\u624b\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4ea4\u4e92\u65f6\u673a\u548c\u7a7a\u95f4\u610f\u56fe\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u63cf\u8ff0\u6027\u8bed\u8a00\u6216\u97f3\u9891\u751f\u6210\u624b\u52bf\uff0c\u7f3a\u4e4f\u4ea4\u4e92\u65f6\u673a\u548c\u7a7a\u95f4\u610f\u56fe\u7684\u523b\u753b\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u6216\u6e38\u620f\u52a8\u753b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u9996\u5148\u5efa\u7acb\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u540c\u65f6\u6355\u6349\u9ad8\u7cbe\u5ea6\u4eba\u4f53\u8fd0\u52a8\u548c\u7a7a\u95f4\u610f\u56fe\uff1b\u7136\u540e\u5f00\u53d1\u4e86\u7531\u97f3\u9891\u3001\u8bed\u8a00\u548c\u7a7a\u95f4\u6570\u636e\u9a71\u52a8\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u89e3\u51b3\u65b9\u6848\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u4e30\u5bcc\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u7406\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u7684\u4ea4\u4e92\u6027\u548c\u7a7a\u95f4\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.23922", "pdf": "https://arxiv.org/pdf/2509.23922", "abs": "https://arxiv.org/abs/2509.23922", "authors": ["Haibao Yu", "Wenxian Yang", "Ruiyang Hao", "Chuanye Wang", "Jiaru Zhong", "Ping Luo", "Zaiqing Nie"], "title": "DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving through Real-to-Simulation", "categories": ["cs.CV", "cs.RO"], "comment": "End-to-End Autonomous Driving Simulation and Benchmark", "summary": "Closed-loop evaluation is increasingly critical for end-to-end autonomous\ndriving. Current closed-loop benchmarks using the CARLA simulator rely on\nmanually configured traffic scenarios, which can diverge from real-world\nconditions, limiting their ability to reflect actual driving performance. To\naddress these limitations, we introduce a simple yet challenging closed-loop\nevaluation framework that closely integrates real-world driving scenarios into\nthe CARLA simulator with infrastructure cooperation. Our approach involves\nextracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour\nvideo dataset captured by high-mounted infrastructure sensors, and creating\nstatic digital twin assets for 15 real-world intersections with consistent\nvisual appearance. These digital twins accurately replicate the traffic and\nenvironmental characteristics of their real-world counterparts, enabling more\nrealistic simulations in CARLA. This evaluation is challenging due to the\ndiversity of driving behaviors, locations, weather conditions, and times of day\nat complex urban intersections. In addition, we provide a comprehensive\nclosed-loop benchmark for evaluating end-to-end autonomous driving models.\nProject URL:\n\\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eCARLA\u6a21\u62df\u5668\u7684\u95ed\u73af\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u573a\u666f\u548c\u57fa\u7840\u8bbe\u65bd\u5408\u4f5c\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8bc4\u4f30\u7684\u903c\u771f\u5ea6\u548c\u6311\u6218\u6027\u3002", "motivation": "\u5f53\u524d\u7684CARLA\u6a21\u62df\u5668\u95ed\u73af\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u624b\u52a8\u914d\u7f6e\u7684\u4ea4\u901a\u573a\u666f\uff0c\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b58\u5728\u5dee\u5f02\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u9a7e\u9a76\u6027\u80fd\u3002", "method": "\u63d0\u53d6800\u4e2a\u52a8\u6001\u4ea4\u901a\u573a\u666f\u5e76\u521b\u5efa15\u4e2a\u771f\u5b9e\u4ea4\u53c9\u8def\u53e3\u7684\u9759\u6001\u6570\u5b57\u5b6a\u751f\u8d44\u4ea7\uff0c\u6574\u5408\u8fdbCARLA\u6a21\u62df\u5668\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u51c6\u786e\u590d\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u548c\u73af\u5883\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u66f4\u903c\u771f\u7684\u6a21\u62df\u73af\u5883\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u95ed\u73af\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2509.23993", "pdf": "https://arxiv.org/pdf/2509.23993", "abs": "https://arxiv.org/abs/2509.23993", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Shaojie Shen"], "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Scalable and realistic simulation of multi-agent traffic behavior is critical\nfor advancing autonomous driving technologies. Although existing data-driven\nsimulators have made significant strides in this domain, they predominantly\nrely on supervised learning to align simulated distributions with real-world\ndriving scenarios. A persistent challenge, however, lies in the distributional\nshift that arises between training and testing, which often undermines model\ngeneralization in unseen environments. To address this limitation, we propose\nSMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for\nnext-token prediction models to better align agent behavior with human\npreferences and evaluation metrics. Our approach introduces a metric-oriented\npolicy optimization algorithm to improve distribution alignment and an\niterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised\nFine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance\ngains. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) validate the effectiveness of this simple yet powerful R1-style training\nframework in enhancing foundation models. The results on the Waymo Open Sim\nAgents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art\nperformance with an overall realism meta score of 0.7858, ranking first on the\nleaderboard at the time of submission.", "AI": {"tldr": "SMART-R1\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bR1\u98ce\u683c\u5f3a\u5316\u5fae\u8c03\u8303\u5f0f\uff0c\u901a\u8fc7\u5ea6\u91cf\u5bfc\u5411\u7684\u7b56\u7565\u4f18\u5316\u548c\u4ea4\u66ff\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u884c\u4e3a\u6a21\u62df\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u95f4\u5206\u5e03\u504f\u79fb\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u672a\u89c1\u8fc7\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528R1\u98ce\u683c\u5f3a\u5316\u5fae\u8c03\u8303\u5f0f\uff0c\u7ed3\u5408\u5ea6\u91cf\u5bfc\u5411\u7b56\u7565\u4f18\u5316\u548cSFT-RFT-SFT\u4ea4\u66ff\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728WOMD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u5728WOSAC\u6311\u6218\u4e2d\u53d6\u5f970.7858\u7684\u603b\u4f53\u771f\u5b9e\u5ea6\u5f97\u5206\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "SMART-R1\u901a\u8fc7\u7b80\u5355\u800c\u5f3a\u5927\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.24001", "pdf": "https://arxiv.org/pdf/2509.24001", "abs": "https://arxiv.org/abs/2509.24001", "authors": ["Matej Palider", "Omar Eldardeer", "Viktor Kocur"], "title": "Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO Platform", "categories": ["cs.CV", "cs.RO", "I.4.9"], "comment": "Code available at http://github.com/kocurvik/nico_gaze", "summary": "This paper evaluates the current gaze estimation methods within an HRI\ncontext of a shared workspace scenario. We introduce a new, annotated dataset\ncollected with the NICO robotic platform. We evaluate four state-of-the-art\ngaze estimation models. The evaluation shows that the angular errors are close\nto those reported on general-purpose benchmarks. However, when expressed in\nterms of distance in the shared workspace the best median error is 16.48 cm\nquantifying the practical limitations of current methods. We conclude by\ndiscussing these limitations and offering recommendations on how to best\nintegrate gaze estimation as a modality in HRI systems.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4eba\u673a\u4ea4\u4e92\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u573a\u666f\u4e2d\u7684\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ecb\u7ecd\u4e86\u65b0\u6570\u636e\u96c6\u5e76\u6d4b\u8bd5\u4e86\u56db\u79cd\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0\u5b9e\u9645\u5e94\u7528\u4e2d\u8bef\u5dee\u8f83\u5927\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u5728\u4eba\u673a\u4ea4\u4e92\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528NICO\u673a\u5668\u4eba\u5e73\u53f0\u6536\u96c6\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u56db\u79cd\u5148\u8fdb\u7684\u89c6\u7ebf\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u89d2\u5ea6\u8bef\u5dee\u63a5\u8fd1\u901a\u7528\u57fa\u51c6\uff0c\u4f46\u5b9e\u9645\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u6700\u4f73\u4e2d\u4f4d\u8bef\u5dee\u4e3a16.48\u5398\u7c73\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u89c6\u7ebf\u4f30\u8ba1\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5e94\u7528\u7684\u5efa\u8bae\u3002"}}
{"id": "2509.24089", "pdf": "https://arxiv.org/pdf/2509.24089", "abs": "https://arxiv.org/abs/2509.24089", "authors": ["Ilari Vallivaara", "Katja Poikselk\u00e4", "Pauli Rikula", "Juha R\u00f6ning"], "title": "Systematic Alias Sampling: an efficient and low-variance way to sample from a discrete distribution", "categories": ["cs.DS", "cs.MS", "cs.RO", "F.2.2; G.3; G.4; I.2.9; I.6.6"], "comment": null, "summary": "In this paper we combine the Alias method with the concept of systematic\nsampling, a method commonly used in particle filters for efficient low-variance\nresampling. The proposed method allows very fast sampling from a discrete\ndistribution: drawing k samples is up to an order of magnitude faster than\nbinary search from the cumulative distribution function (cdf) or inversion\nmethods used in many libraries. The produced empirical distribution function is\nevaluated using a modified Cram\\'er-Von Mises goodness-of-fit statistic,\nshowing that the method compares very favourably to multinomial sampling. As\ncontinuous distributions can often be approximated with discrete ones, the\nproposed method can be used as a very general way to efficiently produce random\nsamples for particle filter proposal distributions, e.g. for motion models in\nrobotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Alias\u65b9\u6cd5\u548c\u7cfb\u7edf\u91c7\u6837\u6280\u672f\u7684\u9ad8\u6548\u79bb\u6563\u5206\u5e03\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c92\u5b50\u6ee4\u6ce2\u4e2d\u7684\u91cd\u91c7\u6837\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u6563\u5206\u5e03\u91c7\u6837\u65b9\u6cd5\uff08\u5982\u4e8c\u8fdb\u5236\u641c\u7d22\u6216\u9006\u53d8\u6362\u6cd5\uff09\u6548\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u578b\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u7ed3\u5408Alias\u65b9\u6cd5\u548c\u7cfb\u7edf\u91c7\u6837\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u79bb\u6563\u5206\u5e03\u91c7\u6837\uff0c\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91c7\u6837\u901f\u5ea6\u548c\u62df\u5408\u4f18\u5ea6\u7edf\u8ba1\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u591a\u9879\u5f0f\u91c7\u6837\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7c92\u5b50\u6ee4\u6ce2\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u79bb\u6563\u5206\u5e03\u91c7\u6837\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24093", "pdf": "https://arxiv.org/pdf/2509.24093", "abs": "https://arxiv.org/abs/2509.24093", "authors": ["Owen Lewis Howell", "Linfeng Zhao", "Xupeng Zhu", "Yaoyao Qian", "Haojie Huang", "Lingfeng Sun", "Wil Thomason", "Robert Platt", "Robin Walters"], "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": null, "summary": "The global attention mechanism is one of the keys to the success of\ntransformer architecture, but it incurs quadratic computational costs in\nrelation to the number of tokens. On the other hand, equivariant models, which\nleverage the underlying geometric structures of problem instance, often achieve\nsuperior accuracy in physical, biochemical, computer vision, and robotic tasks,\nat the cost of additional compute requirements. As a result, existing\nequivariant transformers only support low-order equivariant features and local\ncontext windows, limiting their expressiveness and performance. This work\nproposes Clebsch-Gordan Transformer, achieving efficient global attention by a\nnovel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our\nmethod enables equivariant modeling of features at all orders while achieving\n${O}(N \\log N)$ input token complexity. Additionally, the proposed method\nscales well with high-order irreducible features, by exploiting the sparsity of\nthe Clebsch-Gordon matrix. Lastly, we also incorporate optional token\npermutation equivariance through either weight sharing or data augmentation. We\nbenchmark our method on a diverse set of benchmarks including n-body\nsimulation, QM9, ModelNet point cloud classification and a robotic grasping\ndataset, showing clear gains over existing equivariant transformers in GPU\nmemory size, speed, and accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Clebsch-Gordan Transformer\uff0c\u901a\u8fc7\u65b0\u9896\u7684Clebsch-Gordon\u5377\u79ef\u5728SO(3)\u4e0d\u53ef\u7ea6\u8868\u793a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b49\u53d8Transformer\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u5728Transformer\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u4e0e\u6807\u8bb0\u6570\u91cf\u5448\u5e73\u65b9\u5173\u7cfb\u3002\u540c\u65f6\uff0c\u7b49\u53d8\u6a21\u578b\u5728\u7269\u7406\u3001\u751f\u5316\u7b49\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u9ad8\u3002\u73b0\u6709\u7b49\u53d8Transformer\u4ec5\u652f\u6301\u4f4e\u9636\u7b49\u53d8\u7279\u5f81\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9650\u5236\u4e86\u5176\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faClebsch-Gordan Transformer\uff0c\u5229\u7528Clebsch-Gordon\u5377\u79ef\u5728SO(3)\u4e0d\u53ef\u7ea6\u8868\u793a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u652f\u6301\u9ad8\u9636\u7b49\u53d8\u7279\u5f81\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3aO(N log N)\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u6216\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u53ef\u9009\u7684\u6807\u8bb0\u7f6e\u6362\u7b49\u53d8\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982n-body\u6a21\u62df\u3001QM9\u3001ModelNet\u70b9\u4e91\u5206\u7c7b\u548c\u673a\u5668\u4eba\u6293\u53d6\u6570\u636e\u96c6\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728GPU\u5185\u5b58\u5360\u7528\u3001\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7b49\u53d8Transformer\u3002", "conclusion": "Clebsch-Gordan Transformer\u4e3a\u9ad8\u9636\u7b49\u53d8\u7279\u5f81\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.24230", "pdf": "https://arxiv.org/pdf/2509.24230", "abs": "https://arxiv.org/abs/2509.24230", "authors": ["Shaobin Ling", "Yun Wang", "Chenyou Fan", "Tin Lun Lam", "Junjie Hu"], "title": "ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Large Language Models (LLMs) enable intelligent multi-robot collaboration but\nface fundamental trade-offs: declarative methods lack adaptability in dynamic\nenvironments, while iterative methods incur prohibitive computational costs\nthat scale poorly with team size and task complexity. In this paper, we propose\nELHPlan, a novel framework that introduces Action Chains--sequences of actions\nexplicitly bound to sub-goal intentions--as the fundamental planning primitive.\nELHPlan operates via a cyclical process: 1) constructing intention-bound action\nsequences, 2) proactively validating for conflicts and feasibility, 3) refining\nissues through targeted mechanisms, and 4) executing validated actions. This\ndesign balances adaptability and efficiency by providing sufficient planning\nhorizons while avoiding expensive full re-planning. We further propose\ncomprehensive efficiency metrics, including token consumption and planning\ntime, to more holistically evaluate multi-agent collaboration. Our experiments\non benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable\ntask success rates while consuming only 24% of the tokens required by\nstate-of-the-art methods. Our research establishes a new\nefficiency-effectiveness frontier for LLM-based multi-agent planning systems.", "AI": {"tldr": "ELHPlan\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u94fe\u4f5c\u4e3a\u57fa\u672c\u89c4\u5212\u5355\u5143\uff0c\u5e73\u8861\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ee4\u724c\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u52a8\u4f5c\u94fe\u4f5c\u4e3a\u89c4\u5212\u57fa\u7840\uff0c\u91c7\u7528\u5468\u671f\u6027\u6d41\u7a0b\uff08\u6784\u5efa\u3001\u9a8c\u8bc1\u3001\u4f18\u5316\u3001\u6267\u884c\uff09\u6765\u89c4\u5212\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELHPlan\u8fbe\u5230\u76f8\u8fd1\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4ee4\u724c\u6d88\u8017\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u768424%\u3002", "conclusion": "ELHPlan\u4e3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387-\u6548\u679c\u524d\u6cbf\u3002"}}
{"id": "2509.24241", "pdf": "https://arxiv.org/pdf/2509.24241", "abs": "https://arxiv.org/abs/2509.24241", "authors": ["Seungwook Kim", "Seunghyeon Lee", "Minsu Cho"], "title": "FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 4 figures, accepted to CoRL 2025 LSRW workshop", "summary": "Generating realistic robot videos from explicit action trajectories is a\ncritical step toward building effective world models and robotics foundation\nmodels. We introduce two training-free, inference-time techniques that fully\nexploit explicit action parameters in diffusion-based robot video generation.\nInstead of treating action vectors as passive conditioning signals, our methods\nactively incorporate them to guide both the classifier-free guidance process\nand the initialization of Gaussian latents. First, action-scaled\nclassifier-free guidance dynamically modulates guidance strength in proportion\nto action magnitude, enhancing controllability over motion intensity. Second,\naction-scaled noise truncation adjusts the distribution of initially sampled\nnoise to better align with the desired motion dynamics. Experiments on real\nrobot manipulation datasets demonstrate that these techniques significantly\nimprove action coherence and visual quality across diverse robot environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u95f4\u6280\u672f\uff0c\u901a\u8fc7\u5145\u5206\u5229\u7528\u52a8\u4f5c\u53c2\u6570\u5728\u6269\u6563\u6a21\u578b\u4e2d\u751f\u6210\u66f4\u771f\u5b9e\u7684\u673a\u5668\u4eba\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u57fa\u4e8e\u660e\u786e\u52a8\u4f5c\u8f68\u8ff9\u7684\u771f\u5b9e\u673a\u5668\u4eba\u89c6\u9891\u662f\u6784\u5efa\u6709\u6548\u4e16\u754c\u6a21\u578b\u548c\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u52a8\u6001\u8c03\u5236\u5f15\u5bfc\u5f3a\u5ea6\u7684\u52a8\u4f5c\u7f29\u653e\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u4ee5\u53ca\u8c03\u6574\u521d\u59cb\u91c7\u6837\u566a\u58f0\u5206\u5e03\u7684\u52a8\u4f5c\u7f29\u653e\u566a\u58f0\u622a\u65ad\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u6539\u5584\u4e86\u52a8\u4f5c\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u53ef\u63a7\u6027\u548c\u5bf9\u9f50\u6027\u3002"}}
{"id": "2509.24527", "pdf": "https://arxiv.org/pdf/2509.24527", "abs": "https://arxiv.org/abs/2509.24527", "authors": ["Danijar Hafner", "Wilson Yan", "Timothy Lillicrap"], "title": "Training Agents Inside of Scalable World Models", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "comment": "Website: https://danijar.com/dreamer4/", "summary": "World models learn general knowledge from videos and simulate experience for\ntraining behaviors in imagination, offering a path towards intelligent agents.\nHowever, previous world models have been unable to accurately predict object\ninteractions in complex environments. We introduce Dreamer 4, a scalable agent\nthat learns to solve control tasks by reinforcement learning inside of a fast\nand accurate world model. In the complex video game Minecraft, the world model\naccurately predicts object interactions and game mechanics, outperforming\nprevious world models by a large margin. The world model achieves real-time\ninteractive inference on a single GPU through a shortcut forcing objective and\nan efficient transformer architecture. Moreover, the world model learns general\naction conditioning from only a small amount of data, allowing it to extract\nthe majority of its knowledge from diverse unlabeled videos. We propose the\nchallenge of obtaining diamonds in Minecraft from only offline data, aligning\nwith practical applications such as robotics where learning from environment\ninteraction can be unsafe and slow. This task requires choosing sequences of\nover 20,000 mouse and keyboard actions from raw pixels. By learning behaviors\nin imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft\npurely from offline data, without environment interaction. Our work provides a\nscalable recipe for imagination training, marking a step towards intelligent\nagents.", "AI": {"tldr": "Dreamer 4\u662f\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u5feb\u901f\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u8bad\u7ec3\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7269\u4f53\u4ea4\u4e92\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5728Minecraft\u4e2d\u5b9e\u73b0\u4e86\u7eaf\u79bb\u7ebf\u6570\u636e\u83b7\u53d6\u94bb\u77f3\u7684\u7a81\u7834\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u4e16\u754c\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u590d\u6742\u73af\u5883\u4e2d\u7269\u4f53\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u4e0d\u9700\u8981\u73af\u5883\u4ea4\u4e92\u7684\u667a\u80fd\u4ee3\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u5982\u673a\u5668\u4eba\u5b66\u3002", "method": "\u4f7f\u7528\u9ad8\u6548\u7684Transformer\u67b6\u6784\u548c\u6377\u5f84\u76ee\u6807\uff0c\u6784\u5efa\u5feb\u901f\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u5b66\u4e60\u52a8\u4f5c\u6761\u4ef6\uff0c\u5e76\u5229\u7528\u591a\u6837\u5316\u65e0\u6807\u7b7e\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Dreamer 4\u5728Minecraft\u4e2d\u5927\u5e45\u8d85\u8d8a\u4e4b\u524d\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7eaf\u79bb\u7ebf\u6570\u636e\u4e0b\u83b7\u53d6\u94bb\u77f3\u7684\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u7684\u9884\u6d4b\u548c\u8bad\u7ec3\u80fd\u529b\u3002", "conclusion": "Dreamer 4\u4e3a\u60f3\u8c61\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6807\u5fd7\u7740\u667a\u80fd\u4ee3\u7406\u53d1\u5c55\u7684\u65b0\u4e00\u6b65\u3002"}}
{"id": "2509.24572", "pdf": "https://arxiv.org/pdf/2509.24572", "abs": "https://arxiv.org/abs/2509.24572", "authors": ["Peter H\u00f6nig", "Stefan Thalhammer", "Jean-Baptiste Weibel", "Matthias Hirschmanner", "Markus Vincze"], "title": "SCOPE: Semantic Conditioning for Sim2Real Category-Level Object Pose Estimation in Robotics", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Object manipulation requires accurate object pose estimation. In open\nenvironments, robots encounter unknown objects, which requires semantic\nunderstanding in order to generalize both to known categories and beyond. To\nresolve this challenge, we present SCOPE, a diffusion-based category-level\nobject pose estimation model that eliminates the need for discrete category\nlabels by leveraging DINOv2 features as continuous semantic priors. By\ncombining these DINOv2 features with photorealistic training data and a noise\nmodel for point normals, we reduce the Sim2Real gap in category-level object\npose estimation. Furthermore, injecting the continuous semantic priors via\ncross-attention enables SCOPE to learn canonicalized object coordinate systems\nacross object instances beyond the distribution of known categories. SCOPE\noutperforms the current state of the art in synthetically trained\ncategory-level object pose estimation, achieving a relative improvement of\n31.9\\% on the 5$^\\circ$5cm metric. Additional experiments on two instance-level\ndatasets demonstrate generalization beyond known object categories, enabling\ngrasping of unseen objects from unknown categories with a success rate of up to\n100\\%. Code available: https://github.com/hoenigpeter/scope.", "AI": {"tldr": "SCOPE \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u5229\u7528 DINOv2 \u7279\u5f81\u4f5c\u4e3a\u8fde\u7eed\u8bed\u4e49\u5148\u9a8c\uff0c\u65e0\u9700\u79bb\u6563\u7c7b\u522b\u6807\u7b7e\uff0c\u663e\u8457\u51cf\u5c11 Sim2Real \u5dee\u8ddd\uff0c\u5e76\u5728\u672a\u77e5\u7c7b\u522b\u4e2d\u5b9e\u73b0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u4f1a\u9047\u5230\u672a\u77e5\u7269\u4f53\uff0c\u9700\u8981\u8bed\u4e49\u7406\u89e3\u4ee5\u6cdb\u5316\u5230\u5df2\u77e5\u7c7b\u522b\u53ca\u4e4b\u5916\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7c7b\u522b\u6807\u7b7e\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408 DINOv2 \u7279\u5f81\u3001\u903c\u771f\u8bad\u7ec3\u6570\u636e\u53ca\u70b9\u6cd5\u7ebf\u566a\u58f0\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u8fde\u7eed\u8bed\u4e49\u5148\u9a8c\uff0c\u5b66\u4e60\u8de8\u7269\u4f53\u5b9e\u4f8b\u7684\u89c4\u8303\u5316\u5750\u6807\u7cfb\u7edf\u3002", "result": "SCOPE \u5728\u5408\u6210\u8bad\u7ec3\u7684\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c5\u00b05cm \u6307\u6807\u63d0\u5347 31.9%\uff1b\u5728\u672a\u77e5\u7c7b\u522b\u7269\u4f53\u6293\u53d6\u4e2d\u6210\u529f\u7387\u9ad8\u8fbe 100%\u3002", "conclusion": "SCOPE \u901a\u8fc7\u8fde\u7eed\u8bed\u4e49\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u672a\u77e5\u7269\u4f53\u7684\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.24716", "pdf": "https://arxiv.org/pdf/2509.24716", "abs": "https://arxiv.org/abs/2509.24716", "authors": ["Michael Drolet", "Firas Al-Hafez", "Aditya Bhatt", "Jan Peters", "Oleg Arenz"], "title": "Discrete Variational Autoencoding via Policy Search", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit\nefficiency and can be modeled with autoregressive discrete distributions,\nenabling parameter-efficient multimodal search with transformers. However,\ndiscrete random variables do not allow for exact differentiable\nparameterization; therefore, discrete VAEs typically rely on approximations,\nsuch as Gumbel-Softmax reparameterization or straight-through gradient\nestimates, or employ high-variance gradient-free methods such as REINFORCE that\nhave had limited success on high-dimensional tasks such as image\nreconstruction. Inspired by popular techniques in policy search, we propose a\ntraining framework for discrete VAEs that leverages the natural gradient of a\nnon-parametric encoder to update the parametric encoder without requiring\nreparameterization. Our method, combined with automatic step size adaptation\nand a transformer-based encoder, scales to challenging datasets such as\nImageNet and outperforms both approximate reparameterization methods and\nquantization-based discrete autoencoders in reconstructing high-dimensional\ndata from compact latent spaces, achieving a 20% improvement on FID Score for\nImageNet 256.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u79bb\u6563\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u975e\u53c2\u6570\u7f16\u7801\u5668\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\u7f16\u7801\u5668\uff0c\u65e0\u9700\u91cd\u53c2\u6570\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u6570\u636e\u91cd\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563VAE\u7531\u4e8e\u79bb\u6563\u968f\u673a\u53d8\u91cf\u7684\u4e0d\u53ef\u5fae\u6027\uff0c\u901a\u5e38\u4f9d\u8d56\u8fd1\u4f3c\u65b9\u6cd5\u6216\u9ad8\u65b9\u5dee\u7684\u65e0\u68af\u5ea6\u65b9\u6cd5\uff0c\u5bfc\u81f4\u5728\u9ad8\u7ef4\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u91cd\u6784\uff09\u4e2d\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7b56\u7565\u641c\u7d22\u4e2d\u7684\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u53c2\u6570\u5316\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u975e\u53c2\u6570\u7f16\u7801\u5668\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\u7f16\u7801\u5668\uff0c\u5e76\u91c7\u7528\u81ea\u52a8\u6b65\u957f\u8c03\u6574\u548c\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u7f16\u7801\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728ImageNet\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u91cd\u6784\u9ad8\u7ef4\u6570\u636e\u65f6\u4f18\u4e8e\u4f20\u7edf\u8fd1\u4f3c\u65b9\u6cd5\u548c\u57fa\u4e8e\u91cf\u5316\u7684\u79bb\u6563\u81ea\u7f16\u7801\u5668\uff0cFID\u5f97\u5206\u63d0\u5347\u4e8620%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u6563VAE\u5728\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5c55\u793a\u4e86\u81ea\u7136\u68af\u5ea6\u548c\u975e\u53c2\u6570\u7f16\u7801\u5668\u7ed3\u5408\u7684\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.24731", "pdf": "https://arxiv.org/pdf/2509.24731", "abs": "https://arxiv.org/abs/2509.24731", "authors": ["Luis F. W. Batista", "Tom Bourbon", "Cedric Pradalier"], "title": "Evaluation of Polarimetric Fusion for Semantic Segmentation in Aquatic Environments", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to VCIP 2025", "summary": "Accurate segmentation of floating debris on water is often compromised by\nsurface glare and changing outdoor illumination. Polarimetric imaging offers a\nsingle-sensor route to mitigate water-surface glare that disrupts semantic\nsegmentation of floating objects. We benchmark state-of-the-art fusion networks\non PoTATO, a public dataset of polarimetric images of plastic bottles in inland\nwaterways, and compare their performance with single-image baselines using\ntraditional models. Our results indicate that polarimetric cues help recover\nlow-contrast objects and suppress reflection-induced false positives, raising\nmean IoU and lowering contour error relative to RGB inputs. These sharper masks\ncome at a cost: the additional channels enlarge the models increasing the\ncomputational load and introducing the risk of new false positives. By\nproviding a reproducible, diagnostic benchmark and publicly available code, we\nhope to help researchers choose if polarized cameras are suitable for their\napplications and to accelerate related research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u504f\u632f\u6210\u50cf\u6280\u672f\u6539\u8fdb\u6c34\u4e0a\u6f02\u6d6e\u788e\u7247\u7684\u5206\u5272\u6548\u679c\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u504f\u632f\u7ebf\u7d22\u80fd\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u6c34\u4e0a\u6f02\u6d6e\u7269\u4f53\u7684\u8bed\u4e49\u5206\u5272\u5e38\u53d7\u6c34\u9762\u53cd\u5149\u548c\u6237\u5916\u5149\u7167\u53d8\u5316\u7684\u5e72\u6270\uff0c\u504f\u632f\u6210\u50cf\u6280\u672f\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u516c\u5f00\u6570\u636e\u96c6PoTATO\u4e0a\u6d4b\u8bd5\u4e86\u6700\u5148\u8fdb\u7684\u878d\u5408\u7f51\u7edc\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u5355\u56fe\u50cf\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u504f\u632f\u7ebf\u7d22\u80fd\u6062\u590d\u4f4e\u5bf9\u6bd4\u5ea6\u7269\u4f53\u5e76\u6291\u5236\u53cd\u5c04\u5bfc\u81f4\u7684\u8bef\u68c0\uff0c\u63d0\u5347\u4e86\u5e73\u5747IoU\u5e76\u964d\u4f4e\u4e86\u8f6e\u5ed3\u8bef\u5dee\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u516c\u5f00\u4ee3\u7801\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5224\u65ad\u504f\u632f\u76f8\u673a\u662f\u5426\u9002\u5408\u5176\u5e94\u7528\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2509.24878", "pdf": "https://arxiv.org/pdf/2509.24878", "abs": "https://arxiv.org/abs/2509.24878", "authors": ["Jiuhong Xiao", "Roshan Nayak", "Ning Zhang", "Daniel Tortei", "Giuseppe Loianno"], "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation", "categories": ["cs.CV", "cs.RO"], "comment": "23 pages including the checklist and appendix. Accepted at NeurIPS\n  2025", "summary": "Paired RGB-thermal data is crucial for visual-thermal sensor fusion and\ncross-modality tasks, including important applications such as multi-modal\nimage alignment and retrieval. However, the scarcity of synchronized and\ncalibrated RGB-thermal image pairs presents a major obstacle to progress in\nthese areas. To overcome this challenge, RGB-to-Thermal (RGB-T) image\ntranslation has emerged as a promising solution, enabling the synthesis of\nthermal images from abundant RGB datasets for training purposes. In this study,\nwe propose ThermalGen, an adaptive flow-based generative model for RGB-T image\ntranslation, incorporating an RGB image conditioning architecture and a\nstyle-disentangled mechanism. To support large-scale training, we curated eight\npublic satellite-aerial, aerial, and ground RGB-T paired datasets, and\nintroduced three new large-scale satellite-aerial RGB-T datasets--DJI-day,\nBosonplus-day, and Bosonplus-night--captured across diverse times, sensor\ntypes, and geographic regions. Extensive evaluations across multiple RGB-T\nbenchmarks demonstrate that ThermalGen achieves comparable or superior\ntranslation performance compared to existing GAN-based and diffusion-based\nmethods. To our knowledge, ThermalGen is the first RGB-T image translation\nmodel capable of synthesizing thermal images that reflect significant\nvariations in viewpoints, sensor characteristics, and environmental conditions.\nProject page: http://xjh19971.github.io/ThermalGen", "AI": {"tldr": "ThermalGen\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684RGB-T\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u6761\u4ef6\u67b6\u6784\u548c\u98ce\u683c\u89e3\u8026\u673a\u5236\uff0c\u89e3\u51b3\u4e86RGB-\u70ed\u56fe\u50cf\u5bf9\u7684\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u540c\u6b65\u548c\u6821\u51c6\u7684RGB-\u70ed\u56fe\u50cf\u5bf9\u7684\u7a00\u7f3a\u6027\uff0cRGB-T\u56fe\u50cf\u7ffb\u8bd1\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faThermalGen\uff0c\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u6761\u4ef6\u67b6\u6784\u548c\u98ce\u683c\u89e3\u8026\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u516c\u5f00\u548c\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ThermalGen\u5728\u591a\u4e2aRGB-T\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684GAN\u548c\u6269\u6563\u65b9\u6cd5\uff0c\u80fd\u591f\u5408\u6210\u53cd\u6620\u4e0d\u540c\u89c6\u89d2\u3001\u4f20\u611f\u5668\u7279\u6027\u548c\u73af\u5883\u6761\u4ef6\u7684\u70ed\u56fe\u50cf\u3002", "conclusion": "ThermalGen\u662f\u9996\u4e2a\u80fd\u591f\u5408\u6210\u591a\u6837\u5316\u70ed\u56fe\u50cf\u7684RGB-T\u7ffb\u8bd1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.24927", "pdf": "https://arxiv.org/pdf/2509.24927", "abs": "https://arxiv.org/abs/2509.24927", "authors": ["An Guo", "Shuoxiao Zhang", "Enyi Tang", "Xinyu Gao", "Haomin Pang", "Haoxiang Tian", "Yanzhou Mu", "Wu Wen", "Chunrong Fang", "Zhenyu Chen"], "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?", "categories": ["cs.AI", "cs.RO", "cs.SE"], "comment": "The paper has been accepted by the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "With the tremendous advancement of deep learning and communication\ntechnology, Vehicle-to-Everything (V2X) cooperative perception has the\npotential to address limitations in sensing distant objects and occlusion for a\nsingle-agent perception system. V2X cooperative perception systems are software\nsystems characterized by diverse sensor types and cooperative agents, varying\nfusion schemes, and operation under different communication conditions.\nTherefore, their complex composition gives rise to numerous operational\nchallenges. Furthermore, when cooperative perception systems produce erroneous\npredictions, the types of errors and their underlying causes remain\ninsufficiently explored. To bridge this gap, we take an initial step by\nconducting an empirical study of V2X cooperative perception. To systematically\nevaluate the impact of cooperative perception on the ego vehicle's perception\nperformance, we identify and analyze six prevalent error patterns in\ncooperative perception systems. We further conduct a systematic evaluation of\nthe critical components of these systems through our large-scale study and\nidentify the following key findings: (1) The LiDAR-based cooperation\nconfiguration exhibits the highest perception performance; (2)\nVehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication\nexhibit distinct cooperative perception performance under different fusion\nschemes; (3) Increased cooperative perception errors may result in a higher\nfrequency of driving violations; (4) Cooperative perception systems are not\nrobust against communication interference when running online. Our results\nreveal potential risks and vulnerabilities in critical components of\ncooperative perception systems. We hope that our findings can better promote\nthe design and repair of cooperative perception systems.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86V2X\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u7684\u516d\u5927\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u4e86\u5173\u952e\u7ec4\u4ef6\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6f5c\u5728\u98ce\u9669\u548c\u8106\u5f31\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u8f66\u611f\u77e5\u7cfb\u7edf\u5728\u8fdc\u8ddd\u79bb\u611f\u77e5\u548c\u906e\u6321\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u9519\u8bef\u7c7b\u578b\u53ca\u5176\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u8bc6\u522b\u548c\u5206\u6790\u516d\u79cd\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u53d1\u73b0LiDAR\u534f\u540c\u914d\u7f6e\u6027\u80fd\u6700\u4f73\uff0cV2I\u548cV2V\u5728\u4e0d\u540c\u878d\u5408\u65b9\u6848\u4e0b\u8868\u73b0\u4e0d\u540c\uff1b\u7cfb\u7edf\u5bf9\u901a\u4fe1\u5e72\u6270\u4e0d\u591f\u9c81\u68d2\uff0c\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u66f4\u591a\u9a7e\u9a76\u8fdd\u89c4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u7684\u6f5c\u5728\u98ce\u9669\u548c\u8106\u5f31\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7cfb\u7edf\u8bbe\u8ba1\u548c\u4fee\u590d\u3002"}}
{"id": "2509.25053", "pdf": "https://arxiv.org/pdf/2509.25053", "abs": "https://arxiv.org/abs/2509.25053", "authors": ["Praveen Kumar Ranjan", "Abhinav Sinha", "Yongcan Cao"], "title": "Safety-Critical Input-Constrained Nonlinear Intercept Guidance in Multiple Engagement Zones", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.DS"], "comment": null, "summary": "This paper presents an input-constrained nonlinear guidance law to address\nthe problem of intercepting a stationary target in contested environments with\nmultiple defending agents. Contrary to prior approaches that rely on explicit\nknowledge of defender strategies or utilize conservative safety conditions\nbased on a defender's range, our work characterizes defender threats\ngeometrically through engagement zones that delineate inevitable interception\nregions. Outside these engagement zones, the interceptor remains invulnerable.\nThe proposed guidance law switches between a repulsive safety maneuver near\nthese zones and a pursuit maneuver outside their influence. To deal with\nmultiple engagement zones, we employ a smooth minimum function\n(log-sum-exponent approximation) that aggregates threats from all the zones\nwhile prioritizing the most critical threats. Input saturation is modeled and\nembedded in the non-holonomic vehicle dynamics so the controller respects\nactuator limits while maintaining stability. Numerical simulations with several\ndefenders demonstrate the proposed method's ability to avoid engagement zones\nand achieve interception across diverse initial conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f93\u5165\u53d7\u9650\u7684\u975e\u7ebf\u6027\u5236\u5bfc\u5f8b\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u6709\u591a\u540d\u9632\u5fa1\u8005\u7684\u7ade\u4e89\u73af\u5883\u4e2d\u62e6\u622a\u9759\u6b62\u76ee\u6807\u7684\u95ee\u9898\u3002\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5b9a\u4e49\u4e86\u9632\u5fa1\u8005\u7684\u5a01\u80c1\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5207\u6362\u7b56\u7565\u548c\u5149\u6ed1\u6700\u5c0f\u51fd\u6570\u5904\u7406\u591a\u91cd\u5a01\u80c1\uff0c\u540c\u65f6\u8003\u8651\u4e86\u8f93\u5165\u9971\u548c\u95ee\u9898\u3002", "motivation": "\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u62e6\u622a\u76ee\u6807\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9632\u5fa1\u8005\u7b56\u7565\u7684\u663e\u5f0f\u77e5\u8bc6\u6216\u4fdd\u5b88\u7684\u5b89\u5168\u6761\u4ef6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u66f4\u7075\u6d3b\u5730\u5b9a\u4e49\u5a01\u80c1\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u5207\u6362\u7b56\u7565\u4ee5\u5e94\u5bf9\u591a\u9632\u5fa1\u8005\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5b9a\u4e49\u9632\u5fa1\u8005\u7684\u5a01\u80c1\u533a\u57df\uff08\u63a5\u89e6\u533a\u57df\uff09\uff0c\u5728\u9760\u8fd1\u533a\u57df\u65f6\u91c7\u7528\u6392\u65a5\u5b89\u5168\u673a\u52a8\uff0c\u8fdc\u79bb\u65f6\u91c7\u7528\u8ffd\u8e2a\u673a\u52a8\u3002\u4f7f\u7528\u5149\u6ed1\u6700\u5c0f\u51fd\u6570\u805a\u5408\u591a\u91cd\u5a01\u80c1\uff0c\u5e76\u5c06\u8f93\u5165\u9971\u548c\u5d4c\u5165\u975e\u5b8c\u6574\u8f66\u8f86\u52a8\u529b\u5b66\u4e2d\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u79cd\u521d\u59cb\u6761\u4ef6\u4e0b\u907f\u5f00\u5a01\u80c1\u533a\u57df\u5e76\u6210\u529f\u62e6\u622a\u76ee\u6807\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5236\u5bfc\u5f8b\u901a\u8fc7\u51e0\u4f55\u5a01\u80c1\u5b9a\u4e49\u548c\u52a8\u6001\u5207\u6362\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9632\u5fa1\u8005\u73af\u5883\u4e2d\u7684\u76ee\u6807\u62e6\u622a\u95ee\u9898\uff0c\u517c\u987e\u4e86\u5b89\u5168\u6027\u548c\u63a7\u5236\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.25146", "pdf": "https://arxiv.org/pdf/2509.25146", "abs": "https://arxiv.org/abs/2509.25146", "authors": ["Richeek Das", "Kostas Daniilidis", "Pratik Chaudhari"], "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "39 pages, 9 figures", "summary": "This paper develops a mathematical argument and algorithms for building\nrepresentations of data from event-based cameras, that we call Fast Feature\nField ($\\text{F}^3$). We learn this representation by predicting future events\nfrom past events and show that it preserves scene structure and motion\ninformation. $\\text{F}^3$ exploits the sparsity of event data and is robust to\nnoise and variations in event rates. It can be computed efficiently using ideas\nfrom multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and\n440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous\nspatiotemporal volume as a multi-channel image, enabling a range of downstream\ntasks. We obtain state-of-the-art performance on optical flow estimation,\nsemantic segmentation, and monocular metric depth estimation, on data from\nthree robotic platforms (a car, a quadruped robot and a flying platform),\nacross different lighting conditions (daytime, nighttime), environments\n(indoors, outdoors, urban, as well as off-road) and dynamic vision sensors\n(resolutions and event rates). Our implementations can predict these tasks at\n25-75 Hz at HD resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Fast Feature Field\uff08F3\uff09\uff0c\u4e00\u79cd\u4ece\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u6784\u5efa\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u6765\u4fdd\u7559\u573a\u666f\u7ed3\u6784\u548c\u8fd0\u52a8\u4fe1\u606f\u3002F3\u9ad8\u6548\u3001\u9c81\u68d2\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u5177\u6709\u7a00\u758f\u6027\u548c\u566a\u58f0\u654f\u611f\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u8868\u793a\u65b9\u6cd5\u6765\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "F3\u7ed3\u5408\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801\u548c\u6df1\u5ea6\u96c6\u5408\u6280\u672f\uff0c\u5c06\u4e8b\u4ef6\u6570\u636e\u8868\u793a\u4e3a\u8fde\u7eed\u65f6\u7a7a\u4f53\u79ef\u7684\u591a\u901a\u9053\u56fe\u50cf\u3002", "result": "F3\u5728\u5149\u6d41\u4f30\u8ba1\u3001\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u73af\u5883\u548c\u4f20\u611f\u5668\u3002", "conclusion": "F3\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\u3002"}}
