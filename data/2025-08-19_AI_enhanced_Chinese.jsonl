{"id": "2508.11640", "pdf": "https://arxiv.org/pdf/2508.11640", "abs": "https://arxiv.org/abs/2508.11640", "authors": ["Danny Scott", "William LaForest", "Hritom Das", "Ioannis Polykretis", "Catherine D. Schuman", "Charles Rizzo", "James Plank", "Sai Swaminathan"], "title": "Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": "International Conference on Neuromorphic Systems (ICONS) 2025 9\n  pages, 7 images", "summary": "The deployment of dense, low-cost sensors is critical for realizing\nubiquitous smart environments. However, existing sensing solutions struggle\nwith the energy, scalability, and reliability trade-offs imposed by battery\nmaintenance, wireless transmission overhead, and data processing complexity. In\nthis work, we present Vibe2Spike, a novel battery-free, wireless sensing\nframework that enables vibration-based activity recognition using visible light\ncommunication (VLC) and spiking neural networks (SNNs). Our system uses\nultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and\nan LED, which harvest vibration energy and emit sparse visible light spikes\nwithout requiring batteries or RF radios. These optical spikes are captured by\nevent cameras and classified using optimized SNN models evolved via the EONS\nframework. We evaluate Vibe2Spike across five device classes, achieving 94.9\\%\naverage classification fitness while analyzing the latency-accuracy trade-offs\nof different temporal binning strategies. Vibe2Spike demonstrates a scalable,\nand energy-efficient approach for enabling intelligent environments in a\nbatteryless manner.", "AI": {"tldr": "Vibe2Spike\u662f\u4e00\u79cd\u65e0\u7535\u6c60\u3001\u65e0\u7ebf\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u53ef\u89c1\u5149\u901a\u4fe1\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u632f\u52a8\u6d3b\u52a8\u8bc6\u522b\uff0c\u5177\u6709\u9ad8\u80fd\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7a20\u5bc6\u3001\u4f4e\u6210\u672c\u4f20\u611f\u5668\u7684\u90e8\u7f72\u5bf9\u5b9e\u73b0\u65e0\u5904\u4e0d\u5728\u7684\u667a\u80fd\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u5728\u80fd\u6e90\u3001\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u4f7f\u7528\u4ec5\u7531\u538b\u7535\u76d8\u3001\u9f50\u7eb3\u4e8c\u6781\u7ba1\u548cLED\u7ec4\u6210\u7684\u8d85\u4f4e\u6210\u672c\u6807\u7b7e\uff0c\u901a\u8fc7\u632f\u52a8\u80fd\u91cf\u9a71\u52a8LED\u53d1\u5c04\u7a00\u758f\u53ef\u89c1\u5149\u8109\u51b2\uff0c\u4e8b\u4ef6\u76f8\u673a\u6355\u6349\u5e76\u7531\u4f18\u5316\u7684SNN\u6a21\u578b\u5206\u7c7b\u3002", "result": "\u5728\u4e94\u7c7b\u8bbe\u5907\u4e0a\u7684\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523094.9%\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65f6\u95f4\u5206\u7bb1\u7b56\u7565\u7684\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "Vibe2Spike\u5c55\u793a\u4e86\u65e0\u7535\u6c60\u73af\u5883\u4e0b\u5b9e\u73b0\u667a\u80fd\u73af\u5883\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u80fd\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.11654", "pdf": "https://arxiv.org/pdf/2508.11654", "abs": "https://arxiv.org/abs/2508.11654", "authors": ["Yang Zhao", "Tao Wang", "Said Elhadi"], "title": "Data-driven RF Tomography via Cross-modal Sensing and Continual Learning", "categories": ["eess.SP", "cs.CV"], "comment": "6 pages, 4 figures, to be published in IEEE AVSS Conference", "summary": "Data-driven radio frequency (RF) tomography has demonstrated significant\npotential for underground target detection, due to the penetrative nature of RF\nsignals through soil. However, it is still challenging to achieve accurate and\nrobust performance in dynamic environments. In this work, we propose a\ndata-driven radio frequency tomography (DRIFT) framework with the following key\ncomponents to reconstruct cross section images of underground root tubers, even\nwith significant changes in RF signals. First, we design a cross-modal sensing\nsystem with RF and visual sensors, and propose to train an RF tomography deep\nneural network (DNN) model following the cross-modal learning approach. Then we\npropose to apply continual learning to automatically update the DNN model, once\nenvironment changes are detected in a dynamic environment. Experimental results\nshow that our approach achieves an average equivalent diameter error of 2.29\ncm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and\ndataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5c04\u9891\u65ad\u5c42\u626b\u63cf\uff08DRIFT\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5c04\u9891\u4fe1\u53f7\u7a7f\u900f\u571f\u58e4\u7684\u7279\u6027\u4f7f\u5176\u5728\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u5408\u5c04\u9891\u4e0e\u89c6\u89c9\u4f20\u611f\u5668\u7684\u8de8\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u6301\u7eed\u5b66\u4e60\u5728\u73af\u5883\u53d8\u5316\u65f6\u81ea\u52a8\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u7b49\u6548\u76f4\u5f84\u8bef\u5dee\u4e3a2.29\u5398\u7c73\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u4e8623.2%\u3002", "conclusion": "\u63d0\u51fa\u7684DRIFT\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.11656", "pdf": "https://arxiv.org/pdf/2508.11656", "abs": "https://arxiv.org/abs/2508.11656", "authors": ["Ridma Jayasundara", "Ishan Fernando", "Adeepa Fernando", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "Inductive transfer learning from regression to classification in ECG analysis", "categories": ["eess.SP", "cs.LG", "I.2.6; I.5.1; I.5.4; I.2.1; J.3"], "comment": "This manuscript is 15 pages with 4 tables and 5 figures. The\n  manuscript is under review at Nature Scientific Reports", "summary": "Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,\naccounting for over 30% of global deaths according to the World Health\nOrganization (WHO). Importantly, one-third of these deaths are preventable with\ntimely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive\nmethod for recording the electrical activity of the heart, is crucial for\ndiagnosing CVDs. However, privacy concerns surrounding the use of patient ECG\ndata in research have spurred interest in synthetic data, which preserves the\nstatistical properties of real data without compromising patient\nconfidentiality. This study explores the potential of synthetic ECG data for\ntraining deep learning models from regression to classification tasks and\nevaluates the feasibility of transfer learning to enhance classification\nperformance on real ECG data. We experimented with popular deep learning models\nto predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,\nQT interval, and QRS complex-using separate regression models. Subsequently, we\nleveraged these regression models for transfer learning to perform 5-class ECG\nsignal classification. Our experiments systematically investigate whether\ntransfer learning from regression to classification is viable, enabling better\nutilization of diverse open-access and synthetic ECG datasets. Our findings\ndemonstrate that transfer learning from regression to classification improves\nclassification performance, highlighting its potential to maximize the utility\nof available data and advance deep learning applications in this domain.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5408\u6210\u5fc3\u7535\u56fe\uff08ECG\uff09\u6570\u636e\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u5230\u5206\u7c7b\u7684\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u4e86\u771f\u5b9eECG\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVDs\uff09\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u5176\u8bca\u65ad\u4f9d\u8d56\u5fc3\u7535\u56fe\uff08ECG\uff09\uff0c\u4f46\u60a3\u8005\u6570\u636e\u9690\u79c1\u95ee\u9898\u4fc3\u4f7f\u5bf9\u5408\u6210\u6570\u636e\u7684\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u56db\u4e2a\u5173\u952e\u5fc3\u810f\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5c06\u56de\u5f52\u6a21\u578b\u5e94\u7528\u4e8e5\u7c7bECG\u4fe1\u53f7\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4ece\u56de\u5f52\u5230\u5206\u7c7b\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u53ef\u5145\u5206\u5229\u7528\u5408\u6210\u548c\u5f00\u653eECG\u6570\u636e\uff0c\u63a8\u52a8\u6df1\u5ea6\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2508.11657", "pdf": "https://arxiv.org/pdf/2508.11657", "abs": "https://arxiv.org/abs/2508.11657", "authors": ["Yuanhao Li", "Badong Chen", "Wenjun Bai", "Yasuharu Koike", "Okito Yamashita"], "title": "Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Objective: Sparse Bayesian learning provides an effective scheme to solve the\nhigh-dimensional problem in brain signal decoding. However, traditional\nassumptions regarding data distributions such as Gaussian and binomial are\npotentially inadequate to characterize the noisy signals of brain activity.\nHence, this study aims to propose a robust sparse Bayesian learning framework\nto address noisy highdimensional brain activity decoding. Methods: Motivated by\nthe commendable robustness of the minimum error entropy (MEE) criterion for\nhandling complex data distributions, we proposed an MEE-based likelihood\nfunction to facilitate the accurate inference of sparse Bayesian learning in\nanalyzing noisy brain datasets. Results: Our proposed approach was evaluated\nusing two high-dimensional brain decoding tasks in regression and\nclassification contexts, respectively. The experimental results showed that,\nour approach can realize superior decoding metrics and physiological patterns\nthan the conventional and state-of-the-art methods. Conclusion: Utilizing the\nproposed MEE-based likelihood model, sparse Bayesian learning is empowered to\nsimultaneously address the challenges of noise and high dimensionality in the\nbrain decoding task. Significance: This work provides a powerful tool to\nrealize robust brain decoding, advancing biomedical engineering applications\nsuch as brain-computer interface.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u8bef\u5dee\u71b5\uff08MEE\uff09\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u5927\u8111\u4fe1\u53f7\u89e3\u7801\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u65af\u548c\u4e8c\u9879\u5206\u5e03\u5047\u8bbe\u53ef\u80fd\u4e0d\u8db3\u4ee5\u63cf\u8ff0\u5927\u8111\u6d3b\u52a8\u4fe1\u53f7\u4e2d\u7684\u566a\u58f0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6700\u5c0f\u8bef\u5dee\u71b5\uff08MEE\uff09\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u4ee5\u63d0\u9ad8\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u5728\u566a\u58f0\u6570\u636e\u4e2d\u7684\u63a8\u65ad\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u9ad8\u7ef4\u5ea6\u5927\u8111\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MEE\u6846\u67b6\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u5927\u8111\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u566a\u58f0\u548c\u9ad8\u7ef4\u5ea6\u95ee\u9898\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2508.11759", "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u81ea\u4e3b\u673a\u5668\u4eba\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u6539\u5584\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u8ba4\u77e5\u4ee3\u7406\u3001\u7269\u7406\u673a\u5668\u4eba\u548cLLMs\u7684\u65b9\u6cd5", "motivation": "\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u4eba\u7c7b\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u7684\u613f\u666f\uff0c\u5f25\u8865\u4f20\u7edf\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5b66\u4e60\uff08ITL\uff09\u7cfb\u7edf\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u4e0d\u8db3", "method": "\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u4ee3\u7406\u3001\u7269\u7406\u673a\u5668\u4eba\u548cLLMs\uff0c\u6784\u5efaAI\u7cfb\u7edf\uff0c\u5e76\u9488\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u4e09\u4e2a\u5177\u4f53\u6311\u6218\uff0c\u4f7f\u7528ChatGPT\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u80fd\u7684\u65b9\u6cd5\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86LLMs\u5728\u673a\u5668\u4eba\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5c06\u6982\u5ff5\u9a8c\u8bc1\u53d1\u5c55\u4e3a\u53ef\u64cd\u4f5c\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u5b9e\u73b0LLM\u8f85\u52a9\u7684\u673a\u5668\u4eba\u8bed\u8a00\u534f\u4f5c"}}
{"id": "2508.11658", "pdf": "https://arxiv.org/pdf/2508.11658", "abs": "https://arxiv.org/abs/2508.11658", "authors": ["Honggui Li", "Zhengyang Zhang", "Dingtai Li", "Sinan Chen", "Nahid Md Lokman Hossain", "Xinfeng Xu", "Yuting Feng", "Hantao Lu", "Yinlu Qin", "Ruobing Wang", "Maria Trocan", "Dimitri Galayko", "Amara Amara", "Mohamad Sawan"], "title": "CECGSR: Circular ECG Super-Resolution", "categories": ["eess.SP"], "comment": null, "summary": "The electrocardiogram (ECG) plays a crucial role in the diagnosis and\ntreatment of various cardiac diseases. ECG signals suffer from low-resolution\n(LR) due to the use of convenient acquisition devices, as well as internal and\nexternal noises and artifacts. Classical ECG super-resolution (ECGSR) methods\nadopt an open-loop architecture that converts LR ECG signals to\nsuper-resolution (SR) ones. According to the theory of automatic control, a\nclosed-loop framework exhibits superior dynamic and static performance compared\nwith its open-loop counterpart. This paper proposes a closed-loop approach,\ntermed circular ECGSR (CECGSR), which models the degradation process from SR\nECG signals to LR ones. The negative feedback mechanism of the closed-loop\nsystem is based on the differences between the LR ECG signals. A mathematical\nloop equation is constructed to characterize the closed-loop infrastructure.\nThe Taylor series expansion is employed to demonstrate the near-zero\nsteady-state error of the proposed method. A Plug-and-Play strategy is\nconsidered to establish the SR unit of the proposed architecture, leveraging\nany existing advanced open-loop ECGSR methods. Simulation experiments on both\nnoiseless and noisy subsets of the PTB-XL datasets demonstrate that the\nproposed CECGSR outperforms state-of-the-art open-loop ECGSR algorithms in the\nreconstruction performance of ECG signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u95ed\u73af\u6846\u67b6\u7684\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08CECGSR\uff09\uff0c\u901a\u8fc7\u5efa\u6a21\u9ad8\u5206\u8fa8\u7387\u5230\u4f4e\u5206\u8fa8\u7387\u4fe1\u53f7\u7684\u964d\u7ea7\u8fc7\u7a0b\uff0c\u5229\u7528\u8d1f\u53cd\u9988\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCECGSR\u5728PTB-XL\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u73af\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u5e38\u56e0\u8bbe\u5907\u9650\u5236\u548c\u566a\u58f0\u5e72\u6270\u800c\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u4f20\u7edf\u5f00\u73af\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u95ed\u73af\u6846\u67b6\u63d0\u5347ECG\u4fe1\u53f7\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u95ed\u73af\u65b9\u6cd5CECGSR\uff0c\u5efa\u6a21\u9ad8\u5206\u8fa8\u7387\u5230\u4f4e\u5206\u8fa8\u7387\u4fe1\u53f7\u7684\u964d\u7ea7\u8fc7\u7a0b\uff0c\u5229\u7528\u8d1f\u53cd\u9988\u673a\u5236\u548c\u6570\u5b66\u73af\u8def\u65b9\u7a0b\u4f18\u5316\u6027\u80fd\u3002\u91c7\u7528\u5373\u63d2\u5373\u7528\u7b56\u7565\u6574\u5408\u73b0\u6709\u5f00\u73af\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u7684\u65e0\u566a\u548c\u542b\u566a\u5b50\u96c6\u4e0a\uff0cCECGSR\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u73af\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u95ed\u73af\u6846\u67b6CECGSR\u663e\u8457\u63d0\u5347\u4e86ECG\u4fe1\u53f7\u8d85\u5206\u8fa8\u7387\u7684\u6548\u679c\uff0c\u4e3a\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2508.11802", "pdf": "https://arxiv.org/pdf/2508.11802", "abs": "https://arxiv.org/abs/2508.11802", "authors": ["Luigi Penco", "Beomyeong Park", "Stefan Fasano", "Nehar Poddar", "Stephen McCrory", "Nicholas Kitchel", "Tomasz Bialek", "Dexton Anderson", "Duncan Calvert", "Robert Griffin"], "title": "Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots", "categories": ["cs.RO"], "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots\n  (Humanoids)", "summary": "Achieving seamless synchronization between user and robot motion in\nteleoperation, particularly during high-speed tasks, remains a significant\nchallenge. In this work, we propose a novel approach for transferring stepping\nmotions from the user to the robot in real-time. Instead of directly\nreplicating user foot poses, we retarget user steps to robot footstep\nlocations, allowing the robot to utilize its own dynamics for locomotion,\nensuring better balance and stability. Our method anticipates user footsteps to\nminimize delays between when the user initiates and completes a step and when\nthe robot does it. The step estimates are continuously adapted to converge with\nthe measured user references. Additionally, the system autonomously adjusts the\nrobot's steps to account for its surrounding terrain, overcoming challenges\nposed by environmental mismatches between the user's flat-ground setup and the\nrobot's uneven terrain. Experimental results on the humanoid robot Nadia\ndemonstrate the effectiveness of the proposed system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u6b65\u6001\u540c\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u548c\u8c03\u6574\u7528\u6237\u6b65\u6001\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u590d\u6742\u5730\u5f62\u4e0a\u4fdd\u6301\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u5ea6\u4efb\u52a1\u4e2d\u7528\u6237\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u540c\u6b65\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5730\u5f62\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u91cd\u5b9a\u5411\u7528\u6237\u6b65\u6001\u5230\u673a\u5668\u4eba\u811a\u6b65\u4f4d\u7f6e\uff0c\u9884\u6d4b\u811a\u6b65\u5e76\u52a8\u6001\u8c03\u6574\u4ee5\u9002\u5e94\u5730\u5f62\u3002", "result": "\u5728Nadia\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u5e73\u8861\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.11663", "pdf": "https://arxiv.org/pdf/2508.11663", "abs": "https://arxiv.org/abs/2508.11663", "authors": ["Guangli Li", "Canbiao Wu", "Zhen Liang"], "title": "Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Affective computing is a rapidly developing interdisciplinary research\ndirection in the field of brain-computer interface. In recent years, the\nintroduction of deep learning technology has greatly promoted the development\nof the field of emotion recognition. However, due to physiological differences\nbetween subjects, as well as the variations in experimental environments and\nequipment, cross-corpus emotion recognition faces serious challenges,\nespecially for samples near the decision boundary. To solve the above problems,\nwe propose an optimization method based on domain adversarial transfer learning\nto fine-grained alignment of affective features, named Maximum classifier\ndiscrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a\ndual adversarial classifier (Ada classifier and RMS classifier), and apply a\nthree-stage adversarial training to maximize classification discrepancy and\nminimize feature distribution to align controversy samples near the decision\nboundary. In the process of domain adversarial training, the two classifiers\nalso maintain an adversarial relationship, ultimately enabling precise\ncross-corpus feature alignment. In addition, the introduction of pairwise\nlearning transforms the classification problem of samples into a similarity\nproblem between samples, alleviating the influence of label noise. We conducted\nsystematic experimental evaluation of the model using publicly available SEED,\nSEED-IV and SEED-V databases. The results show that the McdPL model is superior\nto other baseline models in the cross-corpus emotion recognition task, and the\naverage accuracy improvements of 4.76\\% and 3.97\\%, respectively. Our work\nprovides a promising solution for emotion recognition cross-corpus. The source\ncode is available at https://github.com/WuCB-BCI/Mcd_PL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\uff08McdPL\uff09\uff0c\u901a\u8fc7\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\u548c\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\uff0c\u89e3\u51b3\u8de8\u6570\u636e\u5e93\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u53d7\u8bd5\u8005\u751f\u7406\u5dee\u5f02\u53ca\u5b9e\u9a8c\u73af\u5883\u548c\u8bbe\u5907\u7684\u53d8\u5316\uff0c\u8de8\u6570\u636e\u5e93\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u8fb9\u754c\u6837\u672c\u7684\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\uff08Ada\u548cRMS\uff09\u548c\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\uff0c\u7ed3\u5408\u6210\u5bf9\u5b66\u4e60\uff0c\u6700\u5927\u5316\u5206\u7c7b\u5dee\u5f02\u5e76\u6700\u5c0f\u5316\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728SEED\u3001SEED-IV\u548cSEED-V\u6570\u636e\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMcdPL\u6a21\u578b\u5728\u8de8\u6570\u636e\u5e93\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53474.76%\u548c3.97%\u3002", "conclusion": "McdPL\u4e3a\u8de8\u6570\u636e\u5e93\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
