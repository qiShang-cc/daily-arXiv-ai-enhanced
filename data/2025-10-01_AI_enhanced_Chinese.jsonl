{"id": "2509.25277", "pdf": "https://arxiv.org/pdf/2509.25277", "abs": "https://arxiv.org/abs/2509.25277", "authors": ["Kumar Sai Bondada", "Hiten Kothari", "Yibin Liang", "Daniel J. Jakubisin", "R. Michael Buehrer"], "title": "Experimental Demonstration of Robust Distributed Wireless Clock Synchronization", "categories": ["eess.SP"], "comment": null, "summary": "Distributed wireless clock synchronization is essential for aligning the\nclocks of distributed transceivers in support of joint transmission and\nreception techniques. One recently explored method involves synchronizing\ndistributed transceivers using a two-tone waveform, where the tones are\nseparated in frequency by a clock (frequency) reference signal. Prior research\nhas demonstrated frequency accuracy better than 1 Hz; however, this approach\nremains vulnerable to both intentional and unintentional interference. In this\ndemonstration, we present a robust, frequency-hopped two-tone waveform that\nenables transceivers to extract the reference signal without prior knowledge of\nthe exact frequency at which the tones are transmitted.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u8df3\u53d8\u7684\u53cc\u97f3\u6ce2\u5f62\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u65e0\u7ebf\u65f6\u949f\u540c\u6b65\uff0c\u4ee5\u62b5\u6297\u6709\u610f\u548c\u65e0\u610f\u7684\u5e72\u6270\u3002", "motivation": "\u5206\u5e03\u5f0f\u65e0\u7ebf\u65f6\u949f\u540c\u6b65\u5bf9\u8054\u5408\u4f20\u8f93\u548c\u63a5\u6536\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6613\u53d7\u5e72\u6270\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u9891\u7387\u8df3\u53d8\u7684\u53cc\u97f3\u6ce2\u5f62\uff0c\u63a5\u6536\u5668\u65e0\u9700\u9884\u5148\u77e5\u9053\u4f20\u8f93\u9891\u7387\u5373\u53ef\u63d0\u53d6\u53c2\u8003\u4fe1\u53f7\u3002", "result": "\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6297\u51fb\u5e72\u6270\u80fd\u529b\u3002", "conclusion": "\u9891\u7387\u8df3\u53d8\u7684\u53cc\u97f3\u6ce2\u5f62\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u65f6\u949f\u540c\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25385", "pdf": "https://arxiv.org/pdf/2509.25385", "abs": "https://arxiv.org/abs/2509.25385", "authors": ["Yanan Du", "Sai Xu", "Jagmohan Chauhan"], "title": "A Graph-based Hybrid Beamforming Framework for MIMO Cell-Free ISAC Networks", "categories": ["eess.SP"], "comment": null, "summary": "This paper develops a graph-based hybrid beamforming framework for\nmultiple-input multiple-output (MIMO) cell-free integrated sensing and\ncommunication (ISAC) networks. Specifically, we construct a novel MIMO\ncell-free ISAC network model. In this model, multiple dual-function base\nstation (BS) transmitters employ distributed hybrid beamforming to enable\nsimultaneous communication and sensing, while maintaining physical separation\nbetween the transmitters and the radar receiver. Building on this model, we\nformulate a multi-objective optimization problem under a power constraint to\njointly improve communication and sensing performance. To solve it, the problem\nis first reformulated as a single-objective optimization problem. Then, a\ngraph-based method composed of multiple graph neural networks (GNNs) is\ndeveloped to realize hybrid beamforming with either perfect or imperfect\nchannel state information. Once trained, the neural network model can be\ndeployed distributively across BSs, enabling fast and efficient inference. To\nfurther reduce inference latency, a custom field-programmable gate array\n(FPGA)-based accelerator is developed. Numerical simulations validate the\ncommunication and sensing capabilities of the proposed optimization approach,\nwhile experimental evaluations demonstrate remarkable performance gains of\nFPGA-based acceleration in GNN inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u6846\u67b6\uff0c\u7528\u4e8eMIMO\u65e0\u5c0f\u533aISAC\u7f51\u7edc\uff0c\u5b9e\u73b0\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u8054\u5408\u4f18\u5316\uff0c\u5e76\u901a\u8fc7GNN\u548cFPGA\u52a0\u901f\u5668\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MIMO\u65e0\u5c0f\u533a\u7f51\u7edc\u4e2d\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u8d44\u6e90\u5206\u914d\u548c\u6027\u80fd\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u8003\u8651\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5206\u5e03\u5f0f\u9700\u6c42\u4e0e\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u6784\u5efaMIMO\u65e0\u5c0f\u533aISAC\u7f51\u7edc\u6a21\u578b\uff0c\u63d0\u51fa\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u5e76\u901a\u8fc7GNN\u5b9e\u73b0\u6df7\u5408\u6ce2\u675f\u6210\u5f62\uff0c\u5f00\u53d1FPGA\u52a0\u901f\u5668\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aFPGA\u52a0\u901f\u663e\u8457\u63d0\u5347\u4e86GNN\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5206\u5e03\u5f0f\u90e8\u7f72\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3aISAC\u7f51\u7edc\u7684\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25497", "pdf": "https://arxiv.org/pdf/2509.25497", "abs": "https://arxiv.org/abs/2509.25497", "authors": ["Duc Tung Bui", "Le-Nam Tran"], "title": "A closed-loop $2\\times4$ downlink MIMO Framework for 5G New Radio using OpenAirInterface", "categories": ["eess.SP"], "comment": "The paper has been accepted by IEEE VTC2025Spring", "summary": "We present the first-of-a-kind closed-loop $2\\times4$ MIMO implementation for\nthe downlink of 5G Open RAN using OpenAirInterface (OAI), which is capable of\ntransmitting up to two transmission layers. Our implementation is a fully\nfunctional 5G New Radio (5G NR) system, including the 5G Core Network (5G CN),\n5G Radio Access Network (5G RAN), as well as 5G NR User Equipment (UEs). This\nserves as a foundational framework for further advancements in the context of\nemerging Open RAN (O-RAN) development. A key feature of our implementation is\nthe enhanced Channel State Information (CSI) reporting procedure at the UE,\nwhich includes Rank Indicator (RI), Precoding Matrix Indicator (PMI), and\nChannel Quality Indicator (CQI). It is adjusted for the extended configuration\nto maximize data rates. To demonstrate the performance of our implementation,\nwe measure the downlink data rates using $\\textit{iperf3}$ in two scenarios:\n(i) fixed channels to assess two-layer data transmission and (ii)\n$\\textit{Rice1}$ channels for general transmission analysis. The obtained\nsimulation results demonstrate that, compared to the existing $2\\times2$ MIMO\nconfiguration in the OAI, our implementation improves the data rates in almost\nall scenarios, especially at the high Signal-to-Noise-Ratios (SNRs).", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eOpenAirInterface\uff08OAI\uff09\u7684\u95ed\u73af2\u00d74 MIMO\u7cfb\u7edf\uff0c\u7528\u4e8e5G Open RAN\u4e0b\u884c\u94fe\u8def\uff0c\u652f\u6301\u6700\u591a\u4e24\u5c42\u4f20\u8f93\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684CSI\u62a5\u544a\u63d0\u9ad8\u4e86\u6570\u636e\u901f\u7387\u3002", "motivation": "\u4e3a5G Open RAN\u63d0\u4f9b\u529f\u80fd\u6027\u5b9e\u73b0\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684O-RAN\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u91c7\u7528\u5b8c\u6574\u76845G NR\u7cfb\u7edf\uff08\u5305\u62ec\u6838\u5fc3\u7f51\u3001\u65e0\u7ebf\u63a5\u5165\u7f51\u548c\u7528\u6237\u8bbe\u5907\uff09\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u7684CSI\u62a5\u544a\uff08RI\u3001PMI\u3001CQI\uff09\u4f18\u5316\u914d\u7f6e\u3002", "result": "\u5728\u56fa\u5b9a\u548cRice1\u4fe1\u9053\u4e0b\u6d4b\u8bd5\uff0c\u663e\u793a\u6570\u636e\u901f\u7387\u4f18\u4e8e\u73b0\u6709\u76842\u00d72 MIMO\u914d\u7f6e\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u4e3aOpen RAN\u73af\u5883\u4e0b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u5c55\u73b0\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.25512", "pdf": "https://arxiv.org/pdf/2509.25512", "abs": "https://arxiv.org/abs/2509.25512", "authors": ["Duc Tung Bui", "Le-Nam Tran"], "title": "An Implementation of Multi-User MIMO Downlink for O-RAN 5G New Radio using OpenAirInterface", "categories": ["eess.SP"], "comment": "The paper has been accepted by IEEE NFV-SDN2025", "summary": "We present the first implementation of a Multi-User Multiple-Input\nMultiple-Output (MU-MIMO) transmission scheme on the Physical Downlink Shared\nChannel (PDSCH) for 5G Open Radio Access Network (O-RAN) based on\nOpenAirInterface (OAI). Our implementation features a fully functional\nO-RAN-compliant 5G New Radio (5G NR) system, including a 5G Core Network (5G\nCN), a refined 5G RAN, which is split into a Centre Unit (CU) and an\nDistributed Unit (DU), and 5G NR User Equipment (UEs). This implementation\ndemonstrates MU-MIMO performance in the downlink while showcasing the\ndisaggregation capabilities of O-RAN. Specifically, the Base Station (i.e. gNB)\nin our setup is capable of serving two UEs simultaneously over the same\ndownlink Resource Block (RBs). User scheduling is performed based on the\nPrecoding Matrix Indicators (PMIs) reported by the UEs according to the NR\nChannel State Information (CSI) reporting procedure. The system throughput\nperformance is evaluated using $\\textit{iperf}$. The obtained results via\nsimulation and testbed experiments demonstrate that the MU-MIMO scheme achieves\nsignificant downlink throughput gains, particularly in the high\nSignal-to-Noise-Ratio (SNR) regime, while keeping the Block Error Rate (BLER)\nbelow the required threshold of $10^{-1}$ for both UEs.", "AI": {"tldr": "\u9996\u6b21\u57285G O-RAN\u4e2d\u5b9e\u73b0\u4e86PDSCH\u4e0a\u7684MU-MIMO\u4f20\u8f93\u65b9\u6848\uff0c\u5c55\u793a\u4e86O-RAN\u7684\u89e3\u8026\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u4e0e\u6d4b\u8bd5\u8bc1\u660e\u5176\u5728\u9ad8\u8d28\u91cf\u4fe1\u9053\u4e0b\u7684\u663e\u8457\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5c55\u793aO-RAN\u67b6\u6784\u4e0bMU-MIMO\u6280\u672f\u7684\u53ef\u884c\u6027\u4e0e\u6027\u80fd\uff0c\u7279\u522b\u662f\u57285G NR\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u591a\u7528\u6237\u540c\u65f6\u4f20\u8f93\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7OAI\u5b9e\u73b0\u76845G NR\u7cfb\u7edf\uff0c\u5305\u62ec5G CN\u3001CU/DU\u5206\u79bb\u7684RAN\u548cUE\u3002\u7528\u6237\u8c03\u5ea6\u57fa\u4e8ePMI\u548cNR CSI\u62a5\u544a\uff0c\u6027\u80fd\u8bc4\u4f30\u4f7f\u7528iperf\u5de5\u5177\u6d4b\u8bd5\u541e\u5410\u91cf\u548cBLER\u3002", "result": "MU-MIMO\u5728\u9ad8SNR\u4e0b\u663e\u8457\u63d0\u5347\u4e0b\u884c\u541e\u5410\u91cf\uff0c\u4e14BLER\u4fdd\u6301\u572810^-1\u4ee5\u4e0b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u5b9e\u73b0\u4e3aO-RAN\u67b6\u6784\u4e2d\u7684MU-MIMO\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u9645\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u57285G\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u6027\u80fd\u548c\u89e3\u8026\u4f18\u52bf\u3002"}}
{"id": "2509.25200", "pdf": "https://arxiv.org/pdf/2509.25200", "abs": "https://arxiv.org/abs/2509.25200", "authors": ["Christian Arzate Cruz", "Edwin C. Montiel-Vazquez", "Chikara Maeda", "Randy Gomez"], "title": "When and How to Express Empathy in Human-Robot Interaction Scenarios", "categories": ["cs.RO"], "comment": null, "summary": "Incorporating empathetic behavior into robots can improve their social\neffectiveness and interaction quality. In this paper, we present whEE (when and\nhow to express empathy), a framework that enables social robots to detect when\nempathy is needed and generate appropriate responses. Using large language\nmodels, whEE identifies key behavioral empathy cues in human interactions. We\nevaluate it in human-robot interaction scenarios with our social robot, Haru.\nResults show that whEE effectively identifies and responds to empathy cues,\nproviding valuable insights for designing social robots capable of adaptively\nmodulating their empathy levels across various interaction contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fawhEE\u6846\u67b6\uff0c\u5e2e\u52a9\u793e\u4ea4\u673a\u5668\u4eba\u8bc6\u522b\u5e76\u751f\u6210\u5171\u60c5\u884c\u4e3a\uff0c\u63d0\u5347\u4ea4\u4e92\u8d28\u91cf\u3002", "motivation": "\u901a\u8fc7\u8d4b\u4e88\u673a\u5668\u4eba\u5171\u60c5\u80fd\u529b\uff0c\u63d0\u5347\u5176\u793e\u4ea4\u6709\u6548\u6027\u548c\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u5171\u60c5\u884c\u4e3a\u7ebf\u7d22\uff0c\u8bbe\u8ba1whEE\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660ewhEE\u80fd\u6709\u6548\u8bc6\u522b\u548c\u54cd\u5e94\u5171\u60c5\u7ebf\u7d22\uff0c\u9002\u5e94\u4e0d\u540c\u4ea4\u4e92\u573a\u666f\u3002", "conclusion": "whEE\u4e3a\u8bbe\u8ba1\u5177\u5907\u81ea\u9002\u5e94\u5171\u60c5\u80fd\u529b\u7684\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2509.25557", "pdf": "https://arxiv.org/pdf/2509.25557", "abs": "https://arxiv.org/abs/2509.25557", "authors": ["Soroush Mesforush", "Murat Bayraktar", "Nuria Gonz\u00e1lez-Prelcic"], "title": "Joint UE positioning and distributed sensing in the upper mid-band exploiting virtual apertures", "categories": ["eess.SP"], "comment": null, "summary": "Networks exploiting distributed integrated sensing and communication (DISAC)\nnodes can provide enhanced localization and sensing performance, further\nemphasized when operating with large arrays and bandwidths available in the\nupper mid-band (also known as FR3). In this paper, we consider a DISAC system\noperating at FR3 where a single base station (BS) acts as the transmitter and\nseveral vehicular user equipments (UEs) act as the receivers. We tackle the\ndesign of the signal processing chain at the UE side to enable joint UE\npositioning and target localization. The system model exploits a\nmultiple-input-multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) waveform, and incorporates practical effects such as inter-node\ntiming offsets (TOs), extended targets, dense multipath, and realistic uniform\nplanar arrays (UPAs) at both ends. The proposed design includes a multipath\nestimation stage at each UE, clutter removal, a novel clustering and\nassociation scheme, and a final joint estimator of UE positions and target\nlocations. The estimator solves a weighted least squares (WLS) problem to\njointly compute clock offsets and localize UEs and targets. Numerical results\nconsidering two UEs and two targets show that for 80\\% of the cases the target\nlocalization error is below 32cm, while the UE positioning error is below 44cm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728FR3\u9891\u6bb5\u4e0b\u8fd0\u884c\u7684DISAC\u7cfb\u7edf\uff0c\u901a\u8fc7UE\u7aef\u7684\u4fe1\u53f7\u5904\u7406\u94fe\u5b9e\u73b0\u8054\u5408\u5b9a\u4f4d\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u8bef\u5dee\u8f83\u4f4e\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5206\u5e03\u5f0f\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u7f51\u7edc\u4e2d\u63d0\u5347\u5b9a\u4f4d\u548c\u4f20\u611f\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u9635\u5217\u548c\u5bbd\u5e26\u6761\u4ef6\u4e0b\u3002", "method": "\u91c7\u7528MIMO-OFDM\u6ce2\u5f62\uff0c\u8bbe\u8ba1\u4e86\u5305\u62ec\u591a\u5f84\u4f30\u8ba1\u3001\u6742\u6ce2\u6d88\u9664\u3001\u805a\u7c7b\u5173\u8054\u548c\u8054\u5408\u4f30\u8ba1\u7684\u4fe1\u53f7\u5904\u7406\u94fe\u3002", "result": "\u572880%\u7684\u60c5\u51b5\u4e0b\uff0c\u76ee\u6807\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e32cm\uff0cUE\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e44cm\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728FR3\u9891\u6bb5\u4e0b\u80fd\u6709\u6548\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8054\u5408\u5b9a\u4f4d\u548c\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2509.25249", "pdf": "https://arxiv.org/pdf/2509.25249", "abs": "https://arxiv.org/abs/2509.25249", "authors": ["Guancheng Chen", "Sheng Yang", "Tong Zhan", "Jian Wang"], "title": "BEV-VLM: Trajectory Planning via Unified BEV Abstraction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces BEV-VLM, a novel framework for trajectory planning in\nautonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye\nView (BEV) feature maps as visual inputs. Unlike conventional approaches that\nrely solely on raw visual data such as camera images, our method utilizes\nhighly compressed and informative BEV representations, which are generated by\nfusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with\nHD Maps. This unified BEV-HD Map format provides a geometrically consistent and\nrich scene description, enabling VLMs to perform accurate trajectory planning.\nExperimental results on the nuScenes dataset demonstrate 44.8% improvements in\nplanning accuracy and complete collision avoidance. Our work highlights that\nVLMs can effectively interpret processed visual representations like BEV\nfeatures, expanding their applicability beyond raw images in trajectory\nplanning.", "AI": {"tldr": "BEV-VLM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cBEV\u7279\u5f81\u56fe\uff0c\u663e\u8457\u63d0\u5347\u89c4\u5212\u7cbe\u5ea6\u5e76\u907f\u514d\u78b0\u649e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u539f\u59cb\u89c6\u89c9\u6570\u636e\uff08\u5982\u76f8\u673a\u56fe\u50cf\uff09\uff0cBEV-VLM\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u548cHD\u5730\u56fe\uff0c\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u63d0\u9ad8\u8f68\u8ff9\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528BEV\u7279\u5f81\u56fe\u4f5c\u4e3a\u89c6\u89c9\u8f93\u5165\uff0c\u878d\u5408\u76f8\u673a\u548cLiDAR\u6570\u636e\uff0c\u5e76\u4e0eHD\u5730\u56fe\u5bf9\u9f50\uff0c\u751f\u6210\u9ad8\u538b\u7f29\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u4f9bVLM\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u89c4\u5212\u7cbe\u5ea6\u63d0\u534744.8%\uff0c\u5e76\u5b8c\u5168\u907f\u514d\u78b0\u649e\u3002", "conclusion": "VLMs\u53ef\u4ee5\u6709\u6548\u89e3\u6790BEV\u7b49\u5904\u7406\u8fc7\u7684\u89c6\u89c9\u8868\u793a\uff0c\u6269\u5c55\u5176\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.25656", "pdf": "https://arxiv.org/pdf/2509.25656", "abs": "https://arxiv.org/abs/2509.25656", "authors": ["Yanhua Tan", "Beixiong Zheng", "Yi Fang", "Derrick Wing Kwan Ng", "Rui Zhang", "Jie Xu"], "title": "Rotatable Antenna-Enabled Spectrum Sharing in Cognitive Radio Systems", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "5 pages, 4 figures. Submitted to an lEEE journal for possible\n  publication on September 24, 2025", "summary": "Rotatable antenna (RA) technology has recently drawn significant attention in\nwireless systems owing to its unique ability to exploit additional spatial\ndegrees-of-freedom (DoFs) by dynamically adjusting the three-dimensional (3D)\nboresight direction of each antenna. In this letter, we propose a new\nRA-assisted cognitive radio (CR) system designed to achieve efficient spectrum\nsharing while mitigating interference between primary and secondary\ncommunication links. Specifically, we formulate an optimization problem for the\njoint design of the transmit beamforming and the boresight directions of RAs at\nthe secondary transmitter (ST), aimed at maximizing the received\nsignal-to-interference-plus-noise ratio (SINR) at the secondary receiver (SR),\nwhile satisfying both interference constraint at the primary receiver (PR) and\nthe maximum transmit power constraint at the ST. Although the formulated\nproblem is challenging to solve due to its non-convexity and coupled variables,\nwe develop an efficient algorithm by leveraging alternating optimization (AO)\nand successive convex approximation (SCA) techniques to acquire high-quality\nsolutions. Numerical results demonstrate that the proposed RA-assisted system\nsubstantially outperforms conventional benchmark schemes in spectrum-sharing CR\nsystems, validating RA's capability to simultaneously enhance the communication\nquality at the SR and mitigate interference at the PR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65cb\u8f6c\u5929\u7ebf\u8f85\u52a9\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5929\u7ebf\u7684\u6ce2\u675f\u65b9\u5411\u4ee5\u4f18\u5316\u9891\u8c31\u5171\u4eab\u548c\u964d\u4f4e\u5e72\u6270\u3002", "motivation": "\u65cb\u8f6c\u5929\u7ebf\u6280\u672f\u56e0\u5176\u5229\u7528\u7a7a\u95f4\u81ea\u7531\u5ea6\u52a8\u6001\u8c03\u6574\u6ce2\u675f\u65b9\u5411\u7684\u72ec\u7279\u80fd\u529b\uff0c\u8fd1\u5e74\u6765\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u8bbe\u8ba1\u53d1\u9001\u6ce2\u675f\u6210\u5f62\u548c\u65cb\u8f6c\u5929\u7ebf\u7684\u6ce2\u675f\u65b9\u5411\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u8fde\u7eed\u51f8\u8fd1\u4f3c\u6280\u672f\u6c42\u89e3\u8be5\u975e\u51f8\u4e14\u53d8\u91cf\u8026\u5408\u7684\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u9891\u8c31\u5171\u4eab\u8ba4\u77e5\u65e0\u7ebf\u7535\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u65cb\u8f6c\u5929\u7ebf\u5728\u63d0\u5347\u6b21\u7ea7\u63a5\u6536\u5668\u901a\u4fe1\u8d28\u91cf\u548c\u964d\u4f4e\u521d\u7ea7\u63a5\u6536\u5668\u5e72\u6270\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u65cb\u8f6c\u5929\u7ebf\u6280\u672f\u662f\u63d0\u5347\u8ba4\u77e5\u65e0\u7ebf\u7535\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u624b\u6bb5\uff0c\u5c24\u5176\u5728\u9891\u8c31\u5171\u4eab\u548c\u5e72\u6270\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25352", "pdf": "https://arxiv.org/pdf/2509.25352", "abs": "https://arxiv.org/abs/2509.25352", "authors": ["Itamar Mishani", "Yorai Shaoul", "Ramkumar Natarajan", "Jiaoyang Li", "Maxim Likhachev"], "title": "SRMP: Search-Based Robot Motion Planning Library", "categories": ["cs.RO"], "comment": "Submitted for Publication", "summary": "Motion planning is a critical component in any robotic system. Over the\nyears, powerful tools like the Open Motion Planning Library (OMPL) have been\ndeveloped, offering numerous motion planning algorithms. However, existing\nframeworks often struggle to deliver the level of predictability and\nrepeatability demanded by high-stakes applications -- ranging from ensuring\nsafety in industrial environments to the creation of high-quality motion\ndatasets for robot learning. Complementing existing tools, we introduce SRMP\n(Search-based Robot Motion Planning), a new software framework tailored for\nrobotic manipulation. SRMP distinguishes itself by generating consistent and\nreliable trajectories, and is the first software tool to offer motion planning\nalgorithms for multi-robot manipulation tasks. SRMP easily integrates with\nmajor simulators, including MuJoCo, Sapien, Genesis, and PyBullet via a Python\nand C++ API. SRMP includes a dedicated MoveIt! plugin that enables immediate\ndeployment on robot hardware and seamless integration with existing pipelines.\nThrough extensive evaluations, we demonstrate in this paper that SRMP not only\nmeets the rigorous demands of industrial and safety-critical applications but\nalso sets a new standard for consistency in motion planning across diverse\nrobotic systems. Visit srmp.readthedocs.io for SRMP documentation and\ntutorials.", "AI": {"tldr": "SRMP\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u8bbe\u8ba1\u7684\u641c\u7d22\u578b\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u63d0\u4f9b\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u652f\u6301\u591a\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u5e76\u80fd\u4e0e\u4e3b\u6d41\u6a21\u62df\u5668\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u5de5\u5177\u5728\u9ad8\u98ce\u9669\u548c\u4e00\u81f4\u6027\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0cSRMP\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SRMP\u91c7\u7528\u641c\u7d22\u578b\u65b9\u6cd5\u751f\u6210\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7Python\u548cC++ API\u4e0e\u591a\u79cd\u6a21\u62df\u5668\u96c6\u6210\u3002", "result": "SRMP\u5728\u5de5\u4e1a\u548c\u5b89\u5168\u6027\u5173\u952e\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u4e00\u81f4\u6027\u6807\u51c6\u3002", "conclusion": "SRMP\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7cfb\u7edf\u3002"}}
{"id": "2509.25675", "pdf": "https://arxiv.org/pdf/2509.25675", "abs": "https://arxiv.org/abs/2509.25675", "authors": ["Haobo Geng", "Yaoyao Li", "Weiping Tong", "Youwei Meng", "Houpu Xiao", "Yicong Liu"], "title": "A Novel Statistical Analysis Method for Radiation Source Classification", "categories": ["eess.SP"], "comment": null, "summary": "With the rapid advancement of electronic information technology, the number\nand variety of unknown radiation sources have increased significantly. Some of\nthese sources share common characteristics, which offers the potential to\neffectively address the challenge of identifying unknown radiation sources.\nHowever, research on the classification of radiation sources remains relatively\nlimited. This paper proposes a big data analysis method that combines linear\ndiscriminant analysis (LDA) with a rough neighborhood set (NRS) for radiation\nsource classification, and its effectiveness is validated on the RadioML 2018\ndataset. The results indicate that, under certain constraints, all modulation\ntypes can be categorized into four distinct classes, laying a foundation for\nfurther research on cognitive interference signal cancellation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LDA\u548cNRS\u7684\u5927\u6570\u636e\u5206\u6790\u65b9\u6cd5\u7528\u4e8e\u8f90\u5c04\u6e90\u5206\u7c7b\uff0c\u5e76\u5728RadioML 2018\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7279\u5b9a\u7ea6\u675f\u4e0b\uff0c\u6240\u6709\u8c03\u5236\u7c7b\u578b\u53ef\u5206\u4e3a\u56db\u7c7b\u3002", "motivation": "\u968f\u7740\u7535\u5b50\u4fe1\u606f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u672a\u77e5\u8f90\u5c04\u6e90\u7684\u6570\u91cf\u548c\u79cd\u7c7b\u663e\u8457\u589e\u52a0\uff0c\u90e8\u5206\u8f90\u5c04\u6e90\u5177\u6709\u5171\u540c\u7279\u5f81\uff0c\u4e3a\u89e3\u51b3\u5176\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u4f46\u76ee\u524d\u5173\u4e8e\u8f90\u5c04\u6e90\u5206\u7c7b\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u5224\u522b\u5206\u6790\uff08LDA\uff09\u548c\u7c97\u7cd9\u90bb\u57df\u96c6\uff08NRS\uff09\u7684\u5927\u6570\u636e\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u5728RadioML 2018\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u6240\u6709\u8c03\u5236\u7c7b\u578b\u5212\u5206\u4e3a\u56db\u4e2a\u4e0d\u540c\u7684\u7c7b\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba4\u77e5\u5e72\u6270\u4fe1\u53f7\u6d88\u9664\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.25358", "pdf": "https://arxiv.org/pdf/2509.25358", "abs": "https://arxiv.org/abs/2509.25358", "authors": ["Qianzhong Chen", "Justin Yu", "Mac Schwager", "Pieter Abbeel", "Fred Shentu", "Philipp Wu"], "title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Large-scale robot learning has recently shown promise for enabling robots to\nperform complex tasks by integrating perception, control, and language\nunderstanding. Yet, it struggles with long-horizon, contact-rich manipulation\nsuch as deformable object handling, where demonstration quality is\ninconsistent. Reward modeling offers a natural solution: by providing grounded\nprogress signals, it transforms noisy demonstrations into stable supervision\nthat generalizes across diverse trajectories. We introduce a stage-aware,\nvideo-based reward modeling framework that jointly predicts high-level task\nstages and fine-grained progress. Reward labels are automatically derived from\nnatural language subtask annotations, ensuring consistent progress estimation\nacross variable-length demonstrations. This design overcomes frame-index\nlabeling, which fails in variable-duration tasks like folding a T-shirt. Our\nreward model demonstrates robustness to variability, generalization to\nout-of-distribution settings, and strong utility for policy training. Building\non it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters\nhigh-quality data and reweights samples by reward. Experiments show the reward\nmodel alone outperforms baselines on validation and real robot rollouts.\nIntegrated into RA-BC, our approach achieves 83\\% success on folding T-shirts\nfrom the flattened state and 67\\% from the crumpled state -- far surpassing\nvanilla behavior cloning, which attains only 8\\% and 0\\% success. Overall, our\nresults highlight reward modeling as a key enabler for scalable,\nannotation-efficient, and robust imitation learning in long-horizon\nmanipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9636\u6bb5\u611f\u77e5\u7684\u89c6\u9891\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u81ea\u52a8\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u793a\u8303\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86RA-BC\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\uff08\u5982\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\uff09\u793a\u8303\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u7a33\u5b9a\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9636\u6bb5\u611f\u77e5\u7684\u89c6\u9891\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5c42\u6b21\u4efb\u52a1\u9636\u6bb5\u548c\u7ec6\u7c92\u5ea6\u8fdb\u5ea6\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u81ea\u52a8\u751f\u6210\u5956\u52b1\u6807\u7b7e\uff1b\u63d0\u51fa\u4e86RA-BC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u8fc7\u6ee4\u548c\u52a0\u6743\u6837\u672c\u4f18\u5316\u884c\u4e3a\u514b\u9686\u3002", "result": "\u5956\u52b1\u6a21\u578b\u5728\u9a8c\u8bc1\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0cRA-BC\u65b9\u6cd5\u5728\u6298\u53e0T\u6064\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u8fbe\u523083%\uff08\u5e73\u6574\u72b6\u6001\uff09\u548c67%\uff08\u8936\u76b1\u72b6\u6001\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u884c\u4e3a\u514b\u9686\uff08\u4ec58%\u548c0%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5956\u52b1\u5efa\u6a21\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6807\u6ce8\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6a21\u4eff\u5b66\u4e60\u7684\u5173\u952e\uff0c\u7279\u522b\u9002\u7528\u4e8e\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2509.25722", "pdf": "https://arxiv.org/pdf/2509.25722", "abs": "https://arxiv.org/abs/2509.25722", "authors": ["Ruibin Chen", "Haozhe Lei", "Hao Guo", "Marco Mezzavilla", "Hitesh Poddar", "Tomoki Yoshimura", "Sundeep Rangan"], "title": "Transformer-Based Rate Prediction for Multi-Band Cellular Handsets", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Cellular wireless systems are witnessing the proliferation of frequency bands\nover a wide spectrum, particularly with the expansion of new bands in FR3.\nThese bands must be supported in user equipment (UE) handsets with multiple\nantennas in a constrained form factor. Rapid variations in channel quality\nacross the bands from motion and hand blockage, limited field-of-view of\nantennas, and hardware and power-constrained measurement sparsity pose\nsignificant challenges to reliable multi-band channel tracking. This paper\nformulates the problem of predicting achievable rates across multiple antenna\narrays and bands with sparse historical measurements. We propose a\ntransformer-based neural architecture that takes asynchronous rate histories as\ninput and outputs per-array rate predictions. Evaluated on ray-traced\nsimulations in a dense urban micro-cellular setting with FR1 and FR3 arrays,\nour method demonstrates superior performance over baseline predictors, enabling\nmore informed band selection under realistic mobility and hardware constraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u591a\u5929\u7ebf\u548c\u9891\u6bb5\u4e0a\u7a00\u758f\u5386\u53f2\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u53ef\u5b9e\u73b0\u7684\u901f\u7387\u3002", "motivation": "\u968f\u7740FR3\u9891\u6bb5\u7684\u6269\u5c55\uff0c\u7528\u6237\u8bbe\u5907\u9700\u8981\u5728\u6709\u9650\u5c3a\u5bf8\u4e0b\u652f\u6301\u591a\u5929\u7ebf\u548c\u591a\u9891\u6bb5\uff0c\u4f46\u4fe1\u9053\u8d28\u91cf\u7684\u5feb\u901f\u53d8\u5316\u3001\u5929\u7ebf\u89c6\u91ce\u53d7\u9650\u4ee5\u53ca\u6d4b\u91cf\u7a00\u758f\u6027\u7b49\u95ee\u9898\u7ed9\u591a\u9891\u6bb5\u4fe1\u9053\u8ddf\u8e2a\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cdTransformer\u67b6\u6784\uff0c\u8f93\u5165\u5f02\u6b65\u901f\u7387\u5386\u53f2\u6570\u636e\uff0c\u8f93\u51fa\u6bcf\u4e2a\u5929\u7ebf\u9635\u5217\u7684\u901f\u7387\u9884\u6d4b\u3002", "result": "\u5728\u5bc6\u96c6\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\uff0c\u4f7f\u7528FR1\u548cFR3\u9635\u5217\u7684\u5c04\u7ebf\u8ffd\u8e2a\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u9884\u6d4b\u5668\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u652f\u6301\u5b9e\u9645\u79fb\u52a8\u548c\u786c\u4ef6\u7ea6\u675f\u4e0b\u7684\u9891\u6bb5\u9009\u62e9\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u9891\u6bb5\u4fe1\u9053\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u901f\u7387\u9884\u6d4b\u548c\u9891\u6bb5\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2509.25402", "pdf": "https://arxiv.org/pdf/2509.25402", "abs": "https://arxiv.org/abs/2509.25402", "authors": ["Hanlan Yang", "Itamar Mishani", "Luca Pivetti", "Zachary Kingston", "Maxim Likhachev"], "title": "Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models", "categories": ["cs.RO"], "comment": "Submitted for Publication", "summary": "Actor-Critic models are a class of model-free deep reinforcement learning\n(RL) algorithms that have demonstrated effectiveness across various robot\nlearning tasks. While considerable research has focused on improving training\nstability and data sampling efficiency, most deployment strategies have\nremained relatively simplistic, typically relying on direct actor policy\nrollouts. In contrast, we propose \\pachs{} (\\textit{P}arallel\n\\textit{A}ctor-\\textit{C}ritic \\textit{H}euristic \\textit{S}earch), an\nefficient parallel best-first search algorithm for inference that leverages\nboth components of the actor-critic architecture: the actor network generates\nactions, while the critic network provides cost-to-go estimates to guide the\nsearch. Two levels of parallelism are employed within the search -- actions and\ncost-to-go estimates are generated in batches by the actor and critic networks\nrespectively, and graph expansion is distributed across multiple threads. We\ndemonstrate the effectiveness of our approach in robotic manipulation tasks,\nincluding collision-free motion planning and contact-rich interactions such as\nnon-prehensile pushing. Visit p-achs.github.io for demonstrations and examples.", "AI": {"tldr": "PACHS\u662f\u4e00\u79cd\u57fa\u4e8e\u5e76\u884c\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7684Actor-Critic\u63a8\u65ad\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86Actor\u548cCritic\u7f51\u7edc\u7684\u8f93\u51fa\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Actor-Critic\u6a21\u578b\u7684\u90e8\u7f72\u7b56\u7565\u901a\u5e38\u7b80\u5355\uff0c\u4f9d\u8d56\u76f4\u63a5\u7684Actor\u7b56\u7565\u5c55\u5f00\uff0c\u5ffd\u89c6\u4e86Critic\u7f51\u7edc\u7684\u6307\u5bfc\u4f5c\u7528\u3002", "method": "PACHS\u901a\u8fc7\u5e76\u884c\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u540c\u65f6\u5229\u7528Actor\u7f51\u7edc\u7684\u52a8\u4f5c\u751f\u6210\u548cCritic\u7f51\u7edc\u7684\u6210\u672c\u4f30\u8ba1\uff0c\u5e76\u4ee5\u4e24\u7ea7\u5e76\u884c\u6027\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u548c\u975e\u6293\u53d6\u63a8\u52a8\uff09\u4e2d\u8bc1\u660e\u4e86PACHS\u7684\u6709\u6548\u6027\u3002", "conclusion": "PACHS\u901a\u8fc7\u7ed3\u5408Actor-Critic\u67b6\u6784\u7684\u4e24\u90e8\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u65ad\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2509.25732", "pdf": "https://arxiv.org/pdf/2509.25732", "abs": "https://arxiv.org/abs/2509.25732", "authors": ["Chenqing Ji", "Qionghui Liu", "Jiahong Liu", "Chao Yu", "Yifei Sun", "Rui Wang", "Fan Liu"], "title": "Doppler-Based Multistatic Drone Tracking via Cellular Downlink Signals", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, a multistatic Doppler sensing system is proposed for the drone\ntracking via downlink Long-Term Evolution (LTE) signals. Specifically, the LTE\nbase stations (BSs) are exploited as signal illuminators, and three passive\nsensing receivers are deployed at different locations to detect the bistatic\nDoppler frequencies of a target drone from received downlink signals. It is\nshown that even without the measurements of BS-drone-receiver range and angle,\nthe Doppler measurements could provide sufficient information for trajectory\ntracking. Particularly, the trajectory of the target drone, consisting of the\ninitial position and velocities of all the time slots, can be reconstructed by\nsolving a minimum mean-squared error problem according to the above Doppler\nmeasurements. It is demonstrated by experiment that although the target drone\nand all the sensing receivers are around 200 meters away from the illuminating\nBSs, the complicated trajectories can be tracked with 90% errors below 90\ncentimeters. Since this accuracy is notably higher than the typical range\nresolution of LTE signals, the demonstration shows that drone trajectory\ntracking with a high accuracy could be feasible solely according to Doppler\ndetection, as long as the deployment density of receivers is sufficiently high.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528LTE\u4e0b\u884c\u4fe1\u53f7\u7684\u591a\u666e\u52d2\u611f\u77e5\u7cfb\u7edf\u8fdb\u884c\u65e0\u4eba\u673a\u8ffd\u8e2a\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f20\u7edf\u8ddd\u79bb\u548c\u89d2\u5ea6\u6d4b\u91cf\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u91cd\u5efa\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7LTE\u57fa\u7ad9\u4fe1\u53f7\u548c\u591a\u666e\u52d2\u6d4b\u91cf\u5b9e\u73b0\u65e0\u4eba\u673a\u9ad8\u7cbe\u5ea6\u8ffd\u8e2a\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u51cf\u5c11\u5bf9\u4f20\u7edf\u6d4b\u91cf\u65b9\u5f0f\u7684\u4f9d\u8d56\u3002", "method": "\u5229\u7528LTE\u57fa\u7ad9\u4f5c\u4e3a\u4fe1\u53f7\u6e90\uff0c\u90e8\u7f72\u4e09\u4e2a\u88ab\u52a8\u63a5\u6536\u5668\u4ece\u4e0d\u540c\u4f4d\u7f6e\u6355\u83b7\u591a\u666e\u52d2\u9891\u79fb\uff0c\u901a\u8fc7\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u95ee\u9898\u91cd\u5efa\u65e0\u4eba\u673a\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5728200\u7c73\u5916\u7684\u590d\u6742\u8f68\u8ff9\u6761\u4ef6\u4e0b\uff0c90%\u7684\u8ffd\u8e2a\u8bef\u5dee\u4f4e\u4e8e90\u5398\u7c73\u3002", "conclusion": "\u4ec5\u4f9d\u8d56\u591a\u666e\u52d2\u68c0\u6d4b\u5728\u63a5\u6536\u5668\u90e8\u7f72\u5bc6\u5ea6\u8db3\u591f\u9ad8\u65f6\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65e0\u4eba\u673a\u8ffd\u8e2a\u3002"}}
{"id": "2509.25443", "pdf": "https://arxiv.org/pdf/2509.25443", "abs": "https://arxiv.org/abs/2509.25443", "authors": ["Zewen He", "Chenyuan Chen", "Dilshod Azizov", "Yoshihiko Nakamura"], "title": "CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation", "categories": ["cs.RO"], "comment": "Submitted to IEEE for possible publication, under review", "summary": "Humanoid whole-body locomotion control is a critical approach for humanoid\nrobots to leverage their inherent advantages. Learning-based control methods\nderived from retargeted human motion data provide an effective means of\naddressing this issue. However, because most current human datasets lack\nmeasured force data, and learning-based robot control is largely\nposition-based, achieving appropriate compliance during interaction with real\nenvironments remains challenging. This paper presents Compliant Task Pipeline\n(CoTaP): a pipeline that leverages compliance information in the learning-based\nstructure of humanoid robots. A two-stage dual-agent reinforcement learning\nframework combined with model-based compliance control for humanoid robots is\nproposed. In the training process, first a base policy with a position-based\ncontroller is trained; then in the distillation, the upper-body policy is\ncombined with model-based compliance control, and the lower-body agent is\nguided by the base policy. In the upper-body control, adjustable task-space\ncompliance can be specified and integrated with other controllers through\ncompliance modulation on the symmetric positive definite (SPD) manifold,\nensuring system stability. We validated the feasibility of the proposed\nstrategy in simulation, primarily comparing the responses to external\ndisturbances under different compliance settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5CoTaP\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4d\u7f6e\u63a7\u5236\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u987a\u5e94\u6027\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u529b\u6570\u636e\u800c\u96be\u4ee5\u5b9e\u73b0\u4e0e\u73af\u5883\u4e92\u52a8\u65f6\u7684\u987a\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4d\u7f6e\u6570\u636e\uff0c\u7f3a\u4e4f\u529b\u6570\u636e\u652f\u6301\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u9002\u5f53\u7684\u987a\u5e94\u6027\u4e92\u52a8\u3002", "method": "\u63d0\u51fa\u4e86Compliant Task Pipeline\uff08CoTaP\uff09\uff0c\u5305\u542b\u4e24\u9636\u6bb5\u53cc\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u987a\u5e94\u6027\u63a7\u5236\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u57fa\u7b56\u7565\u8bad\u7ec3\u548c\u7b56\u7565\u84b8\u998f\u4e24\u90e8\u5206\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u987a\u5e94\u6027\u8bbe\u7f6e\u4e0b\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5916\u90e8\u5e72\u6270\u3002", "conclusion": "CoTaP\u6846\u67b6\u901a\u8fc7\u6574\u5408\u987a\u5e94\u6027\u63a7\u5236\u548c\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.25793", "pdf": "https://arxiv.org/pdf/2509.25793", "abs": "https://arxiv.org/abs/2509.25793", "authors": ["Hao Luo", "Shuaifeng Jiang", "Saeed R. Khosravirad", "Ahmed Alkhateeb"], "title": "Digital Twin Aided Massive MIMO CSI Feedback: Exploring the Impact of Twinning Fidelity", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "16 pages, 15 figures", "summary": "Deep learning (DL) techniques have demonstrated strong performance in\ncompressing and reconstructing channel state information (CSI) while reducing\nfeedback overhead in massive MIMO systems. A key challenge, however, is their\nreliance on extensive site-specific training data, whose real-world collection\nincurs significant overhead and limits scalability across deployment sites. To\naddress this, we propose leveraging site-specific digital twins to assist the\ntraining of DL-based CSI compression models. The digital twin integrates an\nelectromagnetic (EM) 3D model of the environment, a hardware model, and ray\ntracing to produce site-specific synthetic CSI data, allowing DL models to be\ntrained without the need for extensive real-world measurements. We further\ndevelop a fidelity analysis framework that decomposes digital twin quality into\nfour key aspects: 3D geometry, material properties, ray tracing, and hardware\nmodeling. We explore how these factors influence the reliability of the data\nand model performance. To enhance the adaptability to real-world environments,\nwe propose a refinement strategy that incorporates a limited amount of\nreal-world data to fine-tune the DL model pre-trained on the digital twin\ndataset. Evaluation results show that models trained on site-specific digital\ntwins outperform those trained on generic datasets, with the proposed\nrefinement method effectively enhancing performance using limited real-world\ndata. The simulations also highlight the importance of digital twin fidelity,\nespecially in 3D geometry, ray tracing, and hardware modeling, for improving\nCSI reconstruction quality. This analysis framework offers valuable insights\ninto the critical fidelity aspects, and facilitates more efficient digital twin\ndevelopment and deployment strategies for various wireless communication tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5229\u7528\u7ad9\u70b9\u7279\u5b9a\u7684\u6570\u5b57\u5b6a\u751f\u6280\u672f\u8f85\u52a9\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u5bf9\u5927\u91cf\u771f\u5b9e\u6570\u636e\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u4fdd\u771f\u5ea6\u5206\u6790\u6846\u67b6\u548c\u7ec6\u5316\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728CSI\u538b\u7f29\u4e2d\u4f9d\u8d56\u5927\u91cf\u7ad9\u70b9\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u6536\u96c6\u7684\u5f00\u9500\u548c\u9650\u5236\u3002", "method": "\u6574\u5408\u7535\u78c13D\u6a21\u578b\u3001\u786c\u4ef6\u6a21\u578b\u548c\u5c04\u7ebf\u8ffd\u8e2a\u751f\u6210\u5408\u6210CSI\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u4fdd\u771f\u5ea6\u5206\u6790\u6846\u67b6\u548c\u7ec6\u5316\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728\u6570\u5b57\u5b6a\u751f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6570\u636e\u96c6\uff0c\u7ec6\u5316\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u4fdd\u771f\u5ea6\u5bf9CSI\u91cd\u5efa\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4e3a\u65e0\u7ebf\u901a\u4fe1\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4fdd\u771f\u5ea6\u5206\u6790\u6846\u67b6\u5bf9\u5176\u5f00\u53d1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2509.25542", "pdf": "https://arxiv.org/pdf/2509.25542", "abs": "https://arxiv.org/abs/2509.25542", "authors": ["Zihan Zhang", "Abhijit Ravichandran", "Pragnya Korti", "Luobin Wang", "Henrik I. Christensen"], "title": "Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments", "categories": ["cs.RO", "cs.CV"], "comment": "19th International Symposium on Experimental Robotics", "summary": "High-definition (HD) maps are essential for autonomous driving, providing\nprecise information such as road boundaries, lane dividers, and crosswalks to\nenable safe and accurate navigation. However, traditional HD map generation is\nlabor-intensive, expensive, and difficult to maintain in dynamic environments.\nTo overcome these challenges, we present a real-world deployment of an online\nmapping system on a campus golf cart platform equipped with dual front cameras\nand a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D\nHD map for campus environment; (2) integrating and generalizing the SemVecMap\nmodel onboard; and (3) incrementally generating and updating the predicted HD\nmap to capture environmental changes. By fine-tuning with campus-specific data,\nour pipeline produces accurate map predictions and supports continual updates,\ndemonstrating its practical value in real-world autonomous driving scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\u7684\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u751f\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u6821\u56ed\u9ad8\u5c14\u592b\u7403\u8f66\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u9ad8\u7cbe\u5730\u56fe\u6807\u6ce8\u3001\u6a21\u578b\u96c6\u6210\u548c\u73af\u5883\u52a8\u6001\u66f4\u65b0\u7b49\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5730\u56fe\u751f\u6210\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u7ef4\u62a4\u56f0\u96be\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u66f4\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u53cc\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\uff0c\u6807\u6ce8\u6821\u56ed3D\u9ad8\u7cbe\u5730\u56fe\uff0c\u96c6\u6210SemVecMap\u6a21\u578b\uff0c\u5e76\u652f\u6301\u589e\u91cf\u66f4\u65b0\u4ee5\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u6821\u56ed\u6570\u636e\u5fae\u8c03\uff0c\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u5730\u56fe\u9884\u6d4b\u5e76\u6301\u7eed\u66f4\u65b0\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9ad8\u7cbe\u5730\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2509.25854", "pdf": "https://arxiv.org/pdf/2509.25854", "abs": "https://arxiv.org/abs/2509.25854", "authors": ["Hao Zhou", "Yiyan Ma", "Dan Fei", "Weirong Liu", "Zhengyu Zhang", "Mi Yang", "Guoyu Ma", "Yunlong Lu", "Ruisi He", "Guoyu Wang", "Cheng Li", "Zhaohui Song", "Bo Ai"], "title": "Delay-Doppler Domain Channel Measurements and Modeling in High-Speed Railways", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "13 pages, 11 figures", "summary": "As next-generation wireless communication systems need to be able to operate\nin high-frequency bands and high-mobility scenarios, delay-Doppler (DD) domain\nmulticarrier (DDMC) modulation schemes, such as orthogonal time frequency space\n(OTFS), demonstrate superior reliability over orthogonal frequency division\nmultiplexing (OFDM). Accurate DD domain channel modeling is essential for DDMC\nsystem design. However, since traditional channel modeling approaches are\nmainly confined to time, frequency, and space domains, the principles of DD\ndomain channel modeling remain poorly studied. To address this issue, we\npropose a systematic DD domain channel measurement and modeling methodology in\nhigh-speed railway (HSR) scenarios. First, we design a DD domain channel\nmeasurement method based on the long-term evolution for railway (LTE-R) system.\nSecond, for DD domain channel modeling, we investigate quasi-stationary\ninterval, statistical power modeling of multipath components, and particularly,\nthe quasi-invariant intervals of DD domain channel fading coefficients. Third,\nvia LTE-R measurements at 371 km/h, taking the quasi-stationary interval as the\ndecision criterion, we establish DD domain channel models under different\nchannel time-varying conditions in HSR scenarios. Fourth, the accuracy of\nproposed DD domain channel models is validated via bit error rate comparison of\nOTFS transmission. In addition, simulation verifies that in HSR scenario, the\nquasi-invariant interval of DD domain channel fading coefficient is on\nmillisecond (ms) order of magnitude, which is much smaller than the\nquasi-stationary interval length on $100$ ms order of magnitude. This study\ncould provide theoretical guidance for DD domain modeling in high-mobility\nenvironments, supporting future DDMC and integrated sensing and communication\ndesigns for 6G and beyond.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u901f\u94c1\u8def\u573a\u666f\u4e0b\u7684\u5ef6\u8fdf\u591a\u666e\u52d2\uff08DD\uff09\u57df\u4fe1\u9053\u6d4b\u91cf\u4e0e\u5efa\u6a21\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86DD\u57df\u4fe1\u9053\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u9ad8\u79fb\u52a8\u73af\u5883\u4e2d\u7684DD\u57df\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5728\u9ad8\u9891\u6bb5\u548c\u9ad8\u79fb\u52a8\u573a\u666f\u4e2d\u7684\u9700\u6c42\uff0c\u4f20\u7edf\u7684\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\u5728DD\u57df\u7684\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eLTE-R\u7cfb\u7edf\u7684DD\u57df\u4fe1\u9053\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86DD\u57df\u4fe1\u9053\u7684\u51c6\u9759\u6b62\u533a\u95f4\u3001\u591a\u5f84\u5206\u91cf\u7684\u7edf\u8ba1\u529f\u7387\u5efa\u6a21\u4ee5\u53ca\u8870\u843d\u7cfb\u6570\u7684\u51c6\u4e0d\u53d8\u533a\u95f4\u3002", "result": "\u901a\u8fc7\u9ad8\u901f\u94c1\u8def\u6d4b\u91cf\u9a8c\u8bc1\u4e86DD\u57df\u4fe1\u9053\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u53d1\u73b0\u8870\u843d\u7cfb\u6570\u7684\u51c6\u4e0d\u53d8\u533a\u95f4\u8fdc\u5c0f\u4e8e\u51c6\u9759\u6b62\u533a\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u79fb\u52a8\u73af\u5883\u4e2d\u7684DD\u57df\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u652f\u6301\u672a\u67656G\u53ca\u4ee5\u4e0a\u7684DDMC\u548c\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u8bbe\u8ba1\u3002"}}
{"id": "2509.25556", "pdf": "https://arxiv.org/pdf/2509.25556", "abs": "https://arxiv.org/abs/2509.25556", "authors": ["Mohammad Merati", "David Casta\u00f1\u00f3n"], "title": "Exhaustive-Serve-Longest Control for Multi-robot Scheduling Systems", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We study online task allocation for multi-robot, multi-queue systems with\nstochastic arrivals and switching delays. Time is slotted; each location can\nhost at most one robot per slot; service consumes one slot; switching between\nlocations incurs a one-slot travel delay; and arrivals are independent\nBernoulli processes. We formulate a discounted-cost Markov decision process and\npropose Exhaustive-Serve-Longest (ESL), a simple real-time policy that serves\nexhaustively when the current location is nonempty and, when idle, switches to\na longest unoccupied nonempty location, and we prove the optimality of this\npolicy. As baselines, we tune a fixed-dwell cyclic policy via a discrete-time\ndelay expression and implement a first-come-first-serve policy. Across\nserver-to-location ratios and loads, ESL consistently yields lower discounted\nholding cost and smaller mean queue lengths, with action-time fractions showing\nmore serving and restrained switching. Its simplicity and robustness make ESL a\npractical default for real-time multi-robot scheduling systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u673a\u5668\u4eba\u591a\u961f\u5217\u7cfb\u7edf\u4e2d\u5728\u7ebf\u4efb\u52a1\u5206\u914d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aESL\u7684\u5b9e\u65f6\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u591a\u961f\u5217\u7cfb\u7edf\u4e2d\u4efb\u52a1\u5206\u914d\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u968f\u673a\u5230\u8fbe\u548c\u5207\u6362\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86Exhaustive-Serve-Longest (ESL)\u7b56\u7565\uff0c\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\u3002", "result": "ESL\u7b56\u7565\u5728\u964d\u4f4e\u5ef6\u8fdf\u6210\u672c\u548c\u5e73\u5747\u961f\u5217\u957f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ESL\u7b56\u7565\u56e0\u5176\u7b80\u5355\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6210\u4e3a\u591a\u673a\u5668\u4eba\u5b9e\u65f6\u8c03\u5ea6\u7cfb\u7edf\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2509.25931", "pdf": "https://arxiv.org/pdf/2509.25931", "abs": "https://arxiv.org/abs/2509.25931", "authors": ["Oksana Moryakova", "H\u00e5kan Johansson"], "title": "Closed-Form Least-Squares Design of Fast-Convolution Based Variable-Bandwidth FIR Filters", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a closed-form least-squares (LS) design approach for\nfast-convolution (FC) based variable-bandwidth (VBW) finite-impulse-response\n(FIR) filters. The proposed LS design utilizes frequency sampling and the VBW\nfilter frequency-domain implementation using the overlap-save (OLS) method,\nthat together offer significant savings in implementation and online bandwidth\nreconfiguration complexities. Since combining frequency-domain design and OLS\nimplementation leads to a linear periodic time-varying (LPTV) behavior of the\nVBW filter, a set of the corresponding time-invariant impulse responses is\nconsidered in the proposed design. Through numerical examples, it is\ndemonstrated that the proposed approach enables not only closed-form design of\nFC-based VBW filters with substantial complexity reductions compared to\nexisting solutions for a given performance, but also allows the variable\nbandwidth range to be extended without any increase in complexity. Moreover, a\nway of reducing the maximum approximation error energy over the whole set of\nthe time-invariant filters of the LPTV system is shown by introducing\nappropriate weighting functions in the design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5377\u79ef\uff08FC\uff09\u7684\u53ef\u53d8\u5e26\u5bbd\uff08VBW\uff09\u6709\u9650\u51b2\u6fc0\u54cd\u5e94\uff08FIR\uff09\u6ee4\u6ce2\u5668\u7684\u95ed\u5f0f\u6700\u5c0f\u4e8c\u4e58\uff08LS\uff09\u8bbe\u8ba1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9891\u7387\u91c7\u6837\u548c\u91cd\u53e0-\u5b58\u50a8\uff08OLS\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u73b0\u548c\u5e26\u5bbd\u91cd\u914d\u7f6e\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u8bbe\u8ba1\u548c\u5b9e\u73b0\u53ef\u53d8\u5e26\u5bbd\u6ee4\u6ce2\u5668\u65f6\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u4e14\u5e26\u5bbd\u8303\u56f4\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9891\u7387\u57df\u8bbe\u8ba1\u548cOLS\u65b9\u6cd5\u7ed3\u5408\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u6269\u5c55\u5e26\u5bbd\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9891\u7387\u91c7\u6837\u548cOLS\u65b9\u6cd5\u7684\u95ed\u5f0fLS\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u7ebf\u6027\u5468\u671f\u65f6\u53d8\uff08LPTV\uff09\u884c\u4e3a\u7684\u65f6\u4e0d\u53d8\u51b2\u6fc0\u54cd\u5e94\u96c6\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\uff0c\u8fd8\u6269\u5c55\u4e86\u5e26\u5bbd\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u6743\u91cd\u51fd\u6570\u51cf\u5c11\u4e86\u6700\u5927\u8fd1\u4f3c\u8bef\u5dee\u80fd\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u9ad8\u6027\u80fd\u53ef\u53d8\u5e26\u5bbd\u6ee4\u6ce2\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u95ed\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.25663", "pdf": "https://arxiv.org/pdf/2509.25663", "abs": "https://arxiv.org/abs/2509.25663", "authors": ["Nathaniel Hanson", "Benjamin Pyatski", "Samuel Hibbard", "Gary Lvov", "Oscar De La Garza", "Charles DiMarzio", "Kristen L. Dorsey", "Ta\u015fk\u0131n Pad\u0131r"], "title": "Field Calibration of Hyperspectral Cameras for Terrain Inference", "categories": ["cs.RO", "eess.IV"], "comment": "Accepted to IEEE Robotics & Automation Letters", "summary": "Intra-class terrain differences such as water content directly influence a\nvehicle's ability to traverse terrain, yet RGB vision systems may fail to\ndistinguish these properties. Evaluating a terrain's spectral content beyond\nred-green-blue wavelengths to the near infrared spectrum provides useful\ninformation for intra-class identification. However, accurate analysis of this\nspectral information is highly dependent on ambient illumination. We\ndemonstrate a system architecture to collect and register multi-wavelength,\nhyperspectral images from a mobile robot and describe an approach to\nreflectance calibrate cameras under varying illumination conditions. To\nshowcase the practical applications of our system, HYPER DRIVE, we demonstrate\nthe ability to calculate vegetative health indices and soil moisture content\nfrom a mobile robot platform.", "AI": {"tldr": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u6ce2\u957f\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u6790\u5730\u5f62\u5149\u8c31\u5185\u5bb9\uff0c\u89e3\u51b3\u4e86RGB\u7cfb\u7edf\u5728\u533a\u5206\u5730\u5f62\u5185\u90e8\u5dee\u5f02\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6821\u51c6\u65b9\u6cd5\u53ca\u5e94\u7528\u3002", "motivation": "RGB\u89c6\u89c9\u7cfb\u7edf\u96be\u4ee5\u533a\u5206\u5730\u5f62\u5185\u90e8\u5dee\u5f02\uff08\u5982\u542b\u6c34\u91cf\uff09\uff0c\u5f71\u54cd\u4e86\u8f66\u8f86\u7684\u901a\u8fc7\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7cfb\u7edf\u67b6\u6784\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u8bb0\u5f55\u79fb\u52a8\u673a\u5668\u4eba\u7684\u591a\u6ce2\u957f\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u5149\u7167\u53d8\u5316\u4e0b\u7684\u76f8\u673a\u53cd\u5c04\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86HYPER DRIVE\u7cfb\u7edf\u7684\u5b9e\u7528\u529f\u80fd\uff0c\u5305\u62ec\u8ba1\u7b97\u690d\u88ab\u5065\u5eb7\u6307\u6570\u548c\u571f\u58e4\u542b\u6c34\u91cf\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5730\u5f62\u5206\u6790\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5149\u7167\u53d8\u5316\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.25937", "pdf": "https://arxiv.org/pdf/2509.25937", "abs": "https://arxiv.org/abs/2509.25937", "authors": ["Chandan Kumar Sheemar", "Jorge Querol", "Wali Ullah Khan", "Prabhu Thiruvasagam", "Sourabh Solanki", "Idir Edjekouane", "Alejandro Gonzalez-Garrido", "Mohammed Al-Ansi", "Carla E. Garcia", "Symeon Chatzinotas"], "title": "Joint Communications, Sensing, and Positioning in 6G Multi-Functional Satellite Systems: Survey and Open Challenges", "categories": ["eess.SP"], "comment": null, "summary": "Satellite systems are expected to be a cornerstone of sixth-generation (6G)\nnetworks, providing ubiquitous coverage and supporting a wide range of services\nacross communications, sensing, and positioning, navigation, and timing (PNT).\nMeeting these demands with current function-specific payload architectures is\nchallenging in terms of cost, spectral use, and sustainability. This survey\nintroduces the framework of multi-functional satellite systems (MFSS), which\nintegrate two or more of these core services into a single payload, enabling\nresource sharing and functional synergy. A unified taxonomy is proposed,\ncovering joint communications and sensing (JCAS), joint communications and PNT\n(JCAP), joint sensing and PNT (JSAP), and fully integrated joint\ncommunications, sensing, and PNT (JCSAP) systems. The paper reviews the\nstate-of-the-art in each domain, examines existing payload architectures, and\noutlines cooperative, integrated, and joint design strategies. Key challenges,\nincluding waveform co-design, synchronization, interference mitigation, and\nresource management, are discussed, along with potential solutions and future\nresearch directions. By unifying diverse satellite capabilities within a single\nplatform, MFSS can achieve higher spectral efficiency, reduced launch mass and\ncost, improved energy use, and enhanced service versatility, contributing to\nthe development of sustainable and intelligent non-terrestrial networks (NTNs)\nfor the 6G and beyond space era.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u536b\u661f\u7cfb\u7edf\uff08MFSS\uff09\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b49\u6838\u5fc3\u670d\u52a1\u6574\u5408\u5230\u4e00\u4e2a\u8f7d\u8377\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u548c\u529f\u80fd\u534f\u540c\u6027\u3002", "motivation": "\u5f53\u524d\u536b\u661f\u7cfb\u7edf\u7684\u529f\u80fd\u5355\u4e00\u5316\u67b6\u6784\u5728\u6210\u672c\u3001\u9891\u8c31\u5229\u7528\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0cMFSS\u65e8\u5728\u901a\u8fc7\u591a\u529f\u80fd\u6574\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u8054\u5408\u901a\u4fe1\u4e0e\u611f\u77e5\uff08JCAS\uff09\u3001\u8054\u5408\u901a\u4fe1\u4e0e\u5bfc\u822a\uff08JCAP\uff09\u7b49\uff0c\u5e76\u5206\u6790\u73b0\u6709\u8f7d\u8377\u67b6\u6784\u548c\u8bbe\u8ba1\u7b56\u7565\u3002", "result": "MFSS\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\u3001\u51cf\u5c11\u53d1\u5c04\u8d28\u91cf\u548c\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u80fd\u6e90\u5229\u7528\u548c\u670d\u52a1\u591a\u6837\u6027\u3002", "conclusion": "MFSS\u4e3a6G\u53ca\u672a\u6765\u7684\u975e\u5730\u9762\u7f51\u7edc\uff08NTNs\uff09\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u548c\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25681", "pdf": "https://arxiv.org/pdf/2509.25681", "abs": "https://arxiv.org/abs/2509.25681", "authors": ["Junjie Wen", "Minjie Zhu", "Jiaming Liu", "Zhiyuan Liu", "Yicun Yang", "Linfeng Zhang", "Shanghang Zhang", "Yichen Zhu", "Yi Xu"], "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought", "categories": ["cs.RO", "cs.CV"], "comment": "technique report", "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.", "AI": {"tldr": "dVLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u7edf\u4e00\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u63a8\u7406\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u8303\u5f0f\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6269\u6563\u6846\u67b6\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6269\u6563\u76ee\u6807\u8054\u5408\u4f18\u5316\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u524d\u7f00\u6ce8\u610f\u529b\u63a9\u7801\u548cKV\u7f13\u5b58\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523096.4%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "dVLA\u5c55\u793a\u4e86\u7edf\u4e00\u6269\u6563\u6846\u67b6\u5728\u9ad8\u6027\u80fdVLA\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.25959", "pdf": "https://arxiv.org/pdf/2509.25959", "abs": "https://arxiv.org/abs/2509.25959", "authors": ["Minxing Sun", "Li Miao", "Qingyu Shen", "Yao Mao", "Qiliang Bao"], "title": "Neural Network State-Space Estimators", "categories": ["eess.SP"], "comment": "12 pages, 24 figures. Code and data available at\n  github.com/ShineMinxing/PaperNNSSE.git", "summary": "Classical state estimation algorithms rely on predefined target's state-space\nmodel, which complicates model derivation and limits adaptability when system\ndynamics change. Neural network based estimators offer a data-driven\nalternative, but rarely fuse classical estimation theory into their structure\nand demand large, pre-computed training sets. To overcome these limitations, we\npropose a unified state-space structure without target's state-space model and\ntreats both the input-layer activations and all network weights as latent\nstates to be estimated online. We instantiate this nonlinear model with three\ncanonical estimators-the extended Kalman estimator, the unscented Kalman\nestimator, and the particle estimator to simulate different neural network and\ndemonstrate its generality. We then benchmark our approach against seven\nleading neural network estimators across three representative scenarios.\nResults show that our neural network state-space estimators not only retain the\nrobust learning capability, but also match or exceed the accuracy of both\nclassical and pre-trained neural network methods. Code, data, and more result:\ngithub.com/ShineMinxing/PaperNNSSE.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u7edf\u4e00\u795e\u7ecf\u7f51\u7edc\u72b6\u6001\u7a7a\u95f4\u4f30\u8ba1\u5668\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u5b66\u4e60\u548c\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u72b6\u6001\u4f30\u8ba1\u7b97\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u578b\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u5c06\u8f93\u5165\u5c42\u6fc0\u6d3b\u548c\u7f51\u7edc\u6743\u91cd\u4f5c\u4e3a\u6f5c\u5728\u72b6\u6001\u5728\u7ebf\u4f30\u8ba1\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3a\u6269\u5c55\u5361\u5c14\u66fc\u3001\u65e0\u8ff9\u5361\u5c14\u66fc\u548c\u7c92\u5b50\u4f30\u8ba1\u4e09\u79cd\u7ecf\u5178\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u4e09\u79cd\u573a\u666f\u4e2d\uff0c\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u6301\u4e86\u9c81\u68d2\u5b66\u4e60\u80fd\u529b\uff0c\u8fd8\u8d85\u8d8a\u6216\u5339\u914d\u4e86\u4f20\u7edf\u548c\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u5728\u72b6\u6001\u4f30\u8ba1\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.25685", "pdf": "https://arxiv.org/pdf/2509.25685", "abs": "https://arxiv.org/abs/2509.25685", "authors": ["Amelie Minji Kim", "Anqi Wu", "Ye Zhao"], "title": "Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors", "categories": ["cs.RO"], "comment": null, "summary": "We propose a novel hierarchical diffusion planner that embeds task and motion\nstructure directly in the noise model. Unlike standard diffusion-based planners\nthat use zero-mean, isotropic Gaussian noise, we employ a family of\ntask-conditioned structured Gaussians whose means and covariances are derived\nfrom Gaussian Process Motion Planning (GPMP): sparse, task-centric key states\nor their associated timings (or both) are treated as noisy observations to\nproduce a prior instance. We first generalize the standard diffusion process to\nbiased, non-isotropic corruption with closed-form forward and posterior\nexpressions. Building on this, our hierarchy separates prior instantiation from\ntrajectory denoising: the upper level instantiates a task-conditioned\nstructured Gaussian (mean and covariance), and the lower level denoises the\nfull trajectory under that fixed prior. Experiments on Maze2D goal-reaching and\nKUKA block stacking show improved success rates, smoother trajectories, and\nstronger task alignment compared to isotropic baselines. Ablation studies\nindicate that explicitly structuring the corruption process offers benefits\nbeyond simply conditioning the neural network. Overall, our method concentrates\nprobability mass of prior near feasible, smooth, and semantically meaningful\ntrajectories while maintaining tractability. Our project page is available at\nhttps://hta-diffusion.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5c42\u6269\u6563\u89c4\u5212\u5668\uff0c\u5c06\u4efb\u52a1\u548c\u8fd0\u52a8\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u566a\u58f0\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u4efb\u52a1\u6761\u4ef6\u5316\u7684\u7ed3\u6784\u5316\u9ad8\u65af\u566a\u58f0\u6539\u8fdb\u89c4\u5212\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u6563\u89c4\u5212\u5668\u4f7f\u7528\u96f6\u5747\u503c\u3001\u5404\u5411\u540c\u6027\u9ad8\u65af\u566a\u58f0\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4efb\u52a1\u548c\u8fd0\u52a8\u7ed3\u6784\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u566a\u58f0\u6a21\u578b\u63d0\u5347\u89c4\u5212\u7684\u6210\u529f\u7387\u3001\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u4efb\u52a1\u5bf9\u9f50\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u89c4\u5212\u7684\u4efb\u52a1\u6761\u4ef6\u5316\u7ed3\u6784\u5316\u9ad8\u65af\u566a\u58f0\uff0c\u5206\u5c42\u8bbe\u8ba1\u5c06\u5148\u9a8c\u751f\u6210\u4e0e\u8f68\u8ff9\u53bb\u566a\u5206\u79bb\u3002", "result": "\u5728Maze2D\u76ee\u6807\u8fbe\u6210\u548cKUKA\u79ef\u6728\u5806\u53e0\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u5404\u5411\u540c\u6027\u57fa\u7ebf\uff0c\u672c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3001\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u4efb\u52a1\u5bf9\u9f50\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u566a\u58f0\u6a21\u578b\u80fd\u6709\u6548\u96c6\u4e2d\u6982\u7387\u8d28\u91cf\u4e8e\u53ef\u884c\u3001\u5e73\u6ed1\u4e14\u8bed\u4e49\u6709\u610f\u4e49\u7684\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2509.26018", "pdf": "https://arxiv.org/pdf/2509.26018", "abs": "https://arxiv.org/abs/2509.26018", "authors": ["Jaewon Yu", "Pyo-Woong Son"], "title": "Performance Evaluation of eLoran Spatial ASF Corrections Based on Measured ASF Map", "categories": ["eess.SP"], "comment": "Submitted to ICCE-Asia 2025", "summary": "This paper analyzes the effectiveness of spatial ASF correction methods in\nthe Korean eLoran system using measured ASF maps. Three correction scenarios\nwere evaluated under identical simulation settings: no correction (S0), local\ncorrection using true ASF values (S1), and wide-area correction using a single\nreference station value (S2). Simulation results show that S1 consistently\nachieved the lowest positioning errors, while S0 exhibited the largest errors\nwith extensive high-error regions. S2 provided limited improvements near the\nreference station but degraded with increasing distance and ASF spatial\ngradients. The findings highlight that local ASF correction significantly\nimproves eLoran positioning performance, whereas wide-area correction has only\nlocalized benefits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6d4b\u91cfASF\u5730\u56fe\u5206\u6790\u97e9\u56fdeLoran\u7cfb\u7edf\u4e2d\u7a7a\u95f4ASF\u6821\u6b63\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5c40\u90e8\u6821\u6b63\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\uff0c\u800c\u5e7f\u57df\u6821\u6b63\u4ec5\u5728\u53c2\u8003\u7ad9\u9644\u8fd1\u6709\u6709\u9650\u6539\u5584\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684ASF\u6821\u6b63\u65b9\u6cd5\u5728\u97e9\u56fdeLoran\u7cfb\u7edf\u4e2d\u7684\u5b9a\u4f4d\u6027\u80fd\u8868\u73b0\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u6821\u6b63\u573a\u666f\uff1a\u65e0\u6821\u6b63\uff08S0\uff09\u3001\u4f7f\u7528\u771f\u5b9eASF\u503c\u7684\u5c40\u90e8\u6821\u6b63\uff08S1\uff09\u548c\u57fa\u4e8e\u5355\u4e00\u53c2\u8003\u7ad9\u7684\u5e7f\u57df\u6821\u6b63\uff08S2\uff09\u3002", "result": "S1\u8868\u73b0\u6700\u4f18\uff0c\u5b9a\u4f4d\u8bef\u5dee\u6700\u4f4e\uff1bS0\u8bef\u5dee\u6700\u5927\uff1bS2\u4ec5\u5728\u53c2\u8003\u7ad9\u9644\u8fd1\u6709\u6548\uff0c\u968f\u8ddd\u79bb\u548cASF\u7a7a\u95f4\u68af\u5ea6\u589e\u52a0\u800c\u8868\u73b0\u4e0b\u964d\u3002", "conclusion": "\u5c40\u90e8ASF\u6821\u6b63\u53ef\u663e\u8457\u63d0\u5347eLoran\u5b9a\u4f4d\u6027\u80fd\uff0c\u800c\u5e7f\u57df\u6821\u6b63\u7684\u6548\u679c\u5c40\u9650\u4e8e\u53c2\u8003\u7ad9\u9644\u8fd1\u3002"}}
{"id": "2509.25687", "pdf": "https://arxiv.org/pdf/2509.25687", "abs": "https://arxiv.org/abs/2509.25687", "authors": ["Xinda Xue", "Junjun Hu", "Minghua Luo", "Xie Shichao", "Jintao Chen", "Zixun Xie", "Quan Kuichen", "Guo Wei", "Mu Xu", "Zedong Chu"], "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Embodied navigation presents a core challenge for intelligent robots,\nrequiring the comprehension of visual environments, natural language\ninstructions, and autonomous exploration. Existing models often fall short in\noffering a unified solution across diverse navigation paradigms, resulting in\nlow success rates and limited generalization. We introduce OmniNav, a unified\nframework addressing instruct-goal, object-goal, point-goal navigation, and\nfrontier-based exploration within a single architecture. Our approach features\na lightweight, low-latency policy that accurately predicts continuous-space\nwaypoints (coordinates and orientations). This policy surpasses action-chunk\nmethods in precision and supports real-world deployment at control frequencies\nup to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast\nmodule generates waypoints using short-horizon visual context and subtasks,\nwhile a slow module performs deliberative planning with long-horizon\nobservations and candidate frontiers to select subsequent subgoals and\nsubtasks. This collaboration enhances path efficiency and maintains trajectory\ncoherence, particularly in exploration and memory-intensive scenarios.\nCrucially, we identify that the primary bottleneck isn't merely navigation\npolicy learning, but a robust understanding of general instructions and\nobjects. To boost generalization, OmniNav integrates large-scale,\ngeneral-purpose training datasets, including those for image captioning and\nvisual recognition, into a joint multi-task regimen. This significantly\nimproves success rates and robustness. Extensive experiments confirm OmniNav's\nstate-of-the-art performance across various navigation benchmarks, with\nreal-world deployment further validating its efficacy. OmniNav provides\npractical insights for embodied navigation, charting a scalable path towards\nversatile, highly generalizable robotic intelligence.", "AI": {"tldr": "OmniNav\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u51b3\u591a\u79cd\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5feb\u901f-\u6162\u901f\u7cfb\u7edf\u8bbe\u8ba1\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5bfc\u822a\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "OmniNav\u91c7\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u9884\u6d4b\u8fde\u7eed\u7a7a\u95f4\u8def\u5f84\u70b9\uff0c\u7ed3\u5408\u5feb\u901f-\u6162\u901f\u6a21\u5757\u534f\u4f5c\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOmniNav\u5728\u591a\u4e2a\u5bfc\u822a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "OmniNav\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.26020", "pdf": "https://arxiv.org/pdf/2509.26020", "abs": "https://arxiv.org/abs/2509.26020", "authors": ["Junwoo Song", "Pyo-woong Son"], "title": "Path-Based Correlation Analysis of Meteorological Factors and eLoran Signal Delay Variations", "categories": ["eess.SP"], "comment": "Submitted to ICCE-Asia 2025", "summary": "Unlike GNSS, which is vulnerable to jamming and spoofing due to its\ninherently weak received power, eLoran exhibits robustness owing to its high\nfield strength. Therefore, the eLoran system can maintain reliable operation\neven in scenarios where GNSS becomes unavailable. However, since eLoran signals\npropagate through ground waves, the propagation delay is susceptible to changes\nin surface conditions, including both terrain and meteorological variations.\nThis study aims to analyze the correlation between the temporal variations in\neLoran signal propagation delay and meteorological factors at various points\nalong the signal path.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86eLoran\u4fe1\u53f7\u4f20\u64ad\u5ef6\u8fdf\u7684\u65f6\u95f4\u53d8\u5316\u4e0e\u4fe1\u53f7\u8def\u5f84\u4e0a\u6c14\u8c61\u56e0\u7d20\u7684\u76f8\u5173\u6027\u3002", "motivation": "eLoran\u7cfb\u7edf\u56e0\u5176\u9ad8\u573a\u5f3a\u800c\u8868\u73b0\u51fa\u5bf9\u5e72\u6270\u548c\u6b3a\u9a97\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u5730\u9762\u6ce2\u4f20\u64ad\u5ef6\u8fdf\u53d7\u5730\u8868\u6761\u4ef6\u53d8\u5316\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6c14\u8c61\u56e0\u7d20\u5bf9\u4f20\u64ad\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e0d\u540c\u4fe1\u53f7\u8def\u5f84\u70b9\u4e0aeLoran\u4fe1\u53f7\u4f20\u64ad\u5ef6\u8fdf\u7684\u65f6\u95f4\u53d8\u5316\u4e0e\u6c14\u8c61\u56e0\u7d20\u7684\u76f8\u5173\u6027\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6c14\u8c61\u56e0\u7d20\u4e0eeLoran\u4fe1\u53f7\u4f20\u64ad\u5ef6\u8fdf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u6c14\u8c61\u56e0\u7d20\u663e\u8457\u5f71\u54cdeLoran\u4fe1\u53f7\u7684\u4f20\u64ad\u5ef6\u8fdf\uff0c\u8fd9\u5bf9eLoran\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u6539\u8fdb\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.25718", "pdf": "https://arxiv.org/pdf/2509.25718", "abs": "https://arxiv.org/abs/2509.25718", "authors": ["Si-Cheng Wang", "Tian-Yu Xiang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Ao-Qun Jin", "Zeng-Guang Hou"], "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) is a promising avenue for post-training\nvision-language-action (VLA) models, but practical deployment is hindered by\nsparse rewards and unstable training. This work mitigates these challenges by\nintroducing an action chunk based on proximal policy optimization (PPO) with\nbehavior cloning using self-collected demonstrations. Aggregating consecutive\nactions into chunks improves the temporal consistency of the policy and the\ndensity of informative feedback. In addition, an auxiliary behavior cloning\nloss is applied with a dynamically updated demonstration buffer that\ncontinually collects high-quality task trials during training. The relative\nweight between the action-chunked PPO objective and the self behavior clone\nauxiliary loss is adapted online to stabilize the post-training process.\nExperiments on the MetaWorld benchmark indicate improved performance over\nsupervised fine-tuning, achieving a high success rate (0.93) and few steps to\nsuccess (42.17). These results demonstrate the viability of RL for VLA\npost-training and help lay the groundwork for downstream VLA applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u4f5c\u5757\u7684PPO\u548c\u884c\u4e3a\u514b\u9686\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7a00\u758f\u5956\u52b1\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u4f5c\u5757\u7684PPO\u548c\u884c\u4e3a\u514b\u9686\u7ed3\u5408\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u6f14\u793a\u7f13\u51b2\u533a\u548c\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\uff0c\u63d0\u5347\u7b56\u7565\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u9ad8\u8fbe0.93\uff0c\u4e14\u4efb\u52a1\u5b8c\u6210\u6b65\u6570\u8f83\u5c11\uff0842.17\u6b65\uff09\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u5728VLA\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.26067", "pdf": "https://arxiv.org/pdf/2509.26067", "abs": "https://arxiv.org/abs/2509.26067", "authors": ["S. Fatemeh Bozorgi", "S. Mohammad Razavizadeh", "Mohsen Rezaee"], "title": "Enhancing Connectivity for Emergency Vehicles Through UAV Trajectory and Resource Allocation Optimization", "categories": ["eess.SP"], "comment": "This is the author's accepted manuscript. The final version of record\n  is published in Physical Communication, DOI:\n  https://doi.org/10.1016/j.phycom.2025.102826", "summary": "Effective communication for emergency vehicles - such as ambulances and fire\ntrucks - is essential to support their operations in various traffic and\nenvironmental conditions. In this context, this paper investigates a vehicular\ncommunication system assisted by an Unmanned Aerial Vehicle (UAV), which\nadjusts its trajectory and resource allocation according to communication\nneeds. The system classifies vehicles into two groups to address their varying\nservice requirements: emergency vehicles, which require a minimum instantaneous\ndata rate to access critical information timely, and normal vehicles. To\nsupport both categories effectively, this paper proposes a joint optimization\napproach that coordinates UAV trajectory planning and Dynamic Bandwidth\nAllocation (DBA). The objective is to maximize the minimum average data rate\nfor normal vehicles while ensuring that emergency vehicles maintain an\ninstantaneous rate above a predefined threshold. This approach takes into\naccount some system constraints, including UAV propulsion power consumption,\nmobility limitations, and backhaul capacity. To tackle the resulting non-convex\nproblem, an iterative optimization method is employed, where the original\nproblem is decomposed into two subproblems: bandwidth allocation and UAV\ntrajectory design. In each iteration, the trajectory subproblem is solved using\nthe Successive Convex Approximation (SCA) method. Numerical results confirm\nthat the proposed solution achieves superior performance in meeting service\nrequirements compared to baseline methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u8f66\u8f7d\u901a\u4fe1\u7cfb\u7edf\uff0c\u9488\u5bf9\u7d27\u6025\u8f66\u8f86\u548c\u666e\u901a\u8f66\u8f86\u7684\u4e0d\u540c\u9700\u6c42\u8fdb\u884c\u8054\u5408\u4f18\u5316\u8bbe\u8ba1\uff0c\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u548c\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u7d27\u6025\u8f66\u8f86\uff08\u5982\u6551\u62a4\u8f66\u548c\u6d88\u9632\u8f66\uff09\u5728\u590d\u6742\u4ea4\u901a\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u9700\u8981\u9ad8\u6548\u7684\u901a\u4fe1\u652f\u6301\uff0c\u4ee5\u5b9e\u65f6\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u534f\u8c03\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u52a8\u6001\u5e26\u5bbd\u5206\u914d\uff08DBA\uff09\uff0c\u5206\u89e3\u4e3a\u975e\u51f8\u95ee\u9898\u7684\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u65b9\u6cd5\u6c42\u89e3\u8f68\u8ff9\u5b50\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u6ee1\u8db3\u670d\u52a1\u9700\u6c42\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u8f66\u8f7d\u901a\u4fe1\u7cfb\u7edf\u80fd\u6709\u6548\u652f\u6301\u7d27\u6025\u8f66\u8f86\u548c\u666e\u901a\u8f66\u8f86\u7684\u5dee\u5f02\u5316\u901a\u4fe1\u9700\u6c42\uff0c\u4f18\u5316\u540e\u7684\u8d44\u6e90\u5206\u914d\u548c\u8f68\u8ff9\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.25746", "pdf": "https://arxiv.org/pdf/2509.25746", "abs": "https://arxiv.org/abs/2509.25746", "authors": ["Shuaijun Wang", "Haoran Zhou", "Diyun Xiang", "Yangwei You"], "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses", "categories": ["cs.RO"], "comment": "9 pages, 9 figures", "summary": "Despite progress in both traditional dexterous grasping pipelines and recent\nVision-Language-Action (VLA) approaches, the grasp execution stage remains\nprone to pose inaccuracies, especially in long-horizon tasks, which undermines\noverall performance. To address this \"last-mile\" challenge, we propose\nTacRefineNet, a tactile-only framework that achieves fine in-hand pose\nrefinement of known objects in arbitrary target poses using multi-finger\nfingertip sensing. Our method iteratively adjusts the end-effector pose based\non tactile feedback, aligning the object to the desired configuration. We\ndesign a multi-branch policy network that fuses tactile inputs from multiple\nfingers along with proprioception to predict precise control updates. To train\nthis policy, we combine large-scale simulated data from a physics-based tactile\nmodel in MuJoCo with real-world data collected from a physical system.\nComparative experiments show that pretraining on simulated data and fine-tuning\nwith a small amount of real data significantly improves performance over\nsimulation-only training. Extensive real-world experiments validate the\neffectiveness of the method, achieving millimeter-level grasp accuracy using\nonly tactile input. To our knowledge, this is the first method to enable\narbitrary in-hand pose refinement via multi-finger tactile sensing alone.\nProject website is available at https://sites.google.com/view/tacrefinenet", "AI": {"tldr": "TacRefineNet\u662f\u4e00\u79cd\u4ec5\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u5b9e\u73b0\u5df2\u77e5\u7269\u4f53\u5728\u4efb\u610f\u76ee\u6807\u4f4d\u59ff\u4e0b\u7cbe\u786e\u8c03\u6574\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u6307\u5c16\u89e6\u89c9\u4f20\u611f\u548c\u672c\u4f53\u611f\u77e5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7075\u5de7\u6293\u53d6\u548cVLA\u65b9\u6cd5\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u56e0\u4f4d\u59ff\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u652f\u7b56\u7565\u7f51\u7edc\uff0c\u878d\u5408\u591a\u6307\u5c16\u89e6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u9884\u6d4b\u63a7\u5236\u66f4\u65b0\uff1b\u7ed3\u5408\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728\u4eff\u771f\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u5e76\u7528\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u73b0\u6beb\u7c73\u7ea7\u6293\u53d6\u7cbe\u5ea6\u3002", "conclusion": "TacRefineNet\u662f\u9996\u4e2a\u4ec5\u901a\u8fc7\u591a\u6307\u5c16\u89e6\u89c9\u4f20\u611f\u5b9e\u73b0\u4efb\u610f\u624b\u5185\u4f4d\u59ff\u7cbe\u786e\u8c03\u6574\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.26249", "pdf": "https://arxiv.org/pdf/2509.26249", "abs": "https://arxiv.org/abs/2509.26249", "authors": ["Ali Khandan Boroujeni", "Hyeon Seok Rou", "Ghazal Bagheri", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan K\u00f6psell", "Rafael F. Schaefer"], "title": "Secrecy-Driven Beamforming for Multi-User Integrated Sensing and Communication", "categories": ["eess.SP"], "comment": "Submitted to an IEEE conference", "summary": "This paper proposes a secure integrated sensing and communications (ISAC)\nframework for multi-user systems with multiple communication users (CUs) and\nadversarial targets, where the design problem is formulated to maximize secrecy\nrate under joint sensing and communication constraints. An efficient solution\nis presented based on an accelerated fractional programming method using a\nnon-homogeneous complex quadratic transform (QT), which decomposes the problem\ninto tractable subproblems for beamforming and artificial noise (AN)\noptimization. Unlike conventional artificial noise strategies, the proposed\napproach also exploits AN to enhance sensing while avoiding interference with\nlegitimate users. Simulation results show significant gains in secrecy rate,\ncommunication reliability, and sensing accuracy, confirming the effectiveness\nand scalability of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7528\u6237\u7cfb\u7edf\u4e2d\u517c\u987e\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u5b89\u5168\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u901f\u5206\u6570\u89c4\u5212\u548c\u4e8c\u6b21\u53d8\u6362\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548c\u4eba\u5de5\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u5bc6\u7387\u3001\u901a\u4fe1\u53ef\u9760\u6027\u548c\u611f\u77e5\u7cbe\u5ea6\u3002", "motivation": "\u5728\u591a\u7528\u6237\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5728\u786e\u4fdd\u901a\u4fe1\u5b89\u5168\u7684\u540c\u65f6\u517c\u987e\u611f\u77e5\u9700\u6c42\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u654c\u5bf9\u76ee\u6807\u7684\u573a\u666f\u4e0b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u975e\u9f50\u6b21\u590d\u4e8c\u6b21\u53d8\u6362\u7684\u52a0\u901f\u5206\u6570\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u6ce2\u675f\u6210\u5f62\u548c\u4eba\u5de5\u566a\u58f0\u4f18\u5316\u7684\u5b50\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u4eba\u5de5\u566a\u58f0\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u5bc6\u7387\u3001\u901a\u4fe1\u53ef\u9760\u6027\u548c\u611f\u77e5\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5b89\u5168\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25747", "pdf": "https://arxiv.org/pdf/2509.25747", "abs": "https://arxiv.org/abs/2509.25747", "authors": ["Jialei Huang", "Zhaoheng Yin", "Yingdong Hu", "Shuo Wang", "Xingyu Lin", "Yang Gao"], "title": "Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real", "categories": ["cs.RO", "I.2.9"], "comment": "10 pages, 6 figures", "summary": "Sim-to-real transfer remains a fundamental challenge in robot manipulation\ndue to the entanglement of perception and control in end-to-end learning. We\npresent a decoupled framework that learns each component where it is most\nreliable: control policies are trained in simulation with privileged state to\nmaster spatial layouts and manipulation dynamics, while perception is adapted\nonly at deployment to bridge real observations to the frozen control policy.\nOur key insight is that control strategies and action patterns are universal\nacross environments and can be learned in simulation through systematic\nrandomization, while perception is inherently domain-specific and must be\nlearned where visual observations are authentic. Unlike existing end-to-end\napproaches that require extensive real-world data, our method achieves strong\nperformance with only 10-20 real demonstrations by reducing the complex\nsim-to-real problem to a structured perception alignment task. We validate our\napproach on tabletop manipulation tasks, demonstrating superior data efficiency\nand out-of-distribution generalization compared to end-to-end baselines. The\nlearned policies successfully handle object positions and scales beyond the\ntraining distribution, confirming that decoupling perception from control\nfundamentally improves sim-to-real transfer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u90e8\u7f72\u65f6\u4ec5\u8c03\u6574\u611f\u77e5\u6a21\u5757\uff0c\u4ee5\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684sim-to-real\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u9762\u4e34\u611f\u77e5\u4e0e\u63a7\u5236\u8026\u5408\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u771f\u5b9e\u6570\u636e\u9700\u6c42\u91cf\u5927\u4e14\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u89e3\u8026\u6846\u67b6\uff1a\u63a7\u5236\u7b56\u7565\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\uff08\u5229\u7528\u7279\u6743\u72b6\u6001\uff09\uff0c\u611f\u77e5\u6a21\u5757\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9002\u914d\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u3002", "result": "\u5728\u684c\u9762\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4ec5\u970010-20\u6b21\u771f\u5b9e\u6f14\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u8272\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u89e3\u8026\u611f\u77e5\u4e0e\u63a7\u5236\u80fd\u6709\u6548\u63d0\u5347sim-to-real\u8fc1\u79fb\u6027\u80fd\uff0c\u51cf\u5c11\u771f\u5b9e\u6570\u636e\u9700\u6c42\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.26286", "pdf": "https://arxiv.org/pdf/2509.26286", "abs": "https://arxiv.org/abs/2509.26286", "authors": ["Jiaming Zhang", "Jiajun He", "Jie Zhang", "Okan Yurduseven"], "title": "FinGAN: An Interpretable RSS Generation Network for Scalable Fingerprint Localization", "categories": ["eess.SP"], "comment": null, "summary": "This work introduces FinGAN, a robust received signal strength (RSS) data\ngenerator designed to expand RSS fingerprint datasets. Compared to existing\ngenerative adversarial models that either rely on known reference positions\n(RPs) or depend on predefined priors, FinGAN learns the latent information\nbetween RPs and RSS values by maximizing the mutual information between the\ngenerated RSS data and the RPs, enabling an end-to-end RSS generation directly\nfrom RPs. This allows us to accurately generate RSS data for previously\nunmeasured RPs. Both quantitative and qualitative evaluations demonstrate that\nFinGAN produces synthetic RSS data closely aligned with real RSS sample\ncollected from the on-site experiment, preserving localization performance\ncomparable to that achieved with complete real-world datasets. To further\nvalidate its generalizability, FinGAN is also trained and evaluated on\nopen-source datasets from three typical office environments,and the results\ndemonstrate consistent performance across different scenarios.", "AI": {"tldr": "FinGAN\u662f\u4e00\u79cd\u7528\u4e8e\u6269\u5145RSS\u6307\u7eb9\u6570\u636e\u96c6\u7684\u4fe1\u53f7\u5f3a\u5ea6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u80fd\u591f\u4ece\u672a\u6d4b\u91cf\u7684\u53c2\u8003\u70b9\u751f\u6210\u51c6\u786e\u7684RSS\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5bf9\u6297\u6a21\u578b\u4f9d\u8d56\u5df2\u77e5\u53c2\u8003\u70b9\u6216\u9884\u5b9a\u4e49\u5148\u9a8c\uff0c\u800cFinGAN\u901a\u8fc7\u6700\u5927\u5316\u751f\u6210\u6570\u636e\u4e0e\u53c2\u8003\u70b9\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684RSS\u751f\u6210\u3002", "method": "FinGAN\u4ece\u53c2\u8003\u70b9\u76f4\u63a5\u751f\u6210RSS\u6570\u636e\uff0c\u901a\u8fc7\u5b66\u4e60RSS\u503c\u4e0e\u53c2\u8003\u70b9\u95f4\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u4f9d\u8d56\u9884\u5b9a\u4e49\u5148\u9a8c\u7684\u9700\u6c42\u3002", "result": "FinGAN\u751f\u6210\u7684\u5408\u6210RSS\u6570\u636e\u4e0e\u5b9e\u5730\u5b9e\u9a8c\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5b9a\u4f4d\u6027\u80fd\u63a5\u8fd1\u5b8c\u6574\u771f\u5b9e\u6570\u636e\u96c6\u3002", "conclusion": "FinGAN\u5728\u4e0d\u540c\u529e\u516c\u73af\u5883\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.25756", "pdf": "https://arxiv.org/pdf/2509.25756", "abs": "https://arxiv.org/abs/2509.25756", "authors": ["Yixian Zhang", "Shu'ang Yu", "Tonghe Zhang", "Mo Guang", "Haojia Hui", "Kaiwen Long", "Yu Wang", "Chao Yu", "Wenbo Ding"], "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Training expressive flow-based policies with off-policy reinforcement\nlearning is notoriously unstable due to gradient pathologies in the multi-step\naction sampling process. We trace this instability to a fundamental connection:\nthe flow rollout is algebraically equivalent to a residual recurrent\ncomputation, making it susceptible to the same vanishing and exploding\ngradients as RNNs. To address this, we reparameterize the velocity network\nusing principles from modern sequential models, introducing two stable\narchitectures: Flow-G, which incorporates a gated velocity, and Flow-T, which\nutilizes a decoded velocity. We then develop a practical SAC-based algorithm,\nenabled by a noise-augmented rollout, that facilitates direct end-to-end\ntraining of these policies. Our approach supports both from-scratch and\noffline-to-online learning and achieves state-of-the-art performance on\ncontinuous control and robotic manipulation benchmarks, eliminating the need\nfor common workarounds like policy distillation or surrogate objectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u591a\u6b65\u52a8\u4f5c\u91c7\u6837\u4e2d\u68af\u5ea6\u4e0d\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u901f\u5ea6\u7f51\u7edc\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7a33\u5b9a\u67b6\u6784Flow-G\u548cFlow-T\uff0c\u5e76\u7ed3\u5408SAC\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "motivation": "\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e38\u56e0\u591a\u6b65\u52a8\u4f5c\u91c7\u6837\u8fc7\u7a0b\u7684\u68af\u5ea6\u95ee\u9898\u800c\u4e0d\u7a33\u5b9a\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u901f\u5ea6\u7f51\u7edc\uff0c\u5f15\u5165Flow-G\u548cFlow-T\u4e24\u79cd\u7a33\u5b9a\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u566a\u58f0\u589e\u5f3a\u7684rollout\u548cSAC\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u5e38\u89c1\u7684\u7b56\u7565\u84b8\u998f\u6216\u66ff\u4ee3\u76ee\u6807\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u6d41\u7b56\u7565\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u7b80\u5316\u4e86\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2509.26311", "pdf": "https://arxiv.org/pdf/2509.26311", "abs": "https://arxiv.org/abs/2509.26311", "authors": ["Hassaan Hashmi", "Spyridon Pougkakiotis", "Dionysis Kalogerias"], "title": "Ultra-Reliable Risk-Aggregated Sum Rate Maximization via Model-Aided Deep Learning", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "We consider the problem of maximizing weighted sum rate in a multiple-input\nsingle-output (MISO) downlink wireless network with emphasis on user rate\nreliability. We introduce a novel risk-aggregated formulation of the complex\nWSR maximization problem, which utilizes the Conditional Value-at-Risk (CVaR)\nas a functional for enforcing rate (ultra)-reliability over channel fading\nuncertainty/risk. We establish a WMMSE-like equivalence between the proposed\nprecoding problem and a weighted risk-averse MSE problem, enabling us to design\na tailored unfolded graph neural network (GNN) policy function approximation\n(PFA), named {\\alpha}-Robust Graph Neural Network ({\\alpha}RGNN), trained to\nmaximize lower-tail (CVaR) rates resulting from adverse wireless channel\nrealizations (e.g., deep fading, attenuation). We empirically demonstrate that\na trained {\\alpha}RGNN fully eliminates per user deep rate fades, and\nsubstantially and optimally reduces statistical user rate variability while\nretaining adequate ergodic performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u805a\u5408\u7684\u591a\u8f93\u5165\u5355\u8f93\u51fa\uff08MISO\uff09\u4e0b\u884c\u94fe\u8def\u65e0\u7ebf\u7f51\u7edc\u4e2d\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08CVaR\uff09\u4fdd\u969c\u7528\u6237\u901f\u7387\u53ef\u9760\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3a\u03b1RGNN\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u3002", "motivation": "\u5728\u591a\u8f93\u5165\u5355\u8f93\u51fa\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u9700\u8981\u517c\u987e\u7528\u6237\u901f\u7387\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u4fe1\u9053\u8870\u843d\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u3002\u4f20\u7edf\u7684\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u805a\u5408\u7684\u52a0\u6743\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5229\u7528CVaR\u4f5c\u4e3a\u529f\u80fd\u51fd\u6570\uff0c\u5efa\u7acb\u4e86\u4e0e\u52a0\u6743\u98ce\u9669\u538c\u6076MSE\u95ee\u9898\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u03b1RGNN\u56fe\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u03b1RGNN\u80fd\u591f\u5b8c\u5168\u6d88\u9664\u7528\u6237\u6df1\u5c42\u901f\u7387\u8870\u843d\uff0c\u663e\u8457\u964d\u4f4e\u7528\u6237\u901f\u7387\u7edf\u8ba1\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u904d\u5386\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u901f\u7387\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u98ce\u9669\u805a\u5408\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.25822", "pdf": "https://arxiv.org/pdf/2509.25822", "abs": "https://arxiv.org/abs/2509.25822", "authors": ["Jing Wang", "Weiting Peng", "Jing Tang", "Zeyu Gong", "Xihua Wang", "Bo Tao", "Li Cheng"], "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies", "categories": ["cs.RO"], "comment": "42 pages, 17 figures, 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP--AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle--consistent contrastive\nloss that organizes the gradient flow of the noise predictor into a coherent\nperception--action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP--AG\nsignificantly outperforms state--of--the--art methods across simulation\nbenchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG\noffers a promising step toward bridging biological adaptability and artificial\npolicy learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDP-AG\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u5f15\u5bfc\u7684\u6269\u6563\u7b56\u7565\u7edf\u4e00\u611f\u77e5\u548c\u52a8\u4f5c\u7684\u5b66\u4e60\uff0c\u5229\u7528\u6982\u7387\u6f5c\u5728\u52a8\u6001\u5efa\u6a21\u4e8c\u8005\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5c06\u611f\u77e5\u548c\u52a8\u4f5c\u89e3\u8026\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u884c\u4e3a\u4e2d\u611f\u77e5\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u56e0\u679c\u4ea4\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86DP-AG\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5c06\u6f5c\u5728\u89c2\u5bdf\u7f16\u7801\u4e3a\u9ad8\u65af\u540e\u9a8c\uff0c\u5e76\u7528\u52a8\u4f5c\u5f15\u5bfc\u7684SDE\u6f14\u5316\uff0c\u540c\u65f6\u5f15\u5165\u5faa\u73af\u4e00\u81f4\u5bf9\u6bd4\u635f\u5931\u6765\u4fc3\u8fdb\u53cc\u5411\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDP-AG\u5728\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9eUR5\u64cd\u7eb5\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DP-AG\u4e3a\u751f\u7269\u9002\u5e94\u6027\u4e0e\u4eba\u5de5\u7b56\u7565\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.26333", "pdf": "https://arxiv.org/pdf/2509.26333", "abs": "https://arxiv.org/abs/2509.26333", "authors": ["Kexin Chen", "Yijie Mao", "Wonjae Shin"], "title": "Transmitter-Side Beyond-Diagonal RIS-Enabled Integrated Sensing and Communications", "categories": ["eess.SP"], "comment": null, "summary": "Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) have emerged as\na promising technology for 6G wireless networks, offering more advanced control\nover electromagnetic wave propagation than conventional diagonal RIS. This\npaper proposes a novel integrated sensing and communication (ISAC) framework\nthat incorporates BD-RIS at the transmitter. This not only opens the door to\nenhanced sensing and communication performance, but also alleviates the need\nfor large-scale fully digital radio frequency (RF) chains at the transmitter.\nBased on the proposed system model, we formulate a normalized weighted\noptimization problem to jointly design the active beamforming and the BD-RIS\nscattering matrix with the aim of jointly minimizing the trace of the\nCram\\'er-Rao bound (CRB) for sensing targets and maximizing the sum rate (SR)\nfor communication users. To address this highly coupled optimization problem,\nwe propose a novel and low-complexity iterative algorithm that efficiently\nsolves the active beamforming and scattering matrix subproblems by transforming\neach into a series of tractable projection problems with closed-form solutions.\nNumerical results show the appealing capability of the transmitter-side\nBD-RIS-aided ISAC over conventional diagonal RIS-aided ISAC in enhancing both\nsensing and communication performance. Moreover, compared to the classic\niterative algorithm, the proposed algorithm offers enhanced dual-functional\nperformance while significantly reducing the computational complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6846\u67b6\uff0c\u5229\u7528\u53d1\u5c04\u7aef\u7684BD-RIS\u6280\u672f\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u548c\u6563\u5c04\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "BD-RIS\u6280\u672f\u4e3a6G\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u5148\u8fdb\u7684\u7535\u78c1\u6ce2\u63a7\u5236\u80fd\u529b\uff0c\u5c24\u5176\u5728ISAC\u6846\u67b6\u4e2d\uff0c\u53d1\u5c04\u7aef\u7684BD-RIS\u4e0d\u4ec5\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u80fd\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6570\u5b57RF\u94fe\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u8fed\u4ee3\u7b97\u6cd5\uff0c\u5c06\u8026\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u89e3\u51b3\u7684\u6295\u5f71\u5b50\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u95ed\u5f0f\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u53d1\u5c04\u7aefBD-RIS-aided ISAC\u5728\u4f20\u611f\u548c\u901a\u4fe1\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u5bf9\u89d2RIS-aided ISAC\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "BD-RIS\u6280\u672f\u5728\u53d1\u5c04\u7aef\u7684\u5e94\u7528\u4e3aISAC\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u548c\u590d\u6742\u5ea6\u4f18\u52bf\u3002"}}
{"id": "2509.25852", "pdf": "https://arxiv.org/pdf/2509.25852", "abs": "https://arxiv.org/abs/2509.25852", "authors": ["Zitong Bo", "Yue Hu", "Jinming Ma", "Mingliang Zhou", "Junhui Yin", "Yachen Kang", "Yuqi Liu", "Tong Wu", "Diyun Xiang", "Hao Chen"], "title": "Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Enabling robots to execute long-horizon manipulation tasks from free-form\nlanguage instructions remains a fundamental challenge in embodied AI. While\nvision-language models (VLMs) have shown promise as high-level planners, their\ndeployment in the real world is hindered by two gaps: (i) the scarcity of\nlarge-scale, sequential manipulation data that couples natural language with\nmulti-step action plans, and (ii) the absence of dense, interpretable rewards\nfor fine-tuning VLMs on planning objectives. To address these issues, we\npropose REVER, a framework that empowers VLMs to generate and validate\nlong-horizon manipulation plans from natural language instructions in\nreal-world scenarios. Under REVER we train and release RoboFarseer, a VLM\nincentivized to emit chain-of-thought that perform temporal and spatial\nreasoning, ensuring physically plausible and logically coherent plans. To\nobtain training data, we leverage the Universal Manipulation Interface\nframework to capture hardware-agnostic demonstrations of atomic skills. An\nautomated annotation engine converts each demonstration into\nvision-instruction-plan triplet. We introduce a verifiable reward that scores\nthe generated plan by its ordered bipartite matching overlap with the\nground-truth skill sequence. At run time, the fine-tuned VLM functions both as\na planner and as a monitor, verifying step-wise completion. RoboFarseer matches\nor exceeds the performance of proprietary models that are orders of magnitude\nlarger, while on open-ended planning it surpasses the best baseline by more\nthan 40%. In real-world, long-horizon tasks, the complete system boosts overall\nsuccess by roughly 60% compared with the same low-level controller without the\nplanner. We will open-source both the dataset and the trained model upon\npublication.", "AI": {"tldr": "REVER\u6846\u67b6\u901a\u8fc7RoboFarseer VLM\u89e3\u51b3\u673a\u5668\u4eba\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u7684\u6311\u6218\uff0c\u5229\u7528\u7a20\u5bc6\u5956\u52b1\u548c\u9a8c\u8bc1\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\uff0c\u586b\u8865\u6570\u636e\u96c6\u548c\u7a20\u5bc6\u5956\u52b1\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faREVER\u6846\u67b6\uff0c\u8bad\u7ec3RoboFarseer VLM\uff0c\u5229\u7528\u81ea\u52a8\u5316\u6807\u6ce8\u5f15\u64ce\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u53ef\u9a8c\u8bc1\u5956\u52b1\u3002", "result": "RoboFarseer\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf40%\uff0c\u5b9e\u9645\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534760%\u3002", "conclusion": "REVER\u548cRoboFarseer\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u89c4\u5212\u95ee\u9898\uff0c\u5c06\u5f00\u6e90\u6570\u636e\u548c\u6a21\u578b\u3002"}}
{"id": "2509.26356", "pdf": "https://arxiv.org/pdf/2509.26356", "abs": "https://arxiv.org/abs/2509.26356", "authors": ["Yifeng Zhang", "Xiao Liang"], "title": "A Physics-Informed Multi-Source Domain Adaptation Framework for Label-Free Post-Earthquake Damage Assessment", "categories": ["eess.SP"], "comment": null, "summary": "Efficient and intelligent assessment of post-earthquake structural damage is\ncritical for rapid disaster response. While data-driven approaches have shown\npromise, traditional supervised learning methods rely on extensive labeled\ndatasets, which are often impractical to obtain for damaged structures. To\naddress this limitation, we propose a physics-informed multi-source domain\nadaptation framework to predict post-earthquake structural damage for a target\nbuilding without requiring damage labels. The multi-source domain integrates\nactual damage data and numerical modeling data from buildings similar to the\ntarget structure. The framework operates through three key steps. First, the\nsimilarity of key physics from each domain are analyzed to form a weight\nmatrix, which enhances domain differentiation. Second, features from the\nmulti-source and target domains are extracted and fed into a classifier and a\ndiscriminator. The classifier ensures that the features are damage-sensitive\nand accurately assign damage states, while the discriminator enforces that the\nfeatures remain domain-invariant. Finally, the key parameters matrix is applied\nas weights during adversarial training to optimize the contribution of features\nfrom each source domain. The proposed framework provides a robust solution for\nassessing structural damage in scenarios where labeled data is scarce,\nsignificantly advancing the capabilities of post-earthquake damage evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u591a\u6e90\u57df\u9002\u5e94\u6846\u67b6\uff0c\u65e0\u9700\u76ee\u6807\u5efa\u7b51\u7684\u635f\u4f24\u6807\u7b7e\u5373\u53ef\u9884\u6d4b\u9707\u540e\u7ed3\u6784\u635f\u4f24\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u9707\u540e\u7ed3\u6784\u635f\u4f24\u7684\u9ad8\u6548\u667a\u80fd\u8bc4\u4f30\u5bf9\u707e\u5bb3\u5feb\u901f\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u7ed3\u5408\u5b9e\u9645\u635f\u4f24\u6570\u636e\u4e0e\u6570\u503c\u6a21\u62df\u6570\u636e\uff0c\u901a\u8fc7\u7269\u7406\u76f8\u4f3c\u6027\u5206\u6790\u6784\u5efa\u6743\u91cd\u77e9\u9635\uff0c\u5229\u7528\u5206\u7c7b\u5668\u548c\u5224\u522b\u5668\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u4e0e\u5bf9\u6297\u8bad\u7ec3\u3002", "result": "\u6846\u67b6\u5728\u591a\u6e90\u57df\u548c\u76ee\u6807\u57df\u95f4\u5b9e\u73b0\u4e86\u635f\u4f24\u654f\u611f\u4e14\u57df\u4e0d\u53d8\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u4f18\u5316\u4e86\u5404\u6e90\u57df\u7279\u5f81\u7684\u8d21\u732e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u7ed3\u6784\u635f\u4f24\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9707\u540e\u635f\u4f24\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2509.25945", "pdf": "https://arxiv.org/pdf/2509.25945", "abs": "https://arxiv.org/abs/2509.25945", "authors": ["Valentin Yuryev", "Max Polzin", "Josie Hughes"], "title": "State Estimation for Compliant and Morphologically Adaptive Robots", "categories": ["cs.RO"], "comment": "8 pages, 10 figures, 1 table, submitted to ICRA 2026", "summary": "Locomotion robots with active or passive compliance can show robustness to\nuncertain scenarios, which can be promising for agricultural, research and\nenvironmental industries. However, state estimation for these robots is\nchallenging due to the lack of rigid-body assumptions and kinematic changes\nfrom morphing. We propose a method to estimate typical rigid-body states\nalongside compliance-related states, such as soft robot shape in different\nmorphologies and locomotion modes. Our neural network-based state estimator\nuses a history of states and a mechanism to directly influence unreliable\nsensors. We test our framework on the GOAT platform, a robot capable of passive\ncompliance and active morphing for extreme outdoor terrain. The network is\ntrained on motion capture data in a novel compliance-centric frame that\naccounts for morphing-related states. Our method predicts shape-related\nmeasurements within 4.2% of the robot's size, velocities within 6.3% and 2.4%\nof the top linear and angular speeds, respectively, and orientation within 1.5\ndegrees. We also demonstrate a 300% increase in travel range during a motor\nmalfunction when using our estimator for closed-loop autonomous outdoor\noperation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5177\u5907\u4e3b\u52a8\u6216\u88ab\u52a8\u67d4\u6027\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u573a\u666f\u4e2d\u7684\u72b6\u6001\uff0c\u89e3\u51b3\u4e86\u56e0\u7f3a\u4e4f\u521a\u6027\u5047\u8bbe\u548c\u5f62\u6001\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u67d4\u6027\u673a\u5668\u4eba\u5728\u519c\u4e1a\u3001\u7814\u7a76\u548c\u73af\u5883\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u521a\u6027\u5047\u8bbe\u548c\u5f62\u6001\u53d8\u5316\u5bf9\u72b6\u6001\u4f30\u8ba1\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u5386\u53f2\u72b6\u6001\u548c\u4f20\u611f\u5668\u6570\u636e\uff0c\u76f4\u63a5\u4f5c\u7528\u4e8e\u4e0d\u53ef\u9760\u4f20\u611f\u5668\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u67d4\u6027\u5730\u5fc3\u5750\u6807\u7cfb\u6765\u8bad\u7ec3\u7f51\u7edc\u3002", "result": "\u5728GOAT\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e86\u5f62\u72b6\u6d4b\u91cf\u8bef\u5dee4.2%\u3001\u901f\u5ea6\u548c\u89d2\u5ea6\u8bef\u5dee\u5206\u522b\u4e3a6.3%\u548c2.4%\u3001\u65b9\u5411\u8bef\u5dee1.5\u5ea6\uff0c\u4e14\u5728\u7535\u673a\u6545\u969c\u65f6\u589e\u52a0\u4e86300%\u7684\u79fb\u52a8\u8303\u56f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u67d4\u6027\u673a\u5668\u4eba\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4f20\u611f\u5668\u4e0d\u53ef\u9760\u6216\u5f62\u6001\u53d8\u5316\u65f6\u7684\u8868\u73b0\u3002"}}
{"id": "2509.26395", "pdf": "https://arxiv.org/pdf/2509.26395", "abs": "https://arxiv.org/abs/2509.26395", "authors": ["Ugo Boscain", "Xiangyu Ma", "Dario Prandi", "Giuseppina Turco"], "title": "A solution to the mystery of the sub-harmonic series and to the combination tone via a linear mathematical model of the cochlea", "categories": ["eess.SP", "math.AP", "physics.bio-ph", "physics.class-ph", "physics.med-ph"], "comment": null, "summary": "In this paper, we study a simple linear model of the cochlea as a set of\nvibrating strings. We make hypothesis that the information sent to the auditory\ncortex is the energy stored in the strings and consider all oscillation modes\nof the strings. We show the emergence of the sub-harmonic series whose\nexistence was hypothesized in the XVI century to explain the consonance of the\nminor chord. We additionally show how the nonlinearity of the energy can be\nused to study the emergence of the combination tone (Tartini's third sound)\nshedding new light on this long debated subject.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8033\u8717\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u5047\u8bbe\u4fe1\u606f\u4f20\u9012\u57fa\u4e8e\u5f26\u7684\u80fd\u91cf\uff0c\u53d1\u73b0\u6b21\u8c10\u6ce2\u7cfb\u5217\u5e76\u4e0e\u5386\u53f2\u7406\u8bba\u76f8\u7b26\u3002", "motivation": "\u63a2\u7a76\u542c\u89c9\u4fe1\u606f\u4f20\u9012\u7684\u7269\u7406\u57fa\u7840\uff0c\u9a8c\u8bc1\u5386\u53f2\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u548c\u632f\u52a8\u5f26\u7684\u80fd\u91cf\u5047\u8bbe\uff0c\u5206\u6790\u6240\u6709\u632f\u8361\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u6b21\u8c10\u6ce2\u7cfb\u5217\uff0c\u89e3\u91ca\u4e86\u975e\u7ebf\u6027\u80fd\u91cf\u4e0b\u7684\u7ec4\u5408\u97f3\u73b0\u8c61\u3002", "conclusion": "\u6a21\u578b\u9a8c\u8bc1\u4e86\u5386\u53f2\u5047\u8bbe\uff0c\u4e3a\u542c\u89c9\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.25951", "pdf": "https://arxiv.org/pdf/2509.25951", "abs": "https://arxiv.org/abs/2509.25951", "authors": ["ChunPing Lam", "Xiangjia Chen", "Chenming Wu", "Hao Chen", "Binzhi Sun", "Guoxin Fang", "Charlie C. L. Wang", "Chengkai Dai", "Yeung Yam"], "title": "Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel human-robot interaction (HRI) framework that\nenables intuitive gesture-driven control through a capacitance-based woven\ntactile skin. Unlike conventional interfaces that rely on panels or handheld\ndevices, the woven tactile skin integrates seamlessly with curved robot\nsurfaces, enabling embodied interaction and narrowing the gap between human\nintent and robot response. Its woven design combines fabric-like flexibility\nwith structural stability and dense multi-channel sensing through the\ninterlaced conductive threads. Building on this capability, we define a\ngesture-action mapping of 14 single- and multi-touch gestures that cover\nrepresentative robot commands, including task-space motion and auxiliary\nfunctions. A lightweight convolution-transformer model designed for gesture\nrecognition in real time achieves an accuracy of near-100%, outperforming prior\nbaseline approaches. Experiments on robot arm tasks, including pick-and-place\nand pouring, demonstrate that our system reduces task completion time by up to\n57% compared with keyboard panels and teach pendants. Overall, our proposed\nframework demonstrates a practical pathway toward more natural and efficient\nembodied HRI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u7535\u5bb9\u5f0f\u7f16\u7ec7\u89e6\u89c9\u76ae\u80a4\u5b9e\u73b0\u76f4\u89c2\u7684\u624b\u52bf\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfHRI\u754c\u9762\uff08\u5982\u63a7\u5236\u9762\u677f\u6216\u624b\u6301\u8bbe\u5907\uff09\u7684\u4e0d\u7075\u6d3b\u6027\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u7535\u5bb9\u5f0f\u7f16\u7ec7\u89e6\u89c9\u76ae\u80a4\uff0c\u7ed3\u5408\u7ec7\u7269\u7075\u6d3b\u6027\u4e0e\u591a\u901a\u9053\u4f20\u611f\uff0c\u5b9a\u4e4914\u79cd\u624b\u52bf\u52a8\u4f5c\u6620\u5c04\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u5377\u79ef-Transformer\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u624b\u52bf\u8bc6\u522b\u3002", "result": "\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u63a5\u8fd1100%\uff0c\u673a\u5668\u4eba\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u51cf\u5c1157%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2509.26500", "pdf": "https://arxiv.org/pdf/2509.26500", "abs": "https://arxiv.org/abs/2509.26500", "authors": ["Hossein Nasiri", "Muhammad Iqbal Rochman", "Monisha Ghosh"], "title": "Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers", "categories": ["eess.SP", "cs.AI", "cs.NI"], "comment": "To be published in the proceedings of IEEE Military Communications\n  Conference (MILCOM) 2025", "summary": "The desirability of the mid-band frequency range (1 - 10 GHz) for federal and\ncommercial applications, combined with the growing applications for commercial\nindoor use-cases, such as factory automation, opens up a new approach to\nspectrum sharing: the same frequency bands used outdoors by federal incumbents\ncan be reused by commercial indoor users. A recent example of such sharing,\nbetween commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where\nunlicensed, low-power-indoor (LPI) users share the band with outdoor\nincumbents, primarily fixed microwave links. However, to date, there exist no\nreliable, automatic means of determining whether a device is indoors or\noutdoors, necessitating the use of other mechanisms such as mandating indoor\naccess points (APs) to have integrated antennas and not be battery powered, and\nreducing transmit power of client devices which may be outdoors. An accurate\nindoor/outdoor (I/O) classification addresses these challenges, enabling\nautomatic transmit power adjustments without interfering with incumbents. To\nthis end, we leverage the Global Navigation Satellite System (GNSS) signals for\nI/O classification. GNSS signals, designed inherently for outdoor reception and\nhighly susceptible to indoor attenuation and blocking, provide a robust and\ndistinguishing feature for environmental sensing. We develop various\nmethodologies, including threshold-based techniques and machine learning\napproaches and evaluate them using an expanded dataset gathered from diverse\ngeographical locations. Our results demonstrate that GNSS-based methods alone\ncan achieve greater accuracy than approaches relying solely on wireless (Wi-Fi)\ndata, particularly in unfamiliar locations. Furthermore, the integration of\nGNSS data with Wi-Fi information leads to improved classification accuracy,\nshowcasing the significant benefits of multi-modal data fusion.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528GNSS\u4fe1\u53f7\u8fdb\u884c\u5ba4\u5185/\u5ba4\u5916\uff08I/O\uff09\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e2d\u9891\u6bb5\u9891\u8c31\u5171\u4eab\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u4e2d\u9891\u6bb5\uff081-10 GHz\uff09\u5728\u8054\u90a6\u548c\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u9700\u6c42\u589e\u957f\uff0c\u5c24\u5176\u662f\u5ba4\u5185\u5546\u4e1a\u5e94\u7528\uff08\u5982\u5de5\u5382\u81ea\u52a8\u5316\uff09\uff0c\u4fc3\u4f7f\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u81ea\u52a8\u533a\u5206\u8bbe\u5907\u662f\u5426\u5728\u5ba4\u5185\uff0c\u4ee5\u907f\u514d\u5e72\u6270\u6237\u5916\u73b0\u6709\u7528\u6237\u3002", "method": "\u5229\u7528GNSS\u4fe1\u53f7\u7684\u7279\u6027\uff08\u5ba4\u5916\u63a5\u6536\u5f3a\u3001\u5ba4\u5185\u8870\u51cf\u5927\uff09\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u9608\u503c\u7684\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408Wi-Fi\u6570\u636e\u8fdb\u884c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4ec5\u4f7f\u7528GNSS\u4fe1\u53f7\u7684\u65b9\u6cd5\u5728\u964c\u751f\u73af\u5883\u4e2d\u6bd4\u4ec5\u4f9d\u8d56Wi-Fi\u6570\u636e\u7684\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u4e14GNSS\u4e0eWi-Fi\u6570\u636e\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "GNSS\u4fe1\u53f7\u662f\u4e00\u79cd\u6709\u6548\u7684\u5ba4\u5185/\u5ba4\u5916\u5206\u7c7b\u624b\u6bb5\uff0c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u9891\u8c31\u5171\u4eab\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25966", "pdf": "https://arxiv.org/pdf/2509.25966", "abs": "https://arxiv.org/abs/2509.25966", "authors": ["Peilong Han", "Fan Jia", "Min Zhang", "Yutao Qiu", "Hongyao Tang", "Yan Zheng", "Tiancai Wang", "Jianye Hao"], "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action\nmodel tailored for object navigation. It leverages semantic map abstractions to\nunify and structure historical information, encoding spatial context in a\ncompact and consistent form. MUVLA takes the current and history observations,\nas well as the semantic map, as inputs and predicts the action sequence based\non the description of goal object. Furthermore, it amplifies supervision\nthrough reward-guided return modeling based on dense short-horizon progress\nsignals, enabling the model to develop a detailed understanding of action value\nfor reward maximization. MUVLA employs a three-stage training pipeline:\nlearning map-level spatial understanding, imitating behaviors from\nmixed-quality demonstrations, and reward amplification. This strategy allows\nMUVLA to unify diverse demonstrations into a robust spatial representation and\ngenerate more rational exploration strategies. Experiments on HM3D and Gibson\nbenchmarks demonstrate that MUVLA achieves great generalization and learns\neffective exploration behaviors even from low-quality or partially successful\ntrajectories.", "AI": {"tldr": "MUVLA\u662f\u4e00\u79cd\u7528\u4e8e\u76ee\u6807\u5bfc\u822a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u62bd\u8c61\u6574\u5408\u5386\u53f2\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u5956\u52b1\u5f15\u5bfc\u7684\u5efa\u6a21\u4f18\u5316\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76ee\u6807\u5bfc\u822a\u95ee\u9898\uff0cMUVLA\u65e8\u5728\u7edf\u4e00\u548c\u7ed3\u6784\u5316\u5386\u53f2\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5bc6\u96c6\u77ed\u671f\u8fdb\u5ea6\u4fe1\u53f7\u4f18\u5316\u52a8\u4f5c\u9884\u6d4b\u3002", "method": "MUVLA\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a\u5b66\u4e60\u5730\u56fe\u7ea7\u7a7a\u95f4\u7406\u89e3\u3001\u4ece\u6df7\u5408\u8d28\u91cf\u6f14\u793a\u4e2d\u6a21\u4eff\u884c\u4e3a\uff0c\u4ee5\u53ca\u5956\u52b1\u589e\u5f3a\u3002", "result": "\u5728HM3D\u548cGibson\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMUVLA\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u4ece\u4f4e\u8d28\u91cf\u8f68\u8ff9\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "conclusion": "MUVLA\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5956\u52b1\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5bfc\u822a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.26508", "pdf": "https://arxiv.org/pdf/2509.26508", "abs": "https://arxiv.org/abs/2509.26508", "authors": ["Charlotte Muth", "Benedikt Geiger", "Daniel Gil Gaviria", "Laurent Schmalen"], "title": "Neural Network-Based Single-Carrier Joint Communication and Sensing: Loss Design, Constellation Shaping and Precoding", "categories": ["eess.SP"], "comment": "Published in IEEE Access Source code available at\n  https://github.com/frozenhairdryer/JCAS-loss-shape-precode", "summary": "We investigate the impact of higher-order modulation formats on the sensing\nperformance of single-carrier joint communication and sensing (JCAS) systems.\nSeveral separate components such as a beamformer, a modulator, a target\ndetector, an angle of arrival (AoA) estimator and a communication demapper are\nimplemented as trainable neural networks (NNs). We compare geometrically shaped\nmodulation formats to a classical quadrature amplitude modulation (QAM) scheme.\nWe assess the influence of multi-snapshot sensing and varying signal-to-noise\nratio (SNR) on the overall performance of the autoencoder-based system. To\nimprove the training behavior of the system, we decouple the loss functions\nfrom the respective SNR values and the number of sensing snapshots, using upper\nbounds of the sensing and communication performance, namely the Cram\\'er-Rao\nbound for AoA estimation and the mutual information for communication. The\nNN-based sensing outperforms classical algorithms, such as a Neyman-Pearson\nbased power detector for object detection and ESPRIT for AoA estimation for\nboth the trained constellations and QAM at low SNRs. We show that the gap in\nsensing performance between classical and shaped modulation formats can be\nsignificantly reduced through multi-snapshot sensing. Lastly, we demonstrate\nsystem extension to multi-user multiple-input multiple-output to address the\nimprovement of spatial efficiency when servicing multiple user equipments. Our\ncontribution emphasizes the importance of estimation bounds for training neural\nnetworks, especially when the trained solutions are deployed in varying SNR\nconditions.", "AI": {"tldr": "\u7814\u7a76\u9ad8\u9636\u8c03\u5236\u683c\u5f0f\u5bf9\u5355\u8f7d\u6ce2\u8054\u5408\u901a\u4fe1\u4e0e\u611f\u77e5\uff08JCAS\uff09\u7cfb\u7edf\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u5bf9\u6bd4\u51e0\u4f55\u5f62\u72b6\u8c03\u5236\u4e0e\u4f20\u7edfQAM\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f18\u5316JCAS\u7cfb\u7edf\u7684\u611f\u77e5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0c\u51cf\u5c11\u4f20\u7edf\u8c03\u5236\u4e0e\u51e0\u4f55\u5f62\u72b6\u8c03\u5236\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u5b9e\u73b0\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5982\u6ce2\u675f\u6210\u5f62\u5668\u548c\u8c03\u5236\u5668\uff0c\u5e76\u5229\u7528\u591a\u5feb\u7167\u611f\u77e5\u548c\u53bb\u8026\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u7684\u611f\u77e5\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff08\u5982Neyman-Pearson\u68c0\u6d4b\u5668\u548cESPRIT\uff09\uff0c\u5c24\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\uff1b\u591a\u5feb\u7167\u611f\u77e5\u663e\u8457\u7f29\u5c0f\u4e86\u4f20\u7edf\u4e0e\u51e0\u4f55\u5f62\u72b6\u8c03\u5236\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u4f30\u8ba1\u8fb9\u754c\u5bf9\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u53d8\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u90e8\u7f72\u65f6\u3002"}}
{"id": "2509.25984", "pdf": "https://arxiv.org/pdf/2509.25984", "abs": "https://arxiv.org/abs/2509.25984", "authors": ["Shengpeng Wang", "Yulong Xie", "Qing Liao", "Wei Wang"], "title": "S$^3$E: Self-Supervised State Estimation for Radar-Inertial System", "categories": ["cs.RO"], "comment": null, "summary": "Millimeter-wave radar for state estimation is gaining significant attention\nfor its affordability and reliability in harsh conditions. Existing\nlocalization solutions typically rely on post-processed radar point clouds as\nlandmark points. Nonetheless, the inherent sparsity of radar point clouds,\nghost points from multi-path effects, and limited angle resolution in\nsingle-chirp radar severely degrade state estimation performance. To address\nthese issues, we propose S$^3$E, a \\textbf{S}elf-\\textbf{S}upervised\n\\textbf{S}tate \\textbf{E}stimator that employs more richly informative radar\nsignal spectra to bypass sparse points and fuses complementary inertial\ninformation to achieve accurate localization. S$^3$E fully explores the\nassociation between \\textit{exteroceptive} radar and \\textit{proprioceptive}\ninertial sensor to achieve complementary benefits. To deal with limited angle\nresolution, we introduce a novel cross-fusion technique that enhances spatial\nstructure information by exploiting subtle rotational shift correlations across\nheterogeneous data. The experimental results demonstrate our method achieves\nrobust and accurate performance without relying on localization ground truth\nsupervision. To the best of our knowledge, this is the first attempt to achieve\nstate estimation by fusing radar spectra and inertial data in a complementary\nself-supervised manner.", "AI": {"tldr": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u56e0\u5176\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u7ecf\u6d4e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5728\u72b6\u6001\u4f30\u8ba1\u4e2d\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7684\u96f7\u8fbe\u70b9\u4e91\uff0c\u4f46\u53d7\u9650\u4e8e\u591a\u8def\u5f84\u6548\u5e94\u7684\u4f2a\u70b9\u548c\u89d2\u5ea6\u5206\u8fa8\u7387\uff0c\u6027\u80fd\u4e0d\u4f73\u3002S$^3$E\u901a\u8fc7\u81ea\u76d1\u7763\u878d\u5408\u96f7\u8fbe\u4fe1\u53f7\u9891\u8c31\u548c\u60ef\u6027\u6570\u636e\uff0c\u63d0\u51fa\u8de8\u6a21\u6001\u878d\u5408\u6280\u672f\u63d0\u5347\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65e0\u9700\u5730\u9762\u771f\u503c\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u7cbe\u51c6\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u6beb\u7c73\u6ce2\u96f7\u8fbe\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u70b9\u4e91\uff0c\u5b58\u5728\u591a\u8def\u5f84\u4f2a\u70b9\u548c\u89d2\u5ea6\u5206\u8fa8\u7387\u4f4e\u7684\u7f3a\u9677\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faS$^3$E\u65b9\u6cd5\uff0c\u81ea\u76d1\u7763\u878d\u5408\u96f7\u8fbe\u4fe1\u53f7\u9891\u8c31\u548c\u60ef\u6027\u6570\u636e\uff0c\u5f15\u5165\u8de8\u6a21\u6001\u878d\u5408\u6280\u672f\u589e\u5f3a\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u65e0\u5730\u9762\u771f\u503c\u76d1\u7763\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9c81\u68d2\u4e14\u7cbe\u51c6\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "S$^3$E\u9996\u6b21\u5c1d\u8bd5\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u878d\u5408\u96f7\u8fbe\u9891\u8c31\u548c\u60ef\u6027\u6570\u636e\uff0c\u4e3a\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.26572", "pdf": "https://arxiv.org/pdf/2509.26572", "abs": "https://arxiv.org/abs/2509.26572", "authors": ["Abdelhamid Salem", "Hao Xu", "Kai-Kit Wong", "Chan-Byoung Chae", "Yangyang Zhang"], "title": "Secure ISAC with Fluid Antenna Systems: Joint Precoding and Port Selection", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents a novel framework for enhancing physical-layer security\nin integrated sensing and communication (ISAC) systems by leveraging the\nreconfigurability of fluid antenna systems (FAS). We propose a joint precoding\nand port selection (JPPS) strategy that maximizes the sum secrecy rate while\nsimultaneously ensuring reliable radar sensing. The problem is formulated using\nfractional programming (FP) and solved through an iterative algorithm that\nintegrates FP transformations with successive convex approximation (SCA). To\nreduce computational complexity, we further develop low-complexity schemes\nbased on zero-forcing (ZF) precoding, combined with greedy port selection and\ntrace-inverse minimization. Simulation results demonstrate substantial\nimprovements in both secrecy performance and sensing accuracy compared to\nconventional baselines, across a wide range of FAS ports, user loads, and\nsensing targets. These findings highlight the critical importance of FAS\ngeometry optimization in enabling secure and efficient joint\ncommunication-sensing for next-generation wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u9884\u7f16\u7801\u548c\u7aef\u53e3\u9009\u62e9\uff08JPPS\uff09\u7b56\u7565\u63d0\u5347\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u96f7\u8fbe\u611f\u77e5\u53ef\u9760\u6027\u3002", "motivation": "\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u4e2d\u7269\u7406\u5c42\u5b89\u5168\u6027\u7684\u63d0\u5347\u9700\u6c42\uff0c\u4ee5\u53caFAS\u53ef\u91cd\u6784\u6027\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5206\u6570\u89c4\u5212\uff08FP\uff09\u548c\u9010\u6b21\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u7ed3\u5408\u4f4e\u590d\u6742\u5ea6\u96f6\u8feb\uff08ZF\uff09\u9884\u7f16\u7801\u548c\u8d2a\u5a6a\u7aef\u53e3\u9009\u62e9\u3002", "result": "\u4eff\u771f\u663e\u793a\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b89\u5168\u6027\u548c\u611f\u77e5\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FAS\u51e0\u4f55\u4f18\u5316\u5bf9\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b89\u5168\u9ad8\u6548\u7684\u8054\u5408\u901a\u4fe1-\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.25986", "pdf": "https://arxiv.org/pdf/2509.25986", "abs": "https://arxiv.org/abs/2509.25986", "authors": ["Elisabetta Zibetti", "Sureya Waheed Palmer", "Rebecca Stower", "Salvatore M Anzalone"], "title": "Emotionally Expressive Robots: Implications for Children's Behavior toward Robot", "categories": ["cs.RO"], "comment": null, "summary": "The growing development of robots with artificial emotional expressiveness\nraises important questions about their persuasive potential in children's\nbehavior. While research highlights the pragmatic value of emotional\nexpressiveness in human social communication, the extent to which robotic\nexpressiveness can or should influence empathic responses in children is\ngrounds for debate. In a pilot study with 22 children (aged 7-11) we begin to\nexplore the ways in which different levels of embodied expressiveness (body\nonly, face only, body and face) of two basic emotions (happiness and sadness)\ndisplayed by an anthropomorphic robot (QTRobot) might modify children's\nbehavior in a child-robot cooperative turn-taking game. We observed that\nchildren aligned their behavior to the robot's inferred emotional state.\nHowever, higher levels of expressiveness did not result in increased alignment.\nThe preliminary results reported here provide a starting point for reflecting\non robotic expressiveness and its role in shaping children's social-emotional\nbehavior toward robots as social peers in the near future.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u5bf9\u513f\u7ae5\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u513f\u7ae5\u4f1a\u6a21\u4eff\u673a\u5668\u4eba\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u66f4\u9ad8\u7684\u8868\u8fbe\u6c34\u5e73\u5e76\u672a\u589e\u52a0\u8fd9\u79cd\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740\u5177\u6709\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u7684\u673a\u5668\u4eba\u53d1\u5c55\uff0c\u7814\u7a76\u5176\u5728\u513f\u7ae5\u884c\u4e3a\u4e2d\u7684\u8bf4\u670d\u6f5c\u529b\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u521d\u6b65\u7814\u7a76\uff0822\u540d7-11\u5c81\u513f\u7ae5\uff09\uff0c\u8003\u5bdf\u673a\u5668\u4eba\u4e0d\u540c\u60c5\u611f\u8868\u8fbe\u6c34\u5e73\uff08\u8eab\u4f53\u3001\u9762\u90e8\u6216\u4e24\u8005\uff09\u5bf9\u513f\u7ae5\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u513f\u7ae5\u884c\u4e3a\u4f1a\u4e0e\u673a\u5668\u4eba\u60c5\u7eea\u72b6\u6001\u4e00\u81f4\uff0c\u4f46\u66f4\u9ad8\u8868\u8fbe\u6c34\u5e73\u672a\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u5728\u5851\u9020\u513f\u7ae5\u793e\u4ea4\u884c\u4e3a\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u601d\u8003\u3002"}}
{"id": "2509.26573", "pdf": "https://arxiv.org/pdf/2509.26573", "abs": "https://arxiv.org/abs/2509.26573", "authors": ["Vinay Kulkarni", "V. V. Reddy", "Neha Maheshwari"], "title": "Statistical Inference Framework for Extended Target Detection in mmWave Automotive Radar", "categories": ["eess.SP", "math.ST", "stat.TH"], "comment": "12 pages, 12 figures", "summary": "Millimeter wave (mmWave) radar systems, owing to their large bandwidth,\nprovide fine range resolution that enables the observation of multiple\nscatterers originating from a single automotive target commonly referred to as\nan extended target. Conventional CFAR-based detection algorithms typically\ntreat these scatterers as independent detections, thereby discarding the\nspatial scattering structure intrinsic to the target. To preserve this\nscattering spread, this paper proposes a Range-Doppler (RD) segment framework\ndesigned to encapsulate the typical scattering profile of an automobile. The\nstatistical characterization of the segment is performed using Maximum\nLikelihood Estimation (MLE) and posterior density modeling facilitated through\nGibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test\nstatistic, derived from the estimated statistical model, is introduced for\nbinary hypothesis classification of extended targets. Additionally, the paper\npresents a detection pipeline that incorporates Intersection over Union (IoU)\nand segment centering based on peak response, optimized to work within a single\ndwell. Extensive evaluations using both simulated and real-world datasets\ndemonstrate the effectiveness of the proposed approach, underscoring its\nsuitability for automotive radar applications through improved detection\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRange-Doppler\uff08RD\uff09\u7247\u6bb5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u7559\u6c7d\u8f66\u76ee\u6807\u7684\u7a7a\u95f4\u6563\u5c04\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\u548c\u68c0\u6d4b\u6d41\u7a0b\u4f18\u5316\u63d0\u9ad8\u4e86\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7cfb\u7edf\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfCFAR\u68c0\u6d4b\u7b97\u6cd5\u5c06\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e2d\u7684\u591a\u6563\u5c04\u4f53\u89c6\u4e3a\u72ec\u7acb\u68c0\u6d4b\uff0c\u5ffd\u7565\u4e86\u76ee\u6807\u7684\u6563\u5c04\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u4fdd\u7559\u8fd9\u79cd\u7ed3\u6784\uff0c\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528MLE\u548cGibbs MCMC\u91c7\u6837\u8fdb\u884cRD\u7247\u6bb5\u7684\u7edf\u8ba1\u5efa\u6a21\uff0c\u63d0\u51fa\u57fa\u4e8e\u504f\u5ea6\u7684\u5047\u8bbe\u68c0\u9a8c\uff0c\u5e76\u7ed3\u5408IoU\u548c\u5cf0\u503c\u54cd\u5e94\u4e2d\u5fc3\u5316\u4f18\u5316\u68c0\u6d4b\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6c7d\u8f66\u96f7\u8fbe\u5e94\u7528\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684RD\u7247\u6bb5\u6846\u67b6\u548c\u68c0\u6d4b\u6d41\u7a0b\u5728\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u7528\u6027\uff0c\u5c24\u5176\u9002\u5408\u6c7d\u8f66\u96f7\u8fbe\u5e94\u7528\u3002"}}
{"id": "2509.25999", "pdf": "https://arxiv.org/pdf/2509.25999", "abs": "https://arxiv.org/abs/2509.25999", "authors": ["Yann de Mont-Marin", "Louis Montaut", "Jean Ponce", "Martial Hebert", "Justin Carpentier"], "title": "On the Conic Complementarity of Planar Contacts", "categories": ["cs.RO"], "comment": null, "summary": "We present a unifying theoretical result that connects two foundational\nprinciples in robotics: the Signorini law for point contacts, which underpins\nmany simulation methods for preventing object interpenetration, and the center\nof pressure (also known as the zero-moment point), a key concept used in, for\ninstance, optimization-based locomotion control. Our contribution is the planar\nSignorini condition, a conic complementarity formulation that models general\nplanar contacts between rigid bodies. We prove that this formulation is\nequivalent to enforcing the punctual Signorini law across an entire contact\nsurface, thereby bridging the gap between discrete and continuous contact\nmodels. A geometric interpretation reveals that the framework naturally\ncaptures three physical regimes -sticking, separating, and tilting-within a\nunified complementarity structure. This leads to a principled extension of the\nclassical center of pressure, which we refer to as the extended center of\npressure. By establishing this connection, our work provides a mathematically\nconsistent and computationally tractable foundation for handling planar\ncontacts, with implications for both the accurate simulation of contact\ndynamics and the design of advanced control and optimization algorithms in\nlocomotion and manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\uff0c\u8fde\u63a5\u4e86\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u539f\u7406\uff1aSignorini\u70b9\u63a5\u89e6\u5b9a\u5f8b\u548c\u4e2d\u5fc3\u538b\u529b\uff08\u96f6\u529b\u77e9\u70b9\uff09\uff0c\u5e76\u901a\u8fc7\u5e73\u9762Signorini\u6761\u4ef6\u586b\u8865\u4e86\u79bb\u6563\u4e0e\u8fde\u7eed\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u673a\u5668\u4eba\u5b66\u4e2d\u79bb\u6563\u4e0e\u8fde\u7eed\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u63a5\u89e6\u52a8\u529b\u5b66\u6a21\u62df\u548c\u9ad8\u7ea7\u63a7\u5236\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u6570\u5b66\u4e00\u81f4\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u5e73\u9762Signorini\u6761\u4ef6\u7684\u9525\u4e92\u8865\u6027\u516c\u5f0f\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u5728\u6574\u4e2a\u63a5\u89e6\u9762\u4e0a\u5f3a\u5236Signorini\u5b9a\u5f8b\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u89e3\u91ca\u7edf\u4e00\u4e86\u4e09\u79cd\u7269\u7406\u72b6\u6001\uff08\u7c98\u9644\u3001\u5206\u79bb\u548c\u503e\u659c\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u7ecf\u5178\u4e2d\u5fc3\u538b\u529b\u7684\u6269\u5c55\u7248\u672c\uff08\u6269\u5c55\u4e2d\u5fc3\u538b\u529b\uff09\uff0c\u4e3a\u5e73\u9762\u63a5\u89e6\u63d0\u4f9b\u4e86\u6570\u5b66\u4e00\u81f4\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5e73\u9762\u63a5\u89e6\u7684\u7cbe\u786e\u6a21\u62df\u548c\u673a\u5668\u4eba\u63a7\u5236\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.25215", "pdf": "https://arxiv.org/pdf/2509.25215", "abs": "https://arxiv.org/abs/2509.25215", "authors": ["Pierre Lotte", "Andr\u00e9 P\u00e9ninou", "Olivier Teste"], "title": "Anomaly detection by partitioning of multi-variate time series", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "in French language", "summary": "In this article, we suggest a novel non-supervised partition based anomaly\ndetection method for anomaly detection in multivariate time series called\nPARADISE. This methodology creates a partition of the variables of the time\nseries while ensuring that the inter-variable relations remain untouched. This\npartitioning relies on the clustering of multiple correlation coefficients\nbetween variables to identify subsets of variables before executing anomaly\ndetection algorithms locally for each of those subsets. Through multiple\nexperimentations done on both synthetic and real datasets coming from the\nliterature, we show the relevance of our approach with a significant\nimprovement in anomaly detection performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPARADISE\u7684\u975e\u76d1\u7763\u5206\u533a\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u4fdd\u7559\u53d8\u91cf\u95f4\u5173\u7cfb\u5e76\u5206\u533a\u8fdb\u884c\u5c40\u90e8\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u533a\u4fdd\u7559\u53d8\u91cf\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "\u57fa\u4e8e\u53d8\u91cf\u95f4\u591a\u91cd\u76f8\u5173\u7cfb\u6570\u7684\u805a\u7c7b\u8fdb\u884c\u5206\u533a\uff0c\u5e76\u5728\u6bcf\u4e2a\u5b50\u96c6\u4e0a\u5c40\u90e8\u6267\u884c\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "PARADISE\u65b9\u6cd5\u901a\u8fc7\u5206\u533a\u548c\u5c40\u90e8\u68c0\u6d4b\u7b56\u7565\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.26050", "pdf": "https://arxiv.org/pdf/2509.26050", "abs": "https://arxiv.org/abs/2509.26050", "authors": ["Shaoli Hu", "Shizhe Zhao", "Zhongqiang Ren"], "title": "Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles", "categories": ["cs.RO"], "comment": null, "summary": "This paper investigates Multi-Agent Path Finding Among Movable Obstacles\n(M-PAMO), which seeks collision-free paths for multiple agents from their start\nto goal locations among static and movable obstacles. M-PAMO arises in\nlogistics and warehouses where mobile robots are among unexpected movable\nobjects. Although Multi-Agent Path Finding (MAPF) and single-agent Path\nplanning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains\nunder-explored. Movable obstacles lead to new fundamental challenges as the\nstate space, which includes both agents and movable obstacles, grows\nexponentially with respect to the number of agents and movable obstacles. In\nparticular, movable obstacles often closely couple agents together spatially\nand temporally. This paper makes a first attempt to adapt and fuse the popular\nConflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a\nrecent single-agent PAMO planner called PAMO*, together to address M-PAMO. We\ncompare their performance with up to 20 agents and hundreds of movable\nobstacles, and show the pros and cons of these approaches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5728\u53ef\u79fb\u52a8\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff08M-PAMO\uff09\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u51b2\u7a81\u641c\u7d22\uff08CBS\uff09\u3001\u4f18\u5148\u7ea7\u89c4\u5212\uff08PP\uff09\u548c\u5355\u667a\u80fd\u4f53PAMO*\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "M-PAMO\u95ee\u9898\u5728\u7269\u6d41\u548c\u4ed3\u5e93\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u548c\u5355\u667a\u80fd\u4f53\u53ef\u79fb\u52a8\u969c\u788d\u7269\u8def\u5f84\u89c4\u5212\uff08PAMO\uff09\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u5176\u590d\u6742\u6027\u3002", "method": "\u672c\u6587\u5c1d\u8bd5\u5c06\u51b2\u7a81\u641c\u7d22\uff08CBS\uff09\u3001\u4f18\u5148\u7ea7\u89c4\u5212\uff08PP\uff09\u548c\u5355\u667a\u80fd\u4f53PAMO*\u65b9\u6cd5\u878d\u5408\uff0c\u4ee5\u5e94\u5bf9M-PAMO\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u8fbe20\u4e2a\u667a\u80fd\u4f53\u548c\u6570\u767e\u4e2a\u53ef\u79fb\u52a8\u969c\u788d\u7269\u7684\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5404\u6709\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u7814\u7a76\u4e3aM-PAMO\u95ee\u9898\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u878d\u5408\u65b9\u6cd5\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2509.25236", "pdf": "https://arxiv.org/pdf/2509.25236", "abs": "https://arxiv.org/abs/2509.25236", "authors": ["Gabriele D'Acunto", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "The Causal Abstraction Network: Theory and Learning", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "Causal artificial intelligence aims to enhance explainability,\ntrustworthiness, and robustness in AI by leveraging structural causal models\n(SCMs). In this pursuit, recent advances formalize network sheaves of causal\nknowledge. Pushing in the same direction, we introduce the causal abstraction\nnetwork (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian,\n(ii) restriction maps are transposes of constructive linear causal abstractions\n(CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks\nof more detailed SCMs. We investigate the theoretical properties of CAN,\nincluding algebraic invariants, cohomology, consistency, global sections\ncharacterized via the Laplacian kernel, and smoothness. We then tackle the\nlearning of consistent CANs. Our problem formulation separates into\nedge-specific local Riemannian problems and avoids nonconvex, costly\nobjectives. We propose an efficient search procedure as a solution, solving the\nlocal problems with SPECTRAL, our iterative method with closed-form updates and\nsuitable for positive definite and semidefinite covariance matrices.\nExperiments on synthetic data show competitive performance in the CA learning\ntask, and successful recovery of diverse CAN structures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u56e0\u679c\u62bd\u8c61\u7f51\u7edc\uff08CAN\uff09\uff0c\u4e00\u79cd\u5229\u7528\u9ad8\u65af\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u7684\u7279\u5b9a\u7f51\u7edc\u5c42\uff0c\u7814\u7a76\u4e86\u5176\u7406\u8bba\u6027\u8d28\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165CAN\uff0c\u8bba\u6587\u65e8\u5728\u63d0\u5347\u56e0\u679c\u4eba\u5de5\u667a\u80fd\u7684\u89e3\u91ca\u6027\u3001\u53ef\u4fe1\u8d56\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5229\u7528SCM\u548c\u7f51\u7edc\u5c42\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528\u9ad8\u65afSCM\u548c\u7ebf\u6027\u56e0\u679c\u62bd\u8c61\uff08CA\uff09\u6784\u5efaCAN\uff0c\u63d0\u51faSPECTRAL\u8fed\u4ee3\u65b9\u6cd5\u89e3\u51b3\u5c40\u90e8\u9ece\u66fc\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b66\u4e60\u4e00\u81f4\u7684CAN\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u6062\u590d\u4e86\u591a\u79cdCAN\u7ed3\u6784\u3002", "conclusion": "CAN\u53ca\u5176\u5b66\u4e60\u65b9\u6cd5\u4e3a\u56e0\u679cAI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.26082", "pdf": "https://arxiv.org/pdf/2509.26082", "abs": "https://arxiv.org/abs/2509.26082", "authors": ["Tianyi Jin", "Melya Boukheddimi", "Rohit Kumar", "Gabriele Fadini", "Frank Kirchner"], "title": "Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid robots have seen significant advancements in both design and\ncontrol, with a growing emphasis on integrating these aspects to enhance\noverall performance. Traditionally, robot design has followed a sequential\nprocess, where control algorithms are developed after the hardware is\nfinalized. However, this can be myopic and prevent robots to fully exploit\ntheir hardware capabilities. Recent approaches advocate for co-design,\noptimizing both design and control in parallel to maximize robotic\ncapabilities. This paper presents the Evolutionary Continuous Adaptive RL-based\nCo-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with\nevolutionary strategies to enable continuous adaptation of the control policy\nto the hardware. EA-CoRL comprises two key components: Design Evolution, which\nexplores the hardware choices using an evolutionary algorithm to identify\nefficient configurations, and Policy Continuous Adaptation, which fine-tunes a\ntask-specific control policy across evolving designs to maximize performance\nrewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and\ncontrol policy of the RH5 humanoid for a highly dynamic chin-up task,\npreviously unfeasible due to actuator limitations. Comparative results against\nstate-of-the-art RL-based co-design methods show that EA-CoRL achieves higher\nfitness score and broader design space exploration, highlighting the critical\nrole of continuous policy adaptation in robot co-design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEA-CoRL\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u8fdb\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u8bbe\u8ba1\u548c\u63a7\u5236\u7684\u5e76\u884c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8bbe\u8ba1\u91c7\u7528\u987a\u5e8f\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u786c\u4ef6\u6f5c\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u540c\u65f6\u4f18\u5316\u786c\u4ef6\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "EA-CoRL\u6846\u67b6\u5305\u542b\u8bbe\u8ba1\u8fdb\u5316\u548c\u7b56\u7565\u6301\u7eed\u9002\u5e94\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u63a2\u7d22\u9ad8\u6548\u786c\u4ef6\u914d\u7f6e\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6301\u7eed\u8c03\u4f18\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728RH5\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1EA-CoRL\uff0c\u6210\u529f\u5b8c\u6210\u9ad8\u52a8\u6001\u5f15\u4f53\u5411\u4e0a\u4efb\u52a1\uff0c\u5e76\u5728\u6027\u80fd\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EA-CoRL\u5c55\u793a\u4e86\u6301\u7eed\u7b56\u7565\u9002\u5e94\u5728\u673a\u5668\u4eba\u8054\u5408\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.25284", "pdf": "https://arxiv.org/pdf/2509.25284", "abs": "https://arxiv.org/abs/2509.25284", "authors": ["Oluwaseyi Giwa", "Jonathan Shock", "Jaco Du Toit", "Tobi Awodumila"], "title": "Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning", "categories": ["cs.LG", "cs.NI", "eess.SP"], "comment": "Submitted to IEEE Wireless Communications and Networking Conference,\n  2026", "summary": "Dynamic resource allocation in heterogeneous wireless networks (HetNets) is\nchallenging for traditional methods under varying user loads and channel\nconditions. We propose a deep reinforcement learning (DRL) framework that\njointly optimises transmit power, bandwidth, and scheduling via a\nmulti-objective reward balancing throughput, energy efficiency, and fairness.\nUsing real base station coordinates, we compare Proximal Policy Optimisation\n(PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three\nheuristic algorithms in multiple network scenarios. Our results show that DRL\nframeworks outperform heuristic algorithms in optimising resource allocation in\ndynamic networks. These findings highlight key trade-offs in DRL design for\nfuture HetNets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7528\u6237\u8d1f\u8f7d\u548c\u4fe1\u9053\u6761\u4ef6\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u91c7\u7528DRL\u6846\u67b6\uff08PPO\u548cTD3\uff09\u8054\u5408\u4f18\u5316\u4f20\u8f93\u529f\u7387\u3001\u5e26\u5bbd\u548c\u8c03\u5ea6\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u5e73\u8861\u541e\u5410\u91cf\u3001\u80fd\u6548\u548c\u516c\u5e73\u6027\u3002", "result": "DRL\u6846\u67b6\u5728\u52a8\u6001\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e09\u79cd\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u672a\u6765\u5f02\u6784\u65e0\u7ebf\u7f51\u7edc\u4e2dDRL\u8bbe\u8ba1\u7684\u5173\u952e\u6743\u8861\u3002"}}
{"id": "2509.26106", "pdf": "https://arxiv.org/pdf/2509.26106", "abs": "https://arxiv.org/abs/2509.26106", "authors": ["Nakhul Kalaivanan", "Senthil Arumugam Muthukumaraswamy", "Girish Balasubramanian"], "title": "Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics", "categories": ["cs.RO", "68T40, 68T05", "I.2.9; I.2.6"], "comment": "11 pages, 5 figures, MSc dissertation submission draft, prepared for\n  conference/journal consideration", "summary": "This research presents a multi-robot system for inpatient care, designed\nusing swarm intelligence principles and incorporating wearable health sensors,\nRF-based communication, and AI-driven decision support. Within a simulated\nhospital environment, the system adopts a leader-follower swarm configuration\nto perform patient monitoring, medicine delivery, and emergency assistance. Due\nto ethical constraints, live patient trials were not conducted; instead,\nvalidation was carried out through controlled self-testing with wearable\nsensors. The Leader Robot acquires key physiological parameters, including\ntemperature, SpO2, heart rate, and fall detection, and coordinates other robots\nwhen required. The Assistant Robot patrols corridors for medicine delivery,\nwhile a robotic arm provides direct drug administration. The swarm-inspired\nleader-follower strategy enhanced communication reliability and ensured\ncontinuous monitoring, including automated email alerts to healthcare staff.\nThe system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF\nmodules, and a HuskyLens AI camera. Experimental evaluation showed an overall\nsensor accuracy above 94%, a 92% task-level success rate, and a 96%\ncommunication reliability rate, demonstrating system robustness. Furthermore,\nthe AI-enabled decision support was able to provide early warnings of abnormal\nhealth conditions, highlighting the potential of the system as a cost-effective\nsolution for hospital automation and patient safety.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f4f\u9662\u62a4\u7406\uff0c\u7ed3\u5408\u4e86\u7a7f\u6234\u5f0f\u5065\u5eb7\u4f20\u611f\u5668\u3001RF\u901a\u4fe1\u548cAI\u51b3\u7b56\u652f\u6301\u3002\u5728\u6a21\u62df\u533b\u9662\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u901a\u8fc7\u9886\u5bfc-\u8ddf\u968f\u7fa4\u4f53\u914d\u7f6e\u8fdb\u884c\u60a3\u8005\u76d1\u62a4\u3001\u836f\u7269\u914d\u9001\u548c\u5e94\u6025\u8f85\u52a9\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4f20\u611f\u5668\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u4efb\u52a1\u6210\u529f\u738792%\uff0c\u901a\u4fe1\u53ef\u9760\u602796%\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7fa4\u4f53\u667a\u80fd\u548cAI\u6280\u672f\u89e3\u51b3\u533b\u9662\u81ea\u52a8\u5316\u4e2d\u7684\u60a3\u8005\u76d1\u62a4\u548c\u836f\u7269\u914d\u9001\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u9886\u5bfc-\u8ddf\u968f\u7fa4\u4f53\u7b56\u7565\uff0c\u7ed3\u5408\u7a7f\u6234\u5f0f\u4f20\u611f\u5668\u548cRF\u901a\u4fe1\uff0c\u786c\u4ef6\u7531Arduino\u3001Raspberry Pi\u548cAI\u6444\u50cf\u5934\u5b9e\u73b0\uff0cAI\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u3002", "result": "\u4f20\u611f\u5668\u51c6\u786e\u738794%\u4ee5\u4e0a\uff0c\u4efb\u52a1\u6210\u529f\u738792%\uff0c\u901a\u4fe1\u53ef\u9760\u602796%\uff0cAI\u80fd\u65e9\u671f\u9884\u8b66\u5f02\u5e38\u5065\u5eb7\u72b6\u51b5\u3002", "conclusion": "\u7cfb\u7edf\u5c55\u793a\u4e86\u4f5c\u4e3a\u4f4e\u6210\u672c\u533b\u9662\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u63d0\u9ad8\u4e86\u60a3\u8005\u5b89\u5168\u548c\u62a4\u7406\u6548\u7387\u3002"}}
{"id": "2509.25394", "pdf": "https://arxiv.org/pdf/2509.25394", "abs": "https://arxiv.org/abs/2509.25394", "authors": ["Hui Wang", "Nima Tashakor", "Xiaoyang Tian", "Hans D. Schotten", "Stefan M. Goetz"], "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors", "categories": ["cs.CR", "cs.SY", "eess.SP", "eess.SY"], "comment": "11 pages, 12 figures", "summary": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u65e0\u7ebf\u5145\u7535\u4e2d\u80fd\u91cf\u52a0\u5bc6\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u4f18\u5316\u653b\u51fb\u65b9\u6cd5\u57280.2\u6beb\u79d2\u5185\u7a83\u53d665%\u7684\u7535\u80fd\uff0c\u786c\u4ef6\u66f4\u7b80\u5355\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u5145\u7535\u666e\u53ca\uff0c\u516c\u5171\u573a\u5408\u7684\u80fd\u91cf\u8bbf\u95ee\u4fdd\u62a4\u548c\u7f51\u7edc\u5b89\u5168\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u52a0\u5bc6\u65b9\u6cd5\u4e0d\u53ef\u9760\u3002", "method": "\u4f18\u5316\u653b\u51fb\u7cfb\u7edf\uff0c\u53d6\u6d88\u8017\u65f6\u6b65\u9aa4\u5e76\u4f7f\u7528\u4e3b\u63a5\u6536\u7ebf\u5708\u68c0\u6d4b\u78c1\u573a\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u8bc1\u660e\u653b\u51fb\u901f\u5ea6\u5feb\uff080.2\u6beb\u79d2\uff09\uff0c\u7a83\u53d665%\u7535\u80fd\uff0c\u786c\u4ef6\u66f4\u7b80\u5355\u3002", "conclusion": "\u73b0\u6709\u52a0\u5bc6\u65b9\u6cd5\u9700\u6539\u8fdb\uff0c\u80fd\u91cf\u8bbf\u95ee\u4fdd\u62a4\u9700\u9ad8\u5ea6\u91cd\u89c6\u3002"}}
{"id": "2509.26121", "pdf": "https://arxiv.org/pdf/2509.26121", "abs": "https://arxiv.org/abs/2509.26121", "authors": ["Julian Valdez", "Ignacio Torroba", "John Folkesson", "Ivan Stenius"], "title": "Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring", "categories": ["cs.RO"], "comment": null, "summary": "The transition of seaweed farming to an alternative food source on an\nindustrial scale relies on automating its processes through smart farming,\nequivalent to land agriculture. Key to this process are autonomous underwater\nvehicles (AUVs) via their capacity to automate crop and structural inspections.\nHowever, the current bottleneck for their deployment is ensuring safe\nnavigation within farms, which requires an accurate, online estimate of the AUV\npose and map of the infrastructure. To enable this, we propose an efficient\nside scan sonar-based (SSS) simultaneous localization and mapping (SLAM)\nframework that exploits the geometry of kelp farms via modeling structural\nropes in the back-end as sequences of individual landmarks from each SSS ping\ndetection, instead of combining detections into elongated representations. Our\nmethod outperforms state of the art solutions in hardware in the loop (HIL)\nexperiments on a real AUV survey in a kelp farm. The framework and dataset can\nbe found at https://github.com/julRusVal/sss_farm_slam.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fa7\u626b\u58f0\u7eb3\u7684SLAM\u6846\u67b6\uff0c\u7528\u4e8e\u6d77\u85fb\u519c\u573a\u4e2dAUV\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6d77\u85fb\u519c\u573a\u7684\u81ea\u52a8\u5316\u7ba1\u7406\uff0c\u9700\u8981AUV\u8fdb\u884c\u5b89\u5168\u7684\u5bfc\u822a\u548c\u57fa\u7840\u8bbe\u65bd\u6d4b\u7ed8\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u74f6\u9888\u3002", "method": "\u91c7\u7528\u4fa7\u626b\u58f0\u7eb3\uff08SSS\uff09SLAM\u6846\u67b6\uff0c\u5c06\u519c\u573a\u7ed3\u6784\u7684\u7ef3\u7d22\u5efa\u6a21\u4e3a\u5355\u70b9\u6807\u5fd7\u5e8f\u5217\uff0c\u800c\u975e\u957f\u5f62\u8868\u793a\u3002", "result": "\u5728\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u901a\u8fc7\u771f\u5b9eAUV\u5728\u519c\u573a\u4e2d\u7684\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684SLAM\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301AUV\u5728\u6d77\u85fb\u519c\u573a\u4e2d\u7684\u5bfc\u822a\uff0c\u672a\u6765\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u7ba1\u7406\u3002"}}
{"id": "2509.25406", "pdf": "https://arxiv.org/pdf/2509.25406", "abs": "https://arxiv.org/abs/2509.25406", "authors": ["Anahid Rafieifar", "Hosein Ahmadinejad", "S. Mohammad Razavizadeh", "Jiguang He"], "title": "Secure Beamforming in Multi-User Multi-IRS Millimeter Wave Systems", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "Published in IEEE Transactions on Wireless Communications, Volume: 22\n  Issue: 9, 30 January 2023", "summary": "We study the secrecy rate maximization problem in a millimeter wave (mmWave)\nnetwork, consisting of a base station (BS), multiple intelligent reflecting\nsurfaces (IRSs) (or reconfigurable intelligent surfaces (RISs)), multiple\nusers, and a single eavesdropper. To ensure a fair secrecy rate among all the\nusers, we adopt a max-min fairness criterion which results in a mixed integer\nproblem. We first relax discrete IRSs phase shifts to the continuous ones. To\ncope with the non-convexity of the relaxed optimization problem, we leverage\nthe penalty method and block coordinate descent approach to divide it into two\nsub-problems, which are solved by successive convex approximation (SCA). Then,\nwe propose a low-complexity mapping algorithm where feasible IRSs phase shifts\nare obtained. Mathematical evaluation shows the convergence of sub-problems to\na Karush-Kuhn-Tucker (KKT) point of the original ones. Furthermore, the\nconvergence guarantee of the overall proposed algorithm and computational\ncomplexity are investigated. Finally, simulation results show our proposed\nalgorithm outweighs the conventional solutions based on the semi-definite\nprogramming (SDP) in terms of convergence and secrecy rate, especially in a\nlarger number of IRSs and phase shifts where SDP suffers from rank-one\napproximation. Maximum ratio transmission (MRT) and IRS-free systems are also\nconsidered as other benchmarks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u57fa\u4e8eIRS\u7684\u6700\u5927\u5316\u7528\u6237\u4fdd\u5bc6\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u677e\u5f1b\u76f8\u4f4d\u504f\u79fb\u548c\u5206\u89e3\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u6620\u5c04\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u786e\u4fdd\u7528\u6237\u5728\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u7684\u516c\u5e73\u4fdd\u5bc6\u7387\uff0c\u89e3\u51b3\u6df7\u5408\u6574\u6570\u4f18\u5316\u95ee\u9898\u548c\u975e\u51f8\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u7f5a\u51fd\u6570\u6cd5\u548c\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u5206\u89e3\u95ee\u9898\uff0c\u7ed3\u5408\u8fde\u7eed\u51f8\u903c\u8fd1(SCA)\u6c42\u89e3\uff0c\u5e76\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u6620\u5c04\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u6536\u655b\u6027\u548c\u4fdd\u5bc6\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u534a\u5b9a\u89c4\u5212(SDP)\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591aIRS\u548c\u5927\u76f8\u4f4d\u504f\u79fb\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6beb\u7c73\u6ce2\u7f51\u7edc\u4e2d\u7528\u6237\u4fdd\u5bc6\u7387\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.26222", "pdf": "https://arxiv.org/pdf/2509.26222", "abs": "https://arxiv.org/abs/2509.26222", "authors": ["Yizhe Liu", "Han Zhang"], "title": "Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation", "categories": ["cs.RO"], "comment": null, "summary": "An accurate odometry is essential for legged-wheel robots operating in\nunstructured terrains such as bumpy roads and staircases. Existing methods\noften suffer from pose drift due to their ignorance of terrain geometry. We\npropose a terrain-awared LiDAR-Inertial odometry (LIO) framework that\napproximates the terrain using Radial Basis Functions (RBF) whose centers are\nadaptively selected and weights are recursively updated. The resulting smooth\nterrain manifold enables ``soft constraints\" that regularize the odometry\noptimization and mitigates the $z$-axis pose drift under abrupt elevation\nchanges during robot's maneuver. To ensure the LIO's real-time performance, we\nfurther evaluate the RBF-related terms and calculate the inverse of the sparse\nkernel matrix with GPU parallelization. Experiments on unstructured terrains\ndemonstrate that our method achieves higher localization accuracy than the\nstate-of-the-art baselines, especially in the scenarios that have continuous\nheight changes or sparse features when abrupt height changes occur.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5730\u5f62\u611f\u77e5\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u6846\u67b6\uff0c\u5229\u7528\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u81ea\u9002\u5e94\u62df\u5408\u5730\u5f62\uff0c\u901a\u8fc7\u8f6f\u7ea6\u675f\u4f18\u5316\u91cc\u7a0b\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u65f6\u7684z\u8f74\u4f4d\u59ff\u6f02\u79fb\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u817f\u8f6e\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\uff08\u5982\u5d0e\u5c96\u8def\u9762\u548c\u697c\u68af\uff09\u4e2d\u64cd\u4f5c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u5ffd\u7565\u5730\u5f62\u51e0\u4f55\u7279\u5f81\u800c\u5bfc\u81f4\u4f4d\u59ff\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eRBF\u7684\u5730\u5f62\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u4e2d\u5fc3\u5e76\u9012\u5f52\u66f4\u65b0\u6743\u91cd\uff0c\u7ed3\u5408GPU\u5e76\u884c\u5316\u8ba1\u7b97\u4ee5\u786e\u4fdd\u5b9e\u65f6\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u8fde\u7eed\u9ad8\u5ea6\u53d8\u5316\u6216\u7279\u5f81\u7a00\u758f\u7684\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5730\u5f62\u611f\u77e5\u4f18\u5316\u663e\u8457\u51cf\u5c11\u4e86\u4f4d\u59ff\u6f02\u79fb\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2509.25517", "pdf": "https://arxiv.org/pdf/2509.25517", "abs": "https://arxiv.org/abs/2509.25517", "authors": ["Valdemar Farr\u00e9", "David Vega", "Juan Estrada", "Juan A. V\u00e1squez Peralvo", "Symeon Chatzinotas"], "title": "From Legacy to Leadership Intelligent Radio Network Planning Framework for Cell-Free Massive MIMO in B5G6G Era", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "6 pages, 12 figures, 4 tables, IEEE ETCM 2025 Conference Preprint", "summary": "The proliferation of cell-free Massive MIMO represents a transformative shift\nin wireless network architecture, addressing critical limitations of\nconventional distributed Massive MIMO systems. This paper presents an\nintelligent radio network planning framework that bridges legacy 5G\ninfrastructures with future B5G/6G networks through cell-free architectures. By\nleveraging operational insights from existing 5G deployments, we systematically\naddress coverage optimization, and capacity enhancement. Our scalable framework\nenables seamless evolution from legacy designs to next-generation cell-free\nsystems. Through extensive simulations in dense urban environments, we\ndemonstrate substantial improvements: 45% spectral efficiency gains, 30%\ninterference reduction, and significantly enhanced uniform coverage. The\nproposed framework provides network operators with a practical roadmap for\ntransitioning from traditional cellular architectures to demanding B5G/6G\nrequirements while maximizing existing infrastructure investments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u65e0\u7ebf\u7535\u7f51\u7edc\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u4f20\u7edf5G\u57fa\u7840\u8bbe\u65bd\u4e0e\u672a\u6765B5G/6G\u7f51\u7edc\u901a\u8fc7\u65e0\u5c0f\u533a\u67b6\u6784\u8fde\u63a5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9891\u8c31\u6548\u7387\u548c\u8986\u76d6\u5747\u5300\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u5e03\u5f0fMassive MIMO\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u63a8\u52a8\u65e0\u7ebf\u7f51\u7edc\u67b6\u6784\u7684\u53d8\u9769\u3002", "method": "\u5229\u7528\u73b0\u67095G\u90e8\u7f72\u7684\u8fd0\u8425\u6d1e\u5bdf\uff0c\u8bbe\u8ba1\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4f18\u5316\u8986\u76d6\u548c\u5bb9\u91cf\u3002", "result": "\u5bc6\u96c6\u57ce\u533a\u4eff\u771f\u663e\u793a\u9891\u8c31\u6548\u7387\u63d0\u534745%\uff0c\u5e72\u6270\u51cf\u5c1130%\uff0c\u8986\u76d6\u5747\u5300\u6027\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "\u4e3a\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u4ece\u4f20\u7edf\u8702\u7a9d\u67b6\u6784\u8fc7\u6e21\u5230B5G/6G\u9700\u6c42\u7684\u5b9e\u7528\u8def\u7ebf\u56fe\u3002"}}
{"id": "2509.26236", "pdf": "https://arxiv.org/pdf/2509.26236", "abs": "https://arxiv.org/abs/2509.26236", "authors": ["Benjamin A. Richardson", "Felix Gr\u00fcninger", "Lukas Mack", "Joerg Stueckler", "Katherine J. Kuchenbecker"], "title": "ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm", "categories": ["cs.RO"], "comment": "Accepted at IEEE Humanoids 2025", "summary": "The rapid increase in the development of humanoid robots and customized\nmanufacturing solutions has brought dexterous manipulation to the forefront of\nmodern robotics. Over the past decade, several expensive dexterous hands have\ncome to market, but advances in hardware design, particularly in servo motors\nand 3D printing, have recently facilitated an explosion of cheaper open-source\nhands. Most hands are anthropomorphic to allow use of standard human tools, and\nattempts to increase dexterity often sacrifice anthropomorphism. We introduce\nthe open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost,\neasy-to-manufacture, on-joint servo-driven robot hand. Our hand uses\noff-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be\nassembled within four hours, and has a total material cost of about 1,300 USD.\nThe ISyHands's unique articulated-palm design increases overall dexterity with\nonly a modest sacrifice in anthropomorphism. To demonstrate the utility of the\narticulated palm, we use reinforcement learning in simulation to train the hand\nto perform a classical in-hand manipulation task: cube reorientation. Our\nnovel, systematic experiments show that the simulated ISyHand outperforms the\ntwo most comparable hands in early training phases, that all three perform\nsimilarly well after policy convergence, and that the ISyHand significantly\noutperforms a fixed-palm version of its own design. Additionally, we deploy a\npolicy trained on cube reorientation on the real hand, demonstrating its\nability to perform real-world dexterous manipulation.", "AI": {"tldr": "\u5f00\u6e90ISyHand\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u5236\u9020\u3001\u9ad8\u7075\u5de7\u7684\u673a\u5668\u4eba\u624b\uff0c\u901a\u8fc7\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\u63d0\u5347\u4e86\u7075\u5de7\u6027\uff0c\u5e76\u5728\u6a21\u62df\u8bad\u7ec3\u4e2d\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u4ea7\u54c1\u3002", "motivation": "\u968f\u7740\u4eba\u5f62\u673a\u5668\u4eba\u548c\u5b9a\u5236\u5316\u5236\u9020\u65b9\u6848\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7075\u5de7\u64cd\u4f5c\u6210\u4e3a\u73b0\u4ee3\u673a\u5668\u4eba\u6280\u672f\u7684\u7126\u70b9\u3002\u73b0\u6709\u7075\u5de7\u624b\u901a\u5e38\u4ef7\u683c\u6602\u8d35\u6216\u727a\u7272\u4eba\u5f62\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u5f00\u6e90\u7684ISyHand\u673a\u5668\u4eba\u624b\uff0c\u4f7f\u7528\u73b0\u6210\u7684Dynamixel\u7535\u673a\u3001\u7d27\u56fa\u4ef6\u548c3D\u6253\u5370\u90e8\u4ef6\uff0c\u7ec4\u88c5\u65f6\u95f4\u77ed\uff0c\u6210\u672c\u4f4e\u3002\u901a\u8fc7\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\u63d0\u5347\u7075\u5de7\u6027\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7acb\u65b9\u4f53\u91cd\u5b9a\u5411\u4efb\u52a1\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cISyHand\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u4f18\u4e8e\u540c\u7c7b\u4ea7\u54c1\uff0c\u653f\u7b56\u6536\u655b\u540e\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u624b\u638c\u7248\u672c\u3002\u5b9e\u9645\u90e8\u7f72\u4e5f\u9a8c\u8bc1\u4e86\u5176\u73b0\u5b9e\u7684\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\u3002", "conclusion": "ISyHand\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u7075\u5de7\u6027\u7684\u53ef\u884c\u6027\uff0c\u5176\u5173\u8282\u5f0f\u624b\u638c\u8bbe\u8ba1\u4e3a\u673a\u5668\u4eba\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.25659", "pdf": "https://arxiv.org/pdf/2509.25659", "abs": "https://arxiv.org/abs/2509.25659", "authors": ["Po-Heng Chou", "Chun-Chi Wang", "Wei-Lung Mao"], "title": "YOLO-Based Defect Detection for Metal Sheets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP", "68T45, 68T07", "I.2.10; I.4.7; I.5.4"], "comment": "5 pages, 8 figures, 2 tables, and published in IEEE IST 2024", "summary": "In this paper, we propose a YOLO-based deep learning (DL) model for automatic\ndefect detection to solve the time-consuming and labor-intensive tasks in\nindustrial manufacturing. In our experiments, the images of metal sheets are\nused as the dataset for training the YOLO model to detect the defects on the\nsurfaces and in the holes of metal sheets. However, the lack of metal sheet\nimages significantly degrades the performance of detection accuracy. To address\nthis issue, the ConSinGAN is used to generate a considerable amount of data.\nFour versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined\nwith the ConSinGAN for data augmentation. The proposed YOLOv9 model with\nConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a\ndetection time of 146 ms. The proposed YOLOv9 model is integrated into\nmanufacturing hardware and a supervisory control and data acquisition (SCADA)\nsystem to establish a practical automated optical inspection (AOI) system.\nAdditionally, the proposed automated defect detection is easily applied to\nother components in industrial manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLO\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5de5\u4e1a\u5236\u9020\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\uff0c\u7ed3\u5408ConSinGAN\u751f\u6210\u6570\u636e\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aYOLOv9\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u5236\u9020\u4e2d\u4eba\u5de5\u68c0\u6d4b\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u81ea\u52a8\u7f3a\u9677\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528YOLO\u6a21\u578b\uff08v3\u3001v4\u3001v7\u3001v9\uff09\u7ed3\u5408ConSinGAN\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u8bad\u7ec3\u68c0\u6d4b\u91d1\u5c5e\u677f\u8868\u9762\u548c\u5b54\u6d1e\u7f3a\u9677\u7684\u6a21\u578b\u3002", "result": "YOLOv9\u7ed3\u5408ConSinGAN\u7684\u6548\u679c\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe91.3%\uff0c\u68c0\u6d4b\u65f6\u95f4\u4e3a146\u6beb\u79d2\uff0c\u5e76\u6210\u529f\u96c6\u6210\u5230AOI\u7cfb\u7edf\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7f3a\u9677\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u6613\u4e8e\u63a8\u5e7f\u81f3\u5176\u4ed6\u5de5\u4e1a\u5236\u9020\u573a\u666f\u3002"}}
{"id": "2509.26308", "pdf": "https://arxiv.org/pdf/2509.26308", "abs": "https://arxiv.org/abs/2509.26308", "authors": ["Niklas Grambow", "Lisa-Marie Fenner", "Felipe Kempkes", "Philip Hotz", "Dingyuan Wan", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Out-of-distribution states in robot manipulation often lead to unpredictable\nrobot behavior or task failure, limiting success rates and increasing risk of\ndamage. Anomaly detection (AD) can identify deviations from expected patterns\nin data, which can be used to trigger failsafe behaviors and recovery\nstrategies. Prior work has applied data-driven AD to time series data in\nspecific robotic tasks, but its transferability across control strategies and\ntask types has not been shown. Leveraging time series data, such as\nforce/torque signals, allows to directly capture robot-environment\ninteractions, crucial for manipulation and online failure detection. Their\nbroad availability, high sampling rates, and low dimensionality enable high\ntemporal resolution and efficient processing. As robotic tasks can have widely\nsignal characteristics and requirements, AD methods which can be applied in the\nsame way to a wide range of tasks is needed, ideally with good data efficiency.\nWe examine three industrial robotic tasks, each presenting several anomalies.\nTest scenarios in robotic cabling, screwing, and sanding are built, and\nmultimodal time series data is gathered. Several autoencoder-based methods are\ncompared, evaluating generalization across tasks and control methods (diffusion\npolicy, position, and impedance control). This allows us to validate the\nintegration of AD in complex tasks involving tighter tolerances and variation\nfrom both the robot and its environment. Additionally, we evaluate data\nefficiency, detection latency, and task characteristics which support robust\ndetection. The results indicate reliable detection with AUROC exceeding 0.93 in\nfailures in the cabling and screwing task, such as incorrect or misaligned\nparts and obstructed targets. In the polishing task, only severe failures were\nreliably detected, while more subtle failure types remained undetected.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6545\u969c\u68c0\u6d4b\u3002\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u63a7\u5236\u7b56\u7565\u4e2d\u7684\u901a\u7528\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8d85\u51fa\u5206\u5e03\u7684\u72b6\u6001\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u6216\u4efb\u52a1\u5931\u8d25\uff0c\u589e\u52a0\u4e86\u98ce\u9669\u548c\u635f\u4f24\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u5de5\u4e1a\u673a\u5668\u4eba\u4efb\u52a1\uff08\u7535\u7f06\u94fa\u8bbe\u3001\u62e7\u87ba\u4e1d\u548c\u6253\u78e8\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u3002\u6570\u636e\u5305\u62ec\u529b/\u529b\u77e9\u4fe1\u53f7\u7b49\uff0c\u4ee5\u6355\u6349\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u7279\u5f81\u3002", "result": "\u5728\u7535\u7f06\u94fa\u8bbe\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\uff0c\u5f02\u5e38\u68c0\u6d4b\u7684AUROC\u8d85\u8fc70.93\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u9519\u8bef\u96f6\u4ef6\u6216\u76ee\u6807\u963b\u585e\u7b49\u95ee\u9898\u3002\u4f46\u5bf9\u4e8e\u6253\u78e8\u4efb\u52a1\uff0c\u4ec5\u80fd\u53ef\u9760\u68c0\u6d4b\u4e25\u91cd\u6545\u969c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6548\u7387\u548c\u63a7\u5236\u7b56\u7565\u901a\u7528\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7ec6\u5fae\u6545\u969c\u7684\u68c0\u6d4b\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2509.25660", "pdf": "https://arxiv.org/pdf/2509.25660", "abs": "https://arxiv.org/abs/2509.25660", "authors": ["Chun-Yuan Huang", "Po-Heng Chou", "Wan-Jen Huang", "Ying-Ren Chien", "Yu Tsao"], "title": "Capacity-Net-Based RIS Precoding Design without Channel Estimation for mmWave MIMO System", "categories": ["cs.IT", "cs.AI", "cs.LG", "cs.NI", "eess.SP", "math.IT", "68T07, 94A05", "I.2.6; I.5.1"], "comment": "10 pages, 5 figures, and published in 2024 IEEE PIMRC", "summary": "In this paper, we propose Capacity-Net, a novel unsupervised learning\napproach aimed at maximizing the achievable rate in reflecting intelligent\nsurface (RIS)-aided millimeter-wave (mmWave) multiple input multiple output\n(MIMO) systems. To combat severe channel fading of the mmWave spectrum, we\noptimize the phase-shifting factors of the reflective elements in the RIS to\nenhance the achievable rate. However, most optimization algorithms rely heavily\non complete and accurate channel state information (CSI), which is often\nchallenging to acquire since the RIS is mostly composed of passive components.\nTo circumvent this challenge, we leverage unsupervised learning techniques with\nimplicit CSI provided by the received pilot signals. Specifically, it usually\nrequires perfect CSI to evaluate the achievable rate as a performance metric of\nthe current optimization result of the unsupervised learning method. Instead of\nchannel estimation, the Capacity-Net is proposed to establish a mapping among\nthe received pilot signals, optimized RIS phase shifts, and the resultant\nachievable rates. Simulation results demonstrate the superiority of the\nproposed Capacity-Net-based unsupervised learning approach over learning\nmethods based on traditional channel estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCapacity-Net\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5927\u5316RIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u4e2d\u7684\u53ef\u8fbe\u5230\u901f\u7387\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5b8c\u6574CSI\u7684\u4f9d\u8d56\u3002", "motivation": "\u7531\u4e8e\u6beb\u7c73\u6ce2\u9891\u8c31\u7684\u4e25\u91cd\u4fe1\u9053\u8870\u51cf\u548cRIS\u4e3b\u8981\u7531\u65e0\u6e90\u7ec4\u4ef6\u7ec4\u6210\u5bfc\u81f4\u5b8c\u6574CSI\u83b7\u53d6\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56CSI\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528\u63a5\u6536\u5230\u7684\u5bfc\u9891\u4fe1\u53f7\u63d0\u4f9b\u9690\u5f0fCSI\uff0c\u5efa\u7acb\u5bfc\u9891\u4fe1\u53f7\u3001\u4f18\u5316\u7684RIS\u76f8\u4f4d\u504f\u79fb\u548c\u53ef\u8fbe\u5230\u901f\u7387\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCapacity-Net\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "Capacity-Net\u4e3aRIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9CSI\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.26324", "pdf": "https://arxiv.org/pdf/2509.26324", "abs": "https://arxiv.org/abs/2509.26324", "authors": ["Ruiyang Wang", "Haolun Tsu", "David Hunt", "Shaocheng Luo", "Jiwoo Kim", "Miroslav Pajic"], "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-MCoX\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u667a\u80fd\u534f\u8c03\uff0c\u7528\u4e8e\u5ba4\u5185\u63a2\u7d22\u548c\u76ee\u6807\u641c\u7d22\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u63a2\u7d22\u548c\u641c\u7d22\u6548\u7387\u4f4e\u3001\u534f\u8c03\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6LiDAR\u626b\u63cf\u548c\u591a\u6a21\u6001LLM\u63a8\u7406\uff08\u5982GPT-4o\uff09\uff0c\u751f\u6210\u534f\u4f5c\u7684\u8def\u5f84\u70b9\u5206\u914d\u7b56\u7565\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cLLM-MCoX\u63a2\u7d22\u65f6\u95f4\u7f29\u77ed22.7%\uff0c\u641c\u7d22\u6548\u7387\u63d0\u9ad850%\uff0c\u5e76\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u641c\u7d22\u529f\u80fd\u3002", "conclusion": "LLM-MCoX\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.25661", "pdf": "https://arxiv.org/pdf/2509.25661", "abs": "https://arxiv.org/abs/2509.25661", "authors": ["Po-Heng Chou", "Bo-Ren Zheng", "Wan-Jen Huang", "Walid Saad", "Yu Tsao", "Ronald Y. Chang"], "title": "Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift", "categories": ["cs.IT", "cs.AI", "cs.LG", "cs.NI", "eess.SP", "math.IT", "68T07, 68T05, 90C26, 94A05", "C.2.1; C.2.2; C.4; I.2.6; G.1.6"], "comment": "5 pages, 5 figures, and published in IEEE Wireless Communications\n  Letters", "summary": "This study considers multiple reconfigurable intelligent surfaces\n(RISs)-aided multiuser downlink systems with the goal of jointly optimizing the\ntransmitter precoding and RIS phase shift matrix to maximize spectrum\nefficiency. Unlike prior work that assumed ideal RIS reflectivity, a practical\ncoupling effect is considered between reflecting amplitude and phase shift for\nthe RIS elements. This makes the optimization problem non-convex. To address\nthis challenge, we propose a deep deterministic policy gradient (DDPG)-based\ndeep reinforcement learning (DRL) framework. The proposed model is evaluated\nunder both fixed and random numbers of users in practical mmWave channel\nsettings. Simulation results demonstrate that, despite its complexity, the\nproposed DDPG approach significantly outperforms optimization-based algorithms\nand double deep Q-learning, particularly in scenarios with random user\ndistributions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u591aRIS\u8f85\u52a9\u7684\u591a\u7528\u6237\u4e0b\u884c\u94fe\u8def\u7cfb\u7edf\u7684\u9891\u8c31\u6548\u7387\uff0c\u8003\u8651\u4e86RIS\u53cd\u5c04\u5e45\u5ea6\u4e0e\u76f8\u4f4d\u504f\u79fb\u4e4b\u95f4\u7684\u5b9e\u9645\u8026\u5408\u6548\u5e94\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5047\u8bbeRIS\u5177\u6709\u7406\u60f3\u7684\u53cd\u5c04\u7279\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u53cd\u5c04\u5e45\u5ea6\u4e0e\u76f8\u4f4d\u504f\u79fb\u4e4b\u95f4\u7684\u5b9e\u9645\u8026\u5408\u6548\u5e94\uff0c\u5bfc\u81f4\u4f18\u5316\u95ee\u9898\u590d\u6742\u5316\u3002", "method": "\u91c7\u7528DDPG\u6846\u67b6\u4f18\u5316\u53d1\u5c04\u5668\u9884\u7f16\u7801\u548cRIS\u76f8\u4f4d\u504f\u79fb\u77e9\u9635\uff0c\u4ee5\u5728\u6beb\u7c73\u6ce2\u4fe1\u9053\u73af\u5883\u4e0b\u6700\u5927\u5316\u9891\u8c31\u6548\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u590d\u6742\u5ea6\u9ad8\uff0c\u4f46\u6240\u63d0\u51fa\u7684DDPG\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4f18\u5316\u7684\u7b97\u6cd5\u548c\u53cc\u6df1\u5ea6Q\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5728\u968f\u673a\u7528\u6237\u5206\u5e03\u573a\u666f\u4e2d\u3002", "conclusion": "DDPG\u65b9\u6cd5\u5728\u591aRIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u56e0\u5b9e\u9645\u8026\u5408\u6548\u5e94\u5bfc\u81f4\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.26339", "pdf": "https://arxiv.org/pdf/2509.26339", "abs": "https://arxiv.org/abs/2509.26339", "authors": ["Eric R. Damm", "Thomas M. Howard"], "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models", "categories": ["cs.RO"], "comment": "Presented at the Robotics: Science and Systems (RSS) 2025 Workshop on\n  Resilient Off-road Autonomous Robotics (ROAR)", "summary": "Mobile ground robots lacking prior knowledge of an environment must rely on\nsensor data to develop a model of their surroundings. In these scenarios,\nconsistent identification of obstacles and terrain features can be difficult\ndue to noise and algorithmic shortcomings, which can make it difficult for\nmotion planning systems to generate safe motions. One particular difficulty to\novercome is when regions of the cost map switch between being marked as\nobstacles and free space through successive planning cycles. One potential\nsolution to this, which we refer to as Valid in Every Hypothesis (VEH), is for\nthe planning system to plan motions that are guaranteed to be safe through a\nhistory of world models. Another approach is to track a history of world\nmodels, and adjust node costs according to the potential penalty of needing to\nreroute around previously hazardous areas. This work discusses three major\niterations on this idea. The first iteration, called PEH, invokes a sub-search\nfor every node expansion that crosses through a divergence point in the world\nmodels. The second and third iterations, called GEH and GEGRH respectively,\ndefer the sub-search until after an edge expands into the goal region. GEGRH\nuses an additional step to revise the graph based on divergent nodes in each\nworld. Initial results showed that, although PEH and GEH find more optimistic\nsolutions than VEH, they are unable to generate solutions in less than\none-second, which exceeds our requirements for field deployment. Analysis of\nresults from a field experiment in an unstructured, off-road environment on a\nClearpath Robotics Warthog UGV indicate that GEGRH finds lower cost\ntrajectories and has faster average planning times than VEH. Compared to\nsingle-hypothesis (SH) search, where only the latest world model is considered,\nGEGRH generates more conservative plans with a small increase in average\nplanning time.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u79fb\u52a8\u5730\u9762\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fd0\u52a8\u89c4\u5212\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u5386\u53f2\u6a21\u578b\u8ffd\u8e2a\u65b9\u6cd5\uff08PEH\u3001GEH\u3001GEGRH\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u611f\u5668\u566a\u58f0\u548c\u7b97\u6cd5\u9650\u5236\u5bfc\u81f4\u7684\u969c\u788d\u7269\u8bc6\u522b\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "1. PEH\uff1a\u5728\u8282\u70b9\u6269\u5c55\u65f6\u8fdb\u884c\u5b50\u641c\u7d22\uff1b2. GEH\u548cGEGRH\uff1a\u5ef6\u8fdf\u5b50\u641c\u7d22\u81f3\u76ee\u6807\u533a\u57df\uff0cGEGRH\u989d\u5916\u4fee\u6b63\u56fe\u8282\u70b9\u3002", "result": "GEGRH\u80fd\u751f\u6210\u66f4\u4f4e\u6210\u672c\u8f68\u8ff9\u4e14\u89c4\u5212\u65f6\u95f4\u66f4\u77ed\uff0c\u4f18\u4e8eVEH\u65b9\u6cd5\uff1b\u76f8\u6bd4\u5355\u5047\u8bbe\uff08SH\uff09\u641c\u7d22\uff0c\u89c4\u5212\u66f4\u4fdd\u5b88\u4e14\u65f6\u95f4\u7565\u6709\u589e\u52a0\u3002", "conclusion": "GEGRH\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u6027\u8981\u6c42\u7684\u573a\u666f\u3002"}}
{"id": "2509.25696", "pdf": "https://arxiv.org/pdf/2509.25696", "abs": "https://arxiv.org/abs/2509.25696", "authors": ["Takuya Fujimura", "Kota Dohi", "Natsuo Yamashita", "Yohei Kawaguchi"], "title": "Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?", "categories": ["cs.LG", "cs.CL", "eess.SP"], "comment": null, "summary": "Time-series question answering (TSQA) tasks face significant challenges due\nto the lack of labeled data. Alternatively, with recent advancements in\nlarge-scale models, vision-language models (VLMs) have demonstrated the\npotential to analyze time-series signals in a zero-shot manner. In this paper,\nwe propose a training approach that uses pseudo labels generated by a VLM.\nAlthough VLMs can produce incorrect labels, TSQA models can still be\neffectively trained based on the property that deep neural networks are\ninherently robust to such noisy labels. Our experimental results demonstrate\nthat TSQA models are not only successfully trained with pseudo labels, but also\nsurpass the performance of the VLM itself by leveraging a large amount of\nunlabeled data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\uff08TSQA\uff09\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u9762\u4e34\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u800c\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u5c55\u793a\u4e86\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7VLM\u751f\u6210\u7684\u4f2a\u6807\u7b7e\uff08\u53ef\u80fd\u5b58\u5728\u9519\u8bef\uff09\u8bad\u7ec3TSQA\u6a21\u578b\uff0c\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u566a\u58f0\u6807\u7b7e\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSQA\u6a21\u578b\u4e0d\u4ec5\u80fd\u901a\u8fc7\u4f2a\u6807\u7b7e\u6210\u529f\u8bad\u7ec3\uff0c\u8fd8\u80fd\u501f\u52a9\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u8d85\u8d8aVLM\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86TSQA\u4efb\u52a1\u4e2d\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4f2a\u6807\u7b7e\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9c81\u68d2\u6027\u7684\u7ed3\u5408\u4f18\u52bf\u3002"}}
{"id": "2509.26375", "pdf": "https://arxiv.org/pdf/2509.26375", "abs": "https://arxiv.org/abs/2509.26375", "authors": ["Zichao Shen", "Chen Gao", "Jiaqi Yuan", "Tianchen Zhu", "Xingcheng Fu", "Qingyun Sun"], "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.", "AI": {"tldr": "SDA-PLANNER\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001\u4f9d\u8d56\u56fe\u5904\u7406\u52a8\u4f5c\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u9519\u8bef\u81ea\u9002\u5e94\u7b56\u7565\u6539\u8fdb\u6267\u884c\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u89c4\u5212\u5668\u5b58\u5728\u56fa\u5b9a\u8303\u5f0f\u3001\u7f3a\u4e4f\u52a8\u4f5c\u5e8f\u5217\u7ea6\u675f\u548c\u9519\u8bef\u65e0\u89c6\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u72b6\u6001\u4f9d\u8d56\u56fe\u5efa\u6a21\u52a8\u4f5c\u7ea6\u675f\uff0c\u91c7\u7528\u9519\u8bef\u56de\u6eaf\u548c\u81ea\u9002\u5e94\u5b50\u6811\u751f\u6210\u7b56\u7565\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSDA-PLANNER\u5728\u6210\u529f\u7387\u53ca\u76ee\u6807\u5b8c\u6210\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u9519\u8bef\u573a\u666f\u4e0b\u3002", "conclusion": "SDA-PLANNER\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u672c\u4f53\u4efb\u52a1\u89c4\u5212\u7684\u6548\u679c\u3002"}}
{"id": "2509.25802", "pdf": "https://arxiv.org/pdf/2509.25802", "abs": "https://arxiv.org/abs/2509.25802", "authors": ["Yanan Zhao", "Feng Ji", "Xingchao Jian", "Wee Peng Tay"], "title": "Graph Distribution-valued Signals: A Wasserstein Space Perspective", "categories": ["stat.ML", "eess.SP"], "comment": "Submitted to ICASSP 2026", "summary": "We introduce a novel framework for graph signal processing (GSP) that models\nsignals as graph distribution-valued signals (GDSs), which are probability\ndistributions in the Wasserstein space. This approach overcomes key limitations\nof classical vector-based GSP, including the assumption of synchronous\nobservations over vertices, the inability to capture uncertainty, and the\nrequirement for strict correspondence in graph filtering. By representing\nsignals as distributions, GDSs naturally encode uncertainty and stochasticity,\nwhile strictly generalizing traditional graph signals. We establish a\nsystematic dictionary mapping core GSP concepts to their GDS counterparts,\ndemonstrating that classical definitions are recovered as special cases. The\neffectiveness of the framework is validated through graph filter learning for\nprediction tasks, supported by experimental results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u4fe1\u53f7\u5904\u7406\uff08GSP\uff09\u6846\u67b6\uff0c\u5c06\u4fe1\u53f7\u5efa\u6a21\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u56fe\u5206\u5e03\u503c\u4fe1\u53f7\uff08GDS\uff09\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5411\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfGSP\u65b9\u6cd5\u5047\u8bbe\u9876\u70b9\u540c\u6b65\u89c2\u6d4b\u3001\u65e0\u6cd5\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u4e14\u9700\u8981\u4e25\u683c\u7684\u56fe\u6ee4\u6ce2\u5bf9\u5e94\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5c06\u4fe1\u53f7\u8868\u793a\u4e3a\u6982\u7387\u5206\u5e03\uff0cGDS\u6846\u67b6\u81ea\u7136\u5730\u7f16\u7801\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\uff0c\u540c\u65f6\u63a8\u5e7f\u4e86\u4f20\u7edf\u56fe\u4fe1\u53f7\u7684\u5b9a\u4e49\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u56fe\u6ee4\u6ce2\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "GDS\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u5177\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u7684GSP\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u3002"}}
{"id": "2509.26428", "pdf": "https://arxiv.org/pdf/2509.26428", "abs": "https://arxiv.org/abs/2509.26428", "authors": ["Mattia Piazza", "Mattia Piccinini", "Sebastiano Taddei", "Francesco Biral", "Enrico Bertolazzi"], "title": "Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "The computation of time-optimal velocity profiles along prescribed paths,\nsubject to generic acceleration constraints, is a crucial problem in robot\ntrajectory planning, with particular relevance to autonomous racing. However,\nthe existing methods either support arbitrary acceleration constraints at high\ncomputational cost or use conservative box constraints for computational\nefficiency. We propose FBGA, a new \\underline{F}orward-\\underline{B}ackward\nalgorithm with \\underline{G}eneric \\underline{A}cceleration constraints, which\nachieves both high accuracy and low computation time. FBGA operates forward and\nbackward passes to maximize the velocity profile in short, discretized path\nsegments, while satisfying user-defined performance limits. Tested on five\nracetracks and two vehicle classes, FBGA handles complex, non-convex\nacceleration constraints with custom formulations. Its maneuvers and lap times\nclosely match optimal control baselines (within $0.11\\%$-$0.36\\%$), while being\nup to three orders of magnitude faster. FBGA maintains high accuracy even with\ncoarse discretization, making it well-suited for online multi-query trajectory\nplanning. Our open-source \\texttt{C++} implementation is available at:\nhttps://anonymous.4open.science/r/FB_public_RAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86FBGA\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6ee1\u8db3\u901a\u7528\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u6761\u4ef6\u4e0b\u8ba1\u7b97\u65f6\u95f4\u6700\u4f18\u901f\u5ea6\u5256\u9762\uff0c\u7b97\u6cd5\u9ad8\u6548\u4e14\u7cbe\u51c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4fdd\u5b88\u7684\u7ea6\u675f\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7b49\u9ad8\u52a8\u6001\u73af\u5883\u7684\u9700\u6c42\u3002", "method": "FBGA\u901a\u8fc7\u524d\u5411\u540e\u5411\u7b97\u6cd5\u5728\u79bb\u6563\u5316\u8def\u5f84\u6bb5\u4e0a\u6700\u5927\u5316\u901f\u5ea6\u5256\u9762\uff0c\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u7684\u6027\u80fd\u9650\u5236\u3002", "result": "\u6d4b\u8bd5\u8868\u660eFBGA\u5728\u590d\u6742\u975e\u51f8\u52a0\u901f\u5ea6\u7ea6\u675f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u6700\u4f18\u63a7\u5236\u57fa\u51c6\u63a5\u8fd1\uff0c\u8ba1\u7b97\u901f\u5ea6\u5feb\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FBGA\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u7b97\u6cd5\uff0c\u9002\u5408\u5728\u7ebf\u591a\u67e5\u8be2\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2509.26083", "pdf": "https://arxiv.org/pdf/2509.26083", "abs": "https://arxiv.org/abs/2509.26083", "authors": ["Adi Wijaya", "Said Hasibuan", "Wiga Maulana Baihaqi", "Rizki Darmawan", "Rifkie Primartha", "Catur Supriyanto"], "title": "Bibliometric-enhanced Systematic Literature Review of EEG in Education: Learning Concepts, Computational Methods, and Research Opportunities", "categories": ["q-bio.NC", "eess.SP"], "comment": "29 pages, 6 figures, 5 tables", "summary": "Application of electroencephalography (EEG) in educational research has grown\nsubstantially, yet a comprehensive integration of methodological frameworks,\neducational constructs, computational methods, and research gaps remains\nlimited. This study applies a Bibliometric-enhanced Systematic Literature\nReview (BenSLR) to provide a systematic overview of EEG in education.\nLiterature was extracted from Scopus, screened, and analyzed, with keyword\nco-occurrence evaluated using VOSviewer and emerging trends visualized through\nan Enhanced Strategic Diagram via BiblioPlot. Key findings include engagement,\nattention, and learning style as prominent constructs, with machine learning\nand deep learning frequently employed for modeling complex cognitive states.\nEEG signal processing, feature extraction, and assessment of cognitive and\naffective states were recurrent across studies. Innovative interventions such\nas virtual reality and neurofeedback demonstrate EEG's role in supporting\nadaptive and individualized learning experiences. Challenges remain in linking\nneural markers with observable learning behaviors, extending measurements\nbeyond attention and working memory, and enhancing predictive model\ngeneralizability. The study demonstrates BenSLR's potential to integrate\nqualitative and quantitative perspectives and offers a transferable approach\nfor other research areas to develop methodologies and evidence-based\neducational interventions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6587\u732e\u8ba1\u91cf\u589e\u5f3a\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08BenSLR\uff09\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86EEG\u5728\u6559\u80b2\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5173\u952e\u8bcd\u5171\u73b0\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u603b\u7ed3\u4e86\u8ba4\u77e5\u548c\u60c5\u611f\u72b6\u6001\u8bc4\u4f30\u7684\u7814\u7a76\u8fdb\u5c55\u4ee5\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "EEG\u5728\u6559\u80b2\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u65b9\u6cd5\u8bba\u6846\u67b6\u3001\u6559\u80b2\u6784\u5efa\u3001\u8ba1\u7b97\u65b9\u6cd5\u548c\u7814\u7a76\u7f3a\u53e3\u7684\u5168\u9762\u6574\u5408\u3002", "method": "\u91c7\u7528BenSLR\u65b9\u6cd5\uff0c\u4eceScopus\u63d0\u53d6\u6587\u732e\u5e76\u5206\u6790\u5173\u952e\u8bcd\u5171\u73b0\uff08VOSviewer\uff09\u548c\u65b0\u5174\u8d8b\u52bf\uff08BiblioPlot\uff09\u3002", "result": "\u5173\u6ce8\u7684\u6838\u5fc3\u6784\u5efa\u5305\u62ec\u53c2\u4e0e\u5ea6\u3001\u6ce8\u610f\u529b\u548c\u5b66\u4e60\u98ce\u683c\uff0c\u5e38\u7528\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u590d\u6742\u8ba4\u77e5\u72b6\u6001\uff1b\u865a\u62df\u73b0\u5b9e\u548c\u795e\u7ecf\u53cd\u9988\u5c55\u793a\u4e86EEG\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9eBenSLR\u80fd\u6574\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u89c6\u89d2\uff0c\u4e3a\u89e3\u51b3\u7814\u7a76\u7f3a\u53e3\u548c\u63a8\u5e7f\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2509.26439", "pdf": "https://arxiv.org/pdf/2509.26439", "abs": "https://arxiv.org/abs/2509.26439", "authors": ["Filip Kulisiewicz", "Basak Sakcak", "Evan G. Center", "Juho Kalliokoski", "Katherine J. Mimnaugh", "Steven M. LaValle", "Timo Ojala"], "title": "Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence", "categories": ["cs.RO"], "comment": "24th IEEE International Symposium on Mixed and Augmented Reality\n  (ISMAR)", "summary": "Immersive telepresence, when a user views the video stream of a $360^\\circ$\ncamera in a remote environment using a Head Mounted Display (HMD), has great\npotential to improve the sense of being in a remote environment. In most cases\nof immersive robotic telepresence, the camera is mounted on a mobile robot\nwhich increases the portion of the environment that the remote user can\nexplore. However, robot motions can induce unpleasant symptoms associated with\nVirtual Reality (VR) sickness, degrading the overall user experience. Previous\nresearch has shown that unwinding the rotations of the robot, that is,\ndecoupling the rotations that the camera undergoes due to robot motions from\nwhat is seen by the user, can increase user comfort and reduce VR sickness.\nHowever, that work considered a virtual environment and a simulated robot. In\nthis work, to test whether the same hypotheses hold when the video stream from\na real camera is used, we carried out a user study $(n=36)$ in which the\nunwinding rotations method was compared against coupled rotations in a task\ncompleted through a panoramic camera mounted on a robotic arm. Furthermore,\nwithin an inspection task which involved translations and rotations in three\ndimensions, we tested whether unwinding the robot rotations impacted the\nperformance of users. The results show that the users found the unwinding\nrotations method to be more comfortable and preferable, and that a reduced\nlevel of VR sickness can be achieved without a significant impact on task\nperformance.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u89e3\u8026\u673a\u5668\u4eba\u8fd0\u52a8\u5f15\u8d77\u7684\u6444\u50cf\u5934\u65cb\u8f6c\uff0c\u53ef\u4ee5\u51cf\u5c11VR\u6655\u52a8\u75c7\u5e76\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5728\u771f\u5b9e\u6444\u50cf\u5934\u7684360\u00b0\u89c6\u9891\u6d41\u4e2d\uff0c\u89e3\u8026\u673a\u5668\u4eba\u65cb\u8f6c\u662f\u5426\u80fd\u51cf\u5c11VR\u6655\u52a8\u75c7\u5e76\u63d0\u9ad8\u7528\u6237\u8212\u9002\u5ea6\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08n=36\uff09\u6bd4\u8f83\u89e3\u8026\u65cb\u8f6c\u4e0e\u8026\u5408\u65cb\u8f6c\u65b9\u6cd5\uff0c\u4f7f\u7528\u5168\u666f\u6444\u50cf\u5934\u548c\u673a\u68b0\u81c2\u5b8c\u6210\u4e09\u7ef4\u4efb\u52a1\u3002", "result": "\u7528\u6237\u8ba4\u4e3a\u89e3\u8026\u65cb\u8f6c\u66f4\u8212\u9002\u4e14\u504f\u597d\u4f7f\u7528\uff0c\u540c\u65f6VR\u6655\u52a8\u75c7\u51cf\u5c11\u4e14\u4efb\u52a1\u8868\u73b0\u672a\u53d7\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u89e3\u8026\u673a\u5668\u4eba\u65cb\u8f6c\u53ef\u6709\u6548\u63d0\u5347\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u4e34\u573a\u611f\u7684\u8212\u9002\u5ea6\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u73af\u5883\u3002"}}
{"id": "2509.26489", "pdf": "https://arxiv.org/pdf/2509.26489", "abs": "https://arxiv.org/abs/2509.26489", "authors": ["Sattwik Basu", "Chaitanya Amballa", "Zhongweiyang Xu", "Jorge Van\u010do Sampedro", "Srihari Nelakuditi", "Romit Roy Choudhury"], "title": "Contrastive Diffusion Guidance for Spatial Inverse Problems", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "We consider the inverse problem of reconstructing the spatial layout of a\nplace, a home floorplan for example, from a user`s movements inside that\nlayout. Direct inversion is ill-posed since many floorplans can explain the\nsame movement trajectories. We adopt a diffusion-based posterior sampler to\ngenerate layouts consistent with the measurements. While active research is in\nprogress on generative inverse solvers, we find that the forward operator in\nour problem poses new challenges. The path-planning process inside a floorplan\nis a non-invertible, non-differentiable function, and causes instability while\noptimizing using the likelihood score. We break-away from existing approaches\nand reformulate the likelihood score in a smoother embedding space. The\nembedding space is trained with a contrastive loss which brings compatible\nfloorplans and trajectories close to each other, while pushing mismatched pairs\nfar apart. We show that a surrogate form of the likelihood score in this\nembedding space is a valid approximation of the true likelihood score, making\nit possible to steer the denoising process towards the posterior. Across\nextensive experiments, our model CoGuide produces more consistent floorplans\nfrom trajectories, and is more robust than differentiable-planner baselines and\nguided-diffusion methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u5728\u623f\u95f4\u5185\u7684\u79fb\u52a8\u8f68\u8ff9\u91cd\u5efa\u623f\u95f4\u5e03\u5c40\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u975e\u53ef\u9006\u3001\u975e\u53ef\u5fae\u8def\u5f84\u89c4\u5212\u7684\u95ee\u9898\u3002", "motivation": "\u4ece\u7528\u6237\u7684\u79fb\u52a8\u8f68\u8ff9\u91cd\u5efa\u623f\u95f4\u5e03\u5c40\u662f\u4e00\u4e2a\u75c5\u6001\u95ee\u9898\uff0c\u56e0\u4e3a\u591a\u79cd\u5e03\u5c40\u53ef\u80fd\u5bf9\u5e94\u76f8\u540c\u7684\u8f68\u8ff9\u3002\u73b0\u6709\u7684\u751f\u6210\u9006\u89e3\u65b9\u6cd5\u5728\u8def\u5f84\u89c4\u5212\u7684\u975e\u53ef\u9006\u6027\u548c\u975e\u53ef\u5fae\u6027\u4e0a\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u517c\u5bb9\u7684\u5e03\u5c40\u548c\u8f68\u8ff9\u5728\u7a7a\u95f4\u4e2d\u63a5\u8fd1\uff0c\u4e0d\u517c\u5bb9\u7684\u8fdc\u79bb\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5b9a\u4e49\u4e86\u8fd1\u4f3c\u4f3c\u7136\u5206\u6570\uff0c\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\uff08CoGuide\uff09\u6bd4\u53ef\u5fae\u89c4\u5212\u5668\u548c\u5f15\u5bfc\u6269\u6563\u65b9\u6cd5\u66f4\u80fd\u751f\u6210\u4e00\u81f4\u7684\u5e03\u5c40\uff0c\u4e14\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u7684\u5e73\u6ed1\u91cd\u6784\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9006\u95ee\u9898\u6c42\u89e3\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u975e\u53ef\u5fae\u95ee\u9898\u7684\u540e\u9a8c\u91c7\u6837\u3002"}}
{"id": "2509.26459", "pdf": "https://arxiv.org/pdf/2509.26459", "abs": "https://arxiv.org/abs/2509.26459", "authors": ["Akshay Jaitly", "Devesh K. Jha", "Kei Ota", "Yuki Shirai"], "title": "Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization", "categories": ["cs.RO", "cs.CG"], "comment": "8 pages, 8 figures. Accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Optimization-based methods are widely used for computing fast, diverse\nsolutions for complex tasks such as collision-free movement or planning in the\npresence of contacts. However, most of these methods require enforcing\nnon-penetration constraints between objects, resulting in a non-trivial and\ncomputationally expensive problem. This makes the use of optimization-based\nmethods for planning and control challenging. In this paper, we present a\nmethod to efficiently enforce non-penetration of sets while performing\noptimization over their configuration, which is directly applicable to problems\nlike collision-aware trajectory optimization. We introduce novel differentiable\nconditions with analytic expressions to achieve this. To enforce non-collision\nbetween non-smooth bodies using these conditions, we introduce a method to\napproximate polytopes as smooth semi-algebraic sets. We present several\nnumerical experiments to demonstrate the performance of the proposed method and\ncompare the performance with other baseline methods recently proposed in the\nliterature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u5b9e\u73b0\u96c6\u5408\u7684\u975e\u7a7f\u900f\u6027\uff0c\u9002\u7528\u4e8e\u78b0\u649e\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u7b49\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u53ef\u5fae\u5206\u6761\u4ef6\u548c\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u7ed3\u5408\u5149\u6ed1\u534a\u4ee3\u6570\u96c6\u5408\u8fd1\u4f3c\u591a\u9762\u4f53\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f18\u5316\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u65e0\u78b0\u649e\u8fd0\u52a8\u6216\u63a5\u89e6\u89c4\u5212\uff09\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7684\u975e\u7a7f\u900f\u7ea6\u675f\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u89c4\u5212\u548c\u63a7\u5236\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u53ef\u5fae\u5206\u6761\u4ef6\u548c\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5f3a\u5236\u5b9e\u73b0\u975e\u7a7f\u900f\u6027\uff0c\u5e76\u901a\u8fc7\u5149\u6ed1\u534a\u4ee3\u6570\u96c6\u5408\u8fd1\u4f3c\u591a\u9762\u4f53\u7684\u65b9\u6cd5\u5904\u7406\u975e\u5149\u6ed1\u7269\u4f53\u7684\u78b0\u649e\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4e0e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u4e86\u4f18\u5316\u8fc7\u7a0b\u4e2d\u975e\u7a7f\u900f\u7ea6\u675f\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u78b0\u649e\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.26513", "pdf": "https://arxiv.org/pdf/2509.26513", "abs": "https://arxiv.org/abs/2509.26513", "authors": ["Saad Abdul Ghani", "Kameron Lee", "Xuesu Xiao"], "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments", "categories": ["cs.RO"], "comment": null, "summary": "Generating large and diverse obstacle datasets to learn motion planning in\nenvironments with dynamic obstacles is challenging due to the vast space of\npossible obstacle trajectories. Inspired by hallucination-based data synthesis\napproaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a\nself-supervised framework for creating rich dynamic obstacle datasets based on\nexisting optimal motion plans without requiring expensive expert demonstrations\nor trial-and-error exploration. LfH-CP factorizes hallucination into two\nstages: first identifying when and where obstacles must appear in order to\nresult in an optimal motion plan, i.e., the critical points, and then\nprocedurally generating diverse trajectories that pass through these points\nwhile avoiding collisions. This factorization avoids generative failures such\nas mode collapse and ensures coverage of diverse dynamic behaviors. We further\nintroduce a diversity metric to quantify dataset richness and show that LfH-CP\nproduces substantially more varied training data than existing baselines.\nExperiments in simulation demonstrate that planners trained on LfH-CP datasets\nachieves higher success rates compared to a prior hallucination method.", "AI": {"tldr": "LfH-CP\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5e7b\u89c9\u5173\u952e\u70b9\u751f\u6210\u52a8\u6001\u969c\u788d\u7269\u6570\u636e\u96c6\uff0c\u907f\u514d\u751f\u6210\u5931\u8d25\u5e76\u63d0\u5347\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u52a8\u6001\u969c\u788d\u7269\u8f68\u8ff9\u7a7a\u95f4\u5e9e\u5927\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u5e7b\u89c9\uff1a\u8bc6\u522b\u5173\u952e\u70b9\uff0c\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\u3002", "result": "LfH-CP\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u66f4\u5177\u591a\u6837\u6027\uff0c\u8fd0\u52a8\u89c4\u5212\u5668\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "LfH-CP\u4e3a\u52a8\u6001\u969c\u788d\u7269\u6570\u636e\u96c6\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26518", "pdf": "https://arxiv.org/pdf/2509.26518", "abs": "https://arxiv.org/abs/2509.26518", "authors": ["Shuoyu Yue", "Pengpeng Li", "Yang Xu", "Kunrui Ze", "Xingjian Long", "Huazi Cao", "Guibin Sun"], "title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms", "categories": ["cs.RO"], "comment": null, "summary": "Mean-shift-based approaches have recently emerged as the most effective\nmethods for robot swarm shape assembly tasks. These methods rely on image-based\nrepresentations of target shapes to compute local density gradients and perform\nmean-shift exploration, which constitute their core mechanism. However, such\nimage representations incur substantial memory overhead, which can become\nprohibitive for high-resolution or 3D shapes. To overcome this limitation, we\npropose a memory-efficient tree map representation that hierarchically encodes\nuser-specified shapes and is applicable to both 2D and 3D scenarios. Building\non this representation, we design a behavior-based distributed controller that\nenables assignment-free shape assembly. Comparative 2D and 3D simulations\nagainst a state-of-the-art mean-shift algorithm demonstrate one to two orders\nof magnitude lower memory usage and two to three times faster shape entry while\nmaintaining comparable uniformity. Finally, we validate the framework through\nphysical experiments with 6 to 7 UAVs, confirming its real-world practicality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5f62\u56fe\u8868\u793a\u7684\u8bb0\u5fc6\u9ad8\u6548\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u5f62\u72b6\u7ec4\u88c5\u4efb\u52a1\u4e2d\u4f20\u7edf\u5747\u503c\u6f02\u79fb\u65b9\u6cd5\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5e76\u57282D\u548c3D\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u5747\u503c\u6f02\u79fb\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u5f62\u72b6\u7ec4\u88c5\u4efb\u52a1\u4e2d\u4f9d\u8d56\u56fe\u50cf\u8868\u793a\uff0c\u5bfc\u81f4\u9ad8\u5206\u8fa8\u7387\u62163D\u5f62\u72b6\u4e0b\u5185\u5b58\u5f00\u9500\u8fc7\u5927\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u5c42\u7f16\u7801\u7528\u6237\u6307\u5b9a\u5f62\u72b6\u7684\u6811\u5f62\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u63a7\u5236\u5668\uff0c\u652f\u6301\u65e0\u5206\u914d\u7684\u5f62\u72b6\u7ec4\u88c5\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\u4f7f\u7528\u4e0a\u6bd4\u73b0\u6709\u5747\u503c\u6f02\u79fb\u7b97\u6cd5\u4f4e1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u5f62\u72b6\u8fdb\u5165\u901f\u5ea6\u5feb2-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u5747\u5300\u6027\u3002\u7269\u7406\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u6811\u5f62\u56fe\u8868\u793a\u548c\u5206\u5e03\u5f0f\u63a7\u5236\u5668\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u5f62\u72b6\u7ec4\u88c5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.26558", "pdf": "https://arxiv.org/pdf/2509.26558", "abs": "https://arxiv.org/abs/2509.26558", "authors": ["Andr\u00e9s Mart\u00ednez-Silva", "David Alejo", "Luis Merino", "Fernando Caballero"], "title": "Radio-based Multi-Robot Odometry and Relative Localization", "categories": ["cs.RO"], "comment": null, "summary": "Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And\nRanging (radar), which have traditionally seen limited adoption in robotics,\nare experiencing a boost in popularity thanks to their robustness to harsh\nenvironmental conditions and cluttered environments. This work proposes a\nmulti-robot UGV-UAV localization system that leverages the two technologies\nwith inexpensive and readily-available sensors, such as Inertial Measurement\nUnits (IMUs) and wheel encoders, to estimate the relative position of an aerial\nrobot with respect to a ground robot. The first stage of the system pipeline\nincludes a nonlinear optimization framework to trilaterate the location of the\naerial platform based on UWB range data, and a radar pre-processing module with\nloosely coupled ego-motion estimation which has been adapted for a multi-robot\nscenario. Then, the pre-processed radar data as well as the relative\ntransformation are fed to a pose-graph optimization framework with odometry and\ninter-robot constraints. The system, implemented for the Robotic Operating\nSystem (ROS 2) with the Ceres optimizer, has been validated in\nSoftware-in-the-Loop (SITL) simulations and in a real-world dataset. The\nproposed relative localization module outperforms state-of-the-art closed-form\nmethods which are less robust to noise. Our SITL environment includes a custom\nGazebo plugin for generating realistic UWB measurements modeled after real\ndata. Conveniently, the proposed factor graph formulation makes the system\nreadily extensible to full Simultaneous Localization And Mapping (SLAM).\nFinally, all the code and experimental data is publicly available to support\nreproducibility and to serve as a common open dataset for benchmarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408UWB\u548c\u96f7\u8fbe\u6280\u672f\u7684UGV-UAV\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5229\u7528\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b9e\u73b0\u7a7a\u4e2d\u673a\u5668\u4eba\u76f8\u5bf9\u4e8e\u5730\u9762\u673a\u5668\u4eba\u7684\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u7535\u5b9a\u4f4d\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u6709\u9650\uff0c\u4f46\u5176\u5bf9\u73af\u5883\u6076\u52a3\u548c\u6742\u4e71\u573a\u666f\u7684\u9c81\u68d2\u6027\u4f7f\u5176\u91cd\u65b0\u53d7\u5230\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408UWB\u548c\u96f7\u8fbe\u6280\u672f\uff0c\u5f00\u53d1\u4f4e\u6210\u672c\u7684\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u57fa\u4e8eUWB\u7684\u975e\u7ebf\u6027\u4f18\u5316\u6846\u67b6\u548c\u96f7\u8fbe\u9884\u5904\u7406\u7684\u8fd0\u52a8\u4f30\u8ba1\uff1b2) \u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u878d\u5408\u96f7\u8fbe\u6570\u636e\u548c\u76f8\u5bf9\u53d8\u6362\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86ROS 2\u652f\u6301\uff0c\u5e76\u4f7f\u7528Ceres\u4f18\u5316\u5668\u3002", "result": "\u7cfb\u7edf\u7684\u76f8\u5bf9\u5b9a\u4f4d\u6a21\u5757\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4f18\u4e8e\u73b0\u6709\u95ed\u5f0f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u3002\u7cfb\u7edf\u652f\u6301\u6269\u5c55\u5230SLAM\uff0c\u4e14\u4ee3\u7801\u548c\u6570\u636e\u96c6\u516c\u5f00\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u4f4e\u6210\u672c\u3001\u9c81\u68d2\u6027\u5f3a\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26581", "pdf": "https://arxiv.org/pdf/2509.26581", "abs": "https://arxiv.org/abs/2509.26581", "authors": ["Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework", "categories": ["cs.RO"], "comment": null, "summary": "We present Graphite, a GPU-accelerated nonlinear graph optimization\nframework. It provides a CUDA C++ interface to enable the sharing of code\nbetween a realtime application, such as a SLAM system, and its optimization\ntasks. The framework supports techniques to reduce memory usage, including\nin-place optimization, support for multiple floating point types and\nmixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite\non well-known bundle adjustment problems and find that it achieves similar\nperformance to MegBA, a solver specialized for bundle adjustment, while\nmaintaining generality and using less memory. We also apply Graphite to global\nvisual-inertial bundle adjustment on maps generated from stereo-inertial SLAM\ndatasets, and observe speed ups of up to 59x compared to a CPU baseline. Our\nresults indicate that our solver enables faster large-scale optimization on\nboth desktop and resource-constrained devices.", "AI": {"tldr": "Graphite\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684\u975e\u7ebf\u6027\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u652f\u6301\u5185\u5b58\u4f18\u5316\u548c\u6df7\u5408\u7cbe\u5ea6\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8eCPU\u57fa\u7ebf\u3002", "motivation": "\u4e3a\u4e86\u5728\u5b9e\u65f6\u5e94\u7528\uff08\u5982SLAM\u7cfb\u7edf\uff09\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u4f18\u5316\uff0c\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u63d0\u4f9bCUDA C++\u63a5\u53e3\uff0c\u652f\u6301\u5c31\u5730\u4f18\u5316\u3001\u591a\u79cd\u6d6e\u70b9\u7c7b\u578b\u3001\u6df7\u5408\u7cbe\u5ea6\u6a21\u5f0f\u548c\u52a8\u6001\u8ba1\u7b97Jacobians\u3002", "result": "\u5728\u6346\u7ed1\u8c03\u6574\u95ee\u9898\u4e0a\u6027\u80fd\u63a5\u8fd1\u4e13\u7528\u6c42\u89e3\u5668MegBA\uff0c\u5e76\u5728\u89c6\u89c9-\u60ef\u6027\u6346\u7ed1\u8c03\u6574\u4e2d\u6bd4CPU\u57fa\u7ebf\u5feb59\u500d\u3002", "conclusion": "Graphite\u80fd\u5728\u684c\u9762\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u66f4\u5feb\u7684\u4f18\u5316\u3002"}}
{"id": "2509.26633", "pdf": "https://arxiv.org/pdf/2509.26633", "abs": "https://arxiv.org/abs/2509.26633", "authors": ["Lujie Yang", "Xiaoyu Huang", "Zhen Wu", "Angjoo Kanazawa", "Pieter Abbeel", "Carmelo Sferrazza", "C. Karen Liu", "Rocky Duan", "Guanya Shi"], "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project website: https://omniretarget.github.io", "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.", "AI": {"tldr": "OmniRetarget\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5e76\u4fdd\u6301\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\u4e2d\u7684\u7269\u7406\u4e0d\u81ea\u7136\u95ee\u9898\uff0c\u5e76\u9ad8\u6548\u5730\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\u5b58\u5728\u7269\u7406\u4e0d\u81ea\u7136\uff08\u5982\u811a\u6ed1\u3001\u7a7f\u900f\uff09\u548c\u5ffd\u89c6\u5173\u952e\u4ea4\u4e92\u7684\u95ee\u9898\uff0cOmniRetarget\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u548c\u4fdd\u6301\u4ea4\u4e92\u5173\u7cfb\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "OmniRetarget\u57fa\u4e8e\u4ea4\u4e92\u7f51\u683c\u663e\u5f0f\u5efa\u6a21\u4ee3\u7406\u3001\u5730\u5f62\u548c\u7269\u4f53\u7684\u7a7a\u95f4\u4e0e\u63a5\u89e6\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316Laplacian\u53d8\u5f62\u548c\u6ee1\u8db3\u8fd0\u52a8\u5b66\u7ea6\u675f\u751f\u6210\u53ef\u884c\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e868\u5c0f\u65f6\u7684\u9ad8\u8d28\u91cf\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5728\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u63a5\u89e6\u4fdd\u6301\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e3aRL\u7b56\u7565\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bad\u7ec3\u6570\u636e\uff08\u4ec5\u97005\u4e2a\u5956\u52b1\u9879\uff09\u3002", "conclusion": "OmniRetarget\u6210\u529f\u89e3\u51b3\u4e86\u8fd0\u52a8\u91cd\u5b9a\u5411\u7684\u5173\u952e\u95ee\u9898\uff0c\u652f\u6301\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6267\u884c\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2509.26642", "pdf": "https://arxiv.org/pdf/2509.26642", "abs": "https://arxiv.org/abs/2509.26642", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Jiadong Xu", "Nuowei Han", "Chenyang Gu", "Hao Chen", "Kaichen Zhou", "Renrui Zhang", "Kai Chin Hsieh", "Kun Wu", "Zhengping Che", "Jian Tang", "Shanghang Zhang"], "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u611f\u5b98\u8bed\u8a00-\u52a8\u4f5c\uff08MLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u540c\u611f\u77e5\u5f02\u6784\u611f\u5b98\u6a21\u6001\u5e76\u9884\u6d4b\u672a\u6765\u591a\u611f\u5b98\u76ee\u6807\uff0c\u4ee5\u589e\u5f3a\u5bf9\u7269\u7406\u4e16\u754c\u7684\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u89e3\u91ca\u751f\u6210\u52a8\u4f5c\uff0c\u4f46\u5ffd\u89c6\u4e86\u673a\u5668\u4eba\u9700\u8981\u611f\u77e5\u548c\u4ea4\u4e92\u7684\u7a7a\u95f4\u7269\u7406\u4e16\u754c\u4fe1\u606f\uff0c\u4ece\u800c\u9650\u5236\u4e86\u590d\u6742\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u63a7\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u65b9\u6848\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u89e3\u91ca\u591a\u6a21\u6001\u7ebf\u7d22\uff08\u59822D\u56fe\u50cf\u30013D\u70b9\u4e91\u548c\u89e6\u89c9\u6807\u8bb0\uff09\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u672a\u6765\u591a\u611f\u5b98\u751f\u6210\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u589e\u5f3a\u5bf9\u7269\u7406\u52a8\u6001\u7684\u7406\u89e3\u3002", "result": "MLA\u6a21\u578b\u5728\u590d\u6742\u4e14\u63a5\u89e6\u5bc6\u96c6\u7684\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u76842D\u548c3D VLA\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u4e8612%\u548c24%\u7684\u6027\u80fd\uff0c\u5e76\u663e\u793a\u51fa\u66f4\u597d\u7684\u5bf9\u672a\u89c1\u914d\u7f6e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MLA\u6a21\u578b\u901a\u8fc7\u591a\u611f\u5b98\u4fe1\u606f\u7684\u7efc\u5408\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u3002"}}
{"id": "2509.25222", "pdf": "https://arxiv.org/pdf/2509.25222", "abs": "https://arxiv.org/abs/2509.25222", "authors": ["Yutong Liang", "Chang Hou", "Guy Y. Cornejo Maceda", "Andrea Ianiro", "Stefano Discetti", "Andrea Meil\u00e1n-Vila", "Didier Sornette", "Sandro Claudio Lera", "Jialong Chen", "Xiaozhou He", "Bernd R. Noack"], "title": "Sensor optimization for urban wind estimation with cluster-based probabilistic framework", "categories": ["cs.LG", "cs.RO", "physics.flu-dyn"], "comment": null, "summary": "We propose a physics-informed machine-learned framework for sensor-based flow\nestimation for drone trajectories in complex urban terrain. The input is a rich\nset of flow simulations at many wind conditions. The outputs are velocity and\nuncertainty estimates for a target domain and subsequent sensor optimization\nfor minimal uncertainty. The framework has three innovations compared to\ntraditional flow estimators. First, the algorithm scales proportionally to the\ndomain complexity, making it suitable for flows that are too complex for any\nmonolithic reduced-order representation. Second, the framework extrapolates\nbeyond the training data, e.g., smaller and larger wind velocities. Last, and\nperhaps most importantly, the sensor location is a free input, significantly\nextending the vast majority of the literature. The key enablers are (1) a\nReynolds number-based scaling of the flow variables, (2) a physics-based domain\ndecomposition, (3) a cluster-based flow representation for each subdomain, (4)\nan information entropy correlating the subdomains, and (5) a multi-variate\nprobability function relating sensor input and targeted velocity estimates.\nThis framework is demonstrated using drone flight paths through a\nthree-building cluster as a simple example. We anticipate adaptations and\napplications for estimating complete cities and incorporating weather input.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u8f68\u8ff9\u7684\u4f20\u611f\u5668\u6d41\u52a8\u4f30\u8ba1\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u6570\u636e\u5916\u63a8\u80fd\u529b\u548c\u4f20\u611f\u5668\u4f4d\u7f6e\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u6d41\u52a8\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u6d41\u52a8\u573a\u666f\uff0c\u4e14\u4f20\u611f\u5668\u4f4d\u7f6e\u56fa\u5b9a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "method": "\u7ed3\u5408\u96f7\u8bfa\u6570\u7f29\u653e\u3001\u7269\u7406\u57df\u5206\u89e3\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u5b50\u57df\u6d41\u52a8\u8868\u793a\u3001\u4fe1\u606f\u71b5\u5173\u8054\u548c\u591a\u53d8\u91cf\u6982\u7387\u51fd\u6570\uff0c\u6784\u5efa\u4f20\u611f\u5668\u8f93\u5165\u4e0e\u76ee\u6807\u901f\u5ea6\u4f30\u8ba1\u7684\u5173\u7cfb\u3002", "result": "\u6846\u67b6\u5728\u65e0\u4eba\u673a\u98de\u884c\u8def\u5f84\u793a\u4f8b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6d41\u52a8\u4f30\u8ba1\uff0c\u5e76\u80fd\u5916\u63a8\u8bad\u7ec3\u6570\u636e\u8303\u56f4\u5916\u7684\u98ce\u901f\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u6846\u67b6\u4e3a\u57ce\u5e02\u6d41\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5b8c\u6574\u57ce\u5e02\u548c\u5929\u6c14\u6570\u636e\u96c6\u6210\u3002"}}
{"id": "2509.25452", "pdf": "https://arxiv.org/pdf/2509.25452", "abs": "https://arxiv.org/abs/2509.25452", "authors": ["Suhala Rabab Saba", "Sakib Khan", "Minhaj Uddin Ahmad", "Jiahe Cao", "Mizanur Rahman", "Li Zhao", "Nathan Huynh", "Eren Erman Ozguven"], "title": "Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Infrastructure-based sensing and real-time trajectory generation show promise\nfor improving safety in high-risk roadway segments such as work zones, yet\npractical deployments are hindered by perspective distortion, complex geometry,\nocclusions, and costs. This study tackles these barriers by integrating\nroadside camera and LiDAR sensors into a cosimulation environment to develop a\nscalable, cost-effective vehicle detection and localization framework, and\nemploying a Kalman Filter-based late fusion strategy to enhance trajectory\nconsistency and accuracy. In simulation, the fusion algorithm reduced\nlongitudinal error by up to 70 percent compared to individual sensors while\npreserving lateral accuracy within 1 to 3 meters. Field validation in an active\nwork zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth,\ndemonstrated that the fused trajectories closely match real vehicle paths, even\nwhen single-sensor data are intermittent or degraded. These results confirm\nthat KF based sensor fusion can reliably compensate for individual sensor\nlimitations, providing precise and robust vehicle tracking capabilities. Our\napproach thus offers a practical pathway to deploy infrastructure-enabled\nmulti-sensor systems for proactive safety measures in complex traffic\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u8def\u8fb9\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u540e\u671f\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f66\u8f86\u68c0\u6d4b\u548c\u8f68\u8ff9\u751f\u6210\u7684\u51c6\u786e\u6027\u4e0e\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u3002", "motivation": "\u89e3\u51b3\u9ad8\u98ce\u9669\u8def\u6bb5\uff08\u5982\u65bd\u5de5\u533a\uff09\u4e2d\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u548c\u5b9e\u65f6\u8f68\u8ff9\u751f\u6210\u9762\u4e34\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5982\u89c6\u89d2\u5931\u771f\u3001\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u3001\u906e\u6321\u548c\u9ad8\u6210\u672c\u3002", "method": "\u4f7f\u7528\u8def\u8fb9\u6444\u50cf\u5934\u548cLiDAR\u4f20\u611f\u5668\u7684\u4eff\u771f\u73af\u5883\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8f66\u8f86\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u540e\u671f\u878d\u5408\u7b56\u7565\u63d0\u5347\u8f68\u8ff9\u4e00\u81f4\u6027\u3002", "result": "\u4eff\u771f\u4e2d\u878d\u5408\u7b97\u6cd5\u5c06\u7eb5\u5411\u8bef\u5dee\u51cf\u5c11\u9ad8\u8fbe70%\uff0c\u540c\u65f6\u4fdd\u6301\u6a2a\u5411\u8bef\u5dee\u57281\u52303\u7c73\u5185\uff1b\u5b9e\u5730\u9a8c\u8bc1\u663e\u793a\u878d\u5408\u8f68\u8ff9\u4e0e\u771f\u5b9e\u8f66\u8f86\u8def\u5f84\u9ad8\u5ea6\u543b\u5408\u3002", "conclusion": "\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u4f20\u611f\u5668\u878d\u5408\u80fd\u53ef\u9760\u8865\u507f\u5355\u4e00\u4f20\u611f\u5668\u7684\u9650\u5236\uff0c\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u5b89\u5168\u63aa\u65bd\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2509.25482", "pdf": "https://arxiv.org/pdf/2509.25482", "abs": "https://arxiv.org/abs/2509.25482", "authors": ["Wouter M. Kouw", "Tim N. Nisslbeck", "Wouter L. N. Nuijten"], "title": "Message passing-based inference in an autoregressive active inference agent", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SY", "eess.SY", "stat.ML"], "comment": "14 pages, 4 figures, to be published in the proceedings of the\n  International Workshop on Active Inference 2025", "summary": "We present the design of an autoregressive active inference agent in the form\nof message passing on a factor graph. Expected free energy is derived and\ndistributed across a planning graph. The proposed agent is validated on a robot\nnavigation task, demonstrating exploration and exploitation in a\ncontinuous-valued observation space with bounded continuous-valued actions.\nCompared to a classical optimal controller, the agent modulates action based on\npredictive uncertainty, arriving later but with a better model of the robot's\ndynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u4e0a\u6d88\u606f\u4f20\u9012\u7684\u81ea\u56de\u5f52\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u4e0e\u5f00\u53d1\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728\u8fde\u7eed\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u57fa\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u884c\u52a8\u8c03\u8282\uff0c\u4f18\u5316\u673a\u5668\u4eba\u52a8\u6001\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u81ea\u56de\u5f52\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u56e0\u5b50\u56fe\u4e0a\u7684\u6d88\u606f\u4f20\u9012\u5b9e\u73b0\u9884\u671f\u81ea\u7531\u80fd\u91cf\u7684\u5206\u5e03\u3002", "result": "\u667a\u80fd\u4f53\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u4f20\u7edf\u6700\u4f18\u63a7\u5236\u5668\uff0c\u867d\u884c\u52a8\u8f83\u6162\u4f46\u52a8\u6001\u6a21\u578b\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u63a2\u7d22\u4e0e\u5f00\u53d1\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5bf9\u52a8\u6001\u5efa\u6a21\u7684\u4efb\u52a1\u3002"}}
{"id": "2509.25518", "pdf": "https://arxiv.org/pdf/2509.25518", "abs": "https://arxiv.org/abs/2509.25518", "authors": ["Harry Robertshaw", "Han-Ru Wu", "Alejandro Granados", "Thomas C Booth"], "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy", "categories": ["cs.LG", "cs.RO", "eess.IV"], "comment": "Published in Medical Image Computing and Computer Assisted\n  Intervention - MICCAI 2025, Lecture Notes in Computer Science, vol 15968", "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical\nchallenge due to the complexity of vascular anatomy and the need for precise,\nreal-time decision-making. Reinforcement learning (RL)-based approaches have\ndemonstrated potential in automating endovascular navigation, but current\nmethods often struggle with generalization across multiple patient vasculatures\nand long-horizon tasks. We propose a world model for autonomous endovascular\nnavigation using TD-MPC2, a model-based RL algorithm. We trained a single RL\nagent across multiple endovascular navigation tasks in ten real patient\nvasculatures, comparing performance against the state-of-the-art Soft\nActor-Critic (SAC) method. Results indicate that TD-MPC2 significantly\noutperforms SAC in multi-task learning, achieving a 65% mean success rate\ncompared to SAC's 37%, with notable improvements in path ratio. TD-MPC2\nexhibited increased procedure times, suggesting a trade-off between success\nrate and execution speed. These findings highlight the potential of world\nmodels for improving autonomous endovascular navigation and lay the foundation\nfor future research in generalizable AI-driven robotic interventions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTD-MPC2\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709SAC\u65b9\u6cd5\u3002", "motivation": "\u81ea\u4e3b\u673a\u68b0\u53d6\u6813\u624b\u672f\u5bfc\u822a\u56e0\u8840\u7ba1\u89e3\u5256\u590d\u6742\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u9700\u6c42\u800c\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528TD-MPC2\u7b97\u6cd5\u8bad\u7ec3\u5355\u4e00RL\u4ee3\u7406\uff0c\u572810\u4f8b\u60a3\u8005\u8840\u7ba1\u4e2d\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "TD-MPC2\u6210\u529f\u738765%\uff0c\u4f18\u4e8eSAC\u768437%\uff0c\u4f46\u624b\u672f\u65f6\u95f4\u66f4\u957f\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u53ef\u63d0\u5347\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u901a\u7528AI\u9a71\u52a8\u673a\u5668\u4eba\u5e72\u9884\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.25520", "pdf": "https://arxiv.org/pdf/2509.25520", "abs": "https://arxiv.org/abs/2509.25520", "authors": ["Tu-Hoa Pham", "Philip Bailey", "Daniel Posada", "Georgios Georgakis", "Jorge Enriquez", "Surya Suresh", "Marco Dolci", "Philip Twu"], "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity", "categories": ["cs.CV", "cs.RO"], "comment": "To appear in IEEE Robotics and Automation Letters", "summary": "We consider the problem of vision-based 6-DoF object pose estimation in the\ncontext of the notional Mars Sample Return campaign, in which a robotic arm\nwould need to localize multiple objects of interest for low-clearance pickup\nand insertion, under severely constrained hardware. We propose a novel\nlocalization algorithm leveraging a custom renderer together with a new\ntemplate matching metric tailored to the edge domain to achieve robust pose\nestimation using only low-fidelity, textureless 3D models as inputs. Extensive\nevaluations on synthetic datasets as well as from physical testbeds on Earth\nand in situ Mars imagery shows that our method consistently beats the state of\nthe art in compute and memory-constrained localization, both in terms of\nrobustness and accuracy, in turn enabling new possibilities for cheap and\nreliable localization on general-purpose hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u76846\u81ea\u7531\u5ea6\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\uff0c\u4e13\u95e8\u4e3a\u706b\u661f\u6837\u672c\u8fd4\u56de\u4efb\u52a1\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u548c\u5185\u5b58\u53d7\u9650\u7684\u786c\u4ef6\u73af\u5883\u3002", "motivation": "\u5728\u706b\u661f\u6837\u672c\u8fd4\u56de\u4efb\u52a1\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5728\u786c\u4ef6\u4e25\u91cd\u53d7\u9650\u7684\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u89c6\u89c9\u5b9a\u4f4d\u591a\u4e2a\u76ee\u6807\u7269\u4f53\u4ee5\u5b9e\u73b0\u4f4e\u95f4\u9699\u6293\u53d6\u548c\u63d2\u5165\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u6e32\u67d3\u5668\u548c\u65b0\u7684\u8fb9\u7f18\u57df\u6a21\u677f\u5339\u914d\u5ea6\u91cf\uff0c\u8f93\u5165\u4ec5\u4e3a\u4f4e\u7cbe\u5ea6\u3001\u65e0\u7eb9\u7406\u76843D\u6a21\u578b\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u3001\u5730\u7403\u7269\u7406\u6d4b\u8bd5\u5e8a\u548c\u706b\u661f\u5b9e\u5730\u56fe\u50cf\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u786c\u4ef6\u4e0a\u7684\u4f4e\u6210\u672c\u3001\u9ad8\u53ef\u9760\u6027\u7269\u4f53\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.25528", "pdf": "https://arxiv.org/pdf/2509.25528", "abs": "https://arxiv.org/abs/2509.25528", "authors": ["Pranav Saxena", "Avigyan Bhattacharya", "Ji Zhang", "Wenshan Wang"], "title": "LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Referential grounding in outdoor driving scenes is challenging due to large\nscene variability, many visually similar objects, and dynamic elements that\ncomplicate resolving natural-language references (e.g., \"the black car on the\nright\"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf\nvision-language models for fine-grained attribute extraction with large\nlanguage models for symbolic reasoning. LLM-RG processes an image and a\nfree-form referring expression by using an LLM to extract relevant object types\nand attributes, detecting candidate regions, generating rich visual descriptors\nwith a VLM, and then combining these descriptors with spatial metadata into\nnatural-language prompts that are input to an LLM for chain-of-thought\nreasoning to identify the referent's bounding box. Evaluated on the Talk2Car\nbenchmark, LLM-RG yields substantial gains over both LLM and VLM-based\nbaselines. Additionally, our ablations show that adding 3D spatial cues further\nimproves grounding. Our results demonstrate the complementary strengths of VLMs\nand LLMs, applied in a zero-shot manner, for robust outdoor referential\ngrounding.", "AI": {"tldr": "LLM-RG\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6237\u5916\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6307\u79f0\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u53d6\u5c5e\u6027\u548c\u7a7a\u95f4\u4fe1\u606f\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u6237\u5916\u9a7e\u9a76\u573a\u666f\u7684\u590d\u6742\u6027\uff08\u5982\u89c6\u89c9\u76f8\u4f3c\u7269\u4f53\u548c\u52a8\u6001\u5143\u7d20\uff09\u4f7f\u5f97\u81ea\u7136\u8bed\u8a00\u6307\u79f0\u5339\u914d\uff08\u5982\u201c\u53f3\u8fb9\u7684\u9ed1\u8272\u6c7d\u8f66\u201d\uff09\u975e\u5e38\u56f0\u96be\u3002", "method": "LLM-RG\u7ed3\u5408VLM\u63d0\u53d6\u7cbe\u7ec6\u5c5e\u6027\u548cLLM\u8fdb\u884c\u7b26\u53f7\u63a8\u7406\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u89c9\u63cf\u8ff0\u7b26\u548c\u7a7a\u95f4\u5143\u6570\u636e\uff0c\u8fdb\u884c\u94fe\u5f0f\u63a8\u7406\u5339\u914d\u76ee\u6807\u8fb9\u754c\u6846\u3002", "result": "\u5728Talk2Car\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-RG\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u548cVLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c3D\u7a7a\u95f4\u7ebf\u7d22\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "LLM-RG\u5c55\u793a\u4e86VLM\u548cLLM\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u53ef\u5b9e\u73b0\u9c81\u68d2\u7684\u6237\u5916\u6307\u79f0\u5339\u914d\u3002"}}
{"id": "2509.25575", "pdf": "https://arxiv.org/pdf/2509.25575", "abs": "https://arxiv.org/abs/2509.25575", "authors": ["Velimir Todorovski", "Kwang Hak Kim", "Miroslav Krstic"], "title": "Modular Design of Strict Control Lyapunov Functions for Global Stabilization of the Unicycle in Polar Coordinates", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "Since the mid-1990s, it has been known that, unlike in Cartesian form where\nBrockett's condition rules out static feedback stabilization, the unicycle is\nglobally asymptotically stabilizable by smooth feedback in polar coordinates.\nIn this note, we introduce a modular framework for designing smooth feedback\nlaws that achieve global asymptotic stabilization in polar coordinates. These\nlaws are bidirectional, enabling efficient parking maneuvers, and are paired\nwith families of strict control Lyapunov functions (CLFs) constructed in a\nmodular fashion. The resulting CLFs guarantee global asymptotic stability with\nexplicit convergence rates and include barrier variants that yield \"almost\nglobal\" stabilization, excluding only zero-measure subsets of the rotation\nmanifolds. The strictness of the CLFs is further leveraged in our companion\npaper, where we develop inverse-optimal redesigns with meaningful cost\nfunctions and infinite gain margins.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u53cc\u5411\u5e73\u6ed1\u53cd\u9988\u5f8b\uff0c\u5b9e\u73b0\u6781\u5750\u6807\u4e0b\u7684\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\uff0c\u5e76\u7ed3\u5408\u4e25\u683c\u63a7\u5236Lyapunov\u51fd\u6570\uff08CLF\uff09\u4fdd\u8bc1\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u6781\u5750\u6807\u4e0b\u53ef\u5b9e\u73b0\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\uff0c\u4f46\u7f3a\u4e4f\u6a21\u5757\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u6784\u5efa\u53cc\u5411\u5e73\u6ed1\u53cd\u9988\u5f8b\u548c\u4e25\u683cCLF\uff0c\u652f\u6301\u9ad8\u6548\u505c\u8f66\u64cd\u4f5c\u548c\u5168\u5c40\u7a33\u5b9a\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u53cd\u9988\u5f8b\u7684\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u4f9b\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u540c\u65f6\u901a\u8fc7CLF\u53d8\u4f53\u5b9e\u73b0\u201c\u51e0\u4e4e\u5168\u5c40\u201d\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6781\u5750\u6807\u4e0b\u7684\u7a33\u5b9a\u63a7\u5236\u6027\u80fd\uff0c\u8fd8\u4e3a\u540e\u7eed\u4f18\u5316\u8bbe\u8ba1\uff08\u5982\u6210\u672c\u51fd\u6570\u548c\u65e0\u9650\u589e\u76ca\u88d5\u5ea6\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.25579", "pdf": "https://arxiv.org/pdf/2509.25579", "abs": "https://arxiv.org/abs/2509.25579", "authors": ["Miroslav Krstic", "Velimir Todorovski", "Kwang Hak Kim", "Alessandro Astolfi"], "title": "Integrator Forwading Design for Unicycles with Constant and Actuated Velocity in Polar Coordinates", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "In a companion paper, we present a modular framework for unicycle\nstabilization in polar coordinates that provides smooth steering laws through\nbackstepping. Surprisingly, the same problem also allows the application of\nintegrator forwarding. In this work, we leverage this feature and construct new\nsmooth steering laws together with control Lyapunov functions (CLFs), expanding\nthe set of CLFs available for inverse optimal control design. In the case of\nconstant forward velocity (Dubins car), backstepping produces finite-time\n(deadbeat) parking, and we show that integrator forwarding yields the very same\nclass of solutions. This reveals a fundamental connection between backstepping\nand forwarding in addressing both the unicycle and, the Dubins car parking\nproblems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u5728\u6781\u5750\u6807\u4e0b\u901a\u8fc7\u53cd\u5411\u6b65\u8fdb\u548c\u79ef\u5206\u524d\u9988\u6280\u672f\u8bbe\u8ba1\u5e73\u6ed1\u8f6c\u5411\u5f8b\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u63a7\u5236Lyapunov\u51fd\u6570\u7684\u96c6\u5408\uff0c\u63ed\u793a\u4e86\u8fd9\u4e24\u79cd\u6280\u672f\u5728\u89e3\u51b3\u72ec\u8f6e\u8f66\u548cDubins\u8f66\u505c\u8f66\u95ee\u9898\u65f6\u7684\u6df1\u5c42\u8054\u7cfb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53cd\u5411\u6b65\u8fdb\u548c\u79ef\u5206\u524d\u9988\u6280\u672f\uff0c\u4e3a\u6781\u5750\u6807\u4e0b\u7684\u72ec\u8f6e\u8f66\u7a33\u5b9a\u95ee\u9898\u8bbe\u8ba1\u65b0\u7684\u5e73\u6ed1\u8f6c\u5411\u5f8b\uff0c\u5e76\u6269\u5c55\u63a7\u5236Lyapunov\u51fd\u6570\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5229\u7528\u53cd\u5411\u6b65\u8fdb\u548c\u79ef\u5206\u524d\u9988\u6280\u672f\uff0c\u6784\u5efa\u5e73\u6ed1\u8f6c\u5411\u5f8b\u548c\u63a7\u5236Lyapunov\u51fd\u6570\u3002", "result": "\u5728Dubins\u8f66\u7684\u60c5\u51b5\u4e0b\uff0c\u53cd\u5411\u6b65\u8fdb\u5b9e\u73b0\u4e86\u6709\u9650\u65f6\u95f4\u505c\u8f66\uff0c\u79ef\u5206\u524d\u9988\u4e5f\u80fd\u4ea7\u751f\u76f8\u540c\u7684\u89e3\u7c7b\uff0c\u63ed\u793a\u4e86\u4e24\u79cd\u6280\u672f\u7684\u6df1\u5c42\u8054\u7cfb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u53cd\u5411\u6b65\u8fdb\u548c\u79ef\u5206\u524d\u9988\u6280\u672f\u5728\u89e3\u51b3\u72ec\u8f6e\u8f66\u548cDubins\u8f66\u505c\u8f66\u95ee\u9898\u65f6\u5177\u6709\u672c\u8d28\u8054\u7cfb\uff0c\u4e3a\u6700\u4f18\u63a7\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u591a\u53ef\u80fd\u3002"}}
{"id": "2509.25600", "pdf": "https://arxiv.org/pdf/2509.25600", "abs": "https://arxiv.org/abs/2509.25600", "authors": ["Wontaek Kim", "Tianyu Li", "Sehoon Ha"], "title": "MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Motion retargeting holds a premise of offering a larger set of motion data\nfor characters and robots with different morphologies. Many prior works have\napproached this problem via either handcrafted constraints or paired motion\ndatasets, limiting their applicability to humanoid characters or narrow\nbehaviors such as locomotion. Moreover, they often assume a fixed notion of\nretargeting, overlooking domain-specific objectives like style preservation in\nanimation or task-space alignment in robotics. In this work, we propose\nMoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that\nlearns correspondences between characters' motion embedding spaces. Our method\nconsists of two stages. First, we train tokenized motion embeddings for each\ncharacter using a VQ-VAE, yielding compact latent representations. Then, we\nemploy flow matching with conditional coupling to align the latent spaces\nacross characters, which simultaneously learns conditioned and unconditioned\nmatching to achieve robust but flexible retargeting. Once trained, MoReFlow\nenables flexible and reversible retargeting without requiring paired data.\nExperiments demonstrate that MoReFlow produces high-quality motions across\ndiverse characters and tasks, offering improved controllability,\ngeneralization, and motion realism compared to the baselines.", "AI": {"tldr": "MoReFlow\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u52a8\u4f5c\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u5b66\u4e60\u4e0d\u540c\u89d2\u8272\u52a8\u4f5c\u5d4c\u5165\u7a7a\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u4f5c\u91cd\u5b9a\u5411\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u91cd\u5b9a\u5411\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7ea6\u675f\u6216\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u9002\u7528\u8303\u56f4\u6709\u9650\u4e14\u5ffd\u89c6\u4e86\u7279\u5b9a\u9886\u57df\u76ee\u6807\uff08\u5982\u52a8\u753b\u98ce\u683c\u4fdd\u7559\u6216\u673a\u5668\u4eba\u4efb\u52a1\u7a7a\u95f4\u5bf9\u9f50\uff09\u3002", "method": "MoReFlow\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09\u4f7f\u7528VQ-VAE\u8bad\u7ec3\u6bcf\u4e2a\u89d2\u8272\u7684\u6807\u8bb0\u5316\u52a8\u4f5c\u5d4c\u5165\uff1b2\uff09\u901a\u8fc7\u6761\u4ef6\u8026\u5408\u7684\u6d41\u5339\u914d\u5bf9\u9f50\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoReFlow\u5728\u591a\u6837\u5316\u89d2\u8272\u548c\u4efb\u52a1\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u4f5c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u63a7\u6027\u3001\u6cdb\u5316\u6027\u548c\u52a8\u4f5c\u771f\u5b9e\u611f\u3002", "conclusion": "MoReFlow\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u9006\u7684\u52a8\u4f5c\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.25727", "pdf": "https://arxiv.org/pdf/2509.25727", "abs": "https://arxiv.org/abs/2509.25727", "authors": ["Huikang Su", "Dengyun Peng", "Zifeng Zhuang", "YuHan Liu", "Qiguang Chen", "Donglin Wang", "Qinghe Liu"], "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025", "summary": "Offline safe reinforcement learning aims to learn policies that satisfy\npredefined safety constraints from static datasets. Existing\nsequence-model-based methods condition action generation on symmetric input\ntokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:\nreturn-to-go (RTG) serves as a flexible performance target, while cost-to-go\n(CTG) should represent a rigid safety boundary. This symmetric conditioning\nleads to unreliable constraint satisfaction, especially when encountering\nout-of-distribution cost trajectories. To address this, we propose\nBoundary-to-Region (B2R), a framework that enables asymmetric conditioning\nthrough cost signal realignment . B2R redefines CTG as a boundary constraint\nunder a fixed safety budget, unifying the cost distribution of all feasible\ntrajectories while preserving reward structures. Combined with rotary\npositional embeddings , it enhances exploration within the safe region.\nExperimental results show that B2R satisfies safety constraints in 35 out of 38\nsafety-critical tasks while achieving superior reward performance over baseline\nmethods. This work highlights the limitations of symmetric token conditioning\nand establishes a new theoretical and practical approach for applying sequence\nmodels to safe RL. Our code is available at https://github.com/HuikangSu/B2R.", "AI": {"tldr": "B2R\u6846\u67b6\u901a\u8fc7\u4e0d\u5bf9\u79f0\u6761\u4ef6\u5316\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5b89\u5168\u7ea6\u675f\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u8fd4\u56de\u76ee\u6807\uff08RTG\uff09\u548c\u6210\u672c\u76ee\u6807\uff08CTG\uff09\u4f7f\u7528\u5bf9\u79f0\u8f93\u5165\u6807\u8bb0\uff0c\u5ffd\u7565\u4e86\u5176\u672c\u8d28\u4e0d\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u7ea6\u675f\u6ee1\u8db3\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faBoundary-to-Region\uff08B2R\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u672c\u4fe1\u53f7\u91cd\u5bf9\u9f50\u5b9e\u73b0\u4e0d\u5bf9\u79f0\u6761\u4ef6\u5316\uff0c\u5e76\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u589e\u5f3a\u5b89\u5168\u533a\u57df\u63a2\u7d22\u3002", "result": "B2R\u572838\u4e2a\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u6210\u529f\u6ee1\u8db3\u4e8635\u4e2a\u4efb\u52a1\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u5956\u52b1\u6027\u80fd\u3002", "conclusion": "B2R\u63ed\u793a\u4e86\u5bf9\u79f0\u6807\u8bb0\u6761\u4ef6\u5316\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5e8f\u5217\u6a21\u578b\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.25929", "pdf": "https://arxiv.org/pdf/2509.25929", "abs": "https://arxiv.org/abs/2509.25929", "authors": ["Yuan Li", "Xiaoxue Xu", "Xiang Dong", "Junfeng Hao", "Tao Li", "Sana Ullaha", "Chuangrui Huang", "Junjie Niu", "Ziyan Zhao", "Ting Peng"], "title": "Preemptive Spatiotemporal Trajectory Adjustment for Heterogeneous Vehicles in Highway Merging Zones", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Aiming at the problem of driver's perception lag and low utilization\nefficiency of space-time resources in expressway ramp confluence area, based on\nthe preemptive spatiotemporal trajectory Adjustment system, from the\nperspective of coordinating spatiotemporal resources, the reasonable value of\nsafe space-time distance in trajectory pre-preparation is quantitatively\nanalyzed. The minimum safety gap required for ramp vehicles to merge into the\nmainline is analyzed by introducing double positioning error and spatiotemporal\ntrajectory tracking error. A merging control strategy for autonomous driving\nheterogeneous vehicles is proposed, which integrates vehicle type, driving\nintention, and safety spatiotemporal distance. The specific confluence\nstrategies of ramp target vehicles and mainline cooperative vehicles under\ndifferent vehicle types are systematically expounded. A variety of traffic flow\nand speed scenarios are used for full combination simulation. By comparing the\ntime-position-speed diagram, the vehicle operation characteristics and the\ndynamic difference of confluence are qualitatively analyzed, and the average\nspeed and average delay are used as the evaluation indices to quantitatively\nevaluate the performance advantages of the preemptive cooperative confluence\ncontrol strategy. The results show that the maximum average delay improvement\nrates of mainline and ramp vehicles are 90.24 % and 74.24 %, respectively. The\nproposed strategy can effectively avoid potential vehicle conflicts and\nemergency braking behaviors, improve driving safety in the confluence area, and\nshow significant advantages in driving stability and overall traffic efficiency\noptimization.", "AI": {"tldr": "\u9488\u5bf9\u9ad8\u901f\u516c\u8def\u531d\u9053\u6c47\u5408\u533a\u9a7e\u9a76\u5458\u611f\u77e5\u6ede\u540e\u548c\u65f6\u7a7a\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62a2\u5360\u5f0f\u65f6\u7a7a\u8f68\u8ff9\u8c03\u6574\u7cfb\u7edf\u7684\u81ea\u4e3b\u9a7e\u9a76\u5f02\u6784\u8f66\u8f86\u6c47\u5408\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c47\u5408\u533a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u9ad8\u901f\u516c\u8def\u531d\u9053\u6c47\u5408\u533a\u5b58\u5728\u7684\u9a7e\u9a76\u5458\u611f\u77e5\u6ede\u540e\u548c\u65f6\u7a7a\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4f18\u5316\u8f66\u8f86\u6c47\u5408\u7684\u5b89\u5168\u6027\u548c\u6574\u4f53\u4ea4\u901a\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u53cc\u91cd\u5b9a\u4f4d\u8bef\u5dee\u548c\u65f6\u7a7a\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5206\u6790\u6700\u5c0f\u5b89\u5168\u95f4\u8ddd\uff0c\u5e76\u63d0\u51fa\u6574\u5408\u8f66\u8f86\u7c7b\u578b\u3001\u9a7e\u9a76\u610f\u56fe\u548c\u5b89\u5168\u65f6\u7a7a\u8ddd\u79bb\u7684\u81ea\u4e3b\u9a7e\u9a76\u5f02\u6784\u8f66\u8f86\u6c47\u5408\u63a7\u5236\u7b56\u7565\u3002\u901a\u8fc7\u591a\u79cd\u4ea4\u901a\u6d41\u548c\u901f\u5ea6\u573a\u666f\u7684\u4eff\u771f\u9a8c\u8bc1\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e3b\u7ebf\u8f66\u8f86\u548c\u531d\u9053\u8f66\u8f86\u7684\u6700\u5927\u5e73\u5747\u5ef6\u8bef\u6539\u5584\u7387\u5206\u522b\u4e3a90.24%\u548c74.24%\uff0c\u7b56\u7565\u6709\u6548\u907f\u514d\u4e86\u6f5c\u5728\u8f66\u8f86\u51b2\u7a81\u548c\u7d27\u6025\u5236\u52a8\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u62a2\u5360\u5f0f\u534f\u540c\u6c47\u5408\u63a7\u5236\u7b56\u7565\u5728\u9a7e\u9a76\u7a33\u5b9a\u6027\u548c\u6574\u4f53\u4ea4\u901a\u6548\u7387\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c47\u5408\u533a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.26002", "pdf": "https://arxiv.org/pdf/2509.26002", "abs": "https://arxiv.org/abs/2509.26002", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Towards Human Engagement with Realistic AI Combat Pilots", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA", "cs.RO"], "comment": "13th International Conference on Human-Agent Interaction (HAI) 2025", "summary": "We present a system that enables real-time interaction between human users\nand agents trained to control fighter jets in simulated 3D air combat\nscenarios. The agents are trained in a dedicated environment using Multi-Agent\nReinforcement Learning. A communication link is developed to allow seamless\ndeployment of trained agents into VR-Forces, a widely used defense simulation\ntool for realistic tactical scenarios. This integration allows mixed\nsimulations where human-controlled entities engage with intelligent agents\nexhibiting distinct combat behaviors. Our interaction model creates new\nopportunities for human-agent teaming, immersive training, and the exploration\nof innovative tactics in defense contexts.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u4f7f\u4eba\u7c7b\u7528\u6237\u4e0e\u7ecf\u8fc7\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u5728\u6a21\u62df3D\u7a7a\u6218\u573a\u666f\u4e2d\u5b9e\u65f6\u4ea4\u4e92\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u5c06\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u65e0\u7f1d\u90e8\u7f72\u5230VR-Forces\u4eff\u771f\u5de5\u5177\u4e2d\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u5728\u9632\u5fa1\u573a\u666f\u4e2d\u7684\u534f\u4f5c\uff0c\u63d0\u4f9b\u6c89\u6d78\u5f0f\u8bad\u7ec3\u548c\u6218\u672f\u521b\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4e13\u7528\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5e76\u901a\u8fc7\u901a\u4fe1\u94fe\u8def\u5c06\u5176\u96c6\u6210\u5230VR-Forces\u4eff\u771f\u5de5\u5177\u4e2d\u3002", "result": "\u5b9e\u73b0\u4e86\u4eba\u7c7b\u63a7\u5236\u5b9e\u4f53\u4e0e\u5177\u6709\u4e0d\u540c\u6218\u6597\u884c\u4e3a\u7684\u667a\u80fd\u4f53\u7684\u6df7\u5408\u4eff\u771f\uff0c\u6269\u5c55\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u9632\u5fa1\u9886\u57df\u7684\u4eba\u673a\u534f\u4f5c\u3001\u8bad\u7ec3\u548c\u6218\u672f\u521b\u65b0\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2509.26536", "pdf": "https://arxiv.org/pdf/2509.26536", "abs": "https://arxiv.org/abs/2509.26536", "authors": ["Yida Xue", "Mingjun Mao", "Xiangyuan Ru", "Yuqi Zhu", "Baochang Ren", "Shuofei Qiao", "Mengru Wang", "Shumin Deng", "Xinyu An", "Ningyu Zhang", "Ying Chen", "Huajun Chen"], "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Work in progress", "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.", "AI": {"tldr": "OceanGym\u662f\u9996\u4e2a\u9488\u5bf9\u6c34\u4e0bAI\u667a\u80fd\u4f53\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u65e8\u5728\u63a8\u52a8AI\u5728\u6781\u7aef\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u4f4e\u53ef\u89c1\u6027\u548c\u52a8\u6001\u6d0b\u6d41\u5bf9AI\u667a\u80fd\u4f53\u7684\u611f\u77e5\u548c\u51b3\u7b56\u63d0\u51fa\u4e86\u6781\u9ad8\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u516b\u9879\u4efb\u52a1\u7684OceanGym\u5e73\u53f0\uff0c\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9a71\u52a8\u667a\u80fd\u4f53\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dMLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u3001\u89c4\u5212\u548c\u9002\u5e94\u6027\u65b9\u9762\u3002", "conclusion": "OceanGym\u4e3a\u5f00\u53d1\u6297\u5e72\u6270\u6c34\u4e0bAI\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u662f\u63a2\u7d22\u6c34\u4e0b\u524d\u6cbf\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2509.26575", "pdf": "https://arxiv.org/pdf/2509.26575", "abs": "https://arxiv.org/abs/2509.26575", "authors": ["Kevin Tracy", "John Z. Zhang", "Jon Arrizabalaga", "Stefan Schaal", "Yuval Tassa", "Tom Erez", "Zachary Manchester"], "title": "The Trajectory Bundle Method: Unifying Sequential-Convex Programming and Sampling-Based Trajectory Optimization", "categories": ["math.OC", "cs.RO"], "comment": null, "summary": "We present a unified framework for solving trajectory optimization problems\nin a derivative-free manner through the use of sequential convex programming.\nTraditionally, nonconvex optimization problems are solved by forming and\nsolving a sequence of convex optimization problems, where the cost and\nconstraint functions are approximated locally through Taylor series expansions.\nThis presents a challenge for functions where differentiation is expensive or\nunavailable. In this work, we present a derivative-free approach to form these\nconvex approximations by computing samples of the dynamics, cost, and\nconstraint functions and letting the solver interpolate between them. Our\nframework includes sample-based trajectory optimization techniques like\nmodel-predictive path integral (MPPI) control as a special case and generalizes\nthem to enable features like multiple shooting and general equality and\ninequality constraints that are traditionally associated with derivative-based\nsequential convex programming methods. The resulting framework is simple,\nflexible, and capable of solving a wide variety of practical motion planning\nand control problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u5bfc\u6570\u65b9\u6cd5\u3001\u5e8f\u5217\u51f8\u89c4\u5212\u7684\u8f68\u8ff9\u4f18\u5316\u7edf\u4e00\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u52a8\u529b\u5b66\u96be\u4ee5\u6c42\u5bfc\u7684\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6cf0\u52d2\u5c55\u5f00\u65b9\u6cd5\u5728\u52a8\u529b\u5b66\u96be\u4ee5\u6c42\u5bfc\u6216\u4ee3\u4ef7\u9ad8\u6602\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u91c7\u6837\u52a8\u529b\u5b66\u3001\u6210\u672c\u548c\u7ea6\u675f\u51fd\u6570\uff0c\u5e76\u8ba9\u6c42\u89e3\u5668\u63d2\u503c\u903c\u8fd1\uff0c\u5b9e\u73b0\u65e0\u5bfc\u6570\u51f8\u4f18\u5316\u3002", "result": "\u6846\u67b6\u7075\u6d3b\uff0c\u652f\u6301\u591a\u9636\u6bb5\u5c04\u51fb\u548c\u901a\u7528\u7ea6\u675f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u65e0\u5bfc\u6570\u4f18\u5316\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2509.26627", "pdf": "https://arxiv.org/pdf/2509.26627", "abs": "https://arxiv.org/abs/2509.26627", "authors": ["Yuyang Liu", "Chuan Wen", "Yihang Hu", "Dinesh Jayaraman", "Yang Gao"], "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.", "AI": {"tldr": "TimeRewarder\u662f\u4e00\u79cd\u901a\u8fc7\u89c6\u9891\u5b66\u4e60\u4efb\u52a1\u8fdb\u5c55\u4fe1\u53f7\u7684\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u901a\u5e38\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u4efb\u52a1\u662f\u63a2\u7d22\u5982\u4f55\u5229\u7528\u4efb\u52a1\u8fdb\u5c55\u4f5c\u4e3a\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u3002", "method": "TimeRewarder\u901a\u8fc7\u5efa\u6a21\u89c6\u9891\u5e27\u95f4\u7684\u65f6\u95f4\u8ddd\u79bb\uff0c\u4ece\u88ab\u52a8\u89c6\u9891\uff08\u5982\u673a\u5668\u4eba\u6f14\u793a\u548c\u4eba\u7c7b\u89c6\u9891\uff09\u4e2d\u5b66\u4e60\u8fdb\u5c55\u4fe1\u53f7\uff0c\u5e76\u63d0\u4f9b\u9010\u6b65\u5956\u52b1\u6307\u5bfcRL\u3002", "result": "\u5728\u5341\u4e2aMeta-World\u4efb\u52a1\u4e2d\uff0cTimeRewarder\u8868\u73b0\u51fa\u8272\uff0c9/10\u4efb\u52a1\u63a5\u8fd1\u5b8c\u7f8e\u6210\u529f\uff0c\u4e14\u6837\u672c\u6548\u7387\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u3002", "conclusion": "TimeRewarder\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u591a\u6837\u89c6\u9891\u6e90\u4e2d\u83b7\u5f97\u4e30\u5bcc\u7684\u5956\u52b1\u4fe1\u53f7\u3002"}}
{"id": "2509.26639", "pdf": "https://arxiv.org/pdf/2509.26639", "abs": "https://arxiv.org/abs/2509.26639", "authors": ["Anusha Krishnan", "Shaohui Liu", "Paul-Edouard Sarlin", "Oscar Gentilhomme", "David Caruso", "Maurizio Monge", "Richard Newcombe", "Jakob Engel", "Marc Pollefeys"], "title": "Benchmarking Egocentric Visual-Inertial SLAM at City Scale", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025", "summary": "Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard\nsensors is critical for wearable devices capturing egocentric data, which\nexhibits specific challenges, such as a wider diversity of motions and\nviewpoints, prevalent dynamic visual content, or long sessions affected by\ntime-varying sensor calibration. While recent progress on SLAM has been swift,\nacademic research is still driven by benchmarks that do not reflect these\nchallenges or do not offer sufficiently accurate ground truth poses. In this\npaper, we introduce a new dataset and benchmark for visual-inertial SLAM with\negocentric, multi-modal data. We record hours and kilometers of trajectories\nthrough a city center with glasses-like devices equipped with various sensors.\nWe leverage surveying tools to obtain control points as indirect pose\nannotations that are metric, centimeter-accurate, and available at city scale.\nThis makes it possible to evaluate extreme trajectories that involve walking at\nnight or traveling in a vehicle. We show that state-of-the-art systems\ndeveloped by academia are not robust to these challenges and we identify\ncomponents that are responsible for this. In addition, we design tracks with\ndifferent levels of difficulty to ease in-depth analysis and evaluation of less\nmature approaches. The dataset and benchmark are available at\nhttps://www.lamaria.ethz.ch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8bc4\u6d4b\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u60ef\u6027SLAM\u5728\u7a7f\u6234\u5f0f\u8bbe\u5907\u4e0a\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u8bc4\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dSLAM\u7814\u7a76\u7f3a\u4e4f\u80fd\u53cd\u6620\u7a7f\u6234\u5f0f\u8bbe\u5907\u7279\u6709\u6311\u6218\u7684\u6570\u636e\u96c6\uff0c\u5982\u591a\u6837\u8fd0\u52a8\u3001\u52a8\u6001\u5185\u5bb9\u3001\u957f\u65f6\u6821\u51c6\u53d8\u5316\u7b49\u3002", "method": "\u901a\u8fc7\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u773c\u955c\u8bbe\u5907\u5728\u57ce\u5e02\u4e2d\u5fc3\u8bb0\u5f55\u6570\u636e\uff0c\u5229\u7528\u6d4b\u91cf\u5de5\u5177\u83b7\u53d6\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u95f4\u63a5\u4f4d\u59ff\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709SLAM\u7cfb\u7edf\u5728\u9762\u5bf9\u8fd9\u4e9b\u6311\u6218\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u8bc6\u522b\u4e86\u5bfc\u81f4\u95ee\u9898\u7684\u7ec4\u4ef6\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u4e3aSLAM\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u652f\u6301\u5bf9\u4e0d\u6210\u719f\u65b9\u6cd5\u7684\u6df1\u5165\u5206\u6790\u3002"}}
