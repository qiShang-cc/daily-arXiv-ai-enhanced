{"id": "2510.14166", "pdf": "https://arxiv.org/pdf/2510.14166", "abs": "https://arxiv.org/abs/2510.14166", "authors": ["Yanqing Xu", "Jingjing Cui", "Yongxu Zhu", "Zhiguo Ding", "Tsung-Hui Chang", "Robert Schober", "Vincent W. S. Wong", "Octavia A. Dobre", "George K. Karagiannidis", "H. Vincent Poor", "Xiaohu You"], "title": "Generalized Pinching-Antenna Systems: A Tutorial on Principles, Design Strategies, and Future Directions", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "31 pages, 13 figures", "summary": "Pinching-antenna systems have emerged as a novel and transformative\nflexible-antenna architecture for next-generation wireless networks. They offer\nunprecedented flexibility and spatial reconfigurability by enabling dynamic\npositioning and activation of radiating elements along a signal-guiding medium\n(e.g., dielectric waveguides), which is not possible with conventional fixed\nantenna systems. In this paper, we introduce the concept of generalized\npinching antenna systems, which retain the core principle of creating localized\nradiation points on demand, but can be physically realized in a variety of\nsettings. These include implementations based on dielectric waveguides, leaky\ncoaxial cables, surface-wave guiding structures, and other types of media,\nemploying different feeding methods and activation mechanisms (e.g.,\nmechanical, electronic, or hybrid). Despite differences in their physical\nrealizations, they all share the same inherent ability to form, reposition, or\ndeactivate radiation sites as needed, enabling user-centric and dynamic\ncoverage. We first describe the underlying physical mechanisms of\nrepresentative generalized pinching-antenna realizations and their associated\nwireless channel models, highlighting their unique propagation and\nreconfigurability characteristics compared with conventional antennas. Then, we\nreview several representative pinching-antenna system architectures, ranging\nfrom single- to multiple-waveguide configurations, and discuss advanced design\nstrategies tailored to these flexible deployments. Furthermore, we examine\ntheir integration with emerging wireless technologies to enable synergistic,\nuser-centric solutions. Finally, we identify key open research challenges and\noutline future directions, charting a pathway toward the practical deployment\nof generalized pinching antennas in next-generation wireless networks."}
{"id": "2510.14000", "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "categories": ["cs.RO"], "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios."}
{"id": "2510.14281", "pdf": "https://arxiv.org/pdf/2510.14281", "abs": "https://arxiv.org/abs/2510.14281", "authors": ["Junyuan Gao", "Weifeng Zhu", "Shuowen Zhang", "Yongpeng Wu", "Jiannong Cao", "Giuseppe Caire", "Liang Liu"], "title": "Integrated Massive Communication and Target Localization in 6G Cell-Free Networks", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "submitted to IEEE TWC", "summary": "This paper presents an initial investigation into the combination of\nintegrated sensing and communication (ISAC) and massive communication, both of\nwhich are largely regarded as key scenarios in sixth-generation (6G) wireless\nnetworks. Specifically, we consider a cell-free network comprising a large\nnumber of users, multiple targets, and distributed base stations (BSs). In each\ntime slot, a random subset of users becomes active, transmitting pilot signals\nthat can be scattered by the targets before reaching the BSs. Unlike\nconventional massive random access schemes, where the primary objectives are\ndevice activity detection and channel estimation, our framework also enables\ntarget localization by leveraging the multipath propagation effects introduced\nby the targets. However, due to the intricate dependency between user channels\nand target locations, characterizing the posterior distribution required for\nminimum mean-square error (MMSE) estimation presents significant computational\nchallenges. To handle this problem, we propose a hybrid message passing-based\nframework that incorporates multiple approximations to mitigate computational\ncomplexity. Numerical results demonstrate that the proposed approach achieves\nhigh-accuracy device activity detection, channel estimation, and target\nlocalization simultaneously, validating the feasibility of embedding\nlocalization functionality into massive communication systems for future 6G\nnetworks."}
{"id": "2510.14018", "pdf": "https://arxiv.org/pdf/2510.14018", "abs": "https://arxiv.org/abs/2510.14018", "authors": ["Adam Morris", "Timothy Pelham", "Edmund R. Hunt"], "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance."}
{"id": "2510.14358", "pdf": "https://arxiv.org/pdf/2510.14358", "abs": "https://arxiv.org/abs/2510.14358", "authors": ["Yuanhao Cui", "Jiali Nie", "Fan Liu", "Weijie Yuan", "Zhiyong Feng", "Xiaojun Jing", "Yulin Liu", "Jie Xu", "Christos Masouros", "Shuguang Cui"], "title": "Integrated Sensing and Communication: Towards Multifunctional Perceptive Network", "categories": ["eess.SP"], "comment": null, "summary": "The capacity-maximization design philosophy has driven the growth of wireless\nnetworks for decades. However, with the slowdown in recent data traffic demand,\nthe mobile industry can no longer rely solely on communication services to\nsustain development. In response, Integrated Sensing and Communications (ISAC)\nhas emerged as a transformative solution, embedding sensing capabilities into\ncommunication networks to enable multifunctional wireless systems. This\nparadigm shift expands the role of networks from sole data transmission to\nversatile platforms supporting diverse applications. In this review, we provide\na bird's-eye view of ISAC for new researchers, highlighting key challenges,\nopportunities, and application scenarios to guide future exploration in this\nfield."}
{"id": "2510.14063", "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "categories": ["cs.RO"], "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/."}
{"id": "2510.14507", "pdf": "https://arxiv.org/pdf/2510.14507", "abs": "https://arxiv.org/abs/2510.14507", "authors": ["Qin Yi", "Zeping Sui", "Zilong Liu"], "title": "Error Rate Analysis and Low-Complexity Receiver Design for Zero-Padded AFDM", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "5 pages, 7 figures, submitted to IEEE TVT", "summary": "This paper studies the error rate performance and low-complexity receiver\ndesign for zero-padded affine frequency division multiplexing (ZP-AFDM)\nsystems. By exploiting the unique ZP-aided lower triangular structure of the\ntime domain (TD) channel matrix, we propose {a novel low-complexity} minimum\nmean square error (MMSE) detector and {a} maximum ratio combining-based TD\n(MRC-TD) detector. Furthermore, the theoretical bit error rate (BER)\n{performance} of both MMSE and maximum likelihood detectors {is} analyzed.\nSimulation results demonstrate {that} the proposed detectors can achieve\nidentical BER performance to that of {the conventional MMSE detector based on\nmatrix inversion} while {enjoying significantly reduced complexity.}"}
{"id": "2510.14065", "pdf": "https://arxiv.org/pdf/2510.14065", "abs": "https://arxiv.org/abs/2510.14065", "authors": ["Gaoyuan Liu", "Joris de Winter", "Yuri Durodie", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning", "categories": ["cs.RO"], "comment": null, "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods."}
{"id": "2510.14530", "pdf": "https://arxiv.org/pdf/2510.14530", "abs": "https://arxiv.org/abs/2510.14530", "authors": ["Jiangong Chen", "Xia Lei", "Yuchen Zhang", "Kaitao Meng", "Christos Masouros"], "title": "Integrated Sensing and Communication with Tri-Hybrid Beamforming Across Electromagnetically Reconfigurable Antennas", "categories": ["eess.SP"], "comment": null, "summary": "Beamforming with a sufficient number of antennas is one of the most\nsignificant technologies for both Multi-user (MU) Multiple-input\nMultiple-output (MIMO) communication and MIMO radar sensing in Integrated\nSensing and Communication (ISAC) systems. However, its performance suffers from\nlimited Degrees of Freedom (DoFs) in conventional hybrid beamforming systems.\nTo overcome this, we propose an Electromagnetically Reconfigurable Antenna\n(ERA)-aided ISAC system, where transmit ERAs dynamically adjust their radiation\npatterns to enhance system DoFs and improve overall performance. Specifically,\nwe design a tri-hybrid beamforming optimization framework combining digital,\nanalog, and Electromagnetic (EM) beamforming to jointly maximize communication\nrate and sensing Signal-to-Clutter-plus-Noise Ratio (SCNR). Furthermore, an\nintegrated Fractional Programming (FP) and Manifold Optimization (MO) approach\nis developed to transform the problem into tractable subproblems with\nclosed-form updates. Simulation results verify that the proposed ERA-ISAC\nsystem achieves almost 10 dB Sensing and Communication (S&C) performance gain\ncompared to its conventional hybrid beamforming counterparts with\nOmnidirectional Antenna (OA)."}
{"id": "2510.14072", "pdf": "https://arxiv.org/pdf/2510.14072", "abs": "https://arxiv.org/abs/2510.14072", "authors": ["Hemjyoti Das", "Christian Ott"], "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load", "categories": ["cs.RO"], "comment": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach."}
{"id": "2510.14604", "pdf": "https://arxiv.org/pdf/2510.14604", "abs": "https://arxiv.org/abs/2510.14604", "authors": ["Thomas Feuillen", "Amirafshar Moshtaghpour"], "title": "Proceedings of the second edition of the International Symposium on Computational Sensing (ISCS25)", "categories": ["eess.SP"], "comment": "This is the proceedings of the second edition of ISCS which took\n  place in June 2025 in Clervaux (LU)", "summary": "The International Symposium on Computational Sensing (ISCS) brings together\nresearchers from optical microscopy, electron microscopy, RADAR, astronomical\nimaging, biomedical imaging, remote sensing, and signal processing. With a\nparticular focus on applications and demonstrators, the purpose of this\nsymposium is to be a forum where researchers in computational sensing working\nin seemingly unrelated applications can learn, discover, and exchange on their\nnew findings and challenges. This 3-day symposium in the heart of Europe\nfeatures 6 keynotes speakers and is open to extended abstracts for scientific\npresentations and show-and-tell demonstrations."}
{"id": "2510.14117", "pdf": "https://arxiv.org/pdf/2510.14117", "abs": "https://arxiv.org/abs/2510.14117", "authors": ["Zhiyuan Wu", "Yijiong Lin", "Yongqiang Zhao", "Xuyang Zhang", "Zhuo Chen", "Nathan Lepora", "Shan Luo"], "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation", "categories": ["cs.RO"], "comment": null, "summary": "Robotic pushing is a fundamental manipulation task that requires tactile\nfeedback to capture subtle contact forces and dynamics between the end-effector\nand the object. However, real tactile sensors often face hardware limitations\nsuch as high costs and fragility, and deployment challenges involving\ncalibration and variations between different sensors, while vision-only\npolicies struggle with satisfactory performance. Inspired by humans' ability to\ninfer tactile states from vision, we propose ViTacGen, a novel robot\nmanipulation framework designed for visual robotic pushing with vision-to-touch\ngeneration in reinforcement learning to eliminate the reliance on\nhigh-resolution real tactile sensors, enabling effective zero-shot deployment\non visual-only robotic systems. Specifically, ViTacGen consists of an\nencoder-decoder vision-to-touch generation network that generates contact depth\nimages, a standardized tactile representation, directly from visual image\nsequence, followed by a reinforcement learning policy that fuses visual-tactile\ndata with contrastive learning based on visual and generated tactile\nobservations. We validate the effectiveness of our approach in both simulation\nand real world experiments, demonstrating its superior performance and\nachieving a success rate of up to 86\\%."}
{"id": "2510.14794", "pdf": "https://arxiv.org/pdf/2510.14794", "abs": "https://arxiv.org/abs/2510.14794", "authors": ["Halvin Yang", "Yizhe Zhao", "Kai-Kit Wong", "Hsiao-Hwa Chen", "Chan-Byoung Chae"], "title": "Bridging Theory and Practice in Reconfigurable Fluid Antenna Systems", "categories": ["eess.SP"], "comment": "Accepted into IEEE Communications Magazine", "summary": "Fluid antennas, including those based on liquid, mechanical, and pixel-based\ntechnologies, are poised to significantly enhance next-generation wireless\nsystems by adaptively optimizing their radiation characteristics. Many\ntheoretical analyses assumed near-instant reconfiguration, perfect channel\nknowledge, static or slowly varying propagation environments, and ideal\nmaterial properties that rarely hold in practice. In this article, we dissect\nthese common assumptions and contrast them with the realities of finite\nactuation time, limited and imperfect channel state information, rapidly\nchanging fading conditions, electromagnetic coupling, and mechanical\nconstraints. Through illustrative examples and simulations, we demonstrate how\nignoring these factors can lead to overestimated gains in capacity, coverage,\netc.. We then propose modeling refinements, experimental validation methods,\nand emerging control algorithms that better account for real-world constraints.\nOur findings highlight that, while reconfigurable antennas remain highly\npromising for B5G/6G and Internet of things (IoT) applications, their full\npotential can only be realized by incorporating practical considerations into\nsystem design and performance evaluation."}
{"id": "2510.14234", "pdf": "https://arxiv.org/pdf/2510.14234", "abs": "https://arxiv.org/abs/2510.14234", "authors": ["Ning Han", "Gu Gong", "Bin Zhang", "Yuexuan Xu", "Bohan Yang", "Yunhui Liu", "David Navarro-Alarcon"], "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod."}
{"id": "2510.14802", "pdf": "https://arxiv.org/pdf/2510.14802", "abs": "https://arxiv.org/abs/2510.14802", "authors": ["Sanjaya Herath", "Armin Gerami", "Kevin Wagner", "Ramani Duraiswami", "Christopher A. Metzler"], "title": "A Scalable MVDR Beamforming Algorithm That is Linear in the Number of Antennas", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, Asilomar 2025", "summary": "The Minimum Variance Distortionless Response (MVDR) beamforming technique is\nwidely applied in array systems to mitigate interference. However, applying\nMVDR to large arrays is computationally challenging; its computational\ncomplexity scales cubically with the number of antenna elements. In this paper,\nwe introduce a scalable MVDR beamforming method tailored for massive arrays.\nOur approach, which is specific to scenarios where the signal of interest is\nbelow the noise floor (e.g.,~GPS), leverages the Sherman-Morrison formula,\nlow-rank Singular Value Decomposition (SVD) approximations, and algebraic\nmanipulation. Using our approach, we reduce the computational complexity from\ncubic to linear in the number of antennas. We evaluate the proposed method\nthrough simulations, comparing its computational efficiency and beamforming\naccuracy with the conventional MVDR approach. Our method significantly reduces\nthe computational load while maintaining high beamforming accuracy for\nlarge-scale arrays. This solution holds promise for real-time applications of\nMVDR beamforming in fields like radar, sonar, and wireless communications,\nwhere massive antenna arrays are proliferating."}
{"id": "2510.14293", "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment."}
{"id": "2510.14806", "pdf": "https://arxiv.org/pdf/2510.14806", "abs": "https://arxiv.org/abs/2510.14806", "authors": ["Bowen Li", "Junting Chen", "Nikolaos Pappas"], "title": "Joint Channel and CFO Estimation From Beam-Swept Synchronization Signal Under Strong Inter-Cell Interference", "categories": ["eess.SP"], "comment": null, "summary": "Complete awareness of the wireless environment, crucial for future\nintelligent networks, requires sensing all transmitted signals, not just the\nstrongest. A fundamental barrier is estimating the target signal when it is\nburied under strong co-channel interference from other transmitters, a failure\nof which renders the signal unusable. This work proposes a maximum likelihood\n(ML)-based cross-preamble estimation framework that exploits carrier frequency\noffset (CFO) constancy across beam-swept synchronization signals (SS),\ncoherently aggregating information across multiple observations to reinforce\nthe desired signal against overwhelming interference. Cramer-Rao lower bound\n(CRLB) analysis and simulation demonstrate reliable estimation even when the\nsignal is over a thousand times weaker than the interference. A low-altitude\nradio-map case study further verifies the framework's practical effectiveness."}
{"id": "2510.14300", "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks."}
{"id": "2510.14939", "pdf": "https://arxiv.org/pdf/2510.14939", "abs": "https://arxiv.org/abs/2510.14939", "authors": ["Ken R. Duffy", "Moritz Grundei", "Jane A. Millward", "Muralidhar Rangaswamy", "Muriel Medard"], "title": "Decoding in the presence of ISI without interleaving ORBGRAND AI", "categories": ["eess.SP"], "comment": null, "summary": "Inter symbol interference (ISI), which occurs in a wide variety of channels,\nis a result of time dispersion. It can be mitigated by equalization which\nresults in noise coloring. For such colored noise, we propose a decoder called\nOrdered Reliability Bit Guessing Random Additive Noise Decoding (ORBGRANDAI)\nwhich is inspired by the development of approximate independence in statistical\nphysics. By foregoing interleaving, ORBGRAND-AI can deliver the same, or lower,\nblock error rate (BLER) for the same amount of energy per information bit in an\nISI channel as a state-of-the-art soft input decoder, such as Cyclic Redundancy\nCheck Assisted-Successive Cancellation List (CA-SCL) decoding, with an\ninterleaver. To assess the decoding performance of ORBGRAND-AI, we consider\ndelay tap models and their associated colored noise. In particular, we examine\na two-tap dicode ISI channel as well as an ISI channel derived from data from\nRFView, a physics-informed modeling and simulation tool. We investigate the\ndicode and RFView channel under a variety of imperfect channel state\ninformation assumptions and show that a second order autoregressive model\nadequately represents the RFView channel effect."}
{"id": "2510.14338", "pdf": "https://arxiv.org/pdf/2510.14338", "abs": "https://arxiv.org/abs/2510.14338", "authors": ["Yuanhong Zeng", "Anushri Dixit"], "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation."}
{"id": "2510.13886", "pdf": "https://arxiv.org/pdf/2510.13886", "abs": "https://arxiv.org/abs/2510.13886", "authors": ["Pierre Fayolle", "Alexandre Bône", "Noëlie Debs", "Mathieu Naudin", "Pascal Bourdon", "Remy Guillevin", "David Helbert"], "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading", "categories": ["q-bio.QM", "cs.AI", "eess.IV", "eess.SP"], "comment": "5 pages, 5 figures, IEEE ISBI 2025, Houston, Tx, USA", "summary": "DSC-MRI perfusion is a medical imaging technique for diagnosing and\nprognosing brain tumors and strokes. Its analysis relies on mathematical\ndeconvolution, but noise or motion artifacts in a clinical environment can\ndisrupt this process, leading to incorrect estimate of perfusion parameters.\nAlthough deep learning approaches have shown promising results, their\ncalibration typically rely on third-party deconvolution algorithms to generate\nreference outputs and are bound to reproduce their limitations.\n  To adress this problem, we propose a physics-informed autoencoder that\nleverages an analytical model to decode the perfusion parameters and guide the\nlearning of the encoding network. This autoencoder is trained in a\nself-supervised fashion without any third-party software and its performance is\nevaluated on a database with glioma patients. Our method shows reliable results\nfor glioma grading in accordance with other well-known deconvolution algorithms\ndespite a lower computation time. It also achieved competitive performance even\nin the presence of high noise which is critical in a medical environment."}
{"id": "2510.14357", "pdf": "https://arxiv.org/pdf/2510.14357", "abs": "https://arxiv.org/abs/2510.14357", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN."}
{"id": "2510.13904", "pdf": "https://arxiv.org/pdf/2510.13904", "abs": "https://arxiv.org/abs/2510.13904", "authors": ["Akarsh Prabhakara", "Yawen Liu", "Aswin C. Sankaranarayanan", "Anthony Rowe", "Swarun Kumar"], "title": "Millimeter Wave Inverse Pinhole Imaging", "categories": ["eess.IV", "cs.NI", "eess.SP"], "comment": null, "summary": "Millimeter wave (mmWave) radars are popular for perception in vision-denied\ncontexts due to their compact size. This paper explores emerging use-cases that\ninvolve static mount or momentarily-static compact radars, for example, a\nhovering drone. The key challenge with static compact radars is that their\nlimited form-factor also limits their angular resolution. This paper presents\nUmbra, a mmWave high resolution imaging system, that introduces the concept of\nrotating mmWave \"inverse pinholes\" for angular resolution enhancement. We\npresent the imaging system model, design, and evaluation of mmWave inverse\npinholes. The inverse pinhole is attractive for its lightweight nature, which\nenables low-power rotation, upgrading static-mount radars. We also show how\npropellers in aerial vehicles act as natural inverse pinholes and can enjoy the\nbenefits of high-resolution imaging even while they are momentarily static,\ne.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\\circ}$ with\njust a single antenna, a 5$\\times$ improvement compared to 14$^{\\circ}$ from a\ncompact mmWave radar baseline."}
{"id": "2510.14414", "pdf": "https://arxiv.org/pdf/2510.14414", "abs": "https://arxiv.org/abs/2510.14414", "authors": ["Baris Baysal", "Omid Arfaie", "Ramazan Unal"], "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle."}
{"id": "2510.14858", "pdf": "https://arxiv.org/pdf/2510.14858", "abs": "https://arxiv.org/abs/2510.14858", "authors": ["Yifeng Qin", "Jing Chen", "Zhi Hao Jiang", "Zhining Chen", "Yongming Huang", "Lingyang Song"], "title": "Exploiting Non-Diffracting Beams for Resilient Near-Field Millimeter-Wave Communications A Quantitative Roadmap", "categories": ["physics.optics", "eess.SP"], "comment": null, "summary": "Non diffracting (ND) beams are often cited as a promising solution to\nmitigate blockage in millimeter wave (mmWave) systems. However, a quantitative\nanswer to the fundamental question, under what specific conditions do ND beams\nactually outperform conventional pencil beams, has remained elusive, especially\nin the emerging context of near-field communications. This paper provides the\nfirst systematic answer by mapping the performance advantage regimes of ND\nbeams for blockage-resilient near-field links. We propose a unified holographic\ngenerator that synthesizes various structured beams (e.g., Bessel, Mathieu)\nunder the physical constraints of a planar phased array, ensuring a fair\ncomparison against a boresight baseline with identical EIRP and aperture.\nThrough extensive, unbiased Monte Carlo simulations, we construct advantage\nregime maps that delineate the specific regions where ND beams offer a tangible\nlink-level gain. Our key finding is that the advantage of ND beams is a\npowerful but conditional near field phenomenon. While offering a positive\naverage gain, its performance is highly variable, with a 60-70% probability of\noutperforming the baseline in its optimal range. Crucially, this performance is\nstrongly modulated by the obstacle's geometry, revealing a significant weakness\nagainst large blockers. These findings provide not just a practical roadmap for\njudiciously employing ND beams but also a clear motivation for future work in\nenvironment-aware, adaptively shaped structured beams."}
{"id": "2510.14454", "pdf": "https://arxiv.org/pdf/2510.14454", "abs": "https://arxiv.org/abs/2510.14454", "authors": ["Tao Huang", "Huayi Wang", "Junli Ren", "Kangning Yin", "Zirui Wang", "Xiao Chen", "Feiyu Jia", "Wentao Zhang", "Junfeng Long", "Jingbo Wang", "Jiangmiao Pang"], "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages", "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/."}
{"id": "2510.14922", "pdf": "https://arxiv.org/pdf/2510.14922", "abs": "https://arxiv.org/abs/2510.14922", "authors": ["Annisaa Fitri Nurfidausi", "Eleonora Mancini", "Paolo Torroni"], "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.SP"], "comment": null, "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection."}
{"id": "2510.14467", "pdf": "https://arxiv.org/pdf/2510.14467", "abs": "https://arxiv.org/abs/2510.14467", "authors": ["Shang-Fu Chen", "Co Yong", "Shao-Hua Sun"], "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models", "categories": ["cs.RO"], "comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)", "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data."}
{"id": "2510.14511", "pdf": "https://arxiv.org/pdf/2510.14511", "abs": "https://arxiv.org/abs/2510.14511", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Simone Kager", "Domenico Campolo"], "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies."}
{"id": "2510.14546", "pdf": "https://arxiv.org/pdf/2510.14546", "abs": "https://arxiv.org/abs/2510.14546", "authors": ["Matti Pekkanen", "Francesco Verdoja", "Ville Kyrki"], "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training."}
{"id": "2510.14584", "pdf": "https://arxiv.org/pdf/2510.14584", "abs": "https://arxiv.org/abs/2510.14584", "authors": ["Benno Wingender", "Nils Dengler", "Rohit Menon", "Sicong Pan", "Maren Bennewitz"], "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning", "categories": ["cs.RO"], "comment": null, "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors."}
{"id": "2510.14612", "pdf": "https://arxiv.org/pdf/2510.14612", "abs": "https://arxiv.org/abs/2510.14612", "authors": ["Gabriel Fischer Abati", "João Carlos Virgolino Soares", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size."}
{"id": "2510.14615", "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decré"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "categories": ["cs.RO"], "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods."}
{"id": "2510.14627", "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios."}
{"id": "2510.14643", "pdf": "https://arxiv.org/pdf/2510.14643", "abs": "https://arxiv.org/abs/2510.14643", "authors": ["Lara Brudermüller", "Brandon Hung", "Xinghao Zhu", "Jiuguang Wang", "Nick Hawes", "Preston Culbertson", "Simon Le Cleac'h"], "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": "9 pages, 5 figures", "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations."}
{"id": "2510.14647", "pdf": "https://arxiv.org/pdf/2510.14647", "abs": "https://arxiv.org/abs/2510.14647", "authors": ["Jialei Huang", "Yang Ye", "Yuanqing Gong", "Xuezhou Zhu", "Yang Gao", "Kaifeng Zhang"], "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage."}
{"id": "2510.14677", "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop."}
{"id": "2510.14768", "pdf": "https://arxiv.org/pdf/2510.14768", "abs": "https://arxiv.org/abs/2510.14768", "authors": ["Fan Yang", "Zixuan Huang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery", "categories": ["cs.RO"], "comment": null, "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries."}
{"id": "2510.14771", "pdf": "https://arxiv.org/pdf/2510.14771", "abs": "https://arxiv.org/abs/2510.14771", "authors": ["Xu Chi", "Chao Zhang", "Yang Su", "Lingfeng Dou", "Fujia Yang", "Jiakuo Zhao", "Haoyu Zhou", "Xiaoyou Jia", "Yong Zhou", "Shan An"], "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation", "categories": ["cs.RO"], "comment": "17 pages", "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL."}
{"id": "2510.14783", "pdf": "https://arxiv.org/pdf/2510.14783", "abs": "https://arxiv.org/abs/2510.14783", "authors": ["Aderik Verraest", "Stavrow Bahnam", "Robin Ferede", "Guido de Croon", "Christophe De Wagter"], "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight."}
{"id": "2510.14827", "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns."}
{"id": "2510.14830", "pdf": "https://arxiv.org/pdf/2510.14830", "abs": "https://arxiv.org/abs/2510.14830", "authors": ["Kun Lei", "Huanyu Li", "Dongjie Yu", "Zhenyu Wei", "Lingxiao Guo", "Zhennan Jiang", "Ziyu Wang", "Shiyu Liang", "Huazhe Xu"], "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "https://lei-kun.github.io/RL-100/", "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours."}
{"id": "2510.14849", "pdf": "https://arxiv.org/pdf/2510.14849", "abs": "https://arxiv.org/abs/2510.14849", "authors": ["Marcello Sorge", "Nicola Cigarini", "Riccardo Lorigiola", "Giulia Michieletto", "Andrea Masiero", "Angelo Cenedese", "Alberto Guarnieri"], "title": "Multi Agent Switching Mode Controller for Sound Source localization", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others."}
{"id": "2510.14851", "pdf": "https://arxiv.org/pdf/2510.14851", "abs": "https://arxiv.org/abs/2510.14851", "authors": ["Jakob Bichler", "Andreu Matoses Gimenez", "Javier Alonso-Mora"], "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time", "categories": ["cs.RO", "cs.MA"], "comment": "7 pages, 5 figures. 2025 IEEE Int. Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS 2025). Website and Code:\n  https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/"}
{"id": "2510.14893", "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners."}
{"id": "2510.14902", "pdf": "https://arxiv.org/pdf/2510.14902", "abs": "https://arxiv.org/abs/2510.14902", "authors": ["Han Zhao", "Jiaxuan Zhang", "Wenxuan Song", "Pengxiang Ding", "Donglin Wang"], "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io."}
{"id": "2510.14930", "pdf": "https://arxiv.org/pdf/2510.14930", "abs": "https://arxiv.org/abs/2510.14930", "authors": ["Binghao Huang", "Jie Xu", "Iretiayo Akinola", "Wei Yang", "Balakumar Sundaralingam", "Rowland O'Flaherty", "Dieter Fox", "Xiaolong Wang", "Arsalan Mousavian", "Yu-Wei Chao", "Yunzhu Li"], "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted by 9th Conference on Robot Learning (CoRL 2025); Website:\n  https://binghao-huang.github.io/vt_refine/", "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/."}
{"id": "2510.14947", "pdf": "https://arxiv.org/pdf/2510.14947", "abs": "https://arxiv.org/abs/2510.14947", "authors": ["Blake Werner", "Lizhi Yang", "Aaron D. Ames"], "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages", "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion."}
{"id": "2510.14952", "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems."}
{"id": "2510.14959", "pdf": "https://arxiv.org/pdf/2510.14959", "abs": "https://arxiv.org/abs/2510.14959", "authors": ["Lizhi Yang", "Blake Werner", "Massimiliano de Sa Aaron D. Ames"], "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages", "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter."}
{"id": "2510.14968", "pdf": "https://arxiv.org/pdf/2510.14968", "abs": "https://arxiv.org/abs/2510.14968", "authors": ["Mingxuan Yan", "Yuping Wang", "Zechun Liu", "Jiachen Li"], "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025); Project Website: rdd-neurips.github.io", "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io."}
{"id": "2509.26255", "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines."}
{"id": "2510.13810", "pdf": "https://arxiv.org/pdf/2510.13810", "abs": "https://arxiv.org/abs/2510.13810", "authors": ["Minja Axelsson", "Lea Luka Sikau"], "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces", "categories": ["cs.HC", "cs.CY", "cs.RO", "I.2.9; J.5; K.4.2"], "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "Delivering groceries or cleaning airports, mobile robots exist in public\nspaces. While these examples showcase robots that execute tasks, this paper\nexplores mobile robots that encourage posthuman collaboration rather than\nmanaging environments independently. With feigned fragility, cuteness and\nincomplete functionalities, the so-called \"weak robots\" invite passersby to\nengage not only on a utilitarian level, but also through imaginative and\nemotional responses. After examining the workings of \"weak robots\" by queering\nnotions of function and ability, we introduce two speculative design fiction\nvignettes that describe choreographies of such robots in future urban spaces --\none exploring a utopian weak robot and the other a dystopian weak robot. We\nintroduce these speculations in order to discuss how different values may drive\ndesign decisions, and how such decisions may shape and drive different\nsocio-technical futures in which robots and humans share public spaces that\nincentivise collaboration."}
{"id": "2510.14354", "pdf": "https://arxiv.org/pdf/2510.14354", "abs": "https://arxiv.org/abs/2510.14354", "authors": ["Siddharth Tourani", "Jayaram Reddy", "Sarvesh Thakur", "K Madhava Krishna", "Muhammad Haris Khan", "N Dinesh Reddy"], "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, accepted at ICRA 2024 (International Conference on Robotics\n  and Automation)", "summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has\nbecome available. This prompts the question of how to utilize this data for\ngeometric reasoning of scenes. While many RGB-D registration meth- ods rely on\ngeometric and feature-based similarity, we take a different approach. We use\ncycle-consistent keypoints as salient points to enforce spatial coherence\nconstraints during matching, improving correspondence accuracy. Additionally,\nwe introduce a novel pose block that combines a GRU recurrent unit with\ntransformation synchronization, blending historical and multi-view data. Our\napproach surpasses previous self- supervised registration methods on ScanNet\nand 3DMatch, even outperforming some older supervised methods. We also\nintegrate our components into existing methods, showing their effectiveness."}
{"id": "2510.14653", "pdf": "https://arxiv.org/pdf/2510.14653", "abs": "https://arxiv.org/abs/2510.14653", "authors": ["Sven Tarlowski", "Lutz Eckstein"], "title": "Requirement Identification for Traffic Simulations in Driving Simulators", "categories": ["cs.SE", "cs.RO"], "comment": "2 Pages, 1 figure", "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing."}
{"id": "2510.14828", "pdf": "https://arxiv.org/pdf/2510.14828", "abs": "https://arxiv.org/abs/2510.14828", "authors": ["Jinrui Liu", "Bingyan Nie", "Boyu Li", "Yaran Chen", "Yuze Wang", "Shunsen He", "Haoran Li"], "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark."}
{"id": "2510.14836", "pdf": "https://arxiv.org/pdf/2510.14836", "abs": "https://arxiv.org/abs/2510.14836", "authors": ["Yixuan Li", "Yuhui Chen", "Mingcai Zhou", "Haoran Li"], "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks."}
{"id": "2510.14914", "pdf": "https://arxiv.org/pdf/2510.14914", "abs": "https://arxiv.org/abs/2510.14914", "authors": ["Ruhan Yang", "Ellen Yi-Luen Do"], "title": "Design of Paper Robot Building Kits", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits."}
{"id": "2510.14946", "pdf": "https://arxiv.org/pdf/2510.14946", "abs": "https://arxiv.org/abs/2510.14946", "authors": ["Romina Aalishah", "Mozhgan Navardi", "Tinoosh Mohsenin"], "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices", "categories": ["eess.IV", "cs.RO"], "comment": "The 11th IEEE International Conference on Edge Computing and Scalable\n  Cloud (IEEE EdgeCom 2025)", "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity."}
{"id": "2510.14976", "pdf": "https://arxiv.org/pdf/2510.14976", "abs": "https://arxiv.org/abs/2510.14976", "authors": ["Shaowei Liu", "Chuan Guo", "Bing Zhou", "Jian Wang"], "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Accepted to ICCV 2025. Project page:\n  https://stevenlsw.github.io/ponimator/", "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework."}
