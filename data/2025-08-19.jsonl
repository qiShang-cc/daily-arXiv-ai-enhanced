{"id": "2508.11640", "pdf": "https://arxiv.org/pdf/2508.11640", "abs": "https://arxiv.org/abs/2508.11640", "authors": ["Danny Scott", "William LaForest", "Hritom Das", "Ioannis Polykretis", "Catherine D. Schuman", "Charles Rizzo", "James Plank", "Sai Swaminathan"], "title": "Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": "International Conference on Neuromorphic Systems (ICONS) 2025 9\n  pages, 7 images", "summary": "The deployment of dense, low-cost sensors is critical for realizing\nubiquitous smart environments. However, existing sensing solutions struggle\nwith the energy, scalability, and reliability trade-offs imposed by battery\nmaintenance, wireless transmission overhead, and data processing complexity. In\nthis work, we present Vibe2Spike, a novel battery-free, wireless sensing\nframework that enables vibration-based activity recognition using visible light\ncommunication (VLC) and spiking neural networks (SNNs). Our system uses\nultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and\nan LED, which harvest vibration energy and emit sparse visible light spikes\nwithout requiring batteries or RF radios. These optical spikes are captured by\nevent cameras and classified using optimized SNN models evolved via the EONS\nframework. We evaluate Vibe2Spike across five device classes, achieving 94.9\\%\naverage classification fitness while analyzing the latency-accuracy trade-offs\nof different temporal binning strategies. Vibe2Spike demonstrates a scalable,\nand energy-efficient approach for enabling intelligent environments in a\nbatteryless manner."}
{"id": "2508.11654", "pdf": "https://arxiv.org/pdf/2508.11654", "abs": "https://arxiv.org/abs/2508.11654", "authors": ["Yang Zhao", "Tao Wang", "Said Elhadi"], "title": "Data-driven RF Tomography via Cross-modal Sensing and Continual Learning", "categories": ["eess.SP", "cs.CV"], "comment": "6 pages, 4 figures, to be published in IEEE AVSS Conference", "summary": "Data-driven radio frequency (RF) tomography has demonstrated significant\npotential for underground target detection, due to the penetrative nature of RF\nsignals through soil. However, it is still challenging to achieve accurate and\nrobust performance in dynamic environments. In this work, we propose a\ndata-driven radio frequency tomography (DRIFT) framework with the following key\ncomponents to reconstruct cross section images of underground root tubers, even\nwith significant changes in RF signals. First, we design a cross-modal sensing\nsystem with RF and visual sensors, and propose to train an RF tomography deep\nneural network (DNN) model following the cross-modal learning approach. Then we\npropose to apply continual learning to automatically update the DNN model, once\nenvironment changes are detected in a dynamic environment. Experimental results\nshow that our approach achieves an average equivalent diameter error of 2.29\ncm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and\ndataset are publicly available on https://github.com/Data-driven-RTI/DRIFT."}
{"id": "2508.11656", "pdf": "https://arxiv.org/pdf/2508.11656", "abs": "https://arxiv.org/abs/2508.11656", "authors": ["Ridma Jayasundara", "Ishan Fernando", "Adeepa Fernando", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "Inductive transfer learning from regression to classification in ECG analysis", "categories": ["eess.SP", "cs.LG", "I.2.6; I.5.1; I.5.4; I.2.1; J.3"], "comment": "This manuscript is 15 pages with 4 tables and 5 figures. The\n  manuscript is under review at Nature Scientific Reports", "summary": "Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,\naccounting for over 30% of global deaths according to the World Health\nOrganization (WHO). Importantly, one-third of these deaths are preventable with\ntimely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive\nmethod for recording the electrical activity of the heart, is crucial for\ndiagnosing CVDs. However, privacy concerns surrounding the use of patient ECG\ndata in research have spurred interest in synthetic data, which preserves the\nstatistical properties of real data without compromising patient\nconfidentiality. This study explores the potential of synthetic ECG data for\ntraining deep learning models from regression to classification tasks and\nevaluates the feasibility of transfer learning to enhance classification\nperformance on real ECG data. We experimented with popular deep learning models\nto predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,\nQT interval, and QRS complex-using separate regression models. Subsequently, we\nleveraged these regression models for transfer learning to perform 5-class ECG\nsignal classification. Our experiments systematically investigate whether\ntransfer learning from regression to classification is viable, enabling better\nutilization of diverse open-access and synthetic ECG datasets. Our findings\ndemonstrate that transfer learning from regression to classification improves\nclassification performance, highlighting its potential to maximize the utility\nof available data and advance deep learning applications in this domain."}
{"id": "2508.11657", "pdf": "https://arxiv.org/pdf/2508.11657", "abs": "https://arxiv.org/abs/2508.11657", "authors": ["Yuanhao Li", "Badong Chen", "Wenjun Bai", "Yasuharu Koike", "Okito Yamashita"], "title": "Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Objective: Sparse Bayesian learning provides an effective scheme to solve the\nhigh-dimensional problem in brain signal decoding. However, traditional\nassumptions regarding data distributions such as Gaussian and binomial are\npotentially inadequate to characterize the noisy signals of brain activity.\nHence, this study aims to propose a robust sparse Bayesian learning framework\nto address noisy highdimensional brain activity decoding. Methods: Motivated by\nthe commendable robustness of the minimum error entropy (MEE) criterion for\nhandling complex data distributions, we proposed an MEE-based likelihood\nfunction to facilitate the accurate inference of sparse Bayesian learning in\nanalyzing noisy brain datasets. Results: Our proposed approach was evaluated\nusing two high-dimensional brain decoding tasks in regression and\nclassification contexts, respectively. The experimental results showed that,\nour approach can realize superior decoding metrics and physiological patterns\nthan the conventional and state-of-the-art methods. Conclusion: Utilizing the\nproposed MEE-based likelihood model, sparse Bayesian learning is empowered to\nsimultaneously address the challenges of noise and high dimensionality in the\nbrain decoding task. Significance: This work provides a powerful tool to\nrealize robust brain decoding, advancing biomedical engineering applications\nsuch as brain-computer interface."}
{"id": "2508.11759", "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans."}
{"id": "2508.11658", "pdf": "https://arxiv.org/pdf/2508.11658", "abs": "https://arxiv.org/abs/2508.11658", "authors": ["Honggui Li", "Zhengyang Zhang", "Dingtai Li", "Sinan Chen", "Nahid Md Lokman Hossain", "Xinfeng Xu", "Yuting Feng", "Hantao Lu", "Yinlu Qin", "Ruobing Wang", "Maria Trocan", "Dimitri Galayko", "Amara Amara", "Mohamad Sawan"], "title": "CECGSR: Circular ECG Super-Resolution", "categories": ["eess.SP"], "comment": null, "summary": "The electrocardiogram (ECG) plays a crucial role in the diagnosis and\ntreatment of various cardiac diseases. ECG signals suffer from low-resolution\n(LR) due to the use of convenient acquisition devices, as well as internal and\nexternal noises and artifacts. Classical ECG super-resolution (ECGSR) methods\nadopt an open-loop architecture that converts LR ECG signals to\nsuper-resolution (SR) ones. According to the theory of automatic control, a\nclosed-loop framework exhibits superior dynamic and static performance compared\nwith its open-loop counterpart. This paper proposes a closed-loop approach,\ntermed circular ECGSR (CECGSR), which models the degradation process from SR\nECG signals to LR ones. The negative feedback mechanism of the closed-loop\nsystem is based on the differences between the LR ECG signals. A mathematical\nloop equation is constructed to characterize the closed-loop infrastructure.\nThe Taylor series expansion is employed to demonstrate the near-zero\nsteady-state error of the proposed method. A Plug-and-Play strategy is\nconsidered to establish the SR unit of the proposed architecture, leveraging\nany existing advanced open-loop ECGSR methods. Simulation experiments on both\nnoiseless and noisy subsets of the PTB-XL datasets demonstrate that the\nproposed CECGSR outperforms state-of-the-art open-loop ECGSR algorithms in the\nreconstruction performance of ECG signals."}
{"id": "2508.11802", "pdf": "https://arxiv.org/pdf/2508.11802", "abs": "https://arxiv.org/abs/2508.11802", "authors": ["Luigi Penco", "Beomyeong Park", "Stefan Fasano", "Nehar Poddar", "Stephen McCrory", "Nicholas Kitchel", "Tomasz Bialek", "Dexton Anderson", "Duncan Calvert", "Robert Griffin"], "title": "Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots", "categories": ["cs.RO"], "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots\n  (Humanoids)", "summary": "Achieving seamless synchronization between user and robot motion in\nteleoperation, particularly during high-speed tasks, remains a significant\nchallenge. In this work, we propose a novel approach for transferring stepping\nmotions from the user to the robot in real-time. Instead of directly\nreplicating user foot poses, we retarget user steps to robot footstep\nlocations, allowing the robot to utilize its own dynamics for locomotion,\nensuring better balance and stability. Our method anticipates user footsteps to\nminimize delays between when the user initiates and completes a step and when\nthe robot does it. The step estimates are continuously adapted to converge with\nthe measured user references. Additionally, the system autonomously adjusts the\nrobot's steps to account for its surrounding terrain, overcoming challenges\nposed by environmental mismatches between the user's flat-ground setup and the\nrobot's uneven terrain. Experimental results on the humanoid robot Nadia\ndemonstrate the effectiveness of the proposed system."}
{"id": "2508.11663", "pdf": "https://arxiv.org/pdf/2508.11663", "abs": "https://arxiv.org/abs/2508.11663", "authors": ["Guangli Li", "Canbiao Wu", "Zhen Liang"], "title": "Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Affective computing is a rapidly developing interdisciplinary research\ndirection in the field of brain-computer interface. In recent years, the\nintroduction of deep learning technology has greatly promoted the development\nof the field of emotion recognition. However, due to physiological differences\nbetween subjects, as well as the variations in experimental environments and\nequipment, cross-corpus emotion recognition faces serious challenges,\nespecially for samples near the decision boundary. To solve the above problems,\nwe propose an optimization method based on domain adversarial transfer learning\nto fine-grained alignment of affective features, named Maximum classifier\ndiscrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a\ndual adversarial classifier (Ada classifier and RMS classifier), and apply a\nthree-stage adversarial training to maximize classification discrepancy and\nminimize feature distribution to align controversy samples near the decision\nboundary. In the process of domain adversarial training, the two classifiers\nalso maintain an adversarial relationship, ultimately enabling precise\ncross-corpus feature alignment. In addition, the introduction of pairwise\nlearning transforms the classification problem of samples into a similarity\nproblem between samples, alleviating the influence of label noise. We conducted\nsystematic experimental evaluation of the model using publicly available SEED,\nSEED-IV and SEED-V databases. The results show that the McdPL model is superior\nto other baseline models in the cross-corpus emotion recognition task, and the\naverage accuracy improvements of 4.76\\% and 3.97\\%, respectively. Our work\nprovides a promising solution for emotion recognition cross-corpus. The source\ncode is available at https://github.com/WuCB-BCI/Mcd_PL."}
{"id": "2508.11849", "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Allen Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "categories": ["cs.RO"], "comment": null, "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget."}
{"id": "2508.11664", "pdf": "https://arxiv.org/pdf/2508.11664", "abs": "https://arxiv.org/abs/2508.11664", "authors": ["Zahra Mohammadi", "Parnian Fazel", "Siamak Mohammadi"], "title": "Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Sleep stage classification is crucial for diagnosing and managing disorders\nsuch as sleep apnea and insomnia. Conventional clinical methods like\npolysomnography are costly and impractical for long-term home use. We present\nan energy-efficient pipeline that detects four sleep stages (wake, REM, light,\nand deep) from a single-lead ECG. Two windowing strategies are introduced: (1)\na 5-minute window with 30-second steps for machine-learning models that use\nhandcrafted features, and (2) a 30-second window with 10-second steps for\ndeep-learning models, enabling near-real-time 10-second resolution. Lightweight\nnetworks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score\nbut still draw significant energy. We therefore design SleepLiteCNN, a custom\nmodel that achieves 89 percent accuracy and 89 percent F1-score while lowering\nenergy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit\nquantization preserves accuracy and further reduces power, and FPGA deployment\nconfirms low resource usage. The proposed system offers a practical solution\nfor continuous, wearable ECG-based sleep monitoring."}
{"id": "2508.11868", "pdf": "https://arxiv.org/pdf/2508.11868", "abs": "https://arxiv.org/abs/2508.11868", "authors": ["Lida Xu"], "title": "Data Shift of Object Detection in Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "With the widespread adoption of machine learning technologies in autonomous\ndriving systems, their role in addressing complex environmental perception\nchallenges has become increasingly crucial. However, existing machine learning\nmodels exhibit significant vulnerability, as their performance critically\ndepends on the fundamental assumption that training and testing data satisfy\nthe independent and identically distributed condition, which is difficult to\nguarantee in real-world applications. Dynamic variations in data distribution\ncaused by seasonal changes, weather fluctuations lead to data shift problems in\nautonomous driving systems. This study investigates the data shift problem in\nautonomous driving object detection tasks, systematically analyzing its\ncomplexity and diverse manifestations. We conduct a comprehensive review of\ndata shift detection methods and employ shift detection analysis techniques to\nperform dataset categorization and balancing. Building upon this foundation, we\nconstruct an object detection model. To validate our approach, we optimize the\nmodel by integrating CycleGAN-based data augmentation techniques with the\nYOLOv5 framework. Experimental results demonstrate that our method achieves\nsuperior performance compared to baseline models on the BDD100K dataset."}
{"id": "2508.11666", "pdf": "https://arxiv.org/pdf/2508.11666", "abs": "https://arxiv.org/abs/2508.11666", "authors": ["Timothy Oladunni", "Ehimen Aneni"], "title": "Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The limitations of unimodal deep learning models, particularly their tendency\nto overfit and limited generalizability, have renewed interest in multimodal\nfusion strategies. Multimodal deep neural networks (MDNN) have the capability\nof integrating diverse data domains and offer a promising solution for robust\nand accurate predictions. However, the optimal fusion strategy, intermediate\nfusion (feature-level) versus late fusion (decision-level) remains\ninsufficiently examined, especially in high-stakes clinical contexts such as\nECG-based cardiovascular disease (CVD) classification. This study investigates\nthe comparative effectiveness of intermediate and late fusion strategies using\nECG signals across three domains: time, frequency, and time-frequency. A series\nof experiments were conducted to identify the highest-performing fusion\narchitecture. Results demonstrate that intermediate fusion consistently\noutperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's\nd > 0.8 relative to standalone models and d = 0.40 compared to late fusion.\nInterpretability analyses using saliency maps reveal that both models align\nwith the discretized ECG signals. Statistical dependency between the\ndiscretized ECG signals and corresponding saliency maps for each class was\nconfirmed using Mutual Information (MI). The proposed ECG domain-based\nmultimodal model offers superior predictive capability and enhanced\nexplainability, crucial attributes in medical AI applications, surpassing\nstate-of-the-art models."}
{"id": "2508.11883", "pdf": "https://arxiv.org/pdf/2508.11883", "abs": "https://arxiv.org/abs/2508.11883", "authors": ["Lei Li", "Boyang Qin", "Wenzhuo Gao", "Yanyu Li", "Yiyuan Zhang", "Bo Wang", "Shihan Kong", "Jian Wang", "Dekui He", "Junzhi Yu"], "title": "Bioinspired underwater soft robots: from biology to robotics and back", "categories": ["cs.RO"], "comment": null, "summary": "The ocean vast unexplored regions and diverse soft-bodied marine organisms\nhave spurred interest in bio-inspired underwater soft robotics. Recent advances\nhave enabled new capabilities in underwater movement, sensing, and interaction.\nHowever, these efforts are largely unidirectional, with biology guiding\nrobotics while insights from robotics rarely feed back into biology. Here we\npropose a holistic, bidirectional framework that integrates biological\nprinciples, robotic implementation, and biological validation. We show that\nsoft robots can serve as experimental tools to probe biological functions and\neven test evolutionary hypotheses. Their inherent compliance also allows them\nto outperform rigid systems in unstructured environments, supporting\napplications in marine exploration, manipulation, and medicine. Looking\nforward, we introduce bio-universal-inspired robotics, a paradigm that\ntranscends species-specific mimicry by identifying convergent principles across\nspecies to inspire more adaptable designs. Despite rapid progress, challenges\npersist in material robustness, actuation efficiency, autonomy, and\nintelligence. By uniting biology and engineering, soft robots can advance ocean\nexploration and deepen scientific discovery."}
{"id": "2508.11668", "pdf": "https://arxiv.org/pdf/2508.11668", "abs": "https://arxiv.org/abs/2508.11668", "authors": ["Muhammad Umer", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "John M. Cioffi"], "title": "Neural Gaussian Radio Fields for Channel Estimation", "categories": ["eess.SP", "cs.NI"], "comment": "This paper has been submitted to NeurIPS 2025", "summary": "Accurate channel state information (CSI) remains the most critical bottleneck\nin modern wireless networks, with pilot overhead consuming up to 11-21% of\ntransmission bandwidth, increasing latency by 20-40% in massive MIMO systems,\nand reducing potential spectral efficiency by over 53%. Traditional estimation\ntechniques fundamentally fail under mobility, with feedback delays as small as\n4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We\npresent neural Gaussian radio fields (nGRF), a novel framework that leverages\nexplicit 3D Gaussian primitives to synthesize complex channel matrices\naccurately and efficiently. Unlike NeRF-based approaches that rely on slow\nimplicit representations or existing Gaussian splatting methods that use\nnon-physical 2D projections, nGRF performs direct 3D electromagnetic field\naggregation, with each Gaussian acting as a localized radio modulator. nGRF\ndemonstrates superior performance across diverse environments: in indoor\nscenarios, it achieves a 10.9$\\times$ higher prediction SNR than state of the\nart methods while reducing inference latency from 242 ms to just 1.1 ms (a\n220$\\times$ speedup). For large-scale outdoor environments, where existing\napproaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF\nrequires only 0.011 measurements per cubic foot compared to 0.2-178.1 for\nexisting methods, thereby reducing data collection burden by 18$\\times$.\nTraining time is similarly reduced from hours to minutes (a 180$\\times$\nreduction), enabling rapid adaptation to dynamic environments. The code and\ndatasets are available at: https://github.com/anonym-auth/n-grf"}
{"id": "2508.11884", "pdf": "https://arxiv.org/pdf/2508.11884", "abs": "https://arxiv.org/abs/2508.11884", "authors": ["Havel Liu", "Mingzhang Zhu", "Arturo Moises Flores Alvarez", "Yuan Hung Lo", "Conrad Ku", "Federico Parres", "Justin Quan", "Colin Togashi", "Aditya Navghare", "Quanyou Wang", "Dennis W. Hong"], "title": "From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics", "categories": ["cs.RO"], "comment": "8 pages, 14 figures, accepted by IEEE Humanoids 2025", "summary": "Humanoid robots represent the cutting edge of robotics research, yet their\npotential in entertainment remains largely unexplored. Entertainment as a field\nprioritizes visuals and form, a principle that contrasts with the purely\nfunctional designs of most contemporary humanoid robots. Designing\nentertainment humanoid robots capable of fluid movement presents a number of\nunique challenges. In this paper, we present Kid Cosmo, a research platform\ndesigned for robust locomotion and life-like motion generation while imitating\nthe look and mannerisms of its namesake character from Netflix's movie The\nElectric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall\nand weighing 25 kg. It contains 28 degrees of freedom and primarily uses\nproprioceptive actuators, enabling torque-control walking and lifelike motion\ngeneration. Following worldwide showcases as part of the movie's press tour, we\npresent the system architecture, challenges of a functional entertainment robot\nand unique solutions, and our initial findings on stability during simultaneous\nupper and lower body movement. We demonstrate the viability of\nperformance-oriented humanoid robots that prioritize both character embodiment\nand technical functionality."}
{"id": "2508.11675", "pdf": "https://arxiv.org/pdf/2508.11675", "abs": "https://arxiv.org/abs/2508.11675", "authors": ["Amgad A. Salama"], "title": "Direction of Arrival Estimation: A Tutorial Survey of Classical and Modern Methods", "categories": ["eess.SP", "53A45", "G.1"], "comment": "DOA Survey, 44 pages, Not published yet", "summary": "Direction of arrival (DOA) estimation is a fundamental problem in array\nsignal processing with applications spanning radar, sonar, wireless\ncommunications, and acoustic signal processing. This tutorial survey provides a\ncomprehensive introduction to classical and modern DOA estimation methods,\nspecifically designed for students and researchers new to the field. We focus\non narrowband signal processing using uniform linear arrays, presenting\nstep-by-step mathematical derivations with geometric intuition. The survey\ncovers classical beamforming methods, subspace-based techniques (MUSIC,\nESPRIT), maximum likelihood approaches, and sparse signal processing methods.\nEach method is accompanied by Python implementations available in an\nopen-source repository, enabling reproducible research and hands-on learning.\nThrough systematic performance comparisons across various scenarios, we provide\npractical guidelines for method selection and parameter tuning. This work aims\nto bridge the gap between theoretical foundations and practical implementation,\nmaking DOA estimation accessible to beginners while serving as a comprehensive\nreference for the field. See https://github.com/AmgadSalama/DOA for detail\nimplementation of the methods."}
{"id": "2508.11885", "pdf": "https://arxiv.org/pdf/2508.11885", "abs": "https://arxiv.org/abs/2508.11885", "authors": ["Haixin Gong", "Chen Zhang", "Yanan Sui"], "title": "Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System", "categories": ["cs.RO"], "comment": "IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids\n  2025)", "summary": "The human foot serves as the critical interface between the body and\nenvironment during locomotion. Existing musculoskeletal models typically\noversimplify foot-ground contact mechanics, limiting their ability to\naccurately simulate human gait dynamics. We developed a novel contact-rich and\ndeformable model of the human foot integrated within a complete musculoskeletal\nsystem that captures the complex biomechanical interactions during walking. To\novercome the control challenges inherent in modeling multi-point contacts and\ndeformable material, we developed a two-stage policy training strategy to learn\nnatural walking patterns for this interface-enhanced model. Comparative\nanalysis between our approach and conventional rigid musculoskeletal models\ndemonstrated improvements in kinematic, kinetic, and gait stability metrics.\nValidation against human subject data confirmed that our simulation closely\nreproduced real-world biomechanical measurements. This work advances\ncontact-rich interface modeling for human musculoskeletal systems and\nestablishes a robust framework that can be extended to humanoid robotics\napplications requiring precise foot-ground interaction control."}
{"id": "2508.11682", "pdf": "https://arxiv.org/pdf/2508.11682", "abs": "https://arxiv.org/abs/2508.11682", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Non-invasive glucose monitoring remains a critical challenge in the\nmanagement of diabetes. HRV during sleep shows promise for glucose prediction\nhowever, age-related autonomic changes significantly confound traditional HRV\nanalyses. We analyzed 43 subjects with multi-modal data including sleep-stage\nspecific ECG, HRV features, and clinical measurements. A novel\nage-normalization technique was applied to the HRV features by, dividing the\nraw values by age-scaled factors. BayesianRidge regression with 5-fold\ncross-validation was employed for log-glucose prediction. Age-normalized HRV\nfeatures achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,\nrepresenting a 25.6% improvement over non-normalized features (R2 = 0.132). The\ntop predictive features were hrv rem mean rr age normalized (r = 0.443, p =\n0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic\nblood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed\nage-normalization as the critical component, with sleep-stage specific features\nproviding additional predictive value. Age-normalized HRV features\nsignificantly enhance glucose prediction accuracy compared with traditional\napproaches. This sleep-aware methodology addresses fundamental limitations in\nautonomic function assessment and suggests a preliminary feasibility for\nnon-invasive glucose monitoring applications. However, these results require\nvalidation in larger cohorts before clinical consideration."}
{"id": "2508.11887", "pdf": "https://arxiv.org/pdf/2508.11887", "abs": "https://arxiv.org/abs/2508.11887", "authors": ["Yousra Shleibik", "Jordan Sinclair", "Kerstin Haring"], "title": "Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The advent of autonomous driving systems promises to transform transportation\nby enhancing safety, efficiency, and comfort. As these technologies evolve\ntoward higher levels of autonomy, the need for integrated systems that\nseamlessly support human involvement in decision-making becomes increasingly\ncritical. Certain scenarios necessitate human involvement, including those\nwhere the vehicle is unable to identify an object or element in the scene, and\nas such cannot take independent action. Therefore, situational awareness is\nessential to mitigate potential risks during a takeover, where a driver must\nassume control and autonomy from the vehicle. The need for driver attention is\nimportant to avoid collisions with external agents and ensure a smooth\ntransition during takeover operations. This paper explores the integration of\nattention redirection techniques, such as gaze manipulation through targeted\nvisual and auditory cues, to help drivers maintain focus on emerging hazards\nand reduce target fixation in semi-autonomous driving scenarios. We propose a\nconceptual framework that combines real-time gaze tracking, context-aware\nsaliency analysis, and synchronized visual and auditory alerts to enhance\nsituational awareness, proactively address potential hazards, and foster\neffective collaboration between humans and autonomous systems."}
{"id": "2508.11684", "pdf": "https://arxiv.org/pdf/2508.11684", "abs": "https://arxiv.org/abs/2508.11684", "authors": ["BG Tong"], "title": "A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Objective: This study proposes and preliminarily validates a novel\n\"Functional-Energetic Topology Model\" to uncover neurodynamic mechanisms of\nNon-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode\nbrain network patterns from single-channel EEG in real-world settings.Methods:\nEEG data were collected over ~1 month from three adolescents with NSSI using a\nsmartphone app and a portable Fp1 EEG headband during impulsive and\nnon-impulsive states. A theory-driven GNN with seven functional nodes was\nbuilt. Performance was evaluated via intra-subject (80/20 split) and\nleave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for\ninterpretability.Results: The model achieved high intra-subject accuracy (>85%)\nand significantly above-chance cross-subject performance (approximately73.7%).\nExplainability analysis revealed a key finding: during NSSI states, a critical\nfeedback loop regulating somatic sensation exhibits dysfunction and directional\nreversal. Specifically, the brain loses its ability to self-correct via\nnegative bodily feedback, and the regulatory mechanism enters an \"ineffective\nidling\" state.Conclusion: This work demonstrates the feasibility of applying\ntheory-guided GNNs to sparse, single-channel EEG for decoding complex mental\nstates. The identified \"feedback loop reversal\" offers a novel, dynamic, and\ncomputable model of NSSI mechanisms, paving the way for objective biomarkers\nand next-generation Digital Therapeutics (DTx)."}
{"id": "2508.11890", "pdf": "https://arxiv.org/pdf/2508.11890", "abs": "https://arxiv.org/abs/2508.11890", "authors": ["Sangwoo Jeon", "Juchul Shin", "YeonJe Cho", "Gyeong-Tae Kim", "Seongwoo Kim"], "title": "Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Modern autonomous drone missions increasingly require software frameworks\ncapable of seamlessly integrating structured symbolic planning with adaptive\nreinforcement learning (RL). Although traditional rule-based architectures\noffer robust structured reasoning for drone autonomy, their capabilities fall\nshort in dynamically complex operational environments that require adaptive\nsymbolic planning. Symbolic RL (SRL), using the Planning Domain Definition\nLanguage (PDDL), explicitly integrates domain-specific knowledge and\noperational constraints, significantly improving the reliability and safety of\nunmanned aerial vehicle (UAV) decision making. In this study, we propose the\nAMAD-SRL framework, an extended and refined version of the Autonomous Mission\nAgents for Drones (AMAD) cognitive multi-agent architecture, enhanced with\nsymbolic reinforcement learning for dynamic mission planning and execution. We\nvalidated our framework in a Software-in-the-Loop (SIL) environment structured\nidentically to an intended Hardware-In-the-Loop Simulation (HILS) platform,\nensuring seamless transition to real hardware. Experimental results demonstrate\nstable integration and interoperability of modules, successful transitions\nbetween BDI-driven and symbolic RL-driven planning phases, and consistent\nmission performance. Specifically, we evaluate a target acquisition scenario in\nwhich the UAV plans a surveillance path followed by a dynamic reentry path to\nsecure the target while avoiding threat zones. In this SIL evaluation, mission\nefficiency improved by approximately 75% over a coverage-based baseline,\nmeasured by travel distance reduction. This study establishes a robust\nfoundation for handling complex UAV missions and discusses directions for\nfurther enhancement and validation."}
{"id": "2508.11685", "pdf": "https://arxiv.org/pdf/2508.11685", "abs": "https://arxiv.org/abs/2508.11685", "authors": ["Farnaz Kaboudvand", "Maham Khalid", "Nydia Assaf", "Vardaan Sahgal", "Jon P. Ruffley", "Brian J. McDermott"], "title": "Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling", "categories": ["eess.SP", "cond-mat.mtrl-sci", "cs.LG", "stat.ML"], "comment": "Manuscript length: 11 pages, 6 figures", "summary": "Corrosion poses a significant challenge to the performance of aluminum\nalloys, particularly in marine environments. This study investigates the\napplication of machine learning (ML) algorithms to predict and optimize\ncorrosion resistance, utilizing a comprehensive open-source dataset compiled\nfrom various sources. The dataset encompasses corrosion rate data and\nenvironmental conditions, preprocessed to standardize units and formats. We\nexplored two different approaches, a direct approach, where the material's\ncomposition and environmental conditions were used as inputs to predict\ncorrosion rates; and an inverse approach, where corrosion rate served as the\ninput to identify suitable material compositions as output. We employed and\ncompared three distinct ML methodologies for forward predictions: Random Forest\nregression, optimized via grid search; a feed-forward neural network, utilizing\nReLU activation and Adam optimization; and Gaussian Process Regression (GPR),\nimplemented with GPyTorch and employing various kernel functions. The Random\nForest and neural network models provided predictive capabilities based on\nelemental compositions and environmental conditions. Notably, Gaussian Process\nRegression demonstrated superior performance, particularly with hybrid kernel\nfunctions. Log-transformed GPR further refined predictions. This study\nhighlights the efficacy of ML, particularly GPR, in predicting corrosion rates\nand material properties."}
{"id": "2508.11898", "pdf": "https://arxiv.org/pdf/2508.11898", "abs": "https://arxiv.org/abs/2508.11898", "authors": ["Jilei Mao", "Jiarui Guan", "Yingjuan Tang", "Qirui Hu", "Zhihang Li", "Junjie Yu", "Yongjie Mao", "Yunzhe Sun", "Shuang Liu", "Xiaozhu Ju"], "title": "OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation", "categories": ["cs.RO"], "comment": null, "summary": "The visuomotor policy can easily overfit to its training datasets, such as\nfixed camera positions and backgrounds. This overfitting makes the policy\nperform well in the in-distribution scenarios but underperform in the\nout-of-distribution generalization. Additionally, the existing methods also\nhave difficulty fusing multi-view information to generate an effective 3D\nrepresentation. To tackle these issues, we propose Omni-Vision Diffusion Policy\n(OmniD), a multi-view fusion framework that synthesizes image observations into\na unified bird's-eye view (BEV) representation. We introduce a deformable\nattention-based Omni-Feature Generator (OFG) to selectively abstract\ntask-relevant features while suppressing view-specific noise and background\ndistractions. OmniD achieves 11\\%, 17\\%, and 84\\% average improvement over the\nbest baseline model for in-distribution, out-of-distribution, and few-shot\nexperiments, respectively. Training code and simulation benchmark are\navailable: https://github.com/1mather/omnid.git"}
{"id": "2508.11686", "pdf": "https://arxiv.org/pdf/2508.11686", "abs": "https://arxiv.org/abs/2508.11686", "authors": ["Shuai Jiao", "Jian Fang", "Tianshu Zhou", "Jinsong Li", "Yanhong Liu", "Ye Liu", "Ming Ju"], "title": "The Lost-K and Shorter-J Phenomenon in Non-Standard Ballistocardiography Data", "categories": ["eess.SP"], "comment": null, "summary": "Non-standard ballistocardiogram(BCG) data generally do not have prominent J\npeaks. This paper introduces two phenomena that reduce the prominence of\nJpeaks: the shorter-J phenomenon and the lost-K phenomenon, both of which are\ncommonly observed in non-standard BCG signals . This paper also proposes three\nsignal transformation methods that effectively improve the lost-K and shorter-J\nphenomena. The methods were evaluated on a time-aligned ECG-BCG dataset with 40\nsubjects. The results show that based on the transformed signal, simple\nJ-peak-based methods using only the detection of local maxima or minima show\nbetter performance in locating J-peaks and extracting BCG cycles, especially\nfor non-standard BCG data."}
{"id": "2508.11917", "pdf": "https://arxiv.org/pdf/2508.11917", "abs": "https://arxiv.org/abs/2508.11917", "authors": ["Hossein Keshavarz", "Alejandro Ramirez-Serrano", "Majid Khadiv"], "title": "Control of Legged Robots using Model Predictive Optimized Path Integral", "categories": ["cs.RO"], "comment": "8 pages, 13 figures, Humanoid conference", "summary": "Legged robots possess a unique ability to traverse rough terrains and\nnavigate cluttered environments, making them well-suited for complex,\nreal-world unstructured scenarios. However, such robots have not yet achieved\nthe same level as seen in natural systems. Recently, sampling-based predictive\ncontrollers have demonstrated particularly promising results. This paper\ninvestigates a sampling-based model predictive strategy combining model\npredictive path integral (MPPI) with cross-entropy (CE) and covariance matrix\nadaptation (CMA) methods to generate real-time whole-body motions for legged\nrobots across multiple scenarios. The results show that combining the benefits\nof MPPI, CE and CMA, namely using model predictive optimized path integral\n(MPOPI), demonstrates greater sample efficiency, enabling robots to attain\nsuperior locomotion results using fewer samples when compared to typical MPPI\nalgorithms. Extensive simulation experiments in multiple scenarios on a\nquadruped robot show that MPOPI can be used as an anytime control strategy,\nincreasing locomotion capabilities at each iteration."}
{"id": "2508.11687", "pdf": "https://arxiv.org/pdf/2508.11687", "abs": "https://arxiv.org/abs/2508.11687", "authors": ["Jingpu Yang", "Mingxuan Cui", "Hang Zhang", "Fengxian Ji", "Zhengzhao Lai", "Yufeng Wang"], "title": "Agent-Based Anti-Jamming Techniques for UAV Communications in Adversarial Environments: A Comprehensive Survey", "categories": ["eess.SP", "cs.GT"], "comment": null, "summary": "Unmanned Aerial Vehicle communications are encountering increasingly severe\nmulti-source interference challenges in dynamic adversarial environments, which\nimpose higher demands on their reliability and resilience. To address these\nchallenges, agent-based autonomous anti-jamming techniques have emerged as a\ncrucial research direction. This paper presents a comprehensive survey that\nfirst formalizes the concept of intelligent anti-jamming agents for UAV\ncommunications and establishes a closed-loop decision-making framework centered\non the \"Perception-Decision-Action\" (P-D-A) paradigm. Within this framework, we\nsystematically review key technologies at each stage, with particular emphasis\non employing game theory to model UAV-jammer interactions and integrating\nreinforcement learning-based intelligent algorithms to derive adaptive\nanti-jamming strategies. Furthermore, we discuss potential limitations of\ncurrent approaches, identify critical engineering challenges, and outline\npromising future research directions, aiming to provide valuable references for\ndeveloping more intelligent and robust anti-jamming communication systems for\nUAVs."}
{"id": "2508.11918", "pdf": "https://arxiv.org/pdf/2508.11918", "abs": "https://arxiv.org/abs/2508.11918", "authors": ["Zhichen Lou", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong"], "title": "ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models", "categories": ["cs.RO"], "comment": null, "summary": "The advancement of embodied intelligence is accelerating the integration of\nrobots into daily life as human assistants. This evolution requires robots to\nnot only interpret high-level instructions and plan tasks but also perceive and\nadapt within dynamic environments. Vision-Language Models (VLMs) present a\npromising solution by combining visual understanding and language reasoning.\nHowever, existing VLM-based methods struggle with interactive exploration,\naccurate perception, and real-time plan adaptation. To address these\nchallenges, we propose ExploreVLM, a novel closed-loop task planning framework\npowered by Vision-Language Models (VLMs). The framework is built around a\nstep-wise feedback mechanism that enables real-time plan adjustment and\nsupports interactive exploration. At its core is a dual-stage task planner with\nself-reflection, enhanced by an object-centric spatial relation graph that\nprovides structured, language-grounded scene representations to guide\nperception and planning. An execution validator supports the closed loop by\nverifying each action and triggering re-planning. Extensive real-world\nexperiments demonstrate that ExploreVLM significantly outperforms\nstate-of-the-art baselines, particularly in exploration-centric tasks. Ablation\nstudies further validate the critical role of the reflective planner and\nstructured perception in achieving robust and efficient task execution."}
{"id": "2508.11691", "pdf": "https://arxiv.org/pdf/2508.11691", "abs": "https://arxiv.org/abs/2508.11691", "authors": ["Mathis Rezzouk", "Fabrice Gagnon", "Alyson Champagne", "Mathieu Roy", "Philippe Albouy", "Michel-Pierre Coll", "Cem Subakan"], "title": "Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 2 tables, MLSP IEEE conference", "summary": "EEG-based analysis of pain perception, enhanced by machine learning, reveals\nhow the brain encodes pain by identifying neural patterns evoked by noxious\nstimulation. However, a major challenge that remains is the generalization of\nmachine learning models across individuals, given the high cross-participant\nvariability inherent to EEG signals and the limited focus on direct pain\nperception identification in current research. In this study, we systematically\nevaluate the performance of cross-participant generalization of a wide range of\nmodels, including traditional classifiers and deep neural classifiers for\nidentifying the sensory modality of thermal pain and aversive auditory\nstimulation from EEG recordings. Using a novel dataset of EEG recordings from\n108 participants, we benchmark model performance under both within- and\ncross-participant evaluation settings. Our findings show that traditional\nmodels suffered the largest drop from within- to cross-participant performance,\nwhile deep learning models proved more resilient, underscoring their potential\nfor subject-invariant EEG decoding. Even though performance variability\nremained high, the strong results of the graph-based model highlight its\npotential to capture subject-invariant structure in EEG signals. On the other\nhand, we also share the preprocessed dataset used in this study, providing a\nstandardized benchmark for evaluating future algorithms under the same\ngeneralization constraints."}
{"id": "2508.11929", "pdf": "https://arxiv.org/pdf/2508.11929", "abs": "https://arxiv.org/abs/2508.11929", "authors": ["Mohitvishnu S. Gadde", "Pranay Dugar", "Ashish Malik", "Alan Fern"], "title": "No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Effective bipedal locomotion in dynamic environments, such as cluttered\nindoor spaces or uneven terrain, requires agile and adaptive movement in all\ndirections. This necessitates omnidirectional terrain sensing and a controller\ncapable of processing such input. We present a learning framework for\nvision-based omnidirectional bipedal locomotion, enabling seamless movement\nusing depth images. A key challenge is the high computational cost of rendering\nomnidirectional depth images in simulation, making traditional sim-to-real\nreinforcement learning (RL) impractical. Our method combines a robust blind\ncontroller with a teacher policy that supervises a vision-based student policy,\ntrained on noise-augmented terrain data to avoid rendering costs during RL and\nensure robustness. We also introduce a data augmentation technique for\nsupervised student training, accelerating training by up to 10 times compared\nto conventional methods. Our framework is validated through simulation and\nreal-world tests, demonstrating effective omnidirectional locomotion with\nminimal reliance on expensive rendering. This is, to the best of our knowledge,\nthe first demonstration of vision-based omnidirectional bipedal locomotion,\nshowcasing its adaptability to diverse terrains."}
{"id": "2508.11692", "pdf": "https://arxiv.org/pdf/2508.11692", "abs": "https://arxiv.org/abs/2508.11692", "authors": ["Eduardo Di Santi", "Ruixiang Ci", "Clment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Jonathan Brown", "Victor Martn", "Kenza Saiah"], "title": "Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning", "categories": ["eess.SP", "cs.AI", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025, Dresden,\n  Germany. Conference: https://tu-dresden.de/raildresden2025. Book of\n  abstracts: https://tu-dresden.de/raildresden2025/BoA.pdf. 8 pages, 6 figures,\n  1 table", "summary": "The Point Machine (PM) is a critical piece of railway equipment that switches\ntrain routes by diverting tracks through a switchblade. As with any critical\nsafety equipment, a failure will halt operations leading to service\ndisruptions; therefore, pre-emptive maintenance may avoid unnecessary\ninterruptions by detecting anomalies before they become failures. Previous work\nrelies on several inputs and crafting custom features by segmenting the signal.\nThis not only adds additional requirements for data collection and processing,\nbut it is also specific to the PM technology, the installed locations and\noperational conditions limiting scalability. Based on the available maintenance\nrecords, the main failure causes for PM are obstacles, friction, power source\nissues and misalignment. Those failures affect the energy consumption pattern\nof PMs, altering the usual (or healthy) shape of the power signal during the PM\nmovement. In contrast to the current state-of-the-art, our method requires only\none input. We apply a deep learning model to the power signal pattern to\nclassify if the PM is nominal or associated with any failure type, achieving\n>99.99\\% precision, <0.01\\% false positives and negligible false negatives. Our\nmethodology is generic and technology-agnostic, proven to be scalable on\nseveral electromechanical PM types deployed in both real-world and test bench\nenvironments. Finally, by using conformal prediction the maintainer gets a\nclear indication of the certainty of the system outputs, adding a confidence\nlayer to operations and making the method compliant with the ISO-17359\nstandard."}
{"id": "2508.11960", "pdf": "https://arxiv.org/pdf/2508.11960", "abs": "https://arxiv.org/abs/2508.11960", "authors": ["Sandeep Kanta", "Mehrdad Tavassoli", "Varun Teja Chirkuri", "Venkata Akhil Kumar", "Santhi Bharath Punati", "Praveen Damacharla", "Sunny Katyara"], "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation", "categories": ["cs.RO"], "comment": "Advanced Engineering Informatics", "summary": "Agile and human-centric manufacturing stipulates resilient robotic solutions\ncapable of contextual reasoning and safe interaction in unstructured\nenvironments. Foundation models particularly the Vision Language Action (VLA)\nmodels have emerged to fuse multimodal perception, reasoning and physically\ngrounded action across varied embodiments into unified representation, termed\nas General Physical Intelligence (GPI). While GPI has already been described in\nthe literature but its practical application and evolving role in contemporary\nagile manufacturing processes have yet to be duly explored. To bridge this gap,\nthis practical review systematically surveys recent advancements in VLA models\nwithin GPI context, performs comprehensive comparative analysis of leading\nimplementations and evaluates their readiness for industrial deployment through\nstructured ablation study. Our analysis has organized state-of-the-art into\nfive thematic pillars including multisensory representation learning, sim2real\ntransfer, planning and control, uncertainty and safety measures and\nbenchmarking. Finally, we articulate open research challenges and propose\ndirections to better integrate GPI into next-generation industrial ecosystems\nin line with Industry 5.0."}
{"id": "2508.11693", "pdf": "https://arxiv.org/pdf/2508.11693", "abs": "https://arxiv.org/abs/2508.11693", "authors": ["Francisco Lpez", "Eduardo Di Santi", "Clment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Victor Martn", "Kenza Saiah"], "title": "Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data", "categories": ["eess.SP", "cs.AI", "cs.LG", "68T05, 68T10", "I.2.6; I.5.1; I.5.4"], "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany", "summary": "Track Circuits (TC) are the main signalling devices used to detect the\npresence of a train on a rail track. It has been used since the 19th century\nand nowadays there are many types depending on the technology. As a general\nclassification, Track Circuits can be divided into 2 main groups, DC (Direct\nCurrent) and AC (Alternating Current) circuits. This work is focused on a\nparticular AC track circuit, called \"Smart Train Detection System\" (STDS),\ndesigned with both high and low-frequency bands. This approach uses STDS\ncurrent data applied to an SVM (support vector machine) classifier as a type of\nfailure identifier. The main purpose of this work consists on determine\nautomatically which is the component of the track that is failing to improve\nthe maintenance action. Model was trained to classify 15 different failures\nthat belong to 3 more general categories. The method was tested with field data\nfrom 10 different track circuits and validated by the STDS track circuit expert\nand maintainers. All use cases were correctly classified by the method."}
{"id": "2508.12038", "pdf": "https://arxiv.org/pdf/2508.12038", "abs": "https://arxiv.org/abs/2508.12038", "authors": ["Liwen Zhang", "Heng Deng", "Guanghui Sun"], "title": "Fully Spiking Actor-Critic Neural Network for Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This study proposes a hybrid curriculum reinforcement learning (CRL)\nframework based on a fully spiking neural network (SNN) for 9-degree-of-freedom\nrobotic arms performing target reaching and grasping tasks. To reduce network\ncomplexity and inference latency, the SNN architecture is simplified to include\nonly an input and an output layer, which shows strong potential for\nresource-constrained environments. Building on the advantages of SNNs-high\ninference speed, low energy consumption, and spike-based biological\nplausibility, a temporal progress-partitioned curriculum strategy is integrated\nwith the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy\nconsumption modeling framework is introduced to quantitatively compare the\ntheoretical energy consumption between SNNs and conventional Artificial Neural\nNetworks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized\nobservation space further improve learning efficiency and policy accuracy.\nExperiments on the Isaac Gym simulation platform demonstrate that the proposed\nmethod achieves superior performance under realistic physical constraints.\nComparative evaluations with conventional PPO and ANN baselines validate the\nscalability and energy efficiency of the proposed approach in dynamic robotic\nmanipulation tasks."}
{"id": "2508.11700", "pdf": "https://arxiv.org/pdf/2508.11700", "abs": "https://arxiv.org/abs/2508.11700", "authors": ["Mesut Koyiit", "Bahman Javadi", "Russell Thomson", "Sebastian Pfautsch", "Oliver Obst"], "title": "Operational machine learning for park-scale irrigation to support urban cooling", "categories": ["eess.SP"], "comment": "6 pages, 3 figures", "summary": "Urban parks can mitigate local heat, yet irrigation control is usually tuned\nfor water savings rather than cooling. We report on SIMPaCT (Smart Irrigation\nManagement for Parks and Cool Towns), a park-scale deployment that links\nper-zone soil-moisture forecasts to overnight irrigation set-points in support\nof urban cooling. SIMPaCT ingests data from 202 soil-moisture sensors, 50\ntemperature-relative humidity (TRH) nodes, and 13 weather stations, and trains\na per-sensor k-nearest neighbours (kNN) predictor on short rolling windows\n(200-900h). A rule-first anomaly pipeline screens missing and stuck-at signals,\nwith model-based checks (Isolation Forest and ARIMA). When a device fails, a\nmutual-information neighbourhood selects the most informative neighbour and a\nsmall multilayer perceptron supplies a \"virtual sensor\" until restoration.\nAcross sensors the mean absolute error was 0.78%, comparable to more complex\nbaselines; the upper-quartile error (P75) was lower for kNN than SARIMA (0.71%\nvs 0.93%). SIMPaCT runs daily and writes proposed set-points to the existing\ncontroller for operator review. This short communication reports an operational\nrecipe for robust, cooling-oriented irrigation at city-park scale."}
{"id": "2508.12043", "pdf": "https://arxiv.org/pdf/2508.12043", "abs": "https://arxiv.org/abs/2508.12043", "authors": ["Fei Lin", "Tengchao Zhang", "Qinghua Ni", "Jun Huang", "Siji Ma", "Yonglin Tian", "Yisheng Lv", "Naiqi Wu"], "title": "Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs", "categories": ["cs.RO"], "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) in unmanned systems has\nsignificantly enhanced the semantic understanding and autonomous task execution\ncapabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited\ncommunication bandwidth and the need for high-frequency interactions pose\nsevere challenges to semantic information transmission within the swarm. This\npaper explores the feasibility of LLM-driven UAV swarms for autonomous semantic\ncompression communication, aiming to reduce communication load while preserving\ncritical task semantics. To this end, we construct four types of 2D simulation\nscenarios with different levels of environmental complexity and design a\ncommunication-execution pipeline that integrates system prompts with task\ninstruction prompts. On this basis, we systematically evaluate the semantic\ncompression performance of nine mainstream LLMs in different scenarios and\nanalyze their adaptability and stability through ablation studies on\nenvironmental complexity and swarm size. Experimental results demonstrate that\nLLM-based UAV swarms have the potential to achieve efficient collaborative\ncommunication under bandwidth-constrained and multi-hop link conditions."}
{"id": "2508.11790", "pdf": "https://arxiv.org/pdf/2508.11790", "abs": "https://arxiv.org/abs/2508.11790", "authors": ["Oveys Delafrooz Noroozi", "Jiyoon Han", "Wei Tang", "Zhengya Zhang", "Upamanyu Madhow"], "title": "Scaling Wideband Massive MIMO Radar via Beamspace Dimension Reduction", "categories": ["eess.SP"], "comment": null, "summary": "We present an architecture for scaling digital beamforming for wideband\nmassive MIMO radar. Conventional spatial processing becomes computationally\nprohibitive as array size grows; for example, the computational complexity of\nMVDR beamforming scales as O(N^3) for an N-element array. In this paper, we\nshow that energy concentration in beamspace provides the basis for drastic\ncomplexity reduction, with array scaling governed by the O(NlogN) complexity of\nthe spatial FFT used for beamspace transformation. Specifically, we propose an\narchitecture for windowed beamspace MVDR beamforming, parallelized across\ntargets and subbands, and evaluate its efficacy for beamforming and\ninterference suppression for government-supplied wideband radar data from the\nDARPA SOAP (Scalable On-Array Processing) program. We demonstrate that our\napproach achieves detection performance comparable to full-dimensional\nbenchmarks while significantly reducing computational and training overhead,\nand provide insight into tradeoffs between beamspace window size and FFT\nresolution in balancing complexity, detection accuracy, and interference\nsuppression."}
{"id": "2508.12071", "pdf": "https://arxiv.org/pdf/2508.12071", "abs": "https://arxiv.org/abs/2508.12071", "authors": ["Amy Phung", "Richard Camilli"], "title": "OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments", "categories": ["cs.RO"], "comment": "This paper has been accepted for publication in IROS 2025. Copyright\n  IEEE", "summary": "High resolution underwater 3D scene reconstruction is crucial for various\napplications, including construction, infrastructure maintenance, monitoring,\nexploration, and scientific investigation. Prior work has leveraged the\ncomplementary sensing modalities of imaging sonars and optical cameras for\nopti-acoustic 3D scene reconstruction, demonstrating improved results over\nmethods which rely solely on either sensor. However, while most existing\napproaches focus on offline reconstruction, real-time spatial awareness is\nessential for both autonomous and piloted underwater vehicle operations. This\npaper presents OASIS, an opti-acoustic fusion method that integrates data from\noptical images with voxel carving techniques to achieve real-time 3D\nreconstruction unstructured underwater workspaces. Our approach utilizes an\n\"eye-in-hand\" configuration, which leverages the dexterity of robotic\nmanipulator arms to capture multiple workspace views across a short baseline.\nWe validate OASIS through tank-based experiments and present qualitative and\nquantitative results that highlight its utility for underwater manipulation\ntasks."}
{"id": "2508.11792", "pdf": "https://arxiv.org/pdf/2508.11792", "abs": "https://arxiv.org/abs/2508.11792", "authors": ["Daniel Schufele", "Jochen Fink", "Renato L. G. Cavalcante", "Sawomir Staczak"], "title": "Digital Post-Distortion Architectures for Nonlinear Power Amplifiers: Volterra and Kernel Methods", "categories": ["eess.SP"], "comment": null, "summary": "In modern 5G user equipments (UEs), the power amplifier (PA) contributes\nsignificantly to power consumption during uplink transmissions, especially in\ncell-edge scenarios. While reducing power backoff can enhance PA efficiency, it\nintroduces nonlinear distortions that degrade signal quality. Existing\nsolutions, such as digital pre-distortion, require complex feedback mechanisms\nfor optimal performance, leading to increased UE complexity and power\nconsumption. Instead, in this study we explore digital post-distortion (DPoD)\ntechniques, which compensate for these distortions at the base station,\nleveraging its superior computational resources. In this study, we conduct an\ncomprehensive study concerning the challenges and advantages associated with\napplying DPoD in time-domain, frequency-domain, and DFT-s-domain. Our findings\nsuggest that implementing DPoD in the time-domain, complemented by\nfrequency-domain channel equalization, strikes a good balance between low\ncomputational complexity and efficient nonlinearity compensation. In addition,\nwe demonstrate that memory has to be taken into account regardless of the\nmemory of the PA. Subsequently, we show how to pose the complex-valued problem\nof nonlinearity compensation in a real Hilbert space, emphasizing the potential\nperformance enhancements as a result. We then discuss the traditional Volterra\nseries and show an equivalent kernel method that can reduce algorithmic\ncomplexity. Simulations validate the results of our analysis and show that our\nproposed algorithm can significantly improve performance compared to\nstate-of-the-art algorithms."}
{"id": "2508.12075", "pdf": "https://arxiv.org/pdf/2508.12075", "abs": "https://arxiv.org/abs/2508.12075", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "title": "Into the Wild: When Robots Are Not Welcome", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies."}
{"id": "2508.12012", "pdf": "https://arxiv.org/pdf/2508.12012", "abs": "https://arxiv.org/abs/2508.12012", "authors": ["Yi Wang", "Yingyang Chen", "Li Wang", "Donghong Cai", "Xiaofan Li", "Pingzhi Fan"], "title": "Autonomous Driving with RSMA-Enabled Finite Blocklength Transmissions: Ergodic Performance Analysis and Optimization", "categories": ["eess.SP"], "comment": "This work has been accepted by IEEE Transactions on Wireless\n  Communications", "summary": "Rate-splitting multiple access (RSMA) is a key technology for next-generation\nmultiple access systems due to its robustness against imperfect channel state\ninformation (CSI). This makes RSMA particularly suitable for high-mobility\nautonomous driving, where ultra-reliable and low-latency communication (URLLC)\nis essential. To address the stringent requirements, this study enables RSMA\nfinite blocklength (FBL) transmissions and explicitly evaluates the ergodic\nperformance. We derive the closed-form lower bound for the ergodic sum-rate of\nRSMA, considering vital factors such as the vehicle velocities, vehicle\npositions, power allocation of each stream, blocklengths, and block error rates\n(BLERs). To further enhance the ergodic sum-rate while complying with quality\nof service (QoS) rate constraints, we jointly optimize the global power\ncoefficient, private power distribution, and common rate splitting. Guided by\ngradient descent, we first adjust the global power coefficient based on its\nsum-rate solution. This parameter regulates the power state of the common\nstream, allowing for dynamic activation or deactivation: if active, we optimize\nthe private power distribution and adjust the common rate splitting to meet\nminimum transmission constraints; if inactive, we use the sequential quadratic\nprogramming for private power distribution optimization. Simulation results\nconfirm that our RSMA scheme significantly improves the ergodic performance,\nreduces blocklength and BLER, surpassing the RSMA counterpart with average\nprivate power and space division multiple access (SDMA). Furthermore, our\napproach is validated to guarantee the rates for users with the poorest channel\nconditions, thereby enhancing fairness across the network."}
{"id": "2508.12166", "pdf": "https://arxiv.org/pdf/2508.12166", "abs": "https://arxiv.org/abs/2508.12166", "authors": ["Gokul Puthumanaillam", "Aditya Penumarti", "Manav Vora", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Jane Shin", "Melkior Ornik"], "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to CoRL 2025 (Conference on Robot Learning)", "summary": "Robots equipped with rich sensor suites can localize reliably in\npartially-observable environments, but powering every sensor continuously is\nwasteful and often infeasible. Belief-space planners address this by\npropagating pose-belief covariance through analytic models and switching\nsensors heuristically--a brittle, runtime-expensive approach. Data-driven\napproaches--including diffusion models--learn multi-modal trajectories from\ndemonstrations, but presuppose an accurate, always-on state estimate. We\naddress the largely open problem: for a given task in a mapped environment,\nwhich \\textit{minimal sensor subset} must be active at each location to\nmaintain state uncertainty \\textit{just low enough} to complete the task? Our\nkey insight is that when a diffusion planner is explicitly conditioned on a\npose-belief raster and a sensor mask, the spread of its denoising trajectories\nyields a calibrated, differentiable proxy for the expected localisation error.\nBuilding on this insight, we present Belief-Conditioned One-Step Diffusion\n(B-COD), the first planner that, in a 10 ms forward pass, returns a\nshort-horizon trajectory, per-waypoint aleatoric variances, and a proxy for\nlocalisation error--eliminating external covariance rollouts. We show that this\nsingle proxy suffices for a soft-actor-critic to choose sensors online,\noptimising energy while bounding pose-covariance growth. We deploy B-COD in\nreal-time marine trials on an unmanned surface vehicle and show that it reduces\nsensing energy consumption while matching the goal-reach performance of an\nalways-on baseline."}
{"id": "2508.12099", "pdf": "https://arxiv.org/pdf/2508.12099", "abs": "https://arxiv.org/abs/2508.12099", "authors": ["Guangpu Guo", "Xiang-Gen Xia"], "title": "A Generalized Multidimensional Chinese Remainder Theorem (MD-CRT) for Multiple Integer Vectors", "categories": ["eess.SP"], "comment": null, "summary": "Chinese remainder theorem (CRT) is widely applied in cryptography, coding\ntheory, and signal processing. It has been extended to the multidimensional CRT\n(MD-CRT), which reconstructs an integer vector from its vector remainders\nmodulo multiple integer matrices. This paper investigates a generalized MD-CRT\nfor multiple integer vectors, where the goal is to determine multiple integer\nvectors from multiple vector residue sets modulo multiple integer\nmatrices.Comparing to the existing generalized CRT for multiple scalar\nintegers, the challenge is that the moduli in MD-CRT are matrices that do not\ncommute and the corresponding uniquely determinable range is multidimensional\nand the inclusion relationship is much more complicated. In this paper,we\naddress two fundamental questions regarding the generalized MD-CRT. The first\nquestion concerns the uniquely determinable range of multiple integer vectors\nwhen no prior information about them is available. The second question is about\nthe conditions under which the maximal possible dynamic range can be\nachieved.To answer these two questions, we first derive a uniquely determinable\nrange without prior information and accordingly propose an algorithm to achieve\nit. A special case involving only two integer vectors is investigated for the\nsecond question, leading to a new condition for achieving the maximal possible\ndynamic range. Interestingly, this newly obtained condition, when the dimension\nis reduced to $1$, is even better than the existing ones for the conventional\ngeneralized CRT for scalar integers.These results may have applications for\nfrequency detection in multidimensional signal processing."}
{"id": "2508.12170", "pdf": "https://arxiv.org/pdf/2508.12170", "abs": "https://arxiv.org/abs/2508.12170", "authors": ["Aryan Gupta"], "title": "Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)", "categories": ["cs.RO"], "comment": null, "summary": "This study presents a systematic literature review of software-level\napproaches to energy efficiency in robotics published from 2020 through 2024,\nupdating and extending pre-2020 evidence. An automated-but-audited pipeline\ncombined Google Scholar seeding, backward/forward snowballing, and\nlarge-language-model (LLM) assistance for screening and data extraction, with\n~10% human audits at each automated step and consensus-with-tie-breaks for\nfull-text decisions. The final corpus comprises 79 peer-reviewed studies\nanalyzed across application domain, metrics, evaluation type, energy models,\nmajor energy consumers, software technique families, and energy-quality\ntrade-offs. Industrial settings dominate (31.6%) followed by exploration\n(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of\nstudies, with computing/controllers a distant second (13.9%). Simulation-only\nevaluations remain most common (51.9%), though hybrid evaluations are frequent\n(25.3%). Representational (physics-grounded) energy models predominate (87.3%).\nMotion and trajectory optimization is the leading technique family (69.6%),\noften paired with learning/prediction (40.5%) and computation\nallocation/scheduling (26.6%); power management/idle control (11.4%) and\ncommunication/data efficiency (3.8%) are comparatively underexplored. Reporting\nis heterogeneous: composite objectives that include energy are most common,\nwhile task-normalized and performance-per-energy metrics appear less often,\nlimiting cross-paper comparability. The review offers a minimal reporting\nchecklist (e.g., total energy and average power plus a task-normalized metric\nand clear baselines) and highlights opportunities in cross-layer designs and in\nquantifying non-performance trade-offs (accuracy, stability). A replication\npackage with code, prompts, and frozen datasets accompanies the review."}
{"id": "2508.12106", "pdf": "https://arxiv.org/pdf/2508.12106", "abs": "https://arxiv.org/abs/2508.12106", "authors": ["Hao Chen", "Rui Jin", "Dayuan Tan"], "title": "RFSS: A Comprehensive Multi-Standard RF Signal Source Separation Dataset with Advanced Channel Modeling", "categories": ["eess.SP"], "comment": null, "summary": "The rapid evolution of wireless communication systems has created complex\nelectromagnetic environments where multiple cellular standards (2G/3G/4G/5G)\ncoexist, necessitating advanced signal source separation techniques. We present\nRFSS (RF Signal Source Separation), a comprehensive open-source dataset\ncontaining 52,847 realistic multi-standard RF signal samples with complete 3GPP\nstandards compliance. Our framework generates authentic baseband signals for\nGSM, UMTS, LTE, and 5G NR with advanced channel modeling including multipath\nfading, MIMO processing up to 8 by 8 antennas, and realistic interference\nscenarios. Experimental validation demonstrates superior performance of\nCNN-LSTM architectures achieving 26.7 dB SINR improvement in source separation\ntasks, significantly outperforming traditional ICA (15.2 dB) and NMF (18.3 dB)\napproaches. The RFSS dataset enables reproducible research in RF source\nseparation, cognitive radio, and machine learning applications while\nmaintaining complete open-source accessibility"}
{"id": "2508.12184", "pdf": "https://arxiv.org/pdf/2508.12184", "abs": "https://arxiv.org/abs/2508.12184", "authors": ["Rhea Malhotra", "William Chong", "Catie Cuan", "Oussama Khatib"], "title": "Humanoid Motion Scripting with Postural Synergies", "categories": ["cs.RO"], "comment": null, "summary": "Generating sequences of human-like motions for humanoid robots presents\nchallenges in collecting and analyzing reference human motions, synthesizing\nnew motions based on these reference motions, and mapping the generated motion\nonto humanoid robots. To address these issues, we introduce SynSculptor, a\nhumanoid motion analysis and editing framework that leverages postural\nsynergies for training-free human-like motion scripting. To analyze human\nmotion, we collect 3+ hours of motion capture data across 20 individuals where\na real-time operational space controller mimics human motion on a simulated\nhumanoid robot. The major postural synergies are extracted using principal\ncomponent analysis (PCA) for velocity trajectories segmented by changes in\nrobot momentum, constructing a style-conditioned synergy library for free-space\nmotion generation. To evaluate generated motions using the synergy library, the\nfoot-sliding ratio and proposed metrics for motion smoothness involving total\nmomentum and kinetic energy deviations are computed for each generated motion,\nand compared with reference motions. Finally, we leverage the synergies with a\nmotion-language transformer, where the humanoid, during execution of motion\ntasks with its end-effectors, adapts its posture based on the chosen synergy.\nSupplementary material, code, and videos are available at\nhttps://rhea-mal.github.io/humanoidsynergies.io."}
{"id": "2508.12114", "pdf": "https://arxiv.org/pdf/2508.12114", "abs": "https://arxiv.org/abs/2508.12114", "authors": ["Mustafa Gusaibat", "Mohammed Hnaish", "Abdelhamid Salem", "Khaled Rabie", "Zubair Md Fadlullah", "Wali Ullah Khan", "Mohamad A. Alawad", "Yazeed Alkhrijah"], "title": "Effect of Phase Shift Errors on the Security of UAV-assisted STAR-RIS IoT Networks", "categories": ["eess.SP"], "comment": null, "summary": "Unmanned aerial vehicles (UAV)-mounted simultaneous transmitting and\nreflecting reconfigurable intelligent surface (STAR-RIS) systems can provide\nfull-dimensional coverage and flexible deployment opportunities in future\n6G-enabled IoT networks. However, practical imperfections such as jittering and\nairflow of UAV could affect the phase shift of STAR-RIS, and consequently\ndegrade network security. In this respect, this paper investigates the impact\nof phase shift errors on the secrecy performance of UAV-mounted\nSTAR-RIS-assisted IoT systems. More specifically, we consider a UAV-mounted\nSTAR-RIS-assisted non-orthogonal multiple access (NOMA) system where IoT\ndevices are grouped into two groups: one group on each side of the STAR-RIS.\nThe nodes in each group are considered as potential Malicious nodes for the\nones on the other side. By modeling phase estimation errors using a von Mises\ndistribution, analytical closed-form expressions for the ergodic secrecy rates\nunder imperfect phase adjustment are derived. An optimization problem to\nmaximize the weighted sum secrecy rate (WSSR) by optimizing the UAV placement\nis formulated and is then solved using a linear grid-based algorithm. Monte\nCarlo simulations are provided to validate the analytical derivations. The\nimpact of phase estimation errors on system's secrecy performance is analyzed,\nproviding critical insights for the practical realisation of STAR-RIS\ndeployments for secure UAV-enabled IoT networks."}
{"id": "2508.12189", "pdf": "https://arxiv.org/pdf/2508.12189", "abs": "https://arxiv.org/abs/2508.12189", "authors": ["Rhea Malhotra", "Yuejiang Liu", "Chelsea Finn"], "title": "Self-Guided Action Diffusion", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent works have shown the promise of inference-time search over action\nsamples for improving generative robot policies. In particular, optimizing\ncross-chunk coherence via bidirectional decoding has proven effective in\nboosting the consistency and reactivity of diffusion policies. However, this\napproach remains computationally expensive as the diversity of sampled actions\ngrows. In this paper, we introduce self-guided action diffusion, a more\nefficient variant of bidirectional decoding tailored for diffusion-based\npolicies. At the core of our method is to guide the proposal distribution at\neach diffusion step based on the prior decision. Experiments in simulation\ntasks show that the proposed self-guidance enables near-optimal performance at\nnegligible inference cost. Notably, under a tight sampling budget, our method\nachieves up to 70% higher success rates than existing counterparts on\nchallenging dynamic tasks. See project website at\nhttps://rhea-mal.github.io/selfgad.github.io."}
{"id": "2508.12204", "pdf": "https://arxiv.org/pdf/2508.12204", "abs": "https://arxiv.org/abs/2508.12204", "authors": ["Mauro Belgiovine", "Suyash Pradhan", "Johannes Lange", "Michael Lhning", "Kaushik Chowdhury"], "title": "ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search", "categories": ["eess.SP", "cs.LG", "cs.NI"], "comment": "Accepted at IEEE PIMRC 2025", "summary": "Industry adoption of Artificial Intelligence (AI)-native wireless receivers,\nor even modular, Machine Learning (ML)-aided wireless signal processing blocks,\nhas been slow. The main concern is the lack of explainability of these trained\nML models and the significant risks posed to network functionalities in case of\nfailures, especially since (i) testing on every exhaustive case is infeasible\nand (ii) the data used for model training may not be available. This paper\nproposes ATLAS, an AI-guided approach that generates a battery of tests for\npre-trained AI-native receiver models and benchmarks the performance against a\nclassical receiver architecture. Using gradient-based optimization, it avoids\nspanning the exhaustive set of all environment and channel conditions; instead,\nit generates the next test in an online manner to further probe specific\nconfigurations that offer the highest risk of failure. We implement and\nvalidate our approach by adopting the well-known DeepRx AI-native receiver\nmodel as well as a classical receiver using differentiable tensors in NVIDIA's\nSionna environment. ATLAS uncovers specific combinations of mobility, channel\ndelay spread, and noise, where fully and partially trained variants of\nAI-native DeepRx perform suboptimally compared to the classical receivers. Our\nproposed method reduces the number of tests required per failure found by 19%\ncompared to grid search for a 3-parameters input optimization problem,\ndemonstrating greater efficiency. In contrast, the computational cost of the\ngrid-based approach scales exponentially with the number of variables, making\nit increasingly impractical for high-dimensional problems."}
{"id": "2508.12211", "pdf": "https://arxiv.org/pdf/2508.12211", "abs": "https://arxiv.org/abs/2508.12211", "authors": ["Cyrus Neary", "Omar G. Younis", "Artur Kuramshin", "Ozgur Aslan", "Glen Berseth"], "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Pre-trained vision-language-action (VLA) models offer a promising foundation\nfor generalist robot policies, but often produce brittle behaviours or unsafe\nfailures when deployed zero-shot in out-of-distribution scenarios. We present\nVision-Language-Action Planning & Search (VLAPS) -- a novel framework and\naccompanying algorithms that embed model-based search into the inference\nprocedure of pre-trained VLA policies to improve their performance on robotic\ntasks. Specifically, our method biases a modified Monte Carlo Tree Search\n(MCTS) algorithm -- run using a model of the target environment -- using action\npriors defined by the VLA policy. By using VLA-derived abstractions and priors\nin model-based search, VLAPS efficiently explores language-conditioned robotics\ntasks whose search spaces would otherwise be intractably large. Conversely, by\nintegrating model-based search with the VLA policy's inference procedure, VLAPS\nyields behaviours that are more performant than those obtained by directly\nfollowing the VLA policy's action predictions. VLAPS offers a principled\nframework to: i) control test-time compute in VLA models, ii) leverage a priori\nknowledge of the robotic environment, and iii) integrate established planning\nand reinforcement learning techniques into the VLA inference process. Across\nall experiments, VLAPS significantly outperforms VLA-only baselines on\nlanguage-specified tasks that would otherwise be intractable for uninformed\nsearch algorithms, increasing success rates by as much as 67 percentage points."}
{"id": "2508.12207", "pdf": "https://arxiv.org/pdf/2508.12207", "abs": "https://arxiv.org/abs/2508.12207", "authors": ["Chenxin Tu", "Xiaowei Cui", "Gang Liu", "Mingquan Lu"], "title": "Weighted Covariance Intersection for Range-based Distributed Cooperative Localization of Multi-Agent Systems", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Precise localization of multi-agent systems (MAS) in harsh environments is a\ncritical challenge for swarm applications, and cooperative localization is\nconsidered a key solution to this issue. Among all solutions, distributed\ncooperative localization (DCL) has garnered widespread attention due to its\nrobustness and scalability. The main challenge of DCL lies in how to fuse\nrelative measurements between agents under unknown correlations. To address\nthis, covariance intersection (CI) was introduced to DCL. However, the\nclassical CI optimization criteria suffer from issues such as scale imbalance\nand correlation mismatch during the fusion process. These deficiencies are not\nas pronounced in 2D scenarios, where the state space is relatively simple and\nthe observability of each state component is well. However, in 3D scenarios,\nwhere the state space is more complex and there are significant disparities in\nthe scale and observability of state components, performance degradation\nbecomes severe. This necessitates the design of specialized mechanisms to\nimprove the data fusion process. In this paper, we identify three main\ndrawbacks of the classical CI optimization criteria in recursive filtering and\nintroduce a weighting mechanism, namely weighted covariance intersection (WCI),\nto improve its performance. We then introduce WCI into range-based distributed\ncooperative localization in 3D scenarios, developing a concurrent fusion\nstrategy for multiple distance measurements and designing a weighting matrix\nbased on the error propagation rule of the inertial navigation system (INS).\nSimulation results demonstrate that the proposed WCI significantly enhances\ncooperative localization performance compared to classical CI, while the\ndistributed approach outperforms the centralized approach in terms of\nrobustness, scalability, and is more suitable for large-scale swarms."}
{"id": "2508.12252", "pdf": "https://arxiv.org/pdf/2508.12252", "abs": "https://arxiv.org/abs/2508.12252", "authors": ["Kaizhe Hu", "Haochen Shi", "Yao He", "Weizhuo Wang", "C. Karen Liu", "Shuran Song"], "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids", "categories": ["cs.RO"], "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025", "summary": "Simulation-based reinforcement learning (RL) has significantly advanced\nhumanoid locomotion tasks, yet direct real-world RL from scratch or adapting\nfrom pretrained policies remains rare, limiting the full potential of humanoid\nrobots. Real-world learning, despite being crucial for overcoming the\nsim-to-real gap, faces substantial challenges related to safety, reward design,\nand learning efficiency. To address these limitations, we propose\nRobot-Trains-Robot (RTR), a novel framework where a robotic arm teacher\nactively supports and guides a humanoid robot student. The RTR system provides\nprotection, learning schedule, reward, perturbation, failure detection, and\nautomatic resets. It enables efficient long-term real-world humanoid training\nwith minimal human intervention. Furthermore, we propose a novel RL pipeline\nthat facilitates and stabilizes sim-to-real transfer by optimizing a single\ndynamics-encoded latent variable in the real world. We validate our method\nthrough two challenging real-world humanoid tasks: fine-tuning a walking policy\nfor precise speed tracking and learning a humanoid swing-up task from scratch,\nillustrating the promising capabilities of real-world humanoid learning\nrealized by RTR-style systems. See https://robot-trains-robot.github.io/ for\nmore info."}
{"id": "2508.12213", "pdf": "https://arxiv.org/pdf/2508.12213", "abs": "https://arxiv.org/abs/2508.12213", "authors": ["Yize Cai", "Baoshen Guo", "Flora Salim", "Zhiqing Hong"], "title": "Towards Generalizable Human Activity Recognition: A Survey", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "As a critical component of Wearable AI, IMU-based Human Activity Recognition\n(HAR) has attracted increasing attention from both academia and industry in\nrecent years. Although HAR performance has improved considerably in specific\nscenarios, its generalization capability remains a key barrier to widespread\nreal-world adoption. For example, domain shifts caused by variations in users,\nsensor positions, or environments can significantly decrease the performance in\npractice. As a result, in this survey, we explore the rapidly evolving field of\nIMU-based generalizable HAR, reviewing 229 research papers alongside 25\npublicly available datasets to provide a broad and insightful overview. We\nfirst present the background and overall framework of IMU-based HAR tasks, as\nwell as the generalization-oriented training settings. Then, we categorize\nrepresentative methodologies from two perspectives: (i) model-centric\napproaches, including pre-training method, end-to-end method, and large\nlanguage model (LLM)-based learning method; and (ii) data-centric approaches,\nincluding multi-modal learning and data augmentation techniques. In addition,\nwe summarize widely used datasets in this field, as well as relevant tools and\nbenchmarks. Building on these methodological advances, the broad applicability\nof IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent\nchallenges (e.g., data scarcity, efficient training, and reliable evaluation)\nand also outline future directions for HAR, including the adoption of\nfoundation and large language models, physics-informed and context-aware\nreasoning, generative modeling, and resource-efficient training and inference.\nThe complete list of this survey is available at\nhttps://github.com/rh20624/Awesome-IMU-Sensing, which will be updated\ncontinuously."}
{"id": "2508.12274", "pdf": "https://arxiv.org/pdf/2508.12274", "abs": "https://arxiv.org/abs/2508.12274", "authors": ["Jian Zhao", "Yunlong Lian", "Andy M Tyrrell", "Michael Gienger", "Jihong Zhu"], "title": "Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments", "categories": ["cs.RO"], "comment": "8 pages, 41 figures", "summary": "Robot-assisted dressing is a popular but challenging topic in the field of\nrobotic manipulation, offering significant potential to improve the quality of\nlife for individuals with mobility limitations. Currently, the majority of\nresearch on robot-assisted dressing focuses on how to put on loose-fitting\nclothing, with little attention paid to tight garments. For the former, since\nthe armscye is larger, a single robotic arm can usually complete the dressing\ntask successfully. However, for the latter, dressing with a single robotic arm\noften fails due to the narrower armscye and the property of diminishing\nrigidity in the armscye, which eventually causes the armscye to get stuck. This\npaper proposes a bimanual dressing strategy suitable for dressing tight-fitting\nclothing. To facilitate the encoding of dressing trajectories that adapt to\ndifferent human arm postures, a spherical coordinate system for dressing is\nestablished. We uses the azimuthal angle of the spherical coordinate system as\na task-relevant feature for bimanual manipulation. Based on this new\ncoordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture\nRegression (GMR) for imitation learning of bimanual dressing trajectories,\ngenerating dressing strategies that adapt to different human arm postures. The\neffectiveness of the proposed method is validated through various experiments."}
{"id": "2508.12215", "pdf": "https://arxiv.org/pdf/2508.12215", "abs": "https://arxiv.org/abs/2508.12215", "authors": ["Shuntian Tang", "Zesong Fei", "Xinyi Wang", "Dongkai Zhou", "Zhiqiang Wei", "Christos Masouros"], "title": "A Novel Symbol Level Precoding based AFDM Transmission Framework: Offloading Equalization Burden to Transmitter Side", "categories": ["eess.SP"], "comment": "13 pages, 9 figures; submitted to IEEE journals for possible\n  publication", "summary": "Affine Frequency Division Multiplexing (AFDM) has attracted considerable\nattention for its robustness to Doppler effects. However, its high\nreceiver-side computational complexity remains a major barrier to practical\ndeployment. To address this, we propose a novel symbol-level precoding\n(SLP)-based AFDM transmission framework, which shifts the signal processing\nburden in downlink communications from user side to the base station (BS),\nenabling direct symbol detection without requiring channel estimation or\nequalization at the receiver. Specifically, in the uplink phase, we propose a\nSparse Bayesian Learning (SBL) based channel estimation algorithm by exploiting\nthe inherent sparsity of affine frequency (AF) domain channels. In particular,\nthe sparse prior is modeled via a hierarchical Laplace distribution, and\nparameters are iteratively updated using the Expectation-Maximization (EM)\nalgorithm. We also derive the Bayesian Cramer-Rao Bound (BCRB) to characterize\nthe theoretical performance limit. In the downlink phase, the BS employs the\nSLP technology to design the transmitted waveform based on the estimated uplink\nchannel state information (CSI) and channel reciprocity. The resulting\noptimization problem is formulated as a second-order cone programming (SOCP)\nproblem, and its dual problem is investigated by Lagrangian function and\nKarush-Kuhn-Tucker conditions. Simulation results demonstrate that the proposed\nSBL estimator outperforms traditional orthogonal matching pursuit (OMP) in\naccuracy and robustness to off-grid effects, while the SLP-based waveform\ndesign scheme achieves performance comparable to conventional AFDM receivers\nwhile significantly reducing the computational complexity at receiver,\nvalidating the practicality of our approach."}
{"id": "2508.12296", "pdf": "https://arxiv.org/pdf/2508.12296", "abs": "https://arxiv.org/abs/2508.12296", "authors": ["Bin Wang", "Jiwen Zhang", "Song Wang", "Dan Wu"], "title": "A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts", "categories": ["cs.RO"], "comment": null, "summary": "In some high-precision industrial applications, robots are deployed to\nperform precision assembly tasks on mass batches of manufactured pegs and\nholes. If the peg and hole are designed with transition fit, machining errors\nmay lead to either a clearance or an interference fit for a specific pair of\ncomponents, with uncertain fit amounts. This paper focuses on the robotic batch\nprecision assembly task involving components with uncertain fit types and fit\namounts, and proposes an efficient methodology to construct the robust and\ncompliant assembly control strategy. Specifically, the batch precision assembly\ntask is decomposed into multiple deterministic subtasks, and a force-vision\nfusion controller-driven reinforcement learning method and a multi-task\nreinforcement learning training method (FVFC-MTRL) are proposed to jointly\nlearn multiple compliance control strategies for these subtasks. Subsequently,\nthe multi-teacher policy distillation approach is designed to integrate\nmultiple trained strategies into a unified student network, thereby\nestablishing a robust control strategy. Real-world experiments demonstrate that\nthe proposed method successfully constructs the robust control strategy for\nhigh-precision assembly task with different fit types and fit amounts.\nMoreover, the MTRL framework significantly improves training efficiency, and\nthe final developed control strategy achieves superior force compliance and\nhigher success rate compared with many existing methods."}
{"id": "2508.12298", "pdf": "https://arxiv.org/pdf/2508.12298", "abs": "https://arxiv.org/abs/2508.12298", "authors": ["Seungcheol Oh", "Han Han", "Joongheon Kim", "Sean Kwon"], "title": "Polarization Reconfigurable Transmit-Receive Beam Alignment with Interpretable Transformer", "categories": ["eess.SP"], "comment": null, "summary": "Recent advancement in next generation reconfigurable antenna and fluid\nantenna technology has influenced the wireless system with polarization\nreconfigurable (PR) channels to attract significant attention for promoting\nbeneficial channel condition. We exploit the benefit of PR antennas by\nintegrating such technology into massive multiple-input-multiple-output (MIMO)\nsystem. In particular, we aim to jointly design the polarization and\nbeamforming vectors on both transceivers for simultaneous channel\nreconfiguration and beam alignment, which remarkably enhance the beamforming\ngain. However, joint optimization over polarization and beamforming vectors\nwithout channel state information (CSI) is a challenging task, since\ndepolarization increases the channel dimension; whereas massive MIMO systems\ntypically have low-dimensional pilot measurement from limited radio frequency\n(RF) chain. This leads to pilot overhead because the transceivers can only\nobserve low-dimensional measurement of the high-dimension channel. This paper\npursues the reduction of the pilot overhead in such systems by proposing to\nemploy \\emph{interpretable transformer}-based deep learning framework on both\ntransceivers to actively design the polarization and beamforming vectors for\npilot stage and transmission stage based on the sequence of accumulated\nreceived pilots. Numerical experiments demonstrate the significant performance\ngain of our proposed framework over the existing non-adaptive and active\ndata-driven methods. Furthermore, we exploit the interpretability of our\nproposed framework to analyze the learning capabilities of the model."}
{"id": "2508.12312", "pdf": "https://arxiv.org/pdf/2508.12312", "abs": "https://arxiv.org/abs/2508.12312", "authors": ["Marco Leon Rapp"], "title": "Implementation and evaluation of a prediction algorithm for an autonomous vehicle", "categories": ["cs.RO"], "comment": "7 pages, 7 figures", "summary": "This paper presents a prediction algorithm that estimates the vehicle\ntrajectory every five milliseconds for an autonomous vehicle. A kinematic and a\ndynamic bicycle model are compared, with the dynamic model exhibiting superior\naccuracy at higher speeds. Vehicle parameters such as mass, center of gravity,\nmoment of inertia, and cornering stiffness are determined experimentally. For\ncornering stiffness, a novel measurement procedure using optical position\ntracking is introduced. The model is incorporated into an extended Kalman\nfilter and implemented in a ROS node in C++. The algorithm achieves a\npositional deviation of only 1.25 cm per meter over the entire test drive and\nis up to 82.6% more precise than the kinematic model."}
{"id": "2508.12320", "pdf": "https://arxiv.org/pdf/2508.12320", "abs": "https://arxiv.org/abs/2508.12320", "authors": ["Pengyu Wang", "Zhaocheng Wang", "Tianqi Mao", "Weijie Yuan", "Haijun Zhang", "George K. Karagiannidis"], "title": "Jamming Identification with Differential Transformer for Low-Altitude Wireless Networks", "categories": ["eess.SP"], "comment": null, "summary": "Wireless jamming identification, which detects and classifies electromagnetic\njamming from non-cooperative devices, is crucial for emerging low-altitude\nwireless networks consisting of many drone terminals that are highly\nsusceptible to electromagnetic jamming. However, jamming identification schemes\nadopting deep learning (DL) are vulnerable to attacks involving carefully\ncrafted adversarial samples, resulting in inevitable robustness degradation. To\naddress this issue, we propose a differential transformer framework for\nwireless jamming identification. Firstly, we introduce a differential\ntransformer network in order to distinguish jamming signals, which overcomes\nthe attention noise when compared with its traditional counterpart by\nperforming self-attention operations in a differential manner. Secondly, we\npropose a randomized masking training strategy to improve network robustness,\nwhich leverages the patch partitioning mechanism inherent to transformer\narchitectures in order to create parallel feature extraction branches. Each\nbranch operates on a distinct, randomly masked subset of patches, which\nfundamentally constrains the propagation of adversarial perturbations across\nthe network. Additionally, the ensemble effect generated by fusing predictions\nfrom these diverse branches demonstrates superior resilience against\nadversarial attacks. Finally, we introduce a novel consistent training\nframework that significantly enhances adversarial robustness through dualbranch\nregularization. Simulation results demonstrate that our proposed methodology is\nsuperior to existing methods in boosting robustness to adversarial samples."}
{"id": "2508.12335", "pdf": "https://arxiv.org/pdf/2508.12335", "abs": "https://arxiv.org/abs/2508.12335", "authors": ["Yunfan Gao", "Florian Messerer", "Niels van Duijkeren", "Rashmi Dabir", "Moritz Diehl"], "title": "Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "21 pages, 15 figures", "summary": "This paper presents a novel approach for collision avoidance in optimal and\nmodel predictive control, in which the environment is represented by a large\nnumber of points and the robot as a union of padded polygons. The conditions\nthat none of the points shall collide with the robot can be written in terms of\nan infinite number of constraints per obstacle point. We show that the\nresulting semi-infinite programming (SIP) optimal control problem (OCP) can be\nefficiently tackled through a combination of two methods: local reduction and\nan external active-set method. Specifically, this involves iteratively\nidentifying the closest point obstacles, determining the lower-level distance\nminimizer among all feasible robot shape parameters, and solving the\nupper-level finitely-constrained subproblems.\n  In addition, this paper addresses robust collision avoidance in the presence\nof ellipsoidal state uncertainties. Enforcing constraint satisfaction over all\npossible uncertainty realizations extends the dimension of constraint\ninfiniteness. The infinitely many constraints arising from translational\nuncertainty are handled by local reduction together with the robot shape\nparameterization, while rotational uncertainty is addressed via a backoff\nreformulation.\n  A controller implemented based on the proposed method is demonstrated on a\nreal-world robot running at 20Hz, enabling fast and collision-free navigation\nin tight spaces. An application to 3D collision avoidance is also demonstrated\nin simulation."}
{"id": "2508.12371", "pdf": "https://arxiv.org/pdf/2508.12371", "abs": "https://arxiv.org/abs/2508.12371", "authors": ["Lin Wang", "Zhiqing Wei", "Xu Chen", "Zhiyong Feng"], "title": "Coherent Compensation-Based Sensing for Long-Range Targets in Integrated Sensing and Communication System", "categories": ["eess.SP"], "comment": "15 pages, 10 figures", "summary": "Integrated sensing and communication (ISAC) is a promising candidate\ntechnology for 6G due to its improvement in spectral efficiency and energy\nefficiency. Orthogonal frequency division multiplexing (OFDM) signal is a\nmainstream candidate ISAC waveform. However, there are inter-symbol\ninterference (ISI) and inter-carrier interference (ICI) when the round-trip\ndelay exceeds the cyclic prefix (CP) duration for OFDM signals, which limits\nthe maximum sensing range of ISAC system. When detecting a long-range target,\nthe wide beam inevitably covers the close-range target, of which the echo's\npower is much larger than that of the long-range target. In order to tackle the\nabove problem, a multiple signal classification (MUSIC) and least squares\n(LS)-based spatial signal separation method is proposed to separate the echo\nsignals reflected from different targets. Moreover, a coherent\ncompensation-based sensing signal processing method at the receiver is proposed\nto enhance the signal to interference plus noise power ratio (SINR) of the OFDM\nblock for generating the range-Doppler map (RDM) with higher SINR. Simulation\nresults reveal that the proposed method greatly enhances the SINR of RDM by 10\ndB for a target at 500 m compared with two-dimensional fast Fourier transform\n(2D-FFT) method. Besides, the detection probability is also significantly\nimproved compared to the benchmarking method."}
{"id": "2508.12394", "pdf": "https://arxiv.org/pdf/2508.12394", "abs": "https://arxiv.org/abs/2508.12394", "authors": ["Zichen Yan", "Rui Huang", "Lei He", "Shao Guo", "Lin Zhao"], "title": "SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an\nunknown environment and reaching a location that visually matches a given\ntarget image. While prior works primarily study ImageNav for ground robots,\nenabling this capability for autonomous drones is substantially more\nchallenging due to their need for high-frequency feedback control and global\nlocalization for stable flight. In this paper, we propose a novel sim-to-real\nframework that leverages visual reinforcement learning (RL) to achieve ImageNav\nfor drones. To enhance visual representation ability, our approach trains the\nvision backbone with auxiliary tasks, including image perturbations and future\ntransition prediction, which results in more effective policy training. The\nproposed algorithm enables end-to-end ImageNav with direct velocity control,\neliminating the need for external localization. Furthermore, we integrate a\ndepth-based safety module for real-time obstacle avoidance, allowing the drone\nto safely navigate in cluttered environments. Unlike most existing drone\nnavigation methods that focus solely on reference tracking or obstacle\navoidance, our framework supports comprehensive navigation\nbehaviors--autonomous exploration, obstacle avoidance, and image-goal\nseeking--without requiring explicit global mapping. Code and model checkpoints\nwill be released upon acceptance."}
{"id": "2508.12403", "pdf": "https://arxiv.org/pdf/2508.12403", "abs": "https://arxiv.org/abs/2508.12403", "authors": ["Federico Miotello", "Davide Albertini", "Alberto Bernardini"], "title": "On the Extension of Differential Beamforming Theory to Arbitrary Planar Arrays of First-Order Elements", "categories": ["eess.SP", "eess.AS"], "comment": null, "summary": "Small-size acoustic arrays exploit spatial diversity to achieve capabilities\nbeyond those of single-element devices, with applications ranging from\nteleconferencing to immersive multimedia. A key requirement for broadband array\nprocessing is a frequency-invariant spatial response, which ensures consistent\ndirectivity across wide bandwidths and prevents spectral coloration.\nDifferential beamforming offers an inherently frequency-invariant solution by\nleveraging pressure differences between closely spaced elements of small-size\narrays. Traditional approaches, however, assume the array elements to be\nomnidirectional, whereas real transducers exhibit frequency-dependent\ndirectivity that can degrade performance if not properly modeled. To address\nthis limitation, we propose a generalized modal matching framework for\nfrequency-invariant differential beamforming, applicable to unconstrained\nplanar arrays of first-order directional elements. By representing the desired\nbeampattern as a truncated circular harmonic expansion and fitting it to the\nactual element responses, our method accommodates arbitrary planar geometries\nand element orientations. This approach enables the synthesis of beampatterns\nof any order and steering direction without imposing rigid layout requirements.\nSimulations confirm that accounting for sensor directivity at the design stage\nyields accurate and robust performance across varying frequencies, geometries,\nand noise conditions."}
{"id": "2508.12395", "pdf": "https://arxiv.org/pdf/2508.12395", "abs": "https://arxiv.org/abs/2508.12395", "authors": ["Zihan Wang"], "title": "PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications."}
{"id": "2508.12614", "pdf": "https://arxiv.org/pdf/2508.12614", "abs": "https://arxiv.org/abs/2508.12614", "authors": ["Zhongqin Wang", "J. Andrew Zhang", "Kai Wu", "Min Xu", "Y. Jay Guo"], "title": "Towards SISO Bistatic Sensing for ISAC", "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": null, "summary": "Integrated Sensing and Communication (ISAC) is a key enabler for\nnext-generation wireless systems. However, real-world deployment is often\nlimited to low-cost, single-antenna transceivers. In such bistatic Single-Input\nSingle-Output (SISO) setup, clock asynchrony introduces random phase offsets in\nChannel State Information (CSI), which cannot be mitigated using conventional\nmulti-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic\nSISO sensing framework that enables accurate delay and Doppler estimation from\ndistorted CSI by effectively suppressing Doppler mirroring ambiguity. It\noperates with only a single antenna at both the transmitter and receiver,\nmaking it suitable for low-complexity deployments. We propose a\nself-referencing cross-correlation (SRCC) method for SISO random phase removal\nand employ delay-domain beamforming to resolve Doppler ambiguity. The resulting\nunambiguous delay-Doppler-time features enable robust sensing with compact\nneural networks. Extensive experiments show that WiDFS 3.0 achieves accurate\nparameter estimation, with performance comparable to or even surpassing that of\nprior multi-antenna methods, especially in delay estimation. Validated under\nsingle- and multi-target scenarios, the extracted ambiguity-resolved features\nshow strong sensing accuracy and generalization. For example, when deployed on\nthe embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0\nconsistently outperforms conventional features such as CSI amplitude, mirrored\nDoppler, and multi-receiver aggregated Doppler."}
{"id": "2508.12435", "pdf": "https://arxiv.org/pdf/2508.12435", "abs": "https://arxiv.org/abs/2508.12435", "authors": ["Deqing Song", "Weimin Yang", "Maryam Rezayati", "Hans Wernher van de Venn"], "title": "Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "While gesture recognition using vision or robot skins is an active research\narea in Human-Robot Collaboration (HRC), this paper explores deep learning\nmethods relying solely on a robot's built-in joint sensors, eliminating the\nneed for external sensors. We evaluated various convolutional neural network\n(CNN) architectures and collected two datasets to study the impact of data\nrepresentation and model architecture on the recognition accuracy. Our results\nshow that spectrogram-based representations significantly improve accuracy,\nwhile model architecture plays a smaller role. We also tested generalization to\nnew robot poses, where spectrogram-based models performed better. Implemented\non a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,\nachieved over 95% accuracy in contact detection and gesture classification.\nThese findings demonstrate the feasibility of external-sensor-free tactile\nrecognition and promote further research toward cost-effective, scalable\nsolutions for HRC."}
{"id": "2508.12660", "pdf": "https://arxiv.org/pdf/2508.12660", "abs": "https://arxiv.org/abs/2508.12660", "authors": ["Yezhuo Zhang", "Zinan Zhou", "Guangyu Li", "Xuanpeng Li"], "title": "Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprint", "categories": ["eess.SP"], "comment": "14 pages, 8 figures", "summary": "In response to the rapid growth of Internet of Things (IoT) devices and\nrising security risks, Radio Frequency Fingerprint (RFF) has become key for\ndevice identification and authentication. However, various changing factors -\nbeyond the RFF itself - can be entangled from signal transmission to reception,\nreducing the effectiveness of RFF Identification (RFFI). Existing RFFI methods\nmainly rely on domain adaptation techniques, which often lack explicit factor\nrepresentations, resulting in less robustness and limited controllability for\ndownstream tasks. To tackle this problem, we propose a novel Disentangled\nRepresentation Learning (DRL) framework that learns explicit and independent\nrepresentations of multiple factors, including the RFF. Our framework\nintroduces modules for disentanglement, guided by the principles of\nexplicitness, modularity, and compactness. We design two dedicated modules for\nfactor classification and signal reconstruction, each with tailored loss\nfunctions that encourage effective disentanglement and enhance support for\ndownstream tasks. Thus, the framework can extract a set of interpretable\nvectors that explicitly represent corresponding factors. We evaluate our\napproach on two public benchmark datasets and a self-collected dataset. Our\nmethod achieves impressive performance on multiple DRL metrics. We also analyze\nthe effectiveness of our method on downstream RFFI task and conditional signal\ngeneration task. All modules of the framework contribute to improved\nclassification accuracy, and enable precise control over conditional generated\nsignals. These results highlight the potential of our DRL framework for\ninterpretable and explicit RFFs."}
{"id": "2508.12439", "pdf": "https://arxiv.org/pdf/2508.12439", "abs": "https://arxiv.org/abs/2508.12439", "authors": ["Sunyu Wang", "Arjun S. Lakshmipathy", "Jean Oh", "Nancy S. Pollard"], "title": "Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Reasoning about rolling and sliding contact, or roll-slide contact for short,\nis critical for dexterous manipulation tasks that involve intricate geometries.\nBut existing works on roll-slide contact mostly focus on continuous shapes with\ndifferentiable parametrizations. This work extends roll-slide contact modeling\nto manifold meshes. Specifically, we present an integration scheme based on\ngeodesic tracing to first-order time-integrate roll-slide contact directly on\nmeshes, enabling dexterous manipulation to reason over high-fidelity discrete\nrepresentations of an object's true geometry. Using our method, we planned\ndexterous motions of a multi-finger robotic hand manipulating five objects\nin-hand in simulation. The planning was achieved with a least-squares optimizer\nthat strives to maintain the most stable instantaneous grasp by minimizing\ncontact sliding and spinning. Then, we evaluated our method against a baseline\nusing collision detection and a baseline using primitive shapes. The results\nshow that our method performed the best in accuracy and precision, even for\ncoarse meshes. We conclude with a future work discussion on incorporating\nmultiple contacts and contact forces to achieve accurate and robust mesh-based\nsurface contact modeling."}
{"id": "2508.12689", "pdf": "https://arxiv.org/pdf/2508.12689", "abs": "https://arxiv.org/abs/2508.12689", "authors": ["Ning Gao", "Tianrui Zeng", "Bowen Chen", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition", "categories": ["eess.SP"], "comment": null, "summary": "5G-Advanced (5G-A) has enabled the vibrant development of low altitude\nintegrated sensing and communication (LA-ISAC) networks. As a core component of\nthese networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth in\nrecent years. However, due to the lag in traditional industry regulatory norms,\nunauthorized flight incidents occur frequently, posing a severe security threat\nto LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, we\npropose a multi-domain supervised contrastive learning (MD-SupContrast)\nframework for UAV radio frequency (RF) open-set recognition. Specifically,\nfirst, the texture features and the time-frequency position features from the\nResNet and the TransformerEncoder are fused, and then the supervised\ncontrastive learning is applied to optimize the feature representation of the\nclosed-set samples. Next, to surveil the invasive UAVs that appear in real\nlife, we propose an improved generative OpenMax (IG-OpenMax) algorithm and\nconstruct an open-set recognition model, namely Open-RFNet. According to the\nunknown samples, we first freeze the feature extraction layers and then only\nretrain the classification layer, which achieves excellent recognition\nperformance both in closed-set and open-set recognitions. We analyze the\ncomputational complexity of the proposed model. Experiments are conducted with\na large-scale UAV open dataset. The results show that the proposed Open-RFNet\noutperforms the existing benchmark methods in terms of recognition accuracy\nbetween the known and the unknown UAVs, as it achieves 95.12% in closed-set and\n96.08% in open-set under 25 UAV types, respectively."}
{"id": "2508.12456", "pdf": "https://arxiv.org/pdf/2508.12456", "abs": "https://arxiv.org/abs/2508.12456", "authors": ["Hadas C. Kuzmenko", "David Ehevich", "Oren Gal"], "title": "Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics", "categories": ["cs.RO", "68T07, 93C85, 86A05", "I.2.6; I.2.9; J.2"], "comment": "30 pages, 40 figures. Framework combining Liquid Time-Constant Neural\n  Networks with autonomous marine robotics for oil spill trajectory prediction\n  and response coordination", "summary": "Marine oil spills pose grave environmental and economic risks, threatening\nmarine ecosystems, coastlines, and dependent industries. Predicting and\nmanaging oil spill trajectories is highly complex, due to the interplay of\nphysical, chemical, and environmental factors such as wind, currents, and\ntemperature, which makes timely and effective response challenging. Accurate\nreal-time trajectory forecasting and coordinated mitigation are vital for\nminimizing the impact of these disasters. This study introduces an integrated\nframework combining a multi-agent swarm robotics system built on the MOOS-IvP\nplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system\nfuses adaptive machine learning with autonomous marine robotics, enabling\nreal-time prediction, dynamic tracking, and rapid response to evolving oil\nspills. By leveraging LTCNs--well-suited for modeling complex, time-dependent\nprocesses--the framework achieves real-time, high-accuracy forecasts of spill\nmovement. Swarm intelligence enables decentralized, scalable, and resilient\ndecision-making among robot agents, enhancing collective monitoring and\ncontainment efforts. Our approach was validated using data from the Deepwater\nHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,\nsurpassing LSTM approaches by 23%. The integration of advanced neural modeling\nwith autonomous, coordinated robotics demonstrates substantial improvements in\nprediction precision, flexibility, and operational scalability. Ultimately,\nthis research advances the state-of-the-art for sustainable, autonomous oil\nspill management and environmental protection by enhancing both trajectory\nprediction and response coordination."}
{"id": "2508.12728", "pdf": "https://arxiv.org/pdf/2508.12728", "abs": "https://arxiv.org/abs/2508.12728", "authors": ["Yunsong Huang", "Hui-Ming Wang", "Qingli Yan", "Zhaowei Wang"], "title": "LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent Metasurface Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "The evolution of 6G networks demands ultra-massive connectivity and\nintelligent radio environments, yet existing reconfigurable intelligent surface\n(RIS) technologies face critical limitations in hardware efficiency, dynamic\ncontrol, and scalability. This paper introduces LLM-RIMSA, a transformative\nframework that integrates large language models (LLMs) with a novel\nreconfigurable intelligent metasurface antenna (RIMSA) architecture to address\nthese challenges. Unlike conventional RIS designs, RIMSA employs parallel\ncoaxial feeding and 2D metasurface integration, enabling each individual\nmetamaterial element to independently adjust both its amplitude and phase.\nWhile traditional optimization and deep learning (DL) methods struggle with\nhigh-dimensional state spaces and prohibitive training costs for RIMSA control,\nLLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot\nlearning capabilities to dynamically optimize RIMSA configurations. Simulations\ndemonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming\nconventional DL-based methods in sum rate while reducing training overhead. The\nproposed framework pave the way for LLM-driven intelligent radio environments."}
{"id": "2508.12469", "pdf": "https://arxiv.org/pdf/2508.12469", "abs": "https://arxiv.org/abs/2508.12469", "authors": ["Abhinav Chalise", "Nimesh Gopal Pradhan", "Nishan Khanal", "Prashant Raj Bista", "Dinesh Baniya Kshatri"], "title": "Mechanical Automation with Vision: A Design for Rubik's Cube Solver", "categories": ["cs.RO", "cs.CV"], "comment": "Presented at the 15th IOE Graduate Conference, Tribhuvan University,\n  May 2024. Original paper available at\n  https://conference.ioe.edu.np/publications/ioegc15/IOEGC-15-023-C1-2-42.pdf", "summary": "The core mechanical system is built around three stepper motors for physical\nmanipulation, a microcontroller for hardware control, a camera and YOLO\ndetection model for real-time cube state detection. A significant software\ncomponent is the development of a user-friendly graphical user interface (GUI)\ndesigned in Unity. The initial state after detection from real-time YOLOv8\nmodel (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)\nis virtualized on GUI. To get the solution, the system employs the Kociemba's\nalgorithm while physical manipulation with a single degree of freedom is done\nby combination of stepper motors' interaction with the cube achieving the\naverage solving time of ~2.2 minutes."}
{"id": "2508.12746", "pdf": "https://arxiv.org/pdf/2508.12746", "abs": "https://arxiv.org/abs/2508.12746", "authors": ["Muhammad Ammad", "Paul Schwarzbach", "Michael Schultz", "Oliver Michler"], "title": "Range-Angle Likelihood Maps for Indoor Positioning Using Deep Neural Networks", "categories": ["eess.SP"], "comment": null, "summary": "Accurate and high precision of the indoor positioning is as important as\nensuring reliable navigation in outdoor environments. Using the\nstate-of-the-art deep learning models provides better reliability and accuracy\nto navigate and monitor the accurate positions in the aircraft cabin\nenvironment. We utilize the simulated aircraft cabin environment measurements\nand propose a residual neural network (ResNet) model to predict the accurate\npositions inside the cabin. The measurements include the ranges and angles\nbetween a tag and the anchors points which are then mapped onto a grid as range\nand angle residuals. These residual maps are then transformed into the\nlikelihood grid maps where each cell of the grid shows the likelihood of being\na true location. These grid maps along with the true positions are then passed\nas inputs to train the ResNet model. Since any deep learning model involve\nnumerous parameter settings, hyperparameter optimization is performed to get\nthe optimal parameters for training the model effectively with the highest\naccuracy. Once we get the best hyperparameters settings of the model, it is\nthen trained to predict the positions which provides a centimeter-level\naccuracy of the localization."}
{"id": "2508.12554", "pdf": "https://arxiv.org/pdf/2508.12554", "abs": "https://arxiv.org/abs/2508.12554", "authors": ["Hamza El-Kebir"], "title": "PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for presentation at the 2025 IEEE Conference on Decision and\n  Control (CDC)", "summary": "We introduce PROD (Palpative Reconstruction of Deformables), a novel method\nfor reconstructing the shape and mechanical properties of deformable objects\nusing elastostatic signed distance functions (SDFs). Unlike traditional\napproaches that rely on purely geometric or visual data, PROD integrates\npalpative interaction -- measured through force-controlled surface probing --\nto estimate both the static and dynamic response of soft materials. We model\nthe deformation of an object as an elastostatic process and derive a governing\nPoisson equation for estimating its SDF from a sparse set of pose and force\nmeasurements. By incorporating steady-state elastodynamic assumptions, we show\nthat the undeformed SDF can be recovered from deformed observations with\nprovable convergence. Our approach also enables the estimation of material\nstiffness by analyzing displacement responses to varying force inputs. We\ndemonstrate the robustness of PROD in handling pose errors, non-normal force\napplication, and curvature errors in simulated soft body interactions. These\ncapabilities make PROD a powerful tool for reconstructing deformable objects in\napplications ranging from robotic manipulation to medical imaging and haptic\nfeedback systems."}
{"id": "2508.12892", "pdf": "https://arxiv.org/pdf/2508.12892", "abs": "https://arxiv.org/abs/2508.12892", "authors": ["Mahdi Abdollahpour", "Marco Bertuletti", "Yichao Zhang", "Yawei Li", "Luca Benini", "Alessandro Vanelli-Coralli"], "title": "A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN", "categories": ["eess.SP"], "comment": "Accepted to IEEE GLOBECOM 2025", "summary": "Artificial intelligence approaches for base-band processing for radio\nreceivers have demonstrated significant performance gains. Most of the proposed\nmethods are characterized by high compute and memory requirements, hindering\ntheir deployment at the edge of the Radio Access Networks (RAN) and limiting\ntheir scalability to large bandwidths and many antenna 6G systems. In this\npaper, we propose a low-complexity, model-driven neural network-based receiver,\ndesigned for multi-user multiple-input multiple-output (MU-MIMO) systems and\nsuitable for implementation at the RAN edge. The proposed solution is compliant\nwith the 5G New Radio (5G NR), and supports different modulation schemes,\nbandwidths, number of users, and number of base-station antennas with a single\ntrained model without the need for further training. Numerical simulations of\nthe Physical Uplink Shared Channel (PUSCH) processing show that the proposed\nsolution outperforms the state-of-the-art methods in terms of achievable\nTransport Block Error Rate (TBLER), while reducing the Floating Point\nOperations (FLOPs) by 66$\\times$, and the learnable parameters by 396$\\times$."}
{"id": "2508.12564", "pdf": "https://arxiv.org/pdf/2508.12564", "abs": "https://arxiv.org/abs/2508.12564", "authors": ["Jiayao Mai", "Xiuyuan Lu", "Kuan Dai", "Shaojie Shen", "Yi Zhou"], "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems", "categories": ["cs.RO", "cs.CV", "I.2.9"], "comment": "8 pages, 5 figures", "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib."}
{"id": "2508.12941", "pdf": "https://arxiv.org/pdf/2508.12941", "abs": "https://arxiv.org/abs/2508.12941", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ozgur Ozdemir", "Mani Bharathi Pandian", "Ismail Guvenc"], "title": "Interference-Asymmetric UAV Remote Control Links: Measurements and Performance Evaluation", "categories": ["eess.SP"], "comment": null, "summary": "Reliable and secure connectivity is crucial for remote control (RC) and\nuncrewed aerial vehicles (UAVs) links. A major problem for UAV RC links is that\ninterference sources within the coverage may degrade the link quality. Such\ninterference problems are a higher concern for the UAV than the RC unit on the\nground due to the UAV being in line of sight (LoS) with a larger number of\ninterference sources. As a result, lost hybrid automatic repeat request (HARQ)\nindicators (ACK/NACK) feedback in the uplink (UL, RC to UAV) may degrade the\ndownlink (DL, UAV to RC) throughput. To get physical evidence for our\ninterference asymmetry argument, we first conducted a measurement campaign\nusing a helikite platform at the Main Campus area of NC State University during\nthe 2024 Packapalooza festival. Subsequently, we evaluated the throughput\nimpact of the loss of HARQ indicator feedback caused by UL asymmetry using\nMATLAB long-term-evolution (LTE) and fifth-generation (5G) toolboxes. Our\nnumerical results confirm that UL interference asymmetry substantially degrades\nthe throughput performance due to the loss of HARQ indicator feedback."}
{"id": "2508.12681", "pdf": "https://arxiv.org/pdf/2508.12681", "abs": "https://arxiv.org/abs/2508.12681", "authors": ["Johann Licher", "Max Bartholdt", "Henrik Krauss", "Tim-Lukas Habich", "Thomas Seel", "Moritz Schappler"], "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 15 figures", "summary": "Dynamic control of soft continuum robots (SCRs) holds great potential for\nexpanding their applications, but remains a challenging problem due to the high\ncomputational demands of accurate dynamic models. While data-driven approaches\nlike Koopman-operator-based methods have been proposed, they typically lack\nadaptability and cannot capture the full robot shape, limiting their\napplicability. This work introduces a real-time-capable nonlinear\nmodel-predictive control (MPC) framework for SCRs based on a domain-decoupled\nphysics-informed neural network (DD-PINN) with adaptable bending stiffness. The\nDD-PINN serves as a surrogate for the dynamic Cosserat rod model with a\nspeed-up factor of 44000. It is also used within an unscented Kalman filter for\nestimating the model states and bending compliance from end-effector position\nmeasurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the\nGPU. In simulation, it demonstrates accurate tracking of dynamic trajectories\nand setpoint control with end-effector position errors below 3 mm (2.3% of the\nactuator's length). In real-world experiments, the controller achieves similar\naccuracy and accelerations up to 3.55 m/s2."}
{"id": "2508.12964", "pdf": "https://arxiv.org/pdf/2508.12964", "abs": "https://arxiv.org/abs/2508.12964", "authors": ["Osman Tokluoglu", "Enver Cavus", "Ebrahim Bedeer", "Halim Yanikomeroglu"], "title": "A Novel CNN Based Standalone Detector for Faster-than-Nyquist Signaling", "categories": ["eess.SP"], "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Communications (IEEE TCOM)", "summary": "This paper presents a novel convolutional neural network (CNN)-based detector\nfor faster-than-Nyquist (FTN) signaling, introducing structured fixed kernel\nlayers with domain-informed masking to effectively mitigate intersymbol\ninterference (ISI). Unlike standard CNN architectures that rely on moving\nkernels, the proposed approach employs fixed convolutional kernels at\npredefined positions to explicitly learn ISI patterns at varying distances from\nthe central symbol. To enhance feature extraction, a hierarchical filter\nallocation strategy is employed, assigning more filters to earlier layers for\nstronger ISI components and fewer to later layers for weaker components. This\nstructured design improves feature representation, eliminates redundant\ncomputations, and enhances detection accuracy while maintaining computational\nefficiency. Simulation results demonstrate that the proposed detector achieves\nnear-optimal bit error rate (BER) performance, comparable to the BCJR algorithm\nfor the compression factor $\\tau \\geq 0.7$, while offering up to $46\\%$ and\n$84\\%$ computational cost reduction over M-BCJR for BPSK and QPSK,\nrespectively. Additional evaluations confirm the method's adaptability to\nhigh-order modulations (up to 64-QAM), resilience in quasi-static multipath\nRayleigh fading channels, and effectiveness under LDPC-coded FTN transmission,\nhighlighting its robustness and practicality."}
{"id": "2508.12729", "pdf": "https://arxiv.org/pdf/2508.12729", "abs": "https://arxiv.org/abs/2508.12729", "authors": ["Junhao Ye", "Cheng Hu", "Yiqin Wang", "Weizhan Huang", "Nicolas Baumann", "Jie He", "Meixun Qu", "Lei Xie", "Hongye Su"], "title": "MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In autonomous racing, reactive controllers eliminate the computational burden\nof the full See-Think-Act autonomy stack by directly mapping sensor inputs to\ncontrol actions. This bypasses the need for explicit localization and\ntrajectory planning. A widely adopted baseline in this category is the\nFollow-The-Gap method, which performs trajectory planning using LiDAR data.\nBuilding on FTG, the Delaunay Triangulation-based Racing algorithm introduces\nfurther enhancements. However, DTR's use of circumcircles for trajectory\ngeneration often results in insufficiently smooth paths, ultimately degrading\nperformance. Additionally, the commonly used F1TENTH-simulator for autonomous\nracing competitions lacks support for 3D LiDAR perception, limiting its\neffectiveness in realistic testing. To address these challenges, this work\nproposes the MCTR algorithm. MCTR improves trajectory smoothness through the\nuse of Curvature Corrected Moving Average and implements a digital twin system\nwithin the CARLA simulator to validate the algorithm's robustness under 3D\nLiDAR perception. The proposed algorithm has been thoroughly validated through\nboth simulation and real-world vehicle experiments."}
{"id": "2508.13017", "pdf": "https://arxiv.org/pdf/2508.13017", "abs": "https://arxiv.org/abs/2508.13017", "authors": ["Scott Schoen Jr", "Brian Lause", "Marko Jakovljevic", "Rimon Tadross", "Mike Washburn", "Anthony E. Samir"], "title": "Wavefield Correlation Imaging in Arbitrary Media with Inherent Aberration Correction", "categories": ["eess.SP"], "comment": null, "summary": "Ultrasound (US) imaging is an indispensable tool for diagnostic imaging,\nparticularly given its cost, safety, and portability profiles compared to other\nmodalities. However, US is challenged in subjects with morphological\nheterogeneity (e.g., those with overweight or obesity), largely because\nconventional imaging algorithms do not account for such variation in the\nbeamforming process. Specific knowledge of the these spatial variations enables\nsupplemental corrections of these algorithms, but with added computational\ncomplexity. Wavefield correlation imaging (WCI) enables efficient image\nformation in the spatial frequency domain that, in its canonical formulation,\nassumes a uniform medium. In this work, we present an extension of WCI to\narbitrary known speed-of-sound distributions directly in the image formation\nprocess, and demonstrate its feasibility in silico, in vitro, and in vivo. We\nreport resolution improvements of over 30% and contrast improvements of order\n10% over conventional WCI imaging. Together our results suggest heterogeneous\nWCI (HWCI) may have high translational potential to improve the objective\nquality, and thus clinical utility, of ultrasound images."}
{"id": "2508.12916", "pdf": "https://arxiv.org/pdf/2508.12916", "abs": "https://arxiv.org/abs/2508.12916", "authors": ["Hecheng Wang", "Jiankun Ren", "Jia Yu", "Lizhe Qi", "Yunquan Sun"], "title": "RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph", "categories": ["cs.RO"], "comment": null, "summary": "Humans effortlessly retrieve objects in cluttered, partially observable\nenvironments by combining visual reasoning, active viewpoint adjustment, and\nphysical interaction-with only a single pair of eyes. In contrast, most\nexisting robotic systems rely on carefully positioned fixed or multi-camera\nsetups with complete scene visibility, which limits adaptability and incurs\nhigh hardware costs. We present \\textbf{RoboRetriever}, a novel framework for\nreal-world object retrieval that operates using only a \\textbf{single}\nwrist-mounted RGB-D camera and free-form natural language instructions.\nRoboRetriever grounds visual observations to build and update a \\textbf{dynamic\nhierarchical scene graph} that encodes object semantics, geometry, and\ninter-object relations over time. The supervisor module reasons over this\nmemory and task instruction to infer the target object and coordinate an\nintegrated action module combining \\textbf{active perception},\n\\textbf{interactive perception}, and \\textbf{manipulation}. To enable\ntask-aware scene-grounded active perception, we introduce a novel visual\nprompting scheme that leverages large reasoning vision-language models to\ndetermine 6-DoF camera poses aligned with the semantic task goal and geometry\nscene context. We evaluate RoboRetriever on diverse real-world object retrieval\ntasks, including scenarios with human intervention, demonstrating strong\nadaptability and robustness in cluttered scenes with only one RGB-D camera."}
{"id": "2508.13067", "pdf": "https://arxiv.org/pdf/2508.13067", "abs": "https://arxiv.org/abs/2508.13067", "authors": ["Ivn Alexander Morales Sandoval", "Getuar Rexhepi", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu"], "title": "Low-complexity Leakage Minimization Beamforming for Large-scale Multi-user Cell-Free Massive MIMO", "categories": ["eess.SP"], "comment": "Submitted to an IEEE journal for possible publication", "summary": "We propose a low-complexity beamforming (BF) design for information leakage\nminimization in multi-user (MU) cell-free massive multiple-input\nmultiple-output (CF-mMIMO) systems. Our approach leverages fractional\nprogramming (FP) to reformulate the secrecy rate maximization problem into a\ntractable difference-of-convex form. To efficiently solve the resulting\nnon-convex problem, we employ the Concave-Convex Procedure (CCP), enabling fast\nconvergence to a local optimum. Simulation results demonstrate that the\nproposed scheme achieves secrecy rates comparable to state-of-the-art (SotA)\nmethods, while significantly reducing computational complexity and improving\nconvergence speed."}
{"id": "2508.12925", "pdf": "https://arxiv.org/pdf/2508.12925", "abs": "https://arxiv.org/abs/2508.12925", "authors": ["Eetu Laukka", "Evan G. Center", "Timo Ojala", "Steven M. LaValle", "Matti Pouke"], "title": "Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users", "categories": ["cs.RO", "cs.HC"], "comment": "2025 IEEE Conference on Telepresence", "summary": "Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable."}
{"id": "2508.13075", "pdf": "https://arxiv.org/pdf/2508.13075", "abs": "https://arxiv.org/abs/2508.13075", "authors": ["Arav Sharma", "Lei Chi", "Ari Gebhardt", "Alon S. Levin", "Timothy R. Hoerning", "Sam Keene"], "title": "BeamSeek: Deep Learning-based DOA Estimation for Low-Complexity mmWave Phased Arrays", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A novel approach combining agile beam switching with deep learning to enhance\nthe speed and accuracy of Direction of Arrival (DOA) estimation for\nmillimeter-wave (mmWave) phased array systems with low-complexity hardware\nimplementations is proposed and evaluated. Traditional DOA methods requiring\ndirect access to individual antenna elements are impractical for analog or\nhybrid beamforming systems prevalent in modern mmWave implementations. Recent\nagile beam switching techniques have demonstrated rapid DOA estimation, but\ntheir accuracy and robustness can be further improved via deep learning.\nBeamSeek addresses these limitations by employing a Multi-Layer Perceptron\n(MLP) and specialized data augmentation that emulates real-world propagation\nconditions. The proposed approach was experimentally validated at 60 GHz using\nthe NSF PAWR COSMOS testbed, demonstrating significant improvements over a\ncorrelation-based method across various Signal-to-Noise Ratio (SNR) levels.\nResults show that BeamSeek achieves up to an 8 degree reduction in average\nestimation error compared to this baseline, with particular advantages in noisy\nchannels. This makes it especially suitable for practical mmWave deployments in\nenvironments characterized by multipath interference and hardware constraints."}
{"id": "2508.12928", "pdf": "https://arxiv.org/pdf/2508.12928", "abs": "https://arxiv.org/abs/2508.12928", "authors": ["Victor Dhdin", "Haizhou Zhao", "Majid Khadiv"], "title": "Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Legged robots have the potential to traverse highly constrained environments\nwith agile maneuvers. However, planning such motions requires solving a highly\nchallenging optimization problem with a mixture of continuous and discrete\ndecision variables. In this paper, we present a full pipeline based on\nMonte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to\nperform simultaneous contact sequence and patch selection on highly challenging\nenvironments. Through extensive simulation experiments, we show that our\nframework can quickly find a diverse set of dynamically consistent plans. We\nexperimentally show that these plans are transferable to a real quadruped\nrobot. We further show that the same framework can find highly complex acyclic\nhumanoid maneuvers. To the best of our knowledge, this is the first\ndemonstration of simultaneous contact sequence and patch selection for acyclic\nmulti-contact locomotion using the whole-body dynamics of a quadruped."}
{"id": "2508.11655", "pdf": "https://arxiv.org/pdf/2508.11655", "abs": "https://arxiv.org/abs/2508.11655", "authors": ["Cai Lin", "Yunyi Ding", "Kai Lin", "Ru Wang", "Yichen Luo", "Xiaofen Wu"], "title": "Stretchable and self-adhesive triboelectric sensor for real-time musculoskeletal monitoring and personalized recovery", "categories": ["physics.med-ph", "eess.SP", "physics.app-ph"], "comment": null, "summary": "Recent advances in medical diagnostics have highlighted the importance of\nwearable technologies for continuous and real-time physiological monitoring. In\nthis study, we introduce a flexible, self-powered triboelectric nanogenerator\n(MB-TENG) engineered from commercially available medical elastic bandages for\nbiomechanical sensing during rehabilitation and gait analysis. Leveraging the\nporous and skin-friendly properties of the bandage combined with a PTFE film,\nthe MB-TENG delivers robust electrical performance, achieving a peak\nopen-circuit voltage (VOC) of 122~V, a short-circuit current (ISC) of\n25~$\\mu$A, and a transferred charge (QSC) of 110~nC, while maintaining\nlong-term stability across 40{,}000 mechanical cycles. Its inherent\nself-adhesive property allows for multi-layer assembly without extra bonding\nagents, and mechanical stretching enhances output, enabling dual\nconfigurability. A stacked design further improves the power capacity,\nsupporting applications in wearable medical electronics. The MB-TENG device\nseamlessly conforms to joint surfaces and foot regions, providing accurate\ndetection of motion states and abnormal gait patterns. These features\nunderscore the MB-TENG's potential as a low-cost, scalable platform for\npersonalized rehabilitation, injury monitoring, and early musculoskeletal\ndiagnosis."}
{"id": "2508.12946", "pdf": "https://arxiv.org/pdf/2508.12946", "abs": "https://arxiv.org/abs/2508.12946", "authors": ["Ann-Sophie Schenk", "Stefan Schiffer", "Heqiu Song"], "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "In this paper we report on first insights from interviews with teachers and\nstudents on using social robots in computer science class in sixth grade. Our\nfocus is on learning about requirements and potential applications. We are\nparticularly interested in getting both perspectives, the teachers' and the\nlearners' view on how robots could be used and what features they should or\nshould not have. Results show that teachers as well as students are very open\nto robots in the classroom. However, requirements are partially quite\nheterogeneous among the groups. This leads to complex design challenges which\nwe discuss at the end of this paper."}
{"id": "2508.11791", "pdf": "https://arxiv.org/pdf/2508.11791", "abs": "https://arxiv.org/abs/2508.11791", "authors": ["Christian Forsch", "Zilu Zhao", "Dirk Slock", "Laura Cottatellucci"], "title": "Bayesian Learning for Pilot Decontamination in Cell-Free Massive MIMO", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "7 pages, 8 figures, accepted for publication in Proceedings of the\n  28th International Workshop on Smart Antennas (WSA)", "summary": "Pilot contamination (PC) arises when the pilot sequences assigned to user\nequipments (UEs) are not mutually orthogonal, eventually due to their reuse. In\nthis work, we propose a novel expectation propagation (EP)-based joint channel\nestimation and data detection (JCD) algorithm specifically designed to mitigate\nthe effects of PC in the uplink of cell-free massive multiple-input\nmultiple-output (CF-MaMIMO) systems. This modified bilinear-EP algorithm is\ndistributed, scalable, demonstrates strong robustness to PC, and outperforms\nstate-of-the-art Bayesian learning algorithms. Through a comprehensive\nperformance evaluation, we assess the performance of Bayesian learning\nalgorithms for different pilot sequences and observe that the use of\nnon-orthogonal pilots can lead to better performance compared to shared\northogonal sequences. Motivated by this analysis, we introduce a new metric to\nquantify PC at the UE level. We show that the performance of the considered\nalgorithms degrades monotonically with respect to this metric, providing a\nvaluable theoretical and practical tool for understanding and managing PC via\niterative JCD algorithms."}
{"id": "2508.12980", "pdf": "https://arxiv.org/pdf/2508.12980", "abs": "https://arxiv.org/abs/2508.12980", "authors": ["Victor Lev", "Joo Moura", "Sachiya Fujita", "Tamon Miyake", "Steve Tonneau", "Sethu Vijayakumar"], "title": "Scaling Whole-body Multi-contact Manipulation with Contact Optimization", "categories": ["cs.RO"], "comment": "This work has been accepted for publication in IEEE-RAS 24th\n  International Conference on Humanoid Robots (Humanoids 2025). Copyrights to\n  IEEE", "summary": "Daily tasks require us to use our whole body to manipulate objects, for\ninstance when our hands are unavailable. We consider the issue of providing\nhumanoid robots with the ability to autonomously perform similar whole-body\nmanipulation tasks. In this context, the infinite possibilities for where and\nhow contact can occur on the robot and object surfaces hinder the scalability\nof existing planning methods, which predominantly rely on discrete sampling.\nGiven the continuous nature of contact surfaces, gradient-based optimization\noffers a more suitable approach for finding solutions. However, a key remaining\nchallenge is the lack of an efficient representation of robot surfaces. In this\nwork, we propose (i) a representation of robot and object surfaces that enables\nclosed-form computation of proximity points, and (ii) a cost design that\neffectively guides whole-body manipulation planning. Our experiments\ndemonstrate that the proposed framework can solve problems unaddressed by\nexisting methods, and achieves a 77% improvement in planning time over the\nstate of the art. We also validate the suitability of our approach on real\nhardware through the whole-body manipulation of boxes by a humanoid robot."}
{"id": "2508.12024", "pdf": "https://arxiv.org/pdf/2508.12024", "abs": "https://arxiv.org/abs/2508.12024", "authors": ["Georg K. J. Fischer", "Thomas Schaechtle", "Moritz Schabinger", "Alexander Richter", "Ivo Hring", "Fabian Hflinger", "Stefan J. Rupitsch"], "title": "MASSLOC: A Massive Sound Source Localization System based on Direction-of-Arrival Estimation", "categories": ["eess.AS", "eess.SP"], "comment": "IEEE Transactions on Instrumentation and Measurement", "summary": "Acoustic indoor localization offers the potential for highly accurate\nposition estimation while generally exhibiting low hardware requirements\ncompared to Radio Frequency (RF)-based solutions. Furthermore, angular-based\nlocalization significantly reduces installation effort by minimizing the number\nof required fixed anchor nodes. In this contribution, we propose the so-called\nMASSLOC system, which leverages sparse two-dimensional array geometries to\nlocalize and identify a large number of concurrently active sources.\nAdditionally, the use of complementary Zadoff-Chu sequences is introduced to\nenable efficient, beamforming-based source identification. These sequences\nprovide a trade-off between favorable correlation properties and accurate,\nunsynchronized direction-of-arrival estimation by exhibiting a spectrally\nbalanced waveform. The system is evaluated in both a controlled anechoic\nchamber and a highly reverberant lobby environment with a reverberation time of\n1.6 s. In a laboratory setting, successful direction-of-arrival estimation and\nidentification of up to 14 simultaneously emitting sources are demonstrated.\nAdopting a Perspective-n-Point (PnP) calibration approach, the system achieves\na median three-dimensional localization error of 55.7 mm and a median angular\nerror of 0.84 deg with dynamic source movement of up to 1.9 mps in the\nchallenging reverberant environment. The multi-source capability is also\ndemonstrated and evaluated in that environment with a total of three tags.\nThese results indicate the scalability and robustness of the MASSLOC system,\neven under challenging acoustic conditions."}
{"id": "2508.13052", "pdf": "https://arxiv.org/pdf/2508.13052", "abs": "https://arxiv.org/abs/2508.13052", "authors": ["Sourav Raxit", "Abdullah Al Redwan Newaz", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla"], "title": "BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces the BOW Planner, a scalable motion planning algorithm\ndesigned to navigate robots through complex environments using constrained\nBayesian optimization (CBO). Unlike traditional methods, which often struggle\nwith kinodynamic constraints such as velocity and acceleration limits, the BOW\nPlanner excels by concentrating on a planning window of reachable velocities\nand employing CBO to sample control inputs efficiently. This approach enables\nthe planner to manage high-dimensional objective functions and stringent safety\nconstraints with minimal sampling, ensuring rapid and secure trajectory\ngeneration. Theoretical analysis confirms the algorithm's asymptotic\nconvergence to near-optimal solutions, while extensive evaluations in cluttered\nand constrained settings reveal substantial improvements in computation times,\ntrajectory lengths, and solution times compared to existing techniques.\nSuccessfully deployed across various real-world robotic systems, the BOW\nPlanner demonstrates its practical significance through exceptional sample\nefficiency, safety-aware optimization, and rapid planning capabilities, making\nit a valuable tool for advancing robotic applications. The BOW Planner is\nreleased as an open-source package and videos of real-world and simulated\nexperiments are available at https://bow-web.github.io."}
{"id": "2508.12030", "pdf": "https://arxiv.org/pdf/2508.12030", "abs": "https://arxiv.org/abs/2508.12030", "authors": ["Shiraz Khan", "Jikai Ye", "Gregory S. Chirikjian"], "title": "Means of Random Variables in Lie Groups", "categories": ["math.ST", "eess.SP", "stat.TH"], "comment": null, "summary": "The concepts of mean (i.e., average) and covariance of a random variable are\nfundamental in statistics, and are used to solve real-world problems such as\nthose that arise in robotics, computer vision, and medical imaging. On matrix\nLie groups, multiple competing definitions of the mean arise, including the\nEuclidean, projected, distance-based (i.e., Fr\\'echet and Karcher),\ngroup-theoretic, and parametric means. This article provides a comprehensive\nreview of these definitions, investigates their relationships to each other,\nand determines the conditions under which the group-theoretic means minimize a\nleast-squares type cost function. We also highlight the dependence of these\ndefinitions on the choice of inner product on the Lie algebra. The goal of this\narticle is to guide practitioners in selecting an appropriate notion of the\nmean in applications involving matrix Lie groups."}
{"id": "2508.13073", "pdf": "https://arxiv.org/pdf/2508.13073", "abs": "https://arxiv.org/abs/2508.13073", "authors": ["Rui Shao", "Wei Li", "Lingsen Zhang", "Renshan Zhang", "Zhiyang Liu", "Ran Chen", "Liqiang Nie"], "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey", "categories": ["cs.RO"], "comment": "Project Page:\n  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation", "summary": "Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation."}
{"id": "2508.12233", "pdf": "https://arxiv.org/pdf/2508.12233", "abs": "https://arxiv.org/abs/2508.12233", "authors": ["Sagar Shrestha"], "title": "Communication-Efficient Distributed Asynchronous ADMM", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "In distributed optimization and federated learning, asynchronous alternating\ndirection method of multipliers (ADMM) serves as an attractive option for\nlarge-scale optimization, data privacy, straggler nodes and variety of\nobjective functions. However, communication costs can become a major bottleneck\nwhen the nodes have limited communication budgets or when the data to be\ncommunicated is prohibitively large. In this work, we propose introducing\ncoarse quantization to the data to be exchanged in aynchronous ADMM so as to\nreduce communication overhead for large-scale federated learning and\ndistributed optimization applications. We experimentally verify the convergence\nof the proposed method for several distributed learning tasks, including neural\nnetworks."}
{"id": "2508.13103", "pdf": "https://arxiv.org/pdf/2508.13103", "abs": "https://arxiv.org/abs/2508.13103", "authors": ["Tianyi Zhang", "Haonan Duan", "Haoran Hao", "Yu Qiao", "Jifeng Dai", "Zhi Hou"], "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable."}
{"id": "2508.12705", "pdf": "https://arxiv.org/pdf/2508.12705", "abs": "https://arxiv.org/abs/2508.12705", "authors": ["Yashaswini Murthy", "Bassam Bamieh", "R. Srikant"], "title": "On the Gaussian Limit of the Output of IIR Filters", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "8 pages, 1 figure, accepted for publication IEEE Conference on\n  Decision and Control 2025", "summary": "We study the asymptotic distribution of the output of a stable Linear\nTime-Invariant (LTI) system driven by a non-Gaussian stochastic input.\nMotivated by longstanding heuristics in the stochastic describing function\nmethod, we rigorously characterize when the output process becomes\napproximately Gaussian, even when the input is not. Using the Wasserstein-1\ndistance as a quantitative measure of non-Gaussianity, we derive upper bounds\non the distance between the appropriately scaled output and a standard normal\ndistribution. These bounds are obtained via Stein's method and depend\nexplicitly on the system's impulse response and the dependence structure of the\ninput process. We show that when the dominant pole of the system approaches the\nedge of stability and the input satisfies one of the following conditions: (i)\nindependence, (ii) positive correlation with a real and positive dominant pole,\nor (iii) sufficient correlation decay, the output converges to a standard\nnormal distribution at rate $O(1/\\sqrt{t})$. We also present counterexamples\nwhere convergence fails, thereby motivating the stated assumptions. Our results\nprovide a rigorous foundation for the widespread observation that outputs of\nlow-pass LTI systems tend to be approximately Gaussian."}
{"id": "2508.13151", "pdf": "https://arxiv.org/pdf/2508.13151", "abs": "https://arxiv.org/abs/2508.13151", "authors": ["Yuying Zhang", "Joni Pajarinen"], "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Mobile manipulation in dynamic environments is challenging due to movable\nobstacles blocking the robot's path. Traditional methods, which treat\nnavigation and manipulation as separate tasks, often fail in such\n'manipulate-to-navigate' scenarios, as obstacles must be removed before\nnavigation. In these cases, active interaction with the environment is required\nto clear obstacles while ensuring sufficient space for movement. To address the\nmanipulate-to-navigate problem, we propose a reinforcement learning-based\napproach for learning manipulation actions that facilitate subsequent\nnavigation. Our method combines manipulability priors to focus the robot on\nhigh manipulability body positions with affordance maps for selecting\nhigh-quality manipulation actions. By focusing on feasible and meaningful\nactions, our approach reduces unnecessary exploration and allows the robot to\nlearn manipulation strategies more effectively. We present two new\nmanipulate-to-navigate simulation tasks called Reach and Door with the Boston\nDynamics Spot robot. The first task tests whether the robot can select a good\nhand position in the target area such that the robot base can move effectively\nforward while keeping the end effector position fixed. The second task requires\nthe robot to move a door aside in order to clear the navigation path. Both of\nthese tasks need first manipulation and then navigating the base forward.\nResults show that our method allows a robot to effectively interact with and\ntraverse dynamic environments. Finally, we transfer the learned policy to a\nreal Boston Dynamics Spot robot, which successfully performs the Reach task."}
{"id": "2508.12742", "pdf": "https://arxiv.org/pdf/2508.12742", "abs": "https://arxiv.org/abs/2508.12742", "authors": ["Theodoros Bermperidis", "Joe Vero", "Elizabeth B Torres"], "title": "On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.SP", "nlin.CD"], "comment": "This paper is under review in IEEE Transactions on Affective\n  Computing", "summary": "There is a tradeoff between attaining statistical power with large, difficult\nto gather data sets, and producing highly scalable assays that register brief\ndata samples. Often, as grand-averaging techniques a priori assume\nnormally-distributed parameters and linear, stationary processes in\nbiorhythmic, time series data, important information is lost, averaged out as\ngross data. We developed an affective computing platform that enables taking\nbrief data samples while maintaining personalized statistical power. This is\nachieved by combining a new data type derived from the micropeaks present in\ntime series data registered from brief (5-second-long) face videos with recent\nadvances in AI-driven face-grid estimation methods. By adopting geometric and\nnonlinear dynamical systems approaches to analyze the kinematics, especially\nthe speed data, the new methods capture all facial micropeaks. These include as\nwell the nuances of different affective micro expressions. We offer new ways to\ndifferentiate dynamical and geometric patterns present in autistic individuals\nfrom those found more commonly in neurotypical development."}
{"id": "2508.11679", "pdf": "https://arxiv.org/pdf/2508.11679", "abs": "https://arxiv.org/abs/2508.11679", "authors": ["Shaodi Feng", "Zhuoyi Lin", "Jianan Zhou", "Cong Zhang", "Jingwen Li", "Kuan-Wen Chen", "Senthilnath Jayavelu", "Yew-Soon Ong"], "title": "Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Deep learning has been extensively explored to solve vehicle routing problems\n(VRPs), which yields a range of data-driven neural solvers with promising\noutcomes. However, most neural solvers are trained to tackle VRP instances in a\nrelatively monotonous context, e.g., simplifying VRPs by using Euclidean\ndistance between nodes and adhering to a single problem size, which harms their\noff-the-shelf application in different scenarios. To enhance their versatility,\nthis paper presents a novel lifelong learning framework that incrementally\ntrains a neural solver to manage VRPs in distinct contexts. Specifically, we\npropose a lifelong learner (LL), exploiting a Transformer network as the\nbackbone, to solve a series of VRPs. The inter-context self-attention mechanism\nis proposed within LL to transfer the knowledge obtained from solving preceding\nVRPs into the succeeding ones. On top of that, we develop a dynamic context\nscheduler (DCS), employing the cross-context experience replay to further\nfacilitate LL looking back on the attained policies of solving preceding VRPs.\nExtensive results on synthetic and benchmark instances (problem sizes up to\n18k) show that our LL is capable of discovering effective policies for tackling\ngeneric VRPs in varying contexts, which outperforms other neural solvers and\nachieves the best performance for most VRPs."}
{"id": "2508.11799", "pdf": "https://arxiv.org/pdf/2508.11799", "abs": "https://arxiv.org/abs/2508.11799", "authors": ["Arshiya Taj Abdul", "Augustinos D. Saravanos", "Evangelos A. Theodorou"], "title": "Scaling Robust Optimization for Swarms: A Distributed Perspective", "categories": ["math.OC", "cs.RO"], "comment": null, "summary": "This article introduces a decentralized robust optimization framework for\nsafe multi-agent control under uncertainty. Although stochastic noise has been\nthe primary form of modeling uncertainty in such systems, these formulations\nmight fall short in addressing uncertainties that are deterministic in nature\nor simply lack probabilistic data. To ensure safety under such scenarios, we\nemploy the concept of robust constraints that must hold for all possible\nuncertainty realizations lying inside a bounded set. Nevertheless, standard\nrobust optimization approaches become intractable due to the large number or\nnon-convexity of the constraints involved in safe multi-agent control. To\naddress this, we introduce novel robust reformulations that significantly\nreduce complexity without compromising safety. The applicability of the\nframework is further broadened to address both deterministic and stochastic\nuncertainties by incorporating robust chance constraints and distribution\nsteering techniques. To achieve scalability, we derive a distributed approach\nbased on the Alternating Direction Method of Multipliers (ADMM), supported by a\nconvergence study that accounts for the underlying non-convexity. In addition,\ncomputational complexity bounds highlighting the efficiency of the proposed\nframeworks against standard approaches are presented. Finally, the robustness\nand scalability of the framework is demonstrated through extensive simulation\nresults across diverse scenarios, including environments with nonconvex\nobstacles and up to 246 agents."}
{"id": "2508.11805", "pdf": "https://arxiv.org/pdf/2508.11805", "abs": "https://arxiv.org/abs/2508.11805", "authors": ["Xinyun Zou", "Jorge Gamez", "Meghna Menon", "Phillip Ring", "Chadwick Boulay", "Likhith Chitneni", "Jackson Brennecke", "Shana R. Melby", "Gracy Kureel", "Kelsie Pejsa", "Emily R. Rosario", "Ausaf A. Bari", "Aniruddh Ravindran", "Tyson Aflalo", "Spencer S. Kellis", "Dimitar Filev", "Florian Solzbacher", "Richard A. Andersen"], "title": "Control of a commercial vehicle by a tetraplegic human using a bimanual brain-computer interface", "categories": ["eess.SY", "cs.NE", "cs.RO", "cs.SY"], "comment": "41 pages, 7 figures, 1 table. 22 supplementary pages, 6 supplementary\n  figures, 11 supplementary tables, 9 supplementary movies available as\n  ancillary files", "summary": "Brain-computer interfaces (BCIs) read neural signals directly from the brain\nto infer motor planning and execution. However, the implementation of this\ntechnology has been largely limited to laboratory settings, with few real-world\napplications. We developed a bimanual BCI system to drive a vehicle in both\nsimulated and real-world environments. We demonstrate that an individual with\ntetraplegia, implanted with intracortical BCI electrodes in the posterior\nparietal cortex (PPC) and the hand knob region of the motor cortex (MC), reacts\nat least as fast and precisely as motor intact participants, and drives a\nsimulated vehicle as proficiently as the same control group. This BCI\nparticipant, living in California, could also remotely drive a Ford Mustang\nMach-E vehicle in Michigan. Our first teledriving task relied on cursor control\nfor speed and steering in a closed urban test facility. However, the final BCI\nsystem added click control for full-stop braking and thus enabled bimanual\ncursor-and-click control for both simulated driving through a virtual town with\ntraffic and teledriving through an obstacle course without traffic in the real\nworld. We also demonstrate the safety and feasibility of BCI-controlled\ndriving. This first-of-its-kind implantable BCI application not only highlights\nthe versatility and innovative potentials of BCIs but also illuminates the\npromising future for the development of life-changing solutions to restore\nindependence to those who suffer catastrophic neurological injury."}
{"id": "2508.11834", "pdf": "https://arxiv.org/pdf/2508.11834", "abs": "https://arxiv.org/abs/2508.11834", "authors": ["Hamza Kheddar", "Yassine Habchi", "Mohamed Chahine Ghanem", "Mustapha Hemis", "Dusit Niyato"], "title": "Recent Advances in Transformer and Large Language Models for UAV Applications", "categories": ["cs.CV", "cs.AI", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "The rapid advancement of Transformer-based models has reshaped the landscape\nof uncrewed aerial vehicle (UAV) systems by enhancing perception,\ndecision-making, and autonomy. This review paper systematically categorizes and\nevaluates recent developments in Transformer architectures applied to UAVs,\nincluding attention mechanisms, CNN-Transformer hybrids, reinforcement learning\nTransformers, and large language models (LLMs). Unlike previous surveys, this\nwork presents a unified taxonomy of Transformer-based UAV models, highlights\nemerging applications such as precision agriculture and autonomous navigation,\nand provides comparative analyses through structured tables and performance\nbenchmarks. The paper also reviews key datasets, simulators, and evaluation\nmetrics used in the field. Furthermore, it identifies existing gaps in the\nliterature, outlines critical challenges in computational efficiency and\nreal-time deployment, and offers future research directions. This comprehensive\nsynthesis aims to guide researchers and practitioners in understanding and\nadvancing Transformer-driven UAV technologies."}
{"id": "2508.11950", "pdf": "https://arxiv.org/pdf/2508.11950", "abs": "https://arxiv.org/abs/2508.11950", "authors": ["Tingbang Liang", "Yixin Zeng", "Jiatong Xie", "Boyu Zhou"], "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects."}
{"id": "2508.12304", "pdf": "https://arxiv.org/pdf/2508.12304", "abs": "https://arxiv.org/abs/2508.12304", "authors": ["Hao Li"], "title": "Adjustable AprilTags For Identity Secured Tasks", "categories": ["cs.CR", "cs.RO"], "comment": null, "summary": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones."}
{"id": "2508.13032", "pdf": "https://arxiv.org/pdf/2508.13032", "abs": "https://arxiv.org/abs/2508.13032", "authors": ["Nicolas Bousquet", "Remy El Sabeh", "Amer E. Mouawad", "Naomi Nishimura"], "title": "On the complexity of constrained reconfiguration and motion planning", "categories": ["cs.CC", "cs.DM", "cs.DS", "cs.RO", "math.CO"], "comment": null, "summary": "Coordinating the motion of multiple agents in constrained environments is a\nfundamental challenge in robotics, motion planning, and scheduling. A\nmotivating example involves $n$ robotic arms, each represented as a line\nsegment. The objective is to rotate each arm to its vertical orientation, one\nat a time (clockwise or counterclockwise), without collisions nor rotating any\narm more than once. This scenario is an example of the more general\n$k$-Compatible Ordering problem, where $n$ agents, each capable of $k$\nstate-changing actions, must transition to specific target states under\nconstraints encoded as a set $\\mathcal{G}$ of $k$ pairs of directed graphs.\n  We show that $k$-Compatible Ordering is $\\mathsf{NP}$-complete, even when\n$\\mathcal{G}$ is planar, degenerate, or acyclic. On the positive side, we\nprovide polynomial-time algorithms for cases such as when $k = 1$ or\n$\\mathcal{G}$ has bounded treewidth. We also introduce generalized variants\nsupporting multiple state-changing actions per agent, broadening the\napplicability of our framework. These results extend to a wide range of\nscheduling, reconfiguration, and motion planning applications in constrained\nenvironments."}
{"id": "2508.13104", "pdf": "https://arxiv.org/pdf/2508.13104", "abs": "https://arxiv.org/abs/2508.13104", "authors": ["Yuang Wang", "Chao Wen", "Haoyu Guo", "Sida Peng", "Minghan Qin", "Hujun Bao", "Xiaowei Zhou", "Ruizhen Hu"], "title": "Precise Action-to-Video Generation Through Visual Action Prompts", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICCV 2025. Project page: https://zju3dv.github.io/VAP/", "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/."}
{"id": "2508.13142", "pdf": "https://arxiv.org/pdf/2508.13142", "abs": "https://arxiv.org/abs/2508.13142", "authors": ["Zhongang Cai", "Yubo Wang", "Qingping Sun", "Ruisi Wang", "Chenyang Gu", "Wanqi Yin", "Zhiqian Lin", "Zhitao Yang", "Chen Wei", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Jiaqi Li", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.RO"], "comment": null, "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models."}
