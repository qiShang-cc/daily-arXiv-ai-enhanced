{"id": "2509.09695", "pdf": "https://arxiv.org/pdf/2509.09695", "abs": "https://arxiv.org/abs/2509.09695", "authors": ["Fabio Magarelli", "Geraldine B. Boylan", "Saeed Montazeri", "Feargal O'Sullivan", "Dominic Lightbody", "Minoo Ashoori", "Tamara Skoric Ceranic", "John M. O'Toole"], "title": "Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy", "categories": ["eess.SP", "cs.LG"], "comment": "29 pages, supplementary materials: \"supplementary materials ML\n  Comp.docx\"", "summary": "Machine learning (ML) has the potential to support and improve expert\nperformance in monitoring the brain function of at-risk newborns. Developing\naccurate and reliable ML models depends on access to high-quality, annotated\ndata, a resource in short supply. ML competitions address this need by\nproviding researchers access to expertly annotated datasets, fostering shared\nlearning through direct model comparisons, and leveraging the benefits of\ncrowdsourcing diverse expertise. We compiled a retrospective dataset containing\n353 hours of EEG from 102 individual newborns from a multi-centre study. The\ndata was fully anonymised and divided into training, testing, and held-out\nvalidation datasets. EEGs were graded for the severity of abnormal background\npatterns. Next, we created a web-based competition platform and hosted a\nmachine learning competition to develop ML models for classifying the severity\nof EEG background patterns in newborns. After the competition closed, the top 4\nperforming models were evaluated offline on a separate held-out validation\ndataset. Although a feature-based model ranked first on the testing dataset,\ndeep learning models generalised better on the validation sets. All methods had\na significant decline in validation performance compared to the testing\nperformance. This highlights the challenges for model generalisation on unseen\ndata, emphasising the need for held-out validation datasets in ML studies with\nneonatal EEG. The study underscores the importance of training ML models on\nlarge and diverse datasets to ensure robust generalisation. The competition's\noutcome demonstrates the potential for open-access data and collaborative ML\ndevelopment to foster a collaborative research environment and expedite the\ndevelopment of clinical decision-support tools for neonatal neuromonitoring.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7ade\u8d5b\u5e73\u53f0\u5f00\u53d1\u65b0\u751f\u513f\u8111\u7535\u56fe\u80cc\u666f\u6a21\u5f0f\u5206\u7c7b\u6a21\u578b\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63d0\u5347\u65b0\u751f\u513f\u8111\u529f\u80fd\u76d1\u6d4b\u7684\u4e13\u5bb6\u8868\u73b0\uff0c\u89e3\u51b3\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b353\u5c0f\u65f6\u8111\u7535\u56fe\u7684\u591a\u4e2d\u5fc3\u56de\u987e\u6027\u6570\u636e\u96c6\uff0c\u4e3e\u529e\u673a\u5668\u5b66\u4e60\u7ade\u8d5b\uff0c\u8bc4\u4f30\u524d4\u540d\u6a21\u578b\u5728\u72ec\u7acb\u9a8c\u8bc1\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7279\u5f81\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6cdb\u5316\u66f4\u597d\uff1b\u6240\u6709\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8bad\u7ec3\u6a21\u578b\u9700\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u4ee5\u4fdd\u8bc1\u6cdb\u5316\uff0c\u5f00\u653e\u6570\u636e\u548c\u534f\u4f5c\u5f0f\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u80fd\u52a0\u901f\u4e34\u5e8a\u51b3\u7b56\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.09820", "pdf": "https://arxiv.org/pdf/2509.09820", "abs": "https://arxiv.org/abs/2509.09820", "authors": ["Ahmed Ali Abbasi", "Namrata Vaswani"], "title": "Locally Permuted Low Rank Column-wise Sensing", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "We precisely formulate, and provide a solution for, the Low Rank Columnwise\nSensing (LRCS) problem when some of the observed data is\nscrambled/permuted/unlabeled. This problem, which we refer to as permuted LRCS,\nlies at the intersection of two distinct topics of recent research: unlabeled\nsensing and low rank column-wise (matrix) sensing. We introduce a novel\ngeneralization of the recently developed Alternating Gradient Descent and\nMinimization (AltGDMin) algorithm to solve this problem. We also develop an\nalternating minimization (AltMin) solution. We show, using simulation\nexperiments, that both converge but PermutedAltGDmin is much faster than\nPermuted-AltMin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u89e3\u51b3\u4f4e\u79e9\u5217\u611f\u77e5\uff08LRCS\uff09\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u6570\u636e\u88ab\u6270\u4e71\u6216\u6392\u5217\u7684\u60c5\u51b5\uff0c\u79f0\u4e3a\u6392\u5217LRCS\u3002\u901a\u8fc7\u6539\u8fdb\u7684\u4ea4\u66ff\u68af\u5ea6\u4e0b\u964d\u4e0e\u6700\u5c0f\u5316\uff08AltGDMin\uff09\u7b97\u6cd5\u548c\u4ea4\u66ff\u6700\u5c0f\u5316\uff08AltMin\uff09\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u524d\u8005\u6536\u655b\u66f4\u5feb\u3002", "motivation": "\u7814\u7a76\u6392\u5217LRCS\u95ee\u9898\uff0c\u7ed3\u5408\u4e86\u65e0\u6807\u8bb0\u611f\u77e5\u548c\u4f4e\u79e9\u5217\u611f\u77e5\u7684\u4e24\u4e2a\u524d\u6cbf\u9886\u57df\uff0c\u65e8\u5728\u89e3\u51b3\u90e8\u5206\u6570\u636e\u88ab\u6270\u4e71\u65f6\u7684\u611f\u77e5\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684AltGDMin\u7b97\u6cd5\u548cAltMin\u7b97\u6cd5\uff0c\u5206\u522b\u7528\u4e8e\u89e3\u51b3\u6392\u5217LRCS\u95ee\u9898\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u5747\u80fd\u6536\u655b\uff0c\u4f46AltGDMin\u7684\u6536\u655b\u901f\u5ea6\u663e\u8457\u5feb\u4e8eAltMin\u3002", "conclusion": "\u6392\u5217LRCS\u95ee\u9898\u53ef\u901a\u8fc7\u6539\u8fdb\u7684AltGDMin\u9ad8\u6548\u89e3\u51b3\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6548\u679c\u4e0a\u4f18\u4e8e\u4f20\u7edf\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\u3002"}}
{"id": "2509.09837", "pdf": "https://arxiv.org/pdf/2509.09837", "abs": "https://arxiv.org/abs/2509.09837", "authors": ["Jiapei Tian", "Abolfazl Zakeri", "Marian Codreanu", "David Gundleg\u00e5rd"], "title": "Real-Time Remote Tracking with State-Dependent Detection Probability: A POMDP Framework", "categories": ["eess.SP"], "comment": null, "summary": "We consider a real-time tracking system where a binary Markov source is\nmonitored by two heterogeneous sensors. Upon command, sensors send their\nobservations to a remote sink over error-prone channels. We assume each sensor\nexhibits state-dependent detection accuracy and may occasionally fail to detect\nthe source state. At most one sensor is scheduled for sampling at each time\nslot. We assess the effectiveness of data communication using a generic\ndistortion function that captures the end application's objective. We derive\noptimal sink-side command policies to minimize the weighted sum of distortion\nand transmission costs. To model the uncertainty introduced by sensing failures\n(of the sensors) and packet loss, we formulate the problem as a partially\nobservable Markov decision process (POMDP), which we then cast into a\nbelief-MDP. Since the belief evolves continuously, the belief space is\ndiscretized into a finite grid and the belief value is quantized to the nearest\ngrid point after each update. This formulation leads to a finite-state MDP\nproblem, which is solved using the relative value iteration algorithm (RVIA).\nSimulation results demonstrate that the proposed policy significantly\noutperforms benchmark strategies and highlights the importance of accounting\nfor state-dependent sensing reliability in sensor scheduling.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u7531\u4e24\u4e2a\u5f02\u6784\u4f20\u611f\u5668\u76d1\u63a7\u7684\u5b9e\u65f6\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u611f\u5668\u8c03\u5ea6\u548c\u6570\u636e\u4f20\u8f93\u7b56\u7565\uff0c\u4f7f\u7528POMDP\u6a21\u578b\u89e3\u51b3\u4f20\u611f\u5931\u8d25\u548c\u4e22\u5305\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6700\u7ec8\u5c55\u793a\u6240\u63d0\u7b56\u7565\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u4f20\u611f\u5668\u5728\u4f20\u611f\u5931\u8d25\u548c\u4f20\u8f93\u4e22\u5305\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u8c03\u5ea6\u95ee\u9898\uff0c\u4ee5\u6700\u5c0f\u5316\u5931\u771f\u548c\u4f20\u8f93\u6210\u672c\u3002", "method": "\u91c7\u7528POMDP\u6a21\u578b\u5e76\u5c06\u5176\u8f6c\u5316\u4e3abelief-MDP\uff0c\u901a\u8fc7\u7f51\u683c\u79bb\u6563\u5316\u548cRVIA\u7b97\u6cd5\u6c42\u89e3\u6709\u9650\u72b6\u6001MDP\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u6240\u63d0\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8003\u8651\u72b6\u6001\u4f9d\u8d56\u4f20\u611f\u53ef\u9760\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4f20\u611f\u5668\u8c03\u5ea6\u7b56\u7565\uff0c\u7cfb\u7edf\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u7c7b\u4f3c\u5b9e\u65f6\u8ddf\u8e2a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09842", "pdf": "https://arxiv.org/pdf/2509.09842", "abs": "https://arxiv.org/abs/2509.09842", "authors": ["Anu Tripathi", "Yang Wan", "Zhiren Zhu", "Furkan Camci", "Sheila Turcsanyi", "Jeneel Pravin Kachhadiya", "Mauricio Araiza Canizales", "Alison Brooks", "Haneesh Kesari", "Joseph Andrews", "Traci Snedden", "Peter Ferrazzano", "Christian Franck", "Rika Wright Carlsen"], "title": "Field evaluation of a wearable instrumented headband designed for measuring head kinematics", "categories": ["eess.SP", "physics.med-ph"], "comment": null, "summary": "Purpose: To study the relationship between soccer heading and the risk of\nmild traumatic brain injury (mTBI), we previously developed an instrumented\nheadband and data processing scheme to measure the angular head kinematics of\nsoccer headers. Laboratory evaluation of the headband on an anthropomorphic\ntest device showed good agreement with a reference sensor for soccer ball\nimpacts to the front of the head. In this study, we evaluate the headband in\nmeasuring the full head kinematics of soccer headers in the field. Methods: The\nheadband was evaluated under typical soccer heading scenarios (throw-ins,\ngoal-kicks, and corner-kicks) on a human subject. The measured time history and\npeak kinematics from the headband were compared with those from an instrumented\nmouthpiece, which is a widely accepted method for measuring head kinematics in\nthe field. Results: The time history agreement (CORA scores) between the\nheadband and the mouthpiece ranged from 'fair' to 'excellent', with the highest\nagreement for angular velocities (0.79 \\pm 0.08) and translational\naccelerations (0.73 \\pm 0.05) and lowest for angular accelerations (0.67 \\pm\n0.06). A Bland-Altman analysis of the peak kinematics from the headband and\nmouthpiece found the mean bias to be 40.9% (of the maximum mouthpiece reading)\nfor the angular velocity, 16.6% for the translational acceleration, and-14.1%\nfor the angular acceleration. Conclusion: The field evaluation of the\ninstrumented headband showed reasonable agreement with the mouthpiece for some\nkinematic measures and impact conditions. Future work should focus on improving\nthe headband performance across all kinematic measures.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5934\u5e26\u8bbe\u5907\u6d4b\u91cf\u8db3\u7403\u5934\u7403\u65f6\u7684\u5934\u90e8\u8fd0\u52a8\u5b66\u6570\u636e\uff0c\u5e76\u4e0e\u53e3\u8154\u4f20\u611f\u5668\u6570\u636e\u5bf9\u6bd4\uff0c\u7ed3\u679c\u663e\u793a\u4e24\u8005\u5728\u90e8\u5206\u8fd0\u52a8\u5b66\u6570\u636e\u4e0a\u5177\u6709\u8f83\u597d\u4e00\u81f4\u6027\uff0c\u672a\u6765\u9700\u6539\u8fdb\u5934\u5e26\u8bbe\u5907\u7684\u5168\u9762\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8db3\u7403\u5934\u7403\u52a8\u4f5c\u4e0e\u8f7b\u5ea6\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08mTBI\uff09\u98ce\u9669\u7684\u5173\u7cfb\uff0c\u9700\u8981\u901a\u8fc7\u53ef\u9760\u7684\u8bbe\u5907\u6d4b\u91cf\u5934\u90e8\u8fd0\u52a8\u5b66\u6570\u636e\u3002", "method": "\u5229\u7528\u5934\u5e26\u8bbe\u5907\u5728\u5178\u578b\u8db3\u7403\u5934\u7403\u573a\u666f\uff08\u63b7\u754c\u5916\u7403\u3001\u7403\u95e8\u7403\u3001\u89d2\u7403\uff09\u4e0b\u6d4b\u91cf\u5934\u90e8\u8fd0\u52a8\u5b66\uff0c\u5e76\u4e0e\u53e3\u8154\u4f20\u611f\u5668\u7684\u6d4b\u91cf\u6570\u636e\u5bf9\u6bd4\u3002", "result": "\u5934\u5e26\u4e0e\u53e3\u8154\u4f20\u611f\u5668\u7684\u65f6\u95f4\u5386\u53f2\u6570\u636e\u4e00\u81f4\u6027\u4ece\u201c\u4e00\u822c\u201d\u5230\u201c\u4f18\u79c0\u201d\uff0c\u89d2\u901f\u5ea6\u548c\u7ebf\u52a0\u901f\u5ea6\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff080.79\u00b10.08\u548c0.73\u00b10.05\uff09\uff0c\u89d2\u52a0\u901f\u5ea6\u6700\u4f4e\uff080.67\u00b10.06\uff09\u3002\u5cf0\u503c\u8fd0\u52a8\u5b66\u6570\u636e\u5b58\u5728\u4e00\u5b9a\u504f\u5dee\u3002", "conclusion": "\u5934\u5e26\u8bbe\u5907\u5728\u90e8\u5206\u8fd0\u52a8\u5b66\u6d4b\u91cf\u4e2d\u8868\u73b0\u5408\u7406\uff0c\u672a\u6765\u9700\u63d0\u5347\u5176\u5168\u9762\u6027\u80fd\u3002"}}
{"id": "2509.09769", "pdf": "https://arxiv.org/pdf/2509.09769", "abs": "https://arxiv.org/abs/2509.09769", "authors": ["Rutav Shah", "Shuijing Liu", "Qi Wang", "Zhenyu Jiang", "Sateesh Kumar", "Mingyo Seo", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos", "categories": ["cs.RO"], "comment": "11 pages, 9 figures, 5 tables", "summary": "We aim to enable humanoid robots to efficiently solve new manipulation tasks\nfrom a few video examples. In-context learning (ICL) is a promising framework\nfor achieving this goal due to its test-time data efficiency and rapid\nadaptability. However, current ICL methods rely on labor-intensive teleoperated\ndata for training, which restricts scalability. We propose using human play\nvideos -- continuous, unlabeled videos of people interacting freely with their\nenvironment -- as a scalable and diverse training data source. We introduce\nMimicDroid, which enables humanoids to perform ICL using human play videos as\nthe only training data. MimicDroid extracts trajectory pairs with similar\nmanipulation behaviors and trains the policy to predict the actions of one\ntrajectory conditioned on the other. Through this process, the model acquired\nICL capabilities for adapting to novel objects and environments at test time.\nTo bridge the embodiment gap, MimicDroid first retargets human wrist poses\nestimated from RGB videos to the humanoid, leveraging kinematic similarity. It\nalso applies random patch masking during training to reduce overfitting to\nhuman-specific cues and improve robustness to visual differences. To evaluate\nfew-shot learning for humanoids, we introduce an open-source simulation\nbenchmark with increasing levels of generalization difficulty. MimicDroid\noutperformed state-of-the-art methods and achieved nearly twofold higher\nsuccess rates in the real world. Additional materials can be found on:\nut-austin-rpl.github.io/MimicDroid", "AI": {"tldr": "\u5229\u7528\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u8fdb\u884c\u9ad8\u6548\u7684\u65b0\u4efb\u52a1\u64cd\u7eb5\uff0c\u63d0\u51faMimicDroid\u65b9\u6cd5\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u6269\u5c55ICL\u8bad\u7ec3\u6570\u636e\u6e90\uff08\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\uff09\u4ee5\u89e3\u51b3\u4f20\u7edf\u4f9d\u8d56\u4eba\u5de5\u9065\u63a7\u6570\u636e\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u65b0\u4efb\u52a1\u548c\u73af\u5883\u7684\u5b66\u4e60\u9002\u5e94\u6027\u3002", "method": "MimicDroid\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u884c\u4e3a\u76f8\u4f3c\u7684\u8f68\u8ff9\u5bf9\uff0c\u8bad\u7ec3\u7b56\u7565\u9884\u6d4b\u52a8\u4f5c\uff1b\u901a\u8fc7\u8fd0\u52a8\u5b66\u76f8\u4f3c\u6027\u6620\u5c04\u4eba\u7c7b\u624b\u8155\u59ff\u52bf\u5230\u673a\u5668\u4eba\uff0c\u5e76\u5f15\u5165\u968f\u673a\u63a9\u7801\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u6d4b\u8bd5\u4e2d\uff0cMimicDroid\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u73b0\u5b9e\u6210\u529f\u7387\u63d0\u9ad8\u8fd1\u4e00\u500d\u3002", "conclusion": "\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\u662f\u9ad8\u6548\u8bad\u7ec3ICL\u7684\u53ef\u884c\u6570\u636e\u6e90\uff0cMimicDroid\u4e3a\u673a\u5668\u4eba\u9002\u5e94\u65b0\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10009", "pdf": "https://arxiv.org/pdf/2509.10009", "abs": "https://arxiv.org/abs/2509.10009", "authors": ["Zhiwei Liang", "Bin Chen", "Jiwei Xu", "Yi Lei", "Qingqing Hu", "Fan Zhang", "Gabriele Liga"], "title": "A General Nonlinear Model for Arbitrary Modulation Formats in the Presence of Inter-Channel Simulated Raman Scattering", "categories": ["eess.SP"], "comment": "4 Pages, 2 figures", "summary": "The four-dimensional nonlinear model is extended to include the inter-channel\nstimulated Raman scattering, enabling accurate prediction of dual-polarization\nfour-dimensional modulation formats and probabilistically shaped constellations\nin high-dispersion regimes. The proposed model is validated via comparisons\nwith the split-step Fourier method and enhanced Gaussian noise model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u56db\u7ef4\u975e\u7ebf\u6027\u6a21\u578b\uff0c\u52a0\u5165\u901a\u9053\u95f4\u53d7\u6fc0\u62c9\u66fc\u6563\u5c04\u6548\u5e94\uff0c\u63d0\u5347\u4e86\u5728\u5f3a\u8272\u6563\u6761\u4ef6\u4e0b\u5bf9\u53cc\u504f\u632f\u56db\u7ef4\u8c03\u5236\u683c\u5f0f\u548c\u6982\u7387\u6574\u5f62\u661f\u5ea7\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9ad8\u8272\u6563\u6761\u4ef6\u4e0b\u5bf9\u53cc\u504f\u632f\u56db\u7ef4\u8c03\u5236\u683c\u5f0f\u548c\u6982\u7387\u6574\u5f62\u661f\u5ea7\u7684\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u6269\u5c55\u56db\u7ef4\u975e\u7ebf\u6027\u6a21\u578b\uff0c\u7eb3\u5165\u901a\u9053\u95f4\u53d7\u6fc0\u62c9\u66fc\u6563\u5c04\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u4e0e\u5206\u6b65\u5085\u91cc\u53f6\u65b9\u6cd5\u548c\u589e\u5f3a\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u7684\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u9ad8\u8272\u6563\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6269\u5c55\u6a21\u578b\u4e3a\u590d\u6742\u8c03\u5236\u683c\u5f0f\u5728\u5f3a\u8272\u6563\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2509.09805", "pdf": "https://arxiv.org/pdf/2509.09805", "abs": "https://arxiv.org/abs/2509.09805", "authors": ["Francisco M. L\u00f3pez", "Miles Lenz", "Marco G. Fedozzi", "Arthur Aubret", "Jochen Triesch"], "title": "MIMo grows! Simulating body and sensory development in a multimodal infant model", "categories": ["cs.RO"], "comment": "Accepted at IEEE ICDL 2025. 6 pages, 6 figures", "summary": "Infancy is characterized by rapid body growth and an explosive change of\nsensory and motor abilities. However, developmental robots and simulation\nplatforms are typically designed in the image of a specific age, which limits\ntheir ability to capture the changing abilities and constraints of developing\ninfants. To address this issue, we present MIMo v2, a new version of the\nmultimodal infant model. It includes a growing body with increasing actuation\nstrength covering the age range from birth to 24 months. It also features\nfoveated vision with developing visual acuity as well as sensorimotor delays\nmodeling finite signal transmission speeds to and from an infant's brain.\nFurther enhancements of this MIMo version include an inverse kinematics module,\na random environment generator and updated compatiblity with third-party\nsimulation and learning libraries. Overall, this new MIMo version permits\nincreased realism when modeling various aspects of sensorimotor development.\nThe code is available on the official repository\n(https://github.com/trieschlab/MIMo).", "AI": {"tldr": "MIMo v2\u662f\u4e00\u4e2a\u6a21\u62df\u5a74\u513f\u53d1\u5c55\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u65b0\u589e\u4e86\u52a8\u6001\u6210\u957f\u7684\u8eab\u4f53\u3001\u89c6\u89c9\u7cfb\u7edf\u548c\u4fe1\u53f7\u4f20\u8f93\u5ef6\u8fdf\u7b49\u7279\u6027\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u5e73\u53f0\u65e0\u6cd5\u6a21\u62df\u5a74\u513f\u52a8\u6001\u53d1\u5c55\u80fd\u529b\u7684\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86MIMo v2\uff0c\u5305\u542b\u52a8\u6001\u6210\u957f\u7684\u8eab\u4f53\u3001\u53d1\u80b2\u4e2d\u7684\u89c6\u89c9\u7cfb\u7edf\u3001\u4fe1\u53f7\u4f20\u8f93\u5ef6\u8fdf\u7b49\u529f\u80fd\u3002", "result": "\u65b0\u7248\u672c\u80fd\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u5a74\u513f\u611f\u5b98\u8fd0\u52a8\u53d1\u5c55\u7684\u591a\u4e2a\u65b9\u9762\u3002", "conclusion": "MIMo v2\u4e3a\u7814\u7a76\u5a74\u513f\u53d1\u5c55\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u5de5\u5177\u3002"}}
{"id": "2509.10076", "pdf": "https://arxiv.org/pdf/2509.10076", "abs": "https://arxiv.org/abs/2509.10076", "authors": ["Apostolos A. Tegos", "Yue Xiao", "Sotiris A. Tegos", "George K. Karagiannidis", "Panagiotis D. Diamantoulakis"], "title": "Uplink RSMA for Pinching-Antenna Systems", "categories": ["eess.SP"], "comment": null, "summary": "One of the key goals of next-generation wireless networks is to adapt to\nchanging conditions and meet the growing demand for reliable, high-capacity\ncommunications from emerging applications. Overcoming the limitations of\nconventional technologies, such as fixed antenna positions, is essential to\nachieving this objective because it mitigates the impact of path loss on the\nreceived signal and creates strong line-of-sight links, enhancing system\nperformance. With this in mind, the newly proposed pinching antenna systems\n(PASs) are a promising solution for indoor applications because they can\nactivate antennas across a waveguide deployed in a room, thus reducing the\ndistance between the transmitter and receiver. In this paper, we investigate a\ntwo-user, two-pinching-antenna uplink PAS, in which the transmitters use rate\nsplitting to create a more resilient framework than non-orthogonal multiple\naccess (NOMA). For this network, we derive novel closed-form expressions for\nthe outage probability. Numerical results validate these expressions, proving\nthat the proposed rate-splitting multiple access (RSMA) scheme outperforms NOMA\nPAS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86pinching\u5929\u7ebf\u7cfb\u7edf\uff08PAS\uff09\u7528\u4e8e\u5ba4\u5185\u901a\u4fe1\uff0c\u901a\u8fc7\u51cf\u5c11\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u91c7\u7528\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165\uff08RSMA\uff09\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684NOMA\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u5bf9\u9ad8\u53ef\u9760\u6027\u548c\u9ad8\u5bb9\u91cf\u901a\u4fe1\u7684\u9700\u6c42\uff0c\u9700\u8981\u514b\u670d\u4f20\u7edf\u6280\u672f\u7684\u9650\u5236\uff0c\u5982\u56fa\u5b9a\u5929\u7ebf\u4f4d\u7f6e\u5bf9\u4fe1\u53f7\u8def\u5f84\u635f\u8017\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u79cd\u4e24\u7528\u6237\u3001\u4e24pinching\u5929\u7ebf\u4e0a\u884c\u94fe\u8defPAS\uff0c\u91c7\u7528\u901f\u7387\u5206\u5272\u6280\u672f\u6784\u5efa\u6bd4\u4f20\u7edf\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\uff08NOMA\uff09\u66f4\u7a33\u5065\u7684\u6846\u67b6\u3002", "result": "\u63a8\u5bfc\u4e86\u65b0\u7684\u4e2d\u65ad\u6982\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86RSMA\u65b9\u6848\u4f18\u4e8eNOMA PAS\u3002", "conclusion": "PAS\u7ed3\u5408RSMA\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5ba4\u5185\u901a\u4fe1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09889", "pdf": "https://arxiv.org/pdf/2509.09889", "abs": "https://arxiv.org/abs/2509.09889", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "title": "Using the Pepper Robot to Support Sign Language Communication", "categories": ["cs.RO", "cs.HC"], "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4ebaPepper\u662f\u5426\u80fd\u751f\u6210\u53ef\u7406\u89e3\u7684\u610f\u5927\u5229\u624b\u8bed\uff08LIS\uff09\u52a8\u4f5c\u548c\u77ed\u53e5\uff0c\u7ed3\u679c\u8868\u660e\u5355\u4e2a\u624b\u52bf\u8bc6\u522b\u7387\u9ad8\uff0c\u4f46\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u8f83\u4f4e\uff0c\u672a\u6765\u9700\u6539\u8fdb\u591a\u6a21\u6001\u652f\u6301\u548c\u7528\u6237\u53c2\u4e0e\u8bbe\u8ba1\u3002", "motivation": "\u63a2\u7d22\u793e\u4ea4\u673a\u5668\u4eba\u5728\u804b\u4eba\u7528\u6237\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u63a8\u52a8\u66f4\u5177\u5305\u5bb9\u6027\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u4e0e\u804b\u4eba\u5b66\u751f\u548c\u624b\u8bed\u4e13\u5bb6\u5408\u4f5c\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b052\u4e2aLIS\u624b\u52bf\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e512\u540d\u719f\u7ec3LIS\u7684\u53c2\u4e0e\u8005\u3002", "result": "\u5355\u4e2a\u624b\u52bf\u8bc6\u522b\u7387\u8f83\u9ad8\uff0c\u4f46\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u663e\u8457\u8f83\u4f4e\uff0c\u53d7\u9650\u4e8e\u673a\u5668\u4eba\u5173\u8282\u548c\u65f6\u95f4\u9650\u5236\u3002", "conclusion": "\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4eba\u53ef\u5b9e\u73b0\u90e8\u5206LIS\u624b\u52bf\uff0c\u672a\u6765\u9700\u6539\u8fdb\u591a\u6a21\u6001\u652f\u6301\u548c\u7528\u6237\u53c2\u4e0e\u8bbe\u8ba1\u3002"}}
{"id": "2509.10082", "pdf": "https://arxiv.org/pdf/2509.10082", "abs": "https://arxiv.org/abs/2509.10082", "authors": ["Weitao Tang", "Johann Vargas-Calixto", "Nasim Katebi", "Nhi Tran", "Sharmony B. Kelly", "Gari D. Clifford", "Robert Galinsky", "Faezeh Marzbanrad"], "title": "FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification", "categories": ["eess.SP", "cs.LG"], "comment": "13 pages, 4 tables, 5 figures, submitted to IEEE Journal of\n  Biomedical and Health Informatics", "summary": "Introduction: This study presents FetalSleepNet, the first published deep\nlearning approach to classifying sleep states from the ovine\nelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and\nlaborious to interpret consistently. However, accurate sleep stage\nclassification may aid in the early detection of abnormal brain maturation\nassociated with pregnancy complications (e.g. hypoxia or intrauterine growth\nrestriction).\n  Methods: EEG electrodes were secured onto the ovine dura over the parietal\ncortices of 24 late gestation fetal sheep. A lightweight deep neural network\noriginally developed for adult EEG sleep staging was trained on the ovine EEG\nusing transfer learning from adult EEG. A spectral equalisation-based domain\nadaptation strategy was used to reduce cross-domain mismatch.\n  Results: We demonstrated that while direct transfer performed poorly, full\nfine tuning combined with spectral equalisation achieved the best overall\nperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming\nbaseline models.\n  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep\nlearning framework specifically developed for automated sleep staging from the\nfetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier\nfunctions as a label engine, enabling large scale weak/semi supervised labeling\nand distillation to facilitate training on less invasive signals that can be\nacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.\nFetalSleepNet's lightweight design makes it well suited for deployment in low\npower, real time, and wearable fetal monitoring systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bFetalSleepNet\uff0c\u7528\u4e8e\u4ece\u80ce\u513f\u7f8a\u76ae\u8111\u7535\u56fe(EEG)\u4e2d\u5206\u7c7b\u7761\u7720\u72b6\u6001\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u57df\u9002\u5e94\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u80ce\u513fEEG\u6570\u636e\u83b7\u53d6\u548c\u89e3\u91ca\u7684\u56f0\u96be\uff0c\u5e76\u901a\u8fc7\u51c6\u786e\u5206\u7c7b\u7761\u7720\u72b6\u6001\uff0c\u65e9\u671f\u68c0\u6d4b\u4e0e\u598a\u5a20\u5e76\u53d1\u75c7\u76f8\u5173\u7684\u8111\u53d1\u80b2\u5f02\u5e38\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u5149\u8c31\u5747\u8861\u5316\u7b56\u7565\uff0c\u51cf\u5c11\u57df\u95f4\u5dee\u5f02\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5b8c\u5168\u5fae\u8c03\u7ed3\u5408\u5149\u8c31\u5747\u8861\u5316\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738786.6%\uff0c\u5b8f\u89c2F1\u5206\u657062.5\uff09\uff0c\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "FetalSleepNet\u662f\u9996\u4e2a\u4e13\u4e3a\u80ce\u513fEEG\u7761\u7720\u5206\u7c7b\u8bbe\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u8f7b\u91cf\u8bbe\u8ba1\u9002\u5408\u4f4e\u529f\u8017\u5b9e\u65f6\u7cfb\u7edf\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u4e34\u5e8a\u5176\u4ed6\u4fe1\u53f7\u5904\u7406\u3002"}}
{"id": "2509.09893", "pdf": "https://arxiv.org/pdf/2509.09893", "abs": "https://arxiv.org/abs/2509.09893", "authors": ["Hanbit Oh", "Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Yukiyasu Domae"], "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision", "categories": ["cs.RO", "cs.AI"], "comment": "Under review", "summary": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .", "AI": {"tldr": "SART\u662f\u4e00\u79cd\u4ece\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u589e\u5f3a\u6570\u636e\uff0c\u51cf\u5c11\u4eba\u529b\u548c\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4e14\u63a2\u7d22\u8fc7\u7a0b\u7f3a\u4e4f\u5b89\u5168\u4fdd\u8bc1\uff0cSART\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SART\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u548c\u673a\u5668\u4eba\u81ea\u4e3b\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "SART\u5728\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u6bd4\u4ec5\u4f9d\u8d56\u4eba\u7c7b\u6f14\u793a\u7684\u7b56\u7565\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "SART\u9ad8\u6548\u4e14\u5b89\u5168\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2509.10088", "pdf": "https://arxiv.org/pdf/2509.10088", "abs": "https://arxiv.org/abs/2509.10088", "authors": ["Christian Eckrich", "Abdelhak M. Zoubir", "Vahid Jamali"], "title": "Resilient Vital Sign Monitoring Using RIS-Assisted Radar", "categories": ["eess.SP"], "comment": null, "summary": "Vital sign monitoring plays a critical role in healthcare and well-being, as\nparameters such as respiration and heart rate offer valuable insights into an\nindividual's physiological state. While wearable devices allow for continuous\nmeasurement, their use in settings like in-home elderly care is often hindered\nby discomfort or user noncompliance. As a result, contactless solutions based\non radar sensing have garnered increasing attention. This is due to their\nunobtrusive design and preservation of privacy advantages compared to\ncamera-based systems. However, a single radar perspective can fail to capture\nbreathing-induced chest movements reliably, particularly when the subject's\norientation is unfavorable. To address this limitation, we integrate a\nreconfigurable intelligent surface (RIS) that provides an additional sensing\npath, thereby enhancing the robustness of respiratory monitoring. We present a\nnovel model for multi-path vital sign sensing that leverages both the direct\nradar path and an RIS-reflected path. We further discuss the potential benefits\nand improved performance our approach offers in continuous, privacy-preserving\nvital sign monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u589e\u5f3a\u96f7\u8fbe\u4f20\u611f\u7684\u547c\u5438\u76d1\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5355\u4e00\u8def\u5f84\u96f7\u8fbe\u5728\u4e0d\u826f\u65b9\u5411\u4e0b\u76d1\u6d4b\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u4e0d\u8212\u9002\u6216\u7528\u6237\u4e0d\u914d\u5408\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u5c45\u5bb6\u8001\u5e74\u62a4\u7406\uff09\u5f80\u5f80\u65e0\u6cd5\u6301\u7eed\u76d1\u6d4b\u751f\u547d\u4f53\u5f81\uff0c\u56e0\u6b64\u975e\u63a5\u89e6\u5f0f\u89e3\u51b3\u65b9\u6848\uff08\u5982\u96f7\u8fbe\u4f20\u611f\uff09\u53d7\u5230\u5173\u6ce8\u3002\u4f20\u7edf\u5355\u4e00\u8def\u5f84\u96f7\u8fbe\u5728\u67d0\u4e9b\u65b9\u5411\u4e0b\u76d1\u6d4b\u547c\u5438\u8fd0\u52a8\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u8def\u5f84\u751f\u547d\u4f53\u5f81\u4f20\u611f\u6a21\u578b\uff0c\u7ed3\u5408\u76f4\u63a5\u96f7\u8fbe\u8def\u5f84\u548cRIS\u53cd\u5c04\u8def\u5f84\uff0c\u4ee5\u63d0\u9ad8\u547c\u5438\u76d1\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7RIS\u63d0\u4f9b\u989d\u5916\u7684\u4f20\u611f\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u547c\u5438\u76d1\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u53d7\u8bd5\u8005\u65b9\u5411\u4e0d\u5229\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u96c6\u6210RIS\u7684\u591a\u8def\u5f84\u4f20\u611f\u6a21\u578b\u4e3a\u8fde\u7eed\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u5728\u4f18\u52bf\u548c\u6027\u80fd\u6539\u8fdb\u3002"}}
{"id": "2509.09953", "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "categories": ["cs.RO"], "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u6a21\u578b\uff0c\u7814\u7a76\u53d1\u73b0\u6700\u4f18\u6a21\u578b\u7684\u9009\u62e9\u4e0e\u4e0a\u4e0b\u6587\u76f8\u5173\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u68c0\u6d4b\u7cfb\u7edf\u65e5\u5fd7\u4e2d\u7684\u5f02\u5e38\u6765\u9884\u9632\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528CoppeliaSim\u6536\u96c6\u4e24\u79cd\u573a\u666f\u7684\u7cfb\u7edf\u65e5\u5fd7\uff0c\u5e76\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u81ea\u7f16\u7801\u5668\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u903b\u8f91\u56de\u5f52\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u81ea\u7f16\u7801\u5668\u5728Pioneer\u673a\u5668\u4eba\u573a\u666f\u4e2d\u6548\u679c\u6700\u597d\uff0c\u8868\u660e\u6700\u4f18\u6a21\u578b\u4f9d\u4e0a\u4e0b\u6587\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u6bd4\u8f83\u65b9\u6cd5\u7684\u4ef7\u503c\uff0c\u5e76\u8bc1\u660e\u81ea\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u590d\u6742\u5f02\u5e38\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.10281", "pdf": "https://arxiv.org/pdf/2509.10281", "abs": "https://arxiv.org/abs/2509.10281", "authors": ["Sudeepini Darapu", "Subrata Ghosh", "Dibakar Ghosh", "Chittaranjan Hens", "Santosh Nannuru"], "title": "Real-time identification and control of influential pandemic regions using graph signal variation", "categories": ["eess.SP"], "comment": "12 pages, 13 figures", "summary": "The global spread of pandemics is facilitated by the mobility of populations,\ntransforming localized infections into widespread phenomena. To contain it,\ntimely identification of influential regions that accelerate this process is\nnecessary. In this work, we model infection as a temporally evolving graph\nsignal and propose graph signal variation-based metrics to capture\nspatio-temporal changes. Both graph domain and time domain locality are\nmodeled. Based on this metric, we propose an online algorithm to identify\ninfluential regions. Simulations demonstrate that the proposed method\neffectively identifies geographical regions with a higher capacity to spread\nthe infection. Isolating these regions leads to a significant reduction in\ncumulative infection. Simulations, along with analyses of hybrid H1N1 data and\nreal-world Indian COVID-19 data, underscore the utility of proposed metric in\nenhancing our understanding and control of infection spread", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4fe1\u53f7\u53d8\u5316\u5ea6\u91cf\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u8bc6\u522b\u75ab\u60c5\u4f20\u64ad\u4e2d\u7684\u5173\u952e\u533a\u57df\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5168\u7403\u75ab\u60c5\u7684\u6269\u6563\u4e0e\u4eba\u53e3\u6d41\u52a8\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u53ca\u65f6\u8bc6\u522b\u52a0\u901f\u4f20\u64ad\u7684\u5173\u952e\u533a\u57df\u4ee5\u5b9e\u65bd\u9694\u79bb\u63aa\u65bd\u3002", "method": "\u5c06\u611f\u67d3\u5efa\u6a21\u4e3a\u65f6\u95f4\u6f14\u5316\u7684\u56fe\u4fe1\u53f7\uff0c\u7ed3\u5408\u56fe\u57df\u548c\u65f6\u95f4\u57df\u5c40\u90e8\u6027\uff0c\u63d0\u51fa\u56fe\u4fe1\u53f7\u53d8\u5316\u5ea6\u91cf\uff0c\u5e76\u5f00\u53d1\u5728\u7ebf\u7b97\u6cd5\u8bc6\u522b\u5173\u952e\u533a\u57df\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\uff08H1N1\u548c\u5370\u5ea6COVID-19\u6570\u636e\uff09\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4f20\u64ad\u80fd\u529b\u5f3a\u7684\u533a\u57df\uff0c\u9694\u79bb\u8fd9\u4e9b\u533a\u57df\u53ef\u663e\u8457\u51cf\u5c11\u7d2f\u8ba1\u611f\u67d3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7406\u89e3\u548c\u63a7\u5236\u75ab\u60c5\u4f20\u64ad\uff0c\u4e3a\u516c\u5171\u536b\u751f\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2509.10007", "pdf": "https://arxiv.org/pdf/2509.10007", "abs": "https://arxiv.org/abs/2509.10007", "authors": ["Samuli Soutukorva", "Markku Suomalainen", "Martin Kollingbaum", "Tapio Heikkil\u00e4"], "title": "Gaussian path model library for intuitive robot motion programming by demonstration", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a system for generating Gaussian path models from\nteaching data representing the path shape. In addition, methods for using these\npath models to classify human demonstrations of paths are introduced. By\ngenerating a library of multiple Gaussian path models of various shapes, human\ndemonstrations can be used for intuitive robot motion programming. A method for\nmodifying existing Gaussian path models by demonstration through geometric\nanalysis is also presented.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6559\u5b66\u6570\u636e\u751f\u6210\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u5bf9\u8def\u5f84\u7684\u4eba\u7c7b\u6f14\u793a\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u6f14\u793a\u4fee\u6539\u73b0\u6709\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u5e93\u5b9e\u73b0\u76f4\u89c2\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\uff0c\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u8fdb\u884c\u8def\u5f84\u5206\u7c7b\u548c\u6a21\u578b\u4fee\u6539\u3002", "method": "\u4ece\u6559\u5b66\u6570\u636e\u751f\u6210\u9ad8\u65af\u8def\u5f84\u6a21\u578b\uff0c\u5efa\u7acb\u591a\u79cd\u5f62\u72b6\u7684\u6a21\u578b\u5e93\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u6f14\u793a\u4fee\u6539\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u548c\u4fee\u6539\u9ad8\u65af\u8def\u5f84\u6a21\u578b\uff0c\u5e76\u7528\u4e8e\u8def\u5f84\u5206\u7c7b\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u4e14\u9ad8\u6548\u7684\u65b9\u5f0f\uff0c\u9002\u7528\u4e8e\u4eba\u7c7b\u6f14\u793a\u7684\u8def\u5f84\u5206\u7c7b\u4e0e\u6a21\u578b\u66f4\u65b0\u3002"}}
{"id": "2509.10296", "pdf": "https://arxiv.org/pdf/2509.10296", "abs": "https://arxiv.org/abs/2509.10296", "authors": ["Cheng Luo", "Jie Hu", "Luping Xiang", "Kun Yang", "Zhiqin Wang"], "title": "Low-Complexity Null-Space-Based Simultaneous Wireless Information and Power Transfer Scheme", "categories": ["eess.SP"], "comment": null, "summary": "Simultaneous wireless information and power transfer (SWIPT) has attracted\nsustained interest. We propose a null-space-based transmission scheme for\nmultiuser SWIPT serving both energy users (EUs) and information users (IUs).\nUnder a practical nonlinear energy-harvesting (EH) model and multiple waveform\noptions, we revisit the role of dedicated energy beams (EBs). We show that, in\ngeneral, dedicated EBs are unnecessary because information beams (IBs) with\nGaussian signaling can simultaneously support wireless energy transfer (WET)\nand wireless information transfer (WIT), unless special energy-centric\nwaveforms (e.g., deterministic sinusoidal waveforms) are employed and provide\nsufficient gains. Guided by these insights, we formulate an optimization\nproblem for EB design to enable dedicated waveform transmission for WET, and we\ndevelop a low-complexity algorithm that reduces computation by ignoring the WET\ncontribution of IBs during optimization. Numerical results corroborate that\ndeterministic sinusoidal waveforms outperform Gaussian signaling when the\nreceived RF power lies in the EH high-efficiency region, making dedicated EBs\nbeneficial. The proposed scheme achieves computational complexity reductions of\n91.43\\% and 98.54\\% for the cases $M=8,,K^I=K^E=2$ and $M=16,,K^I=K^E=4$,\nrespectively, with negligible performance loss, thereby validating the\nefficiency of the low-complexity algorithm.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u975e\u7ebf\u6027\u80fd\u91cf\u6536\u96c6\u6a21\u578b\u4e0b\uff0c\u591a\u7528\u6237SWIPT\u4e2d\u4e13\u7528\u80fd\u91cf\u6ce2\u675f\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u7814\u7a76\u5728\u591a\u7528\u6237SWIPT\u7cfb\u7edf\u4e2d\uff0c\u4e13\u7528\u80fd\u91cf\u6ce2\u675f\uff08EBs\uff09\u662f\u5426\u5fc5\u8981\uff0c\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u8bbe\u8ba1\u4ee5\u5b9e\u73b0\u9ad8\u6548\u80fd\u7684\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u7a7a\u95f4\u7684\u4f20\u8f93\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u8bbe\u8ba1\u548c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u9a8c\u8bc1\u4e86EB\u7684\u4f5c\u7528\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u4e13\u7528EBs\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4e14\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u5728\u975e\u7ebf\u6027EH\u6a21\u578b\u4e2d\uff0c\u4fe1\u606f\u6ce2\u675f\u901a\u5e38\u8db3\u4ee5\u652f\u6301WET\u548cWIT\uff0c\u4f46\u5728\u7279\u5b9a\u6ce2\u5f62\u4e0b\uff0c\u4e13\u7528EBs\u80fd\u63d0\u5347\u6027\u80fd\uff1b\u63d0\u51fa\u7684\u7b97\u6cd5\u9ad8\u6548\u4e14\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002"}}
{"id": "2509.10012", "pdf": "https://arxiv.org/pdf/2509.10012", "abs": "https://arxiv.org/abs/2509.10012", "authors": ["Richard Matthias Hartisch", "Alexander Rother", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Towards simulation-based optimization of compliant fingers for high-speed connector assembly", "categories": ["cs.RO"], "comment": null, "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u7684\u67d4\u987a\u673a\u6784\u8bbe\u8ba1\u5de5\u5177\uff0c\u7528\u4e8e\u4f18\u5316\u4efb\u52a1\u7ea7\u76ee\u6807\u7684\u8bbe\u8ba1\u53c2\u6570\uff0c\u63d0\u5347\u63d2\u5165\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u67d4\u987a\u6027\u662f\u52a8\u6001\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff0c\u73b0\u6709\u8bbe\u8ba1\u65b9\u6cd5\uff08\u786c\u4ef6\u8fed\u4ee3\u6216\u7b80\u5316\u6a21\u578b\uff09\u6548\u7387\u4f4e\u6216\u65e0\u6cd5\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4eff\u771f\u5de5\u5177\uff0c\u4f18\u5316\u67d4\u987a\u624b\u6307\u7684\u8bbe\u8ba1\u53c2\u6570\uff0c\u5e76\u901a\u8fc7NIST\u57fa\u51c6\u4efb\u52a1\u677f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u9c81\u68d2\u6027\u3002", "result": "\u4f18\u5316\u53c2\u6570\u53ef\u663e\u8457\u63d0\u5347\u5bb9\u5fcd\u8303\u56f4\uff08\u8fbe2.29\u500d\uff09\uff0c\u8865\u507f\u5de5\u4ef6\u53d8\u5f02\u6027\u8fbe8.6 mm\uff0c\u4f46\u8d8b\u52bf\u56e0\u4efb\u52a1\u800c\u5f02\u3002", "conclusion": "\u8bbe\u8ba1\u5de5\u5177\u9700\u8003\u8651\u5e94\u7528\u7279\u5b9a\u7684\u51e0\u4f55\u4e0e\u52a8\u6001\u7279\u6027\uff0c\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u4f18\u5316\u67d4\u987a\u6027\u3002"}}
{"id": "2509.10357", "pdf": "https://arxiv.org/pdf/2509.10357", "abs": "https://arxiv.org/abs/2509.10357", "authors": ["Simon Svendsen", "Dimitri Gold", "Christian Rom", "Volker Pauli", "Vuokko Nurmela"], "title": "Realistic UE Antennas for 6G in the 3GPP Channel Model", "categories": ["eess.SP", "cs.NI", "94A05, 78M31", "C.2.1; I.6.5"], "comment": "This is a tutorial paper with the limit of 4500 words, 6\n  Fgiures/Tables and 15 refernces", "summary": "The transition to 6G has driven significant updates to the 3GPP channel\nmodel, particularly in modeling UE antennas and user-induced blockage for\nhandheld devices. The 3GPP Rel.19 revision of TR 38.901 introduces a more\nrealistic framework that captures directive antenna patterns, practical antenna\nplacements, polarization effects, and element-specific blockage. These updates\nare based on high-fidelity simulations and measurements of a reference\nsmartphone across multiple frequency ranges. By aligning link- and system-level\nsimulations with real-world device behavior, the new model enables more\naccurate evaluation of 6G technologies and supports consistent performance\nassessment across industry and research.", "AI": {"tldr": "3GPP Rel.19\u4fee\u8ba2\u4e86TR 38.901\uff0c\u5f15\u5165\u66f4\u771f\u5b9e\u76846G\u4fe1\u9053\u6a21\u578b\uff0c\u6db5\u76d6\u5b9a\u5411\u5929\u7ebf\u3001\u7528\u6237\u906e\u6321\u7b49\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u6280\u672f\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u6a21\u62df6G\u73af\u5883\u4e0b\u624b\u6301\u8bbe\u5907\u7684\u4fe1\u9053\u7279\u6027\uff0c\u6539\u8fdb\u5929\u7ebf\u548c\u906e\u6321\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u6d4b\u91cf\uff0c\u66f4\u65b0\u4e86\u5929\u7ebf\u6a21\u5f0f\u3001\u906e\u6321\u6548\u5e94\u7b49\u5185\u5bb9\u3002", "result": "\u65b0\u6a21\u578b\u63d0\u4f9b\u66f4\u771f\u5b9e\u76846G\u6280\u672f\u8bc4\u4f30\uff0c\u652f\u6301\u884c\u4e1a\u548c\u7814\u7a76\u7684\u4e00\u81f4\u6027\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u4fee\u8ba2\u540e\u7684\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e866G\u4fe1\u9053\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.10032", "pdf": "https://arxiv.org/pdf/2509.10032", "abs": "https://arxiv.org/abs/2509.10032", "authors": ["Marawan Khalil", "Fabian Arzberger", "Andreas N\u00fcchter"], "title": "Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping", "categories": ["cs.RO"], "comment": "6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction\n  with 12th ECMR 2025", "summary": "Spherical robots offer unique advantages for mapping applications in\nhazardous or confined environments, thanks to their protective shells and\nomnidirectional mobility. This work presents two complementary spherical\nmapping systems: a lightweight, non-actuated design and an actuated variant\nfeaturing internal pendulum-driven locomotion. Both systems are equipped with a\nLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)\nalgorithms on resource-constrained hardware. We assess the mapping accuracy of\nthese systems by comparing the resulting 3D point-clouds from the LIO\nalgorithms to a ground truth map. The results indicate that the performance of\nstate-of-the-art LIO algorithms deteriorates due to the high dynamic movement\nintroduced by the spherical locomotion, leading to globally inconsistent maps\nand sometimes unrecoverable drift.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7403\u5f62\u673a\u5668\u4eba\u5728\u5371\u9669\u6216\u72ed\u7a84\u73af\u5883\u4e2d\u8fdb\u884c\u5730\u56fe\u6784\u5efa\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u7403\u5f62\u6d4b\u7ed8\u7cfb\u7edf\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u52a8\u6001\u8fd0\u52a8\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7403\u5f62\u673a\u5668\u4eba\u56e0\u5176\u4fdd\u62a4\u58f3\u548c\u5168\u5411\u79fb\u52a8\u6027\uff0c\u5728\u5371\u9669\u6216\u72ed\u7a84\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u52a8\u6001\u8fd0\u52a8\u53ef\u80fd\u5bfc\u81f4\u5730\u56fe\u6784\u5efa\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7403\u5f62\u6d4b\u7ed8\u7cfb\u7edf\uff08\u8f7b\u91cf\u975e\u9a71\u52a8\u578b\u548c\u9a71\u52a8\u578b\uff09\uff0c\u5e76\u57fa\u4e8e\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u8fd0\u884cLiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u7b97\u6cd5\uff0c\u8bc4\u4f30\u5176\u5730\u56fe\u51c6\u786e\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7403\u5f62\u8fd0\u52a8\u5f15\u5165\u7684\u9ad8\u52a8\u6001\u8fd0\u52a8\u4f1a\u5bfc\u81f4LIO\u7b97\u6cd5\u6027\u80fd\u4e0b\u964d\uff0c\u4ea7\u751f\u5168\u5c40\u4e0d\u4e00\u81f4\u7684\u5730\u56fe\u6216\u4e0d\u53ef\u6062\u590d\u7684\u6f02\u79fb\u3002", "conclusion": "\u7403\u5f62\u673a\u5668\u4eba\u7684\u52a8\u6001\u8fd0\u52a8\u5bf9LIO\u7b97\u6cd5\u7684\u5730\u56fe\u6784\u5efa\u7cbe\u5ea6\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.10433", "pdf": "https://arxiv.org/pdf/2509.10433", "abs": "https://arxiv.org/abs/2509.10433", "authors": ["Junshi Chen", "Xuhong Li", "Russ Whiton", "Erik Leitinger", "Fredrik Tufvesson"], "title": "Robust Localization in Modern Cellular Networks using Global Map Features", "categories": ["eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Radio frequency (RF) signal-based localization using modern cellular networks\nhas emerged as a promising solution to accurately locate objects in challenging\nenvironments. One of the most promising solutions for situations involving\nobstructed-line-of-sight (OLoS) and multipath propagation is multipathbased\nsimultaneous localization and mapping (MP-SLAM) that employs map features\n(MFs), such as virtual anchors. This paper presents an extended MP-SLAM method\nthat is augmented with a global map feature (GMF) repository. This repository\nstores consistent MFs of high quality that are collected during prior\ntraversals. We integrate these GMFs back into the MP-SLAM framework via a\nprobability hypothesis density (PHD) filter, which propagates GMF intensity\nfunctions over time. Extensive simulations, together with a challenging\nreal-world experiment using LTE RF signals in a dense urban scenario with\nsevere multipath propagation and inter-cell interference, demonstrate that our\nframework achieves robust and accurate localization, thereby showcasing its\neffectiveness in realistic modern cellular networks such as 5G or future 6G\nnetworks. It outperforms conventional proprioceptive sensor-based localization\nand conventional MP-SLAM methods, and achieves reliable localization even under\nadverse signal conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u591a\u8def\u5f84\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08MP-SLAM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u5730\u56fe\u7279\u5f81\uff08GMF\uff09\u5e93\u63d0\u5347\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u4ee3\u8702\u7a9d\u7f51\u7edc\u4e2d\uff0c\u57fa\u4e8e\u5c04\u9891\u4fe1\u53f7\u7684\u5b9a\u4f4d\u5728\u89c6\u7ebf\u906e\u6321\u548c\u591a\u8def\u5f84\u4f20\u64ad\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u6269\u5c55\u7684MP-SLAM\u65b9\u6cd5\u5f15\u5165GMF\u5e93\uff0c\u7ed3\u5408PHD\u6ee4\u6ce2\u5668\u5904\u7406\u9ad8\u8d28\u91cf\u5386\u53f2\u5730\u56fe\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u5b9a\u4f4d\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u51c6\u786e\u6027\u9ad8\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u9002\u7528\u4e8e5G/6G\u7f51\u7edc\u3002", "conclusion": "GMF\u589e\u5f3a\u7684MP-SLAM\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10063", "pdf": "https://arxiv.org/pdf/2509.10063", "abs": "https://arxiv.org/abs/2509.10063", "authors": ["Xiyan Huang", "Zhe Xu", "Chenxi Xiao"], "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model", "categories": ["cs.RO", "cs.AI", "I.2.9"], "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.", "AI": {"tldr": "TwinTac\u7cfb\u7edf\u7ed3\u5408\u7269\u7406\u89e6\u89c9\u4f20\u611f\u5668\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u586b\u8865\u4e86\u89e6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u6a21\u62df\u7a7a\u767d\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u6a21\u62df\u6a21\u578b\u7f3a\u5931\uff0c\u9650\u5236\u4e86\u89e6\u89c9\u611f\u77e5\u9a71\u52a8\u7684\u6709\u6548\u7b56\u7565\u5f00\u53d1\u3002", "method": "\u8bbe\u8ba1\u9ad8\u7075\u654f\u5ea6\u786c\u4ef6\u4f20\u611f\u5668\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u5230\u6a21\u62df\u65b9\u6cd5\u5f00\u53d1\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06\u4eff\u771f\u6570\u636e\u6620\u5c04\u5230\u771f\u5b9e\u4f20\u611f\u5668\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7269\u7406\u4f20\u611f\u5668\u7684\u7075\u654f\u5ea6\uff0c\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u80fd\u51c6\u786e\u590d\u5236\u7269\u7406\u4f20\u611f\u5668\u8f93\u51fa\uff0c\u4eff\u771f\u6570\u636e\u663e\u8457\u63d0\u5347\u7269\u4f53\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "TwinTac\u4e3a\u8de8\u9886\u57df\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09823", "pdf": "https://arxiv.org/pdf/2509.09823", "abs": "https://arxiv.org/abs/2509.09823", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Shuang He", "Zhongqi Cheng", "Rajalakshmi Nandakumar"], "title": "SoilSound: Smartphone-based Soil Moisture Estimation", "categories": ["cs.SD", "cs.AI", "cs.ET", "cs.HC", "eess.SP"], "comment": "12 pages, 8 figures", "summary": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings.", "AI": {"tldr": "SoilSound\u662f\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u65e0\u635f\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u58f0\u5b66\u53cd\u5c04\u6a21\u578b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6d4b\u91cf\u3002", "motivation": "\u73b0\u6709\u571f\u58e4\u6e7f\u5ea6\u76d1\u6d4b\u65b9\u6cd5\u9700\u8981\u4fb5\u5165\u6027\u63a2\u6d4b\u6216\u4e13\u4e1a\u8bbe\u5907\uff0c\u9650\u5236\u4e86\u516c\u4f17\u4f7f\u7528\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u6270\u52a8\u571f\u58e4\u7684\u4fbf\u6377\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673a\u5185\u7f6e\u626c\u58f0\u5668\u548c\u9ea6\u514b\u98ce\u8fdb\u884c\u5782\u76f4\u626b\u63cf\uff0c\u901a\u8fc7\u58f0\u5b66\u53cd\u5c04\u6a21\u578b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5b9e\u65f6\u4f30\u7b97\u571f\u58e4\u6e7f\u5ea6\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u5730\u70b9\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a2.39%\uff0c\u6e7f\u5ea6\u8303\u56f415.9%\u81f334.0%\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u571f\u58e4\u548c\u73af\u5883\u3002", "conclusion": "SoilSound\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u6216\u6270\u52a8\u571f\u58e4\u7684\u5e7f\u6cdb\u53ef\u7528\u7684\u6e7f\u5ea6\u76d1\u6d4b\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5bb6\u5ead\u56ed\u827a\u3001\u57ce\u5e02\u519c\u4e1a\u548c\u8d44\u6e90\u6709\u9650\u5730\u533a\u3002"}}
{"id": "2509.10065", "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u4e2d\u673a\u68b0\u81c2\u8fd0\u52a8\u8ddf\u8e2a\u63a7\u5236\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u8bbe\u8f68\u8ff9\u548c\u4e8c\u6b21\u89c4\u5212\u53c2\u8003\u5206\u914d\uff0c\u786e\u4fdd\u672b\u7aef\u6267\u884c\u5668\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u4fdd\u6301\u8bef\u5dee\u5728\u4efb\u52a1\u8981\u6c42\u8303\u56f4\u5185\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u8ddf\u8e2a\u63a7\u5236\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5b9e\u73b0\u8ddf\u8e2a\u76ee\u6807\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u8bbe\u8f68\u8ff9\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u63a7\u5236\u548c\u4e8c\u6b21\u89c4\u5212\u53c2\u8003\u5206\u914d\u7684\u63a7\u5236\u6846\u67b6\uff0c\u8003\u8651\u4e86\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u786e\u4fdd\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u8ddf\u8e2a\u8bef\u5dee\u5728\u53ef\u63a7\u8303\u56f4\u5185\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u8ddf\u8e2a\u63a7\u5236\u4e2d\u7684\u65f6\u95f4\u7ea6\u675f\u548c\u7269\u7406\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2509.10061", "pdf": "https://arxiv.org/pdf/2509.10061", "abs": "https://arxiv.org/abs/2509.10061", "authors": ["Yi-Qun Zhao", "Zhi-Ming Ma", "Geoffrey Ye Li", "Shuai Yuan", "Tong Ye", "Chuan Zhou"], "title": "Semantic Rate-Distortion Theory with Applications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Artificial intelligence (AI) is ushering in a new era for communication. As a\nresult, the establishment of a semantic communication framework is putting on\nthe agenda. Based on a realistic semantic communication model, this paper\ndevelops a rate-distortion framework for semantic compression. Different from\nthe existing works primarily focusing on decoder-side estimation of intrinsic\nmeaning and ignoring its inherent issues, such as ambiguity and polysemy, we\nexploit a constraint of conditional semantic probability distortion to\neffectively capture the essential features of practical semantic exchanges in\nan AI-assisted communication system. With the help of the methods in\nrate-distortion-perception theory, we establish a theorem specifying the\nminimum achievable rate under this semantic constraint and a traditional\nsymbolic constraint and obtain its closed-form limit for a particular semantic\nscenario. From the experiments in this paper, bounding conditional semantic\nprobability distortion can effectively improve both semantic transmission\naccuracy and bit-rate efficiency. Our framework bridges information theory and\nAI, enabling potential applications in bandwidth-efficient semantic-aware\nnetworks, enhanced transceiver understanding, and optimized semantic\ntransmission for AI-driven systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901f\u7387-\u5931\u771f\u7406\u8bba\u7684\u8bed\u4e49\u538b\u7f29\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u8bed\u4e49\u6982\u7387\u5931\u771f\u7ea6\u675f\u6709\u6548\u6355\u6349AI\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u63d0\u9ad8\u8bed\u4e49\u4f20\u8f93\u51c6\u786e\u6027\u548c\u6bd4\u7279\u7387\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u901a\u4fe1\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u89e3\u7801\u7aef\u5bf9\u5185\u5728\u610f\u4e49\u7684\u4f30\u8ba1\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u7cca\u6027\u548c\u591a\u4e49\u6027\u7b49\u56fa\u6709\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u66f4\u6709\u6548\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5b9e\u9645\u8bed\u4e49\u901a\u4fe1\u6a21\u578b\uff0c\u7ed3\u5408\u901f\u7387-\u5931\u771f-\u611f\u77e5\u7406\u8bba\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6761\u4ef6\u8bed\u4e49\u6982\u7387\u5931\u771f\u7ea6\u675f\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u6b64\u7ea6\u675f\u4e0b\u53ef\u5b9e\u73b0\u7684\u6700\u4f4e\u901f\u7387\uff0c\u5e76\u5728\u7279\u5b9a\u8bed\u4e49\u573a\u666f\u4e2d\u5f97\u5230\u4e86\u95ed\u5408\u5f62\u5f0f\u7684\u6781\u9650\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u8bed\u4e49\u4f20\u8f93\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u4fe1\u606f\u7406\u8bba\u4e0eAI\u7ed3\u5408\uff0c\u4e3a\u5e26\u5bbd\u9ad8\u6548\u7684\u8bed\u4e49\u611f\u77e5\u7f51\u7edc\u3001\u589e\u5f3a\u7684\u6536\u53d1\u5668\u7406\u89e3\u4ee5\u53caAI\u9a71\u52a8\u7cfb\u7edf\u7684\u8bed\u4e49\u4f20\u8f93\u4f18\u5316\u63d0\u4f9b\u4e86\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2509.10096", "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u52b3\u52a8\u77ed\u7f3a\u548c\u8001\u9f84\u5316\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u8f85\u52a9\u673a\u5668\u4eba\u6280\u672f\uff0c\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u548cTransformer\u6269\u6563\u6a21\u578b\u63d0\u9ad8\u4ea4\u4e92\u52a8\u6001\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u4eba\u53e3\u8001\u9f84\u5316\u4f7f\u5f97\u8f85\u52a9\u673a\u5668\u4eba\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u52a8\u6001\u8026\u5408\u590d\u6742\u6027\u4f7f\u5176\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86HHI-Assist\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u4e92\u884c\u4e3a\u8005\u7684\u59ff\u6001\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u62a4\u7406\u8005\u4e0e\u88ab\u62a4\u7406\u8005\u4e4b\u95f4\u7684\u8026\u5408\u52a8\u6001\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5e76\u9002\u7528\u4e8e\u65b0\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u4ea4\u4e92\u611f\u77e5\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.10097", "pdf": "https://arxiv.org/pdf/2509.10097", "abs": "https://arxiv.org/abs/2509.10097", "authors": ["Ahmed Al-Tahmeesschi", "Yi Chu", "Gurdeep Singh", "Charles Turyagyenda", "Dritan Kaleshi", "David Grace", "Hamed Ahmadi"], "title": "Maximising Energy Efficiency in Large-Scale Open RAN: Hybrid xApps and Digital Twin Integration", "categories": ["cs.NI", "eess.SP"], "comment": "Accepted in GLOBECOM WS 2025", "summary": "The growing demand for high-speed, ultra-reliable, and low-latency\ncommunications in 5G and beyond networks has significantly driven up power\nconsumption, particularly within the Radio Access Network (RAN). This surge in\nenergy demand poses critical operational and sustainability challenges for\nmobile network operators, necessitating innovative solutions that enhance\nenergy efficiency without compromising Quality of Service (QoS). Open Radio\nAccess Network (O-RAN), spearheaded by the O-RAN Alliance, offers\ndisaggregated, programmable, and intelligent architectures, promoting\nflexibility, interoperability, and cost-effectiveness. However, this\ndisaggregated approach adds complexity, particularly in managing power\nconsumption across diverse network components such as Open Radio Units (RUs).\nIn this paper, we propose a hybrid xApp leveraging heuristic methods and\nunsupervised machine learning, integrated with digital twin technology through\nthe TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically\nmanages RU sleep modes to effectively reduce energy consumption. Our\nexperimental evaluation in a realistic, large-scale emulated Open RAN scenario\ndemonstrates that the hybrid xApp achieves approximately 13% energy savings,\nhighlighting its practicality and significant potential for real-world\ndeployments without compromising user QoS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684\u6df7\u5408xApp\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u52a8\u6001\u7ba1\u7406\u5f00\u653e\u65e0\u7ebf\u7535\u5355\u5143\uff08RU\uff09\u7684\u7761\u7720\u6a21\u5f0f\uff0c\u4ee5\u51cf\u5c115G\u53ca\u4ee5\u540e\u7f51\u7edc\u4e2d\u7684\u80fd\u8017\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8282\u7701\u7ea613%\u7684\u80fd\u6e90\u800c\u4e0d\u5f71\u54cd\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "5G\u53ca\u4ee5\u540e\u7f51\u7edc\u7684\u9ad8\u901f\u7387\u3001\u8d85\u53ef\u9760\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\u5bfc\u81f4\u65e0\u7ebf\u63a5\u5165\u7f51\uff08RAN\uff09\u80fd\u8017\u6fc0\u589e\uff0c\u5bf9\u8fd0\u8425\u5546\u5728\u8fd0\u8425\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u5e26\u6765\u6311\u6218\u3002O-RAN\u7684\u5f00\u653e\u67b6\u6784\u589e\u52a0\u4e86\u590d\u6742\u6027\uff0c\u9700\u89e3\u51b3\u80fd\u8017\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6df7\u5408xApp\uff0c\u96c6\u6210\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff08TeraVM AI-RSG\uff09\uff0c\u52a8\u6001\u7ba1\u7406RU\u7684\u7761\u7720\u6a21\u5f0f\u3002", "result": "\u5728\u5927\u578b\u4eff\u771fO-RAN\u573a\u666f\u4e2d\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6848\u8282\u7701\u7ea613%\u7684\u80fd\u6e90\uff0c\u4e14\u4e0d\u5f71\u54cd\u7528\u6237\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "\u6df7\u5408xApp\u65b9\u6848\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4eO-RAN\u80fd\u8017\u3002"}}
{"id": "2509.10128", "pdf": "https://arxiv.org/pdf/2509.10128", "abs": "https://arxiv.org/abs/2509.10128", "authors": ["Philip Arm", "Oliver Fischer", "Joseph Church", "Adrian Fuhrer", "Hendrik Kolvenbach", "Marco Hutter"], "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u91cd\u529b\u73af\u5883\u4e0b\u7684\u817f\u5f0f\u673a\u5668\u4eba\uff0c\u4f18\u5316\u4e86\u529f\u7387\u6d88\u8017\u3002\u8be5\u65b9\u6cd5\u5728\u6708\u7403\u5230\u8d85\u7ea7\u5730\u7403\u7684\u4e0d\u540c\u91cd\u529b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u529f\u7387\u6d88\u8017\u3002", "motivation": "\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u4e2d\uff0c\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0b\u7684\u9ad8\u6548\u79fb\u52a8\u80fd\u529b\u5f88\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u529f\u8017\u548c\u70ed\u7ba1\u7406\u53d7\u9650\uff0c\u9700\u5f00\u53d1\u9002\u5e94\u591a\u91cd\u529b\u73af\u5883\u7684\u8282\u80fd\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u91cd\u529b\u6bd4\u4f8b\u529f\u7387\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5f00\u53d1\u4e86\u8fd0\u52a8\u63a7\u5236\u5668\u548c\u59ff\u6001\u63a7\u5236\u5668\uff0c\u5e76\u5229\u7528\u6052\u529b\u5f39\u7c27\u5378\u8f7d\u7cfb\u7edf\u8fdb\u884c\u4e86\u6708\u7403\u91cd\u529b\u7684\u771f\u5b9e\u5b9e\u9a8c\u3002", "result": "\u5728\u5730\u7403\u91cd\u529b\u4e0b\uff0c\u529f\u8017\u964d\u4f4e23%\uff0823.4W\uff09\uff1b\u5728\u6708\u7403\u91cd\u529b\u4e0b\uff0c\u529f\u8017\u964d\u4f4e36%\uff0812.2W\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u91cd\u529b\u73af\u5883\u4e0b\u7684\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8282\u80fd\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10369", "pdf": "https://arxiv.org/pdf/2509.10369", "abs": "https://arxiv.org/abs/2509.10369", "authors": ["Gul Rukh Khattak", "Konstantinos Patlatzoglou", "Joseph Barker", "Libor Pastika", "Boroumand Zeidaabadi", "Ahmed El-Medany", "Hesham Aggour", "Yixiu Liang", "Antonio H. Ribeiro", "Jeffrey Annis", "Antonio Luiz Pinho Ribeiro", "Junbo Ge", "Daniel B. Kramer", "Jonathan W. Waks", "Evan Brittain", "Nicholas Peters", "Fu Siong Ng", "Arunashis Sau"], "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.TO"], "comment": "Currently under review at npj Digital Medicine", "summary": "Contrastive learning is a widely adopted self-supervised pretraining\nstrategy, yet its dependence on cohort composition remains underexplored. We\npresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation\nmodel and pretrain on four cohorts (n = 5,203,352), from diverse populations\nacross three continents (North America, South America, Asia). We systematically\nassess how cohort demographics, health status, and population diversity\ninfluence the downstream performance for prediction tasks also including two\nadditional cohorts from another continent (Europe). We find that downstream\nperformance depends on the distributional properties of the pretraining cohort,\nincluding demographics and health status. Moreover, while pretraining with a\nmulti-centre, demographically diverse cohort improves in-distribution accuracy,\nit reduces out-of-distribution (OOD) generalisation of our contrastive approach\nby encoding cohort-specific artifacts. To address this, we propose the\nIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency\nduring pretraining and enhances OOD robustness. This work provides important\ninsights for developing clinically fair and generalisable foundation models.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u4e0d\u540c\u7fa4\u4f53\u5fc3\u7535\u56fe\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7fa4\u4f53\u591a\u6837\u6027\u548c\u5065\u5eb7\u72b6\u51b5\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86IDB\u7b56\u7565\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u6bd4\u5b66\u4e60\u5728\u4e0d\u540c\u4eba\u7fa4\u5fc3\u7535\u56fe\u6570\u636e\u4e0a\u7684\u4f9d\u8d56\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCAPE\u6a21\u578b\uff0c\u5728\u56db\u5927\u6d32\u7684\u5fc3\u7535\u56fe\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u8bc4\u4f30\u7fa4\u4f53\u7279\u6027\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u8bbe\u8ba1\u4e86IDB\u7b56\u7565\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7fa4\u4f53\u591a\u6837\u6027\u548c\u5065\u5eb7\u72b6\u6001\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0cIDB\u7b56\u7565\u6539\u5584\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u4e34\u5e8a\u516c\u5e73\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.10139", "pdf": "https://arxiv.org/pdf/2509.10139", "abs": "https://arxiv.org/abs/2509.10139", "authors": ["Santiago Montiel-Mar\u00edn", "Angel Llamazares", "Miguel Antunes-Garc\u00eda", "Fabio S\u00e1nchez-Garc\u00eda", "Luis M. Bergasa"], "title": "CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion", "categories": ["cs.RO"], "comment": "4 pages, 2 figures", "summary": "Camera-radar fusion offers a robust and cost-effective alternative to\nLiDAR-based autonomous driving systems by combining complementary sensing\ncapabilities: cameras provide rich semantic cues but unreliable depth, while\nradar delivers sparse yet reliable position and motion information. We\nintroduce CaR1, a novel camera-radar fusion architecture for BEV vehicle\nsegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar\nencoding that discretizes point clouds into structured BEV features and an\nadaptive fusion mechanism that dynamically balances sensor contributions.\nExperiments on nuScenes demonstrate competitive segmentation performance (57.6\nIoU), on par with state-of-the-art methods. Code is publicly available\n\\href{https://www.github.com/santimontiel/car1}{online}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaR1\u7684\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u67b6\u6784\uff0c\u7528\u4e8eBEV\uff08\u9e1f\u77b0\u56fe\uff09\u8f66\u8f86\u5206\u5272\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u96f7\u8fbe\u7f16\u7801\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u768457.6 IoU\u6027\u80fd\u3002", "motivation": "LiDAR\u7cfb\u7edf\u6210\u672c\u9ad8\u6602\uff0c\u76f8\u673a\u548c\u96f7\u8fbe\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u878d\u5408\u4e24\u8005\u53ef\u4ee5\u63d0\u4f9b\u66f4\u9c81\u68d2\u4e14\u7ecf\u6d4e\u7684\u81ea\u52a8\u9a7e\u9a76\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eBEVFusion\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u683c\u5316\u96f7\u8fbe\u7f16\u7801\u65b9\u6cd5\uff0c\u5c06\u70b9\u4e91\u79bb\u6563\u5316\u4e3a\u7ed3\u6784\u5316BEV\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u52a8\u6001\u5e73\u8861\u4f20\u611f\u5668\u7684\u8d21\u732e\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCaR1\u7684\u8f66\u8f86\u5206\u5272\u6027\u80fd\u8fbe\u523057.6 IoU\uff0c\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "CaR1\u901a\u8fc7\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u5728BEV\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.10247", "pdf": "https://arxiv.org/pdf/2509.10247", "abs": "https://arxiv.org/abs/2509.10247", "authors": ["Xinhong Zhang", "Runqing Wang", "Yunfan Ren", "Jian Sun", "Hao Fang", "Jie Chen", "Gang Wang"], "title": "DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning", "categories": ["cs.RO"], "comment": "8 pages, 11 figures, 1 table", "summary": "This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully\ndifferentiable simulation framework designed for efficient quadrotor control\npolicy learning. DiffAero supports both environment-level and agent-level\nparallelism and integrates multiple dynamics models, customizable sensor stacks\n(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,\nGPU-native training interface. By fully parallelizing both physics and\nrendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and\ndelivers orders-of-magnitude improvements in simulation throughput. In contrast\nto existing simulators, DiffAero not only provides high-performance simulation\nbut also serves as a research platform for exploring differentiable and hybrid\nlearning algorithms. Extensive benchmarks and real-world flight experiments\ndemonstrate that DiffAero and hybrid learning algorithms combined can learn\nrobust flight policies in hours on consumer-grade hardware. The code is\navailable at https://github.com/flyingbitac/diffaero.", "AI": {"tldr": "DiffAero\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001GPU\u52a0\u901f\u4e14\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u56db\u65cb\u7ffc\u98de\u884c\u5668\u63a7\u5236\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u73af\u5883\u548c\u4ee3\u7406\u7ea7\u5e76\u884c\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u52a8\u529b\u5b66\u6a21\u578b\u548c\u4f20\u611f\u5668\uff0c\u901a\u8fc7GPU\u539f\u751f\u8bad\u7ec3\u63a5\u53e3\u63d0\u5347\u4eff\u771f\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5668\u5728\u6027\u80fd\u548c\u5b66\u4e60\u7b97\u6cd5\u63a2\u7d22\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cDiffAero\u65e8\u5728\u63d0\u4f9b\u9ad8\u6027\u80fd\u4eff\u771f\u5e76\u652f\u6301\u53ef\u5fae\u5206\u548c\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u7684\u7814\u7a76\u3002", "method": "DiffAero\u5229\u7528GPU\u5e76\u884c\u5316\u7269\u7406\u548c\u6e32\u67d3\uff0c\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u6574\u5408\u591a\u79cd\u52a8\u6001\u6a21\u578b\u548c\u4f20\u611f\u5668\u5806\u6808\uff0c\u652f\u6301\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiffAero\u53ca\u5176\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u80fd\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u6570\u5c0f\u65f6\u5185\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u98de\u884c\u7b56\u7565\u3002", "conclusion": "DiffAero\u4e3a\u9ad8\u6548\u4eff\u771f\u548c\u5b66\u4e60\u7b97\u6cd5\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684GPU\u539f\u751f\u5e73\u53f0\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10305", "pdf": "https://arxiv.org/pdf/2509.10305", "abs": "https://arxiv.org/abs/2509.10305", "authors": ["Yutong Shen", "Ruizhe Xia", "Bokai Yan", "Shunqi zhang", "Pengrui Xiang", "Sicheng He", "Yixin Xu"], "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning", "categories": ["cs.RO"], "comment": "6 pages, 5 figures", "summary": "In dynamic and uncertain environments, robotic path planning demands accurate\nspatiotemporal environment understanding combined with robust decision-making\nunder partial observability. However, current deep reinforcement learning-based\npath planning methods face two fundamental limitations: (1) insufficient\nmodeling of multi-scale temporal dependencies, resulting in suboptimal\nadaptability in dynamic scenarios, and (2) inefficient exploration-exploitation\nbalance, leading to degraded path quality. To address these challenges, we\npropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path\nPlanning. The framework comprises two key modules: (i) the Spatiotemporal\nPerception module, which hierarchically extracts multi-granularity spatial\nfeatures and multi-scale temporal dependencies ranging from instantaneous to\nextended time horizons, thereby improving perception accuracy in dynamic\nenvironments; and (ii) the Adaptive Policy Optimization module, which balances\nexploration and exploitation during training while optimizing for smoothness\nand collision probability through constrained policy updates. Experiments in\ndynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in\nsuccess rate and a 21.7\\% increase in overall path quality, significantly\noutperforming existing state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGundamQ\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u8def\u5f84\u8d28\u91cf\u3002", "motivation": "\u9488\u5bf9\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u4e0d\u8db3\u548c\u63a2\u7d22-\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "GundamQ\u5305\u542b\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\uff08\u63d0\u53d6\u591a\u7c92\u5ea6\u7a7a\u95f4\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\uff09\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u6a21\u5757\uff08\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGundamQ\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6210\u529f\u7387\u548c\u8def\u5f84\u8d28\u91cf\u5206\u522b\u63d0\u534715.3%\u548c21.7%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GundamQ\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u5efa\u6a21\u548c\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2509.10317", "pdf": "https://arxiv.org/pdf/2509.10317", "abs": "https://arxiv.org/abs/2509.10317", "authors": ["Elizaveta D. Moskovskaya", "Anton D. Moscowsky"], "title": "Robot guide with multi-agent control and automatic scenario generation with LLM", "categories": ["cs.RO", "cs.LG", "93C85", "I.2.9; I.2.7; I.2.11"], "comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link", "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u63a7\u5236\u67b6\u6784\u7684\u65c5\u6e38\u5411\u5bfc\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u573a\u666f\u81ea\u52a8\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7cfb\u7edf\u624b\u52a8\u8c03\u53c2\u3001\u7075\u6d3b\u6027\u4f4e\u548c\u4e0d\u591f\u81ea\u7136\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u914d\u7f6e\u884c\u4e3a\u573a\u666f\uff0c\u7075\u6d3b\u6027\u4f4e\u4e14\u884c\u4e3a\u4e0d\u591f\u81ea\u7136\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\uff1a\u9996\u5148\u751f\u6210\u98ce\u683c\u5316\u53d9\u8ff0\uff0c\u518d\u5c06\u975e\u8bed\u8a00\u884c\u4e3a\u6807\u7b7e\u96c6\u6210\u5230\u6587\u672c\u4e2d\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u5e76\u884c\u884c\u4e3a\u3002", "result": "\u8bd5\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u548c\u6269\u5c55\u793e\u4ea4\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u884c\u4e3a\u7684\u81ea\u7136\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2509.10349", "pdf": "https://arxiv.org/pdf/2509.10349", "abs": "https://arxiv.org/abs/2509.10349", "authors": ["Weiyan Lu", "Huizhe Li", "Yuhao Fang", "Zhexuan Zhou", "Junda Wu", "Yude Li", "Youmin Gong", "Jie Mei"], "title": "Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System", "categories": ["cs.RO"], "comment": null, "summary": "Unmanned aerial vehicles (UAVs) with suspended payloads offer significant\nadvantages for aerial transportation in complex and cluttered environments.\nHowever, existing systems face critical limitations, including unreliable\nperception of the cable-payload dynamics, inefficient planning in large-scale\nenvironments, and the inability to guarantee whole-body safety under cable\nbending and external disturbances. This paper presents Acetrans, an Autonomous,\nCorridor-based, and Efficient UAV suspended transport system that addresses\nthese challenges through a unified perception, planning, and control framework.\nA LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and\ncable shape under taut and bent modes, enabling robust whole-body state\nestimation and real-time filtering of cable point clouds. To enhance planning\nscalability, we introduce the Multi-size-Aware Configuration-space Iterative\nRegional Inflation (MACIRI) algorithm, which generates safe flight corridors\nwhile accounting for varying UAV and payload geometries. A spatio-temporal,\ncorridor-constrained trajectory optimization scheme is then developed to ensure\ndynamically feasible and collision-free trajectories. Finally, a nonlinear\nmodel predictive controller (NMPC) augmented with cable-bending constraints\nprovides robust whole-body safety during execution. Simulation and experimental\nresults validate the effectiveness of Acetrans, demonstrating substantial\nimprovements in perception accuracy, planning efficiency, and control safety\ncompared to state-of-the-art methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Acetrans\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93\u4e2d\u7684\u611f\u77e5\u4e0d\u53ef\u9760\u3001\u89c4\u5212\u6548\u7387\u4f4e\u548c\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u611f\u77e5\u4e0d\u53ef\u9760\u3001\u89c4\u5212\u6548\u7387\u4f4e\u548c\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u5c40\u9650\u6027\uff0cAcetrans\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528LiDAR-IMU\u878d\u5408\u6a21\u5757\u4f30\u8ba1\u8d1f\u8f7d\u548c\u7535\u7f06\u72b6\u6001\uff1b\u5f15\u5165MACIRI\u7b97\u6cd5\u751f\u6210\u5b89\u5168\u98de\u884c\u8d70\u5eca\uff1b\u63d0\u51fa\u65f6\u7a7a\u8d70\u5eca\u7ea6\u675f\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6848\uff1b\u8bbe\u8ba1\u589e\u5f3a\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668(NMPC)\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0cAcetrans\u5728\u611f\u77e5\u7cbe\u5ea6\u3001\u89c4\u5212\u6548\u7387\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Acetrans\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u60ac\u6302\u8d1f\u8f7d\u8fd0\u8f93\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.10405", "pdf": "https://arxiv.org/pdf/2509.10405", "abs": "https://arxiv.org/abs/2509.10405", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States", "categories": ["cs.RO"], "comment": "accepted at CoRL 2025", "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u6807\u7b7e\u6216\u673a\u5668\u4eba\u5916\u89c2\u5148\u9a8c\u77e5\u8bc6\u7684\u5355\u76eeRGB\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7LED\u72b6\u6001\u9884\u6d4b\u4efb\u52a1\u5b66\u4e60\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u59ff\u6001\u6807\u7b7e\u6216\u673a\u5668\u4ebaCAD\u6a21\u578b\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591aLED\u7684\u673a\u5668\u4eba\u6536\u96c6\u6570\u636e\uff0c\u901a\u8fc7\u9884\u6d4bLED\u72b6\u6001\u95f4\u63a5\u5b66\u4e60\u59ff\u6001\u4f30\u8ba1\uff0c\u4ec5\u9700\u521d\u59cb\u6821\u51c6\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7ade\u4e89\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u9886\u57df\u548c\u591a\u673a\u5668\u4eba\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2509.10416", "pdf": "https://arxiv.org/pdf/2509.10416", "abs": "https://arxiv.org/abs/2509.10416", "authors": ["Ze Fu", "Pinhao Song", "Yutong Hu", "Renaud Detry"], "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.", "AI": {"tldr": "TASC\u662f\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u7684\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u6784\u5efa\u4ea4\u4e92\u56fe\uff0c\u63a8\u65ad\u7528\u6237\u610f\u56fe\u5e76\u63d0\u4f9b\u8f85\u52a9\uff0c\u9002\u7528\u4e8e\u65e5\u5e38\u4efb\u52a1\u4e14\u65e0\u9700\u9884\u5b9a\u4e49\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u3001\u957f\u671f\u5171\u4eab\u63a7\u5236\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u4efb\u52a1\u7ea7\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\u4e0e\u63a8\u65ad\uff0c\u4ee5\u53ca\u8f85\u52a9\u529f\u80fd\u7684\u591a\u6837\u5316\u5bf9\u8c61\u548c\u4efb\u52a1\u6cdb\u5316\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8f93\u5165\u6784\u5efa\u5f00\u653e\u8bcd\u6c47\u7684\u4ea4\u4e92\u56fe\u6765\u8868\u793a\u5bf9\u8c61\u95f4\u7684\u529f\u80fd\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7a7a\u95f4\u7ea6\u675f\u6765\u6307\u5bfc\u5171\u4eab\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTASC\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u5747\u80fd\u63d0\u9ad8\u4efb\u52a1\u6548\u7387\u5e76\u51cf\u5c11\u7528\u6237\u8f93\u5165\u8d1f\u62c5\uff0c\u4e14\u662f\u9996\u4e2a\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u7684\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u7684\u5171\u4eab\u63a7\u5236\u6846\u67b6\u3002", "conclusion": "TASC\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u5b9a\u4e49\u77e5\u8bc6\u7684\u4efb\u52a1\u7ea7\u8f85\u52a9\u63a7\u5236\uff0c\u5e76\u5728\u591a\u6837\u5316\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.10426", "pdf": "https://arxiv.org/pdf/2509.10426", "abs": "https://arxiv.org/abs/2509.10426", "authors": ["Jianxin Shi", "Zengqi Peng", "Xiaolong Chen", "Tianyu Wo", "Jun Ma"], "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86DECAMP\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u89e3\u8026\u884c\u4e3a\u6a21\u5f0f\u5b66\u4e60\u548c\u6f5c\u5728\u7279\u5f81\u91cd\u6784\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u573a\u666f\u4e2d\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u6027\u80fd\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "method": "\u5f15\u5165DECAMP\u6846\u67b6\uff0c\u89e3\u8026\u884c\u4e3a\u5b66\u4e60\u4e0e\u7279\u5f81\u91cd\u6784\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u534f\u4f5c\u7a7a\u95f4-\u8fd0\u52a8\u9884\u4efb\u52a1\u3002", "result": "\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DECAMP\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u81ea\u7f16\u7801\u5668\u6846\u67b6\u3002"}}
{"id": "2509.10444", "pdf": "https://arxiv.org/pdf/2509.10444", "abs": "https://arxiv.org/abs/2509.10444", "authors": ["Chaerim Moon", "Joohyung Kim"], "title": "Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction", "categories": ["cs.RO"], "comment": "Presented in IROS 2023 Workshop (Multilimb Coordination in Human\n  Neuroscience and Robotics: Classical and Learning Perspectives)", "summary": "Supernumerary Robotic Limbs (SRLs) can enhance human capability within close\nproximity. However, as a wearable device, the generated moment from its\noperation acts on the human body as an external torque. When the moments\nincrease, more muscle units are activated for balancing, and it can result in\nreduced muscular null space. Therefore, this paper suggests a concept of a\nmotion planning layer that reduces the generated moment for enhanced\nHuman-Robot Interaction. It modifies given trajectories with desirable angular\nacceleration and position deviation limits. Its performance to reduce the\nmoment is demonstrated through the simulation, which uses simplified human and\nrobotic system models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u8fd0\u52a8\u89c4\u5212\u5c42\uff0c\u4ee5\u51cf\u5c11\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53\u7684\u529b\u77e9\uff0c\u4ece\u800c\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53\u867d\u7136\u80fd\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\uff0c\u4f46\u5176\u8fd0\u884c\u65f6\u4ea7\u751f\u7684\u529b\u77e9\u4f1a\u589e\u52a0\u4eba\u4f53\u808c\u8089\u8d1f\u8377\uff0c\u51cf\u5c11\u808c\u8089\u5197\u4f59\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u8fd0\u52a8\u89c4\u5212\u51cf\u5c11\u8fd9\u79cd\u529b\u77e9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fd0\u52a8\u89c4\u5212\u5c42\uff0c\u4fee\u6539\u7ed9\u5b9a\u8f68\u8ff9\uff0c\u9650\u5236\u89d2\u52a0\u901f\u5ea6\u548c\u4f4d\u7f6e\u504f\u5dee\uff0c\u4ee5\u51cf\u5c11\u529b\u77e9\u3002\u901a\u8fc7\u7b80\u5316\u7684\u4eba\u4f53\u548c\u673a\u5668\u4eba\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8fd0\u52a8\u89c4\u5212\u5c42\u80fd\u6709\u6548\u51cf\u5c11\u673a\u5668\u4eba\u80a2\u4f53\u64cd\u4f5c\u65f6\u4ea7\u751f\u7684\u529b\u77e9\u3002", "conclusion": "\u8be5\u8fd0\u52a8\u89c4\u5212\u5c42\u901a\u8fc7\u4f18\u5316\u8f68\u8ff9\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u529b\u77e9\uff0c\u6539\u5584\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u8212\u9002\u6027\u3002"}}
{"id": "2509.10454", "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bfc\u822a\u6307\u4ee4\u5206\u89e3\u4e3a\u7a7a\u95f4\u7ea6\u675f\u56fe\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u65b0\u73af\u5883\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u8fde\u7eed\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u7a7a\u95f4\u7ea6\u675f\u5e93\uff0c\u5c06\u6307\u4ee4\u89e3\u6790\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u901a\u8fc7\u7ea6\u675f\u6c42\u89e3\u786e\u5b9a\u5bfc\u822a\u8def\u5f84\u548c\u7ec8\u70b9\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.09720", "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "ASOS\u662f\u4e00\u4e2a\u5305\u542b50\u79cd\u8d85\u5e02\u7269\u54c1\u7684\u9ad8\u8d28\u91cf3D\u7eb9\u7406\u7f51\u683c\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4f9d\u8d56\u5408\u6210\u6a21\u578b\u6216\u7279\u6b8a\u7269\u54c1\u7684\u95ee\u9898\uff0cASOS\u63d0\u4f9b\u4e86\u6613\u4e8e\u83b7\u53d6\u7684\u771f\u5b9e\u7269\u54c1\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u4ece\u8fd0\u52a8\u6280\u672f\u548c\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u751f\u6210\u9632\u6c34\u7f51\u683c\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d610\u4e2a\u7c7b\u522b\uff0c\u5177\u6709\u591a\u6837\u5316\u7684\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u91cd\u91cf\u3002", "conclusion": "ASOS\u56e0\u5176\u6613\u83b7\u53d6\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.09747", "pdf": "https://arxiv.org/pdf/2509.09747", "abs": "https://arxiv.org/abs/2509.09747", "authors": ["Leen Daher", "Zhaobo Wang", "Malcolm Mielle"], "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Cross-modal transfer learning is used to improve multi-modal classification\nmodels (e.g., for human activity recognition in human-robot collaboration).\nHowever, existing methods require paired sensor data at both training and\ninference, limiting deployment in resource-constrained environments where full\nsensor suites are not economically and technically usable. To address this, we\npropose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns\nmodality-specific representations without requiring joint sensor modality\nduring inference. Our approach combines a self-attention module for feature\nextraction with a novel cross-attention alignment loss, which enforces the\nalignment of sensors' feature spaces without requiring the coupling of the\nclassification pipelines of both modalities. We evaluate D-CAT on three\nmulti-modal human activity datasets (IMU, video, and audio) under both\nin-distribution and out-of-distribution scenarios, comparing against uni-modal\nmodels. Results show that in in-distribution scenarios, transferring from\nhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains\nover uni-modal training. In out-of-distribution scenarios, even weaker source\nmodalities (e.g., IMU to video) improve target performance, as long as the\ntarget model isn't overfitted on the training data. By enabling single-sensor\ninference with cross-modal knowledge, D-CAT reduces hardware redundancy for\nperception systems while maintaining accuracy, which is critical for\ncost-sensitive or adaptive deployments (e.g., assistive robots in homes with\nvariable sensor availability). Code is available at\nhttps://github.com/Schindler-EPFL-Lab/D-CAT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faD-CAT\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u8f6c\u79fb\uff0c\u5b9e\u73b0\u5728\u65e0\u914d\u5bf9\u4f20\u611f\u5668\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u4f7f\u7528\u914d\u5bf9\u4f20\u611f\u5668\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8054\u5408\u4f20\u611f\u5668\u6a21\u6001\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7684D-CAT\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u65b0\u578b\u8de8\u6ce8\u610f\u529b\u5bf9\u9f50\u635f\u5931\u51fd\u6570\uff0c\u65e0\u9700\u8026\u5408\u5206\u7c7b\u6d41\u7a0b\u5373\u53ef\u5bf9\u9f50\u4f20\u611f\u5668\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cD-CAT\u5728\u4e0d\u540c\u5206\u5e03\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u76ee\u6807\u6a21\u6001\u6027\u80fd\uff0c\u6700\u9ad8\u63d0\u534710%\u7684F1\u5206\u6570\u3002", "conclusion": "D-CAT\u901a\u8fc7\u51cf\u5c11\u786c\u4ef6\u5197\u4f59\u5e76\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u6210\u672c\u654f\u611f\u6216\u4f20\u611f\u5668\u53ef\u7528\u6027\u591a\u53d8\u7684\u73af\u5883\uff08\u5982\u5bb6\u5ead\u8f85\u52a9\u673a\u5668\u4eba\uff09\u3002"}}
{"id": "2509.09828", "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f15\u5bfc\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5DGFusion\uff0c\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\u5b9e\u73b0\u72b6\u6001\u611f\u77e5\u7684\u4f20\u611f\u5668\u878d\u5408\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u7edf\u4e00\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "DGFusion\u5c06\u591a\u6a21\u6001\u5206\u5272\u5efa\u6a21\u4e3a\u591a\u4efb\u52a1\u95ee\u9898\uff0c\u5229\u7528\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\uff0c\u901a\u8fc7\u5c40\u90e8\u6df1\u5ea6\u4ee4\u724c\u548c\u5168\u5c40\u6761\u4ef6\u4ee4\u724c\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MUSES\u548cDELIVER\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u89c6\u548c\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u7684\u52a8\u6001\u878d\u5408\u7b56\u7565\u548c\u9c81\u68d2\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0cDGFusion\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.09863", "pdf": "https://arxiv.org/pdf/2509.09863", "abs": "https://arxiv.org/abs/2509.09863", "authors": ["Sarvan Gill", "Daniela Constantinescu"], "title": "Off Policy Lyapunov Stability in Reinforcement Learning", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "comment": "Conference on Robot Learning (CORL) 2025", "summary": "Traditional reinforcement learning lacks the ability to provide stability\nguarantees. More recent algorithms learn Lyapunov functions alongside the\ncontrol policies to ensure stable learning. However, the current self-learned\nLyapunov functions are sample inefficient due to their on-policy nature. This\npaper introduces a method for learning Lyapunov functions off-policy and\nincorporates the proposed off-policy Lyapunov function into the Soft Actor\nCritic and Proximal Policy Optimization algorithms to provide them with a data\nefficient stability certificate. Simulations of an inverted pendulum and a\nquadrotor illustrate the improved performance of the two algorithms when\nendowed with the proposed off-policy Lyapunov function.", "AI": {"tldr": "\u901a\u8fc7\u5b66\u4e60\u79bb\u7b56\u7565\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6570\u636e\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u4e14\u73b0\u6709\u81ea\u5b66\u4e60\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u56e0\u7b56\u7565\u6027\u7279\u70b9\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u79bb\u7b56\u7565\u5b66\u4e60\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u878d\u5165Soft Actor Critic\u548cProximal Policy Optimization\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\uff08\u5012\u7acb\u6446\u548c\u56db\u65cb\u7ffc\uff09\u8868\u660e\uff0c\u589e\u5f3a\u540e\u7684\u7b97\u6cd5\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u79bb\u7b56\u7565\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u662f\u4e00\u79cd\u9ad8\u6548\u7a33\u5b9a\u6027\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2509.10021", "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas K\u00fchne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u6d41\u7a0b\uff0c\u4e13\u4e3a\u5fae\u578b\u548c\u7eb3\u7c73\u65e0\u4eba\u673a\u4f18\u5316\uff0c\u901a\u8fc7\u4f7f\u7528\u5148\u8fdb\u7684\u7279\u5f81\u68c0\u6d4b\u548c\u8ddf\u8e2a\u65b9\u6cd5\u4ee5\u53ca\u5728\u4f4e\u529f\u8017SoC\u4e0a\u7684\u91cf\u5316\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5ea6VIO\u6d41\u7a0b\u901a\u5e38\u9700\u8981\u5f3a\u5927\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u800c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4e3a\u5fae\u578b\u548c\u7eb3\u7c73\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u8bba\u6587\u7ed3\u5408\u4e86SuperPoint\u3001PX4FLOW\u548cORB\u7b49\u5148\u8fdb\u7279\u5f81\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u521a\u4f53\u8fd0\u52a8\u6a21\u578b\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff0c\u540c\u65f6\u5728RISC-V\u4f4e\u529f\u8017SoC\u4e0a\u8fdb\u884c\u4e86\u4f18\u5316\u548c\u91cf\u5316\u3002", "result": "\u5728GAP9\u4f4e\u529f\u8017SoC\u4e0a\uff0c\u4f18\u5316\u540e\u7684\u6d41\u7a0b\u5728\u4f7f\u7528ORB\u7279\u5f81\u8ddf\u8e2a\u5668\u65f6\uff0c\u5e73\u5747RMSE\u964d\u4f4e\u4e863.65\u500d\uff1bPX4FLOW\u5728\u4f4e\u901f\u8fd0\u52a8\u4e0b\u5b9e\u73b0\u4e86\u4e0eORB\u76f8\u5f53\u7684\u7cbe\u5ea6\u4e14\u8fd0\u884c\u65f6\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6VIO\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u8f7b\u91cf\u5316\u90e8\u7f72\uff0c\u4e3a\u5fae\u578b\u65e0\u4eba\u673a\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10284", "pdf": "https://arxiv.org/pdf/2509.10284", "abs": "https://arxiv.org/abs/2509.10284", "authors": ["David Zahr\u00e1dka", "Denisa Mu\u017e\u00edkov\u00e1", "David Woller", "Miroslav Kulich", "Ji\u0159\u00ed \u0160vancara", "Roman Bart\u00e1k"], "title": "A Holistic Architecture for Monitoring and Optimization of Robust Multi-Agent Path Finding Plan Execution", "categories": ["cs.MA", "cs.RO"], "comment": "23 pages, 10 figures", "summary": "The goal of Multi-Agent Path Finding (MAPF) is to find a set of paths for a\nfleet of agents moving in a shared environment such that the agents reach their\ngoals without colliding with each other. In practice, some of the robots\nexecuting the plan may get delayed, which can introduce collision risk.\nAlthough robust execution methods are used to ensure safety even in the\npresence of delays, the delays may still have a significant impact on the\nduration of the execution. At some point, the accumulated delays may become\nsignificant enough that instead of continuing with the execution of the\noriginal plan, even if it was optimal, there may now exist an alternate plan\nwhich will lead to a shorter execution. However, the problem is how to decide\nwhen to search for the alternate plan, since it is a costly procedure. In this\npaper, we propose a holistic architecture for robust execution of MAPF plans,\nits monitoring and optimization. We exploit a robust execution method called\nAction Dependency Graph to maintain an estimate of the expected execution\nduration during the plan's execution. This estimate is used to predict the\npotential that finding an alternate plan would lead to shorter execution. We\nempirically evaluate the architecture in experiments in a real-time simulator\nwhich we designed to mimic our real-life demonstrator of an autonomous\nwarehouse robotic fleet.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u7684\u9c81\u68d2\u6267\u884c\u67b6\u6784\uff0c\u901a\u8fc7Action Dependency Graph\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u4ee5\u9884\u6d4b\u662f\u5426\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u8ba1\u5212\u4ece\u800c\u7f29\u77ed\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u5728\u6a21\u62df\u4ed3\u5e93\u673a\u5668\u4eba\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u5728\u5b9e\u8df5\u4e2d\uff0c\u6267\u884cMAPF\u8ba1\u5212\u65f6\u673a\u5668\u4eba\u53ef\u80fd\u56e0\u5ef6\u8fdf\u800c\u5f71\u54cd\u6267\u884c\u65f6\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51b3\u5b9a\u4f55\u65f6\u5bfb\u627e\u66ff\u4ee3\u8ba1\u5212\u4ee5\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u9ad8\u6210\u672c\u3002", "method": "\u91c7\u7528Action Dependency Graph\u4f5c\u4e3a\u9c81\u68d2\u6267\u884c\u65b9\u6cd5\uff0c\u5b9e\u65f6\u4f30\u8ba1\u8ba1\u5212\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u9884\u6d4b\u5bfb\u627e\u66ff\u4ee3\u8ba1\u5212\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u5728\u6a21\u62df\u4ed3\u5e93\u673a\u5668\u4eba\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u80fd\u591f\u6709\u6548\u76d1\u63a7\u548c\u4f18\u5316MAPF\u8ba1\u5212\u7684\u6267\u884c\uff0c\u51cf\u5c11\u56e0\u5ef6\u8fdf\u5bfc\u81f4\u7684\u6267\u884c\u65f6\u95f4\u5ef6\u957f\u3002"}}
{"id": "2509.10353", "pdf": "https://arxiv.org/pdf/2509.10353", "abs": "https://arxiv.org/abs/2509.10353", "authors": ["Davide Gorbani", "Mohamed Elobaid", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Daniele Pucci"], "title": "Data-fused Model Predictive Control with Guarantees: Application to Flying Humanoid Robots", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "8 pages, 3 figures", "summary": "This paper introduces a Data-Fused Model Predictive Control (DFMPC) framework\nthat combines physics-based models with data-driven representations of unknown\ndynamics. Leveraging Willems' Fundamental Lemma and an artificial equilibrium\nformulation, the method enables tracking of changing, potentially unreachable\nsetpoints while explicitly handling measurement noise through slack variables\nand regularization. We provide guarantees of recursive feasibility and\npractical stability under input-output constraints for a specific class of\nreference signals. The approach is validated on the iRonCub flying humanoid\nrobot, integrating analytical momentum models with data-driven turbine\ndynamics. Simulations show improved tracking and robustness compared to a\npurely model-based MPC, while maintaining real-time feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u8868\u793a\u7684\u6570\u636e\u878d\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u5177\u6709\u8ddf\u8e2a\u53d8\u5316\u8bbe\u5b9a\u70b9\u3001\u5904\u7406\u566a\u58f0\u548c\u63d0\u4f9b\u7a33\u5b9a\u6027\u7684\u7279\u70b9\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u89e3\u51b3\u672a\u77e5\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5e76\u5904\u7406\u6d4b\u91cf\u566a\u58f0\u548c\u53d8\u5316\u8bbe\u5b9a\u70b9\u3002", "method": "\u4f7f\u7528Willems\u57fa\u672c\u5f15\u7406\u548c\u4eba\u5de5\u5e73\u8861\u70b9\u516c\u5f0f\uff0c\u7ed3\u5408\u677e\u5f1b\u53d8\u91cf\u548c\u6b63\u5219\u5316\uff0c\u5904\u7406\u566a\u58f0\u548c\u8bbe\u5b9a\u70b9\u53d8\u5316\u3002", "result": "\u5728\u98de\u884c\u4eba\u5f62\u673a\u5668\u4ebaiRonCub\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u7eaf\u6a21\u578b\u65b9\u6cd5\uff0c\u8ddf\u8e2a\u6027\u80fd\u548c\u9c81\u68d2\u6027\u5f97\u5230\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "\u6570\u636e\u878d\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u6709\u6f5c\u529b\uff0c\u517c\u5177\u7406\u8bba\u548c\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2509.10423", "pdf": "https://arxiv.org/pdf/2509.10423", "abs": "https://arxiv.org/abs/2509.10423", "authors": ["Cameron Reid", "Wael Hafez", "Amirhossein Nazeri"], "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "10 pages, 4 figures, 1 table", "summary": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65adRL\u4ee3\u7406\u5728\u90e8\u7f72\u65f6\u51fa\u73b0\u7684\u5f02\u5e38\uff0c\u901a\u8fc7\u5206\u6790\u72b6\u6001-\u52a8\u4f5c\u4e92\u4fe1\u606f\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u6210\u529f\u7684\u7279\u5f81\uff0c\u5e76\u80fd\u5dee\u5f02\u5316\u8bca\u65ad\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u6545\u969c\u3002", "motivation": "RL\u4ee3\u7406\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u4f1a\u56e0\u4f20\u611f\u5668\u6545\u969c\u3001\u6267\u884c\u5668\u78e8\u635f\u548c\u73af\u5883\u53d8\u5316\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u673a\u5236\u6765\u68c0\u6d4b\u548c\u8bca\u65ad\u8fd9\u4e9b\u6545\u969c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u72b6\u6001-\u52a8\u4f5c\u4e92\u4fe1\u606f\u6a21\u5f0f\u548c\u8054\u5408\u4e92\u4fe1\u606f\u66f2\u7ebf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u90e8\u7f72\u65f6\u7684\u5f02\u5e38\u548c\u7cfb\u7edf\u6545\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4fe1\u606f\u6307\u6807\u80fd\u5dee\u5f02\u5316\u8bca\u65ad\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u6545\u969c\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u9002\u5e94RL\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u5176\u80fd\u591f\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u5219\u8fdb\u884c\u81ea\u4e3b\u6545\u969c\u68c0\u6d4b\u548c\u653f\u7b56\u8c03\u6574\u3002"}}
