{"id": "2601.02394", "pdf": "https://arxiv.org/pdf/2601.02394", "abs": "https://arxiv.org/abs/2601.02394", "authors": ["Yuan-Jie Chen"], "title": "Hydrodynamic Whispering: Enabling Near-Field Silent Communication via Artificial Lateral Line Arrays", "categories": ["eess.SP", "physics.flu-dyn"], "comment": "7 pages, 6 figures,1 table", "summary": "To address the imperative for covert underwater swarm coordination, this paper introduces \"Hydrodynamic Whispering,\" a near-field silent communication paradigm utilizing Artificial Lateral Line (ALL) arrays. Grounded in potential flow theory, we model the transmitter as an oscillating dipole source. The resulting pressure field exhibits steep nearfield attenuation (scaling with 1/r^2, naturally delimiting a secure \"communication bubble\" with intrinsic Low Probability of Interception (LPI) properties. We propose a transceiver architecture featuring a Binary Phase Shift Keying (BPSK) modulation scheme adapted for mechanical actuator inertia, coupled with a bio-inspired 24-sensor conformal array. To mitigate low Signal-to-Noise Ratio (SNR) in turbulent environments,a Spatio-Temporal Joint Processing framework incorporating Spatial Matched-Field Beamforming is developed. Simulation results demonstrate that the system achieves an array gain of approximately 13.8 dB and maintains a near-zero Bit Error Rate (BER) within the effective range. This study validates the feasibility of utilizing localized hydrodynamic pressure fluctuations for reliable and secure short-range underwater networking."}
{"id": "2601.02605", "pdf": "https://arxiv.org/pdf/2601.02605", "abs": "https://arxiv.org/abs/2601.02605", "authors": ["Amir Hossein Fahim Raouf", "İsmail Güvenç"], "title": "Beyond Path Loss: Altitude-Dependent Spectral Structure Modeling for UAV Measurements", "categories": ["eess.SP"], "comment": null, "summary": "This paper presents a measurement-based framework for characterizing altitude-dependent spectral behavior of signals received by a tethered Helikite unmanned aerial vehicle (UAV). Using a multi-year spectrum measurement campaign in an outdoor urban environment, power spectral density snapshots are collected over the 89 MHz--6 GHz range. Three altitude-dependent spectral metrics are extracted: band-average power, spectral entropy, and spectral sparsity. We introduce the Altitude-Dependent Spectral Structure Model (ADSSM) to characterize the spectral power and entropy using first-order altitude-domain differential equations, and spectral sparsity using a logistic function, yielding closed-form expressions with physically consistent asymptotic behavior. The model is fitted to altitude-binned measurements from three annual campaigns at the AERPAW testbed across six licensed and unlicensed sub-6 GHz bands. Across all bands and years, the ADSSM achieves low root-mean-square error and high coefficients of determination. Results indicate that power transitions occur over narrow low-altitude regions, while entropy and sparsity evolve over broader, band-dependent altitude ranges, demonstrating that altitude-dependent spectrum behavior is inherently multidimensional. By explicitly modeling altitude-dependent transitions in spectral structure beyond received power, the proposed framework enables spectrum-aware UAV sensing and band selection decisions that are not achievable with conventional power- or threshold-based occupancy models."}
{"id": "2601.02827", "pdf": "https://arxiv.org/pdf/2601.02827", "abs": "https://arxiv.org/abs/2601.02827", "authors": ["Xufei Zheng", "Han Xiao", "Shi Jin", "Zhiqin Wang", "Wenqiang Tian", "Wendong Liu", "Jianfei Cao", "Jia Shen", "Zhihua Shi", "Zhi Zhang", "Ning Yang"], "title": "AI-Native 6G Physical Layer with Cross-Module Optimization and Cooperative Control Agents", "categories": ["eess.SP"], "comment": null, "summary": "In this article, a framework of AI-native cross-module optimized physical layer with cooperative control agents is proposed, which involves optimization across global AI/ML modules of the physical layer with innovative design of multiple enhancement mechanisms and control strategies. Specifically, it achieves simultaneous optimization across global modules of uplink AI/ML-based joint source-channel coding with modulation, and downlink AI/ML-based modulation with precoding and corresponding data detection, reducing traditional inter-module information barriers to facilitate end-to-end optimization toward global objectives. Moreover, multiple enhancement mechanisms are also proposed, including i) an AI/ML-based cross-layer modulation approach with theoretical analysis for downlink transmission that breaks the isolation of inter-layer features to expand the solution space for determining improved constellation, ii) a utility-oriented precoder construction method that shifts the role of the AI/ML-based CSI feedback decoder from recovering the original CSI to directly generating precoding matrices aiming to improve end-to-end performance, and iii) incorporating modulation into AI/ML-based CSI feedback to bypass bit-level bottlenecks that introduce quantization errors, non-differentiable gradients, and limitations in constellation solution spaces. Furthermore, AI/ML based control agents for optimized transmission schemes are proposed that leverage AI/ML to perform model switching according to channel state, thereby enabling integrated control for global throughput optimization. Finally, simulation results demonstrate the superiority of the proposed solutions in terms of BLER and throughput. These extensive simulations employ more practical assumptions that are aligned with the requirements of the 3GPP, which hopefully provides valuable insights for future standardization discussions."}
{"id": "2601.02874", "pdf": "https://arxiv.org/pdf/2601.02874", "abs": "https://arxiv.org/abs/2601.02874", "authors": ["Mina Shahbazifar", "Zolfa Zeinalpour-Yazdi", "Matthias Hollick", "Arash Asadi", "Vahid Jamali"], "title": "Transparent and Resilient Activity Recognition via Attention-Based Distributed Radar Sensing", "categories": ["eess.SP"], "comment": null, "summary": "Distributed radar sensors enable robust human activity recognition. However, scaling the number of coordinated nodes introduces challenges in feature extraction from large datasets, and transparent data fusion. We propose an end-to-end framework that operates directly on raw radar data. Each radar node employs a lightweight 2D Convolutional Neural Network (CNN) to extract local features. A self-attention fusion block then models inter-node relationships and performs adaptive information fusion. Local feature extraction reduces the input dimensionality by up to 480x. This significantly lowers communication overhead and latency. The attention mechanism provides inherent interpretability by quantifying the contribution of each radar node. A hybrid supervised contrastive loss further improves feature separability, especially for fine-grained and imbalanced activity classes. Experiments on real-world distributed Ultra Wide Band (UWB) radar data demonstrate that the proposed method reduces model complexity by 70.8\\%, while achieving higher average accuracy than baseline approaches. Overall, the framework enables transparent, efficient, and low-overhead distributed radar sensing."}
{"id": "2601.02377", "pdf": "https://arxiv.org/pdf/2601.02377", "abs": "https://arxiv.org/abs/2601.02377", "authors": ["Xinyu Huang", "Shyam Karthick V B", "Taozhao Chen", "Mitch Bryson", "Thomas Chaffey", "Huaming Chen", "Kim-Kwang Raymond Choo", "Ian R. Manchester"], "title": "Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges", "categories": ["cs.RO"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into robotics has revolutionized their ability to interpret complex human commands and execute sophisticated tasks. However, such paradigm shift introduces critical security vulnerabilities stemming from the ''embodiment gap'', a discord between the LLM's abstract reasoning and the physical, context-dependent nature of robotics. While security for text-based LLMs is an active area of research, existing solutions are often insufficient to address the unique threats for the embodied robotic agents, where malicious outputs manifest not merely as harmful text but as dangerous physical actions. In this work, we present a systematic survey, summarizing the emerging threat landscape and corresponding defense strategies for LLM-controlled robotics. Specifically, we discuss a comprehensive taxonomy of attack vectors, covering topics such as jailbreaking, backdoor attacks, and multi-modal prompt injection. In response, we analyze and categorize a range of defense mechanisms, from formal safety specifications and runtime enforcement to multi-LLM oversight and prompt hardening. Furthermore, we review key datasets and benchmarks used to evaluate the robustness of these embodied systems. By synthesizing current research, this work highlights the urgent need for context-aware security solutions and provides a foundational roadmap for the development of safe, secure, and reliable LLM-controlled robotics."}
{"id": "2601.03063", "pdf": "https://arxiv.org/pdf/2601.03063", "abs": "https://arxiv.org/abs/2601.03063", "authors": ["Rundong Jiang", "Jun Hu", "Yunqi Song", "Zhiyuan Xie", "Shiyou Xu"], "title": "Study of Class-Incremental Radio Frequency Fingerprint Recognition Without Storing Exemplars", "categories": ["eess.SP"], "comment": null, "summary": "The rapid proliferation of wireless devices makes robust identity authentication essential. Radio Frequency Fingerprinting (RFF) exploits device-specific, hard-to-forge physical-layer impairments for identification, and is promising for IoT and unmanned systems. In practice, however, new devices continuously join deployed systems while per-class training data are limited. Conventional static training and naive replay of stored exemplars are impractical due to growing class cardinality, storage cost, and privacy concerns.\n  We propose an exemplar-free class-incremental learning framework tailored to RFF recognition. Starting from a pretrained feature extractor, we freeze the backbone during incremental stages and train only a classifier together with lightweight Adapter modules that perform small task-specific feature adjustments. For each class we fit a diagonal Gaussian Mixture Model (GMM) to the backbone features and sample pseudo-features from these fitted distributions to rehearse past classes without storing raw signals. To improve robustness under few-shot conditions we introduce a time-domain random-masking augmentation and adopt a multi-teacher distillation scheme to compress stage-wise Adapters into a single inference Adapter, trading off accuracy and runtime efficiency.\n  We evaluate the method on large, self-collected ADS-B datasets: the backbone is pretrained on 2,175 classes and incremental experiments are run on a disjoint set of 669 classes with multiple rounds and step sizes. Against several representative baselines, our approach consistently yields higher average accuracy and lower forgetting, while using substantially less storage and avoiding raw-data retention.\n  The proposed pipeline is reproducible and provides a practical, low-storage solution for RFF deployment in resource- and privacy-constrained environments."}
{"id": "2601.02378", "pdf": "https://arxiv.org/pdf/2601.02378", "abs": "https://arxiv.org/abs/2601.02378", "authors": ["Biyuan Liu", "Daigang Xu", "Lei Jiang", "Wenjun Guo", "Ping Chen"], "title": "Modeling the Mental World for Embodied AI: A Comprehensive Review", "categories": ["cs.RO"], "comment": null, "summary": "As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice.\n  To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction."}
{"id": "2601.03084", "pdf": "https://arxiv.org/pdf/2601.03084", "abs": "https://arxiv.org/abs/2601.03084", "authors": ["Mohsen Kazemian", "Jürgen Jasperneite"], "title": "A Conditional Variational Framework for Channel Prediction in High-Mobility 6G OTFS Networks", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a machine learning (ML) based method for channel prediction in high mobility orthogonal time frequency space (OTFS) channels. In these scenarios, rapid variations caused by Doppler spread and time varying multipath propagation lead to fast channel decorrelation, making conventional pilot based channel estimation methods prone to outdated channel state information (CSI) and excessive overhead. Therefore, reliable channel prediction methods become essential to support robust detection and decoding in OTFS systems. In this paper, we propose conditional variational autoencoder for channel prediction (CVAE4CP) method, which learns the conditional distribution of OTFS delay Doppler channel coefficients given physical system and mobility parameters. By incorporating these parameters as conditioning information, the proposed method enables the prediction of future channel coefficients before their actual realization, while accounting for inherent channel uncertainty through a low dimensional latent representation. The proposed framework is evaluated through extensive simulations under high mobility conditions. Numerical results demonstrate that CVAE4CP consistently outperforms a competing learning based baseline in terms of normalized mean squared error (NMSE), particularly at high Doppler frequencies and extended prediction horizons. These results confirm the effectiveness and robustness of the proposed approach for channel prediction in rapidly time varying OTFS systems."}
{"id": "2601.02379", "pdf": "https://arxiv.org/pdf/2601.02379", "abs": "https://arxiv.org/abs/2601.02379", "authors": ["Nolan B. Gutierrez", "William J. Beksi"], "title": "Movement Primitives in Robotics: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": "105 pages, 3 figures, and 6 tables", "summary": "Biological systems exhibit a continuous stream of movements, consisting of sequential segments, that allow them to perform complex tasks in a creative and versatile fashion. This observation has led researchers towards identifying elementary building blocks of motion known as movement primitives, which are well-suited for generating motor commands in autonomous systems, such as robots. In this survey, we provide an encyclopedic overview of movement primitive approaches and applications in chronological order. Concretely, we present movement primitive frameworks as a way of representing robotic control trajectories acquired through human demonstrations. Within the area of robotics, movement primitives can encode basic motions at the trajectory level, such as how a robot would grasp a cup or the sequence of motions necessary to toss a ball. Furthermore, movement primitives have been developed with the desirable analytical properties of a spring-damper system, probabilistic coupling of multiple demonstrations, using neural networks in high-dimensional systems, and more, to address difficult challenges in robotics. Although movement primitives have widespread application to a variety of fields, the goal of this survey is to inform practitioners on the use of these frameworks in the context of robotics. Specifically, we aim to (i) present a systematic review of major movement primitive frameworks and examine their strengths and weaknesses; (ii) highlight applications that have successfully made use of movement primitives; and (iii) examine open questions and discuss practical challenges when applying movement primitives in robotics."}
{"id": "2601.03108", "pdf": "https://arxiv.org/pdf/2601.03108", "abs": "https://arxiv.org/abs/2601.03108", "authors": ["Mahesh Ganesh Bhat", "Shana Moothedath", "Prasanna Chaporkar"], "title": "Post-Decision State-Based Online Learning for Delay-Energy-Aware Flow Allocation in Wireless Systems", "categories": ["eess.SP"], "comment": "This work has been submitted to IEEE ICC 2026 for possible publication", "summary": "We develop a structure-aware reinforcement learning (RL) approach for delay- and energy-aware flow allocation in 5G User Plane Functions (UPFs). We consider a dynamic system with $K$ heterogeneous UPFs of varying capacities that handle stochastic arrivals of $M$ flow types, each with distinct rate requirements. We model the system as a Markov decision process (MDP) to capture the stochastic nature of flow arrivals and departures (possibly unknown), as well as the impact of flow allocation in the system. To solve this problem, we propose a post-decision state (PDS) based value iteration algorithm that exploits the underlying structure of the MDP. By separating action-controlled dynamics from exogenous factors, PDS enables faster convergence and efficient adaptive flow allocation, even in the absence of statistical knowledge about exogenous variables. Simulation results demonstrate that the proposed method converges faster and achieves lower long-term cost than standard Q-learning, highlighting the effectiveness of PDS-based RL for resource allocation in wireless networks."}
{"id": "2601.02456", "pdf": "https://arxiv.org/pdf/2601.02456", "abs": "https://arxiv.org/abs/2601.02456", "authors": ["Junhao Cai", "Zetao Cai", "Jiafei Cao", "Yilun Chen", "Zeyu He", "Lei Jiang", "Hang Li", "Hengjie Li", "Yang Li", "Yufei Liu", "Yanan Lu", "Qi Lv", "Haoxiang Ma", "Jiangmiao Pang", "Yu Qiao", "Zherui Qiu", "Yanqing Shen", "Xu Shi", "Yang Tian", "Bolun Wang", "Hanqing Wang", "Jiaheng Wang", "Tai Wang", "Xueyuan Wei", "Chao Wu", "Yiman Xie", "Boyang Xing", "Yuqiang Yang", "Yuyin Yang", "Qiaojun Yu", "Feng Yuan", "Jia Zeng", "Jingjing Zhang", "Shenghan Zhang", "Shi Zhang", "Zhuoma Zhaxi", "Bowen Zhou", "Yuanzhen Zhou", "Yunsong Zhou", "Hongrui Zhu", "Yangkun Zhu", "Yuchen Zhu"], "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation", "categories": ["cs.RO"], "comment": "Homepage: https://internrobotics.github.io/internvla-a1.github.io/", "summary": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting."}
{"id": "2601.03148", "pdf": "https://arxiv.org/pdf/2601.03148", "abs": "https://arxiv.org/abs/2601.03148", "authors": ["Alireza Maleki", "Ebrahim Bedeer", "Robert Barton"], "title": "Spectral-Efficient LoRa with Low Complexity Detection", "categories": ["eess.SP"], "comment": null, "summary": "In this paper, we propose a spectral-efficient LoRa (SE-LoRa) modulation scheme with a low complexity successive interference cancellation (SIC)-based detector. The proposed communication scheme significantly improves the spectral efficiency of LoRa modulation, while achieving an acceptable error performance compared to conventional LoRa modulation, especially in higher spreading factor (SF) settings. We derive the joint maximum likelihood (ML) detection rule for the SE-LoRa transmission scheme that turns out to be of high computational complexity. To overcome this issue, and by exploiting the frequency-domain characteristics of the dechirped SE-LoRa signal, we propose a low complexity SIC-based detector with a computation complexity at the order of conventional LoRa detection. By computer simulations, we show that the proposed SE-LoRa with low complexity SIC-based detector can improve the spectral efficiency of LoRa modulation up to $445.45\\%$, $1011.11\\%$, and $1071.88\\%$ for SF values of $7$, $9$, and $11$, respectively, while maintaining the error performance within less than $3$ dB of conventional LoRa at symbol error rate (SER) of $10^{-3}$ in Rician channel conditions."}
{"id": "2601.02505", "pdf": "https://arxiv.org/pdf/2601.02505", "abs": "https://arxiv.org/abs/2601.02505", "authors": ["Jiazhen Liu", "Glen Neville", "Jinwoo Park", "Sonia Chernova", "Harish Ravichandar"], "title": "Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints", "categories": ["cs.RO"], "comment": "The journal extension version of our conference paper: arXiv:2404.07902, which has been accepted by ISRR 2024", "summary": "Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency."}
{"id": "2601.03234", "pdf": "https://arxiv.org/pdf/2601.03234", "abs": "https://arxiv.org/abs/2601.03234", "authors": ["Amir Hossein Fahim Raouf", "İsmail Güvenc"], "title": "Inter-Year Transfer of Altitude-Dependent Spectrum Activity Models Using Minimal Calibration", "categories": ["eess.SP"], "comment": null, "summary": "This paper studies the transferability of altitude-dependent spectrum activity models and measurements across years. We introduce a physics-informed, mean-only stochastic-geometry model of aggregate interference to altitude-binned received power, yielding three interpretable parameters for a given band and campaign: 1) line-of-sight transition slope, 2) transition altitude, and 3) effective activity constant. Analysis of aerial spectrum measurements collected from 2023 to 2025 across multiple sub-6 GHz bands reveals that downlink (DL) and shared-access bands preserve a persistent geometry-driven altitude structure that is stable across years. In contrast, uplink (UL) bands exhibit weak altitude dependence with no identifiable transition, indicating that interference is dominated by activity dynamics rather than propagation geometry. To quantify the practical limits of model reuse, we evaluate a minimal-calibration method in which the transition altitude is fixed from a reference year and the remaining parameters are estimated from only two altitude bins in the target year. The results further indicate that the proposed approach provides accurate predictions for DL and CBRS bands, suggesting the feasibility of low-cost model transfer in stable environments, while highlighting the reduced applicability of mean-field models for UL scenarios."}
{"id": "2601.02645", "pdf": "https://arxiv.org/pdf/2601.02645", "abs": "https://arxiv.org/abs/2601.02645", "authors": ["Samarth Kalluraya", "Yiannis Kantaros"], "title": "Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects", "categories": ["cs.RO"], "comment": null, "summary": "Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness."}
{"id": "2601.02790", "pdf": "https://arxiv.org/pdf/2601.02790", "abs": "https://arxiv.org/abs/2601.02790", "authors": ["Xiucheng Wang", "Peilin Zheng", "Honggang Jia", "Nan Cheng", "Ruijin Sun", "Conghao Zhou", "Xuemin Shen"], "title": "RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Accurate radio map (RM) construction is essential to enabling environment-aware and adaptive wireless communication. However, in future 6G scenarios characterized by high-speed network entities and fast-changing environments, it is very challenging to meet real-time requirements. Although generative diffusion models (DMs) can achieve state-of-the-art accuracy with second-level delay, their iterative nature leads to prohibitive inference latency in delay-sensitive scenarios. In this paper, by uncovering a key structural property of diffusion processes: the latent midpoints remain highly consistent across semantically similar scenes, we propose RadioDiff-Flux, a novel two-stage latent diffusion framework that decouples static environmental modeling from dynamic refinement, enabling the reuse of precomputed midpoints to bypass redundant denoising. In particular, the first stage generates a coarse latent representation using only static scene features, which can be cached and shared across similar scenarios. The second stage adapts this representation to dynamic conditions and transmitter locations using a pre-trained model, thereby avoiding repeated early-stage computation. The proposed RadioDiff-Flux significantly reduces inference time while preserving fidelity. Experiment results show that RadioDiff-Flux can achieve up to 50 acceleration with less than 0.15% accuracy loss, demonstrating its practical utility for fast, scalable RM generation in future 6G networks."}
{"id": "2601.02649", "pdf": "https://arxiv.org/pdf/2601.02649", "abs": "https://arxiv.org/abs/2601.02649", "authors": ["Jiangyi Fang", "Bowen Zhou", "Haotian Wang", "Xin Zhu", "Leye Wang"], "title": "Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\\% gains under distributional shifts, 4\\% average improvement in online deployment, and up to more than 8\\% in the best case--demonstrating the effectiveness of our framework."}
{"id": "2601.03171", "pdf": "https://arxiv.org/pdf/2601.03171", "abs": "https://arxiv.org/abs/2601.03171", "authors": ["Silvano Cortesi", "Lukas Schulthess", "Davide Plozza", "Christian Vogt", "Michele Magno"], "title": "Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System", "categories": ["cs.NI", "cs.ET", "eess.SP"], "comment": "This work has been accepted for publication in the IEEE Sensors Journal, specifically the Special Issue on \"Special Issue on Advances in Resource-Efficient Sensors and Interfaces Fostered by Artificial Intelligence\"", "summary": "Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling."}
{"id": "2601.02686", "pdf": "https://arxiv.org/pdf/2601.02686", "abs": "https://arxiv.org/abs/2601.02686", "authors": ["Haixin Jin", "Nikhil Uday Shinde", "Soofiyan Atar", "Hongzhan Yu", "Dylan Hirsch", "Sicun Gao", "Michael C. Yip", "Sylvia Herbert"], "title": "Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter", "categories": ["cs.RO"], "comment": null, "summary": "Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings."}
{"id": "2601.03241", "pdf": "https://arxiv.org/pdf/2601.03241", "abs": "https://arxiv.org/abs/2601.03241", "authors": ["Lei Hu", "Sennur Ulukus"], "title": "On the Capacity Region of Individual Key Rates in Vector Linear Secure Aggregation", "categories": ["cs.IT", "cs.CR", "cs.NI", "eess.SP"], "comment": null, "summary": "We provide new insights into an open problem recently posed by Yuan-Sun [ISIT 2025], concerning the minimum individual key rate required in the vector linear secure aggregation problem. Consider a distributed system with $K$ users, where each user $k\\in [K]$ holds a data stream $W_k$ and an individual key $Z_k$. A server aims to compute a linear function $\\mathbf{F}[W_1;\\ldots;W_K]$ without learning any information about another linear function $\\mathbf{G}[W_1;\\ldots;W_K]$, where $[W_1;\\ldots;W_K]$ denotes the row stack of $W_1,\\ldots,W_K$. The open problem is to determine the minimum required length of $Z_k$, denoted as $R_k$, $k\\in [K]$. In this paper, we characterize a new achievable region for the rate tuple $(R_1,\\ldots,R_K)$. The region is polyhedral, with vertices characterized by a binary rate assignment $(R_1,\\ldots,R_K) = (\\mathbf{1}(1 \\in \\mathcal{I}),\\ldots,\\mathbf{1}(K\\in \\mathcal{I}))$, where $\\mathcal{I}\\subseteq [K]$ satisfies the \\textit{rank-increment condition}: $\\mathrm{rank}\\left(\\bigl[\\mathbf{F}_{\\mathcal{I}};\\mathbf{G}_{\\mathcal{I}}\\bigr]\\right) =\\mathrm{rank}\\bigl(\\mathbf{F}_{\\mathcal{I}}\\bigr)+N$. Here, $\\mathbf{F}_\\mathcal{I}$ and $\\mathbf{G}_\\mathcal{I}$ are the submatrices formed by the columns indexed by $\\mathcal{I}$. Our results uncover the novel fact that it is not necessary for every user to hold a key, thereby strictly enlarging the best-known achievable region in the literature. Furthermore, we provide a converse analysis to demonstrate its optimality when minimizing the number of users that hold keys."}
{"id": "2601.02704", "pdf": "https://arxiv.org/pdf/2601.02704", "abs": "https://arxiv.org/abs/2601.02704", "authors": ["Kento Kawaharazuka", "Keita Yoneda", "Takahiro Hattori", "Shintaro Inoue", "Kei Okada"], "title": "Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization", "categories": ["cs.RO"], "comment": "Accepted to Advanced Robotics, website: https://haraduka.github.io/bbo-manip-design", "summary": "Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design."}
{"id": "2601.02723", "pdf": "https://arxiv.org/pdf/2601.02723", "abs": "https://arxiv.org/abs/2601.02723", "authors": ["Wenzheng Zhang", "Kazuki Adachi", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at IEEE/SICE International Symposium on System Integration(SII) 2026. 6 pages, 14 figures", "summary": "Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems."}
{"id": "2601.02738", "pdf": "https://arxiv.org/pdf/2601.02738", "abs": "https://arxiv.org/abs/2601.02738", "authors": ["Kexin Guo", "Zihan Yang", "Yuhang Liu", "Jindou Jia", "Xiang Yu"], "title": "Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning", "categories": ["cs.RO"], "comment": "10 pages, 9 figures", "summary": "Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked."}
{"id": "2601.02762", "pdf": "https://arxiv.org/pdf/2601.02762", "abs": "https://arxiv.org/abs/2601.02762", "authors": ["Zihan Yang", "Jindou Jia", "Meng Wang", "Yuhang Liu", "Kexin Guo", "Xiang Yu"], "title": "Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation", "categories": ["cs.RO"], "comment": "8 pages, 10 figures", "summary": "Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io."}
{"id": "2601.02766", "pdf": "https://arxiv.org/pdf/2601.02766", "abs": "https://arxiv.org/abs/2601.02766", "authors": ["Md. Anowar Hossain", "Mohd. Ehsanul Hoque"], "title": "Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs", "categories": ["cs.RO", "cs.AR"], "comment": null, "summary": "Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability.\n  Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements."}
{"id": "2601.02777", "pdf": "https://arxiv.org/pdf/2601.02777", "abs": "https://arxiv.org/abs/2601.02777", "authors": ["Jingcheng Cao", "Chaoran Xiong", "Jianmin Song", "Shang Yan", "Jiachen Liu", "Ling Pei"], "title": "M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination", "categories": ["cs.RO"], "comment": "6 pages, 7 figures", "summary": "Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments."}
{"id": "2601.02778", "pdf": "https://arxiv.org/pdf/2601.02778", "abs": "https://arxiv.org/abs/2601.02778", "authors": ["Haoyu Dong", "Zhengmao He", "Yang Li", "Zhibin Li", "Xinyu Yi", "Zhe Zhao"], "title": "Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware."}
{"id": "2601.02798", "pdf": "https://arxiv.org/pdf/2601.02798", "abs": "https://arxiv.org/abs/2601.02798", "authors": ["Sicong Gao", "Chen Qian", "Laurence Xian", "Liao Wu", "Maurice Pagnucco", "Yang Song"], "title": "Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \\textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $δ_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach."}
{"id": "2601.02857", "pdf": "https://arxiv.org/pdf/2601.02857", "abs": "https://arxiv.org/abs/2601.02857", "authors": ["Chunzheng Wang", "Yiyuan Zhang", "Annan Tang", "Ziqiu Zeng", "Haoran Chen", "Quan Gao", "Zixuan Zhuang", "Boyu Li", "Zhilin Xiong", "Aoqian Zhang", "Ce Hao", "Siyuan Luo", "Tongyang Zhao", "Cecilia Laschi", "Fan Shi"], "title": "Soft Responsive Materials Enhance Humanoid Safety", "categories": ["cs.RO"], "comment": "40 pages, 11 figures", "summary": "Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots."}
{"id": "2601.02873", "pdf": "https://arxiv.org/pdf/2601.02873", "abs": "https://arxiv.org/abs/2601.02873", "authors": ["Arthur Haffemayer", "Alexandre Chapin", "Armand Jordana", "Krzysztof Wojciechowski", "Florent Lamiraux", "Nicolas Mansard", "Vladimir Petrik"], "title": "Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion", "categories": ["cs.RO"], "comment": "An open-source implementation is provided https://cozy-fairy-0e0139.netlify.app/", "summary": "Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC."}
{"id": "2601.02905", "pdf": "https://arxiv.org/pdf/2601.02905", "abs": "https://arxiv.org/abs/2601.02905", "authors": ["Sara Micol Ferraina", "Michele Brienza", "Francesco Argenziano", "Emanuele Musumeci", "Vincenzo Suriani", "Domenico D. Bloisi", "Daniele Nardi"], "title": "LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/."}
{"id": "2601.02948", "pdf": "https://arxiv.org/pdf/2601.02948", "abs": "https://arxiv.org/abs/2601.02948", "authors": ["Matti Vahs", "Jaeyoun Choi", "Niklas Schmid", "Jana Tumova", "Chuchu Fan"], "title": "Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters", "categories": ["cs.RO"], "comment": null, "summary": "Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines."}
{"id": "2601.02994", "pdf": "https://arxiv.org/pdf/2601.02994", "abs": "https://arxiv.org/abs/2601.02994", "authors": ["Youngjoon Jeong", "Junha Chun", "Taesup Kim"], "title": "Learning to Act Robustly with View-Invariant Latent Actions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Website: https://joon-stack.github.io/VILA/", "summary": "Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance."}
{"id": "2601.03037", "pdf": "https://arxiv.org/pdf/2601.03037", "abs": "https://arxiv.org/abs/2601.03037", "authors": ["Chunhui Zhao", "Xirui Kao", "Yilin Lu", "Yang Lyu"], "title": "A Bi-directional Adaptive Framework for Agile UAV Landing", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE Robotics and Automation Letters (RA-L) for possible publication", "summary": "Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions."}
{"id": "2601.03038", "pdf": "https://arxiv.org/pdf/2601.03038", "abs": "https://arxiv.org/abs/2601.03038", "authors": ["Changwen Li", "Rongjie Yan", "Chih-Hong Cheng", "Jian Zhang"], "title": "Validating Generalist Robots with Situation Calculus and STL Falsification", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy."}
{"id": "2601.03040", "pdf": "https://arxiv.org/pdf/2601.03040", "abs": "https://arxiv.org/abs/2601.03040", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "11 pages and 7 figures", "summary": "A fundamental requirement for full autonomy is the ability to sustain accurate navigation in the absence of external data, such as GNSS signals or visual information. In these challenging environments, the platform must rely exclusively on inertial sensors, leading to pure inertial navigation. However, the inherent noise and other error terms of the inertial sensors in such real-world scenarios will cause the navigation solution to drift over time. Although conventional deep-learning models have emerged as a possible approach to inertial navigation, they are inherently black-box in nature. Furthermore, they struggle to learn effectively with limited supervised sensor data and often fail to preserve physical principles. To address these limitations, we propose PiDR, a physics-informed inertial dead-reckoning framework for autonomous platforms in situations of pure inertial navigation. PiDR offers transparency by explicitly integrating inertial navigation principles into the network training process through the physics-informed residual component. PiDR plays a crucial role in mitigating abrupt trajectory deviations even under limited or sparse supervision. We evaluated PiDR on real-world datasets collected by a mobile robot and an autonomous underwater vehicle. We obtained more than 29% positioning improvement in both datasets, demonstrating the ability of PiDR to generalize different platforms operating in various environments and dynamics. Thus, PiDR offers a robust, lightweight, yet effective architecture and can be deployed on resource-constrained platforms, enabling real-time pure inertial navigation in adverse scenarios."}
{"id": "2601.03044", "pdf": "https://arxiv.org/pdf/2601.03044", "abs": "https://arxiv.org/abs/2601.03044", "authors": ["Mingjie Pan", "Siyuan Feng", "Qinglin Zhang", "Xinchen Li", "Jianheng Song", "Chendi Qu", "Yi Wang", "Chuankang Li", "Ziyu Xiong", "Zhi Chen", "Yi Liu", "Jianlan Luo"], "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world."}
{"id": "2601.03055", "pdf": "https://arxiv.org/pdf/2601.03055", "abs": "https://arxiv.org/abs/2601.03055", "authors": ["Shiying Dong", "Zhipeng Shen", "Rudolf Reiter", "Hailong Huang", "Bingzhao Gao", "Hong Chen", "Wen-Hua Chen"], "title": "A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments."}
{"id": "2601.03070", "pdf": "https://arxiv.org/pdf/2601.03070", "abs": "https://arxiv.org/abs/2601.03070", "authors": ["Tamlin Love", "Ferran Gebellí", "Pradip Pramanick", "Antonio Andriella", "Guillem Alenyà", "Anais Garrell", "Raquel Ros", "Silvia Rossi"], "title": "HEXAR: a Hierarchical Explainability Architecture for Robots", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems."}
{"id": "2601.03097", "pdf": "https://arxiv.org/pdf/2601.03097", "abs": "https://arxiv.org/abs/2601.03097", "authors": ["Omayra Yago Nieto", "Alexandre Anahory Simoes", "Juan I. Giribet", "Leonardo Colombo"], "title": "Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees", "categories": ["cs.RO", "eess.SY", "math.OC"], "comment": null, "summary": "We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.\n  The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.\n  Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems."}
{"id": "2601.03200", "pdf": "https://arxiv.org/pdf/2601.03200", "abs": "https://arxiv.org/abs/2601.03200", "authors": ["Ziyang Sun", "Lingfan Bao", "Tianhu Peng", "Jingcheng Sun", "Chengxu Zhou"], "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting", "categories": ["cs.RO"], "comment": "Under review of Journal of Robot Learning", "summary": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments."}
{"id": "2601.02560", "pdf": "https://arxiv.org/pdf/2601.02560", "abs": "https://arxiv.org/abs/2601.02560", "authors": ["Emre Sariyildiz"], "title": "AMC26: High-performance DOb for robust position control", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "This paper presents a new HPDOb that significantly improves disturbance estimation accuracy and robustness in motion control systems, surpassing the capabilities of conventional DObs. The proposed observer is analysed and synthesised in the discrete-time domain, providing a realistic representation of their dynamic behaviour and enabling enhanced controller design for practical applications. The core contribution of the HPDOb is a novel synthesis method that incorporates higher-order truncation error dynamics into disturbance estimation. Unlike conventional DObs, which are limited to zero-order truncation error, the HPDOb achieves first-order truncation error, yielding markedly improved estimation accuracy and robustness against disturbances in motion control systems. Simulation and experiments verify the stability and performance of HPDOb."}
{"id": "2601.02759", "pdf": "https://arxiv.org/pdf/2601.02759", "abs": "https://arxiv.org/abs/2601.02759", "authors": ["Hyungtae Lim", "Minkyun Seo", "Luca Carlone", "Jaesik Park"], "title": "Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups", "categories": ["cs.CV", "cs.RO"], "comment": "18 pages, 15 figures. Extended version of our ICCV 2025 highlight paper [arXiv:2503.07940]. arXiv admin note: substantial text overlap with arXiv:2503.07940", "summary": "Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X."}
{"id": "2601.03136", "pdf": "https://arxiv.org/pdf/2601.03136", "abs": "https://arxiv.org/abs/2601.03136", "authors": ["Selma Wanna", "Agnes Luhtaru", "Jonathan Salfity", "Ryan Barron", "Juston Moore", "Cynthia Matuszek", "Mitch Pryor"], "title": "Limited Linguistic Diversity in Embodied AI Datasets", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": null, "summary": "Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage."}
{"id": "2601.03247", "pdf": "https://arxiv.org/pdf/2601.03247", "abs": "https://arxiv.org/abs/2601.03247", "authors": ["Leonardo Bettini", "Amirhossein Kazemipour", "Robert K. Katzschmann", "George Haller"], "title": "Nonlinear Spectral Modeling and Control of Soft-Robotic Muscles from Data", "categories": ["math.DS", "cs.CE", "cs.RO", "eess.SY", "math.OC"], "comment": null, "summary": "Artificial muscles are essential for compliant musculoskeletal robotics but complicate control due to nonlinear multiphysics dynamics. Hydraulically amplified electrostatic (HASEL) actuators, a class of soft artificial muscles, offer high performance but exhibit memory effects and hysteresis. Here we present a data-driven reduction and control strategy grounded in spectral submanifold (SSM) theory. In the adiabatic regime, where inputs vary slowly relative to intrinsic transients, trajectories rapidly converge to a low-dimensional slow manifold. We learn an explicit input-to-output map on this manifold from forced-response trajectories alone, avoiding decay experiments that can trigger hysteresis. We deploy the SSM-based model for real-time control of an antagonistic HASEL-clutch joint. This approach yields a substantial reduction in tracking error compared to feedback-only and feedforward-only baselines under identical settings. This record-and-control workflow enables rapid characterization and high-performance control of soft muscles and muscle-driven joints without detailed physics-based modeling."}
