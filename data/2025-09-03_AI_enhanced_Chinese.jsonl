{"id": "2509.00012", "pdf": "https://arxiv.org/pdf/2509.00012", "abs": "https://arxiv.org/abs/2509.00012", "authors": ["Chun Hin Siu", "Hossein Miri"], "title": "Exploring the Efficacy of Convolutional Neural Networks in Sleep Apnea Detection from Single Channel EEG", "categories": ["eess.SP", "cs.LG", "F.2.2; I.2.7"], "comment": "5 pages, 6 figures, 1 table", "summary": "Sleep apnea, a prevalent sleep disorder, involves repeated episodes of\nbreathing interruptions during sleep, leading to various health complications,\nincluding cognitive impairments, high blood pressure, heart disease, stroke,\nand even death. One of the main challenges in diagnosing and treating sleep\napnea is identifying individuals at risk. The current gold standard for\ndiagnosis, Polysomnography (PSG), is costly, labor intensive, and inconvenient,\noften resulting in poor quality sleep data. This paper presents a novel\napproach to the detection of sleep apnea using a Convolutional Neural Network\n(CNN) trained on single channel EEG data. The proposed CNN achieved an accuracy\nof 85.1% and a Matthews Correlation Coefficient (MCC) of 0.22, demonstrating a\nsignificant potential for home based applications by addressing the limitations\nof PSG in automated sleep apnea detection. Key contributions of this work also\ninclude the development of a comprehensive preprocessing pipeline with an\nInfinite Impulse Response (IIR) Butterworth filter, a dataset construction\nmethod providing broader temporal context, and the application of SMOTETomek to\naddress class imbalance. This research underscores the feasibility of\ntransitioning from traditional laboratory based diagnostics to more accessible,\nautomated home based solutions, improving patient outcomes and broadening the\naccessibility of sleep disorder diagnostics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u901a\u9053EEG\u6570\u636e\u68c0\u6d4b\u7761\u7720\u547c\u5438\u6682\u505c\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u591a\u5bfc\u7761\u7720\u56fe\uff08PSG\uff09\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u7761\u7720\u547c\u5438\u6682\u505c\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u7761\u7720\u969c\u788d\uff0c\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5PSG\u6210\u672c\u9ad8\u4e14\u4e0d\u4fbf\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u4fbf\u6377\u7684\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u91c7\u7528CNN\u6a21\u578b\u5904\u7406\u5355\u901a\u9053EEG\u6570\u636e\uff0c\u7ed3\u5408IIR Butterworth\u6ee4\u6ce2\u5668\u9884\u5904\u7406\u3001\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u548cSMOTETomek\u6280\u672f\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe85.1%\uff0cMCC\u4e3a0.22\uff0c\u5c55\u793a\u4e86\u5728\u5bb6\u7528\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u52a8\u5316\u5bb6\u5ead\u8bca\u65ad\u65b9\u6848\u53ef\u884c\uff0c\u6709\u671b\u6539\u5584\u60a3\u8005\u8bca\u7597\u4f53\u9a8c\u5e76\u6269\u5927\u8bca\u65ad\u53ef\u53ca\u6027\u3002"}}
{"id": "2509.00016", "pdf": "https://arxiv.org/pdf/2509.00016", "abs": "https://arxiv.org/abs/2509.00016", "authors": ["Marcin Kolakowski"], "title": "Conditional Generative Adversarial Networks Based Inertial Signal Translation", "categories": ["eess.SP", "cs.LG"], "comment": "Originally presented at: 2025 Signal Processing Symposium (SPSympo)\n  Warsaw, Poland; Associated data available at: M. Kolakowski, \"Wrist and\n  Tibia/Shoe Mounted IMU Measurement Results for Gait Analysis.\" Zenodo, Dec.\n  27, 2023. doi: https://doi.org/10.5281/ZENODO.10436579", "summary": "The paper presents an approach in which inertial signals measured with a\nwrist-worn sensor (e.g., a smartwatch) are translated into those that would be\nrecorded using a shoe-mounted sensor, enabling the use of state-of-the-art gait\nanalysis methods. In the study, the signals are translated using Conditional\nGenerative Adversarial Networks (GANs). Two different GAN versions are used for\nexperimental verification: traditional ones trained using binary cross-entropy\nloss and Wasserstein GANs (WGANs). For the generator, two architectures, a\nconvolutional autoencoder, and a convolutional U-Net, are tested. The\nexperiment results have shown that the proposed approach allows for an accurate\ntranslation, enabling the use of wrist sensor inertial signals for efficient,\nevery-day gait analysis.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.00018", "pdf": "https://arxiv.org/pdf/2509.00018", "abs": "https://arxiv.org/abs/2509.00018", "authors": ["Jiacheng Guo", "Ning Gao", "Yiping Zuo", "Hao Xu", "Shi Jin", "Kai Kit Wong"], "title": "A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks", "categories": ["eess.SP", "cs.AI", "cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "As a promising physical layer security technique, physical layer key\ngeneration (PLKG) enables legitimate users to obtain secret keys from wireless\nchannel without security infrastructures. However, in harsh propagation\nenvironments, the channel characteristic becomes unsatisfactory, the key\ngeneration rate (KGR) is significantly deteriorated. In this paper, we propose\na novel fluid antenna (FA) enabled PLKG system to address this challenge.\nSpecifically, we first derive the closed-form expression of the KGR for FA\narray, and then jointly optimize the precoding matrix and the antenna positions\nvia a particle swarm optimization (PSO) algorithm. Next, to further reduce the\ncomputational complexity of the optimization procedure, we develop an\nalternating optimization (AO) algorithm, which combines the projected gradient\ndescent (PGD) and the PSO. Simulation results demonstrate that by exploiting\nthe additional spatial degree of freedom (DoF), our FA enabled PLKG system is\nsuperior to the benchmarks, such as the conventional fixed-position antenna\n(FPA) array and the reconfigurable intelligent surface (RIS). It is worth\nhighlighting that compared to the conventional uniform planar antenna (UPA),\nthe FA enabled PLKG achieves a 35.42\\% KGR performance improvement under PSO\nalgorithm and a 67.73\\% KGR performance improvement under AO algorithm,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\uff08FA\uff09\u7684\u7269\u7406\u5c42\u5bc6\u94a5\u751f\u6210\uff08PLKG\uff09\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u6076\u52a3\u4f20\u64ad\u73af\u5883\u4e2d\u5bc6\u94a5\u751f\u6210\u7387\uff08KGR\uff09\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u7406\u5c42\u5bc6\u94a5\u751f\u6210\u6280\u672f\u5728\u6076\u52a3\u4f20\u64ad\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u5bc6\u94a5\u751f\u6210\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdFA\u9635\u5217\u7684PLKG\u7cfb\u7edf\uff0c\u63a8\u5bfc\u4e86KGR\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u5206\u522b\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7b97\u6cd5\u548c\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\u4f18\u5316\u4e86\u9884\u7f16\u7801\u77e9\u9635\u548c\u5929\u7ebf\u4f4d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFA-PLKG\u7cfb\u7edf\u5728KGR\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\uff08FPA\uff09\u548c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\uff0c\u5206\u522b\u5b9e\u73b0\u4e8635.42%\uff08PSO\uff09\u548c67.73%\uff08AO\uff09\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FA-PLKG\u7cfb\u7edf\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u81ea\u7531\u5ea6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6076\u52a3\u73af\u5883\u4e0b\u7684\u5bc6\u94a5\u751f\u6210\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.00260", "pdf": "https://arxiv.org/pdf/2509.00260", "abs": "https://arxiv.org/abs/2509.00260", "authors": ["Bastian Latsch", "Felix Herbst", "Mark Suppelt", "Julian Seiler", "Stephan Schaumann", "Sven Suppelt", "Alexander A. Altmann", "Martin Grimmer", "and Mario Kupnik"], "title": "A Review of Sensor Insoles", "categories": ["eess.SP"], "comment": "18 pages, 8 figures", "summary": "Plantar pressure measurement, or pedobarography, is an essential tool for\nanalyzing human motion in healthy individuals and patients. Across the reviewed\nliterature, sensor insoles are motivated as wearable, mobile solutions for\nassessing pressure distribution in applications including diabetic foot\nmonitoring, rehabilitation guidance, assistive device control, and sports\nperformance analysis. This review evaluates the current state of the art with\nparticular attention to sensor technologies, sensor quantity and placement,\nparticipant cohorts, and reference standards. The focus lies on original works\nwith innovative designs, preferably supported by ambulation experiments. The\nmodalities covered include resistive, capacitive, inductive, piezoelectric,\ntriboelectric, and optical sensing approaches. We identify a lack of proper\nsensor calibration, gait-based verification, and human study validation, and\npropose a gold standard based on testing machines and instrumented treadmills\nto ensure comparability across studies. The bidirectional interaction between\ninsole insertion and foot-sole mechanics is examined, with tissue stiffness\nidentified as a key source of uncertainty in sensor signals. Guidelines are\nprovided for sensor dimensions and unobtrusive insole designs to foster natural\ngait. Finally, future directions include the development of multimodal sensors\nto compensate for limitations of individual modalities and the emerging trend\nof multiaxial sensing for capturing shear components in pressure distributions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8db3\u5e95\u538b\u529b\u6d4b\u91cf\u6280\u672f\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4f20\u611f\u5668\u6280\u672f\u548c\u8bbe\u8ba1\u521b\u65b0\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u6821\u51c6\u548c\u9a8c\u8bc1\u7684\u5efa\u8bae\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u591a\u6a21\u6001\u548c\u591a\u8f74\u4f20\u611f\u7684\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u8db3\u5e95\u538b\u529b\u6d4b\u91cf\u5728\u5065\u5eb7\u4eba\u7fa4\u548c\u60a3\u8005\u8fd0\u52a8\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u53ef\u7a7f\u6234\u5f0f\u4f20\u611f\u5668\u978b\u57ab\u5728\u7cd6\u5c3f\u75c5\u8db3\u76d1\u6d4b\u3001\u5eb7\u590d\u6307\u5bfc\u3001\u8f85\u52a9\u8bbe\u5907\u63a7\u5236\u548c\u8fd0\u52a8\u8868\u73b0\u5206\u6790\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u7efc\u8ff0\u901a\u8fc7\u8bc4\u4f30\u4f20\u611f\u5668\u6280\u672f\u3001\u6570\u91cf\u4e0e\u5e03\u5c40\u3001\u53c2\u4e0e\u8005\u7fa4\u4f53\u548c\u53c2\u8003\u6807\u51c6\uff0c\u91cd\u70b9\u5206\u6790\u539f\u59cb\u7814\u7a76\u548c\u521b\u65b0\u8bbe\u8ba1\uff0c\u5305\u62ec\u7535\u963b\u3001\u7535\u5bb9\u3001\u7535\u611f\u3001\u538b\u7535\u3001\u6469\u64e6\u7535\u548c\u5149\u5b66\u4f20\u611f\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f20\u611f\u5668\u6821\u51c6\u3001\u6b65\u6001\u9a8c\u8bc1\u548c\u4eba\u4f53\u7814\u7a76\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6d4b\u8bd5\u8bbe\u5907\u548c\u4eea\u5668\u8dd1\u6b65\u673a\u7684\u6807\u51c6\u5316\u65b9\u6cd5\uff1b\u540c\u65f6\u63a2\u8ba8\u4e86\u978b\u57ab\u63d2\u5165\u4e0e\u8db3\u5e95\u529b\u5b66\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4f20\u611f\u5668\u5c3a\u5bf8\u548c\u975e\u4fb5\u5165\u6027\u978b\u57ab\u8bbe\u8ba1\u7684\u6307\u5357\uff0c\u5e76\u6307\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u4e3a\u591a\u6a21\u6001\u4f20\u611f\u5668\u548c\u591a\u8f74\u4f20\u611f\u6280\u672f\u3002"}}
{"id": "2509.00054", "pdf": "https://arxiv.org/pdf/2509.00054", "abs": "https://arxiv.org/abs/2509.00054", "authors": ["Haimei Pan", "Jiyun Zhang", "Qinxi Wei", "Xiongnan Jin", "Chen Xinkai", "Jie Cheng"], "title": "Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Fire is a highly destructive disaster, but effective prevention can\nsignificantly reduce its likelihood of occurrence. When it happens, deploying\nemergency robots in fire-risk scenarios can help minimize the danger to human\nresponders. However, current research on pre-disaster warnings and\ndisaster-time rescue still faces significant challenges due to incomplete\nperception, inadequate fire situational awareness, and delayed response. To\nenhance intelligent perception and response planning for robots in fire\nscenarios, we first construct a knowledge graph (KG) by leveraging large\nlanguage models (LLMs) to integrate fire domain knowledge derived from fire\nprevention guidelines and fire rescue task information from robotic emergency\nresponse documents. We then propose a new framework called Insights-on-Graph\n(IOG), which integrates the structured fire information of KG and Large\nMultimodal Models (LMMs). The framework generates perception-driven risk graphs\nfrom real-time scene imagery to enable early fire risk detection and provide\ninterpretable emergency responses for task module and robot component\nconfiguration based on the evolving risk situation. Extensive simulations and\nreal-world experiments show that IOG has good applicability and practical\napplication value in fire risk detection and rescue decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u6a21\u6001\u6a21\u578b\u7684\u6846\u67b6\uff08IOG\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u5728\u706b\u707e\u573a\u666f\u4e2d\u7684\u667a\u80fd\u611f\u77e5\u548c\u5e94\u6025\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5728\u706b\u707e\u9884\u9632\u548c\u6551\u63f4\u4e2d\u56e0\u611f\u77e5\u4e0d\u5b8c\u6574\u3001\u60c5\u5883\u610f\u8bc6\u4e0d\u8db3\u548c\u54cd\u5e94\u5ef6\u8fdf\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u706b\u707e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u5b9e\u65f6\u611f\u77e5\u9a71\u52a8\u7684\u98ce\u9669\u56fe\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cIOG\u5728\u706b\u707e\u98ce\u9669\u68c0\u6d4b\u548c\u6551\u63f4\u51b3\u7b56\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "IOG\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u706b\u707e\u573a\u666f\u4e2d\u7684\u667a\u80fd\u611f\u77e5\u4e0e\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2509.00314", "pdf": "https://arxiv.org/pdf/2509.00314", "abs": "https://arxiv.org/abs/2509.00314", "authors": ["Ang Li", "Zikai Wang", "Liuyin Yang", "Zhenyu Wang", "Tianheng Xu", "Honglin Hu", "Marc M. Van Hulle"], "title": "CoMET: A Contrastive-Masked Brain Foundation Model for Universal EEG Representation", "categories": ["eess.SP"], "comment": null, "summary": "Electroencephalography (EEG) is a non-invasive technique for recording brain\nactivity, widely used in brain-computer interfaces, clinic, and healthcare.\nTraditional EEG deep models typically focus on specific dataset and task,\nlimiting model size and generalization. Recently, self-supervised brain\nfoundation models have emerged and been applied to various downstream tasks.\nNevertheless, these models still have limitations: current SOTA models\ntypically rely on masked reconstruction strategy; however, EEG features of\nadjacent channels are highly correlated, which causes the pre-training to\noverly focus on low-dimensional signal-similarity features in local regions and\nneglect the global discriminative patterns vital for downstream tasks. To\naddress these limitations, we propose a brain foundation model called CoMET.\nSpecifically, we employ the masked autoencoder with redesigned patching and\nembedding for EEG as backbone and devise a novel contrastive learning framework\nwith mirror-scale augmentation to strengthen the global discrimination ability.\nCoMET is pre-trained on mixed EEG datasets over 3000 subjects with over one\nmillion samples. It is evaluated on ten different downstream datasets, and the\nSOTA results demonstrate CoMET's superior ability in extracting universal EEG\nrepresentations and strong clinical potential.", "AI": {"tldr": "CoMET\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5927\u8111\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u63d0\u5347EEG\u4fe1\u53f7\u7684\u5168\u5c40\u533a\u5206\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfEEG\u6df1\u5ea6\u6a21\u578b\u5c40\u9650\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\u548c\u4efb\u52a1\uff0c\u4e14\u73b0\u6709\u81ea\u76d1\u7763\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u5c40\u90e8\u4fe1\u53f7\u76f8\u4f3c\u6027\u800c\u5ffd\u89c6\u5168\u5c40\u533a\u5206\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u7ed3\u5408\u955c\u50cf\u5c3a\u5ea6\u589e\u5f3a\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u9884\u8bad\u7ec3\u4e8e\u8d85\u8fc73000\u540d\u88ab\u8bd5\u7684EEG\u6570\u636e\u3002", "result": "\u5728\u5341\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cCoMET\u5177\u6709\u63d0\u53d6\u901a\u7528EEG\u8868\u5f81\u7684\u80fd\u529b\u548c\u663e\u8457\u7684\u4e34\u5e8a\u6f5c\u529b\u3002", "conclusion": "CoMET\u901a\u8fc7\u589e\u5f3a\u5168\u5c40\u533a\u5206\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u8868\u5f81\u7684\u901a\u7528\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2509.00055", "pdf": "https://arxiv.org/pdf/2509.00055", "abs": "https://arxiv.org/abs/2509.00055", "authors": ["Tongtong Feng", "Xin Wang", "Feilin Han", "Leping Zhang", "Wenwu Zhu"], "title": "U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.MM"], "comment": null, "summary": "Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for\nadvancing the low-altitude economy. However, existing methods focus only on\nspecific basic tasks due to dataset limitations, failing in real-world\ndeployment for LH tasks. LH tasks are not mere concatenations of basic tasks,\nrequiring handling long-term dependencies, maintaining persistent states, and\nadapting to dynamic goal shifts. This paper presents U2UData-2, the first\nlarge-scale swarm UAV autonomous flight dataset for LH tasks and the first\nscalable swarm UAV data online collection and algorithm closed-loop\nverification platform. The dataset is captured by 15 UAVs in autonomous\ncollaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120\nhours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.\nThis dataset also includes brightness, temperature, humidity, smoke, and\nairflow values covering all flight routes. The platform supports the\ncustomization of simulators, UAVs, sensors, flight algorithms, formation modes,\nand LH tasks. Through a visual control window, this platform allows users to\ncollect customized datasets through one-click deployment online and to verify\nalgorithms by closed-loop simulation. U2UData-2 also introduces an LH task for\nwildlife conservation and provides comprehensive benchmarks with 9 SOTA models.\nU2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.", "AI": {"tldr": "U2UData-2\u662f\u9996\u4e2a\u9488\u5bf9\u957f\u65f6\u6bb5\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7fa4\u81ea\u4e3b\u98de\u884c\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6570\u636e\u6536\u96c6\u4e0e\u7b97\u6cd5\u95ed\u73af\u9a8c\u8bc1\u5e73\u53f0\uff0c\u652f\u630112\u79cd\u573a\u666f\u7684720\u6761\u8f68\u8ff9\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u9650\u5236\u4ec5\u5173\u6ce8\u7279\u5b9a\u57fa\u7840\u4efb\u52a1\uff0c\u65e0\u6cd5\u9002\u5e94\u957f\u65f6\u6bb5\u4efb\u52a1\u7684\u590d\u6742\u9700\u6c42\uff08\u5982\u957f\u65f6\u4f9d\u8d56\u3001\u52a8\u6001\u76ee\u6807\u8c03\u6574\u7b49\uff09\uff0c\u9700\u63d0\u51fa\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc715\u67b6\u65e0\u4eba\u673a\u534f\u540c\u98de\u884c\u91c7\u96c6\u6570\u636e\uff0c\u6db5\u76d6\u591a\u573a\u666f\u3001\u4f20\u611f\u5668\u548c\u52a8\u6001\u73af\u5883\u53c2\u6570\uff1b\u63d0\u4f9b\u53ef\u5b9a\u5236\u5316\u5e73\u53f0\u652f\u6301\u4eff\u771f\u3001\u7b97\u6cd5\u9a8c\u8bc1\u548c\u5728\u7ebf\u90e8\u7f72\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b120\u5c0f\u65f6\u98de\u884c\u6570\u636e\u3001432\u4e07\u6fc0\u5149\u96f7\u8fbe\u5e27\u53ca1296\u4e07RGB\u5e27\uff0c\u540c\u65f6\u63d0\u4f9b\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e0e9\u79cdSOTA\u6a21\u578b\u5bf9\u6bd4\u3002", "conclusion": "U2UData-2\u586b\u8865\u4e86\u957f\u65f6\u6bb5\u4efb\u52a1\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5176\u5e73\u53f0\u548c\u6570\u636e\u96c6\u4e3a\u65e0\u4eba\u673a\u7fa4\u81ea\u4e3b\u98de\u884c\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u95ed\u73af\u9a8c\u8bc1\u5de5\u5177\u548c\u4e30\u5bcc\u8d44\u6e90\u3002"}}
{"id": "2509.00323", "pdf": "https://arxiv.org/pdf/2509.00323", "abs": "https://arxiv.org/abs/2509.00323", "authors": ["R. Abhishek Shankar", "Hyungjun Ha", "Byunghoo Jung"], "title": "Gait Analysis using 6DoF Magnetic Tracking", "categories": ["eess.SP"], "comment": "10 pages, 5 figures, submitted to IEEE Sensors Journal", "summary": "Gait analysis using wearable devices has advantages over non-wearable devices\nwhen it comes to portability and accessibility. However, non-wearable devices\nhave consistently shown superior performance in terms of the gait information\nthey can provide. This calls for the need to improve the performance of\nwearable device based gait analysis. To that end, we developed a 6\nDegrees-of-Freedom (6DoF) magnetic tracking based gait analysis system as a\nstep in this direction. The system is portable, minimally intrusive, wireless\nand power efficient. As a proof-of-concept, the system was used for the task of\nHuman Activity Recognition (HAR) to classify four tasks - walking (W), walking\nwith weight (WW), jogging (J) and marching on the spot (M). Gait data of 12\nparticipants was collected. The classification performance of two deep learning\n(DL) classifiers - Convolutional Neural Networks (CNN) and Long Short Term\nMemory (LSTM) - was compared. The performance of the magnetic tracking based\ngait analysis system was also compared with an Inertial Measurement Unit (IMU)\n+ magnetometer based system. The magnetic tracking based system showed an\noverall classification accuracy of 92\\% compared to 86.69\\% for the IMU +\nmagnetometer system. Moreover, the magnetic tracking system showed an\nimprovement of about 8\\% in being able to differentiate between W and WW. This\nhighlights the insufficiency in the information content in the data from IMU +\nmagnetometer, warranting the need for a complete 6DoF tracking. Our work, thus,\nproves the feasibility of using magnetic tracking systems for the purpose of\ngait analysis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e6\u81ea\u7531\u5ea6\u78c1\u8ddf\u8e2a\u7684\u6b65\u6001\u5206\u6790\u7cfb\u7edf\uff0c\u4fbf\u643a\u4e14\u9ad8\u6548\uff0c\u7528\u4e8e\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff0c\u5206\u7c7b\u6027\u80fd\u4f18\u4e8eIMU+\u78c1\u529b\u8ba1\u7cfb\u7edf\u3002", "motivation": "\u63d0\u5347\u7a7f\u6234\u5f0f\u8bbe\u5907\u5728\u6b65\u6001\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5f25\u8865\u5176\u76f8\u5bf9\u4e8e\u975e\u7a7f\u6234\u8bbe\u5907\u5728\u4fe1\u606f\u63d0\u4f9b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u75286\u81ea\u7531\u5ea6\u78c1\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7ed3\u5408CNN\u548cLSTM\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5206\u7c7b\u56db\u79cd\u6d3b\u52a8\uff08\u6b65\u884c\u3001\u8d1f\u91cd\u6b65\u884c\u3001\u6162\u8dd1\u3001\u539f\u5730\u8e0f\u6b65\uff09\u3002", "result": "\u78c1\u8ddf\u8e2a\u7cfb\u7edf\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe92%\uff0c\u6bd4IMU+\u78c1\u529b\u8ba1\u7cfb\u7edf\uff0886.69%\uff09\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u533a\u5206\u6b65\u884c\u548c\u8d1f\u91cd\u6b65\u884c\u4e0a\u63d0\u53478%\u3002", "conclusion": "6\u81ea\u7531\u5ea6\u78c1\u8ddf\u8e2a\u7cfb\u7edf\u53ef\u884c\u4e14\u9ad8\u6548\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6b65\u6001\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.00060", "pdf": "https://arxiv.org/pdf/2509.00060", "abs": "https://arxiv.org/abs/2509.00060", "authors": ["Yingjun Tian", "Guoxin Fang", "Renbo Su", "Aoran Lyu", "Neelotpal Dutta", "Simeon Gill", "Andrew Weightman", "Charlie C. L. Wang"], "title": "Correspondence-Free, Function-Based Sim-to-Real Learning for Deformable Surface Control", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a correspondence-free, function-based sim-to-real\nlearning method for controlling deformable freeform surfaces. Unlike\ntraditional sim-to-real transfer methods that strongly rely on marker points\nwith full correspondences, our approach simultaneously learns a deformation\nfunction space and a confidence map -- both parameterized by a neural network\n-- to map simulated shapes to their real-world counterparts. As a result, the\nsim-to-real learning can be conducted by input from either a 3D scanner as\npoint clouds (without correspondences) or a motion capture system as marker\npoints (tolerating missed markers). The resultant sim-to-real transfer can be\nseamlessly integrated into a neural network-based computational pipeline for\ninverse kinematics and shape control. We demonstrate the versatility and\nadaptability of our method on both vision devices and across four pneumatically\nactuated soft robots: a deformable membrane, a robotic mannequin, and two soft\nmanipulators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u5bf9\u5e94\u70b9\u3001\u51fd\u6570\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u53ef\u53d8\u5f62\u81ea\u7531\u66f2\u9762\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u53d8\u5f62\u51fd\u6570\u7a7a\u95f4\u548c\u7f6e\u4fe1\u56fe\uff0c\u9002\u7528\u4e8e\u70b9\u4e91\u6216\u6807\u8bb0\u70b9\u8f93\u5165\u3002", "motivation": "\u4f20\u7edf\u6a21\u62df\u5230\u73b0\u5b9e\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u5168\u5bf9\u5e94\u7684\u6807\u8bb0\u70b9\uff0c\u9650\u5236\u4e86\u9002\u7528\u6027\u3002", "method": "\u540c\u65f6\u5b66\u4e60\u53c2\u6570\u5316\u7684\u53d8\u5f62\u51fd\u6570\u7a7a\u95f4\u548c\u7f6e\u4fe1\u56fe\uff0c\u9002\u7528\u4e8e\u65e0\u5bf9\u5e94\u70b9\u4e91\u6216\u5bb9\u5fcd\u7f3a\u5931\u6807\u8bb0\u7684\u8f93\u5165\u3002", "result": "\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u89c6\u89c9\u8bbe\u5907\u548c\u56db\u79cd\u6c14\u52a8\u8f6f\u673a\u5668\u4eba\u4e0a\u7684\u591a\u529f\u80fd\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u65e0\u7f1d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u6362\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63a7\u5236\u548c\u5f62\u72b6\u8c03\u6574\u3002"}}
{"id": "2509.00331", "pdf": "https://arxiv.org/pdf/2509.00331", "abs": "https://arxiv.org/abs/2509.00331", "authors": ["Yaqian Yi", "Guangchi Zhang", "Miao Cui", "Changsheng You", "Qingqing Wu"], "title": "AN-Aided Secure Beamforming for ELAA-SWIPT in Mixed Near- and Far-Field", "categories": ["eess.SP"], "comment": null, "summary": "This letter investigates secure hybrid beamforming (HB) design for an\nextremely large-scale antenna array-aided simultaneous wireless information and\npower transfer (SWIPT) system operating in a mixed near-field (NF)/far-field\n(FF) environment. A base station (BS) employs HB to transmit information and\nartificial noise (AN) signals simultaneously to multiple FF information\nreceivers (IRs) and NF energy receivers (ERs). The objective is to maximize the\nweighted sum secrecy rate for the IRs, considering both Type-I (unable to\ncancel AN) and Type-II (capable of canceling AN) IRs, subject to minimum energy\nharvesting requirements at the ERs and a BS transmit power constraint. We\nformulate optimization problems for both IR types and develop an efficient\niterative algorithm based on successive convex approximation. Simulation\nresults validate the proposed scheme and provide crucial insights into the\nsecurity performance of mixed-field SWIPT systems, highlighting the influence\nof visibility regions and angular user separation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u8f85\u52a9\u7684\u6df7\u5408\u8fd1\u573a/\u8fdc\u573a\u73af\u5883\u4e0b\u5b89\u5168\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u65e8\u5728\u6700\u5927\u5316\u4fe1\u606f\u63a5\u6536\u5668\u7684\u52a0\u6743\u548c\u4fdd\u5bc6\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u91cf\u63a5\u6536\u5668\u7684\u6700\u5c0f\u80fd\u91cf\u6536\u96c6\u8981\u6c42\u3002", "motivation": "\u63a2\u7d22\u5728\u6df7\u5408\u8fd1\u573a/\u8fdc\u573a\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u5b89\u5168\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u540c\u65f6\u5b9e\u73b0\u4fe1\u606f\u4f20\u8f93\u4e0e\u80fd\u91cf\u4f20\u8f93\u7684\u5b89\u5168\u6027\u548c\u9ad8\u6548\u6027\u3002", "method": "\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u89e3\u51b3\u9488\u5bf9\u4e24\u79cd\u7c7b\u578b\u4fe1\u606f\u63a5\u6536\u5668\u7684\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u6df7\u5408\u573aSWIPT\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5206\u6790\u4e86\u53ef\u89c1\u533a\u57df\u548c\u89d2\u5ea6\u7528\u6237\u5206\u79bb\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df7\u5408\u573aSWIPT\u7cfb\u7edf\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4f18\u5316\u4e86\u52a0\u6743\u548c\u4fdd\u5bc6\u7387\u5e76\u6ee1\u8db3\u80fd\u91cf\u6536\u96c6\u9700\u6c42\u3002"}}
{"id": "2509.00064", "pdf": "https://arxiv.org/pdf/2509.00064", "abs": "https://arxiv.org/abs/2509.00064", "authors": ["Mingze Liu", "Sai Fan", "Haozhen Li", "Haobo Liang", "Yixing Yuan", "Yanke Wang"], "title": "OpenTie: Open-vocabulary Sequential Rebar Tying System", "categories": ["cs.RO", "cs.CV"], "comment": "This article is under its initial revision", "summary": "Robotic practices on the construction site emerge as an attention-attracting\nmanner owing to their capability of tackle complex challenges, especially in\nthe rebar-involved scenarios. Most of existing products and research are mainly\nfocused on flat rebar setting with model training demands. To fulfill this gap,\nwe propose OpenTie, a 3D training-free rebar tying framework utilizing a\nRGB-to-point-cloud generation and an open-vocabulary detection. We implements\nthe OpenTie via a robotic arm with a binocular camera and guarantees a high\naccuracy by applying the prompt-based object detection method on the image\nfiltered by our propose post-processing procedure based a image to point cloud\ngeneration framework. The system is flexible for horizontal and vertical rebar\ntying tasks and the experiments on the real-world rebar setting verifies that\nthe effectiveness of the system in practice.", "AI": {"tldr": "OpenTie\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u76843D\u94a2\u7b4b\u7ed1\u624e\u6846\u67b6\uff0c\u901a\u8fc7RGB\u5230\u70b9\u4e91\u7684\u751f\u6210\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u7075\u6d3b\u7684\u94a2\u7b4b\u7ed1\u624e\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u94a2\u7b4b\u7ed1\u624e\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u5728\u5e73\u9762\u94a2\u7b4b\u5e03\u7f6e\u4e14\u6709\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528RGB\u5230\u70b9\u4e91\u7684\u751f\u6210\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0c\u7ed3\u5408\u540e\u5904\u7406\u56fe\u50cf\u8fc7\u6ee4\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cc\u76ee\u6444\u50cf\u5934\u548c\u673a\u68b0\u81c2\u5b9e\u73b0\u3002", "result": "\u7cfb\u7edf\u5728\u6c34\u5e73\u548c\u5782\u76f4\u94a2\u7b4b\u7ed1\u624e\u4efb\u52a1\u4e2d\u8868\u73b0\u7075\u6d3b\uff0c\u5b9e\u9645\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "OpenTie\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u94a2\u7b4b\u7ed1\u624e\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2509.00478", "pdf": "https://arxiv.org/pdf/2509.00478", "abs": "https://arxiv.org/abs/2509.00478", "authors": ["Getuar Rexhepi", "Kuranage Roche Rayan Ranasinghe", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu", "David Gonzalez G"], "title": "Pilot Allocation and Receiver Design for Cell-Free Massive MIMO ISAC Systems", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "This paper tackles two key challenges in cell-freemassive multiple input\nmultiple output (CF-mMIMO) systems:efficient pilot allocation and practical\nreceiver design. To thisend, we introduce a novel pilot allocation framework\nleveragingmanifold optimization to maximize the system sum rate, wherepilot\nsequences are designed as nearly orthogonal sequences. Theproposed pilot design\nenforces unimodularity constraints in thefrequency domain, ensuring pilots are\nsuitable for both communi-cation and sensing tasks. Additionally, a gaussian\nbelief propaga-tion (GaBP)-based receiver is introduced, providing\nnear-optimaldetection performance with substantially reduced\ncomputationalcomplexity. Simulation results demonstrate that the proposedpilot\nallocation method achieves communication performancecomparable to\nstate-of-the-art (SotA) algorithms, while deliveringsuperior sensing\ncapabilities due to its unimodular pilot design.The GaBP-based receiver\nachieves robust performance andlower complexity compared to conventional\napproaches. Thesecontributions advance the practical deployment of CF-mMIMOfor\nintegrated sensing and communications (ISAC).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bfc\u9891\u5206\u914d\u6846\u67b6\u548cGaBP\u63a5\u6536\u5668\u8bbe\u8ba1\u65b9\u6848\uff0c\u89e3\u51b3\u4e86CF-mMIMO\u7cfb\u7edf\u4e2d\u7684\u5bfc\u9891\u5206\u914d\u548c\u63a5\u6536\u5668\u8bbe\u8ba1\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3CF-mMIMO\u7cfb\u7edf\u4e2d\u9ad8\u6548\u7684\u5bfc\u9891\u5206\u914d\u548c\u5b9e\u9645\u63a5\u6536\u5668\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u5229\u7528\u6d41\u5f62\u4f18\u5316\u8bbe\u8ba1\u8fd1\u4f3c\u6b63\u4ea4\u5bfc\u9891\u5e8f\u5217\uff0c\u5e76\u5f15\u5165GaBP\u63a5\u6536\u5668\u4ee5\u5b9e\u73b0\u8fd1\u6700\u4f18\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u901a\u4fe1\u6027\u80fd\u4e0a\u5ab2\u7f8eSotA\u7b97\u6cd5\uff0c\u540c\u65f6\u5177\u5907\u4f18\u8d8a\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4e14\u63a5\u6536\u5668\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u63a8\u52a8\u4e86CF-mMIMO\u5728ISAC\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.00065", "pdf": "https://arxiv.org/pdf/2509.00065", "abs": "https://arxiv.org/abs/2509.00065", "authors": ["Zhitao Wang", "Yirong Xiong", "Roberto Horowitz", "Yanke Wang", "Yuxing Han"], "title": "Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by The IEEE International Conference on Automation Science\n  and Engineering (CASE) 2025", "summary": "Rebar tying is a repetitive but critical task in reinforced concrete\nconstruction, typically performed manually at considerable ergonomic risk.\nRecent advances in robotic manipulation hold the potential to automate the\ntying process, yet face challenges in accurately estimating tying poses in\ncongested rebar nodes. In this paper, we introduce a hybrid perception and\nmotion planning approach that integrates geometry-based perception with\nEquivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust\nmulti-node rebar tying with minimal training data. Our perception module\nutilizes density-based clustering (DBSCAN), geometry-based node feature\nextraction, and principal component analysis (PCA) to segment rebar bars,\nidentify rebar nodes, and estimate orientation vectors for sequential ranking,\neven in complex, unstructured environments. The motion planner, based on\nDiffusion-EDFs, is trained on as few as 5-10 demonstrations to generate\nsequential end-effector poses that optimize collision avoidance and tying\nefficiency. The proposed system is validated on various rebar meshes, including\nsingle-layer, multi-layer, and cluttered configurations, demonstrating high\nsuccess rates in node detection and accurate sequential tying. Compared with\nconventional approaches that rely on large datasets or extensive manual\nparameter tuning, our method achieves robust, efficient, and adaptable\nmulti-node tying while significantly reducing data requirements. This result\nunderscores the potential of hybrid perception and diffusion-driven planning to\nenhance automation in on-site construction tasks, improving both safety and\nlabor efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u4e0eDiffusion-EDFs\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u8282\u70b9\u94a2\u7b4b\u7ed1\u624e\uff0c\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u548c\u4eba\u5de5\u8c03\u53c2\u9700\u6c42\u3002", "motivation": "\u94a2\u7b4b\u7ed1\u624e\u662f\u91cd\u590d\u6027\u9ad8\u4f46\u5173\u952e\u7684\u4efb\u52a1\uff0c\u624b\u5de5\u64cd\u4f5c\u5b58\u5728\u5de5\u6548\u98ce\u9669\uff0c\u673a\u5668\u4eba\u81ea\u52a8\u5316\u9762\u4e34\u590d\u6742\u8282\u70b9\u59ff\u6001\u4f30\u8ba1\u7684\u6311\u6218\u3002", "method": "\u878d\u5408DBSCAN\u3001\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u4e0ePCA\u7684\u611f\u77e5\u6a21\u5757\uff0c\u7ed3\u5408\u57fa\u4e8eDiffusion-EDFs\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4ec5\u97005-10\u6b21\u6f14\u793a\u8bad\u7ec3\u3002", "result": "\u5728\u5355\u5c42\u3001\u591a\u5c42\u53ca\u6742\u4e71\u94a2\u7b4b\u7f51\u4e2d\u9a8c\u8bc1\u4e86\u9ad8\u6210\u529f\u7387\u68c0\u6d4b\u548c\u7cbe\u51c6\u7ed1\u624e\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u6df7\u5408\u611f\u77e5\u4e0e\u6269\u6563\u89c4\u5212\u53ef\u63d0\u5347\u73b0\u573a\u65bd\u5de5\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u6539\u5584\u5b89\u5168\u6027\u548c\u52b3\u52a8\u6548\u7387\u3002"}}
{"id": "2509.00492", "pdf": "https://arxiv.org/pdf/2509.00492", "abs": "https://arxiv.org/abs/2509.00492", "authors": ["Liesbet Van der Perre", "Gilles Callebaut", "Thomas Eriksson", "Muris Sarajlic", "Christian Fager", "Fredrik Tufvesson", "Buon Kiong Lau", "Erik G. Larsson"], "title": "Distributed Deployment and Dual-Frequency Concepts to Strengthen Sub-THz Wireless Systems", "categories": ["eess.SP"], "comment": null, "summary": "The vast bandwidth available at sub-THz frequencies holds great promise for\nhigh-speed wireless access, precise localization, and advanced sensing\napplications. However, fundamental physical constraints and technological\nlimitations make the deployment of reliable sub-THz networks challenging. We\npropose a new paradigm for sub-THz coverage by transmitting the RF signals over\npolymer microwave fibers (PMFs) that interconnect low-complexity radio units\n(RUs) in a daisy-chain configuration. The distributed architecture ensures that\nuser equipments (UEs) connect to RUs in their proximity, reducing path loss and\nmitigating blocking. The RUs leverage low-complexity, compact integrated\nantenna modules. Additionally, dual-frequency tandem operation is proposed,\nintegrating the sub-THz system with a sub-10 GHz system that provides control\nsignalling and a robust fallback solution for the sub-THz system. This proposed\ntandem architecture can open up the full potential of sub-THz technology and\npaves the way to cost- and energy-efficient, high-performance, real-time\nconnectivity in dynamic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e9a\u592a\u8d6b\u5179\u8986\u76d6\u65b9\u6848\uff0c\u5229\u7528\u805a\u5408\u7269\u5fae\u6ce2\u5149\u7ea4\uff08PMF\uff09\u548c\u4f4e\u590d\u6742\u5ea6\u7684\u65e0\u7ebf\u7535\u5355\u5143\uff08RU\uff09\u5206\u5e03\u67b6\u6784\uff0c\u4ee5\u51cf\u5c11\u8def\u5f84\u635f\u8017\u5e76\u89e3\u51b3\u906e\u6321\u95ee\u9898\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u53cc\u9891\u4e32\u8054\u64cd\u4f5c\u6574\u5408\u4e9a10 GHz\u7cfb\u7edf\uff0c\u63d0\u4f9b\u63a7\u5236\u4fe1\u53f7\u548c\u5907\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e9a\u592a\u8d6b\u5179\u9891\u6bb5\u7684\u5e7f\u9614\u5e26\u5bbd\u4e3a\u9ad8\u901f\u65e0\u7ebf\u63a5\u5165\u3001\u7cbe\u786e\u5b9a\u4f4d\u548c\u9ad8\u7ea7\u4f20\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7269\u7406\u7ea6\u675f\u548c\u6280\u672f\u9650\u5236\u4f7f\u5176\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u805a\u5408\u7269\u5fae\u6ce2\u5149\u7ea4\uff08PMF\uff09\u8fde\u63a5\u4f4e\u590d\u6742\u5ea6RU\uff0c\u5206\u5e03\u5f0f\u67b6\u6784\u786e\u4fdd\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u4e0e\u6700\u8fd1\u7684RU\u8fde\u63a5\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u53cc\u9891\u4e32\u8054\u64cd\u4f5c\uff0c\u7ed3\u5408\u4e9a10 GHz\u7cfb\u7edf\u3002", "result": "\u8be5\u67b6\u6784\u80fd\u663e\u8457\u51cf\u5c11\u8def\u5f84\u635f\u8017\u548c\u906e\u6321\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u63a7\u5236\u4fe1\u53f7\u548c\u5907\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u52a8\u6001\u73af\u5883\u8fde\u63a5\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u578b\u67b6\u6784\u53ef\u5145\u5206\u53d1\u6325\u4e9a\u592a\u8d6b\u5179\u6280\u672f\u7684\u6f5c\u529b\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u63d0\u4f9b\u6210\u672c\u4f4e\u3001\u80fd\u6548\u9ad8\u7684\u5b9e\u65f6\u8fde\u63a5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00119", "pdf": "https://arxiv.org/pdf/2509.00119", "abs": "https://arxiv.org/abs/2509.00119", "authors": ["Jake Robbennolt", "Sirajum Munira", "Stephen D. Boyles"], "title": "A Comparative Study of Spline-Based Trajectory Reconstruction Methods Across Varying Automatic Vehicle Location Data Densities", "categories": ["cs.RO"], "comment": null, "summary": "Automatic vehicle location (AVL) data offers insights into transit dynamics,\nbut its effectiveness is often hampered by inconsistent update frequencies,\nnecessitating trajectory reconstruction. This research evaluates 13 trajectory\nreconstruction methods, including several novel approaches, using\nhigh-resolution AVL data from Austin, Texas. We examine the interplay of four\ncritical factors -- velocity, position, smoothing, and data density -- on\nreconstruction performance. A key contribution of this study is evaluation of\nthese methods across sparse and dense datasets, providing insights into the\ntrade-off between accuracy and resource allocation. Our evaluation framework\ncombines traditional mathematical error metrics for positional and velocity\nwith practical considerations, such as physical realism (e.g., aligning\nvelocity and acceleration with stopped states, deceleration rates, and speed\nvariability). In addition, we provide insight into the relative value of each\nmethod in calculating realistic metrics for infrastructure evaluations. Our\nfindings indicate that velocity-aware methods consistently outperform\nposition-only approaches. Interestingly, we discovered that smoothing-based\nmethods can degrade overall performance in complex, congested urban\nenvironments, although enforcing monotonicity remains critical. The velocity\nconstrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)\nyields optimal results, offering a balance between high accuracy and\ncomputational efficiency. Its minimal overhead makes it suitable for both\nhistorical analysis and real-time applications, providing significant\npredictive power when combined with dense datasets. These findings offer\npractical guidance for researchers and practitioners implementing trajectory\nreconstruction systems and emphasize the importance of investing in\nhigher-frequency AVL data collection for improved analysis.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8613\u79cd\u8f68\u8ff9\u91cd\u5efa\u65b9\u6cd5\uff0c\u53d1\u73b0\u901f\u5ea6\u611f\u77e5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u4f4d\u7f6e\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faVCHIP-ME\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u81ea\u52a8\u8f66\u8f86\u5b9a\u4f4d\uff08AVL\uff09\u6570\u636e\u66f4\u65b0\u9891\u7387\u4e0d\u4e00\u81f4\uff0c\u9700\u8f68\u8ff9\u91cd\u5efa\u4ee5\u63d0\u9ad8\u5206\u6790\u6548\u679c\u3002", "method": "\u8bc4\u4f3013\u79cd\u65b9\u6cd5\uff08\u542b\u65b0\u9896\u65b9\u6cd5\uff09\uff0c\u5206\u6790\u901f\u5ea6\u3001\u4f4d\u7f6e\u3001\u5e73\u6ed1\u548c\u6570\u636e\u5bc6\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u6570\u5b66\u8bef\u5dee\u4e0e\u5b9e\u9645\u8003\u91cf\u3002", "result": "\u901f\u5ea6\u611f\u77e5\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u5e73\u6ed1\u65b9\u6cd5\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\uff0cVCHIP-ME\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528VCHIP-ME\u65b9\u6cd5\uff0c\u5e76\u6295\u8d44\u9ad8\u9891AVL\u6570\u636e\u6536\u96c6\u4ee5\u63d0\u5347\u5206\u6790\u8d28\u91cf\u3002"}}
{"id": "2509.00568", "pdf": "https://arxiv.org/pdf/2509.00568", "abs": "https://arxiv.org/abs/2509.00568", "authors": ["Zahra Rostamikafaki", "Francois Chan", "Claude D'Amours"], "title": "Robust Resource Allocation for LEO Satellite-Assisted Secure SWIPT via STAR-RIS under CSI Uncertainty", "categories": ["eess.SP"], "comment": null, "summary": "This paper proposes a robust resource allocation framework for a low Earth\norbit (LEO) satellite-enabled simultaneous wireless information and power\ntransfer (SWIPT) system, assisted by a ground-deployed simultaneously\ntransmitting and reflecting reconfigurable intelligent surface (STAR-RIS). We\nconsider a scenario where direct satellite-to-ground links are obstructed, and\nthe satellite serves multiple single-antenna energy receivers, information\nreceivers, and eavesdroppers exclusively via the STAR-RIS. A robust\noptimization problem is formulated to maximize the total harvested power,\nsubject to secrecy rate requirements, transmit power limits, and STAR-RIS\ncoefficient constraints, under a practical bounded channel state information\n(CSI) error model. To achieve optimal robust resource allocation, we address\nthe challenges posed by coupled optimization variables and bounded channel\nestimation errors by first applying the S-procedure to handle robustness\nagainst channel uncertainty. An alternating optimization (AO) framework is\nsubsequently proposed, where the active beamforming at the LEO satellite and\nthe passive beamforming at the STAR-RIS are jointly optimized, and a\npenalty-based strategy is incorporated to enforce the STAR-RIS beamforming\ndesign. Simulation results validate the effectiveness of the proposed algorithm\nand demonstrate that the STAR-RIS architecture achieves substantial performance\ngains in total harvested power over conventional RIS and other baseline\nschemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053(LEO)\u536b\u661f\u652f\u6301\u7684\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u548c\u80fd\u91cf\u4f20\u8f93(SWIPT)\u7cfb\u7edf\uff0c\u501f\u52a9\u5730\u9762\u90e8\u7f72\u7684\u540c\u65f6\u53d1\u5c04\u548c\u53cd\u5c04\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(STAR-RIS)\u3002\u5728\u76f4\u63a5\u536b\u661f-\u5730\u9762\u94fe\u8def\u53d7\u963b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7STAR-RIS\u670d\u52a1\u591a\u4e2a\u63a5\u6536\u5668\u548c\u7a83\u542c\u5668\uff0c\u4f18\u5316\u95ee\u9898\u65e8\u5728\u6700\u5927\u5316\u603b\u80fd\u91cf\u6536\u96c6\uff0c\u540c\u65f6\u6ee1\u8db3\u4fdd\u5bc6\u7387\u548c\u529f\u7387\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u536b\u661f-\u5730\u9762\u94fe\u8def\u53d7\u963b\u65f6\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7STAR-RIS\u63d0\u5347SWIPT\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u5904\u7406\u4fe1\u9053\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316(AO)\u6846\u67b6\u8054\u5408\u4f18\u5316\u4e3b\u52a8\u548c\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\uff0c\u5e76\u7ed3\u5408\u60e9\u7f5a\u7b56\u7565\u786e\u4fddSTAR-RIS\u8bbe\u8ba1\u3002", "result": "\u4eff\u771f\u8868\u660eSTAR-RIS\u67b6\u6784\u5728\u603b\u80fd\u91cf\u6536\u96c6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfRIS\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "STAR-RIS\u5728LEO\u536b\u661f\u652f\u6301\u7684SWIPT\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2509.00178", "pdf": "https://arxiv.org/pdf/2509.00178", "abs": "https://arxiv.org/abs/2509.00178", "authors": ["Marina Y. Aoyama", "Joao Moura", "Juan Del Aguila Ferrandis", "Sethu Vijayakumar"], "title": "Poke and Strike: Learning Task-Informed Exploration Policies", "categories": ["cs.RO"], "comment": "8 pages (main paper), 27 pages (including references and appendices),\n  6 figures (main paper), 21 figures (including appendices), Conference of\n  Robot Learning 2025, For videos and the project website, see\n  https://marina-aoyama.github.io/poke-and-strike/", "summary": "In many dynamic robotic tasks, such as striking pucks into a goal outside the\nreachable workspace, the robot must first identify the relevant physical\nproperties of the object for successful task execution, as it is unable to\nrecover from failure or retry without human intervention. To address this\nchallenge, we propose a task-informed exploration approach, based on\nreinforcement learning, that trains an exploration policy using rewards\nautomatically generated from the sensitivity of a privileged task policy to\nerrors in estimated properties. We also introduce an uncertainty-based\nmechanism to determine when to transition from exploration to task execution,\nensuring sufficient property estimation accuracy with minimal exploration time.\nOur method achieves a 90% success rate on the striking task with an average\nexploration time under 1.2 seconds, significantly outperforming baselines that\nachieve at most 40% success or require inefficient querying and retraining in a\nsimulator at test time. Additionally, we demonstrate that our task-informed\nrewards capture the relative importance of physical properties in both the\nstriking task and the classical CartPole example. Finally, we validate our\napproach by demonstrating its ability to identify object properties and adjust\ntask execution in a physical setup using the KUKA iiwa robot arm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4efb\u52a1\u4fe1\u606f\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u7279\u6743\u4efb\u52a1\u7b56\u7565\u5bf9\u5c5e\u6027\u8bef\u5dee\u7684\u654f\u611f\u6027\u81ea\u52a8\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5728\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8bc6\u522b\u5bf9\u8c61\u7684\u7269\u7406\u5c5e\u6027\u624d\u80fd\u6210\u529f\u6267\u884c\u4efb\u52a1\uff0c\u4e14\u65e0\u6cd5\u81ea\u4e3b\u6062\u590d\u5931\u8d25\u3002\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u6216\u6210\u529f\u7387\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u4fe1\u606f\u63a2\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u673a\u5236\u51b3\u5b9a\u63a2\u7d22\u4e0e\u4efb\u52a1\u6267\u884c\u7684\u8fc7\u6e21\u65f6\u673a\uff0c\u786e\u4fdd\u5c5e\u6027\u4f30\u8ba1\u51c6\u786e\u6027\u5e76\u6700\u5c0f\u5316\u63a2\u7d22\u65f6\u95f4\u3002", "result": "\u65b9\u6cd5\u5728\u51fb\u7403\u4efb\u52a1\u4e2d\u8fbe\u523090%\u6210\u529f\u7387\uff0c\u63a2\u7d22\u65f6\u95f4\u77ed\u4e8e1.2\u79d2\uff0c\u8fdc\u8d85\u57fa\u7ebf\uff1b\u5e76\u5728CartPole\u793a\u4f8b\u548cKUKA iiwa\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u4efb\u52a1\u4fe1\u606f\u5956\u52b1\u80fd\u6709\u6548\u6355\u6349\u7269\u7406\u5c5e\u6027\u7684\u91cd\u8981\u6027\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u8bbe\u7f6e\u3002"}}
{"id": "2509.00670", "pdf": "https://arxiv.org/pdf/2509.00670", "abs": "https://arxiv.org/abs/2509.00670", "authors": ["Gursimran Singh", "Aviral Chharia", "Rahul Upadhyay", "Vinay Kumar", "Luca Longo"], "title": "PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces", "categories": ["eess.SP", "cs.HC", "q-bio.NC"], "comment": "PLoS One 2025. Project Website: https://neurodiag.github.io/PyNoetic", "summary": "Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have\nemerged as a transformative technology with applications spanning robotics,\nvirtual reality, medicine, and rehabilitation. However, existing BCI frameworks\nface several limitations, including a lack of stage-wise flexibility essential\nfor experimental research, steep learning curves for researchers without\nprogramming expertise, elevated costs due to reliance on proprietary software,\nand a lack of all-inclusive features leading to the use of multiple external\ntools affecting research outcomes. To address these challenges, we present\nPyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI\nresearch. PyNoetic is one of the very few frameworks in Python that encompasses\nthe entire BCI design pipeline, from stimulus presentation and data acquisition\nto channel selection, filtering, feature extraction, artifact removal, and\nfinally simulation and visualization. Notably, PyNoetic introduces an intuitive\nand end-to-end GUI coupled with a unique pick-and-place configurable flowchart\nfor no-code BCI design, making it accessible to researchers with minimal\nprogramming experience. For advanced users, it facilitates the seamless\nintegration of custom functionalities and novel algorithms with minimal coding,\nensuring adaptability at each design stage. PyNoetic also includes a rich array\nof analytical tools such as machine learning models, brain-connectivity\nindices, systematic testing functionalities via simulation, and evaluation\nmethods of novel paradigms. PyNoetic's strengths lie in its versatility for\nboth offline and real-time BCI development, which streamlines the design\nprocess, allowing researchers to focus on more intricate aspects of BCI\ndevelopment and thus accelerate their research endeavors. Project Website:\nhttps://neurodiag.github.io/PyNoetic", "AI": {"tldr": "PyNoetic\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684BCI\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709BCI\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u5982\u7f3a\u4e4f\u7075\u6d3b\u6027\u3001\u9ad8\u5b66\u4e60\u6210\u672c\u548c\u4f9d\u8d56\u4e13\u6709\u8f6f\u4ef6\u3002\u5b83\u63d0\u4f9b\u7aef\u5230\u7aefGUI\u548c\u65e0\u4ee3\u7801\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u9002\u5408\u5404\u79cd\u7814\u7a76\u9700\u6c42\u3002", "motivation": "\u73b0\u6709BCI\u6846\u67b6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u7f3a\u4e4f\u7075\u6d3b\u6027\u3001\u9ad8\u5b66\u4e60\u6210\u672c\u548c\u9ad8\u8d39\u7528\uff0c\u5f71\u54cd\u4e86\u7814\u7a76\u6548\u679c\u3002PyNoetic\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PyNoetic\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u6db5\u76d6BCI\u8bbe\u8ba1\u5168\u6d41\u7a0b\uff0c\u63d0\u4f9bGUI\u548c\u65e0\u4ee3\u7801\u914d\u7f6e\u6d41\u7a0b\uff0c\u652f\u6301\u9ad8\u7ea7\u7528\u6237\u81ea\u5b9a\u4e49\u529f\u80fd\u3002", "result": "PyNoetic\u652f\u6301\u79bb\u7ebf\u548c\u5b9e\u65f6BCI\u5f00\u53d1\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5206\u6790\u5de5\u5177\uff0c\u7b80\u5316\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u52a0\u901f\u7814\u7a76\u8fdb\u5c55\u3002", "conclusion": "PyNoetic\u662f\u4e00\u4e2a\u7075\u6d3b\u3001\u6613\u7528\u4e14\u529f\u80fd\u5168\u9762\u7684BCI\u6846\u67b6\uff0c\u9002\u5408\u4e0d\u540c\u7814\u7a76\u80cc\u666f\u7684\u7528\u6237\uff0c\u6709\u671b\u63a8\u52a8BCI\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.00215", "pdf": "https://arxiv.org/pdf/2509.00215", "abs": "https://arxiv.org/abs/2509.00215", "authors": ["Joseph Amigo", "Rooholla Khorrambakht", "Elliot Chane-Sane", "Nicolas Mansard", "Ludovic Righetti"], "title": "First Order Model-Based RL through Decoupled Backpropagation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/", "summary": "There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u5668\u8f68\u8ff9\u751f\u6210\u4e0e\u5b66\u4e60\u6a21\u578b\u68af\u5ea6\u8ba1\u7b97\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMBRL\u4e2d\u7684\u9884\u6d4b\u8bef\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u4e00\u9636\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u7531\u4e8e\u4eff\u771f\u5668\u68af\u5ea6\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u83b7\u53d6\u6216\u5176\u5b9e\u73b0\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7684\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53d7\u5230\u9650\u5236\u3002MBRL\u867d\u80fd\u8fd1\u4f3c\u8fd9\u4e9b\u68af\u5ea6\uff0c\u4f46\u7531\u4e8e\u8bad\u7ec3\u4e2d\u7684\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\uff0c\u7b56\u7565\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u5c06\u8f68\u8ff9\u751f\u6210\u4e0e\u68af\u5ea6\u8ba1\u7b97\u89e3\u8026\uff1a\u8f68\u8ff9\u901a\u8fc7\u4eff\u771f\u5668\u5c55\u5f00\uff0c\u68af\u5ea6\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u53ef\u5fae\u5206\u6a21\u578b\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u63a7\u5236\u4efb\u52a1\u548c\u771f\u5b9eGo2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9ad8\u6548\u4f18\u5316\u5668\u7684\u6837\u672c\u6548\u7387\u548c\u6807\u51c6\u65b9\u6cd5\u7684\u901a\u7528\u6027\uff0c\u907f\u514d\u4e86\u5176\u4ed6\u4e00\u9636MBRL\u65b9\u6cd5\u7684\u4e0d\u826f\u884c\u4e3a\u3002"}}
{"id": "2509.00727", "pdf": "https://arxiv.org/pdf/2509.00727", "abs": "https://arxiv.org/abs/2509.00727", "authors": ["Lin Guo", "Tiejun Lv", "Yashuai Cao", "Mugen Peng"], "title": "Uninformed-to-Informed Estimation: A Ping-Pong Positioning Method for Multi-user Wideband mmWave Systems", "categories": ["eess.SP"], "comment": "16 pages, 13 figures, Accepted by IEEE Transactions on Communications", "summary": "To enhance the positioning and tracking performance of dynamic user equipment\n(UE) in wideband millimeter-wave (mmWave) systems, we propose a novel\npositioning error lower bound (PELB)-driven ping-pong positioning framework,\nwhere the base station (BS) and UE alternately transmit and receive adaptive\nbeamforming signals for positioning. All beam-formers are scheduled based on\nthe locally evaluated PELB. In this framework, we exploit multi-dimensional\ninformation fusion to assist in positioning. Firstly, a multi-subcarrier\ncollaborative positioning error lower bound (MSCPEB) is proposed to evaluate\nthe positioning error limits of wideband mmWave systems, which quantifies the\ncontribution of all subcarriers to positioning accuracy. Moreover, we prove\nthat the MSCPEB does not exceed the arithmetic mean of the PELBs of the\nindividual subcarriers. Subsequently, we develop an alternating optimization\n(AO) algorithm to optimize the hybrid beamformers targeted for MSCPEB\nminimization. By convexifying this problem, closed-form solutions of\nbeamformers are derived. Finally, we develop a multipath collaborative\npositioning method that quantifies the impact of path reliability on\npositioning accuracy, with a closed-form solution for user position derived.\nThe proposed method does not rely on path resolution and traditional triangular\nrelationships. Numerical results validate that the proposed method improves\nestimation accuracy by at least 16% compared to potential schemes without\noptimized beam configurations, while requiring only approximately one-quarter\nof the slot resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9a\u4f4d\u8bef\u5dee\u4e0b\u9650\uff08PELB\uff09\u7684\u4e52\u4e53\u5b9a\u4f4d\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u52a8\u6001\u7528\u6237\u8bbe\u5907\u7684\u5b9a\u4f4d\u4e0e\u8ffd\u8e2a\u6027\u80fd\u3002\u901a\u8fc7\u591a\u7ef4\u4fe1\u606f\u878d\u5408\u548c\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u9ad8\u51c6\u786e\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u63d0\u5347\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u52a8\u6001\u7528\u6237\u8bbe\u5907\u7684\u5b9a\u4f4d\u4e0e\u8ffd\u8e2a\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5bbd\u9891\u5e26\u548c\u591a\u8def\u5f84\u73af\u5883\u4e0b\u3002", "method": "1) \u63d0\u51fa\u591a\u5b50\u8f7d\u6ce2\u534f\u4f5c\u5b9a\u4f4d\u8bef\u5dee\u4e0b\u9650\uff08MSCPEB\uff09\uff1b2) \u8bbe\u8ba1\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\u4f18\u5316\u6df7\u5408\u6ce2\u675f\u5f62\u6210\uff1b3) \u5f00\u53d1\u591a\u8def\u5f84\u534f\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u5728\u65e0\u9700\u8def\u5f84\u5206\u8fa8\u548c\u4f20\u7edf\u4e09\u89d2\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u4f30\u8ba1\u51c6\u786e\u6027\u81f3\u5c11\u63d0\u534716%\uff0c\u4e14\u4ec5\u9700\u7ea6\u56db\u5206\u4e4b\u4e00\u7684\u65f6\u9699\u8d44\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u8d44\u6e90\u6548\u7387\u9ad8\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.00218", "pdf": "https://arxiv.org/pdf/2509.00218", "abs": "https://arxiv.org/abs/2509.00218", "authors": ["Aleksandra Landowska", "Aislinn D Gomez Bergin", "Ayodeji O. Abioye", "Jayati Deshmukh", "Andriana Bouadouki", "Maria Wheadon", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Shuang Ao", "Lokesh Singh", "Yi Long", "Raffaele Miele", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "title": "Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces and overviews a multidisciplinary project aimed at\ndeveloping responsible and adaptive multi-human multi-robot (MHMR) systems for\ncomplex, dynamic settings. The project integrates co-design, ethical\nframeworks, and multimodal sensing to create AI-driven robots that are\nemotionally responsive, context-aware, and aligned with the needs of diverse\nusers. We outline the project's vision, methodology, and early outcomes,\ndemonstrating how embodied AI can support sustainable, ethical, and\nhuman-centred futures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u5b66\u79d1\u9879\u76ee\uff0c\u65e8\u5728\u5f00\u53d1\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u8d1f\u8d23\u4efb\u3001\u81ea\u9002\u5e94\u591a\u4eba\u7c7b\u591a\u673a\u5668\u4eba\uff08MHMR\uff09\u7cfb\u7edf\u3002\u9879\u76ee\u6574\u5408\u4e86\u534f\u540c\u8bbe\u8ba1\u3001\u4f26\u7406\u6846\u67b6\u548c\u591a\u6a21\u6001\u611f\u77e5\uff0c\u4ee5\u521b\u5efa\u60c5\u611f\u54cd\u5e94\u3001\u60c5\u5883\u611f\u77e5\u4e14\u7b26\u5408\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u7684AI\u673a\u5668\u4eba\u3002", "motivation": "\u9879\u76ee\u7684\u52a8\u673a\u662f\u901a\u8fc7\u591a\u5b66\u79d1\u534f\u4f5c\uff0c\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684AI\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5f3a\u8c03\u4f26\u7406\u548c\u7528\u6237\u9700\u6c42\u7684\u91cd\u8981\u6027\u3002", "method": "\u91c7\u7528\u4e86\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f26\u7406\u6846\u67b6\u548c\u591a\u6a21\u6001\u611f\u77e5\u6280\u672f\uff0c\u5f00\u53d1\u60c5\u611f\u54cd\u5e94\u548c\u60c5\u5883\u611f\u77e5\u7684AI\u673a\u5668\u4eba\u3002", "result": "\u9879\u76ee\u5c55\u793a\u4e86\u521d\u6b65\u6210\u679c\uff0c\u8bc1\u660e\u4e86\u5177\u8eabAI\u5982\u4f55\u652f\u6301\u53ef\u6301\u7eed\u3001\u4f26\u7406\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u672a\u6765\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5b66\u79d1\u65b9\u6cd5\uff0c\u53ef\u5f00\u53d1\u51fa\u9002\u5e94\u6027\u5f3a\u3001\u4f26\u7406\u654f\u611f\u7684MHMR\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u7684AI\u673a\u5668\u4eba\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.00766", "pdf": "https://arxiv.org/pdf/2509.00766", "abs": "https://arxiv.org/abs/2509.00766", "authors": ["G. Maiolini Capez", "M. A. Caceres", "C. P. Bridges", "S. Frey", "R. Armellin", "R. Garello", "P. Bargellini"], "title": "Characterization of Mega-Constellation Links for LEO Missions With Applications to EO and ISS Use Cases", "categories": ["eess.SP"], "comment": null, "summary": "Satellite missions demand ever greater connectivity, especially in the LEO\nregime. In this paper, we introduce the new mega-constellation services in\nspace paradigm: we show that megaconstellations, deployed to offer innovative\nservices to Earth's users, can provide excellent connectivity to LEO spacecraft\nas well. First, we characterise the communication link between space users and\nthe actual OneWeb and Starlink constellations. A full set of results in terms\nof availability, access duration, Doppler, and path losses as a function of\nuser orbital parameters, identifying optimal user orbits, is provided. The\nresults achieved by a multi-system user able to communicate with both fleets\nare also presented. The potential improvements available if geostationary\nconstellations are used to complement LEO megaconstellations in a multi-orbit\nsystem are discussed as well. Finally, we focus on two LEO use cases: the\nInternational Space Station and an Earth Observation Sun Synchronous satellite.\nAll the results demonstrate the numerous advantages of the mega-constellation\nconnectivity solution, which can transform LEO spacecraft into highly\nresponsive nodes of a space-to-space network.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u901a\u8fc7\u5de8\u578b\u661f\u5ea7\uff08\u5982OneWeb\u548cStarlink\uff09\u5b9e\u73b0\u9ad8\u6548\u8fde\u63a5\u7684\u53ef\u80fd\u6027\uff0c\u5206\u6790\u4e86\u901a\u4fe1\u94fe\u8def\u7684\u6027\u80fd\u5e76\u63d0\u51fa\u591a\u7cfb\u7edf\u591a\u8f68\u9053\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6ee1\u8db3\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u5bf9\u66f4\u9ad8\u8fde\u63a5\u6027\u7684\u9700\u6c42\uff0c\u63a2\u7d22\u5229\u7528\u73b0\u6709\u5de8\u578b\u661f\u5ea7\u4e3a\u592a\u7a7a\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790OneWeb\u548cStarlink\u661f\u5ea7\u7684\u901a\u4fe1\u94fe\u8def\u6027\u80fd\uff0c\u5305\u62ec\u53ef\u7528\u6027\u3001\u8bbf\u95ee\u65f6\u957f\u3001\u591a\u666e\u52d2\u6548\u5e94\u548c\u8def\u5f84\u635f\u8017\uff0c\u5e76\u7ed3\u5408\u7528\u6237\u8f68\u9053\u53c2\u6570\uff0c\u8bc6\u522b\u6700\u4f18\u8f68\u9053\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u591a\u7cfb\u7edf\u548c\u591a\u8f68\u9053\uff08\u5305\u62ec\u5730\u7403\u9759\u6b62\u8f68\u9053\u661f\u5ea7\uff09\u7684\u6f5c\u5728\u6539\u8fdb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5de8\u578b\u661f\u5ea7\u80fd\u663e\u8457\u63d0\u5347LEO\u822a\u5929\u5668\u7684\u8fde\u63a5\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u9ad8\u6548\u54cd\u5e94\u7684\u592a\u7a7a\u7f51\u7edc\u8282\u70b9\uff0c\u5c24\u5176\u662f\u5bf9\u56fd\u9645\u7a7a\u95f4\u7ad9\u548c\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7b49\u7528\u4f8b\u3002", "conclusion": "\u5de8\u578b\u661f\u5ea7\u4e3aLEO\u822a\u5929\u5668\u63d0\u4f9b\u4e86\u9ad8\u6548\u8fde\u63a5\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u8fdb\u4e00\u6b65\u901a\u8fc7\u591a\u8f68\u9053\u7cfb\u7edf\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2509.00271", "pdf": "https://arxiv.org/pdf/2509.00271", "abs": "https://arxiv.org/abs/2509.00271", "authors": ["Yishu Li", "Xinyi Mao", "Ying Yuan", "Kyutae Sim", "Ben Eisner", "David Held"], "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online", "categories": ["cs.RO"], "comment": "CoRL 2025", "summary": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain\nscenarios online by leveraging past interactions. Robots frequently encounter\nvisually ambiguous objects whose manipulation outcomes remain uncertain until\nphysically interacted with. While generative models alone could theoretically\nadapt to such ambiguity, in practice they obtain suboptimal performance in\nambiguous cases, even when conditioned on action history. To address this, we\npropose explicitly decoupling action generation from verification: we use an\nunconditional diffusion-based generator to propose multiple candidate actions\nand employ our history-aware verifier to select the most promising action by\nreasoning about past interactions. Through theoretical analysis, we demonstrate\nthat employing a verifier significantly improves expected action quality.\nEmpirical evaluations and analysis across multiple simulated and real-world\nenvironments including articulated objects, multi-modal doors, and uneven\nobject pick-up confirm the effectiveness of our method and improvements over\nbaselines. Our project website is available at:\nhttps://liy1shu.github.io/HAVE_CoRL25/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAVE\u7684\u65b0\u578b\u5386\u53f2\u611f\u77e5\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u5229\u7528\u8fc7\u53bb\u7684\u4ea4\u4e92\u6765\u5728\u7ebf\u89e3\u51b3\u4e0d\u786e\u5b9a\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6a21\u7cca\u5bf9\u8c61\u64cd\u4f5c\u4e2d\u8868\u73b0\u4f18\u4e8e\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u9762\u5bf9\u89c6\u89c9\u6a21\u7cca\u5bf9\u8c61\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u64cd\u4f5c\u95ee\u9898\uff0c\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u52a8\u4f5c\u751f\u6210\u4e0e\u9a8c\u8bc1\u5206\u79bb\uff0c\u4f7f\u7528\u65e0\u6761\u4ef6\u6269\u6563\u751f\u6210\u5668\u63d0\u51fa\u5019\u9009\u52a8\u4f5c\uff0c\u5229\u7528\u5386\u53f2\u611f\u77e5\u9a8c\u8bc1\u5668\u9009\u62e9\u6700\u4f18\u52a8\u4f5c\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u4f5c\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HAVE\u65b9\u6cd5\u5728\u89e3\u51b3\u89c6\u89c9\u6a21\u7cca\u573a\u666f\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u95ee\u9898\u4e0a\u662f\u6709\u6548\u7684\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.00774", "pdf": "https://arxiv.org/pdf/2509.00774", "abs": "https://arxiv.org/abs/2509.00774", "authors": ["Okyanus Oral"], "title": "Fast Regularized 3D Near-Field MIMO Imaging Using Stochastic Proximal Gradient Method", "categories": ["eess.SP"], "comment": "Presented in ISCS25", "summary": "Near-field multiple-input multiple-output (MIMO) radar imaging suffers from\nhigh computational load inherently due to irregular spatial sampling with\ndistributed antennas. Existing acceleration methods for near-field MIMO imaging\ntypically rely on interpolation or compensation of measurements and are\nprimarily developed for direct reconstruction. This hinders their ease of\nadoption for different MIMO geometries and requires further modification for\nregularized inversion. In this study, we address these challenges by developing\na fast regularized reconstruction approach for three-dimensional near-field\nMIMO imaging based on the Stochastic Proximal Gradient Method. We demonstrate\nthe performance of the developed approach through experimental measurements.\nThe results show a significant improvement in runtime without any notable\ncompromise in reconstruction quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u6cd5\u7684\u5feb\u901f\u6b63\u5219\u5316\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e09\u7ef4\u8fd1\u573aMIMO\u6210\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u884c\u6548\u7387\u4e14\u4e0d\u5f71\u54cd\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u8fd1\u573aMIMO\u96f7\u8fbe\u6210\u50cf\u7531\u4e8e\u5206\u5e03\u5f0f\u5929\u7ebf\u7684\u4e0d\u89c4\u5219\u7a7a\u95f4\u91c7\u6837\u800c\u9762\u4e34\u9ad8\u8ba1\u7b97\u8d1f\u8377\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u63d2\u503c\u6216\u6d4b\u91cf\u8865\u507f\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e0d\u540cMIMO\u51e0\u4f55\u7ed3\u6784\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u6cd5\u5f00\u53d1\u4e86\u4e00\u79cd\u5feb\u901f\u6b63\u5219\u5316\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u8fd1\u573aMIMO\u6210\u50cf\u7684\u8ba1\u7b97\u8d1f\u8377\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00310", "pdf": "https://arxiv.org/pdf/2509.00310", "abs": "https://arxiv.org/abs/2509.00310", "authors": ["Yuxuan Ding", "Shuangge Wang", "Tesca Fitzgerald"], "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots often struggle to generalize from a single demonstration due to the\nlack of a transferable and interpretable spatial representation. In this work,\nwe introduce TReF-6, a method that infers a simplified, abstracted 6DoF\nTask-Relevant Frame from a single trajectory. Our approach identifies an\ninfluence point purely from the trajectory geometry to define the origin for a\nlocal frame, which serves as a reference for parameterizing a Dynamic Movement\nPrimitive (DMP). This influence point captures the task's spatial structure,\nextending the standard DMP formulation beyond start-goal imitation. The\ninferred frame is semantically grounded via a vision-language model and\nlocalized in novel scenes by Grounded-SAM, enabling functionally consistent\nskill generalization. We validate TReF-6 in simulation and demonstrate\nrobustness to trajectory noise. We further deploy an end-to-end pipeline on\nreal-world manipulation tasks, showing that TReF-6 supports one-shot imitation\nlearning that preserves task intent across diverse object configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTReF-6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u8f68\u8ff9\u63a8\u65ad\u51fa6DoF\u4efb\u52a1\u76f8\u5173\u5e27\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cGrounded-SAM\u5b9e\u73b0\u4efb\u52a1\u610f\u56fe\u7684\u6cdb\u5316\u3002", "motivation": "\u673a\u5668\u4eba\u901a\u5e38\u96be\u4ee5\u901a\u8fc7\u5355\u6b21\u6f14\u793a\u6cdb\u5316\u4efb\u52a1\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u53ef\u8f6c\u79fb\u548c\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u8868\u793a\u3002", "method": "\u4ece\u8f68\u8ff9\u51e0\u4f55\u4e2d\u63a8\u65ad\u4e00\u4e2a\u62bd\u8c61\u7684\u4efb\u52a1\u76f8\u5173\u5e27\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cGrounded-SAM\u8fdb\u884c\u8bed\u4e49\u548c\u7a7a\u95f4\u5b9a\u4f4d\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TReF-6\u652f\u6301\u5355\u6b21\u6a21\u4eff\u5b66\u4e60\uff0c\u80fd\u591f\u8de8\u591a\u6837\u5bf9\u8c61\u914d\u7f6e\u4fdd\u6301\u4efb\u52a1\u610f\u56fe\u3002"}}
{"id": "2509.00782", "pdf": "https://arxiv.org/pdf/2509.00782", "abs": "https://arxiv.org/abs/2509.00782", "authors": ["Dvir Avrahami", "Amit Milstein", "Caroline Chaux", "Tirza Routtenberg", "Nir Shlezinger"], "title": "Deep Unfolding with Approximated Computations for Rapid Optimization", "categories": ["eess.SP"], "comment": "Under review for publication in the IEEE", "summary": "Optimization-based solvers play a central role in a wide range of signal\nprocessing and communication tasks. However, their applicability in\nlatency-sensitive systems is limited by the sequential nature of iterative\nmethods and the high computational cost per iteration. While deep unfolding has\nemerged as a powerful paradigm for converting iterative algorithms into learned\nmodels that operate with a fixed number of iterations, it does not inherently\naddress the cost of each iteration. In this paper, we introduce a learned\noptimization framework that jointly tackles iteration count and per-iteration\ncomplexity. Our approach is based on unfolding a fixed number of optimization\nsteps, replacing selected iterations with low-complexity approximated\ncomputations, and learning extended hyperparameters from data to compensate for\nthe introduced approximations. We demonstrate the effectiveness of our method\non two representative problems: (i) hybrid beamforming; and (ii) robust\nprincipal component analysis. These fundamental case studies show that our\nlearned approximated optimizers can achieve state-of-the-art performance while\nreducing computational complexity by over three orders of magnitude. Our\nresults highlight the potential of our approach to enable rapid, interpretable,\nand efficient decision-making in real-time systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8054\u5408\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\u548c\u6bcf\u6b21\u8fed\u4ee3\u590d\u6742\u5ea6\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3c\u8ba1\u7b97\u548c\u8d85\u53c2\u6570\u5b66\u4e60\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u6c42\u89e3\u5668\u5728\u5ef6\u8fdf\u654f\u611f\u7cfb\u7edf\u4e2d\u56e0\u8fed\u4ee3\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u53c8\u80fd\u964d\u4f4e\u6bcf\u6b21\u8fed\u4ee3\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u5c55\u5f00\u56fa\u5b9a\u6570\u91cf\u7684\u4f18\u5316\u6b65\u9aa4\uff0c\u7528\u4f4e\u590d\u6742\u5ea6\u8fd1\u4f3c\u8ba1\u7b97\u66ff\u6362\u90e8\u5206\u8fed\u4ee3\uff0c\u5e76\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8d85\u53c2\u6570\u4ee5\u8865\u507f\u8fd1\u4f3c\u5e26\u6765\u7684\u8bef\u5dee\u3002", "result": "\u5728\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u548c\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u540c\u65f6\u8fbe\u5230\u6700\u65b0\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u51b3\u7b56\u65b9\u6848\u3002"}}
{"id": "2509.00317", "pdf": "https://arxiv.org/pdf/2509.00317", "abs": "https://arxiv.org/abs/2509.00317", "authors": ["Fulvio Mastrogiovanni", "Antony Thomas"], "title": "A Framework for Task and Motion Planning based on Expanding AND/OR Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for an oral presentation at ASTRA Conference, 2025", "summary": "Robot autonomy in space environments presents unique challenges, including\nhigh perception and motion uncertainty, strict kinematic constraints, and\nlimited opportunities for human intervention. Therefore, Task and Motion\nPlanning (TMP) may be critical for autonomous servicing, surface operations, or\neven in-orbit missions, just to name a few, as it models tasks as discrete\naction sequencing integrated with continuous motion feasibility assessments. In\nthis paper, we introduce a TMP framework based on expanding AND/OR graphs,\nreferred to as TMP-EAOG, and demonstrate its adaptability to different\nscenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,\nwhich expands iteratively as the plan is executed, and performs in-the-loop\nmotion planning assessments to ascertain their feasibility. As a consequence,\nTMP-EAOG is characterised by the desirable properties of (i) robustness to a\ncertain degree of uncertainty, because AND/OR graph expansion can accommodate\nfor unpredictable information about the robot environment, (ii) controlled\nautonomy, since an AND/OR graph can be validated by human experts, and (iii)\nbounded flexibility, in that unexpected events, including the assessment of\nunfeasible motions, can lead to different courses of action as alternative\npaths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We\nuse a simulated mobile manipulator as a proxy for space-grade autonomous\nrobots. Our evaluation shows that TMP-EAOG can deal with a wide range of\nchallenges in the benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eAND/OR\u56fe\u6269\u5c55\u7684TMP\u6846\u67b6TMP-EAOG\uff0c\u7528\u4e8e\u89e3\u51b3\u7a7a\u95f4\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u7a7a\u95f4\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u9762\u4e34\u9ad8\u611f\u77e5\u548c\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u3001\u4e25\u683c\u8fd0\u52a8\u7ea6\u675f\u53ca\u6709\u9650\u4eba\u5de5\u5e72\u9884\u673a\u4f1a\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684TMP\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86TMP-EAOG\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6269\u5c55AND/OR\u56fe\u5e76\u7ed3\u5408\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u8bc4\u4f30\uff0c\u786e\u4fdd\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u53ef\u884c\u6027\u3002", "result": "TMP-EAOG\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5bf9\u5404\u7c7b\u6311\u6218\u7684\u9002\u5e94\u6027\uff0c\u5c55\u73b0\u4e86\u9c81\u68d2\u6027\u3001\u53ef\u63a7\u81ea\u4e3b\u6027\u548c\u6709\u754c\u7075\u6d3b\u6027\u3002", "conclusion": "TMP-EAOG\u6846\u67b6\u5728\u590d\u6742\u7a7a\u95f4\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.00851", "pdf": "https://arxiv.org/pdf/2509.00851", "abs": "https://arxiv.org/abs/2509.00851", "authors": ["Hao Zhang", "Fuhui Zhou", "Qihui Wuand Chau Yuen"], "title": "Spectrum Cognition: Semantic Situation for Next-Generation Spectrum Management", "categories": ["eess.SP"], "comment": "accpeted by IEEE Network", "summary": "In response to the growing complexity and demands of future wireless\ncommunication networks, spectrum cognition has emerged as an essential\ntechnique for optimizing spectrum utilization in next-generation wireless\nnetworks. This article presents a comprehensive overview of spectrum cognition,\nunderscoring its critical role in enhancing the efficiency and security of\nfuture wireless systems through the innovative perspective of \"data processing\nto signal analysis to semantic situation\". Semantic situation, as the highest\nlevel of spectrum cognition, enables the extraction of meaningful information\nfrom raw spectrum data to provide intelligent support for network decisions. We\nformally define spectrum cognition, clearly distinguishing it from traditional\nspectrum sensing, and delve into the latest advancements in both traditional\nand intelligent spectrum cognition frameworks, addressing key challenges in\nspectrum cognition. Furthermore, we propose concrete technical solutions to\naddress these challenges, highlighting the transformative potential of semantic\nsituation in shaping next-generation wireless systems. Our findings not only\ncontribute to the theoretical understanding of spectrum cognition but also\noffer practical insights for its implementation in real-world scenarios.", "AI": {"tldr": "\u9891\u8c31\u8ba4\u77e5\u662f\u4f18\u5316\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u9891\u8c31\u5229\u7528\u7684\u5173\u952e\u6280\u672f\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u5176\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6570\u636e\u5904\u7406\u5230\u8bed\u4e49\u60c5\u5883\u7684\u521b\u65b0\u6846\u67b6\u3002", "motivation": "\u5e94\u5bf9\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u7684\u590d\u6742\u6027\u548c\u9700\u6c42\uff0c\u63d0\u5347\u9891\u8c31\u5229\u7528\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u5b9a\u4e49\u9891\u8c31\u8ba4\u77e5\uff0c\u533a\u5206\u4f20\u7edf\u9891\u8c31\u611f\u77e5\uff0c\u5206\u6790\u4f20\u7edf\u4e0e\u667a\u80fd\u6846\u67b6\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u9488\u5bf9\u6311\u6218\u63d0\u51fa\u6280\u672f\u65b9\u6848\u3002", "result": "\u9891\u8c31\u8ba4\u77e5\uff08\u5c24\u5176\u662f\u8bed\u4e49\u60c5\u5883\uff09\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u667a\u80fd\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u9891\u8c31\u8ba4\u77e5\u7684\u7406\u8bba\uff0c\u8fd8\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.00319", "pdf": "https://arxiv.org/pdf/2509.00319", "abs": "https://arxiv.org/abs/2509.00319", "authors": ["Chi Kit Ng", "Huxin Gao", "Tian-Ao Ren", "Jiewen Lai", "Hongliang Ren"], "title": "Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Navigating a flexible robotic endoscope (FRE) through the gastrointestinal\ntract is critical for surgical diagnosis and treatment. However, navigation in\nthe dynamic stomach is particularly challenging because the FRE must learn to\neffectively use contact with the deformable stomach walls to reach target\nlocations. To address this, we introduce a deep reinforcement learning (DRL)\nbased Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact\nforce feedback to enhance motion stability and navigation precision. The\ntraining environment is established using a physics-based finite element method\n(FEM) simulation of a deformable stomach. Trained with the Proximal Policy\nOptimization (PPO) algorithm, our approach achieves high navigation success\nrates (within 3 mm error between the FRE's end-effector and target) and\nsignificantly outperforms baseline policies. In both static and dynamic stomach\nenvironments, the CAN agent achieved a 100% success rate with 1.6 mm average\nerror, and it maintained an 85% success rate in challenging unseen scenarios\nwith stronger external disturbances. These results validate that the DRL-based\nCAN strategy substantially enhances FRE navigation performance over prior\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u63a5\u89e6\u8f85\u52a9\u5bfc\u822a\u7b56\u7565\uff08CAN\uff09\uff0c\u7528\u4e8e\u67d4\u6027\u673a\u5668\u4eba\u5185\u7aa5\u955c\u5728\u52a8\u6001\u80c3\u90e8\u73af\u5883\u4e2d\u7684\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5728\u52a8\u6001\u80c3\u90e8\u73af\u5883\u4e2d\u5bfc\u822a\u67d4\u6027\u673a\u5668\u4eba\u5185\u7aa5\u955c\u6781\u5177\u6311\u6218\uff0c\u9700\u6709\u6548\u5229\u7528\u4e0e\u53d8\u5f62\u80c3\u58c1\u7684\u63a5\u89e6\u529b\u4ee5\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u80c3\u90e8\u73af\u5883\uff0c\u7ed3\u5408\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u8bad\u7ec3DRL\u6a21\u578b\uff0c\u5229\u7528\u63a5\u89e6\u529b\u53cd\u9988\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u80c3\u90e8\u73af\u5883\u4e2d\uff0cCAN\u7b56\u7565\u5b9e\u73b0100%\u6210\u529f\u7387\u548c1.6\u6beb\u7c73\u5e73\u5747\u8bef\u5dee\uff1b\u5728\u66f4\u5177\u6311\u6218\u7684\u672a\u89c1\u8fc7\u573a\u666f\u4e2d\u4fdd\u630185%\u6210\u529f\u7387\u3002", "conclusion": "DRL-based CAN\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u67d4\u6027\u673a\u5668\u4eba\u5185\u7aa5\u955c\u5bfc\u822a\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.00962", "pdf": "https://arxiv.org/pdf/2509.00962", "abs": "https://arxiv.org/abs/2509.00962", "authors": ["Yerzhan Mustafa", "Berker Pek\u00f6z", "Sel\u00e7uk K\u00f6se"], "title": "Lightweight Error-Correction Code Encoders in Superconducting Electronic Systems", "categories": ["eess.SP", "94C10, 81V80, 94A24, 94B05, 68M15", "B.6.1; B.7.1; C.3; E.4"], "comment": "5 pages, will be presented at IEEE SOCC 2025 Session 5: Emerging and\n  Disruptive Technologies from Mon, September 29, 2025 15:40 +04 until 17:20\n  (4th paper) in Luxor 2 (20 min.)", "summary": "Data transmission from superconducting electronic circuits, such as single\nflux quantum (SFQ) logic, to room-temperature electronics is susceptible to bit\nerrors, which may result from flux trapping, fabrication defects, and process\nparameter variations (PPV). Due to the cooling power budget at 4.2 K and\nconstraints on the chip area, the size of the error-correction code encoders is\nlimited. In this work, three lightweight error-correction code encoders are\nproposed that are based on Hamming(7,4), Hamming(8,4), and Reed-Muller(1,3)\ncodes and implemented with SFQ logic. The performance of these encoders is\nanalyzed in the presence of PPV. The trade-offs between the theoretical\ncomplexity and physical size of error-correction code encoders are identified.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u57fa\u4e8eSFQ\u903b\u8f91\u7684\u8f7b\u91cf\u7ea7\u7ea0\u9519\u7f16\u7801\u5668\uff0c\u5206\u6790\u5176\u5728\u5de5\u827a\u53c2\u6570\u53d8\u5316\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u7406\u8bba\u590d\u6742\u5ea6\u4e0e\u7269\u7406\u5c3a\u5bf8\u7684\u6743\u8861\u3002", "motivation": "\u8d85\u5bfc\u7535\u5b50\u7535\u8def\uff08\u5982SFQ\u903b\u8f91\uff09\u7684\u6570\u636e\u4f20\u8f93\u6613\u53d7\u4f4d\u9519\u8bef\u5f71\u54cd\uff0c\u9700\u5728\u6709\u9650\u51b7\u5374\u529f\u7387\u548c\u82af\u7247\u9762\u79ef\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7ea0\u9519\u3002", "method": "\u91c7\u7528Hamming(7,4)\u3001Hamming(8,4)\u548cReed-Muller(1,3)\u7801\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff0c\u901a\u8fc7SFQ\u903b\u8f91\u5b9e\u73b0\u5e76\u5206\u6790\u5176\u6027\u80fd\u3002", "result": "\u5728\u5de5\u827a\u53c2\u6570\u53d8\u5316\u4e0b\uff0c\u4e09\u79cd\u7f16\u7801\u5668\u8868\u73b0\u51fa\u4e0d\u540c\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u7406\u8bba\u590d\u6742\u5ea6\u4e0e\u7269\u7406\u5c3a\u5bf8\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u5728\u8d85\u5bfc\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u5b9e\u9645\u7ea6\u675f\u3002"}}
{"id": "2509.00328", "pdf": "https://arxiv.org/pdf/2509.00328", "abs": "https://arxiv.org/abs/2509.00328", "authors": ["Bear H\u00e4on", "Kaylene Stocking", "Ian Chuang", "Claire Tomlin"], "title": "Mechanistic interpretability for steering vision-language-action models", "categories": ["cs.RO", "cs.LG"], "comment": "CoRL 2025. Project website: https://vla-mech-interp.github.io/", "summary": "Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5185\u90e8\u8868\u793a\u89e3\u91ca\u548c\u5f15\u5bfcVLA\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u989d\u5916\u4fe1\u53f7\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u96f6\u6837\u672c\u63a7\u5236\u6548\u679c\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5065\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6295\u5f71\u53d8\u6362\u5668\u5c42\u7684\u6fc0\u6d3b\u5230\u8bed\u4e49\u65b9\u5411\uff0c\u8bc6\u522b\u4e0e\u52a8\u4f5c\u9009\u62e9\u76f8\u5173\u7684\u7a00\u758f\u8bed\u4e49\u65b9\u5411\uff0c\u5e76\u5f15\u5165\u5b9e\u65f6\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u3002", "result": "\u5728Pi0\u548cOpenVLA\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\uff08UR5\uff09\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u884c\u4e3a\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVLA\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u7ec4\u4ef6\u53ef\u88ab\u7cfb\u7edf\u6027\u5730\u7528\u4e8e\u63a7\u5236\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u900f\u660e\u548c\u53ef\u5f15\u5bfc\u7684\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.00964", "pdf": "https://arxiv.org/pdf/2509.00964", "abs": "https://arxiv.org/abs/2509.00964", "authors": ["Kuranage Roche Rayan Ranasinghe", "Zhaolin Wang", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "Emil Bj\u00f6rnson"], "title": "Doubly-Dispersive Continuous MIMO Systems: Channel Modeling and Beamforming Design", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "We address the modeling and optimal beamforming (BF) design for\nmultiple-input multiple-output (MIMO) continuous aperture array (CAPA) systems\noperating over doubly-dispersive (DD) channels. First, a comprehensive DD\ncontinuous MIMO (DDC MIMO) channel model that incorporates CAPAs at both the\ntransmitter (TX) and receiver (RX) is derived, which is used to obtain explicit\ninput-output (I/O) relations for various waveforms well suited to integrated\nsensing and communications (ISAC) and robust to DD channels, namely orthogonal\nfrequency division multiplexing (OFDM), orthogonal time frequency space (OTFS),\nand affine frequency division multiplexing (AFDM). Then, functional\noptimization problems are formulated for the design of TX and RX BF matrices\nthat maximize received power, in which novel low-complexity, closed-form\nsolutions are obtained via the calculus of variations (CoV) method, yielding\nexpressions closely related to the classical matched filter commonly used in\nconventional MIMO systems. Simulation results confirm that the proposed TX/RX\nBF designs with CAPAs provide significant performance and computational\ncomplexity gains over conventional MIMO systems in DD channels.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u53cc\u6563\uff08DD\uff09\u4fe1\u9053\u4e2d\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u8fde\u7eed\u5b54\u5f84\u9635\u5217\uff08CAPA\uff09\u7cfb\u7edf\u7684\u5efa\u6a21\u53ca\u6700\u4f18\u6ce2\u675f\u6210\u5f62\uff08BF\uff09\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u4fe1\u9053\u6a21\u578b\u548c\u4f4e\u590d\u6742\u5ea6BF\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "CAPA\u7cfb\u7edf\u5728DD\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfMIMO\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u65b0\u7684BF\u8bbe\u8ba1\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u63a5\u6536\u529f\u7387\u5e76\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u63a8\u5bfc\u4e86DDC MIMO\u4fe1\u9053\u6a21\u578b\uff0c\u9488\u5bf9OFDM\u3001OTFS\u548cAFDM\u6ce2\u5f62\u8bbe\u8ba1\u4e86BF\u77e9\u9635\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u6cd5\uff08CoV\uff09\u5f97\u5230\u95ed\u5f0f\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684CAPA BF\u8bbe\u8ba1\u5728DD\u4fe1\u9053\u4e2d\u6027\u80fd\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u5747\u4f18\u4e8e\u4f20\u7edfMIMO\u7cfb\u7edf\u3002", "conclusion": "CAPA\u7cfb\u7edf\u7684BF\u8bbe\u8ba1\u4e3aDD\u4fe1\u9053\u4e2d\u7684ISAC\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00329", "pdf": "https://arxiv.org/pdf/2509.00329", "abs": "https://arxiv.org/abs/2509.00329", "authors": ["Yu Tian", "Chi Kit Ng", "Hongliang Ren"], "title": "Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Deformable continuum robots (DCRs) present unique planning challenges due to\nnonlinear deformation mechanics and partial state observability, violating the\nMarkov assumptions of conventional reinforcement learning (RL) methods. While\nJacobian-based approaches offer theoretical foundations for rigid manipulators,\ntheir direct application to DCRs remains limited by time-varying kinematics and\nunderactuated deformation dynamics. This paper proposes Jacobian Exploratory\nDual-Phase RL (JEDP-RL), a framework that decomposes planning into phased\nJacobian estimation and policy execution. During each training step, we first\nperform small-scale local exploratory actions to estimate the deformation\nJacobian matrix, then augment the state representation with Jacobian features\nto restore approximate Markovianity. Extensive SOFA surgical dynamic\nsimulations demonstrate JEDP-RL's three key advantages over proximal policy\noptimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy\nconvergence, 2) Navigation efficiency: requires 25% fewer steps to reach the\ntarget, and 3) Generalization ability: achieve 92% success rate under material\nproperty variations and achieve 83% (33% higher than PPO) success rate in the\nunseen tissue environment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faJEDP-RL\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u96c5\u53ef\u6bd4\u77e9\u9635\u4f30\u8ba1\u4e0e\u7b56\u7565\u6267\u884c\u89e3\u51b3\u4e86\u53ef\u53d8\u5f62\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5728\u6536\u655b\u901f\u5ea6\u3001\u5bfc\u822a\u6548\u7387\u53ca\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u53ef\u53d8\u5f62\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08DCRs\uff09\u7684\u975e\u7ebf\u6027\u53d8\u5f62\u529b\u5b66\u548c\u90e8\u5206\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u9002\u7528\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u89c4\u5212\u6846\u67b6\u3002", "method": "\u63d0\u51faJEDP-RL\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a\u5c40\u90e8\u63a2\u7d22\u52a8\u4f5c\u4ee5\u4f30\u8ba1\u53d8\u5f62\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u968f\u540e\u5c06\u96c5\u53ef\u6bd4\u7279\u5f81\u878d\u5165\u72b6\u6001\u8868\u793a\u4ee5\u6062\u590d\u8fd1\u4f3c\u9a6c\u5c14\u53ef\u592b\u6027\u3002", "result": "\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cJEDP-RL\u6bd4PPO\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u5feb3.2\u500d\uff0c\u5bfc\u822a\u6b65\u9aa4\u51cf\u5c1125%\uff0c\u4e14\u5728\u6750\u6599\u5c5e\u6027\u53d8\u5316\u548c\u672a\u77e5\u7ec4\u7ec7\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "JEDP-RL\u901a\u8fc7\u5206\u9636\u6bb5\u96c5\u53ef\u6bd4\u4f30\u8ba1\u6709\u6548\u89e3\u51b3\u4e86DCRs\u7684\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00968", "pdf": "https://arxiv.org/pdf/2509.00968", "abs": "https://arxiv.org/abs/2509.00968", "authors": ["Vinith Kishore", "Valentin Debarnot", "AmirEhsan Khorashadizadeh", "Ivan Dokmani\u0107"], "title": "Localized Supervised Learning for Cryo-ET Reconstruction", "categories": ["eess.SP"], "comment": "Presented in ISCS25", "summary": "Cryo-electron tomography (Cryo-ET) is a powerful tool in structural biology\nfor 3D visualization of cells and biological systems at resolutions sufficient\nto identify individual proteins in situ. The measurements are collected by\ntilting the frozen specimen and exposing it to an electron beam of known\ndosage. As the biological samples are prone to electron damage, the samples can\nbe exposed to only a limited dosage of electrons, leading to noisy and\nincomplete measurements. Thus, the reconstructions are noisy and incomplete,\nleading to the missing wedge problem. Currently, self-supervised learning is\nused to compensate for this issue. This typically involves, for each volume to\nrecover, training a large 3D UNet on the initial noisy reconstruction, leading\nto large training time and memory requirements. In this work, we exploit the\nlocal nature of the forward model to train a lightweight network using only\nlocalized data from the measurements. This design provides flexibility in\nbalancing computational and time requirements while reconstructing the volumes\nwith high accuracy. We observe experimentally that this network can work well\non unseen datasets, despite using a network trained on a few measurements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u5904\u7406\u51b7\u51bb\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ba1\u7b97\u91cf\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u51b7\u51bb\u7535\u5b50\u65ad\u5c42\u626b\u63cf(Cryo-ET)\u5728\u6210\u50cf\u8fc7\u7a0b\u4e2d\u56e0\u7535\u5b50\u635f\u4f24\u5bfc\u81f4\u6570\u636e\u566a\u58f0\u5927\u4e14\u4e0d\u5b8c\u6574\uff0c\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u8017\u65f6\u957f\u3002", "method": "\u901a\u8fc7\u524d\u5411\u6a21\u578b\u7684\u5c40\u90e8\u6027\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u4ec5\u5904\u7406\u5c40\u90e8\u6d4b\u91cf\u6570\u636e\uff0c\u5e73\u8861\u8ba1\u7b97\u548c\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u4e5f\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7f51\u7edc\u8bbe\u8ba1\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2509.00339", "pdf": "https://arxiv.org/pdf/2509.00339", "abs": "https://arxiv.org/abs/2509.00339", "authors": ["Md. Taherul Islam Shawon", "Yuan Li", "Yincai Cai", "Junjie Niu", "Ting Peng"], "title": "Autonomous Aggregate Sorting in Construction and Mining via Computer Vision-Aided Robotic Arm Systems", "categories": ["cs.RO"], "comment": null, "summary": "Traditional aggregate sorting methods, whether manual or mechanical, often\nsuffer from low precision, limited flexibility, and poor adaptability to\ndiverse material properties such as size, shape, and lithology. To address\nthese limitations, this study presents a computer vision-aided robotic arm\nsystem designed for autonomous aggregate sorting in construction and mining\napplications. The system integrates a six-degree-of-freedom robotic arm, a\nbinocular stereo camera for 3D perception, and a ROS-based control framework.\nCore techniques include an attention-augmented YOLOv8 model for aggregate\ndetection, stereo matching for 3D localization, Denavit-Hartenberg kinematic\nmodeling for arm motion control, minimum enclosing rectangle analysis for size\nestimation, and hand-eye calibration for precise coordinate alignment.\nExperimental validation with four aggregate types achieved an average grasping\nand sorting success rate of 97.5%, with comparable classification accuracy.\nRemaining challenges include the reliable handling of small aggregates and\ntexture-based misclassification. Overall, the proposed system demonstrates\nsignificant potential to enhance productivity, reduce operational costs, and\nimprove safety in aggregate handling, while providing a scalable framework for\nadvancing smart automation in construction, mining, and recycling industries.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u9aa8\u6599\u5206\u62e3\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7cbe\u5ea6\u4f4e\u3001\u7075\u6d3b\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9aa8\u6599\u5206\u62e3\u65b9\u6cd5\u7cbe\u5ea6\u4f4e\u3001\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u3001\u53cc\u76ee\u7acb\u4f53\u76f8\u673a\u548cROS\u63a7\u5236\u6846\u67b6\uff0c\u91c7\u7528YOLOv8\u6a21\u578b\u3001\u7acb\u4f53\u5339\u914d\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u5206\u62e3\u6210\u529f\u7387\u8fbe97.5%\uff0c\u5206\u7c7b\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u751f\u4ea7\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u5e76\u4e3a\u667a\u80fd\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2509.01070", "pdf": "https://arxiv.org/pdf/2509.01070", "abs": "https://arxiv.org/abs/2509.01070", "authors": ["Erqi Huang", "John Restrepo", "Xun Cao", "Ivo Ihrke"], "title": "BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot Multispectral Light-field Imaging", "categories": ["eess.SP"], "comment": "Presented in ISCS25", "summary": "Snapshot Multispectral Light-field Imaging (SMLI) is an emerging\ncomputational imaging technique that captures high-dimensional data (x, y, z,\n$\\theta$, $\\phi$, $\\lambda$) in a single shot using a low-dimensional sensor.\nThe accuracy of high-dimensional data reconstruction depends on representing\nthe spectrum using neural radiance field models, which requires consideration\nof broadband spectral decoupling during optimization. Currently, some SMLI\napproaches avoid the challenge of model decoupling by either reducing\nlight-throughput or prolonging imaging time. In this work, we propose a\nbroadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments\nshow that our model successfully decouples a broadband multiplexed spectrum.\nConsequently, this approach enhances multispectral light-field image\nreconstruction and further advances plenoptic imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5feb\u7167\u591a\u5149\u8c31\u5149\u573a\u6210\u50cf\uff08SMLI\uff09\u7684\u5bbd\u5e26\u5149\u8c31\u795e\u7ecf\u8f90\u5c04\u573a\uff08BSNeRF\uff09\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5bbd\u5e26\u5149\u8c31\u89e3\u8026\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dSMLI\u65b9\u6cd5\u5728\u91cd\u5efa\u9ad8\u7ef4\u6570\u636e\u65f6\u9700\u8981\u89e3\u51b3\u5bbd\u5e26\u5149\u8c31\u89e3\u8026\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u727a\u7272\u5149\u901a\u91cf\u6216\u5ef6\u957f\u6210\u50cf\u65f6\u95f4\u89c4\u907f\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86BSNeRF\u6a21\u578b\uff0c\u7528\u4e8e\u5728SMLI\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5bbd\u5e26\u5149\u8c31\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u6210\u529f\u89e3\u8026\u5bbd\u5e26\u590d\u7528\u5149\u8c31\uff0c\u6539\u5584\u4e86\u591a\u5149\u8c31\u5149\u573a\u56fe\u50cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5149\u573a\u6210\u50cf\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2509.00361", "pdf": "https://arxiv.org/pdf/2509.00361", "abs": "https://arxiv.org/abs/2509.00361", "authors": ["Chuye Zhang", "Xiaoxiong Zhang", "Wei Pan", "Linfang Zheng", "Wei Zhang"], "title": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-Top Manipulation", "categories": ["cs.RO"], "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Robotic manipulation in unstructured environments requires systems that can\ngeneralize across diverse tasks while maintaining robust and reliable\nperformance. We introduce {GVF-TAPE}, a closed-loop framework that combines\ngenerative visual foresight with task-agnostic pose estimation to enable\nscalable robotic manipulation. GVF-TAPE employs a generative video model to\npredict future RGB-D frames from a single side-view RGB image and a task\ndescription, offering visual plans that guide robot actions. A decoupled pose\nestimation model then extracts end-effector poses from the predicted frames,\ntranslating them into executable commands via low-level controllers. By\niteratively integrating video foresight and pose estimation in a closed loop,\nGVF-TAPE achieves real-time, adaptive manipulation across a broad range of\ntasks. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach reduces reliance on task-specific action data and\ngeneralizes effectively, providing a practical and scalable solution for\nintelligent robotic systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.01117", "pdf": "https://arxiv.org/pdf/2509.01117", "abs": "https://arxiv.org/abs/2509.01117", "authors": ["Gyoseung Lee", "Junil Choi"], "title": "A Bayesian Framework For Cascaded Channel Estimation in RIS-Aided mmWave Systems", "categories": ["eess.SP"], "comment": "Accepted to IEEE Wireless Communications Letters", "summary": "In this paper, we investigate cascaded channel estimation for reconfigurable\nintelligent surface (RIS)-aided millimeter-wave multi-user communication\nsystems. Since the complex channel gains of the cascaded RIS channel are\ngenerally non-Gaussian, the use of the linear minimum mean squared error\n(LMMSE) estimator leads to inevitable performance degradation. To tackle this\nissue, we propose a variational inference-based framework that approximates the\ncomplex channel gains using a complex adaptive Laplace prior, which effectively\ncaptures their probability distributions in a tractable way. Numerical results\ndemonstrate that the proposed estimator outperforms conventional estimators\nincluding least squares and LMMSE in terms of cascaded channel estimation\nerror.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8eRIS\u8f85\u52a9\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\uff0c\u901a\u8fc7\u590d\u81ea\u9002\u5e94\u62c9\u666e\u62c9\u65af\u5148\u9a8c\u6355\u6349\u590d\u6742\u4fe1\u9053\u589e\u76ca\u7684\u975e\u9ad8\u65af\u7279\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u7ea7\u8054RIS\u4fe1\u9053\u7684\u590d\u6742\u4fe1\u9053\u589e\u76ca\u901a\u5e38\u662f\u975e\u9ad8\u65af\u7684\uff0c\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08LMMSE\uff09\u4f30\u8ba1\u5668\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53d8\u5206\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u590d\u81ea\u9002\u5e94\u62c9\u666e\u62c9\u65af\u5148\u9a8c\u8fd1\u4f3c\u590d\u6742\u4fe1\u9053\u589e\u76ca\u7684\u6982\u7387\u5206\u5e03\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728\u7ea7\u8054\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u65b9\u9762\u4f18\u4e8e\u6700\u5c0f\u4e8c\u4e58\u548cLMMSE\u7b49\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u975e\u9ad8\u65af\u4fe1\u9053\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2509.00465", "pdf": "https://arxiv.org/pdf/2509.00465", "abs": "https://arxiv.org/abs/2509.00465", "authors": ["Jiading Fang"], "title": "Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This thesis introduces \"Embodied Spatial Intelligence\" to address the\nchallenge of creating robots that can perceive and act in the real world based\non natural language instructions. To bridge the gap between Large Language\nModels (LLMs) and physical embodiment, we present contributions on two fronts:\nscene representation and spatial reasoning. For perception, we develop robust,\nscalable, and accurate scene representations using implicit neural models, with\ncontributions in self-supervised camera calibration, high-fidelity depth field\ngeneration, and large-scale reconstruction. For spatial reasoning, we enhance\nthe spatial capabilities of LLMs by introducing a novel navigation benchmark, a\nmethod for grounding language in 3D, and a state-feedback mechanism to improve\nlong-horizon decision-making. This work lays a foundation for robots that can\nrobustly perceive their surroundings and intelligently act upon complex,\nlanguage-based commands.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa'\u5177\u8eab\u7a7a\u95f4\u667a\u80fd'\uff0c\u901a\u8fc7\u573a\u666f\u8868\u793a\u548c\u7a7a\u95f4\u63a8\u7406\u4e24\u4e2a\u65b9\u9762\uff0c\u7ed3\u5408LLMs\u4e0e\u673a\u5668\u4eba\u7269\u7406\u5b9e\u73b0\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u611f\u77e5\u4e0e\u884c\u52a8\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u611f\u77e5\u4e0e\u884c\u52a8\u95ee\u9898\uff0c\u7ed3\u5408LLMs\u4e0e\u7269\u7406\u5b9e\u4f53\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "1. \u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u6a21\u578b\u5f00\u53d1\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u548c\u51c6\u786e\u7684\u573a\u666f\u8868\u793a\uff1b2. \u63d0\u51fa\u5bfc\u822a\u57fa\u51c6\u3001\u8bed\u8a00\u57283D\u4e2d\u7684\u843d\u5730\u65b9\u6cd5\u548c\u72b6\u6001\u53cd\u9988\u673a\u5236\uff0c\u589e\u5f3aLLMs\u7684\u7a7a\u95f4\u80fd\u529b\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6df1\u5ea6\u573a\u751f\u6210\u3001\u5927\u89c4\u6a21\u91cd\u5efa\u548c\u957f\u7a0b\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u611f\u77e5\u4e0e\u884c\u52a8\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u7269\u7406\u73af\u5883\u4ea4\u4e92\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2509.01121", "pdf": "https://arxiv.org/pdf/2509.01121", "abs": "https://arxiv.org/abs/2509.01121", "authors": ["Yali Zhang", "Haifan Yin", "Weidong Li", "Emil Bjornson", "Merouane Debbah"], "title": "Fluid Antenna Port Prediction based on Large Language Models", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, 1 table, To appear in IEEE Globecom 2025 SAC -\n  MLCN", "summary": "This study seeks to utilize large language models (LLMs) to forecast the\nmoving ports of fluid antenna (FA). By repositioning the antenna to the\nlocations identified by our proposed model, we intend to address the mobility\nchallenges faced by user equipment (UE). To the best of our knowledge, this\npaper introduces, for the first time, the application of LLMs in the prediction\nof FA ports, presenting a novel model termed Port-LLM. The architecture of our\nmodel is based on the pre-trained GPT-2 framework. We designed specialized data\npreprocessing, input embedding, and output projection modules to effectively\nbridge the disparities between the wireless communication data and the data\nformat utilized by the pre-trained LLM. Simulation results demonstrate that our\nmodel exhibits superior predictive performance under different numbers of base\nstation (BS) antennas and varying UE speeds, indicating strong generalization\nand robustness ability. Furthermore, the spectral efficiency (SE) attained by\nour model surpasses that achieved by traditional methods in both medium and\nhigh-speed mobile environments.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9884\u6d4b\u6d41\u4f53\u5929\u7ebf\uff08FA\uff09\u7684\u79fb\u52a8\u7aef\u53e3\uff0c\u63d0\u51fa\u4e86\u65b0\u9896\u7684Port-LLM\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u7684\u79fb\u52a8\u6027\u95ee\u9898\u3002", "motivation": "\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u5929\u7ebf\u5230\u6a21\u578b\u9884\u6d4b\u7684\u4f4d\u7f6e\uff0c\u89e3\u51b3UE\u9762\u4e34\u7684\u79fb\u52a8\u6027\u6311\u6218\uff0c\u9996\u6b21\u5c06LLMs\u5e94\u7528\u4e8eFA\u7aef\u53e3\u9884\u6d4b\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684GPT-2\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u6570\u636e\u9884\u5904\u7406\u3001\u8f93\u5165\u5d4c\u5165\u548c\u8f93\u51fa\u6295\u5f71\u6a21\u5757\uff0c\u4ee5\u9002\u914d\u65e0\u7ebf\u901a\u4fe1\u6570\u636e\u4e0eLLM\u6570\u636e\u683c\u5f0f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u4e0d\u540cBS\u5929\u7ebf\u6570\u91cf\u548cUE\u901f\u5ea6\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u5728\u4e2d\u9ad8\u901f\u79fb\u52a8\u73af\u5883\u4e2dSE\u8d85\u8fc7\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Port-LLM\u5c55\u73b0\u4e86\u5f3a\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u80fd\u529b\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00491", "pdf": "https://arxiv.org/pdf/2509.00491", "abs": "https://arxiv.org/abs/2509.00491", "authors": ["Masaki Saito", "Shunki Itadera", "Toshiyuki Murakami"], "title": "Extended Diffeomorphism for Real-Time Motion Replication in Workspaces with Different Spatial Arrangements", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents two types of extended diffeomorphism designs to\ncompensate for spatial placement differences between robot workspaces.\nTeleoperation of multiple robots is attracting attention to expand the\nutilization of the robot embodiment. Real-time reproduction of robot motion\nwould facilitate the efficient execution of similar tasks by multiple robots. A\nchallenge in the motion reproduction is compensating for the spatial\narrangement errors of target keypoints in robot workspaces. This paper proposes\na methodology for smooth mappings that transform primary robot poses into\nfollower robot poses based on the predefined key points in each workspace.\nThrough a picking task experiment using a dual-arm UR5 robot, this study\ndemonstrates that the proposed mapping generation method can balance lower\nmapping errors for precise operation and lower mapping gradients for smooth\nreplicated movement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6269\u5c55\u5fae\u5206\u540c\u80da\u8bbe\u8ba1\uff0c\u7528\u4e8e\u8865\u507f\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u4e4b\u95f4\u7684\u7a7a\u95f4\u4f4d\u7f6e\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u6620\u5c04\u8bef\u5dee\u548c\u5e73\u6ed1\u8fd0\u52a8\u590d\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u5174\u8d77\u4e3a\u63d0\u9ad8\u673a\u5668\u4eba\u5229\u7528\u7387\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u76ee\u6807\u5173\u952e\u70b9\u7684\u7a7a\u95f4\u6392\u5217\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u5b9a\u4e49\u5173\u952e\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e3b\u673a\u5668\u4eba\u59ff\u6001\u5e73\u6ed1\u6620\u5c04\u5230\u4ece\u673a\u5668\u4eba\u59ff\u6001\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u53ccUR5\u673a\u5668\u4eba\u6293\u53d6\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u4f4e\u6620\u5c04\u8bef\u5dee\u548c\u5e73\u6ed1\u8fd0\u52a8\u590d\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u673a\u5668\u4eba\u9ad8\u6548\u6267\u884c\u76f8\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u6620\u5c04\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2509.01125", "pdf": "https://arxiv.org/pdf/2509.01125", "abs": "https://arxiv.org/abs/2509.01125", "authors": ["Yuan Gao", "Zichen Lu", "Yifan Wu", "Yanliang Jin", "Shunqing Zhang", "Xiaoli Chu", "Shugong Xu", "Cheng-Xiang Wang"], "title": "Enabling 6G Through Multi-Domain Channel Extrapolation: Opportunities and Challenges of Generative Artificial Intelligence", "categories": ["eess.SP"], "comment": null, "summary": "Channel extrapolation has attracted wide attention due to its potential to\nacquire channel state information (CSI) with high accuracy and minimal\noverhead. This is becoming increasingly crucial as the sixth-generation (6G)\nmobile networks aim to support complex scenarios, for example, high-mobility\ncommunications utilizing ultra-massive multiple-input multiple-output (MIMO)\ntechnologies and broad spectrum bands, necessitating multi-domain channel\nextrapolation. Current research predominantly addresses channel extrapolation\nwithin a single domain, lacking a comprehensive approach to multi-domain\nchannel extrapolation. To bridge the gap, we propose the concept of\nmulti-domain channel extrapolation, detailing the essential performance\nrequirements for 6G networks. These include precise channel extrapolation,\nadaptability to varying scenarios, and manageable computational complexity\nduring both training and inference stages. In light of these requirements, we\nelaborate the potential and challenges of incorporating generative artificial\nintelligence (GAI)-based models for effective multi-domain channel\nextrapolation. Given the ability of the Transformer to capture long-range\ndependencies and hidden patterns, we propose a novel Transformer encoder-like\nmodel by eliminating the positional encoding module and replacing the original\nmulti-head attention with a multilayer perceptron (MLP) for multi-domain\nchannel extrapolation. Simulation results indicate that this model surpasses\nexisting baseline models in terms of extrapolation accuracy and inference\nspeed. Ablation studies further demonstrate the effectiveness of the module\ndesign of the proposed design. Finally, we pose several open questions for the\ndevelopment of practical GAI-based multi-domain channel extrapolation models,\nincluding the issues of explainability, generalization, and dataset collection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u57df\u4fe1\u9053\u5916\u63a8\u7684\u65b0\u6982\u5ff5\uff0c\u4ee5\u6ee1\u8db36G\u7f51\u7edc\u4e2d\u590d\u6742\u573a\u666f\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684Transformer\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd\u5916\u63a8\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u652f\u6301\u8d85\u5927\u89c4\u6a21MIMO\u548c\u5bbd\u5e26\u9891\u8c31\u7b49\u590d\u6742\u573a\u666f\uff0c\u5f53\u524d\u5355\u57df\u4fe1\u9053\u5916\u63a8\u65b9\u6cd5\u4e0d\u8db3\uff0c\u4e9f\u9700\u591a\u57df\u4fe1\u9053\u5916\u63a8\u7684\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bTransformer\u7f16\u7801\u5668\u6a21\u578b\uff0c\u53bb\u9664\u4e86\u4f4d\u7f6e\u7f16\u7801\u6a21\u5757\u5e76\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u66ff\u4ee3\u591a\u5934\u6ce8\u610f\u529b\uff0c\u7528\u4e8e\u591a\u57df\u4fe1\u9053\u5916\u63a8\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u5916\u63a8\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u5757\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u5728\u591a\u57df\u4fe1\u9053\u5916\u63a8\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5982\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u96c6\u6536\u96c6\u3002"}}
{"id": "2509.00497", "pdf": "https://arxiv.org/pdf/2509.00497", "abs": "https://arxiv.org/abs/2509.00497", "authors": ["Yiyang Chen", "Zhigang Wu", "Guohong Zheng", "Xuesong Wu", "Liwen Xu", "Haoyuan Tang", "Zhaocheng He", "Haipeng Zeng"], "title": "FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories", "categories": ["cs.RO", "cs.CV"], "comment": "26 pages, 14 figures", "summary": "The trajectory data of traffic participants (TPs) is a fundamental resource\nfor evaluating traffic conditions and optimizing policies, especially at urban\nintersections. Although data acquisition using drones is efficient, existing\ndatasets still have limitations in scene representativeness, information\nrichness, and data fidelity. This study introduces FLUID, comprising a\nfine-grained trajectory dataset that captures dense conflicts at typical urban\nsignalized intersections, and a lightweight, full-pipeline framework for\ndrone-based trajectory processing. FLUID covers three distinct intersection\ntypes, with approximately 5 hours of recording time and featuring over 20,000\nTPs across 8 categories. Notably, the dataset averages two vehicle conflicts\nper minute, involving roughly 25% of all motor vehicles. FLUID provides\ncomprehensive data, including trajectories, traffic signals, maps, and raw\nvideos. Comparison with the DataFromSky platform and ground-truth measurements\nvalidates its high spatio-temporal accuracy. Through a detailed classification\nof motor vehicle conflicts and violations, FLUID reveals a diversity of\ninteractive behaviors, demonstrating its value for human preference mining,\ntraffic behavior modeling, and autonomous driving research.", "AI": {"tldr": "FLUID \u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u4ea4\u901a\u53c2\u4e0e\u8005\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u57ce\u5e02\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7684\u5bc6\u96c6\u51b2\u7a81\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u6570\u636e\u5e76\u9a8c\u8bc1\u4e86\u5176\u65f6\u7a7a\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4ea4\u901a\u53c2\u4e0e\u8005\u8f68\u8ff9\u6570\u636e\u5728\u573a\u666f\u4ee3\u8868\u6027\u3001\u4fe1\u606f\u4e30\u5bcc\u6027\u548c\u6570\u636e\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u91c7\u96c6\u548c\u5904\u7406\u65b9\u6cd5\u3002", "method": "FLUID \u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u8f7b\u91cf\u7ea7\u5168\u6d41\u7a0b\u6846\u67b6\uff0c\u91c7\u96c6\u4e09\u7c7b\u5178\u578b\u4ea4\u53c9\u53e3\u7684\u6570\u636e\uff0c\u6db5\u76d6\u8f68\u8ff9\u3001\u4ea4\u901a\u4fe1\u53f7\u3001\u5730\u56fe\u548c\u539f\u59cb\u89c6\u9891\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b 20,000 \u591a\u4e2a\u4ea4\u901a\u53c2\u4e0e\u8005\uff0c\u5e73\u5747\u6bcf\u5206\u949f\u4e24\u6b21\u8f66\u8f86\u51b2\u7a81\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u65f6\u7a7a\u7cbe\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u591a\u79cd\u4ea4\u4e92\u884c\u4e3a\u3002", "conclusion": "FLUID \u6570\u636e\u96c6\u5728\u4eba\u7c7b\u504f\u597d\u6316\u6398\u3001\u4ea4\u901a\u884c\u4e3a\u5efa\u6a21\u548c\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.01127", "pdf": "https://arxiv.org/pdf/2509.01127", "abs": "https://arxiv.org/abs/2509.01127", "authors": ["Asli Alpman", "Mustafa Utkur", "Emine Ulku Saritas"], "title": "A Model-Based Dictionary Approach for Magnetic Nanoparticle Signal Prediction", "categories": ["eess.SP"], "comment": null, "summary": "Magnetic particle imaging (MPI) is a tracer-based medical imaging modality\nthat enables quantification and spatial mapping of magnetic nanoparticle (MNP)\ndistribution. The magnetization response of MNPs depends on experimental\nconditions such as drive field (DF) settings and medium viscosity, as well as\non magnetic parameters of MNPs such as magnetic core diameter, hydrodynamic\ndiameter, and magnetic anisotropy constant. A comprehensive understanding of\nthe magnetization response of MNPs can facilitate the optimization of DF and\nMNP type for a given MPI application, without the need for extensive\nexperimentation. In this work, we propose a calibration-free iterative\nalgorithm using model-based dictionaries for MNP signal prediction at untested\nsettings. Dictionaries were constructed with the MNP signals simulated using\nthe coupled Brown-N\\'eel rotation model. Based on the available measurements,\nthe proposed algorithm jointly estimates the dictionary weights and the\ntransfer functions due to non-model-based dynamics. These dynamics include the\nsystem response of the measurement setup as well as magnetization dynamics not\naccounted for by the employed coupled Brown-N\\'eel rotation model. The\nalgorithm was first validated on synthetic signals at SNR levels of 1 and 10,\nand then tested on an in-house MPS setup across six viscosity levels\n(0.89-15.33 mPa.s) and DF frequencies of 0.25-2 kHz using two commercial MNPs.\nValidation on synthetic signals showed accurate weight and transfer function\nestimation even at SNR 1. MPS experiments demonstrated successful prediction of\nMNP signals at untested viscosities, with NRMSE below 1.51% and 3.5% for the\ntwo tested MNPs across all DF settings. Predicted signals captured viscosity\ndependent trends, and NWD values remained low (<0.10 and <0.07 for the two\ntested MNPs), confirming robust weight estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6821\u51c6\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5b57\u5178\u9884\u6d4b\u78c1\u6027\u7eb3\u7c73\u7c92\u5b50\uff08MNP\uff09\u5728\u672a\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u7684\u4fe1\u53f7\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5408\u6210\u4fe1\u53f7\u548c\u5b9e\u9645MPS\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u78c1\u6027\u7c92\u5b50\u6210\u50cf\uff08MPI\uff09\u4e2d\u7684MNP\u78c1\u5316\u54cd\u5e94\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u4f18\u5316\u9a71\u52a8\u573a\u548cMNP\u7c7b\u578b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u51cf\u5c11\u5b9e\u9a8c\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8026\u5408Brown-N\u00e9el\u65cb\u8f6c\u6a21\u578b\u7684MNP\u4fe1\u53f7\u6a21\u62df\u5b57\u5178\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u975e\u6a21\u578b\u52a8\u529b\u5b66\u56e0\u7d20\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u4f30\u8ba1\u6743\u91cd\u548c\u4f20\u9012\u51fd\u6570\u3002", "result": "\u7b97\u6cd5\u5728SNR 1\u548c10\u4e0b\u5bf9\u5408\u6210\u4fe1\u53f7\u9a8c\u8bc1\u51c6\u786e\uff0c\u5b9e\u9645MPS\u5b9e\u9a8c\u4e2d\u57280.89-15.33 mPa.s\u7c98\u5ea6\u8303\u56f4\u5185\u6210\u529f\u9884\u6d4b\u4fe1\u53f7\uff0cNRMSE\u4f4e\u4e8e1.51%\u548c3.5%\uff0cNWD\u503c\u4f4e\uff08<0.10\u548c<0.07\uff09\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4bMNP\u4fe1\u53f7\uff0c\u51cf\u5c11\u5b9e\u9a8c\u4f18\u5316\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u9a71\u52a8\u573a\u548c\u7c98\u5ea6\u6761\u4ef6\u4e0b\u7684MPI\u5e94\u7528\u3002"}}
{"id": "2509.00499", "pdf": "https://arxiv.org/pdf/2509.00499", "abs": "https://arxiv.org/abs/2509.00499", "authors": ["Dongwon Son", "Hojin Jung", "Beomjoon Kim"], "title": "NeuralSVCD for Efficient Swept Volume Collision Detection", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "CoRL 2025", "summary": "Robot manipulation in unstructured environments requires efficient and\nreliable Swept Volume Collision Detection (SVCD) for safe motion planning.\nTraditional discrete methods potentially miss collisions between these points,\nwhereas SVCD continuously checks for collisions along the entire trajectory.\nExisting SVCD methods typically face a trade-off between efficiency and\naccuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a\nnovel neural encoder-decoder architecture tailored to overcome this trade-off.\nOur approach leverages shape locality and temporal locality through distributed\ngeometric representations and temporal optimization. This enhances\ncomputational efficiency without sacrificing accuracy. Comprehensive\nexperiments show that NeuralSVCD consistently outperforms existing\nstate-of-the-art SVCD methods in terms of both collision detection accuracy and\ncomputational efficiency, demonstrating its robust applicability across diverse\nrobotic manipulation scenarios. Code and videos are available at\nhttps://neuralsvcd.github.io/.", "AI": {"tldr": "NeuralSVCD\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u51e0\u4f55\u8868\u793a\u548c\u65f6\u95f4\u4f18\u5316\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u68c0\u6d4b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u78b0\u649e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u9ad8\u6548\u53ef\u9760\u7684\u626b\u63a0\u4f53\u79ef\u78b0\u649e\u68c0\u6d4b\uff08SVCD\uff09\uff0c\u4f20\u7edf\u79bb\u6563\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u78b0\u649e\u70b9\uff0c\u73b0\u6709SVCD\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u5f15\u5165NeuralSVCD\uff0c\u5229\u7528\u5f62\u72b6\u5c40\u90e8\u6027\u548c\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u51e0\u4f55\u8868\u793a\u548c\u65f6\u95f4\u4f18\u5316\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeuralSVCD\u5728\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709SVCD\u65b9\u6cd5\u3002", "conclusion": "NeuralSVCD\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.01163", "pdf": "https://arxiv.org/pdf/2509.01163", "abs": "https://arxiv.org/abs/2509.01163", "authors": ["Duc Viet Nguyen", "Haiquan Zhao", "Jinhui Hu"], "title": "Dynamic State Estimation of Power System Utilizing Cauchy Kernel-Based Maximum Mixture Correntropy UKF over Beluga Whale-Bat Optimization", "categories": ["eess.SP", "53-04", "I.6.3"], "comment": "11 pages, 10 figures", "summary": "Non-Gaussian noise, outliers, sudden load changes, and bad measurement data\nare key factors that diminish the accuracy of dynamic state estimation in power\nsystems. Additionally, unscented Kalman filters (UKF) based on correntropy\ncriteria utilize bandwidth-sensitive Gaussian kernels, which may lead to\nsingular matrices in the Cholesky decomposition. To overcome all the above\nproblems, in this paper, a robust UKF based on Cauchy kernel maximum mixture\ncorrentropy (CKMMC) criteria over hybrid Beluga Whale-Bat (BWB) optimization\n(BWB-CKMMC-UKF) is proposed, in which the kernel is merged of two Cauchy\nfunctions. Specifically, the measurement error and state error are unified in\nthe cost function by the statistical linearization technique, and the optimal\nvalue of state estimation is obtained by fixed-point iteration. Because of its\ninsensitive feature to kernel bandwidth and notable thick-tailed feature, the\nCauchy kernel function is utilized instead of the Gaussian kernel in the\noptimization criteria. Additionally, to fit the power system model, the shape\ncoefficients of the kernel in the CKMMC criterion and scale coefficients that\ninfluence the selection of sigma points in the unscented transform are\ndetermined based on the BWB algorithm. Simulation results on IEEE 14, 30, and\n57-bus test systems validated the performance of the proposed algorithm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.00530", "pdf": "https://arxiv.org/pdf/2509.00530", "abs": "https://arxiv.org/abs/2509.00530", "authors": ["Fanxin Wang", "Yikun Cheng", "Chuyuan Tao", "Rohit Bhargava", "Thenkurussi Kesavadas"], "title": "Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Presented in EMBC 2025", "summary": "Tissue biopsy is the gold standard for diagnosing many diseases, involving\nthe extraction of diseased tissue for histopathology analysis by expert\npathologists. However, this procedure has two main limitations: 1) Manual\nsampling through tissue biopsy is prone to inaccuracies; 2) The extraction\nprocess is followed by a time-consuming pathology test. To address these\nlimitations, we present a compact, accurate, and maneuverable robotic insertion\nplatform to overcome the limitations in traditional histopathology. Our\nplatform is capable of steering a variety of tools with different sizes,\nincluding needle for tissue extraction and optical fibers for vibrational\nspectroscopy applications. This system facilitates the guidance of end-effector\nto the tissue and assists surgeons in navigating to the biopsy target area for\nmulti-modal diagnosis. In this paper, we outline the general concept of our\ndevice, followed by a detailed description of its mechanical design and control\nscheme. We conclude with the validation of the system through a series of\ntests, including positioning accuracy, admittance performance, and tool\ninsertion efficacy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u51c6\u786e\u4e14\u7075\u6d3b\u7684\u673a\u5668\u4eba\u63d2\u5165\u5e73\u53f0\uff0c\u7528\u4e8e\u6539\u8fdb\u4f20\u7edf\u7ec4\u7ec7\u6d3b\u68c0\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u8bca\u65ad\u3002", "motivation": "\u4f20\u7edf\u7ec4\u7ec7\u6d3b\u68c0\u5b58\u5728\u91c7\u6837\u4e0d\u51c6\u786e\u548c\u75c5\u7406\u6d4b\u8bd5\u8017\u65f6\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u53ef\u5f15\u5bfc\u591a\u79cd\u5de5\u5177\uff08\u5982\u9488\u5934\u548c\u5149\u5b66\u7ea4\u7ef4\uff09\u8fdb\u884c\u7ec4\u7ec7\u63d0\u53d6\u548c\u5149\u8c31\u5206\u6790\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5176\u673a\u68b0\u8bbe\u8ba1\u548c\u63a7\u5236\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u5bfc\u7eb3\u6027\u80fd\u548c\u5de5\u5177\u63d2\u5165\u6548\u679c\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5e73\u53f0\u80fd\u591f\u514b\u670d\u4f20\u7edf\u6d3b\u68c0\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.01180", "pdf": "https://arxiv.org/pdf/2509.01180", "abs": "https://arxiv.org/abs/2509.01180", "authors": ["Fabian Kruse", "Vinith Kishore", "Valentin Debarnot", "Ivan Dokmani\u0107"], "title": "Beyond Exhaustive Sampling: Efficient Rotational Matching via Ball Harmonics", "categories": ["eess.SP", "q-bio.QM"], "comment": "Presented in ISCS25", "summary": "Cryo-ET allows to generate tomograms of biological samples in situ, capturing\ncomplex structures in their native context. Despite low signal-to-noise ratio\nin reconstructed volumes, the large number of copies of the same macromolecules\nmakes it possible to retrieve high-resolution maps by averaging many aligned\nsubtomograms. To keep up with technical advances in the imaging process and the\nresulting huge amounts of data available, there is a need for scalable, fast\nand robust procedures to align subtomograms. We propose a subtomogram alignment\nframework based on the ball harmonics expansion that combines frequency- and\ngradient-based optimization strategies to avoid exhaustive rotation sampling,\nenabling a speed-up of an order of magnitude compared to current approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u8c10\u5c55\u5f00\u7684\u5b50\u65ad\u5c42\u5bf9\u9f50\u6846\u67b6\uff0c\u7ed3\u5408\u9891\u7387\u548c\u68af\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u901f\u5ea6\u3002", "motivation": "\u5e94\u5bf9\u51b7\u51bb\u7535\u5b50\u65ad\u5c42\u626b\u63cf\u6280\u672f\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u7684\u5feb\u901f\u5bf9\u9f50\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7403\u8c10\u5c55\u5f00\uff0c\u7ed3\u5408\u9891\u7387\u548c\u68af\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u51cf\u5c11\u65cb\u8f6c\u91c7\u6837\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u5feb\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b50\u65ad\u5c42\u5bf9\u9f50\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00564", "pdf": "https://arxiv.org/pdf/2509.00564", "abs": "https://arxiv.org/abs/2509.00564", "authors": ["Philip Lorimer", "Jack Saunders", "Alan Hunter", "Wenbin Li"], "title": "Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Authors' accepted manuscript (IROS 2024, Abu Dhabi, Oct 14-18, 2024).\n  Please cite the version of record: DOI 10.1109/IROS58592.2024.10802717. 8\n  pages", "summary": "Free-roaming dollies enhance filmmaking with dynamic movement, but challenges\nin automated camera control remain unresolved. Our study advances this field by\napplying Reinforcement Learning (RL) to automate dolly-in shots using\nfree-roaming ground-based filming robots, overcoming traditional control\nhurdles. We demonstrate the effectiveness of combined control for precise film\ntasks by comparing it to independent control strategies. Our robust RL pipeline\nsurpasses traditional Proportional-Derivative controller performance in\nsimulation and proves its efficacy in real-world tests on a modified ROSBot 2.0\nplatform equipped with a camera turret. This validates our approach's\npracticality and sets the stage for further research in complex filming\nscenarios, contributing significantly to the fusion of technology with\ncinematic creativity. This work presents a leap forward in the field and opens\nnew avenues for research and development, effectively bridging the gap between\ntechnological advancement and creative filmmaking.", "AI": {"tldr": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u81ea\u52a8\u5316\u81ea\u7531\u79fb\u52a8\u6444\u5f71\u673a\u5668\u4eba\u7684\u63a8\u8f68\u955c\u5934\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u7531\u79fb\u52a8\u7684\u6444\u5f71\u63a8\u8f66\u867d\u80fd\u589e\u5f3a\u7535\u5f71\u52a8\u6001\u6548\u679c\uff0c\u4f46\u81ea\u52a8\u5316\u63a7\u5236\u4ecd\u5177\u6311\u6218\u6027\u3002\u6b64\u7814\u7a76\u65e8\u5728\u901a\u8fc7RL\u6280\u672f\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u63d0\u5347\u7535\u5f71\u5236\u4f5c\u7684\u6548\u7387\u4e0e\u7cbe\u786e\u6027\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u8054\u5408\u63a7\u5236\u7b56\u7565\uff0c\u4e0e\u4f20\u7edf\u72ec\u7acb\u63a7\u5236\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u5728\u6539\u88c5ROSBot 2.0\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9645\u6d4b\u8bd5\u3002", "result": "RL\u7ba1\u9053\u5728\u4eff\u771f\u4e2d\u8d85\u8d8a\u4f20\u7edfPD\u63a7\u5236\u5668\uff0c\u5e76\u5728\u771f\u5b9e\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6280\u672f\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u590d\u6742\u62cd\u6444\u573a\u666f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u6280\u672f\u4e0e\u7535\u5f71\u521b\u4f5c\u7684\u878d\u5408\uff0c\u5177\u6709\u91cd\u8981\u7814\u7a76\u4e0e\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.01197", "pdf": "https://arxiv.org/pdf/2509.01197", "abs": "https://arxiv.org/abs/2509.01197", "authors": ["Shugong Xu", "Jun Jiang", "Wenjun Yu", "Yilin Gao", "Guangjin Pan", "Shiyi Mu", "Zhiqi Ai", "Yuan Gao", "Peigang Jiang", "Cheng-Xiang Wang"], "title": "Enhanced Fingerprint-based Positioning With Practical Imperfections: Deep learning-based approaches", "categories": ["eess.SP"], "comment": "accepted by IEEE Wireless Communications Magazine", "summary": "High-precision positioning is vital for cellular networks to support\ninnovative applications such as extended reality, unmanned aerial vehicles\n(UAVs), and industrial Internet of Things (IoT) systems. Existing positioning\nalgorithms using deep learning techniques require vast amounts of labeled data,\nwhich are difficult to obtain in real-world cellular environments, and these\nmodels often struggle to generalize effectively. To advance cellular\npositioning techniques, the 2024 Wireless Communication Algorithm Elite\nCompetition as conducted, which provided a dataset from a three-sector outdoor\ncellular system, incorporating practical challenges such as limited\nlabeled-dataset, dynamic wireless environments within the target and\nunevenly-spaced anchors, Our team developed three innovative positioning\nframeworks that swept the top three awards of this competition, namely the\nsemi-supervised framework with consistency, ensemble learning-based algorithm\nand decoupled mapping heads-based algorithm. Specifically, the semi-supervised\nframework with consistency effectively generates high-quality pseudo-labels,\nenlarging the labeled-dataset for model training. The ensemble learning-based\nalgorithm amalgamates the positioning coordinates from models trained under\ndifferent strategies, effectively combating the dynamic positioning\nenvironments. The decoupled mapping heads-based algorithm utilized sector\nrotation scheme to resolve the uneven-spaced anchor issue. Simulation results\ndemonstrate the superior performance of our proposed positioning algorithms\ncompared to existing benchmarks in terms of the {90%, 80%, 67%, 50%} percentile\nand mean distance error.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.00570", "pdf": "https://arxiv.org/pdf/2509.00570", "abs": "https://arxiv.org/abs/2509.00570", "authors": ["Alessandro Leanza", "Angelo Moroncelli", "Giuseppe Vizzari", "Francesco Braghin", "Loris Roveda", "Blerina Spahiu"], "title": "ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph", "categories": ["cs.RO"], "comment": null, "summary": "ConceptBot is a modular robotic planning framework that combines Large\nLanguage Models and Knowledge Graphs to generate feasible and risk-aware plans\ndespite ambiguities in natural language instructions and correctly analyzing\nthe objects present in the environment - challenges that typically arise from a\nlack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object\nProperty Extraction (OPE) module that enriches scene understanding with\nsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) module\nthat disambiguates and structures instructions, and (iii) a Planner that\ngenerates context-aware, feasible pick-and-place policies. In comparative\nevaluations against Google SayCan, ConceptBot achieved 100% success on explicit\ntasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),\nreached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in\napplication-specific scenarios, including material classification (70% vs. 20%)\nand toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an\noverall score of 80% (versus 46% for the next-best baseline). These results,\nvalidated in both simulation and laboratory experiments, demonstrate\nConceptBot's ability to generalize without domain-specific training and to\nsignificantly improve the reliability of robotic policies in unstructured\nenvironments. Website: https://sites.google.com/view/conceptbot", "AI": {"tldr": "ConceptBot\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u751f\u6210\u53ef\u884c\u4e14\u98ce\u9669\u611f\u77e5\u7684\u8ba1\u5212\uff0c\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6a21\u7cca\u6027\u548c\u73af\u5883\u5bf9\u8c61\u5206\u6790\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u56e0\u7f3a\u4e4f\u5e38\u8bc6\u63a8\u7406\u800c\u5bfc\u81f4\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6a21\u7cca\u548c\u73af\u5883\u5bf9\u8c61\u5206\u6790\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u6a21\u5757\uff1a\u5bf9\u8c61\u5c5e\u6027\u63d0\u53d6\uff08OPE\uff09\u589e\u5f3a\u573a\u666f\u7406\u89e3\uff0c\u7528\u6237\u8bf7\u6c42\u5904\u7406\uff08URP\uff09\u89e3\u6790\u6307\u4ee4\uff0c\u4ee5\u53ca\u89c4\u5212\u5668\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u62fe\u653e\u7b56\u7565\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u663e\u5f0f\u4efb\u52a1100%\u6210\u529f\uff0c\u9690\u5f0f\u4efb\u52a187%\u51c6\u786e\u7387\uff0c\u98ce\u9669\u611f\u77e5\u4efb\u52a176%\u51c6\u786e\u7387\uff0c\u8fdc\u8d85Google SayCan\u3002", "conclusion": "ConceptBot\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7b56\u7565\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.01208", "pdf": "https://arxiv.org/pdf/2509.01208", "abs": "https://arxiv.org/abs/2509.01208", "authors": ["Niclas F\u00fchrling", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "David Gonz\u00e1lez G.", "Gonzalo Seco-Granados", "Osvaldo Gonsa"], "title": "Rigid Body Localization and Tracking for 6G V2X: Algorithms, Applications, and Road to Adoption", "categories": ["eess.SP"], "comment": null, "summary": "Vehicle-to-everything (V2X) perception refers to a suite of technologies that\nempower vehicles to sense their environment and communicate with other\nentities, including surrounding vehicles, infrastructure, and cloud/edge\nnetworks. With the growing demands of autonomous driving, V2X perception has\ngained significant attention, particularly through the emergence of integrated\nsensing and communication (ISAC) frameworks. Within this landscape, rigid body\nlocalization (RBL) has emerged as a promising paradigm, enabling the estimation\nof not only the position and velocity of the targets, but also its\nthree-dimensional (3D) geometric structure and orientation. This article\nintroduces the concept of RBL, highlights its unique advantages and\napplications, identifies key technical challenges, and finally outlines future\nresearch directions. In addition, the potential of RBL in next-generation -\ne.g. beyond fifth generation (B5G) and sixth-generation (6G) - wireless systems\napplied to V2X perception is also discussed, with a focus on its role in\nstandardization efforts and its relevance across automotive and industrial\ndomains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u8f66\u8054\u7f51(V2X)\u611f\u77e5\u4e2d\u7684\u521a\u6027\u4f53\u5b9a\u4f4d(RBL)\u6280\u672f\uff0c\u5f3a\u8c03\u5176\u5728\u76ee\u6807\u4f4d\u7f6e\u3001\u901f\u5ea6\u30013D\u51e0\u4f55\u7ed3\u6784\u548c\u65b9\u5411\u4f30\u8ba1\u4e2d\u7684\u4f18\u52bf\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u5c55\u671b\u4e86RBL\u5728B5G\u548c6G\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u9700\u6c42\u7684\u589e\u957f\uff0c\u8f66\u8054\u7f51(V2X)\u611f\u77e5\u6280\u672f\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u6846\u67b6\u7684\u5174\u8d77\u3002RBL\u56e0\u5176\u80fd\u591f\u63d0\u4f9b\u76ee\u6807\u7684\u5168\u9762\u72b6\u6001\u4fe1\u606f\u800c\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u6587\u7ae0\u6982\u8ff0\u4e86RBL\u7684\u6982\u5ff5\uff0c\u63a2\u8ba8\u4e86\u5176\u6280\u672f\u6311\u6218\u548c\u5e94\u7528\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "RBL\u5728V2X\u611f\u77e5\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf(B5G\u548c6G)\u4e2d\uff0c\u5bf9\u6807\u51c6\u5316\u548c\u8de8\u9886\u57df\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "RBL\u662fV2X\u611f\u77e5\u9886\u57df\u7684\u91cd\u8981\u6280\u672f\uff0c\u672a\u6765\u5e94\u5728\u6280\u672f\u7a81\u7834\u548c\u6807\u51c6\u5316\u65b9\u9762\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u63a8\u52a8\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5de5\u4e1a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.00571", "pdf": "https://arxiv.org/pdf/2509.00571", "abs": "https://arxiv.org/abs/2509.00571", "authors": ["Arman Javan Sekhavat Pishkhani"], "title": "Gray-Box Computed Torque Control for Differential-Drive Mobile Robot Tracking", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This study presents a learning-based nonlinear algorithm for tracking control\nof differential-drive mobile robots. The Computed Torque Method (CTM) suffers\nfrom inaccurate knowledge of system parameters, while Deep Reinforcement\nLearning (DRL) algorithms are known for sample inefficiency and weak stability\nguarantees. The proposed method replaces the black-box policy network of a DRL\nagent with a gray-box Computed Torque Controller (CTC) to improve sample\nefficiency and ensure closed-loop stability. This approach enables finding an\noptimal set of controller parameters for an arbitrary reward function using\nonly a few short learning episodes. The Twin-Delayed Deep Deterministic Policy\nGradient (TD3) algorithm is used for this purpose. Additionally, some\ncontroller parameters are constrained to lie within known value ranges,\nensuring the RL agent learns physically plausible values. A technique is also\napplied to enforce a critically damped closed-loop time response. The\ncontroller's performance is evaluated on a differential-drive mobile robot\nsimulated in the MuJoCo physics engine and compared against the raw CTC and a\nconventional kinematic controller.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408CTC\u548cDRL\u7684\u5b66\u4e60\u578b\u975e\u7ebf\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5dee\u901f\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8ddf\u8e2a\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "CTM\u5bf9\u7cfb\u7edf\u53c2\u6570\u4e0d\u51c6\u786e\u654f\u611f\uff0c\u800cDRL\u7b97\u6cd5\u6837\u672c\u6548\u7387\u4f4e\u4e14\u7a33\u5b9a\u6027\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7528\u7070\u76d2CTC\u66ff\u4ee3DRL\u7684\u9ed1\u76d2\u7b56\u7565\u7f51\u7edc\uff0c\u7ed3\u5408TD3\u7b97\u6cd5\u4f18\u5316\u63a7\u5236\u5668\u53c2\u6570\uff0c\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u548c\u4e34\u754c\u963b\u5c3c\u54cd\u5e94\u3002", "result": "\u5728MuJoCo\u4eff\u771f\u4e2d\uff0c\u8be5\u63a7\u5236\u5668\u8868\u73b0\u4f18\u4e8e\u539f\u59cbCTC\u548c\u4f20\u7edf\u8fd0\u52a8\u63a7\u5236\u5668\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5dee\u901f\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8ddf\u8e2a\u63a7\u5236\u3002"}}
{"id": "2509.01210", "pdf": "https://arxiv.org/pdf/2509.01210", "abs": "https://arxiv.org/abs/2509.01210", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "title": "High-Density MIMO Localization Using a 32x64 Ultrasonic Transducer-Microphone Array with Real-Time Data Streaming", "categories": ["eess.SP", "eess.AS"], "comment": "Accepted for publication at IEEE IUS 2025", "summary": "In this work, we present a novel ultrasonic array system designed for\nhigh-precision localization using a large-scale MIMO (Multiple-Input\nMultiple-Output) architecture. The system combines 32 transmitters with 62\nmicrophones, creating an extended virtual aperture that improves channel\nseparability and spatial resolution. Each transmitter is excited by a\nrandom-phase multisine within the ultrasonic band, which reduces inter-channel\ncorrelation and increases robustness against multipath. The feasibility of the\napproach is demonstrated through simulations of reflector imaging and analysis\nof channel separation under realistic transducer bandwidth constraints. Results\nshow that MIMO processing enables improved separation of reflectors compared to\nsingle-emitter configurations, although practical limitations such as\ntransducer bandwidth reduce the achievable channel isolation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21MIMO\u67b6\u6784\u7684\u65b0\u578b\u8d85\u58f0\u9635\u5217\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\uff0c\u901a\u8fc7\u7ed3\u540832\u4e2a\u53d1\u5c04\u5668\u548c62\u4e2a\u9ea6\u514b\u98ce\uff0c\u6269\u5c55\u865a\u62df\u5b54\u5f84\u4ee5\u63d0\u9ad8\u901a\u9053\u5206\u79bb\u6027\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u8d85\u58f0\u9635\u5217\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u4e2d\u901a\u9053\u5206\u79bb\u6027\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u5927\u89c4\u6a21MIMO\u67b6\u6784\uff0c\u4f7f\u752832\u4e2a\u53d1\u5c04\u5668\u548c62\u4e2a\u9ea6\u514b\u98ce\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u76f8\u4f4d\u591a\u9891\u4fe1\u53f7\u6fc0\u53d1\u53d1\u5c04\u5668\uff0c\u51cf\u5c11\u901a\u9053\u95f4\u76f8\u5173\u6027\u5e76\u589e\u5f3a\u591a\u8def\u5f84\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cMIMO\u5904\u7406\u76f8\u6bd4\u5355\u53d1\u5c04\u5668\u914d\u7f6e\u80fd\u66f4\u597d\u5730\u5206\u79bb\u53cd\u5c04\u5668\uff0c\u4f46\u53d7\u9650\u4e8e\u53d1\u5c04\u5668\u5e26\u5bbd\u7b49\u56e0\u7d20\uff0c\u901a\u9053\u9694\u79bb\u6548\u679c\u4ecd\u6709\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7MIMO\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u8d85\u58f0\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u89e3\u51b3\u53d1\u5c04\u5668\u5e26\u5bbd\u7b49\u9650\u5236\u3002"}}
{"id": "2509.00574", "pdf": "https://arxiv.org/pdf/2509.00574", "abs": "https://arxiv.org/abs/2509.00574", "authors": ["Philip Lorimer", "Alan Hunter", "Wenbin Li"], "title": "Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot", "categories": ["cs.RO", "cs.LG"], "comment": "Preprint; under double-anonymous review. 6 pages", "summary": "Cinematic camera control demands a balance of precision and artistry -\nqualities that are difficult to encode through handcrafted reward functions.\nWhile reinforcement learning (RL) has been applied to robotic filmmaking, its\nreliance on bespoke rewards and extensive tuning limits creative usability. We\npropose a Learning from Demonstration (LfD) approach using Generative\nAdversarial Imitation Learning (GAIL) to automate dolly-in shots with a\nfree-roaming, ground-based filming robot. Expert trajectories are collected via\njoystick teleoperation in simulation, capturing smooth, expressive motion\nwithout explicit objective design.\n  Trained exclusively on these demonstrations, our GAIL policy outperforms a\nPPO baseline in simulation, achieving higher rewards, faster convergence, and\nlower variance. Crucially, it transfers directly to a real-world robot without\nfine-tuning, achieving more consistent framing and subject alignment than a\nprior TD3-based method. These results show that LfD offers a robust,\nreward-free alternative to RL in cinematic domains, enabling real-time\ndeployment with minimal technical effort. Our pipeline brings intuitive,\nstylized camera control within reach of creative professionals, bridging the\ngap between artistic intent and robotic autonomy.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u793a\u8303\uff08LfD\uff09\u548c\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff08GAIL\uff09\u6765\u5b9e\u73b0\u673a\u5668\u4eba\u7535\u5f71\u62cd\u6444\u4e2d\u66f4\u81ea\u7136\u7684\u76f8\u673a\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u51fd\u6570\u96be\u4ee5\u8bbe\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u7535\u5f71\u6444\u5f71\u9700\u8981\u5e73\u8861\u7cbe\u786e\u6027\u548c\u827a\u672f\u6027\uff0c\u4f46\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u96be\u4ee5\u6355\u6349\u8fd9\u79cd\u590d\u6742\u7684\u5e73\u8861\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u5236\u5956\u52b1\u548c\u5927\u91cf\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u5176\u521b\u610f\u5b9e\u7528\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAIL\u7684LfD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u4e0b\u7684\u4e13\u5bb6\u6f14\u793a\uff08\u4f7f\u7528\u64cd\u7eb5\u6746\u63a7\u5236\uff09\u6765\u8bad\u7ec3\u673a\u5668\u4eba\uff0c\u4ece\u800c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u7684\u81ea\u52a8\u5316\u76f8\u673a\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cGAIL\u7b56\u7565\u4f18\u4e8e\u57fa\u7ebfPPO\uff0c\u5956\u52b1\u66f4\u9ad8\u3001\u6536\u655b\u66f4\u5feb\u3001\u65b9\u5dee\u66f4\u4f4e\uff0c\u5e76\u4e14\u65e0\u9700\u8c03\u4f18\u5373\u53ef\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4e0a\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "LfD\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u7535\u5f71\u884c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u673a\u5668\u4eba\u76f8\u673a\u63a7\u5236\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u6280\u672f\u969c\u788d\u3002"}}
{"id": "2509.01212", "pdf": "https://arxiv.org/pdf/2509.01212", "abs": "https://arxiv.org/abs/2509.01212", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "title": "nRTIS: Low-Cost Real-Time 3D Sonar Imaging Circular Array Supporting Beamforming for Industrial Applications", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": "Accepted for publication at IEEE IUS 2025", "summary": "Conventional ultrasonic inspection systems rely on phased arrays and\nhigh-performance computing hardware, making them costly, bulky, and unsuitable\nfor portable or embedded use. In this work, we present nRTIS (nano Real-Time 3D\nImaging Sonar), a compact ultrasonic sensing platform built around a circular\narray of MEMS microphones and a central ultrasonic transducer. The device\nachieves real-time acquisition through an RP2350 microcontroller and high-speed\nUSB transfer. We validate the system using both simulations and controlled\nexperiments: point spread function (PSF) simulations demonstrate beamforming\nresolution and sidelobe suppression, while reflector measurements confirm\nrobust data acquisition. These results highlight the potential of nRTIS for\nscalable industrial applications such as weld inspection, pipe mapping, and\nrobotic navigation.", "AI": {"tldr": "nRTIS\u662f\u4e00\u79cd\u7d27\u51d1\u578b\u8d85\u58f0\u6ce2\u4f20\u611f\u5e73\u53f0\uff0c\u5229\u7528MEMS\u9ea6\u514b\u98ce\u9635\u5217\u548c\u5355\u7247\u673a\u5b9e\u73b0\u5b9e\u65f63D\u6210\u50cf\uff0c\u9002\u7528\u4e8e\u4fbf\u643a\u5f0f\u5de5\u4e1a\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u8d85\u58f0\u6ce2\u68c0\u6d4b\u7cfb\u7edf\u6210\u672c\u9ad8\u3001\u4f53\u79ef\u5927\u4e14\u4e0d\u4fbf\u4e8e\u643a\u5e26\uff0cnRTIS\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7d27\u51d1\u4e14\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eMEMS\u9ea6\u514b\u98ce\u9635\u5217\u548cRP2350\u5355\u7247\u673a\uff0c\u901a\u8fc7\u9ad8\u901fUSB\u4f20\u8f93\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86nRTIS\u7684\u5206\u8fa8\u7387\u548c\u6570\u636e\u91c7\u96c6\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "nRTIS\u4e3a\u4fbf\u643a\u5f0f\u8d85\u58f0\u6ce2\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2509.00576", "pdf": "https://arxiv.org/pdf/2509.00576", "abs": "https://arxiv.org/abs/2509.00576", "authors": ["Tao Jiang", "Tianyuan Yuan", "Yicheng Liu", "Chenhao Lu", "Jianning Cui", "Xiao Liu", "Shuiqi Cheng", "Jiyang Gao", "Huazhe Xu", "Hang Zhao"], "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model", "categories": ["cs.RO", "cs.CV"], "comment": "https://opengalaxea.github.io/G0/", "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Galaxea\u5f00\u653e\u4e16\u754c\u6570\u636e\u96c6\u548cG0\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u5728\u591a\u4efb\u52a1\u673a\u5668\u4eba\u884c\u4e3a\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u652f\u6301\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "method": "\u63d0\u51faG0\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u7ed3\u5408VLM\u548cVLA\u6a21\u578b\uff0c\u5206\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a\u8de8\u8f7d\u4f53\u9884\u8bad\u7ec3\u3001\u5355\u8f7d\u4f53\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "result": "Galaxea\u6570\u636e\u96c6\u548c\u5355\u8f7d\u4f53\u9884\u8bad\u7ec3\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Galaxea\u6570\u636e\u96c6\u4e0eG0\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01222", "pdf": "https://arxiv.org/pdf/2509.01222", "abs": "https://arxiv.org/abs/2509.01222", "authors": ["Tong Lin", "Jianyue Zhu", "Wei Huang", "Meng Hua", "Zhizhong Zhang"], "title": "Rate Optimization for Downlink URLLC via Pinching Antenna Arrays", "categories": ["eess.SP"], "comment": null, "summary": "This work studies an ultra-reliable and low-latency communications (uRLLC)\ndownlink system using pinching antennas which are realized by activating small\ndielectric particles along a dielectric waveguide. Our goal is to maximize the\ndata rate by optimizing the positions of the pinching antennas. By proposing a\ncompact and cost-efficient antenna architecture and formulating a finite\nblocklength-based optimization model, we derive a closed-form solution for the\noptimal antenna placement under quality-of-service (QoS) and antenna spacing\nconstraints. Meanwhile, a phase-alignment strategy is integrated into the\ndesign, enabling coherent signal superposition across the array. Simulation\nresults confirm significant rate improvements over conventional antenna systems\nwhile satisfying uRLLC requirements, making the proposed design well-suited for\ncompact and latency-critical future applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u7528\u4e8euRLLC\u4e0b\u884c\u94fe\u8def\u7cfb\u7edf\u7684\u5939\u6301\u5929\u7ebf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u76f8\u4f4d\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u7684\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u5929\u7ebf\u7cfb\u7edf\u5728uRLLC\uff08\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff09\u573a\u666f\u4e2d\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u9ad8\u6027\u80fd\u548c\u7d27\u51d1\u6027\u9700\u6c42\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5939\u6301\u5929\u7ebf\u7684\u65b0\u8bbe\u8ba1\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5c0f\u578b\u4ecb\u7535\u7c92\u5b50\u6cbf\u4ecb\u8d28\u6ce2\u5bfc\u6fc0\u6d3b\u5b9e\u73b0\u5939\u6301\u5929\u7ebf\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5757\u957f\u4f18\u5316\u6a21\u578b\u548c\u76f8\u4f4d\u5bf9\u9f50\u7b56\u7565\uff0c\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u4ee5\u5b9e\u73b0\u76f8\u5e72\u4fe1\u53f7\u53e0\u52a0\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u8bbe\u8ba1\u5728\u6ee1\u8db3uRLLC\u8981\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u901f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u5929\u7ebf\u7cfb\u7edf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5939\u6301\u5929\u7ebf\u8bbe\u8ba1\u4e3a\u672a\u6765\u7d27\u51d1\u4e14\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u901a\u4fe1\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00582", "pdf": "https://arxiv.org/pdf/2509.00582", "abs": "https://arxiv.org/abs/2509.00582", "authors": ["Rui Bai", "Rui Xu", "Teng Rui", "Jiale Liu", "Qi Wei Oung", "Hoi Leong Lee", "Zhen Tian", "Fujiang Yuan"], "title": "Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved Double Quintic Polynomial Approach with Time-to-Collision Evaluation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Autonomous driving technology has made significant advancements in recent\nyears, yet challenges remain in ensuring safe and comfortable interactions with\nhuman-driven vehicles (HDVs), particularly during lane-changing maneuvers. This\npaper proposes an improved double quintic polynomial approach for safe and\nefficient lane-changing in mixed traffic environments. The proposed method\nintegrates a time-to-collision (TTC) based evaluation mechanism directly into\nthe trajectory optimization process, ensuring that the ego vehicle proactively\nmaintains a safe gap from surrounding HDVs throughout the maneuver. The\nframework comprises state estimation for both the autonomous vehicle (AV) and\nHDVs, trajectory generation using double quintic polynomials, real-time TTC\ncomputation, and adaptive trajectory evaluation. To the best of our knowledge,\nthis is the first work to embed an analytic TTC penalty directly into the\nclosed-form double-quintic polynomial solver, enabling real-time safety-aware\ntrajectory generation without post-hoc validation. Extensive simulations\nconducted under diverse traffic scenarios demonstrate the safety, efficiency,\nand comfort of the proposed approach compared to conventional methods such as\nquintic polynomials, Bezier curves, and B-splines. The results highlight that\nthe improved method not only avoids collisions but also ensures smooth\ntransitions and adaptive decision-making in dynamic environments. This work\nbridges the gap between model-based and adaptive trajectory planning\napproaches, offering a stable solution for real-world autonomous driving\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u53d8\u9053\uff0c\u901a\u8fc7\u6574\u5408\u5b9e\u65f6TTC\u8bc4\u4f30\u673a\u5236\uff0c\u786e\u4fdd\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u5730\u884c\u9a76\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5728\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HDV\uff09\u7684\u4ea4\u4e92\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u53d8\u9053\u8fc7\u7a0b\u4e2d\u4ecd\u5b58\u5728\u5b89\u5168\u4e0e\u8212\u9002\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u4e94\u6b21\u591a\u9879\u5f0f\u65b9\u6cd5\uff0c\u7ed3\u5408TTC\u8bc4\u4f30\u673a\u5236\uff0c\u76f4\u63a5\u5d4c\u5165\u8f68\u8ff9\u4f18\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u611f\u77e5\u7684\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u4e94\u6b21\u591a\u9879\u5f0f\u3001Bezier\u66f2\u7ebf\u548cB-\u6837\u6761\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u5b89\u5168\u7684\u6df7\u5408\u4ea4\u901a\u8f68\u8ff9\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01223", "pdf": "https://arxiv.org/pdf/2509.01223", "abs": "https://arxiv.org/abs/2509.01223", "authors": ["Niclas F\u00fchrling", "Giuseppe Abreu", "David Gonz\u00e1lez G.", "Osvaldo Gonsa"], "title": "SMDS-based Rigid Body Localization", "categories": ["eess.SP"], "comment": null, "summary": "We consider a novel rigid body localization (RBL) method, based only on a set\nof measurements of the distances, as well as the angles between sensors of the\nvehicle to the anchor landmark points. A key point of the proposed method is to\nuse a variation of the super multidimensional scaling (SMDS) algorithm, where\nonly a minor part of the complex edge kernel is used, based on the available\ninformation, which in the case of RBL is anchor-to-anchor and target-to-target\ninformation. Simulation results illustrate the good performance of the proposed\ntechnique in terms of mean square error (MSE) of the estimates, compared also\nto the corresponding Cram\\'er-Rao Lower Bound (CRLB).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u548c\u89d2\u5ea6\u6d4b\u91cf\u503c\u7684\u521a\u6027\u4f53\u5b9a\u4f4d\uff08RBL\uff09\u65b0\u65b9\u6cd5\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u8d85\u591a\u7ef4\u5c3a\u5ea6\uff08SMDS\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u590d\u6742\u8fb9\u7f18\u6838\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ec5\u57fa\u4e8e\u8ddd\u79bb\u548c\u89d2\u5ea6\u6d4b\u91cf\u7684\u521a\u6027\u4f53\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u8d85\u591a\u7ef4\u5c3a\u5ea6\uff08SMDS\uff09\u7b97\u6cd5\uff0c\u4ec5\u5229\u7528\u53ef\u7528\u4fe1\u606f\u4e2d\u7684\u90e8\u5206\u590d\u6742\u8fb9\u7f18\u6838\uff08\u5982\u951a\u70b9\u5230\u951a\u70b9\u548c\u76ee\u6807\u70b9\u5230\u76ee\u6807\u70b9\u7684\u4fe1\u606f\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4e14\u63a5\u8fd1Cram\u00e9r-Rao\u4e0b\u754c\uff08CRLB\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684RBL\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u6709\u9650\u4fe1\u606f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002"}}
{"id": "2509.00624", "pdf": "https://arxiv.org/pdf/2509.00624", "abs": "https://arxiv.org/abs/2509.00624", "authors": ["Haochong Chen", "Xincheng Cao", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Extensive research has already been conducted in the autonomous driving field\nto help vehicles navigate safely and efficiently. At the same time, plenty of\ncurrent research on vulnerable road user (VRU) safety is performed which\nlargely concentrates on perception, localization, or trajectory prediction of\nVRUs. However, existing research still exhibits several gaps, including the\nlack of a unified planning and collision avoidance system for autonomous\nvehicles, limited investigation into delay tolerant control strategies, and the\nabsence of an efficient and standardized testing methodology. Ensuring VRU\nsafety remains one of the most pressing challenges in autonomous driving,\nparticularly in dynamic and unpredictable environments. In this two year\nproject, we focused on applying the Vehicle in Virtual Environment (VVE) method\nto develop, evaluate, and demonstrate safety functions for Vulnerable Road\nUsers (VRUs) using automated steering and braking of ADS. In this current\nsecond year project report, our primary focus was on enhancing the previous\nyear results while also considering bicyclist safety.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2dVRU\uff08\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff09\u5b89\u5168\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u4ecb\u7ecd\u4e86\u91c7\u7528VVE\u65b9\u6cd5\u5f00\u53d1\u7684VRU\u5b89\u5168\u529f\u80fd\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u81ea\u884c\u8f66\u5b89\u5168\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u89c4\u5212\u548c\u78b0\u649e\u907f\u514d\u7cfb\u7edf\u3001\u5ef6\u8fdf\u5bb9\u5fcd\u63a7\u5236\u7b56\u7565\u53ca\u6807\u51c6\u5316\u6d4b\u8bd5\u65b9\u6cd5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cVRU\u5b89\u5168\u4ecd\u662f\u7d27\u8feb\u6311\u6218\u3002", "method": "\u91c7\u7528VVE\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u52a8\u8f6c\u5411\u548c\u5236\u52a8\u6280\u672f\uff0c\u5f00\u53d1\u548c\u8bc4\u4f30VRU\u7684\u5b89\u5168\u529f\u80fd\u3002", "result": "\u9879\u76ee\u5728\u7b2c\u4e00\u5e74\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u7279\u522b\u5173\u6ce8\u81ea\u884c\u8f66\u5b89\u5168\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86VVE\u65b9\u6cd5\u5728\u63d0\u5347VRU\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.01331", "pdf": "https://arxiv.org/pdf/2509.01331", "abs": "https://arxiv.org/abs/2509.01331", "authors": ["Koshi Nagahisa", "Ryo Hayakawa", "Youji Iiguni"], "title": "Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery", "categories": ["eess.SP"], "comment": "This work will be submitted to the IEEE for possible publication", "summary": "This paper investigates the impact of loss function selection in deep\nunfolding techniques for sparse signal recovery algorithms. Deep unfolding\ntransforms iterative optimization algorithms into trainable lightweight neural\nnetworks by unfolding their iterations as network layers, with various loss\nfunctions employed for parameter learning depending on application contexts. We\nfocus on deep unfolded versions of the fundamental iterative shrinkage\nthresholding algorithm (ISTA) and the iterative hard thresholding algorithm\n(IHT), comparing supervised learning using mean squared error with unsupervised\nlearning using the objective function of the original optimization problem. Our\nsimulation results reveal that the effect of the choice of loss function\nsignificantly depends on the convexity of the optimization problem. For convex\n$\\ell_1$-regularized problems, supervised-ISTA achieves better final recovery\naccuracy but fails to minimize the original objective function, whereas we\nempirically observe that unsupervised-ISTA converges to a nearly identical\nsolution as conventional ISTA but with accelerated convergence. Conversely, for\nnonconvex $\\ell_0$-regularized problems, both supervised-IHT and\nunsupervised-IHT converge to better local minima than the original IHT, showing\nsimilar performance regardless of the loss function employed. These findings\nprovide valuable insights into the design of effective deep unfolded networks\nfor sparse signal recovery applications.", "AI": {"tldr": "\u7814\u7a76\u4e86\u635f\u5931\u51fd\u6570\u9009\u62e9\u5728\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u7b97\u6cd5\u4e2d\u7684\u6df1\u5ea6\u5c55\u5f00\u6280\u672f\u4e2d\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u76d1\u7763\u5b66\u4e60\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u635f\u5931\u51fd\u6570\uff08\u76d1\u7763\u548c\u65e0\u76d1\u7763\uff09\u5bf9\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u5c55\u5f00\u7684ISTA\u548cIHT\u7b97\u6cd5\uff0c\u6bd4\u8f83\u76d1\u7763\u5b66\u4e60\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u539f\u59cb\u4f18\u5316\u76ee\u6807\u51fd\u6570\u3002", "result": "\u635f\u5931\u51fd\u6570\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u95ee\u9898\u7684\u51f8\u6027\uff1a\u51f8\u95ee\u9898\u4e2d\u76d1\u7763\u5b66\u4e60\u66f4\u51c6\u4f46\u65e0\u6cd5\u6700\u5c0f\u5316\u539f\u59cb\u76ee\u6807\uff0c\u975e\u51f8\u95ee\u9898\u4e2d\u4e24\u79cd\u5b66\u4e60\u8868\u73b0\u76f8\u4f3c\u4e14\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u5e94\u7528\u4e2d\u3002"}}
{"id": "2509.00643", "pdf": "https://arxiv.org/pdf/2509.00643", "abs": "https://arxiv.org/abs/2509.00643", "authors": ["Zhen Tian", "Zhihao Lin", "Dezong Zhao", "Christos Anagnostopoulos", "Qiyuan Wang", "Wenjing Zhao", "Xiaodan Wang", "Chongfeng Wei"], "title": "A Risk-aware Spatial-temporal Trajectory Planning Framework for Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Trajectory planning is a critical component in ensuring the safety,\nstability, and efficiency of autonomous vehicles. While existing trajectory\nplanning methods have achieved progress, they often suffer from high\ncomputational costs, unstable performance in dynamic environments, and limited\nvalidation across diverse scenarios. To overcome these challenges, we propose\nan enhanced QP-MPC-based framework that incorporates three key innovations: (i)\na novel cost function designed with a dynamic hazard field, which explicitly\nbalances safety, efficiency, and comfort; (ii) seamless integration of this\ncost function into the QP-MPC formulation, enabling direct optimization of\ndesired driving behaviors; and (iii) extensive validation of the proposed\nframework across complex tasks. The spatial safe planning is guided by a\ndynamic hazard field (DHF) for risk assessment, while temporal safe planning is\nbased on a space-time graph. Besides, the quintic polynomial sampling and\nsub-reward of comforts are used to ensure comforts during lane-changing. The\nsub-reward of efficiency is used to maintain driving efficiency. Finally, the\nproposed DHF-enhanced objective function integrates multiple objectives,\nproviding a proper optimization tasks for QP-MPC. Extensive simulations\ndemonstrate that the proposed framework outperforms benchmark optimization\nmethods in terms of efficiency, stability, and comfort across a variety of\nscenarios likes lane-changing, overtaking, and crossing intersections.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQP-MPC\u7684\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5371\u9669\u573a\uff08DHF\uff09\u548c\u65f6\u7a7a\u56fe\u7b49\u6280\u672f\uff0c\u9ad8\u6548\u5e73\u8861\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u53ca\u591a\u6837\u5316\u573a\u666f\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684QP-MPC\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u6001\u5371\u9669\u573a\u8bbe\u8ba1\u7684\u6210\u672c\u51fd\u6570\u3001QP-MPC\u65e0\u7f1d\u96c6\u6210\u4ee5\u53ca\u591a\u6837\u5316\u573a\u666f\u9a8c\u8bc1\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u8212\u9002\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u548c\u8212\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01410", "pdf": "https://arxiv.org/pdf/2509.01410", "abs": "https://arxiv.org/abs/2509.01410", "authors": ["Debraj Banerjee", "Amitava Chatterjee"], "title": "A James-Stein Estimator based Generalized OMP Algorithm for Robust Signal Recovery using Sparse Representation", "categories": ["eess.SP", "math.ST", "stat.TH", "94A12, 41A45, 94A20"], "comment": "5 pages, 3 figures, conference paper", "summary": "In this paper, we introduce a novel algorithm named JS-gOMP, which enhances\nthe generalized Orthogonal Matching Pursuit (gOMP) algorithm for improved noise\nrobustness in sparse signal processing. The JS-gOMP algorithm uniquely\nincorporates the James-Stein estimator, optimizing the trade-off between signal\nrecovery and noise suppression. This modification addresses the challenges\nposed by noise in the dictionary, a common issue in sparse representation\nscenarios. Comparative analyses demonstrate that JS-gOMP outperforms\ntraditional gOMP, especially in noisy environments, offering a more effective\nsolution for signal and image processing applications where noise presence is\nsignificant.", "AI": {"tldr": "JS-gOMP\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165James-Stein\u4f30\u8ba1\u5668\u4f18\u5316\u4e86gOMP\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u7a00\u758f\u4fe1\u53f7\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u7a00\u758f\u8868\u793a\u4e2d\u5e38\u89c1\u7684\u566a\u58f0\u95ee\u9898\uff0c\u6539\u8fdbgOMP\u7b97\u6cd5\u4ee5\u63d0\u5347\u566a\u58f0\u9c81\u68d2\u6027\u3002", "method": "JS-gOMP\u7b97\u6cd5\u7ed3\u5408James-Stein\u4f30\u8ba1\u5668\uff0c\u4f18\u5316\u4fe1\u53f7\u6062\u590d\u4e0e\u566a\u58f0\u6291\u5236\u7684\u6743\u8861\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u663e\u793a\uff0cJS-gOMP\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4f18\u4e8e\u4f20\u7edfgOMP\uff0c\u9002\u7528\u4e8e\u4fe1\u53f7\u548c\u56fe\u50cf\u5904\u7406\u3002", "conclusion": "JS-gOMP\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u7a00\u758f\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.00660", "pdf": "https://arxiv.org/pdf/2509.00660", "abs": "https://arxiv.org/abs/2509.00660", "authors": ["Felipe Arias-Russi", "Yuanchen Bai", "Angelique Taylor"], "title": "CARIS: A Context-Adaptable Robot Interface System for Personalized and Scalable Human-Robot Interaction", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz\n(WoZ) controlled robots to explore navigation, conversational dynamics,\nhuman-in-the-loop interactions, and more to explore appropriate robot behaviors\nin everyday settings. However, existing WoZ tools are often limited to one\ncontext, making them less adaptable across different settings, users, and\nrobotic platforms. To mitigate these issues, we introduce a Context-Adaptable\nRobot Interface System (CARIS) that combines advanced robotic capabilities such\nteleoperation, human perception, human-robot dialogue, and multimodal data\nrecording. Through pilot studies, we demonstrate the potential of CARIS to WoZ\ncontrol a robot in two contexts: 1) mental health companion and as a 2) tour\nguide. Furthermore, we identified areas of improvement for CARIS, including\nsmoother integration between movement and communication, clearer functionality\nseparation, recommended prompts, and one-click communication options to enhance\nthe usability wizard control of CARIS. This project offers a publicly\navailable, context-adaptable tool for the HRI community, enabling researchers\nto streamline data-driven approaches to intelligent robot behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63a5\u53e3\u7cfb\u7edfCARIS\uff0c\u6539\u8fdb\u4f20\u7edfWizard-of-Oz\u5de5\u5177\u7684\u591a\u573a\u666f\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709WoZ\u5de5\u5177\u5c40\u9650\u5355\u4e00\u573a\u666f\u7684\u95ee\u9898\uff0c\u4e3aHRI\u793e\u533a\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u4ea4\u4e92\u7814\u7a76\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u9065\u64cd\u4f5c\u3001\u4eba\u7c7b\u611f\u77e5\u3001\u4eba\u673a\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\u6570\u636e\u8bb0\u5f55\uff0c\u5f00\u53d1CARIS\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5e94\u7528\u573a\u666f\uff08\u5fc3\u7406\u5065\u5eb7\u4f34\u4fa3\u548c\u5bfc\u6e38\uff09\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "CARIS\u5c55\u73b0\u51fa\u591a\u573a\u666f\u9002\u5e94\u6f5c\u529b\uff0c\u4f46\u4e5f\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd0\u52a8\u4e0e\u901a\u4fe1\u96c6\u6210\u3001\u529f\u80fd\u5206\u79bb\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "CARIS\u4e3aHRI\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u516c\u5f00\u53ef\u7528\u7684\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5de5\u5177\uff0c\u652f\u6301\u667a\u80fd\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5feb\u901f\u5f00\u53d1\u3002"}}
{"id": "2509.01506", "pdf": "https://arxiv.org/pdf/2509.01506", "abs": "https://arxiv.org/abs/2509.01506", "authors": ["Marcel Grec", "Federico Clazzer", "Israel Leyva-Mayorga", "Andrea Munari", "Gianluigi Liva", "Petar Popovski"], "title": "To Share, or Not to Share: A Study on GEO-LEO Systems for IoT Services with Random Access", "categories": ["eess.SP"], "comment": "6 pages, 7 figures; accepted to be presented at the 2025 IEEE Global\n  Communications Conference", "summary": "The increasing number of satellite deployments, both in the low and\ngeostationary Earth orbit exacerbates the already ongoing scarcity of wireless\nresources when targeting ubiquitous connectivity. For the aim of supporting a\nmassive number of IoT devices characterized by bursty traffic and modern\nvariants of random access, we pose the following question: Should competing\nsatellite operators share spectrum resources or is an exclusive allocation\npreferable? This question is addressed by devising a communication model for\ntwo operators which serve overlapping coverage areas with independent IoT\nservices. Analytical approximations, validated by Monte Carlo simulations,\nreveal that spectrum sharing can yield significant throughput gains for both\noperators under certain conditions tied to the relative serviced user\npopulations and coding rates in use. These gains are sensitive also to the\nsystem parameters and may not always render the spectral coexistence mutually\nadvantageous. Our model captures basic trade-offs in uplink spectrum sharing\nand provides novel actionable insights for the design and regulation of future\n6G non-terrestrial networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u536b\u661f\u8fd0\u8425\u5546\u662f\u5426\u5e94\u8be5\u5171\u4eab\u9891\u8c31\u8d44\u6e90\u4ee5\u652f\u6301\u5927\u91cf\u7269\u8054\u7f51\u8bbe\u5907\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u4fe1\u6a21\u578b\u5e76\u901a\u8fc7\u5206\u6790\u548c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5171\u4eab\u9891\u8c31\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u536b\u661f\u90e8\u7f72\u6570\u91cf\u7684\u589e\u52a0\uff0c\u65e0\u7ebf\u8d44\u6e90\u65e5\u76ca\u7a00\u7f3a\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u652f\u6301\u5927\u89c4\u6a21\u7269\u8054\u7f51\u8bbe\u5907\u7684\u9891\u8c31\u8d44\u6e90\u5171\u4eab\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u4fe1\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4e24\u5bb6\u8fd0\u8425\u5546\u5728\u91cd\u53e0\u8986\u76d6\u533a\u57df\u72ec\u7acb\u670d\u52a1\u7269\u8054\u7f51\u8bbe\u5907\u7684\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u3002", "result": "\u9891\u8c31\u5171\u4eab\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u5982\u7528\u6237\u4eba\u53e3\u548c\u7f16\u7801\u901f\u7387\u7684\u76f8\u5bf9\u6bd4\u4f8b\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u6548\u679c\u4f9d\u8d56\u4e8e\u7cfb\u7edf\u53c2\u6570\uff0c\u5e76\u975e\u603b\u80fd\u4e92\u60e0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u67656G\u975e\u5730\u9762\u7f51\u7edc\u7684\u9891\u8c31\u5171\u4eab\u8bbe\u8ba1\u548c\u76d1\u7ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u6743\u8861\u8003\u91cf\u3002"}}
{"id": "2509.00741", "pdf": "https://arxiv.org/pdf/2509.00741", "abs": "https://arxiv.org/abs/2509.00741", "authors": ["Yi Liu", "Keyu Fan", "Bin Lan", "Houde Liu"], "title": "DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments", "categories": ["cs.RO"], "comment": "Accepted by ICME 2025(Oral)", "summary": "Visual SLAM algorithms have been enhanced through the exploration of Gaussian\nSplatting representations, particularly in generating high-fidelity dense maps.\nWhile existing methods perform reliably in static environments, they often\nencounter camera tracking drift and fuzzy mapping when dealing with the\ndisturbances caused by moving objects. This paper presents DyPho-SLAM, a\nreal-time, resource-efficient visual SLAM system designed to address the\nchallenges of localization and photorealistic mapping in environments with\ndynamic objects. Specifically, the proposed system integrates prior image\ninformation to generate refined masks, effectively minimizing noise from mask\nmisjudgment. Additionally, to enhance constraints for optimization after\nremoving dynamic obstacles, we devise adaptive feature extraction strategies\nsignificantly improving the system's resilience. Experiments conducted on\npublicly dynamic RGB-D datasets demonstrate that the proposed system achieves\nstate-of-the-art performance in camera pose estimation and dense map\nreconstruction, while operating in real-time in dynamic scenes.", "AI": {"tldr": "DyPho-SLAM \u662f\u4e00\u79cd\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u89c6\u89c9 SLAM \u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u5148\u9a8c\u56fe\u50cf\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u5b9a\u4f4d\u548c\u5efa\u56fe\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9 SLAM \u65b9\u6cd5\u5728\u9759\u6001\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5e38\u51fa\u73b0\u76f8\u673a\u8ddf\u8e2a\u6f02\u79fb\u548c\u6a21\u7cca\u5efa\u56fe\u95ee\u9898\uff0cDyPho-SLAM \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5229\u7528\u5148\u9a8c\u56fe\u50cf\u4fe1\u606f\u751f\u6210\u7cbe\u786e\u7684\u63a9\u6a21\uff0c\u51cf\u5c11\u8bef\u5224\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u589e\u5f3a\u4f18\u5316\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDyPho-SLAM \u5728\u52a8\u6001 RGB-D \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u5bc6\u96c6\u5730\u56fe\u91cd\u5efa\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u3002", "conclusion": "DyPho-SLAM \u6210\u529f\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d SLAM \u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2509.01641", "pdf": "https://arxiv.org/pdf/2509.01641", "abs": "https://arxiv.org/abs/2509.01641", "authors": ["Yuzhi Yang", "Omar Alhussein", "M\u00e9rouane Debbah"], "title": "Non-Identical Diffusion Models in MIMO-OFDM Channel Generation", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose a novel diffusion model, termed the non-identical diffusion model,\nand investigate its application to wireless orthogonal frequency division\nmultiplexing (OFDM) channel generation. Unlike the standard diffusion model\nthat uses a scalar-valued time index to represent the global noise level, we\nextend this notion to an element-wise time indicator to capture local error\nvariations more accurately. Non-identical diffusion enables us to characterize\nthe reliability of each element (e.g., subcarriers in OFDM) within the noisy\ninput, leading to improved generation results when the initialization is\nbiased. Specifically, we focus on the recovery of wireless multi-input\nmulti-output (MIMO) OFDM channel matrices, where the initial channel estimates\nexhibit highly uneven reliability across elements due to the pilot scheme.\nConventional time embeddings, which assume uniform noise progression, fail to\ncapture such variability across pilot schemes and noise levels. We introduce a\nmatrix that matches the input size to control element-wise noise progression.\nFollowing a similar diffusion procedure to existing methods, we show the\ncorrectness and effectiveness of the proposed non-identical diffusion scheme\nboth theoretically and numerically. For MIMO-OFDM channel generation, we\npropose a dimension-wise time embedding strategy. We also develop and evaluate\nmultiple training and generation methods and compare them through numerical\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u540c\u4e00\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u7ebfOFDM\u4fe1\u9053\u751f\u6210\uff0c\u901a\u8fc7\u5143\u7d20\u7ea7\u65f6\u95f4\u6307\u6807\u6355\u6349\u5c40\u90e8\u8bef\u5dee\u53d8\u5316\uff0c\u63d0\u9ad8\u751f\u6210\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4f7f\u7528\u6807\u91cf\u65f6\u95f4\u7d22\u5f15\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u65e0\u7ebfMIMO-OFDM\u4fe1\u9053\u4e2d\u5143\u7d20\u95f4\u4e0d\u5747\u5300\u7684\u566a\u58f0\u53d8\u5316\uff0c\u5c24\u5176\u5728\u4fe1\u9053\u4f30\u8ba1\u521d\u59cb\u504f\u5dee\u8f83\u5927\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u4e0e\u8f93\u5165\u5c3a\u5bf8\u5339\u914d\u7684\u77e9\u9635\u63a7\u5236\u5143\u7d20\u7ea7\u566a\u58f0\u8fdb\u7a0b\uff0c\u5e76\u63d0\u51fa\u7ef4\u5ea6\u7ea7\u65f6\u95f4\u5d4c\u5165\u7b56\u7565\uff0c\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u548c\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u975e\u540c\u4e00\u6269\u6563\u65b9\u6848\u7684\u6b63\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u4fe1\u9053\u6062\u590d\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u975e\u540c\u4e00\u6269\u6563\u6a21\u578b\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u5c40\u90e8\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u65e0\u7ebfMIMO-OFDM\u4fe1\u9053\u751f\u6210\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.00823", "pdf": "https://arxiv.org/pdf/2509.00823", "abs": "https://arxiv.org/abs/2509.00823", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gr\u00f6bner Systems", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "comment": "24 pages", "summary": "We propose an effective method for solving the inverse kinematic problem of a\nspecific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer\nalgebra. It is known that when the rotation axes of three consecutive\nrotational joints of a manipulator intersect at a single point, the inverse\nkinematics problem can be divided into determining position and orientation. We\nextend this method to more general manipulators in which the rotational axes of\ntwo consecutive joints intersect. This extension broadens the class of 6-DOF\nmanipulators for which the inverse kinematics problem can be solved, and is\nexpected to enable more efficient solutions. The inverse kinematic problem is\nsolved using the Comprehensive Gr\\\"obner System (CGS) with joint parameters of\nthe robot appearing as parameters in the coefficients to prevent repetitive\ncalculations of the Gr\\\"obner bases. The effectiveness of the proposed method\nis shown by experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u4ee3\u6570\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e866-DOF\u673a\u68b0\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u53ef\u6c42\u89e3\u7684\u673a\u68b0\u81c2\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf96-DOF\u673a\u68b0\u81c2\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u6c42\u4e09\u4e2a\u8fde\u7eed\u65cb\u8f6c\u5173\u8282\u7684\u8f74\u7ebf\u4ea4\u4e8e\u4e00\u70b9\uff0c\u9650\u5236\u4e86\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u8be5\u65b9\u6cd5\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u591a\u673a\u68b0\u81c2\u7c7b\u578b\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u673a\u4ee3\u6570\uff08Comprehensive Gr\"obner System, CGS\uff09\u89e3\u51b3\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u5c06\u5173\u8282\u53c2\u6570\u4f5c\u4e3a\u7cfb\u6570\u4e2d\u7684\u53c2\u6570\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97Gr\"obner\u57fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6269\u5c55\u53ef\u6c42\u89e3\u7684\u673a\u68b0\u81c2\u7c7b\u522b\uff0c\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u6269\u5c55\u4e86\u73b0\u6709\u6280\u672f\u7684\u9002\u7528\u8303\u56f4\uff0c\u8fd8\u901a\u8fc7CGS\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u673a\u68b0\u81c2\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01705", "pdf": "https://arxiv.org/pdf/2509.01705", "abs": "https://arxiv.org/abs/2509.01705", "authors": ["Junting Chen", "Bowen Li", "Hao Sun", "Shuguang Cui", "Nikolaos Pappas"], "title": "Predictive Communications for Low-Altitude Networks", "categories": ["eess.SP"], "comment": null, "summary": "The emergence of dense, mission-driven aerial networks supporting the\nlow-altitude economy presents unique communication challenges, including\nextreme channel dynamics and severe cross-tier interference. Traditional\nreactive communication paradigms are ill-suited to these environments, as they\nfail to leverage the network's inherent predictability. This paper introduces\npredictive communication, a novel paradigm transforming network management from\nreactive adaptation to proactive optimization. The approach is enabled by\nfusing predictable mission trajectories with stable, large-scale radio\nenvironment models (e.g., radio maps). Specifically, we present a hierarchical\nframework that decomposes the predictive cross-layer resource allocation\nproblem into three layers: strategic (routing), tactical (timing), and\noperational (power). This structure aligns decision-making timescales with the\naccuracy levels and ranges of available predictive information. We demonstrate\nthat this foresight-driven framework achieves an order-of-magnitude reduction\nin cross-tier interference, laying the groundwork for robust and scalable\nlow-altitude communication systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6027\u901a\u4fe1\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u8f68\u8ff9\u548c\u65e0\u7ebf\u7535\u73af\u5883\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u9002\u5e94\u5230\u4e3b\u52a8\u4f18\u5316\u7684\u7f51\u7edc\u7ba1\u7406\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8de8\u5c42\u5e72\u6270\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u7684\u5bc6\u96c6\u4efb\u52a1\u9a71\u52a8\u7f51\u7edc\u9762\u4e34\u6781\u7aef\u4fe1\u9053\u52a8\u6001\u548c\u4e25\u91cd\u8de8\u5c42\u5e72\u6270\uff0c\u4f20\u7edf\u88ab\u52a8\u901a\u4fe1\u8303\u5f0f\u65e0\u6cd5\u5229\u7528\u7f51\u7edc\u56fa\u6709\u7684\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u6027\u8de8\u5c42\u8d44\u6e90\u5206\u914d\u5206\u89e3\u4e3a\u6218\u7565\uff08\u8def\u7531\uff09\u3001\u6218\u672f\uff08\u65f6\u5e8f\uff09\u548c\u64cd\u4f5c\uff08\u529f\u7387\uff09\u4e09\u5c42\uff0c\u51b3\u7b56\u65f6\u95f4\u5c3a\u5ea6\u4e0e\u9884\u6d4b\u4fe1\u606f\u51c6\u786e\u6027\u5339\u914d\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u5c42\u5e72\u6270\u7684\u6570\u91cf\u7ea7\u51cf\u5c11\uff0c\u4e3a\u4f4e\u7a7a\u901a\u4fe1\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u9884\u6d4b\u6027\u901a\u4fe1\u8303\u5f0f\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u7a7a\u7f51\u7edc\u7684\u52a8\u6001\u548c\u5e72\u6270\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.00828", "pdf": "https://arxiv.org/pdf/2509.00828", "abs": "https://arxiv.org/abs/2509.00828", "authors": ["Takumu Okazaki", "Akira Terui", "Masahiko Mikawa"], "title": "An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator", "categories": ["cs.RO", "cs.SC", "math.AC", "68W30, 13P10, 13P25, 68U07, 68R10"], "comment": "26 pages", "summary": "An effective method for optimizing path planning for a specific model of a\n6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the\nmotion planning of the manipulator using computer algebra. We assume that we\nare given a path in the form of a set of line segments that the end-effector\nshould follow. We also assume that we have a method to solve the inverse\nkinematic problem of the manipulator at each via-point of the trajectory. The\nproposed method consists of three steps. First, we calculate the feasible\nregion of the manipulator under a specific configuration of the end-effector.\nNext, we aim to find a trajectory on the line segments and a sequence of joint\nconfigurations the manipulator should follow to move the end-effector along the\nspecified trajectory. Finally, we find the optimal combination of solutions to\nthe inverse kinematic problem at each via-point along the trajectory by\nreducing the problem to a shortest-path problem of the graph and applying\nDijkstra's algorithm. We show the effectiveness of the proposed method by\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u4ee3\u6570\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u548c\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u5b9e\u73b0\u4f18\u5316\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad86\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "1. \u8ba1\u7b97\u673a\u68b0\u81c2\u5728\u7279\u5b9a\u914d\u7f6e\u4e0b\u7684\u53ef\u884c\u533a\u57df\uff1b2. \u5728\u8def\u5f84\u7ebf\u6bb5\u4e0a\u89c4\u5212\u8f68\u8ff9\u548c\u5173\u8282\u914d\u7f6e\u5e8f\u5217\uff1b3. \u4f7f\u7528Dijkstra\u7b97\u6cd5\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u673a\u4ee3\u6570\u548c\u56fe\u7b97\u6cd5\u4f18\u5316\u4e86\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u3002"}}
{"id": "2509.01802", "pdf": "https://arxiv.org/pdf/2509.01802", "abs": "https://arxiv.org/abs/2509.01802", "authors": ["Anouar Boumeftah", "Gunes Karabulut Kurt"], "title": "Leveraging Orbital Dynamics with RF Signal Features for Satellite Multi-Orbit Proximity Threat Detection", "categories": ["eess.SP"], "comment": null, "summary": "Proximity-based interference is a growing threat to satellite communications,\ndriven by dense multi-orbit constellations and increasingly agile adversarial\nmaneuvers. We propose a hybrid simulation framework that integrates orbital\nmaneuver modeling with RF signal degradation analysis to detect and classify\nsuspicious proximity operations. Using the open-source Maneuver Detection Data\nGeneration (MaDDG) library from MIT Lincoln Laboratory, we generate labeled\ndatasets combining impulsive maneuver profiles with radio-frequency (RF)\nimpacts across a range of behavioral intents: routine station-keeping, covert\nshadowing, and overt jamming. Our approach fuses kinematic features such as\nrange, velocity, acceleration, and Time of Closest Approach (TCA), with RF\nmetrics including Received Signal Strength Indicator (RSSI), throughput, and\nJammer-to-Signal Ratio (JSR). These features are further enhanced with temporal\nderivatives and rolling-window statistics to capture subtle or transient\ninterference patterns. A Random Forest classifier trained on this fused feature\nset achieves 94.67% accuracy and a macro F1 score of 0.9471, outperforming\nmodels using only kinematic or RF inputs. The system is particularly effective\nin detecting covert threats, such as surveillance or intermittent jamming, that\nevade RF-only methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8f68\u9053\u673a\u52a8\u5efa\u6a21\u4e0e\u5c04\u9891\u4fe1\u53f7\u5206\u6790\u7684\u6df7\u5408\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u7c7b\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u53ef\u7591\u63a5\u8fd1\u64cd\u4f5c\uff0c\u6548\u679c\u4f18\u4e8e\u4ec5\u7528\u8fd0\u52a8\u5b66\u6216\u5c04\u9891\u8f93\u5165\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5bc6\u96c6\u591a\u8f68\u9053\u661f\u5ea7\u548c\u654c\u5bf9\u673a\u52a8\u884c\u4e3a\u7684\u589e\u52a0\uff0c\u57fa\u4e8e\u63a5\u8fd1\u7684\u5e72\u6270\u5bf9\u536b\u661f\u901a\u4fe1\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\u3002", "method": "\u878d\u5408\u8fd0\u52a8\u5b66\u7279\u5f81\uff08\u5982\u8ddd\u79bb\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\uff09\u548c\u5c04\u9891\u6307\u6807\uff08\u5982RSSI\u3001\u541e\u5410\u91cf\u3001JSR\uff09\uff0c\u5e76\u4f7f\u7528\u65f6\u5e8f\u5bfc\u6570\u548c\u6ed1\u52a8\u7a97\u53e3\u7edf\u8ba1\u589e\u5f3a\u7279\u5f81\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u878d\u5408\u7279\u5f81\u96c6\u4e0a\u8fbe\u523094.67%\u7684\u51c6\u786e\u7387\u548c0.9471\u7684\u5b8fF1\u5206\u6570\uff0c\u7279\u522b\u64c5\u957f\u68c0\u6d4b\u5c04\u9891\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u9690\u853d\u5a01\u80c1\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u536b\u661f\u901a\u4fe1\u4e2d\u7684\u590d\u6742\u5e72\u6270\u884c\u4e3a\uff0c\u4e3a\u5bf9\u6297\u654c\u5bf9\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2509.00836", "pdf": "https://arxiv.org/pdf/2509.00836", "abs": "https://arxiv.org/abs/2509.00836", "authors": ["Yulin Li", "Tetsuro Miyazaki", "Kenji Kawashima"], "title": "One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields", "categories": ["cs.RO"], "comment": null, "summary": "Motion planning for robotic manipulators is a fundamental problem in\nrobotics. Classical optimization-based methods typically rely on the gradients\nof signed distance fields (SDFs) to impose collision-avoidance constraints.\nHowever, these methods are susceptible to local minima and may fail when the\nSDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have\nbeen introduced, which directly model distances in the robot's configuration\nspace. Unlike workspace SDFs, CDFs are differentiable almost everywhere and\nthus provide reliable gradient information. On the other hand, gradient-free\napproaches such as Model Predictive Path Integral (MPPI) control leverage\nlong-horizon rollouts to achieve collision avoidance. While effective, these\nmethods are computationally expensive due to the large number of trajectory\nsamples, repeated collision checks, and the difficulty of designing cost\nfunctions with heterogeneous physical units. In this paper, we propose a\nframework that integrates CDFs with MPPI to enable direct navigation in the\nrobot's configuration space. Leveraging CDF gradients, we unify the MPPI cost\nin joint-space and reduce the horizon to one step, substantially cutting\ncomputation while preserving collision avoidance in practice. We demonstrate\nthat our approach achieves nearly 100% success rates in 2D environments and\nconsistently high success rates in challenging 7-DOF Franka manipulator\nsimulations with complex obstacles. Furthermore, our method attains control\nfrequencies exceeding 750 Hz, substantially outperforming both\noptimization-based and standard MPPI baselines. These results highlight the\neffectiveness and efficiency of the proposed CDF-MPPI framework for\nhigh-dimensional motion planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u914d\u7f6e\u7a7a\u95f4\u8ddd\u79bb\u573a\uff08CDFs\uff09\u4e0e\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\uff08MPPI\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u907f\u969c\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u68af\u5ea6\u4fe1\u606f\u4f46\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u800c\u65e0\u68af\u5ea6\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u65e8\u5728\u901a\u8fc7CDFs\u548cMPPI\u7684\u7ed3\u5408\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06CDFs\u4e0eMPPI\u7ed3\u5408\uff0c\u5229\u7528CDF\u7684\u68af\u5ea6\u4fe1\u606f\u7b80\u5316MPPI\u6210\u672c\u51fd\u6570\uff0c\u5e76\u5c06\u89c4\u5212\u8303\u56f4\u7f29\u51cf\u5230\u5355\u4e00\u6b65\u9aa4\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u57282D\u73af\u5883\u548c7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4eff\u771f\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u5230\u8fd1100%\u6216\u63a5\u8fd1100%\uff0c\u63a7\u5236\u9891\u7387\u8d85\u8fc7750 Hz\u3002", "conclusion": "CDF-MPPI\u6846\u67b6\u5728\u9ad8\u7ef4\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u548c\u65e0\u68af\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2509.01905", "pdf": "https://arxiv.org/pdf/2509.01905", "abs": "https://arxiv.org/abs/2509.01905", "authors": ["Khawaja Fahad Masood", "Kai Wu", "Zhongqin Wang", "J. Andrew Zhang", "Shu-Lin Chen", "Y. Jay Guo"], "title": "Efficient River Water Level Sensing Using Cellular CSI and Joint Space-Time Processing", "categories": ["eess.SP"], "comment": "12 pages, 13 figures, submitted to an ieee journal for possible\n  publication", "summary": "Accurate and timely water level monitoring is critical for flood prevention,\nenvironmental management, and emerging smart infrastructure systems.\nTraditional water sensing methods often rely on dedicated sensors, which can be\ncostly to deploy and difficult to maintain and are vulnerable to damage during\nfloods.In this work, we propose a novel cellular signalbased sensing scheme\nthat passively estimates water level changes using downlink mobile signals from\nexisting communication infrastructure. By capturing subtle variations in\nchannel state information (CSI), the proposed method estimates the length\nchanges of the water-reflected signal path, which correspond to water level\nvariations. A space-time processing framework is developed to jointly estimate\nthe angle of arrival and Doppler shift, enabling isolation and enhancement of\nthe water-reflected path via beamforming, while effectively suppressing\nenvironmental noise. The phase evolution of the beamformed signal is then\nextracted to infer water level changes. To address clock asynchronism between\nthe transmitter and receiver inherent in bistatic systems, we introduce a\nbeamforming-based compensation technique for removing time-varying random phase\noffsets in CSI. Field experiments conducted across a river demonstrate that the\nproposed method enables accurate and reliable water level estimation, achieving\na mean accuracy ranging from 1.5 cm to 3.05 cm across different receiver\nconfigurations and deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8702\u7a9d\u4fe1\u53f7\u7684\u88ab\u52a8\u6c34\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u4e0b\u884c\u4fe1\u53f7\uff0c\u901a\u8fc7\u6355\u83b7\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u7ec6\u5fae\u53d8\u5316\u6765\u4f30\u8ba1\u6c34\u4f4d\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u6c34\u4f4d\u76d1\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u7ef4\u62a4\u56f0\u96be\u4e14\u6613\u53d7\u6d2a\u6c34\u635f\u574f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ecf\u6d4e\u3001\u53ef\u9760\u7684\u65b0\u578b\u76d1\u6d4b\u6280\u672f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7a7a\u65f6\u5904\u7406\u6846\u67b6\uff0c\u8054\u5408\u4f30\u8ba1\u5230\u8fbe\u89d2\u548c\u591a\u666e\u52d2\u9891\u79fb\uff0c\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u589e\u5f3a\u53cd\u5c04\u4fe1\u53f7\u8def\u5f84\uff0c\u5e76\u63d0\u53d6\u76f8\u4f4d\u53d8\u5316\u4ee5\u63a8\u65ad\u6c34\u4f4d\u53d8\u5316\u3002", "result": "\u5b9e\u5730\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u6c34\u4f4d\u4f30\u8ba1\u5e73\u5747\u7cbe\u5ea6\u57281.5\u5398\u7c73\u52303.05\u5398\u7c73\u4e4b\u95f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c34\u4f4d\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6d2a\u6c34\u9884\u9632\u548c\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u3002"}}
{"id": "2509.00981", "pdf": "https://arxiv.org/pdf/2509.00981", "abs": "https://arxiv.org/abs/2509.00981", "authors": ["Liancheng Zheng", "Zhen Tian", "Yangfan He", "Shuo Liu", "Ke Gong", "Huilin Chen", "Zhihao Lin"], "title": "Enhanced Mean Field Game for Interactive Decision-Making with Varied Stylish Multi-Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents an MFG-based decision-making framework for autonomous\ndriving in heterogeneous traffic. To capture diverse human behaviors, we\npropose a quantitative driving style representation that maps abstract traits\nto parameters such as speed, safety factors, and reaction time. These\nparameters are embedded into the MFG through a spatial influence field model.\nTo ensure safe operation in dense traffic, we introduce a safety-critical\nlane-changing algorithm that leverages dynamic safety margins,\ntime-to-collision analysis, and multi-layered constraints. Real-world NGSIM\ndata is employed for style calibration and empirical validation. Experimental\nresults demonstrate zero collisions across six style combinations, two\n15-vehicle scenarios, and NGSIM-based trials, consistently outperforming\nconventional game-theoretic baselines. Overall, our approach provides a\nscalable, interpretable, and behavior-aware planning framework for real-world\nautonomous driving applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMFG\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u4ea4\u901a\u4e2d\u7684\u884c\u4e3a\u89c4\u5212\uff0c\u901a\u8fc7\u5b9a\u91cf\u9a7e\u9a76\u98ce\u683c\u8868\u8fbe\u548c\u52a8\u6001\u5b89\u5168\u7b97\u6cd5\u5b9e\u73b0\u96f6\u78b0\u649e\u3002", "motivation": "\u5728\u5f02\u6784\u4ea4\u901a\u4e2d\u6355\u6349\u591a\u6837\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u5b9a\u91cf\u9a7e\u9a76\u98ce\u683c\u8868\u793a\uff0c\u5d4c\u5165\u7a7a\u95f4\u5f71\u54cd\u573a\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u5b89\u5168\u5173\u952e\u53d8\u9053\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u96f6\u78b0\u649e\uff0c\u4f18\u4e8e\u4f20\u7edf\u535a\u5f08\u8bba\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u884c\u4e3a\u611f\u77e5\u7684\u89c4\u5212\u6846\u67b6\u3002"}}
{"id": "2509.01923", "pdf": "https://arxiv.org/pdf/2509.01923", "abs": "https://arxiv.org/abs/2509.01923", "authors": ["Md. Mohibbul Haque Chowdhury", "Nafisa Anjum", "Md. Rokonuzzaman Mim"], "title": "ECG-Based Stress Prediction with Power Spectral Density Features and Classification Models", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, 2 tables", "summary": "Stress has emerged as a critical global health issue, contributing to\ncardiovascular disorders, depression, and several other long-term illnesses.\nConsequently, accurate and reliable stress monitoring systems are of growing\nimportance. In this work, we propose a stress prediction framework based on\nelectrocardiogram (ECG) signals recorded during multiple daily activities such\nas sitting, walking, and jogging. Frequency-domain indicators of autonomic\nnervous system activity were obtained through Power Spectral Density (PSD)\nanalysis and utilized as input for machine learning models including Decision\nTree, Random Forest, XGBoost, LightGBM, and CatBoost. In addition, deep\nlearning approaches, namely Convolutional Neural Networks (CNN) and Long\nShort-Term Memory (LSTM) networks, were directly applied to the raw ECG\nsignals. Our experiments highlight the effectiveness of ensemble-based\nclassifiers, with CatBoost achieving 90% accuracy. Moreover, the LSTM model\nprovided superior results, attaining 94% accuracy with balanced precision,\nrecall, and F1-score, reflecting its strength in modeling temporal dependencies\nin ECG data. Overall, the findings suggest that integrating frequency-domain\nfeature extraction with advanced learning algorithms enhances stress prediction\nand paves the way for real-time healthcare monitoring solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eECG\u4fe1\u53f7\u7684\u5e94\u529b\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u9891\u57df\u7279\u5f81\u548c\u673a\u5668\u5b66\u4e60/\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0cCatBoost\u548cLSTM\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u538b\u529b\u662f\u5168\u7403\u5065\u5eb7\u95ee\u9898\uff0c\u9700\u8981\u51c6\u786e\u53ef\u9760\u7684\u76d1\u6d4b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528ECG\u4fe1\u53f7\uff0c\u901a\u8fc7PSD\u5206\u6790\u83b7\u53d6\u9891\u57df\u7279\u5f81\uff0c\u7ed3\u5408\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CatBoost\u3001LSTM\uff09\u3002", "result": "CatBoost\u8fbe\u523090%\u51c6\u786e\u7387\uff0cLSTM\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u8fbe94%\u3002", "conclusion": "\u9891\u57df\u7279\u5f81\u4e0e\u5148\u8fdb\u5b66\u4e60\u7b97\u6cd5\u7ed3\u5408\u53ef\u63d0\u5347\u5e94\u529b\u9884\u6d4b\uff0c\u4e3a\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2509.01010", "pdf": "https://arxiv.org/pdf/2509.01010", "abs": "https://arxiv.org/abs/2509.01010", "authors": ["Hai-Jun Su"], "title": "A Robust Numerical Method for Solving Trigonometric Equations in Robotic Kinematics", "categories": ["cs.RO", "math.AG"], "comment": null, "summary": "This paper presents a robust numerical method for solving systems of\ntrigonometric equations commonly encountered in robotic kinematics. Our\napproach employs polynomial substitution techniques combined with eigenvalue\ndecomposition to handle singular matrices and edge cases effectively. The\nmethod demonstrates superior numerical stability compared to traditional\napproaches and has been implemented as an open-source Python package. For\nnon-singular matrices, we employ Weierstrass substitution to transform the\nsystem into a quartic polynomial, ensuring all analytical solutions are found.\nFor singular matrices, we develop specialized geometric constraint methods\nusing SVD analysis. The solver demonstrates machine precision accuracy ($<\n10^{-15}$ error) with 100\\% success rate on extensive test cases, making it\nparticularly valuable for robotics applications such as inverse kinematics\nproblems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4e09\u89d2\u51fd\u6570\u65b9\u7a0b\u7ec4\uff0c\u7ed3\u5408\u591a\u9879\u5f0f\u66ff\u6362\u548c\u7279\u5f81\u503c\u5206\u89e3\uff0c\u5904\u7406\u5947\u5f02\u77e9\u9635\u548c\u8fb9\u7f18\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4e09\u89d2\u51fd\u6570\u65b9\u7a0b\u7ec4\u65f6\uff0c\u6570\u503c\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5947\u5f02\u77e9\u9635\u548c\u8fb9\u7f18\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u591a\u9879\u5f0f\u66ff\u6362\u6280\u672f\u548c\u7279\u5f81\u503c\u5206\u89e3\u5904\u7406\u5947\u5f02\u77e9\u9635\uff1b\u5bf9\u975e\u5947\u5f02\u77e9\u9635\u91c7\u7528Weierstrass\u66ff\u6362\u8f6c\u5316\u4e3a\u56db\u6b21\u591a\u9879\u5f0f\uff1b\u5947\u5f02\u77e9\u9635\u5219\u901a\u8fc7SVD\u5206\u6790\u7684\u51e0\u4f55\u7ea6\u675f\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u8bef\u5dee\u5c0f\u4e8e10^-15\uff0c\u6210\u529f\u7387\u8fbe100%\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90Python\u5305\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01935", "pdf": "https://arxiv.org/pdf/2509.01935", "abs": "https://arxiv.org/abs/2509.01935", "authors": ["Thai-Hoc Vu", "Anh-Tu Le", "Ngo Hoang Tu", "Tan N. Nguyen", "Miroslav Voznak"], "title": "On Performance of IoT Networks with Coordinated NOMA Transmission: Covert Monitoring and Information Decoding", "categories": ["eess.SP"], "comment": null, "summary": "This work investigates the covertness and security performance of\nInternet-of-Things (IoTs) networks under Rayleigh fading environments.\nSpecifically, a cellular source transmits covert information to cell-edge users\nwith the assistance of an IoT master node, employing a coordinated direct and\nrelay transmission strategy combined with non-orthogonal multiple access\n(NOMA). This approach not only enhances spectrum utilization but also generates\nfriendly interference to complicate a warden's surveillance or an\neavesdropper's decoding efforts. From a covertness perspective, we derive exact\nclosed-form expressions for the detection error probability (DEP) under\narbitrary judgment thresholds. We then identify the optimal judgment threshold\nfor the worst-case scenario, at which the warden minimizes its DEP performance.\nAccordingly, we determine the effective region for user power allocation (PA)\nin NOMA transmission that satisfies the DEP constraint. From a security\nperspective, we derive analytical expressions for the secrecy outage\nprobability under two eavesdropping strategies using selection combining and\nmaximal ratio combining. Based on this analysis, we propose an adaptive PA\nscheme that maximizes covert rate while ensuring the quality-of-service (QoS)\nrequirements of legitimate users, the system's minimum covertness requirements,\nand supporting successive interference cancellation (SIC) procedures.\nFurthermore, we design an adaptive PA scheme that maximizes the secrecy rate\nwhile ensuring the QoS requirements of legitimate users and SIC conditions.\nNumerical results demonstrate the accuracy of the analytical framework, while\nthe proposed optimization strategies effectively adjust PA coefficients to\nmaximize either the covert rate or the secrecy rate.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u745e\u5229\u8870\u843d\u73af\u5883\u4e0b\u7814\u7a76\u4e86\u7269\u8054\u7f51\u7f51\u7edc\u7684\u9690\u853d\u6027\u548c\u5b89\u5168\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NOMA\u7684\u4f20\u8f93\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u529f\u7387\u5206\u914d\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u7269\u8054\u7f51\u7f51\u7edc\u5728\u9690\u853d\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u5e94\u5bf9\u6f5c\u5728\u7684\u76d1\u63a7\u548c\u7a83\u542c\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u534f\u4f5c\u76f4\u4f20\u548c\u4e2d\u7ee7\u4f20\u8f93\u7b56\u7565\u4e0eNOMA\u7ed3\u5408\uff0c\u63a8\u5bfc\u9690\u853d\u6027\u548c\u5b89\u5168\u6027\u7684\u7406\u8bba\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u529f\u7387\u5206\u914d\u65b9\u6848\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4f18\u5316\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u9690\u853d\u7387\u6216\u5b89\u5168\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7269\u8054\u7f51\u7f51\u7edc\u7684\u9690\u853d\u6027\u548c\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u529f\u7387\u5206\u914d\u5b9e\u73b0\u4e86\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2509.01043", "pdf": "https://arxiv.org/pdf/2509.01043", "abs": "https://arxiv.org/abs/2509.01043", "authors": ["Thays Leach Mitre"], "title": "TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics Education", "categories": ["cs.RO"], "comment": "6 pages, 5 figures. Preprint submission", "summary": "The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.", "AI": {"tldr": "TARA\u662f\u4e00\u6b3e\u4f4e\u6210\u672c\u30013D\u6253\u5370\u7684\u673a\u68b0\u81c2\uff0c\u65e8\u5728\u964d\u4f4e\u673a\u5668\u4eba\u6559\u80b2\u7684\u95e8\u69db\uff0c\u63d0\u4f9b\u5f00\u6e90\u7684\u8d44\u6e90\u548c\u529f\u80fd\u5e73\u8861\u7684\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6559\u80b2\u4e2d\u56e0\u9ad8\u6210\u672c\u8bbe\u5907\u5bfc\u81f4\u7684\u5b66\u751f\u5b9e\u8df5\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1TARA\u673a\u68b0\u81c2\uff0c\u5305\u62ec\u5f00\u6e90\u7684\u8bbe\u8ba1\u6587\u4ef6\u3001\u7ec4\u88c5\u6307\u5bfc\u548c\u57fa\u7840\u4ee3\u7801\u3002", "result": "TARA\u4ee5\u7ea6200\u7f8e\u5143\u7684\u6210\u672c\u63d0\u4f9b\u57fa\u672c\u64cd\u4f5c\u4efb\u52a1\u7684\u51c6\u786e\u6027\u80fd\uff0c\u8fdc\u4f4e\u4e8e\u5de5\u4e1a\u7cfb\u7edf\u7684\u6210\u672c\u3002", "conclusion": "TARA\u662f\u4e00\u4e2a\u53ef\u590d\u5236\u548c\u6269\u5c55\u7684\u6559\u80b2\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u6559\u80b2\u5b9e\u7528\u6027\u800c\u975e\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2509.01958", "pdf": "https://arxiv.org/pdf/2509.01958", "abs": "https://arxiv.org/abs/2509.01958", "authors": ["Jongmin Park", "Junwoo Song", "Taewon Kang", "Jaewon Yu", "Pyo-Woong Son"], "title": "Correlation Analysis Between MF R-Mode Temporal ASF and Meteorological Factors", "categories": ["eess.SP"], "comment": "Submitted to ICCAS 2025", "summary": "As the vulnerabilities of global navigation satellite systems (GNSS) have\nbecome more widely recognized, the need for complementary navigation systems\nhas grown. Medium frequency ranging mode (MF R-Mode) has gained attention as an\neffective backup system during GNSS outages, owing to its strong signal\nstrength and cost-effective scalability. However, to achieve accurate\npositioning, MF R-Mode requires correction for the additional secondary factor\n(ASF), a propagation delay affected by terrain. The temporal variation of ASF,\nknown as temporal ASF, is typically corrected using reference stations;\nhowever, the effectiveness of this method decreases with distance from the\nreference station. In this study, we analyzed the correlation between temporal\nASF and meteorological factors to evaluate the feasibility of predicting\ntemporal ASF based on meteorological factors. Among these factors, temperature\nand humidity showed significant correlations with temporal ASF, suggesting\ntheir potential utility in ASF correction.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6c14\u8c61\u56e0\u7d20\u4e0e\u65f6\u95f4ASF\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u6e29\u5ea6\u548c\u6e7f\u5ea6\u53ef\u7528\u4e8e\u6539\u8fdbASF\u6821\u6b63\u3002", "motivation": "\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff08GNSS\uff09\u6613\u53d7\u653b\u51fb\uff0c\u9700\u8981\u5907\u7528\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f46MF R-Mode\u9700\u6821\u6b63ASF\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u53d7\u8ddd\u79bb\u9650\u5236\uff0c\u56e0\u6b64\u63a2\u7d22\u6c14\u8c61\u56e0\u7d20\u9884\u6d4bASF\u7684\u53ef\u884c\u6027\u3002", "method": "\u5206\u6790\u65f6\u95f4ASF\u4e0e\u6c14\u8c61\u56e0\u7d20\uff08\u5982\u6e29\u5ea6\u548c\u6e7f\u5ea6\uff09\u7684\u76f8\u5173\u6027\u3002", "result": "\u6e29\u5ea6\u548c\u6e7f\u5ea6\u4e0e\u65f6\u95f4ASF\u663e\u8457\u76f8\u5173\uff0c\u53ef\u7528\u4e8eASF\u6821\u6b63\u3002", "conclusion": "\u6c14\u8c61\u56e0\u7d20\uff08\u5c24\u5176\u662f\u6e29\u5ea6\u548c\u6e7f\u5ea6\uff09\u53ef\u7528\u4e8e\u9884\u6d4b\u548c\u6821\u6b63\u65f6\u95f4ASF\uff0c\u63d0\u9ad8MF R-Mode\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2509.01044", "pdf": "https://arxiv.org/pdf/2509.01044", "abs": "https://arxiv.org/abs/2509.01044", "authors": ["Yonghyeon Lee", "Tzu-Yuan Lin", "Alexander Alexiev", "Sangbae Kim"], "title": "A Reactive Grasping Framework for Multi-DoF Grippers via Task Space Velocity Fields and Joint Space QP", "categories": ["cs.RO"], "comment": "8 pages, 12 figures, under review", "summary": "We present a fast and reactive grasping framework for multi-DoF grippers that\ncombines task-space velocity fields with a joint-space Quadratic Program (QP)\nin a hierarchical structure. Reactive, collision-free global motion planning is\nparticularly challenging for high-DoF systems, since simultaneous increases in\nstate dimensionality and planning horizon trigger a combinatorial explosion of\nthe search space, making real-time planning intractable. To address this, we\nplan globally in a lower-dimensional task space, such as fingertip positions,\nand track locally in the full joint space while enforcing all constraints. This\napproach is realized by constructing velocity fields in multiple task-space\ncoordinates (or in some cases a subset of joint coordinates) and solving a\nweighted joint-space QP to compute joint velocities that track these fields\nwith appropriately assigned priorities. Through simulation experiments with\nprivileged knowledge and real-world tests using the recent pose-tracking\nalgorithm FoundationPose, we verify that our method enables high-DoF arm-hand\nsystems to perform real-time, collision-free reaching motions while adapting to\ndynamic environments and external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4efb\u52a1\u7a7a\u95f4\u901f\u5ea6\u573a\u548c\u5173\u8282\u7a7a\u95f4QP\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u9ad8\u81ea\u7531\u5ea6\u591a\u6307\u624b\u7684\u5feb\u901f\u53cd\u5e94\u6293\u53d6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u5b9e\u65f6\u89c4\u5212\u4e2d\u72b6\u6001\u7ef4\u5ea6\u548c\u89c4\u5212\u8303\u56f4\u5bfc\u81f4\u7684\u8ba1\u7b97\u7206\u70b8\u95ee\u9898\u3002", "method": "\u5728\u4f4e\u7ef4\u4efb\u52a1\u7a7a\u95f4\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u5173\u8282\u7a7a\u95f4\u5c40\u90e8\u8ddf\u8e2a\uff0c\u5e76\u5229\u7528\u901f\u5ea6\u573a\u548c\u52a0\u6743QP\u5b9e\u73b0\u7ea6\u675f\u8ddf\u8e2a\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u78b0\u649e\u7684\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u7684\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2509.02030", "pdf": "https://arxiv.org/pdf/2509.02030", "abs": "https://arxiv.org/abs/2509.02030", "authors": ["Zehra Yigit", "Sefa Kayraklik", "Ertugrul Basar", "Ali Gorcin"], "title": "Dual Target-Mounted RISs-Assisted ISAC Against Eavesdropping and Malicious Interference", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": "9 pages, 8 figures", "summary": "The synergy between integrated sensing and communication (ISAC) and\nreconfigurable intelligent surfaces (RISs) unlocks novel applications and\nadvanced services for next-generation wireless networks, yet also introduces\nnew security challenges. In this study, a novel dual target-mounted\nRISs-assisted ISAC scheme is proposed, where a base station with ISAC\ncapability performs sensing of two unmanned aerial vehicle (UAV) targets, one\nof which is legitimate and the other is eavesdropper, while communicating with\nthe users through an RIS mounted on the legitimate UAV target. The proposed\nscheme addresses dual security threats posed by a hostile UAV target:\neavesdropping on legitimate user communications and random interference attacks\nlaunched by a malicious RIS mounted on this eavesdropper UAV target, aiming to\ndisrupt secure transmissions. A non-convex optimization problem maximizing the\nsecrecy rate of the users is formulated, and a semi-definite relaxation\n(SDR)-based two-stage solution is developed to optimize the transmit\nbeamforming matrix of the base station and the phase shift coefficients of the\nlegitimate RIS. Extensive computer simulations are conducted to evaluate the\nrobustness of the proposed solution under various system configurations. The\nproposed system's communication performance is assessed using the secrecy rate\nmetric, while the sensing performance is evaluated through the\nsignal-to-interference-plus-noise ratio and the Cramer-Rao bound (CRB) for\nangle-of-departure (AoD) estimation of the eavesdropper UAV target.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u76ee\u6807RIS\u8f85\u52a9\u7684ISAC\u65b9\u6848\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u76ee\u6807\u7684\u5b89\u5168\u5a01\u80c1\u95ee\u9898\uff0c\u4f18\u5316\u7528\u6237\u7684\u4fdd\u5bc6\u901f\u7387\u3002", "motivation": "\u9488\u5bf9\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u548c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5e26\u6765\u7684\u65b0\u5b89\u5168\u6311\u6218\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7RIS\u8f85\u52a9\u65b9\u6848\u63d0\u5347\u5b89\u5168\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u76ee\u6807RIS\u7684ISAC\u65b9\u6848\uff0c\u901a\u8fc7SDR\u65b9\u6cd5\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u548cRIS\u76f8\u4f4d\u7cfb\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u65b9\u6848\u5728\u591a\u79cd\u914d\u7f6e\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u5747\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u5e94\u5bf9\u4e86\u6076\u610f\u65e0\u4eba\u673a\u76ee\u6807\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u7684\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.01065", "pdf": "https://arxiv.org/pdf/2509.01065", "abs": "https://arxiv.org/abs/2509.01065", "authors": ["Sumitaka Honji", "Takahiro Wada"], "title": "Model Predictive Control for a Soft Robotic Finger with Stochastic Behavior based on Fokker-Planck Equation", "categories": ["cs.RO"], "comment": "6 pages, 7 figures, presented/published at 2025 IEEE 8th\n  International Conference on Soft Robotics (RoboSoft)", "summary": "The inherent flexibility of soft robots offers numerous advantages, such as\nenhanced adaptability and improved safety. However, this flexibility can also\nintroduce challenges regarding highly uncertain and nonlinear motion. These\nchallenges become particularly problematic when using open-loop control\nmethods, which lack a feedback mechanism and are commonly employed in soft\nrobot control. Though one potential solution is model-based control, typical\ndeterministic models struggle with uncertainty as mentioned above. The idea is\nto use the Fokker-Planck Equation (FPE), a master equation of a stochastic\nprocess, to control not the state of soft robots but the probabilistic\ndistribution. In this study, we propose and implement a stochastic-based\ncontrol strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a\nsoft robotic finger. Two numerical simulation case studies examine the\nperformance and characteristics of this control method, revealing its efficacy\nin managing the uncertainty inherent in soft robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPE\u7684\u968f\u673a\u63a7\u5236\u7b56\u7565\uff08FPE-MPC\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u7684\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u7684\u7075\u6d3b\u6027\u5728\u63d0\u5347\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u8fd0\u52a8\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u548c\u975e\u7ebf\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f00\u73af\u63a7\u5236\u65b9\u6cd5\u7f3a\u4e4f\u53cd\u9988\u673a\u5236\u65f6\u66f4\u4e3a\u660e\u663e\u3002", "method": "\u7814\u7a76\u4f7f\u7528FPE\uff08\u968f\u673a\u8fc7\u7a0b\u7684\u4e3b\u65b9\u7a0b\uff09\u63a7\u5236\u8f6f\u673a\u5668\u4eba\u7684\u6982\u7387\u5206\u5e03\uff0c\u800c\u975e\u76f4\u63a5\u63a7\u5236\u72b6\u6001\uff0c\u5e76\u63d0\u51faFPE-MPC\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6570\u503c\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86FPE-MPC\u5728\u7ba1\u7406\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "FPE-MPC\u7b56\u7565\u4e3a\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.02031", "pdf": "https://arxiv.org/pdf/2509.02031", "abs": "https://arxiv.org/abs/2509.02031", "authors": ["Sijiang Li", "Rongqing Zhang", "Xiang Cheng", "Jian Tang"], "title": "Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "To support cooperative perception (CP) of networked mobile agents in dynamic\nscenarios, the efficient and robust transmission of sensory data is a critical\nchallenge. Deep learning-based joint source-channel coding (JSCC) has\ndemonstrated promising results for image transmission under adverse channel\nconditions, outperforming traditional rule-based codecs. While recent works\nhave explored to combine JSCC with the widely adopted multiple-input\nmultiple-output (MIMO) technology, these approaches are still limited to the\ndiscrete-time analog transmission (DTAT) model and simple tasks. Given the\nlimited performance of existing MIMO JSCC schemes in supporting complex CP\ntasks for networked mobile agents with digital MIMO communication systems, this\npaper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system\nfor image transmission, referred to as SoM-MIMO. By leveraging the structural\nproperties of the feature pyramid for perceptual tasks and the channel\nproperties of the closed-loop MIMO communication system, SoM-MIMO enables\nefficient and robust digital MIMO transmission of images. Experimental results\nhave shown that compared with two JSCC baseline schemes, our approach achieves\naverage mAP improvements of 6.30 and 10.48 across all SNR levels, while\nmaintaining identical communication overhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u8054\u89c9\uff08SoM-MIMO\uff09\u7684\u4efb\u52a1\u9a71\u52a8MIMO\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u56fe\u50cf\u4f20\u8f93\uff0c\u5728CP\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709JSCC\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u5316\u79fb\u52a8\u4ee3\u7406\u5728\u590d\u6742CP\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709MIMO JSCC\u65b9\u6848\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7279\u5f81\u91d1\u5b57\u5854\u7684\u7ed3\u6784\u7279\u6027\u548c\u95ed\u73afMIMO\u901a\u4fe1\u7cfb\u7edf\u7684\u4fe1\u9053\u7279\u6027\uff0c\u8bbe\u8ba1SoM-MIMO\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSoM-MIMO\u5728\u76f8\u540c\u901a\u4fe1\u5f00\u9500\u4e0b\uff0c\u5e73\u5747mAP\u63d0\u53476.30\u548c10.48\u3002", "conclusion": "SoM-MIMO\u4e3a\u590d\u6742CP\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u4f20\u8f93\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01111", "pdf": "https://arxiv.org/pdf/2509.01111", "abs": "https://arxiv.org/abs/2509.01111", "authors": ["Haolan Zhang", "Chenghao Li", "Thanh Nguyen Canh", "Lijun Wang", "Nak Young Chong"], "title": "SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments", "categories": ["cs.RO"], "comment": "submitted", "summary": "Visual simultaneous localization and mapping (SLAM) plays a critical role in\nautonomous robotic systems, especially where accurate and reliable measurements\nare essential for navigation and sensing. In feature-based SLAM, the\nquantityand quality of extracted features significantly influence system\nperformance. Due to the variations in feature quantity and quality across\ndiverse environments, current approaches face two major challenges: (1) limited\nadaptability in dynamic feature culling and pose estimation, and (2)\ninsufficient environmental awareness in assessment and optimization strategies.\nTo address these issues, we propose SRR-SLAM, a scene-reliability based\nframework that enhances feature-based SLAM through environment-aware\nprocessing. Our method introduces a unified scene reliability assessment\nmechanism that incorporates multiple metrics and historical observations to\nguide system behavior. Based on this assessment, we develop: (i) adaptive\ndynamic region selection with flexible geometric constraints, (ii)\ndepth-assisted self-adjusting clustering for efficient dynamic feature removal\nin high-dimensional settings, and (iii) reliability-aware pose refinement that\ndynamically integrates direct methods when features are insufficient.\nFurthermore, we propose (iv) reliability-based keyframe selection and a\nweighted optimization scheme to reduce computational overhead while improving\nestimation accuracy. Extensive experiments on public datasets and real world\nscenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,\nachieving up to 90% improvement in accuracy and robustness across diverse\nenvironments. These improvements directly contribute to enhanced measurement\nprecision and reliability in autonomous robotic sensing systems.", "AI": {"tldr": "SRR-SLAM\u901a\u8fc7\u573a\u666f\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6\u63d0\u5347\u7279\u5f81SLAM\u7684\u81ea\u9002\u5e94\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u52a8\u6001\u7279\u5f81\u5254\u9664\u548c\u59ff\u6001\u4f30\u8ba1\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\u53ca\u73af\u5883\u611f\u77e5\u7f3a\u5931\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u573a\u666f\u53ef\u9760\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u7ed3\u5408\u52a8\u6001\u533a\u57df\u9009\u62e9\u3001\u6df1\u5ea6\u805a\u7c7b\u3001\u53ef\u9760\u6027\u4f18\u5316\u548c\u5173\u952e\u5e27\u52a0\u6743\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9645\u573a\u666f\u4e2dSRR-SLAM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u63d0\u5347\u8fbe90%\u3002", "conclusion": "SRR-SLAM\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u7684\u6d4b\u91cf\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.02088", "pdf": "https://arxiv.org/pdf/2509.02088", "abs": "https://arxiv.org/abs/2509.02088", "authors": ["Yejian Lyu", "Zhiqiang Yuan", "Henk Wymeersch", "Chong Han"], "title": "Environment-Aware Channel Measurement and Modeling for Terahertz Monostatic Sensing", "categories": ["eess.SP"], "comment": null, "summary": "Integrated sensing and communication (ISAC) at terahertz (THz) frequencies\nholds significant promise for unifying ultra-high-speed wireless connectivity\nwith fine-grained environmental awareness. Realistic and interpretable channel\nmodeling is essential to fully realize the potential of such systems. This work\npresents a comprehensive investigation of monostatic sensing channels at\n300~GHz, based on an extensive measurement campaign conducted at 57 co-located\ntransceiver (TRx) positions across three representative indoor scenarios.\nMultipath component (MPC) parameters, including amplitude, delay, and angle,\nare extracted using a high-resolution space-alternating generalized\nexpectation-maximization (SAGE) algorithm. To cluster the extracted MPCs, an\nimage-processing-based clustering method, i.e., connected component labeling\n(CCL), is applied to group MPCs based on delay-angle consistency. Based on the\nmeasurement data, an environment-aware channel modeling framework is proposed\nto establish mappings between physical scenario attributes (e.g., reflector\ngeometry, surface materials, and roughness) and their corresponding\nchannel-domain manifestations. The framework incorporates both specular and\ndiffuse reflections and leverages several channel parameters, e.g., reflection\nloss, Lambertian scattering, and intra-cluster dispersion models, to\ncharacterize reflection behavior. Experimental results demonstrate that the\nproposed approach can reliably extract physical characteristics, e.g.,\nstructural and material information, from the observed channel characteristics,\noffering a promising foundation for advanced THz ISAC channel modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e300 GHz\u9891\u6bb5\u7684\u5ba4\u5185\u5355\u9759\u6001\u4f20\u611f\u4fe1\u9053\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u7b97\u6cd5\u548c\u73af\u5883\u611f\u77e5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4ece\u4fe1\u9053\u7279\u5f81\u4e2d\u63d0\u53d6\u7269\u7406\u573a\u666f\u5c5e\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u592a\u8d6b\u5179\u9891\u6bb5\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5177\u6709\u9ad8\u901f\u65e0\u7ebf\u8fde\u63a5\u548c\u73af\u5883\u611f\u77e5\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u771f\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u4fe1\u9053\u5efa\u6a21\u4ee5\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "method": "\u901a\u8fc757\u4e2a\u5171\u7f6e\u6536\u53d1\u5668\u7684\u5b9e\u6d4b\u6570\u636e\uff0c\u4f7f\u7528SAGE\u7b97\u6cd5\u63d0\u53d6\u591a\u5f84\u53c2\u6570\uff0c\u5e76\u5229\u7528\u56fe\u50cf\u5904\u7406\u805a\u7c7b\u65b9\u6cd5\uff08CCL\uff09\u5bf9\u591a\u5f84\u5206\u7ec4\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u955c\u9762\u548c\u6f2b\u53cd\u5c04\u7684\u73af\u5883\u611f\u77e5\u4fe1\u9053\u5efa\u6a21\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u4ece\u89c2\u6d4b\u4fe1\u9053\u7279\u5f81\u4e2d\u63d0\u53d6\u7269\u7406\u573a\u666f\u7684\u7ed3\u6784\u548c\u6750\u6599\u4fe1\u606f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u7ea7\u592a\u8d6b\u5179ISAC\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002"}}
{"id": "2509.01113", "pdf": "https://arxiv.org/pdf/2509.01113", "abs": "https://arxiv.org/abs/2509.01113", "authors": ["Haiyun Zhang", "Kelvin HoLam Heung", "Gabrielle J. Naquila", "Ashwin Hingwe", "Ashish D. Deshpande"], "title": "A novel parameter estimation method for pneumatic soft hand control applying logarithmic decrement for pseudo rigid body modeling", "categories": ["cs.RO"], "comment": null, "summary": "The rapid advancement in physical human-robot interaction (HRI) has\naccelerated the development of soft robot designs and controllers. Controlling\nsoft robots, especially soft hand grasping, is challenging due to their\ncontinuous deformation, motivating the use of reduced model-based controllers\nfor real-time dynamic performance. Most existing models, however, suffer from\ncomputational inefficiency and complex parameter identification, limiting their\nreal-time applicability. To address this, we propose a paradigm coupling\nPseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter\nestimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate\nPRBM plus LDM for predicting position and force output from pressure input and\nbenchmark its performance. We then implement PRBM plus LDM as the basis for\nclosed-loop position and force controllers. Compared to a simple PID\ncontroller, the PRBM plus LDM position controller achieves lower error (average\nmaximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force\ncontrol, PRBM plus LDM outperforms constant pressure grasping in pinching tasks\non delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,\nbrass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a\ncomputationally efficient and accurate modeling technique for soft actuators,\nenabling stable and flexible grasping with precise force regulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f2a\u521a\u4f53\u6a21\u578b(PRBM)\u548c\u5bf9\u6570\u9012\u51cf\u6cd5(LDM)\u7684\u65b0\u65b9\u6cd5(PRBM+LDM)\uff0c\u7528\u4e8e\u63d0\u9ad8\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u53d6\u7684\u5b9e\u65f6\u52a8\u6001\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edfPID\u63a7\u5236\u5668\u8868\u73b0\u51fa\u66f4\u4f4e\u8bef\u5dee\u548c\u66f4\u5f3a\u7684\u529b\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\uff08\u5c24\u5176\u662f\u8f6f\u4f53\u624b\uff09\u7684\u8fde\u7eed\u53d8\u5f62\u7279\u6027\u4f7f\u5f97\u5176\u63a7\u5236\u6781\u5177\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u53c2\u6570\u8bc6\u522b\u590d\u6742\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u4f2a\u521a\u4f53\u6a21\u578b(PRBM)\u4e0e\u5bf9\u6570\u9012\u51cf\u6cd5(LDM)\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5(PRBM+LDM)\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9e\u73b0\u4e86\u95ed\u73af\u4f4d\u7f6e\u548c\u529b\u63a7\u5236\u5668\u3002", "result": "PRBM+LDM\u5728\u4f4d\u7f6e\u63a7\u5236\u4e2d\u5e73\u5747\u6700\u5927\u8bef\u5dee\u4e3a4.37\u5ea6\uff08PID\u4e3a20.38\u5ea6\uff09\uff1b\u5728\u529b\u63a7\u5236\u4e2d\uff0c\u5bf9\u8106\u5f31\u7269\u4f53\u7684\u6293\u53d6\u8868\u73b0\u4f18\u4e8e\u6052\u538b\u6293\u53d6\uff08\u5982\u85af\u724786 vs. 82.5\uff09\u3002", "conclusion": "PRBM+LDM\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u8f6f\u4f53\u81f4\u52a8\u5668\u5efa\u6a21\u6280\u672f\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u3001\u7075\u6d3b\u7684\u6293\u53d6\u548c\u7cbe\u786e\u7684\u529b\u8c03\u8282\u3002"}}
{"id": "2509.02116", "pdf": "https://arxiv.org/pdf/2509.02116", "abs": "https://arxiv.org/abs/2509.02116", "authors": ["Yuanfang Ma", "Zulin Wang", "Peng Yuan", "Qin Huang", "Yuanhan Ni"], "title": "Affine-Doppler Division Multiplexing for High-Mobility Wireless Communications Systems", "categories": ["eess.SP"], "comment": "6 pages, 4 figures, 1 table", "summary": "Affine Frequency Division Multiplexing (AFDM) has been regarded as a\ncandidate integrated sensing and communications (ISAC) waveform owing to its\nsuperior communication performance, outperforming the Orthogonal Time-Frequency\nSpace (OTFS) that has been researched for a longer time. However, since the\nabove two waveforms are incompatible with each other, the state-of-the-art\nmethods well-designed for OTFS may not be directly applicable to AFDM. This\npaper introduces a new orthogonal multicarrier waveform, namely Affine-Doppler\nDivision Multiplexing (ADDM), which can provide a generic framework and subsume\nthe existing OTFS and AFDM as a particular case. ADDM modulating information\nsymbols in the Affine-Doppler (A-D) domain based on a two-dimensional (2D)\ntransform can enjoy both excellent unambiguous Doppler and Doppler resolution,\nwhich is the same as AFDM but outperforms OTFS. Moreover, benefiting from the\n2D transform, the symbols block of ADDM in the A-D domain undergoes a 2D cyclic\nshift produced by the delay and the Doppler of the channel, similar to the 2D\ncyclic shift in the delay-Doppler domain of cyclic prefix (CP)-OTFS. This\noffers a potential to directly apply the state-of-the-art methods well-designed\nfor OTFS and AFDM to ADDM. Numerical results show that ADDM achieves comparable\nBER performance with AFDM but outperforms OTFS in high-mobility scenarios.", "AI": {"tldr": "ADDM\u662f\u4e00\u79cd\u65b0\u7684\u6b63\u4ea4\u591a\u8f7d\u6ce2\u6ce2\u5f62\uff0c\u517c\u5bb9OTFS\u548cAFDM\uff0c\u63d0\u4f9b\u4f18\u5f02\u7684\u591a\u666e\u52d2\u6027\u80fd\u3002", "motivation": "AFDM\u548cOTFS\u4e0d\u517c\u5bb9\uff0c\u7814\u7a76\u9700\u8981\u4e00\u79cd\u901a\u7528\u6846\u67b6\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u57fa\u4e8e2D\u53d8\u6362\u5728Affine-Doppler\u57df\u8c03\u5236\u4fe1\u606f\u7b26\u53f7\u3002", "result": "ADDM\u6027\u80fd\u4e0eAFDM\u76f8\u5f53\uff0c\u4f18\u4e8eOTFS\u3002", "conclusion": "ADDM\u4e3a\u901a\u4fe1\u6ce2\u5f62\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2509.01145", "pdf": "https://arxiv.org/pdf/2509.01145", "abs": "https://arxiv.org/abs/2509.01145", "authors": ["Haiyun Zhang", "Gabrielle Naquila", "Jung Hyun Bae", "Zonghuan Wu", "Ashwin Hingwe", "Ashish Deshpande"], "title": "Novel bio-inspired soft actuators for upper-limb exoskeletons: design, fabrication and feasibility study", "categories": ["cs.RO"], "comment": null, "summary": "Soft robots have been increasingly utilized as sophisticated tools in\nphysical rehabilitation, particularly for assisting patients with neuromotor\nimpairments. However, many soft robotics for rehabilitation applications are\ncharacterized by limitations such as slow response times, restricted range of\nmotion, and low output force. There are also limited studies on the precise\nposition and force control of wearable soft actuators. Furthermore, not many\nstudies articulate how bellow-structured actuator designs quantitatively\ncontribute to the robots' capability. This study introduces a paradigm of upper\nlimb soft actuator design. This paradigm comprises two actuators: the\nLobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the\nScallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is\ncharacterized by higher bandwidth, increased output force/torque, and high\nlinearity. SCASPER is characterized by high output force/torque and simplified\nfabrication processes. Comprehensive analytical models that describe the\nrelationship between pressure, bending angles, and output force for both\nactuators were presented so the geometric configuration of the actuators can be\nset to modify the range of motion and output forces. The preliminary test on a\ndummy arm is conducted to test the capability of the actuators.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0a\u80a2\u5eb7\u590d\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5305\u62ec\u4e24\u79cd\u4eff\u751f\u6c14\u52a8\u6267\u884c\u5668\uff08LISPER\u548cSCASPER\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u8fd0\u52a8\u8303\u56f4\u548c\u8f93\u51fa\u529b\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u5eb7\u590d\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u5b58\u5728\u54cd\u5e94\u6162\u3001\u8fd0\u52a8\u8303\u56f4\u53d7\u9650\u548c\u8f93\u51fa\u529b\u4f4e\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5173\u4e8e\u6267\u884c\u5668\u8bbe\u8ba1\u4e0e\u91cf\u5316\u6027\u80fd\u5173\u7cfb\u7684\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4eff\u751f\u6c14\u52a8\u6267\u884c\u5668\uff08LISPER\u548cSCASPER\uff09\uff0c\u5206\u522b\u7528\u4e8e\u8098\u90e8\u548c\u80a9\u90e8\u5eb7\u590d\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u63cf\u8ff0\u538b\u529b\u3001\u5f2f\u66f2\u89d2\u5ea6\u548c\u8f93\u51fa\u529b\u7684\u5173\u7cfb\u3002", "result": "LISPER\u5177\u6709\u9ad8\u5e26\u5bbd\u3001\u9ad8\u7ebf\u6027\u5ea6\u548c\u66f4\u5927\u8f93\u51fa\u529b/\u626d\u77e9\uff1bSCASPER\u8f93\u51fa\u529b/\u626d\u77e9\u9ad8\u4e14\u5236\u9020\u5de5\u827a\u7b80\u5316\u3002\u521d\u6b65\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6267\u884c\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u8303\u5f0f\u4e3a\u5eb7\u590d\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6027\u80fd\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4eff\u751f\u7ed3\u6784\u548c\u5206\u6790\u6a21\u578b\u63d0\u5347\u4e86\u6267\u884c\u5668\u7684\u53ef\u63a7\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.02137", "pdf": "https://arxiv.org/pdf/2509.02137", "abs": "https://arxiv.org/abs/2509.02137", "authors": ["Salmane Naoumi", "Ahmad Bazzi", "Roberto Bomfin", "Marwa Chafii"], "title": "High-Resolution Sensing in Communication-Centric ISAC: Deep Learning and Parametric Methods", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "This paper introduces two novel algorithms designed to address the challenge\nof super-resolution sensing parameter estimation in bistatic configurations\nwithin communication-centric integrated sensing and communication (ISAC)\nsystems. Our approach leverages the estimated channel state information derived\nfrom reference symbols originally intended for communication to achieve\nsuper-resolution sensing parameter estimation. The first algorithm, IFFT-C2VNN,\nemploys complex-valued convolutional neural networks to estimate the parameters\nof different targets, achieving significant reductions in computational\ncomplexity compared to traditional methods. The second algorithm, PARAMING,\nutilizes a parametric method that capitalizes on the knowledge of the system\nmodel, including the transmit and receive array geometries, to extract the\nsensing parameters accurately. Through a comprehensive performance analysis, we\ndemonstrate the effectiveness and robustness of both algorithms across a range\nof signal-to-noise ratios, underscoring their applicability in realistic ISAC\nscenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e24\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u901a\u4fe1\u4e2d\u5fc3\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u53cc\u57fa\u5730\u914d\u7f6e\u8d85\u5206\u8fa8\u7387\u4f20\u611f\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53cc\u57fa\u5730\u914d\u7f6e\u4e2d\u4f20\u611f\u53c2\u6570\u7684\u9ad8\u5206\u8fa8\u7387\u4f30\u8ba1\u6311\u6218\uff0c\u63d0\u5347\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u7684\u6548\u7387\u3002", "method": "IFFT-C2VNN\u4f7f\u7528\u590d\u503c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1bPARAMING\u5229\u7528\u7cfb\u7edf\u6a21\u578b\u77e5\u8bc6\u7cbe\u786e\u63d0\u53d6\u53c2\u6570\u3002", "result": "\u4e24\u79cd\u7b97\u6cd5\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u4e0b\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7b97\u6cd5\u5728\u73b0\u5b9eISAC\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.01228", "pdf": "https://arxiv.org/pdf/2509.01228", "abs": "https://arxiv.org/abs/2509.01228", "authors": ["Jianyu Dou", "Yinan Deng", "Jiahui Wang", "Xingsi Tang", "Yi Yang", "Yufeng Yue"], "title": "OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed Implicit Mapping", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters. Project website:\n  https://openmulti666.github.io/", "summary": "Multi-agent distributed collaborative mapping provides comprehensive and\nefficient representations for robots. However, existing approaches lack\ninstance-level awareness and semantic understanding of environments, limiting\ntheir effectiveness for downstream applications. To address this issue, we\npropose OpenMulti, an open-vocabulary instance-level multi-agent distributed\nimplicit mapping framework. Specifically, we introduce a Cross-Agent Instance\nAlignment module, which constructs an Instance Collaborative Graph to ensure\nconsistent instance understanding across agents. To alleviate the degradation\nof mapping accuracy due to the blind-zone optimization trap, we leverage Cross\nRendering Supervision to enhance distributed learning of the scene.\nExperimental results show that OpenMulti outperforms related algorithms in both\nfine-grained geometric accuracy and zero-shot semantic accuracy. In addition,\nOpenMulti supports instance-level retrieval tasks, delivering semantic\nannotations for downstream applications. The project website of OpenMulti is\npublicly available at https://openmulti666.github.io/.", "AI": {"tldr": "OpenMulti\u662f\u4e00\u4e2a\u5f00\u8bcd\u6c47\u5b9e\u4f8b\u7ea7\u522b\u7684\u591a\u667a\u80fd\u4f53\u5206\u5e03\u5f0f\u9690\u5f0f\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u667a\u80fd\u4f53\u5b9e\u4f8b\u5bf9\u9f50\u548c\u8de8\u6e32\u67d3\u76d1\u7763\u63d0\u5347\u6620\u5c04\u7cbe\u5ea6\u548c\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u73af\u5883\u5b9e\u4f8b\u7ea7\u522b\u7684\u611f\u77e5\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faCross-Agent Instance Alignment\u6a21\u5757\u6784\u5efaInstance Collaborative Graph\uff0c\u5e76\u5229\u7528Cross Rendering Supervision\u589e\u5f3a\u5206\u5e03\u5f0f\u5b66\u4e60\u3002", "result": "OpenMulti\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u96f6\u6837\u672c\u8bed\u4e49\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u76f8\u5173\u7b97\u6cd5\uff0c\u5e76\u652f\u6301\u5b9e\u4f8b\u7ea7\u522b\u68c0\u7d22\u4efb\u52a1\u3002", "conclusion": "OpenMulti\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bed\u4e49\u6ce8\u91ca\u548c\u6620\u5c04\u6846\u67b6\u3002"}}
{"id": "2509.02166", "pdf": "https://arxiv.org/pdf/2509.02166", "abs": "https://arxiv.org/abs/2509.02166", "authors": ["Enzhi Zhou", "Yue Xiao", "Ziyue Liu", "Sotiris A. Tegos", "Panagiotis D. Diamantoulakis", "George K. Karagiannidis"], "title": "Beamforming Design for Pinching Antenna Systems with Multiple Receive Antennas", "categories": ["eess.SP", "cs.IT", "math.IT"], "comment": null, "summary": "Next-generation networks require intelligent and robust channel conditions to\nsupport ultra-high data rates, seamless connectivity, and large-scale device\ndeployments in dynamic environments. While flexible antenna technologies such\nas fluid and movable antennas offer some degree of adaptability, their limited\nreconfiguration range and structural rigidity reduce their effectiveness in\nrestoring line-of-sight (LoS) links. As a complementary solution, pinching\nantenna systems (PASs) enable fine-grained, hardware-free control of radiation\nlocations along a waveguide, offering enhanced flexibility in challenging\npropagation environments, especially under non-LoS (NLoS) conditions. This\npaper introduces a general and novel modeling framework for downlink PASs\ntargeting users equipped with multiple receive antennas, addressing a practical\nyet underexplored scenario in the existing literature. Specifically, we first\nderive an analytical relationship between the received signal-to-noise ratio\nand the pinching antenna (PA) positions, and based on this, we propose a\ntwo-layer placement strategy. First, we optimize the central radiation point\nusing large-scale channel characteristics, and then we use a heuristic\ncompressed placement algorithm to approximate phase alignment across multiple\nreceive antennas and select a spatially compact set of active elements.\nSimulation results demonstrate notable performance gains over conventional\nsingle-antenna schemes, particularly in short-range scenarios with dense PAs\nand widely spaced user antennas.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u63a5\u6536\u5929\u7ebf\u7528\u6237\u7684\u634f\u5236\u5929\u7ebf\u7cfb\u7edf\uff08PAS\uff09\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8f90\u5c04\u70b9\u4f4d\u7f6e\u548c\u542f\u53d1\u5f0f\u538b\u7f29\u653e\u7f6e\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u53f7\u6027\u80fd\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u9700\u8981\u667a\u80fd\u4e14\u7a33\u5065\u7684\u4fe1\u9053\u6761\u4ef6\u652f\u6301\u8d85\u9ad8\u6570\u636e\u901f\u7387\u548c\u65e0\u7f1d\u8fde\u63a5\u3002\u4f20\u7edf\u5929\u7ebf\u6280\u672f\u9002\u5e94\u6027\u6709\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u6062\u590d\u89c6\u8ddd\u94fe\u8def\uff0cPAS\u4e3a\u6b64\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u5c42\u7684\u653e\u7f6e\u7b56\u7565\uff1a\u9996\u5148\u4f18\u5316\u4e2d\u5fc3\u8f90\u5c04\u70b9\uff0c\u968f\u540e\u4f7f\u7528\u542f\u53d1\u5f0f\u538b\u7f29\u653e\u7f6e\u7b97\u6cd5\u9009\u62e9\u7d27\u51d1\u7684\u6d3b\u8dc3\u5929\u7ebf\u96c6\u5408\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u77ed\u8ddd\u79bb\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u5bc6\u96c6PA\u548c\u5bbd\u95f4\u8ddd\u7528\u6237\u5929\u7ebf\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u5929\u7ebf\u65b9\u6848\u3002", "conclusion": "PAS\u5728\u591a\u63a5\u6536\u5929\u7ebf\u7528\u6237\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01251", "pdf": "https://arxiv.org/pdf/2509.01251", "abs": "https://arxiv.org/abs/2509.01251", "authors": ["Pilar Bachiller-Burgos", "Ulysses Bernardet", "Luis V. Calderita", "Pranup Chhetri", "Anthony Francis", "Noriaki Hirose", "No\u00e9 P\u00e9rez", "Dhruv Shah", "Phani T. Singamaneni", "Xuesu Xiao", "Luis J. Manso"], "title": "Towards Data-Driven Metrics for Social Robot Navigation Benchmarking", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a joint effort towards the development of a data-driven\nSocial Robot Navigation metric to facilitate benchmarking and policy\noptimization. We provide our motivations for our approach and describe our\nproposal for storing rated social navigation trajectory datasets. Following\nthese guidelines, we compiled a dataset with 4427 trajectories -- 182 real and\n4245 simulated -- and presented it to human raters, yielding a total of 4402\nrated trajectories after data quality assurance. We also trained an RNN-based\nbaseline metric on the dataset and present quantitative and qualitative\nresults. All data, software, and model weights are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u6307\u6807\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u548c\u653f\u7b56\u4f18\u5316\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "motivation": "\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u6307\u6807\uff0c\u4ee5\u652f\u6301\u57fa\u51c6\u6d4b\u8bd5\u548c\u653f\u7b56\u4f18\u5316\u3002", "method": "\u6536\u96c6\u5e76\u8bc4\u4f30\u4e864427\u6761\u8f68\u8ff9\u6570\u636e\uff08182\u6761\u771f\u5b9e\u6570\u636e\u548c4245\u6761\u6a21\u62df\u6570\u636e\uff09\uff0c\u5e76\u57fa\u4e8eRNN\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u7ebf\u6307\u6807\u3002", "result": "\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u540e\u83b7\u5f97\u4e864402\u6761\u8bc4\u5206\u8f68\u8ff9\uff0c\u5e76\u5c55\u793a\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u3002", "conclusion": "\u6240\u6709\u6570\u636e\u3001\u8f6f\u4ef6\u548c\u6a21\u578b\u6743\u91cd\u5747\u5df2\u516c\u5f00\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u5229\u3002"}}
{"id": "2509.02260", "pdf": "https://arxiv.org/pdf/2509.02260", "abs": "https://arxiv.org/abs/2509.02260", "authors": ["Yifan Guo", "Junshan Luo", "Fanggang Wang", "Haiyang Ding", "Shilian Wang", "Zhenhai Xu"], "title": "Dual-end Fluid Antennas For Robust Anti-jamming in Low-altitude Air-ground Communications", "categories": ["eess.SP"], "comment": "14 pages, 8 figures, submitted to IEEE journal for possible\n  publications", "summary": "This paper addresses the challenge of co-channel interference and intentional\njamming in low-altitude air-ground communications. Since conventional\nfixed-position antenna (FPA) systems lack spatial adaptability to dynamically\nbalance signal enhancement against interference suppression, we propose a\ntransformative fluid antenna system (FAS)-assisted heterogeneous dual-layer\ntransmission architecture. Specifically, a terrestrial base station with FPA\nserves ground users, while a low altitude-serving base station equipped with\nFAS communicates with the aerial user, also equipped with FAS, under the attack\nof a malicious jammer. We formulate a worst-case achievable rate maximization\nproblem for aerial user subject to constraints including quality-of-service for\nterrestrial users, imperfect jamming directions, minimum antenna separation,\netc. To address the non-convex problem, we propose a fractional\nprogramming-block coordinate descent algorithm that alternately optimizes the\ntransmit precoders, receive combiner, and antenna positions at both transceiver\nsides. Convex hull-based approach and geometric boundary method are used to\nhandle the jamming uncertainty and antenna placement constraints in confined\nspatial regions, respectively. Extensive simulations validate significant\nperformance gains. The FAS achieves up to 56\\% higher data rates than FPA under\nequivalent power constraints. Strategic antenna repositioning demonstrably\nenhances signal quality while suppressing interference, maintaining robustness\nacross diverse jammer channel uncertainties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u7684\u5f02\u6784\u53cc\u5c42\u4f20\u8f93\u67b6\u6784\uff0c\u89e3\u51b3\u4f4e\u7a7a\u7a7a\u5730\u901a\u4fe1\u4e2d\u7684\u540c\u9891\u5e72\u6270\u548c\u6076\u610f\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6027\u80fd\u6570\u636e\u4f20\u8f93\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\uff08FPA\uff09\u7cfb\u7edf\u7f3a\u4e4f\u7a7a\u95f4\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u52a8\u6001\u5e73\u8861\u4fe1\u53f7\u589e\u5f3a\u4e0e\u5e72\u6270\u6291\u5236\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u89e3\u51b3\u4f4e\u7a7a\u901a\u4fe1\u4e2d\u7684\u5e72\u6270\u95ee\u9898\u3002", "method": "\u63d0\u51faFAS\u8f85\u52a9\u7684\u5f02\u6784\u53cc\u5c42\u4f20\u8f93\u67b6\u6784\uff0c\u7ed3\u5408FPA\u548cFAS\uff0c\u901a\u8fc7\u5206\u6570\u89c4\u5212-\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\u53d1\u5c04\u9884\u7f16\u7801\u5668\u3001\u63a5\u6536\u7ec4\u5408\u5668\u548c\u5929\u7ebf\u4f4d\u7f6e\u3002", "result": "FAS\u5728\u76f8\u540c\u529f\u7387\u7ea6\u675f\u4e0b\u6bd4FPA\u63d0\u9ad856%\u7684\u6570\u636e\u901f\u7387\uff0c\u4e14\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u80fd\u663e\u8457\u589e\u5f3a\u4fe1\u53f7\u8d28\u91cf\u5e76\u6291\u5236\u5e72\u6270\u3002", "conclusion": "FAS\u67b6\u6784\u5728\u4f4e\u7a7a\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5e72\u6270\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01291", "pdf": "https://arxiv.org/pdf/2509.01291", "abs": "https://arxiv.org/abs/2509.01291", "authors": ["Nouhed Naidja", "St\u00e9phane Font", "Marc Revilloud", "Guillaume Sandou"], "title": "Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for Autonomous Driving in Mixed Traffic Environment", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a unified framework for the evaluation and optimization\nof autonomous vehicle trajectories, integrating formal safety, comfort, and\nefficiency criteria. An innovative geometric indicator, based on the analysis\nof safety zones using adaptive ellipses, is used to accurately quantify\ncollision risks. Our method applies the Shoelace formula to compute the\nintersection area in the case of misaligned and time-varying configurations.\nComfort is modeled using indicators centered on longitudinal and lateral jerk,\nwhile efficiency is assessed by overall travel time. These criteria are\naggregated into a comprehensive objective function solved using a PSO based\nalgorithm. The approach was successfully validated under real traffic\nconditions via experiments conducted in an urban intersection involving an\nautonomous vehicle interacting with a human-operated vehicle, and in simulation\nusing data recorded from human driving in real traffic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u6807\u51c6\u3002", "motivation": "\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u8bc4\u4f30\u4e0e\u4f18\u5316\u4e2d\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u7684\u7efc\u5408\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u81ea\u9002\u5e94\u692d\u5706\u5b89\u5168\u533a\u5206\u6790\u7684\u51e0\u4f55\u6307\u6807\u91cf\u5316\u78b0\u649e\u98ce\u9669\uff0c\u5e94\u7528Shoelace\u516c\u5f0f\u8ba1\u7b97\u9519\u4f4d\u548c\u65f6\u95f4\u53d8\u5316\u914d\u7f6e\u4e0b\u7684\u4ea4\u96c6\u9762\u79ef\u3002\u8212\u9002\u6027\u901a\u8fc7\u7eb5\u5411\u548c\u6a2a\u5411\u52a0\u52a0\u901f\u5ea6\u6307\u6807\u5efa\u6a21\uff0c\u6548\u7387\u901a\u8fc7\u603b\u884c\u7a0b\u65f6\u95f4\u8bc4\u4f30\u3002\u7ed3\u5408\u8fd9\u4e9b\u6807\u51c6\u6784\u5efa\u7efc\u5408\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u7528PSO\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u65b9\u6cd5\u5728\u771f\u5b9e\u4ea4\u901a\u6761\u4ef6\u4e0b\u7684\u5b9e\u9a8c\u4e2d\u6210\u529f\u9a8c\u8bc1\uff0c\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\uff0c\u4ee5\u53ca\u57fa\u4e8e\u771f\u5b9e\u4ea4\u901a\u6570\u636e\u7684\u4eff\u771f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u591a\u6807\u51c6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02352", "pdf": "https://arxiv.org/pdf/2509.02352", "abs": "https://arxiv.org/abs/2509.02352", "authors": ["Kexin Chen", "Yijie Mao", "Wonjae Shin", "Bruno Clerckx", "Christos Masouros"], "title": "Interference Management for Integrated Sensing and Communications: A Multiple Access Perspective", "categories": ["eess.SP"], "comment": null, "summary": "The integrated sensing and communication (ISAC) technique has been considered\na key enabler for 6G radio access networks. ISAC fulfills a brand new paradigm\nshift in wireless networks via the seamless interplay between communication and\nsensing within a unified network. However, the tight integration of these\nfunctionalities inevitably gives rise to various types of interference, posing\nsignificant challenges to existing ISAC waveform designs and rendering\ninterference management a critical concern. Inspired by the development\ntrajectory of wireless communications, different multiple access (MA)\ntechniques, such as orthogonal multiple access (OMA), space-division multiple\naccess (SDMA), and more recently, non-orthogonal multiple access (NOMA) and\nrate-splitting multiple access (RSMA), have been demonstrated to play a pivotal\nrole in efficiently utilizing limited spectrum resources, designing ISAC\nwaveforms, as well as managing inter-user interference and inter-functionality\ninterference in ISAC. Notably, the interplay between MA and ISAC presents\nmutually beneficial integration. On the one hand, ISAC helps MA techniques\nbetter exploit their interference management capability beyond the\ncommunication-only networks. On the other hand, different MA techniques serve\nas promising solutions for inter-functionality and inter-user interference\nmanagement in ISAC. In this paper, we deliver the first comprehensive tutorial\nof MA techniques in ISAC networks. Specifically, we illustrate the fundamental\nprinciples of ISAC, classify the diverse types of interference in different\nISAC systems, and compare MA-assisted ISAC designs, highlighting their\nrespective advantages and limitations. Moreover, we provide an outlook on the\nemerging applications and future research directions of different MA-assisted\nISAC.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u5740\u63a5\u5165\uff08MA\uff09\u6280\u672f\u5728\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e0d\u540cMA\u6280\u672f\u5728\u5e72\u6270\u7ba1\u7406\u548c\u6ce2\u5f62\u8bbe\u8ba1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "ISAC\u4f5c\u4e3a6G\u65e0\u7ebf\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u901a\u4fe1\u4e0e\u4f20\u611f\u7684\u6df1\u5ea6\u878d\u5408\u5e26\u6765\u4e86\u5e72\u6270\u7ba1\u7406\u7684\u6311\u6218\u3002MA\u6280\u672f\u56e0\u5176\u5728\u5e72\u6270\u7ba1\u7406\u548c\u9891\u8c31\u5229\u7528\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u88ab\u8ba4\u4e3a\u53ef\u4ee5\u4f18\u5316ISAC\u8bbe\u8ba1\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5bf9\u4e0d\u540cMA\u6280\u672f\uff08\u5982OMA\u3001SDMA\u3001NOMA\u3001RSMA\uff09\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u5206\u6790\u5176\u5728ISAC\u6ce2\u5f62\u8bbe\u8ba1\u548c\u5e72\u6270\u7ba1\u7406\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "result": "MA\u6280\u672f\u4e0eISAC\u7684\u7ed3\u5408\u5177\u6709\u4e92\u60e0\u6027\uff0cISAC\u6269\u5c55\u4e86MA\u6280\u672f\u7684\u5e94\u7528\u573a\u666f\uff0c\u800cMA\u6280\u672f\u5219\u4e3aISAC\u4e2d\u7684\u5e72\u6270\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MA\u6280\u672f\u5728ISAC\u7f51\u7edc\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u5176\u65b0\u5174\u5e94\u7528\u548c\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2509.01297", "pdf": "https://arxiv.org/pdf/2509.01297", "abs": "https://arxiv.org/abs/2509.01297", "authors": ["Seonsoo Kim", "Jun-Gill Kang", "Taehong Kim", "Seongil Hong"], "title": "Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning", "categories": ["cs.RO"], "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025 Project\n  Page: seonsoo-p1.github.io/DMCM", "summary": "In meta-learning and its downstream tasks, many methods rely on implicit\nadaptation to task variations, where multiple factors are mixed together in a\nsingle entangled representation. This makes it difficult to interpret which\nfactors drive performance and can hinder generalization. In this work, we\nintroduce a disentangled multi-context meta-learning framework that explicitly\nassigns each task factor to a distinct context vector. By decoupling these\nvariations, our approach improves robustness through deeper task understanding\nand enhances generalization by enabling context vector sharing across tasks\nwith shared factors. We evaluate our approach in two domains. First, on a\nsinusoidal regression task, our model outperforms baselines on\nout-of-distribution tasks and generalizes to unseen sine functions by sharing\ncontext vectors associated with shared amplitudes or phase shifts. Second, in a\nquadruped robot locomotion task, we disentangle the robot-specific properties\nand the characteristics of the terrain in the robot dynamics model. By\ntransferring disentangled context vectors acquired from the dynamics model into\nreinforcement learning, the resulting policy achieves improved robustness under\nout-of-distribution conditions, surpassing the baselines that rely on a single\nunified context. Furthermore, by effectively sharing context, our model enables\nsuccessful sim-to-real policy transfer to challenging terrains with\nout-of-distribution robot-specific properties, using just 20 seconds of real\ndata from flat terrain, a result not achievable with single-task adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u660e\u786e\u5206\u79bb\u4efb\u52a1\u56e0\u7d20\u5230\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c06\u4efb\u52a1\u4e2d\u7684\u591a\u56e0\u7d20\u6df7\u5408\u5728\u5355\u4e00\u7ea0\u7f20\u8868\u793a\u4e2d\uff0c\u5bfc\u81f4\u96be\u4ee5\u89e3\u91ca\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\u5e76\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u89e3\u8026\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06\u6bcf\u4e2a\u4efb\u52a1\u56e0\u7d20\u5206\u914d\u5230\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u4e2d\uff0c\u89e3\u8026\u4efb\u52a1\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u5177\u6709\u5171\u540c\u56e0\u7d20\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6b63\u5f26\u56de\u5f52\u4efb\u52a1\u548c\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u5728\u56db\u8db3\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5feb\u901f\u7b56\u7565\u8fc1\u79fb\uff0c\u4ec5\u752820\u79d2\u7684\u5e73\u5766\u5730\u5f62\u6570\u636e\u3002", "conclusion": "\u89e3\u8026\u591a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u4efb\u52a1\u5171\u4eab\u548c\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2509.02442", "pdf": "https://arxiv.org/pdf/2509.02442", "abs": "https://arxiv.org/abs/2509.02442", "authors": ["Chen Sun", "Wenqi Zhang", "Bizhu Wang", "Xiaodong Xu", "Chau Yuen", "Yan Zhang", "Ping Zhang"], "title": "Know What, Know Why: Semantic Hazard Communication for Intelligent V2X Systems", "categories": ["eess.SP", "cs.HC"], "comment": null, "summary": "In current vehicle-to-everything (V2X) communication systems, roadside units\n(RSUs) broadcast brief warning messages that alert nearby vehicles to avoid\npotential hazards. However, these messages lack contextual information on why a\nwarning is issued, leading to excessive caution or inefficient driving\nbehaviors. To avoid such a situation, we propose a semantic-enhanced and\nexplainable V2X (SEE-V2X) system. In the proposed system, RSUs equipped with\nsmart cameras detect obstructions and transmit context-aware messages to\nvehicles. By understanding both what the hazard is and why it occurs, drivers\ncan make more intelligent decisions based on their specific driving situation.\nFurthermore, through a real-field demonstration, we show the new \"see-through\"\nfeature in the proposed system, which enables drivers to visualize hidden\npedestrians behind obstacles. We also perform simulations to compare\ntraditional V2X with SEE-V2X under different traffic conditions. The results\nshow that SEE-V2X significantly improves traffic efficiency and reduces\nunnecessary deceleration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u589e\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684V2X\u7cfb\u7edf\uff08SEE-V2X\uff09\uff0c\u901a\u8fc7\u667a\u80fd\u6444\u50cf\u5934\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8b66\u544a\u4fe1\u606f\uff0c\u5e2e\u52a9\u9a7e\u9a76\u5458\u505a\u51fa\u66f4\u667a\u80fd\u7684\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\u3002", "motivation": "\u4f20\u7edfV2X\u7cfb\u7edf\u4e2d\uff0c\u8def\u8fb9\u5355\u5143\uff08RSUs\uff09\u5e7f\u64ad\u7684\u8b66\u544a\u4fe1\u606f\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u9a7e\u9a76\u5458\u8fc7\u5ea6\u8c28\u614e\u6216\u9a7e\u9a76\u884c\u4e3a\u4f4e\u6548\u3002", "method": "\u5728SEE-V2X\u7cfb\u7edf\u4e2d\uff0cRSUs\u914d\u5907\u667a\u80fd\u6444\u50cf\u5934\u68c0\u6d4b\u969c\u788d\u7269\uff0c\u5e76\u4f20\u8f93\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89c6\u5316\u529f\u80fd\u5e2e\u52a9\u9a7e\u9a76\u5458\u770b\u6e05\u9690\u85cf\u7684\u884c\u4eba\u3002", "result": "\u5b9e\u9645\u6f14\u793a\u548c\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cSEE-V2X\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u51cf\u901f\u3002", "conclusion": "SEE-V2X\u901a\u8fc7\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u4f18\u5316\u4e86\u9a7e\u9a76\u51b3\u7b56\uff0c\u63d0\u5347\u4e86\u4ea4\u901a\u7cfb\u7edf\u7684\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2509.01364", "pdf": "https://arxiv.org/pdf/2509.01364", "abs": "https://arxiv.org/abs/2509.01364", "authors": ["Peiran Liu", "Qiang Zhang", "Daojie Peng", "Lingfeng Zhang", "Yihao Qin", "Hang Zhou", "Jun Ma", "Renjing Xu", "Yiding Ji"], "title": "TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Object Navigation (ObjectNav) has made great progress with large language\nmodels (LLMs), but still faces challenges in memory management, especially in\nlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a\nnew framework that leverages topological structures as spatial memory. By\nbuilding and updating a topological graph that captures scene connections,\nadjacency, and semantic meaning, TopoNav helps agents accumulate spatial\nknowledge over time, retrieve key information, and reason effectively toward\ndistant goals. Our experiments show that TopoNav achieves state-of-the-art\nperformance on benchmark ObjectNav datasets, with higher success rates and more\nefficient paths. It particularly excels in diverse and complex environments, as\nit connects temporary visual inputs with lasting spatial understanding.", "AI": {"tldr": "TopoNav\u662f\u4e00\u79cd\u5229\u7528\u62d3\u6251\u7ed3\u6784\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u4f53\u5bfc\u822a\u4efb\u52a1\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7269\u4f53\u5bfc\u822a\uff08ObjectNav\uff09\u5728\u957f\u7a0b\u4efb\u52a1\u548c\u52a8\u6001\u573a\u666f\u4e2d\u5b58\u5728\u5185\u5b58\u7ba1\u7406\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7a7a\u95f4\u8bb0\u5fc6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTopoNav\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u548c\u66f4\u65b0\u62d3\u6251\u56fe\u6765\u6355\u6349\u573a\u666f\u8fde\u63a5\u3001\u90bb\u8fd1\u5173\u7cfb\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5e2e\u52a9\u4ee3\u7406\u79ef\u7d2f\u7a7a\u95f4\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTopoNav\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "TopoNav\u901a\u8fc7\u5c06\u4e34\u65f6\u89c6\u89c9\u8f93\u5165\u4e0e\u6301\u4e45\u7a7a\u95f4\u7406\u89e3\u7ed3\u5408\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u6837\u5316\u548c\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.02540", "pdf": "https://arxiv.org/pdf/2509.02540", "abs": "https://arxiv.org/abs/2509.02540", "authors": ["Halvin Yang", "Sangarapillai Lambotharan", "Mahsa Derakhshani", "Lajos Hanzo"], "title": "LLM-Enhanced Space-Air-Ground-Sea Integrated Networks", "categories": ["eess.SP"], "comment": "6 figures, 7 pages, magazine", "summary": "The space-air-ground-sea integrated networking (SAGSIN) concept promises\nseamless global multimedia connectivity, yet two obstacles still limit its\npractical deployment. Firstly, high-velocity satellites, aerial relays and\nsea-surface platforms suffer from obsolete channel state information (CSI),\nundermining feedback-based adaptation. Secondly, data-rate disparity across the\nprotocol stack is extreme: terabit optical links in space coexist with kilobit\nacoustic under-water links. This article shows that a single large language\nmodel (LLM) backbone, trained jointly on radio, optical and acoustic traces,\ncan provide a unified, data-driven adaptation layer that addresses both rapid\nCSI ageing and severe bandwidth disparity across the SAGSIN protocol stack.\nExplicitly, an LLM-based long-range channel predictor forecasts the strongest\ndelay-Doppler components several coherence intervals ahead, facilitating\nnear-capacity reception despite violent channel fluctuations. Furthermore, our\nLLM-based semantic encoder turns raw sensor payloads into task-oriented tokens.\nThis substantially reduces the SNR required for high-fidelity image delivery in\na coastal underwater link, circumventing the data rate limitation by semantic\ncommunications. Inclusion of these tools creates a medium-agnostic adaptation\nlayer that spans radio, optical and acoustic channels. We conclude with\npromising open research directions in on-device model compression, multimodal\nfidelity control, cross-layer resource orchestration and trustworthy operation,\ncharting a path from laboratory prototypes to field deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.01450", "pdf": "https://arxiv.org/pdf/2509.01450", "abs": "https://arxiv.org/abs/2509.01450", "authors": ["Ane San Martin", "Michael Hagenow", "Julie Shah", "Johan Kildal", "Elena Lazkano"], "title": "Analyzing Reluctance to Ask for Help When Cooperating With Robots: Insights to Integrate Artificial Agents in HRC", "categories": ["cs.RO", "cs.HC"], "comment": "8 pages, 5 figures. Accepted for IEEE RO-MAN 2025", "summary": "As robot technology advances, collaboration between humans and robots will\nbecome more prevalent in industrial tasks. When humans run into issues in such\nscenarios, a likely future involves relying on artificial agents or robots for\naid. This study identifies key aspects for the design of future user-assisting\nagents. We analyze quantitative and qualitative data from a user study\nexamining the impact of on-demand assistance received from a remote human in a\nhuman-robot collaboration (HRC) assembly task. We study scenarios in which\nusers require help and we assess their experiences in requesting and receiving\nassistance. Additionally, we investigate participants' perceptions of future\nnon-human assisting agents and whether assistance should be on-demand or\nunsolicited. Through a user study, we analyze the impact that such design\ndecisions (human or artificial assistant, on-demand or unsolicited help) can\nhave on elicited emotional responses, productivity, and preferences of humans\nengaged in HRC tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u63a5\u6536\u8fdc\u7a0b\u4eba\u7c7b\u534f\u52a9\u7684\u8bbe\u8ba1\u5173\u952e\u70b9\uff0c\u5e76\u8bc4\u4f30\u4e86\u7528\u6237\u5bf9\u672a\u6765\u975e\u4eba\u7c7b\u8f85\u52a9\u4ee3\u7406\u7684\u611f\u77e5\u4ee5\u53ca\u5bf9\u6309\u9700\u6216\u4e3b\u52a8\u5e2e\u52a9\u7684\u504f\u597d\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4eba\u673a\u534f\u4f5c\u5728\u5de5\u4e1a\u4efb\u52a1\u4e2d\u5c06\u66f4\u52a0\u666e\u904d\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u672a\u6765\u7528\u6237\u8f85\u52a9\u4ee3\u7406\u8bbe\u8ba1\u7684\u5173\u952e\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u6790\u5b9a\u91cf\u548c\u5b9a\u6027\u6570\u636e\uff0c\u8003\u5bdf\u5728HRC\u7ec4\u88c5\u4efb\u52a1\u4e2d\u63a5\u6536\u8fdc\u7a0b\u4eba\u7c7b\u534f\u52a9\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u7528\u6237\u8bf7\u6c42\u548c\u63a5\u6536\u5e2e\u52a9\u7684\u7ecf\u5386\uff0c\u4ee5\u53ca\u4ed6\u4eec\u5bf9\u975e\u4eba\u7c7b\u4ee3\u7406\u548c\u5e2e\u52a9\u65b9\u5f0f\u7684\u611f\u77e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bbe\u8ba1\u548c\u51b3\u7b56\uff08\u4eba\u7c7b\u6216\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u3001\u6309\u9700\u6216\u4e3b\u52a8\u5e2e\u52a9\uff09\u4f1a\u5f71\u54cd\u7528\u6237\u7684\u60c5\u7eea\u53cd\u5e94\u3001\u751f\u4ea7\u529b\u548c\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u8bbe\u8ba1\u7528\u6237\u8f85\u52a9\u4ee3\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u6309\u9700\u5e2e\u52a9\u548c\u975e\u4eba\u7c7b\u4ee3\u7406\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2509.00268", "pdf": "https://arxiv.org/pdf/2509.00268", "abs": "https://arxiv.org/abs/2509.00268", "authors": ["Nader Shakibay Senobari"], "title": "Revealing Hidden Precursors to Earthquakes via a Stress-Sensitive Transformation of Seismic Noise", "categories": ["physics.geo-ph", "cs.AI", "eess.SP", "86A15 (Seismology), 62M10 (Time series, stochastic processes)", "I.5.4; I.2.6"], "comment": "20 pages, 7 figures. Github code included. Submitted to Science\n  Advances", "summary": "Earthquake prediction has long been one of the most elusive challenges in\nscience. Laboratory experiments and simulations suggest that failure precursors\nshould exist, yet reliable signals have remained unobserved in real-world\nseismic records, leaving open the question of whether they are absent in nature\nor simply hidden within noise. Here we introduce a stress-sensitive\nfrequency-domain transformation that tracks energy differences between adjacent\nfrequency bands, isolating subtle spectral changes linked to evolving shear and\nnormal stress. Applied to both laboratory acoustic emission data and seismic\nrecords from seven major earthquakes (Mw 5.9-9.0), including the 2011 Tohoku\nand 2023 Turkey-Syria events, the transform consistently reveals precursory\nsignatures, arc-like trajectories and accelerations toward extrema, emerging\nhours to days before rupture. These features are robust across diverse tectonic\nsettings, from induced seismicity and volcanic collapse to continental\nstrike-slip and subduction megathrust earthquakes. Our findings demonstrate\nthat hidden precursors are indeed encoded in ambient seismic noise, offering a\npathway toward real-time fault monitoring and actionable short-term earthquake\nforecasting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5730\u9707\u566a\u58f0\u4e2d\u63d0\u53d6\u5730\u9707\u524d\u5146\u4fe1\u53f7\uff0c\u5e76\u5728\u5b9e\u9a8c\u6570\u636e\u548c\u5b9e\u9645\u5730\u9707\u8bb0\u5f55\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5730\u9707\u9884\u6d4b\u4e00\u76f4\u662f\u79d1\u5b66\u754c\u7684\u91cd\u8981\u6311\u6218\uff0c\u5c3d\u7ba1\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u8868\u660e\u5b58\u5728\u524d\u5146\u4fe1\u53f7\uff0c\u4f46\u5728\u5b9e\u9645\u5730\u9707\u8bb0\u5f55\u4e2d\u5c1a\u672a\u53ef\u9760\u89c2\u5bdf\u5230\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u9690\u85cf\u7684\u524d\u5146\u4fe1\u53f7\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u5bf9\u538b\u529b\u654f\u611f\u7684\u9891\u57df\u53d8\u6362\u65b9\u6cd5\uff0c\u8ffd\u8e2a\u76f8\u90bb\u9891\u5e26\u4e4b\u95f4\u7684\u80fd\u91cf\u5dee\u5f02\uff0c\u4ece\u800c\u8bc6\u522b\u4e0e\u526a\u5e94\u529b\u548c\u6b63\u5e94\u529b\u53d8\u5316\u76f8\u5173\u7684\u5fae\u5999\u9891\u8c31\u53d8\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u6570\u636e\u548c\u4e03\u6b21\u5927\u5730\u9707\u7684\u8bb0\u5f55\u4e2d\u5747\u68c0\u6d4b\u5230\u4e86\u524d\u5146\u7279\u5f81\uff0c\u8868\u73b0\u4e3a\u5f27\u7ebf\u8f68\u8ff9\u548c\u52a0\u901f\u8d8b\u52bf\uff0c\u51fa\u73b0\u5728\u7834\u88c2\u524d\u51e0\u5c0f\u65f6\u5230\u51e0\u5929\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5730\u9707\u524d\u5146\u4fe1\u53f7\u786e\u5b9e\u9690\u85cf\u5728\u73af\u5883\u5730\u9707\u566a\u58f0\u4e2d\uff0c\u8fd9\u4e3a\u5b9e\u65f6\u65ad\u5c42\u76d1\u6d4b\u548c\u77ed\u671f\u5730\u9707\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.01547", "pdf": "https://arxiv.org/pdf/2509.01547", "abs": "https://arxiv.org/abs/2509.01547", "authors": ["Fan Zhu", "Yifan Zhao", "Ziyu Chen", "Biao Yu", "Hui Zhu"], "title": "FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field", "categories": ["cs.RO"], "comment": "ICRA 2025", "summary": "Visual SLAM has regained attention due to its ability to provide perceptual\ncapabilities and simulation test data for Embodied AI. However, traditional\nSLAM methods struggle to meet the demands of high-quality scene reconstruction,\nand Gaussian SLAM systems, despite their rapid rendering and high-quality\nmapping capabilities, lack effective pose optimization methods and face\nchallenges in geometric reconstruction. To address these issues, we introduce\nFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the\nscene representation to enhance geometric mapping performance. After initial\npose estimation, we apply global adjustment to optimize camera poses and sparse\npoint cloud, ensuring robust tracking of our approach. Additionally, we\nmaintain a globally consistent opacity radiance field based on 3D Gaussians and\nintroduce depth distortion and normal consistency terms to refine the scene\nrepresentation. Furthermore, after constructing tetrahedral grids, we identify\nlevel sets to directly extract surfaces from 3D Gaussians. Results across\nvarious real-world and large-scale synthetic datasets demonstrate that our\nmethod achieves state-of-the-art tracking accuracy and mapping performance.", "AI": {"tldr": "FGO-SLAM\u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65afSLAM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u548c\u5168\u5c40\u8c03\u6574\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u573a\u666f\u91cd\u5efa\u7684\u51e0\u4f55\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSLAM\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9ad8\u8d28\u91cf\u573a\u666f\u91cd\u5efa\u9700\u6c42\uff0c\u9ad8\u65afSLAM\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u4f4d\u59ff\u4f18\u5316\u65b9\u6cd5\uff0c\u51e0\u4f55\u91cd\u5efa\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e0d\u900f\u660e\u5ea6\u8f90\u5c04\u573a\u8868\u793a\u573a\u666f\uff0c\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u548c\u7a00\u758f\u70b9\u4e91\uff0c\u5f15\u5165\u6df1\u5ea6\u626d\u66f2\u548c\u6cd5\u5411\u4e00\u81f4\u6027\u9879\uff0c\u57fa\u4e8e\u56db\u9762\u4f53\u7f51\u683c\u63d0\u53d6\u8868\u9762\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6620\u5c04\u6027\u80fd\u3002", "conclusion": "FGO-SLAM\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u7684\u51e0\u4f55\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2509.00608", "pdf": "https://arxiv.org/pdf/2509.00608", "abs": "https://arxiv.org/abs/2509.00608", "authors": ["Siyu Xiao", "Guohui Ren", "Tianhao Mao", "Yuqiao Chen", "YiAn Liu", "Junjie Wang", "Kai Tang", "Xindi Zhao", "Zhijian Yu", "Shuang Liu", "Tupei Chen", "Yang Liu"], "title": "Realization of Precise Perforating Using Dynamic Threshold and Physical Plausibility Algorithm for Self-Locating Perforating in Oil and Gas Wells", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Accurate depth measurement is essential for optimizing oil and gas resource\ndevelopment, as it directly impacts production efficiency. However, achieving\nprecise depth and perforating at the correct location remains a significant\nchallenge due to field operational constraints and equipment limitations. In\nthis work, we propose the Dynamic Threshold and Physical Plausibility Depth\nMeasurement and Perforation Control (DTPPMP) system, a solution integrated into\nperforating guns that enables real-time, precise depth measurement and\nperforation at designated perforating intervals. The system autonomously\nsamples, processes and identifies signals from a casing collar locator (CCL) in\nsitu within oil and gas wells. Casing collar identification is achieved using a\nlightweight dynamic threshold and physical plausibility algorithm deployed on\nan embedded platform, which serves as the system's processor. Field tests\nconducted in an actual oil well in Sichuan, China, demonstrated the DTPPMP's\nability to accurately identify casing collar signals, measure depths, and\neffectively perforate at designated perforating intervals in real-time. The\nsystem achieved a perforation variation of less than the length of a single\nperforating interval and a F1 score of 98.6% for casing collar identification.\nThese results provide valuable recommendations for advancing automation and\nintelligence in future perforation operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86DTPPMP\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u7cbe\u786e\u6d4b\u91cf\u6cb9\u6c14\u4e95\u6df1\u5ea6\u5e76\u5728\u6307\u5b9a\u4f4d\u7f6e\u7a7f\u5b54\uff0c\u901a\u8fc7\u52a8\u6001\u9608\u503c\u548c\u7269\u7406\u5408\u7406\u6027\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u73b0\u573a\u6d4b\u8bd5\u6548\u679c\u4f18\u5f02\u3002", "motivation": "\u6cb9\u6c14\u8d44\u6e90\u5f00\u53d1\u4e2d\u7cbe\u786e\u6df1\u5ea6\u6d4b\u91cf\u548c\u7a7f\u5b54\u4f4d\u7f6e\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u8bbe\u5907\u548c\u64cd\u4f5c\u9650\u5236\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u96c6\u6210\u4e8e\u7a7f\u5b54\u67aa\u7684DTPPMP\u7cfb\u7edf\uff0c\u91c7\u7528\u52a8\u6001\u9608\u503c\u548c\u7269\u7406\u5408\u7406\u6027\u7b97\u6cd5\u5b9e\u65f6\u5904\u7406CCL\u4fe1\u53f7\uff0c\u5d4c\u5165\u5f0f\u8ba1\u7b97\u5e73\u53f0\u5b9e\u73b0\u81ea\u4e3b\u64cd\u4f5c\u3002", "result": "\u56db\u5ddd\u6cb9\u7530\u73b0\u573a\u6d4b\u8bd5\u663e\u793a\u7cfb\u7edf\u7a7f\u5b54\u8bef\u5dee\u5c0f\u4e8e\u5355\u5b54\u95f4\u9694\u957f\u5ea6\uff0cCCL\u4fe1\u53f7\u8bc6\u522bF1\u5206\u6570\u8fbe98.6%\u3002", "conclusion": "DTPPMP\u7cfb\u7edf\u4e3a\u672a\u6765\u7a7f\u5b54\u4f5c\u4e1a\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01583", "pdf": "https://arxiv.org/pdf/2509.01583", "abs": "https://arxiv.org/abs/2509.01583", "authors": ["Thomas Jantos", "Stephan Weiss", "Jan Steinbrener"], "title": "Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for Object-relative State Estimation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Deep Learning (DL) has become essential in various robotics applications due\nto excelling at processing raw sensory data to extract task specific\ninformation from semantic objects. For example, vision-based object-relative\nnavigation relies on a DL-based 6D object pose predictor to provide the\nrelative pose between the object and the robot as measurements to the robot's\nstate estimator. Accurately knowing the uncertainty inherent in such Deep\nNeural Network (DNN) based measurements is essential for probabilistic state\nestimators subsequently guiding the robot's tasks. Thus, in this letter, we\nshow that we can extend any existing DL-based object-relative pose predictor\nfor aleatoric uncertainty inference simply by including two multi-layer\nperceptrons detached from the translational and rotational part of the DL\npredictor. This allows for efficient training while freezing the existing\npre-trained predictor. We then use the inferred 6D pose and its uncertainty as\na measurement and corresponding noise covariance matrix in an extended Kalman\nfilter (EKF). Our approach induces minimal computational overhead such that the\nstate estimator can be deployed on edge devices while benefiting from the\ndynamically inferred measurement uncertainty. This increases the performance of\nthe object-relative state estimation task compared to a fix-covariance\napproach. We conduct evaluations on synthetic data and real-world data to\nunderline the benefits of aleatoric uncertainty inference for the\nobject-relative state estimation task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u591a\u5c42\u611f\u77e5\u5668\u6765\u6269\u5c55\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u57fa\u4f4d\u59ff\u9884\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u63a8\u65ad\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u5347\u7269\u4f53\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u5904\u7406\u539f\u59cb\u611f\u5b98\u6570\u636e\uff0c\u4f46\u51c6\u786e\u5730\u4e86\u89e3\u5176\u4e0d\u786e\u5b9a\u6027\u5bf9\u6982\u7387\u72b6\u6001\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\u4ee5\u63a8\u65ad\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u4e0e\u4f4d\u59ff\u9884\u6d4b\u5668\u5206\u79bb\u7684\u591a\u5c42\u611f\u77e5\u5668\u6765\u63a8\u65ad\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u63a8\u65ad\u4e0d\u786e\u5b9a\u6027\u6bd4\u56fa\u5b9a\u534f\u65b9\u5dee\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7269\u4f53\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2509.00894", "pdf": "https://arxiv.org/pdf/2509.00894", "abs": "https://arxiv.org/abs/2509.00894", "authors": ["Yaodong Ma", "Kai Liu", "Lipeng Zhu", "Yanming Liu", "Yanbo Zhu", "Daniel Benevides da Costa"], "title": "Movable Antenna-Enhanced Secure Communication: Opportunities, Challenges, and Solutions", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "7 pages", "summary": "The broadcast nature of wireless communication renders it inherently\nvulnerable to security threats such as jamming and eavesdropping. While\ntraditional array beamforming techniques help to mitigate these threats, they\nusually incur high hardware and processing costs, particularly in large-scale\narrays with fixed-position antennas (FPAs). In contrast, movable antenna (MA)\narrays can fully exploit the channel variation in spatial regions by enabling\nflexible antenna movement, which has emerged as a promising technology for\nsecure communications. This article provides a magazine-type overview of\nMA-aided secure communications. Specifically, we first illuminate the promising\napplication scenarios for MA-enhanced secure communication systems. Then, we\nexamine the security advantages of MAs over conventional FPA systems,\nfundamentally stemming from their ability to adjust channel correlations\nbetween legitimate users, eavesdroppers, and jammers. Furthermore, we discuss\nimportant technical challenges and their potential solutions related to MA\nhardware architecture, channel acquisition, and antenna position optimization\nto realize secure transmissions. Finally, several promising directions for\nMA-aided secure communications are presented to inspire future research.", "AI": {"tldr": "\u6587\u7ae0\u6982\u8ff0\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u5728\u5b89\u5168\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\uff0c\u5bf9\u6bd4\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\uff08FPA\uff09\u7684\u6280\u672f\u4f18\u52bf\uff0c\u5e76\u63a2\u8ba8\u4e86\u76f8\u5173\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7684\u5e7f\u64ad\u7279\u6027\u4f7f\u5176\u6613\u53d7\u5e72\u6270\u548c\u7a83\u542c\u7b49\u5b89\u5168\u5a01\u80c1\uff0c\u4f20\u7edf\u9635\u5217\u6ce2\u675f\u8d4b\u5f62\u6280\u672f\u6210\u672c\u9ad8\u6602\uff0c\u800cMA\u6280\u672f\u901a\u8fc7\u7075\u6d3b\u5929\u7ebf\u79fb\u52a8\u53ef\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u4e86\u7efc\u8ff0\u5f62\u5f0f\uff0c\u5206\u6790\u4e86MA\u5728\u5b89\u5168\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u573a\u666f\u3001\u6280\u672f\u4f18\u52bf\uff08\u5982\u8c03\u6574\u4fe1\u9053\u76f8\u5173\u6027\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u786c\u4ef6\u67b6\u6784\u3001\u4fe1\u9053\u83b7\u53d6\u548c\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u7b49\u6280\u672f\u6311\u6218\u53ca\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "result": "MA\u6280\u672f\u76f8\u8f83FPA\u7cfb\u7edf\u5728\u5b89\u5168\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u8c03\u6574\u4fe1\u9053\u76f8\u5173\u6027\u65b9\u9762\uff0c\u4f46\u5b9e\u73b0\u5b89\u5168\u4f20\u8f93\u4ecd\u9700\u89e3\u51b3\u786c\u4ef6\u548c\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "MA\u6280\u672f\u4e3a\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2509.01611", "pdf": "https://arxiv.org/pdf/2509.01611", "abs": "https://arxiv.org/abs/2509.01611", "authors": ["Ziteng Gao", "Jiaqi Qu", "Chaoyu Chen"], "title": "A Hybrid Input based Deep Reinforcement Learning for Lane Change Decision-Making of Autonomous Vehicle", "categories": ["cs.RO"], "comment": null, "summary": "Lane change decision-making for autonomous vehicles is a complex but\nhigh-reward behavior. In this paper, we propose a hybrid input based deep\nreinforcement learning (DRL) algorithm, which realizes abstract lane change\ndecisions and lane change actions for autonomous vehicles within traffic flow.\nFirstly, a surrounding vehicles trajectory prediction method is proposed to\nreduce the risk of future behavior of surrounding vehicles to ego vehicle, and\nthe prediction results are input into the reinforcement learning model as\nadditional information. Secondly, to comprehensively leverage environmental\ninformation, the model extracts feature from high-dimensional images and\nlow-dimensional sensor data simultaneously. The fusion of surrounding vehicle\ntrajectory prediction and multi-modal information are used as state space of\nreinforcement learning to improve the rationality of lane change decision.\nFinally, we integrate reinforcement learning macro decisions with end-to-end\nvehicle control to achieve a holistic lane change process. Experiments were\nconducted within the CARLA simulator, and the results demonstrated that the\nutilization of a hybrid state space significantly enhances the safety of\nvehicle lane change decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u8f93\u5165\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6362\u9053\u51b3\u7b56\uff0c\u7ed3\u5408\u8f68\u8ff9\u9884\u6d4b\u548c\u591a\u6a21\u6001\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6362\u9053\u51b3\u7b56\u590d\u6742\u4f46\u6709\u9ad8\u56de\u62a5\uff0c\u9700\u63d0\u5347\u5b89\u5168\u6027\u548c\u5408\u7406\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8f93\u5165\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u8f68\u8ff9\u9884\u6d4b\u548c\u9ad8\u4f4e\u9891\u6570\u636e\u7279\u5f81\u63d0\u53d6\uff0c\u6574\u5408\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u72b6\u6001\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u6362\u9053\u51b3\u7b56\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u8f93\u5165\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6362\u9053\u7684\u5408\u7406\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.00901", "pdf": "https://arxiv.org/pdf/2509.00901", "abs": "https://arxiv.org/abs/2509.00901", "authors": ["Yaodong Ma", "Kai Liu", "Yanming Liu", "Lipeng Zhu"], "title": "Movable Antenna Empowered Secure Near-Field MIMO Communications", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": "13 pages", "summary": "This paper investigates movable antenna (MA) empowered secure transmission in\nnear-field multiple-input multiple-output (MIMO) communication systems, where\nthe base station (BS) equipped with an MA array transmits confidential\ninformation to a legitimate user under the threat of a potential eavesdropper.\nTo enhance physical layer security (PLS) of the considered system, we aim to\nmaximize the secrecy rate by jointly designing the hybrid digital and analog\nbeamformers, as well as the positions of MAs at the BS. To solve the formulated\nnon-convex problem with highly coupled variables, an alternating optimization\n(AO)-based algorithm is introduced by decoupling the original problem into two\nseparate subproblems. Specifically, for the subproblem of designing hybrid\nbeamformers, a semi-closed-form solution for the fully-digital beamformer is\nfirst derived by a weighted minimum mean-square error (WMMSE)-based algorithm.\nSubsequently, the digital and analog beamformers are determined by\napproximating the fully-digital beamformer through the manifold optimization\n(MO) technique. For the MA positions design subproblem, we utilize the\nmajorization-minimization (MM) algorithm to iteratively optimize each MA's\nposition while keeping others fixed. Extensive simulation results validate the\nconsiderable benefits of the proposed MA-aided near-field beam focusing\napproach in enhancing security performance compared to the traditional\nfar-field and/or the fixed position antenna (FPA)-based systems. In addition,\nthe proposed scheme can realize secure transmission even if the eavesdropper is\nlocated in the same direction as the user and closer to the BS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u5728\u8fd1\u573aMIMO\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u4f20\u8f93\uff0c\u901a\u8fc7\u4f18\u5316\u6df7\u5408\u6570\u5b57\u548c\u6a21\u62df\u6ce2\u675f\u5f62\u6210\u5668\u4ee5\u53ca\u5929\u7ebf\u4f4d\u7f6e\uff0c\u63d0\u5347\u7269\u7406\u5c42\u5b89\u5168\u6027\u3002", "motivation": "\u8fd1\u573aMIMO\u7cfb\u7edf\u4e2d\uff0c\u7269\u7406\u5c42\u5b89\u5168\u53d7\u5230\u7a83\u542c\u8005\u5a01\u80c1\uff0c\u9700\u901a\u8fc7\u53ef\u79fb\u52a8\u5929\u7ebf\u548c\u6ce2\u675f\u5f62\u6210\u6280\u672f\u63d0\u5347\u4fdd\u5bc6\u4f20\u8f93\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\u5206\u89e3\u95ee\u9898\uff0c\u4f7f\u7528WMMSE\u548c\u6d41\u5f62\u4f18\u5316\uff08MO\uff09\u8bbe\u8ba1\u6ce2\u675f\u5f62\u6210\u5668\uff0c\u5e76\u901a\u8fc7MM\u7b97\u6cd5\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8fdc\u573a\u6216\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\uff0c\u5e76\u80fd\u5b9e\u73b0\u7a83\u542c\u8005\u4e0e\u7528\u6237\u540c\u65b9\u5411\u65f6\u7684\u5b89\u5168\u4f20\u8f93\u3002", "conclusion": "\u53ef\u79fb\u52a8\u5929\u7ebf\u548c\u8fd1\u573a\u6ce2\u675f\u805a\u7126\u6280\u672f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u7684\u4fdd\u5bc6\u901a\u4fe1\u3002"}}
{"id": "2509.01643", "pdf": "https://arxiv.org/pdf/2509.01643", "abs": "https://arxiv.org/abs/2509.01643", "authors": ["Minja Axelsson"], "title": "Speculative Design of Equitable Robotics: Queer Fictions and Futures", "categories": ["cs.RO", "cs.CY", "cs.HC", "I.2.9; J.5; K.4.2"], "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "This paper examines the speculative topic of equitable robots through an\nexploratory essay format. It focuses specifically on robots by and for LGBTQ+\npopulations. It aims to provoke thought and conversations in the field about\nwhat aspirational queer robotics futures may look like, both in the arts and\nsciences. First, it briefly reviews the state-of-the-art of queer robotics in\nfiction and science, drawing together threads from each. Then, it discusses\nqueering robots through three speculative design proposals for queer robot\nroles: 1) reflecting the queerness of their ''in-group'' queer users, building\nand celebrating ''in-group'' identity, 2) a new kind of queer activism by\nimplementing queer robot identity performance to interact with ''out-group''\nusers, with a goal of reducing bigotry through familiarisation, and 3) a\nnetwork of queer-owned robots, through which the community could reach each\nother, and distribute and access important resources. The paper then questions\nwhether robots should be queered, and what ethical implications this raises.\nFinally, the paper makes suggestions for what aspirational queer robotics\nfutures may look like, and what would be required to get there.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86LGBTQ+\u7fa4\u4f53\u76f8\u5173\u7684\u673a\u5668\u4eba\u516c\u5e73\u6027\uff0c\u901a\u8fc7\u4e09\u4e2a\u8bbe\u8ba1\u63d0\u6848\u5c55\u671b\u4e86\u673a\u5668\u4eba\u5982\u4f55\u652f\u6301\u793e\u7fa4\u8ba4\u540c\u4e0e\u884c\u52a8\u4e3b\u4e49\u3002", "motivation": "\u7814\u7a76LGBTQ+\u793e\u7fa4\u7684\u673a\u5668\u4eba\u5e94\u7528\u6f5c\u529b\uff0c\u63a8\u52a8\u5bf9\u673a\u5668\u4eba\u516c\u5e73\u6027\u4e0e\u4f26\u7406\u7684\u8ba8\u8bba\u3002", "method": "\u7ed3\u5408\u865a\u6784\u4e0e\u79d1\u5b66\u9886\u57df\u7684\u73b0\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u4e09\u4e2a\u2018\u9177\u513f\u673a\u5668\u4eba\u2019\u8bbe\u8ba1\u63d0\u6848\uff1a\u53cd\u6620\u7528\u6237\u8eab\u4efd\u3001\u884c\u52a8\u4e3b\u4e49\u5de5\u5177\u53ca\u793e\u7fa4\u8d44\u6e90\u7f51\u7edc\u3002", "result": "\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5982\u4f55\u6210\u4e3aLGBTQ+\u793e\u7fa4\u7684\u8d4b\u80fd\u5de5\u5177\uff0c\u5e76\u5f15\u53d1\u4e86\u5bf9\u4f26\u7406\u95ee\u9898\u7684\u601d\u8003\u3002", "conclusion": "\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u673a\u5668\u4eba\u9177\u513f\u5316\u7684\u53ef\u884c\u6027\u53ca\u4f26\u7406\u8fb9\u754c\u3002"}}
{"id": "2509.01073", "pdf": "https://arxiv.org/pdf/2509.01073", "abs": "https://arxiv.org/abs/2509.01073", "authors": ["Yuhong Zhang", "Xusheng Zhu", "Yuchen Xu", "ChiaEn Lu", "Hsinyu Shih", "Gert Cauwenberghs", "Tzyy-Ping Jung"], "title": "IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "comment": "Accepted to IEEE EMBS 12th International Conference on Neural\n  Engineering (NER 2025)", "summary": "Electroencephalography (EEG) is a non-invasive method for measuring brain\nactivity with high temporal resolution; however, EEG signals often exhibit low\nsignal-to-noise ratios because of contamination from physiological and\nenvironmental artifacts. One of the major challenges hindering the real-world\ndeployment of brain-computer interfaces (BCIs) involves the frequent occurrence\nof motion-related EEG artifacts. Most prior studies on EEG motion artifact\nremoval rely on single-modality approaches, such as Artifact Subspace\nReconstruction (ASR) and Independent Component Analysis (ICA), without\nincorporating simultaneously recorded modalities like inertial measurement\nunits (IMUs), which directly capture the extent and dynamics of motion. This\nwork proposes a fine-tuned large brain model (LaBraM)-based correlation\nattention mapping method that leverages spatial channel relationships in IMU\ndata to identify motion-related artifacts in EEG signals. The fine-tuned model\ncontains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU\nrecordings for training, just 0.2346\\% of the 2500 hours used to train the base\nmodel. We compare our results against the established ASR-ICA benchmark across\nvarying time scales and motion activities, showing that incorporating IMU\nreference signals significantly improves robustness under diverse motion\nscenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLaBraM\u7684\u6ce8\u610f\u529b\u6620\u5c04\u65b9\u6cd5\uff0c\u5229\u7528IMU\u6570\u636e\u4e2d\u7684\u7a7a\u95f4\u901a\u9053\u5173\u7cfb\u8bc6\u522bEEG\u4fe1\u53f7\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "EEG\u4fe1\u53f7\u5e38\u56e0\u751f\u7406\u548c\u73af\u5883\u4f2a\u5f71\u800c\u566a\u58f0\u8f83\u5927\uff0c\u5c24\u5176\u662f\u8fd0\u52a8\u76f8\u5173\u7684\u4f2a\u5f71\u963b\u788d\u4e86\u8111\u673a\u63a5\u53e3\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528IMU\u6570\u636e\u7a7a\u95f4\u901a\u9053\u5173\u7cfb\u7684LaBraM\u6ce8\u610f\u529b\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684ASR-ICA\u65b9\u6cd5\uff0c\u7ed3\u5408IMU\u53c2\u8003\u4fe1\u53f7\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\uff08EEG-IMU\uff09\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347EEG\u8fd0\u52a8\u4f2a\u5f71\u53bb\u9664\u7684\u6548\u679c\u3002"}}
{"id": "2509.01657", "pdf": "https://arxiv.org/pdf/2509.01657", "abs": "https://arxiv.org/abs/2509.01657", "authors": ["Amber Xie", "Rahul Chand", "Dorsa Sadigh", "Joey Hejna"], "title": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Conference on Robot Learning 2025", "summary": "While large-scale robot datasets have propelled recent progress in imitation\nlearning, learning from smaller task specific datasets remains critical for\ndeployment in new environments and unseen tasks. One such approach to few-shot\nimitation learning is retrieval-based imitation learning, which extracts\nrelevant samples from large, widely available prior datasets to augment a\nlimited demonstration dataset. To determine the relevant data from prior\ndatasets, retrieval-based approaches most commonly calculate a prior data\npoint's minimum distance to a point in the target dataset in latent space.\nWhile retrieval-based methods have shown success using this metric for data\nselection, we demonstrate its equivalence to the limit of a Gaussian kernel\ndensity (KDE) estimate of the target data distribution. This reveals two\nshortcomings of the retrieval rule used in prior work. First, it relies on\nhigh-variance nearest neighbor estimates that are susceptible to noise. Second,\nit does not account for the distribution of prior data when retrieving data. To\naddress these issues, we introduce Importance Weighted Retrieval (IWR), which\nestimates importance weights, or the ratio between the target and prior data\ndistributions for retrieval, using Gaussian KDEs. By considering the\nprobability ratio, IWR seeks to mitigate the bias of previous selection rules,\nand by using reasonable modeling parameters, IWR effectively smooths estimates\nusing all data points. Across both simulation environments and real-world\nevaluations on the Bridge dataset we find that our method, IWR, consistently\nimproves performance of existing retrieval-based methods, despite only\nrequiring minor modifications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u91cd\u8981\u6027\u52a0\u6743\u68c0\u7d22\uff08IWR\uff09\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u68c0\u7d22\u7684\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u65b9\u5dee\u548c\u5ffd\u7565\u5148\u9a8c\u6570\u636e\u5206\u5e03\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u68c0\u7d22\u7684\u6a21\u4eff\u5b66\u4e60\u5728\u4ece\u5c0f\u89c4\u6a21\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u9009\u62e9\u65f6\u4f9d\u8d56\u9ad8\u65b9\u5dee\u7684\u6700\u8fd1\u90bb\u4f30\u8ba1\u4e14\u672a\u8003\u8651\u5148\u9a8c\u6570\u636e\u5206\u5e03\u3002", "method": "\u4f5c\u8005\u63d0\u51faIWR\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u8ba1\u7b97\u76ee\u6807\u6570\u636e\u5206\u5e03\u4e0e\u5148\u9a8c\u6570\u636e\u5206\u5e03\u7684\u91cd\u8981\u6027\u6743\u91cd\uff0c\u4ee5\u5e73\u6ed1\u4f30\u8ba1\u5e76\u8003\u8651\u6570\u636e\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cIWR\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u7684Bridge\u6570\u636e\u96c6\u4e2d\u5747\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0cIWR\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u5373\u53ef\u6709\u6548\u6539\u5584\u6570\u636e\u9009\u62e9\u7684\u504f\u5dee\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u68c0\u7d22\u7684\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01119", "pdf": "https://arxiv.org/pdf/2509.01119", "abs": "https://arxiv.org/abs/2509.01119", "authors": ["Senura Hansaja Wanasekara", "Van-Dinh Nguyen", "Kok-Seng", "M. -Duong Nguyen", "Symeon Chatzinotas", "Octavia A. Dobre"], "title": "SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "16 pages, Accepted to IEEE Transactions on Mobile Computing", "summary": "Goal-oriented semantic communication (SC) aims to revolutionize communication\nsystems by transmitting only task-essential information. However, current\napproaches face challenges such as joint training at transceivers, leading to\nredundant data exchange and reliance on labeled datasets, which limits their\ntask-agnostic utility. To address these challenges, we propose a novel\nframework called Goal-oriented Invariant Representation-based SC (SC-GIR) for\nimage transmission. Our framework leverages self-supervised learning to extract\nan invariant representation that encapsulates crucial information from the\nsource data, independent of the specific downstream task. This compressed\nrepresentation facilitates efficient communication while retaining key features\nfor successful downstream task execution. Focusing on machine-to-machine tasks,\nwe utilize covariance-based contrastive learning techniques to obtain a latent\nrepresentation that is both meaningful and semantically dense. To evaluate the\neffectiveness of the proposed scheme on downstream tasks, we apply it to\nvarious image datasets for lossy compression. The compressed representations\nare then used in a goal-oriented AI task. Extensive experiments on several\ndatasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,\nand achieves over 85% classification accuracy for compressed data under\ndifferent SNR conditions. These results underscore the effectiveness of the\nproposed framework in learning compact and informative latent representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSC-GIR\u7684\u65b0\u578b\u76ee\u6807\u5bfc\u5411\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u56fe\u50cf\u4f20\u8f93\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u53d6\u4efb\u52a1\u65e0\u5173\u7684\u5173\u952e\u4fe1\u606f\u8868\u793a\uff0c\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u5bfc\u5411\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u5b58\u5728\u8054\u5408\u8bad\u7ec3\u5197\u4f59\u548c\u6570\u636e\u6807\u7b7e\u4f9d\u8d56\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u4efb\u52a1\u65e0\u5173\u7684\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSC-GIR\u6846\u67b6\uff0c\u5229\u7528\u534f\u65b9\u5dee\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u63d0\u53d6\u4e0d\u53d8\u8868\u793a\uff0c\u4ee5\u7d27\u51d1\u7684\u5f62\u5f0f\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u652f\u6301\u591a\u4efb\u52a1\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSC-GIR\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u8fd110%\uff0c\u5e76\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u4e0b\u5b9e\u73b085%\u4ee5\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "SC-GIR\u6846\u67b6\u80fd\u6709\u6548\u5b66\u4e60\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.01658", "pdf": "https://arxiv.org/pdf/2509.01658", "abs": "https://arxiv.org/abs/2509.01658", "authors": ["Zhenyu Wu", "Angyuan Ma", "Xiuwei Xu", "Hang Yin", "Yinan Liang", "Ziwei Wang", "Jiwen Lu", "Haibin Yan"], "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation", "categories": ["cs.RO"], "comment": "Accepted to CoRL 2025. Project Page: https://gary3410.github.io/MoTo/", "summary": "Mobile manipulation stands as a core challenge in robotics, enabling robots\nto assist humans across varied tasks and dynamic daily environments.\nConventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments due to the lack of large-scale training.\nHowever, recent advances in manipulation foundation models demonstrate\nimpressive generalization capability on a wide range of fixed-base manipulation\ntasks, which are still limited to a fixed setting. Therefore, we devise a\nplug-in module named MoTo, which can be combined with any off-the-shelf\nmanipulation foundation model to empower them with mobile manipulation ability.\nSpecifically, we propose an interaction-aware navigation policy to generate\nrobot docking points for generalized mobile manipulation. To enable zero-shot\nability, we propose an interaction keypoints framework via vision-language\nmodels (VLM) under multi-view consistency for both target object and robotic\narm following instructions, where fixed-base manipulation foundation models can\nbe employed. We further propose motion planning objectives for the mobile base\nand robot arm, which minimize the distance between the two keypoints and\nmaintain the physical feasibility of trajectories. In this way, MoTo guides the\nrobot to move to the docking points where fixed-base manipulation can be\nsuccessfully performed, and leverages VLM generation and trajectory\noptimization to achieve mobile manipulation in a zero-shot manner, without any\nrequirement on mobile manipulation expert data. Extensive experimental results\non OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%\nand 16.67% higher than the state-of-the-art mobile manipulation methods,\nrespectively, without requiring additional training data.", "AI": {"tldr": "MoTo\u662f\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0c\u80fd\u4e0e\u73b0\u6709\u64cd\u7eb5\u57fa\u7840\u6a21\u578b\u7ed3\u5408\uff0c\u8d4b\u4e88\u5176\u79fb\u52a8\u64cd\u7eb5\u80fd\u529b\uff0c\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u5bfc\u822a\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u79fb\u52a8\u64cd\u7eb5\u3002", "motivation": "\u4f20\u7edf\u79fb\u52a8\u64cd\u7eb5\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bad\u7ec3\u800c\u96be\u4ee5\u6cdb\u5316\uff0c\u73b0\u6709\u56fa\u5b9a\u57fa\u7840\u64cd\u7eb5\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\u4f46\u9650\u4e8e\u56fa\u5b9a\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u6570\u636e\u7684\u79fb\u52a8\u64cd\u7eb5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4ea4\u4e92\u611f\u77e5\u5bfc\u822a\u7b56\u7565\u751f\u6210\u5bf9\u63a5\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u76ee\u6807\u5bf9\u8c61\u548c\u673a\u68b0\u81c2\u7684\u4ea4\u4e92\u5173\u952e\u70b9\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u89c4\u5212\u4f18\u5316\u8f68\u8ff9\u53ef\u884c\u6027\u3002", "result": "MoTo\u5728OVMM\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa2.68%\u548c16.67%\u7684\u6210\u529f\u7387\u3002", "conclusion": "MoTo\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u79fb\u52a8\u64cd\u7eb5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2509.01177", "pdf": "https://arxiv.org/pdf/2509.01177", "abs": "https://arxiv.org/abs/2509.01177", "authors": ["Junxiang Liu", "Junming Lin", "Jiangtong Li", "Jie Li"], "title": "DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.SP"], "comment": "14 pages, 6 figures", "summary": "Reconstruction dynamic visual scenes from electroencephalography (EEG)\nsignals remains a primary challenge in brain decoding, limited by the low\nspatial resolution of EEG, a temporal mismatch between neural recordings and\nvideo dynamics, and the insufficient use of semantic information within brain\nactivity. Therefore, existing methods often inadequately resolve both the\ndynamic coherence and the complex semantic context of the perceived visual\nstimuli. To overcome these limitations, we introduce DynaMind, a novel\nframework that reconstructs video by jointly modeling neural dynamics and\nsemantic features via three core modules: a Regional-aware Semantic Mapper\n(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video\nReconstructor (DGVR). The RSM first utilizes a regional-aware encoder to\nextract multimodal semantic features from EEG signals across distinct brain\nregions, aggregating them into a unified diffusion prior. In the mean time, the\nTDA generates a dynamic latent sequence, or blueprint, to enforce temporal\nconsistency between the feature representations and the original neural\nrecordings. Together, guided by the semantic diffusion prior, the DGVR\ntranslates the temporal-aware blueprint into a high-fidelity video\nreconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art\n(SOTA), boosting reconstructed video accuracies (video- and frame-based) by\n12.5 and 10.3 percentage points, respectively. It also achieves a leap in\npixel-level quality, showing exceptional visual fidelity and temporal coherence\nwith a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical\nadvancement, bridging the gap between neural dynamics and high-fidelity visual\nsemantics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDynaMind\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u795e\u7ecf\u52a8\u6001\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u51b3EEG\u4fe1\u53f7\u91cd\u5efa\u52a8\u6001\u89c6\u89c9\u573a\u666f\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u8d28\u91cf\u3002", "motivation": "EEG\u4fe1\u53f7\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u4e0e\u89c6\u9891\u52a8\u6001\u65f6\u95f4\u4e0d\u5339\u914d\u3001\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u89e3\u6790\u52a8\u6001\u4e00\u81f4\u6027\u548c\u590d\u6742\u8bed\u4e49\u80cc\u666f\u3002", "method": "DynaMind\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aRSM\uff08\u533a\u57df\u611f\u77e5\u8bed\u4e49\u6620\u5c04\u5668\uff09\u3001TDA\uff08\u65f6\u95f4\u611f\u77e5\u52a8\u6001\u5bf9\u9f50\u5668\uff09\u548cDGVR\uff08\u53cc\u5f15\u5bfc\u89c6\u9891\u91cd\u5efa\u5668\uff09\uff0c\u5206\u522b\u63d0\u53d6\u591a\u6a21\u6001\u8bed\u4e49\u7279\u5f81\u3001\u751f\u6210\u52a8\u6001\u6f5c\u5728\u5e8f\u5217\uff0c\u5e76\u91cd\u5efa\u9ad8\u4fdd\u771f\u89c6\u9891\u3002", "result": "\u5728SEED-DV\u6570\u636e\u96c6\u4e0a\uff0cDynaMind\u5728\u89c6\u9891\u548c\u5e27\u7ea7\u522b\u7684\u51c6\u786e\u6027\u4e0a\u5206\u522b\u63d0\u5347\u4e8612.5\u548c10.3\u4e2a\u767e\u5206\u70b9\uff0cSSIM\u548cFVMD\u6307\u6807\u4e5f\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "DynaMind\u586b\u8865\u4e86\u795e\u7ecf\u52a8\u6001\u4e0e\u9ad8\u4fdd\u771f\u89c6\u89c9\u8bed\u4e49\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u662f\u8111\u89e3\u7801\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2509.01708", "pdf": "https://arxiv.org/pdf/2509.01708", "abs": "https://arxiv.org/abs/2509.01708", "authors": ["Abdelrhman Werby", "Martin B\u00fcchner", "Adrian R\u00f6fer", "Chenguang Huang", "Wolfram Burgard", "Abhinav Valada"], "title": "Articulated Object Estimation in the Wild", "categories": ["cs.RO", "cs.CV"], "comment": "9th Conference on Robot Learning (CoRL), 2025", "summary": "Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.", "AI": {"tldr": "ArtiPoint\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u76f8\u673a\u8fd0\u52a8\u548c\u90e8\u5206\u89c2\u6d4b\u4e0b\u63a8\u65ad\u5173\u8282\u7269\u4f53\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u70b9\u8ddf\u8e2a\u548c\u56fe\u4f18\u5316\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5e76\u63a8\u51fa\u4e86\u9996\u4e2a\u91ce\u751f\u73af\u5883\u4e0b\u7684\u6570\u636e\u96c6Arti4D\u3002", "motivation": "\u7814\u7a76\u4e09\u7ef4\u5173\u8282\u7269\u4f53\u8fd0\u52a8\u5bf9\u673a\u5668\u4eba\u573a\u666f\u7406\u89e3\u3001\u79fb\u52a8\u64cd\u4f5c\u548c\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u65b9\u6cd5\u5728\u975e\u63a7\u5236\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u70b9\u8ddf\u8e2a\u548c\u56e0\u5b50\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4eceRGB-D\u89c6\u9891\u4e2d\u4f30\u8ba1\u5173\u8282\u90e8\u5206\u8f68\u8ff9\u548c\u8f74\u3002", "result": "\u5728Arti4D\u6570\u636e\u96c6\u4e0a\uff0cArtiPoint\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u548c\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ArtiPoint\u4e3a\u5173\u8282\u7269\u4f53\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.01497", "pdf": "https://arxiv.org/pdf/2509.01497", "abs": "https://arxiv.org/abs/2509.01497", "authors": ["Anna Pastuszczak", "Rafa\u0142 Stojek", "Piotr Wr\u00f3bel", "Magdalena Cwojdzi\u0144ska", "Kacper Sobczak", "Rafa\u0142 Koty\u0144ski"], "title": "High-resolution single-pixel imaging in real time with iterative or deep learning-based reconstruction enhancement", "categories": ["eess.IV", "eess.SP"], "comment": "Presented at ISCS25", "summary": "We introduce a compressive single-pixel imaging (SPI) framework for\nhigh-resolution image capture in fractions of a second. This framework combines\na dedicated sampling strategy with a tailored reconstruction method to enable\nhigh-quality imaging of spatially sparse scenes at the native 1024x768\nresolution of a digital micromirror device (DMD). The reconstruction process\nconsists of two phases: first, the measured data is processed using the\ngeneralized inverse of the measurement matrix for quick image recovery. Then,\nthe spatial sparsity of the scene is leveraged to enhance reconstruction in\ndense areas, using either an iterative method or a neural network-based\napproach. With a compression ratio of 0.41% and an image acquisition rate of\n6.8 Hz at 22 kHz DMD operation, this framework supports real-time,\nhigh-resolution dynamic imaging with the reconstruction that matches the\nacquisition rate on a mid-tier desktop GPU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u7f29\u5355\u50cf\u7d20\u6210\u50cf\uff08SPI\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u77ed\u65f6\u95f4\u5185\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u7ed3\u5408\u4e13\u7528\u91c7\u6837\u7b56\u7565\u548c\u5b9a\u5236\u91cd\u5efa\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u6210\u50cf\u3002", "motivation": "\u4f20\u7edf\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6355\u83b7\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u590d\u6742\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u52a8\u6001\u6210\u50cf\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u4e13\u7528\u91c7\u6837\u7b56\u7565\u4e0e\u5b9a\u5236\u91cd\u5efa\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u91cd\u5efa\uff1a\u5148\u901a\u8fc7\u6d4b\u91cf\u77e9\u9635\u5e7f\u4e49\u9006\u5feb\u901f\u6062\u590d\u56fe\u50cf\uff0c\u518d\u5229\u7528\u7a7a\u95f4\u7a00\u758f\u6027\u901a\u8fc7\u8fed\u4ee3\u65b9\u6cd5\u6216\u795e\u7ecf\u7f51\u7edc\u589e\u5f3a\u5bc6\u96c6\u533a\u57df\u91cd\u5efa\u3002", "result": "\u5728\u538b\u7f29\u6bd40.41%\u548c22kHz DMD\u64cd\u4f5c\u4e0b\uff0c\u6210\u50cf\u901f\u7387\u4e3a6.8Hz\uff0c\u652f\u6301\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u6210\u50cf\uff0c\u91cd\u5efa\u901f\u5ea6\u4e0e\u4e2d\u7aef\u684c\u9762GPU\u5339\u914d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6355\u83b7\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u52a8\u6001\u6210\u50cf\u573a\u666f\u3002"}}
{"id": "2509.01728", "pdf": "https://arxiv.org/pdf/2509.01728", "abs": "https://arxiv.org/abs/2509.01728", "authors": ["Parv Kapoor", "Akila Ganlath", "Changliu Liu", "Sebastian Scherer", "Eunsuk Kang"], "title": "Constrained Decoding for Robotics Foundation Models", "categories": ["cs.RO", "cs.LG", "cs.LO"], "comment": null, "summary": "Recent advances in the development of robotic foundation models have led to\npromising end-to-end and general-purpose capabilities in robotic systems. These\nmodels are pretrained on vast datasets of robot trajectories to process multi-\nmodal inputs and directly output a sequence of action that the system then\nexecutes in the real world. Although this approach is attractive from the\nperspective of im- proved generalization across diverse tasks, these models are\nstill data-driven and, therefore, lack explicit notions of behavioral\ncorrectness and safety constraints. We address these limitations by introducing\na constrained decoding framework for robotics foundation models that enforces\nlogical constraints on action trajec- tories in dynamical systems. Our method\nensures that generated actions provably satisfy signal temporal logic (STL)\nspecifications at runtime without retraining, while remaining agnostic of the\nunderlying foundation model. We perform com- prehensive evaluation of our\napproach across state-of-the-art navigation founda- tion models and we show\nthat our decoding-time interventions are useful not only for filtering unsafe\nactions but also for conditional action-generation. Videos available on our\nwebsite: https://constrained-robot-fms.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ea6\u675f\u89e3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u52a8\u4f5c\u6ee1\u8db3\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u884c\u4e3a\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u867d\u7136\u80fd\u6cdb\u5316\u5230\u591a\u6837\u5316\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u884c\u4e3a\u6b63\u786e\u6027\u548c\u5b89\u5168\u7ea6\u675f\u7684\u663e\u5f0f\u5b9a\u4e49\uff0c\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u89e3\u7801\u6846\u67b6\u5728\u8fd0\u884c\u65f6\u5f3a\u5236\u6267\u884c\u903b\u8f91\u7ea6\u675f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u786e\u4fdd\u52a8\u4f5c\u8f68\u8ff9\u6ee1\u8db3STL\u89c4\u8303\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u4e0d\u5b89\u5168\u52a8\u4f5c\u5e76\u652f\u6301\u6761\u4ef6\u6027\u52a8\u4f5c\u751f\u6210\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b89\u5168\u6027\u548c\u884c\u4e3a\u7ea6\u675f\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.02149", "pdf": "https://arxiv.org/pdf/2509.02149", "abs": "https://arxiv.org/abs/2509.02149", "authors": ["Jintao Liang", "Pablo G. Madoery", "Chung-Horng Lung", "Halim Yanikomeroglu", "Gunes Karabulut Kurt"], "title": "Green Traffic Engineering for Satellite Networks Using Segment Routing Flexible Algorithm", "categories": ["cs.NI", "cs.SY", "eess.SP", "eess.SY"], "comment": "Accepted for at GlobeCom 2025 GCSN", "summary": "Large-scale low-Earth-orbit (LEO) constellations demand routing that\nsimultaneously minimizes energy, guarantees delivery under congestion, and\nmeets latency requirements for time-critical flows. We present a segment\nrouting over IPv6 (SRv6) flexible algorithm (Flex-Algo) framework that consists\nof three logical slices: an energy-efficient slice (Algo 130), a\nhigh-reliability slice (Algo 129), and a latency-sensitive slice (Algo 128).\nThe framework provides a unified mixed-integer linear program (MILP) that\ncombines satellite CPU power, packet delivery rate (PDR), and end-to-end\nlatency into a single objective, allowing a lightweight software-defined\nnetwork (SDN) controller to steer traffic from the source node. Emulation of\nTelesat's Lightspeed constellation shows that, compared with different routing\nschemes, the proposed design reduces the average CPU usage by 73%, maintains a\nPDR above 91% during traffic bursts, and decreases urgent flow delay by 18 ms\nbetween Ottawa and Vancouver. The results confirm Flex-Algo's value as a\nslice-based traffic engineering (TE) tool for resource-constrained satellite\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSRv6 Flex-Algo\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728LEO\u536b\u661f\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u3001\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u7684\u8def\u7531\uff0c\u901a\u8fc7MILP\u4f18\u5316CPU\u529f\u8017\u3001\u6570\u636e\u5305\u4f20\u8f93\u7387\u548c\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21LEO\u661f\u5ea7\u7f51\u7edc\u4e2d\u5982\u4f55\u5728\u8282\u7701\u80fd\u6e90\u3001\u4fdd\u8bc1\u62e5\u5835\u4e0b\u53ef\u9760\u4f20\u8f93\u548c\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u8981\u6c42\u7684\u591a\u76ee\u6807\u8def\u7531\u95ee\u9898\u3002", "method": "\u91c7\u7528SRv6 Flex-Algo\u6846\u67b6\uff0c\u5212\u5206\u4e3a\u4e09\u4e2a\u903b\u8f91\u5207\u7247\uff08\u80fd\u8017\u4f18\u5316\u3001\u53ef\u9760\u6027\u4f18\u5148\u548c\u4f4e\u5ef6\u8fdf\uff09\uff0c\u5e76\u901a\u8fc7MILP\u7edf\u4e00\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5728Telesat Lightspeed\u661f\u5ea7\u7684\u4eff\u771f\u4e2d\uff0c\u5e73\u5747CPU\u4f7f\u7528\u7387\u964d\u4f4e73%\uff0c\u6570\u636e\u5305\u4f20\u8f93\u7387\u5728\u6d41\u91cf\u9ad8\u5cf0\u65f6\u4fdd\u630191%\u4ee5\u4e0a\uff0c\u7d27\u6025\u6d41\u5ef6\u8fdf\u51cf\u5c1118\u6beb\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f5c\u4e3a\u5207\u7247\u6d41\u91cf\u5de5\u7a0b\u5de5\u5177\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u536b\u661f\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u80fd\u3002"}}
{"id": "2509.01746", "pdf": "https://arxiv.org/pdf/2509.01746", "abs": "https://arxiv.org/abs/2509.01746", "authors": ["Yixuan Huang", "Novella Alvina", "Mohanraj Devendran Shanthi", "Tucker Hermans"], "title": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference", "categories": ["cs.RO"], "comment": "Project page: sites.google.com/view/fail2progress. 25 pages, 8\n  figures. Accepted to the Conference on Robot Learning (CoRL) 2025", "summary": "Skill effect models for long-horizon manipulation tasks are prone to failures\nin conditions not covered by training data distributions. Therefore, enabling\nrobots to reason about and learn from failures is necessary. We investigate the\nproblem of efficiently generating a dataset targeted to observed failures.\nAfter fine-tuning a skill effect model on this dataset, we evaluate the extent\nto which the model can recover from failures and minimize future failures. We\npropose Fail2Progress, an approach that leverages Stein variational inference\nto generate multiple simulation environments in parallel, enabling efficient\ndata sample generation similar to observed failures. Our method is capable of\nhandling several challenging mobile manipulation tasks, including transporting\nmultiple objects, organizing a constrained shelf, and tabletop organization.\nThrough large-scale simulation and real-world experiments, we demonstrate that\nour approach excels at learning from failures across different numbers of\nobjects. Furthermore, we show that Fail2Progress outperforms several baselines.", "AI": {"tldr": "Fail2Progress\u5229\u7528Stein\u53d8\u5206\u63a8\u7406\u751f\u6210\u5e76\u884c\u4eff\u771f\u73af\u5883\uff0c\u4ee5\u63d0\u9ad8\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6280\u80fd\u6548\u679c\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u6761\u4ef6\u4e0b\u7684\u5931\u8d25\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u751f\u6210\u9488\u5bf9\u5931\u8d25\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faFail2Progress\u65b9\u6cd5\uff0c\u57fa\u4e8eStein\u53d8\u5206\u63a8\u7406\u751f\u6210\u591a\u4e2a\u5e76\u884c\u4eff\u771f\u73af\u5883\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u7c7b\u4f3c\u5931\u8d25\u7684\u6570\u636e\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFail2Progress\u5728\u5904\u7406\u591a\u79cd\u590d\u6742\u4efb\u52a1\uff08\u5982\u642c\u8fd0\u591a\u4e2a\u7269\u4f53\u3001\u6574\u7406\u53d7\u9650\u8d27\u67b6\u7b49\uff09\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u3002", "conclusion": "Fail2Progress\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5931\u8d25\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.02426", "pdf": "https://arxiv.org/pdf/2509.02426", "abs": "https://arxiv.org/abs/2509.02426", "authors": ["Ashutossh Gupta", "Vassilis Kekatos", "Ruoyu Yang", "Dionysios Aliprantis", "Steve Pekarek"], "title": "Frequency-Domain Characterization of Load Demand from Electrified Highways", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "10 Pages, 6 figures", "summary": "Electrified roadways (ER) equipped with dynamic wireless power transfer\n(DWPT) capabilities can patently extend the driving range and reduce the\nbattery size of electric vehicles (EVs). However, due to the spatial\narrangement of the transmitter coils in the ER, the DWPT load exhibits\nfrequency content that could excite power system frequency dynamics. In this\ncontext, this work aims to study the spectrum of DWPT loads under different\ntraffic conditions. We develop statistical models for EVs moving at constant\nspeeds to identify the location and magnitude of DWPT load harmonics. Our\nanalysis reveals that the fundamental frequency is dependent on the ER coil\nspacing and the average EV speed. In the worst-case yet unlikely scenario that\nEVs move in a synchronized fashion, the amplitude of harmonics scales with the\nnumber of EVs. On the contrary, when EVs move freely, harmonics scale with the\nsquare root of the number of EVs. Platoon formations can accentuate harmonics.\nWe also show that for higher-order harmonics, the spectral content around\nharmonics decreases in magnitude and increases in bandwidth. Despite the\nsimplified models, our analysis offers valuable insights for ER planners and\ngrid operators. Numerical tests using a traffic simulator corroborate some of\nthese insights.", "AI": {"tldr": "\u7814\u7a76\u52a8\u6001\u65e0\u7ebf\u5145\u7535\u9053\u8def\uff08ER\uff09\u5728\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u4e0b\u7684\u8d1f\u8f7d\u9891\u8c31\u7279\u6027\uff0c\u63ed\u793a\u5176\u4e0e\u8f66\u8f86\u901f\u5ea6\u548c\u7ebf\u5708\u95f4\u8ddd\u7684\u5173\u7cfb\u3002", "motivation": "\u52a8\u6001\u65e0\u7ebf\u5145\u7535\u9053\u8def\uff08ER\uff09\u53ef\u4ee5\u5ef6\u957f\u7535\u52a8\u8f66\uff08EV\uff09\u7684\u884c\u9a76\u91cc\u7a0b\u5e76\u51cf\u5c0f\u7535\u6c60\u5c3a\u5bf8\uff0c\u4f46\u5145\u7535\u8d1f\u8f7d\u7684\u9891\u8c31\u53ef\u80fd\u5f71\u54cd\u7535\u7f51\u9891\u7387\u52a8\u6001\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u5efa\u7acbEV\u4ee5\u6052\u5b9a\u901f\u5ea6\u884c\u9a76\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5206\u6790DWPT\u8d1f\u8f7d\u8c10\u6ce2\u7684\u4f4d\u7f6e\u548c\u5e45\u503c\uff0c\u5e76\u901a\u8fc7\u4ea4\u901a\u6a21\u62df\u5668\u9a8c\u8bc1\u90e8\u5206\u7ed3\u679c\u3002", "result": "\u8c10\u6ce2\u57fa\u9891\u4e0eER\u7ebf\u5708\u95f4\u8ddd\u548cEV\u5e73\u5747\u901f\u5ea6\u76f8\u5173\uff1b\u540c\u6b65\u884c\u9a76\u65f6\u8c10\u6ce2\u5e45\u503c\u4e0eEV\u6570\u91cf\u6210\u6b63\u6bd4\uff0c\u81ea\u7531\u884c\u9a76\u65f6\u4e0eEV\u6570\u91cf\u7684\u5e73\u65b9\u6839\u6210\u6b63\u6bd4\uff1b\u8f66\u961f\u5f62\u5f0f\u4f1a\u52a0\u5267\u8c10\u6ce2\u3002", "conclusion": "\u5c3d\u7ba1\u6a21\u578b\u7b80\u5316\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u4e3aER\u89c4\u5212\u8005\u548c\u7535\u7f51\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2509.01765", "pdf": "https://arxiv.org/pdf/2509.01765", "abs": "https://arxiv.org/abs/2509.01765", "authors": ["Skand Peri", "Akhil Perincherry", "Bikram Pandit", "Stefan Lee"], "title": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control", "categories": ["cs.RO"], "comment": "17 pages, 6 figures. Accepted as Oral presentation at Conference on\n  Robot Learning (CoRL) 2025", "summary": "Efficient robot control often requires balancing task performance with energy\nexpenditure. A common approach in reinforcement learning (RL) is to penalize\nenergy use directly as part of the reward function. This requires carefully\ntuning weight terms to avoid undesirable trade-offs where energy minimization\nharms task success. In this work, we propose a hyperparameter-free gradient\noptimization method to minimize energy expenditure without conflicting with\ntask performance. Inspired by recent works in multitask learning, our method\napplies policy gradient projection between task and energy objectives to derive\npolicy updates that minimize energy expenditure in ways that do not impact task\nperformance. We evaluate this technique on standard locomotion benchmarks of\nDM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage\nwhile maintaining comparable task performance. Further, we conduct experiments\non a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient\npolicies. Our method is easy to implement in standard RL pipelines with minimal\ncode changes, is applicable to any policy gradient method, and offers a\nprincipled alternative to reward shaping for energy efficient control policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u6574\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6295\u5f71\u5728\u4efb\u52a1\u548c\u80fd\u8017\u76ee\u6807\u4e4b\u95f4\u5e73\u8861\uff0c\u5b9e\u73b0\u80fd\u8017\u6700\u5c0f\u5316\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u4e8664%\u80fd\u8017\uff0c\u5e76\u5c55\u793a\u4e86Sim2Real\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u5728\u5956\u52b1\u51fd\u6570\u4e2d\u76f4\u63a5\u60e9\u7f5a\u80fd\u8017\uff0c\u4f46\u9700\u7cbe\u5fc3\u8c03\u6574\u6743\u91cd\u4ee5\u907f\u514d\u4efb\u52a1\u8868\u73b0\u53d7\u635f\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u65e0\u9700\u8d85\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7b56\u7565\u68af\u5ea6\u6295\u5f71\u6280\u672f\uff0c\u5728\u4efb\u52a1\u76ee\u6807\u548c\u80fd\u8017\u76ee\u6807\u4e4b\u95f4\u4f18\u5316\u7b56\u7565\u66f4\u65b0\uff0c\u907f\u514d\u80fd\u8017\u6700\u5c0f\u5316\u4e0e\u4efb\u52a1\u8868\u73b0\u7684\u51b2\u7a81\u3002", "result": "\u5728DM-Control\u548cHumanoidBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u8017\u51cf\u5c1164%\u4e14\u4efb\u52a1\u8868\u73b0\u4e0d\u53d7\u5f71\u54cd\uff0c\u540c\u65f6\u5728Unitree GO2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86Sim2Real\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u5956\u52b1\u5851\u5f62\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u6613\u4e8e\u5728\u6807\u51c6RL\u6d41\u7a0b\u4e2d\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2509.02538", "pdf": "https://arxiv.org/pdf/2509.02538", "abs": "https://arxiv.org/abs/2509.02538", "authors": ["Rui Zhang", "Wenlong Mou"], "title": "Federated learning over physical channels: adaptive algorithms with near-optimal guarantees", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT", "stat.ML"], "comment": null, "summary": "In federated learning, communication cost can be significantly reduced by\ntransmitting the information over the air through physical channels. In this\npaper, we propose a new class of adaptive federated stochastic gradient descent\n(SGD) algorithms that can be implemented over physical channels, taking into\naccount both channel noise and hardware constraints. We establish theoretical\nguarantees for the proposed algorithms, demonstrating convergence rates that\nare adaptive to the stochastic gradient noise level. We also demonstrate the\npractical effectiveness of our algorithms through simulation studies with deep\nlearning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u8054\u90a6SGD\u7b97\u6cd5\uff0c\u8003\u8651\u4e86\u4fe1\u9053\u566a\u58f0\u548c\u786c\u4ef6\u9650\u5236\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4eff\u771f\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6210\u672c\u53ef\u4ee5\u901a\u8fc7\u7269\u7406\u4fe1\u9053\u7a7a\u4e2d\u4f20\u8f93\u5927\u5e45\u964d\u4f4e\uff0c\u4f46\u9700\u8981\u8003\u8651\u4fe1\u9053\u566a\u58f0\u548c\u786c\u4ef6\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u7c7b\u81ea\u9002\u5e94\u8054\u90a6SGD\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u7269\u7406\u4fe1\u9053\u5b9e\u73b0\uff0c\u7ed3\u5408\u4fe1\u9053\u566a\u58f0\u548c\u786c\u4ef6\u7ea6\u675f\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u81ea\u9002\u5e94\u8054\u90a6SGD\u7b97\u6cd5\u5728\u7269\u7406\u4fe1\u9053\u73af\u5883\u4e0b\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.01819", "pdf": "https://arxiv.org/pdf/2509.01819", "abs": "https://arxiv.org/abs/2509.01819", "authors": ["Ge Yan", "Jiyue Zhu", "Yuquan Deng", "Shiqi Yang", "Ri-Zhao Qiu", "Xuxin Cheng", "Marius Memmel", "Ranjay Krishna", "Ankit Goyal", "Xiaolong Wang", "Dieter Fox"], "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.", "AI": {"tldr": "ManiFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u4f5c\uff0c\u7ed3\u5408DiT-X\u67b6\u6784\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u751f\u6210\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u7ef4\u5ea6\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u6311\u6218\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6d41\u5339\u914d\u4e0e\u4e00\u81f4\u6027\u8bad\u7ec3\u6280\u672f\u751f\u6210\u52a8\u4f5c\uff0c\u63d0\u51faDiT-X\u67b6\u6784\uff08\u6269\u6563\u53d8\u6362\u5668\uff09\u4ee5\u5b9e\u73b0\u591a\u6a21\u6001\u8f93\u5165\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u63a5\u8fd1\u7ffb\u500d\uff0c\u4e14\u5bf9\u65b0\u9896\u5bf9\u8c61\u548c\u73af\u5883\u53d8\u5316\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ManiFlow\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.01836", "pdf": "https://arxiv.org/pdf/2509.01836", "abs": "https://arxiv.org/abs/2509.01836", "authors": ["Md Mahbub Alam", "Jose F. Rodrigues-Jr", "Gabriel Spadon"], "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate vessel trajectory prediction is essential for enhancing situational\nawareness and preventing collisions. Still, existing data-driven models are\nconstrained mainly to single-vessel forecasting, overlooking vessel\ninteractions, navigation rules, and explicit collision risk assessment. We\npresent a transformer-based framework for multi-vessel trajectory prediction\nwith integrated collision risk analysis. For a given target vessel, the\nframework identifies nearby vessels. It jointly predicts their future\ntrajectories through parallel streams encoding kinematic and derived physical\nfeatures, causal convolutions for temporal locality, spatial transformations\nfor positional encoding, and hybrid positional embeddings that capture both\nlocal motion patterns and long-range dependencies. Evaluated on large-scale\nreal-world AIS data using joint multi-vessel metrics, the model demonstrates\nsuperior forecasting capabilities beyond traditional single-vessel displacement\nerrors. By simulating interactions among predicted trajectories, the framework\nfurther quantifies potential collision risks, offering actionable insights to\nstrengthen maritime safety and decision support.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u78b0\u649e\u98ce\u9669\u5206\u6790\uff0c\u901a\u8fc7\u5e76\u884c\u6d41\u7f16\u7801\u7279\u5f81\u548c\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\u7b49\u6280\u672f\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u4e00\u8239\u8236\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u8239\u8236\u95f4\u7684\u4ea4\u4e92\u3001\u5bfc\u822a\u89c4\u5219\u548c\u663e\u5f0f\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5e76\u884c\u6d41\u7f16\u7801\u8fd0\u52a8\u5b66\u4e0e\u7269\u7406\u7279\u5f81\uff0c\u7ed3\u5408\u56e0\u679c\u5377\u79ef\u3001\u7a7a\u95f4\u53d8\u6362\u548c\u6df7\u5408\u4f4d\u7f6e\u5d4c\u5165\uff0c\u8054\u5408\u9884\u6d4b\u591a\u8239\u8236\u8f68\u8ff9\u3002", "result": "\u5728\u5927\u89c4\u6a21\u771f\u5b9eAIS\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u5355\u8239\u8236\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u91cf\u5316\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u589e\u5f3a\u6d77\u4e8b\u5b89\u5168\u548c\u51b3\u7b56\u652f\u6301\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002"}}
{"id": "2509.01878", "pdf": "https://arxiv.org/pdf/2509.01878", "abs": "https://arxiv.org/abs/2509.01878", "authors": ["Scarlett Raine", "Tobias Fischer"], "title": "AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6c34\u4e0bAI\u5982\u4f55\u56e0\u73af\u5883\u9700\u6c42\u3001\u6570\u636e\u6c11\u4e3b\u5316\u548c\u7814\u7a76\u4eba\u5458\u8fc1\u79fb\u800c\u8fc5\u901f\u53d1\u5c55\uff0c\u5e76\u5206\u6790\u4e86\u6c34\u4e0b\u6311\u6218\u5982\u4f55\u63a8\u52a8AI\u6280\u672f\u5728\u5f31\u76d1\u7763\u5b66\u4e60\u3001\u5f00\u653e\u96c6\u8bc6\u522b\u7b49\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u8fdb\u800c\u5f71\u54cd\u66f4\u5e7f\u6cdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u73af\u5883\u76d1\u6d4b\u9886\u57df\u3002", "motivation": "\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u9762\u4e34\u6c14\u5019\u53d8\u5316\u7684\u538b\u529b\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u7684AI\u76d1\u6d4b\u65b9\u6848\uff0c\u6c34\u4e0bAI\u56e0\u6b64\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u5206\u6790\u4e86\u73af\u5883\u9700\u6c42\u3001\u6570\u636e\u6c11\u4e3b\u5316\u548c\u7814\u7a76\u4eba\u5458\u8fc1\u79fb\u4e09\u4e2a\u9a71\u52a8\u56e0\u7d20\uff0c\u63a2\u8ba8\u4e86\u6c34\u4e0b\u72ec\u7279\u6311\u6218\u5bf9AI\u6280\u672f\u7684\u5f71\u54cd\uff0c\u5e76\u8c03\u67e5\u4e86\u6570\u636e\u96c6\u3001\u573a\u666f\u7406\u89e3\u548c3D\u91cd\u5efa\u7684\u65b0\u8d8b\u52bf\u3002", "result": "\u6c34\u4e0b\u7ea6\u675f\u63a8\u52a8\u4e86\u57fa\u7840\u6a21\u578b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u611f\u77e5\u6280\u672f\u7684\u8fb9\u754c\u6269\u5c55\uff0c\u65b9\u6cd5\u8bba\u521b\u65b0\u4e0d\u4ec5\u9650\u4e8e\u6d77\u6d0b\u5e94\u7528\uff0c\u8fd8\u60e0\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002", "conclusion": "\u6c34\u4e0bAI\u7684\u7814\u7a76\u4e0d\u4ec5\u89e3\u51b3\u4e86\u6d77\u6d0b\u751f\u6001\u95ee\u9898\uff0c\u8fd8\u4e3a\u66f4\u5e7f\u6cdb\u7684AI\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.01944", "pdf": "https://arxiv.org/pdf/2509.01944", "abs": "https://arxiv.org/abs/2509.01944", "authors": ["Zhenlong Yuan", "Jing Tang", "Jinguo Luo", "Rui Chen", "Chengxuan Qian", "Lei Sun", "Xiangxiang Chu", "Yujun Cai", "Dapeng Zhang", "Shuo Li"], "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.", "AI": {"tldr": "AutoDrive-R\u00b2\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u548c\u81ea\u6211\u53cd\u601d\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u548c\u51b3\u7b56\u5408\u7406\u6027\uff0c\u5e76\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u51b3\u7b56\u8fc7\u7a0b\u7684\u89e3\u91ca\u6027\u548c\u52a8\u4f5c\u5e8f\u5217\u7684\u5408\u7406\u6027\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faAutoDrive-R\u00b2\u6846\u67b6\uff0c\u7ed3\u5408\u94fe\u5f0f\u63a8\u7406\u548c\u81ea\u6211\u53cd\u601d\u7684CoT\u6570\u636e\u96c6nuScenesR\u00b2-6K\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u5728RL\u9636\u6bb5\u91c7\u7528GRPO\u7b97\u6cd5\u4ee5\u786e\u4fdd\u53ef\u9760\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728nuScenes\u548cWaymo\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5148\u8fdb\u6027\u80fd\u548c\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AutoDrive-R\u00b2\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u63a8\u7406\u548c\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2509.01980", "pdf": "https://arxiv.org/pdf/2509.01980", "abs": "https://arxiv.org/abs/2509.01980", "authors": ["Luca Di Pierno", "Robert Hewitt", "Stephan Weiss", "Roland Brockers"], "title": "Hybrid Autonomy Framework for a Future Mars Science Helicopter", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, IEEE CASE 2025 Conference", "summary": "Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary\nsurface exploration beyond the reach of ground-based robots. Thus, NASA is\nstudying a Mars Science Helicopter (MSH), an advanced concept capable of\nperforming long-range science missions and autonomously navigating challenging\nMartian terrain. Given significant Earth-Mars communication delays and mission\ncomplexity, an advanced autonomy framework is required to ensure safe and\nefficient operation by continuously adapting behavior based on mission\nobjectives and real-time conditions, without human intervention. This study\npresents a deterministic high-level control framework for aerial exploration,\nintegrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a\nscalable, robust, and computationally efficient autonomy solution for critical\nscenarios like deep space exploration. In this paper we outline key\ncapabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework\nwhich orchestrates them to achieve the desired objectives. Monte Carlo\nsimulations and real field tests validate the framework, demonstrating its\nrobustness and adaptability to both discrete events and real-time system\nfeedback. These inputs trigger state transitions or dynamically adjust behavior\nexecution, enabling reactive and context-aware responses. The framework is\nmiddleware-agnostic, supporting integration with systems like F-Prime and\nextending beyond aerial robotics.", "AI": {"tldr": "NASA \u63d0\u51fa\u4e00\u79cd Mars Science Helicopter (MSH) \u9ad8\u7ea7\u63a7\u5236\u6846\u67b6\uff0c\u96c6\u6210\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u884c\u4e3a\u6811\uff08BTs\uff09\uff0c\u4ee5\u5b9e\u73b0\u5728\u706b\u661f\u7b49\u6781\u7aef\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u98de\u884c\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u5730\u7403-\u706b\u661f\u901a\u4fe1\u5ef6\u8fdf\u548c\u4efb\u52a1\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u786e\u4fdd\u706b\u661f\u76f4\u5347\u673a\u5728\u6ca1\u6709\u4eba\u7c7b\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u5b89\u5168\u9ad8\u6548\u5730\u9002\u5e94\u5b9e\u65f6\u4efb\u52a1\u9700\u6c42\u548c\u73af\u5883\u53d8\u5316\u3002", "method": "\u91c7\u7528\u786e\u5b9a\u6027\u9ad8\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408 FSM \u548c BTs\uff0c\u901a\u8fc7 Monte Carlo \u6a21\u62df\u548c\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u6846\u67b6\u5728\u79bb\u6563\u4e8b\u4ef6\u548c\u5b9e\u65f6\u53cd\u9988\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u4e0e F-Prime \u7b49\u7cfb\u7edf\u7684\u96c6\u6210\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u81ea\u4e3b\u673a\u5668\u4eba\u9886\u57df\u3002", "conclusion": "\u8be5 FSM-BT \u6df7\u5408\u6846\u67b6\u4e3a\u6df1\u7a7a\u63a2\u7d22\uff08\u5982\u706b\u661f\u4efb\u52a1\uff09\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.01985", "pdf": "https://arxiv.org/pdf/2509.01985", "abs": "https://arxiv.org/abs/2509.01985", "authors": ["Eduardo Espindola", "Yu Tang"], "title": "Geometric Control of Mechanical Systems with Symmetries Based on Sliding Modes", "categories": ["cs.RO"], "comment": "32 pages, 3 figures, journal submission", "summary": "In this paper, we propose a framework for designing sliding mode controllers\nfor a class of mechanical systems with symmetry, both unconstrained and\nconstrained, that evolve on principal fiber bundles. Control laws are developed\nbased on the reduced motion equations by exploring symmetries, leading to a\nsliding mode control strategy where the reaching stage is executed on the base\nspace, and the sliding stage is performed on the structure group. Thus, design\ncomplexity is reduced, and difficult choices for coordinate representations\nwhen working with a particular Lie group are avoided. For this purpose, a\nsliding subgroup is constructed on the structure group based on a kinematic\ncontroller, and the sliding variable will converge to the identity of the state\nmanifold upon reaching the sliding subgroup. A reaching law based on a general\nsliding vector field is then designed on the base space using the local form of\nthe mechanical connection to drive the sliding variable to the sliding\nsubgroup, and its time evolution is given according to the appropriate\ncovariant derivative. Almost global asymptotic stability and local exponential\nstability are demonstrated using a Lyapunov analysis. We apply the results to a\nfully actuated system (a rigid spacecraft actuated by reaction wheels) and a\nsubactuated nonholonomic system (unicycle mobile robot actuated by wheels),\nwhich is also simulated for illustration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u6ed1\u52a8\u6a21\u614b\u63a7\u5236\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u68b0\u7cfb\u7edf\u7684\u6ed1\u52a8\u63a7\u5236\u8bbe\u8ba1\uff0c\u7b80\u5316\u4e86\u590d\u6742\u6027\u5e76\u907f\u514d\u4e86\u5750\u6807\u8868\u793a\u7684\u56f0\u96be\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528\u5bf9\u79f0\u6027\u7b80\u5316\u673a\u68b0\u7cfb\u7edf\u7684\u6ed1\u52a8\u6a21\u614b\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u907f\u514d\u7279\u5b9aLie\u7fa4\u4e0b\u5750\u6807\u9009\u62e9\u7684\u590d\u6742\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u7b80\u5316\u8fd0\u52a8\u65b9\u7a0b\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u5206\u4e24\u9636\u6bb5\uff08\u57fa\u7a7a\u95f4\u548c\u76ee\u6807\u7fa4\uff09\u6267\u884c\u6ed1\u52a8\u63a7\u5236\uff0c\u5e76\u6784\u9020\u6ed1\u52a8\u5b50\u7fa4\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u53ef\u5b9e\u73b0\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u548c\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\uff0c\u5e76\u901a\u8fc7\u822a\u5929\u5668\u548c\u65e0\u673a\u5668\u4eba\u7cfb\u7edf\u9a8c\u8bc1\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u7b80\u5316\u4e86\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u7684\u6ed1\u52a8\u6a21\u614b\u63a7\u5236\u8bbe\u8ba1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.01996", "pdf": "https://arxiv.org/pdf/2509.01996", "abs": "https://arxiv.org/abs/2509.01996", "authors": ["Chi Sun", "Xian Wang", "Abhishek Kumar", "Chengbin Cui", "Lik-Hang Lee"], "title": "MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-based Multi-object Teleoperation", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted by ISMAR 2025", "summary": "Effective human-robot interaction (HRI) in multi-object teleoperation tasks\nfaces significant challenges due to perceptual ambiguities in virtual reality\n(VR) environments and the limitations of single-modality intention recognition.\nThis paper proposes a shared control framework that combines a virtual\nadmittance (VA) model with a Multimodal-CNN-based Human Intention Perception\nNetwork (MMIPN) to enhance teleoperation performance and user experience. The\nVA model employs artificial potential fields to guide operators toward target\nobjects by adjusting admittance force and optimizing motion trajectories. MMIPN\nprocesses multimodal inputs, including gaze movement, robot motions, and\nenvironmental context, to estimate human grasping intentions, helping to\novercome depth perception challenges in VR. Our user study evaluated four\nconditions across two factors, and the results showed that MMIPN significantly\nimproved grasp success rates, while the VA model enhanced movement efficiency\nby reducing path lengths. Gaze data emerged as the most crucial input modality.\nThese findings demonstrate the effectiveness of combining multimodal cues with\nimplicit guidance in VR-based teleoperation, providing a robust solution for\nmulti-object grasping tasks and enabling more natural interactions across\nvarious applications in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u865a\u62df\u5bfc\u7eb3\u6a21\u578b\u548c\u591a\u6a21\u6001CNN\u7684\u7f51\u7edc\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u8fdc\u7a0b\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u73b0\u5b9e\u73af\u5883\u4e2d\u591a\u7269\u4f53\u8fdc\u7a0b\u64cd\u4f5c\u7684\u611f\u77e5\u6a21\u7cca\u6027\u548c\u5355\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u865a\u62df\u5bfc\u7eb3\u6a21\u578b\u548c\u591a\u6a21\u6001CNN\uff08MMIPN\uff09\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u76ee\u5149\u79fb\u52a8\u3001\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u73af\u5883\u80cc\u666f\uff09\u6765\u4f30\u8ba1\u4eba\u7c7b\u6293\u53d6\u610f\u56fe\u3002", "result": "MMIPN\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\uff0c\u865a\u62df\u5bfc\u7eb3\u6a21\u578b\u51cf\u5c11\u4e86\u8def\u5f84\u957f\u5ea6\uff0c\u63d0\u5347\u4e86\u8fd0\u52a8\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408\u591a\u6a21\u6001\u7ebf\u7d22\u548c\u9690\u5f0f\u5f15\u5bfc\u5728\u865a\u62df\u73b0\u5b9e\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u6709\u6548\uff0c\u4e3a\u591a\u7269\u4f53\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02011", "pdf": "https://arxiv.org/pdf/2509.02011", "abs": "https://arxiv.org/abs/2509.02011", "authors": ["Beibei Zhou", "Zhiyuan Zhang", "Zhenbo Song", "Jianhui Guo", "Hui Kong"], "title": "Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy Weather Conditions", "categories": ["cs.RO"], "comment": null, "summary": "Deep learning-based LiDAR odometry is crucial for autonomous driving and\nrobotic navigation, yet its performance under adverse weather, especially\nsnowfall, remains challenging. Existing models struggle to generalize across\nconditions due to sensitivity to snow-induced noise, limiting real-world use.\nIn this work, we present an unsupervised LiDAR odometry model to close the gap\nbetween clear and snowy weather conditions. Our approach focuses on effective\ndenoising to mitigate the impact of snowflake noise and outlier points on pose\nestimation, while also maintaining computational efficiency for real-time\napplications.\n  To achieve this, we introduce a Patch Spatial Measure (PSM) module that\nevaluates the dispersion of points within each patch, enabling effective\ndetection of sparse and discrete noise.\n  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive\npoint-wise weights, enhancing their discriminative capacity within local\nregions. To support real-time performance, we first apply an intensity\nthreshold mask to quickly suppress dense snowflake clusters near the LiDAR, and\nthen perform multi-modal feature fusion to refine the point-wise weight\nprediction, improving overall robustness under adverse weather. Our model is\ntrained in clear weather conditions and rigorously tested across various\nscenarios, including snowy and dynamic. Extensive experimental results confirm\nthe effectiveness of our method, demonstrating robust performance in both clear\nand snowy weather. This advancement enhances the model's generalizability and\npaves the way for more reliable autonomous systems capable of operating across\na wider range of environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684LiDAR\u91cc\u7a0b\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u63d0\u5347\u5728\u96ea\u5929\u7b49\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LiDAR\u91cc\u7a0b\u8ba1\u5728\u6076\u52a3\u5929\u6c14\uff08\u5982\u96ea\u5929\uff09\u4e0b\u6027\u80fd\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528Patch Spatial Measure\u6a21\u5757\u68c0\u6d4b\u566a\u58f0\uff0c\u5e76\u901a\u8fc7Patch Point Weight Predictor\u5206\u914d\u6743\u91cd\uff0c\u7ed3\u5408\u5f3a\u5ea6\u9608\u503c\u548c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u4f18\u5316\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u8868\u73b0\u826f\u597d\uff0c\u63d0\u5347\u4e86\u5728\u6076\u52a3\u5929\u6c14\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u66f4\u5e7f\u6cdb\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.02055", "pdf": "https://arxiv.org/pdf/2509.02055", "abs": "https://arxiv.org/abs/2509.02055", "authors": ["Yang Zhang", "Chenwei Wang", "Ouyang Lu", "Yuan Zhao", "Yunfei Ge", "Zhenglong Sun", "Xiu Li", "Chi Zhang", "Chenjia Bai", "Xuelong Li"], "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance", "categories": ["cs.RO", "cs.AI"], "comment": "The first three authors contributed equally", "summary": "Vision-Language-Action (VLA) models pre-trained on large, diverse datasets\nshow remarkable potential for general-purpose robotic manipulation. However, a\nprimary bottleneck remains in adapting these models to downstream tasks,\nespecially when the robot's embodiment or the task itself differs from the\npre-training data. This discrepancy leads to a significant mismatch in action\ndistributions, demanding extensive data and compute for effective fine-tuning.\nTo address this challenge, we introduce \\textbf{Align-Then-stEer\n(\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation\nframework. \\texttt{ATE} first aligns disparate action spaces by constructing a\nunified latent space, where a variational autoencoder constrained by reverse KL\ndivergence embeds adaptation actions into modes of the pre-training action\nlatent distribution. Subsequently, it steers the diffusion- or flow-based VLA's\ngeneration process during fine-tuning via a guidance mechanism that pushes the\nmodel's output distribution towards the target domain. We conduct extensive\nexperiments on cross-embodiment and cross-task manipulation in both simulation\nand real world. Compared to direct fine-tuning of representative VLAs, our\nmethod improves the average multi-task success rate by up to \\textbf{9.8\\%} in\nsimulation and achieves a striking \\textbf{32\\% success rate gain} in a\nreal-world cross-embodiment setting. Our work presents a general and\nlightweight solution that greatly enhances the practicality of deploying VLA\nmodels to new robotic platforms and tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAlign-Then-stEer (ATE)\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9002\u5e94Vision-Language-Action (VLA)\u6a21\u578b\u5230\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u6216\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u52a8\u4f5c\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u9002\u5e94\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u6216\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u52a8\u4f5c\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\u3002", "method": "ATE\u6846\u67b6\u9996\u5148\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0d\u540c\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7136\u540e\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u53cd\u5411KL\u6563\u5ea6\u5d4c\u5165\u9002\u5e94\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u673a\u5236\u8c03\u6574\u751f\u6210\u8fc7\u7a0b\u4ee5\u9002\u5e94\u76ee\u6807\u57df\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cATE\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u8de8\u5e73\u53f0\u548c\u8de8\u4efb\u52a1\u64cd\u4f5c\u4e2d\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u6210\u529f\u7387\u6700\u9ad89.8%\u548c32%\u7684\u63d0\u5347\u3002", "conclusion": "ATE\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.02071", "pdf": "https://arxiv.org/pdf/2509.02071", "abs": "https://arxiv.org/abs/2509.02071", "authors": ["Guangzhen Sun", "Ye Ding", "Xiangyang Zhu"], "title": "A Geometric Method for Base Parameter Analysis in Robot Inertia Identification Based on Projective Geometric Algebra", "categories": ["cs.RO"], "comment": "20 pages, 10 figures", "summary": "This paper proposes a novel geometric method for analytically determining the\nbase inertial parameters of robotic systems. The rigid body dynamics is\nreformulated using projective geometric algebra, leading to a new\nidentification model named ``tetrahedral-point (TP)\" model. Based on the rigid\nbody TP model, coefficients in the regresoor matrix of the identification model\nare derived in closed-form, exhibiting clear geometric interpretations.\nBuilding directly from the dynamic model, three foundational principles for\nbase parameter analysis are proposed: the shared points principle, fixed points\nprinciple, and planar rotations principle. With these principles, algorithms\nare developed to automatically determine all the base parameters. The core\nalgorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),\nachieves $O(1)$-complexity theoretically following an $O(N)$-complexity\npreprocessing stage, where $N$ is the number of rigid bodies. The proposed\nmethod and algorithms are validated across four robots: Puma560, Unitree Go2, a\n2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all\ncases, the algorithms successfully identify the complete set of base\nparameters. Notably, the approach demonstrates high robustness and\ncomputational efficiency, particularly in the cases of PKMs. Through the\ncomprehensive demonstrations, the method is shown to be general, robust, and\nefficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u6790\u786e\u5b9a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u57fa\u7840\u60ef\u6027\u53c2\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u57fa\u7840\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u4e2d\u7684\u590d\u6742\u6027\u548c\u7f3a\u4e4f\u51e0\u4f55\u76f4\u89c2\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6295\u5f71\u51e0\u4f55\u4ee3\u6570\u91cd\u65b0\u8868\u8ff0\u521a\u4f53\u52a8\u529b\u5b66\uff0c\u63d0\u51fa\u201c\u56db\u9762\u4f53\u70b9\u201d\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u8be5\u6a21\u578b\u5f00\u53d1\u4e86\u52a8\u6001\u56de\u5f52\u5668\u96f6\u7a7a\u95f4\u751f\u6210\u5668(DRNG)\u7b97\u6cd5\u3002", "result": "\u65b9\u6cd5\u5728\u56db\u79cd\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u6210\u529f\uff0c\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5e76\u8054\u673a\u6784(PKMs)\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u53c2\u6570\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.02134", "pdf": "https://arxiv.org/pdf/2509.02134", "abs": "https://arxiv.org/abs/2509.02134", "authors": ["Andrea Eirale", "Matteo Leonetti", "Marcello Chiaberge"], "title": "Learning Social Heuristics for Human-Aware Path Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Social robotic navigation has been at the center of numerous studies in\nrecent years. Most of the research has focused on driving the robotic agent\nalong obstacle-free trajectories, respecting social distances from humans, and\npredicting their movements to optimize navigation. However, in order to really\nbe socially accepted, the robots must be able to attain certain social norms\nthat cannot arise from conventional navigation, but require a dedicated\nlearning process. We propose Heuristic Planning with Learned Social Value\n(HPLSV), a method to learn a value function encapsulating the cost of social\nnavigation, and use it as an additional heuristic in heuristic-search path\nplanning. In this preliminary work, we apply the methodology to the common\nsocial scenario of joining a queue of people, with the intention of\ngeneralizing to further human activities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u793e\u4f1a\u4ef7\u503c\u51fd\u6570\uff08HPLSV\uff09\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u4f5c\u4e3a\u542f\u53d1\u5f0f\u641c\u7d22\u8def\u5f84\u89c4\u5212\u7684\u989d\u5916\u542f\u53d1\uff0c\u7528\u4e8e\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002", "motivation": "\u76ee\u524d\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u907f\u969c\u548c\u4fdd\u6301\u793e\u4ea4\u8ddd\u79bb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u89c4\u8303\u7684\u9002\u5e94\u80fd\u529b\u3002\u4e3a\u4e86\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4f1a\u63a5\u53d7\uff0c\u673a\u5668\u4eba\u9700\u8981\u901a\u8fc7\u4e13\u95e8\u7684\u5b66\u4e60\u8fc7\u7a0b\u638c\u63e1\u793e\u4f1a\u89c4\u8303\u3002", "method": "\u63d0\u51faHeuristic Planning with Learned Social Value (HPLSV)\uff0c\u901a\u8fc7\u5b66\u4e60\u793e\u4f1a\u5bfc\u822a\u7684\u6210\u672c\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u542f\u53d1\u5f0f\u641c\u7d22\u8def\u5f84\u89c4\u5212\u7684\u989d\u5916\u542f\u53d1\u3002\u521d\u6b65\u5e94\u7528\u4e8e\u6392\u961f\u573a\u666f\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660eHPLSV\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u673a\u5668\u4eba\u5728\u6392\u961f\u7b49\u793e\u4ea4\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u672a\u6765\u53ef\u63a8\u5e7f\u5230\u66f4\u591a\u4eba\u7c7b\u6d3b\u52a8\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u793e\u4f1a\u4ef7\u503c\u51fd\u6570\u5e76\u5c06\u5176\u6574\u5408\u5230\u8def\u5f84\u89c4\u5212\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u793e\u4f1a\u9002\u5e94\u6027\u548c\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2509.02146", "pdf": "https://arxiv.org/pdf/2509.02146", "abs": "https://arxiv.org/abs/2509.02146", "authors": ["G. de Mathelin", "C. Hartl-Nesic", "A. Kugi"], "title": "Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for Optimal Industrial Robotic Work Cell Design", "categories": ["cs.RO"], "comment": "This work has been accepted to IFAC for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "The performance of industrial robotic work cells depends on optimizing\nvarious hyperparameters referring to the cell layout, such as robot base\nplacement, tool placement, and kinematic design. Achieving this requires a\nbilevel optimization approach, where the high-level optimization adjusts these\nhyperparameters, and the low-level optimization computes robot motions.\nHowever, computing the optimal robot motion is computationally infeasible,\nintroducing trade-offs in motion planning to make the problem tractable. These\ntrade-offs significantly impact the overall performance of the bilevel\noptimization, but their effects still need to be systematically evaluated. In\nthis paper, we introduce metrics to assess these trade-offs regarding\noptimality, time gain, robustness, and consistency. Through extensive\nsimulation studies, we investigate how simplifications in motion-level\noptimization affect the high-level optimization outcomes, balancing\ncomputational complexity with solution quality. The proposed algorithms are\napplied to find the time-optimal kinematic design for a modular robot in two\npalletization scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5ea6\u91cf\u6807\u51c6\u8bc4\u4f30\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6743\u8861\u5bf9\u6574\u4f53\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u4eff\u771f\u7814\u7a76\u4e2d\u9a8c\u8bc1\u7b97\u6cd5\u6548\u679c\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u5de5\u4f5c\u5355\u5143\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u4f18\u5316\u5404\u79cd\u8d85\u53c2\u6570\uff0c\u4f46\u8fd0\u52a8\u89c4\u5212\u7684\u6743\u8861\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u9ad8\u5c42\u4f18\u5316\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u4f4e\u5c42\u4f18\u5316\u8ba1\u7b97\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u9a8c\u8bc1\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u8bc4\u4f30\u8fd0\u52a8\u89c4\u5212\u6743\u8861\u7684\u6307\u6807\uff0c\u5e76\u5728\u4e24\u4e2a\u7801\u579b\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u8ba1\u7b97\u590d\u6742\u6027\u548c\u89e3\u7684\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u4f18\u5316\u5de5\u4e1a\u673a\u5668\u4eba\u5e03\u5c40\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.02163", "pdf": "https://arxiv.org/pdf/2509.02163", "abs": "https://arxiv.org/abs/2509.02163", "authors": ["Wenxiao Zhang", "Xiangrui Kong", "Conan Dewitt", "Thomas Br\u00e4unl", "Jin B. Hong"], "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u7ec4\u88c5\u3001\u72b6\u6001\u7ba1\u7406\u548c\u5b89\u5168\u9a8c\u8bc1\uff0c\u4ee5\u63d0\u9ad8LLM\u96c6\u6210\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u63a8\u52a8\u4e86\u51b3\u7b56\u548c\u9002\u5e94\u80fd\u529b\u7684\u8fdb\u6b65\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u653b\u51fb\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u62ec\u63d0\u793a\u7ec4\u88c5\u3001\u72b6\u6001\u7ba1\u7406\u548c\u5b89\u5168\u9a8c\u8bc1\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u6027\u80fd\u548c\u5b89\u5168\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u6ce8\u5165\u653b\u51fb\u4e0b\u6539\u5584\u4e8630.8%\uff0c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u590d\u6742\u73af\u5883\u4e2d\u63d0\u5347\u4e86325%\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86LLM\u673a\u5668\u4eba\u7cfb\u7edf\u5b89\u5168\u4e0e\u5b89\u5168\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2509.02204", "pdf": "https://arxiv.org/pdf/2509.02204", "abs": "https://arxiv.org/abs/2509.02204", "authors": ["Dario Ruggiero", "Mauro Mancini", "Elisa Capello"], "title": "Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai", "summary": "This paper presents an adaptive observer-based navigation strategy for\nspacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in\nproximity operations like formation flight and uncooperative target inspection.\nThe proposed method adjusts observer gains based on the estimated state to\nachieve fast convergence and low noise sensitivity in state estimation. A\nLyapunov-based analysis ensures stability and accuracy, while simulations using\nvision-based sensor data validate the approach under realistic conditions.\nCompared to classical observers with time-invariant gains, the proposed method\nenhances trajectory tracking precision and reduces control input switching,\nmaking it a promising solution for autonomous spacecraft localization and\ncontrol.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u89c2\u6d4b\u5668\u589e\u76ca\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u7528\u4e8e\u822a\u5929\u5668\u5728\u5706\u5f62\u76f8\u5bf9\u8f68\u9053\uff08CRO\uff09\u4e2d\u7684\u81ea\u4e3b\u5b9a\u4f4d\u548c\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u63a7\u5236\u8f93\u5165\u7684\u5207\u6362\u3002", "motivation": "\u89e3\u51b3\u822a\u5929\u5668\u5728\u8fd1\u8ddd\u79bb\u64cd\u4f5c\uff08\u5982\u7f16\u961f\u98de\u884c\u548c\u975e\u5408\u4f5c\u76ee\u6807\u68c0\u67e5\uff09\u4e2d\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLyapunov\u5206\u6790\u7684\u9002\u5e94\u6027\u89c2\u6d4b\u5668\u589e\u76ca\u8c03\u6574\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u4f4e\u566a\u58f0\u654f\u611f\u6027\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u4e0d\u53d8\u589e\u76ca\u89c2\u6d4b\u5668\u3002", "conclusion": "\u8be5\u7b56\u7565\u662f\u822a\u5929\u5668\u81ea\u4e3b\u5b9a\u4f4d\u548c\u63a7\u5236\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02275", "pdf": "https://arxiv.org/pdf/2509.02275", "abs": "https://arxiv.org/abs/2509.02275", "authors": ["Fengyi Wang", "Xiangyu Fu", "Nitish Thakor", "Gordon Cheng"], "title": "Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object and Pose Recognition Using Multimodal Signals", "categories": ["cs.RO"], "comment": null, "summary": "The human somatosensory system integrates multimodal sensory feedback,\nincluding tactile, proprioceptive, and thermal signals, to enable comprehensive\nperception and effective interaction with the environment. Inspired by the\nbiological mechanism, we present a sensorized soft anthropomorphic hand\nequipped with diverse sensors designed to emulate the sensory modalities of the\nhuman hand. This system incorporates biologically inspired encoding schemes\nthat convert multimodal sensory data into spike trains, enabling\nhighly-efficient processing through Spiking Neural Networks (SNNs). By\nutilizing these neuromorphic signals, the proposed framework achieves 97.14%\naccuracy in object recognition across varying poses, significantly\noutperforming previous studies on soft hands. Additionally, we introduce a\nnovel differentiator neuron model to enhance material classification by\ncapturing dynamic thermal responses. Our results demonstrate the benefits of\nmultimodal sensory fusion and highlight the potential of neuromorphic\napproaches for achieving efficient, robust, and human-like perception in\nrobotic systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u751f\u8f6f\u4f53\u673a\u68b0\u624b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u548c\u795e\u7ecf\u5f62\u6001\u4fe1\u53f7\u5904\u7406\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7269\u4f53\u8bc6\u522b\u548c\u6750\u6599\u5206\u7c7b\u3002", "motivation": "\u53d7\u4eba\u7c7b\u89e6\u89c9\u7cfb\u7edf\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u6a21\u62df\u4eba\u7c7b\u624b\u90e8\u591a\u6a21\u6001\u611f\u77e5\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u611f\u77e5\u4e0e\u4ea4\u4e92\u3002", "method": "\u8bbe\u8ba1\u4e86\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u7684\u8f6f\u4f53\u673a\u68b0\u624b\uff0c\u5e76\u91c7\u7528\u4eff\u751f\u7f16\u7801\u65b9\u6848\u5c06\u591a\u6a21\u6001\u6570\u636e\u8f6c\u6362\u4e3a\u8109\u51b2\u4fe1\u53f7\uff0c\u5229\u7528SNN\u8fdb\u884c\u5904\u7406\uff1b\u8fd8\u5f15\u5165\u4e86\u65b0\u578b\u5fae\u5206\u795e\u7ecf\u5143\u6a21\u578b\u4ee5\u63d0\u9ad8\u6750\u6599\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u7269\u4f53\u8bc6\u522b\u4e2d\u8fbe\u5230\u4e8697.14%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7814\u7a76\uff0c\u5e76\u5728\u52a8\u6001\u70ed\u54cd\u5e94\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u591a\u6a21\u6001\u4f20\u611f\u878d\u5408\u548c\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u4eff\u4eba\u5316\u7684\u611f\u77e5\u5c55\u793a\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.02283", "pdf": "https://arxiv.org/pdf/2509.02283", "abs": "https://arxiv.org/abs/2509.02283", "authors": ["Ruibin Zhang", "Fei Gao"], "title": "Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments", "categories": ["cs.RO"], "comment": null, "summary": "Accurate and robust environmental perception is crucial for robot autonomous\nnavigation. While current methods typically adopt optical sensors (e.g.,\ncamera, LiDAR) as primary sensing modalities, their susceptibility to visual\nocclusion often leads to degraded performance or complete system failure. In\nthis paper, we focus on agricultural scenarios where robots are exposed to the\nrisk of onboard sensor contamination. Leveraging radar's strong penetration\ncapability, we introduce a radar-based 3D environmental perception framework as\na viable alternative. It comprises three core modules designed for dense and\naccurate semantic perception: 1) Parallel frame accumulation to enhance\nsignal-to-noise ratio of radar raw data. 2) A diffusion model-based\nhierarchical learning framework that first filters radar sidelobe artifacts\nthen generates fine-grained 3D semantic point clouds. 3) A specifically\ndesigned sparse 3D network optimized for processing large-scale radar raw data.\nWe conducted extensive benchmark comparisons and experimental evaluations on a\nself-built dataset collected in real-world agricultural field scenes. Results\ndemonstrate that our method achieves superior structural and semantic\nprediction performance compared to existing methods, while simultaneously\nreducing computational and memory costs by 51.3% and 27.5%, respectively.\nFurthermore, our approach achieves complete reconstruction and accurate\nclassification of thin structures such as poles and wires-which existing\nmethods struggle to perceive-highlighting its potential for dense and accurate\n3D radar perception.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f7\u8fbe\u76843D\u73af\u5883\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u5149\u5b66\u4f20\u611f\u5668\u6613\u53d7\u6c61\u67d3\u548c\u906e\u6321\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5bfc\u822a\u4e3b\u8981\u4f9d\u8d56\u5149\u5b66\u4f20\u611f\u5668\uff0c\u4f46\u6613\u53d7\u89c6\u89c9\u906e\u6321\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u519c\u4e1a\u573a\u666f\u4e2d\u4f20\u611f\u5668\u6613\u53d7\u6c61\u67d3\u3002\u96f7\u8fbe\u56e0\u5176\u7a7f\u900f\u80fd\u529b\u5f3a\uff0c\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5e76\u884c\u5e27\u7d2f\u79ef\u589e\u5f3a\u4fe1\u566a\u6bd4\uff1b2) \u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5c42\u6b21\u5b66\u4e60\u6846\u67b6\uff1b3) \u7a00\u758f3D\u7f51\u7edc\u5904\u7406\u5927\u89c4\u6a21\u96f7\u8fbe\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u9884\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u5206\u522b\u51cf\u5c1151.3%\u548c27.5%\uff0c\u80fd\u51c6\u786e\u91cd\u5efa\u7ec6\u957f\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u96f7\u8fbe\u5728\u5bc6\u96c6\u4e14\u7cbe\u786e\u76843D\u611f\u77e5\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u519c\u4e1a\u7b49\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.02324", "pdf": "https://arxiv.org/pdf/2509.02324", "abs": "https://arxiv.org/abs/2509.02324", "authors": ["Changshi Zhou", "Haichuan Xu", "Ningquan Gu", "Zhipeng Wang", "Bin Cheng", "Pengpeng Zhang", "Yanchao Dong", "Mitsuhiro Hayashibe", "Yanmin Zhou", "Bin He"], "title": "Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception", "categories": ["cs.RO"], "comment": null, "summary": "Language-guided long-horizon manipulation of deformable objects presents\nsignificant challenges due to high degrees of freedom, complex dynamics, and\nthe need for accurate vision-language grounding. In this work, we focus on\nmulti-step cloth folding, a representative deformable-object manipulation task\nthat requires both structured long-horizon planning and fine-grained visual\nperception. To this end, we propose a unified framework that integrates a Large\nLanguage Model (LLM)-based planner, a Vision-Language Model (VLM)-based\nperception system, and a task execution module. Specifically, the LLM-based\nplanner decomposes high-level language instructions into low-level action\nprimitives, bridging the semantic-execution gap, aligning perception with\naction, and enhancing generalization. The VLM-based perception module employs a\nSigLIP2-driven architecture with a bidirectional cross-attention fusion\nmechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to\nachieve language-conditioned fine-grained visual grounding. Experiments in both\nsimulation and real-world settings demonstrate the method's effectiveness. In\nsimulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3\non seen instructions, unseen instructions, and unseen tasks, respectively. On a\nreal robot, it robustly executes multi-step folding sequences from language\ninstructions across diverse cloth materials and configurations, demonstrating\nstrong generalization in practical scenarios. Project page:\nhttps://language-guided.netlify.app/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408LLM\u89c4\u5212\u548cVLM\u611f\u77e5\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bed\u8a00\u5f15\u5bfc\u7684\u5e03\u6599\u6298\u53e0\u4efb\u52a1\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6267\u884c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u65f6\u7a0b\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u4e2d\u7684\u9ad8\u81ea\u7531\u5ea6\u3001\u590d\u6742\u52a8\u6001\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6574\u5408LLM\u89c4\u5212\u5668\u548cVLM\u611f\u77e5\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5176\u4e2dLLM\u5206\u89e3\u6307\u4ee4\u4e3a\u52a8\u4f5c\u539f\u8bed\uff0cVLM\u901a\u8fc7SigLIP2\u67b6\u6784\u548cDoRA\u5fae\u8c03\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u63d0\u5347\u57fa\u7ebf\u65b9\u6cd52.23\u30011.87\u548c33.3\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u65f6\u7a0b\u5e03\u6599\u6298\u53e0\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.02343", "pdf": "https://arxiv.org/pdf/2509.02343", "abs": "https://arxiv.org/abs/2509.02343", "authors": ["Lan Wei", "Lou Genoud", "Dandan Zhang"], "title": "Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation", "categories": ["cs.RO"], "comment": "2025 IEEE International Conference on Cyborg and Bionic Systems (CBS\n  2025)", "summary": "Optical microrobots actuated by optical tweezers (OT) offer great potential\nfor biomedical applications such as cell manipulation and microscale assembly.\nThese tasks demand accurate three-dimensional perception to ensure precise\ncontrol in complex and dynamic biological environments. However, the\ntransparent nature of microrobots and low-contrast microscopic imaging\nchallenge conventional deep learning methods, which also require large\nannotated datasets that are costly to obtain. To address these challenges, we\npropose a physics-informed, data-efficient framework for depth estimation of\noptical microrobots. Our method augments convolutional feature extraction with\nphysics-based focus metrics, such as entropy, Laplacian of Gaussian, and\ngradient sharpness, calculated using an adaptive grid strategy. This approach\nallocates finer grids over microrobot regions and coarser grids over background\nareas, enhancing depth sensitivity while reducing computational complexity. We\nevaluate our framework on multiple microrobot types and demonstrate significant\nimprovements over baseline models. Specifically, our approach reduces mean\nsquared error (MSE) by over 60% and improves the coefficient of determination\n(R^2) across all test cases. Notably, even when trained on only 20% of the\navailable data, our model outperforms ResNet50 trained on the full dataset,\nhighlighting its robustness under limited data conditions. Our code is\navailable at: https://github.com/LannWei/CBS2025.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u9ad8\u6548\u6570\u636e\u6846\u67b6\uff0c\u7528\u4e8e\u5149\u5b66\u5fae\u673a\u5668\u4eba\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6307\u6807\u548c\u81ea\u9002\u5e94\u7f51\u683c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5149\u5b66\u5fae\u673a\u5668\u4eba\u7684\u900f\u660e\u6027\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u663e\u5fae\u6210\u50cf\uff0c\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u4e14\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u800c\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5377\u79ef\u7279\u5f81\u63d0\u53d6\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u805a\u7126\u6307\u6807\uff08\u5982\u71b5\u3001\u62c9\u666e\u62c9\u65af\u9ad8\u65af\u548c\u68af\u5ea6\u9510\u5ea6\uff09\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u7f51\u683c\u7b56\u7565\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5747\u65b9\u8bef\u5dee\u964d\u4f4e\u4e8660%\u4ee5\u4e0a\uff0c\u51b3\u5b9a\u7cfb\u6570\u63d0\u5347\uff0c\u4e14\u5728\u4ec520%\u6570\u636e\u8bad\u7ec3\u65f6\u4ecd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u751f\u7269\u73af\u5883\u4e2d\u7684\u5fae\u673a\u5668\u4eba\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2509.02425", "pdf": "https://arxiv.org/pdf/2509.02425", "abs": "https://arxiv.org/abs/2509.02425", "authors": ["Yifan Xu", "Qianwei Wang", "Vineet Kamat", "Carol Menassa"], "title": "OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments", "categories": ["cs.RO", "cs.HC"], "comment": "32 pages, 6 figures", "summary": "Indoor built environments like homes and offices often present complex and\ncluttered layouts that pose significant challenges for individuals who are\nblind or visually impaired, especially when performing tasks that involve\nlocating and gathering multiple objects. While many existing assistive\ntechnologies focus on basic navigation or obstacle avoidance, few systems\nprovide scalable and efficient multi-object search capabilities in real-world,\npartially observable settings. To address this gap, we introduce OpenGuide, an\nassistive mobile robot system that combines natural language understanding with\nvision-language foundation models (VLM), frontier-based exploration, and a\nPartially Observable Markov Decision Process (POMDP) planner. OpenGuide\ninterprets open-vocabulary requests, reasons about object-scene relationships,\nand adaptively navigates and localizes multiple target items in novel\nenvironments. Our approach enables robust recovery from missed detections\nthrough value decay and belief-space reasoning, resulting in more effective\nexploration and object localization. We validate OpenGuide in simulated and\nreal-world experiments, demonstrating substantial improvements in task success\nrate and search efficiency over prior methods. This work establishes a\nfoundation for scalable, human-centered robotic assistance in assisted living\nenvironments.", "AI": {"tldr": "OpenGuide\u662f\u4e00\u4e2a\u8f85\u52a9\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e2e\u52a9\u76f2\u4eba\u6216\u89c6\u89c9\u969c\u788d\u8005\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u9ad8\u6548\u5b9a\u4f4d\u591a\u4e2a\u76ee\u6807\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u6280\u672f\u591a\u4e3a\u57fa\u672c\u5bfc\u822a\u6216\u907f\u969c\uff0c\u7f3a\u4e4f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u9ad8\u6548\u5b9a\u4f4d\u591a\u7269\u4f53\u7684\u80fd\u529b\u3002OpenGuide\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u524d\u6cbf\u63a2\u7d22\u548cPOMDP\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u8bf7\u6c42\u89e3\u91ca\u3001\u7269\u4f53-\u573a\u666f\u5173\u7cfb\u63a8\u7406\u53ca\u81ea\u9002\u5e94\u5bfc\u822a\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0cOpenGuide\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u641c\u7d22\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OpenGuide\u4e3a\u8f85\u52a9\u751f\u6d3b\u73af\u5883\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u5960\u5b9a\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2509.02437", "pdf": "https://arxiv.org/pdf/2509.02437", "abs": "https://arxiv.org/abs/2509.02437", "authors": ["Yanwen Zou", "Zhaoye Zhou", "Chenyang Shi", "Zewei Ye", "Junda Huang", "Yan Ding", "Bo Zhao"], "title": "U-ARM : Ultra low-cost general teleoperation interface for robot manipulation", "categories": ["cs.RO"], "comment": null, "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\$50.5 for the 6-DoF leader arm and\n\\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.", "AI": {"tldr": "U-Arm\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u5feb\u901f\u9002\u914d\u7684\u9886\u5bfc\u8005-\u8ffd\u968f\u8005\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4e09\u79cd\u4e0d\u540c\u7ed3\u6784\u76843D\u6253\u5370\u9886\u5bfc\u8005\u624b\u81c2\uff0c\u517c\u5bb9\u591a\u79cd\u5546\u7528\u673a\u5668\u4eba\uff0cBOM\u6210\u672c\u4f4e\uff0c\u6570\u636e\u6536\u96c6\u6548\u7387\u9ad8\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u6210\u672c\u9ad8\u3001\u9002\u914d\u6027\u5dee\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e09\u79cd3D\u6253\u5370\u9886\u5bfc\u8005\u624b\u81c2\uff0c\u4f18\u5316\u673a\u68b0\u8bbe\u8ba1\u548c\u4f3a\u670d\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u673a\u68b0\u548c\u63a7\u5236\u4f18\u5316\u51cf\u5c11\u5197\u4f59\u81ea\u7531\u5ea6\u63a7\u5236\u7684\u6311\u6218\u3002", "result": "U-Arm\u7684BOM\u6210\u672c\u4ec5\u4e3a50.5\u7f8e\u5143\uff086-DoF\uff09\u548c56.8\u7f8e\u5143\uff087-DoF\uff09\uff0c\u6570\u636e\u6536\u96c6\u6548\u7387\u63d0\u9ad839%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u540c\u7c7b\u4ea7\u54c1\u76f8\u5f53\u3002", "conclusion": "U-Arm\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u8fdc\u7a0b\u64cd\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5546\u4e1a\u673a\u5668\u4eba\u914d\u7f6e\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2509.02453", "pdf": "https://arxiv.org/pdf/2509.02453", "abs": "https://arxiv.org/abs/2509.02453", "authors": ["Steven Swanbeck", "Mitch Pryor"], "title": "Coral: A Unifying Abstraction Layer for Composable Robotics Software", "categories": ["cs.RO"], "comment": null, "summary": "Despite the multitude of excellent software components and tools available in\nthe robotics and broader software engineering communities, successful\nintegration of software for robotic systems remains a time-consuming and\nchallenging task for users of all knowledge and skill levels. And with robotics\nsoftware often being built into tightly coupled, monolithic systems, even minor\nalterations to improve performance, adjust to changing task requirements, or\ndeploy to new hardware can require significant engineering investment. To help\nsolve this problem, this paper presents Coral, an abstraction layer for\nbuilding, deploying, and coordinating independent software components that\nmaximizes composability to allow for rapid system integration without modifying\nlow-level code. Rather than replacing existing tools, Coral complements them by\nintroducing a higher-level abstraction that constrains the integration process\nto semantically meaningful choices, reducing the configuration burden without\nlimiting adaptability to diverse domains, systems, and tasks. We describe Coral\nin detail and demonstrate its utility in integrating software for scenarios of\nincreasing complexity, including LiDAR-based SLAM and multi-robot corrosion\nmitigation tasks. By enabling practical composability in robotics software,\nCoral offers a scalable solution to a broad range of robotics system\nintegration challenges, improving component reusability, system\nreconfigurability, and accessibility to both expert and non-expert users. We\nrelease Coral open source.", "AI": {"tldr": "Coral\u662f\u4e00\u4e2a\u62bd\u8c61\u5c42\uff0c\u65e8\u5728\u901a\u8fc7\u6700\u5927\u5316\u7ec4\u5408\u6027\u6765\u5feb\u901f\u96c6\u6210\u673a\u5668\u4eba\u7cfb\u7edf\u8f6f\u4ef6\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u4ee3\u7801\u3002\u5b83\u63d0\u5347\u4e86\u7ec4\u4ef6\u53ef\u91cd\u7528\u6027\u548c\u7cfb\u7edf\u53ef\u91cd\u6784\u6027\uff0c\u9002\u7528\u4e8e\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8f6f\u4ef6\u96c6\u6210\u8017\u65f6\u4e14\u590d\u6742\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u56e0\u7d27\u5bc6\u8026\u5408\u7684\u5355\u4e00\u7cfb\u7edf\u5bfc\u81f4\u5fae\u5c0f\u6539\u52a8\u9700\u8981\u5927\u91cf\u6295\u5165\u7684\u6311\u6218\u3002", "method": "Coral\u4f5c\u4e3a\u62bd\u8c61\u5c42\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u7ea7\u522b\u7684\u8bed\u4e49\u7ea6\u675f\u6765\u534f\u8c03\u72ec\u7acb\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u51cf\u5c11\u914d\u7f6e\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u5e94\u6027\u3002", "result": "\u5728LiDAR-based SLAM\u548c\u591a\u673a\u5668\u4eba\u8150\u8680\u7f13\u89e3\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86Coral\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u63d0\u5347\u7ec4\u4ef6\u53ef\u91cd\u7528\u6027\u548c\u7cfb\u7edf\u53ef\u91cd\u6784\u6027\u3002", "conclusion": "Coral\u4e3a\u5e7f\u6cdb\u673a\u5668\u4eba\u7cfb\u7edf\u96c6\u6210\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u53d1\u5e03\u4fc3\u8fdb\u4e86\u5de5\u5177\u7684\u666e\u53ca\u3002"}}
{"id": "2509.02478", "pdf": "https://arxiv.org/pdf/2509.02478", "abs": "https://arxiv.org/abs/2509.02478", "authors": ["Haoran Li", "Yijiong Lin", "Chenghua Lu", "Max Yang", "Efi Psomopoulou", "Nathan F Lepora"], "title": "Classification of Vision-Based Tactile Sensors: A Review", "categories": ["cs.RO"], "comment": "15 pages", "summary": "Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u7684\u65b0\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u57fa\u4e8e\u6807\u8bb0\u7684\u4f20\u5bfc\u539f\u7406\u548c\u57fa\u4e8e\u5f3a\u5ea6\u7684\u4f20\u5bfc\u539f\u7406\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ec6\u5206\u4e3a\u56db\u79cd\u673a\u5236\u3002\u540c\u65f6\u8fdb\u884c\u4e86\u786c\u4ef6\u7279\u6027\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "VBTS\u56e0\u5176\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u4f4e\u5236\u9020\u6210\u672c\u548c\u6613\u5b9a\u5236\u6027\u5728\u673a\u5668\u4eba\u624b\u3001\u5939\u6301\u5668\u548c\u5047\u80a2\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u4f20\u611f\u5668\u5728\u4f20\u611f\u539f\u7406\u3001\u6750\u6599\u7ec4\u6210\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u4e0a\u5b58\u5728\u591a\u6837\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86VBTS\u7684\u4e24\u79cd\u4f20\u5bfc\u539f\u7406\uff1a\u57fa\u4e8e\u6807\u8bb0\u7684\u4f20\u5bfc\uff08\u68c0\u6d4b\u6807\u8bb0\u4f4d\u79fb\u548c\u5bc6\u5ea6\u53d8\u5316\uff09\u548c\u57fa\u4e8e\u5f3a\u5ea6\u7684\u4f20\u5bfc\uff08\u901a\u8fc7\u50cf\u7d20\u503c\u53d8\u5316\u6620\u5c04\u5916\u90e8\u5e72\u6270\uff09\uff0c\u5e76\u7ec6\u5206\u4e3a\u56db\u79cd\u673a\u5236\u3002\u901a\u8fc7\u6bd4\u8f83\u786c\u4ef6\u7279\u6027\u548c\u6570\u636e\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u56db\u79cdVBTS\u673a\u5236\u5404\u6709\u7279\u70b9\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86VBTS\u7684\u5206\u7c7b\u65b9\u6cd5\u548c\u7814\u7a76\u73b0\u72b6\uff0c\u4e3a\u672a\u6765\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.02527", "pdf": "https://arxiv.org/pdf/2509.02527", "abs": "https://arxiv.org/abs/2509.02527", "authors": ["Raphael St\u00f6ckner", "Pedro Roque", "Maria Charitidou", "Dimos V. Dimarogonas"], "title": "Fault-tolerant Model Predictive Control for Spacecraft", "categories": ["cs.RO"], "comment": "The paper has been submitted to CDC2025", "summary": "Given the cost and critical functions of satellite constellations, ensuring\nmission longevity and safe decommissioning is essential for space\nsustainability. This article presents a Model Predictive Control for spacecraft\ntrajectory and setpoint stabilization under multiple actuation failures. The\nproposed solution allows us to efficiently control the faulty spacecraft\nenabling safe navigation towards servicing or collision-free trajectories. The\nproposed scheme ensures closed-loop asymptotic stability and is shown to be\nrecursively feasible. We demonstrate its efficacy through open-source numerical\nresults and realistic experiments using the ATMOS platform.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u536b\u661f\u591a\u6267\u884c\u5668\u6545\u969c\u60c5\u51b5\u4e0b\u7a33\u5b9a\u822a\u5929\u5668\u8f68\u8ff9\uff0c\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u548c\u4efb\u52a1\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u536b\u661f\u661f\u5ea7\u7684\u6210\u672c\u548c\u5173\u952e\u529f\u80fd\u4f7f\u5176\u4efb\u52a1\u5bff\u547d\u548c\u5b89\u5168\u9000\u5f79\u5bf9\u7a7a\u95f4\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u5904\u7406\u591a\u6267\u884c\u5668\u6545\u969c\u4e0b\u7684\u822a\u5929\u5668\u8f68\u8ff9\u548c\u8bbe\u5b9a\u70b9\u7a33\u5b9a\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u786e\u4fdd\u4e86\u95ed\u73af\u6e10\u8fdb\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6570\u503c\u7ed3\u679c\u548cATMOS\u5e73\u53f0\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u63a7\u5236\u6545\u969c\u822a\u5929\u5668\uff0c\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\uff0c\u4e3a\u7a7a\u95f4\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u6280\u672f\u4fdd\u969c\u3002"}}
{"id": "2509.02530", "pdf": "https://arxiv.org/pdf/2509.02530", "abs": "https://arxiv.org/abs/2509.02530", "authors": ["Minghuan Liu", "Zhengbang Zhu", "Xiaoshen Han", "Peng Hu", "Haotong Lin", "Xinyao Li", "Jingxiao Chen", "Jiafeng Xu", "Yichu Yang", "Yunfeng Lin", "Xinghang Li", "Yong Yu", "Weinan Zhang", "Tao Kong", "Bingyi Kang"], "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "32 pages, 18 figures, project page:\n  https://manipulation-as-in-simulation.github.io/", "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDMs\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u76f8\u673a\u7ed3\u5408RGB\u56fe\u50cf\u548c\u539f\u59cb\u6df1\u5ea6\u4fe1\u53f7\u751f\u6210\u53bb\u566a\u7684\u7cbe\u786e\u5ea6\u91cf\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u4e3b\u8981\u4f9d\u8d562D\u89c6\u89c9\u4fe1\u606f\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u4eba\u7c7b\u4f9d\u8d563D\u51e0\u4f55\u4fe1\u606f\u3002\u5982\u4f55\u5229\u7528\u6df1\u5ea6\u76f8\u673a\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u63d0\u51fa\u4e86Camera Depth Models (CDMs)\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u5f15\u64ce\u4ece\u6a21\u62df\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u914d\u5bf9\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u4ee5\u8f93\u51fa\u7cbe\u786e\u6df1\u5ea6\u3002", "result": "CDMs\u5728\u6df1\u5ea6\u9884\u6d4b\u4e0a\u8fbe\u5230\u63a5\u8fd1\u6a21\u62df\u7ea7\u522b\u7684\u51c6\u786e\u5ea6\uff0c\u65e0\u9700\u6dfb\u52a0\u566a\u58f0\u6216\u771f\u5b9e\u4e16\u754c\u5fae\u8c03\uff0c\u5373\u53ef\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4e3a\u5229\u75283D\u4fe1\u606f\u7684\u673a\u5668\u4eba\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.00040", "pdf": "https://arxiv.org/pdf/2509.00040", "abs": "https://arxiv.org/abs/2509.00040", "authors": ["Chengkai Dai", "Tao Liu", "Dezhao Guo", "Binzhi Sun", "Guoxin Fang", "Yeung Yam", "Charlie C. L. Wang"], "title": "Curve-based slicer for multi-axis DLP 3D printing", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "This paper introduces a novel curve-based slicing method for generating\nplanar layers with dynamically varying orientations in digital light processing\n(DLP) 3D printing. Our approach effectively addresses key challenges in DLP\nprinting, such as regions with large overhangs and staircase artifacts, while\npreserving its intrinsic advantages of high resolution and fast printing\nspeeds. We formulate the slicing problem as an optimization task, in which\nparametric curves are computed to define both the slicing layers and the model\npartitioning through their tangent planes. These curves inherently define\nmotion trajectories for the build platform and can be optimized to meet\ncritical manufacturing objectives, including collision-free motion and\nfloating-free deposition. We validate our method through physical experiments\non a robotic multi-axis DLP printing setup, demonstrating that the optimized\ncurves can robustly guide smooth, high-quality fabrication of complex\ngeometries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u66f2\u7ebf\u7684\u5207\u7247\u65b9\u6cd5\uff0c\u7528\u4e8eDLP 3D\u6253\u5370\u4e2d\u52a8\u6001\u53d8\u5316\u65b9\u5411\u7684\u5e73\u9762\u5c42\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5927\u60ac\u5782\u548c\u9636\u68af\u6548\u5e94\u7b49\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3DLP\u6253\u5370\u4e2d\u7684\u5927\u60ac\u5782\u533a\u57df\u548c\u9636\u68af\u6548\u5e94\u7b49\u5173\u952e\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u548c\u5feb\u901f\u6253\u5370\u901f\u5ea6\u7684\u4f18\u52bf\u3002", "method": "\u5c06\u5207\u7247\u95ee\u9898\u5efa\u6a21\u4e3a\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u53c2\u6570\u66f2\u7ebf\u5b9a\u4e49\u5207\u7247\u5c42\u548c\u6a21\u578b\u5206\u533a\uff0c\u4f18\u5316\u66f2\u7ebf\u4ee5\u6ee1\u8db3\u65e0\u78b0\u649e\u8fd0\u52a8\u548c\u65e0\u60ac\u7a7a\u6c89\u79ef\u7b49\u5236\u9020\u76ee\u6807\u3002", "result": "\u5728\u673a\u5668\u4eba\u591a\u8f74DLP\u6253\u5370\u88c5\u7f6e\u4e0a\u8fdb\u884c\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f18\u5316\u540e\u7684\u66f2\u7ebf\u80fd\u591f\u7a33\u5065\u5730\u6307\u5bfc\u590d\u6742\u51e0\u4f55\u4f53\u7684\u9ad8\u8d28\u91cf\u6253\u5370\u3002", "conclusion": "\u8be5\u66f2\u7ebf\u4f18\u5316\u65b9\u6cd5\u5728DLP\u6253\u5370\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u51e0\u4f55\u4f53\u7684\u6253\u5370\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2509.00048", "pdf": "https://arxiv.org/pdf/2509.00048", "abs": "https://arxiv.org/abs/2509.00048", "authors": ["Methusela Sulle", "Judith Mwakalonge", "Gurcan Comert", "Saidi Siuhi", "Nana Kankam Gyimah"], "title": "Harnessing ADAS for Pedestrian Safety: A Data-Driven Exploration of Fatality Reduction", "categories": ["cs.CY", "cs.RO"], "comment": null, "summary": "Pedestrian fatalities continue to rise in the United States, driven by\nfactors such as human distraction, increased vehicle size, and complex traffic\nenvironments. Advanced Driver Assistance Systems (ADAS) offer a promising\navenue for improving pedestrian safety by enhancing driver awareness and\nvehicle responsiveness. This study conducts a comprehensive data-driven\nanalysis utilizing the Fatality Analysis Reporting System (FARS) to quantify\nthe effectiveness of specific ADAS features like Pedestrian Automatic Emergency\nBraking (PAEB), Forward Collision Warning (FCW), and Lane Departure Warning\n(LDW), in lowering pedestrian fatalities. By linking vehicle specifications\nwith crash data, we assess how ADAS performance varies under different\nenvironmental and behavioral conditions, such as lighting, weather, and\ndriver/pedestrian distraction. Results indicate that while ADAS can reduce\ncrash severity and prevent some fatalities, its effectiveness is diminished in\nlow-light and adverse weather. The findings highlight the need for enhanced\nsensor technologies and improved driver education. This research informs\npolicymakers, transportation planners, and automotive manufacturers on\noptimizing ADAS deployment to improve pedestrian safety and reduce\ntraffic-related deaths.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8eFARS\u6570\u636e\u5206\u6790\u4e86ADAS\uff08\u5982PAEB\u3001FCW\u548cLDW\uff09\u5728\u964d\u4f4e\u884c\u4eba\u6b7b\u4ea1\u7387\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u4f4e\u5149\u548c\u6076\u52a3\u5929\u6c14\u4e0b\u6548\u679c\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u4f20\u611f\u5668\u6280\u672f\u548c\u9a7e\u9a76\u5458\u6559\u80b2\u3002", "motivation": "\u7f8e\u56fd\u884c\u4eba\u6b7b\u4ea1\u7387\u6301\u7eed\u4e0a\u5347\uff0cADAS\u88ab\u8ba4\u4e3a\u662f\u901a\u8fc7\u63d0\u5347\u9a7e\u9a76\u5458\u610f\u8bc6\u548c\u8f66\u8f86\u53cd\u5e94\u80fd\u529b\u6765\u6539\u5584\u884c\u4eba\u5b89\u5168\u7684\u6709\u6f5c\u529b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528FARS\u6570\u636e\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u8bc4\u4f30ADAS\u5728\u4e0d\u540c\u73af\u5883\u548c\u884c\u4e3a\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u5982\u5149\u7167\u3001\u5929\u6c14\u548c\u9a7e\u9a76\u5458/\u884c\u4eba\u5206\u5fc3\u3002", "result": "ADAS\u80fd\u964d\u4f4e\u4e8b\u6545\u4e25\u91cd\u6027\u548c\u90e8\u5206\u6b7b\u4ea1\u7387\uff0c\u4f46\u5728\u4f4e\u5149\u548c\u6076\u52a3\u5929\u6c14\u65f6\u6548\u679c\u4e0b\u964d\u3002", "conclusion": "\u9700\u6539\u8fdb\u4f20\u611f\u5668\u6280\u672f\u548c\u9a7e\u9a76\u5458\u6559\u80b2\uff0c\u4ee5\u4f18\u5316ADAS\u90e8\u7f72\uff0c\u63d0\u5347\u884c\u4eba\u5b89\u5168\u6027\u3002"}}
{"id": "2509.00061", "pdf": "https://arxiv.org/pdf/2509.00061", "abs": "https://arxiv.org/abs/2509.00061", "authors": ["Ekansh Singh"], "title": "Design and Testing of a Low-Cost 3D-Printed Servo Gimbal for Thrust Vector Control in Model Rockets", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "8 pages, 6 figures, 1 table", "summary": "Thrust vector control (TVC) is a key mechanism for stabilizing rockets during\nflight, yet conventional implementations remain costly and technically\ninaccessible to students and hobbyists. This paper presents the design,\nfabrication, and testing of a low-cost, 3D-printed, servo-driven\ntwo-dimensional gimbal developed for model rocket applications. The gimbal\nunderwent more than 60 CAD iterations, with servo selection guided by torque,\nresponse time, and stability requirements. A high-speed camera and Fusion 360\nparameter simulations were used to emulate dynamic instability, enabling\nevaluation of angular deflection, servo responsiveness, and structural\ndurability. The results demonstrated stable actuation within plus or minus 5\ndegrees, with response times on the average order of 44.5 ms, while limitations\nincluded servo fatigue and pin-joint stress under extended loading. The project\nhighlights the feasibility of student-accessible thrust vector control systems\nand their potential as a reproducible platform for STEM education and\nexperimental aerospace research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u30013D\u6253\u5370\u7684\u4e8c\u7ef4\u4e07\u5411\u8282\u8bbe\u8ba1\uff0c\u7528\u4e8e\u6a21\u578b\u706b\u7bad\u63a8\u529b\u77e2\u91cf\u63a7\u5236\uff08TVC\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728STEM\u6559\u80b2\u548c\u5b9e\u9a8c\u822a\u7a7a\u822a\u5929\u7814\u7a76\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u7cfb\u7edf\u6210\u672c\u9ad8\u3001\u6280\u672f\u590d\u6742\uff0c\u96be\u4ee5\u666e\u53ca\u7ed9\u5b66\u751f\u548c\u7231\u597d\u8005\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u6613\u5b9e\u73b0\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7CAD\u8fed\u4ee3\u8bbe\u8ba1\u3001\u4f3a\u670d\u7535\u673a\u9009\u62e9\u548c\u9ad8\u901f\u6444\u50cf\u6a21\u62df\uff0c\u6d4b\u8bd5\u4e86\u4e07\u5411\u8282\u7684\u89d2\u504f\u8f6c\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u7ed3\u6784\u8010\u4e45\u6027\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u00b15\u5ea6\u7684\u7a33\u5b9a\u9a71\u52a8\uff0c\u5e73\u5747\u54cd\u5e94\u65f6\u95f444.5\u6beb\u79d2\uff0c\u4f46\u5b58\u5728\u4f3a\u670d\u75b2\u52b3\u548c\u9500\u63a5\u5e94\u529b\u95ee\u9898\u3002", "conclusion": "\u8be5\u9879\u76ee\u8bc1\u660e\u4e86\u4f4e\u6210\u672cTVC\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u9002\u5408\u4f5c\u4e3aSTEM\u6559\u80b2\u548c\u5b9e\u9a8c\u822a\u7a7a\u822a\u5929\u7814\u7a76\u7684\u5e73\u53f0\u3002"}}
{"id": "2509.00117", "pdf": "https://arxiv.org/pdf/2509.00117", "abs": "https://arxiv.org/abs/2509.00117", "authors": ["Jared Perlo", "Alexander Robey", "Fazl Barez", "Luciano Floridi", "Jakob M\u00f6kander"], "title": "Embodied AI: Emerging Risks and Opportunities for Policy Action", "categories": ["cs.CY", "cs.AI", "cs.RO"], "comment": null, "summary": "The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI\ncan exist in, learn from, reason about, and act in the physical world. Given\nrecent innovations in large language and multimodal models, along with\nincreasingly advanced and responsive hardware, EAI systems are rapidly growing\nin capabilities and operational domains. These advances present significant\nrisks, including physical harm from malicious use, mass surveillance, and\neconomic and societal disruption. However, these risks have been severely\noverlooked by policymakers. Existing policies, such as international standards\nfor industrial robots or statutes governing autonomous vehicles, are\ninsufficient to address the full range of concerns. While lawmakers are\nincreasingly focused on AI, there is now an urgent need to extend and adapt\nexisting frameworks to account for the unique risks of EAI. To help bridge this\ngap, this paper makes three contributions: first, we provide a foundational\ntaxonomy of key physical, informational, economic, and social EAI risks.\nSecondly, we analyze policies in the US, EU, and UK to identify how existing\nframeworks address these risks and where these policies leave critical gaps. We\nconclude by offering concrete policy recommendations to address the coming wave\nof EAI innovation, including mandatory testing and certification for EAI\nsystems, clarified liability frameworks, and forward-looking strategies to\nmanage and prepare for transformative economic and societal impacts.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u5b9e\u4f53\u5316AI\uff08EAI\uff09\u7684\u5feb\u901f\u53d1\u5c55\u53ca\u5176\u5e26\u6765\u7684\u98ce\u9669\uff0c\u5305\u62ec\u7269\u7406\u4f24\u5bb3\u3001\u5927\u89c4\u6a21\u76d1\u63a7\u548c\u793e\u4f1a\u7ecf\u6d4e\u7834\u574f\u3002\u73b0\u6709\u653f\u7b56\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u8fd9\u4e9b\u98ce\u9669\uff0c\u56e0\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u5206\u7c7b\u98ce\u9669\u3001\u5206\u6790\u653f\u7b56\u7f3a\u53e3\u53ca\u5177\u4f53\u653f\u7b56\u5efa\u8bae\u4e09\u9879\u8d21\u732e\u3002", "motivation": "\u5b9e\u4f53\u5316AI\u7684\u80fd\u529b\u548c\u5f71\u54cd\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u653f\u7b56\u6846\u67b6\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u5176\u72ec\u7279\u98ce\u9669\uff0c\u4e9f\u9700\u65b0\u653f\u7b56\u548c\u8c03\u6574\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5efa\u7acbEAI\u98ce\u9669\u5206\u7c7b\u4f53\u7cfb\uff0c\u5206\u6790\u7f8e\u6b27\u82f1\u73b0\u6709\u653f\u7b56\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u586b\u8865\u653f\u7b56\u7f3a\u53e3\u7684\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u653f\u7b56\u5b58\u5728\u663e\u8457\u7f3a\u53e3\uff0c\u5e76\u63d0\u51fa\u4e86\u5f3a\u5236\u6d4b\u8bd5\u8ba4\u8bc1\u3001\u8d23\u4efb\u6846\u67b6\u548c\u7ecf\u6d4e\u793e\u4f1a\u5f71\u54cd\u7ba1\u7406\u7b49\u5efa\u8bae\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86EAI\u98ce\u9669\u7684\u72ec\u7279\u6027\uff0c\u547c\u5401\u653f\u7b56\u5236\u5b9a\u8005\u8fc5\u901f\u884c\u52a8\uff0c\u63d0\u51fa\u5177\u4f53\u5efa\u8bae\u4ee5\u5e94\u5bf9\u5373\u5c06\u5230\u6765\u7684EAI\u521b\u65b0\u6d6a\u6f6e\u3002"}}
{"id": "2509.00294", "pdf": "https://arxiv.org/pdf/2509.00294", "abs": "https://arxiv.org/abs/2509.00294", "authors": ["Sergio A. Esteban", "Max H. Cohen", "Adrian B. Ghansah", "Aaron D. Ames"], "title": "A Layered Control Perspective on Legged Locomotion: Embedding Reduced Order Models via Hybrid Zero Dynamics", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Reduced-order models (ROMs) provide a powerful means of synthesizing dynamic\nwalking gaits on legged robots. Yet this approach lacks the formal guarantees\nenjoyed by methods that utilize the full-order model (FOM) for gait synthesis,\ne.g., hybrid zero dynamics. This paper aims to unify these approaches through a\nlayered control perspective. In particular, we establish conditions on when a\nROM of locomotion yields stable walking on the full-order hybrid dynamics. To\nachieve this result, given an ROM we synthesize a zero dynamics manifold\nencoding the behavior of the ROM -- controllers can be synthesized that drive\nthe FOM to this surface, yielding hybrid zero dynamics. We prove that a stable\nperiodic orbit in the ROM implies an input-to-state stable periodic orbit of\nthe FOM's hybrid zero dynamics, and hence the FOM dynamics. This result is\ndemonstrated in simulation on a linear inverted pendulum ROM and a 5-link\nplanar walking FOM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u7b80\u5316\u6a21\u578b\uff08ROM\uff09\u4e0e\u5168\u9636\u6a21\u578b\uff08FOM\uff09\u7ed3\u5408\uff0c\u8bc1\u660e\u4e86ROM\u4e2d\u7684\u7a33\u5b9a\u5468\u671f\u6027\u8f68\u9053\u53ef\u4ee5\u5bfc\u81f4FOM\u7684\u52a8\u6001\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709ROM\u65b9\u6cd5\u7f3a\u4e4fFOM\u65b9\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u56e0\u6b64\u8bba\u6587\u5e0c\u671b\u901a\u8fc7\u5206\u5c42\u63a7\u5236\u89c6\u89d2\u7edf\u4e00\u4e24\u8005\u3002", "method": "\u901a\u8fc7ROM\u5408\u6210\u96f6\u52a8\u6001\u6d41\u5f62\uff0c\u8bbe\u8ba1\u63a7\u5236\u5668\u9a71\u52a8FOM\u5230\u8be5\u6d41\u5f62\uff0c\u4ece\u800c\u5b9e\u73b0\u6df7\u5408\u96f6\u52a8\u6001\u3002", "result": "\u8bc1\u660e\u4e86ROM\u4e2d\u7684\u7a33\u5b9a\u5468\u671f\u6027\u8f68\u9053\u4f1a\u8f6c\u5316\u4e3aFOM\u7684\u52a8\u6001\u7a33\u5b9a\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aROM\u5728\u52a8\u6001\u6b65\u6001\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.00379", "pdf": "https://arxiv.org/pdf/2509.00379", "abs": "https://arxiv.org/abs/2509.00379", "authors": ["Jialiang Kang", "Jiawen Wang", "Dingsheng Luo"], "title": "Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "ICRA 2025", "summary": "Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous\ndriving. Traditional approaches rely on extensive annotated data for point\ncloud analysis, incurring high costs and time investments. In contrast,\nrealworld image datasets offer abundant availability and substantial scale. To\nmitigate the burden of annotating 3D LiDAR point clouds, we propose two\ncrossmodal knowledge distillation methods: Unsupervised Domain Adaptation\nKnowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge\nDistillation (FSKD). Leveraging readily available spatio-temporally\nsynchronized data from cameras and LiDARs in autonomous driving scenarios, we\ndirectly apply a pretrained 2D image model to unlabeled 2D data. Through\ncrossmodal knowledge distillation with known 2D-3D correspondence, we actively\nalign the output of the 3D network with the corresponding points of the 2D\nnetwork, thereby obviating the necessity for 3D annotations. Our focus is on\npreserving modality-general information while filtering out modality-specific\ndetails during crossmodal distillation. To achieve this, we deploy\nself-calibrated convolution on 3D point clouds as the foundation of our domain\nadaptation module. Rigorous experimentation validates the effectiveness of our\nproposed methods, consistently surpassing the performance of state-of-the-art\napproaches in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08UDAKD\u548cFSKD\uff09\uff0c\u901a\u8fc7\u5229\u75282D\u56fe\u50cf\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u51cf\u5c11\u5bf93D\u70b9\u4e91\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf3D LiDAR\u70b9\u4e91\u5206\u5272\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\uff1b\u53ef\u5229\u7528\u4e30\u5bcc\u76842D\u56fe\u50cf\u6570\u636e\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "method": "\u63d0\u51faUDAKD\u548cFSKD\u4e24\u79cd\u65b9\u6cd5\uff0c\u5229\u75282D-3D\u5bf9\u5e94\u5173\u7cfb\u5bf9\u9f503D\u7f51\u7edc\u8f93\u51fa\u4e0e2D\u7f51\u7edc\uff0c\u91c7\u7528\u81ea\u6821\u51c6\u5377\u79ef\u5904\u74063D\u70b9\u4e91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u57283D LiDAR\u8bed\u4e49\u5206\u5272\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u80fd\u6709\u6548\u51cf\u5c113D\u6807\u6ce8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u8de8\u6a21\u6001\u4efb\u52a1\u3002"}}
{"id": "2509.00433", "pdf": "https://arxiv.org/pdf/2509.00433", "abs": "https://arxiv.org/abs/2509.00433", "authors": ["Houshu He", "Naifeng Jing", "Li Jiang", "Xiaoyao Liang", "Zhuoran Song"], "title": "AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection", "categories": ["cs.AR", "cs.RO"], "comment": "15 pages", "summary": "Simultaneous Localization and Mapping (SLAM) is a critical task that enables\nautonomous vehicles to construct maps and localize themselves in unknown\nenvironments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting\n(3DGS) to achieve exceptional reconstruction fidelity. However, existing\n3DGS-SLAM systems provide insufficient throughput due to the need for multiple\ntraining iterations per frame and the vast number of Gaussians.\n  In this paper, we propose AGS, an algorithm-hardware co-design framework to\nboost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems\nprocess frames in a streaming manner, where adjacent frames exhibit high\nsimilarity that can be utilized for acceleration. On the software level: 1) We\npropose a coarse-then-fine-grained pose tracking method with respect to the\nrobot's movement. 2) We avoid redundant computations of Gaussians by sharing\ntheir contribution information across frames. On the hardware level, we propose\na frame covisibility detection engine to extract intermediate data from the\nvideo CODEC. We also implement a pose tracking engine and a mapping engine with\nworkload schedulers to efficiently deploy the AGS algorithm. Our evaluation\nshows that AGS achieves up to $17.12\\times$, $6.71\\times$, and $5.41\\times$\nspeedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS\naccelerator, GSCore.", "AI": {"tldr": "AGS\u662f\u4e00\u79cd\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u90bb\u5e27\u7684\u9ad8\u76f8\u4f3c\u6027\uff0c\u63d0\u53473DGS-SLAM\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u67093DGS-SLAM\u7cfb\u7edf\u56e0\u6bcf\u5e27\u9700\u8981\u591a\u6b21\u8bad\u7ec3\u8fed\u4ee3\u548c\u5927\u91cf\u9ad8\u65af\u5206\u5e03\u8ba1\u7b97\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002AGS\u65e8\u5728\u901a\u8fc7\u7b97\u6cd5\u548c\u786c\u4ef6\u7684\u534f\u540c\u4f18\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8f6f\u4ef6\u5c42\u9762\uff1a1\uff09\u63d0\u51fa\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u4f4d\u59ff\u8ddf\u8e2a\u65b9\u6cd5\u30022\uff09\u901a\u8fc7\u8de8\u5e27\u5171\u4eab\u9ad8\u65af\u5206\u5e03\u8d21\u732e\u4fe1\u606f\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002\u786c\u4ef6\u5c42\u9762\uff1a\u8bbe\u8ba1\u5e27\u5171\u89c6\u68c0\u6d4b\u5f15\u64ce\u3001\u4f4d\u59ff\u8ddf\u8e2a\u5f15\u64ce\u548c\u6620\u5c04\u5f15\u64ce\uff0c\u5e76\u914d\u5907\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAGS\u76f8\u6bd4\u79fb\u52a8\u548c\u9ad8\u6027\u80fdGPU\u53ca\u73b0\u67093DGS\u52a0\u901f\u5668GSCore\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad817.12\u500d\u30016.71\u500d\u548c5.41\u500d\u7684\u52a0\u901f\u3002", "conclusion": "AGS\u901a\u8fc7\u5145\u5206\u5229\u7528\u5e27\u95f4\u76f8\u4f3c\u6027\u548c\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863DGS-SLAM\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2509.00649", "pdf": "https://arxiv.org/pdf/2509.00649", "abs": "https://arxiv.org/abs/2509.00649", "authors": ["Aviral Chharia", "Wenbo Gou", "Haoye Dong"], "title": "MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025; Project Website: https://aviralchharia.github.io/MV-SSM", "summary": "While significant progress has been made in single-view 3D human pose\nestimation, multi-view 3D human pose estimation remains challenging,\nparticularly in terms of generalizing to new camera configurations. Existing\nattention-based transformers often struggle to accurately model the spatial\narrangement of keypoints, especially in occluded scenarios. Additionally, they\ntend to overfit specific camera arrangements and visual scenes from training\ndata, resulting in substantial performance drops in new settings. In this\nstudy, we introduce a novel Multi-View State Space Modeling framework, named\nMV-SSM, for robustly estimating 3D human keypoints. We explicitly model the\njoint spatial sequence at two distinct levels: the feature level from\nmulti-view images and the person keypoint level. We propose a Projective State\nSpace (PSS) block to learn a generalized representation of joint spatial\narrangements using state space modeling. Moreover, we modify Mamba's\ntraditional scanning into an effective Grid Token-guided Bidirectional Scanning\n(GTBS), which is integral to the PSS block. Multiple experiments demonstrate\nthat MV-SSM achieves strong generalization, outperforming state-of-the-art\nmethods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU\nPanoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP\n(+38%) on Campus A1 in cross-dataset evaluations. Project Website:\nhttps://aviralchharia.github.io/MV-SSM", "AI": {"tldr": "MV-SSM\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u89d2\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7279\u5f81\u548c\u5173\u952e\u70b9\u7ea7\u522b\u7684\u7a7a\u95f4\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65b0\u6444\u50cf\u5934\u914d\u7f6e\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u591a\u89c6\u89d23D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u65b0\u6444\u50cf\u5934\u914d\u7f6e\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u7a7a\u95f4\u5173\u952e\u70b9\u5e03\u5c40\uff0c\u5c24\u5176\u5728\u906e\u6321\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faMV-SSM\u6846\u67b6\uff0c\u5305\u62ec\u6295\u5f71\u72b6\u6001\u7a7a\u95f4\uff08PSS\uff09\u5757\u548c\u7f51\u683c\u4ee4\u724c\u5f15\u5bfc\u53cc\u5411\u626b\u63cf\uff08GTBS\uff09\uff0c\u663e\u5f0f\u5efa\u6a21\u591a\u89c6\u89d2\u56fe\u50cf\u7279\u5f81\u548c\u5173\u952e\u70b9\u7a7a\u95f4\u5e8f\u5217\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982CMU Panoptic\u4e09\u6444\u50cf\u5934\u8bbe\u7f6e\u4e0bAP25\u63d0\u534724%\uff0c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2dPCP\u63d0\u534738%\u3002", "conclusion": "MV-SSM\u5728\u6cdb\u5316\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d23D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.00665", "pdf": "https://arxiv.org/pdf/2509.00665", "abs": "https://arxiv.org/abs/2509.00665", "authors": ["Weilong Yan", "Xin Zhang", "Robby T. Tan"], "title": "ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Monocular depth estimation under adverse weather conditions (e.g.\\ rain, fog,\nsnow, and nighttime) remains highly challenging due to the lack of reliable\nground truth and the difficulty of learning from unlabeled real-world data.\nExisting methods often rely on synthetic adverse data with pseudo-labels, which\nsuffer from domain gaps, or employ self-supervised learning, which violates\nphotometric assumptions in adverse scenarios. In this work, we propose to\nachieve weather--generalized depth estimation by Parameter--Efficient\nFine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small\namount of high--visibility (normal) data. While PEFT has shown strong\nperformance in semantic tasks such as segmentation, it remains underexplored\nfor geometry--centric tasks like depth estimation -- especially in terms of\nbalancing effective adaptation with the preservation of pretrained knowledge.\nTo this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy,\nwhich structurally decomposes the pretrained weights of VFMs based on two kinds\nof effective ranks (entropy--rank and stable--rank). In the tuning phase, we\nadaptively select the proper rank number as well as the task--aware singular\ndirections for initialization, based on the entropy--rank and full--tuned\nweight; while in the maintaining stage, we enforce a principal direction\nregularization based on the stable--rank. This design guarantees flexible task\nadaptation while preserving the strong generalization capability of the\npretrained VFM. Extensive experiments on four real--world benchmarks across\ndiverse weather conditions demonstrate that STM not only outperforms existing\nPEFT methods and full fine--tuning but also surpasses methods trained with\nadverse synthetic data, and even the depth foundation model", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u9009\u62e9-\u8c03\u4f18-\u4fdd\u6301\uff08STM\uff09\u7b56\u7565\u5e73\u8861\u9002\u5e94\u6027\u4e0e\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4fdd\u7559\u3002", "motivation": "\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f3a\u4e4f\u53ef\u9760\u7684\u5730\u9762\u771f\u5b9e\u6570\u636e\u4e14\u96be\u4ee5\u4ece\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u8fdd\u53cd\u5149\u5ea6\u5047\u8bbe\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u91c7\u7528PEFT\u65b9\u6cd5\uff0c\u8bbe\u8ba1STM\u7b56\u7565\uff0c\u57fa\u4e8e\u71b5-\u79e9\u548c\u7a33\u5b9a-\u79e9\u7684\u7ed3\u6784\u5316\u5206\u89e3\uff0c\u9009\u62e9\u4efb\u52a1\u611f\u77e5\u7684\u5947\u5f02\u65b9\u5411\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u4fdd\u6301\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u56db\u79cd\u771f\u5b9e\u6076\u52a3\u5929\u6c14\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709PEFT\u65b9\u6cd5\u3001\u5168\u5fae\u8c03\u65b9\u6cd5\u53ca\u4f7f\u7528\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "STM\u7b56\u7565\u6709\u6548\u5e73\u8861\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u4e0e\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4fdd\u7559\uff0c\u5b9e\u73b0\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u9ad8\u6027\u80fd\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2509.01019", "pdf": "https://arxiv.org/pdf/2509.01019", "abs": "https://arxiv.org/abs/2509.01019", "authors": ["Scarlett Raine", "Benjamin Moshirian", "Tobias Fischer"], "title": "AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "6 pages, 3 figures", "summary": "Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u81ea\u52a8\u5316\u73ca\u745a\u7901\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5e95\u8d28\u5206\u7c7b\u63d0\u9ad8\u6062\u590d\u6548\u7387\u548c\u8303\u56f4\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\u3001\u6d77\u6d0b\u9178\u5316\u548c\u6c61\u67d3\uff0c\u73ca\u745a\u7901\u9762\u4e34\u5d29\u6e83\uff0c\u6062\u590d\u5de5\u4f5c\u4e9f\u9700\u81ea\u52a8\u5316\u4ee5\u6269\u5927\u89c4\u6a21\u548c\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u4eba\u5de5\u667a\u80fd\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\uff0c\u81ea\u52a8\u5316\u90e8\u7f72\u73ca\u745a\u518d\u64ad\u79cd\u8bbe\u5907\uff0c\u91cd\u70b9\u5b9e\u73b0\u5e95\u8d28\u5206\u7c7b\uff0c\u8bc6\u522b\u9002\u5408\u73ca\u745a\u751f\u957f\u7684\u6d77\u5e8a\u533a\u57df\u3002", "result": "\u5728\u5927\u5821\u7901\u7684\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u90e8\u7f72\u51c6\u786e\u7387\u8fbe77.8%\uff0c\u5b50\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u8fbe89.1%\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u4e3a5.5\u5e27/\u79d2\uff0c\u5e76\u516c\u5f00\u4e86\u6807\u6ce8\u5e95\u8d28\u56fe\u50cf\u6570\u636e\u96c6\u3002", "conclusion": "\u81ea\u52a8\u5316\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u73ca\u745a\u6062\u590d\u7684\u6548\u7387\u548c\u8303\u56f4\uff0c\u516c\u5f00\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.01022", "pdf": "https://arxiv.org/pdf/2509.01022", "abs": "https://arxiv.org/abs/2509.01022", "authors": ["Bo Fu", "Zhe Chen", "Rahul Chandan", "Alex Barbosa", "Michael Caldara", "Joey Durham", "Federico Pecora"], "title": "Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles", "categories": ["cs.AI", "cs.MA", "cs.RO", "93A16 93A16"], "comment": null, "summary": "We introduce the Block Rearrangement Problem (BRaP), a challenging component\nof large warehouse management which involves rearranging storage blocks within\ndense grids to achieve a target state. We formally define the BRaP as a graph\nsearch problem. Building on intuitions from sliding puzzle problems, we propose\nfive search-based solution algorithms, leveraging joint configuration space\nsearch, classical planning, multi-agent pathfinding, and expert heuristics. We\nevaluate the five approaches empirically for plan quality and scalability.\nDespite the exponential relation between search space size and block number,\nour methods demonstrate efficiency in creating rearrangement plans for deeply\nburied blocks in up to 80x80 grids.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4ed3\u5e93\u7ba1\u7406\u4e2d\u7684\u5757\u91cd\u6392\u95ee\u9898\uff08BRaP\uff09\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u590d\u6742\u5ea6\u7f51\u683c\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u4ed3\u5e93\u7ba1\u7406\u4e2d\u5757\u91cd\u6392\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u76ee\u6807\u662f\u8bbe\u8ba1\u9ad8\u6548\u7684\u91cd\u6392\u7b97\u6cd5\u3002", "method": "\u5c06BRaP\u5b9a\u4e49\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u8054\u5408\u914d\u7f6e\u7a7a\u95f4\u641c\u7d22\u3001\u7ecf\u5178\u89c4\u5212\u3001\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u548c\u4e13\u5bb6\u542f\u53d1\u5f0f\u3002", "result": "\u7b97\u6cd5\u5728\u9ad8\u8fbe80x80\u7684\u7f51\u683c\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u5757\u6570\u91cf\u4e0e\u641c\u7d22\u7a7a\u95f4\u6307\u6570\u589e\u957f\u7684\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4ed3\u5e93\u7ba1\u7406\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u5757\u91cd\u6392\u4efb\u52a1\u3002"}}
{"id": "2509.01106", "pdf": "https://arxiv.org/pdf/2509.01106", "abs": "https://arxiv.org/abs/2509.01106", "authors": ["Huang Fang", "Mengxi Zhang", "Heng Dong", "Wei Li", "Zixuan Wang", "Qifeng Zhang", "Xueyun Tian", "Yucheng Hu", "Hang Li"], "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Tech report. Project page: https://robix-seed.github.io/robix/", "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.", "AI": {"tldr": "Robix\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u67b6\u6784\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u673a\u5668\u4eba\u63a8\u7406\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u652f\u6301\u590d\u6742\u6307\u4ee4\u6267\u884c\u548c\u81ea\u7136\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u4e3b\u52a8\u5bf9\u8bdd\u3001\u5b9e\u65f6\u4e2d\u65ad\u5904\u7406\u7b49\u65b0\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u9ad8\u5c42\u6b21\u8ba4\u77e5\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u9762\u7684\u4e0d\u8db3\uff0cRobix\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "Robix\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u94fe\u5f0f\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRobix\u5728\u4ea4\u4e92\u4efb\u52a1\u6267\u884c\u4e0a\u4f18\u4e8e\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\uff08\u5982GPT-4o\u548cGemini 2.5 Pro\uff09\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Robix\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u9ad8\u5c42\u6b21\u8ba4\u77e5\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u548c\u81ea\u7136\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.01246", "pdf": "https://arxiv.org/pdf/2509.01246", "abs": "https://arxiv.org/abs/2509.01246", "authors": ["Larissa R. de S. Shibata", "Ankit A. Ravankar", "Jose Victorio Salazar Luces", "Yasuhisa Hirata"], "title": "An AI-Based Shopping Assistant System to Support the Visually Impaired", "categories": ["cs.HC", "cs.RO"], "comment": "7 pages, Accepted for 2025 SICE-FES conference (IEEE)", "summary": "Shopping plays a significant role in shaping consumer identity and social\nintegration. However, for individuals with visual impairments, navigating in\nsupermarkets and identifying products can be an overwhelming and challenging\nexperience. This paper presents an AI-based shopping assistant prototype\ndesigned to enhance the autonomy and inclusivity of visually impaired\nindividuals in supermarket environments. The system integrates multiple\ntechnologies, including computer vision, speech recognition, text-to-speech\nsynthesis, and indoor navigation, into a single, user-friendly platform. Using\ncameras for ArUco marker detection and real-time environmental scanning, the\nsystem helps users navigate the store, identify product locations, provide\nreal-time auditory guidance, and gain context about their surroundings. The\nassistant interacts with the user through voice commands and multimodal\nfeedback, promoting a more dynamic and engaging shopping experience. The system\nwas evaluated through experiments, which demonstrated its ability to guide\nusers effectively and improve their shopping experience. This paper contributes\nto the development of inclusive AI-driven assistive technologies aimed at\nenhancing accessibility and user independence for the shopping experience.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u8d2d\u7269\u52a9\u624b\u539f\u578b\uff0c\u65e8\u5728\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u5728\u8d85\u5e02\u73af\u5883\u4e2d\u589e\u5f3a\u81ea\u4e3b\u6027\u548c\u5305\u5bb9\u6027\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u591a\u79cd\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u5728\u8d85\u5e02\u8d2d\u7269\u65f6\u9762\u4e34\u5bfc\u822a\u548c\u4ea7\u54c1\u8bc6\u522b\u7684\u5de8\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u4ed6\u4eec\u7684\u8d2d\u7269\u4f53\u9a8c\u548c\u72ec\u7acb\u6027\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u548c\u5ba4\u5185\u5bfc\u822a\u6280\u672f\uff0c\u901a\u8fc7ArUco\u6807\u8bb0\u68c0\u6d4b\u548c\u5b9e\u65f6\u73af\u5883\u626b\u63cf\u4e3a\u7528\u6237\u63d0\u4f9b\u5bfc\u822a\u3001\u4ea7\u54c1\u5b9a\u4f4d\u548c\u5b9e\u65f6\u8bed\u97f3\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5f15\u5bfc\u7528\u6237\u5e76\u63d0\u5347\u5176\u8d2d\u7269\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a8\u52a8\u5305\u5bb9\u6027AI\u8f85\u52a9\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8d2d\u7269\u4f53\u9a8c\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u7528\u6237\u72ec\u7acb\u6027\u3002"}}
{"id": "2509.01294", "pdf": "https://arxiv.org/pdf/2509.01294", "abs": "https://arxiv.org/abs/2509.01294", "authors": ["Helge Spieker", "Nadjib Lazaar", "Arnaud Gotlieb", "Nassim Belmecheri"], "title": "Metamorphic Testing of Multimodal Human Trajectory Prediction", "categories": ["cs.SE", "cs.RO"], "comment": "Information and Software Technology", "summary": "Context: Predicting human trajectories is crucial for the safety and\nreliability of autonomous systems, such as automated vehicles and mobile\nrobots. However, rigorously testing the underlying multimodal Human Trajectory\nPrediction (HTP) models, which typically use multiple input sources (e.g.,\ntrajectory history and environment maps) and produce stochastic outputs\n(multiple possible future paths), presents significant challenges. The primary\ndifficulty lies in the absence of a definitive test oracle, as numerous future\ntrajectories might be plausible for any given scenario. Objectives: This\nresearch presents the application of Metamorphic Testing (MT) as a systematic\nmethodology for testing multimodal HTP systems. We address the oracle problem\nthrough metamorphic relations (MRs) adapted for the complexities and stochastic\nnature of HTP. Methods: We present five MRs, targeting transformations of both\nhistorical trajectory data and semantic segmentation maps used as an\nenvironmental context. These MRs encompass: 1) label-preserving geometric\ntransformations (mirroring, rotation, rescaling) applied to both trajectory and\nmap inputs, where outputs are expected to transform correspondingly. 2)\nMap-altering transformations (changing semantic class labels, introducing\nobstacles) with predictable changes in trajectory distributions. We propose\nprobabilistic violation criteria based on distance metrics between probability\ndistributions, such as the Wasserstein or Hellinger distance. Conclusion: This\nstudy introduces tool, a MT framework for the oracle-less testing of\nmultimodal, stochastic HTP systems. It allows for assessment of model\nrobustness against input transformations and contextual changes without\nreliance on ground-truth trajectories.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u53d8\u5f62\u6d4b\u8bd5\uff08MT\uff09\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u6a21\u6001\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\uff08HTP\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u9002\u5e94\u6027\u53d8\u5f62\u5173\u7cfb\u89e3\u51b3\u6d4b\u8bd5\u4e2d\u7684\u9884\u8a00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e94\u4e2a\u53d8\u5f62\u5173\u7cfb\u548c\u6982\u7387\u8fdd\u53cd\u6807\u51c6\u3002", "motivation": "\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u6a21\u6001HTP\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u968f\u673a\u6027\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6d4b\u8bd5\u9884\u8a00\uff0c\u6d4b\u8bd5\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e94\u4e2a\u53d8\u5f62\u5173\u7cfb\uff0c\u6db5\u76d6\u8f68\u8ff9\u548c\u5730\u56fe\u8f93\u5165\u7684\u51e0\u4f55\u53d8\u6362\uff08\u5982\u955c\u50cf\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\u53ca\u5730\u56fe\u5185\u5bb9\u53d8\u6362\uff08\u5982\u8bed\u4e49\u6807\u7b7e\u66f4\u6539\u3001\u969c\u788d\u7269\u5f15\u5165\uff09\uff0c\u5e76\u57fa\u4e8e\u6982\u7387\u5206\u5e03\u8ddd\u79bb\u5ea6\u91cf\uff08\u5982Wasserstein\u8ddd\u79bb\uff09\u786e\u5b9a\u8fdd\u53cd\u6807\u51c6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2aMT\u6846\u67b6\uff0c\u80fd\u591f\u5728\u65e0\u9700\u771f\u5b9e\u8f68\u8ff9\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u6a21\u578b\u5bf9\u8f93\u5165\u53d8\u6362\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d4b\u8bd5\u591a\u6a21\u6001\u968f\u673aHTP\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9884\u8a00\u4f9d\u8d56\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.01388", "pdf": "https://arxiv.org/pdf/2509.01388", "abs": "https://arxiv.org/abs/2509.01388", "authors": ["Philipp Hartmann", "Jannick Strangh\u00f6ner", "Klaus Neumann"], "title": "End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic Levitation System", "categories": ["eess.SY", "cs.AI", "cs.RO", "cs.SY", "I.2.9; I.2.8; I.2.6; D.4.7; C.3; J.7"], "comment": "8 pages, 7 figures, 2 tables", "summary": "Magnetic levitation is poised to revolutionize industrial automation by\nintegrating flexible in-machine product transport and seamless manipulation. It\nis expected to become the standard drive for automated manufacturing. However,\ncontrolling such systems is inherently challenging due to their complex,\nunstable dynamics. Traditional control approaches, which rely on hand-crafted\ncontrol engineering, typically yield robust but conservative solutions, with\ntheir performance closely tied to the expertise of the engineering team. In\ncontrast, neural control learning presents a promising alternative. This paper\npresents the first neural controller for 6D magnetic levitation. Trained\nend-to-end on interaction data from a proprietary controller, it directly maps\nraw sensor data and 6D reference poses to coil current commands. The neural\ncontroller can effectively generalize to previously unseen situations while\nmaintaining accurate and robust control. These results underscore the practical\nfeasibility of learning-based neural control in complex physical systems and\nsuggest a future where such a paradigm could enhance or even substitute\ntraditional engineering approaches in demanding real-world applications. The\ntrained neural controller, source code, and demonstration videos are publicly\navailable at https://sites.google.com/view/neural-maglev.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd6D\u78c1\u60ac\u6d6e\u7cfb\u7edf\u7684\u795e\u7ecf\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u6570\u636e\u548c\u76ee\u6807\u4f4d\u59ff\u6620\u5c04\u5230\u7ebf\u5708\u7535\u6d41\u547d\u4ee4\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u4e2d\u5b66\u4e60\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u78c1\u60ac\u6d6e\u6280\u672f\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u590d\u6742\u4e0d\u7a33\u5b9a\u7684\u52a8\u529b\u5b66\u7279\u6027\u4f7f\u63a7\u5236\u6210\u4e3a\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u8bbe\u8ba1\uff0c\u6027\u80fd\u53d7\u9650\uff0c\u800c\u795e\u7ecf\u63a7\u5236\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u9996\u4e2a6D\u78c1\u60ac\u6d6e\u795e\u7ecf\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4e13\u6709\u63a7\u5236\u5668\u7684\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u76f4\u63a5\u4ece\u4f20\u611f\u5668\u6570\u636e\u548c6D\u76ee\u6807\u4f4d\u59ff\u751f\u6210\u7ebf\u5708\u7535\u6d41\u547d\u4ee4\u3002", "result": "\u795e\u7ecf\u63a7\u5236\u5668\u80fd\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u60c5\u666f\uff0c\u4fdd\u6301\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u63a7\u5236\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u795e\u7ecf\u63a7\u5236\u53ef\u80fd\u5728\u672a\u6765\u589e\u5f3a\u751a\u81f3\u53d6\u4ee3\u4f20\u7edf\u5de5\u7a0b\u65b9\u6cd5\uff0c\u63a8\u52a8\u5de5\u4e1a\u81ea\u52a8\u5316\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.01582", "pdf": "https://arxiv.org/pdf/2509.01582", "abs": "https://arxiv.org/abs/2509.01582", "authors": ["Karim Essalmi", "Fernando Garrido", "Fawzi Nashashibi"], "title": "Quantum game models for interaction-aware decision-making in automated driving", "categories": ["cs.GT", "cs.RO"], "comment": "8 pages, 8 figures, submitted to ICAR 2025", "summary": "Decision-making in automated driving must consider interactions with\nsurrounding agents to be effective. However, traditional methods often neglect\nor oversimplify these interactions because they are difficult to model and\nsolve, which can lead to overly conservative behavior of the ego vehicle. To\naddress this gap, we propose two quantum game models, QG-U1 (Quantum Game -\nUnitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware\ndecision-making. These models extend classical game theory by incorporating\nprinciples of quantum mechanics, such as superposition, interference, and\nentanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games\nwith two strategies per player and can be executed in real time on a standard\ncomputer without requiring quantum hardware. We evaluate both models in merging\nand roundabout scenarios and compare them with classical game-theoretic methods\nand baseline approaches (IDM, MOBIL, and a utility-based technique). Results\nshow that QG-G4 achieves lower collision rates and higher success rates\ncompared to baseline methods, while both quantum models yield higher expected\npayoffs than classical game approaches under certain parameter settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u91cf\u5b50\u6e38\u620f\u6a21\u578b\uff08QG-U1\u548cQG-G4\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8003\u8651\u4ea4\u4e92\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u964d\u4f4e\u78b0\u649e\u7387\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u6216\u7b80\u5316\u5468\u56f4\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\uff0c\u5bfc\u81f4\u4fdd\u5b88\u884c\u4e3a\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u5408\u91cf\u5b50\u529b\u5b66\u539f\u7406\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86QG-U1\u548cQG-G4\u4e24\u79cd\u91cf\u5b50\u6e38\u620f\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e24\u73a9\u5bb6\u3001\u4e24\u7b56\u7565\u7684\u6e38\u620f\uff0c\u65e0\u9700\u91cf\u5b50\u786c\u4ef6\u5373\u53ef\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u5728\u5408\u5e76\u548c\u73af\u5c9b\u573a\u666f\u4e2d\uff0cQG-G4\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08IDM\u3001MOBIL\u7b49\uff09\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u78b0\u649e\u7387\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff1b\u4e24\u79cd\u91cf\u5b50\u6a21\u578b\u7684\u9884\u671f\u6536\u76ca\u4e5f\u4f18\u4e8e\u7ecf\u5178\u535a\u5f08\u8bba\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u6e38\u620f\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4ea4\u4e92\u611f\u77e5\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2509.01605", "pdf": "https://arxiv.org/pdf/2509.01605", "abs": "https://arxiv.org/abs/2509.01605", "authors": ["Pedram Fekri", "Mehrdad Zadeh", "Javad Dargahi"], "title": "TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Preprint version. This work is intended for future journal submission", "summary": "Recently, the emergence of multitask deep learning models has enhanced\ncatheterization procedures by providing tactile and visual perception data\nthrough an end-to-end architec- ture. This information is derived from a\nsegmentation and force estimation head, which localizes the catheter in X-ray\nimages and estimates the applied pressure based on its deflection within the\nimage. These stereo vision architectures incorporate a CNN- based\nencoder-decoder that captures the dependencies between X-ray images from two\nviewpoints, enabling simultaneous 3D force estimation and stereo segmentation\nof the catheter. With these tasks in mind, this work approaches the problem\nfrom a new perspective. We propose a novel encoder-decoder Vision Transformer\nmodel that processes two input X-ray images as separate sequences. Given\nsequences of X-ray patches from two perspectives, the transformer captures\nlong-range dependencies without the need to gradually expand the receptive\nfield for either image. The embeddings generated by both the encoder and\ndecoder are fed into two shared segmentation heads, while a regression head\nemploys the fused information from the decoder for 3D force estimation. The\nproposed model is a stereo Vision Transformer capable of simultaneously\nsegmenting the catheter from two angles while estimating the generated forces\nat its tip in 3D. This model has undergone extensive experiments on synthetic\nX-ray images with various noise levels and has been compared against\nstate-of-the-art pure segmentation models, vision-based catheter force\nestimation methods, and a multitask catheter segmentation and force estimation\napproach. It outperforms existing models, setting a new state-of-the-art in\nboth catheter segmentation and force estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u540c\u65f6\u4ece\u591a\u89d2\u5ea6\u5206\u5272\u5bfc\u7ba1\u5e76\u4f30\u8ba1\u5176\u7aef\u90e83D\u529b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u611f\u77e5\u6570\u636e\uff0c\u589e\u5f3a\u5bfc\u7ba1\u5316\u624b\u672f\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u5904\u7406\u53cc\u89c6\u89d2X\u5c04\u7ebf\u56fe\u50cf\u5e8f\u5217\uff0c\u901a\u8fc7\u5171\u4eab\u5206\u5272\u5934\u548c\u56de\u5f52\u5934\u5b9e\u73b0\u5206\u5272\u4e0e\u529b\u4f30\u8ba1\u3002", "result": "\u65b0\u6a21\u578b\u5728\u5408\u6210X\u5c04\u7ebf\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5bfc\u7ba1\u5206\u5272\u548c\u529b\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7acb\u4f53\u89c6\u89c9Transformer\u6a21\u578b\u4e3a\u5bfc\u7ba1\u5206\u5272\u548c\u529b\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.01630", "pdf": "https://arxiv.org/pdf/2509.01630", "abs": "https://arxiv.org/abs/2509.01630", "authors": ["Bingheng Wang", "Yichao Gao", "Tianchen Sun", "Lin Zhao"], "title": "Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP", "categories": ["cs.LG", "cs.MA", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Distributed trajectory optimization via ADMM-DDP is a powerful approach for\ncoordinating multi-agent systems, but it requires extensive tuning of tightly\ncoupled hyperparameters that jointly govern local task performance and global\ncoordination. In this paper, we propose Learning to Coordinate (L2C), a general\nframework that meta-learns these hyperparameters, modeled by lightweight\nagent-wise neural networks, to adapt across diverse tasks and agent\nconfigurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in\na distributed manner. It also enables efficient meta-gradient computation by\nreusing DDP components such as Riccati recursions and feedback gains. These\ngradients correspond to the optimal solutions of distributed matrix-valued LQR\nproblems, coordinated across agents via an auxiliary ADMM framework that\nbecomes convex under mild assumptions. Training is further accelerated by\ntruncating iterations and meta-learning ADMM penalty parameters optimized for\nrapid residual reduction, with provable Lipschitz-bounded gradient errors. On a\nchallenging cooperative aerial transport task, L2C generates dynamically\nfeasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures\nquadrotor formations for safe 6-DoF load manipulation in tight spaces, and\nadapts robustly to varying team sizes and task conditions, while achieving up\nto $88\\%$ faster gradient computation than state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86L2C\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316ADMM-DDP\u7b97\u6cd5\u7684\u8d85\u53c2\u6570\uff0c\u4ee5\u9002\u914d\u591a\u4efb\u52a1\u548c\u591a\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u534f\u540c\u548c\u52a8\u6001\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u914d\u7f6e\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "L2C\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5143\u5b66\u4e60\u8d85\u53c2\u6570\uff0c\u5e76\u5229\u7528ADMM-DDP\u7684\u5206\u5e03\u5f0f\u68af\u5ea6\u548cRiccati\u9012\u5f52\u52a0\u901f\u8ba1\u7b97\u3002", "result": "\u5728\u5408\u4f5c\u7a7a\u4e2d\u8fd0\u8f93\u4efb\u52a1\u4e2d\uff0cL2C\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u9002\u5e94\u6027\u5f3a\uff0c\u68af\u5ea6\u8ba1\u7b97\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb88%\u3002", "conclusion": "L2C\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u540c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8d85\u53c2\u6570\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.01786", "pdf": "https://arxiv.org/pdf/2509.01786", "abs": "https://arxiv.org/abs/2509.01786", "authors": ["Vimal Mollyn", "Chris Harrison"], "title": "EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras", "categories": ["cs.HC", "cs.CV", "cs.RO"], "comment": "Published at UIST 2024. More info at\n  https://www.figlab.com/research/2024/egotouch", "summary": "In augmented and virtual reality (AR/VR) experiences, a user's arms and hands\ncan provide a convenient and tactile surface for touch input. Prior work has\nshown on-body input to have significant speed, accuracy, and ergonomic benefits\nover in-air interfaces, which are common today. In this work, we demonstrate\nhigh accuracy, bare hands (i.e., no special instrumentation of the user) skin\ninput using just an RGB camera, like those already integrated into all modern\nXR headsets. Our results show this approach can be accurate, and robust across\ndiverse lighting conditions, skin tones, and body motion (e.g., input while\nwalking). Finally, our pipeline also provides rich input metadata including\ntouch force, finger identification, angle of attack, and rotation. We believe\nthese are the requisite technical ingredients to more fully unlock on-skin\ninterfaces that have been well motivated in the HCI literature but have lacked\nrobust and practical methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u65e0\u9700\u7a7f\u6234\u8bbe\u5907\u7684\u624b\u90e8\u76ae\u80a4\u8f93\u5165\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eAR/VR\u573a\u666f\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u4e30\u5bcc\u8f93\u5165\u529f\u80fd\u3002", "motivation": "AR/VR\u4e2d\u624b\u90e8\u548c\u624b\u81c2\u4f5c\u4e3a\u89e6\u63a7\u8f93\u5165\u754c\u9762\u5177\u6709\u901f\u5ea6\u548c\u4eba\u4f53\u5de5\u5b66\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u73b0\u6709XR\u5934\u663e\u4e2d\u7684RGB\u6444\u50cf\u5934\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u7279\u6b8a\u8bbe\u5907\u7684\u76ae\u80a4\u8f93\u5165\u6280\u672f\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u89e6\u6478\u8f93\u5165\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u6837\u5316\u5149\u7167\u3001\u80a4\u8272\u548c\u8eab\u4f53\u8fd0\u52a8\u4e0b\u8868\u73b0\u51c6\u786e\uff0c\u5e76\u80fd\u63d0\u4f9b\u89e6\u6478\u529b\u5ea6\u3001\u624b\u6307\u8bc6\u522b\u7b49\u4e30\u5bcc\u8f93\u5165\u5143\u6570\u636e\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u76ae\u80a4\u754c\u9762\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8AR/VR\u8f93\u5165\u65b9\u5f0f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.01820", "pdf": "https://arxiv.org/pdf/2509.01820", "abs": "https://arxiv.org/abs/2509.01820", "authors": ["Xincheng Cao", "Haochong Chen", "Bilin Aksun-Guvenc", "Levent Guvenc", "Brian Link", "Peter J Richmond", "Dokyung Yim", "Shihong Fan", "John Harber"], "title": "Nonlinear Model Predictive Control-Based Reverse Path-Planning and Path-Tracking Control of a Vehicle with Trailer System", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Reverse parking maneuvers of a vehicle with trailer system is a challenging\ntask to complete for human drivers due to the unstable nature of the system and\nunintuitive controls required to orientate the trailer properly. This paper\nhence proposes an optimization-based automation routine to handle the\npath-planning and path-tracking control process of such type of maneuvers. The\nproposed approach utilizes nonlinear model predictive control (NMPC) to\nrobustly guide the vehicle-trailer system into the desired parking space, and\nan optional forward repositioning maneuver can be added as an additional stage\nof the parking process to obtain better system configurations, before backward\nmotion can be attempted again to get a good final pose. The novelty of the\nproposed approach is the simplicity of its formulation, as the path-planning\nand path-tracking operations are only conducted on the trailer being viewed as\na standalone vehicle, before the control inputs are propagated to the tractor\nvehicle via inverse kinematic relationships also derived in this paper.\nSimulation case studies and hardware-in-the-loop tests are performed, and the\nresults demonstrate the efficacy of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e26\u62d6\u8f66\u8f66\u8f86\u7684\u5012\u8f66\u505c\u8f66\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u9006\u5411\u8fd0\u52a8\u5b66\u5b9e\u73b0\u8def\u5f84\u89c4\u5212\u548c\u8ddf\u8e2a\u3002", "motivation": "\u7531\u4e8e\u5e26\u62d6\u8f66\u7684\u8f66\u8f86\u7cfb\u7edf\u5728\u5012\u8f66\u65f6\u4e0d\u7a33\u5b9a\u4e14\u63a7\u5236\u590d\u6742\uff0c\u4eba\u5de5\u64cd\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u548c\u8ddf\u8e2a\uff0c\u5c06\u62d6\u8f66\u89c6\u4e3a\u72ec\u7acb\u8f66\u8f86\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u8fd0\u52a8\u5b66\u5c06\u63a7\u5236\u8f93\u5165\u4f20\u9012\u5230\u7275\u5f15\u8f66\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u5e26\u62d6\u8f66\u8f66\u8f86\u7684\u5012\u8f66\u505c\u8f66\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7b80\u5316\u4e86\u63a7\u5236\u6d41\u7a0b\uff0c\u901a\u8fc7\u62d6\u8f66\u7684\u72ec\u7acb\u5904\u7406\u548c\u9006\u5411\u8fd0\u52a8\u5b66\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u505c\u8f66\u3002"}}
{"id": "2509.01951", "pdf": "https://arxiv.org/pdf/2509.01951", "abs": "https://arxiv.org/abs/2509.01951", "authors": ["Tianhua Gao", "Kohji Tomita", "Akiya Kamimura"], "title": "Online Identification using Adaptive Laws and Neural Networks for Multi-Quadrotor Centralized Transportation System", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper introduces an adaptive-neuro identification method that enhances\nthe robustness of a centralized multi-quadrotor transportation system. This\nmethod leverages online tuning and learning on decomposed error subspaces,\nenabling efficient real-time compensation to time-varying disturbances and\nmodel uncertainties acting on the payload. The strategy is to decompose the\nhigh-dimensional error space into a set of low-dimensional subspaces. In this\nway, the identification problem for unseen features is naturally transformed\ninto submappings (``slices'') addressed by multiple adaptive laws and shallow\nneural networks, which are updated online via Lyapunov-based adaptation without\nrequiring persistent excitation (PE) and offline training. Due to the\nmodel-free nature of neural networks, this approach can be well adapted to\nhighly coupled and nonlinear centralized transportation systems. It serves as a\nfeedforward compensator for the payload controller without explicitly relying\non the dynamics coupled with the payload, such as cables and quadrotors. The\nproposed control system has been proven to be stable in the sense of Lyapunov,\nand its enhanced robustness under time-varying disturbances and model\nuncertainties was demonstrated by numerical simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u795e\u7ecf\u8fa8\u8bc6\u65b9\u6cd5\uff0c\u589e\u5f3a\u591a\u56db\u65cb\u7ffc\u8fd0\u8f93\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5206\u89e3\u8bef\u5dee\u5b50\u7a7a\u95f4\u5b9e\u73b0\u5728\u7ebf\u8865\u507f\u3002", "motivation": "\u89e3\u51b3\u591a\u56db\u65cb\u7ffc\u8fd0\u8f93\u7cfb\u7edf\u4e2d\u65f6\u95f4\u53d8\u5316\u6270\u52a8\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5206\u89e3\u9ad8\u7ef4\u8bef\u5dee\u7a7a\u95f4\u4e3a\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5f8b\u548c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u5728\u7ebf\u66f4\u65b0\u3002", "result": "\u7cfb\u7edf\u5728Lyapunov\u610f\u4e49\u4e0b\u7a33\u5b9a\uff0c\u6570\u503c\u6a21\u62df\u663e\u793a\u5bf9\u6270\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "conclusion": "\u8be5\u6a21\u578b\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u548c\u6301\u7eed\u6fc0\u52b1\uff0c\u9002\u7528\u4e8e\u9ad8\u5ea6\u8026\u5408\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u3002"}}
{"id": "2509.01952", "pdf": "https://arxiv.org/pdf/2509.01952", "abs": "https://arxiv.org/abs/2509.01952", "authors": ["Tianhua Gao", "Kohji Tomita", "Akiya Kamimura"], "title": "Robustness Enhancement for Multi-Quadrotor Centralized Transportation System via Online Tuning and Learning", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "This paper introduces an adaptive-neuro geometric control for a centralized\nmulti-quadrotor cooperative transportation system, which enhances both\nadaptivity and disturbance rejection. Our strategy is to coactively tune the\nmodel parameters and learn the external disturbances in real-time. To realize\nthis, we augmented the existing geometric control with multiple neural networks\nand adaptive laws, where the estimated model parameters and the weights of the\nneural networks are simultaneously tuned and adjusted online. The\nLyapunov-based adaptation guarantees bounded estimation errors without\nrequiring either pre-training or the persistent excitation (PE) condition. The\nproposed control system has been proven to be stable in the sense of Lyapunov\nunder certain preconditions, and its enhanced robustness under scenarios of\ndisturbed environment and model-unmatched plant was demonstrated by numerical\nsimulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u795e\u7ecf\u51e0\u4f55\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u56db\u65cb\u7ffc\u534f\u540c\u8fd0\u8f93\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u6a21\u578b\u53c2\u6570\u548c\u5b66\u4e60\u5916\u90e8\u5e72\u6270\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u5f8b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u6301\u7eed\u6fc0\u52b1\u6761\u4ef6\u7684\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u5e72\u6270\u73af\u5883\u548c\u6a21\u578b\u4e0d\u5339\u914d\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u795e\u7ecf\u51e0\u4f55\u63a7\u5236\u4ee5\u589e\u5f3a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u63a7\u5236\u548c\u591a\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u65f6\u8c03\u6574\u6a21\u578b\u53c2\u6570\u548c\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\uff0c\u5229\u7528Lyapunov\u81ea\u9002\u5e94\u4fdd\u8bc1\u8bef\u5dee\u6709\u754c\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e72\u6270\u73af\u5883\u548c\u6a21\u578b\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u56db\u65cb\u7ffc\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2509.01968", "pdf": "https://arxiv.org/pdf/2509.01968", "abs": "https://arxiv.org/abs/2509.01968", "authors": ["Therese Joseph", "Tobias Fischer", "Michael Milford"], "title": "Ensemble-Based Event Camera Place Recognition Under Varying Illumination", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Compared to conventional cameras, event cameras provide a high dynamic range\nand low latency, offering greater robustness to rapid motion and challenging\nlighting conditions. Although the potential of event cameras for visual place\nrecognition (VPR) has been established, developing robust VPR frameworks under\nsevere illumination changes remains an open research problem. In this paper, we\nintroduce an ensemble-based approach to event camera place recognition that\ncombines sequence-matched results from multiple event-to-frame reconstructions,\nVPR feature extractors, and temporal resolutions. Unlike previous event-based\nensemble methods, which only utilise temporal resolution, our broader fusion\nstrategy delivers significantly improved robustness under varied lighting\nconditions (e.g., afternoon, sunset, night), achieving a 57% relative\nimprovement in Recall@1 across day-night transitions. We evaluate our approach\non two long-term driving datasets (with 8 km per traverse) without metric\nsubsampling, thereby preserving natural variations in speed and stop duration\nthat influence event density. We also conduct a comprehensive analysis of key\ndesign choices, including binning strategies, polarity handling, reconstruction\nmethods, and feature extractors, to identify the most critical components for\nrobust performance. Additionally, we propose a modification to the standard\nsequence matching framework that enhances performance at longer sequence\nlengths. To facilitate future research, we will release our codebase and\nbenchmarking framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u91cd\u5efa\u3001\u591a\u7279\u5f81\u63d0\u53d6\u5668\u548c\u591a\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u5149\u7167\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u76f8\u673a\uff0c\u4f46\u5728\u6781\u7aef\u5149\u7167\u53d8\u5316\u4e0b\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e8b\u4ef6\u5230\u5e27\u7684\u591a\u79cd\u91cd\u5efa\u65b9\u6cd5\u3001VPR\u7279\u5f81\u63d0\u53d6\u5668\u548c\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u5339\u914d\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u65e5\u591c\u95f4\u5149\u7167\u53d8\u5316\u4e0b\uff0cRecall@1\u76f8\u5bf9\u63d0\u5347\u4e8657%\uff0c\u5e76\u5728\u4e24\u4e2a\u957f\u8ddd\u79bb\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u6781\u7aef\u5149\u7167\u4e0b\u7684\u5730\u70b9\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u4f18\u5316\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
