{"id": "2511.03837", "pdf": "https://arxiv.org/pdf/2511.03837", "abs": "https://arxiv.org/abs/2511.03837", "authors": ["Saúl Fenollosa", "Narcis Cardona", "Wenfei Yang", "Jian Li"], "title": "Correlation and Temporal Consistency Analysis of Mono-static and Bi-static ISAC Channels", "categories": ["eess.SP"], "comment": "6 pages, 7 figures, 2 tables. Accepted for publication at the 2025\n  IEEE Global Communications Conference (GLOBECOM), WS-26: 4th Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "Integrated Sensing and Communication (ISAC) is critical for efficient\nspectrum and hardware utilization in future wireless networks like 6G. However,\nexisting channel models lack comprehensive characterization of ISAC-specific\ndynamics, particularly the relationship between mono-static (co-located Tx/Rx)\nand bi-static (separated Tx/Rx) sensing configurations. Empirical measurements\nin dynamic urban microcell (UMi) environments using a 79-GHz FMCW channel\nsounder help bridge this gap. Two key findings are demonstrated: (1)\nmono-static and bi-static channels exhibit consistently low instantaneous\ncorrelation due to divergent propagation geometries; (2) despite low\ninstantaneous correlation, both channels share unified temporal consistency,\nevolving predictably under environmental kinematics. These insights, validated\nacross seven real-world scenarios with moving targets/transceivers, inform\nrobust ISAC system design and future standardization."}
{"id": "2511.03923", "pdf": "https://arxiv.org/pdf/2511.03923", "abs": "https://arxiv.org/abs/2511.03923", "authors": ["Xianhua Yu", "Dong Li", "Bowen Gu", "Liuqing Yang", "Sumei Sun", "George K. Karagiannidis"], "title": "Adaptive Phase Shift Information Compression for IRS Systems: A Prompt Conditioned Variable Rate Framework", "categories": ["eess.SP"], "comment": null, "summary": "Intelligent reflecting surfaces (IRSs) have become a vital technology for\nimproving the spectrum and energy efficiency of forthcoming wireless networks.\nNevertheless, practical implementation is obstructed by the excessive overhead\nassociated with the frequent transmission of phase shift information (PSI) over\nbandwidth-constrained control lines. Current deep learning-based compression\nmethods mitigate this problem but are constrained by elevated decoder\ncomplexity, inadequate flexibility to dynamic channels, and static compression\nratios. This research presents a prompt-conditioned PSI compression system that\nintegrates prompt learning inspired by large models into the PSI compression\nprocess to address these difficulties. A hybrid prompt technique that\nintegrates soft prompt concatenation with feature-wise linear modulation (FiLM)\nfacilitates adaptive encoding across diverse signal-to-noise ratios (SNRs),\nfading kinds, and compression ratios. Furthermore, a variable rate technique\nincorporates the compression ratio into the prompt embeddings through latent\nmasking, enabling a singular model to adeptly balance reconstruction accuracy.\nAdditionally, a lightweight depthwise convolutional gating (DWCG) decoder\nfacilitates precise feature reconstruction with minimal complexity.\nComprehensive simulations indicate that the proposed framework significantly\nreduces NMSE compared to traditional autoencoder baselines, while ensuring\nrobustness across various channel circumstances and accommodating variable\ncompression ratios within a single model. These findings underscore the\nframework's promise as a scalable and efficient solution for real-time IRS\ncontrol in next-generation wireless networks."}
{"id": "2511.03967", "pdf": "https://arxiv.org/pdf/2511.03967", "abs": "https://arxiv.org/abs/2511.03967", "authors": ["Wuxia Chen", "Sean Moushegian", "Vahid Tarokh", "Taposh Banerjee"], "title": "Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals", "categories": ["eess.SP", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "This paper introduces an approach to multi-stream quickest change detection\nand fault isolation for unnormalized and score-based statistical models.\nTraditional optimal algorithms in the quickest change detection literature\nrequire explicit pre-change and post-change distributions to calculate the\nlikelihood ratio of the observations, which can be computationally expensive\nfor higher-dimensional data and sometimes even infeasible for complex machine\nlearning models. To address these challenges, we propose the min-SCUSUM method,\na Hyvarinen score-based algorithm that computes the difference of score\nfunctions in place of log-likelihood ratios. We provide a delay and false alarm\nanalysis of the proposed algorithm, showing that its asymptotic performance\ndepends on the Fisher divergence between the pre- and post-change\ndistributions. Furthermore, we establish an upper bound on the probability of\nfault misidentification in distinguishing the affected stream from the\nunaffected ones."}
{"id": "2511.03984", "pdf": "https://arxiv.org/pdf/2511.03984", "abs": "https://arxiv.org/abs/2511.03984", "authors": ["Hanfu Zhang", "Erwu Liu"], "title": "Joint Beamforming and Position Design for Movable Antenna Assisted LEO ISAC Systems", "categories": ["eess.SP"], "comment": "13 pages, 8 figures", "summary": "Low earth orbit (LEO) satellite-assisted integrated sensing and\ncommunications (ISAC) systems have been extensively studied to achieve\nubiquitous connectivity. However, the severe signal attenuation and limited\ntransmit power at LEO satellites can degrade ISAC performance. To address this\nissue, this paper investigated movable antenna (MA)-assisted LEO ISAC systems.\nWe derive the communication signal-to-interference-plus-noise ratio (SINR) and\nthe sensing squared position error bound (SPEB) for evaluating the ISAC\nperformance. Then, we jointly optimize the transmit beamforming and the MA\npositions to minimize the SPEB under the SINR constraints, total transmit power\nconstraint, and several inherent physical constraints of the MA array. We first\nsimplify the complex problem using the semidefinite relaxation (SDR). Then, we\npresent a novel alternating optimization (AO)-based algorithm to decouple the\noriginal problem into two subproblems, consequently convexified and solved.\nSimulations demonstrate the convergence and effectiveness of the proposed\nalgorithm. Better trade-off between communication and sensing performance, and\nat least 25% gain in sensing performance are achieved, compared to the\nbenchmarks."}
{"id": "2511.03931", "pdf": "https://arxiv.org/pdf/2511.03931", "abs": "https://arxiv.org/abs/2511.03931", "authors": ["Iman Adibnazari", "Harsh Sharma", "Myungsun Park", "Jacobo Cervera-Torralba", "Boris Kramer", "Michael T. Tolley"], "title": "Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction", "categories": ["cs.RO"], "comment": "20 Pages, 8 Figures", "summary": "Soft robots have shown immense promise in settings where they can leverage\ndynamic control of their entire bodies. However, effective dynamic shape\ncontrol requires a controller that accounts for the robot's high-dimensional\ndynamics--a challenge exacerbated by a lack of general-purpose tools for\nmodeling soft robots amenably for control. In this work, we conduct a\ncomparative study of data-driven model reduction techniques for generating\nlinear models amendable to dynamic shape control. We focus on three\nmethods--the eigensystem realization algorithm, dynamic mode decomposition with\ncontrol, and the Lagrangian operator inference (LOpInf) method. Using each\nclass of model, we explored their efficacy in model predictive control policies\nfor the dynamic shape control of a simulated eel-inspired soft robot in three\nexperiments: 1) tracking simulated reference trajectories guaranteed to be\nfeasible, 2) tracking reference trajectories generated from a biological model\nof eel kinematics, and 3) tracking reference trajectories generated by a\nreduced-scale physical analog. In all experiments, the LOpInf-based policies\ngenerated lower tracking errors than policies based on other models."}
{"id": "2511.03998", "pdf": "https://arxiv.org/pdf/2511.03998", "abs": "https://arxiv.org/abs/2511.03998", "authors": ["Abhishek Rajasekaran", "Mehdi Karbalayghareh", "Xiaoyan Ma", "David J. Love", "Christopher G. Brinton"], "title": "Optimal RIS Placement in a Multi-User MISO System with User Randomness", "categories": ["eess.SP"], "comment": "6 pages, 3 figures", "summary": "It is well established that the performance of reconfigurable intelligent\nsurface (RIS)-assisted systems critically depends on the optimal placement of\nthe RIS. Previous works consider either simple coverage maximization or\nsimultaneous optimization of the placement of the RIS along with the\nbeamforming and reflection coefficients, most of which assume that the location\nof the RIS, base station (BS), and users are known. However, in practice, only\nthe spatial variation of user density and obstacle configuration are likely to\nbe known prior to deployment of the system. Thus, we formulate a non-convex\nproblem that optimizes the position of the RIS over the expected minimum\nsignal-to-interference-plus-noise ratio (SINR) of the system with user\nrandomness, assuming that the system employs joint beamforming after\ndeployment. To solve this problem, we propose a recursive coarse-to-fine\nmethodology that constructs a set of candidate locations for RIS placement\nbased on the obstacle configuration and evaluates them over multiple\ninstantiations from the user distribution. The search is recursively refined\nwithin the optimal region identified in each stage to determine the final\noptimal region for RIS deployment. Numerical results are presented to\ncorroborate our findings."}
{"id": "2511.03996", "pdf": "https://arxiv.org/pdf/2511.03996", "abs": "https://arxiv.org/abs/2511.03996", "authors": ["Yushi Wang", "Changsheng Luo", "Penghui Chen", "Jianran Liu", "Weijian Sun", "Tong Guo", "Kechang Yang", "Biao Hu", "Yangang Zhang", "Mingguo Zhao"], "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots", "categories": ["cs.RO"], "comment": "Project page: https://humanoid-kick.github.io", "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches."}
{"id": "2511.04011", "pdf": "https://arxiv.org/pdf/2511.04011", "abs": "https://arxiv.org/abs/2511.04011", "authors": ["Higo T. P. Da Silva", "Hugerles S. Silva", "Felipe A. P. Figueiredo", "Andre A. Dos Anjos", "Rausley A. A. Souza"], "title": "A Survey on Noise-Based Communication", "categories": ["eess.SP", "math.ST", "stat.TH"], "comment": null, "summary": "The proliferation of sixth-generation (6G) networks and the massive Internet\nof Things (IoT) demand wireless communication technologies that are\nultra-low-power, secure, and covert. Noise-based communication has emerged as a\ntransformative paradigm that meets these demands by encoding information\ndirectly into the statistical properties of noise, rather than using\ntraditional deterministic carriers. This survey provides a comprehensive\nsynthesis of this field, systematically exploring its fundamental principles\nand key methodologies, including thermal noise modulation (TherMod), noise\nmodulation (NoiseMod) and its variants, and the Kirchhoff-law-Johnson-noise\n(KLJN) secure key exchange. We address critical practical challenges such as\nchannel estimation and hardware implementation, and highlight emerging\napplications in simultaneous wireless information and power transfer (SWIPT)\nand non-orthogonal multiple access (NOMA). Our analysis confirms that\nnoise-based systems offer unparalleled advantages in energy efficiency and\ncovertness, and we conclude by outlining future research directions to realize\ntheir potential for enabling the next generation of autonomous and secure\nwireless networks."}
{"id": "2511.04009", "pdf": "https://arxiv.org/pdf/2511.04009", "abs": "https://arxiv.org/abs/2511.04009", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Giacinto Barresi", "Fei Chen"], "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration", "categories": ["cs.RO"], "comment": "7 pages, 7 figures, IROS 2025 accepted", "summary": "This paper introduces an upper limb postural optimization method for\nenhancing physical ergonomics and force manipulability during bimanual\nhuman-robot co-carrying tasks. Existing research typically emphasizes human\nsafety or manipulative efficiency, whereas our proposed method uniquely\nintegrates both aspects to strengthen collaboration across diverse conditions\n(e.g., different grasping postures of humans, and different shapes of objects).\nSpecifically, the joint angles of a simplified human skeleton model are\noptimized by minimizing the cost function to prioritize safety and manipulative\ncapability. To guide humans towards the optimized posture, the reference\nend-effector poses of the robot are generated through a transformation module.\nA bimanual model predictive impedance controller (MPIC) is proposed for our\nhuman-like robot, CURI, to recalibrate the end effector poses through planned\ntrajectories. The proposed method has been validated through various subjects\nand objects during human-human collaboration (HHC) and human-robot\ncollaboration (HRC). The experimental results demonstrate significant\nimprovement in muscle conditions by comparing the activation of target muscles\nbefore and after optimization."}
{"id": "2511.04015", "pdf": "https://arxiv.org/pdf/2511.04015", "abs": "https://arxiv.org/abs/2511.04015", "authors": ["Haotian Zhang", "Shijian Gao", "Xiang Cheng"], "title": "Tiny-WiFo: A Lightweight Wireless Foundation Model for Channel Prediction via Multi-Component Adaptive Knowledge Distillation", "categories": ["eess.SP"], "comment": "5 pages, 1 figures, 3 tables", "summary": "The massive scale of Wireless Foundation Models (FMs) hinders their real-time\ndeployment on edge devices. This letter moves beyond standard knowledge\ndistillation by introducing a novel Multi-Component Adaptive Knowledge\nDistillation (MCAKD) framework. Key innovations include a Cross-Attention-Based\nKnowledge Selection (CA-KS) module that selectively identifies critical\nfeatures from the teacher model, and an Autonomous Learning-Passive Learning\n(AL-PL) strategy that balances knowledge transfer with independent learning to\nachieve high training efficiency at a manageable computational cost. When\napplied to the WiFo FM, the distilled Tiny-WiFo model, with only 5.5M\nparameters, achieves a 1.6 ms inference time on edge hardware while retaining\nover 98% of WiFo's performance and its crucial zero-shot generalization\ncapability, making real-time FM deployment viable."}
{"id": "2511.04042", "pdf": "https://arxiv.org/pdf/2511.04042", "abs": "https://arxiv.org/abs/2511.04042", "authors": ["Kailun Ji", "Xiaoyu Hu", "Xinyu Zhang", "Jun Chen"], "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently\nchallenged by complex terrain and disrupted communications. While Unmanned\nAerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area\nsearch and supply delivery, yet their effective coordination places a\nsignificant cognitive burden on human operators. The core human-machine\ncollaboration bottleneck lies in the ``intention-to-action gap'', which is an\nerror-prone process of translating a high-level rescue objective into a\nlow-level swarm command under high intensity and pressure. To bridge this gap,\nthis study proposes a novel LLM-CRF system that leverages Large Language Models\n(LLMs) to model and augment human-swarm teaming cognition. The proposed\nframework initially captures the operator's intention through natural and\nmulti-modal interactions with the device via voice or graphical annotations. It\nthen employs the LLM as a cognitive engine to perform intention comprehension,\nhierarchical task decomposition, and mission planning for the UAV swarm. This\nclosed-loop framework enables the swarm to act as a proactive partner,\nproviding active feedback in real-time while reducing the need for manual\nmonitoring and control, which considerably advances the efficacy of the SAR\ntask. We evaluate the proposed framework in a simulated SAR scenario.\nExperimental results demonstrate that, compared to traditional order and\ncommand-based interfaces, the proposed LLM-driven approach reduced task\ncompletion time by approximately $64.2\\%$ and improved task success rate by\n$7\\%$. It also leads to a considerable reduction in subjective cognitive\nworkload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the\npotential of LLMs to create more intuitive and effective human-swarm\ncollaborations in high-stakes scenarios."}
{"id": "2511.04200", "pdf": "https://arxiv.org/pdf/2511.04200", "abs": "https://arxiv.org/abs/2511.04200", "authors": ["Yuanhan Ni", "Fan Liu", "Haoran Yin", "Yanqun Tang", "Zulin Wang"], "title": "Ambiguity Function Analysis of AFDM Under Pulse-Shaped Random ISAC Signaling", "categories": ["eess.SP"], "comment": null, "summary": "This paper investigates the ambiguity function (AF) of the emerging affine\nfrequency division multiplexing (AFDM) waveform for Integrated Sensing and\nCommunication (ISAC) signaling under a pulse shaping regime. Specifically, we\nfirst derive the closed-form expression of the average squared discrete period\nAF (DPAF) for AFDM waveform without pulse shaping, revealing that the AF\ndepends on the parameter $c_1$ and the kurtosis of random communication data,\nwhile being independent of the parameter $c_2$. As a step further, we conduct a\ncomprehensive analysis on the AFs of various waveforms, including AFDM,\northogonal frequency division multiplexing (OFDM) and orthogonal chirp-division\nmultiplexing (OCDM). Our results indicate that all three waveforms exhibit the\nsame number of regular depressions in the sidelobes of their AFs, which incurs\nperformance loss for detecting and estimating weak targets. However, the AFDM\nwaveform can flexibly control the positions of depressions by adjusting the\nparameter $c_1$, which motivates a novel design approach of the AFDM parameters\nto mitigate the adverse impact of depressions of the strong target on the weak\ntarget. Furthermore, a closed-form expression of the average squared DPAF for\npulse-shaped random AFDM waveform is derived, which demonstrates that the pulse\nshaping filter generates the shaped mainlobe along the delay axis and the rapid\nroll-off sidelobes along the Doppler axis. Numerical results verify the\neffectiveness of our theoretical analysis and proposed design methodology for\nthe AFDM modulation."}
{"id": "2511.04052", "pdf": "https://arxiv.org/pdf/2511.04052", "abs": "https://arxiv.org/abs/2511.04052", "authors": ["Kyongsik Yun", "David Bayard", "Gerik Kubiak", "Austin Owens", "Andrew Johnson", "Ryan Johnson", "Dan Scharf", "Thomas Lu"], "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors", "categories": ["cs.RO"], "comment": null, "summary": "Future planetary exploration missions demand high-performance, fault-tolerant\ncomputing to enable autonomous Guidance, Navigation, and Control (GNC) and\nLander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).\nThis paper evaluates the deployment of GNC and LVS algorithms on\nnext-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx\nVersal--demonstrating up to 15x speedup for LVS image processing and over 250x\nspeedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory\noptimization compared to legacy spaceflight hardware. To ensure computational\nreliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for\nTrusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that\nperforms real-time fault detection and correction across redundant cores.\nARBITER is validated in both static optimization tasks (GFOLD) and dynamic\nclosed-loop control (Attitude Control System). A fault injection study further\nidentifies the gradient computation stage in GFOLD as the most sensitive to\nbit-level errors, motivating selective protection strategies and vector-based\noutput arbitration. This work establishes a scalable and energy-efficient\narchitecture for future missions, including Mars Sample Return, Enceladus\nOrbilander, and Ceres Sample Return, where onboard autonomy, low latency, and\nfault resilience are critical."}
{"id": "2511.04292", "pdf": "https://arxiv.org/pdf/2511.04292", "abs": "https://arxiv.org/abs/2511.04292", "authors": ["Arne Van Den Kerchove", "Hakim Si-Mohammed", "François Cabestaing", "Marc M. Van Hulle"], "title": "BTTDA: Block-Term Tensor Discriminant Analysis for Brain-Computer Interfacing", "categories": ["eess.SP", "q-bio.NC"], "comment": "This archive contains 26 pages, 7 figures, 2 tables, 3 appendices and\n  3 ancillary files (erp_results.csv, mi_results.csv, block_theta_results.csv).\n  Source code is available at https://github.com/arnevdk/bttda", "summary": "Brain-computer interfaces (BCIs) allow direct communication between the brain\nand external devices, frequently using electroencephalography (EEG) to record\nneural activity. Dimensionality reduction and structured regularization are\nessential for effectively classifying task-related brain signals, including\nevent-related potentials (ERPs) and motor imagery (MI) rhythms. Current\ntensor-based approaches, such as Tucker and PARAFAC decompositions, often lack\nthe flexibility needed to fully capture the complexity of EEG data. This study\nintroduces Block-Term Tensor Discriminant Analysis (BTTDA): a novel\ntensor-based and supervised feature extraction method designed to enhance\nclassification accuracy by providing flexible multilinear dimensionality\nreduction. Extending Higher Order Discriminant Analysis (HODA), BTTDA uses a\nnovel and interpretable forward model for HODA combined with a deflation scheme\nto iteratively extract discriminant block terms, improving feature\nrepresentation for classification. BTTDA and a sum-of-rank-1-terms variant\nPARAFACDA were evaluated on publicly available ERP (second-order tensors) and\nMI (third-order tensors) EEG datasets from the MOABB benchmarking framework.\nBenchmarking revealed that BTTDA and PARAFACDA significantly outperform the\ntraditional HODA method in ERP decoding, resulting in state-of-the art\nperformance (ROC-AUC = 91.25%). For MI, decoding results of HODA, BTTDA and\nPARAFACDA were subpar, but BTTDA still significantly outperformed HODA (64.52%\n> 61.00%). The block-term structure of BTTDA enables interpretable and more\nefficient dimensionality reduction without compromising discriminative power.\nThis offers a promising and adaptable approach for feature extraction in BCI\nand broader neuroimaging applications."}
{"id": "2511.04109", "pdf": "https://arxiv.org/pdf/2511.04109", "abs": "https://arxiv.org/abs/2511.04109", "authors": ["Yanbo Pang", "Qingkai Li", "Mingguo Zhao"], "title": "CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN", "categories": ["cs.RO"], "comment": null, "summary": "As robotic arm applications extend beyond industrial settings into\nhealthcare, service, and daily life, existing control algorithms struggle to\nachieve the agile manipulation required for complex environments with dynamic\ntrajectories, unpredictable interactions, and diverse objects. This paper\npresents a biomimetic control framework based on Spiking Neural Networks (SNN),\ninspired by the human Central Nervous System (CNS), to achieve agile control in\nsuch environments. The proposed framework features five control modules\n(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three\nhierarchical control levels (first-order, second-order, third-order), and two\ninformation pathways (ascending, descending). Each module is fully implemented\nusing SNN. The spinal cord module uses spike encoding and Leaky\nIntegrate-and-Fire (LIF) neurons for feedback control. The brainstem module\nemploys a network of LIF and non-spiking LIF neurons to dynamically adjust\nspinal cord parameters via reinforcement learning. The thalamus module\nsimilarly adjusts the cerebellum's torque outputs. The cerebellum module uses a\nrecurrent SNN to learn the robotic arm's dynamics through regression, providing\nfeedforward gravity compensation torques. The framework is validated both in\nsimulation and on real-world robotic arm platform under various loads and\ntrajectories. Results demonstrate that our method outperforms the\nindustrial-grade position control in manipulation agility."}
{"id": "2511.04351", "pdf": "https://arxiv.org/pdf/2511.04351", "abs": "https://arxiv.org/abs/2511.04351", "authors": ["Hasan Akgul", "Mari Eplik", "Javier Rojas", "Akira Yamamoto", "Rajesh Kumar", "Maya Singh"], "title": "RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding", "categories": ["eess.SP"], "comment": "11 pages, 6 figures,", "summary": "Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,\npoint cloud) can achieve high accuracy but typically relies on large labeled\ndatasets and degrades sharply when sensors fail or are noisy. We present Robust\nCross-Modal Contrastive Learning (RCMCL), a self-supervised framework that\nlearns modality-invariant representations and remains reliable under modality\ndropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastive\nobjective that aligns heterogeneous streams, (ii) an intra-modal\nself-distillation objective that improves view-invariance and reduces\nredundancy, and (iii) a degradation simulation objective that explicitly trains\nmodels to recover from masked or corrupted inputs. At inference, an Adaptive\nModality Gating (AMG) network assigns data-driven reliability weights to each\nmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCL\nattains state-of-the-art accuracy in standard settings and exhibits markedly\nbetter robustness: under severe dual-modality dropout it shows only an 11.5%\ndegradation, significantly outperforming strong supervised fusion baselines.\nThese results indicate that self-supervised cross-modal alignment, coupled with\nexplicit degradation modeling and adaptive fusion, is key to deployable\nmulti-modal HAR."}
{"id": "2511.04131", "pdf": "https://arxiv.org/pdf/2511.04131", "abs": "https://arxiv.org/abs/2511.04131", "authors": ["Yitang Li", "Zhengyi Luo", "Tonghe Zhang", "Cunxi Dai", "Anssi Kanervisto", "Andrea Tirinzoni", "Haoyang Weng", "Kris Kitani", "Mateusz Guzek", "Ahmed Touati", "Alessandro Lazaric", "Matteo Pirotta", "Guanya Shi"], "title": "BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Building Behavioral Foundation Models (BFMs) for humanoid robots has the\npotential to unify diverse control tasks under a single, promptable generalist\npolicy. However, existing approaches are either exclusively deployed on\nsimulated humanoid characters, or specialized to specific tasks such as\ntracking. We propose BFM-Zero, a framework that learns an effective shared\nlatent representation that embeds motions, goals, and rewards into a common\nspace, enabling a single policy to be prompted for multiple downstream tasks\nwithout retraining. This well-structured latent space in BFM-Zero enables\nversatile and robust whole-body skills on a Unitree G1 humanoid in the real\nworld, via diverse inference methods, including zero-shot motion tracking, goal\nreaching, and reward optimization, and few-shot optimization-based adaptation.\nUnlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds\nupon recent advancements in unsupervised RL and Forward-Backward (FB) models,\nwhich offer an objective-centric, explainable, and smooth latent representation\nof whole-body motions. We further extend BFM-Zero with critical reward shaping,\ndomain randomization, and history-dependent asymmetric learning to bridge the\nsim-to-real gap. Those key design choices are quantitatively ablated in\nsimulation. A first-of-its-kind model, BFM-Zero establishes a step toward\nscalable, promptable behavioral foundation models for whole-body humanoid\ncontrol."}
{"id": "2511.04362", "pdf": "https://arxiv.org/pdf/2511.04362", "abs": "https://arxiv.org/abs/2511.04362", "authors": ["Chiara Telli", "Oleg Antropov", "Anne Lönnqvist", "Marco Lavalle"], "title": "High-Resolution Forest Mapping from L-Band Interferometric SAR Time Series using Deep Learning over Northern Spain", "categories": ["eess.SP"], "comment": null, "summary": "In this study, we examine the potential of high-resolution forest mapping\nusing L-band interferometric time series datasets and deep learning modeling.\nOur SAR data are represented by a time series of nine ALOS-2 PALSAR-2 dual-pol\nSAR images acquired at near-zero spatial baseline over a study site in\nAsturias, Northern Spain. Reference data are collected using airborne laser\nscanning. We examine the performance of several candidate deep learning models\nfrom UNet-family with various combinations of input polarimetric and\ninterferometric features. In addition to basic Vanilla UNet, attention\nreinforced UNet model with squeeze-excitation blocks (SeU-Net) and advanced\nUNet model with nested structure and skip pathways are used. Studied features\ninclude dual pol interferometric observables additionally incorporating\nmodel-based derived measures. Results show that adding model-based inverted\nInSAR features or InSAR coherence layers improves retrieval accuracy compared\nto using backscatter intensity only. Use of attention mechanisms and nested\nconnection fusion provides better predictions than using Vanilla UNet or\ntraditional machine learning methods. Forest height retrieval accuracies range\nbetween 3.1-3.8 m (R2 = 0.45--0.55) at 20 m resolution when only intensity data\nare used, and improve to less than 2.8 m when both intensity and\ninterferometric coherence features are included. At 40 m and 60 m resolution,\nretrieval performance further improves, primarily due to higher SNR in both the\nintensity and interferometric layers. When using intensity at 60 m resolution,\nbest achieved RMSE is 2.2 m, while when using all suitable input features the\nachieved error is 1.95 m. We recommend this hybrid approach for L-band SAR\nretrievals also suitable for NISAR and future ROSE-L missions."}
{"id": "2511.04180", "pdf": "https://arxiv.org/pdf/2511.04180", "abs": "https://arxiv.org/abs/2511.04180", "authors": ["Yizhen Yin", "Dapeng Feng", "Hongbo Chen", "Yuhua Qi"], "title": "PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration", "categories": ["cs.RO"], "comment": null, "summary": "Existing Active SLAM methodologies face issues such as slow exploration speed\nand suboptimal paths. To address these limitations, we propose a hybrid\nframework combining a Path-Uncertainty Co-Optimization Deep Reinforcement\nLearning framework and a Lightweight Stagnation Detection mechanism. The\nPath-Uncertainty Co-Optimization framework jointly optimizes travel distance\nand map uncertainty through a dual-objective reward function, balancing\nexploration and exploitation. The Lightweight Stagnation Detection reduces\nredundant exploration through Lidar Static Anomaly Detection and Map Update\nStagnation Detection, terminating episodes on low expansion rates. Experimental\nresults show that compared with the frontier-based method and RRT method, our\napproach shortens exploration time by up to 65% and reduces path distance by up\nto 42%, significantly improving exploration efficiency in complex environments\nwhile maintaining reliable map completeness. Ablation studies confirm that the\ncollaborative mechanism accelerates training convergence. Empirical validation\non a physical robotic platform demonstrates the algorithm's practical\napplicability and its successful transferability from simulation to real-world\nenvironments."}
{"id": "2511.04448", "pdf": "https://arxiv.org/pdf/2511.04448", "abs": "https://arxiv.org/abs/2511.04448", "authors": ["Chu Li", "Kevin Weinberger", "Aydin Sezgin"], "title": "A Lightweight Framework for Integrated Sensing and Communications with RIS", "categories": ["eess.SP"], "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have been recognized as a promising\ntechnology to enhance both communication and sensing performance in integrated\nsensing and communication (ISAC) systems for future 6G networks. However,\nexisting RIS optimization methods for improving ISAC performance are mainly\nbased on semidefinite relaxation (SDR) or iterative algorithms. The former\nsuffers from high computational complexity and limited scalability, especially\nwhen the number of RIS elements becomes large, while the latter yields\nsuboptimal solutions whose performance depends on initialization. In this work,\nwe introduce a lightweight RIS phase design framework that provides a\nclosed-form solution and explicitly accounts for the trade-off between\ncommunication and sensing, as well as proportional beam gain distribution\ntoward multiple sensing targets. The key idea is to partition the RIS\nconfiguration into two parts: the first part is designed to maximize the\ncommunication performance, while the second introduces small perturbations to\ngenerate multiple beams for multi-target sensing. Simulation results validate\nthe effectiveness of the proposed approach and demonstrate that it achieves\nperformance comparable to SDR but with significantly lower computational\ncomplexity."}
{"id": "2511.04199", "pdf": "https://arxiv.org/pdf/2511.04199", "abs": "https://arxiv.org/abs/2511.04199", "authors": ["Shenglin Wang", "Mingtong Dai", "Jingxuan Su", "Lingbo Liu", "Chunjie Chen", "Xinyu Wu", "Liang Lin"], "title": "GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments", "categories": ["cs.RO"], "comment": null, "summary": "Robotic grasping is a fundamental capability for autonomous manipulation, yet\nremains highly challenging in cluttered environments where occlusion, poor\nperception quality, and inconsistent 3D reconstructions often lead to unstable\nor failed grasps. Conventional pipelines have widely relied on RGB-D cameras to\nprovide geometric information, which fail on transparent or glossy objects and\ndegrade at close range. We present GraspView, an RGB-only robotic grasping\npipeline that achieves accurate manipulation in cluttered environments without\ndepth sensors. Our framework integrates three key components: (i) global\nperception scene reconstruction, which provides locally consistent, up-to-scale\ngeometry from a single RGB view and fuses multi-view projections into a\ncoherent global 3D scene; (ii) a render-and-score active perception strategy,\nwhich dynamically selects next-best-views to reveal occluded regions; and (iii)\nan online metric alignment module that calibrates VGGT predictions against\nrobot kinematics to ensure physical scale consistency. Building on these\ntailor-designed modules, GraspView performs best-view global grasping, fusing\nmulti-view reconstructions and leveraging GraspNet for robust execution.\nExperiments on diverse tabletop objects demonstrate that GraspView\nsignificantly outperforms both RGB-D and single-view RGB baselines, especially\nunder heavy occlusion, near-field sensing, and with transparent objects. These\nresults highlight GraspView as a practical and versatile alternative to RGB-D\npipelines, enabling reliable grasping in unstructured real-world environments."}
{"id": "2511.04635", "pdf": "https://arxiv.org/pdf/2511.04635", "abs": "https://arxiv.org/abs/2511.04635", "authors": ["Qingbin Li", "Jian Pang"], "title": "An Area-Efficient 20-100-GHz Phase-Invariant Switch-Type Attenuator Achieving 0.1-dB Tuning Step in 65-nm CMOS", "categories": ["eess.SP"], "comment": "Accepted paper at IEEE UCMMT 2025. 3 pages, 7 figures. Uploaded as\n  preprint for open access", "summary": "This paper presents a switch-type attenuator working from 20 to 100 GHz. The\nattenuator adopts a capacitive compensation technique to reduce phase error.\nThe small resistors in this work are implemented with metal lines to reduce the\nintrinsic parasitic capacitance, which helps minimize the amplitude and phase\nerrors over a wide frequency range. Moreover, the utilization of metal lines\nalso reduces the chip area. In addition, a continuous tuning attenuation unit\nis employed to improve the overall attenuation accuracy of the attenuator. The\npassive attenuator is designed and fabricated in a standard 65nm CMOS. The\nmeasurement results reveal a relative attenuation range of 7.5 dB with a\ncontinuous tuning step within 20-100 GHz. The insertion loss is 1.6-3.8 dB\nwithin the operation band, while the return losses of all states are better\nthan 11.5 dB. The RMS amplitude and phase errors are below 0.15 dB and\n1.6{\\deg}, respectively."}
{"id": "2511.04249", "pdf": "https://arxiv.org/pdf/2511.04249", "abs": "https://arxiv.org/abs/2511.04249", "authors": ["Marco Iannotta", "Yuxuan Yang", "Johannes A. Stork", "Erik Schaffernicht", "Todor Stoyanov"], "title": "Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies", "categories": ["cs.RO"], "comment": null, "summary": "Sim-to-real transfer remains a major challenge in reinforcement learning (RL)\nfor robotics, as policies trained in simulation often fail to generalize to the\nreal world due to discrepancies in environment dynamics. Domain Randomization\n(DR) mitigates this issue by exposing the policy to a wide range of randomized\ndynamics during training, yet leading to a reduction in performance. While\nstandard approaches typically train policies agnostic to these variations, we\ninvestigate whether sim-to-real transfer can be improved by conditioning the\npolicy on an estimate of the dynamics parameters -- referred to as context. To\nthis end, we integrate a context estimation module into a DR-based RL framework\nand systematically compare SOTA supervision strategies. We evaluate the\nresulting context-aware policies in both a canonical control benchmark and a\nreal-world pushing task using a Franka Emika Panda robot. Results show that\ncontext-aware policies outperform the context-agnostic baseline across all\nsettings, although the best supervision strategy depends on the task."}
{"id": "2511.03743", "pdf": "https://arxiv.org/pdf/2511.03743", "abs": "https://arxiv.org/abs/2511.03743", "authors": ["Marios Impraimakis"], "title": "A convolutional neural network deep learning method for model class selection", "categories": ["eess.SY", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SP", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "comment": "31 pages, 16 figures, published in Earthquake Engineering &\n  Structural Dynamics", "summary": "The response-only model class selection capability of a novel deep\nconvolutional neural network method is examined herein in a simple, yet\neffective, manner. Specifically, the responses from a unique degree of freedom\nalong with their class information train and validate a one-dimensional\nconvolutional neural network. In doing so, the network selects the model class\nof new and unlabeled signals without the need of the system input information,\nor full system identification. An optional physics-based algorithm enhancement\nis also examined using the Kalman filter to fuse the system response signals\nusing the kinematics constraints of the acceleration and displacement data.\nImportantly, the method is shown to select the model class in slight signal\nvariations attributed to the damping behavior or hysteresis behavior on both\nlinear and nonlinear dynamic systems, as well as on a 3D building finite\nelement model, providing a powerful tool for structural health monitoring\napplications."}
{"id": "2511.04251", "pdf": "https://arxiv.org/pdf/2511.04251", "abs": "https://arxiv.org/abs/2511.04251", "authors": ["Jinfeng Liang", "Haocheng Guo", "Ximin Lyu"], "title": "Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism", "categories": ["cs.RO"], "comment": "8 pages 12 figures", "summary": "The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to\nits lower dead weight, which eliminates the actuators and mechanisms for\ntilting. However, the tailsitter UAV is susceptible to wind disturbances in\nmulti-rotor mode, as it exposes a large frontal fuselage area. To address this\nissue, our tailsitter UAV features a reconfigurable wing design, allowing wings\nto retract in multi-rotor mode and extend in fixed- wing mode. Considering\npower efficiency, we design a coaxial heterogeneous dual-rotor configuration,\nwhich significantly re- duces the total power consumption. To reduce structural\nweight and simplify structural complexity, we employ a swashplateless mechanism\nwith an improved design to control pitch and roll in multi-rotor mode. We\noptimize the structure of the swashplateless mechanism by adding flapping\nhinges, which reduces vibration during cyclic acceleration and deceleration.\nFinally, we perform comprehensive transition flight tests to validate stable\nflight performance across the entire flight envelope of the tailsitter UAV."}
{"id": "2511.03953", "pdf": "https://arxiv.org/pdf/2511.03953", "abs": "https://arxiv.org/abs/2511.03953", "authors": ["Wuxia Chen", "Taposh Banerjee", "Vahid Tarokh"], "title": "Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels", "categories": ["cs.LG", "eess.SP", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": null, "summary": "We address the problem of quickest change detection in Markov processes with\nunknown transition kernels. The key idea is to learn the conditional score\n$\\nabla_{\\mathbf{y}} \\log p(\\mathbf{y}|\\mathbf{x})$ directly from sample pairs\n$( \\mathbf{x},\\mathbf{y})$, where both $\\mathbf{x}$ and $\\mathbf{y}$ are\nhigh-dimensional data generated by the same transition kernel. In this way, we\navoid explicit likelihood evaluation and provide a practical way to learn the\ntransition dynamics. Based on this estimation, we develop a score-based CUSUM\nprocedure that uses conditional Hyvarinen score differences to detect changes\nin the kernel. To ensure bounded increments, we propose a truncated version of\nthe statistic. With Hoeffding's inequality for uniformly ergodic Markov\nprocesses, we prove exponential lower bounds on the mean time to false alarm.\nWe also prove asymptotic upper bounds on detection delay. These results give\nboth theoretical guarantees and practical feasibility for score-based detection\nin high-dimensional Markov models."}
{"id": "2511.04320", "pdf": "https://arxiv.org/pdf/2511.04320", "abs": "https://arxiv.org/abs/2511.04320", "authors": ["Kuankuan Sima", "Longbin Tang", "Haozhe Ma", "Lin Zhao"], "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance."}
{"id": "2511.04037", "pdf": "https://arxiv.org/pdf/2511.04037", "abs": "https://arxiv.org/abs/2511.04037", "authors": ["Arfina Rahman", "Mahesh Banavar"], "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals", "categories": ["cs.CV", "eess.SP"], "comment": "This work has been submitted to IEEE Transactions on Biometrics,\n  Behavior, and Identity Science (TBIOM) for possible publication", "summary": "Photoplethysmography (PPG) signals, which measure changes in blood volume in\nthe skin using light, have recently gained attention in biometric\nauthentication because of their non-invasive acquisition, inherent liveness\ndetection, and suitability for low-cost wearable devices. However, PPG signal\nquality is challenged by motion artifacts, illumination changes, and\ninter-subject physiological variability, making robust feature extraction and\nclassification crucial. This study proposes a lightweight and cost-effective\nbiometric authentication framework based on PPG signals extracted from\nlow-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings\nfrom 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The\nraw PPG signals undergo a standard preprocessing pipeline involving baseline\ndrift removal, motion artifact suppression using Principal Component Analysis\n(PCA), bandpass filtering, Fourier-based resampling, and amplitude\nnormalization. To generate robust representations, each one-dimensional PPG\nsegment is converted into a two-dimensional time-frequency scalogram via the\nContinuous Wavelet Transform (CWT), effectively capturing transient\ncardiovascular dynamics. We developed a hybrid deep learning model, termed\nCVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision\nTransformer (CVT) and ConvMixer branches with temporal features from a Long\nShort-Term Memory network (LSTM). The experimental results on 46 subjects\ndemonstrate an authentication accuracy of 98%, validating the robustness of the\nmodel to noise and variability between subjects. Due to its efficiency,\nscalability, and inherent liveness detection capability, the proposed system is\nwell-suited for real-world mobile and embedded biometric security applications."}
{"id": "2511.04357", "pdf": "https://arxiv.org/pdf/2511.04357", "abs": "https://arxiv.org/abs/2511.04357", "authors": ["Maëlic Neau", "Zoe Falomir", "Paulo E. Santos", "Anne-Gwenn Bosser", "Cédric Buche"], "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks."}
{"id": "2511.04291", "pdf": "https://arxiv.org/pdf/2511.04291", "abs": "https://arxiv.org/abs/2511.04291", "authors": ["Giovanni Barbarino", "Nicolas Gillis", "Subhayan Saha"], "title": "Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition", "categories": ["stat.ML", "cs.LG", "cs.NA", "eess.SP", "math.NA"], "comment": "38 pages, 4 figures", "summary": "Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used\nsuccessfully in many applications, such as hyperspectral imaging, chemical\nkinetics, spectroscopy, topic modeling, and audio source separation. However,\nits robustness to noise has been a long-standing open problem. In this paper,\nwe prove that min-vol NMF identifies the groundtruth factors in the presence of\nnoise under a condition referred to as the expanded sufficiently scattered\ncondition which requires the data points to be sufficiently well scattered in\nthe latent simplex generated by the basis vectors."}
{"id": "2511.04375", "pdf": "https://arxiv.org/pdf/2511.04375", "abs": "https://arxiv.org/abs/2511.04375", "authors": ["Anna Mészáros", "Javier Alonso-Mora", "Jens Kober"], "title": "Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories", "categories": ["cs.RO"], "comment": null, "summary": "Effectively capturing the joint distribution of all agents in a scene is\nrelevant for predicting the true evolution of the scene and in turn providing\nmore accurate information to the decision processes of autonomous vehicles.\nWhile new models have been developed for this purpose in recent years, it\nremains unclear how to best represent the joint distributions particularly from\nthe perspective of the interactions between agents. Thus far there is no clear\nconsensus on how best to represent interactions between agents; whether they\nshould be learned implicitly from data by neural networks, or explicitly\nmodeled using the spatial and temporal relations that are more grounded in\nhuman decision-making. This paper aims to study various means of describing\ninteractions within the same network structure and their effect on the final\nlearned joint distributions. Our findings show that more often than not, simply\nallowing a network to establish interactive connections between agents based on\ndata has a detrimental effect on performance. Instead, having well defined\ninteractions (such as which agent of an agent pair passes first at an\nintersection) can often bring about a clear boost in performance."}
{"id": "2511.04471", "pdf": "https://arxiv.org/pdf/2511.04471", "abs": "https://arxiv.org/abs/2511.04471", "authors": ["Ali Bemani", "Nassar Ksairi", "Marios Kountouris"], "title": "Affine Frequency Division Multiplexing: From Communication to Sensing", "categories": ["cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "Affine Frequency Division Multiplexing (AFDM) has been proposed as an\neffective waveform for achieving the full diversity of doubly-dispersive\n(delay-Doppler) channels. While this property is closely related to range and\nvelocity estimation in sensing, this article focuses on other AFDM features\nthat are particularly relevant for addressing two challenges in integrated\nsensing and communication (ISAC) systems: (1) maintaining receiver complexity\nand energy consumption at acceptable levels while supporting the large\nbandwidths required for high delay/range resolution, and (2) mitigating\ninterference in multiradar environments. In monostatic sensing, where direct\ntransmitter-receiver leakage is a major impairment, we show that AFDM-based\nISAC receivers can address the first challenge through their compatibility with\nlow-complexity self-interference cancellation (SIC) schemes and reduced\nsampling rates via analog dechirping. In bistatic sensing, where such analog\nsolutions may not be feasible, we demonstrate that AFDM supports sub-Nyquist\nsampling without requiring hardware modifications while preserving delay\nresolution. Finally, we show that the second challenge can be addressed by\nleveraging the resource-assignment flexibility of the discrete affine Fourier\ntransform (DAFT) underlying the AFDM waveform."}
{"id": "2511.04381", "pdf": "https://arxiv.org/pdf/2511.04381", "abs": "https://arxiv.org/abs/2511.04381", "authors": ["Dexin wang", "Faliang Chang", "Chunsheng Liu"], "title": "ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Efficiently leveraging simulation to acquire advanced manipulation skills is\nboth challenging and highly significant. We introduce \\textit{ForeRobo}, a\ngenerative robotic agent that utilizes generative simulations to autonomously\nacquire manipulation skills driven by envisioned goal states. Instead of\ndirectly learning low-level policies, we advocate integrating generative\nparadigms with classical control. Our approach equips a robotic agent with a\nself-guided \\textit{propose-generate-learn-actuate} cycle. The agent first\nproposes the skills to be acquired and constructs the corresponding simulation\nenvironments; it then configures objects into appropriate arrangements to\ngenerate skill-consistent goal states (\\textit{ForeGen}). Subsequently, the\nvirtually infinite data produced by ForeGen are used to train the proposed\nstate generation model (\\textit{ForeFormer}), which establishes point-wise\ncorrespondences by predicting the 3D goal position of every point in the\ncurrent state, based on the scene state and task instructions. Finally,\nclassical control algorithms are employed to drive the robot in real-world\nenvironments to execute actions based on the envisioned goal states. Compared\nwith end-to-end policy learning methods, ForeFormer offers superior\ninterpretability and execution efficiency. We train and benchmark ForeFormer\nacross a variety of rigid-body and articulated-object manipulation tasks, and\nobserve an average improvement of 56.32\\% over the state-of-the-art state\ngeneration models, demonstrating strong generality across different\nmanipulation patterns. Moreover, in real-world evaluations involving more than\n20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits\nremarkable generalization capabilities, attaining an average success rate of\n79.28\\%."}
{"id": "2511.04630", "pdf": "https://arxiv.org/pdf/2511.04630", "abs": "https://arxiv.org/abs/2511.04630", "authors": ["Stavros Mitrolaris", "Subhankar Banerjee", "Sennur Ulukus"], "title": "Age of Job Completion Minimization with Stable Queues", "categories": ["cs.IT", "cs.NI", "cs.SY", "eess.SP", "eess.SY", "math.IT", "math.PR"], "comment": null, "summary": "We consider a time-slotted job-assignment system with a central server, N\nusers and a machine which changes its state according to a Markov chain (hence\ncalled a Markov machine). The users submit their jobs to the central server\naccording to a stochastic job arrival process. For each user, the server has a\ndedicated job queue. Upon receiving a job from a user, the server stores that\njob in the corresponding queue. When the machine is not working on a job\nassigned by the server, the machine can be either in internally busy or in free\nstate, and the dynamics of these states follow a binary symmetric Markov chain.\nUpon sampling the state information of the machine, if the server identifies\nthat the machine is in the free state, it schedules a user and submits a job to\nthe machine from the job queue of the scheduled user. To maximize the number of\njobs completed per unit time, we introduce a new metric, referred to as the age\nof job completion. To minimize the age of job completion and the sampling cost,\nwe propose two policies and numerically evaluate their performance. For both of\nthese policies, we find sufficient conditions under which the job queues will\nremain stable."}
{"id": "2511.04421", "pdf": "https://arxiv.org/pdf/2511.04421", "abs": "https://arxiv.org/abs/2511.04421", "authors": ["Yueyang Weng", "Xiaopeng Zhang", "Yongjin Mu", "Yingcong Zhu", "Yanjie Li", "Qi Liu"], "title": "Temporal Action Selection for Action Chunking", "categories": ["cs.RO"], "comment": null, "summary": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy."}
{"id": "2511.04555", "pdf": "https://arxiv.org/pdf/2511.04555", "abs": "https://arxiv.org/abs/2511.04555", "authors": ["Tao Lin", "Yilei Zhong", "Yuxin Du", "Jingjing Zhang", "Jiting Liu", "Yinxinyu Chen", "Encheng Gu", "Ziyan Liu", "Hongyi Cai", "Yanwen Zou", "Lixing Zou", "Zhaoye Zhou", "Gen Li", "Bo Zhao"], "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment", "categories": ["cs.RO", "cs.CV"], "comment": "Github: https://github.com/MINT-SJTU/Evo-1", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models."}
{"id": "2511.04664", "pdf": "https://arxiv.org/pdf/2511.04664", "abs": "https://arxiv.org/abs/2511.04664", "authors": ["Phat Nguyen", "Erfan Aasi", "Shiva Sreeram", "Guy Rosman", "Andrew Silva", "Sertac Karaman", "Daniela Rus"], "title": "SAFe-Copilot: Unified Shared Autonomy Framework", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent."}
{"id": "2511.04665", "pdf": "https://arxiv.org/pdf/2511.04665", "abs": "https://arxiv.org/abs/2511.04665", "authors": ["Kaifeng Zhang", "Shuo Sha", "Hanxiao Jiang", "Matthew Loper", "Hyunjong Song", "Guangyan Cai", "Zhuo Xu", "Xiaochen Hu", "Changxi Zheng", "Yunzhu Li"], "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Website: https://real2sim-eval.github.io/", "summary": "Robotic manipulation policies are advancing rapidly, but their direct\nevaluation in the real world remains costly, time-consuming, and difficult to\nreproduce, particularly for tasks involving deformable objects. Simulation\nprovides a scalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of soft-body\ninteractions. We present a real-to-sim policy evaluation framework that\nconstructs soft-body digital twins from real-world videos and renders robots,\nobjects, and environments with photorealistic fidelity using 3D Gaussian\nSplatting. We validate our approach on representative deformable manipulation\ntasks, including plush toy packing, rope routing, and T-block pushing,\ndemonstrating that simulated rollouts correlate strongly with real-world\nexecution performance and reveal key behavioral patterns of learned policies.\nOur results suggest that combining physics-informed reconstruction with\nhigh-quality rendering enables reproducible, scalable, and accurate evaluation\nof robotic manipulation policies. Website: https://real2sim-eval.github.io/"}
{"id": "2511.04671", "pdf": "https://arxiv.org/pdf/2511.04671", "abs": "https://arxiv.org/abs/2511.04671", "authors": ["Maximus A. Pace", "Prithwish Dan", "Chuanruo Ning", "Atiksh Bhardwaj", "Audrey Du", "Edward W. Duan", "Wei-Chiu Ma", "Kushal Kedia"], "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/."}
{"id": "2511.04679", "pdf": "https://arxiv.org/pdf/2511.04679", "abs": "https://arxiv.org/abs/2511.04679", "authors": ["Qingzhou Lu", "Yao Feng", "Baiyu Shi", "Michael Piseno", "Zhenan Bao", "C. Karen Liu"], "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Home page: https://gentle-humanoid.axell.top", "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments."}
{"id": "2511.03733", "pdf": "https://arxiv.org/pdf/2511.03733", "abs": "https://arxiv.org/abs/2511.03733", "authors": ["Pratham Gandhi"], "title": "HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "This thesis introduces the Haptic-Audio Code Interface (HACI), an educational\ntool designed to enhance programming education for visually impaired (VI)\nstudents by integrating haptic and audio feedback to compensate for the absence\nof visual cues. HACI consists of a non-resource-intensive web application\nsupporting JavaScript program development, execution, and debugging, connected\nvia a cable to an Arduino-powered glove with six integrated haptic motors to\nprovide physical feedback to VI programmers. Motivated by the need to provide\nequitable educational opportunities in computer science, HACI aims to improve\nnon-visual code navigation, comprehension, summarizing, editing, and debugging\nfor students with visual impairments while minimizing cognitive load. This work\ndetails HACI's design principles, technical implementation, and a preliminary\nevaluation through a pilot study conducted with undergraduate Computer Science\nstudents. Findings indicate that HACI aids in the non-visual navigation and\nunderstanding of programming constructs, although challenges remain in refining\nfeedback mechanisms to ensure consistency and reliability, as well as\nsupplementing the current functionality with a more feature-reach and\ncustomizable accessible learning experience which will allow visually impaired\nstudents to fully utilize interleaved haptic and audio feedback. The study\nunderscores the transformative potential of haptic and audio feedback in\neducational practices for the visually impaired, setting a foundation for\nfuture research and development in accessible programming education. This\nthesis contributes to the field of accessible technology by demonstrating how\ntactile and auditory feedback can be effectively integrated into educational\ntools, thereby broadening accessibility in STEM education."}
{"id": "2511.03882", "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation."}
{"id": "2511.04388", "pdf": "https://arxiv.org/pdf/2511.04388", "abs": "https://arxiv.org/abs/2511.04388", "authors": ["Chang Liu", "Juan Li", "Sheng Zhang", "Chang Liu", "Jie Li", "Xu Zhang"], "title": "BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 5 figures, published to IROS 2025", "summary": "Depth estimation is one of the key technologies for realizing 3D perception\nin unmanned systems. Monocular depth estimation has been widely researched\nbecause of its low-cost advantage, but the existing methods face the challenges\nof poor depth estimation performance and blurred object boundaries on embedded\nsystems. In this paper, we propose a novel monocular depth estimation model,\nBoRe-Depth, which contains only 8.7M parameters. It can accurately estimate\ndepth maps on embedded systems and significantly improves boundary quality.\nFirstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which\nadaptively fuses depth features to enhance boundary detail representation.\nSecondly, we integrate semantic knowledge into the encoder to improve the\nobject recognition and boundary perception capabilities. Finally, BoRe-Depth is\ndeployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We\ndemonstrate that the proposed model significantly outperforms previous\nlightweight models on multiple challenging datasets, and we provide detailed\nablation studies for the proposed methods. The code is available at\nhttps://github.com/liangxiansheng093/BoRe-Depth."}
