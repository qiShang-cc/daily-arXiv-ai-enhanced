{"id": "2601.03387", "pdf": "https://arxiv.org/pdf/2601.03387", "abs": "https://arxiv.org/abs/2601.03387", "authors": ["Amila Ravinath", "Minhua Ding", "Bikshapathi Gouda", "Italo Atzeni", "Antti Tölli"], "title": "SEP Analysis of a Low-Resolution SIMO System with M-PSK over Fading Channels", "categories": ["eess.SP"], "comment": "13 pages, 8 figures, Submitted to IEEE Transactions on Communications", "summary": "In this paper, the average symbol error probability (SEP) of a phase-quantized single-input multiple-output (SIMO) system with M-ary phase-shift keying (PSK) modulation is analyzed under Rayleigh fading and additive white Gaussian noise. By leveraging a novel method, we derive exact SEP expressions for a quadrature PSK (QPSK)-modulated n-bit phase-quantized SIMO system with maximum ratio combining (SIMO-MRC), along with the corresponding high signal-to-noise ratio (SNR) characterizations in terms of diversity and coding gains. For a QPSK-modulated 2-bit phase-quantized SIMO system with selection combining, the diversity and coding gains are further obtained for an arbitrary number of receive antennas, complementing existing results. Interestingly, the proposed method also reveals a duality between a SIMO-MRC system and a phase-quantized multiple-input single-output (MISO) system with maximum ratio transmission, when the modulation order, phase-quantization resolution, antenna configuration, and the channel state information (CSI) conditions are reciprocal. This duality enables direct inference to obtain the diversity of a general M-PSK-modulated n-bit phase-quantized SIMO-MRC system, and extends the results to its MISO counterpart. All the above results have been obtained assuming perfect CSI at the receiver (CSIR). Finally, the SEP analysis of a QPSK-modulated 2-bit phase-quantized SIMO system is extended to the limited CSIR case, where the CSI at each receive antenna is represented by only 2 bits of channel phase information. In this scenario, the diversity gain is shown to be further halved in general."}
{"id": "2601.03427", "pdf": "https://arxiv.org/pdf/2601.03427", "abs": "https://arxiv.org/abs/2601.03427", "authors": ["Mohammad Ghassemi", "Han Zhang", "Ali Afana", "Akram Bin Sediq", "Melike Erol-Kantarci"], "title": "Foundation Model-Aided Hierarchical Control for Robust RIS-Assisted Near-Field Communications", "categories": ["eess.SP"], "comment": "12 pages, 8 figures,", "summary": "The deployment of extremely large aperture arrays (ELAAs) in sixth-generation (6G) networks could shift communication into the near-field communication (NFC) regime. In this regime, signals exhibit spherical wave propagation, unlike the planar waves in conventional far-field systems. Reconfigurable intelligent surfaces (RISs) can dynamically adjust phase shifts to support NFC beamfocusing, concentrating signal energy at specific spatial coordinates. However, effective RIS utilization depends on both rapid channel state information (CSI) estimation and proactive blockage mitigation, which occur on inherently different timescales. CSI varies at millisecond intervals due to small-scale fading, while blockage events evolve over seconds, posing challenges for conventional single-level control algorithms. To address this issue, we propose a dual-transformer (DT) hierarchical framework that integrates two specialized transformer models within a hierarchical deep reinforcement learning (HDRL) architecture, referred to as the DT-HDRL framework. A fast-timescale transformer processes ray-tracing data for rapid CSI estimation, while a vision transformer (ViT) analyzes visual data to predict impending blockages. In HDRL, the high-level controller selects line-of-sight (LoS) or RIS-assisted non-line-of-sight (NLoS) transmission paths and sets goals, while the low-level controller optimizes base station (BS) beamfocusing and RIS phase shifts using instantaneous CSI. This dual-timescale coordination maximizes spectral efficiency (SE) while ensuring robust performance under dynamic conditions. Simulation results demonstrate that our approach improves SE by approximately 18% compared to single-timescale baselines, while the proposed blockage predictor achieves an F1-score of 0.92, providing a 769 ms advance warning window in dynamic scenarios."}
{"id": "2601.03446", "pdf": "https://arxiv.org/pdf/2601.03446", "abs": "https://arxiv.org/abs/2601.03446", "authors": ["Melek Tuylu", "Eylem Erdogan"], "title": "Energy Harvesting in High Altitude Platform Station Enabled Sensor Networks", "categories": ["eess.SP"], "comment": "PDF-only submission approved by arXiv support due to unresolved TeX compilation issues", "summary": "High altitude platform station (HAPS) systems are becoming crucial facilitators for future wireless communication networks, enhancing connectivity across all vertical communication layers, including small Internet of Things (IoT) sensors and devices, terrestrial users, and aerial devices. In the context of the widely recognized vertical heterogeneous network (VHetNet) architecture, HAPS systems can provide service to both aerial and ground users. However, integrating HAPS systems as a core element in the VHetNet architecture presents a considerable energy challenge, marking a prominent constraint for their operation. Driven by this challenge, we introduce an energy harvesting (EH) strategy tailored for HAPS systems, enabling a HAPS system to gather energy from another HAPS system, which is not constrained by energy limitations. To assess the performance capabilities of the proposed model, we derive outage probability (OP), ergodic capacity (EC) and verify them by using Monte Carlo (MC) simulations. Moreover, we explore the system in terms of throughput. The findings reveal that harnessing full potential of EH stands as a viable approach to meet the energy demands of HAPS systems."}
{"id": "2601.03527", "pdf": "https://arxiv.org/pdf/2601.03527", "abs": "https://arxiv.org/abs/2601.03527", "authors": ["Ravneel Prasad", "Emanuele Viterbo"], "title": "Intensity Fluctuation Dynamics in XPM", "categories": ["eess.SP"], "comment": null, "summary": "Cross-Phase Modulation (XPM) constitutes a critical nonlinear impairment in high-capacity Wavelength Division Multiplexing (WDM) systems, significantly driven by intensity fluctuations (IFs) that evolve due to chromatic dispersion. This paper presents an enhanced XPM model that explicitly incorporates frequency-domain IF growth along the fiber, improving upon prior models that focused primarily on temporal pulse deformation. A direct correlation between this frequency-domain growth and XPM-induced phase distortions is established and analyzed. Results demonstrate that IF evolution, particularly at lower frequencies, profoundly affects XPM phase fluctuation spectra and phase variance. Validated through simulations, the model accurately predicts these spectral characteristics across various system parameters. Furthermore, the derived phase variance enables accurate prediction of system performance in terms of Bit Error Ratio (BER). These findings highlight the necessity of modeling frequency-domain IF evolution to accurately characterize XPM impairments, offering guidance for the design of advanced optical networks."}
{"id": "2601.03360", "pdf": "https://arxiv.org/pdf/2601.03360", "abs": "https://arxiv.org/abs/2601.03360", "authors": ["Timothy Barfoot", "Cedric Le Gentil", "Sven Lilge"], "title": "Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion", "categories": ["cs.RO", "eess.SY"], "comment": "21 pages, 12 figures", "summary": "Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits."}
{"id": "2601.03535", "pdf": "https://arxiv.org/pdf/2601.03535", "abs": "https://arxiv.org/abs/2601.03535", "authors": ["Zhiwen Zhou", "Chaoyue Zhang", "Xiaoli Xu", "Yong Zeng"], "title": "OpenISAC: An Open-Source Real-Time Experimentation Platform for OFDM-ISAC with Over-the-Air Synchronization", "categories": ["eess.SP"], "comment": "Submitted to IEEE Transactions on Wireless Communications for possible publication", "summary": "Integrated sensing and communication (ISAC) is envisioned to be one of the key usage scenarios for the sixth generation (6G) mobile communication networks. While significant progresses have been achieved for the theoretical studies, the further advancement of ISAC is hampered by the lack of accessible, open-source, and real-time experimental platforms. To address this gap, we introduce OpenISAC, a versatile and high-performance open-source platform for real-time ISAC experimentation. OpenISAC utilizes orthogonal frequency division multiplexing (OFDM) waveform and implements crucial sensing functionalities, including both monostatic and bistatic delay-Doppler sensing. A key feature of our platform is a novel over-the-air (OTA) synchronization mechanism that enables robust bistatic operations without requiring a wired connection between nodes. The platform is built entirely on open-source software, leveraging the universal software radio peripheral (USRP) hardware driver (UHD) library, thus eliminating the need for any commercial licenses. It supports a wide range of software-defined radios, from the cost-effective USRP B200 series to the high-performance X400 series. The physical layer modulator and demodulator are implemented with C++ for high-speed processing, while the sensing data is streamed to a Python environment, providing a user-friendly interface for rapid prototyping and validation of sensing signal processing algorithms. With flexible parameter selection and real-time communication and sensing operation, OpenISAC serves as a powerful and accessible tool for the academic and research communities to explore and innovate within the field of OFDM-ISAC."}
{"id": "2601.03371", "pdf": "https://arxiv.org/pdf/2601.03371", "abs": "https://arxiv.org/abs/2601.03371", "authors": ["Alexander Krawciw", "Nicolas Olmedo", "Faizan Rehmatullah", "Maxime Desjardins-Goulet", "Pascal Toupin", "Timothy D. Barfoot"], "title": "Lunar Rover Cargo Transport: Mission Concept and Field Test", "categories": ["cs.RO"], "comment": "15 Pages, 13 Figures, to appear in IEEE Transactions on Field Robotics", "summary": "In future operations on the lunar surface, automated vehicles will be required to transport cargo between known locations. Such vehicles must be able to navigate precisely in safe regions to avoid natural hazards, human-constructed infrastructure, and dangerous dark shadows. Rovers must be able to park their cargo autonomously within a small tolerance to achieve a successful pickup and delivery. In this field test, Lidar Teach and Repeat provides an ideal autonomy solution for transporting cargo in this way. A one-tonne path-to-flight rover was driven in a semi-autonomous remote-control mode to create a network of safe paths. Once the route was taught, the rover immediately repeated the entire network of paths autonomously while carrying cargo. The closed-loop performance is accurate enough to align the vehicle to the cargo and pick it up. This field report describes a two-week deployment at the Canadian Space Agency's Analogue Terrain, culminating in a simulated lunar operation to evaluate the system's capabilities. Successful cargo collection and delivery were demonstrated in harsh environmental conditions."}
{"id": "2601.03601", "pdf": "https://arxiv.org/pdf/2601.03601", "abs": "https://arxiv.org/abs/2601.03601", "authors": ["Kequan Zhou", "Guangyi Zhang", "Hanlei Li", "Yunlong Cai", "Shengli Liu", "Guanding Yu"], "title": "F$^4$-CKM: Learning Channel Knowledge Map with Radio Frequency Radiance Field Rendering", "categories": ["eess.SP"], "comment": null, "summary": "In 6G mobile communications, acquiring accurate and timely channel state information (CSI) becomes increasingly challenging due to the growing antenna array size and bandwidth. To alleviate the CSI feedback burden, the channel knowledge map (CKM) has emerged as a promising approach by leveraging environment-aware techniques to predict CSI based solely on user locations. However, how to effectively construct a CKM remains an open issue. In this paper, we propose F$^4$-CKM, a novel CKM construction framework characterized by four distinctive features: radiance Field rendering, spatial-Frequency-awareness, location-Free usage, and Fast learning. Central to our design is the adaptation of radiance field rendering techniques from computer vision to the radio frequency (RF) domain, enabled by a novel Wireless Radiator Representation (WiRARE) network that captures the spatial-frequency characteristics of wireless channels. Additionally, a novel shaping filter module and an angular sampling strategy are introduced to facilitate CKM construction. Extensive experiments demonstrate that F$^4$-CKM significantly outperforms existing baselines in terms of wireless channel prediction accuracy and efficiency."}
{"id": "2601.03398", "pdf": "https://arxiv.org/pdf/2601.03398", "abs": "https://arxiv.org/abs/2601.03398", "authors": ["Liam Merz Hoffmeister", "Brian Scassellati", "Daniel Rakita"], "title": "Towards Zero-Knowledge Task Planning via a Language-based Approach", "categories": ["cs.RO"], "comment": null, "summary": "In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup."}
{"id": "2601.03639", "pdf": "https://arxiv.org/pdf/2601.03639", "abs": "https://arxiv.org/abs/2601.03639", "authors": ["Kecheng Zhang", "Weijie Yuan", "Maria Sabrina Greco"], "title": "Zak-OTFS ISAC with Bistatic Sensing via Semi-Blind Atomic Norm Denoising Scheme", "categories": ["eess.SP"], "comment": "Submitted to IEEE for possible publication", "summary": "Integrated sensing and communication (ISAC) through Zak-transform-based orthogonal time frequency space (Zak-OTFS) modulation is a promising solution for high-mobility scenarios. Realizing accurate bistatic sensing and robust communication necessitates precise channel estimation; however, this remains a formidable challenge in doubly dispersive environments, where fractional delay-Doppler shifts induce severe channel spreading. This paper proposes a semi-blind atomic norm denoising scheme for Zak-OTFS ISAC with bistatic sensing. We first derive the discrete-time input-output (I/O) relationship of Zak-OTFS under fractional delay-Doppler shifts and rectangular windowing. Based on this I/O relation, we formulate the joint channel parameter estimation and data detection task as an atomic norm denoising problem, utilizing the negative square penalty method to handle the non-convex discrete constellation constraints. To solve this problem efficiently, we develop an accelerated iterative algorithm that integrates majorization-minimization, accelerated projected gradient, and inexact accelerated proximal gradient methods. We provide a rigorous convergence proof for the proposed algorithm. Simulation results demonstrate that the proposed scheme achieves super-resolution sensing accuracy and communication performance approaching the perfect channel state information lower bound."}
{"id": "2601.03447", "pdf": "https://arxiv.org/pdf/2601.03447", "abs": "https://arxiv.org/abs/2601.03447", "authors": ["Anna Zavei-Boroda", "J. Toby Minear", "Kyle Harlow", "Dusty Woods", "Christoffer Heckman"], "title": "Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy", "categories": ["cs.RO"], "comment": "10 pages, 6 figures. Preliminary results presented as a poster at an academic conference", "summary": "Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms."}
{"id": "2601.03735", "pdf": "https://arxiv.org/pdf/2601.03735", "abs": "https://arxiv.org/abs/2601.03735", "authors": ["Carl Collmann", "Ahmad Nimr", "Gerhard Fettweis"], "title": "Cramer-Rao Bound for Angle of Arrival Estimates in True-Time-Delay Systems", "categories": ["eess.SP"], "comment": "5 pages, 7 figures", "summary": "In the context of joint communication and sensing JC&S, the challenge of obtaining accurate parameter estimates is of interest. Parameter estimates, such as the AoA can be utilized for solving the initial access problem, interference mitigation, localization of users or monitoring of the environment and synchronization of MIMO systems. Recently, TTD systems have gained attention for fast beam training during initial access and mitigation of beam squinting. This work derives the CRB for angle estimates in typical TTD systems. Properties of the CRB and the Fisher information are investigated and numerically evaluated. Finally, methods for angle estimation such as ML and established estimators are utilized to solve the angle estimation problem using a uniform linear array."}
{"id": "2601.03449", "pdf": "https://arxiv.org/pdf/2601.03449", "abs": "https://arxiv.org/abs/2601.03449", "authors": ["Chris Webb", "Mobin Habibpour", "Mayamin Hamid Raha", "Ali Reza Tavakkoli", "Janice Coen", "Fatemeh Afghah"], "title": "FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin", "categories": ["cs.RO"], "comment": null, "summary": "Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires."}
{"id": "2601.03745", "pdf": "https://arxiv.org/pdf/2601.03745", "abs": "https://arxiv.org/abs/2601.03745", "authors": ["Weijia Wang", "Changsheng You", "Xiaodan Shao", "Rui Zhang"], "title": "Two-stage Multi-beam Training for Multiuser Millimeter-Wave Communications", "categories": ["eess.SP"], "comment": null, "summary": "In this letter, we study an efficient multi-beam training method for multiuser millimeter-wave communication systems. Unlike the conventional single-beam training method that relies on exhaustive search, multi-beam training design faces a key challenge in balancing the trade-off between beam training overhead and success beam-identification rate, exacerbated by severe inter-beam interference. To tackle this challenge, we propose a new two-stage multi-beam training method with two distinct multi-beam patterns to enable fast and accurate user angle identification. Specifically, in the first stage, the antenna array is divided into sparse subarrays to generate multiple beams (with high array gains), for identifying candidate user angles. In the second stage, the array is redivided into dense subarrays to generate flexibly steered wide beams, for which a cross-validation method is employed to effectively resolve the remaining angular ambiguity in the first stage. Last, numerical results demonstrate that the proposed method significantly improves the success beam-identification rate compared to existing multi-beam training methods, while retaining or even reducing the required beam training overhead."}
{"id": "2601.03519", "pdf": "https://arxiv.org/pdf/2601.03519", "abs": "https://arxiv.org/abs/2601.03519", "authors": ["Liangdong Zhang", "Yiming Nie", "Haoyang Li", "Fanjie Kong", "Baobao Zhang", "Shunxin Huang", "Kai Fu", "Chen Min", "Liang Xiao"], "title": "A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%."}
{"id": "2601.03789", "pdf": "https://arxiv.org/pdf/2601.03789", "abs": "https://arxiv.org/abs/2601.03789", "authors": ["Jun Jiang", "Xiaolong Ruan", "Shugong Xu"], "title": "CSI-MAE: A Masked Autoencoder-based Channel Foundation Model", "categories": ["eess.SP"], "comment": "6 pages", "summary": "Self-Supervised Learning (SSL) has emerged as a key technique in machine learning, tackling challenges such as limited labeled data, high annotation costs, and variable wireless channel conditions. It is essential for developing Channel Foundation Models (CFMs), which extract latent features from channel state information (CSI) and adapt to different wireless settings. Yet, existing CFMs have notable drawbacks: heavy reliance on scenario-specific data hinders generalization, they focus on single/dual tasks, and lack zero-shot learning ability. In this paper, we propose CSI-MAE, a generalized CFM leveraging masked autoencoder for cross-scenario generalization. Trained on 3GPP channel model datasets, it integrates sensing and communication via CSI perception and generation, proven effective across diverse tasks. A lightweight decoder finetuning strategy cuts training costs while maintaining competitive performance. Under this approach, CSI-MAE matches or surpasses supervised models. With full-parameter finetuning, it achieves the state-of-the-art performance. Its exceptional zero-shot transferability also rivals supervised techniques in cross-scenario applications, driving wireless communication innovation."}
{"id": "2601.03562", "pdf": "https://arxiv.org/pdf/2601.03562", "abs": "https://arxiv.org/abs/2601.03562", "authors": ["Samantha Sudhoff", "Pranesh Velmurugan", "Jiashu Liu", "Vincent Zhao", "Yung-Hsiang Lu", "Kristen Yeon-Ji Yun"], "title": "From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance", "categories": ["cs.RO"], "comment": null, "summary": "Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.\n  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality."}
{"id": "2601.03944", "pdf": "https://arxiv.org/pdf/2601.03944", "abs": "https://arxiv.org/abs/2601.03944", "authors": ["Xin Wang", "Héctor Delgado", "Nicholas Evans", "Xuechen Liu", "Tomi Kinnunen", "Hemlata Tak", "Kong Aik Lee", "Ivan Kukanov", "Md Sahidullah", "Massimiliano Todisco", "Junichi Yamagishi"], "title": "ASVspoof 5: Evaluation of Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech", "categories": ["eess.SP", "cs.SD"], "comment": "Submitted", "summary": "ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake detection solutions. A significant change from previous challenge editions is a new crowdsourced database collected from a substantially greater number of speakers under diverse recording conditions, and a mix of cutting-edge and legacy generative speech technology. With the new database described elsewhere, we provide in this paper an overview of the ASVspoof 5 challenge results for the submissions of 53 participating teams. While many solutions perform well, performance degrades under adversarial attacks and the application of neural encoding/compression schemes. Together with a review of post-challenge results, we also report a study of calibration in addition to other principal challenges and outline a road-map for the future of ASVspoof."}
{"id": "2601.03607", "pdf": "https://arxiv.org/pdf/2601.03607", "abs": "https://arxiv.org/abs/2601.03607", "authors": ["Tae Hoon Yang", "Haochen Shi", "Jiacheng Hu", "Zhicong Zhang", "Daniel Jiang", "Weizhuo Wang", "Yao He", "Zhen Wu", "Yuming Chen", "Yifan Hou", "Monroe Kennedy", "Shuran Song", "C. Karen Liu"], "title": "Locomotion Beyond Feet", "categories": ["cs.RO"], "comment": "Project website: https://locomotion-beyond-feet.github.io/", "summary": "Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences."}
{"id": "2601.04069", "pdf": "https://arxiv.org/pdf/2601.04069", "abs": "https://arxiv.org/abs/2601.04069", "authors": ["Lukas Schynol", "Marius Pesavento"], "title": "Hybrid Downlink Beamforming with Outage Constraints under Imperfect CSI using Model-Driven Deep Learning", "categories": ["eess.SP"], "comment": null, "summary": "We consider energy-efficient multi-user hybrid downlink beamforming (BF) and power allocation under imperfect channel state information (CSI) and probabilistic outage constraints. In this domain, classical optimization methods resort to computationally costly conic optimization problems. Meanwhile, generic deep network (DN) architectures lack interpretability and require large training data sets to generalize well. In this paper, we therefore propose a lightweight model-aided deep learning architecture based on a greedy selection algorithm for analog beam codewords. The architecture relies on an instance-adaptive augmentation of the signal model to estimate the impact of the CSI error. To learn the DN parameters, we derive a novel and efficient implicit representation of the nested constrained BF problem and prove sufficient conditions for the existence of the corresponding gradient. In the loss function, we utilize an annealing-based approximation of the outage compared to conventional quantile-based loss terms. This approximation adaptively anneals towards the exact probabilistic constraint depending on the current level of quality of service (QoS) violation. Simulations validate that the proposed DN can achieve the nominal outage level under CSI error due to channel estimation and channel compression, while allocating less power than benchmarks. Thereby, a single trained model generalizes to different numbers of users, QoS requirements and levels of CSI quality. We further show that the adaptive annealing-based loss function can accelerate the training and yield a better power-outage trade-off."}
{"id": "2601.03686", "pdf": "https://arxiv.org/pdf/2601.03686", "abs": "https://arxiv.org/abs/2601.03686", "authors": ["Lina Zhu", "Jiyu Cheng", "Yuehu Liu", "Wei Zhang"], "title": "Dual-Attention Heterogeneous GNN for Multi-robot Collaborative Area Search via Deep Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "In multi-robot collaborative area search, a key challenge is to dynamically balance the two objectives of exploring unknown areas and covering specific targets to be rescued. Existing methods are often constrained by homogeneous graph representations, thus failing to model and balance these distinct tasks. To address this problem, we propose a Dual-Attention Heterogeneous Graph Neural Network (DA-HGNN) trained using deep reinforcement learning. Our method constructs a heterogeneous graph that incorporates three entity types: robot nodes, frontier nodes, and interesting nodes, as well as their historical states. The dual-attention mechanism comprises the relational-aware attention and type-aware attention operations. The relational-aware attention captures the complex spatio-temporal relationships among robots and candidate goals. Building on this relational-aware heterogeneous graph, the type-aware attention separately computes the relevance between robots and each goal type (frontiers vs. points of interest), thereby decoupling the exploration and coverage from the unified tasks. Extensive experiments conducted in interactive 3D scenarios within the iGibson simulator, leveraging the Gibson and MatterPort3D datasets, validate the superior scalability and generalization capability of the proposed approach."}
{"id": "2601.03443", "pdf": "https://arxiv.org/pdf/2601.03443", "abs": "https://arxiv.org/abs/2601.03443", "authors": ["Mikhail Silaev", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Discriminating real and synthetic super-resolved audio samples using embedding-based classifiers", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": "Accepted for publication in Workshop Proceedingsof the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing", "summary": "Generative adversarial networks (GANs) and diffusion models have recently achieved state-of-the-art performance in audio super-resolution (ADSR), producing perceptually convincing wideband audio from narrowband inputs. However, existing evaluations primarily rely on signal-level or perceptual metrics, leaving open the question of how closely the distributions of synthetic super-resolved and real wideband audio match. Here we address this problem by analyzing the separability of real and super-resolved audio in various embedding spaces. We consider both middle-band ($4\\to 16$~kHz) and full-band ($16\\to 48$~kHz) upsampling tasks for speech and music, training linear classifiers to distinguish real from synthetic samples based on multiple types of audio embeddings. Comparisons with objective metrics and subjective listening tests reveal that embedding-based classifiers achieve near-perfect separation, even when the generated audio attains high perceptual quality and state-of-the-art metric scores. This behavior is consistent across datasets and models, including recent diffusion-based approaches, highlighting a persistent gap between perceptual quality and true distributional fidelity in ADSR models."}
{"id": "2601.03782", "pdf": "https://arxiv.org/pdf/2601.03782", "abs": "https://arxiv.org/abs/2601.03782", "authors": ["Wenlong Huang", "Yu-Wei Chao", "Arsalan Mousavian", "Ming-Yu Liu", "Dieter Fox", "Kaichun Mo", "Li Fei-Fei"], "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/."}
{"id": "2601.03831", "pdf": "https://arxiv.org/pdf/2601.03831", "abs": "https://arxiv.org/abs/2601.03831", "authors": ["Matteo Nerini", "Zheyu Wu", "Shanpu Shen", "Bruno Clerckx"], "title": "Low-Complexity Planar Beyond-Diagonal RIS Architecture Design Using Graph Theory", "categories": ["cs.IT", "eess.SP"], "comment": "Submitted to IEEE for publication", "summary": "Reconfigurable intelligent surfaces (RISs) enable programmable control of the wireless propagation environment and are key enablers for future networks. Beyond-diagonal RIS (BD-RIS) architectures enhance conventional RIS by interconnecting elements through tunable impedance components, offering greater flexibility with higher circuit complexity. However, excessive interconnections between BD-RIS elements require multi-layer printed circuit board (PCB) designs, increasing fabrication difficulty. In this letter, we use graph theory to characterize the BD-RIS architectures that can be realized on double-layer PCBs, denoted as planar-connected RISs. Among the possible planar-connected RISs, we identify the ones with the most degrees of freedom, expected to achieve the best performance under practical constraints."}
{"id": "2601.03807", "pdf": "https://arxiv.org/pdf/2601.03807", "abs": "https://arxiv.org/abs/2601.03807", "authors": ["K. Ege de Bruin", "Kyrre Glette", "Kai Olav Ellefsen"], "title": "Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots", "categories": ["cs.RO"], "comment": null, "summary": "Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions."}
{"id": "2601.04011", "pdf": "https://arxiv.org/pdf/2601.04011", "abs": "https://arxiv.org/abs/2601.04011", "authors": ["Wei Shi", "Wei Xu", "Yongming Huang", "Jiacheng Yao", "Wenhao Hu", "Dongming Wang"], "title": "Flexible-Duplex Cell-Free Architecture for Secure Uplink Communications in Low-Altitude Wireless Networks", "categories": ["cs.IT", "eess.SP"], "comment": "Submitted to an IEEE Journal", "summary": "Low-altitude wireless networks (LAWNs) are expected to play a central role in future 6G infrastructures, yet uplink transmissions of uncrewed aerial vehicles (UAVs) remain vulnerable to eavesdropping due to their limited transmit power, constrained antenna resources, and highly exposed air-ground propagation conditions. To address this fundamental bottleneck, we propose a flexible-duplex cell-free (CF) architecture in which each distributed access point (AP) can dynamically operate either as a receive AP for UAV uplink collection or as a transmit AP that generates cooperative artificial noise (AN) for secrecy enhancement. Such AP-level duplex flexibility introduces an additional spatial degree of freedom that enables distributed and adaptive protection against wiretapping in LAWNs. Building upon this architecture, we formulate a max-min secrecy-rate problem that jointly optimizes AP mode selection, receive combining, and AN covariance design. This tightly coupled and nonconvex optimization is tackled by first deriving the optimal receive combiners in closed form, followed by developing a penalty dual decomposition (PDD) algorithm with guaranteed convergence to a stationary solution. To further reduce computational burden, we propose a low-complexity sequential scheme that determines AP modes via a heuristic metric and then updates the AN covariance matrices through closed-form iterations embedded in the PDD framework. Simulation results show that the proposed flexible-duplex architecture yields substantial secrecy-rate gains over CF systems with fixed AP roles. The joint optimization method attains the highest secrecy performance, while the low-complexity approach achieves over 90% of the optimal performance with an order-of-magnitude lower computational complexity, offering a practical solution for secure uplink communications in LAWNs."}
{"id": "2601.03813", "pdf": "https://arxiv.org/pdf/2601.03813", "abs": "https://arxiv.org/abs/2601.03813", "authors": ["K. Ege de Bruin", "Kyrre Glette", "Kai Olav Ellefsen"], "title": "Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics", "categories": ["cs.RO"], "comment": null, "summary": "In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design."}
{"id": "2601.04166", "pdf": "https://arxiv.org/pdf/2601.04166", "abs": "https://arxiv.org/abs/2601.04166", "authors": ["Christian Forsch", "Laura Cottatellucci"], "title": "Expectation Propagation for Distributed Inference in Grant-Free Cell-Free Massive MIMO", "categories": ["cs.IT", "eess.SP"], "comment": "13 pages, 5 figures, submitted for possible journal publication", "summary": "Grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) systems are anticipated to be a key enabling technology for next-generation Internet-of-Things (IoT) networks, as they support massive connectivity without explicit scheduling. However, the large amount of connected devices prevents the use of orthogonal pilot sequences, resulting in severe pilot contamination (PC) that degrades channel estimation and data detection performance. Furthermore, scalable GF-CF-MaMIMO networks inherently rely on distributed signal processing. In this work, we consider the uplink of a GF-CF-MaMIMO system and propose two novel distributed algorithms for joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP). The first algorithm, denoted as JACD-EP, uses Gaussian approximations for the channel variables, whereas the second, referred to as JACD-EP-BG, models them as Bernoulli-Gaussian (BG) random variables. To integrate the BG distribution into the EP framework, we derive its exponential family representation and develop the two algorithms as efficient message passing over a factor graph constructed from the a posteriori probability (APP) distribution. The proposed framework is inherently scalable with respect to both the number of access points (APs) and user equipments (UEs). Simulation results show the efficient mitigation of PC by the proposed distributed algorithms and their superior detection accuracy compared to (genie-aided) centralized linear detectors."}
{"id": "2601.03904", "pdf": "https://arxiv.org/pdf/2601.03904", "abs": "https://arxiv.org/abs/2601.03904", "authors": ["Korbinian Moller", "Glenn Johannes Tungka", "Lucas Jürgens", "Johannes Betz"], "title": "Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware", "categories": ["cs.RO"], "comment": "7 pages, submitted to the IEEE Intelligent Vehicles Symposium (IV 2026), Detroit, MI, United States", "summary": "Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning"}
{"id": "2601.04190", "pdf": "https://arxiv.org/pdf/2601.04190", "abs": "https://arxiv.org/abs/2601.04190", "authors": ["Juan F. Gutierrez", "Nhung Nguyen", "Jesus M. Quintero", "Andres Gomez"], "title": "Solar Panel-based Visible Light Communication for Batteryless Systems", "categories": ["eess.SY", "eess.SP"], "comment": "This is an open-access, author-archived version of a manuscript published in ApplePies 2025 Conference", "summary": "This paper presents a batteryless wireless communication node for the Internet of Things, powered entirely by ambient light and capable of receiving data through visible light communication. A solar panel serves dual functions as an energy harvester and an optical antenna, capturing modulated signals from LED light sources. A lightweight analog front-end filters and digitizes the signals for an 8-bit low-power processor, which manages the system's operational states based on stored energy levels. The main processor is selectively activated to minimize energy consumption. Data reception is synchronized with the harvester's open-circuit phase, reducing interference and improving signal quality. The prototype reliably decodes 32-bit VLC frames at 800\\,Herz, consuming less than 2.8\\,mJ, and maintains sleep-mode power below 30\\,uW."}
{"id": "2601.03907", "pdf": "https://arxiv.org/pdf/2601.03907", "abs": "https://arxiv.org/abs/2601.03907", "authors": ["Mohammadreza Koolani", "Simeon Bamford", "Petr Trunin", "Simon F. Müller-Cleve", "Matteo Lo Preti", "Fulvio Mastrogiovanni", "Lucia Beccai", "Chiara Bartolozzi"], "title": "An Event-Based Opto-Tactile Skin", "categories": ["cs.RO"], "comment": "Accepted for publication in Frontiers in Neuromorphic Engineering. 23 pages, 9 figures", "summary": "This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms."}
{"id": "2601.03956", "pdf": "https://arxiv.org/pdf/2601.03956", "abs": "https://arxiv.org/abs/2601.03956", "authors": ["Kangjie Zhou", "Zhejia Wen", "Zhiyong Zhuo", "Zike Yan", "Pengying Wu", "Ieng Hou U", "Shuaiyang Li", "Han Gao", "Kang Ding", "Wenhan Cao", "Wei Pan", "Chang Liu"], "title": "CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM", "categories": ["cs.RO"], "comment": "17 pages, 13 figures", "summary": "Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\\% higher overall success rate and over 80\\% improvement in complex long-horizon scenarios compared to the best-performing baseline"}
{"id": "2601.04052", "pdf": "https://arxiv.org/pdf/2601.04052", "abs": "https://arxiv.org/abs/2601.04052", "authors": ["Zhihao Zhan", "Yuhao Chen", "Jiaying Zhou", "Qinhan Lv", "Hao Liu", "Keze Wang", "Liang Lin", "Guangrun Wang"], "title": "Stable Language Guidance for Vision-Language-Action Models", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \\textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \\textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \\textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations."}
{"id": "2601.04061", "pdf": "https://arxiv.org/pdf/2601.04061", "abs": "https://arxiv.org/abs/2601.04061", "authors": ["Chubin Zhang", "Jianan Wang", "Zifeng Gao", "Yue Su", "Tianru Dai", "Cai Zhou", "Jiwen Lu", "Yansong Tang"], "title": "CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://lin-shan.com/CLAP/", "summary": "Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/."}
{"id": "2601.04137", "pdf": "https://arxiv.org/pdf/2601.04137", "abs": "https://arxiv.org/abs/2601.04137", "authors": ["Chun-Kai Fan", "Xiaowei Chi", "Xiaozhu Ju", "Hao Li", "Yong Bao", "Yu-Kai Wang", "Lizhang Chen", "Zhiyuan Jiang", "Kuangzhi Ge", "Ying Li", "Weishi Mi", "Qingpo Wuwu", "Peidong Jia", "Yulin Luo", "Kevin Zhang", "Zhiyuan Qin", "Yong Dai", "Sirui Han", "Yike Guo", "Shanghang Zhang", "Jian Tang"], "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI."}
{"id": "2601.04177", "pdf": "https://arxiv.org/pdf/2601.04177", "abs": "https://arxiv.org/abs/2601.04177", "authors": ["Haoran Su"], "title": "Hierarchical GNN-Based Multi-Agent Learning for Dynamic Queue-Jump Lane and Emergency Vehicle Corridor Formation", "categories": ["cs.RO", "eess.SY"], "comment": "16 Pages, 5 Figures, 9 Tables, submitted to IEEE TITS", "summary": "Emergency vehicles require rapid passage through congested traffic, yet existing strategies fail to adapt to dynamic conditions. We propose a novel hierarchical graph neural network (GNN)-based multi-agent reinforcement learning framework to coordinate connected vehicles for emergency corridor formation. Our approach uses a high-level planner for global strategy and low-level controllers for trajectory execution, utilizing graph attention networks to scale with variable agent counts. Trained via Multi-Agent Proximal Policy Optimization (MAPPO), the system reduces emergency vehicle travel time by 28.3% compared to baselines and 44.6% compared to uncoordinated traffic in simulations. The design achieves near-zero collision rates (0.3%) while maintaining 81% of background traffic efficiency. Ablation and generalization studies confirm the framework's robustness across diverse scenarios. These results demonstrate the effectiveness of combining GNNs with hierarchical learning for intelligent transportation systems."}
{"id": "2601.04191", "pdf": "https://arxiv.org/pdf/2601.04191", "abs": "https://arxiv.org/abs/2601.04191", "authors": ["Negar Halakou", "Juan F. Gutierrez", "Ye Sun", "Han Jiang", "Xueming Wu", "Yilun Song", "Andres Gomez"], "title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms", "categories": ["cs.RO", "cs.AI"], "comment": "This is an open-access, author-archived version of a manuscript published in European Conference on Multi-Agent Systems 2025", "summary": "Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation."}
{"id": "2601.03302", "pdf": "https://arxiv.org/pdf/2601.03302", "abs": "https://arxiv.org/abs/2601.03302", "authors": ["Mohammad Rostami", "Atik Faysal", "Hongtao Xia", "Hadi Kasasbeh", "Ziang Gao", "Huaxia Wang"], "title": "CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models."}
{"id": "2601.03386", "pdf": "https://arxiv.org/pdf/2601.03386", "abs": "https://arxiv.org/abs/2601.03386", "authors": ["Zongyang Lv", "Yanmei Jia", "Yongqing Liu", "Alan F. Lynch", "Qing Zhao", "Yuhu Wu"], "title": "Modeling and Control for UAV with Off-center Slung Load", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) with slung load system is a classic air transportation system. In practical applications, the suspension point of the slung load does not always align with the center of mass (CoM) of the UAV due to mission requirements or mechanical interference. This offset creates coupling in the system's nonlinear dynamics which leads to a complicated motion control problem. In existing research, modeling of the system are performed about the UAV's CoM. In this work we use the point of suspension instead. Based on the new model, a cascade control strategy is developed. In the middle-loop controller, the acceleration of the suspension point is used to regulate the swing angle of the slung load without the need for considering the coupling between the slung load and the UAV. Using the off-center reference frame, an inner-loop controller is designed to track the UAV's attitude without the need of simplification on the coupling effects. We prove local exponential stability of the closed-loop using Lyapunov approach. Finally, simulations and experiments are conducted to validate the proposed control system."}
{"id": "2601.03520", "pdf": "https://arxiv.org/pdf/2601.03520", "abs": "https://arxiv.org/abs/2601.03520", "authors": ["Bekarys Dukenbaev", "Andrew Gerstenslager", "Alexander Johnson", "Ali A. Minai"], "title": "A Reinforcement Learning-Based Model for Mapping and Goal-Directed Navigation Using Multiscale Place Fields", "categories": ["cs.NE", "cs.AI", "cs.RO"], "comment": "11 pages, 8 figures. Submitted to IEEE Transactions on Cognitive and Developmental Systems", "summary": "Autonomous navigation in complex and partially observable environments remains a central challenge in robotics. Several bio-inspired models of mapping and navigation based on place cells in the mammalian hippocampus have been proposed. This paper introduces a new robust model that employs parallel layers of place fields at multiple spatial scales, a replay-based reward mechanism, and dynamic scale fusion. Simulations show that the model improves path efficiency and accelerates learning compared to single-scale baselines, highlighting the value of multiscale spatial representations for adaptive robot navigation."}
{"id": "2601.03617", "pdf": "https://arxiv.org/pdf/2601.03617", "abs": "https://arxiv.org/abs/2601.03617", "authors": ["Samson Oseiwe Ajadalu"], "title": "Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "7 pages, 4 figures", "summary": "Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection."}
{"id": "2601.03869", "pdf": "https://arxiv.org/pdf/2601.03869", "abs": "https://arxiv.org/abs/2601.03869", "authors": ["Arun Muthukkumar"], "title": "Bayesian Monocular Depth Refinement via Neural Radiance Fields", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "comment": "IEEE 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025). Oral presentation; Best Presenter Award", "summary": "Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset."}
{"id": "2601.04194", "pdf": "https://arxiv.org/pdf/2601.04194", "abs": "https://arxiv.org/abs/2601.04194", "authors": ["Yanzhe Lyu", "Chen Geng", "Karthik Dharmarajan", "Yunzhi Zhang", "Hadi Alzayer", "Shangzhe Wu", "Jiajun Wu"], "title": "Choreographing a World of Dynamic Objects", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": null, "summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord"}
